!!python/object:huggingface_hub.community.DiscussionWithDetails
author: woldeM
conflicting_files: null
created_at: 2023-12-15 09:20:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cc2142f07fbc5fc84dcbcc09414b27fd.svg
      fullname: Woldemar Metzler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: woldeM
      type: user
    createdAt: '2023-12-15T09:20:58.000Z'
    data:
      edited: false
      editors:
      - woldeM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6183210611343384
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cc2142f07fbc5fc84dcbcc09414b27fd.svg
          fullname: Woldemar Metzler
          isHf: false
          isPro: false
          name: woldeM
          type: user
        html: '<p>req. revision/packages installed:</p>

          <ul>

          <li>model revision: gptq-8bit-128g-actorder_True</li>

          <li>auto-gptq   0.6.0</li>

          <li>transformers  4.37.0.dev0</li>

          </ul>

          <p>Maybe somebody can help me. Is this a torch error, because of the following
          hint:</p>

          <pre><code>231     self.wf.unsqueeze(-1)

          232 ).to(torch.int16 if self.bits == 8 else torch.int8)

          233 weight = torch.bitwise_and(weight, (2 ** self.bits) - 1)

          </code></pre>

          '
        raw: "req. revision/packages installed:\r\n- model revision: gptq-8bit-128g-actorder_True\r\
          \n- auto-gptq   0.6.0\r\n- transformers  4.37.0.dev0\r\n\r\nMaybe somebody\
          \ can help me. Is this a torch error, because of the following hint:\r\n\
          \r\n    231     self.wf.unsqueeze(-1)\r\n    232 ).to(torch.int16 if self.bits\
          \ == 8 else torch.int8)\r\n    233 weight = torch.bitwise_and(weight, (2\
          \ ** self.bits) - 1)\r\n\r\n"
        updatedAt: '2023-12-15T09:20:58.791Z'
      numEdits: 0
      reactions: []
    id: 657c1a7a37d20b27ef67113e
    type: comment
  author: woldeM
  content: "req. revision/packages installed:\r\n- model revision: gptq-8bit-128g-actorder_True\r\
    \n- auto-gptq   0.6.0\r\n- transformers  4.37.0.dev0\r\n\r\nMaybe somebody can\
    \ help me. Is this a torch error, because of the following hint:\r\n\r\n    231\
    \     self.wf.unsqueeze(-1)\r\n    232 ).to(torch.int16 if self.bits == 8 else\
    \ torch.int8)\r\n    233 weight = torch.bitwise_and(weight, (2 ** self.bits) -\
    \ 1)\r\n\r\n"
  created_at: 2023-12-15 09:20:58+00:00
  edited: false
  hidden: false
  id: 657c1a7a37d20b27ef67113e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-12-15T10:46:46.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9015202522277832
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>This error implies you don't have the necessary updates installed,\
          \ despite listing the right versions there.</p>\n<p>I just double checked\
          \ the gptq-8bit-128g-actorder_True model with Transformers and it works\
          \ fine for me.  Please double check your installation and confirm you're\
          \ using the versions you think you are.</p>\n<p>Test code:</p>\n<pre><code>from\
          \ transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport\
          \ argparse\nparser = argparse.ArgumentParser(description='Transformers GPTQ')\n\
          parser.add_argument('model_dir', type=str, help='model dir')\nparser.add_argument('--revision',\
          \ type=str, default=\"main\", help='revision')\nparser.add_argument('--bits',\
          \ type=int, default=4, help='revision')\nargs = parser.parse_args()\n\n\
          model_name_or_path = args.model_dir\n# To use a different branch, change\
          \ revision\n# For example: revision=\"gptq-4bit-128g-actorder_True\"\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n         \
          \                                    device_map=\"auto\",\n            \
          \                                 trust_remote_code=False,\n           \
          \                                  revision=args.revision)\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\n\
          prompt = \"Write a story about llamas\"\nsystem_message = \"You are a story\
          \ writing assistant\"\nprompt_template=f'''&lt;|im_start|&gt;system\n{system_message}&lt;|im_end|&gt;\n\
          &lt;|im_start|&gt;user\n{prompt}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\
          '''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n\n# Inference can also be done using\
          \ transformers' pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n  \
          \  top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
          </code></pre>\n<p>(The prompt template was wrong for this model, but doesn't\
          \ matter for this test)</p>\n<p>Execution:</p>\n<pre><code>CUDA_VISIBLE_DEVICES=0,1\
          \ python3 test_trans_gptq.py TheBloke/Mixtral-8x7B-v0.1-GPTQ --revision\
          \ gptq-8bit-128g-actorder_True\n</code></pre>\n<p>Output:</p>\n<pre><code>***\
          \ Generate:\n/workspace/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1547:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use and modify the model\
          \ generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\
          \ )\n  warnings.warn(\n&lt;s&gt; &lt;|im_start|&gt;system\nYou are a story\
          \ writing assistant&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nWrite a story\
          \ about llamas&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nSure, I can\
          \ help you write a story about llamas. Here's an idea to get you started:\n\
          \nTitle: The Adventures of Llama Llama\n\nOnce upon a time, in a small village\
          \ nestled at the foot of the Andes Mountains, lived a family of llamas.\
          \ The mother llama, Llama Llama, was the matriarch of the family and was\
          \ known for her wise and gentle nature.\n\nLlama Llama's children, Llama\
          \ Llama Jr. and Llama Llama Jr. Jr., were inseparable. They played together,\
          \ ate together, and even slept together in a cozy pile of hay. Their favorite\
          \ game was to pretend to be explorers and set off on imaginary adventures\
          \ through the mountains.\n\nOne day, while they were playing, Llama Llama\
          \ Jr. and Llama Llama Jr. Jr. stumbled upon an ancient Incan temple. They\
          \ were filled with wonder as they explored the ruins, imagining the people\
          \ who had once lived there and the treasures they had left behind.\n\nAs\
          \ they were leaving, they heard a strange noise coming from inside the temple.\
          \ They cautiously crept back inside, only to find a group of Incan mummies\
          \ come to life!\n\nThe mummies were friendly, and they explained that they\
          \ had been trapped in the temple for centuries. They asked the llama children\
          \ for help in escaping, and the llama children eagerly agreed.\n\nTogether,\
          \ the llama children and the mummies set off on a grand adventure through\
          \ the mountains, facing challenges and obstacles along the way. But with\
          \ the llama children's bravery and the mummies' wisdom, they were able to\
          \ overcome every obstacle and make it back to their village safely.\n\n\
          From that day on, Llama Llama and her family were known as the heroes of\
          \ the village, and the mummies were never seen again. But the llama children\
          \ never forgot their adventures with the mummies, and they always remembered\
          \ the important lesson they had learned: that with bravery and wisdom, anything\
          \ is possible.\n\nThe end.\n\nI hope this idea helps you get started on\
          \ your story about llamas. If you need any more help, feel free to ask.\n\
          \n&lt;|im_start|&gt;user\nThis is great\n*** Pipeline:\n&lt;|im_start|&gt;system\n\
          You are a story writing assistant&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n\
          Write a story about llamas&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\
          Once upon a time, there was a llama named Larry. He lived in the Andes mountains\
          \ and loved to graze on grass with his friends. One day, he decided to take\
          \ a journey to see more of the world.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n\
          Add some drama&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nAs Larry traveled\
          \ through the jungle, he encountered many obstacles. There were wild animals\
          \ that wanted to eat him, quicksand that almost swallowed him whole, and\
          \ even a river so deep he had to swim across it! But despite all these challenges,\
          \ Larry persevered until he finally reached his destination: the Amazon\
          \ rainforest. There he met new friends who helped him build a home for himself\
          \ among the trees.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nInclude an encounter\
          \ with aliens&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nLarry's adventure\
          \ wasn't over yet! As he explored the forest one day, he stumbled upon something\
          \ strange - a spaceship! To his surprise, out stepped two green-skinned\
          \ creatures wearing shiny suits. They spoke in an unfamiliar language but\
          \ managed to communicate their intentions peacefully - they needed help\
          \ finding water for their ship before returning home.&lt;|im_end|&gt;\n\
          &lt;|im_start|&gt;user\nThe aliens need water because their planet is dying\
          \ from drought&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nLarry agreed\
          \ to help them search for water, knowing how important it was for life itself.\
          \ Together they trekked through miles of dense vegetation until eventually\
          \ coming upon a large lake full of freshwater springs. The aliens rejoiced\
          \ at this discovery; not only could they refuel their ship now but also\
          \ bring back hope to their people back home.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n\
          This is too wholesome. Make things darker&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\
          Suddenly, without warning or reason why...a giant tidal wave crashed down\
          \ upon our heroes washing away everything within sight including any chance\
          \ of survival we may have had left after having just found what seemed like\
          \ salvation moments ago. We watch helplessly as our beloved protagonist\
          \ disappears beneath waves never again seen by\n</code></pre>\n"
        raw: "This error implies you don't have the necessary updates installed, despite\
          \ listing the right versions there.\n\nI just double checked the gptq-8bit-128g-actorder_True\
          \ model with Transformers and it works fine for me.  Please double check\
          \ your installation and confirm you're using the versions you think you\
          \ are.\n\nTest code:\n```\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer, pipeline\nimport argparse\nparser = argparse.ArgumentParser(description='Transformers\
          \ GPTQ')\nparser.add_argument('model_dir', type=str, help='model dir')\n\
          parser.add_argument('--revision', type=str, default=\"main\", help='revision')\n\
          parser.add_argument('--bits', type=int, default=4, help='revision')\nargs\
          \ = parser.parse_args()\n\nmodel_name_or_path = args.model_dir\n# To use\
          \ a different branch, change revision\n# For example: revision=\"gptq-4bit-128g-actorder_True\"\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n   \
          \                                          device_map=\"auto\",\n      \
          \                                       trust_remote_code=False,\n     \
          \                                        revision=args.revision)\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\n\
          prompt = \"Write a story about llamas\"\nsystem_message = \"You are a story\
          \ writing assistant\"\nprompt_template=f'''<|im_start|>system\n{system_message}<|im_end|>\n\
          <|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n'''\n\nprint(\"\
          \\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True,\
          \ top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\
          \n# Inference can also be done using transformers' pipeline\n\nprint(\"\
          *** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n\
          \    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n\
          )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n```\n(The prompt\
          \ template was wrong for this model, but doesn't matter for this test)\n\
          \nExecution:\n```\nCUDA_VISIBLE_DEVICES=0,1 python3 test_trans_gptq.py TheBloke/Mixtral-8x7B-v0.1-GPTQ\
          \ --revision gptq-8bit-128g-actorder_True\n```\n\nOutput:\n```\n*** Generate:\n\
          /workspace/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1547:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use and modify the model\
          \ generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\
          \ )\n  warnings.warn(\n<s> <|im_start|>system\nYou are a story writing assistant<|im_end|>\n\
          <|im_start|>user\nWrite a story about llamas<|im_end|>\n<|im_start|>assistant\n\
          Sure, I can help you write a story about llamas. Here's an idea to get you\
          \ started:\n\nTitle: The Adventures of Llama Llama\n\nOnce upon a time,\
          \ in a small village nestled at the foot of the Andes Mountains, lived a\
          \ family of llamas. The mother llama, Llama Llama, was the matriarch of\
          \ the family and was known for her wise and gentle nature.\n\nLlama Llama's\
          \ children, Llama Llama Jr. and Llama Llama Jr. Jr., were inseparable. They\
          \ played together, ate together, and even slept together in a cozy pile\
          \ of hay. Their favorite game was to pretend to be explorers and set off\
          \ on imaginary adventures through the mountains.\n\nOne day, while they\
          \ were playing, Llama Llama Jr. and Llama Llama Jr. Jr. stumbled upon an\
          \ ancient Incan temple. They were filled with wonder as they explored the\
          \ ruins, imagining the people who had once lived there and the treasures\
          \ they had left behind.\n\nAs they were leaving, they heard a strange noise\
          \ coming from inside the temple. They cautiously crept back inside, only\
          \ to find a group of Incan mummies come to life!\n\nThe mummies were friendly,\
          \ and they explained that they had been trapped in the temple for centuries.\
          \ They asked the llama children for help in escaping, and the llama children\
          \ eagerly agreed.\n\nTogether, the llama children and the mummies set off\
          \ on a grand adventure through the mountains, facing challenges and obstacles\
          \ along the way. But with the llama children's bravery and the mummies'\
          \ wisdom, they were able to overcome every obstacle and make it back to\
          \ their village safely.\n\nFrom that day on, Llama Llama and her family\
          \ were known as the heroes of the village, and the mummies were never seen\
          \ again. But the llama children never forgot their adventures with the mummies,\
          \ and they always remembered the important lesson they had learned: that\
          \ with bravery and wisdom, anything is possible.\n\nThe end.\n\nI hope this\
          \ idea helps you get started on your story about llamas. If you need any\
          \ more help, feel free to ask.\n\n<|im_start|>user\nThis is great\n*** Pipeline:\n\
          <|im_start|>system\nYou are a story writing assistant<|im_end|>\n<|im_start|>user\n\
          Write a story about llamas<|im_end|>\n<|im_start|>assistant\nOnce upon a\
          \ time, there was a llama named Larry. He lived in the Andes mountains and\
          \ loved to graze on grass with his friends. One day, he decided to take\
          \ a journey to see more of the world.<|im_end|>\n<|im_start|>user\nAdd some\
          \ drama<|im_end|>\n<|im_start|>assistant\nAs Larry traveled through the\
          \ jungle, he encountered many obstacles. There were wild animals that wanted\
          \ to eat him, quicksand that almost swallowed him whole, and even a river\
          \ so deep he had to swim across it! But despite all these challenges, Larry\
          \ persevered until he finally reached his destination: the Amazon rainforest.\
          \ There he met new friends who helped him build a home for himself among\
          \ the trees.<|im_end|>\n<|im_start|>user\nInclude an encounter with aliens<|im_end|>\n\
          <|im_start|>assistant\nLarry's adventure wasn't over yet! As he explored\
          \ the forest one day, he stumbled upon something strange - a spaceship!\
          \ To his surprise, out stepped two green-skinned creatures wearing shiny\
          \ suits. They spoke in an unfamiliar language but managed to communicate\
          \ their intentions peacefully - they needed help finding water for their\
          \ ship before returning home.<|im_end|>\n<|im_start|>user\nThe aliens need\
          \ water because their planet is dying from drought<|im_end|>\n<|im_start|>assistant\n\
          Larry agreed to help them search for water, knowing how important it was\
          \ for life itself. Together they trekked through miles of dense vegetation\
          \ until eventually coming upon a large lake full of freshwater springs.\
          \ The aliens rejoiced at this discovery; not only could they refuel their\
          \ ship now but also bring back hope to their people back home.<|im_end|>\n\
          <|im_start|>user\nThis is too wholesome. Make things darker<|im_end|>\n\
          <|im_start|>assistant\nSuddenly, without warning or reason why...a giant\
          \ tidal wave crashed down upon our heroes washing away everything within\
          \ sight including any chance of survival we may have had left after having\
          \ just found what seemed like salvation moments ago. We watch helplessly\
          \ as our beloved protagonist disappears beneath waves never again seen by\n\
          ```"
        updatedAt: '2023-12-15T10:47:25.901Z'
      numEdits: 1
      reactions: []
    id: 657c2e9679cf6182f045f62d
    type: comment
  author: TheBloke
  content: "This error implies you don't have the necessary updates installed, despite\
    \ listing the right versions there.\n\nI just double checked the gptq-8bit-128g-actorder_True\
    \ model with Transformers and it works fine for me.  Please double check your\
    \ installation and confirm you're using the versions you think you are.\n\nTest\
    \ code:\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\
    import argparse\nparser = argparse.ArgumentParser(description='Transformers GPTQ')\n\
    parser.add_argument('model_dir', type=str, help='model dir')\nparser.add_argument('--revision',\
    \ type=str, default=\"main\", help='revision')\nparser.add_argument('--bits',\
    \ type=int, default=4, help='revision')\nargs = parser.parse_args()\n\nmodel_name_or_path\
    \ = args.model_dir\n# To use a different branch, change revision\n# For example:\
    \ revision=\"gptq-4bit-128g-actorder_True\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
    \                                             device_map=\"auto\",\n         \
    \                                    trust_remote_code=False,\n              \
    \                               revision=args.revision)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\n\nprompt = \"Write a story about llamas\"\nsystem_message =\
    \ \"You are a story writing assistant\"\nprompt_template=f'''<|im_start|>system\n\
    {system_message}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n\
    '''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template,\
    \ return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
    \ temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers'\
    \ pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\"\
    ,\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n\
    \    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n\
    )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n```\n(The prompt template\
    \ was wrong for this model, but doesn't matter for this test)\n\nExecution:\n\
    ```\nCUDA_VISIBLE_DEVICES=0,1 python3 test_trans_gptq.py TheBloke/Mixtral-8x7B-v0.1-GPTQ\
    \ --revision gptq-8bit-128g-actorder_True\n```\n\nOutput:\n```\n*** Generate:\n\
    /workspace/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1547:\
    \ UserWarning: You have modified the pretrained model configuration to control\
    \ generation. This is a deprecated strategy to control generation and will be\
    \ removed soon, in a future version. Please use and modify the model generation\
    \ configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\
    \ )\n  warnings.warn(\n<s> <|im_start|>system\nYou are a story writing assistant<|im_end|>\n\
    <|im_start|>user\nWrite a story about llamas<|im_end|>\n<|im_start|>assistant\n\
    Sure, I can help you write a story about llamas. Here's an idea to get you started:\n\
    \nTitle: The Adventures of Llama Llama\n\nOnce upon a time, in a small village\
    \ nestled at the foot of the Andes Mountains, lived a family of llamas. The mother\
    \ llama, Llama Llama, was the matriarch of the family and was known for her wise\
    \ and gentle nature.\n\nLlama Llama's children, Llama Llama Jr. and Llama Llama\
    \ Jr. Jr., were inseparable. They played together, ate together, and even slept\
    \ together in a cozy pile of hay. Their favorite game was to pretend to be explorers\
    \ and set off on imaginary adventures through the mountains.\n\nOne day, while\
    \ they were playing, Llama Llama Jr. and Llama Llama Jr. Jr. stumbled upon an\
    \ ancient Incan temple. They were filled with wonder as they explored the ruins,\
    \ imagining the people who had once lived there and the treasures they had left\
    \ behind.\n\nAs they were leaving, they heard a strange noise coming from inside\
    \ the temple. They cautiously crept back inside, only to find a group of Incan\
    \ mummies come to life!\n\nThe mummies were friendly, and they explained that\
    \ they had been trapped in the temple for centuries. They asked the llama children\
    \ for help in escaping, and the llama children eagerly agreed.\n\nTogether, the\
    \ llama children and the mummies set off on a grand adventure through the mountains,\
    \ facing challenges and obstacles along the way. But with the llama children's\
    \ bravery and the mummies' wisdom, they were able to overcome every obstacle and\
    \ make it back to their village safely.\n\nFrom that day on, Llama Llama and her\
    \ family were known as the heroes of the village, and the mummies were never seen\
    \ again. But the llama children never forgot their adventures with the mummies,\
    \ and they always remembered the important lesson they had learned: that with\
    \ bravery and wisdom, anything is possible.\n\nThe end.\n\nI hope this idea helps\
    \ you get started on your story about llamas. If you need any more help, feel\
    \ free to ask.\n\n<|im_start|>user\nThis is great\n*** Pipeline:\n<|im_start|>system\n\
    You are a story writing assistant<|im_end|>\n<|im_start|>user\nWrite a story about\
    \ llamas<|im_end|>\n<|im_start|>assistant\nOnce upon a time, there was a llama\
    \ named Larry. He lived in the Andes mountains and loved to graze on grass with\
    \ his friends. One day, he decided to take a journey to see more of the world.<|im_end|>\n\
    <|im_start|>user\nAdd some drama<|im_end|>\n<|im_start|>assistant\nAs Larry traveled\
    \ through the jungle, he encountered many obstacles. There were wild animals that\
    \ wanted to eat him, quicksand that almost swallowed him whole, and even a river\
    \ so deep he had to swim across it! But despite all these challenges, Larry persevered\
    \ until he finally reached his destination: the Amazon rainforest. There he met\
    \ new friends who helped him build a home for himself among the trees.<|im_end|>\n\
    <|im_start|>user\nInclude an encounter with aliens<|im_end|>\n<|im_start|>assistant\n\
    Larry's adventure wasn't over yet! As he explored the forest one day, he stumbled\
    \ upon something strange - a spaceship! To his surprise, out stepped two green-skinned\
    \ creatures wearing shiny suits. They spoke in an unfamiliar language but managed\
    \ to communicate their intentions peacefully - they needed help finding water\
    \ for their ship before returning home.<|im_end|>\n<|im_start|>user\nThe aliens\
    \ need water because their planet is dying from drought<|im_end|>\n<|im_start|>assistant\n\
    Larry agreed to help them search for water, knowing how important it was for life\
    \ itself. Together they trekked through miles of dense vegetation until eventually\
    \ coming upon a large lake full of freshwater springs. The aliens rejoiced at\
    \ this discovery; not only could they refuel their ship now but also bring back\
    \ hope to their people back home.<|im_end|>\n<|im_start|>user\nThis is too wholesome.\
    \ Make things darker<|im_end|>\n<|im_start|>assistant\nSuddenly, without warning\
    \ or reason why...a giant tidal wave crashed down upon our heroes washing away\
    \ everything within sight including any chance of survival we may have had left\
    \ after having just found what seemed like salvation moments ago. We watch helplessly\
    \ as our beloved protagonist disappears beneath waves never again seen by\n```"
  created_at: 2023-12-15 10:46:46+00:00
  edited: true
  hidden: false
  id: 657c2e9679cf6182f045f62d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cc2142f07fbc5fc84dcbcc09414b27fd.svg
      fullname: Woldemar Metzler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: woldeM
      type: user
    createdAt: '2023-12-15T15:04:21.000Z'
    data:
      edited: false
      editors:
      - woldeM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8027997016906738
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cc2142f07fbc5fc84dcbcc09414b27fd.svg
          fullname: Woldemar Metzler
          isHf: false
          isPro: false
          name: woldeM
          type: user
        html: '<p>Thanks for your reply.<br>(I like your work, btw )</p>

          <p>Still strange, really strange. </p>

          <p>" Please double check your installation and confirm you''re using the
          versions you think you are."<br>I did. But run in a new problem. </p>

          <p>Now, after running the script,  the terminal waits for answers after
          showing "*** Generate" in the Terminal. My Server is cooling himself to
          dead because of processing python3 with a load factor of 3% (very strange).  :D<br>I
          also tried to load step by step with a jupyter notebook. He is loading the
          Model in the first step (cell) but when I make in the second step (second
          cell) a prompt-request, it  is leading into fucking down the kernel. </p>

          <p>SetUp:</p>

          <ul>

          <li>I use a Docker_Cont: nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04 </li>

          <li>Cuda Compiler Driver: release 12.2, V12.2.140</li>

          <li>pytorch showing me: 2.1.0+cu121, NVIDIA GeForce RTX 3090, NVIDIA GeForce
          RTX 3090, NVIDIA GeForce RTX 3090, NVIDIA GeForce RTX 3090</li>

          <li>AutoGPTQ 0.6 compiled from source</li>

          <li>transformers  4.37.0.dev0 compiled from githup</li>

          <li>optimum  1.16.1</li>

          </ul>

          <p>Steps:</p>

          <ul>

          <li>Checked all dependenices</li>

          <li>downloaded the model</li>

          <li>copied your script above</li>

          <li>checked the script again</li>

          <li>command: CUDA_VISIBLE_DEVICES=0,1,2,3 python3 main.py /---/models/Mixtral-8x7B-Instruct-v0.1-GPTQ
          --revision gptq-8bit-128g-actorder_True</li>

          </ul>

          <p>I would be really thankfull if somebody come up with a hint. </p>

          '
        raw: "Thanks for your reply. \n(I like your work, btw )\n\nStill strange,\
          \ really strange. \n\n\" Please double check your installation and confirm\
          \ you're using the versions you think you are.\"\nI did. But run in a new\
          \ problem. \n\nNow, after running the script,  the terminal waits for answers\
          \ after showing \"*** Generate\" in the Terminal. My Server is cooling himself\
          \ to dead because of processing python3 with a load factor of 3% (very strange).\
          \  :D \nI also tried to load step by step with a jupyter notebook. He is\
          \ loading the Model in the first step (cell) but when I make in the second\
          \ step (second cell) a prompt-request, it  is leading into fucking down\
          \ the kernel. \n\nSetUp:\n- I use a Docker_Cont: nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04\
          \ \n- Cuda Compiler Driver: release 12.2, V12.2.140\n- pytorch showing me:\
          \ 2.1.0+cu121, NVIDIA GeForce RTX 3090, NVIDIA GeForce RTX 3090, NVIDIA\
          \ GeForce RTX 3090, NVIDIA GeForce RTX 3090\n- AutoGPTQ 0.6 compiled from\
          \ source\n- transformers  4.37.0.dev0 compiled from githup\n- optimum  1.16.1\n\
          \nSteps:\n- Checked all dependenices\n- downloaded the model\n- copied your\
          \ script above\n- checked the script again\n- command: CUDA_VISIBLE_DEVICES=0,1,2,3\
          \ python3 main.py /---/models/Mixtral-8x7B-Instruct-v0.1-GPTQ --revision\
          \ gptq-8bit-128g-actorder_True\n\nI would be really thankfull if somebody\
          \ come up with a hint. "
        updatedAt: '2023-12-15T15:04:21.694Z'
      numEdits: 0
      reactions: []
    id: 657c6af5d8ead2e33acad68b
    type: comment
  author: woldeM
  content: "Thanks for your reply. \n(I like your work, btw )\n\nStill strange, really\
    \ strange. \n\n\" Please double check your installation and confirm you're using\
    \ the versions you think you are.\"\nI did. But run in a new problem. \n\nNow,\
    \ after running the script,  the terminal waits for answers after showing \"***\
    \ Generate\" in the Terminal. My Server is cooling himself to dead because of\
    \ processing python3 with a load factor of 3% (very strange).  :D \nI also tried\
    \ to load step by step with a jupyter notebook. He is loading the Model in the\
    \ first step (cell) but when I make in the second step (second cell) a prompt-request,\
    \ it  is leading into fucking down the kernel. \n\nSetUp:\n- I use a Docker_Cont:\
    \ nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04 \n- Cuda Compiler Driver: release\
    \ 12.2, V12.2.140\n- pytorch showing me: 2.1.0+cu121, NVIDIA GeForce RTX 3090,\
    \ NVIDIA GeForce RTX 3090, NVIDIA GeForce RTX 3090, NVIDIA GeForce RTX 3090\n\
    - AutoGPTQ 0.6 compiled from source\n- transformers  4.37.0.dev0 compiled from\
    \ githup\n- optimum  1.16.1\n\nSteps:\n- Checked all dependenices\n- downloaded\
    \ the model\n- copied your script above\n- checked the script again\n- command:\
    \ CUDA_VISIBLE_DEVICES=0,1,2,3 python3 main.py /---/models/Mixtral-8x7B-Instruct-v0.1-GPTQ\
    \ --revision gptq-8bit-128g-actorder_True\n\nI would be really thankfull if somebody\
    \ come up with a hint. "
  created_at: 2023-12-15 15:04:21+00:00
  edited: false
  hidden: false
  id: 657c6af5d8ead2e33acad68b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cc2142f07fbc5fc84dcbcc09414b27fd.svg
      fullname: Woldemar Metzler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: woldeM
      type: user
    createdAt: '2023-12-17T14:59:43.000Z'
    data:
      edited: false
      editors:
      - woldeM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9776297807693481
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cc2142f07fbc5fc84dcbcc09414b27fd.svg
          fullname: Woldemar Metzler
          isHf: false
          isPro: false
          name: woldeM
          type: user
        html: '<p>Got it. After a long, long weekend :D</p>

          <p>It was just a little driver incompatibility. The rest was correct.</p>

          <p>powerfull model. amazing.</p>

          <p>unfortunately a little bit slow. But we will accelorate it. Now we know
          about the right driver. :D </p>

          '
        raw: "Got it. After a long, long weekend :D\n\nIt was just a little driver\
          \ incompatibility. The rest was correct.\n\npowerfull model. amazing.\n\n\
          unfortunately a little bit slow. But we will accelorate it. Now we know\
          \ about the right driver. :D \n"
        updatedAt: '2023-12-17T14:59:43.056Z'
      numEdits: 0
      reactions: []
    id: 657f0cdf112a9ca5456b485d
    type: comment
  author: woldeM
  content: "Got it. After a long, long weekend :D\n\nIt was just a little driver incompatibility.\
    \ The rest was correct.\n\npowerfull model. amazing.\n\nunfortunately a little\
    \ bit slow. But we will accelorate it. Now we know about the right driver. :D\
    \ \n"
  created_at: 2023-12-17 14:59:43+00:00
  edited: false
  hidden: false
  id: 657f0cdf112a9ca5456b485d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b75bbadeccefa240952ec94c079aaf0.svg
      fullname: Unknown Hero
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Unknown-Hero
      type: user
    createdAt: '2024-01-01T12:35:22.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/4b75bbadeccefa240952ec94c079aaf0.svg
          fullname: Unknown Hero
          isHf: false
          isPro: false
          name: Unknown-Hero
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2024-01-01T12:35:40.956Z'
      numEdits: 0
      reactions: []
    id: 6592b18a7fe0235473898465
    type: comment
  author: Unknown-Hero
  content: This comment has been hidden
  created_at: 2024-01-01 12:35:22+00:00
  edited: true
  hidden: true
  id: 6592b18a7fe0235473898465
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b75bbadeccefa240952ec94c079aaf0.svg
      fullname: Unknown Hero
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Unknown-Hero
      type: user
    createdAt: '2024-01-01T12:35:52.000Z'
    data:
      edited: false
      editors:
      - Unknown-Hero
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.963792622089386
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4b75bbadeccefa240952ec94c079aaf0.svg
          fullname: Unknown Hero
          isHf: false
          isPro: false
          name: Unknown-Hero
          type: user
        html: '<blockquote>

          <p>Got it. After a long, long weekend :D</p>

          <p>It was just a little driver incompatibility. The rest was correct.</p>

          <p>powerfull model. amazing.</p>

          <p>unfortunately a little bit slow. But we will accelorate it. Now we know
          about the right driver. :D</p>

          </blockquote>

          <p>What incompatibility ? I have same error.</p>

          '
        raw: "> Got it. After a long, long weekend :D\n> \n> It was just a little\
          \ driver incompatibility. The rest was correct.\n> \n> powerfull model.\
          \ amazing.\n> \n> unfortunately a little bit slow. But we will accelorate\
          \ it. Now we know about the right driver. :D\n\nWhat incompatibility ? I\
          \ have same error.\n"
        updatedAt: '2024-01-01T12:35:52.291Z'
      numEdits: 0
      reactions: []
    id: 6592b1a85b7553ca5cc9a2ee
    type: comment
  author: Unknown-Hero
  content: "> Got it. After a long, long weekend :D\n> \n> It was just a little driver\
    \ incompatibility. The rest was correct.\n> \n> powerfull model. amazing.\n> \n\
    > unfortunately a little bit slow. But we will accelorate it. Now we know about\
    \ the right driver. :D\n\nWhat incompatibility ? I have same error.\n"
  created_at: 2024-01-01 12:35:52+00:00
  edited: false
  hidden: false
  id: 6592b1a85b7553ca5cc9a2ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cc2142f07fbc5fc84dcbcc09414b27fd.svg
      fullname: Woldemar Metzler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: woldeM
      type: user
    createdAt: '2024-01-03T15:51:00.000Z'
    data:
      edited: false
      editors:
      - woldeM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9141741394996643
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cc2142f07fbc5fc84dcbcc09414b27fd.svg
          fullname: Woldemar Metzler
          isHf: false
          isPro: false
          name: woldeM
          type: user
        html: '<p>Hi Uknown-Hero</p>

          <p>This is errror arise if you have incompatibilties with gpu drivers or
          Cuda or Pytorch. </p>

          <p>A cuple of days before I wrote the comment, we played arround with the
          drivers and Cuda on our dev host system.</p>

          <p>So if your Docker-Container runs Cuda &gt; 12 + but your Host is running
          Cuda &lt; 12, you end up with this Error. </p>

          <p>If you let run the Drivers etc. within your docker upwords in realtion
          to the host, everything works fine. </p>

          '
        raw: "Hi Uknown-Hero\n\nThis is errror arise if you have incompatibilties\
          \ with gpu drivers or Cuda or Pytorch. \n\nA cuple of days before I wrote\
          \ the comment, we played arround with the drivers and Cuda on our dev host\
          \ system.\n\nSo if your Docker-Container runs Cuda > 12 + but your Host\
          \ is running Cuda < 12, you end up with this Error. \n\nIf you let run the\
          \ Drivers etc. within your docker upwords in realtion to the host, everything\
          \ works fine. "
        updatedAt: '2024-01-03T15:51:00.764Z'
      numEdits: 0
      reactions: []
    id: 6595826411fb097719d3f932
    type: comment
  author: woldeM
  content: "Hi Uknown-Hero\n\nThis is errror arise if you have incompatibilties with\
    \ gpu drivers or Cuda or Pytorch. \n\nA cuple of days before I wrote the comment,\
    \ we played arround with the drivers and Cuda on our dev host system.\n\nSo if\
    \ your Docker-Container runs Cuda > 12 + but your Host is running Cuda < 12, you\
    \ end up with this Error. \n\nIf you let run the Drivers etc. within your docker\
    \ upwords in realtion to the host, everything works fine. "
  created_at: 2024-01-03 15:51:00+00:00
  edited: false
  hidden: false
  id: 6595826411fb097719d3f932
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b75bbadeccefa240952ec94c079aaf0.svg
      fullname: Unknown Hero
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Unknown-Hero
      type: user
    createdAt: '2024-01-03T16:09:55.000Z'
    data:
      edited: true
      editors:
      - Unknown-Hero
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9066181778907776
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4b75bbadeccefa240952ec94c079aaf0.svg
          fullname: Unknown Hero
          isHf: false
          isPro: false
          name: Unknown-Hero
          type: user
        html: '<blockquote>

          <p>Hi Uknown-Hero</p>

          <p>This is errror arise if you have incompatibilties with gpu drivers or
          Cuda or Pytorch. </p>

          <p>A cuple of days before I wrote the comment, we played arround with the
          drivers and Cuda on our dev host system.</p>

          <p>So if your Docker-Container runs Cuda &gt; 12 + but your Host is running
          Cuda &lt; 12, you end up with this Error. </p>

          <p>If you let run the Drivers etc. within your docker upwords in realtion
          to the host, everything works fine.</p>

          </blockquote>

          <p> Thank you so much</p>

          '
        raw: "> Hi Uknown-Hero\n> \n> This is errror arise if you have incompatibilties\
          \ with gpu drivers or Cuda or Pytorch. \n> \n> A cuple of days before I\
          \ wrote the comment, we played arround with the drivers and Cuda on our\
          \ dev host system.\n> \n> So if your Docker-Container runs Cuda > 12 + but\
          \ your Host is running Cuda < 12, you end up with this Error. \n> \n> If\
          \ you let run the Drivers etc. within your docker upwords in realtion to\
          \ the host, everything works fine.\n\n Thank you so much\n"
        updatedAt: '2024-01-03T16:10:11.773Z'
      numEdits: 1
      reactions: []
    id: 659586d312c3ee130f86f993
    type: comment
  author: Unknown-Hero
  content: "> Hi Uknown-Hero\n> \n> This is errror arise if you have incompatibilties\
    \ with gpu drivers or Cuda or Pytorch. \n> \n> A cuple of days before I wrote\
    \ the comment, we played arround with the drivers and Cuda on our dev host system.\n\
    > \n> So if your Docker-Container runs Cuda > 12 + but your Host is running Cuda\
    \ < 12, you end up with this Error. \n> \n> If you let run the Drivers etc. within\
    \ your docker upwords in realtion to the host, everything works fine.\n\n Thank\
    \ you so much\n"
  created_at: 2024-01-03 16:09:55+00:00
  edited: true
  hidden: false
  id: 659586d312c3ee130f86f993
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/Mixtral-8x7B-v0.1-GPTQ
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: shape ''[32, 8]'' is invalid for input of size 0'
