!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YanaS
conflicting_files: null
created_at: 2023-09-13 18:39:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3a7b00385f85338c009de202174ef7c.svg
      fullname: Yana Stamenova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YanaS
      type: user
    createdAt: '2023-09-13T19:39:54.000Z'
    data:
      edited: false
      editors:
      - YanaS
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8677430152893066
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3a7b00385f85338c009de202174ef7c.svg
          fullname: Yana Stamenova
          isHf: false
          isPro: false
          name: YanaS
          type: user
        html: '<p>Hi, I am trying to quantize a model, and I see you have achieved
          it. So, could you share the process? I downloaded the model and llama.cpp.<br>Then
          I move the model to the model''s folder of llama.cpp and run convert.py
          file. So I get the gguf file. But then I run<br>!python /llama.cpp/examples/quantize
          models/[model-folder]/[model]-f32.gguf /models/llama2-bg/[model]-q4_0.bin
          q4_0</p>

          <p>and I get error: /usr/bin/python3: can''t find ''<strong>main</strong>''
          module in ''/content/llama.cpp/examples/quantize''</p>

          <p>I would highly appreciate it if you could help me with this.</p>

          '
        raw: "Hi, I am trying to quantize a model, and I see you have achieved it.\
          \ So, could you share the process? I downloaded the model and llama.cpp.\r\
          \nThen I move the model to the model's folder of llama.cpp and run convert.py\
          \ file. So I get the gguf file. But then I run \r\n!python /llama.cpp/examples/quantize\
          \ models/[model-folder]/[model]-f32.gguf /models/llama2-bg/[model]-q4_0.bin\
          \ q4_0\r\n\r\nand I get error: /usr/bin/python3: can't find '__main__' module\
          \ in '/content/llama.cpp/examples/quantize'\r\n\r\nI would highly appreciate\
          \ it if you could help me with this."
        updatedAt: '2023-09-13T19:39:54.807Z'
      numEdits: 0
      reactions: []
    id: 6502100a25fd8025a01db764
    type: comment
  author: YanaS
  content: "Hi, I am trying to quantize a model, and I see you have achieved it. So,\
    \ could you share the process? I downloaded the model and llama.cpp.\r\nThen I\
    \ move the model to the model's folder of llama.cpp and run convert.py file. So\
    \ I get the gguf file. But then I run \r\n!python /llama.cpp/examples/quantize\
    \ models/[model-folder]/[model]-f32.gguf /models/llama2-bg/[model]-q4_0.bin q4_0\r\
    \n\r\nand I get error: /usr/bin/python3: can't find '__main__' module in '/content/llama.cpp/examples/quantize'\r\
    \n\r\nI would highly appreciate it if you could help me with this."
  created_at: 2023-09-13 18:39:54+00:00
  edited: false
  hidden: false
  id: 6502100a25fd8025a01db764
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fefa33288c15fe180782a6e0a4433862.svg
      fullname: Abraham Munonoka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abrahamn
      type: user
    createdAt: '2023-09-14T07:19:29.000Z'
    data:
      edited: false
      editors:
      - abrahamn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8962792754173279
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fefa33288c15fe180782a6e0a4433862.svg
          fullname: Abraham Munonoka
          isHf: false
          isPro: false
          name: abrahamn
          type: user
        html: '<p>You might have better luck by downloading the llamacpp package from
          github releases, extract the one that matches your arch then run the quantize
          from there. What I usually do once I have the built package is $ ./quantize
          "path/to/model.gguf" "path/to/new_model.gguf" q4_0</p>

          '
        raw: You might have better luck by downloading the llamacpp package from github
          releases, extract the one that matches your arch then run the quantize from
          there. What I usually do once I have the built package is $ ./quantize "path/to/model.gguf"
          "path/to/new_model.gguf" q4_0
        updatedAt: '2023-09-14T07:19:29.902Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - YanaS
    id: 6502b4015b67fa2846dda78f
    type: comment
  author: abrahamn
  content: You might have better luck by downloading the llamacpp package from github
    releases, extract the one that matches your arch then run the quantize from there.
    What I usually do once I have the built package is $ ./quantize "path/to/model.gguf"
    "path/to/new_model.gguf" q4_0
  created_at: 2023-09-14 06:19:29+00:00
  edited: false
  hidden: false
  id: 6502b4015b67fa2846dda78f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3a7b00385f85338c009de202174ef7c.svg
      fullname: Yana Stamenova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YanaS
      type: user
    createdAt: '2023-09-14T07:53:45.000Z'
    data:
      edited: false
      editors:
      - YanaS
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8044193983078003
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3a7b00385f85338c009de202174ef7c.svg
          fullname: Yana Stamenova
          isHf: false
          isPro: false
          name: YanaS
          type: user
        html: '<p>I do this in Google Colab:<br>!git clone <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a><br>!cd
          llama.cpp &amp;&amp; git pull &amp;&amp; make clean &amp;&amp; LLAMA_CUBLAS=1
          make<br>!pip install -r llama.cpp/requirements.txt<br>Is it possible I have
          to explicitly update llama.cpp after cloning? Or, maybe in Colab there is
          some other issue I don''t notice?</p>

          '
        raw: 'I do this in Google Colab:

          !git clone https://github.com/ggerganov/llama.cpp

          !cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make

          !pip install -r llama.cpp/requirements.txt

          Is it possible I have to explicitly update llama.cpp after cloning? Or,
          maybe in Colab there is some other issue I don''t notice?'
        updatedAt: '2023-09-14T07:53:45.478Z'
      numEdits: 0
      reactions: []
    id: 6502bc09f10502379a845394
    type: comment
  author: YanaS
  content: 'I do this in Google Colab:

    !git clone https://github.com/ggerganov/llama.cpp

    !cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make

    !pip install -r llama.cpp/requirements.txt

    Is it possible I have to explicitly update llama.cpp after cloning? Or, maybe
    in Colab there is some other issue I don''t notice?'
  created_at: 2023-09-14 06:53:45+00:00
  edited: false
  hidden: false
  id: 6502bc09f10502379a845394
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/CodeLlama-34B-GGUF
repo_type: model
status: open
target_branch: null
title: GGUF quantize
