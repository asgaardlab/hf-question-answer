!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Goldenblood56
conflicting_files: null
created_at: 2023-05-08 01:05:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-05-08T02:05:38.000Z'
    data:
      edited: false
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Windows 10<br>RTX 4080 using fully updated Oobabooga at time of
          posting this. </p>

          <p>Arguments command line<br>"call python server.py --auto-devices --chat
          --trust-remote-code --model OccamRazor_mpt-7b-storywriter-4bit-128g --wbits
          4 --groupsize 128 --model_type llama "</p>

          <p>Error </p>

          <p>Starting the web UI...<br>INFO:Gradio HTTP request redirected to localhost
          :)<br>WARNING:trust_remote_code is enabled. This is dangerous.<br>INFO:Loading
          OccamRazor_mpt-7b-storywriter-4bit-128g...<br>INFO:Found the following quantized
          model: models\OccamRazor_mpt-7b-storywriter-4bit-128g\model.safetensors<br>Traceback
          (most recent call last):<br>  File "C:\AI\oobabooga-windowsBest\text-generation-webui\server.py",
          line 872, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "C:\AI\oobabooga-windowsBest\text-generation-webui\modules\models.py", line
          159, in load_model<br>    model = load_quantized(model_name)<br>  File "C:\AI\oobabooga-windowsBest\text-generation-webui\modules\GPTQ_loader.py",
          line 179, in load_quantized<br>    model = load_quant(str(path_to_model),
          str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)<br>  File
          "C:\AI\oobabooga-windowsBest\text-generation-webui\modules\GPTQ_loader.py",
          line 45, in <em>load_quant<br>    model = AutoModelForCausalLM.from_config(config,
          trust_remote_code=shared.args.trust_remote_code)<br>  File "C:\AI\oobabooga-windowsBest\installer_files\env\lib\site-packages\transformers\models\auto\auto_factory.py",
          line 407, in from_config<br>    model_class = get_class_from_dynamic_module(config.name_or_path,
          module_file + ".py", class_name, **kwargs)<br>  File "C:\AI\oobabooga-windowsBest\installer_files\env\lib\site-packages\transformers\dynamic_module_utils.py",
          line 388, in get_class_from_dynamic_module<br>    return get_class_in_module(class_name,
          final_module.replace(".py", ""))<br>  File "C:\AI\oobabooga-windowsBest\installer_files\env\lib\site-packages\transformers\dynamic_module_utils.py",
          line 157, in get_class_in_module<br>    module = importlib.import_module(module_path)<br>  File
          "C:\AI\oobabooga-windowsBest\installer_files\env\lib\importlib_<em>init</em></em>.py",
          line 126, in import_module<br>    return _bootstrap._gcd_import(name[level:],
          package, level)<br>  File "", line 1050, in _gcd_import<br>  File "", line
          1027, in _find_and_load<br>  File "", line 1006, in _find_and_load_unlocked<br>  File
          "", line 688, in _load_unlocked<br>  File "", line 883, in exec_module<br>  File
          "", line 241, in _call_with_frames_removed<br>  File "C:\Users\Mainuser/.cache\huggingface\modules\transformers_modules\OccamRazor_mpt-7b-storywriter-4bit-128g\modeling_mpt.py",
          line 13, in <br>    from .attention import attn_bias_shape, build_attn_bias<br>  File
          "C:\Users\Mainuser/.cache\huggingface\modules\transformers_modules\OccamRazor_mpt-7b-storywriter-4bit-128g\attention.py",
          line 7, in <br>    from einops import rearrange<br>ModuleNotFoundError:
          No module named ''einops''<br>Press any key to continue . . .</p>

          <p>What I have tried</p>

          <ol>

          <li>pip install einops<br>Here is what I got.<br>C:\AI\oobabooga-windowsBest\text-generation-webui\models\OccamRazor_mpt-7b-storywriter-4bit-128g&gt;pip
          install einops<br>Requirement already satisfied: einops in c:\users\Mainuser\appdata\local\programs\python\python310\lib\site-packages
          (0.6.1)</li>

          </ol>

          <p>It still does not work? Anyone have an suggestions? Thanks</p>

          '
        raw: "Windows 10\r\nRTX 4080 using fully updated Oobabooga at time of posting\
          \ this. \r\n\r\nArguments command line\r\n\"call python server.py --auto-devices\
          \ --chat --trust-remote-code --model OccamRazor_mpt-7b-storywriter-4bit-128g\
          \ --wbits 4 --groupsize 128 --model_type llama \"\r\n\r\nError \r\n\r\n\
          Starting the web UI...\r\nINFO:Gradio HTTP request redirected to localhost\
          \ :)\r\nWARNING:trust_remote_code is enabled. This is dangerous.\r\nINFO:Loading\
          \ OccamRazor_mpt-7b-storywriter-4bit-128g...\r\nINFO:Found the following\
          \ quantized model: models\\OccamRazor_mpt-7b-storywriter-4bit-128g\\model.safetensors\r\
          \nTraceback (most recent call last):\r\n  File \"C:\\AI\\oobabooga-windowsBest\\\
          text-generation-webui\\server.py\", line 872, in <module>\r\n    shared.model,\
          \ shared.tokenizer = load_model(shared.model_name)\r\n  File \"C:\\AI\\\
          oobabooga-windowsBest\\text-generation-webui\\modules\\models.py\", line\
          \ 159, in load_model\r\n    model = load_quantized(model_name)\r\n  File\
          \ \"C:\\AI\\oobabooga-windowsBest\\text-generation-webui\\modules\\GPTQ_loader.py\"\
          , line 179, in load_quantized\r\n    model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\r\
          \n  File \"C:\\AI\\oobabooga-windowsBest\\text-generation-webui\\modules\\\
          GPTQ_loader.py\", line 45, in _load_quant\r\n    model = AutoModelForCausalLM.from_config(config,\
          \ trust_remote_code=shared.args.trust_remote_code)\r\n  File \"C:\\AI\\\
          oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\auto\\auto_factory.py\", line 407, in from_config\r\n    model_class\
          \ = get_class_from_dynamic_module(config.name_or_path, module_file + \"\
          .py\", class_name, **kwargs)\r\n  File \"C:\\AI\\oobabooga-windowsBest\\\
          installer_files\\env\\lib\\site-packages\\transformers\\dynamic_module_utils.py\"\
          , line 388, in get_class_from_dynamic_module\r\n    return get_class_in_module(class_name,\
          \ final_module.replace(\".py\", \"\"))\r\n  File \"C:\\AI\\oobabooga-windowsBest\\\
          installer_files\\env\\lib\\site-packages\\transformers\\dynamic_module_utils.py\"\
          , line 157, in get_class_in_module\r\n    module = importlib.import_module(module_path)\r\
          \n  File \"C:\\AI\\oobabooga-windowsBest\\installer_files\\env\\lib\\importlib\\\
          __init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:],\
          \ package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050,\
          \ in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027,\
          \ in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006,\
          \ in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\"\
          , line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\"\
          , line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\",\
          \ line 241, in _call_with_frames_removed\r\n  File \"C:\\Users\\Mainuser/.cache\\\
          huggingface\\modules\\transformers_modules\\OccamRazor_mpt-7b-storywriter-4bit-128g\\\
          modeling_mpt.py\", line 13, in <module>\r\n    from .attention import attn_bias_shape,\
          \ build_attn_bias\r\n  File \"C:\\Users\\Mainuser/.cache\\huggingface\\\
          modules\\transformers_modules\\OccamRazor_mpt-7b-storywriter-4bit-128g\\\
          attention.py\", line 7, in <module>\r\n    from einops import rearrange\r\
          \nModuleNotFoundError: No module named 'einops'\r\nPress any key to continue\
          \ . . .\r\n\r\nWhat I have tried\r\n\r\n1. pip install einops\r\nHere is\
          \ what I got. \r\nC:\\AI\\oobabooga-windowsBest\\text-generation-webui\\\
          models\\OccamRazor_mpt-7b-storywriter-4bit-128g>pip install einops\r\nRequirement\
          \ already satisfied: einops in c:\\users\\Mainuser\\appdata\\local\\programs\\\
          python\\python310\\lib\\site-packages (0.6.1)\r\n\r\nIt still does not work?\
          \ Anyone have an suggestions? Thanks"
        updatedAt: '2023-05-08T02:05:38.765Z'
      numEdits: 0
      reactions: []
    id: 645858f2ead43697df2c54ab
    type: comment
  author: Goldenblood56
  content: "Windows 10\r\nRTX 4080 using fully updated Oobabooga at time of posting\
    \ this. \r\n\r\nArguments command line\r\n\"call python server.py --auto-devices\
    \ --chat --trust-remote-code --model OccamRazor_mpt-7b-storywriter-4bit-128g --wbits\
    \ 4 --groupsize 128 --model_type llama \"\r\n\r\nError \r\n\r\nStarting the web\
    \ UI...\r\nINFO:Gradio HTTP request redirected to localhost :)\r\nWARNING:trust_remote_code\
    \ is enabled. This is dangerous.\r\nINFO:Loading OccamRazor_mpt-7b-storywriter-4bit-128g...\r\
    \nINFO:Found the following quantized model: models\\OccamRazor_mpt-7b-storywriter-4bit-128g\\\
    model.safetensors\r\nTraceback (most recent call last):\r\n  File \"C:\\AI\\oobabooga-windowsBest\\\
    text-generation-webui\\server.py\", line 872, in <module>\r\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name)\r\n  File \"C:\\AI\\oobabooga-windowsBest\\\
    text-generation-webui\\modules\\models.py\", line 159, in load_model\r\n    model\
    \ = load_quantized(model_name)\r\n  File \"C:\\AI\\oobabooga-windowsBest\\text-generation-webui\\\
    modules\\GPTQ_loader.py\", line 179, in load_quantized\r\n    model = load_quant(str(path_to_model),\
    \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\r\
    \n  File \"C:\\AI\\oobabooga-windowsBest\\text-generation-webui\\modules\\GPTQ_loader.py\"\
    , line 45, in _load_quant\r\n    model = AutoModelForCausalLM.from_config(config,\
    \ trust_remote_code=shared.args.trust_remote_code)\r\n  File \"C:\\AI\\oobabooga-windowsBest\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\"\
    , line 407, in from_config\r\n    model_class = get_class_from_dynamic_module(config.name_or_path,\
    \ module_file + \".py\", class_name, **kwargs)\r\n  File \"C:\\AI\\oobabooga-windowsBest\\\
    installer_files\\env\\lib\\site-packages\\transformers\\dynamic_module_utils.py\"\
    , line 388, in get_class_from_dynamic_module\r\n    return get_class_in_module(class_name,\
    \ final_module.replace(\".py\", \"\"))\r\n  File \"C:\\AI\\oobabooga-windowsBest\\\
    installer_files\\env\\lib\\site-packages\\transformers\\dynamic_module_utils.py\"\
    , line 157, in get_class_in_module\r\n    module = importlib.import_module(module_path)\r\
    \n  File \"C:\\AI\\oobabooga-windowsBest\\installer_files\\env\\lib\\importlib\\\
    __init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:],\
    \ package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\
    \n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n \
    \ File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\
    \n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File\
    \ \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File\
    \ \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n\
    \  File \"C:\\Users\\Mainuser/.cache\\huggingface\\modules\\transformers_modules\\\
    OccamRazor_mpt-7b-storywriter-4bit-128g\\modeling_mpt.py\", line 13, in <module>\r\
    \n    from .attention import attn_bias_shape, build_attn_bias\r\n  File \"C:\\\
    Users\\Mainuser/.cache\\huggingface\\modules\\transformers_modules\\OccamRazor_mpt-7b-storywriter-4bit-128g\\\
    attention.py\", line 7, in <module>\r\n    from einops import rearrange\r\nModuleNotFoundError:\
    \ No module named 'einops'\r\nPress any key to continue . . .\r\n\r\nWhat I have\
    \ tried\r\n\r\n1. pip install einops\r\nHere is what I got. \r\nC:\\AI\\oobabooga-windowsBest\\\
    text-generation-webui\\models\\OccamRazor_mpt-7b-storywriter-4bit-128g>pip install\
    \ einops\r\nRequirement already satisfied: einops in c:\\users\\Mainuser\\appdata\\\
    local\\programs\\python\\python310\\lib\\site-packages (0.6.1)\r\n\r\nIt still\
    \ does not work? Anyone have an suggestions? Thanks"
  created_at: 2023-05-08 01:05:38+00:00
  edited: false
  hidden: false
  id: 645858f2ead43697df2c54ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24f25375b46eaa9f51a52748a2cc35a5.svg
      fullname: DIOGO FERNANDES ELVAS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: max14354
      type: user
    createdAt: '2023-05-08T04:30:17.000Z'
    data:
      edited: true
      editors:
      - max14354
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24f25375b46eaa9f51a52748a2cc35a5.svg
          fullname: DIOGO FERNANDES ELVAS
          isHf: false
          isPro: false
          name: max14354
          type: user
        html: '<p>Are you sure you pip installed in the correct environment? running
          cmd_windows.bat or cmd_linux.hs will take you to the correct place</p>

          '
        raw: Are you sure you pip installed in the correct environment? running cmd_windows.bat
          or cmd_linux.hs will take you to the correct place
        updatedAt: '2023-05-08T04:31:28.186Z'
      numEdits: 1
      reactions: []
    id: 64587ad95fc3b8a21ead7e12
    type: comment
  author: max14354
  content: Are you sure you pip installed in the correct environment? running cmd_windows.bat
    or cmd_linux.hs will take you to the correct place
  created_at: 2023-05-08 03:30:17+00:00
  edited: true
  hidden: false
  id: 64587ad95fc3b8a21ead7e12
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663145520122-noauth.png?w=200&h=200&f=face
      fullname: J. Curwen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Praytheomnissiah
      type: user
    createdAt: '2023-05-08T07:39:30.000Z'
    data:
      edited: true
      editors:
      - Praytheomnissiah
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663145520122-noauth.png?w=200&h=200&f=face
          fullname: J. Curwen
          isHf: false
          isPro: false
          name: Praytheomnissiah
          type: user
        html: '<p>Same problem here,<br>Ubuntu 22.04.2 LTS</p>

          <p>(textgen) user@host:$ pip install einops<br>Requirement already satisfied:
          einops in ./.conda/envs/textgen/lib/python3.10/site-packages (0.6.1)<br>(textgen)
          user@host:$ pip list | grep einops<br>einops                   0.6.1</p>

          '
        raw: 'Same problem here,

          Ubuntu 22.04.2 LTS


          (textgen) user@host:$ pip install einops

          Requirement already satisfied: einops in ./.conda/envs/textgen/lib/python3.10/site-packages
          (0.6.1)

          (textgen) user@host:$ pip list | grep einops

          einops                   0.6.1'
        updatedAt: '2023-05-08T07:39:45.299Z'
      numEdits: 1
      reactions: []
    id: 6458a73239e6aea69cb60abd
    type: comment
  author: Praytheomnissiah
  content: 'Same problem here,

    Ubuntu 22.04.2 LTS


    (textgen) user@host:$ pip install einops

    Requirement already satisfied: einops in ./.conda/envs/textgen/lib/python3.10/site-packages
    (0.6.1)

    (textgen) user@host:$ pip list | grep einops

    einops                   0.6.1'
  created_at: 2023-05-08 06:39:30+00:00
  edited: true
  hidden: false
  id: 6458a73239e6aea69cb60abd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c028bc145e6951a42dc05fc7e111d660.svg
      fullname: Occam Razor
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: OccamRazor
      type: user
    createdAt: '2023-05-08T18:51:33.000Z'
    data:
      edited: false
      editors:
      - OccamRazor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c028bc145e6951a42dc05fc7e111d660.svg
          fullname: Occam Razor
          isHf: false
          isPro: false
          name: OccamRazor
          type: user
        html: '<p>It was made for <a rel="nofollow" href="https://github.com/0cc4m/koboldAI">KoboldAI
          (4bit-fork)</a>.</p>

          '
        raw: It was made for [KoboldAI (4bit-fork)](https://github.com/0cc4m/koboldAI).
        updatedAt: '2023-05-08T18:51:33.158Z'
      numEdits: 0
      reactions: []
    id: 645944b539e6aea69cc31cb3
    type: comment
  author: OccamRazor
  content: It was made for [KoboldAI (4bit-fork)](https://github.com/0cc4m/koboldAI).
  created_at: 2023-05-08 17:51:33+00:00
  edited: false
  hidden: false
  id: 645944b539e6aea69cc31cb3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-05-08T21:05:20.000Z'
    data:
      edited: false
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<blockquote>

          <p>It was made for <a rel="nofollow" href="https://github.com/0cc4m/koboldAI">KoboldAI
          (4bit-fork)</a>.</p>

          </blockquote>

          <p>So your basically saying that I need to download KobaltAI if I want to
          run this? And if I did want to get it working on Oobabooga I guess it''s
          up to someone else like the develops of Oobabooga to incorporate support
          somehow? If that is the case I understand and thank you.</p>

          '
        raw: '> It was made for [KoboldAI (4bit-fork)](https://github.com/0cc4m/koboldAI).


          So your basically saying that I need to download KobaltAI if I want to run
          this? And if I did want to get it working on Oobabooga I guess it''s up
          to someone else like the develops of Oobabooga to incorporate support somehow?
          If that is the case I understand and thank you.'
        updatedAt: '2023-05-08T21:05:20.173Z'
      numEdits: 0
      reactions: []
    id: 64596410c5d0d57ba426f0c8
    type: comment
  author: Goldenblood56
  content: '> It was made for [KoboldAI (4bit-fork)](https://github.com/0cc4m/koboldAI).


    So your basically saying that I need to download KobaltAI if I want to run this?
    And if I did want to get it working on Oobabooga I guess it''s up to someone else
    like the develops of Oobabooga to incorporate support somehow? If that is the
    case I understand and thank you.'
  created_at: 2023-05-08 20:05:20+00:00
  edited: false
  hidden: false
  id: 64596410c5d0d57ba426f0c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/713ee03ccb97a141008e1685ef553e7b.svg
      fullname: Nathan Nitzel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheNitzel
      type: user
    createdAt: '2023-05-09T04:34:59.000Z'
    data:
      edited: false
      editors:
      - TheNitzel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/713ee03ccb97a141008e1685ef553e7b.svg
          fullname: Nathan Nitzel
          isHf: false
          isPro: false
          name: TheNitzel
          type: user
        html: '<p>On line 51 of your start_windows.bat add the line:<br>call pip install
          einops<br>Then run it again.  Once you''re done you can remove the line
          for future launches.</p>

          '
        raw: 'On line 51 of your start_windows.bat add the line:

          call pip install einops

          Then run it again.  Once you''re done you can remove the line for future
          launches.'
        updatedAt: '2023-05-09T04:34:59.174Z'
      numEdits: 0
      reactions: []
    id: 6459cd73f92601affa3ec8e7
    type: comment
  author: TheNitzel
  content: 'On line 51 of your start_windows.bat add the line:

    call pip install einops

    Then run it again.  Once you''re done you can remove the line for future launches.'
  created_at: 2023-05-09 03:34:59+00:00
  edited: false
  hidden: false
  id: 6459cd73f92601affa3ec8e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b4718018c3c055a48a6b9672f587fc32.svg
      fullname: Alan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Regenagade
      type: user
    createdAt: '2023-05-10T20:56:25.000Z'
    data:
      edited: false
      editors:
      - Regenagade
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b4718018c3c055a48a6b9672f587fc32.svg
          fullname: Alan
          isHf: false
          isPro: false
          name: Regenagade
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheNitzel&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheNitzel\">@<span class=\"\
          underline\">TheNitzel</span></a></span>\n\n\t</span></span> Thank you! That\
          \ worked!</p>\n"
        raw: '@TheNitzel Thank you! That worked!'
        updatedAt: '2023-05-10T20:56:25.787Z'
      numEdits: 0
      reactions: []
    id: 645c04f92c76efd4c6698788
    type: comment
  author: Regenagade
  content: '@TheNitzel Thank you! That worked!'
  created_at: 2023-05-10 19:56:25+00:00
  edited: false
  hidden: false
  id: 645c04f92c76efd4c6698788
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-05-10T21:58:03.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Thanks that worked for me too TheNitzel. But now I get this? lol</p>

          <p>What I have tried so far? Updating "ooba" that''s it really no clue what
          else to do.<br>Windows 10<br>RTX 4080 </p>

          <p>Arguments command line<br>"call python server.py --auto-devices --chat
          --trust-remote-code --model OccamRazor_mpt-7b-storywriter-4bit-128g --wbits
          4 --groupsize 128 --model_type llama "</p>

          <p>Starting the web UI...<br>INFO:Gradio HTTP request redirected to localhost
          :)<br>WARNING:trust_remote_code is enabled. This is dangerous.<br>INFO:Loading
          OccamRazor_mpt-7b-storywriter-4bit-128g...<br>INFO:Found the following quantized
          model: models\OccamRazor_mpt-7b-storywriter-4bit-128g\model.safetensors<br>C:\Users\xxxxx/.cache\huggingface\modules\transformers_modules\OccamRazor_mpt-7b-storywriter-4bit-128g\attention.py:148:
          UserWarning: Using <code>attn_impl: torch</code>. If your model does not
          use <code>alibi</code> or <code>prefix_lm</code> we recommend using <code>attn_impl:
          flash</code> otherwise we recommend using <code>attn_impl: triton</code>.<br>  warnings.warn(''Using
          <code>attn_impl: torch</code>. If your model does not use <code>alibi</code>
          or '' + ''<code>prefix_lm</code> we recommend using <code>attn_impl: flash</code>
          otherwise '' + ''we recommend using <code>attn_impl: triton</code>.'')<br>You
          are using config.init_device=''cpu'', but you can also use config.init_device="meta"
          with Composer + FSDP for fast initialization.</p>

          <p>Any ideas? Thanks.</p>

          '
        raw: "Thanks that worked for me too TheNitzel. But now I get this? lol\n\n\
          What I have tried so far? Updating \"ooba\" that's it really no clue what\
          \ else to do. \nWindows 10\nRTX 4080 \n\nArguments command line\n\"call\
          \ python server.py --auto-devices --chat --trust-remote-code --model OccamRazor_mpt-7b-storywriter-4bit-128g\
          \ --wbits 4 --groupsize 128 --model_type llama \"\n\nStarting the web UI...\n\
          INFO:Gradio HTTP request redirected to localhost :)\nWARNING:trust_remote_code\
          \ is enabled. This is dangerous.\nINFO:Loading OccamRazor_mpt-7b-storywriter-4bit-128g...\n\
          INFO:Found the following quantized model: models\\OccamRazor_mpt-7b-storywriter-4bit-128g\\\
          model.safetensors\nC:\\Users\\xxxxx/.cache\\huggingface\\modules\\transformers_modules\\\
          OccamRazor_mpt-7b-storywriter-4bit-128g\\attention.py:148: UserWarning:\
          \ Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm`\
          \ we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl:\
          \ triton`.\n  warnings.warn('Using `attn_impl: torch`. If your model does\
          \ not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl: flash`\
          \ otherwise ' + 'we recommend using `attn_impl: triton`.')\nYou are using\
          \ config.init_device='cpu', but you can also use config.init_device=\"meta\"\
          \ with Composer + FSDP for fast initialization.\n\nAny ideas? Thanks."
        updatedAt: '2023-05-10T22:02:34.304Z'
      numEdits: 2
      reactions: []
    id: 645c136b11b04b05ad065404
    type: comment
  author: Goldenblood56
  content: "Thanks that worked for me too TheNitzel. But now I get this? lol\n\nWhat\
    \ I have tried so far? Updating \"ooba\" that's it really no clue what else to\
    \ do. \nWindows 10\nRTX 4080 \n\nArguments command line\n\"call python server.py\
    \ --auto-devices --chat --trust-remote-code --model OccamRazor_mpt-7b-storywriter-4bit-128g\
    \ --wbits 4 --groupsize 128 --model_type llama \"\n\nStarting the web UI...\n\
    INFO:Gradio HTTP request redirected to localhost :)\nWARNING:trust_remote_code\
    \ is enabled. This is dangerous.\nINFO:Loading OccamRazor_mpt-7b-storywriter-4bit-128g...\n\
    INFO:Found the following quantized model: models\\OccamRazor_mpt-7b-storywriter-4bit-128g\\\
    model.safetensors\nC:\\Users\\xxxxx/.cache\\huggingface\\modules\\transformers_modules\\\
    OccamRazor_mpt-7b-storywriter-4bit-128g\\attention.py:148: UserWarning: Using\
    \ `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend\
    \ using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n\
    \  warnings.warn('Using `attn_impl: torch`. If your model does not use `alibi`\
    \ or ' + '`prefix_lm` we recommend using `attn_impl: flash` otherwise ' + 'we\
    \ recommend using `attn_impl: triton`.')\nYou are using config.init_device='cpu',\
    \ but you can also use config.init_device=\"meta\" with Composer + FSDP for fast\
    \ initialization.\n\nAny ideas? Thanks."
  created_at: 2023-05-10 20:58:03+00:00
  edited: true
  hidden: false
  id: 645c136b11b04b05ad065404
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: OccamRazor/mpt-7b-storywriter-4bit-128g
repo_type: model
status: open
target_branch: null
title: '   Oobabooga Error  einops import rearrange ModuleNotFoundError: No module
  named ''einops'' Press any key to continue . . .'
