!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Coves
conflicting_files: null
created_at: 2023-05-09 18:46:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fbd32c91e3393accd6a2095c48a0027f.svg
      fullname: Henriette Bonneville
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Coves
      type: user
    createdAt: '2023-05-09T19:46:18.000Z'
    data:
      edited: false
      editors:
      - Coves
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fbd32c91e3393accd6a2095c48a0027f.svg
          fullname: Henriette Bonneville
          isHf: false
          isPro: false
          name: Coves
          type: user
        html: "<p>Is it normal that the model produces almost gibberish text?</p>\n\
          <p>The first sentence is my prompt, the rest is the generated text:</p>\n\
          <blockquote>\n<p>The studio was filled with the rich odour of roses, and\
          \ when the light summer wind stirred amidst the trees of the garden, there\
          \ came through the open door the heavy scent of the lilac, or the more delicate\
          \ perfume of the pink-flowering thorn.<br>\uFFFD</p>\n<h2 id=\"chapter-iii\"\
          >CHAPTER III</h2>\n<p>Theodore had been in his office all day; he'd spent\
          \ that morning and early afternoon at a desk for ten hours\u2014and it seems\
          \ to me I'm not going to be able\uFFFD</p>\n<p>Theo's absence from her apartment\
          \ has left Juliana feeling guilty about what she should have done last night:\
          \ if you're ever going to get married, then why did we spend so much time\
          \ together? Did they see us talking? Is my hair still wet?\"</p>\n<p>But\
          \ now this evening is also an opportunity: as soonsextendednesses, though\
          \ perhaps less than usual since most of them are never seen again because\
          \ their lives stop abruptly after being rejected by the girl who loves him.\
          \ And yet, even before the war ended, people would ask where one might find\
          \ Mr. Theodore Hebert \u2013's wife! It isn't true that every woman wants\
          \ a husband,\" said Mrs. George Hyman on the occasion of having lunch with\
          \ Mme. Ethel Lucey, but in fact some men do marry women like Jules heretically,\
          \ especially those ones, which is often quoted as evidence that he can make\
          \ it clear how good a man may be born, but believed in Londonderry Street\
          \ appeared to think otherwise. The truth behind such conversations is always\
          \ that although many marriages were made precisely because of the marriage\
          \ bureau, nevertheless there will remain other reasons too, whether love\
          \ or passion makes no difference, excepting that some girls donatafternoon,'\
          \ says the author of \"A Day With You\" (Londonerysmith)</p>\n<p>He knew\
          \ everything about her life, including the things I've told you and others\
          \ say, saying nothing else remains silent, nor does he want to talk about\
          \ anything besides a desire for knowledge.\"</p>\n<p>However, however, this\
          \ evening was very different: How could someone come into your eyesight,\
          \ sir?' asked Marilynne Kensington-Lacey? Well, yes, but three times I'm\
          \ afraid,\" answered Burleigh Nugent, while a few years ago, just before\
          \ the end of World War Two, Lorraine Cattalonis had been put down, despite\
          \ the initial success of his career, there seemed to accept the idea that\
          \ there was something wrong with the way she looked, yet another example\
          \ of the 'I'll try to forget you!\"</p>\n</blockquote>\n<p>I'm running it\
          \ on a RTX 3060 12GB, KoboldAI fork, with <code>init_device</code> set to\
          \ <code>cpu</code> (which causes the initialization to take forever). When\
          \ I set it to <code>cuda</code> I run out of VRAM and I can't use <code>meta</code>\
          \ because I need triton I guess, and I'm on Windows.</p>\n"
        raw: "Is it normal that the model produces almost gibberish text?\r\n\r\n\
          The first sentence is my prompt, the rest is the generated text:\r\n\r\n\
          > The studio was filled with the rich odour of roses, and when the light\
          \ summer wind stirred amidst the trees of the garden, there came through\
          \ the open door the heavy scent of the lilac, or the more delicate perfume\
          \ of the pink-flowering thorn.\r\n> \uFFFD\r\n>\r\n> ## CHAPTER III\r\n\
          >\r\n> Theodore had been in his office all day; he'd spent that morning\
          \ and early afternoon at a desk for ten hours\u2014and it seems to me I'm\
          \ not going to be able\uFFFD\r\n>\r\n> Theo's absence from her apartment\
          \ has left Juliana feeling guilty about what she should have done last night:\
          \ if you're ever going to get married, then why did we spend so much time\
          \ together? Did they see us talking? Is my hair still wet?\"\r\n>\r\n> But\
          \ now this evening is also an opportunity: as soonsextendednesses, though\
          \ perhaps less than usual since most of them are never seen again because\
          \ their lives stop abruptly after being rejected by the girl who loves him.\
          \ And yet, even before the war ended, people would ask where one might find\
          \ Mr. Theodore Hebert \u2013's wife! It isn't true that every woman wants\
          \ a husband,\" said Mrs. George Hyman on the occasion of having lunch with\
          \ Mme. Ethel Lucey, but in fact some men do marry women like Jules heretically,\
          \ especially those ones, which is often quoted as evidence that he can make\
          \ it clear how good a man may be born, but believed in Londonderry Street\
          \ appeared to think otherwise. The truth behind such conversations is always\
          \ that although many marriages were made precisely because of the marriage\
          \ bureau, nevertheless there will remain other reasons too, whether love\
          \ or passion makes no difference, excepting that some girls donatafternoon,'\
          \ says the author of \"A Day With You\" (Londonerysmith)\r\n>\r\n> He knew\
          \ everything about her life, including the things I've told you and others\
          \ say, saying nothing else remains silent, nor does he want to talk about\
          \ anything besides a desire for knowledge.\"\r\n>\r\n> However, however,\
          \ this evening was very different: How could someone come into your eyesight,\
          \ sir?' asked Marilynne Kensington-Lacey? Well, yes, but three times I'm\
          \ afraid,\" answered Burleigh Nugent, while a few years ago, just before\
          \ the end of World War Two, Lorraine Cattalonis had been put down, despite\
          \ the initial success of his career, there seemed to accept the idea that\
          \ there was something wrong with the way she looked, yet another example\
          \ of the 'I'll try to forget you!\"\r\n\r\nI'm running it on a RTX 3060\
          \ 12GB, KoboldAI fork, with `init_device` set to `cpu` (which causes the\
          \ initialization to take forever). When I set it to `cuda` I run out of\
          \ VRAM and I can't use `meta` because I need triton I guess, and I'm on\
          \ Windows."
        updatedAt: '2023-05-09T19:46:18.233Z'
      numEdits: 0
      reactions: []
    id: 645aa30af5e01b23089474a6
    type: comment
  author: Coves
  content: "Is it normal that the model produces almost gibberish text?\r\n\r\nThe\
    \ first sentence is my prompt, the rest is the generated text:\r\n\r\n> The studio\
    \ was filled with the rich odour of roses, and when the light summer wind stirred\
    \ amidst the trees of the garden, there came through the open door the heavy scent\
    \ of the lilac, or the more delicate perfume of the pink-flowering thorn.\r\n\
    > \uFFFD\r\n>\r\n> ## CHAPTER III\r\n>\r\n> Theodore had been in his office all\
    \ day; he'd spent that morning and early afternoon at a desk for ten hours\u2014\
    and it seems to me I'm not going to be able\uFFFD\r\n>\r\n> Theo's absence from\
    \ her apartment has left Juliana feeling guilty about what she should have done\
    \ last night: if you're ever going to get married, then why did we spend so much\
    \ time together? Did they see us talking? Is my hair still wet?\"\r\n>\r\n> But\
    \ now this evening is also an opportunity: as soonsextendednesses, though perhaps\
    \ less than usual since most of them are never seen again because their lives\
    \ stop abruptly after being rejected by the girl who loves him. And yet, even\
    \ before the war ended, people would ask where one might find Mr. Theodore Hebert\
    \ \u2013's wife! It isn't true that every woman wants a husband,\" said Mrs. George\
    \ Hyman on the occasion of having lunch with Mme. Ethel Lucey, but in fact some\
    \ men do marry women like Jules heretically, especially those ones, which is often\
    \ quoted as evidence that he can make it clear how good a man may be born, but\
    \ believed in Londonderry Street appeared to think otherwise. The truth behind\
    \ such conversations is always that although many marriages were made precisely\
    \ because of the marriage bureau, nevertheless there will remain other reasons\
    \ too, whether love or passion makes no difference, excepting that some girls\
    \ donatafternoon,' says the author of \"A Day With You\" (Londonerysmith)\r\n\
    >\r\n> He knew everything about her life, including the things I've told you and\
    \ others say, saying nothing else remains silent, nor does he want to talk about\
    \ anything besides a desire for knowledge.\"\r\n>\r\n> However, however, this\
    \ evening was very different: How could someone come into your eyesight, sir?'\
    \ asked Marilynne Kensington-Lacey? Well, yes, but three times I'm afraid,\" answered\
    \ Burleigh Nugent, while a few years ago, just before the end of World War Two,\
    \ Lorraine Cattalonis had been put down, despite the initial success of his career,\
    \ there seemed to accept the idea that there was something wrong with the way\
    \ she looked, yet another example of the 'I'll try to forget you!\"\r\n\r\nI'm\
    \ running it on a RTX 3060 12GB, KoboldAI fork, with `init_device` set to `cpu`\
    \ (which causes the initialization to take forever). When I set it to `cuda` I\
    \ run out of VRAM and I can't use `meta` because I need triton I guess, and I'm\
    \ on Windows."
  created_at: 2023-05-09 18:46:18+00:00
  edited: false
  hidden: false
  id: 645aa30af5e01b23089474a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31bd51455caca0c67f79d3c088d6af5d.svg
      fullname: Stephen Hall
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: taven1
      type: user
    createdAt: '2023-05-09T22:47:18.000Z'
    data:
      edited: false
      editors:
      - taven1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31bd51455caca0c67f79d3c088d6af5d.svg
          fullname: Stephen Hall
          isHf: false
          isPro: false
          name: taven1
          type: user
        html: '<p>I''m seeing similar results. So far nothing I have tried has given
          anything even close to a usable response. I am using cuda on a 4090 and
          it''s relatively fast, but regardless of the prompts I enter (or random
          prompts) or the settings I adjust, everything has been nonsensical (just
          like the previous post). I have entered short prompts as well as many paragraphs
          at once (some very long and coherent story sessions with ChatGPT), but that
          doesn''t seem to matter. </p>

          <p>As I mentioned in another post, maybe we need to adjust some settings
          in KoboldAI or we are using the model incorrectly?</p>

          '
        raw: "I'm seeing similar results. So far nothing I have tried has given anything\
          \ even close to a usable response. I am using cuda on a 4090 and it's relatively\
          \ fast, but regardless of the prompts I enter (or random prompts) or the\
          \ settings I adjust, everything has been nonsensical (just like the previous\
          \ post). I have entered short prompts as well as many paragraphs at once\
          \ (some very long and coherent story sessions with ChatGPT), but that doesn't\
          \ seem to matter. \n\nAs I mentioned in another post, maybe we need to adjust\
          \ some settings in KoboldAI or we are using the model incorrectly?"
        updatedAt: '2023-05-09T22:47:18.822Z'
      numEdits: 0
      reactions: []
    id: 645acd76214503ad17ae508e
    type: comment
  author: taven1
  content: "I'm seeing similar results. So far nothing I have tried has given anything\
    \ even close to a usable response. I am using cuda on a 4090 and it's relatively\
    \ fast, but regardless of the prompts I enter (or random prompts) or the settings\
    \ I adjust, everything has been nonsensical (just like the previous post). I have\
    \ entered short prompts as well as many paragraphs at once (some very long and\
    \ coherent story sessions with ChatGPT), but that doesn't seem to matter. \n\n\
    As I mentioned in another post, maybe we need to adjust some settings in KoboldAI\
    \ or we are using the model incorrectly?"
  created_at: 2023-05-09 21:47:18+00:00
  edited: false
  hidden: false
  id: 645acd76214503ad17ae508e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-05-10T04:38:38.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>How do you load this in KoboldAI? I am getting a lot of errors that
          refuse for it to load.<br>It comes down to that it is an unrecognized model
          or that the config.json is missing however the config.json is in the folder.<br>This
          it the KoboldAI that I am using:<br><a rel="nofollow" href="https://github.com/0cc4m/koboldAI">https://github.com/0cc4m/koboldAI</a></p>

          '
        raw: 'How do you load this in KoboldAI? I am getting a lot of errors that
          refuse for it to load.

          It comes down to that it is an unrecognized model or that the config.json
          is missing however the config.json is in the folder.

          This it the KoboldAI that I am using:

          https://github.com/0cc4m/koboldAI'
        updatedAt: '2023-05-10T04:38:38.133Z'
      numEdits: 0
      reactions: []
    id: 645b1fce340133cc1c2174ad
    type: comment
  author: CR2022
  content: 'How do you load this in KoboldAI? I am getting a lot of errors that refuse
    for it to load.

    It comes down to that it is an unrecognized model or that the config.json is missing
    however the config.json is in the folder.

    This it the KoboldAI that I am using:

    https://github.com/0cc4m/koboldAI'
  created_at: 2023-05-10 03:38:38+00:00
  edited: false
  hidden: false
  id: 645b1fce340133cc1c2174ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-05-10T05:34:23.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>ValueError: Loading D:\KoboldAI\models\OccamRazor_mpt-7b-storywriter-4bit-128g
          requires you to execute the configuration file in that repo on your local
          machine. Make sure you have read the code there to avoid malicious use,
          then set the option <code>trust_remote_code=True</code> to remove this error.<br>WARNING    |
          <strong>main</strong>:load_model:2259 - No model type detected, assuming
          Neo (If this is a GPT2 model use the other menu option or --model GPT2Custom)<br>INIT       |
          Searching  | GPU support<br>INIT       | Found      | GPU support<br>INIT       |
          Starting   | Transformers<br>WARNING    | <strong>main</strong>:device_config:840
          - --breakmodel_gpulayers is malformatted. Please use the --help option to
          see correct usage of --breakmodel_gpulayers. Defaulting to all layers on
          device 0.<br>INIT       | Info       | Final device configuration:<br>       DEVICE
          ID  |  LAYERS  |  DEVICE NAME<br>Exception in thread Thread-13:<br>Traceback
          (most recent call last):<br>  File "B:\python\lib\threading.py", line 932,
          in _bootstrap_inner<br>    self.run()<br>  File "B:\python\lib\threading.py",
          line 870, in run<br>    self._target(*self._args, **self._kwargs)<br>  File
          "B:\python\lib\site-packages\socketio\server.py", line 731, in _handle_event_internal<br>    r
          = server._trigger_event(data[0], namespace, sid, *data[1:])<br>  File "B:\python\lib\site-packages\socketio\server.py",
          line 756, in <em>trigger_event<br>    return self.handlers[namespace]<a
          rel="nofollow" href="*args">event</a><br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 282, in _handler<br>    return self.<em>handle_event(handler, message,
          namespace, sid,<br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 828, in _handle_event<br>    ret = handler(*args)<br>  File "aiserver.py",
          line 469, in g<br>    return f(*a, **k)<br>  File "aiserver.py", line 3918,
          in get_message<br>    load_model(use_gpu=msg[''use_gpu''], gpu_layers=msg[''gpu_layers''],
          disk_layers=msg[''disk_layers''], online_model=msg[''online_model''])<br>  File
          "aiserver.py", line 2526, in load_model<br>    device_config(model_config)<br>  File
          "aiserver.py", line 907, in device_config<br>    device_list(n_layers, primary=breakmodel.primary_device)<br>  File
          "aiserver.py", line 805, in device_list<br>    print(f"{row_color}{colors.YELLOW
          + ''-&gt;'' + row_color if i == selected else ''  ''} {''(primary)'' if
          i == primary else '' ''*9} {i:3}  {sep_color}|{row_color}     {gpu_blocks[i]:3}  {sep_color}|{row_color}  {name}{colors.END}")<br>TypeError:
          unsupported format string passed to NoneType.<strong>format</strong></p>

          '
        raw: "ValueError: Loading D:\\KoboldAI\\models\\OccamRazor_mpt-7b-storywriter-4bit-128g\
          \ requires you to execute the configuration file in that repo on your local\
          \ machine. Make sure you have read the code there to avoid malicious use,\
          \ then set the option `trust_remote_code=True` to remove this error.\nWARNING\
          \    | __main__:load_model:2259 - No model type detected, assuming Neo (If\
          \ this is a GPT2 model use the other menu option or --model GPT2Custom)\n\
          INIT       | Searching  | GPU support\nINIT       | Found      | GPU support\n\
          INIT       | Starting   | Transformers\nWARNING    | __main__:device_config:840\
          \ - --breakmodel_gpulayers is malformatted. Please use the --help option\
          \ to see correct usage of --breakmodel_gpulayers. Defaulting to all layers\
          \ on device 0.\nINIT       | Info       | Final device configuration:\n\
          \       DEVICE ID  |  LAYERS  |  DEVICE NAME\nException in thread Thread-13:\n\
          Traceback (most recent call last):\n  File \"B:\\python\\lib\\threading.py\"\
          , line 932, in _bootstrap_inner\n    self.run()\n  File \"B:\\python\\lib\\\
          threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n\
          \  File \"B:\\python\\lib\\site-packages\\socketio\\server.py\", line 731,\
          \ in _handle_event_internal\n    r = server._trigger_event(data[0], namespace,\
          \ sid, *data[1:])\n  File \"B:\\python\\lib\\site-packages\\socketio\\server.py\"\
          , line 756, in _trigger_event\n    return self.handlers[namespace][event](*args)\n\
          \  File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
          , line 282, in _handler\n    return self._handle_event(handler, message,\
          \ namespace, sid,\n  File \"B:\\python\\lib\\site-packages\\flask_socketio\\\
          __init__.py\", line 828, in _handle_event\n    ret = handler(*args)\n  File\
          \ \"aiserver.py\", line 469, in g\n    return f(*a, **k)\n  File \"aiserver.py\"\
          , line 3918, in get_message\n    load_model(use_gpu=msg['use_gpu'], gpu_layers=msg['gpu_layers'],\
          \ disk_layers=msg['disk_layers'], online_model=msg['online_model'])\n  File\
          \ \"aiserver.py\", line 2526, in load_model\n    device_config(model_config)\n\
          \  File \"aiserver.py\", line 907, in device_config\n    device_list(n_layers,\
          \ primary=breakmodel.primary_device)\n  File \"aiserver.py\", line 805,\
          \ in device_list\n    print(f\"{row_color}{colors.YELLOW + '->' + row_color\
          \ if i == selected else '  '} {'(primary)' if i == primary else ' '*9} {i:3}\
          \  {sep_color}|{row_color}     {gpu_blocks[i]:3}  {sep_color}|{row_color}\
          \  {name}{colors.END}\")\nTypeError: unsupported format string passed to\
          \ NoneType.__format__"
        updatedAt: '2023-05-10T05:34:23.557Z'
      numEdits: 0
      reactions: []
    id: 645b2cdfbca1aab2a2825162
    type: comment
  author: CR2022
  content: "ValueError: Loading D:\\KoboldAI\\models\\OccamRazor_mpt-7b-storywriter-4bit-128g\
    \ requires you to execute the configuration file in that repo on your local machine.\
    \ Make sure you have read the code there to avoid malicious use, then set the\
    \ option `trust_remote_code=True` to remove this error.\nWARNING    | __main__:load_model:2259\
    \ - No model type detected, assuming Neo (If this is a GPT2 model use the other\
    \ menu option or --model GPT2Custom)\nINIT       | Searching  | GPU support\n\
    INIT       | Found      | GPU support\nINIT       | Starting   | Transformers\n\
    WARNING    | __main__:device_config:840 - --breakmodel_gpulayers is malformatted.\
    \ Please use the --help option to see correct usage of --breakmodel_gpulayers.\
    \ Defaulting to all layers on device 0.\nINIT       | Info       | Final device\
    \ configuration:\n       DEVICE ID  |  LAYERS  |  DEVICE NAME\nException in thread\
    \ Thread-13:\nTraceback (most recent call last):\n  File \"B:\\python\\lib\\threading.py\"\
    , line 932, in _bootstrap_inner\n    self.run()\n  File \"B:\\python\\lib\\threading.py\"\
    , line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"B:\\\
    python\\lib\\site-packages\\socketio\\server.py\", line 731, in _handle_event_internal\n\
    \    r = server._trigger_event(data[0], namespace, sid, *data[1:])\n  File \"\
    B:\\python\\lib\\site-packages\\socketio\\server.py\", line 756, in _trigger_event\n\
    \    return self.handlers[namespace][event](*args)\n  File \"B:\\python\\lib\\\
    site-packages\\flask_socketio\\__init__.py\", line 282, in _handler\n    return\
    \ self._handle_event(handler, message, namespace, sid,\n  File \"B:\\python\\\
    lib\\site-packages\\flask_socketio\\__init__.py\", line 828, in _handle_event\n\
    \    ret = handler(*args)\n  File \"aiserver.py\", line 469, in g\n    return\
    \ f(*a, **k)\n  File \"aiserver.py\", line 3918, in get_message\n    load_model(use_gpu=msg['use_gpu'],\
    \ gpu_layers=msg['gpu_layers'], disk_layers=msg['disk_layers'], online_model=msg['online_model'])\n\
    \  File \"aiserver.py\", line 2526, in load_model\n    device_config(model_config)\n\
    \  File \"aiserver.py\", line 907, in device_config\n    device_list(n_layers,\
    \ primary=breakmodel.primary_device)\n  File \"aiserver.py\", line 805, in device_list\n\
    \    print(f\"{row_color}{colors.YELLOW + '->' + row_color if i == selected else\
    \ '  '} {'(primary)' if i == primary else ' '*9} {i:3}  {sep_color}|{row_color}\
    \     {gpu_blocks[i]:3}  {sep_color}|{row_color}  {name}{colors.END}\")\nTypeError:\
    \ unsupported format string passed to NoneType.__format__"
  created_at: 2023-05-10 04:34:23+00:00
  edited: false
  hidden: false
  id: 645b2cdfbca1aab2a2825162
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31bd51455caca0c67f79d3c088d6af5d.svg
      fullname: Stephen Hall
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: taven1
      type: user
    createdAt: '2023-05-10T05:42:30.000Z'
    data:
      edited: false
      editors:
      - taven1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31bd51455caca0c67f79d3c088d6af5d.svg
          fullname: Stephen Hall
          isHf: false
          isPro: false
          name: taven1
          type: user
        html: '<p>Here is how I got it installed and the model loaded in Windows 11
          (as best I can remember):</p>

          <ol>

          <li>Installed KoboldAI using the link mentioned (<a rel="nofollow" href="https://github.com/0cc4m/koboldAI">https://github.com/0cc4m/koboldAI</a>)
          (I went with the B: mount)</li>

          <li>Once installed, I created a folder KoboldAI\models folder named it "OccamRazor_mpt-7b-storywriter-4bit-128g"</li>

          <li>Downloaded all of the files from this repo and placed them in this new
          folder</li>

          <li>Renamed the model file from "model.safetensors" to "4bit-128g.safetensors"</li>

          <li>Started up KoboldAI by running the "Play.bat" file</li>

          <li>Clicked the AI button in the top corner and loaded the model by using
          "Load a model from its directory" and then selected the aforementioned folder
          containing the model</li>

          </ol>

          '
        raw: 'Here is how I got it installed and the model loaded in Windows 11 (as
          best I can remember):


          1. Installed KoboldAI using the link mentioned (https://github.com/0cc4m/koboldAI)
          (I went with the B: mount)

          2. Once installed, I created a folder KoboldAI\models folder named it "OccamRazor_mpt-7b-storywriter-4bit-128g"

          3. Downloaded all of the files from this repo and placed them in this new
          folder

          4. Renamed the model file from "model.safetensors" to "4bit-128g.safetensors"

          5. Started up KoboldAI by running the "Play.bat" file

          6. Clicked the AI button in the top corner and loaded the model by using
          "Load a model from its directory" and then selected the aforementioned folder
          containing the model'
        updatedAt: '2023-05-10T05:42:30.355Z'
      numEdits: 0
      reactions: []
    id: 645b2ec6821677aa5f6bb129
    type: comment
  author: taven1
  content: 'Here is how I got it installed and the model loaded in Windows 11 (as
    best I can remember):


    1. Installed KoboldAI using the link mentioned (https://github.com/0cc4m/koboldAI)
    (I went with the B: mount)

    2. Once installed, I created a folder KoboldAI\models folder named it "OccamRazor_mpt-7b-storywriter-4bit-128g"

    3. Downloaded all of the files from this repo and placed them in this new folder

    4. Renamed the model file from "model.safetensors" to "4bit-128g.safetensors"

    5. Started up KoboldAI by running the "Play.bat" file

    6. Clicked the AI button in the top corner and loaded the model by using "Load
    a model from its directory" and then selected the aforementioned folder containing
    the model'
  created_at: 2023-05-10 04:42:30+00:00
  edited: false
  hidden: false
  id: 645b2ec6821677aa5f6bb129
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fbd32c91e3393accd6a2095c48a0027f.svg
      fullname: Henriette Bonneville
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Coves
      type: user
    createdAt: '2023-05-10T06:29:57.000Z'
    data:
      edited: false
      editors:
      - Coves
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fbd32c91e3393accd6a2095c48a0027f.svg
          fullname: Henriette Bonneville
          isHf: false
          isPro: false
          name: Coves
          type: user
        html: '<blockquote>

          <p>As I mentioned in another post, maybe we need to adjust some settings
          in KoboldAI or we are using the model incorrectly?</p>

          </blockquote>

          <p>I tried giving it the same parameters that Mosaic uses in their demo
          but that didn''t seem to make much difference.</p>

          '
        raw: '> As I mentioned in another post, maybe we need to adjust some settings
          in KoboldAI or we are using the model incorrectly?


          I tried giving it the same parameters that Mosaic uses in their demo but
          that didn''t seem to make much difference.'
        updatedAt: '2023-05-10T06:29:57.697Z'
      numEdits: 0
      reactions: []
    id: 645b39e5f49fa48ea3bbe579
    type: comment
  author: Coves
  content: '> As I mentioned in another post, maybe we need to adjust some settings
    in KoboldAI or we are using the model incorrectly?


    I tried giving it the same parameters that Mosaic uses in their demo but that
    didn''t seem to make much difference.'
  created_at: 2023-05-10 05:29:57+00:00
  edited: false
  hidden: false
  id: 645b39e5f49fa48ea3bbe579
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-05-10T09:48:38.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<blockquote>

          <p>Here is how I got it installed and the model loaded in Windows 11 (as
          best I can remember):</p>

          <ol>

          <li>Installed KoboldAI using the link mentioned (<a rel="nofollow" href="https://github.com/0cc4m/koboldAI">https://github.com/0cc4m/koboldAI</a>)
          (I went with the B: mount)</li>

          <li>Once installed, I created a folder KoboldAI\models folder named it "OccamRazor_mpt-7b-storywriter-4bit-128g"</li>

          <li>Downloaded all of the files from this repo and placed them in this new
          folder</li>

          <li>Renamed the model file from "model.safetensors" to "4bit-128g.safetensors"</li>

          <li>Started up KoboldAI by running the "Play.bat" file</li>

          <li>Clicked the AI button in the top corner and loaded the model by using
          "Load a model from its directory" and then selected the aforementioned folder
          containing the model</li>

          </ol>

          </blockquote>

          <p>Still does not work getting this error:</p>

          <p>ValueError: Loading D:\KoboldAI\models\OccamRazor_mpt-7b-storywriter-4bit-128g
          requires you to execute the configuration file in that repo on your local
          machine. Make sure you have read the code there to avoid malicious use,
          then set the option <code>trust_remote_code=True</code> to remove this error.<br>WARNING    |
          <strong>main</strong>:load_model:2259 - No model type detected, assuming
          Neo (If this is a GPT2 model use the other menu option or --model GPT2Custom)<br>INIT       |
          Searching  | GPU support<br>INIT       | Found      | GPU support<br>INIT       |
          Starting   | Transformers<br>WARNING    | <strong>main</strong>:device_config:840
          - --breakmodel_gpulayers is malformatted. Please use the --help option to
          see correct usage of --breakmodel_gpulayers. Defaulting to all layers on
          device 0.<br>INIT       | Info       | Final device configuration:<br>       DEVICE
          ID  |  LAYERS  |  DEVICE NAME<br>Exception in thread Thread-18:<br>Traceback
          (most recent call last):<br>  File "B:\python\lib\threading.py", line 932,
          in _bootstrap_inner<br>    self.run()<br>  File "B:\python\lib\threading.py",
          line 870, in run<br>    self._target(*self._args, **self._kwargs)<br>  File
          "B:\python\lib\site-packages\socketio\server.py", line 731, in _handle_event_internal<br>    r
          = server._trigger_event(data[0], namespace, sid, *data[1:])<br>  File "B:\python\lib\site-packages\socketio\server.py",
          line 756, in <em>trigger_event<br>    return self.handlers[namespace]<a
          rel="nofollow" href="*args">event</a><br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 282, in _handler<br>    return self.<em>handle_event(handler, message,
          namespace, sid,<br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 828, in _handle_event<br>    ret = handler(*args)<br>  File "aiserver.py",
          line 469, in g<br>    return f(*a, **k)<br>  File "aiserver.py", line 3918,
          in get_message<br>    load_model(use_gpu=msg[''use_gpu''], gpu_layers=msg[''gpu_layers''],
          disk_layers=msg[''disk_layers''], online_model=msg[''online_model''])<br>  File
          "aiserver.py", line 2526, in load_model<br>    device_config(model_config)<br>  File
          "aiserver.py", line 907, in device_config<br>    device_list(n_layers, primary=breakmodel.primary_device)<br>  File
          "aiserver.py", line 805, in device_list<br>    print(f"{row_color}{colors.YELLOW
          + ''-&gt;'' + row_color if i == selected else ''  ''} {''(primary)'' if
          i == primary else '' ''*9} {i:3}  {sep_color}|{row_color}     {gpu_blocks[i]:3}  {sep_color}|{row_color}  {name}{colors.END}")<br>TypeError:
          unsupported format string passed to NoneType.<strong>format</strong></p>

          <p>How do you do this in KobaltAI?</p>

          <p><code>trust_remote_code=True</code> to remove this error.</p>

          '
        raw: "> Here is how I got it installed and the model loaded in Windows 11\
          \ (as best I can remember):\n> \n> 1. Installed KoboldAI using the link\
          \ mentioned (https://github.com/0cc4m/koboldAI) (I went with the B: mount)\n\
          > 2. Once installed, I created a folder KoboldAI\\models folder named it\
          \ \"OccamRazor_mpt-7b-storywriter-4bit-128g\"\n> 3. Downloaded all of the\
          \ files from this repo and placed them in this new folder\n> 4. Renamed\
          \ the model file from \"model.safetensors\" to \"4bit-128g.safetensors\"\
          \n> 5. Started up KoboldAI by running the \"Play.bat\" file\n> 6. Clicked\
          \ the AI button in the top corner and loaded the model by using \"Load a\
          \ model from its directory\" and then selected the aforementioned folder\
          \ containing the model\n\nStill does not work getting this error:\n\nValueError:\
          \ Loading D:\\KoboldAI\\models\\OccamRazor_mpt-7b-storywriter-4bit-128g\
          \ requires you to execute the configuration file in that repo on your local\
          \ machine. Make sure you have read the code there to avoid malicious use,\
          \ then set the option `trust_remote_code=True` to remove this error.\nWARNING\
          \    | __main__:load_model:2259 - No model type detected, assuming Neo (If\
          \ this is a GPT2 model use the other menu option or --model GPT2Custom)\n\
          INIT       | Searching  | GPU support\nINIT       | Found      | GPU support\n\
          INIT       | Starting   | Transformers\nWARNING    | __main__:device_config:840\
          \ - --breakmodel_gpulayers is malformatted. Please use the --help option\
          \ to see correct usage of --breakmodel_gpulayers. Defaulting to all layers\
          \ on device 0.\nINIT       | Info       | Final device configuration:\n\
          \       DEVICE ID  |  LAYERS  |  DEVICE NAME\nException in thread Thread-18:\n\
          Traceback (most recent call last):\n  File \"B:\\python\\lib\\threading.py\"\
          , line 932, in _bootstrap_inner\n    self.run()\n  File \"B:\\python\\lib\\\
          threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n\
          \  File \"B:\\python\\lib\\site-packages\\socketio\\server.py\", line 731,\
          \ in _handle_event_internal\n    r = server._trigger_event(data[0], namespace,\
          \ sid, *data[1:])\n  File \"B:\\python\\lib\\site-packages\\socketio\\server.py\"\
          , line 756, in _trigger_event\n    return self.handlers[namespace][event](*args)\n\
          \  File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
          , line 282, in _handler\n    return self._handle_event(handler, message,\
          \ namespace, sid,\n  File \"B:\\python\\lib\\site-packages\\flask_socketio\\\
          __init__.py\", line 828, in _handle_event\n    ret = handler(*args)\n  File\
          \ \"aiserver.py\", line 469, in g\n    return f(*a, **k)\n  File \"aiserver.py\"\
          , line 3918, in get_message\n    load_model(use_gpu=msg['use_gpu'], gpu_layers=msg['gpu_layers'],\
          \ disk_layers=msg['disk_layers'], online_model=msg['online_model'])\n  File\
          \ \"aiserver.py\", line 2526, in load_model\n    device_config(model_config)\n\
          \  File \"aiserver.py\", line 907, in device_config\n    device_list(n_layers,\
          \ primary=breakmodel.primary_device)\n  File \"aiserver.py\", line 805,\
          \ in device_list\n    print(f\"{row_color}{colors.YELLOW + '->' + row_color\
          \ if i == selected else '  '} {'(primary)' if i == primary else ' '*9} {i:3}\
          \  {sep_color}|{row_color}     {gpu_blocks[i]:3}  {sep_color}|{row_color}\
          \  {name}{colors.END}\")\nTypeError: unsupported format string passed to\
          \ NoneType.__format__\n\n\nHow do you do this in KobaltAI?\n\n`trust_remote_code=True`\
          \ to remove this error."
        updatedAt: '2023-05-10T09:48:38.744Z'
      numEdits: 0
      reactions: []
    id: 645b6876b4e65f04f7f25c78
    type: comment
  author: CR2022
  content: "> Here is how I got it installed and the model loaded in Windows 11 (as\
    \ best I can remember):\n> \n> 1. Installed KoboldAI using the link mentioned\
    \ (https://github.com/0cc4m/koboldAI) (I went with the B: mount)\n> 2. Once installed,\
    \ I created a folder KoboldAI\\models folder named it \"OccamRazor_mpt-7b-storywriter-4bit-128g\"\
    \n> 3. Downloaded all of the files from this repo and placed them in this new\
    \ folder\n> 4. Renamed the model file from \"model.safetensors\" to \"4bit-128g.safetensors\"\
    \n> 5. Started up KoboldAI by running the \"Play.bat\" file\n> 6. Clicked the\
    \ AI button in the top corner and loaded the model by using \"Load a model from\
    \ its directory\" and then selected the aforementioned folder containing the model\n\
    \nStill does not work getting this error:\n\nValueError: Loading D:\\KoboldAI\\\
    models\\OccamRazor_mpt-7b-storywriter-4bit-128g requires you to execute the configuration\
    \ file in that repo on your local machine. Make sure you have read the code there\
    \ to avoid malicious use, then set the option `trust_remote_code=True` to remove\
    \ this error.\nWARNING    | __main__:load_model:2259 - No model type detected,\
    \ assuming Neo (If this is a GPT2 model use the other menu option or --model GPT2Custom)\n\
    INIT       | Searching  | GPU support\nINIT       | Found      | GPU support\n\
    INIT       | Starting   | Transformers\nWARNING    | __main__:device_config:840\
    \ - --breakmodel_gpulayers is malformatted. Please use the --help option to see\
    \ correct usage of --breakmodel_gpulayers. Defaulting to all layers on device\
    \ 0.\nINIT       | Info       | Final device configuration:\n       DEVICE ID\
    \  |  LAYERS  |  DEVICE NAME\nException in thread Thread-18:\nTraceback (most\
    \ recent call last):\n  File \"B:\\python\\lib\\threading.py\", line 932, in _bootstrap_inner\n\
    \    self.run()\n  File \"B:\\python\\lib\\threading.py\", line 870, in run\n\
    \    self._target(*self._args, **self._kwargs)\n  File \"B:\\python\\lib\\site-packages\\\
    socketio\\server.py\", line 731, in _handle_event_internal\n    r = server._trigger_event(data[0],\
    \ namespace, sid, *data[1:])\n  File \"B:\\python\\lib\\site-packages\\socketio\\\
    server.py\", line 756, in _trigger_event\n    return self.handlers[namespace][event](*args)\n\
    \  File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\", line\
    \ 282, in _handler\n    return self._handle_event(handler, message, namespace,\
    \ sid,\n  File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
    , line 828, in _handle_event\n    ret = handler(*args)\n  File \"aiserver.py\"\
    , line 469, in g\n    return f(*a, **k)\n  File \"aiserver.py\", line 3918, in\
    \ get_message\n    load_model(use_gpu=msg['use_gpu'], gpu_layers=msg['gpu_layers'],\
    \ disk_layers=msg['disk_layers'], online_model=msg['online_model'])\n  File \"\
    aiserver.py\", line 2526, in load_model\n    device_config(model_config)\n  File\
    \ \"aiserver.py\", line 907, in device_config\n    device_list(n_layers, primary=breakmodel.primary_device)\n\
    \  File \"aiserver.py\", line 805, in device_list\n    print(f\"{row_color}{colors.YELLOW\
    \ + '->' + row_color if i == selected else '  '} {'(primary)' if i == primary\
    \ else ' '*9} {i:3}  {sep_color}|{row_color}     {gpu_blocks[i]:3}  {sep_color}|{row_color}\
    \  {name}{colors.END}\")\nTypeError: unsupported format string passed to NoneType.__format__\n\
    \n\nHow do you do this in KobaltAI?\n\n`trust_remote_code=True` to remove this\
    \ error."
  created_at: 2023-05-10 08:48:38+00:00
  edited: false
  hidden: false
  id: 645b6876b4e65f04f7f25c78
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: OccamRazor/mpt-7b-storywriter-4bit-128g
repo_type: model
status: open
target_branch: null
title: Gibberish output
