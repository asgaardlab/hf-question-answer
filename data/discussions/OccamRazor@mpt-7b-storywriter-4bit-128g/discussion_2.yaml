!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Dingus123
conflicting_files: null
created_at: 2023-05-08 04:20:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7118b5ad83b5c4a8c42df37a9fa69e62.svg
      fullname: Dingus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dingus123
      type: user
    createdAt: '2023-05-08T05:20:57.000Z'
    data:
      edited: false
      editors:
      - Dingus123
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7118b5ad83b5c4a8c42df37a9fa69e62.svg
          fullname: Dingus
          isHf: false
          isPro: false
          name: Dingus123
          type: user
        html: '<p>Fresh install of Oobabooga (on two different machines) cannot determine
          model type, no matter what is chosen within the UI it blanks out bits, groupsize
          and model type then crashes the ui. if run from commandline, it just states
          the model type cant be determined and to add --Model_type= to the command
          line, but the model type is unknown.</p>

          <p>ERROR:Can''t determine model type from model name. Please specify it
          manually using --model_type argument</p>

          <p>Any ideas?</p>

          '
        raw: "Fresh install of Oobabooga (on two different machines) cannot determine\
          \ model type, no matter what is chosen within the UI it blanks out bits,\
          \ groupsize and model type then crashes the ui. if run from commandline,\
          \ it just states the model type cant be determined and to add --Model_type=\
          \ to the command line, but the model type is unknown.\r\n\r\nERROR:Can't\
          \ determine model type from model name. Please specify it manually using\
          \ --model_type argument\r\n\r\nAny ideas?"
        updatedAt: '2023-05-08T05:20:57.582Z'
      numEdits: 0
      reactions: []
    id: 645886b9cfd4a6e2bc004fbd
    type: comment
  author: Dingus123
  content: "Fresh install of Oobabooga (on two different machines) cannot determine\
    \ model type, no matter what is chosen within the UI it blanks out bits, groupsize\
    \ and model type then crashes the ui. if run from commandline, it just states\
    \ the model type cant be determined and to add --Model_type= to the command line,\
    \ but the model type is unknown.\r\n\r\nERROR:Can't determine model type from\
    \ model name. Please specify it manually using --model_type argument\r\n\r\nAny\
    \ ideas?"
  created_at: 2023-05-08 04:20:57+00:00
  edited: false
  hidden: false
  id: 645886b9cfd4a6e2bc004fbd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9fd0c3e4b1af3f82ba83c61c2c971e4d.svg
      fullname: Peter Larsen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boosh
      type: user
    createdAt: '2023-05-08T06:31:20.000Z'
    data:
      edited: false
      editors:
      - Boosh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9fd0c3e4b1af3f82ba83c61c2c971e4d.svg
          fullname: Peter Larsen
          isHf: false
          isPro: false
          name: Boosh
          type: user
        html: '<p>Same here - except my ooba does not crash.</p>

          <p>Whether i try load the model inside my webUI or from cmd env the result
          is the same.</p>

          <p>Win 11</p>

          '
        raw: 'Same here - except my ooba does not crash.


          Whether i try load the model inside my webUI or from cmd env the result
          is the same.


          Win 11'
        updatedAt: '2023-05-08T06:31:20.814Z'
      numEdits: 0
      reactions: []
    id: 64589738c5d0d57ba4168b46
    type: comment
  author: Boosh
  content: 'Same here - except my ooba does not crash.


    Whether i try load the model inside my webUI or from cmd env the result is the
    same.


    Win 11'
  created_at: 2023-05-08 05:31:20+00:00
  edited: false
  hidden: false
  id: 64589738c5d0d57ba4168b46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90d2587a45ac080f101ef09dc404a279.svg
      fullname: Niichan Haou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Niichanhaou
      type: user
    createdAt: '2023-05-08T06:37:05.000Z'
    data:
      edited: false
      editors:
      - Niichanhaou
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90d2587a45ac080f101ef09dc404a279.svg
          fullname: Niichan Haou
          isHf: false
          isPro: false
          name: Niichanhaou
          type: user
        html: '<p>Same here. I suppose because it''s not related to llama or any other
          previously existing type of model. I''m sure they''ll do something about
          it soon. The pace at which AI is going is amazing</p>

          '
        raw: Same here. I suppose because it's not related to llama or any other previously
          existing type of model. I'm sure they'll do something about it soon. The
          pace at which AI is going is amazing
        updatedAt: '2023-05-08T06:37:05.083Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mirek190
    id: 64589891f92601affa278f11
    type: comment
  author: Niichanhaou
  content: Same here. I suppose because it's not related to llama or any other previously
    existing type of model. I'm sure they'll do something about it soon. The pace
    at which AI is going is amazing
  created_at: 2023-05-08 05:37:05+00:00
  edited: false
  hidden: false
  id: 64589891f92601affa278f11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/33645d3caee104f410f199a38a90f844.svg
      fullname: PabloTerres
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PabloTerres
      type: user
    createdAt: '2023-05-08T07:04:02.000Z'
    data:
      edited: false
      editors:
      - PabloTerres
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/33645d3caee104f410f199a38a90f844.svg
          fullname: PabloTerres
          isHf: false
          isPro: false
          name: PabloTerres
          type: user
        html: '<p>I believe so since the Git hub page says the only model_types it
          Currently supports are LLaMA, OPT, and GPT-J. And mpt-7b-storywriter-4bit-128g
          is Mpt which is not supported.</p>

          '
        raw: I believe so since the Git hub page says the only model_types it Currently
          supports are LLaMA, OPT, and GPT-J. And mpt-7b-storywriter-4bit-128g is
          Mpt which is not supported.
        updatedAt: '2023-05-08T07:04:02.538Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mirek190
    id: 64589ee239e6aea69cb53cab
    type: comment
  author: PabloTerres
  content: I believe so since the Git hub page says the only model_types it Currently
    supports are LLaMA, OPT, and GPT-J. And mpt-7b-storywriter-4bit-128g is Mpt which
    is not supported.
  created_at: 2023-05-08 06:04:02+00:00
  edited: false
  hidden: false
  id: 64589ee239e6aea69cb53cab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7548b9e13e00ad1c080c32c05173fca4.svg
      fullname: walker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: itkingtao
      type: user
    createdAt: '2023-05-08T16:01:43.000Z'
    data:
      edited: false
      editors:
      - itkingtao
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7548b9e13e00ad1c080c32c05173fca4.svg
          fullname: walker
          isHf: false
          isPro: false
          name: itkingtao
          type: user
        html: '<p>The same problem</p>

          '
        raw: The same problem
        updatedAt: '2023-05-08T16:01:43.807Z'
      numEdits: 0
      reactions: []
    id: 64591ce7232e5f0712b5e73b
    type: comment
  author: itkingtao
  content: The same problem
  created_at: 2023-05-08 15:01:43+00:00
  edited: false
  hidden: false
  id: 64591ce7232e5f0712b5e73b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
      fullname: Vladimir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vdruts
      type: user
    createdAt: '2023-05-08T18:17:33.000Z'
    data:
      edited: false
      editors:
      - vdruts
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
          fullname: Vladimir
          isHf: false
          isPro: false
          name: vdruts
          type: user
        html: '<p>There are additional arguments that made to be updated for this
          to work in OOba, although I''m not sure whether quantizing and making it
          128 changes anything &gt; <a rel="nofollow" href="https://www.youtube.com/watch?v=O9Y_ZdsuKWQ">https://www.youtube.com/watch?v=O9Y_ZdsuKWQ</a></p>

          '
        raw: There are additional arguments that made to be updated for this to work
          in OOba, although I'm not sure whether quantizing and making it 128 changes
          anything > https://www.youtube.com/watch?v=O9Y_ZdsuKWQ
        updatedAt: '2023-05-08T18:17:33.488Z'
      numEdits: 0
      reactions: []
    id: 64593cbdf92601affa355448
    type: comment
  author: vdruts
  content: There are additional arguments that made to be updated for this to work
    in OOba, although I'm not sure whether quantizing and making it 128 changes anything
    > https://www.youtube.com/watch?v=O9Y_ZdsuKWQ
  created_at: 2023-05-08 17:17:33+00:00
  edited: false
  hidden: false
  id: 64593cbdf92601affa355448
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c028bc145e6951a42dc05fc7e111d660.svg
      fullname: Occam Razor
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: OccamRazor
      type: user
    createdAt: '2023-05-08T18:50:12.000Z'
    data:
      edited: false
      editors:
      - OccamRazor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c028bc145e6951a42dc05fc7e111d660.svg
          fullname: Occam Razor
          isHf: false
          isPro: false
          name: OccamRazor
          type: user
        html: '<p>It can be run in <a rel="nofollow" href="https://github.com/0cc4m/koboldAI">KoboldAI
          (4bit-fork)</a>. I don''t use ooba, personally.</p>

          '
        raw: It can be run in [KoboldAI (4bit-fork)](https://github.com/0cc4m/koboldAI).
          I don't use ooba, personally.
        updatedAt: '2023-05-08T18:50:12.024Z'
      numEdits: 0
      reactions: []
    id: 64594464f92601affa35d5f4
    type: comment
  author: OccamRazor
  content: It can be run in [KoboldAI (4bit-fork)](https://github.com/0cc4m/koboldAI).
    I don't use ooba, personally.
  created_at: 2023-05-08 17:50:12+00:00
  edited: false
  hidden: false
  id: 64594464f92601affa35d5f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31bd51455caca0c67f79d3c088d6af5d.svg
      fullname: Stephen Hall
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: taven1
      type: user
    createdAt: '2023-05-09T00:08:58.000Z'
    data:
      edited: true
      editors:
      - taven1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31bd51455caca0c67f79d3c088d6af5d.svg
          fullname: Stephen Hall
          isHf: false
          isPro: false
          name: taven1
          type: user
        html: '<p>Can you provide some short instructions for setting up this model
          with KoboldAI? I''ve installed KoboldAI (with the link provided) and loaded
          the model, but the responses are completely unusable. I''m assuming there
          are some settings I need to adjust.</p>

          '
        raw: Can you provide some short instructions for setting up this model with
          KoboldAI? I've installed KoboldAI (with the link provided) and loaded the
          model, but the responses are completely unusable. I'm assuming there are
          some settings I need to adjust.
        updatedAt: '2023-05-09T00:09:29.648Z'
      numEdits: 1
      reactions: []
    id: 64598f1af92601affa3a7735
    type: comment
  author: taven1
  content: Can you provide some short instructions for setting up this model with
    KoboldAI? I've installed KoboldAI (with the link provided) and loaded the model,
    but the responses are completely unusable. I'm assuming there are some settings
    I need to adjust.
  created_at: 2023-05-08 23:08:58+00:00
  edited: true
  hidden: false
  id: 64598f1af92601affa3a7735
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9199feb9a773b79032636645b1180ea1.svg
      fullname: Pro Patte
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProPatte
      type: user
    createdAt: '2023-05-09T11:40:05.000Z'
    data:
      edited: false
      editors:
      - ProPatte
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9199feb9a773b79032636645b1180ea1.svg
          fullname: Pro Patte
          isHf: false
          isPro: false
          name: ProPatte
          type: user
        html: '<p>Same issue here.</p>

          '
        raw: Same issue here.
        updatedAt: '2023-05-09T11:40:05.111Z'
      numEdits: 0
      reactions: []
    id: 645a311566c76108af5f7f5f
    type: comment
  author: ProPatte
  content: Same issue here.
  created_at: 2023-05-09 10:40:05+00:00
  edited: false
  hidden: false
  id: 645a311566c76108af5f7f5f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9199feb9a773b79032636645b1180ea1.svg
      fullname: Pro Patte
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProPatte
      type: user
    createdAt: '2023-05-09T12:12:01.000Z'
    data:
      edited: false
      editors:
      - ProPatte
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9199feb9a773b79032636645b1180ea1.svg
          fullname: Pro Patte
          isHf: false
          isPro: false
          name: ProPatte
          type: user
        html: '<p>Same issue here.</p>

          '
        raw: Same issue here.
        updatedAt: '2023-05-09T12:12:01.003Z'
      numEdits: 0
      reactions: []
    id: 645a38915d69b62307af78ec
    type: comment
  author: ProPatte
  content: Same issue here.
  created_at: 2023-05-09 11:12:01+00:00
  edited: false
  hidden: false
  id: 645a38915d69b62307af78ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c028bc145e6951a42dc05fc7e111d660.svg
      fullname: Occam Razor
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: OccamRazor
      type: user
    createdAt: '2023-05-09T18:34:53.000Z'
    data:
      edited: false
      editors:
      - OccamRazor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c028bc145e6951a42dc05fc7e111d660.svg
          fullname: Occam Razor
          isHf: false
          isPro: false
          name: OccamRazor
          type: user
        html: '<p>Yes, there was an issue with the init device. I changed the model
          and removed the unused python files, it is now working properly for me.
          Let me know if it also works for you.</p>

          '
        raw: Yes, there was an issue with the init device. I changed the model and
          removed the unused python files, it is now working properly for me. Let
          me know if it also works for you.
        updatedAt: '2023-05-09T18:34:53.527Z'
      numEdits: 0
      reactions: []
    id: 645a924de505443f8194ee11
    type: comment
  author: OccamRazor
  content: Yes, there was an issue with the init device. I changed the model and removed
    the unused python files, it is now working properly for me. Let me know if it
    also works for you.
  created_at: 2023-05-09 17:34:53+00:00
  edited: false
  hidden: false
  id: 645a924de505443f8194ee11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31bd51455caca0c67f79d3c088d6af5d.svg
      fullname: Stephen Hall
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: taven1
      type: user
    createdAt: '2023-05-09T22:50:53.000Z'
    data:
      edited: false
      editors:
      - taven1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31bd51455caca0c67f79d3c088d6af5d.svg
          fullname: Stephen Hall
          isHf: false
          isPro: false
          name: taven1
          type: user
        html: "<p>It's definitely loading faster for me with the updated files, but\
          \ the responses are still incoherent.  </p>\n<p>Prompt: Can you write a\
          \ short story about a boy that takes a flight to the moon?<br>Response:\
          \ I'm not sure when I'll be back,\" he said. \"We're going on another trip\
          \ together.\" He was still in his shirt and jacket but had changed his mind\
          \ again: this time it's okay for him to go there! This is what we do with\
          \ our big-eyed friend here\u2014he has been working hard all day long!\"\
          </p>\n"
        raw: "It's definitely loading faster for me with the updated files, but the\
          \ responses are still incoherent.  \n\nPrompt: Can you write a short story\
          \ about a boy that takes a flight to the moon?\nResponse: I'm not sure when\
          \ I'll be back,\" he said. \"We're going on another trip together.\" He\
          \ was still in his shirt and jacket but had changed his mind again: this\
          \ time it's okay for him to go there! This is what we do with our big-eyed\
          \ friend here\u2014he has been working hard all day long!\""
        updatedAt: '2023-05-09T22:50:53.901Z'
      numEdits: 0
      reactions: []
    id: 645ace4dbf3f9fbb8c6dde8a
    type: comment
  author: taven1
  content: "It's definitely loading faster for me with the updated files, but the\
    \ responses are still incoherent.  \n\nPrompt: Can you write a short story about\
    \ a boy that takes a flight to the moon?\nResponse: I'm not sure when I'll be\
    \ back,\" he said. \"We're going on another trip together.\" He was still in his\
    \ shirt and jacket but had changed his mind again: this time it's okay for him\
    \ to go there! This is what we do with our big-eyed friend here\u2014he has been\
    \ working hard all day long!\""
  created_at: 2023-05-09 21:50:53+00:00
  edited: false
  hidden: false
  id: 645ace4dbf3f9fbb8c6dde8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a69e79db410131da95e5c02bef70fc90.svg
      fullname: Michael Polanski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mpolio
      type: user
    createdAt: '2023-05-10T03:46:41.000Z'
    data:
      edited: true
      editors:
      - mpolio
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a69e79db410131da95e5c02bef70fc90.svg
          fullname: Michael Polanski
          isHf: false
          isPro: false
          name: mpolio
          type: user
        html: '<blockquote>

          <p>Yes, there was an issue with the init device. I changed the model and
          removed the unused python files, it is now working properly for me. Let
          me know if it also works for you.</p>

          </blockquote>

          <p>It seems we need those files:<br><code>models/OccamRazor_mpt-7b-storywriter-4bit-128g
          does not appear to have a file named configuration_mpt.py.</code></p>

          '
        raw: '> Yes, there was an issue with the init device. I changed the model
          and removed the unused python files, it is now working properly for me.
          Let me know if it also works for you.


          It seems we need those files:

          `models/OccamRazor_mpt-7b-storywriter-4bit-128g does not appear to have
          a file named configuration_mpt.py.`'
        updatedAt: '2023-05-10T04:01:03.387Z'
      numEdits: 2
      reactions: []
    id: 645b13a180ba386ec56fd20f
    type: comment
  author: mpolio
  content: '> Yes, there was an issue with the init device. I changed the model and
    removed the unused python files, it is now working properly for me. Let me know
    if it also works for you.


    It seems we need those files:

    `models/OccamRazor_mpt-7b-storywriter-4bit-128g does not appear to have a file
    named configuration_mpt.py.`'
  created_at: 2023-05-10 02:46:41+00:00
  edited: true
  hidden: false
  id: 645b13a180ba386ec56fd20f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-05-10T04:39:32.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<blockquote>

          <p>It can be run in <a rel="nofollow" href="https://github.com/0cc4m/koboldAI">KoboldAI
          (4bit-fork)</a>. I don''t use ooba, personally.</p>

          </blockquote>

          <p>I tried that fork does not work it says the model is not recognized or
          that config.json is missing.</p>

          '
        raw: '> It can be run in [KoboldAI (4bit-fork)](https://github.com/0cc4m/koboldAI).
          I don''t use ooba, personally.


          I tried that fork does not work it says the model is not recognized or that
          config.json is missing.'
        updatedAt: '2023-05-10T04:39:32.434Z'
      numEdits: 0
      reactions: []
    id: 645b2004f5d5c29281103a71
    type: comment
  author: CR2022
  content: '> It can be run in [KoboldAI (4bit-fork)](https://github.com/0cc4m/koboldAI).
    I don''t use ooba, personally.


    I tried that fork does not work it says the model is not recognized or that config.json
    is missing.'
  created_at: 2023-05-10 03:39:32+00:00
  edited: false
  hidden: false
  id: 645b2004f5d5c29281103a71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-05-10T05:34:55.000Z'
    data:
      edited: true
      editors:
      - CR2022
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>INFO       | <strong>main</strong>:do_connect:3545 - Client connected!<br>Exception
          in thread Thread-12:<br>Traceback (most recent call last):<br>  File "B:\python\lib\threading.py",
          line 932, in _bootstrap_inner<br>    self.run()<br>  File "B:\python\lib\threading.py",
          line 870, in run<br>    self._target(*self._args, **self._kwargs)<br>  File
          "B:\python\lib\site-packages\socketio\server.py", line 731, in _handle_event_internal<br>    r
          = server._trigger_event(data[0], namespace, sid, *data[1:])<br>  File "B:\python\lib\site-packages\socketio\server.py",
          line 756, in <em>trigger_event<br>    return self.handlers[namespace]<a
          rel="nofollow" href="*args">event</a><br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 282, in _handler<br>    return self.<em>handle_event(handler, message,
          namespace, sid,<br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 828, in <em>handle_event<br>    ret = handler(*args)<br>  File "aiserver.py",
          line 469, in g<br>    return f(*a, **k)<br>  File "aiserver.py", line 3955,
          in get_message<br>    get_model_info(msg[''data''], directory=msg[''path''])<br>  File
          "aiserver.py", line 1550, in get_model_info<br>    layer_count = get_layer_count(model,
          directory=directory)<br>  File "aiserver.py", line 1596, in get_layer_count<br>    model_config
          = AutoConfig.from_pretrained(model.replace(''/'', ''</em>''), revision=args.revision,
          cache_dir="cache")<br>  File "B:\python\lib\site-packages\transformers\models\auto\configuration_auto.py",
          line 779, in from_pretrained<br>    raise ValueError(<br>ValueError: Loading
          D:\KoboldAI\models\OccamRazor_mpt-7b-storywriter-4bit-128g requires you
          to execute the configuration file in that repo on your local machine. Make
          sure you have read the code there to avoid malicious use, then set the option
          <code>trust_remote_code=True</code> to remove this error.<br>WARNING    |
          <strong>main</strong>:load_model:2259 - No model type detected, assuming
          Neo (If this is a GPT2 model use the other menu option or --model GPT2Custom)<br>INIT       |
          Searching  | GPU support<br>INIT       | Found      | GPU support<br>INIT       |
          Starting   | Transformers<br>WARNING    | <strong>main</strong>:device_config:840
          - --breakmodel_gpulayers is malformatted. Please use the --help option to
          see correct usage of --breakmodel_gpulayers. Defaulting to all layers on
          device 0.<br>INIT       | Info       | Final device configuration:<br>       DEVICE
          ID  |  LAYERS  |  DEVICE NAME<br>Exception in thread Thread-13:<br>Traceback
          (most recent call last):<br>  File "B:\python\lib\threading.py", line 932,
          in _bootstrap_inner<br>    self.run()<br>  File "B:\python\lib\threading.py",
          line 870, in run<br>    self._target(*self._args, **self._kwargs)<br>  File
          "B:\python\lib\site-packages\socketio\server.py", line 731, in _handle_event_internal<br>    r
          = server._trigger_event(data[0], namespace, sid, *data[1:])<br>  File "B:\python\lib\site-packages\socketio\server.py",
          line 756, in <em>trigger_event<br>    return self.handlers[namespace]<a
          rel="nofollow" href="*args">event</a><br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 282, in _handler<br>    return self.<em>handle_event(handler, message,
          namespace, sid,<br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 828, in _handle_event<br>    ret = handler(*args)<br>  File "aiserver.py",
          line 469, in g<br>    return f(*a, **k)<br>  File "aiserver.py", line 3918,
          in get_message<br>    load_model(use_gpu=msg[''use_gpu''], gpu_layers=msg[''gpu_layers''],
          disk_layers=msg[''disk_layers''], online_model=msg[''online_model''])<br>  File
          "aiserver.py", line 2526, in load_model<br>    device_config(model_config)<br>  File
          "aiserver.py", line 907, in device_config<br>    device_list(n_layers, primary=breakmodel.primary_device)<br>  File
          "aiserver.py", line 805, in device_list<br>    print(f"{row_color}{colors.YELLOW
          + ''-&gt;'' + row_color if i == selected else ''  ''} {''(primary)'' if
          i == primary else '' ''*9} {i:3}  {sep_color}|{row_color}     {gpu_blocks[i]:3}  {sep_color}|{row_color}  {name}{colors.END}")<br>TypeError:
          unsupported format string passed to NoneType.<strong>format</strong></p>

          '
        raw: "INFO       | __main__:do_connect:3545 - Client connected!\nException\
          \ in thread Thread-12:\nTraceback (most recent call last):\n  File \"B:\\\
          python\\lib\\threading.py\", line 932, in _bootstrap_inner\n    self.run()\n\
          \  File \"B:\\python\\lib\\threading.py\", line 870, in run\n    self._target(*self._args,\
          \ **self._kwargs)\n  File \"B:\\python\\lib\\site-packages\\socketio\\server.py\"\
          , line 731, in _handle_event_internal\n    r = server._trigger_event(data[0],\
          \ namespace, sid, *data[1:])\n  File \"B:\\python\\lib\\site-packages\\\
          socketio\\server.py\", line 756, in _trigger_event\n    return self.handlers[namespace][event](*args)\n\
          \  File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
          , line 282, in _handler\n    return self._handle_event(handler, message,\
          \ namespace, sid,\n  File \"B:\\python\\lib\\site-packages\\flask_socketio\\\
          __init__.py\", line 828, in _handle_event\n    ret = handler(*args)\n  File\
          \ \"aiserver.py\", line 469, in g\n    return f(*a, **k)\n  File \"aiserver.py\"\
          , line 3955, in get_message\n    get_model_info(msg['data'], directory=msg['path'])\n\
          \  File \"aiserver.py\", line 1550, in get_model_info\n    layer_count =\
          \ get_layer_count(model, directory=directory)\n  File \"aiserver.py\", line\
          \ 1596, in get_layer_count\n    model_config = AutoConfig.from_pretrained(model.replace('/',\
          \ '_'), revision=args.revision, cache_dir=\"cache\")\n  File \"B:\\python\\\
          lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\"\
          , line 779, in from_pretrained\n    raise ValueError(\nValueError: Loading\
          \ D:\\KoboldAI\\models\\OccamRazor_mpt-7b-storywriter-4bit-128g requires\
          \ you to execute the configuration file in that repo on your local machine.\
          \ Make sure you have read the code there to avoid malicious use, then set\
          \ the option `trust_remote_code=True` to remove this error.\nWARNING   \
          \ | __main__:load_model:2259 - No model type detected, assuming Neo (If\
          \ this is a GPT2 model use the other menu option or --model GPT2Custom)\n\
          INIT       | Searching  | GPU support\nINIT       | Found      | GPU support\n\
          INIT       | Starting   | Transformers\nWARNING    | __main__:device_config:840\
          \ - --breakmodel_gpulayers is malformatted. Please use the --help option\
          \ to see correct usage of --breakmodel_gpulayers. Defaulting to all layers\
          \ on device 0.\nINIT       | Info       | Final device configuration:\n\
          \       DEVICE ID  |  LAYERS  |  DEVICE NAME\nException in thread Thread-13:\n\
          Traceback (most recent call last):\n  File \"B:\\python\\lib\\threading.py\"\
          , line 932, in _bootstrap_inner\n    self.run()\n  File \"B:\\python\\lib\\\
          threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n\
          \  File \"B:\\python\\lib\\site-packages\\socketio\\server.py\", line 731,\
          \ in _handle_event_internal\n    r = server._trigger_event(data[0], namespace,\
          \ sid, *data[1:])\n  File \"B:\\python\\lib\\site-packages\\socketio\\server.py\"\
          , line 756, in _trigger_event\n    return self.handlers[namespace][event](*args)\n\
          \  File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
          , line 282, in _handler\n    return self._handle_event(handler, message,\
          \ namespace, sid,\n  File \"B:\\python\\lib\\site-packages\\flask_socketio\\\
          __init__.py\", line 828, in _handle_event\n    ret = handler(*args)\n  File\
          \ \"aiserver.py\", line 469, in g\n    return f(*a, **k)\n  File \"aiserver.py\"\
          , line 3918, in get_message\n    load_model(use_gpu=msg['use_gpu'], gpu_layers=msg['gpu_layers'],\
          \ disk_layers=msg['disk_layers'], online_model=msg['online_model'])\n  File\
          \ \"aiserver.py\", line 2526, in load_model\n    device_config(model_config)\n\
          \  File \"aiserver.py\", line 907, in device_config\n    device_list(n_layers,\
          \ primary=breakmodel.primary_device)\n  File \"aiserver.py\", line 805,\
          \ in device_list\n    print(f\"{row_color}{colors.YELLOW + '->' + row_color\
          \ if i == selected else '  '} {'(primary)' if i == primary else ' '*9} {i:3}\
          \  {sep_color}|{row_color}     {gpu_blocks[i]:3}  {sep_color}|{row_color}\
          \  {name}{colors.END}\")\nTypeError: unsupported format string passed to\
          \ NoneType.__format__"
        updatedAt: '2023-05-10T05:35:40.350Z'
      numEdits: 1
      reactions: []
    id: 645b2cff340133cc1c21fb45
    type: comment
  author: CR2022
  content: "INFO       | __main__:do_connect:3545 - Client connected!\nException in\
    \ thread Thread-12:\nTraceback (most recent call last):\n  File \"B:\\python\\\
    lib\\threading.py\", line 932, in _bootstrap_inner\n    self.run()\n  File \"\
    B:\\python\\lib\\threading.py\", line 870, in run\n    self._target(*self._args,\
    \ **self._kwargs)\n  File \"B:\\python\\lib\\site-packages\\socketio\\server.py\"\
    , line 731, in _handle_event_internal\n    r = server._trigger_event(data[0],\
    \ namespace, sid, *data[1:])\n  File \"B:\\python\\lib\\site-packages\\socketio\\\
    server.py\", line 756, in _trigger_event\n    return self.handlers[namespace][event](*args)\n\
    \  File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\", line\
    \ 282, in _handler\n    return self._handle_event(handler, message, namespace,\
    \ sid,\n  File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
    , line 828, in _handle_event\n    ret = handler(*args)\n  File \"aiserver.py\"\
    , line 469, in g\n    return f(*a, **k)\n  File \"aiserver.py\", line 3955, in\
    \ get_message\n    get_model_info(msg['data'], directory=msg['path'])\n  File\
    \ \"aiserver.py\", line 1550, in get_model_info\n    layer_count = get_layer_count(model,\
    \ directory=directory)\n  File \"aiserver.py\", line 1596, in get_layer_count\n\
    \    model_config = AutoConfig.from_pretrained(model.replace('/', '_'), revision=args.revision,\
    \ cache_dir=\"cache\")\n  File \"B:\\python\\lib\\site-packages\\transformers\\\
    models\\auto\\configuration_auto.py\", line 779, in from_pretrained\n    raise\
    \ ValueError(\nValueError: Loading D:\\KoboldAI\\models\\OccamRazor_mpt-7b-storywriter-4bit-128g\
    \ requires you to execute the configuration file in that repo on your local machine.\
    \ Make sure you have read the code there to avoid malicious use, then set the\
    \ option `trust_remote_code=True` to remove this error.\nWARNING    | __main__:load_model:2259\
    \ - No model type detected, assuming Neo (If this is a GPT2 model use the other\
    \ menu option or --model GPT2Custom)\nINIT       | Searching  | GPU support\n\
    INIT       | Found      | GPU support\nINIT       | Starting   | Transformers\n\
    WARNING    | __main__:device_config:840 - --breakmodel_gpulayers is malformatted.\
    \ Please use the --help option to see correct usage of --breakmodel_gpulayers.\
    \ Defaulting to all layers on device 0.\nINIT       | Info       | Final device\
    \ configuration:\n       DEVICE ID  |  LAYERS  |  DEVICE NAME\nException in thread\
    \ Thread-13:\nTraceback (most recent call last):\n  File \"B:\\python\\lib\\threading.py\"\
    , line 932, in _bootstrap_inner\n    self.run()\n  File \"B:\\python\\lib\\threading.py\"\
    , line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"B:\\\
    python\\lib\\site-packages\\socketio\\server.py\", line 731, in _handle_event_internal\n\
    \    r = server._trigger_event(data[0], namespace, sid, *data[1:])\n  File \"\
    B:\\python\\lib\\site-packages\\socketio\\server.py\", line 756, in _trigger_event\n\
    \    return self.handlers[namespace][event](*args)\n  File \"B:\\python\\lib\\\
    site-packages\\flask_socketio\\__init__.py\", line 282, in _handler\n    return\
    \ self._handle_event(handler, message, namespace, sid,\n  File \"B:\\python\\\
    lib\\site-packages\\flask_socketio\\__init__.py\", line 828, in _handle_event\n\
    \    ret = handler(*args)\n  File \"aiserver.py\", line 469, in g\n    return\
    \ f(*a, **k)\n  File \"aiserver.py\", line 3918, in get_message\n    load_model(use_gpu=msg['use_gpu'],\
    \ gpu_layers=msg['gpu_layers'], disk_layers=msg['disk_layers'], online_model=msg['online_model'])\n\
    \  File \"aiserver.py\", line 2526, in load_model\n    device_config(model_config)\n\
    \  File \"aiserver.py\", line 907, in device_config\n    device_list(n_layers,\
    \ primary=breakmodel.primary_device)\n  File \"aiserver.py\", line 805, in device_list\n\
    \    print(f\"{row_color}{colors.YELLOW + '->' + row_color if i == selected else\
    \ '  '} {'(primary)' if i == primary else ' '*9} {i:3}  {sep_color}|{row_color}\
    \     {gpu_blocks[i]:3}  {sep_color}|{row_color}  {name}{colors.END}\")\nTypeError:\
    \ unsupported format string passed to NoneType.__format__"
  created_at: 2023-05-10 04:34:55+00:00
  edited: true
  hidden: false
  id: 645b2cff340133cc1c21fb45
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9199feb9a773b79032636645b1180ea1.svg
      fullname: Pro Patte
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProPatte
      type: user
    createdAt: '2023-05-10T05:47:56.000Z'
    data:
      edited: true
      editors:
      - ProPatte
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9199feb9a773b79032636645b1180ea1.svg
          fullname: Pro Patte
          isHf: false
          isPro: false
          name: ProPatte
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Yes, there was an issue with the init device. I changed the model and
          removed the unused python files, it is now working properly for me. Let
          me know if it also works for you.</p>

          </blockquote>

          <p>It seems we need those files:<br><code>models/OccamRazor_mpt-7b-storywriter-4bit-128g
          does not appear to have a file named configuration_mpt.py.</code></p>

          </blockquote>

          <p>That is a separate issue addressed here: <a href="https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/5">https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/5</a></p>

          <blockquote>

          <p>Can you provide some short instructions for setting up this model with
          KoboldAI? I''ve installed KoboldAI (with the link provided) and loaded the
          model, but the responses are completely unusable. I''m assuming there are
          some settings I need to adjust.</p>

          </blockquote>

          <p>Also a separate issue addressed here: <a href="https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/4">https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/4</a></p>

          <blockquote>

          <p>INFO       | <strong>main</strong>:do_connect:3545 - Client connected!<br>Exception
          in thread Thread-12:<br>Traceback (most recent call last):<br>  File "B:\python\lib\threading.py",
          line 932, in _bootstrap_inner<br>    self.run()<br>  File "B:\python\lib\threading.py",
          line 870, in run<br>    self._target(*self._args, **self._kwargs)<br>  File
          "B:\python\lib\site-packages\socketio\server.py", line 731, in _handle_event_internal<br>    r
          = server._trigger_event(data[0], namespace, sid, *data[1:])<br>  File "B:\python\lib\site-packages\socketio\server.py",
          line 756, in <em>trigger_event<br>    return self.handlers[namespace]<a
          rel="nofollow" href="*args">event</a><br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 282, in _handler<br>    return self.<em>handle_event(handler, message,
          namespace, sid,<br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 828, in <em>handle_event<br>    ret = handler(*args)<br>  File "aiserver.py",
          line 469, in g<br>    return f(*a, **k)<br>  File "aiserver.py", line 3955,
          in get_message<br>    get_model_info(msg[''data''], directory=msg[''path''])<br>  File
          "aiserver.py", line 1550, in get_model_info<br>    layer_count = get_layer_count(model,
          directory=directory)<br>  File "aiserver.py", line 1596, in get_layer_count<br>    model_config
          = AutoConfig.from_pretrained(model.replace(''/'', ''</em>''), revision=args.revision,
          cache_dir="cache")<br>  File "B:\python\lib\site-packages\transformers\models\auto\configuration_auto.py",
          line 779, in from_pretrained<br>    raise ValueError(<br>ValueError: Loading
          D:\KoboldAI\models\OccamRazor_mpt-7b-storywriter-4bit-128g requires you
          to execute the configuration file in that repo on your local machine. Make
          sure you have read the code there to avoid malicious use, then set the option
          <code>trust_remote_code=True</code> to remove this error.<br>WARNING    |
          <strong>main</strong>:load_model:2259 - No model type detected, assuming
          Neo (If this is a GPT2 model use the other menu option or --model GPT2Custom)<br>INIT       |
          Searching  | GPU support<br>INIT       | Found      | GPU support<br>INIT       |
          Starting   | Transformers<br>WARNING    | <strong>main</strong>:device_config:840
          - --breakmodel_gpulayers is malformatted. Please use the --help option to
          see correct usage of --breakmodel_gpulayers. Defaulting to all layers on
          device 0.<br>INIT       | Info       | Final device configuration:<br>       DEVICE
          ID  |  LAYERS  |  DEVICE NAME<br>Exception in thread Thread-13:<br>Traceback
          (most recent call last):<br>  File "B:\python\lib\threading.py", line 932,
          in _bootstrap_inner<br>    self.run()<br>  File "B:\python\lib\threading.py",
          line 870, in run<br>    self._target(*self._args, **self._kwargs)<br>  File
          "B:\python\lib\site-packages\socketio\server.py", line 731, in _handle_event_internal<br>    r
          = server._trigger_event(data[0], namespace, sid, *data[1:])<br>  File "B:\python\lib\site-packages\socketio\server.py",
          line 756, in <em>trigger_event<br>    return self.handlers[namespace]<a
          rel="nofollow" href="*args">event</a><br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 282, in _handler<br>    return self.<em>handle_event(handler, message,
          namespace, sid,<br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 828, in _handle_event<br>    ret = handler(*args)<br>  File "aiserver.py",
          line 469, in g<br>    return f(*a, **k)<br>  File "aiserver.py", line 3918,
          in get_message<br>    load_model(use_gpu=msg[''use_gpu''], gpu_layers=msg[''gpu_layers''],
          disk_layers=msg[''disk_layers''], online_model=msg[''online_model''])<br>  File
          "aiserver.py", line 2526, in load_model<br>    device_config(model_config)<br>  File
          "aiserver.py", line 907, in device_config<br>    device_list(n_layers, primary=breakmodel.primary_device)<br>  File
          "aiserver.py", line 805, in device_list<br>    print(f"{row_color}{colors.YELLOW
          + ''-&gt;'' + row_color if i == selected else ''  ''} {''(primary)'' if
          i == primary else '' ''*9} {i:3}  {sep_color}|{row_color}     {gpu_blocks[i]:3}  {sep_color}|{row_color}  {name}{colors.END}")<br>TypeError:
          unsupported format string passed to NoneType.<strong>format</strong></p>

          </blockquote>

          <p>This is also a separate issue addressed here: <a href="https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/3">https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/3</a></p>

          <p>You need to set the option trust_remote_code=True.</p>

          <p>We shouldn''t mix every issue in one thread. Let''s get back on topic.</p>

          '
        raw: "> > Yes, there was an issue with the init device. I changed the model\
          \ and removed the unused python files, it is now working properly for me.\
          \ Let me know if it also works for you.\n> \n> It seems we need those files:\n\
          > `models/OccamRazor_mpt-7b-storywriter-4bit-128g does not appear to have\
          \ a file named configuration_mpt.py.`\n\nThat is a separate issue addressed\
          \ here: https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/5\n\
          \n> Can you provide some short instructions for setting up this model with\
          \ KoboldAI? I've installed KoboldAI (with the link provided) and loaded\
          \ the model, but the responses are completely unusable. I'm assuming there\
          \ are some settings I need to adjust.\n\nAlso a separate issue addressed\
          \ here: https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/4\n\
          \n> INFO       | __main__:do_connect:3545 - Client connected!\n> Exception\
          \ in thread Thread-12:\n> Traceback (most recent call last):\n>   File \"\
          B:\\python\\lib\\threading.py\", line 932, in _bootstrap_inner\n>     self.run()\n\
          >   File \"B:\\python\\lib\\threading.py\", line 870, in run\n>     self._target(*self._args,\
          \ **self._kwargs)\n>   File \"B:\\python\\lib\\site-packages\\socketio\\\
          server.py\", line 731, in _handle_event_internal\n>     r = server._trigger_event(data[0],\
          \ namespace, sid, *data[1:])\n>   File \"B:\\python\\lib\\site-packages\\\
          socketio\\server.py\", line 756, in _trigger_event\n>     return self.handlers[namespace][event](*args)\n\
          >   File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
          , line 282, in _handler\n>     return self._handle_event(handler, message,\
          \ namespace, sid,\n>   File \"B:\\python\\lib\\site-packages\\flask_socketio\\\
          __init__.py\", line 828, in _handle_event\n>     ret = handler(*args)\n\
          >   File \"aiserver.py\", line 469, in g\n>     return f(*a, **k)\n>   File\
          \ \"aiserver.py\", line 3955, in get_message\n>     get_model_info(msg['data'],\
          \ directory=msg['path'])\n>   File \"aiserver.py\", line 1550, in get_model_info\n\
          >     layer_count = get_layer_count(model, directory=directory)\n>   File\
          \ \"aiserver.py\", line 1596, in get_layer_count\n>     model_config = AutoConfig.from_pretrained(model.replace('/',\
          \ '_'), revision=args.revision, cache_dir=\"cache\")\n>   File \"B:\\python\\\
          lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\"\
          , line 779, in from_pretrained\n>     raise ValueError(\n> ValueError: Loading\
          \ D:\\KoboldAI\\models\\OccamRazor_mpt-7b-storywriter-4bit-128g requires\
          \ you to execute the configuration file in that repo on your local machine.\
          \ Make sure you have read the code there to avoid malicious use, then set\
          \ the option `trust_remote_code=True` to remove this error.\n> WARNING \
          \   | __main__:load_model:2259 - No model type detected, assuming Neo (If\
          \ this is a GPT2 model use the other menu option or --model GPT2Custom)\n\
          > INIT       | Searching  | GPU support\n> INIT       | Found      | GPU\
          \ support\n> INIT       | Starting   | Transformers\n> WARNING    | __main__:device_config:840\
          \ - --breakmodel_gpulayers is malformatted. Please use the --help option\
          \ to see correct usage of --breakmodel_gpulayers. Defaulting to all layers\
          \ on device 0.\n> INIT       | Info       | Final device configuration:\n\
          >        DEVICE ID  |  LAYERS  |  DEVICE NAME\n> Exception in thread Thread-13:\n\
          > Traceback (most recent call last):\n>   File \"B:\\python\\lib\\threading.py\"\
          , line 932, in _bootstrap_inner\n>     self.run()\n>   File \"B:\\python\\\
          lib\\threading.py\", line 870, in run\n>     self._target(*self._args, **self._kwargs)\n\
          >   File \"B:\\python\\lib\\site-packages\\socketio\\server.py\", line 731,\
          \ in _handle_event_internal\n>     r = server._trigger_event(data[0], namespace,\
          \ sid, *data[1:])\n>   File \"B:\\python\\lib\\site-packages\\socketio\\\
          server.py\", line 756, in _trigger_event\n>     return self.handlers[namespace][event](*args)\n\
          >   File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
          , line 282, in _handler\n>     return self._handle_event(handler, message,\
          \ namespace, sid,\n>   File \"B:\\python\\lib\\site-packages\\flask_socketio\\\
          __init__.py\", line 828, in _handle_event\n>     ret = handler(*args)\n\
          >   File \"aiserver.py\", line 469, in g\n>     return f(*a, **k)\n>   File\
          \ \"aiserver.py\", line 3918, in get_message\n>     load_model(use_gpu=msg['use_gpu'],\
          \ gpu_layers=msg['gpu_layers'], disk_layers=msg['disk_layers'], online_model=msg['online_model'])\n\
          >   File \"aiserver.py\", line 2526, in load_model\n>     device_config(model_config)\n\
          >   File \"aiserver.py\", line 907, in device_config\n>     device_list(n_layers,\
          \ primary=breakmodel.primary_device)\n>   File \"aiserver.py\", line 805,\
          \ in device_list\n>     print(f\"{row_color}{colors.YELLOW + '->' + row_color\
          \ if i == selected else '  '} {'(primary)' if i == primary else ' '*9} {i:3}\
          \  {sep_color}|{row_color}     {gpu_blocks[i]:3}  {sep_color}|{row_color}\
          \  {name}{colors.END}\")\n> TypeError: unsupported format string passed\
          \ to NoneType.__format__\n\nThis is also a separate issue addressed here:\
          \ https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/3\n\
          \nYou need to set the option trust_remote_code=True.\n\nWe shouldn't mix\
          \ every issue in one thread. Let's get back on topic."
        updatedAt: '2023-05-10T05:53:33.938Z'
      numEdits: 3
      reactions: []
    id: 645b300c80ba386ec57103a8
    type: comment
  author: ProPatte
  content: "> > Yes, there was an issue with the init device. I changed the model\
    \ and removed the unused python files, it is now working properly for me. Let\
    \ me know if it also works for you.\n> \n> It seems we need those files:\n> `models/OccamRazor_mpt-7b-storywriter-4bit-128g\
    \ does not appear to have a file named configuration_mpt.py.`\n\nThat is a separate\
    \ issue addressed here: https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/5\n\
    \n> Can you provide some short instructions for setting up this model with KoboldAI?\
    \ I've installed KoboldAI (with the link provided) and loaded the model, but the\
    \ responses are completely unusable. I'm assuming there are some settings I need\
    \ to adjust.\n\nAlso a separate issue addressed here: https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/4\n\
    \n> INFO       | __main__:do_connect:3545 - Client connected!\n> Exception in\
    \ thread Thread-12:\n> Traceback (most recent call last):\n>   File \"B:\\python\\\
    lib\\threading.py\", line 932, in _bootstrap_inner\n>     self.run()\n>   File\
    \ \"B:\\python\\lib\\threading.py\", line 870, in run\n>     self._target(*self._args,\
    \ **self._kwargs)\n>   File \"B:\\python\\lib\\site-packages\\socketio\\server.py\"\
    , line 731, in _handle_event_internal\n>     r = server._trigger_event(data[0],\
    \ namespace, sid, *data[1:])\n>   File \"B:\\python\\lib\\site-packages\\socketio\\\
    server.py\", line 756, in _trigger_event\n>     return self.handlers[namespace][event](*args)\n\
    >   File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\", line\
    \ 282, in _handler\n>     return self._handle_event(handler, message, namespace,\
    \ sid,\n>   File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
    , line 828, in _handle_event\n>     ret = handler(*args)\n>   File \"aiserver.py\"\
    , line 469, in g\n>     return f(*a, **k)\n>   File \"aiserver.py\", line 3955,\
    \ in get_message\n>     get_model_info(msg['data'], directory=msg['path'])\n>\
    \   File \"aiserver.py\", line 1550, in get_model_info\n>     layer_count = get_layer_count(model,\
    \ directory=directory)\n>   File \"aiserver.py\", line 1596, in get_layer_count\n\
    >     model_config = AutoConfig.from_pretrained(model.replace('/', '_'), revision=args.revision,\
    \ cache_dir=\"cache\")\n>   File \"B:\\python\\lib\\site-packages\\transformers\\\
    models\\auto\\configuration_auto.py\", line 779, in from_pretrained\n>     raise\
    \ ValueError(\n> ValueError: Loading D:\\KoboldAI\\models\\OccamRazor_mpt-7b-storywriter-4bit-128g\
    \ requires you to execute the configuration file in that repo on your local machine.\
    \ Make sure you have read the code there to avoid malicious use, then set the\
    \ option `trust_remote_code=True` to remove this error.\n> WARNING    | __main__:load_model:2259\
    \ - No model type detected, assuming Neo (If this is a GPT2 model use the other\
    \ menu option or --model GPT2Custom)\n> INIT       | Searching  | GPU support\n\
    > INIT       | Found      | GPU support\n> INIT       | Starting   | Transformers\n\
    > WARNING    | __main__:device_config:840 - --breakmodel_gpulayers is malformatted.\
    \ Please use the --help option to see correct usage of --breakmodel_gpulayers.\
    \ Defaulting to all layers on device 0.\n> INIT       | Info       | Final device\
    \ configuration:\n>        DEVICE ID  |  LAYERS  |  DEVICE NAME\n> Exception in\
    \ thread Thread-13:\n> Traceback (most recent call last):\n>   File \"B:\\python\\\
    lib\\threading.py\", line 932, in _bootstrap_inner\n>     self.run()\n>   File\
    \ \"B:\\python\\lib\\threading.py\", line 870, in run\n>     self._target(*self._args,\
    \ **self._kwargs)\n>   File \"B:\\python\\lib\\site-packages\\socketio\\server.py\"\
    , line 731, in _handle_event_internal\n>     r = server._trigger_event(data[0],\
    \ namespace, sid, *data[1:])\n>   File \"B:\\python\\lib\\site-packages\\socketio\\\
    server.py\", line 756, in _trigger_event\n>     return self.handlers[namespace][event](*args)\n\
    >   File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\", line\
    \ 282, in _handler\n>     return self._handle_event(handler, message, namespace,\
    \ sid,\n>   File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
    , line 828, in _handle_event\n>     ret = handler(*args)\n>   File \"aiserver.py\"\
    , line 469, in g\n>     return f(*a, **k)\n>   File \"aiserver.py\", line 3918,\
    \ in get_message\n>     load_model(use_gpu=msg['use_gpu'], gpu_layers=msg['gpu_layers'],\
    \ disk_layers=msg['disk_layers'], online_model=msg['online_model'])\n>   File\
    \ \"aiserver.py\", line 2526, in load_model\n>     device_config(model_config)\n\
    >   File \"aiserver.py\", line 907, in device_config\n>     device_list(n_layers,\
    \ primary=breakmodel.primary_device)\n>   File \"aiserver.py\", line 805, in device_list\n\
    >     print(f\"{row_color}{colors.YELLOW + '->' + row_color if i == selected else\
    \ '  '} {'(primary)' if i == primary else ' '*9} {i:3}  {sep_color}|{row_color}\
    \     {gpu_blocks[i]:3}  {sep_color}|{row_color}  {name}{colors.END}\")\n> TypeError:\
    \ unsupported format string passed to NoneType.__format__\n\nThis is also a separate\
    \ issue addressed here: https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/3\n\
    \nYou need to set the option trust_remote_code=True.\n\nWe shouldn't mix every\
    \ issue in one thread. Let's get back on topic."
  created_at: 2023-05-10 04:47:56+00:00
  edited: true
  hidden: false
  id: 645b300c80ba386ec57103a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-05-10T23:43:03.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>Yes, there was an issue with the init device. I changed the model and
          removed the unused python files, it is now working properly for me. Let
          me know if it also works for you.</p>

          </blockquote>

          <p>It seems we need those files:<br><code>models/OccamRazor_mpt-7b-storywriter-4bit-128g
          does not appear to have a file named configuration_mpt.py.</code></p>

          </blockquote>

          <p>That is a separate issue addressed here: <a href="https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/5">https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/5</a></p>

          <blockquote>

          <p>Can you provide some short instructions for setting up this model with
          KoboldAI? I''ve installed KoboldAI (with the link provided) and loaded the
          model, but the responses are completely unusable. I''m assuming there are
          some settings I need to adjust.</p>

          </blockquote>

          <p>Also a separate issue addressed here: <a href="https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/4">https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/4</a></p>

          <blockquote>

          <p>INFO       | <strong>main</strong>:do_connect:3545 - Client connected!<br>Exception
          in thread Thread-12:<br>Traceback (most recent call last):<br>  File "B:\python\lib\threading.py",
          line 932, in _bootstrap_inner<br>    self.run()<br>  File "B:\python\lib\threading.py",
          line 870, in run<br>    self._target(*self._args, **self._kwargs)<br>  File
          "B:\python\lib\site-packages\socketio\server.py", line 731, in _handle_event_internal<br>    r
          = server._trigger_event(data[0], namespace, sid, *data[1:])<br>  File "B:\python\lib\site-packages\socketio\server.py",
          line 756, in <em>trigger_event<br>    return self.handlers[namespace]<a
          rel="nofollow" href="*args">event</a><br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 282, in _handler<br>    return self.<em>handle_event(handler, message,
          namespace, sid,<br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 828, in <em>handle_event<br>    ret = handler(*args)<br>  File "aiserver.py",
          line 469, in g<br>    return f(*a, **k)<br>  File "aiserver.py", line 3955,
          in get_message<br>    get_model_info(msg[''data''], directory=msg[''path''])<br>  File
          "aiserver.py", line 1550, in get_model_info<br>    layer_count = get_layer_count(model,
          directory=directory)<br>  File "aiserver.py", line 1596, in get_layer_count<br>    model_config
          = AutoConfig.from_pretrained(model.replace(''/'', ''</em>''), revision=args.revision,
          cache_dir="cache")<br>  File "B:\python\lib\site-packages\transformers\models\auto\configuration_auto.py",
          line 779, in from_pretrained<br>    raise ValueError(<br>ValueError: Loading
          D:\KoboldAI\models\OccamRazor_mpt-7b-storywriter-4bit-128g requires you
          to execute the configuration file in that repo on your local machine. Make
          sure you have read the code there to avoid malicious use, then set the option
          <code>trust_remote_code=True</code> to remove this error.<br>WARNING    |
          <strong>main</strong>:load_model:2259 - No model type detected, assuming
          Neo (If this is a GPT2 model use the other menu option or --model GPT2Custom)<br>INIT       |
          Searching  | GPU support<br>INIT       | Found      | GPU support<br>INIT       |
          Starting   | Transformers<br>WARNING    | <strong>main</strong>:device_config:840
          - --breakmodel_gpulayers is malformatted. Please use the --help option to
          see correct usage of --breakmodel_gpulayers. Defaulting to all layers on
          device 0.<br>INIT       | Info       | Final device configuration:<br>       DEVICE
          ID  |  LAYERS  |  DEVICE NAME<br>Exception in thread Thread-13:<br>Traceback
          (most recent call last):<br>  File "B:\python\lib\threading.py", line 932,
          in _bootstrap_inner<br>    self.run()<br>  File "B:\python\lib\threading.py",
          line 870, in run<br>    self._target(*self._args, **self._kwargs)<br>  File
          "B:\python\lib\site-packages\socketio\server.py", line 731, in _handle_event_internal<br>    r
          = server._trigger_event(data[0], namespace, sid, *data[1:])<br>  File "B:\python\lib\site-packages\socketio\server.py",
          line 756, in <em>trigger_event<br>    return self.handlers[namespace]<a
          rel="nofollow" href="*args">event</a><br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 282, in _handler<br>    return self.<em>handle_event(handler, message,
          namespace, sid,<br>  File "B:\python\lib\site-packages\flask_socketio_<em>init</em></em>.py",
          line 828, in _handle_event<br>    ret = handler(*args)<br>  File "aiserver.py",
          line 469, in g<br>    return f(*a, **k)<br>  File "aiserver.py", line 3918,
          in get_message<br>    load_model(use_gpu=msg[''use_gpu''], gpu_layers=msg[''gpu_layers''],
          disk_layers=msg[''disk_layers''], online_model=msg[''online_model''])<br>  File
          "aiserver.py", line 2526, in load_model<br>    device_config(model_config)<br>  File
          "aiserver.py", line 907, in device_config<br>    device_list(n_layers, primary=breakmodel.primary_device)<br>  File
          "aiserver.py", line 805, in device_list<br>    print(f"{row_color}{colors.YELLOW
          + ''-&gt;'' + row_color if i == selected else ''  ''} {''(primary)'' if
          i == primary else '' ''*9} {i:3}  {sep_color}|{row_color}     {gpu_blocks[i]:3}  {sep_color}|{row_color}  {name}{colors.END}")<br>TypeError:
          unsupported format string passed to NoneType.<strong>format</strong></p>

          </blockquote>

          <p>This is also a separate issue addressed here: <a href="https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/3">https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/3</a></p>

          <p>You need to set the option trust_remote_code=True.</p>

          <p>We shouldn''t mix every issue in one thread. Let''s get back on topic.</p>

          </blockquote>

          <p>How do you set trust_remote_code=True in KoboldAI?</p>

          '
        raw: "> > > Yes, there was an issue with the init device. I changed the model\
          \ and removed the unused python files, it is now working properly for me.\
          \ Let me know if it also works for you.\n> > \n> > It seems we need those\
          \ files:\n> > `models/OccamRazor_mpt-7b-storywriter-4bit-128g does not appear\
          \ to have a file named configuration_mpt.py.`\n> \n> That is a separate\
          \ issue addressed here: https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/5\n\
          > \n> > Can you provide some short instructions for setting up this model\
          \ with KoboldAI? I've installed KoboldAI (with the link provided) and loaded\
          \ the model, but the responses are completely unusable. I'm assuming there\
          \ are some settings I need to adjust.\n> \n> Also a separate issue addressed\
          \ here: https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/4\n\
          > \n> > INFO       | __main__:do_connect:3545 - Client connected!\n> > Exception\
          \ in thread Thread-12:\n> > Traceback (most recent call last):\n> >   File\
          \ \"B:\\python\\lib\\threading.py\", line 932, in _bootstrap_inner\n> >\
          \     self.run()\n> >   File \"B:\\python\\lib\\threading.py\", line 870,\
          \ in run\n> >     self._target(*self._args, **self._kwargs)\n> >   File\
          \ \"B:\\python\\lib\\site-packages\\socketio\\server.py\", line 731, in\
          \ _handle_event_internal\n> >     r = server._trigger_event(data[0], namespace,\
          \ sid, *data[1:])\n> >   File \"B:\\python\\lib\\site-packages\\socketio\\\
          server.py\", line 756, in _trigger_event\n> >     return self.handlers[namespace][event](*args)\n\
          > >   File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
          , line 282, in _handler\n> >     return self._handle_event(handler, message,\
          \ namespace, sid,\n> >   File \"B:\\python\\lib\\site-packages\\flask_socketio\\\
          __init__.py\", line 828, in _handle_event\n> >     ret = handler(*args)\n\
          > >   File \"aiserver.py\", line 469, in g\n> >     return f(*a, **k)\n\
          > >   File \"aiserver.py\", line 3955, in get_message\n> >     get_model_info(msg['data'],\
          \ directory=msg['path'])\n> >   File \"aiserver.py\", line 1550, in get_model_info\n\
          > >     layer_count = get_layer_count(model, directory=directory)\n> > \
          \  File \"aiserver.py\", line 1596, in get_layer_count\n> >     model_config\
          \ = AutoConfig.from_pretrained(model.replace('/', '_'), revision=args.revision,\
          \ cache_dir=\"cache\")\n> >   File \"B:\\python\\lib\\site-packages\\transformers\\\
          models\\auto\\configuration_auto.py\", line 779, in from_pretrained\n> >\
          \     raise ValueError(\n> > ValueError: Loading D:\\KoboldAI\\models\\\
          OccamRazor_mpt-7b-storywriter-4bit-128g requires you to execute the configuration\
          \ file in that repo on your local machine. Make sure you have read the code\
          \ there to avoid malicious use, then set the option `trust_remote_code=True`\
          \ to remove this error.\n> > WARNING    | __main__:load_model:2259 - No\
          \ model type detected, assuming Neo (If this is a GPT2 model use the other\
          \ menu option or --model GPT2Custom)\n> > INIT       | Searching  | GPU\
          \ support\n> > INIT       | Found      | GPU support\n> > INIT       | Starting\
          \   | Transformers\n> > WARNING    | __main__:device_config:840 - --breakmodel_gpulayers\
          \ is malformatted. Please use the --help option to see correct usage of\
          \ --breakmodel_gpulayers. Defaulting to all layers on device 0.\n> > INIT\
          \       | Info       | Final device configuration:\n> >        DEVICE ID\
          \  |  LAYERS  |  DEVICE NAME\n> > Exception in thread Thread-13:\n> > Traceback\
          \ (most recent call last):\n> >   File \"B:\\python\\lib\\threading.py\"\
          , line 932, in _bootstrap_inner\n> >     self.run()\n> >   File \"B:\\python\\\
          lib\\threading.py\", line 870, in run\n> >     self._target(*self._args,\
          \ **self._kwargs)\n> >   File \"B:\\python\\lib\\site-packages\\socketio\\\
          server.py\", line 731, in _handle_event_internal\n> >     r = server._trigger_event(data[0],\
          \ namespace, sid, *data[1:])\n> >   File \"B:\\python\\lib\\site-packages\\\
          socketio\\server.py\", line 756, in _trigger_event\n> >     return self.handlers[namespace][event](*args)\n\
          > >   File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
          , line 282, in _handler\n> >     return self._handle_event(handler, message,\
          \ namespace, sid,\n> >   File \"B:\\python\\lib\\site-packages\\flask_socketio\\\
          __init__.py\", line 828, in _handle_event\n> >     ret = handler(*args)\n\
          > >   File \"aiserver.py\", line 469, in g\n> >     return f(*a, **k)\n\
          > >   File \"aiserver.py\", line 3918, in get_message\n> >     load_model(use_gpu=msg['use_gpu'],\
          \ gpu_layers=msg['gpu_layers'], disk_layers=msg['disk_layers'], online_model=msg['online_model'])\n\
          > >   File \"aiserver.py\", line 2526, in load_model\n> >     device_config(model_config)\n\
          > >   File \"aiserver.py\", line 907, in device_config\n> >     device_list(n_layers,\
          \ primary=breakmodel.primary_device)\n> >   File \"aiserver.py\", line 805,\
          \ in device_list\n> >     print(f\"{row_color}{colors.YELLOW + '->' + row_color\
          \ if i == selected else '  '} {'(primary)' if i == primary else ' '*9} {i:3}\
          \  {sep_color}|{row_color}     {gpu_blocks[i]:3}  {sep_color}|{row_color}\
          \  {name}{colors.END}\")\n> > TypeError: unsupported format string passed\
          \ to NoneType.__format__\n> \n> This is also a separate issue addressed\
          \ here: https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/3\n\
          > \n> You need to set the option trust_remote_code=True.\n> \n> We shouldn't\
          \ mix every issue in one thread. Let's get back on topic.\n\nHow do you\
          \ set trust_remote_code=True in KoboldAI?"
        updatedAt: '2023-05-10T23:43:03.839Z'
      numEdits: 0
      reactions: []
    id: 645c2c07d66656e908ac346e
    type: comment
  author: CR2022
  content: "> > > Yes, there was an issue with the init device. I changed the model\
    \ and removed the unused python files, it is now working properly for me. Let\
    \ me know if it also works for you.\n> > \n> > It seems we need those files:\n\
    > > `models/OccamRazor_mpt-7b-storywriter-4bit-128g does not appear to have a\
    \ file named configuration_mpt.py.`\n> \n> That is a separate issue addressed\
    \ here: https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/5\n\
    > \n> > Can you provide some short instructions for setting up this model with\
    \ KoboldAI? I've installed KoboldAI (with the link provided) and loaded the model,\
    \ but the responses are completely unusable. I'm assuming there are some settings\
    \ I need to adjust.\n> \n> Also a separate issue addressed here: https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/4\n\
    > \n> > INFO       | __main__:do_connect:3545 - Client connected!\n> > Exception\
    \ in thread Thread-12:\n> > Traceback (most recent call last):\n> >   File \"\
    B:\\python\\lib\\threading.py\", line 932, in _bootstrap_inner\n> >     self.run()\n\
    > >   File \"B:\\python\\lib\\threading.py\", line 870, in run\n> >     self._target(*self._args,\
    \ **self._kwargs)\n> >   File \"B:\\python\\lib\\site-packages\\socketio\\server.py\"\
    , line 731, in _handle_event_internal\n> >     r = server._trigger_event(data[0],\
    \ namespace, sid, *data[1:])\n> >   File \"B:\\python\\lib\\site-packages\\socketio\\\
    server.py\", line 756, in _trigger_event\n> >     return self.handlers[namespace][event](*args)\n\
    > >   File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\", line\
    \ 282, in _handler\n> >     return self._handle_event(handler, message, namespace,\
    \ sid,\n> >   File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
    , line 828, in _handle_event\n> >     ret = handler(*args)\n> >   File \"aiserver.py\"\
    , line 469, in g\n> >     return f(*a, **k)\n> >   File \"aiserver.py\", line\
    \ 3955, in get_message\n> >     get_model_info(msg['data'], directory=msg['path'])\n\
    > >   File \"aiserver.py\", line 1550, in get_model_info\n> >     layer_count\
    \ = get_layer_count(model, directory=directory)\n> >   File \"aiserver.py\", line\
    \ 1596, in get_layer_count\n> >     model_config = AutoConfig.from_pretrained(model.replace('/',\
    \ '_'), revision=args.revision, cache_dir=\"cache\")\n> >   File \"B:\\python\\\
    lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line\
    \ 779, in from_pretrained\n> >     raise ValueError(\n> > ValueError: Loading\
    \ D:\\KoboldAI\\models\\OccamRazor_mpt-7b-storywriter-4bit-128g requires you to\
    \ execute the configuration file in that repo on your local machine. Make sure\
    \ you have read the code there to avoid malicious use, then set the option `trust_remote_code=True`\
    \ to remove this error.\n> > WARNING    | __main__:load_model:2259 - No model\
    \ type detected, assuming Neo (If this is a GPT2 model use the other menu option\
    \ or --model GPT2Custom)\n> > INIT       | Searching  | GPU support\n> > INIT\
    \       | Found      | GPU support\n> > INIT       | Starting   | Transformers\n\
    > > WARNING    | __main__:device_config:840 - --breakmodel_gpulayers is malformatted.\
    \ Please use the --help option to see correct usage of --breakmodel_gpulayers.\
    \ Defaulting to all layers on device 0.\n> > INIT       | Info       | Final device\
    \ configuration:\n> >        DEVICE ID  |  LAYERS  |  DEVICE NAME\n> > Exception\
    \ in thread Thread-13:\n> > Traceback (most recent call last):\n> >   File \"\
    B:\\python\\lib\\threading.py\", line 932, in _bootstrap_inner\n> >     self.run()\n\
    > >   File \"B:\\python\\lib\\threading.py\", line 870, in run\n> >     self._target(*self._args,\
    \ **self._kwargs)\n> >   File \"B:\\python\\lib\\site-packages\\socketio\\server.py\"\
    , line 731, in _handle_event_internal\n> >     r = server._trigger_event(data[0],\
    \ namespace, sid, *data[1:])\n> >   File \"B:\\python\\lib\\site-packages\\socketio\\\
    server.py\", line 756, in _trigger_event\n> >     return self.handlers[namespace][event](*args)\n\
    > >   File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\", line\
    \ 282, in _handler\n> >     return self._handle_event(handler, message, namespace,\
    \ sid,\n> >   File \"B:\\python\\lib\\site-packages\\flask_socketio\\__init__.py\"\
    , line 828, in _handle_event\n> >     ret = handler(*args)\n> >   File \"aiserver.py\"\
    , line 469, in g\n> >     return f(*a, **k)\n> >   File \"aiserver.py\", line\
    \ 3918, in get_message\n> >     load_model(use_gpu=msg['use_gpu'], gpu_layers=msg['gpu_layers'],\
    \ disk_layers=msg['disk_layers'], online_model=msg['online_model'])\n> >   File\
    \ \"aiserver.py\", line 2526, in load_model\n> >     device_config(model_config)\n\
    > >   File \"aiserver.py\", line 907, in device_config\n> >     device_list(n_layers,\
    \ primary=breakmodel.primary_device)\n> >   File \"aiserver.py\", line 805, in\
    \ device_list\n> >     print(f\"{row_color}{colors.YELLOW + '->' + row_color if\
    \ i == selected else '  '} {'(primary)' if i == primary else ' '*9} {i:3}  {sep_color}|{row_color}\
    \     {gpu_blocks[i]:3}  {sep_color}|{row_color}  {name}{colors.END}\")\n> > TypeError:\
    \ unsupported format string passed to NoneType.__format__\n> \n> This is also\
    \ a separate issue addressed here: https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g/discussions/3\n\
    > \n> You need to set the option trust_remote_code=True.\n> \n> We shouldn't mix\
    \ every issue in one thread. Let's get back on topic.\n\nHow do you set trust_remote_code=True\
    \ in KoboldAI?"
  created_at: 2023-05-10 22:43:03+00:00
  edited: false
  hidden: false
  id: 645c2c07d66656e908ac346e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9199feb9a773b79032636645b1180ea1.svg
      fullname: Pro Patte
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProPatte
      type: user
    createdAt: '2023-05-11T09:53:58.000Z'
    data:
      edited: true
      editors:
      - ProPatte
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9199feb9a773b79032636645b1180ea1.svg
          fullname: Pro Patte
          isHf: false
          isPro: false
          name: ProPatte
          type: user
        html: '<blockquote>

          <p>How do you set trust_remote_code=True in KoboldAI?</p>

          </blockquote>

          <p>The best way to figure that out would be to ask this in the right thread
          as already mentioned and linked before ;)</p>

          '
        raw: '> How do you set trust_remote_code=True in KoboldAI?


          The best way to figure that out would be to ask this in the right thread
          as already mentioned and linked before ;)'
        updatedAt: '2023-05-11T09:54:42.059Z'
      numEdits: 1
      reactions: []
    id: 645cbb36afa77201e2f82bc9
    type: comment
  author: ProPatte
  content: '> How do you set trust_remote_code=True in KoboldAI?


    The best way to figure that out would be to ask this in the right thread as already
    mentioned and linked before ;)'
  created_at: 2023-05-11 08:53:58+00:00
  edited: true
  hidden: false
  id: 645cbb36afa77201e2f82bc9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: OccamRazor/mpt-7b-storywriter-4bit-128g
repo_type: model
status: open
target_branch: null
title: Oobabooga cannot determine model type
