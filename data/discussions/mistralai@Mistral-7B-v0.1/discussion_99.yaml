!!python/object:huggingface_hub.community.DiscussionWithDetails
author: philgrey
conflicting_files: null
created_at: 2023-11-29 16:10:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ixKl5PVwauxFVuRMwv-ON.jpeg?w=200&h=200&f=face
      fullname: John Grey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philgrey
      type: user
    createdAt: '2023-11-29T16:10:52.000Z'
    data:
      edited: false
      editors:
      - philgrey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6332561373710632
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ixKl5PVwauxFVuRMwv-ON.jpeg?w=200&h=200&f=face
          fullname: John Grey
          isHf: false
          isPro: false
          name: philgrey
          type: user
        html: '<p>I''ve deployed this model to AWS sagemaker and now, I wanna use
          endpoint.<br>I''m getting response but its length is too short<br>following
          is my code for deployment.</p>

          <p>hub = {<br>    ''HF_MODEL_ID'':''mistralai/Mistral-7B-v0.1'',<br>    ''SM_NUM_GPUS'':
          json.dumps(1),<br>    ''MAX_TOTAL_TOKENS'': json.dumps(4096)<br>}</p>

          <h1 id="create-hugging-face-model-class">create Hugging Face Model Class</h1>

          <p>huggingface_model = HuggingFaceModel(<br>    image_uri=get_huggingface_llm_image_uri("huggingface",version="1.1.0"),<br>    env=hub,<br>    role=role,<br>)</p>

          <h1 id="deploy-model-to-sagemaker-inference">deploy model to SageMaker Inference</h1>

          <p>predictor = huggingface_model.deploy(<br>    initial_instance_count=1,<br>    instance_type="ml.g5.4xlarge",<br>    container_startup_health_check_timeout=300,<br>  )</p>

          <p>and this is code for response generation</p>

          <p>llm = SagemakerEndpoint(<br>        endpoint_name=endpoint,<br>        region_name="eu-west-2",<br>        model_kwargs={<br>            "temperature":
          0,<br>            "maxTokens": 4096,<br>            "numResults": 3<br>        },<br>        content_handler=content_handler,<br>    )</p>

          <p>llm.generate(["nice to meet you"])</p>

          '
        raw: "I've deployed this model to AWS sagemaker and now, I wanna use endpoint.\r\
          \nI'm getting response but its length is too short \r\nfollowing is my code\
          \ for deployment.\r\n\r\nhub = {\r\n\t'HF_MODEL_ID':'mistralai/Mistral-7B-v0.1',\r\
          \n\t'SM_NUM_GPUS': json.dumps(1),\r\n    'MAX_TOTAL_TOKENS': json.dumps(4096)\r\
          \n}\r\n\r\n\r\n\r\n# create Hugging Face Model Class\r\nhuggingface_model\
          \ = HuggingFaceModel(\r\n\timage_uri=get_huggingface_llm_image_uri(\"huggingface\"\
          ,version=\"1.1.0\"),\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n\r\n# deploy\
          \ model to SageMaker Inference\r\npredictor = huggingface_model.deploy(\r\
          \n\tinitial_instance_count=1,\r\n\tinstance_type=\"ml.g5.4xlarge\",\r\n\t\
          container_startup_health_check_timeout=300,\r\n  )\r\n\r\nand this is code\
          \ for response generation\r\n\r\nllm = SagemakerEndpoint(\r\n        endpoint_name=endpoint,\r\
          \n        region_name=\"eu-west-2\",\r\n        model_kwargs={\r\n     \
          \       \"temperature\": 0,\r\n            \"maxTokens\": 4096,\r\n    \
          \        \"numResults\": 3\r\n        },\r\n        content_handler=content_handler,\r\
          \n    )\r\n\r\nllm.generate([\"nice to meet you\"])"
        updatedAt: '2023-11-29T16:10:52.964Z'
      numEdits: 0
      reactions: []
    id: 6567628cbd65fd41ee4ba17c
    type: comment
  author: philgrey
  content: "I've deployed this model to AWS sagemaker and now, I wanna use endpoint.\r\
    \nI'm getting response but its length is too short \r\nfollowing is my code for\
    \ deployment.\r\n\r\nhub = {\r\n\t'HF_MODEL_ID':'mistralai/Mistral-7B-v0.1',\r\
    \n\t'SM_NUM_GPUS': json.dumps(1),\r\n    'MAX_TOTAL_TOKENS': json.dumps(4096)\r\
    \n}\r\n\r\n\r\n\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\
    \n\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"\
    ),\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n\r\n# deploy model to SageMaker Inference\r\
    \npredictor = huggingface_model.deploy(\r\n\tinitial_instance_count=1,\r\n\tinstance_type=\"\
    ml.g5.4xlarge\",\r\n\tcontainer_startup_health_check_timeout=300,\r\n  )\r\n\r\
    \nand this is code for response generation\r\n\r\nllm = SagemakerEndpoint(\r\n\
    \        endpoint_name=endpoint,\r\n        region_name=\"eu-west-2\",\r\n   \
    \     model_kwargs={\r\n            \"temperature\": 0,\r\n            \"maxTokens\"\
    : 4096,\r\n            \"numResults\": 3\r\n        },\r\n        content_handler=content_handler,\r\
    \n    )\r\n\r\nllm.generate([\"nice to meet you\"])"
  created_at: 2023-11-29 16:10:52+00:00
  edited: false
  hidden: false
  id: 6567628cbd65fd41ee4ba17c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 99
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: how to increase response max token size
