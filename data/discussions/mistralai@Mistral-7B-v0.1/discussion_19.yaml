!!python/object:huggingface_hub.community.DiscussionWithDetails
author: chao0524
conflicting_files: null
created_at: 2023-09-29 10:21:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9f6d21de185e903b67831c1b88779af5.svg
      fullname: wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chao0524
      type: user
    createdAt: '2023-09-29T11:21:10.000Z'
    data:
      edited: false
      editors:
      - chao0524
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9082520604133606
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9f6d21de185e903b67831c1b88779af5.svg
          fullname: wang
          isHf: false
          isPro: false
          name: chao0524
          type: user
        html: '<p>Sorry for asking this silly question. I''m just wondering how I
          can use this model in my local Jupyter notebook! Can anyone help me out?
          Thank you!</p>

          '
        raw: Sorry for asking this silly question. I'm just wondering how I can use
          this model in my local Jupyter notebook! Can anyone help me out? Thank you!
        updatedAt: '2023-09-29T11:21:10.679Z'
      numEdits: 0
      reactions: []
    id: 6516b32601d873dd1927b27f
    type: comment
  author: chao0524
  content: Sorry for asking this silly question. I'm just wondering how I can use
    this model in my local Jupyter notebook! Can anyone help me out? Thank you!
  created_at: 2023-09-29 10:21:10+00:00
  edited: false
  hidden: false
  id: 6516b32601d873dd1927b27f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70c7c10ecc9e0cd3a3cdc79c797d2b07.svg
      fullname: Camille SCHNAKENBOURG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CamilleSchnak
      type: user
    createdAt: '2023-10-04T07:20:14.000Z'
    data:
      edited: false
      editors:
      - CamilleSchnak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9556803107261658
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70c7c10ecc9e0cd3a3cdc79c797d2b07.svg
          fullname: Camille SCHNAKENBOURG
          isHf: false
          isPro: false
          name: CamilleSchnak
          type: user
        html: '<p>I made it work with ooga booga.<br>model loader "transformers" and
          "load in 4 bits".<br>it take ages to load and answer not that quickly but
          it work.</p>

          '
        raw: 'I made it work with ooga booga.

          model loader "transformers" and "load in 4 bits".

          it take ages to load and answer not that quickly but it work.'
        updatedAt: '2023-10-04T07:20:14.461Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - oklahomaguy23
        - elboertjie
    id: 651d122e902c466ff379e703
    type: comment
  author: CamilleSchnak
  content: 'I made it work with ooga booga.

    model loader "transformers" and "load in 4 bits".

    it take ages to load and answer not that quickly but it work.'
  created_at: 2023-10-04 06:20:14+00:00
  edited: false
  hidden: false
  id: 651d122e902c466ff379e703
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e88350c8d905ab09a913cd5a37db028a.svg
      fullname: elboertjieJ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: elboertjie
      type: user
    createdAt: '2023-10-15T16:30:19.000Z'
    data:
      edited: false
      editors:
      - elboertjie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2865455150604248
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e88350c8d905ab09a913cd5a37db028a.svg
          fullname: elboertjieJ
          isHf: false
          isPro: false
          name: elboertjie
          type: user
        html: "<blockquote>\n<p>I made it work with ooga booga.<br>model loader \"\
          transformers\" and \"load in 4 bits\".<br>it take ages to load and answer\
          \ not that quickly but it work.</p>\n</blockquote>\n<p>How did you manage\
          \ to download the model?</p>\n<p>I'm using Ubuntu and when I execute the\
          \ three lines below, I get the error at the bottom:</p>\n<p>from transformers\
          \ import AutoTokenizer, AutoModelForCausalLM<br>tokenizer = AutoTokenizer.from_pretrained(\"\
          mistralai/Mistral-7B-v0.1\")<br>model = AutoModelForCausalLM.from_pretrained(\"\
          mistralai/Mistral-7B-v0.1\")</p>\n<p>Downloading (\u2026)okenizer_config.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 966/966 [00:00&lt;00:00, 4.40MB/s]<br>Downloading tokenizer.model:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 493k/493k [00:00&lt;00:00,\
          \ 18.9MB/s]<br>Downloading (\u2026)/main/tokenizer.json: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.80M/1.80M [00:00&lt;00:00,\
          \ 15.7MB/s]<br>Downloading (\u2026)cial_tokens_map.json: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 72.0/72.0\
          \ [00:00&lt;00:00, 389kB/s]<br>Downloading (\u2026)lve/main/config.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 571/571 [00:00&lt;00:00, 2.84MB/s]<br>Traceback (most\
          \ recent call last):<br>  File \"/home/j/jr_loadmodel\", line 39, in <br>\
          \    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\"\
          )<br>  File \"/home/j/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 444, in from_pretrained<br>    config, kwargs = AutoConfig.from_pretrained(<br>\
          \  File \"/home/j/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 940, in from_pretrained<br>    config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]<br>  File \"/home/j/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 655, in <strong>getitem</strong><br>    raise KeyError(key)<br>KeyError:\
          \ 'mistral'</p>\n"
        raw: "> I made it work with ooga booga.\n> model loader \"transformers\" and\
          \ \"load in 4 bits\".\n> it take ages to load and answer not that quickly\
          \ but it work.\n\nHow did you manage to download the model?\n\nI'm using\
          \ Ubuntu and when I execute the three lines below, I get the error at the\
          \ bottom:\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\"\
          )\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\"\
          )\n\nDownloading (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 966/966 [00:00<00:00,\
          \ 4.40MB/s]\nDownloading tokenizer.model: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 493k/493k [00:00<00:00, 18.9MB/s]\nDownloading (\u2026\
          )/main/tokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 1.80M/1.80M [00:00<00:00, 15.7MB/s]\nDownloading (\u2026)cial_tokens_map.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 72.0/72.0 [00:00<00:00, 389kB/s]\nDownloading (\u2026)lve/main/config.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 571/571 [00:00<00:00, 2.84MB/s]\nTraceback (most recent\
          \ call last):\n  File \"/home/j/jr_loadmodel\", line 39, in <module>\n \
          \   model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\"\
          )\n  File \"/home/j/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 444, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\n\
          \  File \"/home/j/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 940, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]\n  File \"/home/j/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 655, in __getitem__\n    raise KeyError(key)\nKeyError: 'mistral'\n\
          \n\n\n"
        updatedAt: '2023-10-15T16:30:19.473Z'
      numEdits: 0
      reactions: []
    id: 652c139b62a488518999951d
    type: comment
  author: elboertjie
  content: "> I made it work with ooga booga.\n> model loader \"transformers\" and\
    \ \"load in 4 bits\".\n> it take ages to load and answer not that quickly but\
    \ it work.\n\nHow did you manage to download the model?\n\nI'm using Ubuntu and\
    \ when I execute the three lines below, I get the error at the bottom:\n\nfrom\
    \ transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"\
    mistralai/Mistral-7B-v0.1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    mistralai/Mistral-7B-v0.1\")\n\nDownloading (\u2026)okenizer_config.json: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 966/966 [00:00<00:00,\
    \ 4.40MB/s]\nDownloading tokenizer.model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 493k/493k\
    \ [00:00<00:00, 18.9MB/s]\nDownloading (\u2026)/main/tokenizer.json: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.80M/1.80M [00:00<00:00, 15.7MB/s]\n\
    Downloading (\u2026)cial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 72.0/72.0 [00:00<00:00, 389kB/s]\nDownloading\
    \ (\u2026)lve/main/config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 571/571 [00:00<00:00, 2.84MB/s]\nTraceback (most\
    \ recent call last):\n  File \"/home/j/jr_loadmodel\", line 39, in <module>\n\
    \    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\"\
    )\n  File \"/home/j/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 444, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\n\
    \  File \"/home/j/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 940, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
    model_type\"]]\n  File \"/home/j/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 655, in __getitem__\n    raise KeyError(key)\nKeyError: 'mistral'\n\n\n\
    \n"
  created_at: 2023-10-15 15:30:19+00:00
  edited: false
  hidden: false
  id: 652c139b62a488518999951d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e88350c8d905ab09a913cd5a37db028a.svg
      fullname: elboertjieJ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: elboertjie
      type: user
    createdAt: '2023-10-15T16:40:55.000Z'
    data:
      edited: false
      editors:
      - elboertjie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8565638065338135
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e88350c8d905ab09a913cd5a37db028a.svg
          fullname: elboertjieJ
          isHf: false
          isPro: false
          name: elboertjie
          type: user
        html: '<p>I found the answer:</p>

          <ol>

          <li>Re-install transformers</li>

          <li>pip install accelerate==0.20.3</li>

          </ol>

          <p>That worked for me</p>

          '
        raw: 'I found the answer:


          1) Re-install transformers

          2) pip install accelerate==0.20.3


          That worked for me'
        updatedAt: '2023-10-15T16:40:55.984Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - sairanjith7095
    id: 652c16177c5365f2d128f6f3
    type: comment
  author: elboertjie
  content: 'I found the answer:


    1) Re-install transformers

    2) pip install accelerate==0.20.3


    That worked for me'
  created_at: 2023-10-15 15:40:55+00:00
  edited: false
  hidden: false
  id: 652c16177c5365f2d128f6f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cbf5cd207c2a37af504935dbb7642507.svg
      fullname: Arzoo Bapna
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arzoop
      type: user
    createdAt: '2024-01-12T17:30:14.000Z'
    data:
      edited: false
      editors:
      - arzoop
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9740123152732849
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cbf5cd207c2a37af504935dbb7642507.svg
          fullname: Arzoo Bapna
          isHf: false
          isPro: false
          name: arzoop
          type: user
        html: '<p>Hi. Could you please make your jupyter available for us to see?<br>I
          am trying to run this code in google colab, and it says that the ram is
          not enough :(. I dont know what to do now.</p>

          '
        raw: 'Hi. Could you please make your jupyter available for us to see?

          I am trying to run this code in google colab, and it says that the ram is
          not enough :(. I dont know what to do now.'
        updatedAt: '2024-01-12T17:30:14.866Z'
      numEdits: 0
      reactions: []
    id: 65a17726a80b96444b8d1051
    type: comment
  author: arzoop
  content: 'Hi. Could you please make your jupyter available for us to see?

    I am trying to run this code in google colab, and it says that the ram is not
    enough :(. I dont know what to do now.'
  created_at: 2024-01-12 17:30:14+00:00
  edited: false
  hidden: false
  id: 65a17726a80b96444b8d1051
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: How to deploy the model to local?
