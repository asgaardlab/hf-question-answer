!!python/object:huggingface_hub.community.DiscussionWithDetails
author: abdurnawaz
conflicting_files: null
created_at: 2023-10-08 15:49:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cc35dc36555d2c1d28a426031308be09.svg
      fullname: Abdur Nawaz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abdurnawaz
      type: user
    createdAt: '2023-10-08T16:49:07.000Z'
    data:
      edited: false
      editors:
      - abdurnawaz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8037439584732056
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cc35dc36555d2c1d28a426031308be09.svg
          fullname: Abdur Nawaz
          isHf: false
          isPro: false
          name: abdurnawaz
          type: user
        html: '<p>Why does tokenizer work in weird ways:</p>

          <pre><code class="language-python">tokenizer(<span class="hljs-string">"\n\n"</span>,
          add_special_tokens=<span class="hljs-literal">False</span>)

          </code></pre>

          <pre><code class="language-sh">{<span class="hljs-string">''input_ids''</span>:
          [28705, 13, 13], <span class="hljs-string">''attention_mask''</span>: [1,
          1, 1]}

          </code></pre>

          <p>But when you add a "." in the beginning:</p>

          <pre><code class="language-python">tokenizer(<span class="hljs-string">".\n\n"</span>,
          add_special_tokens=<span class="hljs-literal">False</span>)

          </code></pre>

          <pre><code class="language-sh">{<span class="hljs-string">''input_ids''</span>:
          [842, 13, 13], <span class="hljs-string">''attention_mask''</span>: [1,
          1, 1]}

          </code></pre>

          <p>Shouldn''t it be [842, 28705, 13, 13] as the token for "." is 842.</p>

          <p>I''m trying to finetune the model and I want it to stop at "\n\n" . I''m
          use a stopping criteria based on tokens and this problem is not allowing
          the model to stop generation even though it the characters "\n\n" are generated.</p>

          '
        raw: "Why does tokenizer work in weird ways:\r\n\r\n```python\r\ntokenizer(\"\
          \\n\\n\", add_special_tokens=False)\r\n```\r\n```sh\r\n{'input_ids': [28705,\
          \ 13, 13], 'attention_mask': [1, 1, 1]}\r\n```\r\n\r\nBut when you add a\
          \ \".\" in the beginning:\r\n```python\r\ntokenizer(\".\\n\\n\", add_special_tokens=False)\r\
          \n```\r\n```sh\r\n{'input_ids': [842, 13, 13], 'attention_mask': [1, 1,\
          \ 1]}\r\n```\r\n\r\nShouldn't it be [842, 28705, 13, 13] as the token for\
          \ \".\" is 842.\r\n\r\nI'm trying to finetune the model and I want it to\
          \ stop at \"\\n\\n\" . I'm use a stopping criteria based on tokens and this\
          \ problem is not allowing the model to stop generation even though it the\
          \ characters \"\\n\\n\" are generated.\r\n"
        updatedAt: '2023-10-08T16:49:07.479Z'
      numEdits: 0
      reactions: []
    id: 6522dd83c709aaca9abd2b8a
    type: comment
  author: abdurnawaz
  content: "Why does tokenizer work in weird ways:\r\n\r\n```python\r\ntokenizer(\"\
    \\n\\n\", add_special_tokens=False)\r\n```\r\n```sh\r\n{'input_ids': [28705, 13,\
    \ 13], 'attention_mask': [1, 1, 1]}\r\n```\r\n\r\nBut when you add a \".\" in\
    \ the beginning:\r\n```python\r\ntokenizer(\".\\n\\n\", add_special_tokens=False)\r\
    \n```\r\n```sh\r\n{'input_ids': [842, 13, 13], 'attention_mask': [1, 1, 1]}\r\n\
    ```\r\n\r\nShouldn't it be [842, 28705, 13, 13] as the token for \".\" is 842.\r\
    \n\r\nI'm trying to finetune the model and I want it to stop at \"\\n\\n\" . I'm\
    \ use a stopping criteria based on tokens and this problem is not allowing the\
    \ model to stop generation even though it the characters \"\\n\\n\" are generated.\r\
    \n"
  created_at: 2023-10-08 15:49:07+00:00
  edited: false
  hidden: false
  id: 6522dd83c709aaca9abd2b8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-10-08T20:42:50.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7559393644332886
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: "<p>Hey! I am not sure what is wrong here. A prefix space is added as\
          \ expected. See the tokenization:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-meta\">&gt;&gt;&gt; </span><span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\n\
          <span class=\"hljs-meta\">&gt;&gt;&gt; </span>tokenizer  = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"mistralai/Mistral-7B-v0.1\"</span>)\n<span class=\"\
          hljs-meta\">&gt;&gt;&gt; </span>tokenizer.tokenize(<span class=\"hljs-string\"\
          >\"\\n\\n\"</span>, add_special_tokens=<span class=\"hljs-literal\">False</span>)\n\
          \ [<span class=\"hljs-string\">'\u2581'</span>, <span class=\"hljs-string\"\
          >'&lt;0x0A&gt;'</span>, <span class=\"hljs-string\">'&lt;0x0A&gt;'</span>]\n\
          \n<span class=\"hljs-meta\">&gt;&gt;&gt; </span>tokenizer.tokenize(<span\
          \ class=\"hljs-string\">\".\\n\\n\"</span>, add_special_tokens=<span class=\"\
          hljs-literal\">False</span>)\n [<span class=\"hljs-string\">'\u2581.'</span>,\
          \ <span class=\"hljs-string\">'&lt;0x0A&gt;'</span>, <span class=\"hljs-string\"\
          >'&lt;0x0A&gt;'</span>]\n</code></pre>\n<p>The character \"\\n\\n\"  is\
          \ not a token in the vocab, I would recommend you to stop based on two indexes\
          \ (13,13) rather than taking the prefix space into account</p>\n"
        raw: "Hey! I am not sure what is wrong here. A prefix space is added as expected.\
          \ See the tokenization:\n```python \n>>> from transformers import AutoTokenizer\n\
          >>> tokenizer  = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\"\
          )\n>>> tokenizer.tokenize(\"\\n\\n\", add_special_tokens=False)\n ['\u2581\
          ', '<0x0A>', '<0x0A>']\n\n>>> tokenizer.tokenize(\".\\n\\n\", add_special_tokens=False)\n\
          \ ['\u2581.', '<0x0A>', '<0x0A>']\n```\nThe character \"\\n\\n\"  is not\
          \ a token in the vocab, I would recommend you to stop based on two indexes\
          \ (13,13) rather than taking the prefix space into account"
        updatedAt: '2023-10-08T20:42:50.373Z'
      numEdits: 0
      reactions: []
    id: 6523144a578e7da0d76a0030
    type: comment
  author: ArthurZ
  content: "Hey! I am not sure what is wrong here. A prefix space is added as expected.\
    \ See the tokenization:\n```python \n>>> from transformers import AutoTokenizer\n\
    >>> tokenizer  = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n\
    >>> tokenizer.tokenize(\"\\n\\n\", add_special_tokens=False)\n ['\u2581', '<0x0A>',\
    \ '<0x0A>']\n\n>>> tokenizer.tokenize(\".\\n\\n\", add_special_tokens=False)\n\
    \ ['\u2581.', '<0x0A>', '<0x0A>']\n```\nThe character \"\\n\\n\"  is not a token\
    \ in the vocab, I would recommend you to stop based on two indexes (13,13) rather\
    \ than taking the prefix space into account"
  created_at: 2023-10-08 19:42:50+00:00
  edited: false
  hidden: false
  id: 6523144a578e7da0d76a0030
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 48
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: Problems with tokenizer
