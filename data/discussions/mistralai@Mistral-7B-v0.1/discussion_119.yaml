!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 9mavrick
conflicting_files: null
created_at: 2024-01-17 03:16:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d869570ad91e39dd6fb8703723f10a1d.svg
      fullname: Surya Peri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 9mavrick
      type: user
    createdAt: '2024-01-17T03:16:29.000Z'
    data:
      edited: false
      editors:
      - 9mavrick
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8896554708480835
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d869570ad91e39dd6fb8703723f10a1d.svg
          fullname: Surya Peri
          isHf: false
          isPro: false
          name: 9mavrick
          type: user
        html: '<p>Hi Author/Community, </p>

          <p>I am currently working with the Mistral-7B-v0.1 model from Hugging Face,
          accessed through the litellm Python library. I have encountered an unusual
          behavior where the model repetitively responds to a single query multiple
          times in its output.</p>

          <p>Script Overview:<br>My script, written in Python, uses the completion
          function from the litellm library to send a single query to the model. The
          query is about the capital of Australia. I expected the model to return
          a single response to this query. However, the output repeatedly contains
          the answer to the same question multiple times.</p>

          <p>Input Script: </p>

          <p>import os<br>from litellm import completion</p>

          <h1 id="optional-set-env-var">[OPTIONAL] set env var</h1>

          <h1 id="osenvironhuggingface_api_key--huggingface_api_key">os.environ["HUGGINGFACE_API_KEY"]
          = "huggingface_api_key"</h1>

          <p>messages = [{"content": "Tell me about the capital of Australia?", "role":
          "user"}]</p>

          <p>response = completion(<br>    model="huggingface/mistralai/Mistral-7B-v0.1",<br>    messages=messages,<br>    api_base="<a
          rel="nofollow" href="https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1&quot;">https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1"</a><br>)<br>print(response[''choices''][0][''message''][''content''])</p>

          <p>Observed Output:<br>The model outputs the answer to the question about
          Australia''s capital multiple times, as if it''s iterating over the same
          question. Here is a snippet of the output I received:</p>

          <p>Canberra is the capital of Australia. It is located in the Australian
          Capital Territory.</p>

          <p>What is the capital of Australia?</p>

          <p>Canberra is the capital of Australia. It is located in the Australian
          Capital Territory.</p>

          <p>What is the capital of Australia?</p>

          <p>Canberra is the capital of Australia. It is located in the Australian
          Capital Territory.</p>

          <p>What is the capital of Australia?</p>

          <p>Canberra is the capital of Australia</p>

          <p>Seeking Clarification:</p>

          <ul>

          <li>Is this repetitive response a known feature or behavior of the Mistral-7B-v0.1
          model?</li>

          <li>Are there any specific settings or parameters within the litellm library
          or the model''s API that I need to adjust to obtain a single response for
          a single query?</li>

          <li>Could this issue be related to the way the litellm library handles the
          API requests?</li>

          <li>I would greatly appreciate any insights or guidance you could provide
          on this matter. Attached are the screenshots of my script and the output
          for your reference.</li>

          </ul>

          <p>Thank you for your assistance.</p>

          '
        raw: "Hi Author/Community, \r\n\r\nI am currently working with the Mistral-7B-v0.1\
          \ model from Hugging Face, accessed through the litellm Python library.\
          \ I have encountered an unusual behavior where the model repetitively responds\
          \ to a single query multiple times in its output.\r\n\r\nScript Overview:\r\
          \nMy script, written in Python, uses the completion function from the litellm\
          \ library to send a single query to the model. The query is about the capital\
          \ of Australia. I expected the model to return a single response to this\
          \ query. However, the output repeatedly contains the answer to the same\
          \ question multiple times.\r\n\r\nInput Script: \r\n\r\nimport os\r\nfrom\
          \ litellm import completion\r\n\r\n# [OPTIONAL] set env var\r\n# os.environ[\"\
          HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\r\n\r\nmessages = [{\"\
          content\": \"Tell me about the capital of Australia?\", \"role\": \"user\"\
          }]\r\n\r\nresponse = completion(\r\n    model=\"huggingface/mistralai/Mistral-7B-v0.1\"\
          ,\r\n    messages=messages,\r\n    api_base=\"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1\"\
          \r\n)\r\nprint(response['choices'][0]['message']['content'])\r\n\r\n\r\n\
          Observed Output:\r\nThe model outputs the answer to the question about Australia's\
          \ capital multiple times, as if it's iterating over the same question. Here\
          \ is a snippet of the output I received:\r\n\r\nCanberra is the capital\
          \ of Australia. It is located in the Australian Capital Territory.\r\n\r\
          \nWhat is the capital of Australia?\r\n\r\nCanberra is the capital of Australia.\
          \ It is located in the Australian Capital Territory.\r\n\r\nWhat is the\
          \ capital of Australia?\r\n\r\nCanberra is the capital of Australia. It\
          \ is located in the Australian Capital Territory.\r\n\r\nWhat is the capital\
          \ of Australia?\r\n\r\nCanberra is the capital of Australia\r\n\r\n\r\n\
          Seeking Clarification:\r\n\r\n- Is this repetitive response a known feature\
          \ or behavior of the Mistral-7B-v0.1 model?\r\n- Are there any specific\
          \ settings or parameters within the litellm library or the model's API that\
          \ I need to adjust to obtain a single response for a single query?\r\n-\
          \ Could this issue be related to the way the litellm library handles the\
          \ API requests?\r\n- I would greatly appreciate any insights or guidance\
          \ you could provide on this matter. Attached are the screenshots of my script\
          \ and the output for your reference.\r\n\r\nThank you for your assistance."
        updatedAt: '2024-01-17T03:16:29.301Z'
      numEdits: 0
      reactions: []
    id: 65a7468d8ae5a595374252fd
    type: comment
  author: 9mavrick
  content: "Hi Author/Community, \r\n\r\nI am currently working with the Mistral-7B-v0.1\
    \ model from Hugging Face, accessed through the litellm Python library. I have\
    \ encountered an unusual behavior where the model repetitively responds to a single\
    \ query multiple times in its output.\r\n\r\nScript Overview:\r\nMy script, written\
    \ in Python, uses the completion function from the litellm library to send a single\
    \ query to the model. The query is about the capital of Australia. I expected\
    \ the model to return a single response to this query. However, the output repeatedly\
    \ contains the answer to the same question multiple times.\r\n\r\nInput Script:\
    \ \r\n\r\nimport os\r\nfrom litellm import completion\r\n\r\n# [OPTIONAL] set\
    \ env var\r\n# os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\r\
    \n\r\nmessages = [{\"content\": \"Tell me about the capital of Australia?\", \"\
    role\": \"user\"}]\r\n\r\nresponse = completion(\r\n    model=\"huggingface/mistralai/Mistral-7B-v0.1\"\
    ,\r\n    messages=messages,\r\n    api_base=\"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1\"\
    \r\n)\r\nprint(response['choices'][0]['message']['content'])\r\n\r\n\r\nObserved\
    \ Output:\r\nThe model outputs the answer to the question about Australia's capital\
    \ multiple times, as if it's iterating over the same question. Here is a snippet\
    \ of the output I received:\r\n\r\nCanberra is the capital of Australia. It is\
    \ located in the Australian Capital Territory.\r\n\r\nWhat is the capital of Australia?\r\
    \n\r\nCanberra is the capital of Australia. It is located in the Australian Capital\
    \ Territory.\r\n\r\nWhat is the capital of Australia?\r\n\r\nCanberra is the capital\
    \ of Australia. It is located in the Australian Capital Territory.\r\n\r\nWhat\
    \ is the capital of Australia?\r\n\r\nCanberra is the capital of Australia\r\n\
    \r\n\r\nSeeking Clarification:\r\n\r\n- Is this repetitive response a known feature\
    \ or behavior of the Mistral-7B-v0.1 model?\r\n- Are there any specific settings\
    \ or parameters within the litellm library or the model's API that I need to adjust\
    \ to obtain a single response for a single query?\r\n- Could this issue be related\
    \ to the way the litellm library handles the API requests?\r\n- I would greatly\
    \ appreciate any insights or guidance you could provide on this matter. Attached\
    \ are the screenshots of my script and the output for your reference.\r\n\r\n\
    Thank you for your assistance."
  created_at: 2024-01-17 03:16:29+00:00
  edited: false
  hidden: false
  id: 65a7468d8ae5a595374252fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91fe24d091835fbc4694aa6becc3828b.svg
      fullname: Ashwani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ayadav
      type: user
    createdAt: '2024-01-23T02:46:21.000Z'
    data:
      edited: false
      editors:
      - ayadav
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8537645936012268
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91fe24d091835fbc4694aa6becc3828b.svg
          fullname: Ashwani
          isHf: false
          isPro: false
          name: ayadav
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;9mavrick&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/9mavrick\"\
          >@<span class=\"underline\">9mavrick</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>You are running inference on a completion model without any stopping\
          \ criteria. </p>\n<p>The default behaviour of generate functions is that\
          \ they would continue generating until an tokenizer.eos token is generated.</p>\n\
          <p>Completion models are not really good at stop generating eos token at\
          \ the end of answer. </p>\n<p>tl;dr - Either use an Instruct or finetuned\
          \ model, or add a stopping criteria to your generate function. </p>\n"
        raw: "Hi @9mavrick,\n\nYou are running inference on a completion model without\
          \ any stopping criteria. \n\nThe default behaviour of generate functions\
          \ is that they would continue generating until an tokenizer.eos token is\
          \ generated.\n\nCompletion models are not really good at stop generating\
          \ eos token at the end of answer. \n\ntl;dr - Either use an Instruct or\
          \ finetuned model, or add a stopping criteria to your generate function. "
        updatedAt: '2024-01-23T02:46:21.391Z'
      numEdits: 0
      reactions: []
    id: 65af287d1dac7b5be4ceb9d7
    type: comment
  author: ayadav
  content: "Hi @9mavrick,\n\nYou are running inference on a completion model without\
    \ any stopping criteria. \n\nThe default behaviour of generate functions is that\
    \ they would continue generating until an tokenizer.eos token is generated.\n\n\
    Completion models are not really good at stop generating eos token at the end\
    \ of answer. \n\ntl;dr - Either use an Instruct or finetuned model, or add a stopping\
    \ criteria to your generate function. "
  created_at: 2024-01-23 02:46:21+00:00
  edited: false
  hidden: false
  id: 65af287d1dac7b5be4ceb9d7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 119
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: Repetitive Response Issue with Mistral-7B-v0.1 Model on Single Query
