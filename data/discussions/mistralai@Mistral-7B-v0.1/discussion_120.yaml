!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Tejaswi006
conflicting_files: null
created_at: 2024-01-17 11:59:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/efea35f477b79dc5e4cfa8bd1a32758b.svg
      fullname: Tejaswi Kashyap
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tejaswi006
      type: user
    createdAt: '2024-01-17T11:59:53.000Z'
    data:
      edited: false
      editors:
      - Tejaswi006
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9420971274375916
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/efea35f477b79dc5e4cfa8bd1a32758b.svg
          fullname: Tejaswi Kashyap
          isHf: false
          isPro: false
          name: Tejaswi006
          type: user
        html: '<p>Hi, I''m trying to fine tune the mistral on my mother tongue tamil
          but when it fine tune it the output doesn''t make any sense so i got to
          know the tokenizer is not able to understand the tamil. So, is there any
          way to increase the vocab of the tokenizer ?</p>

          '
        raw: Hi, I'm trying to fine tune the mistral on my mother tongue tamil but
          when it fine tune it the output doesn't make any sense so i got to know
          the tokenizer is not able to understand the tamil. So, is there any way
          to increase the vocab of the tokenizer ?
        updatedAt: '2024-01-17T11:59:53.851Z'
      numEdits: 0
      reactions: []
    id: 65a7c1392ad050091522f36e
    type: comment
  author: Tejaswi006
  content: Hi, I'm trying to fine tune the mistral on my mother tongue tamil but when
    it fine tune it the output doesn't make any sense so i got to know the tokenizer
    is not able to understand the tamil. So, is there any way to increase the vocab
    of the tokenizer ?
  created_at: 2024-01-17 11:59:53+00:00
  edited: false
  hidden: false
  id: 65a7c1392ad050091522f36e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91fe24d091835fbc4694aa6becc3828b.svg
      fullname: Ashwani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ayadav
      type: user
    createdAt: '2024-01-20T04:17:42.000Z'
    data:
      edited: false
      editors:
      - ayadav
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8214696645736694
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91fe24d091835fbc4694aa6becc3828b.svg
          fullname: Ashwani
          isHf: false
          isPro: false
          name: ayadav
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Tejaswi006&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Tejaswi006\"\
          >@<span class=\"underline\">Tejaswi006</span></a></span>\n\n\t</span></span>\
          \ ,</p>\n<p>I just tried base Mistral-Instruct model on some text from Wikipedia,\
          \ and looking at the results it looks like it doesn't understands the language\
          \ much.</p>\n<p>However, since it's able to generate the text in Tamil script,\
          \ the tokenizer should ideally work as-is. I think it requires more training\
          \ on Tamil corpora instead of tokenizer modifications.</p>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63c2c17c456b30b44e7b82ce/6jKcpaHesyWw3EuE2UWOS.png\"\
          ><img alt=\"Screenshot 2024-01-20 at 09.44.17.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63c2c17c456b30b44e7b82ce/6jKcpaHesyWw3EuE2UWOS.png\"\
          ></a><br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63c2c17c456b30b44e7b82ce/GlbzWyEn3Z3WaZ5XrIb9r.png\"\
          ><img alt=\"Screenshot 2024-01-20 at 09.44.31.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63c2c17c456b30b44e7b82ce/GlbzWyEn3Z3WaZ5XrIb9r.png\"\
          ></a></p>\n"
        raw: 'Hey @Tejaswi006 ,


          I just tried base Mistral-Instruct model on some text from Wikipedia, and
          looking at the results it looks like it doesn''t understands the language
          much.


          However, since it''s able to generate the text in Tamil script, the tokenizer
          should ideally work as-is. I think it requires more training on Tamil corpora
          instead of tokenizer modifications.


          ![Screenshot 2024-01-20 at 09.44.17.png](https://cdn-uploads.huggingface.co/production/uploads/63c2c17c456b30b44e7b82ce/6jKcpaHesyWw3EuE2UWOS.png)

          ![Screenshot 2024-01-20 at 09.44.31.png](https://cdn-uploads.huggingface.co/production/uploads/63c2c17c456b30b44e7b82ce/GlbzWyEn3Z3WaZ5XrIb9r.png)

          '
        updatedAt: '2024-01-20T04:17:42.704Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Tejaswi006
    id: 65ab4966043d53781a0f3701
    type: comment
  author: ayadav
  content: 'Hey @Tejaswi006 ,


    I just tried base Mistral-Instruct model on some text from Wikipedia, and looking
    at the results it looks like it doesn''t understands the language much.


    However, since it''s able to generate the text in Tamil script, the tokenizer
    should ideally work as-is. I think it requires more training on Tamil corpora
    instead of tokenizer modifications.


    ![Screenshot 2024-01-20 at 09.44.17.png](https://cdn-uploads.huggingface.co/production/uploads/63c2c17c456b30b44e7b82ce/6jKcpaHesyWw3EuE2UWOS.png)

    ![Screenshot 2024-01-20 at 09.44.31.png](https://cdn-uploads.huggingface.co/production/uploads/63c2c17c456b30b44e7b82ce/GlbzWyEn3Z3WaZ5XrIb9r.png)

    '
  created_at: 2024-01-20 04:17:42+00:00
  edited: false
  hidden: false
  id: 65ab4966043d53781a0f3701
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91fe24d091835fbc4694aa6becc3828b.svg
      fullname: Ashwani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ayadav
      type: user
    createdAt: '2024-01-20T04:26:40.000Z'
    data:
      edited: true
      editors:
      - ayadav
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7391545176506042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91fe24d091835fbc4694aa6becc3828b.svg
          fullname: Ashwani
          isHf: false
          isPro: false
          name: ayadav
          type: user
        html: '<p>If you still want to add some new tokens in the tokenizer, you should
          be able to do as following.</p>

          <pre><code class="language-python">new_tokens = [<span class="hljs-string">"new_tok1"</span>,
          <span class="hljs-string">"my_new-tok2"</span>]

          num_added_toks = tokenizer.add_tokens(new_tokens)

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"We have
          added"</span>, num_added_toks, <span class="hljs-string">"tokens"</span>)


          <span class="hljs-comment"># Notice: resize_token_embeddings expect to receive
          the full size of the new vocabulary, i.e., the length of the tokenizer.</span>

          model.resize_token_embeddings(<span class="hljs-built_in">len</span>(tokenizer))

          </code></pre>

          <p>References:</p>

          <ul>

          <li><a href="https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.add_tokens">https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.add_tokens</a></li>

          </ul>

          '
        raw: "If you still want to add some new tokens in the tokenizer, you should\
          \ be able to do as following.\n\n```python\nnew_tokens = [\"new_tok1\",\
          \ \"my_new-tok2\"]\nnum_added_toks = tokenizer.add_tokens(new_tokens)\n\
          print(\"We have added\", num_added_toks, \"tokens\")\n\n# Notice: resize_token_embeddings\
          \ expect to receive the full size of the new vocabulary, i.e., the length\
          \ of the tokenizer.\nmodel.resize_token_embeddings(len(tokenizer))\n```\n\
          \nReferences:\n - https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.add_tokens"
        updatedAt: '2024-01-20T04:27:16.664Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - Tejaswi006
    id: 65ab4b80f8111f40c07dcda0
    type: comment
  author: ayadav
  content: "If you still want to add some new tokens in the tokenizer, you should\
    \ be able to do as following.\n\n```python\nnew_tokens = [\"new_tok1\", \"my_new-tok2\"\
    ]\nnum_added_toks = tokenizer.add_tokens(new_tokens)\nprint(\"We have added\"\
    , num_added_toks, \"tokens\")\n\n# Notice: resize_token_embeddings expect to receive\
    \ the full size of the new vocabulary, i.e., the length of the tokenizer.\nmodel.resize_token_embeddings(len(tokenizer))\n\
    ```\n\nReferences:\n - https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.add_tokens"
  created_at: 2024-01-20 04:26:40+00:00
  edited: true
  hidden: false
  id: 65ab4b80f8111f40c07dcda0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/efea35f477b79dc5e4cfa8bd1a32758b.svg
      fullname: Tejaswi Kashyap
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tejaswi006
      type: user
    createdAt: '2024-01-22T07:57:14.000Z'
    data:
      edited: false
      editors:
      - Tejaswi006
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9944198727607727
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/efea35f477b79dc5e4cfa8bd1a32758b.svg
          fullname: Tejaswi Kashyap
          isHf: false
          isPro: false
          name: Tejaswi006
          type: user
        html: '<p>Thanks, I will look into it. The method you used was instruct fine
          tuning ?</p>

          '
        raw: Thanks, I will look into it. The method you used was instruct fine tuning
          ?
        updatedAt: '2024-01-22T07:57:14.781Z'
      numEdits: 0
      reactions: []
    id: 65ae1fdab68db4f26e2f648e
    type: comment
  author: Tejaswi006
  content: Thanks, I will look into it. The method you used was instruct fine tuning
    ?
  created_at: 2024-01-22 07:57:14+00:00
  edited: false
  hidden: false
  id: 65ae1fdab68db4f26e2f648e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 120
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: Is there any way to increase the vocabulary of the tokenizer and use it fine
  tune the model on the new language
