!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Joseph717171
conflicting_files: null
created_at: 2023-10-15 19:34:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea4398745974d781ae9dc0e95b12cabe.svg
      fullname: Joseph
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Joseph717171
      type: user
    createdAt: '2023-10-15T20:34:17.000Z'
    data:
      edited: false
      editors:
      - Joseph717171
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.840573787689209
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea4398745974d781ae9dc0e95b12cabe.svg
          fullname: Joseph
          isHf: false
          isPro: false
          name: Joseph717171
          type: user
        html: '<p>LLaVA: Large Language and Vision Assistant Visual Instruction Fine-tuning</p>

          <p>Instruction tuning large language models (LLMs) using machine-generated
          instruction-following data has improved zero-shot capabilities on new tasks
          in the language domain, but the idea is less explored in the multimodal
          field.</p>

          <p>Multimodal Instruct Data. We present the first attempt to use language-only
          GPT-4 to generate multimodal language-image instruction-following data.<br>LLaVA
          Model. We introduce LLaVA (Large Language-and-Vision Assistant), an end-to-end
          trained large multimodal model that connects a vision encoder and LLM for
          general-purpose visual and language understanding.<br>Performance. Our early
          experiments show that LLaVA demonstrates impressive multimodel chat abilities,
          sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions,
          and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal
          instruction-following dataset. When fine-tuned on Science QA, the synergy
          of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%.<br>Open-source.
          We make GPT-4 generated visual instruction tuning data, our model and code
          base publicly available.</p>

          <p><a rel="nofollow" href="https://llava-vl.github.io">https://llava-vl.github.io</a></p>

          <p>Thanks for your time and consideration.</p>

          '
        raw: "LLaVA: Large Language and Vision Assistant Visual Instruction Fine-tuning\r\
          \n\r\nInstruction tuning large language models (LLMs) using machine-generated\
          \ instruction-following data has improved zero-shot capabilities on new\
          \ tasks in the language domain, but the idea is less explored in the multimodal\
          \ field.\r\n\r\nMultimodal Instruct Data. We present the first attempt to\
          \ use language-only GPT-4 to generate multimodal language-image instruction-following\
          \ data.\r\nLLaVA Model. We introduce LLaVA (Large Language-and-Vision Assistant),\
          \ an end-to-end trained large multimodal model that connects a vision encoder\
          \ and LLM for general-purpose visual and language understanding.\r\nPerformance.\
          \ Our early experiments show that LLaVA demonstrates impressive multimodel\
          \ chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4\
          \ on unseen images/instructions, and yields a 85.1% relative score compared\
          \ with GPT-4 on a synthetic multimodal instruction-following dataset. When\
          \ fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new\
          \ state-of-the-art accuracy of 92.53%.\r\nOpen-source. We make GPT-4 generated\
          \ visual instruction tuning data, our model and code base publicly available.\r\
          \n\r\nhttps://llava-vl.github.io\r\n\r\nThanks for your time and consideration."
        updatedAt: '2023-10-15T20:34:17.692Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - orkut
    id: 652c4cc9a21958f7db7ab796
    type: comment
  author: Joseph717171
  content: "LLaVA: Large Language and Vision Assistant Visual Instruction Fine-tuning\r\
    \n\r\nInstruction tuning large language models (LLMs) using machine-generated\
    \ instruction-following data has improved zero-shot capabilities on new tasks\
    \ in the language domain, but the idea is less explored in the multimodal field.\r\
    \n\r\nMultimodal Instruct Data. We present the first attempt to use language-only\
    \ GPT-4 to generate multimodal language-image instruction-following data.\r\n\
    LLaVA Model. We introduce LLaVA (Large Language-and-Vision Assistant), an end-to-end\
    \ trained large multimodal model that connects a vision encoder and LLM for general-purpose\
    \ visual and language understanding.\r\nPerformance. Our early experiments show\
    \ that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting\
    \ the behaviors of multimodal GPT-4 on unseen images/instructions, and yields\
    \ a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following\
    \ dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves\
    \ a new state-of-the-art accuracy of 92.53%.\r\nOpen-source. We make GPT-4 generated\
    \ visual instruction tuning data, our model and code base publicly available.\r\
    \n\r\nhttps://llava-vl.github.io\r\n\r\nThanks for your time and consideration."
  created_at: 2023-10-15 19:34:17+00:00
  edited: false
  hidden: false
  id: 652c4cc9a21958f7db7ab796
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-10-16T18:16:52.000Z'
    data:
      edited: true
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5551876425743103
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;Leyo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Leyo\">@<span class=\"\
          underline\">Leyo</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;VictorSanh&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/VictorSanh\">@<span class=\"underline\"\
          >VictorSanh</span></a></span>\n\n\t</span></span> \U0001F440</p>\n"
        raw: "cc @Leyo @VictorSanh \U0001F440"
        updatedAt: '2023-10-16T18:17:21.857Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F917"
        users:
        - Leyo
        - ybelkada
        - orkut
    id: 652d7e147f4b8bd6772faab7
    type: comment
  author: lysandre
  content: "cc @Leyo @VictorSanh \U0001F440"
  created_at: 2023-10-16 17:16:52+00:00
  edited: true
  hidden: false
  id: 652d7e147f4b8bd6772faab7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
      fullname: Victor Sanh
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: true
      name: VictorSanh
      type: user
    createdAt: '2023-10-16T18:25:53.000Z'
    data:
      edited: false
      editors:
      - VictorSanh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9514409899711609
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
          fullname: Victor Sanh
          isHf: true
          isPro: true
          name: VictorSanh
          type: user
        html: '<p>stay tuned</p>

          '
        raw: stay tuned
        updatedAt: '2023-10-16T18:25:53.706Z'
      numEdits: 0
      reactions:
      - count: 8
        reaction: "\U0001F44D"
        users:
        - gnomealone
        - jlzhou
        - Joseph717171
        - zhibor
        - JulesGM
        - ybelkada
        - z-uo
        - orkut
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ybelkada
    id: 652d80312d617ec278b1302f
    type: comment
  author: VictorSanh
  content: stay tuned
  created_at: 2023-10-16 17:25:53+00:00
  edited: false
  hidden: false
  id: 652d80312d617ec278b1302f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/656b1f9aff4d7993088cb54dace07769.svg
      fullname: fadi labi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: expert78
      type: user
    createdAt: '2023-12-24T13:01:22.000Z'
    data:
      edited: false
      editors:
      - expert78
      hidden: false
      identifiedLanguage:
        language: it
        probability: 0.9861560463905334
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/656b1f9aff4d7993088cb54dace07769.svg
          fullname: fadi labi
          isHf: false
          isPro: false
          name: expert78
          type: user
        html: '<p>+1</p>

          '
        raw: '+1'
        updatedAt: '2023-12-24T13:01:22.210Z'
      numEdits: 0
      reactions: []
    id: 65882ba20100bf3373bbd3c9
    type: comment
  author: expert78
  content: '+1'
  created_at: 2023-12-24 13:01:22+00:00
  edited: false
  hidden: false
  id: 65882ba20100bf3373bbd3c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c824379e62a403eab4c7d1d8cce93f76.svg
      fullname: Samir R.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sr5434
      type: user
    createdAt: '2024-01-08T00:39:14.000Z'
    data:
      edited: false
      editors:
      - sr5434
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9018345475196838
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c824379e62a403eab4c7d1d8cce93f76.svg
          fullname: Samir R.
          isHf: false
          isPro: false
          name: sr5434
          type: user
        html: '<p>What if they do LLaVA, but also integrate Wav2Vec2 so that the model
          can understand audio, text, and images?</p>

          '
        raw: What if they do LLaVA, but also integrate Wav2Vec2 so that the model
          can understand audio, text, and images?
        updatedAt: '2024-01-08T00:39:14.449Z'
      numEdits: 0
      reactions: []
    id: 659b443269c89c6abf624382
    type: comment
  author: sr5434
  content: What if they do LLaVA, but also integrate Wav2Vec2 so that the model can
    understand audio, text, and images?
  created_at: 2024-01-08 00:39:14+00:00
  edited: false
  hidden: false
  id: 659b443269c89c6abf624382
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653a2392341143f7774424d8/Ts1QIoXwlQkR741likXrb.png?w=200&h=200&f=face
      fullname: Katy Vetteriano
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KatyTheCutie
      type: user
    createdAt: '2024-01-08T09:14:09.000Z'
    data:
      edited: false
      editors:
      - KatyTheCutie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8011718988418579
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653a2392341143f7774424d8/Ts1QIoXwlQkR741likXrb.png?w=200&h=200&f=face
          fullname: Katy Vetteriano
          isHf: false
          isPro: false
          name: KatyTheCutie
          type: user
        html: '<p>BakLlaVa is the Mistral 7B version</p>

          '
        raw: BakLlaVa is the Mistral 7B version
        updatedAt: '2024-01-08T09:14:09.952Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - ybelkada
        - orkut
    id: 659bbce18c8ac792959317c1
    type: comment
  author: KatyTheCutie
  content: BakLlaVa is the Mistral 7B version
  created_at: 2024-01-08 09:14:09+00:00
  edited: false
  hidden: false
  id: 659bbce18c8ac792959317c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-22T16:18:19.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6123144030570984
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>Indeed, you can find BakLlava implementation here: <a href="https://huggingface.co/llava-hf/bakLlava-v1-hf">https://huggingface.co/llava-hf/bakLlava-v1-hf</a>
          </p>

          '
        raw: 'Indeed, you can find BakLlava implementation here: https://huggingface.co/llava-hf/bakLlava-v1-hf '
        updatedAt: '2024-01-22T16:18:19.193Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - orkut
    id: 65ae954b317cc62a3aac520a
    type: comment
  author: ybelkada
  content: 'Indeed, you can find BakLlava implementation here: https://huggingface.co/llava-hf/bakLlava-v1-hf '
  created_at: 2024-01-22 16:18:19+00:00
  edited: false
  hidden: false
  id: 65ae954b317cc62a3aac520a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 57
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: "Request: Please Make a LLAVA-Like Model from Mistral-7B - It Would be Amazing\
  \ \U0001F929"
