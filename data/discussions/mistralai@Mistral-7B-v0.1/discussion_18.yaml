!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ianuvrat
conflicting_files: null
created_at: 2023-09-29 02:24:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
      fullname: Anuvrat Shukla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ianuvrat
      type: user
    createdAt: '2023-09-29T03:24:16.000Z'
    data:
      edited: false
      editors:
      - ianuvrat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7499107122421265
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
          fullname: Anuvrat Shukla
          isHf: false
          isPro: false
          name: ianuvrat
          type: user
        html: '<p>Can''t run on colab (free tier) . Can anyone guide me how to run
          Mistral 8 bit?</p>

          '
        raw: Can't run on colab (free tier) . Can anyone guide me how to run Mistral
          8 bit?
        updatedAt: '2023-09-29T03:24:16.692Z'
      numEdits: 0
      reactions: []
    id: 651643605671e652966a68f4
    type: comment
  author: ianuvrat
  content: Can't run on colab (free tier) . Can anyone guide me how to run Mistral
    8 bit?
  created_at: 2023-09-29 02:24:16+00:00
  edited: false
  hidden: false
  id: 651643605671e652966a68f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
      fullname: Anuvrat Shukla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ianuvrat
      type: user
    createdAt: '2023-09-29T03:24:32.000Z'
    data:
      from: Quantized version of Mistral 7B (4bit or * bit)
      to: Quantized version of Mistral 7B (4bit or 8 bit)
    id: 651643707f8b9fc0f7a9f394
    type: title-change
  author: ianuvrat
  created_at: 2023-09-29 02:24:32+00:00
  id: 651643707f8b9fc0f7a9f394
  new_title: Quantized version of Mistral 7B (4bit or 8 bit)
  old_title: Quantized version of Mistral 7B (4bit or * bit)
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/449d0c4879ff617fea41e1e8128d3f2f.svg
      fullname: Mostafa Najmi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mnwato
      type: user
    createdAt: '2023-09-29T08:37:05.000Z'
    data:
      edited: true
      editors:
      - mnwato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6143179535865784
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/449d0c4879ff617fea41e1e8128d3f2f.svg
          fullname: Mostafa Najmi
          isHf: false
          isPro: false
          name: mnwato
          type: user
        html: '<p>Same issue.</p>

          <p>After installing requirements using these two lines:</p>

          <pre><code>!pip install git+https://github.com/huggingface/transformers

          !pip install accelerate bitsandbytes

          </code></pre>

          <p>I always use Use <code>load_in_4bit=True</code> and <code>device_map=''cuda''</code>
          while loading model:</p>

          <pre><code># Load model directly

          from transformers import AutoTokenizer, AutoModelForCausalLM


          tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

          model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1",  load_in_8bit=True,
          device_map=''cuda'')

          </code></pre>

          <p>But on colab CPU memory is getting OOM. I dont know why this is loading  on
          CPU memory instead of GPU memory!!!</p>

          '
        raw: 'Same issue.


          After installing requirements using these two lines:

          ```

          !pip install git+https://github.com/huggingface/transformers

          !pip install accelerate bitsandbytes

          ```


          I always use Use `load_in_4bit=True` and `device_map=''cuda''` while loading
          model:

          ```

          # Load model directly

          from transformers import AutoTokenizer, AutoModelForCausalLM


          tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

          model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1",  load_in_8bit=True,
          device_map=''cuda'')

          ```

          But on colab CPU memory is getting OOM. I dont know why this is loading  on
          CPU memory instead of GPU memory!!!'
        updatedAt: '2023-09-29T08:37:18.966Z'
      numEdits: 1
      reactions: []
    id: 65168cb14eb20107d039765a
    type: comment
  author: mnwato
  content: 'Same issue.


    After installing requirements using these two lines:

    ```

    !pip install git+https://github.com/huggingface/transformers

    !pip install accelerate bitsandbytes

    ```


    I always use Use `load_in_4bit=True` and `device_map=''cuda''` while loading model:

    ```

    # Load model directly

    from transformers import AutoTokenizer, AutoModelForCausalLM


    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

    model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1",  load_in_8bit=True,
    device_map=''cuda'')

    ```

    But on colab CPU memory is getting OOM. I dont know why this is loading  on CPU
    memory instead of GPU memory!!!'
  created_at: 2023-09-29 07:37:05+00:00
  edited: true
  hidden: false
  id: 65168cb14eb20107d039765a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/182b1994ad37ed23d8a066caeaef83d5.svg
      fullname: dario
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prudant
      type: user
    createdAt: '2023-09-30T01:59:05.000Z'
    data:
      edited: true
      editors:
      - prudant
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.37818723917007446
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/182b1994ad37ed23d8a066caeaef83d5.svg
          fullname: dario
          isHf: false
          isPro: false
          name: prudant
          type: user
        html: '<p>from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM,
          BitsAndBytesConfig, TextStreamer<br>import torch</p>

          <p>model_name_or_path = "mistralai/Mistral-7B-Instruct-v0.1"<br>config =
          AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)<br>config.max_position_embeddings
          = 8096<br>quantization_config = BitsAndBytesConfig(<br>    llm_int8_enable_fp32_cpu_offload=True,<br>    bnb_4bit_quant_type=''nf4'',<br>    bnb_4bit_use_double_quant=True,<br>    bnb_4bit_compute_dtype=torch.bfloat16,<br>    load_in_4bit=True<br>)</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)<br>model
          = AutoModelForCausalLM.from_pretrained(<br>    model_name_or_path,<br>    config=config,<br>    trust_remote_code=True,<br>    quantization_config=quantization_config,<br>    device_map="auto",<br>    offload_folder="./offload"<br>)</p>

          <p>prompt = "<s>[INST]your prompt[/INST]"<br>print("\n\n*** Generate:")<br>inputs
          = tokenizer(prompt, return_tensors="pt", add_special_tokens=False).to("cuda")<br>streamer
          = TextStreamer(tokenizer, skip_prompt= True)<br>output = model.generate(**inputs,<br>                        streamer=streamer,<br>                        max_new_tokens=512,<br>                        temperature=0.3,<br>                        top_k=20,<br>                        top_p=0.4,<br>                        repetition_penalty=1.1,
          do_sample=True)</s></p><s>

          </s>'
        raw: "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM,\
          \ BitsAndBytesConfig, TextStreamer\nimport torch\n\nmodel_name_or_path =\
          \ \"mistralai/Mistral-7B-Instruct-v0.1\"\nconfig = AutoConfig.from_pretrained(model_name_or_path,\
          \ trust_remote_code=True)\nconfig.max_position_embeddings = 8096\nquantization_config\
          \ = BitsAndBytesConfig(\n    llm_int8_enable_fp32_cpu_offload=True,\n  \
          \  bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n   \
          \ bnb_4bit_compute_dtype=torch.bfloat16,\n    load_in_4bit=True\n)\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n   \
          \ config=config,\n    trust_remote_code=True,\n    quantization_config=quantization_config,\n\
          \    device_map=\"auto\",\n    offload_folder=\"./offload\"\n)\n\nprompt\
          \ = \"<s>[INST]your prompt[/INST]\"\nprint(\"\\n\\n*** Generate:\")\ninputs\
          \ = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"\
          cuda\")\nstreamer = TextStreamer(tokenizer, skip_prompt= True)\noutput =\
          \ model.generate(**inputs,\n                        streamer=streamer,\n\
          \                        max_new_tokens=512,\n                        temperature=0.3,\n\
          \                        top_k=20,\n                        top_p=0.4,\n\
          \                        repetition_penalty=1.1, do_sample=True)"
        updatedAt: '2023-09-30T02:12:01.019Z'
      numEdits: 2
      reactions: []
    id: 651780e9a1a5e5d61783309b
    type: comment
  author: prudant
  content: "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM,\
    \ BitsAndBytesConfig, TextStreamer\nimport torch\n\nmodel_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\
    \nconfig = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n\
    config.max_position_embeddings = 8096\nquantization_config = BitsAndBytesConfig(\n\
    \    llm_int8_enable_fp32_cpu_offload=True,\n    bnb_4bit_quant_type='nf4',\n\
    \    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n\
    \    load_in_4bit=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n\
    \    config=config,\n    trust_remote_code=True,\n    quantization_config=quantization_config,\n\
    \    device_map=\"auto\",\n    offload_folder=\"./offload\"\n)\n\nprompt = \"\
    <s>[INST]your prompt[/INST]\"\nprint(\"\\n\\n*** Generate:\")\ninputs = tokenizer(prompt,\
    \ return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\nstreamer = TextStreamer(tokenizer,\
    \ skip_prompt= True)\noutput = model.generate(**inputs,\n                    \
    \    streamer=streamer,\n                        max_new_tokens=512,\n       \
    \                 temperature=0.3,\n                        top_k=20,\n      \
    \                  top_p=0.4,\n                        repetition_penalty=1.1,\
    \ do_sample=True)"
  created_at: 2023-09-30 00:59:05+00:00
  edited: true
  hidden: false
  id: 651780e9a1a5e5d61783309b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: Quantized version of Mistral 7B (4bit or 8 bit)
