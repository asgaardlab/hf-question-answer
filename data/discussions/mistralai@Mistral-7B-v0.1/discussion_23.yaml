!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Tapendra
conflicting_files: null
created_at: 2023-09-30 05:47:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b5a0e04fde4337091d47c1636d94486.svg
      fullname: Tapendra Baduwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tapendra
      type: user
    createdAt: '2023-09-30T06:47:56.000Z'
    data:
      edited: true
      editors:
      - Tapendra
      hidden: false
      identifiedLanguage:
        language: kn
        probability: 0.08103614300489426
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8b5a0e04fde4337091d47c1636d94486.svg
          fullname: Tapendra Baduwal
          isHf: false
          isPro: false
          name: Tapendra
          type: user
        html: '<p>While training with </p>

          <h1 id="set-supervised-fine-tuning-parameters">Set supervised fine-tuning
          parameters</h1>

          <p>trainer = SFTTrainer(<br>    model=model,<br>    train_dataset=dataset,<br>    peft_config=peft_config,<br>    dataset_text_field="text",<br>    max_seq_length=max_seq_length,<br>    tokenizer=tokenizer,<br>    args=training_arguments,<br>    packing=packing,<br>)</p>

          <h1 id="train-model">Train model</h1>

          <p>trainer.train()</p>

          <p>ValueError: Please specify <code>target_modules</code> in <code>peft_config</code></p>

          '
        raw: "While training with \n# Set supervised fine-tuning parameters\ntrainer\
          \ = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n\
          \    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n\
          \    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n\
          )\n\n# Train model\ntrainer.train()\n\nValueError: Please specify `target_modules`\
          \ in `peft_config`"
        updatedAt: '2023-09-30T06:49:28.633Z'
      numEdits: 1
      reactions: []
    id: 6517c49c8876a95d5a27a26f
    type: comment
  author: Tapendra
  content: "While training with \n# Set supervised fine-tuning parameters\ntrainer\
    \ = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n\
    \    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n\
    \    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()\n\
    \nValueError: Please specify `target_modules` in `peft_config`"
  created_at: 2023-09-30 05:47:56+00:00
  edited: true
  hidden: false
  id: 6517c49c8876a95d5a27a26f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45bd9b7f13cd98119cfee7046e352b0f.svg
      fullname: P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JaganDS
      type: user
    createdAt: '2023-09-30T12:13:59.000Z'
    data:
      edited: false
      editors:
      - JaganDS
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9327649474143982
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45bd9b7f13cd98119cfee7046e352b0f.svg
          fullname: P
          isHf: false
          isPro: false
          name: JaganDS
          type: user
        html: '<p>I''m attempting to use deepspeed for model training, but the script
          crashes when trying to load the model. Additionally, the trainer is not
          assigning the model to the GPU.</p>

          '
        raw: 'I''m attempting to use deepspeed for model training, but the script
          crashes when trying to load the model. Additionally, the trainer is not
          assigning the model to the GPU.

          '
        updatedAt: '2023-09-30T12:13:59.027Z'
      numEdits: 0
      reactions: []
    id: 6518110742097d8c5907f822
    type: comment
  author: JaganDS
  content: 'I''m attempting to use deepspeed for model training, but the script crashes
    when trying to load the model. Additionally, the trainer is not assigning the
    model to the GPU.

    '
  created_at: 2023-09-30 11:13:59+00:00
  edited: false
  hidden: false
  id: 6518110742097d8c5907f822
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638ae4bcf6a0bc48582c4239/LlixeQbFoK0p47FQpa7lz.png?w=200&h=200&f=face
      fullname: Harper Carroll
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harpercarroll
      type: user
    createdAt: '2023-10-02T17:31:27.000Z'
    data:
      edited: false
      editors:
      - harpercarroll
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2055274397134781
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638ae4bcf6a0bc48582c4239/LlixeQbFoK0p47FQpa7lz.png?w=200&h=200&f=face
          fullname: Harper Carroll
          isHf: false
          isPro: false
          name: harpercarroll
          type: user
        html: '<p>You want to specify the <code>target_modules</code> which are the
          layers to apply LoRA to. I did the linear layers:<br>    target_modules=[<br>        "q_proj",<br>        "k_proj",<br>        "v_proj",<br>        "o_proj",<br>        "gate_proj",<br>        "up_proj",<br>        "down_proj",<br>        "lm_head",<br>    ],
          </p>

          '
        raw: "You want to specify the `target_modules` which are the layers to apply\
          \ LoRA to. I did the linear layers: \n    target_modules=[\n        \"q_proj\"\
          , \n        \"k_proj\", \n        \"v_proj\", \n        \"o_proj\", \n \
          \       \"gate_proj\", \n        \"up_proj\", \n        \"down_proj\", \n\
          \        \"lm_head\",\n    ], "
        updatedAt: '2023-10-02T17:31:27.814Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mbkim
    id: 651afe6f77d6b4b1ea518e3f
    type: comment
  author: harpercarroll
  content: "You want to specify the `target_modules` which are the layers to apply\
    \ LoRA to. I did the linear layers: \n    target_modules=[\n        \"q_proj\"\
    , \n        \"k_proj\", \n        \"v_proj\", \n        \"o_proj\", \n       \
    \ \"gate_proj\", \n        \"up_proj\", \n        \"down_proj\", \n        \"\
    lm_head\",\n    ], "
  created_at: 2023-10-02 16:31:27+00:00
  edited: false
  hidden: false
  id: 651afe6f77d6b4b1ea518e3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0878bc930e27371f55e24196c199ff62.svg
      fullname: Dx
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NPap
      type: user
    createdAt: '2023-10-11T10:23:08.000Z'
    data:
      edited: false
      editors:
      - NPap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9712090492248535
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0878bc930e27371f55e24196c199ff62.svg
          fullname: Dx
          isHf: false
          isPro: false
          name: NPap
          type: user
        html: "<p>According to the LoRA paper, the best performance you can get is\
          \ if you apply it to all the linear layers of the model. (Diminishing returns\
          \ if you were to choose just the Q and V but they propose all if I'm not\
          \ mistaken)<br>Which are the ones <span data-props=\"{&quot;user&quot;:&quot;harpercarroll&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/harpercarroll\"\
          >@<span class=\"underline\">harpercarroll</span></a></span>\n\n\t</span></span>\
          \ has mentioned. But if you want to go deeper and apply this to other models\
          \ as well what you would do is instantiate your model, then go and print(model)\
          \ then you will see all the layers with their names and you would want to\
          \ include in the target_modules all layers that have linear or 4bitlinear\
          \ (something like that if its 4bit). \n </p>\n"
        raw: "According to the LoRA paper, the best performance you can get is if\
          \ you apply it to all the linear layers of the model. (Diminishing returns\
          \ if you were to choose just the Q and V but they propose all if I'm not\
          \ mistaken)\nWhich are the ones @harpercarroll has mentioned. But if you\
          \ want to go deeper and apply this to other models as well what you would\
          \ do is instantiate your model, then go and print(model) then you will see\
          \ all the layers with their names and you would want to include in the target_modules\
          \ all layers that have linear or 4bitlinear (something like that if its\
          \ 4bit). \n "
        updatedAt: '2023-10-11T10:23:08.905Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - 1TuanPham
        - andresenric
        - mbkim
        - maraoz
    id: 6526778c5567b6fa615b931d
    type: comment
  author: NPap
  content: "According to the LoRA paper, the best performance you can get is if you\
    \ apply it to all the linear layers of the model. (Diminishing returns if you\
    \ were to choose just the Q and V but they propose all if I'm not mistaken)\n\
    Which are the ones @harpercarroll has mentioned. But if you want to go deeper\
    \ and apply this to other models as well what you would do is instantiate your\
    \ model, then go and print(model) then you will see all the layers with their\
    \ names and you would want to include in the target_modules all layers that have\
    \ linear or 4bitlinear (something like that if its 4bit). \n "
  created_at: 2023-10-11 09:23:08+00:00
  edited: false
  hidden: false
  id: 6526778c5567b6fa615b931d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: 'ValueError: Please specify `target_modules` in `peft_config`'
