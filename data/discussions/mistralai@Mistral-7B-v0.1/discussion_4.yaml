!!python/object:huggingface_hub.community.DiscussionWithDetails
author: stephen-standd
conflicting_files: null
created_at: 2023-09-27 20:52:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63fccb965d2bea4588be8fb1/jQTk9OLceg4Zz_1RyKtj-.png?w=200&h=200&f=face
      fullname: Stephen Solka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: stephen-standd
      type: user
    createdAt: '2023-09-27T21:52:56.000Z'
    data:
      edited: false
      editors:
      - stephen-standd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.975811779499054
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63fccb965d2bea4588be8fb1/jQTk9OLceg4Zz_1RyKtj-.png?w=200&h=200&f=face
          fullname: Stephen Solka
          isHf: false
          isPro: false
          name: stephen-standd
          type: user
        html: '<p>I can''t find this in the blog post or readme but what is the context
          window available for these models? I might have missed it!</p>

          '
        raw: I can't find this in the blog post or readme but what is the context
          window available for these models? I might have missed it!
        updatedAt: '2023-09-27T21:52:56.094Z'
      numEdits: 0
      reactions: []
    id: 6514a43891aa56a7b1f942f8
    type: comment
  author: stephen-standd
  content: I can't find this in the blog post or readme but what is the context window
    available for these models? I might have missed it!
  created_at: 2023-09-27 20:52:56+00:00
  edited: false
  hidden: false
  id: 6514a43891aa56a7b1f942f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bf92acdd143e93dc0b9acf9c93a7b14.svg
      fullname: Darrel Bryan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZeroXClem
      type: user
    createdAt: '2023-09-27T22:12:21.000Z'
    data:
      edited: true
      editors:
      - ZeroXClem
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.829058051109314
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bf92acdd143e93dc0b9acf9c93a7b14.svg
          fullname: Darrel Bryan
          isHf: false
          isPro: false
          name: ZeroXClem
          type: user
        html: '<p>8K sequence length noted from their product page. <a rel="nofollow"
          href="https://mistral.ai/product/">https://mistral.ai/product/</a><br>For
          a more detailed specification read here: <a rel="nofollow" href="https://mistral.ai/news/announcing-mistral-7b/">https://mistral.ai/news/announcing-mistral-7b/</a></p>

          '
        raw: "8K sequence length noted from their product page. https://mistral.ai/product/\
          \ \nFor a more detailed specification read here: https://mistral.ai/news/announcing-mistral-7b/"
        updatedAt: '2023-09-27T22:15:29.324Z'
      numEdits: 2
      reactions:
      - count: 7
        reaction: "\u2764\uFE0F"
        users:
        - clem
        - stephen-standd
        - szelesaron
        - Pkoosha
        - victor
        - aiprm-christophc
        - ingo-m
    id: 6514a8c5be453924e06026f5
    type: comment
  author: ZeroXClem
  content: "8K sequence length noted from their product page. https://mistral.ai/product/\
    \ \nFor a more detailed specification read here: https://mistral.ai/news/announcing-mistral-7b/"
  created_at: 2023-09-27 21:12:21+00:00
  edited: true
  hidden: false
  id: 6514a8c5be453924e06026f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658503618672-noauth.jpeg?w=200&h=200&f=face
      fullname: lerela
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lerela
      type: user
    createdAt: '2023-10-05T07:44:27.000Z'
    data:
      edited: false
      editors:
      - lerela
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8313212394714355
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658503618672-noauth.jpeg?w=200&h=200&f=face
          fullname: lerela
          isHf: false
          isPro: false
          name: lerela
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;ZeroXClem&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ZeroXClem\"\
          >@<span class=\"underline\">ZeroXClem</span></a></span>\n\n\t</span></span>\
          \  for the answer, and it's also in Transformers documentation: <a href=\"\
          https://huggingface.co/docs/transformers/v4.34.0/en/model_doc/mistral#model-details\"\
          >https://huggingface.co/docs/transformers/v4.34.0/en/model_doc/mistral#model-details</a></p>\n"
        raw: 'Thanks @ZeroXClem  for the answer, and it''s also in Transformers documentation:
          https://huggingface.co/docs/transformers/v4.34.0/en/model_doc/mistral#model-details'
        updatedAt: '2023-10-05T07:44:27.661Z'
      numEdits: 0
      reactions: []
      relatedEventId: 651e695b4b6b06ceb867d37e
    id: 651e695b4b6b06ceb867d37d
    type: comment
  author: lerela
  content: 'Thanks @ZeroXClem  for the answer, and it''s also in Transformers documentation:
    https://huggingface.co/docs/transformers/v4.34.0/en/model_doc/mistral#model-details'
  created_at: 2023-10-05 06:44:27+00:00
  edited: false
  hidden: false
  id: 651e695b4b6b06ceb867d37d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658503618672-noauth.jpeg?w=200&h=200&f=face
      fullname: lerela
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lerela
      type: user
    createdAt: '2023-10-05T07:44:27.000Z'
    data:
      status: closed
    id: 651e695b4b6b06ceb867d37e
    type: status-change
  author: lerela
  created_at: 2023-10-05 06:44:27+00:00
  id: 651e695b4b6b06ceb867d37e
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/657be320aac476bbb5353b0e725e8893.svg
      fullname: Yokai Koibito
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YokaiKoibito
      type: user
    createdAt: '2023-10-05T22:47:29.000Z'
    data:
      edited: true
      editors:
      - YokaiKoibito
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9324697852134705
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/657be320aac476bbb5353b0e725e8893.svg
          fullname: Yokai Koibito
          isHf: false
          isPro: false
          name: YokaiKoibito
          type: user
        html: '<p>Technically it''s unlimited with a 4k sliding window context size.</p>

          <p>Under the hood the stacked layers allow the possibility of indirectly
          attending to things more than 4k token previously, but that now requires
          multiple attention-hops, and there''s no backward-in-token-space propagation
          of the attention-query to before the sliding window. So in effect, if something''s
          outside the sliding window, the model can attend to it only if it previously
          attended to it at a token less than 4k tokens previously: so outside the
          sliding window the model can learn to have capabilities more like LSTM-level
          than attention.</p>

          '
        raw: 'Technically it''s unlimited with a 4k sliding window context size.


          Under the hood the stacked layers allow the possibility of indirectly attending
          to things more than 4k token previously, but that now requires multiple
          attention-hops, and there''s no backward-in-token-space propagation of the
          attention-query to before the sliding window. So in effect, if something''s
          outside the sliding window, the model can attend to it only if it previously
          attended to it at a token less than 4k tokens previously: so outside the
          sliding window the model can learn to have capabilities more like LSTM-level
          than attention.'
        updatedAt: '2023-10-05T22:49:25.470Z'
      numEdits: 1
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - visionfield
        - ingo-m
        - vikarti-anatra
        - OliP
        - TravelingMan
        - untilhamza
    id: 651f3d010bb29b2f4eca5790
    type: comment
  author: YokaiKoibito
  content: 'Technically it''s unlimited with a 4k sliding window context size.


    Under the hood the stacked layers allow the possibility of indirectly attending
    to things more than 4k token previously, but that now requires multiple attention-hops,
    and there''s no backward-in-token-space propagation of the attention-query to
    before the sliding window. So in effect, if something''s outside the sliding window,
    the model can attend to it only if it previously attended to it at a token less
    than 4k tokens previously: so outside the sliding window the model can learn to
    have capabilities more like LSTM-level than attention.'
  created_at: 2023-10-05 21:47:29+00:00
  edited: true
  hidden: false
  id: 651f3d010bb29b2f4eca5790
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: closed
target_branch: null
title: context window size
