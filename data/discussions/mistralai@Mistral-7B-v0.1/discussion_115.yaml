!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MaBrThesis2023
conflicting_files: null
created_at: 2023-12-28 15:41:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9f0886e7de01e42f24fb2fc2ae4f3d87.svg
      fullname: Max Brauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaBrThesis2023
      type: user
    createdAt: '2023-12-28T15:41:17.000Z'
    data:
      edited: false
      editors:
      - MaBrThesis2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6678109169006348
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9f0886e7de01e42f24fb2fc2ae4f3d87.svg
          fullname: Max Brauer
          isHf: false
          isPro: false
          name: MaBrThesis2023
          type: user
        html: '<p>I''m trying to fine-tune mistral-7b on a task where it is important
          for the model to only output a label and nothing else. Hence I am formatting
          my train_dataset as follows:</p>

          <p>f"some system prompt\n{user_input}\nLable:{label}"</p>

          <p>my eval_dataset looks like:</p>

          <p>f"some system prompt\n{user_input}\nLable:"</p>

          <p>now I am using the huggingface Trainer to fine-tune:</p>

          <p>run_name = BASE_MODEL_ID.split("/")[-1]  + PROJECT_NAME<br>output_dir
          = "./" + run_name<br>trainer_args = TrainingArguments(<br>               output_dir=output_dir,<br>               warmup_steps=2,<br>               per_device_train_batch_size=2,<br>               gradient_accumulation_steps=16,<br>               gradient_checkpointing=True,<br>               max_steps=200,<br>               learning_rate=2e-5,<br>               bf16=True,<br>               optim="paged_adamw_8bit",<br>               load_best_model_at_end=True,<br>               metric_for_best_model="eval_loss",<br>               logging_steps=32,<br>               logging_dir="./logs",<br>               save_strategy="steps",<br>               save_steps=32,<br>               evaluation_strategy="steps",<br>               eval_steps=32,<br>              )</p>

          <p>trainer = Trainer(<br>    model=model,<br>    train_dataset=train_ds,<br>    eval_dataset=test_ds,<br>    args=trainer_args,<br>    data_collator=DataCollatorForLanguageModeling(tokenizer,
          mlm=False),<br>    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],<br>)<br>However,
          when I use the data collator every padding token is set to -100 as I defined
          pad_token = eos_token. Is there a way to keep this behavior but add a eos
          token to the end of the sequence that doesn''t get converted to -100 by
          the data collator?</p>

          <p>That would look something like this (assuming 2 to be the eos_token_id):</p>

          <p>[-100, -100 -100, .... 55,32,4,2]</p>

          '
        raw: "I'm trying to fine-tune mistral-7b on a task where it is important for\
          \ the model to only output a label and nothing else. Hence I am formatting\
          \ my train_dataset as follows:\r\n\r\nf\"some system prompt\\n{user_input}\\\
          nLable:{label}\"\r\n\r\nmy eval_dataset looks like:\r\n\r\nf\"some system\
          \ prompt\\n{user_input}\\nLable:\"\r\n\r\nnow I am using the huggingface\
          \ Trainer to fine-tune:\r\n\r\nrun_name = BASE_MODEL_ID.split(\"/\")[-1]\
          \  + PROJECT_NAME\r\noutput_dir = \"./\" + run_name\r\ntrainer_args = TrainingArguments(\r\
          \n               output_dir=output_dir,\r\n               warmup_steps=2,\r\
          \n               per_device_train_batch_size=2,\r\n               gradient_accumulation_steps=16,\r\
          \n               gradient_checkpointing=True,\r\n               max_steps=200,\r\
          \n               learning_rate=2e-5, \r\n               bf16=True,\r\n \
          \              optim=\"paged_adamw_8bit\",\r\n               load_best_model_at_end=True,\r\
          \n               metric_for_best_model=\"eval_loss\",\r\n              \
          \ logging_steps=32,              \r\n               logging_dir=\"./logs\"\
          ,        \r\n               save_strategy=\"steps\",     \r\n          \
          \     save_steps=32,                \r\n               evaluation_strategy=\"\
          steps\", \r\n               eval_steps=32,               \r\n          \
          \    )\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    train_dataset=train_ds,\r\
          \n    eval_dataset=test_ds,\r\n    args=trainer_args,\r\n    data_collator=DataCollatorForLanguageModeling(tokenizer,\
          \ mlm=False),\r\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],\r\
          \n)\r\nHowever, when I use the data collator every padding token is set\
          \ to -100 as I defined pad_token = eos_token. Is there a way to keep this\
          \ behavior but add a eos token to the end of the sequence that doesn't get\
          \ converted to -100 by the data collator?\r\n\r\nThat would look something\
          \ like this (assuming 2 to be the eos_token_id):\r\n\r\n[-100, -100 -100,\
          \ .... 55,32,4,2]"
        updatedAt: '2023-12-28T15:41:17.412Z'
      numEdits: 0
      reactions: []
    id: 658d971db0b97fa6642372fd
    type: comment
  author: MaBrThesis2023
  content: "I'm trying to fine-tune mistral-7b on a task where it is important for\
    \ the model to only output a label and nothing else. Hence I am formatting my\
    \ train_dataset as follows:\r\n\r\nf\"some system prompt\\n{user_input}\\nLable:{label}\"\
    \r\n\r\nmy eval_dataset looks like:\r\n\r\nf\"some system prompt\\n{user_input}\\\
    nLable:\"\r\n\r\nnow I am using the huggingface Trainer to fine-tune:\r\n\r\n\
    run_name = BASE_MODEL_ID.split(\"/\")[-1]  + PROJECT_NAME\r\noutput_dir = \"./\"\
    \ + run_name\r\ntrainer_args = TrainingArguments(\r\n               output_dir=output_dir,\r\
    \n               warmup_steps=2,\r\n               per_device_train_batch_size=2,\r\
    \n               gradient_accumulation_steps=16,\r\n               gradient_checkpointing=True,\r\
    \n               max_steps=200,\r\n               learning_rate=2e-5, \r\n   \
    \            bf16=True,\r\n               optim=\"paged_adamw_8bit\",\r\n    \
    \           load_best_model_at_end=True,\r\n               metric_for_best_model=\"\
    eval_loss\",\r\n               logging_steps=32,              \r\n           \
    \    logging_dir=\"./logs\",        \r\n               save_strategy=\"steps\"\
    ,     \r\n               save_steps=32,                \r\n               evaluation_strategy=\"\
    steps\", \r\n               eval_steps=32,               \r\n              )\r\
    \n\r\ntrainer = Trainer(\r\n    model=model,\r\n    train_dataset=train_ds,\r\n\
    \    eval_dataset=test_ds,\r\n    args=trainer_args,\r\n    data_collator=DataCollatorForLanguageModeling(tokenizer,\
    \ mlm=False),\r\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],\r\
    \n)\r\nHowever, when I use the data collator every padding token is set to -100\
    \ as I defined pad_token = eos_token. Is there a way to keep this behavior but\
    \ add a eos token to the end of the sequence that doesn't get converted to -100\
    \ by the data collator?\r\n\r\nThat would look something like this (assuming 2\
    \ to be the eos_token_id):\r\n\r\n[-100, -100 -100, .... 55,32,4,2]"
  created_at: 2023-12-28 15:41:17+00:00
  edited: false
  hidden: false
  id: 658d971db0b97fa6642372fd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 115
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: Data collator removing eos token
