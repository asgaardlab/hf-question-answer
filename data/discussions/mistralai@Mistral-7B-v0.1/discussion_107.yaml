!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Pradeep1995
conflicting_files: null
created_at: 2023-12-20 06:13:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
      fullname: Pradeep T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pradeep1995
      type: user
    createdAt: '2023-12-20T06:13:43.000Z'
    data:
      edited: false
      editors:
      - Pradeep1995
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9694685339927673
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
          fullname: Pradeep T
          isHf: false
          isPro: false
          name: Pradeep1995
          type: user
        html: '<p>So the output of my model ends abruptly and I ideally want it to
          complete the paragraph/sentences/code which it was it between of.<br>Although
          I have provided max_new_tokens = 300 and also in prompt I give to limit
          by 300 words.</p>

          <p>The response is always big and ends abruptly. Any way I can ask for a
          complete output within desired number of output tokens?</p>

          '
        raw: "So the output of my model ends abruptly and I ideally want it to complete\
          \ the paragraph/sentences/code which it was it between of.\r\nAlthough I\
          \ have provided max_new_tokens = 300 and also in prompt I give to limit\
          \ by 300 words.\r\n\r\nThe response is always big and ends abruptly. Any\
          \ way I can ask for a complete output within desired number of output tokens?"
        updatedAt: '2023-12-20T06:13:43.163Z'
      numEdits: 0
      reactions: []
    id: 6582861700d5f548f226e203
    type: comment
  author: Pradeep1995
  content: "So the output of my model ends abruptly and I ideally want it to complete\
    \ the paragraph/sentences/code which it was it between of.\r\nAlthough I have\
    \ provided max_new_tokens = 300 and also in prompt I give to limit by 300 words.\r\
    \n\r\nThe response is always big and ends abruptly. Any way I can ask for a complete\
    \ output within desired number of output tokens?"
  created_at: 2023-12-20 06:13:43+00:00
  edited: false
  hidden: false
  id: 6582861700d5f548f226e203
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/786d40e05f3b873e3f5be4bd78aeb4ca.svg
      fullname: snf sniffski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sniffski
      type: user
    createdAt: '2023-12-20T06:43:27.000Z'
    data:
      edited: false
      editors:
      - sniffski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9016029834747314
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/786d40e05f3b873e3f5be4bd78aeb4ca.svg
          fullname: snf sniffski
          isHf: false
          isPro: false
          name: sniffski
          type: user
        html: '<p>I think you need to put max token output higher than max word count...
          For example put it 350 or 380</p>

          '
        raw: I think you need to put max token output higher than max word count...
          For example put it 350 or 380
        updatedAt: '2023-12-20T06:43:27.488Z'
      numEdits: 0
      reactions: []
    id: 65828d0f594cf42f8fa88104
    type: comment
  author: sniffski
  content: I think you need to put max token output higher than max word count...
    For example put it 350 or 380
  created_at: 2023-12-20 06:43:27+00:00
  edited: false
  hidden: false
  id: 65828d0f594cf42f8fa88104
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
      fullname: Pradeep T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pradeep1995
      type: user
    createdAt: '2023-12-20T06:51:37.000Z'
    data:
      edited: true
      editors:
      - Pradeep1995
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.42747172713279724
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
          fullname: Pradeep T
          isHf: false
          isPro: false
          name: Pradeep1995
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sniffski&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sniffski\">@<span class=\"\
          underline\">sniffski</span></a></span>\n\n\t</span></span>  this is my current\
          \ configuration</p>\n<pre><code>generation_config = GenerationConfig(\n\
          \    do_sample=True,\n    top_k=10,\n    temperature=0.01,\n    pad_token_id=tokenizer.eos_token_id,\n\
          \    early_stopping = True,\n    max_new_tokens=300,\n    return_full_text=False\n\
          )\n</code></pre>\n<p>so what change you are proposing here?</p>\n"
        raw: "@sniffski  this is my current configuration\n```\ngeneration_config\
          \ = GenerationConfig(\n    do_sample=True,\n    top_k=10,\n    temperature=0.01,\n\
          \    pad_token_id=tokenizer.eos_token_id,\n    early_stopping = True,\n\
          \    max_new_tokens=300,\n    return_full_text=False\n)\n```\nso what change\
          \ you are proposing here?\n"
        updatedAt: '2023-12-20T06:52:01.927Z'
      numEdits: 2
      reactions: []
    id: 65828ef9f3006507ea07dfd0
    type: comment
  author: Pradeep1995
  content: "@sniffski  this is my current configuration\n```\ngeneration_config =\
    \ GenerationConfig(\n    do_sample=True,\n    top_k=10,\n    temperature=0.01,\n\
    \    pad_token_id=tokenizer.eos_token_id,\n    early_stopping = True,\n    max_new_tokens=300,\n\
    \    return_full_text=False\n)\n```\nso what change you are proposing here?\n"
  created_at: 2023-12-20 06:51:37+00:00
  edited: true
  hidden: false
  id: 65828ef9f3006507ea07dfd0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/786d40e05f3b873e3f5be4bd78aeb4ca.svg
      fullname: snf sniffski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sniffski
      type: user
    createdAt: '2023-12-20T07:11:16.000Z'
    data:
      edited: false
      editors:
      - sniffski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9466315507888794
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/786d40e05f3b873e3f5be4bd78aeb4ca.svg
          fullname: snf sniffski
          isHf: false
          isPro: false
          name: sniffski
          type: user
        html: '<p>Well, first thing you should know is one word is not one token...
          I think the rule of thumb was one token is 0.75 words... so in that case
          if you are requesting in the prompt answer not more than 300 words you need
          to set <code>max_new_tokens=400,</code> because 300*0.75=400</p>

          '
        raw: Well, first thing you should know is one word is not one token... I think
          the rule of thumb was one token is 0.75 words... so in that case if you
          are requesting in the prompt answer not more than 300 words you need to
          set `max_new_tokens=400,` because 300*0.75=400
        updatedAt: '2023-12-20T07:11:16.322Z'
      numEdits: 0
      reactions: []
    id: 6582939400d5f548f228a71b
    type: comment
  author: sniffski
  content: Well, first thing you should know is one word is not one token... I think
    the rule of thumb was one token is 0.75 words... so in that case if you are requesting
    in the prompt answer not more than 300 words you need to set `max_new_tokens=400,`
    because 300*0.75=400
  created_at: 2023-12-20 07:11:16+00:00
  edited: false
  hidden: false
  id: 6582939400d5f548f228a71b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
      fullname: Pradeep T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pradeep1995
      type: user
    createdAt: '2023-12-20T07:47:45.000Z'
    data:
      edited: false
      editors:
      - Pradeep1995
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9163687825202942
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
          fullname: Pradeep T
          isHf: false
          isPro: false
          name: Pradeep1995
          type: user
        html: '<p>i tried with <code>max_new_tokens=400</code>. but still, the response
          ends abruptly problem exists. the generation suddenly stops as soon as it
          reaches the specified number of max_new_tokens reached, without checking
          whether the sentence is completed or not. </p>

          '
        raw: 'i tried with ```max_new_tokens=400```. but still, the response ends
          abruptly problem exists. the generation suddenly stops as soon as it reaches
          the specified number of max_new_tokens reached, without checking whether
          the sentence is completed or not. '
        updatedAt: '2023-12-20T07:47:45.130Z'
      numEdits: 0
      reactions: []
    id: 65829c2138f102154fe09ac9
    type: comment
  author: Pradeep1995
  content: 'i tried with ```max_new_tokens=400```. but still, the response ends abruptly
    problem exists. the generation suddenly stops as soon as it reaches the specified
    number of max_new_tokens reached, without checking whether the sentence is completed
    or not. '
  created_at: 2023-12-20 07:47:45+00:00
  edited: false
  hidden: false
  id: 65829c2138f102154fe09ac9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/786d40e05f3b873e3f5be4bd78aeb4ca.svg
      fullname: snf sniffski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sniffski
      type: user
    createdAt: '2023-12-20T07:59:18.000Z'
    data:
      edited: true
      editors:
      - sniffski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9496487975120544
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/786d40e05f3b873e3f5be4bd78aeb4ca.svg
          fullname: snf sniffski
          isHf: false
          isPro: false
          name: sniffski
          type: user
        html: '<p>Can you copy the output in a temp file like <code>wc-test.txt</code>
          then run in shell <code>wc wc-test</code> to see how many words are there
          (second number from the output of wc command)... If they are more than 300
          than the model doesn''t obey your request in the prompt for maximum words
          in response and issue is not the max token limit... I guess you would need
          to find better prompt... Try something like starting with "Your task is
          to respond with 300 words or less..."</p>

          '
        raw: Can you copy the output in a temp file like `wc-test.txt` then run in
          shell `wc wc-test` to see how many words are there (second number from the
          output of wc command)... If they are more than 300 than the model doesn't
          obey your request in the prompt for maximum words in response and issue
          is not the max token limit... I guess you would need to find better prompt...
          Try something like starting with "Your task is to respond with 300 words
          or less..."
        updatedAt: '2023-12-20T07:59:46.766Z'
      numEdits: 1
      reactions: []
    id: 65829ed6594cf42f8fabb77b
    type: comment
  author: sniffski
  content: Can you copy the output in a temp file like `wc-test.txt` then run in shell
    `wc wc-test` to see how many words are there (second number from the output of
    wc command)... If they are more than 300 than the model doesn't obey your request
    in the prompt for maximum words in response and issue is not the max token limit...
    I guess you would need to find better prompt... Try something like starting with
    "Your task is to respond with 300 words or less..."
  created_at: 2023-12-20 07:59:18+00:00
  edited: true
  hidden: false
  id: 65829ed6594cf42f8fabb77b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/294428e076de4da54ac7e7a1cc67cdf6.svg
      fullname: Suman Vakare
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SumanVakare
      type: user
    createdAt: '2023-12-21T10:46:35.000Z'
    data:
      edited: true
      editors:
      - SumanVakare
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9618651270866394
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/294428e076de4da54ac7e7a1cc67cdf6.svg
          fullname: Suman Vakare
          isHf: false
          isPro: false
          name: SumanVakare
          type: user
        html: '<p>I have similar case, I am using API interference to run this model,
          my output is incomplete and usually the same length (65-80) words and most
          of the times it doesn''t even end correctly, see below example of input
          and output followed by some part of the code.</p>

          <p>Input : "Write a detailed essay about trees"</p>

          <p>Output : "Trees are one of the most important elements of nature. They
          provide us with oxygen, clean the air, and provide shade from the sun. They
          also provide us with a variety of other benefits, such as providing food
          and shelter for animals, and helping to regulate the climate. Trees are
          also a source of beauty and inspiration, and can be used to create a sense
          of calm and peace in our lives. In this essay, I will explore the many benefits
          of trees, as well as their"</p>

          <p>import requests</p>

          <p>API_URL = "<a rel="nofollow" href="https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1&quot;">https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1"</a><br>HEADERS
          = {"Authorization": "Bearer xxxxxx"}</p>

          <p>def query(payload):<br>    try:<br>response = requests.post(API_URL,
          headers=HEADERS, json=payload)<br>        response.raise_for_status()<br>        return
          response.json()<br>..............</p>

          '
        raw: "I have similar case, I am using API interference to run this model,\
          \ my output is incomplete and usually the same length (65-80) words and\
          \ most of the times it doesn't even end correctly, see below example of\
          \ input and output followed by some part of the code.\n\nInput : \"Write\
          \ a detailed essay about trees\"\n\nOutput : \"Trees are one of the most\
          \ important elements of nature. They provide us with oxygen, clean the air,\
          \ and provide shade from the sun. They also provide us with a variety of\
          \ other benefits, such as providing food and shelter for animals, and helping\
          \ to regulate the climate. Trees are also a source of beauty and inspiration,\
          \ and can be used to create a sense of calm and peace in our lives. In this\
          \ essay, I will explore the many benefits of trees, as well as their\"\n\
          \n\n\nimport requests\n \nAPI_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\"\
          \nHEADERS = {\"Authorization\": \"Bearer xxxxxx\"}\n \n \ndef query(payload):\n\
          \    try:\nresponse = requests.post(API_URL, headers=HEADERS, json=payload)\n\
          \        response.raise_for_status()\n        return response.json()\n.............."
        updatedAt: '2023-12-21T10:49:09.017Z'
      numEdits: 2
      reactions: []
    id: 6584178b2031da25dac0988d
    type: comment
  author: SumanVakare
  content: "I have similar case, I am using API interference to run this model, my\
    \ output is incomplete and usually the same length (65-80) words and most of the\
    \ times it doesn't even end correctly, see below example of input and output followed\
    \ by some part of the code.\n\nInput : \"Write a detailed essay about trees\"\n\
    \nOutput : \"Trees are one of the most important elements of nature. They provide\
    \ us with oxygen, clean the air, and provide shade from the sun. They also provide\
    \ us with a variety of other benefits, such as providing food and shelter for\
    \ animals, and helping to regulate the climate. Trees are also a source of beauty\
    \ and inspiration, and can be used to create a sense of calm and peace in our\
    \ lives. In this essay, I will explore the many benefits of trees, as well as\
    \ their\"\n\n\n\nimport requests\n \nAPI_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\"\
    \nHEADERS = {\"Authorization\": \"Bearer xxxxxx\"}\n \n \ndef query(payload):\n\
    \    try:\nresponse = requests.post(API_URL, headers=HEADERS, json=payload)\n\
    \        response.raise_for_status()\n        return response.json()\n.............."
  created_at: 2023-12-21 10:46:35+00:00
  edited: true
  hidden: false
  id: 6584178b2031da25dac0988d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
      fullname: Pradeep T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pradeep1995
      type: user
    createdAt: '2023-12-21T10:54:10.000Z'
    data:
      edited: false
      editors:
      - Pradeep1995
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9802656769752502
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
          fullname: Pradeep T
          isHf: false
          isPro: false
          name: Pradeep1995
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;SumanVakare&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/SumanVakare\"\
          >@<span class=\"underline\">SumanVakare</span></a></span>\n\n\t</span></span>\
          \ how did you solve this issue?</p>\n"
        raw: '@SumanVakare how did you solve this issue?'
        updatedAt: '2023-12-21T10:54:10.391Z'
      numEdits: 0
      reactions: []
    id: 658419521cb2408e08bfb960
    type: comment
  author: Pradeep1995
  content: '@SumanVakare how did you solve this issue?'
  created_at: 2023-12-21 10:54:10+00:00
  edited: false
  hidden: false
  id: 658419521cb2408e08bfb960
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/294428e076de4da54ac7e7a1cc67cdf6.svg
      fullname: Suman Vakare
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SumanVakare
      type: user
    createdAt: '2023-12-21T11:04:06.000Z'
    data:
      edited: false
      editors:
      - SumanVakare
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9729980230331421
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/294428e076de4da54ac7e7a1cc67cdf6.svg
          fullname: Suman Vakare
          isHf: false
          isPro: false
          name: SumanVakare
          type: user
        html: '<p>Not solved yet, I am looking for solution too.</p>

          '
        raw: Not solved yet, I am looking for solution too.
        updatedAt: '2023-12-21T11:04:06.739Z'
      numEdits: 0
      reactions: []
    id: 65841ba6c5599ad89a31adda
    type: comment
  author: SumanVakare
  content: Not solved yet, I am looking for solution too.
  created_at: 2023-12-21 11:04:06+00:00
  edited: false
  hidden: false
  id: 65841ba6c5599ad89a31adda
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 107
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: Incomplete Output even with max_new_tokens
