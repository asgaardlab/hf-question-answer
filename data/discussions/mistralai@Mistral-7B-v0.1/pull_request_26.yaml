!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lewtun
conflicting_files: []
created_at: 2023-10-02 06:20:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
      fullname: Lewis Tunstall
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lewtun
      type: user
    createdAt: '2023-10-02T07:20:35.000Z'
    data:
      edited: true
      editors:
      - lewtun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5356635451316833
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
          fullname: Lewis Tunstall
          isHf: true
          isPro: false
          name: lewtun
          type: user
        html: "<p>This PR fixes an issue in the Mistral tokenizer where the special\
          \ tokens aren't tokenized correctly if concatenated with other characters,\
          \ e.g.</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\
          \ \n\ntokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\"\
          >\"mistralai/Mistral-7B-v0.1\"</span>)\n\n<span class=\"hljs-comment\">#\
          \ Gives correct IDs: {'input_ids': [2], 'attention_mask': [1]}</span>\n\
          tokenizer(<span class=\"hljs-string\">\"&lt;/s&gt;\"</span>, add_special_tokens=<span\
          \ class=\"hljs-literal\">False</span>) \n\n<span class=\"hljs-comment\"\
          ># Gives correct IDs: {'input_ids': [842], 'attention_mask': [1]}</span>\n\
          tokenizer(<span class=\"hljs-string\">\".\"</span>, add_special_tokens=<span\
          \ class=\"hljs-literal\">False</span>)\n\n<span class=\"hljs-comment\">#\
          \ Gives incorrect IDs: {'input_ids': [842, 700, 28713, 28767], 'attention_mask':\
          \ [1, 1, 1, 1]}</span>\ntokenizer(<span class=\"hljs-string\">\".&lt;/s&gt;\"\
          </span>, add_special_tokens=<span class=\"hljs-literal\">False</span>)\n\
          </code></pre>\n<p>The solution is to disable normalization for all the special\
          \ tokens, which is what this PR does. Note that until this PR is merged,\
          \ the following workaround with the slow tokenizer can be adopted:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\n\
          \ntokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\"\
          >\"mistralai/Mistral-7B-v0.1\"</span>, from_slow = <span class=\"hljs-literal\"\
          >True</span>)\n</code></pre>\n"
        raw: "This PR fixes an issue in the Mistral tokenizer where the special tokens\
          \ aren't tokenized correctly if concatenated with other characters, e.g.\n\
          \n```python\nfrom transformers import AutoTokenizer \n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          mistralai/Mistral-7B-v0.1\")\n\n# Gives correct IDs: {'input_ids': [2],\
          \ 'attention_mask': [1]}\ntokenizer(\"</s>\", add_special_tokens=False)\
          \ \n\n# Gives correct IDs: {'input_ids': [842], 'attention_mask': [1]}\n\
          tokenizer(\".\", add_special_tokens=False)\n\n# Gives incorrect IDs: {'input_ids':\
          \ [842, 700, 28713, 28767], 'attention_mask': [1, 1, 1, 1]}\ntokenizer(\"\
          .</s>\", add_special_tokens=False)\n```\n\nThe solution is to disable normalization\
          \ for all the special tokens, which is what this PR does. Note that until\
          \ this PR is merged, the following workaround with the slow tokenizer can\
          \ be adopted:\n\n```python\nfrom transformers import AutoTokenizer\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", from_slow\
          \ = True)\n```"
        updatedAt: '2023-10-03T07:24:57.045Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - AlxSpKB
    id: 651a6f4342097d8c5950d12e
    type: comment
  author: lewtun
  content: "This PR fixes an issue in the Mistral tokenizer where the special tokens\
    \ aren't tokenized correctly if concatenated with other characters, e.g.\n\n```python\n\
    from transformers import AutoTokenizer \n\ntokenizer = AutoTokenizer.from_pretrained(\"\
    mistralai/Mistral-7B-v0.1\")\n\n# Gives correct IDs: {'input_ids': [2], 'attention_mask':\
    \ [1]}\ntokenizer(\"</s>\", add_special_tokens=False) \n\n# Gives correct IDs:\
    \ {'input_ids': [842], 'attention_mask': [1]}\ntokenizer(\".\", add_special_tokens=False)\n\
    \n# Gives incorrect IDs: {'input_ids': [842, 700, 28713, 28767], 'attention_mask':\
    \ [1, 1, 1, 1]}\ntokenizer(\".</s>\", add_special_tokens=False)\n```\n\nThe solution\
    \ is to disable normalization for all the special tokens, which is what this PR\
    \ does. Note that until this PR is merged, the following workaround with the slow\
    \ tokenizer can be adopted:\n\n```python\nfrom transformers import AutoTokenizer\n\
    \ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", from_slow\
    \ = True)\n```"
  created_at: 2023-10-02 06:20:35+00:00
  edited: true
  hidden: false
  id: 651a6f4342097d8c5950d12e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
      fullname: Lewis Tunstall
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lewtun
      type: user
    createdAt: '2023-10-02T07:20:36.000Z'
    data:
      oid: 5f9cf2a1989fdcad773ff16c3177e0711f895d38
      parents:
      - 78814a934d8ce59ee93599378f3e929ff137da06
      subject: Disable normalization for special tokens
    id: 651a6f440000000000000000
    type: commit
  author: lewtun
  created_at: 2023-10-02 06:20:36+00:00
  id: 651a6f440000000000000000
  oid: 5f9cf2a1989fdcad773ff16c3177e0711f895d38
  summary: Disable normalization for special tokens
  type: commit
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658503618672-noauth.jpeg?w=200&h=200&f=face
      fullname: lerela
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lerela
      type: user
    createdAt: '2023-10-03T12:52:52.000Z'
    data:
      status: merged
    id: 651c0ea43265a1bb83b2a3a7
    type: status-change
  author: lerela
  created_at: 2023-10-03 11:52:52+00:00
  id: 651c0ea43265a1bb83b2a3a7
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: f966a600a1f2c35a067dcb16b24bbf9ac3b85799
num: 26
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: merged
target_branch: refs/heads/main
title: Disable normalization for special tokens
