!!python/object:huggingface_hub.community.DiscussionWithDetails
author: victor314159
conflicting_files: null
created_at: 2023-10-19 19:23:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d990120b7ab569a6ae37057026c432f.svg
      fullname: Victor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: victor314159
      type: user
    createdAt: '2023-10-19T20:23:31.000Z'
    data:
      edited: true
      editors:
      - victor314159
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6238797307014465
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d990120b7ab569a6ae37057026c432f.svg
          fullname: Victor
          isHf: false
          isPro: false
          name: victor314159
          type: user
        html: "<p>I'm brand new to AI. So not familiar with all the concepts yet.\
          \ Still, my minimal chat program is very simple, and I am already getting\
          \ a worrying warning, even if the model seems to work.</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM, AutoTokenizer\n\
          <span class=\"hljs-keyword\">import</span> torch\n\ndevice = <span class=\"\
          hljs-string\">\"cuda\"</span>  <span class=\"hljs-comment\"># The device\
          \ to load the model onto</span>\n\nmodelName = <span class=\"hljs-string\"\
          >\"../Mistral-7B-Instruct-v0.1\"</span>\n\nmodel = AutoModelForCausalLM.from_pretrained(modelName,\
          \ device_map=<span class=\"hljs-string\">\"auto\"</span>, torch_dtype=torch.float16)\
          \ <span class=\"hljs-comment\">#load in fp16 to fit on a RTX4090</span>\n\
          tokenizer = AutoTokenizer.from_pretrained(modelName)\n\n<span class=\"hljs-comment\"\
          ># Initialize an empty conversation history</span>\nconversation_history\
          \ = []\n\n<span class=\"hljs-comment\"># Define a function to generate responses</span>\n\
          <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >generate_response</span>(<span class=\"hljs-params\">input_text, model,\
          \ tokenizer, device, conversation_history</span>):\n\n    messages = conversation_history\
          \ + [{<span class=\"hljs-string\">\"role\"</span>: <span class=\"hljs-string\"\
          >\"user\"</span>, <span class=\"hljs-string\">\"content\"</span>: input_text}]\n\
          \n    encodeds = tokenizer.apply_chat_template(messages, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>)\n    model_inputs = encodeds.to(device)\n\
          \    generated_ids = model.generate(model_inputs, max_new_tokens=<span class=\"\
          hljs-number\">1000</span>, do_sample=<span class=\"hljs-literal\">True</span>)\n\
          \    decoded = tokenizer.batch_decode(generated_ids)\n    <span class=\"\
          hljs-keyword\">return</span> decoded[<span class=\"hljs-number\">0</span>]\n\
          \n<span class=\"hljs-keyword\">while</span> <span class=\"hljs-literal\"\
          >True</span>:\n    user_input = <span class=\"hljs-built_in\">input</span>(<span\
          \ class=\"hljs-string\">\"You: \"</span>)\n    <span class=\"hljs-keyword\"\
          >if</span> user_input.lower() == <span class=\"hljs-string\">\"exit\"</span>:\n\
          \        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"Chatbot: Goodbye!\"</span>)\n        <span class=\"hljs-keyword\">break</span>\n\
          \    response = generate_response(user_input, model, tokenizer, device,\
          \ conversation_history)\n    <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">\"Chatbot:\"</span>, response[response.rfind(<span\
          \ class=\"hljs-string\">\"[/INST]\"</span>) + <span class=\"hljs-built_in\"\
          >len</span>(<span class=\"hljs-string\">\"[/INST]\"</span>):response.rfind(<span\
          \ class=\"hljs-string\">\"&lt;/s&gt;\"</span>) ])\n\n    <span class=\"\
          hljs-comment\"># Update the conversation history with the user's input and\
          \ the bot's response</span>\n    conversation_history.append({<span class=\"\
          hljs-string\">\"role\"</span>: <span class=\"hljs-string\">\"user\"</span>,\
          \ <span class=\"hljs-string\">\"content\"</span>: user_input})\n    conversation_history.append({<span\
          \ class=\"hljs-string\">\"role\"</span>: <span class=\"hljs-string\">\"\
          assistant\"</span>, <span class=\"hljs-string\">\"content\"</span>: response})\n\
          </code></pre>\n<p>I am getting the following warning:<br><code>The attention\
          \ mask and the pad token id were not set. As a consequence, you may observe\
          \ unexpected behavior. Please pass your input's `attention_mask` to obtain\
          \ reliable results. Setting `pad_token_id` to `eos_token_id`:2 for open-end\
          \ generation.</code></p>\n<p>Am I missing something?</p>\n"
        raw: "I'm brand new to AI. So not familiar with all the concepts yet. Still,\
          \ my minimal chat program is very simple, and I am already getting a worrying\
          \ warning, even if the model seems to work.\n\n```python\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ndevice = \"\
          cuda\"  # The device to load the model onto\n\nmodelName = \"../Mistral-7B-Instruct-v0.1\"\
          \n\nmodel = AutoModelForCausalLM.from_pretrained(modelName, device_map=\"\
          auto\", torch_dtype=torch.float16) #load in fp16 to fit on a RTX4090\ntokenizer\
          \ = AutoTokenizer.from_pretrained(modelName)\n\n# Initialize an empty conversation\
          \ history\nconversation_history = []\n\n# Define a function to generate\
          \ responses\ndef generate_response(input_text, model, tokenizer, device,\
          \ conversation_history):\n\n    messages = conversation_history + [{\"role\"\
          : \"user\", \"content\": input_text}]\n\n    encodeds = tokenizer.apply_chat_template(messages,\
          \ return_tensors=\"pt\")\n    model_inputs = encodeds.to(device)\n    generated_ids\
          \ = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n\
          \    decoded = tokenizer.batch_decode(generated_ids)\n    return decoded[0]\n\
          \nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower()\
          \ == \"exit\":\n        print(\"Chatbot: Goodbye!\")\n        break\n  \
          \  response = generate_response(user_input, model, tokenizer, device, conversation_history)\n\
          \    print(\"Chatbot:\", response[response.rfind(\"[/INST]\") + len(\"[/INST]\"\
          ):response.rfind(\"</s>\") ])\n\n    # Update the conversation history with\
          \ the user's input and the bot's response\n    conversation_history.append({\"\
          role\": \"user\", \"content\": user_input})\n    conversation_history.append({\"\
          role\": \"assistant\", \"content\": response})\n```\n\nI am getting the\
          \ following warning: \n```The attention mask and the pad token id were not\
          \ set. As a consequence, you may observe unexpected behavior. Please pass\
          \ your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id`\
          \ to `eos_token_id`:2 for open-end generation.```\n\nAm I missing something?"
        updatedAt: '2023-10-19T20:26:51.047Z'
      numEdits: 2
      reactions: []
    id: 653190431dd8ebbdc19b6229
    type: comment
  author: victor314159
  content: "I'm brand new to AI. So not familiar with all the concepts yet. Still,\
    \ my minimal chat program is very simple, and I am already getting a worrying\
    \ warning, even if the model seems to work.\n\n```python\nfrom transformers import\
    \ AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ndevice = \"cuda\"  # The\
    \ device to load the model onto\n\nmodelName = \"../Mistral-7B-Instruct-v0.1\"\
    \n\nmodel = AutoModelForCausalLM.from_pretrained(modelName, device_map=\"auto\"\
    , torch_dtype=torch.float16) #load in fp16 to fit on a RTX4090\ntokenizer = AutoTokenizer.from_pretrained(modelName)\n\
    \n# Initialize an empty conversation history\nconversation_history = []\n\n# Define\
    \ a function to generate responses\ndef generate_response(input_text, model, tokenizer,\
    \ device, conversation_history):\n\n    messages = conversation_history + [{\"\
    role\": \"user\", \"content\": input_text}]\n\n    encodeds = tokenizer.apply_chat_template(messages,\
    \ return_tensors=\"pt\")\n    model_inputs = encodeds.to(device)\n    generated_ids\
    \ = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n    decoded\
    \ = tokenizer.batch_decode(generated_ids)\n    return decoded[0]\n\nwhile True:\n\
    \    user_input = input(\"You: \")\n    if user_input.lower() == \"exit\":\n \
    \       print(\"Chatbot: Goodbye!\")\n        break\n    response = generate_response(user_input,\
    \ model, tokenizer, device, conversation_history)\n    print(\"Chatbot:\", response[response.rfind(\"\
    [/INST]\") + len(\"[/INST]\"):response.rfind(\"</s>\") ])\n\n    # Update the\
    \ conversation history with the user's input and the bot's response\n    conversation_history.append({\"\
    role\": \"user\", \"content\": user_input})\n    conversation_history.append({\"\
    role\": \"assistant\", \"content\": response})\n```\n\nI am getting the following\
    \ warning: \n```The attention mask and the pad token id were not set. As a consequence,\
    \ you may observe unexpected behavior. Please pass your input's `attention_mask`\
    \ to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for\
    \ open-end generation.```\n\nAm I missing something?"
  created_at: 2023-10-19 19:23:31+00:00
  edited: true
  hidden: false
  id: 653190431dd8ebbdc19b6229
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/153c7801f659ec0f6ec155b2eb4bb144.svg
      fullname: Greg Broadhead
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheGeeBee
      type: user
    createdAt: '2023-10-27T13:51:54.000Z'
    data:
      edited: false
      editors:
      - TheGeeBee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6135725378990173
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/153c7801f659ec0f6ec155b2eb4bb144.svg
          fullname: Greg Broadhead
          isHf: false
          isPro: false
          name: TheGeeBee
          type: user
        html: '<p>You need to use either the AutoConfig or MistralConfig libraries
          to set the configuration details.</p>

          '
        raw: You need to use either the AutoConfig or MistralConfig libraries to set
          the configuration details.
        updatedAt: '2023-10-27T13:51:54.231Z'
      numEdits: 0
      reactions: []
    id: 653bc07ae750bb25a92684fb
    type: comment
  author: TheGeeBee
  content: You need to use either the AutoConfig or MistralConfig libraries to set
    the configuration details.
  created_at: 2023-10-27 12:51:54+00:00
  edited: false
  hidden: false
  id: 653bc07ae750bb25a92684fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2074fd742b8a12960f86181f2328bec4.svg
      fullname: xiaoping Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xiaoping777
      type: user
    createdAt: '2023-11-13T02:54:26.000Z'
    data:
      edited: true
      editors:
      - xiaoping777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.881519615650177
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2074fd742b8a12960f86181f2328bec4.svg
          fullname: xiaoping Yang
          isHf: false
          isPro: false
          name: xiaoping777
          type: user
        html: '<p>I tried to load config and create model from config, while the config
          is not cooperated with device_map:"auto" feature, even I put it in the config.json
          file, so cannot load the models in two GPUs automatically</p>

          '
        raw: I tried to load config and create model from config, while the config
          is not cooperated with device_map:"auto" feature, even I put it in the config.json
          file, so cannot load the models in two GPUs automatically
        updatedAt: '2023-11-13T02:57:57.400Z'
      numEdits: 1
      reactions: []
    id: 65518fe22382f591ee1fe758
    type: comment
  author: xiaoping777
  content: I tried to load config and create model from config, while the config is
    not cooperated with device_map:"auto" feature, even I put it in the config.json
    file, so cannot load the models in two GPUs automatically
  created_at: 2023-11-13 02:54:26+00:00
  edited: true
  hidden: false
  id: 65518fe22382f591ee1fe758
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 64
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: The attention mask and the pad token id were not set.
