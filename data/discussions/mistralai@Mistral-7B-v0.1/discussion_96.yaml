!!python/object:huggingface_hub.community.DiscussionWithDetails
author: shayak
conflicting_files: null
created_at: 2023-11-27 00:40:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e9d367636f6d82e015d31e8889a8985.svg
      fullname: Shayak Dutta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shayak
      type: user
    createdAt: '2023-11-27T00:40:49.000Z'
    data:
      edited: false
      editors:
      - shayak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7274409532546997
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e9d367636f6d82e015d31e8889a8985.svg
          fullname: Shayak Dutta
          isHf: false
          isPro: false
          name: shayak
          type: user
        html: "<p>Completely new to locally running llms. I'm using the following\
          \ code to load the model into memory, using transformers library. What am\
          \ I doing wrong? Using float16 dtype I would expect 14GB memory usage for\
          \ a 7B model no? However it shows me around 5-6 gb of my gpu being used.\
          \ Running nvidia-smi shows the same as well. </p>\n<p>Using model.to(DEVICE)\
          \ throws an error, but without it I'm assuming a portion of it is running\
          \ on gpu? How do I make it load the full 14GB of the model onto gpu?</p>\n\
          <p>// DEVICE variable is set to 'cuda'</p>\n<p>bnb_config = BitsAndBytesConfig(<br>\
          \        load_in_4bit=True,<br>        bnb_4bit_quant_type=\"nf4\",<br>\
          \        bnb_4bit_use_double_quant=True,<br>    )</p>\n<pre><code>model\
          \ = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    load_in_4bit=True,\n\
          \    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n\
          \    device_map=DEVICE,\n    trust_remote_code=True,\n)\n# model.to(DEVICE)\n\
          tokenizer = AutoTokenizer.from_pretrained(model_path)\nprint('loaded.')\n\
          \npynvml.nvmlInit()\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)\ninfo\
          \ = pynvml.nvmlDeviceGetMemoryInfo(handle)\nprint(f\"GPU memory occupied:\
          \ {info.used // 1024 ** 2} MB.\")\n</code></pre>\n"
        raw: "Completely new to locally running llms. I'm using the following code\
          \ to load the model into memory, using transformers library. What am I doing\
          \ wrong? Using float16 dtype I would expect 14GB memory usage for a 7B model\
          \ no? However it shows me around 5-6 gb of my gpu being used. Running nvidia-smi\
          \ shows the same as well. \r\n\r\nUsing model.to(DEVICE) throws an error,\
          \ but without it I'm assuming a portion of it is running on gpu? How do\
          \ I make it load the full 14GB of the model onto gpu?\r\n\r\n// DEVICE variable\
          \ is set to 'cuda'\r\n\r\nbnb_config = BitsAndBytesConfig(\r\n        load_in_4bit=True,\r\
          \n        bnb_4bit_quant_type=\"nf4\",\r\n        bnb_4bit_use_double_quant=True,\r\
          \n    )\r\n\r\n    model = AutoModelForCausalLM.from_pretrained(\r\n   \
          \     model_path,\r\n        load_in_4bit=True,\r\n        quantization_config=bnb_config,\r\
          \n        torch_dtype=torch.bfloat16,\r\n        device_map=DEVICE,\r\n\
          \        trust_remote_code=True,\r\n    )\r\n    # model.to(DEVICE)\r\n\
          \    tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n    print('loaded.')\r\
          \n\r\n    pynvml.nvmlInit()\r\n    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\r\
          \n    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\r\n    print(f\"GPU\
          \ memory occupied: {info.used // 1024 ** 2} MB.\")\r\n\r\n"
        updatedAt: '2023-11-27T00:40:49.710Z'
      numEdits: 0
      reactions: []
    id: 6563e5914e74b2075a38ae3f
    type: comment
  author: shayak
  content: "Completely new to locally running llms. I'm using the following code to\
    \ load the model into memory, using transformers library. What am I doing wrong?\
    \ Using float16 dtype I would expect 14GB memory usage for a 7B model no? However\
    \ it shows me around 5-6 gb of my gpu being used. Running nvidia-smi shows the\
    \ same as well. \r\n\r\nUsing model.to(DEVICE) throws an error, but without it\
    \ I'm assuming a portion of it is running on gpu? How do I make it load the full\
    \ 14GB of the model onto gpu?\r\n\r\n// DEVICE variable is set to 'cuda'\r\n\r\
    \nbnb_config = BitsAndBytesConfig(\r\n        load_in_4bit=True,\r\n        bnb_4bit_quant_type=\"\
    nf4\",\r\n        bnb_4bit_use_double_quant=True,\r\n    )\r\n\r\n    model =\
    \ AutoModelForCausalLM.from_pretrained(\r\n        model_path,\r\n        load_in_4bit=True,\r\
    \n        quantization_config=bnb_config,\r\n        torch_dtype=torch.bfloat16,\r\
    \n        device_map=DEVICE,\r\n        trust_remote_code=True,\r\n    )\r\n \
    \   # model.to(DEVICE)\r\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\r\
    \n    print('loaded.')\r\n\r\n    pynvml.nvmlInit()\r\n    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\r\
    \n    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\r\n    print(f\"GPU memory\
    \ occupied: {info.used // 1024 ** 2} MB.\")\r\n\r\n"
  created_at: 2023-11-27 00:40:49+00:00
  edited: false
  hidden: false
  id: 6563e5914e74b2075a38ae3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090856b1e62cfa4f5c23ccb/XtetET8dL65viJDOKIWzl.jpeg?w=200&h=200&f=face
      fullname: Thomas Wood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: odellus
      type: user
    createdAt: '2023-11-27T02:44:20.000Z'
    data:
      edited: false
      editors:
      - odellus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8300621509552002
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090856b1e62cfa4f5c23ccb/XtetET8dL65viJDOKIWzl.jpeg?w=200&h=200&f=face
          fullname: Thomas Wood
          isHf: false
          isPro: true
          name: odellus
          type: user
        html: "<p>If you want full fp32 model on GPU then just do</p>\n<pre><code\
          \ class=\"language-python\">model = AutoModelForCausalLM.from_pretrained(\n\
          \    model_path,\n    device_map = DEVICE,\n    trust_remote_code = <span\
          \ class=\"hljs-literal\">True</span>,\n)\n</code></pre>\n<p>and it won't\
          \ quantize the model (which shrinks it footprint in memory by using lower\
          \ precision numbers)</p>\n"
        raw: "If you want full fp32 model on GPU then just do\n```python\nmodel =\
          \ AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map\
          \ = DEVICE,\n    trust_remote_code = True,\n)\n```\nand it won't quantize\
          \ the model (which shrinks it footprint in memory by using lower precision\
          \ numbers)"
        updatedAt: '2023-11-27T02:44:20.993Z'
      numEdits: 0
      reactions: []
    id: 656402842c14555119eb1559
    type: comment
  author: odellus
  content: "If you want full fp32 model on GPU then just do\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    model_path,\n    device_map = DEVICE,\n    trust_remote_code = True,\n)\n\
    ```\nand it won't quantize the model (which shrinks it footprint in memory by\
    \ using lower precision numbers)"
  created_at: 2023-11-27 02:44:20+00:00
  edited: false
  hidden: false
  id: 656402842c14555119eb1559
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 96
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: Why is this 7B model only showing 5GB of gpu ram allocation?
