!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nidabijapure
conflicting_files: null
created_at: 2023-10-18 05:55:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e996c5ef51771b1cf6bb2ebc77bdd7ba.svg
      fullname: Nida Bijapure
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nidabijapure
      type: user
    createdAt: '2023-10-18T06:55:09.000Z'
    data:
      edited: false
      editors:
      - nidabijapure
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6960008144378662
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e996c5ef51771b1cf6bb2ebc77bdd7ba.svg
          fullname: Nida Bijapure
          isHf: false
          isPro: false
          name: nidabijapure
          type: user
        html: '<p>I am getting a warning of the "Number of tokens exceeded maximum
          context length (512)" as shown in the screenshot below.<br>how to solve
          this issue.<br>code :<br>def load_llm():<br>    # Load the locally downloaded
          model here<br>    llm = CTransformers(<br>        model = "TheBloke/Mistral-7B-Instruct-v0.1-GGUF",<br>        model_type="llama",<br>        max_new_tokens
          = 512,<br>        temperature = 0.5<br>    )<br>    return llm</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/652e689c0b13cc073968d1d2/i-0Ge5cMnBJWQaiiYsLK_.png"><img
          alt="MicrosoftTeams-image (1).png" src="https://cdn-uploads.huggingface.co/production/uploads/652e689c0b13cc073968d1d2/i-0Ge5cMnBJWQaiiYsLK_.png"></a></p>

          '
        raw: "I am getting a warning of the \"Number of tokens exceeded maximum context\
          \ length (512)\" as shown in the screenshot below.\r\nhow to solve this\
          \ issue.\r\ncode :\r\ndef load_llm():\r\n    # Load the locally downloaded\
          \ model here\r\n    llm = CTransformers(\r\n        model = \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\
          ,\r\n        model_type=\"llama\",\r\n        max_new_tokens = 512,\r\n\
          \        temperature = 0.5\r\n    )\r\n    return llm\r\n\r\n![MicrosoftTeams-image\
          \ (1).png](https://cdn-uploads.huggingface.co/production/uploads/652e689c0b13cc073968d1d2/i-0Ge5cMnBJWQaiiYsLK_.png)\r\
          \n"
        updatedAt: '2023-10-18T06:55:09.259Z'
      numEdits: 0
      reactions: []
    id: 652f814d2424124437802026
    type: comment
  author: nidabijapure
  content: "I am getting a warning of the \"Number of tokens exceeded maximum context\
    \ length (512)\" as shown in the screenshot below.\r\nhow to solve this issue.\r\
    \ncode :\r\ndef load_llm():\r\n    # Load the locally downloaded model here\r\n\
    \    llm = CTransformers(\r\n        model = \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\
    ,\r\n        model_type=\"llama\",\r\n        max_new_tokens = 512,\r\n      \
    \  temperature = 0.5\r\n    )\r\n    return llm\r\n\r\n![MicrosoftTeams-image\
    \ (1).png](https://cdn-uploads.huggingface.co/production/uploads/652e689c0b13cc073968d1d2/i-0Ge5cMnBJWQaiiYsLK_.png)\r\
    \n"
  created_at: 2023-10-18 05:55:09+00:00
  edited: false
  hidden: false
  id: 652f814d2424124437802026
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/153c7801f659ec0f6ec155b2eb4bb144.svg
      fullname: Greg Broadhead
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheGeeBee
      type: user
    createdAt: '2023-10-27T13:52:53.000Z'
    data:
      edited: false
      editors:
      - TheGeeBee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48981645703315735
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/153c7801f659ec0f6ec155b2eb4bb144.svg
          fullname: Greg Broadhead
          isHf: false
          isPro: false
          name: TheGeeBee
          type: user
        html: '<p>Increase your max_new_tokens value.</p>

          '
        raw: Increase your max_new_tokens value.
        updatedAt: '2023-10-27T13:52:53.737Z'
      numEdits: 0
      reactions: []
    id: 653bc0b5ccfb7ba1cbb60e32
    type: comment
  author: TheGeeBee
  content: Increase your max_new_tokens value.
  created_at: 2023-10-27 12:52:53+00:00
  edited: false
  hidden: false
  id: 653bc0b5ccfb7ba1cbb60e32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/cucWq-Y61gZ7K3lF4brBq.png?w=200&h=200&f=face
      fullname: Naveen Pandey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Naveen2000
      type: user
    createdAt: '2024-01-11T07:44:35.000Z'
    data:
      edited: false
      editors:
      - Naveen2000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9605520963668823
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/cucWq-Y61gZ7K3lF4brBq.png?w=200&h=200&f=face
          fullname: Naveen Pandey
          isHf: false
          isPro: false
          name: Naveen2000
          type: user
        html: '<p>i am still facing this issue</p>

          '
        raw: 'i am still facing this issue

          '
        updatedAt: '2024-01-11T07:44:35.610Z'
      numEdits: 0
      reactions: []
    id: 659f9c63e5f0eba1c48c0f6c
    type: comment
  author: Naveen2000
  content: 'i am still facing this issue

    '
  created_at: 2024-01-11 07:44:35+00:00
  edited: false
  hidden: false
  id: 659f9c63e5f0eba1c48c0f6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/153c7801f659ec0f6ec155b2eb4bb144.svg
      fullname: Greg Broadhead
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheGeeBee
      type: user
    createdAt: '2024-01-11T14:36:18.000Z'
    data:
      edited: false
      editors:
      - TheGeeBee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.913024365901947
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/153c7801f659ec0f6ec155b2eb4bb144.svg
          fullname: Greg Broadhead
          isHf: false
          isPro: false
          name: TheGeeBee
          type: user
        html: '<p>Is your maximum context length still 512 tokens?  You should be
          setting your ''max_new_tokens = '' to at least 2048 if you''re planning
          on using that many tokens in an interaction.  Otherwise you need to start
          trimming the context that you''re sending into the LLM.  There are a number
          of options for context trimming, like removing the first x# of tokens, or
          obtaining a condensed summary of the existing context using another LLM
          that does summarization and using that to feed into the conversation instead
          of the actual interaction history.</p>

          <p>At the very least, unless you aren''t planning on feeding anything other
          than a 1-shot prompt into the LLM,  a 512 token context is too small (it
          includes your prompt and the response from the LLM)</p>

          '
        raw: 'Is your maximum context length still 512 tokens?  You should be setting
          your ''max_new_tokens = '' to at least 2048 if you''re planning on using
          that many tokens in an interaction.  Otherwise you need to start trimming
          the context that you''re sending into the LLM.  There are a number of options
          for context trimming, like removing the first x# of tokens, or obtaining
          a condensed summary of the existing context using another LLM that does
          summarization and using that to feed into the conversation instead of the
          actual interaction history.


          At the very least, unless you aren''t planning on feeding anything other
          than a 1-shot prompt into the LLM,  a 512 token context is too small (it
          includes your prompt and the response from the LLM)'
        updatedAt: '2024-01-11T14:36:18.477Z'
      numEdits: 0
      reactions: []
    id: 659ffce221c219062c0eb53e
    type: comment
  author: TheGeeBee
  content: 'Is your maximum context length still 512 tokens?  You should be setting
    your ''max_new_tokens = '' to at least 2048 if you''re planning on using that
    many tokens in an interaction.  Otherwise you need to start trimming the context
    that you''re sending into the LLM.  There are a number of options for context
    trimming, like removing the first x# of tokens, or obtaining a condensed summary
    of the existing context using another LLM that does summarization and using that
    to feed into the conversation instead of the actual interaction history.


    At the very least, unless you aren''t planning on feeding anything other than
    a 1-shot prompt into the LLM,  a 512 token context is too small (it includes your
    prompt and the response from the LLM)'
  created_at: 2024-01-11 14:36:18+00:00
  edited: false
  hidden: false
  id: 659ffce221c219062c0eb53e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 60
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: token limit exceeded
