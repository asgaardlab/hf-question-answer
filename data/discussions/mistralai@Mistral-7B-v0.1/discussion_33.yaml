!!python/object:huggingface_hub.community.DiscussionWithDetails
author: brunoedcf
conflicting_files: null
created_at: 2023-10-03 03:30:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646f43c685ccfb39f62dd325/n-uaflS13rsq1YlzHJcyz.jpeg?w=200&h=200&f=face
      fullname: Bruno Esteves
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brunoedcf
      type: user
    createdAt: '2023-10-03T04:30:59.000Z'
    data:
      edited: false
      editors:
      - brunoedcf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5218226909637451
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646f43c685ccfb39f62dd325/n-uaflS13rsq1YlzHJcyz.jpeg?w=200&h=200&f=face
          fullname: Bruno Esteves
          isHf: false
          isPro: false
          name: brunoedcf
          type: user
        html: '<p>I have a template like this one:</p>

          <ul>

          <li>Is it possible to run mistral-7b using a similar template?</li>

          <li>What pytorch + cuda do I need to install?</li>

          <li>What dependencies are necessary?</li>

          </ul>

          <p>from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(  )</p>

          <p>pipeline = pipeline(<br>    "text-generation",<br>    model= ,<br>    torch_dtype=torch.float16,<br>    device_map="auto",<br>    eos_token_id=tokenizer.eos_token_id,<br>    pad_token_id=tokenizer.eos_token_id,<br>)</p>

          <p>prompt = "prompt"</p>

          <p>sequences = []</p>

          <p>sequences.append(<br>  pipeline(<br>     prompt,<br>     top_k=10,<br>     max_new_tokens=150,<br>     num_return_sequences=1,<br>  )<br>)</p>

          <p>  for sequence in sequences:<br>    generated_text = sequence[0][''generated_text'']</p>

          <p>print(generated_text)</p>

          '
        raw: "I have a template like this one:\r\n\r\n- Is it possible to run mistral-7b\
          \ using a similar template?\r\n- What pytorch + cuda do I need to install?\r\
          \n- What dependencies are necessary?\r\n\r\nfrom transformers import pipeline,\
          \ AutoModelForCausalLM, AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\
          \ <model> )\r\n\r\npipeline = pipeline(\r\n    \"text-generation\",\r\n\
          \    model= <model>, \r\n    torch_dtype=torch.float16,\r\n    device_map=\"\
          auto\",\r\n    eos_token_id=tokenizer.eos_token_id,\r\n    pad_token_id=tokenizer.eos_token_id,\r\
          \n)\r\n\r\nprompt = \"prompt\"\r\n\r\nsequences = []\r\n\r\nsequences.append(\r\
          \n  pipeline(\r\n     prompt,\r\n     top_k=10,\r\n     max_new_tokens=150,\r\
          \n     num_return_sequences=1,\r\n  )\r\n)\r\n\r\n  for sequence in sequences:\r\
          \n    generated_text = sequence[0]['generated_text']\r\n\r\nprint(generated_text)"
        updatedAt: '2023-10-03T04:30:59.023Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - brunoedcf
    id: 651b99036e0cfc098da3bbb7
    type: comment
  author: brunoedcf
  content: "I have a template like this one:\r\n\r\n- Is it possible to run mistral-7b\
    \ using a similar template?\r\n- What pytorch + cuda do I need to install?\r\n\
    - What dependencies are necessary?\r\n\r\nfrom transformers import pipeline, AutoModelForCausalLM,\
    \ AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained( <model> )\r\n\
    \r\npipeline = pipeline(\r\n    \"text-generation\",\r\n    model= <model>, \r\
    \n    torch_dtype=torch.float16,\r\n    device_map=\"auto\",\r\n    eos_token_id=tokenizer.eos_token_id,\r\
    \n    pad_token_id=tokenizer.eos_token_id,\r\n)\r\n\r\nprompt = \"prompt\"\r\n\
    \r\nsequences = []\r\n\r\nsequences.append(\r\n  pipeline(\r\n     prompt,\r\n\
    \     top_k=10,\r\n     max_new_tokens=150,\r\n     num_return_sequences=1,\r\n\
    \  )\r\n)\r\n\r\n  for sequence in sequences:\r\n    generated_text = sequence[0]['generated_text']\r\
    \n\r\nprint(generated_text)"
  created_at: 2023-10-03 03:30:59+00:00
  edited: false
  hidden: false
  id: 651b99036e0cfc098da3bbb7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 33
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: 'Default template and configuration for local run with GPU '
