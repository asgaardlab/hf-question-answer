!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ubermenchh
conflicting_files: null
created_at: 2023-10-25 14:09:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62b08b71c5eb29748f016027/0xftJgrGvdY2QMrAm-zhF.jpeg?w=200&h=200&f=face
      fullname: ubermenchh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ubermenchh
      type: user
    createdAt: '2023-10-25T15:09:52.000Z'
    data:
      edited: false
      editors:
      - ubermenchh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4527115523815155
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62b08b71c5eb29748f016027/0xftJgrGvdY2QMrAm-zhF.jpeg?w=200&h=200&f=face
          fullname: ubermenchh
          isHf: false
          isPro: false
          name: ubermenchh
          type: user
        html: "<p>I am getting the following error:</p>\n<pre><code>---------------------------------------------------------------------------\n\
          ImportError                               Traceback (most recent call last)\n\
          Cell In[6], line 7\n      1 bnb_config = BitsAndBytesConfig(\n      2  \
          \   load_in_4bit=True,\n      3     bnb_4bit_quant_type='nf4',\n      4\
          \     bnb_4bit_compute_dtype=torch.bfloat16,\n      5     bnb_4bit_use_double_quant=False\n\
          \      6 )\n----&gt; 7 model = AutoModelForCausalLM.from_pretrained(base_model,\
          \ quantization_config=bnb_config, device_map={'':0})\n      8 model.config.use_cache\
          \ = False\n      9 model.config.pretraining_tp = 1\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:565,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    563 elif type(config) in cls._model_mapping.keys():\n\
          \    564     model_class = _get_model_class(config, cls._model_mapping)\n\
          --&gt; 565     return model_class.from_pretrained(\n    566         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\n    567     )\n  \
          \  568 raise ValueError(\n    569     f\"Unrecognized configuration class\
          \ {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n\
          \    570     f\"Model type should be one of {', '.join(c.__name__ for c\
          \ in cls._model_mapping.keys())}.\"\n    571 )\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2681,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\n   2679 if load_in_8bit\
          \ or load_in_4bit:\n   2680     if not (is_accelerate_available() and is_bitsandbytes_available()):\n\
          -&gt; 2681         raise ImportError(\n   2682             \"Using `load_in_8bit=True`\
          \ requires Accelerate: `pip install accelerate` and the latest version of\"\
          \n   2683             \" bitsandbytes `pip install -i https://test.pypi.org/simple/\
          \ bitsandbytes` or\"\n   2684             \" pip install bitsandbytes` \"\
          \n   2685         )\n   2687     if torch_dtype is None:\n   2688      \
          \   # We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\n\
          \   2689         logger.info(\n   2690             f\"Overriding torch_dtype={torch_dtype}\
          \ with `torch_dtype=torch.float16` due to \"\n   2691             \"requirements\
          \ of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \"\n   2692\
          \             \"Pass your own torch_dtype to specify the dtype of the remaining\
          \ non-linear layers or pass\"\n   2693             \" torch_dtype=torch.float16\
          \ to remove this warning.\"\n   2694         )\n\nImportError: Using `load_in_8bit=True`\
          \ requires Accelerate: `pip install accelerate` and the latest version of\
          \ bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes`\
          \ or pip install bitsandbytes` \n</code></pre>\n<p>I tried updating the\
          \ transformers lib to the latest version but then it is giving me, <code>KeyError:\
          \ 'mistral'</code><br>I have been upgrading and downgrading the libraries\
          \ for about 2 hours now.</p>\n"
        raw: "I am getting the following error:\r\n```\r\n---------------------------------------------------------------------------\r\
          \nImportError                               Traceback (most recent call\
          \ last)\r\nCell In[6], line 7\r\n      1 bnb_config = BitsAndBytesConfig(\r\
          \n      2     load_in_4bit=True,\r\n      3     bnb_4bit_quant_type='nf4',\r\
          \n      4     bnb_4bit_compute_dtype=torch.bfloat16,\r\n      5     bnb_4bit_use_double_quant=False\r\
          \n      6 )\r\n----> 7 model = AutoModelForCausalLM.from_pretrained(base_model,\
          \ quantization_config=bnb_config, device_map={'':0})\r\n      8 model.config.use_cache\
          \ = False\r\n      9 model.config.pretraining_tp = 1\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:565,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\r\n    563 elif type(config) in cls._model_mapping.keys():\r\
          \n    564     model_class = _get_model_class(config, cls._model_mapping)\r\
          \n--> 565     return model_class.from_pretrained(\r\n    566         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\r\n    567     )\r\n\
          \    568 raise ValueError(\r\n    569     f\"Unrecognized configuration\
          \ class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\\
          n\"\r\n    570     f\"Model type should be one of {', '.join(c.__name__\
          \ for c in cls._model_mapping.keys())}.\"\r\n    571 )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2681,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\r\n   2679 if\
          \ load_in_8bit or load_in_4bit:\r\n   2680     if not (is_accelerate_available()\
          \ and is_bitsandbytes_available()):\r\n-> 2681         raise ImportError(\r\
          \n   2682             \"Using `load_in_8bit=True` requires Accelerate: `pip\
          \ install accelerate` and the latest version of\"\r\n   2683           \
          \  \" bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes`\
          \ or\"\r\n   2684             \" pip install bitsandbytes` \"\r\n   2685\
          \         )\r\n   2687     if torch_dtype is None:\r\n   2688         #\
          \ We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\r\
          \n   2689         logger.info(\r\n   2690             f\"Overriding torch_dtype={torch_dtype}\
          \ with `torch_dtype=torch.float16` due to \"\r\n   2691             \"requirements\
          \ of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \"\r\n  \
          \ 2692             \"Pass your own torch_dtype to specify the dtype of the\
          \ remaining non-linear layers or pass\"\r\n   2693             \" torch_dtype=torch.float16\
          \ to remove this warning.\"\r\n   2694         )\r\n\r\nImportError: Using\
          \ `load_in_8bit=True` requires Accelerate: `pip install accelerate` and\
          \ the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/\
          \ bitsandbytes` or pip install bitsandbytes` \r\n```\r\n\r\nI tried updating\
          \ the transformers lib to the latest version but then it is giving me, ```KeyError:\
          \ 'mistral'```\r\nI have been upgrading and downgrading the libraries for\
          \ about 2 hours now."
        updatedAt: '2023-10-25T15:09:52.834Z'
      numEdits: 0
      reactions: []
    id: 65392fc0d1ca3238d0cb30f6
    type: comment
  author: ubermenchh
  content: "I am getting the following error:\r\n```\r\n---------------------------------------------------------------------------\r\
    \nImportError                               Traceback (most recent call last)\r\
    \nCell In[6], line 7\r\n      1 bnb_config = BitsAndBytesConfig(\r\n      2  \
    \   load_in_4bit=True,\r\n      3     bnb_4bit_quant_type='nf4',\r\n      4  \
    \   bnb_4bit_compute_dtype=torch.bfloat16,\r\n      5     bnb_4bit_use_double_quant=False\r\
    \n      6 )\r\n----> 7 model = AutoModelForCausalLM.from_pretrained(base_model,\
    \ quantization_config=bnb_config, device_map={'':0})\r\n      8 model.config.use_cache\
    \ = False\r\n      9 model.config.pretraining_tp = 1\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:565,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\r\n    563 elif type(config) in cls._model_mapping.keys():\r\n   \
    \ 564     model_class = _get_model_class(config, cls._model_mapping)\r\n--> 565\
    \     return model_class.from_pretrained(\r\n    566         pretrained_model_name_or_path,\
    \ *model_args, config=config, **hub_kwargs, **kwargs\r\n    567     )\r\n    568\
    \ raise ValueError(\r\n    569     f\"Unrecognized configuration class {config.__class__}\
    \ for this kind of AutoModel: {cls.__name__}.\\n\"\r\n    570     f\"Model type\
    \ should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\
    \r\n    571 )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2681,\
    \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config,\
    \ cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token,\
    \ revision, use_safetensors, *model_args, **kwargs)\r\n   2679 if load_in_8bit\
    \ or load_in_4bit:\r\n   2680     if not (is_accelerate_available() and is_bitsandbytes_available()):\r\
    \n-> 2681         raise ImportError(\r\n   2682             \"Using `load_in_8bit=True`\
    \ requires Accelerate: `pip install accelerate` and the latest version of\"\r\n\
    \   2683             \" bitsandbytes `pip install -i https://test.pypi.org/simple/\
    \ bitsandbytes` or\"\r\n   2684             \" pip install bitsandbytes` \"\r\n\
    \   2685         )\r\n   2687     if torch_dtype is None:\r\n   2688         #\
    \ We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\r\
    \n   2689         logger.info(\r\n   2690             f\"Overriding torch_dtype={torch_dtype}\
    \ with `torch_dtype=torch.float16` due to \"\r\n   2691             \"requirements\
    \ of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \"\r\n   2692 \
    \            \"Pass your own torch_dtype to specify the dtype of the remaining\
    \ non-linear layers or pass\"\r\n   2693             \" torch_dtype=torch.float16\
    \ to remove this warning.\"\r\n   2694         )\r\n\r\nImportError: Using `load_in_8bit=True`\
    \ requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes\
    \ `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes`\
    \ \r\n```\r\n\r\nI tried updating the transformers lib to the latest version but\
    \ then it is giving me, ```KeyError: 'mistral'```\r\nI have been upgrading and\
    \ downgrading the libraries for about 2 hours now."
  created_at: 2023-10-25 14:09:52+00:00
  edited: false
  hidden: false
  id: 65392fc0d1ca3238d0cb30f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-10-25T15:13:20.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8928616046905518
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;ubermenchh&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ubermenchh\"\
          >@<span class=\"underline\">ubermenchh</span></a></span>\n\n\t</span></span>,\
          \ are you working in a notebook? If so, are you restarting the kernel after\
          \ updating?<br>Could you share your current environment, given by the output\
          \ of <code>transformers-cli env</code>? Thank you!</p>\n"
        raw: 'Hello @ubermenchh, are you working in a notebook? If so, are you restarting
          the kernel after updating?

          Could you share your current environment, given by the output of `transformers-cli
          env`? Thank you!'
        updatedAt: '2023-10-25T15:13:20.736Z'
      numEdits: 0
      reactions: []
    id: 65393090fc8dea94ef559398
    type: comment
  author: lysandre
  content: 'Hello @ubermenchh, are you working in a notebook? If so, are you restarting
    the kernel after updating?

    Could you share your current environment, given by the output of `transformers-cli
    env`? Thank you!'
  created_at: 2023-10-25 14:13:20+00:00
  edited: false
  hidden: false
  id: 65393090fc8dea94ef559398
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62b08b71c5eb29748f016027/0xftJgrGvdY2QMrAm-zhF.jpeg?w=200&h=200&f=face
      fullname: ubermenchh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ubermenchh
      type: user
    createdAt: '2023-10-25T15:55:29.000Z'
    data:
      edited: false
      editors:
      - ubermenchh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7007747888565063
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62b08b71c5eb29748f016027/0xftJgrGvdY2QMrAm-zhF.jpeg?w=200&h=200&f=face
          fullname: ubermenchh
          isHf: false
          isPro: false
          name: ubermenchh
          type: user
        html: "<pre><code>/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:138:\
          \ UserWarning: CUDA initialization: The NVIDIA driver on your system is\
          \ too old (found version 11040). Please update your GPU driver by downloading\
          \ and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx\
          \ Alternatively, go to: https://pytorch.org to install a PyTorch version\
          \ that has been compiled with your version of the CUDA driver. (Triggered\
          \ internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n  return torch._C._cuda_getDeviceCount()\
          \ &gt; 0\nTraceback (most recent call last):\n  File \"/opt/conda/bin/transformers-cli\"\
          , line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/commands/transformers_cli.py\"\
          , line 55, in main\n    service.run()\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/commands/env.py\"\
          , line 100, in run\n    tf_cuda_available = tf.test.is_gpu_available()\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py\"\
          , line 371, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py\"\
          , line 1932, in is_gpu_available\n    for local_device in device_lib.list_local_devices():\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/client/device_lib.py\"\
          , line 41, in list_local_devices\n    _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)\n\
          RuntimeError: cudaGetDevice() failed. Status: CUDA driver version is insufficient\
          \ for CUDA runtime version\n</code></pre>\n<p>This is the output.<br>Also,\
          \ i am using kaggle notebooks and i have tried restarting the kernel several\
          \ times.</p>\n"
        raw: "```\n/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:138:\
          \ UserWarning: CUDA initialization: The NVIDIA driver on your system is\
          \ too old (found version 11040). Please update your GPU driver by downloading\
          \ and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx\
          \ Alternatively, go to: https://pytorch.org to install a PyTorch version\
          \ that has been compiled with your version of the CUDA driver. (Triggered\
          \ internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n  return torch._C._cuda_getDeviceCount()\
          \ > 0\nTraceback (most recent call last):\n  File \"/opt/conda/bin/transformers-cli\"\
          , line 8, in <module>\n    sys.exit(main())\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/commands/transformers_cli.py\"\
          , line 55, in main\n    service.run()\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/commands/env.py\"\
          , line 100, in run\n    tf_cuda_available = tf.test.is_gpu_available()\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py\"\
          , line 371, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py\"\
          , line 1932, in is_gpu_available\n    for local_device in device_lib.list_local_devices():\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/client/device_lib.py\"\
          , line 41, in list_local_devices\n    _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)\n\
          RuntimeError: cudaGetDevice() failed. Status: CUDA driver version is insufficient\
          \ for CUDA runtime version\n```\nThis is the output.\nAlso, i am using kaggle\
          \ notebooks and i have tried restarting the kernel several times."
        updatedAt: '2023-10-25T15:55:29.092Z'
      numEdits: 0
      reactions: []
    id: 65393a71b5a5431cee291daf
    type: comment
  author: ubermenchh
  content: "```\n/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:138:\
    \ UserWarning: CUDA initialization: The NVIDIA driver on your system is too old\
    \ (found version 11040). Please update your GPU driver by downloading and installing\
    \ a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively,\
    \ go to: https://pytorch.org to install a PyTorch version that has been compiled\
    \ with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n\
    \  return torch._C._cuda_getDeviceCount() > 0\nTraceback (most recent call last):\n\
    \  File \"/opt/conda/bin/transformers-cli\", line 8, in <module>\n    sys.exit(main())\n\
    \  File \"/opt/conda/lib/python3.10/site-packages/transformers/commands/transformers_cli.py\"\
    , line 55, in main\n    service.run()\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/commands/env.py\"\
    , line 100, in run\n    tf_cuda_available = tf.test.is_gpu_available()\n  File\
    \ \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py\"\
    , line 371, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py\"\
    , line 1932, in is_gpu_available\n    for local_device in device_lib.list_local_devices():\n\
    \  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/client/device_lib.py\"\
    , line 41, in list_local_devices\n    _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)\n\
    RuntimeError: cudaGetDevice() failed. Status: CUDA driver version is insufficient\
    \ for CUDA runtime version\n```\nThis is the output.\nAlso, i am using kaggle\
    \ notebooks and i have tried restarting the kernel several times."
  created_at: 2023-10-25 14:55:29+00:00
  edited: false
  hidden: false
  id: 65393a71b5a5431cee291daf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-10-31T13:38:24.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9321656823158264
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: '<p>I fear this is independent of Mistral or transformers, but linked
          to your setup of CUDA and torch. The error indicates a mismatch between
          your CUDA driver and CUDA runtime version. You can try downgrading to an
          older version of PyTorch to see if it solves your problem in your notebook.</p>

          '
        raw: I fear this is independent of Mistral or transformers, but linked to
          your setup of CUDA and torch. The error indicates a mismatch between your
          CUDA driver and CUDA runtime version. You can try downgrading to an older
          version of PyTorch to see if it solves your problem in your notebook.
        updatedAt: '2023-10-31T13:38:24.766Z'
      numEdits: 0
      reactions: []
    id: 654103501c4b203c013fb7bb
    type: comment
  author: lysandre
  content: I fear this is independent of Mistral or transformers, but linked to your
    setup of CUDA and torch. The error indicates a mismatch between your CUDA driver
    and CUDA runtime version. You can try downgrading to an older version of PyTorch
    to see if it solves your problem in your notebook.
  created_at: 2023-10-31 12:38:24+00:00
  edited: false
  hidden: false
  id: 654103501c4b203c013fb7bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2074fd742b8a12960f86181f2328bec4.svg
      fullname: xiaoping Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xiaoping777
      type: user
    createdAt: '2023-11-13T01:59:21.000Z'
    data:
      edited: false
      editors:
      - xiaoping777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.621755838394165
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2074fd742b8a12960f86181f2328bec4.svg
          fullname: xiaoping Yang
          isHf: false
          isPro: false
          name: xiaoping777
          type: user
        html: '<p>it works with me, using the following code<br>model_8bit = AutoModelForCausalLM.from_pretrained(<br>    model_path,
          load_in_8bit=True, device_map=''auto'',<br>)</p>

          '
        raw: "it works with me, using the following code\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n\
          \    model_path, load_in_8bit=True, device_map='auto',\n)"
        updatedAt: '2023-11-13T01:59:21.508Z'
      numEdits: 0
      reactions: []
    id: 655182f950fb5c9bea9e34b7
    type: comment
  author: xiaoping777
  content: "it works with me, using the following code\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n\
    \    model_path, load_in_8bit=True, device_map='auto',\n)"
  created_at: 2023-11-13 01:59:21+00:00
  edited: false
  hidden: false
  id: 655182f950fb5c9bea9e34b7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 68
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: open
target_branch: null
title: 'ImportError: Using `load_in_8bit=True` requires Accelerate'
