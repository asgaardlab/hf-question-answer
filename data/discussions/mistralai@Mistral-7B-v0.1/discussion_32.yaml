!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NickyNicky
conflicting_files: null
created_at: 2023-10-02 22:02:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-10-02T23:02:39.000Z'
    data:
      edited: true
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5291416049003601
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: "<p>GPU A100 40GB (COLAB)</p>\n<pre><code class=\"language-Python\"\
          >\n!python -c <span class=\"hljs-string\">\"import torch; assert torch.cuda.get_device_capability()[0]\
          \ &gt;= 8, 'Hardware not supported for Flash Attention'\"</span> <span class=\"\
          hljs-comment\"># pass</span>\n!export CUDA_HOME=/usr/local/cuda-<span class=\"\
          hljs-number\">11.8</span>\n<span class=\"hljs-comment\"># !MAX_JOBS=4 pip\
          \ install flash-attn --no-build-isolation</span>\n!MAX_JOBS=<span class=\"\
          hljs-number\">4</span> pip install flash-attn --no-build-isolation  -qqq\n\
          !pip install git+<span class=\"hljs-string\">\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\"\
          </span> -qqq\n!python -m pip install optimum -qqq\n\n<span class=\"hljs-keyword\"\
          >import</span> torch, transformers,torchvision\ntorch.__version__,transformers.__version__,\
          \ torchvision.__version__ <span class=\"hljs-comment\"># ('2.0.1+cu118',\
          \ '4.34.0.dev0', '0.15.2+cu118')</span>\n\nmodel_id = <span class=\"hljs-string\"\
          >'mistralai/Mistral-7B-Instruct-v0.1'</span>\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n\
          \                                             device_map=<span class=\"\
          hljs-string\">\"auto\"</span>,\n                                       \
          \      trust_remote_code=<span class=\"hljs-literal\">True</span>,\n   \
          \                                          torch_dtype=torch.bfloat16,\n\
          \                                             load_in_4bit=<span class=\"\
          hljs-literal\">True</span>,\n                                          \
          \   quantization_config=quantization_config,\n                         \
          \                    use_flash_attention_2=<span class=\"hljs-literal\"\
          >True</span>,\n                                             low_cpu_mem_usage=\
          \ <span class=\"hljs-literal\">True</span>,\n\n                        \
          \                     )\n</code></pre>\n<p>ERROR </p>\n<pre><code class=\"\
          language-Python\">---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          &lt;ipython-<span class=\"hljs-built_in\">input</span>-<span class=\"hljs-number\"\
          >5</span>-0aa549cc42a4&gt; <span class=\"hljs-keyword\">in</span> &lt;cell\
          \ line: <span class=\"hljs-number\">32</span>&gt;()\n     <span class=\"\
          hljs-number\">30</span> <span class=\"hljs-comment\"># from optimum.bettertransformer\
          \ import BetterTransformer #flash attention 2</span>\n     <span class=\"\
          hljs-number\">31</span> \n---&gt; <span class=\"hljs-number\">32</span>\
          \ model = AutoModelForCausalLM.from_pretrained(model_id,\n     <span class=\"\
          hljs-number\">33</span>                                              device_map=<span\
          \ class=\"hljs-string\">\"auto\"</span>,\n     <span class=\"hljs-number\"\
          >34</span>                                              trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>,\n\n<span class=\"hljs-number\">2</span>\
          \ frames\n/usr/local/lib/python3<span class=\"hljs-number\">.10</span>/dist-packages/transformers/modeling_utils.py\
          \ <span class=\"hljs-keyword\">in</span> _check_and_enable_flash_attn_2(cls,\
          \ config, torch_dtype, device_map)\n   <span class=\"hljs-number\">1263</span>\
          \         <span class=\"hljs-string\">\"\"\"</span>\n<span class=\"hljs-string\"\
          >   1264         if not cls._supports_flash_attn_2:</span>\n<span class=\"\
          hljs-string\">-&gt; 1265             raise ValueError(</span>\n<span class=\"\
          hljs-string\">   1266                 \"The current architecture does not\
          \ support Flash Attention 2.0. Please open an issue on GitHub to \"</span>\n\
          <span class=\"hljs-string\">   1267                 \"request support for\
          \ this architecture: https://github.com/huggingface/transformers/issues/new\"\
          </span>\n<span class=\"hljs-string\"></span>\n<span class=\"hljs-string\"\
          >ValueError: The current architecture does not support Flash Attention 2.0.\
          \ Please open an issue on GitHub to request support for this architecture:\
          \ https://github.com/huggingface/transformers/issues/new</span>\n</code></pre>\n"
        raw: "\n\nGPU A100 40GB (COLAB)\n\n```Python\n\n!python -c \"import torch;\
          \ assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported\
          \ for Flash Attention'\" # pass\n!export CUDA_HOME=/usr/local/cuda-11.8\n\
          # !MAX_JOBS=4 pip install flash-attn --no-build-isolation\n!MAX_JOBS=4 pip\
          \ install flash-attn --no-build-isolation  -qqq\n!pip install git+\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\"\
          \ -qqq\n!python -m pip install optimum -qqq\n\nimport torch, transformers,torchvision\n\
          torch.__version__,transformers.__version__, torchvision.__version__ # ('2.0.1+cu118',\
          \ '4.34.0.dev0', '0.15.2+cu118')\n\nmodel_id = 'mistralai/Mistral-7B-Instruct-v0.1'\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n             \
          \                                device_map=\"auto\",\n                \
          \                             trust_remote_code=True,\n                \
          \                             torch_dtype=torch.bfloat16,\n            \
          \                                 load_in_4bit=True,\n                 \
          \                            quantization_config=quantization_config,\n\
          \                                             use_flash_attention_2=True,\n\
          \                                             low_cpu_mem_usage= True,\n\
          \n                                             )\n```\n\n\nERROR \n```Python\n\
          ---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          <ipython-input-5-0aa549cc42a4> in <cell line: 32>()\n     30 # from optimum.bettertransformer\
          \ import BetterTransformer #flash attention 2\n     31 \n---> 32 model =\
          \ AutoModelForCausalLM.from_pretrained(model_id,\n     33              \
          \                                device_map=\"auto\",\n     34         \
          \                                     trust_remote_code=True,\n\n2 frames\n\
          /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py in\
          \ _check_and_enable_flash_attn_2(cls, config, torch_dtype, device_map)\n\
          \   1263         \"\"\"\n   1264         if not cls._supports_flash_attn_2:\n\
          -> 1265             raise ValueError(\n   1266                 \"The current\
          \ architecture does not support Flash Attention 2.0. Please open an issue\
          \ on GitHub to \"\n   1267                 \"request support for this architecture:\
          \ https://github.com/huggingface/transformers/issues/new\"\n\nValueError:\
          \ The current architecture does not support Flash Attention 2.0. Please\
          \ open an issue on GitHub to request support for this architecture: https://github.com/huggingface/transformers/issues/new\n\
          ```"
        updatedAt: '2023-10-02T23:55:36.522Z'
      numEdits: 2
      reactions: []
    id: 651b4c0f79d3b8bfd74f38fe
    type: comment
  author: NickyNicky
  content: "\n\nGPU A100 40GB (COLAB)\n\n```Python\n\n!python -c \"import torch; assert\
    \ torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash\
    \ Attention'\" # pass\n!export CUDA_HOME=/usr/local/cuda-11.8\n# !MAX_JOBS=4 pip\
    \ install flash-attn --no-build-isolation\n!MAX_JOBS=4 pip install flash-attn\
    \ --no-build-isolation  -qqq\n!pip install git+\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\"\
    \ -qqq\n!python -m pip install optimum -qqq\n\nimport torch, transformers,torchvision\n\
    torch.__version__,transformers.__version__, torchvision.__version__ # ('2.0.1+cu118',\
    \ '4.34.0.dev0', '0.15.2+cu118')\n\nmodel_id = 'mistralai/Mistral-7B-Instruct-v0.1'\n\
    \nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n                   \
    \                          device_map=\"auto\",\n                            \
    \                 trust_remote_code=True,\n                                  \
    \           torch_dtype=torch.bfloat16,\n                                    \
    \         load_in_4bit=True,\n                                             quantization_config=quantization_config,\n\
    \                                             use_flash_attention_2=True,\n  \
    \                                           low_cpu_mem_usage= True,\n\n     \
    \                                        )\n```\n\n\nERROR \n```Python\n---------------------------------------------------------------------------\n\
    ValueError                                Traceback (most recent call last)\n\
    <ipython-input-5-0aa549cc42a4> in <cell line: 32>()\n     30 # from optimum.bettertransformer\
    \ import BetterTransformer #flash attention 2\n     31 \n---> 32 model = AutoModelForCausalLM.from_pretrained(model_id,\n\
    \     33                                              device_map=\"auto\",\n \
    \    34                                              trust_remote_code=True,\n\
    \n2 frames\n/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\
    \ in _check_and_enable_flash_attn_2(cls, config, torch_dtype, device_map)\n  \
    \ 1263         \"\"\"\n   1264         if not cls._supports_flash_attn_2:\n->\
    \ 1265             raise ValueError(\n   1266                 \"The current architecture\
    \ does not support Flash Attention 2.0. Please open an issue on GitHub to \"\n\
    \   1267                 \"request support for this architecture: https://github.com/huggingface/transformers/issues/new\"\
    \n\nValueError: The current architecture does not support Flash Attention 2.0.\
    \ Please open an issue on GitHub to request support for this architecture: https://github.com/huggingface/transformers/issues/new\n\
    ```"
  created_at: 2023-10-02 22:02:39+00:00
  edited: true
  hidden: false
  id: 651b4c0f79d3b8bfd74f38fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-10-02T23:05:27.000Z'
    data:
      edited: true
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3571093678474426
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: "<p>Other ERROR BetterTransformer --&gt;&gt; flash attention 2</p>\n\
          <pre><code class=\"language-Python\">!python -c <span class=\"hljs-string\"\
          >\"import torch; assert torch.cuda.get_device_capability()[0] &gt;= 8, 'Hardware\
          \ not supported for Flash Attention'\"</span>\n!export CUDA_HOME=/usr/local/cuda-<span\
          \ class=\"hljs-number\">11.8</span>\n<span class=\"hljs-comment\"># !MAX_JOBS=4\
          \ pip install flash-attn --no-build-isolation</span>\n!MAX_JOBS=<span class=\"\
          hljs-number\">4</span> pip install flash-attn --no-build-isolation  -qqq\n\
          !pip install git+<span class=\"hljs-string\">\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\"\
          </span> -qqq\n!python -m pip install optimum -qqq\n\n\n<span class=\"hljs-keyword\"\
          >from</span> optimum.bettertransformer <span class=\"hljs-keyword\">import</span>\
          \ BetterTransformer <span class=\"hljs-comment\">#flash attention 2</span>\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n             \
          \                                device_map=<span class=\"hljs-string\"\
          >\"auto\"</span>,\n                                             trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>,\n                                \
          \             torch_dtype=torch.bfloat16,\n                            \
          \                 load_in_4bit=<span class=\"hljs-literal\">True</span>,\n\
          \                                             quantization_config=quantization_config,\n\
          \                                             <span class=\"hljs-comment\"\
          ># use_flash_attention_2=True,</span>\n                                \
          \             low_cpu_mem_usage= <span class=\"hljs-literal\">True</span>,\n\
          \                                             )\n\n<span class=\"hljs-comment\"\
          ># model = BetterTransformer.transform(model, keep_original_model=False)\
          \ #flash attention 2</span>\n</code></pre>\n<p>ERROR 2</p>\n<pre><code class=\"\
          language-Python\">---------------------------------------------------------------------------\n\
          NotImplementedError                       Traceback (most recent call last)\n\
          &lt;ipython-<span class=\"hljs-built_in\">input</span>-<span class=\"hljs-number\"\
          >1</span>-a5799a87d18b&gt; <span class=\"hljs-keyword\">in</span> &lt;cell\
          \ line: <span class=\"hljs-number\">43</span>&gt;()\n     <span class=\"\
          hljs-number\">41</span>                                              )\n\
          \     <span class=\"hljs-number\">42</span> \n---&gt; <span class=\"hljs-number\"\
          >43</span> model = BetterTransformer.transform(model, keep_original_model=<span\
          \ class=\"hljs-literal\">False</span>) <span class=\"hljs-comment\">#flash\
          \ attention 2</span>\n     <span class=\"hljs-number\">44</span> \n    \
          \ <span class=\"hljs-number\">45</span> \n\n<span class=\"hljs-number\"\
          >1</span> frames\n/usr/local/lib/python3<span class=\"hljs-number\">.10</span>/dist-packages/optimum/bettertransformer/transformation.py\
          \ <span class=\"hljs-keyword\">in</span> transform(model, keep_original_model,\
          \ max_memory, offload_dir, **kwargs)\n    <span class=\"hljs-number\">226</span>\
          \             )\n    <span class=\"hljs-number\">227</span>         <span\
          \ class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span>\
          \ BetterTransformerManager.supports(model.config.model_type):\n--&gt; <span\
          \ class=\"hljs-number\">228</span>             <span class=\"hljs-keyword\"\
          >raise</span> NotImplementedError(\n    <span class=\"hljs-number\">229</span>\
          \                 <span class=\"hljs-string\">f\"The model type <span class=\"\
          hljs-subst\">{model.config.model_type}</span> is not yet supported to be\
          \ used with BetterTransformer. Feel free\"</span>\n    <span class=\"hljs-number\"\
          >230</span>                 <span class=\"hljs-string\">f\" to open an issue\
          \ at https://github.com/huggingface/optimum/issues if you would like this\
          \ model type to be supported.\"</span>\n\nNotImplementedError: The model\
          \ <span class=\"hljs-built_in\">type</span> mistral <span class=\"hljs-keyword\"\
          >is</span> <span class=\"hljs-keyword\">not</span> yet supported to be used\
          \ <span class=\"hljs-keyword\">with</span> BetterTransformer. Feel free\
          \ to <span class=\"hljs-built_in\">open</span> an issue at https://github.com/huggingface/optimum/issues\
          \ \n<span class=\"hljs-keyword\">if</span> you would like this model <span\
          \ class=\"hljs-built_in\">type</span> to be supported. Currently supported\
          \ models are: \ndict_keys([<span class=\"hljs-string\">'albert'</span>,\
          \ <span class=\"hljs-string\">'bark'</span>, <span class=\"hljs-string\"\
          >'bart'</span>, <span class=\"hljs-string\">'bert'</span>, <span class=\"\
          hljs-string\">'bert-generation'</span>, \n<span class=\"hljs-string\">'blenderbot'</span>,\
          \ <span class=\"hljs-string\">'bloom'</span>, <span class=\"hljs-string\"\
          >'camembert'</span>, <span class=\"hljs-string\">'blip-2'</span>, <span\
          \ class=\"hljs-string\">'clip'</span>, <span class=\"hljs-string\">'codegen'</span>,\
          \ <span class=\"hljs-string\">'data2vec-text'</span>, <span class=\"hljs-string\"\
          >'deit'</span>,\n <span class=\"hljs-string\">'distilbert'</span>, <span\
          \ class=\"hljs-string\">'electra'</span>, <span class=\"hljs-string\">'ernie'</span>,\
          \ <span class=\"hljs-string\">'fsmt'</span>, <span class=\"hljs-string\"\
          >'falcon'</span>, <span class=\"hljs-string\">'gpt2'</span>, <span class=\"\
          hljs-string\">'gpt_bigcode'</span>, <span class=\"hljs-string\">'gptj'</span>,\
          \ <span class=\"hljs-string\">'gpt_neo'</span>, \n<span class=\"hljs-string\"\
          >'gpt_neox'</span>, <span class=\"hljs-string\">'hubert'</span>, <span class=\"\
          hljs-string\">'layoutlm'</span>, <span class=\"hljs-string\">'llama'</span>,\
          \ <span class=\"hljs-string\">'m2m_100'</span>, <span class=\"hljs-string\"\
          >'marian'</span>, <span class=\"hljs-string\">'markuplm'</span>, <span class=\"\
          hljs-string\">'mbart'</span>,\n <span class=\"hljs-string\">'opt'</span>,\
          \ <span class=\"hljs-string\">'pegasus'</span>, <span class=\"hljs-string\"\
          >'rembert'</span>, <span class=\"hljs-string\">'prophetnet'</span>, <span\
          \ class=\"hljs-string\">'roberta'</span>, <span class=\"hljs-string\">'roc_bert'</span>,\
          \ <span class=\"hljs-string\">'roformer'</span>, <span class=\"hljs-string\"\
          >'splinter'</span>, \n<span class=\"hljs-string\">'tapas'</span>, <span\
          \ class=\"hljs-string\">'t5'</span>, <span class=\"hljs-string\">'vilt'</span>,\
          \ <span class=\"hljs-string\">'vit'</span>, <span class=\"hljs-string\"\
          >'vit_mae'</span>, <span class=\"hljs-string\">'vit_msn'</span>, <span class=\"\
          hljs-string\">'wav2vec2'</span>, <span class=\"hljs-string\">'whisper'</span>,\
          \ <span class=\"hljs-string\">'xlm-roberta'</span>, <span class=\"hljs-string\"\
          >'yolos'</span>]).\n</code></pre>\n"
        raw: "Other ERROR BetterTransformer -->> flash attention 2\n\n```Python\n\
          !python -c \"import torch; assert torch.cuda.get_device_capability()[0]\
          \ >= 8, 'Hardware not supported for Flash Attention'\"\n!export CUDA_HOME=/usr/local/cuda-11.8\n\
          # !MAX_JOBS=4 pip install flash-attn --no-build-isolation\n!MAX_JOBS=4 pip\
          \ install flash-attn --no-build-isolation  -qqq\n!pip install git+\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\"\
          \ -qqq\n!python -m pip install optimum -qqq\n\n\nfrom optimum.bettertransformer\
          \ import BetterTransformer #flash attention 2\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n\
          \                                             device_map=\"auto\",\n   \
          \                                          trust_remote_code=True,\n   \
          \                                          torch_dtype=torch.bfloat16,\n\
          \                                             load_in_4bit=True,\n     \
          \                                        quantization_config=quantization_config,\n\
          \                                             # use_flash_attention_2=True,\n\
          \                                             low_cpu_mem_usage= True,\n\
          \                                             )\n\n# model = BetterTransformer.transform(model,\
          \ keep_original_model=False) #flash attention 2\n```\n\nERROR 2\n```Python\n\
          ---------------------------------------------------------------------------\n\
          NotImplementedError                       Traceback (most recent call last)\n\
          <ipython-input-1-a5799a87d18b> in <cell line: 43>()\n     41           \
          \                                   )\n     42 \n---> 43 model = BetterTransformer.transform(model,\
          \ keep_original_model=False) #flash attention 2\n     44 \n     45 \n\n\
          1 frames\n/usr/local/lib/python3.10/dist-packages/optimum/bettertransformer/transformation.py\
          \ in transform(model, keep_original_model, max_memory, offload_dir, **kwargs)\n\
          \    226             )\n    227         if not BetterTransformerManager.supports(model.config.model_type):\n\
          --> 228             raise NotImplementedError(\n    229                \
          \ f\"The model type {model.config.model_type} is not yet supported to be\
          \ used with BetterTransformer. Feel free\"\n    230                 f\"\
          \ to open an issue at https://github.com/huggingface/optimum/issues if you\
          \ would like this model type to be supported.\"\n\nNotImplementedError:\
          \ The model type mistral is not yet supported to be used with BetterTransformer.\
          \ Feel free to open an issue at https://github.com/huggingface/optimum/issues\
          \ \nif you would like this model type to be supported. Currently supported\
          \ models are: \ndict_keys(['albert', 'bark', 'bart', 'bert', 'bert-generation',\
          \ \n'blenderbot', 'bloom', 'camembert', 'blip-2', 'clip', 'codegen', 'data2vec-text',\
          \ 'deit',\n 'distilbert', 'electra', 'ernie', 'fsmt', 'falcon', 'gpt2',\
          \ 'gpt_bigcode', 'gptj', 'gpt_neo', \n'gpt_neox', 'hubert', 'layoutlm',\
          \ 'llama', 'm2m_100', 'marian', 'markuplm', 'mbart',\n 'opt', 'pegasus',\
          \ 'rembert', 'prophetnet', 'roberta', 'roc_bert', 'roformer', 'splinter',\
          \ \n'tapas', 't5', 'vilt', 'vit', 'vit_mae', 'vit_msn', 'wav2vec2', 'whisper',\
          \ 'xlm-roberta', 'yolos']).\n```"
        updatedAt: '2023-10-02T23:13:31.605Z'
      numEdits: 2
      reactions: []
    id: 651b4cb71df105269bd3064a
    type: comment
  author: NickyNicky
  content: "Other ERROR BetterTransformer -->> flash attention 2\n\n```Python\n!python\
    \ -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware\
    \ not supported for Flash Attention'\"\n!export CUDA_HOME=/usr/local/cuda-11.8\n\
    # !MAX_JOBS=4 pip install flash-attn --no-build-isolation\n!MAX_JOBS=4 pip install\
    \ flash-attn --no-build-isolation  -qqq\n!pip install git+\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\"\
    \ -qqq\n!python -m pip install optimum -qqq\n\n\nfrom optimum.bettertransformer\
    \ import BetterTransformer #flash attention 2\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n\
    \                                             device_map=\"auto\",\n         \
    \                                    trust_remote_code=True,\n               \
    \                              torch_dtype=torch.bfloat16,\n                 \
    \                            load_in_4bit=True,\n                            \
    \                 quantization_config=quantization_config,\n                 \
    \                            # use_flash_attention_2=True,\n                 \
    \                            low_cpu_mem_usage= True,\n                      \
    \                       )\n\n# model = BetterTransformer.transform(model, keep_original_model=False)\
    \ #flash attention 2\n```\n\nERROR 2\n```Python\n---------------------------------------------------------------------------\n\
    NotImplementedError                       Traceback (most recent call last)\n\
    <ipython-input-1-a5799a87d18b> in <cell line: 43>()\n     41                 \
    \                             )\n     42 \n---> 43 model = BetterTransformer.transform(model,\
    \ keep_original_model=False) #flash attention 2\n     44 \n     45 \n\n1 frames\n\
    /usr/local/lib/python3.10/dist-packages/optimum/bettertransformer/transformation.py\
    \ in transform(model, keep_original_model, max_memory, offload_dir, **kwargs)\n\
    \    226             )\n    227         if not BetterTransformerManager.supports(model.config.model_type):\n\
    --> 228             raise NotImplementedError(\n    229                 f\"The\
    \ model type {model.config.model_type} is not yet supported to be used with BetterTransformer.\
    \ Feel free\"\n    230                 f\" to open an issue at https://github.com/huggingface/optimum/issues\
    \ if you would like this model type to be supported.\"\n\nNotImplementedError:\
    \ The model type mistral is not yet supported to be used with BetterTransformer.\
    \ Feel free to open an issue at https://github.com/huggingface/optimum/issues\
    \ \nif you would like this model type to be supported. Currently supported models\
    \ are: \ndict_keys(['albert', 'bark', 'bart', 'bert', 'bert-generation', \n'blenderbot',\
    \ 'bloom', 'camembert', 'blip-2', 'clip', 'codegen', 'data2vec-text', 'deit',\n\
    \ 'distilbert', 'electra', 'ernie', 'fsmt', 'falcon', 'gpt2', 'gpt_bigcode', 'gptj',\
    \ 'gpt_neo', \n'gpt_neox', 'hubert', 'layoutlm', 'llama', 'm2m_100', 'marian',\
    \ 'markuplm', 'mbart',\n 'opt', 'pegasus', 'rembert', 'prophetnet', 'roberta',\
    \ 'roc_bert', 'roformer', 'splinter', \n'tapas', 't5', 'vilt', 'vit', 'vit_mae',\
    \ 'vit_msn', 'wav2vec2', 'whisper', 'xlm-roberta', 'yolos']).\n```"
  created_at: 2023-10-02 22:05:27+00:00
  edited: true
  hidden: false
  id: 651b4cb71df105269bd3064a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f0684919c3047c59099a12ba17fa2df4.svg
      fullname: Gaurav Sinha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgauravm
      type: user
    createdAt: '2023-10-12T13:49:14.000Z'
    data:
      edited: false
      editors:
      - sgauravm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9802606105804443
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f0684919c3047c59099a12ba17fa2df4.svg
          fullname: Gaurav Sinha
          isHf: false
          isPro: false
          name: sgauravm
          type: user
        html: '<p>Getting the same error. In the supported models they say that Mistral
          is included but don''t know why is it giving this error.</p>

          '
        raw: Getting the same error. In the supported models they say that Mistral
          is included but don't know why is it giving this error.
        updatedAt: '2023-10-12T13:49:14.922Z'
      numEdits: 0
      reactions: []
    id: 6527f95a1055393728b3fd18
    type: comment
  author: sgauravm
  content: Getting the same error. In the supported models they say that Mistral is
    included but don't know why is it giving this error.
  created_at: 2023-10-12 12:49:14+00:00
  edited: false
  hidden: false
  id: 6527f95a1055393728b3fd18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-10-12T14:14:10.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: tr
        probability: 0.10496710985898972
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'cc @ybelkada '
        updatedAt: '2023-10-12T14:14:10.075Z'
      numEdits: 0
      reactions: []
    id: 6527ff32a4c1d9d0aee89908
    type: comment
  author: lysandre
  content: 'cc @ybelkada '
  created_at: 2023-10-12 13:14:10+00:00
  edited: false
  hidden: false
  id: 6527ff32a4c1d9d0aee89908
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-12T16:05:02.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6179283261299133
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;NickyNicky&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/NickyNicky\"\
          >@<span class=\"underline\">NickyNicky</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;sgauravm&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sgauravm\">@<span class=\"\
          underline\">sgauravm</span></a></span>\n\n\t</span></span> </p>\n<p>If you\
          \ install the latest version of transformers</p>\n<pre><code class=\"language-bash\"\
          >pip install -U transformers\n</code></pre>\n<p>Flash Attention-2 should\
          \ be supported</p>\n<p>Check out this specific section of the docs: <a href=\"\
          https://huggingface.co/docs/transformers/model_doc/mistral#combining-mistral-and-flash-attention-2\"\
          >https://huggingface.co/docs/transformers/model_doc/mistral#combining-mistral-and-flash-attention-2</a>\
          \ for more details</p>\n"
        raw: "Hi @NickyNicky and @sgauravm \n\nIf you install the latest version of\
          \ transformers\n\n```bash\npip install -U transformers\n```\n\nFlash Attention-2\
          \ should be supported\n\nCheck out this specific section of the docs: https://huggingface.co/docs/transformers/model_doc/mistral#combining-mistral-and-flash-attention-2\
          \ for more details"
        updatedAt: '2023-10-12T16:06:38.125Z'
      numEdits: 3
      reactions: []
    id: 6528192ed38c87e7fb472049
    type: comment
  author: ybelkada
  content: "Hi @NickyNicky and @sgauravm \n\nIf you install the latest version of\
    \ transformers\n\n```bash\npip install -U transformers\n```\n\nFlash Attention-2\
    \ should be supported\n\nCheck out this specific section of the docs: https://huggingface.co/docs/transformers/model_doc/mistral#combining-mistral-and-flash-attention-2\
    \ for more details"
  created_at: 2023-10-12 15:05:02+00:00
  edited: true
  hidden: false
  id: 6528192ed38c87e7fb472049
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-10-12T18:39:54.000Z'
    data:
      edited: false
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9280652403831482
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: '<p>Thank you very much,<br>I have the latest version of transformers.</p>

          '
        raw: "Thank you very much, \nI have the latest version of transformers."
        updatedAt: '2023-10-12T18:39:54.781Z'
      numEdits: 0
      reactions: []
    id: 65283d7a22296ad012804032
    type: comment
  author: NickyNicky
  content: "Thank you very much, \nI have the latest version of transformers."
  created_at: 2023-10-12 17:39:54+00:00
  edited: false
  hidden: false
  id: 65283d7a22296ad012804032
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-12T21:10:00.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4698079824447632
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;NickyNicky&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/NickyNicky\">@<span class=\"\
          underline\">NickyNicky</span></a></span>\n\n\t</span></span><br>Your first\
          \ script</p>\n<pre><code class=\"language-python\">!python -c <span class=\"\
          hljs-string\">\"import torch; assert torch.cuda.get_device_capability()[0]\
          \ &gt;= 8, 'Hardware not supported for Flash Attention'\"</span> <span class=\"\
          hljs-comment\"># pass</span>\n!export CUDA_HOME=/usr/local/cuda-<span class=\"\
          hljs-number\">11.8</span>\n<span class=\"hljs-comment\"># !MAX_JOBS=4 pip\
          \ install flash-attn --no-build-isolation</span>\n!MAX_JOBS=<span class=\"\
          hljs-number\">4</span> pip install flash-attn --no-build-isolation  -qqq\n\
          !pip install git+<span class=\"hljs-string\">\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\"\
          </span> -qqq\n!python -m pip install optimum -qqq\n\n<span class=\"hljs-keyword\"\
          >import</span> torch, transformers,torchvision\ntorch.__version__,transformers.__version__,\
          \ torchvision.__version__ <span class=\"hljs-comment\"># ('2.0.1+cu118',\
          \ '4.34.0.dev0', '0.15.2+cu118')</span>\n\nmodel_id = <span class=\"hljs-string\"\
          >'mistralai/Mistral-7B-Instruct-v0.1'</span>\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n\
          \                                             device_map=<span class=\"\
          hljs-string\">\"auto\"</span>,\n                                       \
          \      trust_remote_code=<span class=\"hljs-literal\">True</span>,\n   \
          \                                          torch_dtype=torch.bfloat16,\n\
          \                                             load_in_4bit=<span class=\"\
          hljs-literal\">True</span>,\n                                          \
          \   quantization_config=quantization_config,\n                         \
          \                    use_flash_attention_2=<span class=\"hljs-literal\"\
          >True</span>,\n                                             low_cpu_mem_usage=\
          \ <span class=\"hljs-literal\">True</span>,\n\n                        \
          \                     )\n</code></pre>\n<p>Should work if you have latest\
          \ transformers installed, however Mistral is not in <code>BetterTransformer</code>\
          \ yet, we will add the support of F.SDPA natively in transformers core soon</p>\n"
        raw: "@NickyNicky \nYour first script\n\n```python\n!python -c \"import torch;\
          \ assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported\
          \ for Flash Attention'\" # pass\n!export CUDA_HOME=/usr/local/cuda-11.8\n\
          # !MAX_JOBS=4 pip install flash-attn --no-build-isolation\n!MAX_JOBS=4 pip\
          \ install flash-attn --no-build-isolation  -qqq\n!pip install git+\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\"\
          \ -qqq\n!python -m pip install optimum -qqq\n\nimport torch, transformers,torchvision\n\
          torch.__version__,transformers.__version__, torchvision.__version__ # ('2.0.1+cu118',\
          \ '4.34.0.dev0', '0.15.2+cu118')\n\nmodel_id = 'mistralai/Mistral-7B-Instruct-v0.1'\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n             \
          \                                device_map=\"auto\",\n                \
          \                             trust_remote_code=True,\n                \
          \                             torch_dtype=torch.bfloat16,\n            \
          \                                 load_in_4bit=True,\n                 \
          \                            quantization_config=quantization_config,\n\
          \                                             use_flash_attention_2=True,\n\
          \                                             low_cpu_mem_usage= True,\n\
          \n                                             )\n```\n\nShould work if\
          \ you have latest transformers installed, however Mistral is not in `BetterTransformer`\
          \ yet, we will add the support of F.SDPA natively in transformers core soon"
        updatedAt: '2023-10-12T21:10:00.136Z'
      numEdits: 0
      reactions: []
    id: 652860a8147d9119a62da891
    type: comment
  author: ybelkada
  content: "@NickyNicky \nYour first script\n\n```python\n!python -c \"import torch;\
    \ assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for\
    \ Flash Attention'\" # pass\n!export CUDA_HOME=/usr/local/cuda-11.8\n# !MAX_JOBS=4\
    \ pip install flash-attn --no-build-isolation\n!MAX_JOBS=4 pip install flash-attn\
    \ --no-build-isolation  -qqq\n!pip install git+\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\"\
    \ -qqq\n!python -m pip install optimum -qqq\n\nimport torch, transformers,torchvision\n\
    torch.__version__,transformers.__version__, torchvision.__version__ # ('2.0.1+cu118',\
    \ '4.34.0.dev0', '0.15.2+cu118')\n\nmodel_id = 'mistralai/Mistral-7B-Instruct-v0.1'\n\
    \nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n                   \
    \                          device_map=\"auto\",\n                            \
    \                 trust_remote_code=True,\n                                  \
    \           torch_dtype=torch.bfloat16,\n                                    \
    \         load_in_4bit=True,\n                                             quantization_config=quantization_config,\n\
    \                                             use_flash_attention_2=True,\n  \
    \                                           low_cpu_mem_usage= True,\n\n     \
    \                                        )\n```\n\nShould work if you have latest\
    \ transformers installed, however Mistral is not in `BetterTransformer` yet, we\
    \ will add the support of F.SDPA natively in transformers core soon"
  created_at: 2023-10-12 20:10:00+00:00
  edited: false
  hidden: false
  id: 652860a8147d9119a62da891
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-10-12T22:51:21.000Z'
    data:
      edited: false
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5820100903511047
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: '<p>With these versions it works.</p>

          <pre><code class="language-Python"><span class="hljs-keyword">import</span>
          torch, transformers

          torch.__version__,transformers.__version__

          (<span class="hljs-string">''2.0.1+cu118''</span>, <span class="hljs-string">''4.34.0''</span>)

          </code></pre>

          '
        raw: 'With these versions it works.

          ```Python

          import torch, transformers

          torch.__version__,transformers.__version__

          (''2.0.1+cu118'', ''4.34.0'')

          ```'
        updatedAt: '2023-10-12T22:51:21.572Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
      relatedEventId: 6528786922296ad0128a99cb
    id: 6528786922296ad0128a99c2
    type: comment
  author: NickyNicky
  content: 'With these versions it works.

    ```Python

    import torch, transformers

    torch.__version__,transformers.__version__

    (''2.0.1+cu118'', ''4.34.0'')

    ```'
  created_at: 2023-10-12 21:51:21+00:00
  edited: false
  hidden: false
  id: 6528786922296ad0128a99c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-10-12T22:51:21.000Z'
    data:
      status: closed
    id: 6528786922296ad0128a99cb
    type: status-change
  author: NickyNicky
  created_at: 2023-10-12 21:51:21+00:00
  id: 6528786922296ad0128a99cb
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 32
repo_id: mistralai/Mistral-7B-v0.1
repo_type: model
status: closed
target_branch: null
title: flash attention 2
