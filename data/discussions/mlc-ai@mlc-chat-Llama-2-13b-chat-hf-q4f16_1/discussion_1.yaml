!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BlueNipples
conflicting_files: null
created_at: 2023-09-12 02:32:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bb1109aaccfd28b023bcec/fumfSHv9pnW1rMvgQeibP.png?w=200&h=200&f=face
      fullname: Matthew Andrews
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BlueNipples
      type: user
    createdAt: '2023-09-12T03:32:13.000Z'
    data:
      edited: true
      editors:
      - BlueNipples
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9368700981140137
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bb1109aaccfd28b023bcec/fumfSHv9pnW1rMvgQeibP.png?w=200&h=200&f=face
          fullname: Matthew Andrews
          isHf: false
          isPro: false
          name: BlueNipples
          type: user
        html: '<p>I have been having some problems getting mlc llm to run due to 8gb
          of ram giving run out of ram errors on PC. Wondering if you could create
          q0f16 (vulkan) versions of things like:<br>zarakiquemparte/zararp-l2-7b<br>Doctor-Shotgun/Chronohermes-Grad-L2-13b<br>Fredithefish/Guanaco-3B-Uncensored-v2</p>

          <p>For people that don''t have much ram. I have not been able to run the
          conversion myself on collab, perhaps lacking in technical know how. Even
          the 3b model gave me a ram error, which perplexed me as I can run 7b using
          koboldcpp ggml/gguf albiet it slowly, with ram to spare. And I also got
          errors on android and the web app (different errors obviously). This might
          all be ram? IDK, for whatever reason this install/run process is proving
          trickier than oogabooga or koboldcpp.</p>

          <p>Roleplay models are popular, and q0f16 is supposed to be up to 4.5x faster
          than fp16 with only quite a small loss in accuracy, and use considerably
          less ram.  At least according to bing lol (whatever that''s worth). But
          if true, it seems like a no brainer, that''s way better than most quants.
          Just a request/thought?</p>

          '
        raw: 'I have been having some problems getting mlc llm to run due to 8gb of
          ram giving run out of ram errors on PC. Wondering if you could create q0f16
          (vulkan) versions of things like:

          zarakiquemparte/zararp-l2-7b

          Doctor-Shotgun/Chronohermes-Grad-L2-13b

          Fredithefish/Guanaco-3B-Uncensored-v2


          For people that don''t have much ram. I have not been able to run the conversion
          myself on collab, perhaps lacking in technical know how. Even the 3b model
          gave me a ram error, which perplexed me as I can run 7b using koboldcpp
          ggml/gguf albiet it slowly, with ram to spare. And I also got errors on
          android and the web app (different errors obviously). This might all be
          ram? IDK, for whatever reason this install/run process is proving trickier
          than oogabooga or koboldcpp.


          Roleplay models are popular, and q0f16 is supposed to be up to 4.5x faster
          than fp16 with only quite a small loss in accuracy, and use considerably
          less ram.  At least according to bing lol (whatever that''s worth). But
          if true, it seems like a no brainer, that''s way better than most quants.
          Just a request/thought?'
        updatedAt: '2023-09-12T03:34:13.006Z'
      numEdits: 1
      reactions: []
    id: 64ffdbbd69219ce3e48ad374
    type: comment
  author: BlueNipples
  content: 'I have been having some problems getting mlc llm to run due to 8gb of
    ram giving run out of ram errors on PC. Wondering if you could create q0f16 (vulkan)
    versions of things like:

    zarakiquemparte/zararp-l2-7b

    Doctor-Shotgun/Chronohermes-Grad-L2-13b

    Fredithefish/Guanaco-3B-Uncensored-v2


    For people that don''t have much ram. I have not been able to run the conversion
    myself on collab, perhaps lacking in technical know how. Even the 3b model gave
    me a ram error, which perplexed me as I can run 7b using koboldcpp ggml/gguf albiet
    it slowly, with ram to spare. And I also got errors on android and the web app
    (different errors obviously). This might all be ram? IDK, for whatever reason
    this install/run process is proving trickier than oogabooga or koboldcpp.


    Roleplay models are popular, and q0f16 is supposed to be up to 4.5x faster than
    fp16 with only quite a small loss in accuracy, and use considerably less ram.  At
    least according to bing lol (whatever that''s worth). But if true, it seems like
    a no brainer, that''s way better than most quants. Just a request/thought?'
  created_at: 2023-09-12 02:32:13+00:00
  edited: true
  hidden: false
  id: 64ffdbbd69219ce3e48ad374
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mlc-ai/mlc-chat-Llama-2-13b-chat-hf-q4f16_1
repo_type: model
status: open
target_branch: null
title: Request
