!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dennlinger
conflicting_files: null
created_at: 2022-08-03 11:06:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585728817057-noauth.jpeg?w=200&h=200&f=face
      fullname: Dennis Aumiller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dennlinger
      type: user
    createdAt: '2022-08-03T12:06:42.000Z'
    data:
      edited: true
      editors:
      - dennlinger
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585728817057-noauth.jpeg?w=200&h=200&f=face
          fullname: Dennis Aumiller
          isHf: false
          isPro: false
          name: dennlinger
          type: user
        html: '<p>Hi,<br>thanks first of all for contributing the model weights! One
          question that I have is whether it would be generally possible to separate
          the weights for different backends to make it faster to load the model (e.g.,
          only download PyTorch-specific weights, etc.).</p>

          <p>One way to do this would be to utilize the revisions provided through
          the hub, as it is for example done in GPT-J 6B (the <a href="https://huggingface.co/EleutherAI/gpt-j-6B/tree/float16">float16
          version</a> has its own branch there). Of course this is only a sensible
          suggestion if it can be somehow ensured that the correct branch would be
          automatically chosen for all backends...</p>

          <p>For reference, this is the case for people using this model in a distributed
          manner through accelerate, where one has to clone the full repository before
          being able to use the model. It might be that only the relevant files are
          downloaded when using the <code>.from_pretrained()</code> option through
          transformers, but I am unsure whether this is the primary use case for large-scale
          models like this. </p>

          <p>Best,<br>Dennis</p>

          <p>Edit: FWIW, the repository in its entirety takes up about 330GB when
          cloning it locally. Relevant files make up less than 1/6 of it (~50GB),
          plus overhead for the <code>.git/</code> folder with another 30GB or so.</p>

          '
        raw: "Hi,\nthanks first of all for contributing the model weights! One question\
          \ that I have is whether it would be generally possible to separate the\
          \ weights for different backends to make it faster to load the model (e.g.,\
          \ only download PyTorch-specific weights, etc.).\n\nOne way to do this would\
          \ be to utilize the revisions provided through the hub, as it is for example\
          \ done in GPT-J 6B (the [float16 version](https://huggingface.co/EleutherAI/gpt-j-6B/tree/float16)\
          \ has its own branch there). Of course this is only a sensible suggestion\
          \ if it can be somehow ensured that the correct branch would be automatically\
          \ chosen for all backends...\n\nFor reference, this is the case for people\
          \ using this model in a distributed manner through accelerate, where one\
          \ has to clone the full repository before being able to use the model. It\
          \ might be that only the relevant files are downloaded when using the `.from_pretrained()`\
          \ option through transformers, but I am unsure whether this is the primary\
          \ use case for large-scale models like this. \n\nBest,\nDennis\n\nEdit:\
          \ FWIW, the repository in its entirety takes up about 330GB when cloning\
          \ it locally. Relevant files make up less than 1/6 of it (~50GB), plus overhead\
          \ for the `.git/` folder with another 30GB or so."
        updatedAt: '2022-08-03T12:58:41.638Z'
      numEdits: 2
      reactions: []
    id: 62ea64d2d886cd7d62962b3f
    type: comment
  author: dennlinger
  content: "Hi,\nthanks first of all for contributing the model weights! One question\
    \ that I have is whether it would be generally possible to separate the weights\
    \ for different backends to make it faster to load the model (e.g., only download\
    \ PyTorch-specific weights, etc.).\n\nOne way to do this would be to utilize the\
    \ revisions provided through the hub, as it is for example done in GPT-J 6B (the\
    \ [float16 version](https://huggingface.co/EleutherAI/gpt-j-6B/tree/float16) has\
    \ its own branch there). Of course this is only a sensible suggestion if it can\
    \ be somehow ensured that the correct branch would be automatically chosen for\
    \ all backends...\n\nFor reference, this is the case for people using this model\
    \ in a distributed manner through accelerate, where one has to clone the full\
    \ repository before being able to use the model. It might be that only the relevant\
    \ files are downloaded when using the `.from_pretrained()` option through transformers,\
    \ but I am unsure whether this is the primary use case for large-scale models\
    \ like this. \n\nBest,\nDennis\n\nEdit: FWIW, the repository in its entirety takes\
    \ up about 330GB when cloning it locally. Relevant files make up less than 1/6\
    \ of it (~50GB), plus overhead for the `.git/` folder with another 30GB or so."
  created_at: 2022-08-03 11:06:42+00:00
  edited: true
  hidden: false
  id: 62ea64d2d886cd7d62962b3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585728817057-noauth.jpeg?w=200&h=200&f=face
      fullname: Dennis Aumiller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dennlinger
      type: user
    createdAt: '2022-08-03T12:06:50.000Z'
    data:
      from: Possibility to download backend-specific weights
      to: Possibility to download backend-specific weights only
    id: 62ea64da3f988f4be2de52e8
    type: title-change
  author: dennlinger
  created_at: 2022-08-03 11:06:50+00:00
  id: 62ea64da3f988f4be2de52e8
  new_title: Possibility to download backend-specific weights only
  old_title: Possibility to download backend-specific weights
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: facebook/opt-30b
repo_type: model
status: open
target_branch: null
title: Possibility to download backend-specific weights only
