!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dhansmair
conflicting_files: null
created_at: 2022-07-11 16:25:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662381471958-628c808edc59216b22678076.jpeg?w=200&h=200&f=face
      fullname: David Hansmair
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dhansmair
      type: user
    createdAt: '2022-07-11T17:25:31.000Z'
    data:
      edited: false
      editors:
      - dhansmair
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662381471958-628c808edc59216b22678076.jpeg?w=200&h=200&f=face
          fullname: David Hansmair
          isHf: false
          isPro: false
          name: dhansmair
          type: user
        html: '<p>Hello there!<br>I''m not sure if this is the right place to ask
          this. On the machine I am working on I can''t use AutoTokenizer.from_pretrained(),
          because it gives an Exception: "error: no locks available". This is a local
          problem on my side, but I am wondering if there is an alternative way to
          instantiate the tokenizer, like an OPTTokenizer class. For example, for
          GPT-2 there is a GPT2Tokenizer which works on the machine.<br>Thanks a lot
          for your help and for providing this awesome LM!<br>Best, David</p>

          '
        raw: "Hello there!\r\nI'm not sure if this is the right place to ask this.\
          \ On the machine I am working on I can't use AutoTokenizer.from_pretrained(),\
          \ because it gives an Exception: \"error: no locks available\". This is\
          \ a local problem on my side, but I am wondering if there is an alternative\
          \ way to instantiate the tokenizer, like an OPTTokenizer class. For example,\
          \ for GPT-2 there is a GPT2Tokenizer which works on the machine.\r\nThanks\
          \ a lot for your help and for providing this awesome LM!\r\nBest, David"
        updatedAt: '2022-07-11T17:25:31.693Z'
      numEdits: 0
      reactions: []
    id: 62cc5d0b376917c0223c5e42
    type: comment
  author: dhansmair
  content: "Hello there!\r\nI'm not sure if this is the right place to ask this. On\
    \ the machine I am working on I can't use AutoTokenizer.from_pretrained(), because\
    \ it gives an Exception: \"error: no locks available\". This is a local problem\
    \ on my side, but I am wondering if there is an alternative way to instantiate\
    \ the tokenizer, like an OPTTokenizer class. For example, for GPT-2 there is a\
    \ GPT2Tokenizer which works on the machine.\r\nThanks a lot for your help and\
    \ for providing this awesome LM!\r\nBest, David"
  created_at: 2022-07-11 16:25:31+00:00
  edited: false
  hidden: false
  id: 62cc5d0b376917c0223c5e42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662381471958-628c808edc59216b22678076.jpeg?w=200&h=200&f=face
      fullname: David Hansmair
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dhansmair
      type: user
    createdAt: '2022-07-12T10:09:02.000Z'
    data:
      edited: false
      editors:
      - dhansmair
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662381471958-628c808edc59216b22678076.jpeg?w=200&h=200&f=face
          fullname: David Hansmair
          isHf: false
          isPro: false
          name: dhansmair
          type: user
        html: '<p>I figured out it''s as simple as using <code>GPT2Tokenizer.from_pretrained("facebook/opt-30b")</code></p>

          '
        raw: I figured out it's as simple as using `GPT2Tokenizer.from_pretrained("facebook/opt-30b")`
        updatedAt: '2022-07-12T10:09:02.490Z'
      numEdits: 0
      reactions: []
    id: 62cd483e674cdb52444fb1ea
    type: comment
  author: dhansmair
  content: I figured out it's as simple as using `GPT2Tokenizer.from_pretrained("facebook/opt-30b")`
  created_at: 2022-07-12 09:09:02+00:00
  edited: false
  hidden: false
  id: 62cd483e674cdb52444fb1ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2022-07-13T06:50:34.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;dhansmair&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/dhansmair\"\
          >@<span class=\"underline\">dhansmair</span></a></span>\n\n\t</span></span>,\
          \ indeed, the OPT checkpoints leverage the <code>GPT2Tokenizer</code> so\
          \ loading it as in your last comment should work!</p>\n<p>If you'd like\
          \ help regarding the first issue (error: no locks available), please do\
          \ open an issue on the transformers repository and we'll be happy to investigate\
          \ with you.</p>\n"
        raw: 'Hey @dhansmair, indeed, the OPT checkpoints leverage the `GPT2Tokenizer`
          so loading it as in your last comment should work!


          If you''d like help regarding the first issue (error: no locks available),
          please do open an issue on the transformers repository and we''ll be happy
          to investigate with you.'
        updatedAt: '2022-07-13T06:50:34.259Z'
      numEdits: 0
      reactions: []
    id: 62ce6b3a85cfd21c04c7d2f5
    type: comment
  author: lysandre
  content: 'Hey @dhansmair, indeed, the OPT checkpoints leverage the `GPT2Tokenizer`
    so loading it as in your last comment should work!


    If you''d like help regarding the first issue (error: no locks available), please
    do open an issue on the transformers repository and we''ll be happy to investigate
    with you.'
  created_at: 2022-07-13 05:50:34+00:00
  edited: false
  hidden: false
  id: 62ce6b3a85cfd21c04c7d2f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2022-07-13T06:50:34.000Z'
    data:
      status: closed
    id: 62ce6b3a85cfd21c04c7d2f6
    type: status-change
  author: lysandre
  created_at: 2022-07-13 05:50:34+00:00
  id: 62ce6b3a85cfd21c04c7d2f6
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: facebook/opt-30b
repo_type: model
status: closed
target_branch: null
title: create OPTTokenizer without AutoTokenizer.from_pretrained
