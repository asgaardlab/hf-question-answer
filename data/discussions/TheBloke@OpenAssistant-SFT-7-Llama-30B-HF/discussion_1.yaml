!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TaoBlnt
conflicting_files: null
created_at: 2023-06-06 07:27:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/598b56e3dfc56293c59886fc83379cc2.svg
      fullname: Tao Blancheton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TaoBlnt
      type: user
    createdAt: '2023-06-06T08:27:19.000Z'
    data:
      edited: false
      editors:
      - TaoBlnt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4564753472805023
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/598b56e3dfc56293c59886fc83379cc2.svg
          fullname: Tao Blancheton
          isHf: false
          isPro: false
          name: TaoBlnt
          type: user
        html: "<p>Hi, I've got an error while loading the tokenizer :/</p>\n<pre><code>Traceback\
          \ (most recent call last):\n  File \"/home/ubuntu/llm_api.py\", line 14,\
          \ in &lt;module&gt;\n    tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n\
          \  File \"/usr/local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 693, in from_pretrained\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1812, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2021, in _from_pretrained\n    if has_tokenizer_file and index !=\
          \ current_index and tokenizer.convert_tokens_to_ids(token) != index:\n \
          \ File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 250, in convert_tokens_to_ids\n    return self._convert_token_to_id_with_added_voc(tokens)\n\
          \  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 260, in _convert_token_to_id_with_added_voc\n    return self.unk_token_id\n\
          \  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1142, in unk_token_id\n    return self.convert_tokens_to_ids(self.unk_token)\n\
          \  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 250, in convert_tokens_to_ids\n    return self._convert_token_to_id_with_added_voc(tokens)\n\
          \  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 260, in _convert_token_to_id_with_added_voc\n    return self.unk_token_id\n\
          \  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1142, in unk_token_id\n    return self.convert_tokens_to_ids(self.unk_token)\n\
          \n...\n\n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 250, in convert_tokens_to_ids\n    return self._convert_token_to_id_with_added_voc(tokens)\n\
          \  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 260, in _convert_token_to_id_with_added_voc\n    return self.unk_token_id\n\
          \  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1142, in unk_token_id\n    return self.convert_tokens_to_ids(self.unk_token)\n\
          RecursionError: maximum recursion depth exceeded\n</code></pre>\n<p>Any\
          \ idea how to solve this problem? (Sorry by advance if my question is dumb)</p>\n\
          <p>Thanks :)</p>\n"
        raw: "Hi, I've got an error while loading the tokenizer :/\r\n\r\n```\r\n\
          Traceback (most recent call last):\r\n  File \"/home/ubuntu/llm_api.py\"\
          , line 14, in <module>\r\n    tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\r\
          \n  File \"/usr/local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 693, in from_pretrained\r\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1812, in from_pretrained\r\n    return cls._from_pretrained(\r\n\
          \  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2021, in _from_pretrained\r\n    if has_tokenizer_file and index\
          \ != current_index and tokenizer.convert_tokens_to_ids(token) != index:\r\
          \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\
          \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 260, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\
          \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1142, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\
          \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\
          \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 260, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\
          \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1142, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\
          \n\r\n...\r\n\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\
          \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 260, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\
          \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1142, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\
          \nRecursionError: maximum recursion depth exceeded\r\n```\r\n\r\nAny idea\
          \ how to solve this problem? (Sorry by advance if my question is dumb)\r\
          \n\r\nThanks :)"
        updatedAt: '2023-06-06T08:27:19.973Z'
      numEdits: 0
      reactions: []
    id: 647eede735bc6d6aa5f8eba4
    type: comment
  author: TaoBlnt
  content: "Hi, I've got an error while loading the tokenizer :/\r\n\r\n```\r\nTraceback\
    \ (most recent call last):\r\n  File \"/home/ubuntu/llm_api.py\", line 14, in\
    \ <module>\r\n    tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\r\n\
    \  File \"/usr/local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 693, in from_pretrained\r\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1812, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File\
    \ \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 2021, in _from_pretrained\r\n    if has_tokenizer_file and index != current_index\
    \ and tokenizer.convert_tokens_to_ids(token) != index:\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
    , line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\
    \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
    , line 260, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\
    \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1142, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\
    \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
    , line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\
    \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
    , line 260, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\
    \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1142, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\
    \n\r\n...\r\n\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
    , line 250, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\
    \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\"\
    , line 260, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\
    \n  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1142, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\
    \nRecursionError: maximum recursion depth exceeded\r\n```\r\n\r\nAny idea how\
    \ to solve this problem? (Sorry by advance if my question is dumb)\r\n\r\nThanks\
    \ :)"
  created_at: 2023-06-06 07:27:19+00:00
  edited: false
  hidden: false
  id: 647eede735bc6d6aa5f8eba4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-06T08:40:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9769287705421448
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Not a dumb question at all. I think something has changed in transformers
          recently that is breaking on certain tokenizers that are missing data they
          should have.</p>

          <p>I have tried re-generating the tokenizer. Can you please re-download
          the json files in the repo and test and let me know how it goes now.</p>

          '
        raw: 'Not a dumb question at all. I think something has changed in transformers
          recently that is breaking on certain tokenizers that are missing data they
          should have.


          I have tried re-generating the tokenizer. Can you please re-download the
          json files in the repo and test and let me know how it goes now.'
        updatedAt: '2023-06-06T08:40:20.213Z'
      numEdits: 0
      reactions: []
    id: 647ef0f42a7bcaa30798c092
    type: comment
  author: TheBloke
  content: 'Not a dumb question at all. I think something has changed in transformers
    recently that is breaking on certain tokenizers that are missing data they should
    have.


    I have tried re-generating the tokenizer. Can you please re-download the json
    files in the repo and test and let me know how it goes now.'
  created_at: 2023-06-06 07:40:20+00:00
  edited: false
  hidden: false
  id: 647ef0f42a7bcaa30798c092
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/598b56e3dfc56293c59886fc83379cc2.svg
      fullname: Tao Blancheton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TaoBlnt
      type: user
    createdAt: '2023-06-06T09:25:19.000Z'
    data:
      edited: false
      editors:
      - TaoBlnt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9786586761474609
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/598b56e3dfc56293c59886fc83379cc2.svg
          fullname: Tao Blancheton
          isHf: false
          isPro: false
          name: TaoBlnt
          type: user
        html: '<p>Seems to work! Thanks a lot :)</p>

          '
        raw: Seems to work! Thanks a lot :)
        updatedAt: '2023-06-06T09:25:19.669Z'
      numEdits: 0
      reactions: []
      relatedEventId: 647efb7f2a7bcaa3079a939f
    id: 647efb7f2a7bcaa3079a939e
    type: comment
  author: TaoBlnt
  content: Seems to work! Thanks a lot :)
  created_at: 2023-06-06 08:25:19+00:00
  edited: false
  hidden: false
  id: 647efb7f2a7bcaa3079a939e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/598b56e3dfc56293c59886fc83379cc2.svg
      fullname: Tao Blancheton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TaoBlnt
      type: user
    createdAt: '2023-06-06T09:25:19.000Z'
    data:
      status: closed
    id: 647efb7f2a7bcaa3079a939f
    type: status-change
  author: TaoBlnt
  created_at: 2023-06-06 08:25:19+00:00
  id: 647efb7f2a7bcaa3079a939f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/OpenAssistant-SFT-7-Llama-30B-HF
repo_type: model
status: closed
target_branch: null
title: Infinite loop loading tokenizer
