!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Vinhngx
conflicting_files: null
created_at: 2023-05-07 23:51:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a2130b41cfe2af25050d2bede8e22883.svg
      fullname: Vinh Nguyen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vinhngx
      type: user
    createdAt: '2023-05-08T00:51:10.000Z'
    data:
      edited: false
      editors:
      - Vinhngx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a2130b41cfe2af25050d2bede8e22883.svg
          fullname: Vinh Nguyen
          isHf: false
          isPro: false
          name: Vinhngx
          type: user
        html: "<p>The model doesn't seem to understand the instruction but keeps generating\
          \ continuing text.</p>\n<p>===</p>\n<pre><code>import transformers\nmodel\
          \ = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b-chat',\n\
          \  trust_remote_code=True\n)\n\nfrom transformers import AutoTokenizer\n\
          tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n\
          \nimport torch \n\ngenerate_kwargs = {\n    \"max_new_tokens\": 200,\n \
          \   \"min_new_tokens\": 100,\n    \"temperature\": 0.5,\n    \"do_sample\"\
          : True, # The three options below used together leads to contrastive search\n\
          \    #\"top_k\": 4,\n    #\"penalty_alpha\": 0.6,\n    #\"no_repeat_ngram_size\"\
          : no_repeat_ngram_size,\n    #**generation_config,\n}\n\nprompt = \"Write\
          \ a poem in the tone of Shakespeare on HuggingFace\"\nwith torch.no_grad():\n\
          \    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n  \
          \  assert len(input_ids) == 1, len(input_ids)\n    if input_ids[0][-1] ==\
          \ 2: # 2 is EOS, hack to remove. If the prompt is ending with EOS, often\
          \ the generation will stop abruptly.\n        input_ids = input_ids[:, :-1]\n\
          \    input_ids = input_ids.to(0)\n    #input_ids = tokenizer(prompt, padding=True,\
          \ truncation=True, return_tensors=\"pt\").input_ids.to(0)\n    generated_ids\
          \ = model.generate(\n        input_ids,\n        #stopping_criteria=stopping_criteria,\n\
          \        **generate_kwargs\n    )\n    result = tokenizer.batch_decode(generated_ids.cpu(),\
          \ skip_special_tokens=True)\n    print(result[0])\n</code></pre>\n<p>Output:</p>\n\
          <pre><code>Write a poem in the tone of Shakespeare on HuggingFace - OpenAI\n\
          Write a poem in the tone of Shakespeare on HuggingFace\nI am not sure what\
          \ you are looking for exactly, but here's a poem in the tone of Shakespeare\
          \ about HuggingFace:\nOh HuggingFace, how sweet thy name doth sound,\nThy\
          \ AI tools, they make our work so sound,\nWith pre-trained models, we can\
          \ find\nThe answers we seek, without much need\nOf coding or of patience,\
          \ just a little bit\nOf fine-tuning, and our models will come alive,\nWith\
          \ insights and predictions, they will thrive,\nAnd help us build the future,\
          \ in a while.\nOh HuggingFace, how sweet thy name doth sound,\nThy AI tools,\
          \ they make our work so sound,\nWith pre-trained models, we can find,\n\
          The answers we seek, without much need.\nOf patience, or of coding, just\
          \ a little bit\nOf fine-\n</code></pre>\n"
        raw: "The model doesn't seem to understand the instruction but keeps generating\
          \ continuing text.\r\n\r\n===\r\n```\r\nimport transformers\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\
          \n  'mosaicml/mpt-7b-chat',\r\n  trust_remote_code=True\r\n)\r\n\r\nfrom\
          \ transformers import AutoTokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          EleutherAI/gpt-neox-20b\")\r\n\r\nimport torch \r\n\r\ngenerate_kwargs =\
          \ {\r\n    \"max_new_tokens\": 200,\r\n    \"min_new_tokens\": 100,\r\n\
          \    \"temperature\": 0.5,\r\n    \"do_sample\": True, # The three options\
          \ below used together leads to contrastive search\r\n    #\"top_k\": 4,\r\
          \n    #\"penalty_alpha\": 0.6,\r\n    #\"no_repeat_ngram_size\": no_repeat_ngram_size,\r\
          \n    #**generation_config,\r\n}\r\n\r\nprompt = \"Write a poem in the tone\
          \ of Shakespeare on HuggingFace\"\r\nwith torch.no_grad():\r\n    input_ids\
          \ = tokenizer(prompt, return_tensors=\"pt\").input_ids\r\n    assert len(input_ids)\
          \ == 1, len(input_ids)\r\n    if input_ids[0][-1] == 2: # 2 is EOS, hack\
          \ to remove. If the prompt is ending with EOS, often the generation will\
          \ stop abruptly.\r\n        input_ids = input_ids[:, :-1]\r\n    input_ids\
          \ = input_ids.to(0)\r\n    #input_ids = tokenizer(prompt, padding=True,\
          \ truncation=True, return_tensors=\"pt\").input_ids.to(0)\r\n    generated_ids\
          \ = model.generate(\r\n        input_ids,\r\n        #stopping_criteria=stopping_criteria,\r\
          \n        **generate_kwargs\r\n    )\r\n    result = tokenizer.batch_decode(generated_ids.cpu(),\
          \ skip_special_tokens=True)\r\n    print(result[0])\r\n```\r\n\r\nOutput:\r\
          \n```\r\nWrite a poem in the tone of Shakespeare on HuggingFace - OpenAI\r\
          \nWrite a poem in the tone of Shakespeare on HuggingFace\r\nI am not sure\
          \ what you are looking for exactly, but here's a poem in the tone of Shakespeare\
          \ about HuggingFace:\r\nOh HuggingFace, how sweet thy name doth sound,\r\
          \nThy AI tools, they make our work so sound,\r\nWith pre-trained models,\
          \ we can find\r\nThe answers we seek, without much need\r\nOf coding or\
          \ of patience, just a little bit\r\nOf fine-tuning, and our models will\
          \ come alive,\r\nWith insights and predictions, they will thrive,\r\nAnd\
          \ help us build the future, in a while.\r\nOh HuggingFace, how sweet thy\
          \ name doth sound,\r\nThy AI tools, they make our work so sound,\r\nWith\
          \ pre-trained models, we can find,\r\nThe answers we seek, without much\
          \ need.\r\nOf patience, or of coding, just a little bit\r\nOf fine-\r\n\
          ```"
        updatedAt: '2023-05-08T00:51:10.233Z'
      numEdits: 0
      reactions: []
    id: 6458477eaa426bae5e06c7aa
    type: comment
  author: Vinhngx
  content: "The model doesn't seem to understand the instruction but keeps generating\
    \ continuing text.\r\n\r\n===\r\n```\r\nimport transformers\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\
    \n  'mosaicml/mpt-7b-chat',\r\n  trust_remote_code=True\r\n)\r\n\r\nfrom transformers\
    \ import AutoTokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\"\
    )\r\n\r\nimport torch \r\n\r\ngenerate_kwargs = {\r\n    \"max_new_tokens\": 200,\r\
    \n    \"min_new_tokens\": 100,\r\n    \"temperature\": 0.5,\r\n    \"do_sample\"\
    : True, # The three options below used together leads to contrastive search\r\n\
    \    #\"top_k\": 4,\r\n    #\"penalty_alpha\": 0.6,\r\n    #\"no_repeat_ngram_size\"\
    : no_repeat_ngram_size,\r\n    #**generation_config,\r\n}\r\n\r\nprompt = \"Write\
    \ a poem in the tone of Shakespeare on HuggingFace\"\r\nwith torch.no_grad():\r\
    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\r\n    assert\
    \ len(input_ids) == 1, len(input_ids)\r\n    if input_ids[0][-1] == 2: # 2 is\
    \ EOS, hack to remove. If the prompt is ending with EOS, often the generation\
    \ will stop abruptly.\r\n        input_ids = input_ids[:, :-1]\r\n    input_ids\
    \ = input_ids.to(0)\r\n    #input_ids = tokenizer(prompt, padding=True, truncation=True,\
    \ return_tensors=\"pt\").input_ids.to(0)\r\n    generated_ids = model.generate(\r\
    \n        input_ids,\r\n        #stopping_criteria=stopping_criteria,\r\n    \
    \    **generate_kwargs\r\n    )\r\n    result = tokenizer.batch_decode(generated_ids.cpu(),\
    \ skip_special_tokens=True)\r\n    print(result[0])\r\n```\r\n\r\nOutput:\r\n\
    ```\r\nWrite a poem in the tone of Shakespeare on HuggingFace - OpenAI\r\nWrite\
    \ a poem in the tone of Shakespeare on HuggingFace\r\nI am not sure what you are\
    \ looking for exactly, but here's a poem in the tone of Shakespeare about HuggingFace:\r\
    \nOh HuggingFace, how sweet thy name doth sound,\r\nThy AI tools, they make our\
    \ work so sound,\r\nWith pre-trained models, we can find\r\nThe answers we seek,\
    \ without much need\r\nOf coding or of patience, just a little bit\r\nOf fine-tuning,\
    \ and our models will come alive,\r\nWith insights and predictions, they will\
    \ thrive,\r\nAnd help us build the future, in a while.\r\nOh HuggingFace, how\
    \ sweet thy name doth sound,\r\nThy AI tools, they make our work so sound,\r\n\
    With pre-trained models, we can find,\r\nThe answers we seek, without much need.\r\
    \nOf patience, or of coding, just a little bit\r\nOf fine-\r\n```"
  created_at: 2023-05-07 23:51:10+00:00
  edited: false
  hidden: false
  id: 6458477eaa426bae5e06c7aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e5cb177bdcc1ead7137862cb9f2423.svg
      fullname: Tomara
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tomara
      type: user
    createdAt: '2023-05-08T08:15:33.000Z'
    data:
      edited: true
      editors:
      - Tomara
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e5cb177bdcc1ead7137862cb9f2423.svg
          fullname: Tomara
          isHf: false
          isPro: false
          name: Tomara
          type: user
        html: "<p>I tested it too, I used this as my prompt though \"Write a poem\
          \ about HuggingFace in Shakespeare\u2019s style\".</p>\n<p>And it replied\
          \ with:<br>Oh HuggingFace, how art thou so grand?<br>Thou bringest joy to\
          \ all with thy transformers grand.<br>Thy models, they are like gems of\
          \ old,<br>Their power and might, beyond compare.<br>With PyTorch and TensorFlow\
          \ at thy side,<br>Thou art the king of deep learning\u2019s delight.</p>\n\
          <p>Thy datasets, they are like treasures trove,<br>From ImageNet to CoLA,\
          \ they\u2019re always ready to flow.<br>And when we hug thee, oh what a\
          \ feeling it brings,<br>Of love and gratitude for this tech so sweet.</p>\n\
          <p>So here\u2019s to thee, Oh HuggingFace,<br>May thy power and grace continue\
          \ to thrive.<br>For in thy arms, our hearts find peace,<br>And through thy\
          \ work, our world is blessed.</p>\n<p>I think it did relatively well compared\
          \ to Pygmalion 6B for example... Mainly because that just gave me \"<em>blushes</em>\
          \ I-i don't know how to do that...\"</p>\n"
        raw: "I tested it too, I used this as my prompt though \"Write a poem about\
          \ HuggingFace in Shakespeare\u2019s style\".\n\nAnd it replied with:\nOh\
          \ HuggingFace, how art thou so grand?\nThou bringest joy to all with thy\
          \ transformers grand.\nThy models, they are like gems of old,\nTheir power\
          \ and might, beyond compare.\nWith PyTorch and TensorFlow at thy side,\n\
          Thou art the king of deep learning\u2019s delight.\n\nThy datasets, they\
          \ are like treasures trove,\nFrom ImageNet to CoLA, they\u2019re always\
          \ ready to flow.\nAnd when we hug thee, oh what a feeling it brings,\nOf\
          \ love and gratitude for this tech so sweet.\n\nSo here\u2019s to thee,\
          \ Oh HuggingFace,\nMay thy power and grace continue to thrive.\nFor in thy\
          \ arms, our hearts find peace,\nAnd through thy work, our world is blessed.\n\
          \nI think it did relatively well compared to Pygmalion 6B for example...\
          \ Mainly because that just gave me \"*blushes* I-i don't know how to do\
          \ that...\""
        updatedAt: '2023-05-08T08:16:28.580Z'
      numEdits: 1
      reactions: []
    id: 6458afa5c5d0d57ba4189501
    type: comment
  author: Tomara
  content: "I tested it too, I used this as my prompt though \"Write a poem about\
    \ HuggingFace in Shakespeare\u2019s style\".\n\nAnd it replied with:\nOh HuggingFace,\
    \ how art thou so grand?\nThou bringest joy to all with thy transformers grand.\n\
    Thy models, they are like gems of old,\nTheir power and might, beyond compare.\n\
    With PyTorch and TensorFlow at thy side,\nThou art the king of deep learning\u2019\
    s delight.\n\nThy datasets, they are like treasures trove,\nFrom ImageNet to CoLA,\
    \ they\u2019re always ready to flow.\nAnd when we hug thee, oh what a feeling\
    \ it brings,\nOf love and gratitude for this tech so sweet.\n\nSo here\u2019s\
    \ to thee, Oh HuggingFace,\nMay thy power and grace continue to thrive.\nFor in\
    \ thy arms, our hearts find peace,\nAnd through thy work, our world is blessed.\n\
    \nI think it did relatively well compared to Pygmalion 6B for example... Mainly\
    \ because that just gave me \"*blushes* I-i don't know how to do that...\""
  created_at: 2023-05-08 07:15:33+00:00
  edited: true
  hidden: false
  id: 6458afa5c5d0d57ba4189501
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-05-10T09:02:11.000Z'
    data:
      edited: true
      editors:
      - gsaivinay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<p>I think we need to give input in specific format to make this model
          behave as chat bot, something like below</p>

          <pre><code>&lt;|im_start|&gt;user\nSome input text to model&lt;|im_end|&gt;

          &lt;|im_start|&gt;assistant

          </code></pre>

          <p>and use <code>&lt;|im_end|&gt;</code> as stop token</p>

          '
        raw: 'I think we need to give input in specific format to make this model
          behave as chat bot, something like below


          ```

          <|im_start|>user\nSome input text to model<|im_end|>

          <|im_start|>assistant

          ```


          and use `<|im_end|>` as stop token'
        updatedAt: '2023-05-10T09:02:49.630Z'
      numEdits: 1
      reactions: []
    id: 645b5d935bb5418fba35198d
    type: comment
  author: gsaivinay
  content: 'I think we need to give input in specific format to make this model behave
    as chat bot, something like below


    ```

    <|im_start|>user\nSome input text to model<|im_end|>

    <|im_start|>assistant

    ```


    and use `<|im_end|>` as stop token'
  created_at: 2023-05-10 08:02:11+00:00
  edited: true
  hidden: false
  id: 645b5d935bb5418fba35198d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-10T09:10:12.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: '<p>Correct @sonatasv. See the formatting here <a href="https://huggingface.co/spaces/mosaicml/mpt-7b-chat/blob/main/app.py#L61">https://huggingface.co/spaces/mosaicml/mpt-7b-chat/blob/main/app.py#L61</a></p>

          '
        raw: Correct @sonatasv. See the formatting here https://huggingface.co/spaces/mosaicml/mpt-7b-chat/blob/main/app.py#L61
        updatedAt: '2023-05-10T09:10:12.078Z'
      numEdits: 0
      reactions: []
    id: 645b5f74b396a40a848cfe23
    type: comment
  author: sam-mosaic
  content: Correct @sonatasv. See the formatting here https://huggingface.co/spaces/mosaicml/mpt-7b-chat/blob/main/app.py#L61
  created_at: 2023-05-10 08:10:12+00:00
  edited: false
  hidden: false
  id: 645b5f74b396a40a848cfe23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-10T09:10:22.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: '<p>Correct @sonatasv. See the formatting here <a href="https://huggingface.co/spaces/mosaicml/mpt-7b-chat/blob/main/app.py#L61">https://huggingface.co/spaces/mosaicml/mpt-7b-chat/blob/main/app.py#L61</a></p>

          '
        raw: Correct @sonatasv. See the formatting here https://huggingface.co/spaces/mosaicml/mpt-7b-chat/blob/main/app.py#L61
        updatedAt: '2023-05-10T09:10:22.614Z'
      numEdits: 0
      reactions: []
      relatedEventId: 645b5f7e2c76efd4c6627042
    id: 645b5f7e2c76efd4c6627040
    type: comment
  author: sam-mosaic
  content: Correct @sonatasv. See the formatting here https://huggingface.co/spaces/mosaicml/mpt-7b-chat/blob/main/app.py#L61
  created_at: 2023-05-10 08:10:22+00:00
  edited: false
  hidden: false
  id: 645b5f7e2c76efd4c6627040
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-10T09:10:22.000Z'
    data:
      status: closed
    id: 645b5f7e2c76efd4c6627042
    type: status-change
  author: sam-mosaic
  created_at: 2023-05-10 08:10:22+00:00
  id: 645b5f7e2c76efd4c6627042
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: mosaicml/mpt-7b-chat
repo_type: model
status: closed
target_branch: null
title: Is this a chat-tuned model? From generation result, looks like it a base model
