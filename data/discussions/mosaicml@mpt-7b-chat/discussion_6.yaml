!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ehartford
conflicting_files: null
created_at: 2023-05-13 06:16:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-05-13T07:16:05.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>In order to reproduce your results, and in the spirit of open source,
          I would like to request</p>

          <ol>

          <li>The final dataset you used to train mpt-7b-chat (and the scripts you
          used to compose this dataset from the source datasets)</li>

          <li>the code used to train the model.</li>

          <li>the hyperparameters you used to train the model (the exact command line
          and arguments would be lovely)</li>

          <li>the hardware you used and how long it took<br>thank you</li>

          </ol>

          '
        raw: "In order to reproduce your results, and in the spirit of open source,\
          \ I would like to request\r\n1) The final dataset you used to train mpt-7b-chat\
          \ (and the scripts you used to compose this dataset from the source datasets)\r\
          \n2) the code used to train the model.\r\n3) the hyperparameters you used\
          \ to train the model (the exact command line and arguments would be lovely)\r\
          \n4) the hardware you used and how long it took\r\nthank you"
        updatedAt: '2023-05-13T07:16:05.826Z'
      numEdits: 0
      reactions: []
    id: 645f3935236b10b70119eef2
    type: comment
  author: ehartford
  content: "In order to reproduce your results, and in the spirit of open source,\
    \ I would like to request\r\n1) The final dataset you used to train mpt-7b-chat\
    \ (and the scripts you used to compose this dataset from the source datasets)\r\
    \n2) the code used to train the model.\r\n3) the hyperparameters you used to train\
    \ the model (the exact command line and arguments would be lovely)\r\n4) the hardware\
    \ you used and how long it took\r\nthank you"
  created_at: 2023-05-13 06:16:05+00:00
  edited: false
  hidden: false
  id: 645f3935236b10b70119eef2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bafc31a83d43f6a03b3310b2afd97fec.svg
      fullname: Maxwell Ilie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maxilie
      type: user
    createdAt: '2023-05-16T07:19:53.000Z'
    data:
      edited: false
      editors:
      - maxilie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bafc31a83d43f6a03b3310b2afd97fec.svg
          fullname: Maxwell Ilie
          isHf: false
          isPro: false
          name: maxilie
          type: user
        html: '<p>You can find most of the info you''re looking for in MosaicML''s
          <a rel="nofollow" href="https://www.mosaicml.com/blog/mpt-7b">MPT blog post</a>
          and in the <a rel="nofollow" href="https://github.com/mosaicml/llm-foundry/tree/main/scripts/train">github
          repo</a>.</p>

          <ol>

          <li>Datasets used for fine-tuning mpt-7b-chat: ShareGPT-Vicuna, HC3, Alpaca,
          Helpful and Harmless, and Evol-Instruct datasets. Datasets used to pre-train
          the base model: <a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/5ef52a5b8eb76b20aca0a786/b5A-7d4C_fyrIicUbh8am.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/5ef52a5b8eb76b20aca0a786/b5A-7d4C_fyrIicUbh8am.png"></a>
          By the way, their subset of "the stack" coding dataset doesn''t include
          Javascript and Typescript.</li>

          <li>They used their own compute platform. You can use the example command
          provided <a rel="nofollow" href="https://github.com/mosaicml/llm-foundry/tree/main/mcli">here</a>
          (<code>mcli run -f mcli-1b.yaml --cluster CLUSTER --gpus GPUS --name NAME
          --follow</code>) with a modified version of the <a rel="nofollow" href="https://github.com/mosaicml/llm-foundry/blob/main/mcli/mcli-1b.yaml">mcli-1b.yaml
          config</a> probably replacing the training config for mpt-1b (in line 19)
          with the <a rel="nofollow" href="https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/yamls/pretrain/mpt-7b.yaml">training
          config for mpt-7b</a>. Similar process for fine-tuning is mostly explained
          in the <a rel="nofollow" href="https://github.com/mosaicml/llm-foundry/tree/main/scripts/train">github
          repo</a>.</li>

          <li><a rel="nofollow" href="https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/yamls/finetune/mpt-7b_dolly_sft.yaml">This
          the config for fine-tuning</a>.</li>

          <li>Hardware, cost, and training time:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/5ef52a5b8eb76b20aca0a786/ZMjBCrQFdJtIXHdVjDJEl.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/5ef52a5b8eb76b20aca0a786/ZMjBCrQFdJtIXHdVjDJEl.png"></a></li>

          </ol>

          '
        raw: 'You can find most of the info you''re looking for in MosaicML''s [MPT
          blog post](https://www.mosaicml.com/blog/mpt-7b) and in the [github repo](https://github.com/mosaicml/llm-foundry/tree/main/scripts/train).

          1.  Datasets used for fine-tuning mpt-7b-chat: ShareGPT-Vicuna, HC3, Alpaca,
          Helpful and Harmless, and Evol-Instruct datasets. Datasets used to pre-train
          the base model: ![image.png](https://cdn-uploads.huggingface.co/production/uploads/5ef52a5b8eb76b20aca0a786/b5A-7d4C_fyrIicUbh8am.png)
          By the way, their subset of "the stack" coding dataset doesn''t include
          Javascript and Typescript.

          2. They used their own compute platform. You can use the example command
          provided [here](https://github.com/mosaicml/llm-foundry/tree/main/mcli)
          (`mcli run -f mcli-1b.yaml --cluster CLUSTER --gpus GPUS --name NAME --follow`)
          with a modified version of the [mcli-1b.yaml config](https://github.com/mosaicml/llm-foundry/blob/main/mcli/mcli-1b.yaml)
          probably replacing the training config for mpt-1b (in line 19) with the
          [training config for mpt-7b](https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/yamls/pretrain/mpt-7b.yaml).
          Similar process for fine-tuning is mostly explained in the [github repo](https://github.com/mosaicml/llm-foundry/tree/main/scripts/train).

          3. [This the config for fine-tuning](https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/yamls/finetune/mpt-7b_dolly_sft.yaml).

          4. Hardware, cost, and training time:

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/5ef52a5b8eb76b20aca0a786/ZMjBCrQFdJtIXHdVjDJEl.png)'
        updatedAt: '2023-05-16T07:19:53.618Z'
      numEdits: 0
      reactions: []
    id: 64632e9912814d754176972a
    type: comment
  author: maxilie
  content: 'You can find most of the info you''re looking for in MosaicML''s [MPT
    blog post](https://www.mosaicml.com/blog/mpt-7b) and in the [github repo](https://github.com/mosaicml/llm-foundry/tree/main/scripts/train).

    1.  Datasets used for fine-tuning mpt-7b-chat: ShareGPT-Vicuna, HC3, Alpaca, Helpful
    and Harmless, and Evol-Instruct datasets. Datasets used to pre-train the base
    model: ![image.png](https://cdn-uploads.huggingface.co/production/uploads/5ef52a5b8eb76b20aca0a786/b5A-7d4C_fyrIicUbh8am.png)
    By the way, their subset of "the stack" coding dataset doesn''t include Javascript
    and Typescript.

    2. They used their own compute platform. You can use the example command provided
    [here](https://github.com/mosaicml/llm-foundry/tree/main/mcli) (`mcli run -f mcli-1b.yaml
    --cluster CLUSTER --gpus GPUS --name NAME --follow`) with a modified version of
    the [mcli-1b.yaml config](https://github.com/mosaicml/llm-foundry/blob/main/mcli/mcli-1b.yaml)
    probably replacing the training config for mpt-1b (in line 19) with the [training
    config for mpt-7b](https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/yamls/pretrain/mpt-7b.yaml).
    Similar process for fine-tuning is mostly explained in the [github repo](https://github.com/mosaicml/llm-foundry/tree/main/scripts/train).

    3. [This the config for fine-tuning](https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/yamls/finetune/mpt-7b_dolly_sft.yaml).

    4. Hardware, cost, and training time:

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/5ef52a5b8eb76b20aca0a786/ZMjBCrQFdJtIXHdVjDJEl.png)'
  created_at: 2023-05-16 06:19:53+00:00
  edited: false
  hidden: false
  id: 64632e9912814d754176972a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-03T00:22:31.000Z'
    data:
      edited: false
      editors:
      - abhi-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9936800599098206
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: '<p>Closing as stale</p>

          '
        raw: Closing as stale
        updatedAt: '2023-06-03T00:22:31.711Z'
      numEdits: 0
      reactions: []
      relatedEventId: 647a87c744b6a3ae9d297627
    id: 647a87c744b6a3ae9d297625
    type: comment
  author: abhi-mosaic
  content: Closing as stale
  created_at: 2023-06-02 23:22:31+00:00
  edited: false
  hidden: false
  id: 647a87c744b6a3ae9d297625
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-03T00:22:31.000Z'
    data:
      status: closed
    id: 647a87c744b6a3ae9d297627
    type: status-change
  author: abhi-mosaic
  created_at: 2023-06-02 23:22:31+00:00
  id: 647a87c744b6a3ae9d297627
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: mosaicml/mpt-7b-chat
repo_type: model
status: closed
target_branch: null
title: reproduce mpt-7b-chat
