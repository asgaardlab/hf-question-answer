!!python/object:huggingface_hub.community.DiscussionWithDetails
author: antman1p
conflicting_files: null
created_at: 2023-05-15 13:18:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d55f1778ead8eb63a3a8ffd3447704ed.svg
      fullname: Antonio Piazza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: antman1p
      type: user
    createdAt: '2023-05-15T14:18:14.000Z'
    data:
      edited: false
      editors:
      - antman1p
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d55f1778ead8eb63a3a8ffd3447704ed.svg
          fullname: Antonio Piazza
          isHf: false
          isPro: false
          name: antman1p
          type: user
        html: "<p>I am getting an Out of memory error running an RTX 4090.  Tried\
          \ in Win 11 and WSL.  Using Cuda 11.7.</p>\n<pre><code>device = torch.device(\"\
          cuda:0\" if torch.cuda.is_available() else \"cpu\")\nconfig = AutoConfig.from_pretrained(\n\
          \  'mosaicml/mpt-7b-chat',\n  trust_remote_code=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \  'mosaicml/mpt-7b-chat',\n  torch_dtype=torch.bfloat16,\n  config=config,\n\
          \  trust_remote_code=True\n)\nmodel.to(device)\n\nLoading checkpoint shards:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:04&lt;00:00,\
          \  2.16s/it]\n---------------------------------------------------------------------------\n\
          OutOfMemoryError                          Traceback (most recent call last)\n\
          Cell In[5], line 17\n     10 #print(config)\n     11 model = AutoModelForCausalLM.from_pretrained(\n\
          \     12   'mosaicml/mpt-7b-chat',\n     13   torch_dtype=torch.bfloat16,\n\
          \     14   config=config,\n     15   trust_remote_code=True\n     16 )\n\
          ---&gt; 17 model.to(device)\n\nFile ~\\Documents\\DEV\\lib\\site-packages\\\
          transformers\\modeling_utils.py:1878, in PreTrainedModel.to(self, *args,\
          \ **kwargs)\n   1873     raise ValueError(\n   1874         \"`.to` is not\
          \ supported for `8-bit` models. Please use the model as it is, since the\"\
          \n   1875         \" model has already been set to the correct devices and\
          \ casted to the correct `dtype`.\"\n   1876     )\n   1877 else:\n-&gt;\
          \ 1878     return super().to(*args, **kwargs)\n\nFile ~\\Documents\\DEV\\\
          lib\\site-packages\\torch\\nn\\modules\\module.py:1145, in Module.to(self,\
          \ *args, **kwargs)\n   1141         return t.to(device, dtype if t.is_floating_point()\
          \ or t.is_complex() else None,\n   1142                     non_blocking,\
          \ memory_format=convert_to_format)\n   1143     return t.to(device, dtype\
          \ if t.is_floating_point() or t.is_complex() else None, non_blocking)\n\
          -&gt; 1145 return self._apply(convert)\n\nFile ~\\Documents\\DEV\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py:797, in Module._apply(self,\
          \ fn)\n    795 def _apply(self, fn):\n    796     for module in self.children():\n\
          --&gt; 797         module._apply(fn)\n    799     def compute_should_use_set_data(tensor,\
          \ tensor_applied):\n    800         if torch._has_compatible_shallow_copy_type(tensor,\
          \ tensor_applied):\n    801             # If the new tensor has compatible\
          \ tensor type as the existing tensor,\n    802             # the current\
          \ behavior is to change the tensor in-place using `.data =`,\n   (...)\n\
          \    807             # global flag to let the user control whether they\
          \ want the future\n    808             # behavior of overwriting the existing\
          \ tensor or not.\n\nFile ~\\Documents\\DEV\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py:797, in Module._apply(self, fn)\n    795 def _apply(self,\
          \ fn):\n    796     for module in self.children():\n--&gt; 797         module._apply(fn)\n\
          \    799     def compute_should_use_set_data(tensor, tensor_applied):\n\
          \    800         if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\
          \    801             # If the new tensor has compatible tensor type as the\
          \ existing tensor,\n    802             # the current behavior is to change\
          \ the tensor in-place using `.data =`,\n   (...)\n    807             #\
          \ global flag to let the user control whether they want the future\n   \
          \ 808             # behavior of overwriting the existing tensor or not.\n\
          \n    [... skipping similar frames: Module._apply at line 797 (2 times)]\n\
          \nFile ~\\Documents\\DEV\\lib\\site-packages\\torch\\nn\\modules\\module.py:797,\
          \ in Module._apply(self, fn)\n    795 def _apply(self, fn):\n    796   \
          \  for module in self.children():\n--&gt; 797         module._apply(fn)\n\
          \    799     def compute_should_use_set_data(tensor, tensor_applied):\n\
          \    800         if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\
          \    801             # If the new tensor has compatible tensor type as the\
          \ existing tensor,\n    802             # the current behavior is to change\
          \ the tensor in-place using `.data =`,\n   (...)\n    807             #\
          \ global flag to let the user control whether they want the future\n   \
          \ 808             # behavior of overwriting the existing tensor or not.\n\
          \nFile ~\\Documents\\DEV\\lib\\site-packages\\torch\\nn\\modules\\module.py:820,\
          \ in Module._apply(self, fn)\n    816 # Tensors stored in modules are graph\
          \ leaves, and we don't want to\n    817 # track autograd history of `param_applied`,\
          \ so we have to use\n    818 # `with torch.no_grad():`\n    819 with torch.no_grad():\n\
          --&gt; 820     param_applied = fn(param)\n    821 should_use_set_data =\
          \ compute_should_use_set_data(param, param_applied)\n    822 if should_use_set_data:\n\
          \nFile ~\\Documents\\DEV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143,\
          \ in Module.to.&lt;locals&gt;.convert(t)\n   1140 if convert_to_format is\
          \ not None and t.dim() in (4, 5):\n   1141     return t.to(device, dtype\
          \ if t.is_floating_point() or t.is_complex() else None,\n   1142       \
          \          non_blocking, memory_format=convert_to_format)\n-&gt; 1143 return\
          \ t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\
          \ non_blocking)\n\nOutOfMemoryError: CUDA out of memory. Tried to allocate\
          \ 128.00 MiB (GPU 0; 23.99 GiB total capacity; 11.89 GiB already allocated;\
          \ 10.58 GiB free; 11.99 GiB allowed; 11.89 GiB reserved in total by PyTorch)\
          \ If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
          </code></pre>\n"
        raw: "I am getting an Out of memory error running an RTX 4090.  Tried in Win\
          \ 11 and WSL.  Using Cuda 11.7.\r\n```\r\ndevice = torch.device(\"cuda:0\"\
          \ if torch.cuda.is_available() else \"cpu\")\r\nconfig = AutoConfig.from_pretrained(\r\
          \n  'mosaicml/mpt-7b-chat',\r\n  trust_remote_code=True\r\n)\r\n\r\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(\r\n  'mosaicml/mpt-7b-chat',\r\
          \n  torch_dtype=torch.bfloat16,\r\n  config=config,\r\n  trust_remote_code=True\r\
          \n)\r\nmodel.to(device)\r\n\r\nLoading checkpoint shards: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:04<00:00,  2.16s/it]\r\
          \n---------------------------------------------------------------------------\r\
          \nOutOfMemoryError                          Traceback (most recent call\
          \ last)\r\nCell In[5], line 17\r\n     10 #print(config)\r\n     11 model\
          \ = AutoModelForCausalLM.from_pretrained(\r\n     12   'mosaicml/mpt-7b-chat',\r\
          \n     13   torch_dtype=torch.bfloat16,\r\n     14   config=config,\r\n\
          \     15   trust_remote_code=True\r\n     16 )\r\n---> 17 model.to(device)\r\
          \n\r\nFile ~\\Documents\\DEV\\lib\\site-packages\\transformers\\modeling_utils.py:1878,\
          \ in PreTrainedModel.to(self, *args, **kwargs)\r\n   1873     raise ValueError(\r\
          \n   1874         \"`.to` is not supported for `8-bit` models. Please use\
          \ the model as it is, since the\"\r\n   1875         \" model has already\
          \ been set to the correct devices and casted to the correct `dtype`.\"\r\
          \n   1876     )\r\n   1877 else:\r\n-> 1878     return super().to(*args,\
          \ **kwargs)\r\n\r\nFile ~\\Documents\\DEV\\lib\\site-packages\\torch\\nn\\\
          modules\\module.py:1145, in Module.to(self, *args, **kwargs)\r\n   1141\
          \         return t.to(device, dtype if t.is_floating_point() or t.is_complex()\
          \ else None,\r\n   1142                     non_blocking, memory_format=convert_to_format)\r\
          \n   1143     return t.to(device, dtype if t.is_floating_point() or t.is_complex()\
          \ else None, non_blocking)\r\n-> 1145 return self._apply(convert)\r\n\r\n\
          File ~\\Documents\\DEV\\lib\\site-packages\\torch\\nn\\modules\\module.py:797,\
          \ in Module._apply(self, fn)\r\n    795 def _apply(self, fn):\r\n    796\
          \     for module in self.children():\r\n--> 797         module._apply(fn)\r\
          \n    799     def compute_should_use_set_data(tensor, tensor_applied):\r\
          \n    800         if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\r\
          \n    801             # If the new tensor has compatible tensor type as\
          \ the existing tensor,\r\n    802             # the current behavior is\
          \ to change the tensor in-place using `.data =`,\r\n   (...)\r\n    807\
          \             # global flag to let the user control whether they want the\
          \ future\r\n    808             # behavior of overwriting the existing tensor\
          \ or not.\r\n\r\nFile ~\\Documents\\DEV\\lib\\site-packages\\torch\\nn\\\
          modules\\module.py:797, in Module._apply(self, fn)\r\n    795 def _apply(self,\
          \ fn):\r\n    796     for module in self.children():\r\n--> 797        \
          \ module._apply(fn)\r\n    799     def compute_should_use_set_data(tensor,\
          \ tensor_applied):\r\n    800         if torch._has_compatible_shallow_copy_type(tensor,\
          \ tensor_applied):\r\n    801             # If the new tensor has compatible\
          \ tensor type as the existing tensor,\r\n    802             # the current\
          \ behavior is to change the tensor in-place using `.data =`,\r\n   (...)\r\
          \n    807             # global flag to let the user control whether they\
          \ want the future\r\n    808             # behavior of overwriting the existing\
          \ tensor or not.\r\n\r\n    [... skipping similar frames: Module._apply\
          \ at line 797 (2 times)]\r\n\r\nFile ~\\Documents\\DEV\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py:797, in Module._apply(self, fn)\r\n    795\
          \ def _apply(self, fn):\r\n    796     for module in self.children():\r\n\
          --> 797         module._apply(fn)\r\n    799     def compute_should_use_set_data(tensor,\
          \ tensor_applied):\r\n    800         if torch._has_compatible_shallow_copy_type(tensor,\
          \ tensor_applied):\r\n    801             # If the new tensor has compatible\
          \ tensor type as the existing tensor,\r\n    802             # the current\
          \ behavior is to change the tensor in-place using `.data =`,\r\n   (...)\r\
          \n    807             # global flag to let the user control whether they\
          \ want the future\r\n    808             # behavior of overwriting the existing\
          \ tensor or not.\r\n\r\nFile ~\\Documents\\DEV\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py:820, in Module._apply(self, fn)\r\n    816 # Tensors\
          \ stored in modules are graph leaves, and we don't want to\r\n    817 #\
          \ track autograd history of `param_applied`, so we have to use\r\n    818\
          \ # `with torch.no_grad():`\r\n    819 with torch.no_grad():\r\n--> 820\
          \     param_applied = fn(param)\r\n    821 should_use_set_data = compute_should_use_set_data(param,\
          \ param_applied)\r\n    822 if should_use_set_data:\r\n\r\nFile ~\\Documents\\\
          DEV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143, in Module.to.<locals>.convert(t)\r\
          \n   1140 if convert_to_format is not None and t.dim() in (4, 5):\r\n  \
          \ 1141     return t.to(device, dtype if t.is_floating_point() or t.is_complex()\
          \ else None,\r\n   1142                 non_blocking, memory_format=convert_to_format)\r\
          \n-> 1143 return t.to(device, dtype if t.is_floating_point() or t.is_complex()\
          \ else None, non_blocking)\r\n\r\nOutOfMemoryError: CUDA out of memory.\
          \ Tried to allocate 128.00 MiB (GPU 0; 23.99 GiB total capacity; 11.89 GiB\
          \ already allocated; 10.58 GiB free; 11.99 GiB allowed; 11.89 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF\r\n```"
        updatedAt: '2023-05-15T14:18:14.649Z'
      numEdits: 0
      reactions: []
    id: 64623f262a83863b97bd48f5
    type: comment
  author: antman1p
  content: "I am getting an Out of memory error running an RTX 4090.  Tried in Win\
    \ 11 and WSL.  Using Cuda 11.7.\r\n```\r\ndevice = torch.device(\"cuda:0\" if\
    \ torch.cuda.is_available() else \"cpu\")\r\nconfig = AutoConfig.from_pretrained(\r\
    \n  'mosaicml/mpt-7b-chat',\r\n  trust_remote_code=True\r\n)\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\
    \n  'mosaicml/mpt-7b-chat',\r\n  torch_dtype=torch.bfloat16,\r\n  config=config,\r\
    \n  trust_remote_code=True\r\n)\r\nmodel.to(device)\r\n\r\nLoading checkpoint\
    \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:04<00:00,  2.16s/it]\r\n---------------------------------------------------------------------------\r\
    \nOutOfMemoryError                          Traceback (most recent call last)\r\
    \nCell In[5], line 17\r\n     10 #print(config)\r\n     11 model = AutoModelForCausalLM.from_pretrained(\r\
    \n     12   'mosaicml/mpt-7b-chat',\r\n     13   torch_dtype=torch.bfloat16,\r\
    \n     14   config=config,\r\n     15   trust_remote_code=True\r\n     16 )\r\n\
    ---> 17 model.to(device)\r\n\r\nFile ~\\Documents\\DEV\\lib\\site-packages\\transformers\\\
    modeling_utils.py:1878, in PreTrainedModel.to(self, *args, **kwargs)\r\n   1873\
    \     raise ValueError(\r\n   1874         \"`.to` is not supported for `8-bit`\
    \ models. Please use the model as it is, since the\"\r\n   1875         \" model\
    \ has already been set to the correct devices and casted to the correct `dtype`.\"\
    \r\n   1876     )\r\n   1877 else:\r\n-> 1878     return super().to(*args, **kwargs)\r\
    \n\r\nFile ~\\Documents\\DEV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145,\
    \ in Module.to(self, *args, **kwargs)\r\n   1141         return t.to(device, dtype\
    \ if t.is_floating_point() or t.is_complex() else None,\r\n   1142           \
    \          non_blocking, memory_format=convert_to_format)\r\n   1143     return\
    \ t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\
    \n-> 1145 return self._apply(convert)\r\n\r\nFile ~\\Documents\\DEV\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py:797, in Module._apply(self, fn)\r\n    795 def _apply(self,\
    \ fn):\r\n    796     for module in self.children():\r\n--> 797         module._apply(fn)\r\
    \n    799     def compute_should_use_set_data(tensor, tensor_applied):\r\n   \
    \ 800         if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\r\
    \n    801             # If the new tensor has compatible tensor type as the existing\
    \ tensor,\r\n    802             # the current behavior is to change the tensor\
    \ in-place using `.data =`,\r\n   (...)\r\n    807             # global flag to\
    \ let the user control whether they want the future\r\n    808             # behavior\
    \ of overwriting the existing tensor or not.\r\n\r\nFile ~\\Documents\\DEV\\lib\\\
    site-packages\\torch\\nn\\modules\\module.py:797, in Module._apply(self, fn)\r\
    \n    795 def _apply(self, fn):\r\n    796     for module in self.children():\r\
    \n--> 797         module._apply(fn)\r\n    799     def compute_should_use_set_data(tensor,\
    \ tensor_applied):\r\n    800         if torch._has_compatible_shallow_copy_type(tensor,\
    \ tensor_applied):\r\n    801             # If the new tensor has compatible tensor\
    \ type as the existing tensor,\r\n    802             # the current behavior is\
    \ to change the tensor in-place using `.data =`,\r\n   (...)\r\n    807      \
    \       # global flag to let the user control whether they want the future\r\n\
    \    808             # behavior of overwriting the existing tensor or not.\r\n\
    \r\n    [... skipping similar frames: Module._apply at line 797 (2 times)]\r\n\
    \r\nFile ~\\Documents\\DEV\\lib\\site-packages\\torch\\nn\\modules\\module.py:797,\
    \ in Module._apply(self, fn)\r\n    795 def _apply(self, fn):\r\n    796     for\
    \ module in self.children():\r\n--> 797         module._apply(fn)\r\n    799 \
    \    def compute_should_use_set_data(tensor, tensor_applied):\r\n    800     \
    \    if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\r\n \
    \   801             # If the new tensor has compatible tensor type as the existing\
    \ tensor,\r\n    802             # the current behavior is to change the tensor\
    \ in-place using `.data =`,\r\n   (...)\r\n    807             # global flag to\
    \ let the user control whether they want the future\r\n    808             # behavior\
    \ of overwriting the existing tensor or not.\r\n\r\nFile ~\\Documents\\DEV\\lib\\\
    site-packages\\torch\\nn\\modules\\module.py:820, in Module._apply(self, fn)\r\
    \n    816 # Tensors stored in modules are graph leaves, and we don't want to\r\
    \n    817 # track autograd history of `param_applied`, so we have to use\r\n \
    \   818 # `with torch.no_grad():`\r\n    819 with torch.no_grad():\r\n--> 820\
    \     param_applied = fn(param)\r\n    821 should_use_set_data = compute_should_use_set_data(param,\
    \ param_applied)\r\n    822 if should_use_set_data:\r\n\r\nFile ~\\Documents\\\
    DEV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143, in Module.to.<locals>.convert(t)\r\
    \n   1140 if convert_to_format is not None and t.dim() in (4, 5):\r\n   1141 \
    \    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else\
    \ None,\r\n   1142                 non_blocking, memory_format=convert_to_format)\r\
    \n-> 1143 return t.to(device, dtype if t.is_floating_point() or t.is_complex()\
    \ else None, non_blocking)\r\n\r\nOutOfMemoryError: CUDA out of memory. Tried\
    \ to allocate 128.00 MiB (GPU 0; 23.99 GiB total capacity; 11.89 GiB already allocated;\
    \ 10.58 GiB free; 11.99 GiB allowed; 11.89 GiB reserved in total by PyTorch) If\
    \ reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
    \n```"
  created_at: 2023-05-15 13:18:14+00:00
  edited: false
  hidden: false
  id: 64623f262a83863b97bd48f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/072416bbc0f040717af589fac25f579b.svg
      fullname: victor cheng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vic2240
      type: user
    createdAt: '2023-05-16T14:08:05.000Z'
    data:
      edited: false
      editors:
      - vic2240
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/072416bbc0f040717af589fac25f579b.svg
          fullname: victor cheng
          isHf: false
          isPro: false
          name: vic2240
          type: user
        html: '<p>Yes. Same Problem with my 24G RTX3090!</p>

          '
        raw: Yes. Same Problem with my 24G RTX3090!
        updatedAt: '2023-05-16T14:08:05.161Z'
      numEdits: 0
      reactions: []
    id: 64638e45a429ec3af0a64c74
    type: comment
  author: vic2240
  content: Yes. Same Problem with my 24G RTX3090!
  created_at: 2023-05-16 13:08:05+00:00
  edited: false
  hidden: false
  id: 64638e45a429ec3af0a64c74
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/072416bbc0f040717af589fac25f579b.svg
      fullname: victor cheng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vic2240
      type: user
    createdAt: '2023-05-16T14:15:30.000Z'
    data:
      edited: false
      editors:
      - vic2240
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/072416bbc0f040717af589fac25f579b.svg
          fullname: victor cheng
          isHf: false
          isPro: false
          name: vic2240
          type: user
        html: '<p>I tried with adding "torch_dtype=torch.bfloat16" to the model initialization.
          It''s OK now!</p>

          '
        raw: I tried with adding "torch_dtype=torch.bfloat16" to the model initialization.
          It's OK now!
        updatedAt: '2023-05-16T14:15:30.383Z'
      numEdits: 0
      reactions: []
    id: 64639002c758f942090c60fb
    type: comment
  author: vic2240
  content: I tried with adding "torch_dtype=torch.bfloat16" to the model initialization.
    It's OK now!
  created_at: 2023-05-16 13:15:30+00:00
  edited: false
  hidden: false
  id: 64639002c758f942090c60fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d55f1778ead8eb63a3a8ffd3447704ed.svg
      fullname: Antonio Piazza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: antman1p
      type: user
    createdAt: '2023-05-16T16:09:34.000Z'
    data:
      edited: false
      editors:
      - antman1p
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d55f1778ead8eb63a3a8ffd3447704ed.svg
          fullname: Antonio Piazza
          isHf: false
          isPro: false
          name: antman1p
          type: user
        html: '<p>I also have that in my code above, but it doesn''t work for me.</p>

          '
        raw: I also have that in my code above, but it doesn't work for me.
        updatedAt: '2023-05-16T16:09:34.091Z'
      numEdits: 0
      reactions: []
    id: 6463aabe124e3ea89ee75e2e
    type: comment
  author: antman1p
  content: I also have that in my code above, but it doesn't work for me.
  created_at: 2023-05-16 15:09:34+00:00
  edited: false
  hidden: false
  id: 6463aabe124e3ea89ee75e2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b40d17aab9044d851ff9d46c0dd8eab2.svg
      fullname: Praveen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prvnsmpth
      type: user
    createdAt: '2023-05-26T10:10:54.000Z'
    data:
      edited: false
      editors:
      - prvnsmpth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b40d17aab9044d851ff9d46c0dd8eab2.svg
          fullname: Praveen
          isHf: false
          isPro: false
          name: prvnsmpth
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;antman1p&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/antman1p\">@<span class=\"\
          underline\">antman1p</span></a></span>\n\n\t</span></span>  With \"torch_dtype=torch.bfloat16\"\
          , the 7B model should only take up ~14 GB. Here's what my nvidia-smi looks\
          \ like with the model loaded:</p>\n<pre><code>+---------------------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version:\
          \ 12.1     |\n|-----------------------------------------+----------------------+----------------------+\n\
          | GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile\
          \ Uncorr. ECC |\n| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage\
          \ | GPU-Util  Compute M. |\n|                                         |\
          \                      |               MIG M. |\n|=========================================+======================+======================|\n\
          |   0  NVIDIA GeForce RTX 4090         Off| 00000000:01:00.0  On |     \
          \             Off |\n| 30%   36C    P8               10W / 450W|  14624MiB\
          \ / 24564MiB |      0%      Default |\n|                               \
          \          |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\
          </code></pre>\n"
        raw: '@antman1p  With "torch_dtype=torch.bfloat16", the 7B model should only
          take up ~14 GB. Here''s what my nvidia-smi looks like with the model loaded:

          ```

          +---------------------------------------------------------------------------------------+

          | NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version:
          12.1     |

          |-----------------------------------------+----------------------+----------------------+

          | GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile
          Uncorr. ECC |

          | Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute
          M. |

          |                                         |                      |               MIG
          M. |

          |=========================================+======================+======================|

          |   0  NVIDIA GeForce RTX 4090         Off| 00000000:01:00.0  On |                  Off
          |

          | 30%   36C    P8               10W / 450W|  14624MiB / 24564MiB |      0%      Default
          |

          |                                         |                      |                  N/A
          |

          +-----------------------------------------+----------------------+----------------------+

          ```'
        updatedAt: '2023-05-26T10:10:54.380Z'
      numEdits: 0
      reactions: []
    id: 647085ae9fe78d69a8adea47
    type: comment
  author: prvnsmpth
  content: '@antman1p  With "torch_dtype=torch.bfloat16", the 7B model should only
    take up ~14 GB. Here''s what my nvidia-smi looks like with the model loaded:

    ```

    +---------------------------------------------------------------------------------------+

    | NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version:
    12.1     |

    |-----------------------------------------+----------------------+----------------------+

    | GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.
    ECC |

    | Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute
    M. |

    |                                         |                      |               MIG
    M. |

    |=========================================+======================+======================|

    |   0  NVIDIA GeForce RTX 4090         Off| 00000000:01:00.0  On |                  Off
    |

    | 30%   36C    P8               10W / 450W|  14624MiB / 24564MiB |      0%      Default
    |

    |                                         |                      |                  N/A
    |

    +-----------------------------------------+----------------------+----------------------+

    ```'
  created_at: 2023-05-26 09:10:54+00:00
  edited: false
  hidden: false
  id: 647085ae9fe78d69a8adea47
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Pl0oRWq83J8IdqIAH_IhG.png?w=200&h=200&f=face
      fullname: Miles Du Heaume
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: miles-du
      type: user
    createdAt: '2023-06-01T14:42:46.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Pl0oRWq83J8IdqIAH_IhG.png?w=200&h=200&f=face
          fullname: Miles Du Heaume
          isHf: false
          isPro: false
          name: miles-du
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-06-06T15:45:14.313Z'
      numEdits: 1
      reactions: []
    id: 6478ae66ad83f3939b4f98cd
    type: comment
  author: miles-du
  content: This comment has been hidden
  created_at: 2023-06-01 13:42:46+00:00
  edited: true
  hidden: true
  id: 6478ae66ad83f3939b4f98cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-03T00:19:55.000Z'
    data:
      edited: false
      editors:
      - abhi-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8730855584144592
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: '<p>Just want to note that we added <code>device_map</code> support
          in case you have multiple smaller GPUs, in this PR: <a href="https://huggingface.co/mosaicml/mpt-7b-chat/discussions/17">https://huggingface.co/mosaicml/mpt-7b-chat/discussions/17</a></p>

          '
        raw: 'Just want to note that we added `device_map` support in case you have
          multiple smaller GPUs, in this PR: https://huggingface.co/mosaicml/mpt-7b-chat/discussions/17'
        updatedAt: '2023-06-03T00:19:55.097Z'
      numEdits: 0
      reactions: []
    id: 647a872b822b7e8ccbdcddc7
    type: comment
  author: abhi-mosaic
  content: 'Just want to note that we added `device_map` support in case you have
    multiple smaller GPUs, in this PR: https://huggingface.co/mosaicml/mpt-7b-chat/discussions/17'
  created_at: 2023-06-02 23:19:55+00:00
  edited: false
  hidden: false
  id: 647a872b822b7e8ccbdcddc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0187d993670192cd7c3b30eb280f0508.svg
      fullname: michael anslow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mbansl
      type: user
    createdAt: '2023-06-03T00:37:13.000Z'
    data:
      edited: false
      editors:
      - mbansl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.895180344581604
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0187d993670192cd7c3b30eb280f0508.svg
          fullname: michael anslow
          isHf: false
          isPro: false
          name: mbansl
          type: user
        html: '<blockquote>

          <p>I tried with adding "torch_dtype=torch.bfloat16" to the model initialization.
          It''s OK now!</p>

          </blockquote>

          <p>I''ve tried this too. It works fine for inference, but as soon as I try
          to do finetuning I get an out of memory error from back propogation. I couldn''t
          get flash or triton attention to work. Flash isn''t supported on my rtx
          3090 with the message "Expected is_sm80 || is_sm90 to be true, but got false."
          and I''m having trouble configuring my system to get triton running.</p>

          <blockquote>

          <p>/venv/lib/python3.10/site-packages/torch/autograd/<strong>init</strong>.py",
          line 204, in backward<br>    Variable._execution_engine.run_backward(  #
          Calls into the C++ engine to run the backward pass<br>torch.cuda.OutOfMemoryError:
          CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty
          of 23.70 GiB of which 131.25 MiB is free. Including non-PyTorch memory,
          this process has 23.35 GiB memory in use. Of the allocated memory 22.98
          GiB is allocated by PyTorch, and 48.60 MiB is reserved by PyTorch but unallocated.
          If reserved but unallocated memory is large try setting max_split_size_mb
          to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          </blockquote>

          '
        raw: "> I tried with adding \"torch_dtype=torch.bfloat16\" to the model initialization.\
          \ It's OK now!\n\nI've tried this too. It works fine for inference, but\
          \ as soon as I try to do finetuning I get an out of memory error from back\
          \ propogation. I couldn't get flash or triton attention to work. Flash isn't\
          \ supported on my rtx 3090 with the message \"Expected is_sm80 || is_sm90\
          \ to be true, but got false.\" and I'm having trouble configuring my system\
          \ to get triton running.\n\n> /venv/lib/python3.10/site-packages/torch/autograd/__init__.py\"\
          , line 204, in backward\n    Variable._execution_engine.run_backward(  #\
          \ Calls into the C++ engine to run the backward pass\ntorch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty\
          \ of 23.70 GiB of which 131.25 MiB is free. Including non-PyTorch memory,\
          \ this process has 23.35 GiB memory in use. Of the allocated memory 22.98\
          \ GiB is allocated by PyTorch, and 48.60 MiB is reserved by PyTorch but\
          \ unallocated. If reserved but unallocated memory is large try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
        updatedAt: '2023-06-03T00:37:13.851Z'
      numEdits: 0
      reactions: []
    id: 647a8b3942abe277476221a3
    type: comment
  author: mbansl
  content: "> I tried with adding \"torch_dtype=torch.bfloat16\" to the model initialization.\
    \ It's OK now!\n\nI've tried this too. It works fine for inference, but as soon\
    \ as I try to do finetuning I get an out of memory error from back propogation.\
    \ I couldn't get flash or triton attention to work. Flash isn't supported on my\
    \ rtx 3090 with the message \"Expected is_sm80 || is_sm90 to be true, but got\
    \ false.\" and I'm having trouble configuring my system to get triton running.\n\
    \n> /venv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 204,\
    \ in backward\n    Variable._execution_engine.run_backward(  # Calls into the\
    \ C++ engine to run the backward pass\ntorch.cuda.OutOfMemoryError: CUDA out of\
    \ memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 23.70 GiB\
    \ of which 131.25 MiB is free. Including non-PyTorch memory, this process has\
    \ 23.35 GiB memory in use. Of the allocated memory 22.98 GiB is allocated by PyTorch,\
    \ and 48.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated\
    \ memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation\
    \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
  created_at: 2023-06-02 23:37:13+00:00
  edited: false
  hidden: false
  id: 647a8b3942abe277476221a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Pl0oRWq83J8IdqIAH_IhG.png?w=200&h=200&f=face
      fullname: Miles Du Heaume
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: miles-du
      type: user
    createdAt: '2023-06-03T02:31:38.000Z'
    data:
      edited: false
      editors:
      - miles-du
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9095185995101929
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Pl0oRWq83J8IdqIAH_IhG.png?w=200&h=200&f=face
          fullname: Miles Du Heaume
          isHf: false
          isPro: false
          name: miles-du
          type: user
        html: '<p>Same here, (Triton) Errors when trying to install. Should start
          a new thread for that one.</p>

          '
        raw: Same here, (Triton) Errors when trying to install. Should start a new
          thread for that one.
        updatedAt: '2023-06-03T02:31:38.300Z'
      numEdits: 0
      reactions: []
    id: 647aa60a44b6a3ae9d2d01f8
    type: comment
  author: miles-du
  content: Same here, (Triton) Errors when trying to install. Should start a new thread
    for that one.
  created_at: 2023-06-03 01:31:38+00:00
  edited: false
  hidden: false
  id: 647aa60a44b6a3ae9d2d01f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0187d993670192cd7c3b30eb280f0508.svg
      fullname: michael anslow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mbansl
      type: user
    createdAt: '2023-06-03T13:12:59.000Z'
    data:
      edited: false
      editors:
      - mbansl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9051278233528137
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0187d993670192cd7c3b30eb280f0508.svg
          fullname: michael anslow
          isHf: false
          isPro: false
          name: mbansl
          type: user
        html: '<p>It would be nice to know how much vram we need for finetuning with
          different options e.g. torch.bfloat16, optimizer choice, etc.</p>

          '
        raw: It would be nice to know how much vram we need for finetuning with different
          options e.g. torch.bfloat16, optimizer choice, etc.
        updatedAt: '2023-06-03T13:12:59.274Z'
      numEdits: 0
      reactions: []
    id: 647b3c5be8b7333058a2de57
    type: comment
  author: mbansl
  content: It would be nice to know how much vram we need for finetuning with different
    options e.g. torch.bfloat16, optimizer choice, etc.
  created_at: 2023-06-03 12:12:59+00:00
  edited: false
  hidden: false
  id: 647b3c5be8b7333058a2de57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0187d993670192cd7c3b30eb280f0508.svg
      fullname: michael anslow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mbansl
      type: user
    createdAt: '2023-06-03T13:35:14.000Z'
    data:
      edited: false
      editors:
      - mbansl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9260520339012146
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0187d993670192cd7c3b30eb280f0508.svg
          fullname: michael anslow
          isHf: false
          isPro: false
          name: mbansl
          type: user
        html: '<p>Actually, it''s written here: <a rel="nofollow" href="https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#how-many-gpus-do-i-need-to-train-a-llm">https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#how-many-gpus-do-i-need-to-train-a-llm</a>.
          It''d take about 84 GB as a ballpark figure.</p>

          '
        raw: 'Actually, it''s written here: https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#how-many-gpus-do-i-need-to-train-a-llm.
          It''d take about 84 GB as a ballpark figure.'
        updatedAt: '2023-06-03T13:35:14.222Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - sam-mosaic
    id: 647b41926dbad6ab05791ba5
    type: comment
  author: mbansl
  content: 'Actually, it''s written here: https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#how-many-gpus-do-i-need-to-train-a-llm.
    It''d take about 84 GB as a ballpark figure.'
  created_at: 2023-06-03 12:35:14+00:00
  edited: false
  hidden: false
  id: 647b41926dbad6ab05791ba5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-06-14T07:10:59.000Z'
    data:
      status: closed
    id: 6489680391e3ed1a12f8b6ef
    type: status-change
  author: sam-mosaic
  created_at: 2023-06-14 06:10:59+00:00
  id: 6489680391e3ed1a12f8b6ef
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: mosaicml/mpt-7b-chat
repo_type: model
status: closed
target_branch: null
title: Out of memory error with an RTX 4090
