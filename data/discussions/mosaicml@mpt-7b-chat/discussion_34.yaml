!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RonanMcGovern
conflicting_files: null
created_at: 2023-07-20 16:07:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-07-20T17:07:27.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9345543384552002
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>I''m wondering why this model is CC-By-NC-SA-4.0 (non-commercial
          use only)?</p>

          <p>I suppose it''s because of the alpaca and HC3 datasets used for fine
          tuning.</p>

          <p>What was the thinking behind that? Were the datasets used for mpt-7b-instruct
          insufficient for being able to achieve chat-type fine tuning?</p>

          <p>Is there information available on how the fine tuning was done and the
          prompt format? Were any multiple response formats used (i.e. user, then
          assistant, then user, then assistant). Thanks</p>

          '
        raw: "I'm wondering why this model is CC-By-NC-SA-4.0 (non-commercial use\
          \ only)?\r\n\r\nI suppose it's because of the alpaca and HC3 datasets used\
          \ for fine tuning.\r\n\r\nWhat was the thinking behind that? Were the datasets\
          \ used for mpt-7b-instruct insufficient for being able to achieve chat-type\
          \ fine tuning?\r\n\r\nIs there information available on how the fine tuning\
          \ was done and the prompt format? Were any multiple response formats used\
          \ (i.e. user, then assistant, then user, then assistant). Thanks"
        updatedAt: '2023-07-20T17:07:27.940Z'
      numEdits: 0
      reactions: []
    id: 64b969cfbfec89ab32a0c2cb
    type: comment
  author: RonanMcGovern
  content: "I'm wondering why this model is CC-By-NC-SA-4.0 (non-commercial use only)?\r\
    \n\r\nI suppose it's because of the alpaca and HC3 datasets used for fine tuning.\r\
    \n\r\nWhat was the thinking behind that? Were the datasets used for mpt-7b-instruct\
    \ insufficient for being able to achieve chat-type fine tuning?\r\n\r\nIs there\
    \ information available on how the fine tuning was done and the prompt format?\
    \ Were any multiple response formats used (i.e. user, then assistant, then user,\
    \ then assistant). Thanks"
  created_at: 2023-07-20 16:07:27+00:00
  edited: false
  hidden: false
  id: 64b969cfbfec89ab32a0c2cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-07-20T18:26:57.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9322092533111572
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: "<p>We have explained the license, see for example <a href=\"https://huggingface.co/mosaicml/mpt-7b-chat/discussions/15\"\
          >https://huggingface.co/mosaicml/mpt-7b-chat/discussions/15</a></p>\n<p>\"\
          Were the datasets used for mpt-7b-instruct insufficient for being able to\
          \ achieve chat-type fine tuning?\"  \u2014 See the <a rel=\"nofollow\" href=\"\
          https://arxiv.org/abs/2305.11206\">LIMA</a> paper, Instruct could probably\
          \ be adapted to chat without too many samples! We decided to build 2 models\
          \ to do 2 different things: Instruct is meant to be immediately valuable\
          \ for commercial use, chat is meant to be the highest quality model we can\
          \ build regardless of license.</p>\n<p>Prompt format used to be public but\
          \ we took the Spaces down, though you can see the format in the <a rel=\"\
          nofollow\" href=\"https://github.com/mosaicml/llm-foundry/blob/main/scripts/inference/hf_chat.py\"\
          >inference script</a>, we will have to document that again. The format was\
          \ ChatML, n-turn conversations were turned into n samples, so each response\
          \ is loss-generating once.</p>\n<p>The training details are in the readme,\
          \ <a href=\"https://huggingface.co/mosaicml/mpt-7b-chat#training-configuration\"\
          >https://huggingface.co/mosaicml/mpt-7b-chat#training-configuration</a></p>\n"
        raw: "We have explained the license, see for example https://huggingface.co/mosaicml/mpt-7b-chat/discussions/15\n\
          \n\"Were the datasets used for mpt-7b-instruct insufficient for being able\
          \ to achieve chat-type fine tuning?\"  \u2014 See the [LIMA](https://arxiv.org/abs/2305.11206)\
          \ paper, Instruct could probably be adapted to chat without too many samples!\
          \ We decided to build 2 models to do 2 different things: Instruct is meant\
          \ to be immediately valuable for commercial use, chat is meant to be the\
          \ highest quality model we can build regardless of license.\n\nPrompt format\
          \ used to be public but we took the Spaces down, though you can see the\
          \ format in the [inference script](https://github.com/mosaicml/llm-foundry/blob/main/scripts/inference/hf_chat.py),\
          \ we will have to document that again. The format was ChatML, n-turn conversations\
          \ were turned into n samples, so each response is loss-generating once.\n\
          \nThe training details are in the readme, https://huggingface.co/mosaicml/mpt-7b-chat#training-configuration"
        updatedAt: '2023-07-20T18:26:57.026Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - RonanMcGovern
      relatedEventId: 64b97c7110430817356cc53f
    id: 64b97c7110430817356cc53a
    type: comment
  author: sam-mosaic
  content: "We have explained the license, see for example https://huggingface.co/mosaicml/mpt-7b-chat/discussions/15\n\
    \n\"Were the datasets used for mpt-7b-instruct insufficient for being able to\
    \ achieve chat-type fine tuning?\"  \u2014 See the [LIMA](https://arxiv.org/abs/2305.11206)\
    \ paper, Instruct could probably be adapted to chat without too many samples!\
    \ We decided to build 2 models to do 2 different things: Instruct is meant to\
    \ be immediately valuable for commercial use, chat is meant to be the highest\
    \ quality model we can build regardless of license.\n\nPrompt format used to be\
    \ public but we took the Spaces down, though you can see the format in the [inference\
    \ script](https://github.com/mosaicml/llm-foundry/blob/main/scripts/inference/hf_chat.py),\
    \ we will have to document that again. The format was ChatML, n-turn conversations\
    \ were turned into n samples, so each response is loss-generating once.\n\nThe\
    \ training details are in the readme, https://huggingface.co/mosaicml/mpt-7b-chat#training-configuration"
  created_at: 2023-07-20 17:26:57+00:00
  edited: false
  hidden: false
  id: 64b97c7110430817356cc53a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-07-20T18:26:57.000Z'
    data:
      status: closed
    id: 64b97c7110430817356cc53f
    type: status-change
  author: sam-mosaic
  created_at: 2023-07-20 17:26:57+00:00
  id: 64b97c7110430817356cc53f
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-07-24T17:21:42.000Z'
    data:
      edited: true
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7247635722160339
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sam-mosaic&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sam-mosaic\">@<span class=\"\
          underline\">sam-mosaic</span></a></span>\n\n\t</span></span> many thanks!</p>\n\
          <p>To be sure I understand... you're using |im_start| and |im_end| to wrap\
          \ each system/user/assistant response individually (but I assume you're\
          \ then using BOS and EOS tokens to wrap each full turn (i.e. system + user\
          \ + assistant) as well:</p>\n<pre><code>    def __init__(self, system: str,\
          \ user: str, assistant: str) -&gt; None:\n        self.system = system if\
          \ system else '&lt;|im_start|&gt;system\\nA conversation between a user\
          \ and an LLM-based AI assistant. The assistant gives helpful and honest\
          \ answers.&lt;|im_end|&gt;\\n'\n        self.user = user if user else '&lt;|im_start|&gt;user\\\
          n{}&lt;|im_end|&gt;\\n'\n        self.assistant = assistant if assistant\
          \ else '&lt;|im_start|&gt;assistant\\n{}&lt;|im_end|&gt;\\n'\n        self.response_prefix\
          \ = self.assistant.split('{}')[0]\n</code></pre>\n"
        raw: "@sam-mosaic many thanks!\n\nTo be sure I understand... you're using\
          \ |im_start| and |im_end| to wrap each system/user/assistant response individually\
          \ (but I assume you're then using BOS and EOS tokens to wrap each full turn\
          \ (i.e. system + user + assistant) as well:\n\n```\n    def __init__(self,\
          \ system: str, user: str, assistant: str) -> None:\n        self.system\
          \ = system if system else '<|im_start|>system\\nA conversation between a\
          \ user and an LLM-based AI assistant. The assistant gives helpful and honest\
          \ answers.<|im_end|>\\n'\n        self.user = user if user else '<|im_start|>user\\\
          n{}<|im_end|>\\n'\n        self.assistant = assistant if assistant else\
          \ '<|im_start|>assistant\\n{}<|im_end|>\\n'\n        self.response_prefix\
          \ = self.assistant.split('{}')[0]\n```"
        updatedAt: '2023-07-24T17:22:50.079Z'
      numEdits: 1
      reactions: []
    id: 64beb326afd1e46c553c8c2d
    type: comment
  author: RonanMcGovern
  content: "@sam-mosaic many thanks!\n\nTo be sure I understand... you're using |im_start|\
    \ and |im_end| to wrap each system/user/assistant response individually (but I\
    \ assume you're then using BOS and EOS tokens to wrap each full turn (i.e. system\
    \ + user + assistant) as well:\n\n```\n    def __init__(self, system: str, user:\
    \ str, assistant: str) -> None:\n        self.system = system if system else '<|im_start|>system\\\
    nA conversation between a user and an LLM-based AI assistant. The assistant gives\
    \ helpful and honest answers.<|im_end|>\\n'\n        self.user = user if user\
    \ else '<|im_start|>user\\n{}<|im_end|>\\n'\n        self.assistant = assistant\
    \ if assistant else '<|im_start|>assistant\\n{}<|im_end|>\\n'\n        self.response_prefix\
    \ = self.assistant.split('{}')[0]\n```"
  created_at: 2023-07-24 16:21:42+00:00
  edited: true
  hidden: false
  id: 64beb326afd1e46c553c8c2d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 34
repo_id: mosaicml/mpt-7b-chat
repo_type: model
status: closed
target_branch: null
title: How was fine-tuning done?
