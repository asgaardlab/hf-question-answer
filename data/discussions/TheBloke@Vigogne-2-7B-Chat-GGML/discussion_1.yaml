!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LeMoussel
conflicting_files: null
created_at: 2023-08-31 09:48:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
      fullname: LeMoussel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LeMoussel
      type: user
    createdAt: '2023-08-31T10:48:10.000Z'
    data:
      edited: false
      editors:
      - LeMoussel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5102561116218567
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
          fullname: LeMoussel
          isHf: false
          isPro: false
          name: LeMoussel
          type: user
        html: "<pre><code class=\"language-python\"><span class=\"hljs-comment\">#\
          \ https://llama-cpp-python.readthedocs.io/en/latest/</span>\n<span class=\"\
          hljs-keyword\">from</span> llama_cpp <span class=\"hljs-keyword\">import</span>\
          \ Llama\n\n<span class=\"hljs-comment\"># https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGML/tree/main</span>\n\
          MODEL_Q8_0 = Llama(model_path=<span class=\"hljs-string\">\"./models/vigogne-2-7b-chat.ggmlv3.q8_0.bin\"\
          </span>, n_ctx=<span class=\"hljs-number\">512</span>)\n</code></pre>\n\
          <p>I got this error:</p>\n<pre><code>AssertionError                    \
          \        Traceback (most recent call last)\n\n&lt;ipython-input-4-ad0fd4308e15&gt;\
          \ in &lt;cell line: 5&gt;()\n      4 # https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGML/tree/main\n\
          ----&gt; 5 MODEL_Q8_0 = Llama(model_path=\"./models/vigogne-2-7b-chat.ggmlv3.q8_0.bin\"\
          , n_ctx=512)\n\n/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\
          \ in __init__(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv,\
          \ logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch,\
          \ last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split, rope_freq_base,\
          \ rope_freq_scale, n_gqa, rms_norm_eps, mul_mat_q, verbose)\n    321   \
          \                  self.model_path.encode(\"utf-8\"), self.params\n    322\
          \                 )\n--&gt; 323         assert self.model is not None\n\
          \    324 \n    325         if verbose:\n\nAssertionError: \n</code></pre>\n\
          <p>it seems that <code>llama_cpp.llama_load_model_from_file(...)</code>\
          \ failed =&gt; <code>self.model</code> is None.<br>Is there any known solution?<br>Thanks!</p>\n"
        raw: "```python\r\n# https://llama-cpp-python.readthedocs.io/en/latest/\r\n\
          from llama_cpp import Llama\r\n\r\n# https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGML/tree/main\r\
          \nMODEL_Q8_0 = Llama(model_path=\"./models/vigogne-2-7b-chat.ggmlv3.q8_0.bin\"\
          , n_ctx=512)\r\n``` \r\n\r\nI got this error:\r\n```\r\nAssertionError \
          \                           Traceback (most recent call last)\r\n\r\n<ipython-input-4-ad0fd4308e15>\
          \ in <cell line: 5>()\r\n      4 # https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGML/tree/main\r\
          \n----> 5 MODEL_Q8_0 = Llama(model_path=\"./models/vigogne-2-7b-chat.ggmlv3.q8_0.bin\"\
          , n_ctx=512)\r\n\r\n/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\
          \ in __init__(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv,\
          \ logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch,\
          \ last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split, rope_freq_base,\
          \ rope_freq_scale, n_gqa, rms_norm_eps, mul_mat_q, verbose)\r\n    321 \
          \                    self.model_path.encode(\"utf-8\"), self.params\r\n\
          \    322                 )\r\n--> 323         assert self.model is not None\r\
          \n    324 \r\n    325         if verbose:\r\n\r\nAssertionError: \r\n```\r\
          \nit seems that `llama_cpp.llama_load_model_from_file(...)` failed => `self.model`\
          \ is None.\r\nIs there any known solution?\r\nThanks!"
        updatedAt: '2023-08-31T10:48:10.324Z'
      numEdits: 0
      reactions: []
    id: 64f06feaa20e6096c6aa752b
    type: comment
  author: LeMoussel
  content: "```python\r\n# https://llama-cpp-python.readthedocs.io/en/latest/\r\n\
    from llama_cpp import Llama\r\n\r\n# https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGML/tree/main\r\
    \nMODEL_Q8_0 = Llama(model_path=\"./models/vigogne-2-7b-chat.ggmlv3.q8_0.bin\"\
    , n_ctx=512)\r\n``` \r\n\r\nI got this error:\r\n```\r\nAssertionError       \
    \                     Traceback (most recent call last)\r\n\r\n<ipython-input-4-ad0fd4308e15>\
    \ in <cell line: 5>()\r\n      4 # https://huggingface.co/TheBloke/Vigogne-2-7B-Chat-GGML/tree/main\r\
    \n----> 5 MODEL_Q8_0 = Llama(model_path=\"./models/vigogne-2-7b-chat.ggmlv3.q8_0.bin\"\
    , n_ctx=512)\r\n\r\n/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\
    \ in __init__(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv, logits_all,\
    \ vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size,\
    \ lora_base, lora_path, low_vram, tensor_split, rope_freq_base, rope_freq_scale,\
    \ n_gqa, rms_norm_eps, mul_mat_q, verbose)\r\n    321                     self.model_path.encode(\"\
    utf-8\"), self.params\r\n    322                 )\r\n--> 323         assert self.model\
    \ is not None\r\n    324 \r\n    325         if verbose:\r\n\r\nAssertionError:\
    \ \r\n```\r\nit seems that `llama_cpp.llama_load_model_from_file(...)` failed\
    \ => `self.model` is None.\r\nIs there any known solution?\r\nThanks!"
  created_at: 2023-08-31 09:48:10+00:00
  edited: false
  hidden: false
  id: 64f06feaa20e6096c6aa752b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Vigogne-2-7B-Chat-GGML
repo_type: model
status: open
target_branch: null
title: llama-cpp-python 'AssertionError' when loading model
