!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Orick1
conflicting_files: null
created_at: 2023-05-10 12:28:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e642f8c18a0892566e86a869aad3041c.svg
      fullname: Orick1
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Orick1
      type: user
    createdAt: '2023-05-10T13:28:14.000Z'
    data:
      edited: false
      editors:
      - Orick1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e642f8c18a0892566e86a869aad3041c.svg
          fullname: Orick1
          isHf: false
          isPro: false
          name: Orick1
          type: user
        html: '<p>Trying to run it GGML version with oobaabooga, and fails missing
          config.json?</p>

          '
        raw: Trying to run it GGML version with oobaabooga, and fails missing config.json?
        updatedAt: '2023-05-10T13:28:14.013Z'
      numEdits: 0
      reactions: []
    id: 645b9bee87c79b6ec0bb9f1b
    type: comment
  author: Orick1
  content: Trying to run it GGML version with oobaabooga, and fails missing config.json?
  created_at: 2023-05-10 12:28:14+00:00
  edited: false
  hidden: false
  id: 645b9bee87c79b6ec0bb9f1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
      fullname: TeH_Venom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TehVenom
      type: user
    createdAt: '2023-05-10T13:38:49.000Z'
    data:
      edited: false
      editors:
      - TehVenom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
          fullname: TeH_Venom
          isHf: false
          isPro: false
          name: TehVenom
          type: user
        html: '<p>Does ooba even support GGML models? The only file llama.cpp needs
          to work is the .bin file itself.</p>

          '
        raw: Does ooba even support GGML models? The only file llama.cpp needs to
          work is the .bin file itself.
        updatedAt: '2023-05-10T13:38:49.398Z'
      numEdits: 0
      reactions: []
    id: 645b9e690120c98d16a7a2be
    type: comment
  author: TehVenom
  content: Does ooba even support GGML models? The only file llama.cpp needs to work
    is the .bin file itself.
  created_at: 2023-05-10 12:38:49+00:00
  edited: false
  hidden: false
  id: 645b9e690120c98d16a7a2be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e642f8c18a0892566e86a869aad3041c.svg
      fullname: Orick1
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Orick1
      type: user
    createdAt: '2023-05-10T14:50:10.000Z'
    data:
      edited: false
      editors:
      - Orick1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e642f8c18a0892566e86a869aad3041c.svg
          fullname: Orick1
          isHf: false
          isPro: false
          name: Orick1
          type: user
        html: '<p>yes ooba supports GGML models. I only have 8 GB VRAM so can only
          run 13b models when they are GGML.</p>

          '
        raw: yes ooba supports GGML models. I only have 8 GB VRAM so can only run
          13b models when they are GGML.
        updatedAt: '2023-05-10T14:50:10.727Z'
      numEdits: 0
      reactions: []
    id: 645baf228bbb8592d9180bbb
    type: comment
  author: Orick1
  content: yes ooba supports GGML models. I only have 8 GB VRAM so can only run 13b
    models when they are GGML.
  created_at: 2023-05-10 13:50:10+00:00
  edited: false
  hidden: false
  id: 645baf228bbb8592d9180bbb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/5yayanOxol4hi67ORIN5S.jpeg?w=200&h=200&f=face
      fullname: S. Hall
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TravelingMan
      type: user
    createdAt: '2023-05-10T16:00:21.000Z'
    data:
      edited: false
      editors:
      - TravelingMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/5yayanOxol4hi67ORIN5S.jpeg?w=200&h=200&f=face
          fullname: S. Hall
          isHf: false
          isPro: false
          name: TravelingMan
          type: user
        html: '<p>The model works great on llama.cpp and koboldCpp. The version of
          llama.cpp included in ooba might be too outdated to use the newer 5.1 quantization
          method, or maybe you''re on an older version? I imagine they''ll update
          it soon-ish if that''s the case.</p>

          '
        raw: The model works great on llama.cpp and koboldCpp. The version of llama.cpp
          included in ooba might be too outdated to use the newer 5.1 quantization
          method, or maybe you're on an older version? I imagine they'll update it
          soon-ish if that's the case.
        updatedAt: '2023-05-10T16:00:21.453Z'
      numEdits: 0
      reactions: []
    id: 645bbf95c971fbab74214822
    type: comment
  author: TravelingMan
  content: The model works great on llama.cpp and koboldCpp. The version of llama.cpp
    included in ooba might be too outdated to use the newer 5.1 quantization method,
    or maybe you're on an older version? I imagine they'll update it soon-ish if that's
    the case.
  created_at: 2023-05-10 15:00:21+00:00
  edited: false
  hidden: false
  id: 645bbf95c971fbab74214822
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76fa4741c601ea7047f57ada14a80105.svg
      fullname: Pontus Adler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mithrid
      type: user
    createdAt: '2023-05-10T17:22:47.000Z'
    data:
      edited: false
      editors:
      - mithrid
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76fa4741c601ea7047f57ada14a80105.svg
          fullname: Pontus Adler
          isHf: false
          isPro: false
          name: mithrid
          type: user
        html: '<p>Found a fix to get the model wowroking with oobaabooga.<br>The model
          has to have "ggml" in the filename. Simply rename the model with those letters
          in there and it works.<br>found this in the oobaabooga llama.cpp instructions.</p>

          '
        raw: 'Found a fix to get the model wowroking with oobaabooga.

          The model has to have "ggml" in the filename. Simply rename the model with
          those letters in there and it works.

          found this in the oobaabooga llama.cpp instructions.'
        updatedAt: '2023-05-10T17:22:47.152Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\u2764\uFE0F"
        users:
        - Hanssep123
        - D2689
        - mJBHY7L
        - Orick1
        - NameisNam
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Orick1
    id: 645bd2e7c971fbab7421f816
    type: comment
  author: mithrid
  content: 'Found a fix to get the model wowroking with oobaabooga.

    The model has to have "ggml" in the filename. Simply rename the model with those
    letters in there and it works.

    found this in the oobaabooga llama.cpp instructions.'
  created_at: 2023-05-10 16:22:47+00:00
  edited: false
  hidden: false
  id: 645bd2e7c971fbab7421f816
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e642f8c18a0892566e86a869aad3041c.svg
      fullname: Orick1
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Orick1
      type: user
    createdAt: '2023-05-10T20:23:34.000Z'
    data:
      edited: false
      editors:
      - Orick1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e642f8c18a0892566e86a869aad3041c.svg
          fullname: Orick1
          isHf: false
          isPro: false
          name: Orick1
          type: user
        html: '<p>I still have the same problem after renaming the file to "ggml".  But
          I did manage to get Koboldcpp to work.  Still would prefer ooba though.</p>

          '
        raw: I still have the same problem after renaming the file to "ggml".  But
          I did manage to get Koboldcpp to work.  Still would prefer ooba though.
        updatedAt: '2023-05-10T20:23:34.628Z'
      numEdits: 0
      reactions: []
    id: 645bfd460120c98d16ab5a6f
    type: comment
  author: Orick1
  content: I still have the same problem after renaming the file to "ggml".  But I
    did manage to get Koboldcpp to work.  Still would prefer ooba though.
  created_at: 2023-05-10 19:23:34+00:00
  edited: false
  hidden: false
  id: 645bfd460120c98d16ab5a6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/5yayanOxol4hi67ORIN5S.jpeg?w=200&h=200&f=face
      fullname: S. Hall
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TravelingMan
      type: user
    createdAt: '2023-05-10T21:41:15.000Z'
    data:
      edited: false
      editors:
      - TravelingMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/5yayanOxol4hi67ORIN5S.jpeg?w=200&h=200&f=face
          fullname: S. Hall
          isHf: false
          isPro: false
          name: TravelingMan
          type: user
        html: '<p>You can use KoboldCpp as a backend, then use a custom front-end
          such as TavernAI or whatever. The only difference is that the default KoboldCpp
          port is <code>:5001</code> instead of <code>:5000</code>, but you can change
          it to whatever you like with a launch parameter.</p>

          '
        raw: You can use KoboldCpp as a backend, then use a custom front-end such
          as TavernAI or whatever. The only difference is that the default KoboldCpp
          port is `:5001` instead of `:5000`, but you can change it to whatever you
          like with a launch parameter.
        updatedAt: '2023-05-10T21:41:15.346Z'
      numEdits: 0
      reactions: []
    id: 645c0f7b0120c98d16abe8b5
    type: comment
  author: TravelingMan
  content: You can use KoboldCpp as a backend, then use a custom front-end such as
    TavernAI or whatever. The only difference is that the default KoboldCpp port is
    `:5001` instead of `:5000`, but you can change it to whatever you like with a
    launch parameter.
  created_at: 2023-05-10 20:41:15+00:00
  edited: false
  hidden: false
  id: 645c0f7b0120c98d16abe8b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e642f8c18a0892566e86a869aad3041c.svg
      fullname: Orick1
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Orick1
      type: user
    createdAt: '2023-05-11T16:10:01.000Z'
    data:
      edited: false
      editors:
      - Orick1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e642f8c18a0892566e86a869aad3041c.svg
          fullname: Orick1
          isHf: false
          isPro: false
          name: Orick1
          type: user
        html: '<p>mithrid was right. I made a mistake and changed the "bin" part of
          the file name to "ggml". I should have just left it as a bin file and just
          added "ggml" somewhere like "WizardML-Unc-13b-Q5_1.ggml.bin".  It''s working
          now.</p>

          '
        raw: mithrid was right. I made a mistake and changed the "bin" part of the
          file name to "ggml". I should have just left it as a bin file and just added
          "ggml" somewhere like "WizardML-Unc-13b-Q5_1.ggml.bin".  It's working now.
        updatedAt: '2023-05-11T16:10:01.885Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mithrid
    id: 645d1359f1e3b219cb0b9f7b
    type: comment
  author: Orick1
  content: mithrid was right. I made a mistake and changed the "bin" part of the file
    name to "ggml". I should have just left it as a bin file and just added "ggml"
    somewhere like "WizardML-Unc-13b-Q5_1.ggml.bin".  It's working now.
  created_at: 2023-05-11 15:10:01+00:00
  edited: false
  hidden: false
  id: 645d1359f1e3b219cb0b9f7b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TehVenom/WizardLM-13B-Uncensored-Q5_1-GGML
repo_type: model
status: open
target_branch: null
title: missing config.json?
