!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nightcall
conflicting_files: null
created_at: 2023-06-24 14:16:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4e24b64dbc4bcbd5d4b5e232aa814d23.svg
      fullname: Night Call
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nightcall
      type: user
    createdAt: '2023-06-24T15:16:51.000Z'
    data:
      edited: false
      editors:
      - Nightcall
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8557891845703125
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4e24b64dbc4bcbd5d4b5e232aa814d23.svg
          fullname: Night Call
          isHf: false
          isPro: false
          name: Nightcall
          type: user
        html: '<p>Hi, thank you very much for your quantizations. In the prompt template
          example you forgot to include the assistant part of the template:<br>&lt;|im_start|&gt;assistant<br>Assistant
          response&lt;|im_end|&gt;</p>

          <p>Dataset uses ChatML format: <a rel="nofollow" href="https://www.mosaicml.com/blog/mpt-30b#:~:text=The%20dataset%20uses%20the%20ChatML%20format%2C">https://www.mosaicml.com/blog/mpt-30b#:~:text=The%20dataset%20uses%20the%20ChatML%20format%2C</a><br>ChatML:  <a
          rel="nofollow" href="https://github.com/openai/openai-python/blob/main/chatml.md">https://github.com/openai/openai-python/blob/main/chatml.md</a></p>

          <p>Example:<br>&lt;|im_start|&gt;system<br>You are ChatGPT, a large language
          model trained by OpenAI. Answer as concisely as possible.<br>Knowledge cutoff:
          2021-09-01<br>Current date: 2023-03-01&lt;|im_end|&gt;<br>&lt;|im_start|&gt;user<br>How
          are you&lt;|im_end|&gt;<br>&lt;|im_start|&gt;assistant<br>I am doing well!&lt;|im_end|&gt;<br>&lt;|im_start|&gt;user<br>How
          are you now?&lt;|im_end|&gt;</p>

          '
        raw: "Hi, thank you very much for your quantizations. In the prompt template\
          \ example you forgot to include the assistant part of the template:  \r\n\
          <|im_start|>assistant\r\nAssistant response<|im_end|>\r\n\r\nDataset uses\
          \ ChatML format: https://www.mosaicml.com/blog/mpt-30b#:~:text=The%20dataset%20uses%20the%20ChatML%20format%2C\r\
          \nChatML:  https://github.com/openai/openai-python/blob/main/chatml.md\r\
          \n\r\nExample: \r\n<|im_start|>system\r\nYou are ChatGPT, a large language\
          \ model trained by OpenAI. Answer as concisely as possible.\r\nKnowledge\
          \ cutoff: 2021-09-01\r\nCurrent date: 2023-03-01<|im_end|>\r\n<|im_start|>user\r\
          \nHow are you<|im_end|>\r\n<|im_start|>assistant\r\nI am doing well!<|im_end|>\r\
          \n<|im_start|>user\r\nHow are you now?<|im_end|>"
        updatedAt: '2023-06-24T15:16:51.705Z'
      numEdits: 0
      reactions: []
    id: 649708e3457f60023c786fd2
    type: comment
  author: Nightcall
  content: "Hi, thank you very much for your quantizations. In the prompt template\
    \ example you forgot to include the assistant part of the template:  \r\n<|im_start|>assistant\r\
    \nAssistant response<|im_end|>\r\n\r\nDataset uses ChatML format: https://www.mosaicml.com/blog/mpt-30b#:~:text=The%20dataset%20uses%20the%20ChatML%20format%2C\r\
    \nChatML:  https://github.com/openai/openai-python/blob/main/chatml.md\r\n\r\n\
    Example: \r\n<|im_start|>system\r\nYou are ChatGPT, a large language model trained\
    \ by OpenAI. Answer as concisely as possible.\r\nKnowledge cutoff: 2021-09-01\r\
    \nCurrent date: 2023-03-01<|im_end|>\r\n<|im_start|>user\r\nHow are you<|im_end|>\r\
    \n<|im_start|>assistant\r\nI am doing well!<|im_end|>\r\n<|im_start|>user\r\n\
    How are you now?<|im_end|>"
  created_at: 2023-06-24 14:16:51+00:00
  edited: false
  hidden: false
  id: 649708e3457f60023c786fd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
      fullname: Jan Badertscher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: underlines
      type: user
    createdAt: '2023-06-24T21:38:30.000Z'
    data:
      edited: true
      editors:
      - underlines
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8205035924911499
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
          fullname: Jan Badertscher
          isHf: false
          isPro: false
          name: underlines
          type: user
        html: '<blockquote>

          <p>Hi, thank you very much for your quantizations. In the prompt template
          example you forgot to include the assistant part of the template:<br>&lt;|im_start|&gt;assistant<br>Assistant
          response&lt;|im_end|&gt;</p>

          <p>Dataset uses ChatML format: <a rel="nofollow" href="https://www.mosaicml.com/blog/mpt-30b#:~:text=The%20dataset%20uses%20the%20ChatML%20format%2C">https://www.mosaicml.com/blog/mpt-30b#:~:text=The%20dataset%20uses%20the%20ChatML%20format%2C</a><br>ChatML:  <a
          rel="nofollow" href="https://github.com/openai/openai-python/blob/main/chatml.md">https://github.com/openai/openai-python/blob/main/chatml.md</a></p>

          <p>Example:<br>&lt;|im_start|&gt;system<br>You are ChatGPT, a large language
          model trained by OpenAI. Answer as concisely as possible.<br>Knowledge cutoff:
          2021-09-01<br>Current date: 2023-03-01&lt;|im_end|&gt;<br>&lt;|im_start|&gt;user<br>How
          are you&lt;|im_end|&gt;<br>&lt;|im_start|&gt;assistant<br>I am doing well!&lt;|im_end|&gt;<br>&lt;|im_start|&gt;user<br>How
          are you now?&lt;|im_end|&gt;</p>

          </blockquote>

          <p>exactly, and I think it looks like this in the ooba template:</p>

          <pre><code>user: "user"

          bot:  "assistant"

          turn_template: "&lt;|im_start|&gt;&lt;|user|&gt;\n&lt;|user-message|&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;&lt;|bot|&gt;\n&lt;|bot-message|&gt;&lt;|im_end|&gt;\n"

          context: "&lt;|im_start|&gt;system\nA conversation between a user and an
          LLM-based AI assistant. The assistant gives helpful and honest answers.&lt;|im_end|&gt;"

          </code></pre>

          '
        raw: "> Hi, thank you very much for your quantizations. In the prompt template\
          \ example you forgot to include the assistant part of the template:  \n\
          > <|im_start|>assistant\n> Assistant response<|im_end|>\n> \n> Dataset uses\
          \ ChatML format: https://www.mosaicml.com/blog/mpt-30b#:~:text=The%20dataset%20uses%20the%20ChatML%20format%2C\n\
          > ChatML:  https://github.com/openai/openai-python/blob/main/chatml.md\n\
          > \n> Example: \n> <|im_start|>system\n> You are ChatGPT, a large language\
          \ model trained by OpenAI. Answer as concisely as possible.\n> Knowledge\
          \ cutoff: 2021-09-01\n> Current date: 2023-03-01<|im_end|>\n> <|im_start|>user\n\
          > How are you<|im_end|>\n> <|im_start|>assistant\n> I am doing well!<|im_end|>\n\
          > <|im_start|>user\n> How are you now?<|im_end|>\n\nexactly, and I think\
          \ it looks like this in the ooba template:\n\n```\nuser: \"user\"\nbot:\
          \  \"assistant\"\nturn_template: \"<|im_start|><|user|>\\n<|user-message|><|im_end|>\\\
          n<|im_start|><|bot|>\\n<|bot-message|><|im_end|>\\n\"\ncontext: \"<|im_start|>system\\\
          nA conversation between a user and an LLM-based AI assistant. The assistant\
          \ gives helpful and honest answers.<|im_end|>\"\n```"
        updatedAt: '2023-06-24T21:39:19.876Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nightcall
    id: 649762567fbd52a328188fa5
    type: comment
  author: underlines
  content: "> Hi, thank you very much for your quantizations. In the prompt template\
    \ example you forgot to include the assistant part of the template:  \n> <|im_start|>assistant\n\
    > Assistant response<|im_end|>\n> \n> Dataset uses ChatML format: https://www.mosaicml.com/blog/mpt-30b#:~:text=The%20dataset%20uses%20the%20ChatML%20format%2C\n\
    > ChatML:  https://github.com/openai/openai-python/blob/main/chatml.md\n> \n>\
    \ Example: \n> <|im_start|>system\n> You are ChatGPT, a large language model trained\
    \ by OpenAI. Answer as concisely as possible.\n> Knowledge cutoff: 2021-09-01\n\
    > Current date: 2023-03-01<|im_end|>\n> <|im_start|>user\n> How are you<|im_end|>\n\
    > <|im_start|>assistant\n> I am doing well!<|im_end|>\n> <|im_start|>user\n> How\
    \ are you now?<|im_end|>\n\nexactly, and I think it looks like this in the ooba\
    \ template:\n\n```\nuser: \"user\"\nbot:  \"assistant\"\nturn_template: \"<|im_start|><|user|>\\\
    n<|user-message|><|im_end|>\\n<|im_start|><|bot|>\\n<|bot-message|><|im_end|>\\\
    n\"\ncontext: \"<|im_start|>system\\nA conversation between a user and an LLM-based\
    \ AI assistant. The assistant gives helpful and honest answers.<|im_end|>\"\n\
    ```"
  created_at: 2023-06-24 20:38:30+00:00
  edited: true
  hidden: false
  id: 649762567fbd52a328188fa5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-24T21:42:18.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9308555722236633
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thanks guys.  So specifically what I missed was just</p>

          <pre><code>&lt;|im_start|&gt;assistant

          </code></pre>

          <p>is that right? Because then the LLM would generate its own <code>&lt;|im_end|&gt;</code>?</p>

          <p>So is this correct now?</p>

          <pre><code>&lt;|im_start|&gt;system

          A conversation between a user and an LLM-based AI assistant. The assistant
          gives helpful and honest answers.&lt;|im_end|&gt;

          &lt;|im_start|&gt;user

          prompt goes here&lt;|im_end|&gt;

          &lt;|im_start|&gt;assistant

          </code></pre>

          '
        raw: 'Thanks guys.  So specifically what I missed was just

          ```

          <|im_start|>assistant

          ```

          is that right? Because then the LLM would generate its own `<|im_end|>`?


          So is this correct now?


          ```

          <|im_start|>system

          A conversation between a user and an LLM-based AI assistant. The assistant
          gives helpful and honest answers.<|im_end|>

          <|im_start|>user

          prompt goes here<|im_end|>

          <|im_start|>assistant

          ```

          '
        updatedAt: '2023-06-24T21:42:18.747Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Nightcall
        - Suparious
    id: 6497633aa9c42bc6a84fcbec
    type: comment
  author: TheBloke
  content: 'Thanks guys.  So specifically what I missed was just

    ```

    <|im_start|>assistant

    ```

    is that right? Because then the LLM would generate its own `<|im_end|>`?


    So is this correct now?


    ```

    <|im_start|>system

    A conversation between a user and an LLM-based AI assistant. The assistant gives
    helpful and honest answers.<|im_end|>

    <|im_start|>user

    prompt goes here<|im_end|>

    <|im_start|>assistant

    ```

    '
  created_at: 2023-06-24 20:42:18+00:00
  edited: false
  hidden: false
  id: 6497633aa9c42bc6a84fcbec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cbc0ceb6b221b828a3a030ce02e46f26.svg
      fullname: Alex Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: swulling
      type: user
    createdAt: '2023-06-25T09:59:44.000Z'
    data:
      edited: false
      editors:
      - swulling
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7881537079811096
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cbc0ceb6b221b828a3a030ce02e46f26.svg
          fullname: Alex Yang
          isHf: false
          isPro: false
          name: swulling
          type: user
        html: '<blockquote>

          <p>So is this correct now?</p>

          <pre><code>&lt;|im_start|&gt;system

          A conversation between a user and an LLM-based AI assistant. The assistant
          gives helpful and honest answers.&lt;|im_end|&gt;

          &lt;|im_start|&gt;user

          prompt goes here&lt;|im_end|&gt;

          &lt;|im_start|&gt;assistant

          </code></pre>

          </blockquote>

          <p>I think this is correct. Refer to the official Demo prompt:</p>

          <p><a href="https://huggingface.co/spaces/mosaicml/mpt-30b-chat/blob/main/app.py#L49">https://huggingface.co/spaces/mosaicml/mpt-30b-chat/blob/main/app.py#L49</a></p>

          '
        raw: "> So is this correct now?\n> \n> ```\n> <|im_start|>system\n> A conversation\
          \ between a user and an LLM-based AI assistant. The assistant gives helpful\
          \ and honest answers.<|im_end|>\n> <|im_start|>user\n> prompt goes here<|im_end|>\n\
          > <|im_start|>assistant\n> ```\n\nI think this is correct. Refer to the\
          \ official Demo prompt:\n\nhttps://huggingface.co/spaces/mosaicml/mpt-30b-chat/blob/main/app.py#L49\n\
          \n"
        updatedAt: '2023-06-25T09:59:44.168Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nightcall
    id: 64981010d4b5bc740114e726
    type: comment
  author: swulling
  content: "> So is this correct now?\n> \n> ```\n> <|im_start|>system\n> A conversation\
    \ between a user and an LLM-based AI assistant. The assistant gives helpful and\
    \ honest answers.<|im_end|>\n> <|im_start|>user\n> prompt goes here<|im_end|>\n\
    > <|im_start|>assistant\n> ```\n\nI think this is correct. Refer to the official\
    \ Demo prompt:\n\nhttps://huggingface.co/spaces/mosaicml/mpt-30b-chat/blob/main/app.py#L49\n\
    \n"
  created_at: 2023-06-25 08:59:44+00:00
  edited: false
  hidden: false
  id: 64981010d4b5bc740114e726
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4e24b64dbc4bcbd5d4b5e232aa814d23.svg
      fullname: Night Call
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nightcall
      type: user
    createdAt: '2023-06-25T10:50:23.000Z'
    data:
      edited: false
      editors:
      - Nightcall
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8974729180335999
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4e24b64dbc4bcbd5d4b5e232aa814d23.svg
          fullname: Night Call
          isHf: false
          isPro: false
          name: Nightcall
          type: user
        html: '<blockquote>

          <p>Thanks guys.  So specifically what I missed was just</p>

          <pre><code>&lt;|im_start|&gt;assistant

          </code></pre>

          <p>is that right? Because then the LLM would generate its own <code>&lt;|im_end|&gt;</code>?</p>

          <p>So is this correct now?</p>

          <pre><code>&lt;|im_start|&gt;system

          A conversation between a user and an LLM-based AI assistant. The assistant
          gives helpful and honest answers.&lt;|im_end|&gt;

          &lt;|im_start|&gt;user

          prompt goes here&lt;|im_end|&gt;

          &lt;|im_start|&gt;assistant

          </code></pre>

          </blockquote>

          <p>It''s correct. I tried it In Koboldcpp Story Mode several times, and
          it generated the response and its own &lt;|im_end|&gt;</p>

          <p>&lt;|im_start|&gt;system<br>A conversation between a user and an LLM-based
          AI assistant. The assistant gives helpful and honest answers.&lt;|im_end|&gt;<br>&lt;|im_start|&gt;user<br>The
          radius of the earth.&lt;|im_end|&gt;<br>&lt;|im_start|&gt;assistant<br>That
          is approximately 3,959 miles or 6,371 kilometers.&lt;|im_end|&gt;</p>

          '
        raw: "> Thanks guys.  So specifically what I missed was just\n> ```\n> <|im_start|>assistant\n\
          > ```\n> is that right? Because then the LLM would generate its own `<|im_end|>`?\n\
          > \n> So is this correct now?\n> \n> ```\n> <|im_start|>system\n> A conversation\
          \ between a user and an LLM-based AI assistant. The assistant gives helpful\
          \ and honest answers.<|im_end|>\n> <|im_start|>user\n> prompt goes here<|im_end|>\n\
          > <|im_start|>assistant\n> ```\n\nIt's correct. I tried it In Koboldcpp\
          \ Story Mode several times, and it generated the response and its own <|im_end|>\n\
          \n<|im_start|>system\nA conversation between a user and an LLM-based AI\
          \ assistant. The assistant gives helpful and honest answers.<|im_end|>\n\
          <|im_start|>user\nThe radius of the earth.<|im_end|>\n<|im_start|>assistant\
          \ \nThat is approximately 3,959 miles or 6,371 kilometers.<|im_end|>"
        updatedAt: '2023-06-25T10:50:23.674Z'
      numEdits: 0
      reactions: []
    id: 64981bef8fadf0ae762a342c
    type: comment
  author: Nightcall
  content: "> Thanks guys.  So specifically what I missed was just\n> ```\n> <|im_start|>assistant\n\
    > ```\n> is that right? Because then the LLM would generate its own `<|im_end|>`?\n\
    > \n> So is this correct now?\n> \n> ```\n> <|im_start|>system\n> A conversation\
    \ between a user and an LLM-based AI assistant. The assistant gives helpful and\
    \ honest answers.<|im_end|>\n> <|im_start|>user\n> prompt goes here<|im_end|>\n\
    > <|im_start|>assistant\n> ```\n\nIt's correct. I tried it In Koboldcpp Story\
    \ Mode several times, and it generated the response and its own <|im_end|>\n\n\
    <|im_start|>system\nA conversation between a user and an LLM-based AI assistant.\
    \ The assistant gives helpful and honest answers.<|im_end|>\n<|im_start|>user\n\
    The radius of the earth.<|im_end|>\n<|im_start|>assistant \nThat is approximately\
    \ 3,959 miles or 6,371 kilometers.<|im_end|>"
  created_at: 2023-06-25 09:50:23+00:00
  edited: false
  hidden: false
  id: 64981bef8fadf0ae762a342c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-25T10:52:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8444150686264038
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thanks, README is updated</p>

          '
        raw: Thanks, README is updated
        updatedAt: '2023-06-25T10:52:27.379Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Nightcall
        - mikeee
    id: 64981c6b5b5d43c1c77fac89
    type: comment
  author: TheBloke
  content: Thanks, README is updated
  created_at: 2023-06-25 09:52:27+00:00
  edited: false
  hidden: false
  id: 64981c6b5b5d43c1c77fac89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4e24b64dbc4bcbd5d4b5e232aa814d23.svg
      fullname: Night Call
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nightcall
      type: user
    createdAt: '2023-06-25T11:04:28.000Z'
    data:
      edited: false
      editors:
      - Nightcall
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9786685109138489
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4e24b64dbc4bcbd5d4b5e232aa814d23.svg
          fullname: Night Call
          isHf: false
          isPro: false
          name: Nightcall
          type: user
        html: '<p>Glad to help.</p>

          '
        raw: Glad to help.
        updatedAt: '2023-06-25T11:04:28.869Z'
      numEdits: 0
      reactions: []
    id: 64981f3c6a7013b3692f7bc6
    type: comment
  author: Nightcall
  content: Glad to help.
  created_at: 2023-06-25 10:04:28+00:00
  edited: false
  hidden: false
  id: 64981f3c6a7013b3692f7bc6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/mpt-30B-chat-GGML
repo_type: model
status: open
target_branch: null
title: Complete prompt template
