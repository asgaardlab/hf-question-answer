!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nib12345
conflicting_files: null
created_at: 2023-06-29 15:39:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/056370e376f77d75e8092f0c6338956f.svg
      fullname: Nithin I Bhandari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nib12345
      type: user
    createdAt: '2023-06-29T16:39:50.000Z'
    data:
      edited: true
      editors:
      - nib12345
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5168856382369995
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/056370e376f77d75e8092f0c6338956f.svg
          fullname: Nithin I Bhandari
          isHf: false
          isPro: false
          name: nib12345
          type: user
        html: '<p>Hi guys,<br>Does anyone have idea?</p>

          <p>How to split<br>mpt-30b-chat.ggmlv0.q4_1.bin</p>

          <p>to</p>

          <p>mpt-30b-chat.ggmlv0.q4_1_00001_of_00004.bin<br>mpt-30b-chat.ggmlv0.q4_1_00002_of_00004.bin<br>mpt-30b-chat.ggmlv0.q4_1_00003_of_00004.bin<br>mpt-30b-chat.ggmlv0.q4_1_00004_of_00004.bin</p>

          <p>As to load on kaggle (as kaggle has limitation of ram).</p>

          <p>If anyone has idea, please say?<br>I dont know much about how to develop
          model, i am just a Full Stack Developer.<br>Thanks.</p>

          '
        raw: "Hi guys,\nDoes anyone have idea?\n\nHow to split \nmpt-30b-chat.ggmlv0.q4_1.bin\n\
          \nto\n\nmpt-30b-chat.ggmlv0.q4_1_00001_of_00004.bin\nmpt-30b-chat.ggmlv0.q4_1_00002_of_00004.bin\n\
          mpt-30b-chat.ggmlv0.q4_1_00003_of_00004.bin\nmpt-30b-chat.ggmlv0.q4_1_00004_of_00004.bin\n\
          \nAs to load on kaggle (as kaggle has limitation of ram).\n\nIf anyone has\
          \ idea, please say?\nI dont know much about how to develop model, i am just\
          \ a Full Stack Developer.\nThanks."
        updatedAt: '2023-06-29T16:42:37.136Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nib12345
    id: 649db3d659d1d2c885acd8d9
    type: comment
  author: nib12345
  content: "Hi guys,\nDoes anyone have idea?\n\nHow to split \nmpt-30b-chat.ggmlv0.q4_1.bin\n\
    \nto\n\nmpt-30b-chat.ggmlv0.q4_1_00001_of_00004.bin\nmpt-30b-chat.ggmlv0.q4_1_00002_of_00004.bin\n\
    mpt-30b-chat.ggmlv0.q4_1_00003_of_00004.bin\nmpt-30b-chat.ggmlv0.q4_1_00004_of_00004.bin\n\
    \nAs to load on kaggle (as kaggle has limitation of ram).\n\nIf anyone has idea,\
    \ please say?\nI dont know much about how to develop model, i am just a Full Stack\
    \ Developer.\nThanks."
  created_at: 2023-06-29 15:39:50+00:00
  edited: true
  hidden: false
  id: 649db3d659d1d2c885acd8d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-29T16:52:50.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9213764071464539
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>That''s not possible. GGML does not support multi-part GGML files.</p>

          <p>Using KoboldCpp you can offload some of the model to GPU (if you have
          one), which will reduce RAM usage accordingly. </p>

          <p>But there''s no GPU support for MPT GGML models from Python code at this
          time. Only using the KoboldCpp UI.</p>

          '
        raw: "That's not possible. GGML does not support multi-part GGML files.\n\n\
          Using KoboldCpp you can offload some of the model to GPU (if you have one),\
          \ which will reduce RAM usage accordingly. \n\nBut there's no GPU support\
          \ for MPT GGML models from Python code at this time. Only using the KoboldCpp\
          \ UI."
        updatedAt: '2023-06-29T16:52:50.067Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nib12345
    id: 649db6e2290813893d65c82f
    type: comment
  author: TheBloke
  content: "That's not possible. GGML does not support multi-part GGML files.\n\n\
    Using KoboldCpp you can offload some of the model to GPU (if you have one), which\
    \ will reduce RAM usage accordingly. \n\nBut there's no GPU support for MPT GGML\
    \ models from Python code at this time. Only using the KoboldCpp UI."
  created_at: 2023-06-29 15:52:50+00:00
  edited: false
  hidden: false
  id: 649db6e2290813893d65c82f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/056370e376f77d75e8092f0c6338956f.svg
      fullname: Nithin I Bhandari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nib12345
      type: user
    createdAt: '2023-06-30T16:42:19.000Z'
    data:
      status: closed
    id: 649f05eb19a15b1fc975e7f3
    type: status-change
  author: nib12345
  created_at: 2023-06-30 15:42:19+00:00
  id: 649f05eb19a15b1fc975e7f3
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/mpt-30B-chat-GGML
repo_type: model
status: closed
target_branch: null
title: How to split a model
