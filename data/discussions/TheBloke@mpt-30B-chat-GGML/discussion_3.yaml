!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TornButter
conflicting_files: null
created_at: 2023-06-22 20:07:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0f901f41133295f085a227c8a262418f.svg
      fullname: Will Brooks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TornButter
      type: user
    createdAt: '2023-06-22T21:07:40.000Z'
    data:
      edited: true
      editors:
      - TornButter
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40304338932037354
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0f901f41133295f085a227c8a262418f.svg
          fullname: Will Brooks
          isHf: false
          isPro: false
          name: TornButter
          type: user
        html: '<p>Does config.json need to be in a certain location for it to load
          as an MPT model? Edit: config.json is next to the bin</p>

          <p>python3.10 koboldcpp.py --model ../models-koboldcpp/mpt-30B-chat-GGML/mpt-30b-chat.ggmlv0.q5_0.bin
          --useclblast 0 0 --contextsize 8192<br>Welcome to KoboldCpp - Version 1.32<br>Attempting
          to use CLBlast library for faster prompt ingestion. A compatible clblast
          will be required.<br>Initializing dynamic library: koboldcpp_clblast.so<br>==========<br>Loading
          model: /home/sapien/m2/ai-chat/models-koboldcpp/mpt-30B-chat-GGML/mpt-30b-chat.ggmlv0.q5_0.bin<br>[Threads:
          7, BlasThreads: 7, SmartContext: False]</p>

          <hr>

          <p>Identified as GPT-NEO-X model: (ver 406)<br>Attempting to Load...</p>

          <hr>

          <p>System Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI
          = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD
          = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |<br>gpt_neox_model_load: loading model
          from ''/home/sapien/m2/ai-chat/models-koboldcpp/mpt-30B-chat-GGML/mpt-30b-chat.ggmlv0.q5_0.bin''
          - please wait ...<br>gpt_neox_model_load: n_vocab = 7168<br>gpt_neox_model_load:
          n_ctx   = 8192<br>gpt_neox_model_load: n_embd  = 64<br>gpt_neox_model_load:
          n_head  = 48<br>gpt_neox_model_load: n_layer = 50432<br>gpt_neox_model_load:
          n_rot   = 1090519040<br>gpt_neox_model_load: par_res = 0<br>gpt_neox_model_load:
          ftype   = 2008<br>gpt_neox_model_load: qntvr   = 2<br>gpt_neox_model_load:
          ggml ctx size = 104213.61 MB</p>

          <p>Platform:0 Device:0  - NVIDIA CUDA with NVIDIA GeForce RTX 3090</p>

          <p>ggml_opencl: selecting platform: ''NVIDIA CUDA''<br>ggml_opencl: selecting
          device: ''NVIDIA GeForce RTX 3090''<br>ggml_opencl: device FP16 support:
          false<br>CL FP16 temporarily disabled pending further optimization.<br>GGML_ASSERT:
          ggml.c:4164: ctx-&gt;mem_buffer != NULL<br>[1]    29383 IOT instruction
          (core dumped)  python3.10 koboldcpp.py --model  --useclblast 0 0 --contextsize
          8192</p>

          '
        raw: "Does config.json need to be in a certain location for it to load as\
          \ an MPT model? Edit: config.json is next to the bin\n\npython3.10 koboldcpp.py\
          \ --model ../models-koboldcpp/mpt-30B-chat-GGML/mpt-30b-chat.ggmlv0.q5_0.bin\
          \ --useclblast 0 0 --contextsize 8192          \nWelcome to KoboldCpp -\
          \ Version 1.32\nAttempting to use CLBlast library for faster prompt ingestion.\
          \ A compatible clblast will be required.\nInitializing dynamic library:\
          \ koboldcpp_clblast.so\n==========\nLoading model: /home/sapien/m2/ai-chat/models-koboldcpp/mpt-30B-chat-GGML/mpt-30b-chat.ggmlv0.q5_0.bin\
          \ \n[Threads: 7, BlasThreads: 7, SmartContext: False]\n\n---\nIdentified\
          \ as GPT-NEO-X model: (ver 406)\nAttempting to Load...\n---\nSystem Info:\
          \ AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 |\
          \ FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
          \ = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \ngpt_neox_model_load: loading model\
          \ from '/home/sapien/m2/ai-chat/models-koboldcpp/mpt-30B-chat-GGML/mpt-30b-chat.ggmlv0.q5_0.bin'\
          \ - please wait ...\ngpt_neox_model_load: n_vocab = 7168\ngpt_neox_model_load:\
          \ n_ctx   = 8192\ngpt_neox_model_load: n_embd  = 64\ngpt_neox_model_load:\
          \ n_head  = 48\ngpt_neox_model_load: n_layer = 50432\ngpt_neox_model_load:\
          \ n_rot   = 1090519040\ngpt_neox_model_load: par_res = 0\ngpt_neox_model_load:\
          \ ftype   = 2008\ngpt_neox_model_load: qntvr   = 2\ngpt_neox_model_load:\
          \ ggml ctx size = 104213.61 MB\n\nPlatform:0 Device:0  - NVIDIA CUDA with\
          \ NVIDIA GeForce RTX 3090\n\nggml_opencl: selecting platform: 'NVIDIA CUDA'\n\
          ggml_opencl: selecting device: 'NVIDIA GeForce RTX 3090'\nggml_opencl: device\
          \ FP16 support: false\nCL FP16 temporarily disabled pending further optimization.\n\
          GGML_ASSERT: ggml.c:4164: ctx->mem_buffer != NULL\n[1]    29383 IOT instruction\
          \ (core dumped)  python3.10 koboldcpp.py --model  --useclblast 0 0 --contextsize\
          \ 8192"
        updatedAt: '2023-06-22T21:10:25.139Z'
      numEdits: 1
      reactions: []
    id: 6494b81cdceb4de2dff96a7b
    type: comment
  author: TornButter
  content: "Does config.json need to be in a certain location for it to load as an\
    \ MPT model? Edit: config.json is next to the bin\n\npython3.10 koboldcpp.py --model\
    \ ../models-koboldcpp/mpt-30B-chat-GGML/mpt-30b-chat.ggmlv0.q5_0.bin --useclblast\
    \ 0 0 --contextsize 8192          \nWelcome to KoboldCpp - Version 1.32\nAttempting\
    \ to use CLBlast library for faster prompt ingestion. A compatible clblast will\
    \ be required.\nInitializing dynamic library: koboldcpp_clblast.so\n==========\n\
    Loading model: /home/sapien/m2/ai-chat/models-koboldcpp/mpt-30B-chat-GGML/mpt-30b-chat.ggmlv0.q5_0.bin\
    \ \n[Threads: 7, BlasThreads: 7, SmartContext: False]\n\n---\nIdentified as GPT-NEO-X\
    \ model: (ver 406)\nAttempting to Load...\n---\nSystem Info: AVX = 1 | AVX2 =\
    \ 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA\
    \ = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0\
    \ | \ngpt_neox_model_load: loading model from '/home/sapien/m2/ai-chat/models-koboldcpp/mpt-30B-chat-GGML/mpt-30b-chat.ggmlv0.q5_0.bin'\
    \ - please wait ...\ngpt_neox_model_load: n_vocab = 7168\ngpt_neox_model_load:\
    \ n_ctx   = 8192\ngpt_neox_model_load: n_embd  = 64\ngpt_neox_model_load: n_head\
    \  = 48\ngpt_neox_model_load: n_layer = 50432\ngpt_neox_model_load: n_rot   =\
    \ 1090519040\ngpt_neox_model_load: par_res = 0\ngpt_neox_model_load: ftype   =\
    \ 2008\ngpt_neox_model_load: qntvr   = 2\ngpt_neox_model_load: ggml ctx size =\
    \ 104213.61 MB\n\nPlatform:0 Device:0  - NVIDIA CUDA with NVIDIA GeForce RTX 3090\n\
    \nggml_opencl: selecting platform: 'NVIDIA CUDA'\nggml_opencl: selecting device:\
    \ 'NVIDIA GeForce RTX 3090'\nggml_opencl: device FP16 support: false\nCL FP16\
    \ temporarily disabled pending further optimization.\nGGML_ASSERT: ggml.c:4164:\
    \ ctx->mem_buffer != NULL\n[1]    29383 IOT instruction (core dumped)  python3.10\
    \ koboldcpp.py --model  --useclblast 0 0 --contextsize 8192"
  created_at: 2023-06-22 20:07:40+00:00
  edited: true
  hidden: false
  id: 6494b81cdceb4de2dff96a7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/0f901f41133295f085a227c8a262418f.svg
      fullname: Will Brooks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TornButter
      type: user
    createdAt: '2023-06-22T21:10:27.000Z'
    data:
      status: closed
    id: 6494b8c3dceb4de2dff97962
    type: status-change
  author: TornButter
  created_at: 2023-06-22 20:10:27+00:00
  id: 6494b8c3dceb4de2dff97962
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/0f901f41133295f085a227c8a262418f.svg
      fullname: Will Brooks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TornButter
      type: user
    createdAt: '2023-06-22T21:13:05.000Z'
    data:
      status: open
    id: 6494b961546b0d48ce29be7e
    type: status-change
  author: TornButter
  created_at: 2023-06-22 20:13:05+00:00
  id: 6494b961546b0d48ce29be7e
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0f901f41133295f085a227c8a262418f.svg
      fullname: Will Brooks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TornButter
      type: user
    createdAt: '2023-06-22T21:21:09.000Z'
    data:
      edited: false
      editors:
      - TornButter
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6783916354179382
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0f901f41133295f085a227c8a262418f.svg
          fullname: Will Brooks
          isHf: false
          isPro: false
          name: TornButter
          type: user
        html: '<p>Fixed by adding this to my command: --forceversion 500</p>

          '
        raw: 'Fixed by adding this to my command: --forceversion 500'
        updatedAt: '2023-06-22T21:21:09.377Z'
      numEdits: 0
      reactions: []
    id: 6494bb453d119ca5d5f59dd2
    type: comment
  author: TornButter
  content: 'Fixed by adding this to my command: --forceversion 500'
  created_at: 2023-06-22 20:21:09+00:00
  edited: false
  hidden: false
  id: 6494bb453d119ca5d5f59dd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-22T21:50:36.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9500696063041687
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah I should mention that in the README.  It''s a bug in KoboldCpp,
          LostRuins is aware and will fix it in the next release.</p>

          '
        raw: Yeah I should mention that in the README.  It's a bug in KoboldCpp, LostRuins
          is aware and will fix it in the next release.
        updatedAt: '2023-06-22T21:50:36.619Z'
      numEdits: 0
      reactions: []
    id: 6494c22c6f4cdc73571925bf
    type: comment
  author: TheBloke
  content: Yeah I should mention that in the README.  It's a bug in KoboldCpp, LostRuins
    is aware and will fix it in the next release.
  created_at: 2023-06-22 20:50:36+00:00
  edited: false
  hidden: false
  id: 6494c22c6f4cdc73571925bf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/mpt-30B-chat-GGML
repo_type: model
status: open
target_branch: null
title: koboldcpp thinks it is a GPT-NEO-X model?
