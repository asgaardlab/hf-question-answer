!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lazyiitian
conflicting_files: null
created_at: 2023-06-28 21:03:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672889727871-noauth.jpeg?w=200&h=200&f=face
      fullname: Abhishek Singh Bailoo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lazyiitian
      type: user
    createdAt: '2023-06-28T22:03:00.000Z'
    data:
      edited: false
      editors:
      - lazyiitian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9168106317520142
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672889727871-noauth.jpeg?w=200&h=200&f=face
          fullname: Abhishek Singh Bailoo
          isHf: false
          isPro: false
          name: lazyiitian
          type: user
        html: '<p>Why can''t this run on llama.cpp when it can run on others that
          use llama.cpp?<br>What''s missing?</p>

          '
        raw: "Why can't this run on llama.cpp when it can run on others that use llama.cpp?\
          \ \r\nWhat's missing?"
        updatedAt: '2023-06-28T22:03:00.470Z'
      numEdits: 0
      reactions: []
    id: 649cae14cf6506ab6c3e3765
    type: comment
  author: lazyiitian
  content: "Why can't this run on llama.cpp when it can run on others that use llama.cpp?\
    \ \r\nWhat's missing?"
  created_at: 2023-06-28 21:03:00+00:00
  edited: false
  hidden: false
  id: 649cae14cf6506ab6c3e3765
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-28T22:04:49.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9062554836273193
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>llama.cpp only supports Llama models. This is an MPT model - different
          model type, different code required to load it.  </p>

          <p>It''s hoped that some time in the future - maybe in a month or so - llama.cpp
          will support more model types than just Llama, including MPT and Falcon.  But
          for now, it doesn''t. </p>

          <p>But you can use other clients for this, like koboldcpp, LoLLMS-UI, ctransformers,
          etc.</p>

          '
        raw: "llama.cpp only supports Llama models. This is an MPT model - different\
          \ model type, different code required to load it.  \n\nIt's hoped that some\
          \ time in the future - maybe in a month or so - llama.cpp will support more\
          \ model types than just Llama, including MPT and Falcon.  But for now, it\
          \ doesn't. \n\nBut you can use other clients for this, like koboldcpp, LoLLMS-UI,\
          \ ctransformers, etc."
        updatedAt: '2023-06-28T22:04:49.564Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - brad457
    id: 649cae811684bb856053c8f9
    type: comment
  author: TheBloke
  content: "llama.cpp only supports Llama models. This is an MPT model - different\
    \ model type, different code required to load it.  \n\nIt's hoped that some time\
    \ in the future - maybe in a month or so - llama.cpp will support more model types\
    \ than just Llama, including MPT and Falcon.  But for now, it doesn't. \n\nBut\
    \ you can use other clients for this, like koboldcpp, LoLLMS-UI, ctransformers,\
    \ etc."
  created_at: 2023-06-28 21:04:49+00:00
  edited: false
  hidden: false
  id: 649cae811684bb856053c8f9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/mpt-30B-chat-GGML
repo_type: model
status: open
target_branch: null
title: Why not llama.cpp ?
