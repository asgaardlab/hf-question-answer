!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bendiu
conflicting_files: null
created_at: 2023-08-15 15:24:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e556834eedba184dc14b185aa7b2b20.svg
      fullname: Ben Diu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bendiu
      type: user
    createdAt: '2023-08-15T16:24:53.000Z'
    data:
      edited: false
      editors:
      - bendiu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8730190396308899
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e556834eedba184dc14b185aa7b2b20.svg
          fullname: Ben Diu
          isHf: false
          isPro: false
          name: bendiu
          type: user
        html: '<p>I''m trying to use this on my chunked text docs to generate instruction
          formatted data for finetuning, but I''m getting this runtimeerror:</p>

          <p>RuntimeError: Expected all tensors to be on the same device, but found
          at least two devices, cuda:1 and cuda:0!</p>

          <p>Any tips on how to fix this?</p>

          '
        raw: "I'm trying to use this on my chunked text docs to generate instruction\
          \ formatted data for finetuning, but I'm getting this runtimeerror:\r\n\r\
          \nRuntimeError: Expected all tensors to be on the same device, but found\
          \ at least two devices, cuda:1 and cuda:0!\r\n\r\nAny tips on how to fix\
          \ this?"
        updatedAt: '2023-08-15T16:24:53.142Z'
      numEdits: 0
      reactions: []
    id: 64dba6d53725f8d9a90d8bbd
    type: comment
  author: bendiu
  content: "I'm trying to use this on my chunked text docs to generate instruction\
    \ formatted data for finetuning, but I'm getting this runtimeerror:\r\n\r\nRuntimeError:\
    \ Expected all tensors to be on the same device, but found at least two devices,\
    \ cuda:1 and cuda:0!\r\n\r\nAny tips on how to fix this?"
  created_at: 2023-08-15 15:24:53+00:00
  edited: false
  hidden: false
  id: 64dba6d53725f8d9a90d8bbd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-08-17T06:29:01.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9699208736419678
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>I''m not sure, but also why do you need to do that instead of running
          in parallel separately on the GPUs? the model is like 1.8 GB. or do you
          have two 1 GB GPUs?? </p>

          '
        raw: 'I''m not sure, but also why do you need to do that instead of running
          in parallel separately on the GPUs? the model is like 1.8 GB. or do you
          have two 1 GB GPUs?? '
        updatedAt: '2023-08-17T06:29:01.732Z'
      numEdits: 0
      reactions: []
    id: 64ddbe2d2402348182619b5c
    type: comment
  author: pszemraj
  content: 'I''m not sure, but also why do you need to do that instead of running
    in parallel separately on the GPUs? the model is like 1.8 GB. or do you have two
    1 GB GPUs?? '
  created_at: 2023-08-17 05:29:01+00:00
  edited: false
  hidden: false
  id: 64ddbe2d2402348182619b5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e556834eedba184dc14b185aa7b2b20.svg
      fullname: Ben Diu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bendiu
      type: user
    createdAt: '2023-08-17T12:21:02.000Z'
    data:
      edited: true
      editors:
      - bendiu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.989136278629303
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e556834eedba184dc14b185aa7b2b20.svg
          fullname: Ben Diu
          isHf: false
          isPro: false
          name: bendiu
          type: user
        html: '<p>I''m ignorant in parallel and distributed computing. I set the device_map
          to ''auto'' thinking it would speed up inference.</p>

          '
        raw: I'm ignorant in parallel and distributed computing. I set the device_map
          to 'auto' thinking it would speed up inference.
        updatedAt: '2023-08-17T12:21:46.271Z'
      numEdits: 1
      reactions: []
    id: 64de10ae288ed5cc73dcef5c
    type: comment
  author: bendiu
  content: I'm ignorant in parallel and distributed computing. I set the device_map
    to 'auto' thinking it would speed up inference.
  created_at: 2023-08-17 11:21:02+00:00
  edited: true
  hidden: false
  id: 64de10ae288ed5cc73dcef5c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: pszemraj/bart-large-instructiongen-w-inputs
repo_type: model
status: open
target_branch: null
title: How to enable multi-GPU inference?
