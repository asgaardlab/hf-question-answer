!!python/object:huggingface_hub.community.DiscussionWithDetails
author: teknium
conflicting_files: null
created_at: 2023-09-29 00:56:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-09-29T01:56:25.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9722485542297363
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>question in title</p>

          '
        raw: question in title
        updatedAt: '2023-09-29T01:56:25.377Z'
      numEdits: 0
      reactions: []
    id: 65162ec926224cf8b1f85306
    type: comment
  author: teknium
  content: question in title
  created_at: 2023-09-29 00:56:25+00:00
  edited: false
  hidden: false
  id: 65162ec926224cf8b1f85306
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
      fullname: Vasile Ermicioi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vasilee
      type: user
    createdAt: '2023-09-29T02:19:56.000Z'
    data:
      edited: true
      editors:
      - vasilee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6102482080459595
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
          fullname: Vasile Ermicioi
          isHf: false
          isPro: false
          name: vasilee
          type: user
        html: '<p>I guess it is the same procedure as other Synthia-s,<br>but the
          base model is Mistral, not llama-2</p>

          <p>see this thread <a rel="nofollow" href="https://www.reddit.com/r/LocalLLaMA/comments/16ur16s/synthia7bv13_trained_on_the_mistral7b_base/">https://www.reddit.com/r/LocalLLaMA/comments/16ur16s/synthia7bv13_trained_on_the_mistral7b_base/</a></p>

          '
        raw: "I guess it is the same procedure as other Synthia-s, \nbut the base\
          \ model is Mistral, not llama-2\n\nsee this thread https://www.reddit.com/r/LocalLLaMA/comments/16ur16s/synthia7bv13_trained_on_the_mistral7b_base/\n"
        updatedAt: '2023-09-29T02:24:39.405Z'
      numEdits: 1
      reactions: []
    id: 6516344ca546159af98e52f7
    type: comment
  author: vasilee
  content: "I guess it is the same procedure as other Synthia-s, \nbut the base model\
    \ is Mistral, not llama-2\n\nsee this thread https://www.reddit.com/r/LocalLLaMA/comments/16ur16s/synthia7bv13_trained_on_the_mistral7b_base/\n"
  created_at: 2023-09-29 01:19:56+00:00
  edited: true
  hidden: false
  id: 6516344ca546159af98e52f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-29T03:33:41.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.932197630405426
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: "<p>Hey,<br>This was trained with QLoRA, as with all my models. Learning\
          \ rate was 3e-4, 4096 context length. Batch size was 64, trained on a single\
          \ H100.<br>Synthia-v1.2 dataset, which contain Chain-of-Thought (Orca),\
          \ Tree-of-Thought and Long-Form conversation data.<br>Dataset is super high\
          \ quality, and not a massive dataset (about ~125K samples).<br>That\u2019\
          s all I can say for now.<br>Migel</p>\n"
        raw: "Hey,\nThis was trained with QLoRA, as with all my models. Learning rate\
          \ was 3e-4, 4096 context length. Batch size was 64, trained on a single\
          \ H100. \nSynthia-v1.2 dataset, which contain Chain-of-Thought (Orca), Tree-of-Thought\
          \ and Long-Form conversation data.\nDataset is super high quality, and not\
          \ a massive dataset (about ~125K samples).\nThat\u2019s all I can say for\
          \ now.\nMigel"
        updatedAt: '2023-09-29T03:33:41.129Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - mirek190
        - InvidFlower
        - theodotus
        - ljleb
        - arkaprovob
    id: 651645956137c3173018e35f
    type: comment
  author: migtissera
  content: "Hey,\nThis was trained with QLoRA, as with all my models. Learning rate\
    \ was 3e-4, 4096 context length. Batch size was 64, trained on a single H100.\
    \ \nSynthia-v1.2 dataset, which contain Chain-of-Thought (Orca), Tree-of-Thought\
    \ and Long-Form conversation data.\nDataset is super high quality, and not a massive\
    \ dataset (about ~125K samples).\nThat\u2019s all I can say for now.\nMigel"
  created_at: 2023-09-29 02:33:41+00:00
  edited: false
  hidden: false
  id: 651645956137c3173018e35f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-30T04:55:06.000Z'
    data:
      status: closed
    id: 6517aa2a8876a95d5a24f536
    type: status-change
  author: migtissera
  created_at: 2023-09-30 03:55:06+00:00
  id: 6517aa2a8876a95d5a24f536
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-10-01T15:09:22.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9122939109802246
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: "<blockquote>\n<p>Hey,<br>This was trained with QLoRA, as with all my\
          \ models. Learning rate was 3e-4, 4096 context length. Batch size was 64,\
          \ trained on a single H100.<br>Synthia-v1.2 dataset, which contain Chain-of-Thought\
          \ (Orca), Tree-of-Thought and Long-Form conversation data.<br>Dataset is\
          \ super high quality, and not a massive dataset (about ~125K samples).<br>That\u2019\
          s all I can say for now.<br>Migel</p>\n</blockquote>\n<p>Did you use artidoro's\
          \ qlora.py repo, or something else? Thanks!</p>\n"
        raw: "> Hey,\n> This was trained with QLoRA, as with all my models. Learning\
          \ rate was 3e-4, 4096 context length. Batch size was 64, trained on a single\
          \ H100. \n> Synthia-v1.2 dataset, which contain Chain-of-Thought (Orca),\
          \ Tree-of-Thought and Long-Form conversation data.\n> Dataset is super high\
          \ quality, and not a massive dataset (about ~125K samples).\n> That\u2019\
          s all I can say for now.\n> Migel\n\nDid you use artidoro's qlora.py repo,\
          \ or something else? Thanks!"
        updatedAt: '2023-10-01T15:09:22.859Z'
      numEdits: 0
      reactions: []
    id: 65198ba21ea44089e91457a9
    type: comment
  author: teknium
  content: "> Hey,\n> This was trained with QLoRA, as with all my models. Learning\
    \ rate was 3e-4, 4096 context length. Batch size was 64, trained on a single H100.\
    \ \n> Synthia-v1.2 dataset, which contain Chain-of-Thought (Orca), Tree-of-Thought\
    \ and Long-Form conversation data.\n> Dataset is super high quality, and not a\
    \ massive dataset (about ~125K samples).\n> That\u2019s all I can say for now.\n\
    > Migel\n\nDid you use artidoro's qlora.py repo, or something else? Thanks!"
  created_at: 2023-10-01 14:09:22+00:00
  edited: false
  hidden: false
  id: 65198ba21ea44089e91457a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6550b60a4863272fef6c45c3602d9269.svg
      fullname: YuJuLin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nps798
      type: user
    createdAt: '2023-10-09T11:41:23.000Z'
    data:
      edited: false
      editors:
      - nps798
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7462478876113892
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6550b60a4863272fef6c45c3602d9269.svg
          fullname: YuJuLin
          isHf: false
          isPro: false
          name: nps798
          type: user
        html: "<p>Hello,<br>Thanks for all your hard work!<br>I would like to know\
          \ what are the target_modules you use?</p>\n<p>I use the following config\
          \ to QLORA with mistral base but it show training loss instability and eventually\
          \ train loss become zero and eval loss is NaN.<br>Is it related to target_modules\
          \ ?</p>\n<pre><code>config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n\
          \    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n      \
          \  \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"\
          up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    lora_dropout=0.05,\n\
          \    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n</code></pre>\n"
        raw: "Hello, \nThanks for all your hard work!\nI would like to know what are\
          \ the target_modules you use?\n\nI use the following config to QLORA with\
          \ mistral base but it show training loss instability and eventually train\
          \ loss become zero and eval loss is NaN.\nIs it related to target_modules\
          \ ?\n\n```\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\n\
          \        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n       \
          \ \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"\
          down_proj\",\n        \"lm_head\",\n    ],\n    lora_dropout=0.05,\n   \
          \ bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n```"
        updatedAt: '2023-10-09T11:41:23.817Z'
      numEdits: 0
      reactions: []
    id: 6523e6e3db79e53948531a54
    type: comment
  author: nps798
  content: "Hello, \nThanks for all your hard work!\nI would like to know what are\
    \ the target_modules you use?\n\nI use the following config to QLORA with mistral\
    \ base but it show training loss instability and eventually train loss become\
    \ zero and eval loss is NaN.\nIs it related to target_modules ?\n\n```\nconfig\
    \ = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\n       \
    \ \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n\
    \        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n      \
    \  \"lm_head\",\n    ],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"\
    CAUSAL_LM\"\n)\n\n```"
  created_at: 2023-10-09 10:41:23+00:00
  edited: false
  hidden: false
  id: 6523e6e3db79e53948531a54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2b73d19a1006ee284271aa3f8fd95d8.svg
      fullname: Christian Merrill
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: M-Chimiste
      type: user
    createdAt: '2023-10-09T13:06:06.000Z'
    data:
      edited: false
      editors:
      - M-Chimiste
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9673935174942017
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2b73d19a1006ee284271aa3f8fd95d8.svg
          fullname: Christian Merrill
          isHf: false
          isPro: false
          name: M-Chimiste
          type: user
        html: '<p>I''ve been fine tuning with just q, k , v, o with no instability
          issues.  I ran into the same issues when trying to use any of the other
          modules.</p>

          '
        raw: I've been fine tuning with just q, k , v, o with no instability issues.  I
          ran into the same issues when trying to use any of the other modules.
        updatedAt: '2023-10-09T13:06:06.505Z'
      numEdits: 0
      reactions: []
    id: 6523fabe27d1f3d84ab88460
    type: comment
  author: M-Chimiste
  content: I've been fine tuning with just q, k , v, o with no instability issues.  I
    ran into the same issues when trying to use any of the other modules.
  created_at: 2023-10-09 12:06:06+00:00
  edited: false
  hidden: false
  id: 6523fabe27d1f3d84ab88460
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6550b60a4863272fef6c45c3602d9269.svg
      fullname: YuJuLin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nps798
      type: user
    createdAt: '2023-10-10T02:18:22.000Z'
    data:
      edited: false
      editors:
      - nps798
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5457189679145813
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6550b60a4863272fef6c45c3602d9269.svg
          fullname: YuJuLin
          isHf: false
          isPro: false
          name: nps798
          type: user
        html: "<p>Thanks reply !  I am now testing with just q k v... seems stable\
          \ so far, will follow up on it.<br>meanwhile, I examine the original artidoro\
          \ qlora github. In qlora.py, there exist snippet of </p>\n<pre><code>def\
          \ find_all_linear_names(args, model):\n    cls = bnb.nn.Linear4bit if args.bits\
          \ == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n\
          \    lora_module_names = set()\n    for name, module in model.named_modules():\n\
          \        if isinstance(module, cls):\n            names = name.split('.')\n\
          \            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\
          \n    if 'lm_head' in lora_module_names: # needed for 16-bit\n        lora_module_names.remove('lm_head')\n\
          \    return list(lora_module_names)\n</code></pre>\n<p>not sure if <strong>lora_module_names.remove('lm_head')</strong>\
          \ have something to do with what we observe here ? </p>\n"
        raw: "Thanks reply !  I am now testing with just q k v... seems stable so\
          \ far, will follow up on it.\nmeanwhile, I examine the original artidoro\
          \ qlora github. In qlora.py, there exist snippet of \n```\ndef find_all_linear_names(args,\
          \ model):\n    cls = bnb.nn.Linear4bit if args.bits == 4 else (bnb.nn.Linear8bitLt\
          \ if args.bits == 8 else torch.nn.Linear)\n    lora_module_names = set()\n\
          \    for name, module in model.named_modules():\n        if isinstance(module,\
          \ cls):\n            names = name.split('.')\n            lora_module_names.add(names[0]\
          \ if len(names) == 1 else names[-1])\n\n    if 'lm_head' in lora_module_names:\
          \ # needed for 16-bit\n        lora_module_names.remove('lm_head')\n   \
          \ return list(lora_module_names)\n```\n\nnot sure if **lora_module_names.remove('lm_head')**\
          \ have something to do with what we observe here ? "
        updatedAt: '2023-10-10T02:18:22.906Z'
      numEdits: 0
      reactions: []
    id: 6524b46e8c2691925de05bee
    type: comment
  author: nps798
  content: "Thanks reply !  I am now testing with just q k v... seems stable so far,\
    \ will follow up on it.\nmeanwhile, I examine the original artidoro qlora github.\
    \ In qlora.py, there exist snippet of \n```\ndef find_all_linear_names(args, model):\n\
    \    cls = bnb.nn.Linear4bit if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits\
    \ == 8 else torch.nn.Linear)\n    lora_module_names = set()\n    for name, module\
    \ in model.named_modules():\n        if isinstance(module, cls):\n           \
    \ names = name.split('.')\n            lora_module_names.add(names[0] if len(names)\
    \ == 1 else names[-1])\n\n    if 'lm_head' in lora_module_names: # needed for\
    \ 16-bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\
    ```\n\nnot sure if **lora_module_names.remove('lm_head')** have something to do\
    \ with what we observe here ? "
  created_at: 2023-10-10 01:18:22+00:00
  edited: false
  hidden: false
  id: 6524b46e8c2691925de05bee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-10-10T03:55:15.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.986060619354248
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: '<p>Hey, I don''t have anything to add here. I attach LoRA''s to all
          layers, but as you''re suggesting the QLoRA repo may remove <code>lm_head</code>.
          </p>

          '
        raw: 'Hey, I don''t have anything to add here. I attach LoRA''s to all layers,
          but as you''re suggesting the QLoRA repo may remove `lm_head`. '
        updatedAt: '2023-10-10T03:55:15.160Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nps798
    id: 6524cb2327d1f3d84ad4d0a5
    type: comment
  author: migtissera
  content: 'Hey, I don''t have anything to add here. I attach LoRA''s to all layers,
    but as you''re suggesting the QLoRA repo may remove `lm_head`. '
  created_at: 2023-10-10 02:55:15+00:00
  edited: false
  hidden: false
  id: 6524cb2327d1f3d84ad4d0a5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: migtissera/SynthIA-7B-v1.3
repo_type: model
status: closed
target_branch: null
title: Hi what did you train this model with, and what were hyperparams?
