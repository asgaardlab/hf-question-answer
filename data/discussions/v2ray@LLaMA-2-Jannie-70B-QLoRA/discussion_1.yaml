!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rando5000
conflicting_files: null
created_at: 2023-07-29 06:54:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d69dd23b7d93b3558afa1371e28839c.svg
      fullname: Rando
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rando5000
      type: user
    createdAt: '2023-07-29T07:54:02.000Z'
    data:
      edited: false
      editors:
      - rando5000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7188604474067688
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d69dd23b7d93b3558afa1371e28839c.svg
          fullname: Rando
          isHf: false
          isPro: false
          name: rando5000
          type: user
        html: "<p>I was able to load LLaMA-2-Jannie-7B-QLoRA and LLaMA-2-Jannie-13B-QLoRA\
          \ but this one errors out. I tried loading on A100 on runpod.io with identical\
          \ results. Are you sure your config is right?</p>\n<p>Base Model: TheBloke/Llama-2-70B-GPTQ</p>\n\
          <pre><code>File \"/home/user/.local/lib/python3.10/site-packages/exllama/generator.py\"\
          , line 352, in gen_single_token\n    logits = self.model.forward(self.sequence[:,\
          \ -1:], self.cache, lora = self.lora, input_mask = mask)\n  File \"/home/user/.local/lib/python3.10/site-packages/exllama/model.py\"\
          , line 967, in forward\n    r = self._forward(input_ids[:, chunk_begin :\
          \ chunk_end],\n  File \"/home/user/.local/lib/python3.10/site-packages/exllama/model.py\"\
          , line 1051, in _forward\n    hidden_states = _move_tensor(hidden_states,\
          \ device, \"hidden_states\", self.config)\n  File \"/home/user/.local/lib/python3.10/site-packages/exllama/model.py\"\
          , line 707, in _move_tensor\n    tensor = tensor.to(\"cpu\")\nRuntimeError:\
          \ CUDA error: an illegal memory access was encountered\nCUDA kernel errors\
          \ might be asynchronously reported at some other API call, so the stacktrace\
          \ below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\
          Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n</code></pre>\n"
        raw: "I was able to load LLaMA-2-Jannie-7B-QLoRA and LLaMA-2-Jannie-13B-QLoRA\
          \ but this one errors out. I tried loading on A100 on runpod.io with identical\
          \ results. Are you sure your config is right?\r\n\r\nBase Model: TheBloke/Llama-2-70B-GPTQ\r\
          \n\r\n```\r\nFile \"/home/user/.local/lib/python3.10/site-packages/exllama/generator.py\"\
          , line 352, in gen_single_token\r\n    logits = self.model.forward(self.sequence[:,\
          \ -1:], self.cache, lora = self.lora, input_mask = mask)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/exllama/model.py\"\
          , line 967, in forward\r\n    r = self._forward(input_ids[:, chunk_begin\
          \ : chunk_end],\r\n  File \"/home/user/.local/lib/python3.10/site-packages/exllama/model.py\"\
          , line 1051, in _forward\r\n    hidden_states = _move_tensor(hidden_states,\
          \ device, \"hidden_states\", self.config)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/exllama/model.py\"\
          , line 707, in _move_tensor\r\n    tensor = tensor.to(\"cpu\")\r\nRuntimeError:\
          \ CUDA error: an illegal memory access was encountered\r\nCUDA kernel errors\
          \ might be asynchronously reported at some other API call, so the stacktrace\
          \ below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\
          \nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\
          ```"
        updatedAt: '2023-07-29T07:54:02.348Z'
      numEdits: 0
      reactions: []
    id: 64c4c59aa3f7a8107da8f502
    type: comment
  author: rando5000
  content: "I was able to load LLaMA-2-Jannie-7B-QLoRA and LLaMA-2-Jannie-13B-QLoRA\
    \ but this one errors out. I tried loading on A100 on runpod.io with identical\
    \ results. Are you sure your config is right?\r\n\r\nBase Model: TheBloke/Llama-2-70B-GPTQ\r\
    \n\r\n```\r\nFile \"/home/user/.local/lib/python3.10/site-packages/exllama/generator.py\"\
    , line 352, in gen_single_token\r\n    logits = self.model.forward(self.sequence[:,\
    \ -1:], self.cache, lora = self.lora, input_mask = mask)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/exllama/model.py\"\
    , line 967, in forward\r\n    r = self._forward(input_ids[:, chunk_begin : chunk_end],\r\
    \n  File \"/home/user/.local/lib/python3.10/site-packages/exllama/model.py\",\
    \ line 1051, in _forward\r\n    hidden_states = _move_tensor(hidden_states, device,\
    \ \"hidden_states\", self.config)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/exllama/model.py\"\
    , line 707, in _move_tensor\r\n    tensor = tensor.to(\"cpu\")\r\nRuntimeError:\
    \ CUDA error: an illegal memory access was encountered\r\nCUDA kernel errors might\
    \ be asynchronously reported at some other API call, so the stacktrace below might\
    \ be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n\
    Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n```"
  created_at: 2023-07-29 06:54:02+00:00
  edited: false
  hidden: false
  id: 64c4c59aa3f7a8107da8f502
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/fTCV7VLY0eK4OXbwgIT2n.png?w=200&h=200&f=face
      fullname: LagPixelLOL
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: v2ray
      type: user
    createdAt: '2023-07-29T08:32:28.000Z'
    data:
      edited: false
      editors:
      - v2ray
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9410816431045532
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/fTCV7VLY0eK4OXbwgIT2n.png?w=200&h=200&f=face
          fullname: LagPixelLOL
          isHf: false
          isPro: false
          name: v2ray
          type: user
        html: '<p>Don''t use fused attention on 70B, it''s a bit different than the
          2 smaller models.</p>

          '
        raw: Don't use fused attention on 70B, it's a bit different than the 2 smaller
          models.
        updatedAt: '2023-07-29T08:32:28.009Z'
      numEdits: 0
      reactions: []
    id: 64c4ce9c538d4f17b50bc68d
    type: comment
  author: v2ray
  content: Don't use fused attention on 70B, it's a bit different than the 2 smaller
    models.
  created_at: 2023-07-29 07:32:28+00:00
  edited: false
  hidden: false
  id: 64c4ce9c538d4f17b50bc68d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d69dd23b7d93b3558afa1371e28839c.svg
      fullname: Rando
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rando5000
      type: user
    createdAt: '2023-07-29T09:14:47.000Z'
    data:
      edited: false
      editors:
      - rando5000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.464414119720459
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d69dd23b7d93b3558afa1371e28839c.svg
          fullname: Rando
          isHf: false
          isPro: false
          name: rando5000
          type: user
        html: '<p>Sorry I''m not familiar with what fused attention is... I''m using
          ooba''s textgen... is there a param for this?</p>

          <p>Currently using this<br><code>python server.py --chat --verbose --listen
          --loader exllama_HF --model TheBloke_Llama-2-70B-GPTQ --lora v2ray_LLaMA-2-Jannie-70B-QLoRA</code></p>

          '
        raw: 'Sorry I''m not familiar with what fused attention is... I''m using ooba''s
          textgen... is there a param for this?


          Currently using this

          `python server.py --chat --verbose --listen --loader exllama_HF --model
          TheBloke_Llama-2-70B-GPTQ --lora v2ray_LLaMA-2-Jannie-70B-QLoRA`'
        updatedAt: '2023-07-29T09:14:47.176Z'
      numEdits: 0
      reactions: []
    id: 64c4d887c097c6c2b3a03fec
    type: comment
  author: rando5000
  content: 'Sorry I''m not familiar with what fused attention is... I''m using ooba''s
    textgen... is there a param for this?


    Currently using this

    `python server.py --chat --verbose --listen --loader exllama_HF --model TheBloke_Llama-2-70B-GPTQ
    --lora v2ray_LLaMA-2-Jannie-70B-QLoRA`'
  created_at: 2023-07-29 08:14:47+00:00
  edited: false
  hidden: false
  id: 64c4d887c097c6c2b3a03fec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d69dd23b7d93b3558afa1371e28839c.svg
      fullname: Rando
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rando5000
      type: user
    createdAt: '2023-07-29T09:38:58.000Z'
    data:
      edited: false
      editors:
      - rando5000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8805496692657471
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d69dd23b7d93b3558afa1371e28839c.svg
          fullname: Rando
          isHf: false
          isPro: false
          name: rando5000
          type: user
        html: '<p>... I got it working with AUTOGPTQ as I found it has a "--no_inject_fused_attention"
          but this method isn''t ideal as it''s far slower than the exllama loader
          and it''s not as VRAM efficient... is there any way to use exllama without
          fused attention? maybe by modifying one of the configs in the llama-2 model''s
          configs?...</p>

          <p>For now I''m able to get it working like this:<br>python server.py --chat
          --verbose --listen --loader AutoGPTQ --no_inject_fused_attention --model
          TheBloke_Llama-2-70B-GPTQ --lora v2ray_LLaMA-2-Jannie-70B-QLoRA</p>

          '
        raw: '... I got it working with AUTOGPTQ as I found it has a "--no_inject_fused_attention"
          but this method isn''t ideal as it''s far slower than the exllama loader
          and it''s not as VRAM efficient... is there any way to use exllama without
          fused attention? maybe by modifying one of the configs in the llama-2 model''s
          configs?...


          For now I''m able to get it working like this:

          python server.py --chat --verbose --listen --loader AutoGPTQ --no_inject_fused_attention
          --model TheBloke_Llama-2-70B-GPTQ --lora v2ray_LLaMA-2-Jannie-70B-QLoRA'
        updatedAt: '2023-07-29T09:38:58.078Z'
      numEdits: 0
      reactions: []
    id: 64c4de324399efa2fd8c3666
    type: comment
  author: rando5000
  content: '... I got it working with AUTOGPTQ as I found it has a "--no_inject_fused_attention"
    but this method isn''t ideal as it''s far slower than the exllama loader and it''s
    not as VRAM efficient... is there any way to use exllama without fused attention?
    maybe by modifying one of the configs in the llama-2 model''s configs?...


    For now I''m able to get it working like this:

    python server.py --chat --verbose --listen --loader AutoGPTQ --no_inject_fused_attention
    --model TheBloke_Llama-2-70B-GPTQ --lora v2ray_LLaMA-2-Jannie-70B-QLoRA'
  created_at: 2023-07-29 08:38:58+00:00
  edited: false
  hidden: false
  id: 64c4de324399efa2fd8c3666
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/fTCV7VLY0eK4OXbwgIT2n.png?w=200&h=200&f=face
      fullname: LagPixelLOL
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: v2ray
      type: user
    createdAt: '2023-07-29T10:30:28.000Z'
    data:
      edited: false
      editors:
      - v2ray
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4346930682659149
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/fTCV7VLY0eK4OXbwgIT2n.png?w=200&h=200&f=face
          fullname: LagPixelLOL
          isHf: false
          isPro: false
          name: v2ray
          type: user
        html: '<p>You need to modify the code. In <code>your-textgen-webui-folder/modules/exllama.py</code>
          at line 53, add <code>config.fused_attn = False</code>.</p>

          '
        raw: You need to modify the code. In `your-textgen-webui-folder/modules/exllama.py`
          at line 53, add `config.fused_attn = False`.
        updatedAt: '2023-07-29T10:30:28.538Z'
      numEdits: 0
      reactions: []
    id: 64c4ea4477a6473ac3e9726f
    type: comment
  author: v2ray
  content: You need to modify the code. In `your-textgen-webui-folder/modules/exllama.py`
    at line 53, add `config.fused_attn = False`.
  created_at: 2023-07-29 09:30:28+00:00
  edited: false
  hidden: false
  id: 64c4ea4477a6473ac3e9726f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/fTCV7VLY0eK4OXbwgIT2n.png?w=200&h=200&f=face
      fullname: LagPixelLOL
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: v2ray
      type: user
    createdAt: '2023-08-23T22:57:33.000Z'
    data:
      status: closed
    id: 64e68edd5459b63c0057fade
    type: status-change
  author: v2ray
  created_at: 2023-08-23 21:57:33+00:00
  id: 64e68edd5459b63c0057fade
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: v2ray/LLaMA-2-Jannie-70B-QLoRA
repo_type: model
status: closed
target_branch: null
title: Illegal Memory Access
