!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mjw98
conflicting_files: null
created_at: 2023-10-10 06:29:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0845bfa296370456bc71c7f6b2573e06.svg
      fullname: meijingwu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mjw98
      type: user
    createdAt: '2023-10-10T07:29:38.000Z'
    data:
      edited: false
      editors:
      - mjw98
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8688474297523499
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0845bfa296370456bc71c7f6b2573e06.svg
          fullname: meijingwu
          isHf: false
          isPro: false
          name: mjw98
          type: user
        html: '<p>Thanks for your great work. I would like to ask you about your process
          of quantification using gptq? I just used autogptq for quantisation, but
          I can''t load it after quantisation using AutoModelForCausalLM.from_pretrained().
          It can only be loaded using AutoGPTQForCausalLM.from_quantised, can you
          tell me where your modification is?</p>

          '
        raw: Thanks for your great work. I would like to ask you about your process
          of quantification using gptq? I just used autogptq for quantisation, but
          I can't load it after quantisation using AutoModelForCausalLM.from_pretrained().
          It can only be loaded using AutoGPTQForCausalLM.from_quantised, can you
          tell me where your modification is?
        updatedAt: '2023-10-10T07:29:38.358Z'
      numEdits: 0
      reactions: []
    id: 6524fd62a801972b6ed0ed52
    type: comment
  author: mjw98
  content: Thanks for your great work. I would like to ask you about your process
    of quantification using gptq? I just used autogptq for quantisation, but I can't
    load it after quantisation using AutoModelForCausalLM.from_pretrained(). It can
    only be loaded using AutoGPTQForCausalLM.from_quantised, can you tell me where
    your modification is?
  created_at: 2023-10-10 06:29:38+00:00
  edited: false
  hidden: false
  id: 6524fd62a801972b6ed0ed52
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-10T09:39:29.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8146554827690125
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Answered in the other thread.  I suggest using my AutoGPTQ wrapper
          script for easy quantisations which can be loaded with Transformers: <a
          rel="nofollow" href="https://github.com/TheBlokeAI/AIScripts/blob/main/quant_autogptq.py">https://github.com/TheBlokeAI/AIScripts/blob/main/quant_autogptq.py</a></p>

          '
        raw: 'Answered in the other thread.  I suggest using my AutoGPTQ wrapper script
          for easy quantisations which can be loaded with Transformers: https://github.com/TheBlokeAI/AIScripts/blob/main/quant_autogptq.py'
        updatedAt: '2023-10-10T09:39:29.121Z'
      numEdits: 0
      reactions: []
    id: 65251bd18821bac8c9b7816a
    type: comment
  author: TheBloke
  content: 'Answered in the other thread.  I suggest using my AutoGPTQ wrapper script
    for easy quantisations which can be loaded with Transformers: https://github.com/TheBlokeAI/AIScripts/blob/main/quant_autogptq.py'
  created_at: 2023-10-10 08:39:29+00:00
  edited: false
  hidden: false
  id: 65251bd18821bac8c9b7816a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a869d2dede8ee7e3abd4437fcc59d2c3.svg
      fullname: W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: j7j
      type: user
    createdAt: '2023-10-11T14:57:52.000Z'
    data:
      edited: true
      editors:
      - j7j
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8107630014419556
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a869d2dede8ee7e3abd4437fcc59d2c3.svg
          fullname: W
          isHf: false
          isPro: false
          name: j7j
          type: user
        html: "<p>Thank you very much for your suggestion, but I still don't understand\
          \ where to change the \"model_basename\", I tried the AutoGPTQ wrapper script\
          \ you provided, but the output is still gptq-4bit-128g.safetensors. I used\
          \ the following code to quantize it and got the result as shown below<br>\
          \ python quant_autogptq.py &nbsp;\"/content/llama-1B\" \"777\" \"c4\" .\
          \ </p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6526b7495f41ec8f18517a34/Y3LTn4GNYHhGdjl0H2XoV.png\"\
          ><img alt=\"1bf832de4aaacf82ad1b49199fd3b2e.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6526b7495f41ec8f18517a34/Y3LTn4GNYHhGdjl0H2XoV.png\"\
          ></a></p>\n<p>I guess is to change the safetensors\u2019 filename and change\
          \ \"model_file_base_name\": \"gptq_model-4bit-128g\" to \"model_file_base_name\"\
          : \"model\" in quantize_config.json, hope to get your guidance.</p>\n"
        raw: "Thank you very much for your suggestion, but I still don't understand\
          \ where to change the \"model_basename\", I tried the AutoGPTQ wrapper script\
          \ you provided, but the output is still gptq-4bit-128g.safetensors. I used\
          \ the following code to quantize it and got the result as shown below\n\
          \ python quant_autogptq.py \_\"/content/llama-1B\" \"777\" \"c4\" . \n\n\
          ![1bf832de4aaacf82ad1b49199fd3b2e.png](https://cdn-uploads.huggingface.co/production/uploads/6526b7495f41ec8f18517a34/Y3LTn4GNYHhGdjl0H2XoV.png)\n\
          \nI guess is to change the safetensors\u2019 filename and change \"model_file_base_name\"\
          : \"gptq_model-4bit-128g\" to \"model_file_base_name\": \"model\" in quantize_config.json,\
          \ hope to get your guidance.\n"
        updatedAt: '2023-10-11T14:59:18.643Z'
      numEdits: 1
      reactions: []
    id: 6526b7f0fccb9226ab0c4a72
    type: comment
  author: j7j
  content: "Thank you very much for your suggestion, but I still don't understand\
    \ where to change the \"model_basename\", I tried the AutoGPTQ wrapper script\
    \ you provided, but the output is still gptq-4bit-128g.safetensors. I used the\
    \ following code to quantize it and got the result as shown below\n python quant_autogptq.py\
    \ \_\"/content/llama-1B\" \"777\" \"c4\" . \n\n![1bf832de4aaacf82ad1b49199fd3b2e.png](https://cdn-uploads.huggingface.co/production/uploads/6526b7495f41ec8f18517a34/Y3LTn4GNYHhGdjl0H2XoV.png)\n\
    \nI guess is to change the safetensors\u2019 filename and change \"model_file_base_name\"\
    : \"gptq_model-4bit-128g\" to \"model_file_base_name\": \"model\" in quantize_config.json,\
    \ hope to get your guidance.\n"
  created_at: 2023-10-11 13:57:52+00:00
  edited: true
  hidden: false
  id: 6526b7f0fccb9226ab0c4a72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-11T15:07:41.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8183196783065796
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh sorry, I never updated the public quant_autogptq.py for that.</p>

          <p>Add<br><code>model_file_base_name=''model'',</code> into the <code>BaseQuantizeConfig()</code>
          definition starting on line 194 of my quant_autogptq.py script.</p>

          <p>And yes, to fix an already made GPTQ, rename the safetensors file to
          <code>model.safetensors</code> and also set <code>model_file_base_name</code>
          to <code>"model"</code> in quantize_config.json.  This will then be identical
          to a GPTQ made with the change to BaseQuantizeConfig() described above.</p>

          '
        raw: "Oh sorry, I never updated the public quant_autogptq.py for that.\n\n\
          Add \n`model_file_base_name='model',` into the `BaseQuantizeConfig()` definition\
          \ starting on line 194 of my quant_autogptq.py script.\n\nAnd yes, to fix\
          \ an already made GPTQ, rename the safetensors file to `model.safetensors`\
          \ and also set `model_file_base_name` to `\"model\"` in quantize_config.json.\
          \  This will then be identical to a GPTQ made with the change to BaseQuantizeConfig()\
          \ described above."
        updatedAt: '2023-10-11T15:07:41.137Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mjw98
    id: 6526ba3d80dae88f3dee6d06
    type: comment
  author: TheBloke
  content: "Oh sorry, I never updated the public quant_autogptq.py for that.\n\nAdd\
    \ \n`model_file_base_name='model',` into the `BaseQuantizeConfig()` definition\
    \ starting on line 194 of my quant_autogptq.py script.\n\nAnd yes, to fix an already\
    \ made GPTQ, rename the safetensors file to `model.safetensors` and also set `model_file_base_name`\
    \ to `\"model\"` in quantize_config.json.  This will then be identical to a GPTQ\
    \ made with the change to BaseQuantizeConfig() described above."
  created_at: 2023-10-11 14:07:41+00:00
  edited: false
  hidden: false
  id: 6526ba3d80dae88f3dee6d06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0845bfa296370456bc71c7f6b2573e06.svg
      fullname: meijingwu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mjw98
      type: user
    createdAt: '2023-10-14T15:49:04.000Z'
    data:
      edited: true
      editors:
      - mjw98
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9614405035972595
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0845bfa296370456bc71c7f6b2573e06.svg
          fullname: meijingwu
          isHf: false
          isPro: false
          name: mjw98
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span>  Thank you very\
          \ much for your patient guidance, I managed to convert it into a format\
          \ that tarsnfomer recognizes!</p>\n"
        raw: '@TheBloke  Thank you very much for your patient guidance, I managed
          to convert it into a format that tarsnfomer recognizes!'
        updatedAt: '2023-10-14T15:49:28.084Z'
      numEdits: 1
      reactions: []
    id: 652ab8704d0eef4a2b0528b5
    type: comment
  author: mjw98
  content: '@TheBloke  Thank you very much for your patient guidance, I managed to
    convert it into a format that tarsnfomer recognizes!'
  created_at: 2023-10-14 14:49:04+00:00
  edited: true
  hidden: false
  id: 652ab8704d0eef4a2b0528b5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Kimiko-Mistral-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: Please tell me about gptq quantisation.
