!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 9cento
conflicting_files: null
created_at: 2023-06-01 16:10:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
      fullname: 9cento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 9cento
      type: user
    createdAt: '2023-06-01T17:10:32.000Z'
    data:
      edited: false
      editors:
      - 9cento
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
          fullname: 9cento
          isHf: false
          isPro: false
          name: 9cento
          type: user
        html: '<p>I have a 4090 but still all the 24GB of vram get filled. I tried
          "load-in-8bit" but same thing, I also tried "load-in-4bit" but I get the
          following log:</p>

          <p><code>You are using device_map=''auto'' on a 4bit loaded version of the
          model. To automatically compute the appropriate device map, you should upgrade
          your accelerate library,pip install --upgrade accelerare or install it from
          source to support fp4 auto device mapcalculation. You may encounter unexpected
          behavior, or pass your own device map</code></p>

          <p>As a result the model doesn''t get loaded, I also ran <code>pip install
          --upgrade accelerate</code> but nothing changes. I also removed the <code>--auto-devices</code>
          flag, same story.<br>Please help!</p>

          '
        raw: "I have a 4090 but still all the 24GB of vram get filled. I tried \"\
          load-in-8bit\" but same thing, I also tried \"load-in-4bit\" but I get the\
          \ following log:\r\n\r\n`You are using device_map='auto' on a 4bit loaded\
          \ version of the model. To automatically compute the appropriate device\
          \ map, you should upgrade your accelerate library,pip install --upgrade\
          \ accelerare or install it from source to support fp4 auto device mapcalculation.\
          \ You may encounter unexpected behavior, or pass your own device map`\r\n\
          \r\nAs a result the model doesn't get loaded, I also ran `pip install --upgrade\
          \ accelerate` but nothing changes. I also removed the `--auto-devices` flag,\
          \ same story.\r\nPlease help!"
        updatedAt: '2023-06-01T17:10:32.735Z'
      numEdits: 0
      reactions: []
    id: 6478d108ac77bb7603821365
    type: comment
  author: 9cento
  content: "I have a 4090 but still all the 24GB of vram get filled. I tried \"load-in-8bit\"\
    \ but same thing, I also tried \"load-in-4bit\" but I get the following log:\r\
    \n\r\n`You are using device_map='auto' on a 4bit loaded version of the model.\
    \ To automatically compute the appropriate device map, you should upgrade your\
    \ accelerate library,pip install --upgrade accelerare or install it from source\
    \ to support fp4 auto device mapcalculation. You may encounter unexpected behavior,\
    \ or pass your own device map`\r\n\r\nAs a result the model doesn't get loaded,\
    \ I also ran `pip install --upgrade accelerate` but nothing changes. I also removed\
    \ the `--auto-devices` flag, same story.\r\nPlease help!"
  created_at: 2023-06-01 16:10:32+00:00
  edited: false
  hidden: false
  id: 6478d108ac77bb7603821365
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
      fullname: YellowRoseCx
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Monero
      type: user
    createdAt: '2023-06-01T17:25:39.000Z'
    data:
      edited: false
      editors:
      - Monero
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
          fullname: YellowRoseCx
          isHf: false
          isPro: false
          name: Monero
          type: user
        html: '<p>Generally you''d need about 64gb of VRAM for this model unless it
          was quantized. I''m not familiar with bits and bytes'' load in 4 bit but
          I''ve heard it''s not as good as GPTQ so maybe it doesn''t compress the
          model enough to fit on 24gb. I have a 4 bit version posted if you''d wanna
          try that. I uploaded this fp16 model for people to create different quantizations
          off of</p>

          '
        raw: Generally you'd need about 64gb of VRAM for this model unless it was
          quantized. I'm not familiar with bits and bytes' load in 4 bit but I've
          heard it's not as good as GPTQ so maybe it doesn't compress the model enough
          to fit on 24gb. I have a 4 bit version posted if you'd wanna try that. I
          uploaded this fp16 model for people to create different quantizations off
          of
        updatedAt: '2023-06-01T17:25:39.039Z'
      numEdits: 0
      reactions: []
    id: 6478d49342b1805ae2ac86d1
    type: comment
  author: Monero
  content: Generally you'd need about 64gb of VRAM for this model unless it was quantized.
    I'm not familiar with bits and bytes' load in 4 bit but I've heard it's not as
    good as GPTQ so maybe it doesn't compress the model enough to fit on 24gb. I have
    a 4 bit version posted if you'd wanna try that. I uploaded this fp16 model for
    people to create different quantizations off of
  created_at: 2023-06-01 16:25:39+00:00
  edited: false
  hidden: false
  id: 6478d49342b1805ae2ac86d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
      fullname: 9cento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 9cento
      type: user
    createdAt: '2023-06-02T10:17:40.000Z'
    data:
      edited: false
      editors:
      - 9cento
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
          fullname: 9cento
          isHf: false
          isPro: false
          name: 9cento
          type: user
        html: '<blockquote>

          <p>I have a 4 bit version posted if you''d wanna try that</p>

          </blockquote>

          <p>Oh wow that''s super! didn''t notice that.<br>Just to be sure, are you
          referring to <code>Guanaco-SuperCOT-30b-GPTQ-4bit</code>? Is that uncensored
          as well? Does it differ in any way from <code>WizardLM-30B-Uncensored-Guanaco-SuperCOT-30b</code>
          overall?<br>Thanks for your time man!</p>

          '
        raw: '>I have a 4 bit version posted if you''d wanna try that


          Oh wow that''s super! didn''t notice that.

          Just to be sure, are you referring to `Guanaco-SuperCOT-30b-GPTQ-4bit`?
          Is that uncensored as well? Does it differ in any way from `WizardLM-30B-Uncensored-Guanaco-SuperCOT-30b`
          overall?

          Thanks for your time man!'
        updatedAt: '2023-06-02T10:17:40.007Z'
      numEdits: 0
      reactions: []
    id: 6479c1c4a84498f2af46344b
    type: comment
  author: 9cento
  content: '>I have a 4 bit version posted if you''d wanna try that


    Oh wow that''s super! didn''t notice that.

    Just to be sure, are you referring to `Guanaco-SuperCOT-30b-GPTQ-4bit`? Is that
    uncensored as well? Does it differ in any way from `WizardLM-30B-Uncensored-Guanaco-SuperCOT-30b`
    overall?

    Thanks for your time man!'
  created_at: 2023-06-02 09:17:40+00:00
  edited: false
  hidden: false
  id: 6479c1c4a84498f2af46344b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
      fullname: YellowRoseCx
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Monero
      type: user
    createdAt: '2023-06-02T18:45:18.000Z'
    data:
      edited: false
      editors:
      - Monero
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9811233282089233
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
          fullname: YellowRoseCx
          isHf: false
          isPro: false
          name: Monero
          type: user
        html: '<p>I was actually incorrect and I haven''t made a 4bit version . that
          guanaco-supercot one uses base LLAMA as its base model, the WizardLM one
          uses WizardLM. I''ll try to run the quantization later today</p>

          '
        raw: I was actually incorrect and I haven't made a 4bit version . that guanaco-supercot
          one uses base LLAMA as its base model, the WizardLM one uses WizardLM. I'll
          try to run the quantization later today
        updatedAt: '2023-06-02T18:45:18.347Z'
      numEdits: 0
      reactions: []
    id: 647a38be3d84658e53070d2c
    type: comment
  author: Monero
  content: I was actually incorrect and I haven't made a 4bit version . that guanaco-supercot
    one uses base LLAMA as its base model, the WizardLM one uses WizardLM. I'll try
    to run the quantization later today
  created_at: 2023-06-02 17:45:18+00:00
  edited: false
  hidden: false
  id: 647a38be3d84658e53070d2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
      fullname: 9cento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 9cento
      type: user
    createdAt: '2023-06-02T18:55:09.000Z'
    data:
      edited: false
      editors:
      - 9cento
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9560810327529907
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
          fullname: 9cento
          isHf: false
          isPro: false
          name: 9cento
          type: user
        html: '<p>No problem man, but please once you''re done if you could include
          the parameters to run it in the model description it would be great because
          I still didn''t manage to run not even <code>Guanaco-SuperCOT-30b-GPTQ-4bit</code>
          and I want to rule out every potential incompatibilities/conflicts from
          my side (installation-wise) since I can run other Llama-based GPTQ-4bit
          models without issues.<br>Thanks in advance.</p>

          '
        raw: 'No problem man, but please once you''re done if you could include the
          parameters to run it in the model description it would be great because
          I still didn''t manage to run not even `Guanaco-SuperCOT-30b-GPTQ-4bit`
          and I want to rule out every potential incompatibilities/conflicts from
          my side (installation-wise) since I can run other Llama-based GPTQ-4bit
          models without issues.

          Thanks in advance.'
        updatedAt: '2023-06-02T18:55:09.053Z'
      numEdits: 0
      reactions: []
    id: 647a3b0d3d84658e530748a4
    type: comment
  author: 9cento
  content: 'No problem man, but please once you''re done if you could include the
    parameters to run it in the model description it would be great because I still
    didn''t manage to run not even `Guanaco-SuperCOT-30b-GPTQ-4bit` and I want to
    rule out every potential incompatibilities/conflicts from my side (installation-wise)
    since I can run other Llama-based GPTQ-4bit models without issues.

    Thanks in advance.'
  created_at: 2023-06-02 17:55:09+00:00
  edited: false
  hidden: false
  id: 647a3b0d3d84658e530748a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
      fullname: YellowRoseCx
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Monero
      type: user
    createdAt: '2023-06-02T21:01:16.000Z'
    data:
      edited: false
      editors:
      - Monero
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8005881309509277
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
          fullname: YellowRoseCx
          isHf: false
          isPro: false
          name: Monero
          type: user
        html: '<p>make sure you have the 4bit version of KoboldAI installed <a rel="nofollow"
          href="https://github.com/0cc4m/koboldai">https://github.com/0cc4m/koboldai</a>
          if you''re using KoboldAI, the install instructions are in the readme there</p>

          <p>The instructions for Oobabooga''s TextGen UI are here (I don''t use this
          so I''m not familiar with how it works) <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md">https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md</a></p>

          '
        raw: 'make sure you have the 4bit version of KoboldAI installed https://github.com/0cc4m/koboldai
          if you''re using KoboldAI, the install instructions are in the readme there


          The instructions for Oobabooga''s TextGen UI are here (I don''t use this
          so I''m not familiar with how it works) https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md'
        updatedAt: '2023-06-02T21:01:16.400Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - 9cento
    id: 647a589c822b7e8ccbd75462
    type: comment
  author: Monero
  content: 'make sure you have the 4bit version of KoboldAI installed https://github.com/0cc4m/koboldai
    if you''re using KoboldAI, the install instructions are in the readme there


    The instructions for Oobabooga''s TextGen UI are here (I don''t use this so I''m
    not familiar with how it works) https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md'
  created_at: 2023-06-02 20:01:16+00:00
  edited: false
  hidden: false
  id: 647a589c822b7e8ccbd75462
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
      fullname: YellowRoseCx
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Monero
      type: user
    createdAt: '2023-06-04T21:27:19.000Z'
    data:
      edited: false
      editors:
      - Monero
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9977850914001465
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
          fullname: YellowRoseCx
          isHf: false
          isPro: false
          name: Monero
          type: user
        html: '<p>I haven''t forgot, just haven''t had the time to do it yet</p>

          '
        raw: I haven't forgot, just haven't had the time to do it yet
        updatedAt: '2023-06-04T21:27:19.182Z'
      numEdits: 0
      reactions: []
    id: 647d01b783c62f32492717a7
    type: comment
  author: Monero
  content: I haven't forgot, just haven't had the time to do it yet
  created_at: 2023-06-04 20:27:19+00:00
  edited: false
  hidden: false
  id: 647d01b783c62f32492717a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
      fullname: 9cento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 9cento
      type: user
    createdAt: '2023-06-05T04:49:17.000Z'
    data:
      edited: false
      editors:
      - 9cento
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9935291409492493
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7ca49c1e9e39ace4ebf65f7c38397f.svg
          fullname: 9cento
          isHf: false
          isPro: false
          name: 9cento
          type: user
        html: '<blockquote>

          <p>I haven''t forgot, just haven''t had the time to do it yet</p>

          </blockquote>

          <p>That''s ok man, no need to be in a hurry</p>

          '
        raw: '> I haven''t forgot, just haven''t had the time to do it yet


          That''s ok man, no need to be in a hurry'
        updatedAt: '2023-06-05T04:49:17.053Z'
      numEdits: 0
      reactions: []
    id: 647d694dc788767ab5e7d350
    type: comment
  author: 9cento
  content: '> I haven''t forgot, just haven''t had the time to do it yet


    That''s ok man, no need to be in a hurry'
  created_at: 2023-06-05 03:49:17+00:00
  edited: false
  hidden: false
  id: 647d694dc788767ab5e7d350
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
      fullname: YellowRoseCx
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Monero
      type: user
    createdAt: '2023-06-09T06:41:29.000Z'
    data:
      edited: false
      editors:
      - Monero
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9392838478088379
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
          fullname: YellowRoseCx
          isHf: false
          isPro: false
          name: Monero
          type: user
        html: '<p>so I haven''t been able to do it myself, but it looks like someone
          else has already made a 4bit version:<br><a href="https://huggingface.co/benjicolby/WizardLM-30B-Guanaco-SuperCOT-GPTQ-4bit">https://huggingface.co/benjicolby/WizardLM-30B-Guanaco-SuperCOT-GPTQ-4bit</a></p>

          '
        raw: "so I haven't been able to do it myself, but it looks like someone else\
          \ has already made a 4bit version: \nhttps://huggingface.co/benjicolby/WizardLM-30B-Guanaco-SuperCOT-GPTQ-4bit"
        updatedAt: '2023-06-09T06:41:29.153Z'
      numEdits: 0
      reactions: []
    id: 6482c9992e73ce18ad488531
    type: comment
  author: Monero
  content: "so I haven't been able to do it myself, but it looks like someone else\
    \ has already made a 4bit version: \nhttps://huggingface.co/benjicolby/WizardLM-30B-Guanaco-SuperCOT-GPTQ-4bit"
  created_at: 2023-06-09 05:41:29+00:00
  edited: false
  hidden: false
  id: 6482c9992e73ce18ad488531
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Monero/WizardLM-30B-Uncensored-Guanaco-SuperCOT-30b
repo_type: model
status: open
target_branch: null
title: How do I load this model?
