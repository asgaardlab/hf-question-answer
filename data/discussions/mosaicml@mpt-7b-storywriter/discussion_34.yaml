!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jdc4429
conflicting_files: null
created_at: 2023-06-03 01:24:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-06-03T02:24:13.000Z'
    data:
      edited: false
      editors:
      - jdc4429
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6047192215919495
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>Can we get an idea of how much VRAM is needed for the different
          token lengths.<br>ie.  8GB max tokens, 12GB max tokens, 16GB max tokens,
          24GB max tokens, 48gb max tokens..</p>

          '
        raw: "Can we get an idea of how much VRAM is needed for the different token\
          \ lengths.\r\nie.  8GB max tokens, 12GB max tokens, 16GB max tokens, 24GB\
          \ max tokens, 48gb max tokens..\r\n\r\n"
        updatedAt: '2023-06-03T02:24:13.082Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - hfuyhzx
        - averoo
    id: 647aa44d686ecd7bf3f1af67
    type: comment
  author: jdc4429
  content: "Can we get an idea of how much VRAM is needed for the different token\
    \ lengths.\r\nie.  8GB max tokens, 12GB max tokens, 16GB max tokens, 24GB max\
    \ tokens, 48gb max tokens..\r\n\r\n"
  created_at: 2023-06-03 01:24:13+00:00
  edited: false
  hidden: false
  id: 647aa44d686ecd7bf3f1af67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03e3d02ea6d1c4fbfb1ff92b5d80a960.svg
      fullname: rodrigo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rodrigofarias
      type: user
    createdAt: '2023-06-09T13:05:45.000Z'
    data:
      edited: false
      editors:
      - rodrigofarias
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9314545392990112
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03e3d02ea6d1c4fbfb1ff92b5d80a960.svg
          fullname: rodrigo
          isHf: false
          isPro: false
          name: rodrigofarias
          type: user
        html: '<p>Hello!</p>

          <p>Im also interested in this. Besides that, its possible to run in a desktop
          GPU, like an RTX 4070? Even if takes longer than usual time?<br>Also, do
          you Mosaic plan on launching a smaller version of storywriter? Like 32k
          context?</p>

          '
        raw: 'Hello!


          Im also interested in this. Besides that, its possible to run in a desktop
          GPU, like an RTX 4070? Even if takes longer than usual time?

          Also, do you Mosaic plan on launching a smaller version of storywriter?
          Like 32k context?'
        updatedAt: '2023-06-09T13:05:45.061Z'
      numEdits: 0
      reactions: []
    id: 648323a9e3bd340cdca4b6bf
    type: comment
  author: rodrigofarias
  content: 'Hello!


    Im also interested in this. Besides that, its possible to run in a desktop GPU,
    like an RTX 4070? Even if takes longer than usual time?

    Also, do you Mosaic plan on launching a smaller version of storywriter? Like 32k
    context?'
  created_at: 2023-06-09 12:05:45+00:00
  edited: false
  hidden: false
  id: 648323a9e3bd340cdca4b6bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676407968549-63dd38e5a8877129a15a12c5.png?w=200&h=200&f=face
      fullname: Jacob Portes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jacobfulano
      type: user
    createdAt: '2023-06-09T14:09:29.000Z'
    data:
      edited: true
      editors:
      - jacobfulano
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8501117825508118
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676407968549-63dd38e5a8877129a15a12c5.png?w=200&h=200&f=face
          fullname: Jacob Portes
          isHf: false
          isPro: false
          name: jacobfulano
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;rodrigofarias&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rodrigofarias\"\
          >@<span class=\"underline\">rodrigofarias</span></a></span>\n\n\t</span></span>,\
          \ you'll notice that the MPT-7B-StoryWriter model has roughly the same memory\
          \ footprint as MPT-7B and and MPT-7B-Instruct/Chat, which is roughly 12\
          \ GB for the model weights. This is because the linear bias matrices in\
          \ ALiBi can simply be increased or decreased depending on the desired context\
          \ length (see <a rel=\"nofollow\" href=\"https://www.youtube.com/watch?v=Pp61ShI9VGc\"\
          >this video</a> by Ofir Press). If you would like to work with 32k context\
          \ length, you can simply do:</p>\n<pre><code>import transformers\n\nname\
          \ = 'mosaicml/mpt-7b-storywriter'\n\nconfig = transformers.AutoConfig.from_pretrained(name,\
          \ trust_remote_code=True)\nconfig.max_seq_len = 32768 # (input + output)\
          \ tokens can be defined by the user\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \  name,\n  config=config,\n  trust_remote_code=True\n)\n</code></pre>\n\
          <p>The distinction here is that MPT-7B-StoryWriter has been finetuned on\
          \ much longer texts relative to MPT-7B-Instruct/Chat finetuning.</p>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;jdc4429&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/jdc4429\">@<span class=\"underline\"\
          >jdc4429</span></a></span>\n\n\t</span></span> for inference, once the input\
          \ sequence length is increased, the forward pass takes up more memory (roughly\
          \ quadratic in memory as the QK^T matrix is [batch,...,T, T] where T is\
          \ maximum sequence length. The linear bias ALiBi matrix is the same dimension\
          \ as QK^T, and can be increased/decreased accordingly. We don't have a table\
          \ for the memory requirements of 2048, 4196, 8192 etc. max sequence lengths,\
          \ but it should be straightforward to profile.</p>\n<p>A few community efforts\
          \ have quantized MPT-7B-StoryWriter that you might find interesting:</p>\n\
          <ul>\n<li><a rel=\"nofollow\" href=\"https://replicate.com/replicate/mpt-7b-storywriter\"\
          >https://replicate.com/replicate/mpt-7b-storywriter</a></li>\n<li><a href=\"\
          https://huggingface.co/TheBloke/MPT-7B-Storywriter-GGML\">https://huggingface.co/TheBloke/MPT-7B-Storywriter-GGML</a></li>\n\
          <li><a href=\"https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g\"\
          >https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g</a></li>\n\
          <li><a rel=\"nofollow\" href=\"https://github.com/rmihaylov/mpttune\">https://github.com/rmihaylov/mpttune</a>,\
          \ also mentioned in <a href=\"/mosaicml/mpt-7b-storywriter/discussions/26\"\
          >#26</a></li>\n</ul>\n"
        raw: "@rodrigofarias, you'll notice that the MPT-7B-StoryWriter model has\
          \ roughly the same memory footprint as MPT-7B and and MPT-7B-Instruct/Chat,\
          \ which is roughly 12 GB for the model weights. This is because the linear\
          \ bias matrices in ALiBi can simply be increased or decreased depending\
          \ on the desired context length (see [this video](https://www.youtube.com/watch?v=Pp61ShI9VGc)\
          \ by Ofir Press). If you would like to work with 32k context length, you\
          \ can simply do:\n\n```\nimport transformers\n\nname = 'mosaicml/mpt-7b-storywriter'\n\
          \nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\n\
          config.max_seq_len = 32768 # (input + output) tokens can be defined by the\
          \ user\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n \
          \ name,\n  config=config,\n  trust_remote_code=True\n)\n``` \n\nThe distinction\
          \ here is that MPT-7B-StoryWriter has been finetuned on much longer texts\
          \ relative to MPT-7B-Instruct/Chat finetuning.\n\n@jdc4429 for inference,\
          \ once the input sequence length is increased, the forward pass takes up\
          \ more memory (roughly quadratic in memory as the QK^T matrix is [batch,...,T,\
          \ T] where T is maximum sequence length. The linear bias ALiBi matrix is\
          \ the same dimension as QK^T, and can be increased/decreased accordingly.\
          \ We don't have a table for the memory requirements of 2048, 4196, 8192\
          \ etc. max sequence lengths, but it should be straightforward to profile.\n\
          \nA few community efforts have quantized MPT-7B-StoryWriter that you might\
          \ find interesting:\n\n* [https://replicate.com/replicate/mpt-7b-storywriter](https://replicate.com/replicate/mpt-7b-storywriter)\n\
          * [https://huggingface.co/TheBloke/MPT-7B-Storywriter-GGML](https://huggingface.co/TheBloke/MPT-7B-Storywriter-GGML)\n\
          * [https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g](https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g)\n\
          * [https://github.com/rmihaylov/mpttune](https://github.com/rmihaylov/mpttune),\
          \ also mentioned in #26"
        updatedAt: '2023-06-09T14:11:11.432Z'
      numEdits: 1
      reactions: []
    id: 648332998bfd740d4e0c4ba2
    type: comment
  author: jacobfulano
  content: "@rodrigofarias, you'll notice that the MPT-7B-StoryWriter model has roughly\
    \ the same memory footprint as MPT-7B and and MPT-7B-Instruct/Chat, which is roughly\
    \ 12 GB for the model weights. This is because the linear bias matrices in ALiBi\
    \ can simply be increased or decreased depending on the desired context length\
    \ (see [this video](https://www.youtube.com/watch?v=Pp61ShI9VGc) by Ofir Press).\
    \ If you would like to work with 32k context length, you can simply do:\n\n```\n\
    import transformers\n\nname = 'mosaicml/mpt-7b-storywriter'\n\nconfig = transformers.AutoConfig.from_pretrained(name,\
    \ trust_remote_code=True)\nconfig.max_seq_len = 32768 # (input + output) tokens\
    \ can be defined by the user\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
    \  name,\n  config=config,\n  trust_remote_code=True\n)\n``` \n\nThe distinction\
    \ here is that MPT-7B-StoryWriter has been finetuned on much longer texts relative\
    \ to MPT-7B-Instruct/Chat finetuning.\n\n@jdc4429 for inference, once the input\
    \ sequence length is increased, the forward pass takes up more memory (roughly\
    \ quadratic in memory as the QK^T matrix is [batch,...,T, T] where T is maximum\
    \ sequence length. The linear bias ALiBi matrix is the same dimension as QK^T,\
    \ and can be increased/decreased accordingly. We don't have a table for the memory\
    \ requirements of 2048, 4196, 8192 etc. max sequence lengths, but it should be\
    \ straightforward to profile.\n\nA few community efforts have quantized MPT-7B-StoryWriter\
    \ that you might find interesting:\n\n* [https://replicate.com/replicate/mpt-7b-storywriter](https://replicate.com/replicate/mpt-7b-storywriter)\n\
    * [https://huggingface.co/TheBloke/MPT-7B-Storywriter-GGML](https://huggingface.co/TheBloke/MPT-7B-Storywriter-GGML)\n\
    * [https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g](https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g)\n\
    * [https://github.com/rmihaylov/mpttune](https://github.com/rmihaylov/mpttune),\
    \ also mentioned in #26"
  created_at: 2023-06-09 13:09:29+00:00
  edited: true
  hidden: false
  id: 648332998bfd740d4e0c4ba2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-06-14T06:35:52.000Z'
    data:
      status: closed
    id: 64895fc8f4d5239fac746987
    type: status-change
  author: sam-mosaic
  created_at: 2023-06-14 05:35:52+00:00
  id: 64895fc8f4d5239fac746987
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff5189007e1a5e51951a8adf9a829c54.svg
      fullname: Ethan Turok
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eturok
      type: user
    createdAt: '2023-07-07T20:31:51.000Z'
    data:
      edited: false
      editors:
      - eturok
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8039606213569641
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff5189007e1a5e51951a8adf9a829c54.svg
          fullname: Ethan Turok
          isHf: false
          isPro: false
          name: eturok
          type: user
        html: '<p>Some other quantized versions of storywtier include:</p>

          <ul>

          <li><a href="https://huggingface.co/4bit/mpt-7b-storywriter-4bit-128g">https://huggingface.co/4bit/mpt-7b-storywriter-4bit-128g</a></li>

          <li><a href="https://huggingface.co/emozilla/mpt-7b-storywriter-fast">https://huggingface.co/emozilla/mpt-7b-storywriter-fast</a></li>

          </ul>

          '
        raw: 'Some other quantized versions of storywtier include:

          * [https://huggingface.co/4bit/mpt-7b-storywriter-4bit-128g](https://huggingface.co/4bit/mpt-7b-storywriter-4bit-128g)

          * [https://huggingface.co/emozilla/mpt-7b-storywriter-fast](https://huggingface.co/emozilla/mpt-7b-storywriter-fast)'
        updatedAt: '2023-07-07T20:31:51.200Z'
      numEdits: 0
      reactions: []
    id: 64a8763703835e13f95cf247
    type: comment
  author: eturok
  content: 'Some other quantized versions of storywtier include:

    * [https://huggingface.co/4bit/mpt-7b-storywriter-4bit-128g](https://huggingface.co/4bit/mpt-7b-storywriter-4bit-128g)

    * [https://huggingface.co/emozilla/mpt-7b-storywriter-fast](https://huggingface.co/emozilla/mpt-7b-storywriter-fast)'
  created_at: 2023-07-07 19:31:51+00:00
  edited: false
  hidden: false
  id: 64a8763703835e13f95cf247
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 34
repo_id: mosaicml/mpt-7b-storywriter
repo_type: model
status: closed
target_branch: null
title: Question regarding token length and memory
