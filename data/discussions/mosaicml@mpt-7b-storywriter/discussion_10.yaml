!!python/object:huggingface_hub.community.DiscussionWithDetails
author: roy650
conflicting_files: null
created_at: 2023-05-07 14:49:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78a8e7a380f1248fcc42a0a813e202fd.svg
      fullname: Roy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: roy650
      type: user
    createdAt: '2023-05-07T15:49:05.000Z'
    data:
      edited: false
      editors:
      - roy650
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78a8e7a380f1248fcc42a0a813e202fd.svg
          fullname: Roy
          isHf: false
          isPro: false
          name: roy650
          type: user
        html: "<p>When generating with:</p>\n<pre><code>inputs = tokenizer(prompt,\
          \ return_tensors=\"pt\").to(device='cuda:0').input_ids\nres = model.generate(inputs,\
          \ max_new_tokens=20, do_sample=True, temperature=0.7, top_k=0, top_p=40)\
          \ \n</code></pre>\n<p>I get:</p>\n<pre><code>Traceback (most recent call\
          \ last):\n  File \"&lt;string&gt;\", line 21, in _fwd_kernel\nKeyError:\
          \ ('2-.-0-.-0--d6252949da17ceb5f3a278a70250af13-1af5134066c618146d2cd009138944a0-6fb21260a873f1a3458b67752ca56f63-3498c340fd4b6ee7805fd54b882a04f5-e1f133f98d04093da2078dfc51c36b72-b26258bf01f839199e39d64851821f26-d7c06e3b46e708006c15224aac7a1378-f585402118c8a136948ce0a49cfe122c',\
          \ (torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16,\
          \ torch.float32, torch.float32, 'fp32', 'i32', 'i32', 'i32', 'i32', 'i32',\
          \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',\
          \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32'), ('vector', True, 128,\
          \ False, False, True, 128, 128), (True, True, True, True, True, True, True,\
          \ (False,), (True, False), (True, False), (True, False), (True, False),\
          \ (True, False), (True, False), (True, False), (True, False), (True, False),\
          \ (True, False), (False, False), (True, False), (True, False), (True, False),\
          \ (True, False), (True, False), (False, False), (False, False), (True, False),\
          \ (True, False), (False, False), (False, False)))\n\nDuring handling of\
          \ the above exception, another exception occurred:\n\nTraceback (most recent\
          \ call last):\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 937, in build_triton_ir\n    generator.visit(fn.parse())\n  File\
          \ \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 855, in visit\n    return super().visit(node)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
          , line 418, in visit\n    return visitor(node)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 183, in visit_Module\n    ast.NodeVisitor.generic_visit(self, node)\n\
          \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\", line\
          \ 426, in generic_visit\n    self.visit(item)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 855, in visit\n    return super().visit(node)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
          , line 418, in visit\n    return visitor(node)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 252, in visit_FunctionDef\n    has_ret = self.visit_compound_statement(node.body)\n\
          \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 177, in visit_compound_statement\n    self.last_ret_type = self.visit(stmt)\n\
          \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 855, in visit\n    return super().visit(node)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
          , line 418, in visit\n    return visitor(node)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 678, in visit_For\n    self.visit_compound_statement(node.body)\n\
          \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 177, in visit_compound_statement\n    self.last_ret_type = self.visit(stmt)\n\
          \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 855, in visit\n    return super().visit(node)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
          , line 418, in visit\n    return visitor(node)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 319, in visit_AugAssign\n    self.visit(assign)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 855, in visit\n    return super().visit(node)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
          , line 418, in visit\n    return visitor(node)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 301, in visit_Assign\n    values = self.visit(node.value)\n  File\
          \ \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 855, in visit\n    return super().visit(node)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
          , line 418, in visit\n    return visitor(node)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 339, in visit_BinOp\n    rhs = self.visit(node.right)\n  File \"\
          /u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 855, in visit\n    return super().visit(node)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
          , line 418, in visit\n    return visitor(node)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 797, in visit_Call\n    return fn(*args, _builder=self.builder, **kws)\n\
          \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/impl/base.py\"\
          , line 22, in wrapper\n    return fn(*args, **kwargs)\nTypeError: dot()\
          \ got an unexpected keyword argument 'trans_b'\n\nThe above exception was\
          \ the direct cause of the following exception:\n\nTraceback (most recent\
          \ call last):\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 3460, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\
          \  File \"/tmp/ipykernel_791836/924310910.py\", line 2, in &lt;module&gt;\n\
          \    res = model.generate(inputs, max_new_tokens=20) #, do_sample=True,\
          \ temperature=0.7, top_k=0, top_p=40) #, return_full_text=False)\n  File\
          \ \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1437, in generate\n    return self.greedy_search(\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2248, in greedy_search\n    outputs = self(\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/modeling_mpt.py\"\
          , line 237, in forward\n    outputs = self.transformer(input_ids=input_ids,\
          \ past_key_values=past_key_values, attention_mask=attention_mask, prefix_mask=prefix_mask,\
          \ sequence_id=sequence_id, return_dict=return_dict, output_attentions=output_attentions,\
          \ output_hidden_states=output_hidden_states, use_cache=use_cache)\n  File\
          \ \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/modeling_mpt.py\"\
          , line 183, in forward\n    (x, past_key_value) = block(x, past_key_value=past_key_value,\
          \ attn_bias=attn_bias, attention_mask=attention_mask, is_causal=self.is_causal)\n\
          \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/blocks.py\"\
          , line 36, in forward\n    (b, _, past_key_value) = self.attn(a, past_key_value=past_key_value,\
          \ attn_bias=attn_bias, attention_mask=attention_mask, is_causal=is_causal)\n\
          \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/attention.py\"\
          , line 171, in forward\n    (context, attn_weights) = self.attn_fn(query,\
          \ key, value, self.n_heads, softmax_scale=self.softmax_scale, attn_bias=attn_bias,\
          \ key_padding_mask=key_padding_mask, is_causal=is_causal, dropout_p=self.attn_dropout_p,\
          \ training=self.training, needs_weights=needs_weights)\n  File \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/attention.py\"\
          , line 111, in triton_flash_attn_fn\n    attn_output = flash_attn_triton.flash_attn_func(query,\
          \ key, value, attn_bias, reset_is_causal, softmax_scale)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/function.py\"\
          , line 506, in apply\n    return super().apply(*args, **kwargs)  # type:\
          \ ignore[misc]\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/flash_attn/flash_attn_triton.py\"\
          , line 810, in forward\n    o, lse, ctx.softmax_scale = _flash_attn_forward(\n\
          \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/flash_attn/flash_attn_triton.py\"\
          , line 623, in _flash_attn_forward\n    _fwd_kernel[grid](\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/runtime/autotuner.py\"\
          , line 199, in run\n    return self.fn.run(*args, **kwargs)\n  File \"&lt;string&gt;\"\
          , line 41, in _fwd_kernel\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 1620, in compile\n    next_module = compile(module)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 1549, in &lt;lambda&gt;\n    lambda src: ast_to_ttir(src, signature,\
          \ configs[0], constants)),\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 962, in ast_to_ttir\n    mod, _ = build_triton_ir(fn, signature,\
          \ specialization, constants)\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 942, in build_triton_ir\n    raise CompilationError(fn.src, node)\
          \ from e\ntriton.compiler.CompilationError: at 78:24:\ndef _fwd_kernel(\n\
          \    Q, K, V, Bias, Out,\n    Lse, TMP,  # NOTE: TMP is a scratchpad buffer\
          \ to workaround a compiler bug\n    softmax_scale,\n    stride_qb, stride_qh,\
          \ stride_qm,\n    stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh,\
          \ stride_vn,\n    stride_bb, stride_bh, stride_bm,\n    stride_ob, stride_oh,\
          \ stride_om,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n\
          \    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K,\n    BIAS_TYPE: tl.constexpr,\n\
          \    IS_CAUSAL: tl.constexpr,\n    BLOCK_HEADDIM: tl.constexpr,\n    EVEN_M:\
          \ tl.constexpr, EVEN_N: tl.constexpr, EVEN_HEADDIM: tl.constexpr,\n    BLOCK_M:\
          \ tl.constexpr, BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n\
          \    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h\
          \ = off_hb % nheads\n    # off_b = tl.program_id(1)\n    # off_h = tl.program_id(2)\n\
          \    # off_hb = off_b * nheads + off_h\n    # initialize offsets\n    offs_m\
          \ = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0,\
          \ BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    # Initialize pointers\
          \ to Q, K, V\n    # Adding parenthesis around indexing might use int32 math\
          \ instead of int64 math?\n    # https://github.com/openai/triton/issues/741\n\
          \    # I'm seeing a tiny bit of difference (5-7us)\n    q_ptrs = Q + off_b\
          \ * stride_qb + off_h * stride_qh + (offs_m[:, None] * stride_qm + offs_d[None,\
          \ :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:,\
          \ None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb\
          \ + off_h * stride_vh + (offs_n[:, None] * stride_vn + offs_d[None, :])\n\
          \    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb\
          \ + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n      \
          \  b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:, None]\
          \ * stride_bm + offs_n[None, :])\n    # initialize pointer to m and l\n\
          \    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M],\
          \ dtype=tl.float32) - float(\"inf\")\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32)\
          \ - float(\"inf\")\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n\
          \    # load q: it will stay in SRAM throughout\n    # [2022-10-30] TD: Triton\
          \ bug - in the case of EVEN_M=True and EVEN_N=False, if we just call\n \
          \   # tl.load(q_ptrs), we get the wrong output!\n    if EVEN_M &amp; EVEN_N:\n\
          \        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n\
          \            q = tl.load(q_ptrs, mask=offs_d[None, :] &lt; headdim, other=0.0)\n\
          \    else:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m[:,\
          \ None] &lt; seqlen_q, other=0.0)\n        else:\n            q = tl.load(q_ptrs,\
          \ mask=(offs_m[:, None] &lt; seqlen_q) &amp; (offs_d[None, :] &lt; headdim),\n\
          \                        other=0.0)\n    # loop over k, v and update accumulator\n\
          \    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) * BLOCK_M,\
          \ seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n\
          \ = tl.multiple_of(start_n, BLOCK_N)\n        # -- compute qk ----\n   \
          \     if EVEN_N &amp; EVEN_M:  # If we just do \"if EVEN_N\", there seems\
          \ to be some race condition\n            if EVEN_HEADDIM:\n            \
          \    k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n    \
          \            k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\
          \ :] &lt; headdim, other=0.0)\n        else:\n            if EVEN_HEADDIM:\n\
          \                k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n\
          \ + offs_n)[:, None] &lt; seqlen_k,\n                            other=0.0)\n\
          \            else:\n                k = tl.load(k_ptrs + start_n * stride_kn,\n\
          \                            mask=((start_n + offs_n)[:, None] &lt; seqlen_k)\
          \ &amp; (offs_d[None, :] &lt; headdim),\n                            other=0.0)\n\
          \        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk\
          \ += tl.dot(q, k, trans_b=True)\n                        ^\n\nDuring handling\
          \ of the above exception, another exception occurred:\n\nTraceback (most\
          \ recent call last):\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 2057, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n\
          \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1288, in structured_traceback\n    return FormattedTB.structured_traceback(\n\
          \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1177, in structured_traceback\n    return VerboseTB.structured_traceback(\n\
          \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1049, in structured_traceback\n    formatted_exceptions += self.format_exception_as_a_whole(etype,\
          \ evalue, etb, lines_of_context,\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 935, in format_exception_as_a_whole\n    self.get_records(etb, number_of_lines_of_context,\
          \ tb_offset) if etb else []\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1003, in get_records\n    lines, first = inspect.getsourcelines(etb.tb_frame)\n\
          \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/inspect.py\",\
          \ line 1129, in getsourcelines\n    lines, lnum = findsource(object)\n \
          \ File \"/u/user983/miniconda3/envs/llama/lib/python3.10/inspect.py\", line\
          \ 958, in findsource\n    raise OSError('could not get source code')\nOSError:\
          \ could not get source code\n</code></pre>\n<p>It happens only when using\
          \ triton attention (torch attention works fine), using:</p>\n<pre><code>flash-attn==1.0.4\n\
          triton==2.0.0\n</code></pre>\n<p>Any ideas?</p>\n"
        raw: "When generating with:\r\n```\r\ninputs = tokenizer(prompt, return_tensors=\"\
          pt\").to(device='cuda:0').input_ids\r\nres = model.generate(inputs, max_new_tokens=20,\
          \ do_sample=True, temperature=0.7, top_k=0, top_p=40) \r\n```\r\nI get:\r\
          \n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line\
          \ 21, in _fwd_kernel\r\nKeyError: ('2-.-0-.-0--d6252949da17ceb5f3a278a70250af13-1af5134066c618146d2cd009138944a0-6fb21260a873f1a3458b67752ca56f63-3498c340fd4b6ee7805fd54b882a04f5-e1f133f98d04093da2078dfc51c36b72-b26258bf01f839199e39d64851821f26-d7c06e3b46e708006c15224aac7a1378-f585402118c8a136948ce0a49cfe122c',\
          \ (torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16,\
          \ torch.float32, torch.float32, 'fp32', 'i32', 'i32', 'i32', 'i32', 'i32',\
          \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',\
          \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32'), ('vector', True, 128,\
          \ False, False, True, 128, 128), (True, True, True, True, True, True, True,\
          \ (False,), (True, False), (True, False), (True, False), (True, False),\
          \ (True, False), (True, False), (True, False), (True, False), (True, False),\
          \ (True, False), (False, False), (True, False), (True, False), (True, False),\
          \ (True, False), (True, False), (False, False), (False, False), (True, False),\
          \ (True, False), (False, False), (False, False)))\r\n\r\nDuring handling\
          \ of the above exception, another exception occurred:\r\n\r\nTraceback (most\
          \ recent call last):\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 937, in build_triton_ir\r\n    generator.visit(fn.parse())\r\n  File\
          \ \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 855, in visit\r\n    return super().visit(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
          , line 418, in visit\r\n    return visitor(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 183, in visit_Module\r\n    ast.NodeVisitor.generic_visit(self, node)\r\
          \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\", line\
          \ 426, in generic_visit\r\n    self.visit(item)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 855, in visit\r\n    return super().visit(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
          , line 418, in visit\r\n    return visitor(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 252, in visit_FunctionDef\r\n    has_ret = self.visit_compound_statement(node.body)\r\
          \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 177, in visit_compound_statement\r\n    self.last_ret_type = self.visit(stmt)\r\
          \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 855, in visit\r\n    return super().visit(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
          , line 418, in visit\r\n    return visitor(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 678, in visit_For\r\n    self.visit_compound_statement(node.body)\r\
          \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 177, in visit_compound_statement\r\n    self.last_ret_type = self.visit(stmt)\r\
          \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 855, in visit\r\n    return super().visit(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
          , line 418, in visit\r\n    return visitor(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 319, in visit_AugAssign\r\n    self.visit(assign)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 855, in visit\r\n    return super().visit(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
          , line 418, in visit\r\n    return visitor(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 301, in visit_Assign\r\n    values = self.visit(node.value)\r\n \
          \ File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 855, in visit\r\n    return super().visit(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
          , line 418, in visit\r\n    return visitor(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 339, in visit_BinOp\r\n    rhs = self.visit(node.right)\r\n  File\
          \ \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 855, in visit\r\n    return super().visit(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
          , line 418, in visit\r\n    return visitor(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 797, in visit_Call\r\n    return fn(*args, _builder=self.builder,\
          \ **kws)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/impl/base.py\"\
          , line 22, in wrapper\r\n    return fn(*args, **kwargs)\r\nTypeError: dot()\
          \ got an unexpected keyword argument 'trans_b'\r\n\r\nThe above exception\
          \ was the direct cause of the following exception:\r\n\r\nTraceback (most\
          \ recent call last):\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 3460, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\
          \n  File \"/tmp/ipykernel_791836/924310910.py\", line 2, in <module>\r\n\
          \    res = model.generate(inputs, max_new_tokens=20) #, do_sample=True,\
          \ temperature=0.7, top_k=0, top_p=40) #, return_full_text=False)\r\n  File\
          \ \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1437, in generate\r\n    return self.greedy_search(\r\n  File \"\
          /u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2248, in greedy_search\r\n    outputs = self(\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/modeling_mpt.py\"\
          , line 237, in forward\r\n    outputs = self.transformer(input_ids=input_ids,\
          \ past_key_values=past_key_values, attention_mask=attention_mask, prefix_mask=prefix_mask,\
          \ sequence_id=sequence_id, return_dict=return_dict, output_attentions=output_attentions,\
          \ output_hidden_states=output_hidden_states, use_cache=use_cache)\r\n  File\
          \ \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/modeling_mpt.py\"\
          , line 183, in forward\r\n    (x, past_key_value) = block(x, past_key_value=past_key_value,\
          \ attn_bias=attn_bias, attention_mask=attention_mask, is_causal=self.is_causal)\r\
          \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/blocks.py\"\
          , line 36, in forward\r\n    (b, _, past_key_value) = self.attn(a, past_key_value=past_key_value,\
          \ attn_bias=attn_bias, attention_mask=attention_mask, is_causal=is_causal)\r\
          \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/attention.py\"\
          , line 171, in forward\r\n    (context, attn_weights) = self.attn_fn(query,\
          \ key, value, self.n_heads, softmax_scale=self.softmax_scale, attn_bias=attn_bias,\
          \ key_padding_mask=key_padding_mask, is_causal=is_causal, dropout_p=self.attn_dropout_p,\
          \ training=self.training, needs_weights=needs_weights)\r\n  File \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/attention.py\"\
          , line 111, in triton_flash_attn_fn\r\n    attn_output = flash_attn_triton.flash_attn_func(query,\
          \ key, value, attn_bias, reset_is_causal, softmax_scale)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/function.py\"\
          , line 506, in apply\r\n    return super().apply(*args, **kwargs)  # type:\
          \ ignore[misc]\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/flash_attn/flash_attn_triton.py\"\
          , line 810, in forward\r\n    o, lse, ctx.softmax_scale = _flash_attn_forward(\r\
          \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/flash_attn/flash_attn_triton.py\"\
          , line 623, in _flash_attn_forward\r\n    _fwd_kernel[grid](\r\n  File \"\
          /u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/runtime/autotuner.py\"\
          , line 199, in run\r\n    return self.fn.run(*args, **kwargs)\r\n  File\
          \ \"<string>\", line 41, in _fwd_kernel\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 1620, in compile\r\n    next_module = compile(module)\r\n  File \"\
          /u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 1549, in <lambda>\r\n    lambda src: ast_to_ttir(src, signature,\
          \ configs[0], constants)),\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 962, in ast_to_ttir\r\n    mod, _ = build_triton_ir(fn, signature,\
          \ specialization, constants)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
          , line 942, in build_triton_ir\r\n    raise CompilationError(fn.src, node)\
          \ from e\r\ntriton.compiler.CompilationError: at 78:24:\r\ndef _fwd_kernel(\r\
          \n    Q, K, V, Bias, Out,\r\n    Lse, TMP,  # NOTE: TMP is a scratchpad\
          \ buffer to workaround a compiler bug\r\n    softmax_scale,\r\n    stride_qb,\
          \ stride_qh, stride_qm,\r\n    stride_kb, stride_kh, stride_kn,\r\n    stride_vb,\
          \ stride_vh, stride_vn,\r\n    stride_bb, stride_bh, stride_bm,\r\n    stride_ob,\
          \ stride_oh, stride_om,\r\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\
          \ headdim,\r\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K,\r\n    BIAS_TYPE:\
          \ tl.constexpr,\r\n    IS_CAUSAL: tl.constexpr,\r\n    BLOCK_HEADDIM: tl.constexpr,\r\
          \n    EVEN_M: tl.constexpr, EVEN_N: tl.constexpr, EVEN_HEADDIM: tl.constexpr,\r\
          \n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,\r\n):\r\n    start_m\
          \ = tl.program_id(0)\r\n    off_hb = tl.program_id(1)\r\n    off_b = off_hb\
          \ // nheads\r\n    off_h = off_hb % nheads\r\n    # off_b = tl.program_id(1)\r\
          \n    # off_h = tl.program_id(2)\r\n    # off_hb = off_b * nheads + off_h\r\
          \n    # initialize offsets\r\n    offs_m = start_m * BLOCK_M + tl.arange(0,\
          \ BLOCK_M)\r\n    offs_n = tl.arange(0, BLOCK_N)\r\n    offs_d = tl.arange(0,\
          \ BLOCK_HEADDIM)\r\n    # Initialize pointers to Q, K, V\r\n    # Adding\
          \ parenthesis around indexing might use int32 math instead of int64 math?\r\
          \n    # https://github.com/openai/triton/issues/741\r\n    # I'm seeing\
          \ a tiny bit of difference (5-7us)\r\n    q_ptrs = Q + off_b * stride_qb\
          \ + off_h * stride_qh + (offs_m[:, None] * stride_qm + offs_d[None, :])\r\
          \n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None]\
          \ * stride_kn + offs_d[None, :])\r\n    v_ptrs = V + off_b * stride_vb +\
          \ off_h * stride_vh + (offs_n[:, None] * stride_vn + offs_d[None, :])\r\n\
          \    if BIAS_TYPE == 'vector':\r\n        b_ptrs = Bias + off_b * stride_bb\
          \ + off_h * stride_bh + offs_n\r\n    elif BIAS_TYPE == 'matrix':\r\n  \
          \      b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\
          \ None] * stride_bm + offs_n[None, :])\r\n    # initialize pointer to m\
          \ and l\r\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\r\n   \
          \ lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\r\n   \
          \ m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\r\n    acc_o\
          \ = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\r\n    # load q:\
          \ it will stay in SRAM throughout\r\n    # [2022-10-30] TD: Triton bug -\
          \ in the case of EVEN_M=True and EVEN_N=False, if we just call\r\n    #\
          \ tl.load(q_ptrs), we get the wrong output!\r\n    if EVEN_M & EVEN_N:\r\
          \n        if EVEN_HEADDIM:\r\n            q = tl.load(q_ptrs)\r\n      \
          \  else:\r\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim,\
          \ other=0.0)\r\n    else:\r\n        if EVEN_HEADDIM:\r\n            q =\
          \ tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\r\n      \
          \  else:\r\n            q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q)\
          \ & (offs_d[None, :] < headdim),\r\n                        other=0.0)\r\
          \n    # loop over k, v and update accumulator\r\n    end_n = seqlen_k if\
          \ not IS_CAUSAL else tl.minimum((start_m + 1) * BLOCK_M, seqlen_k)\r\n \
          \   for start_n in range(0, end_n, BLOCK_N):\r\n        start_n = tl.multiple_of(start_n,\
          \ BLOCK_N)\r\n        # -- compute qk ----\r\n        if EVEN_N & EVEN_M:\
          \  # If we just do \"if EVEN_N\", there seems to be some race condition\r\
          \n            if EVEN_HEADDIM:\r\n                k = tl.load(k_ptrs + start_n\
          \ * stride_kn)\r\n            else:\r\n                k = tl.load(k_ptrs\
          \ + start_n * stride_kn, mask=offs_d[None, :] < headdim, other=0.0)\r\n\
          \        else:\r\n            if EVEN_HEADDIM:\r\n                k = tl.load(k_ptrs\
          \ + start_n * stride_kn, mask=(start_n + offs_n)[:, None] < seqlen_k,\r\n\
          \                            other=0.0)\r\n            else:\r\n       \
          \         k = tl.load(k_ptrs + start_n * stride_kn,\r\n                \
          \            mask=((start_n + offs_n)[:, None] < seqlen_k) & (offs_d[None,\
          \ :] < headdim),\r\n                            other=0.0)\r\n        qk\
          \ = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\r\n        qk += tl.dot(q,\
          \ k, trans_b=True)\r\n                        ^\r\n\r\nDuring handling of\
          \ the above exception, another exception occurred:\r\n\r\nTraceback (most\
          \ recent call last):\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 2057, in showtraceback\r\n    stb = self.InteractiveTB.structured_traceback(\r\
          \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1288, in structured_traceback\r\n    return FormattedTB.structured_traceback(\r\
          \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1177, in structured_traceback\r\n    return VerboseTB.structured_traceback(\r\
          \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1049, in structured_traceback\r\n    formatted_exceptions += self.format_exception_as_a_whole(etype,\
          \ evalue, etb, lines_of_context,\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 935, in format_exception_as_a_whole\r\n    self.get_records(etb,\
          \ number_of_lines_of_context, tb_offset) if etb else []\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1003, in get_records\r\n    lines, first = inspect.getsourcelines(etb.tb_frame)\r\
          \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/inspect.py\"\
          , line 1129, in getsourcelines\r\n    lines, lnum = findsource(object)\r\
          \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/inspect.py\"\
          , line 958, in findsource\r\n    raise OSError('could not get source code')\r\
          \nOSError: could not get source code\r\n```\r\nIt happens only when using\
          \ triton attention (torch attention works fine), using:\r\n```\r\nflash-attn==1.0.4\r\
          \ntriton==2.0.0\r\n```\r\nAny ideas?"
        updatedAt: '2023-05-07T15:49:05.086Z'
      numEdits: 0
      reactions: []
    id: 6457c871de963aff4190c41d
    type: comment
  author: roy650
  content: "When generating with:\r\n```\r\ninputs = tokenizer(prompt, return_tensors=\"\
    pt\").to(device='cuda:0').input_ids\r\nres = model.generate(inputs, max_new_tokens=20,\
    \ do_sample=True, temperature=0.7, top_k=0, top_p=40) \r\n```\r\nI get:\r\n```\r\
    \nTraceback (most recent call last):\r\n  File \"<string>\", line 21, in _fwd_kernel\r\
    \nKeyError: ('2-.-0-.-0--d6252949da17ceb5f3a278a70250af13-1af5134066c618146d2cd009138944a0-6fb21260a873f1a3458b67752ca56f63-3498c340fd4b6ee7805fd54b882a04f5-e1f133f98d04093da2078dfc51c36b72-b26258bf01f839199e39d64851821f26-d7c06e3b46e708006c15224aac7a1378-f585402118c8a136948ce0a49cfe122c',\
    \ (torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.bfloat16,\
    \ torch.float32, torch.float32, 'fp32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',\
    \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',\
    \ 'i32', 'i32', 'i32', 'i32', 'i32'), ('vector', True, 128, False, False, True,\
    \ 128, 128), (True, True, True, True, True, True, True, (False,), (True, False),\
    \ (True, False), (True, False), (True, False), (True, False), (True, False), (True,\
    \ False), (True, False), (True, False), (True, False), (False, False), (True,\
    \ False), (True, False), (True, False), (True, False), (True, False), (False,\
    \ False), (False, False), (True, False), (True, False), (False, False), (False,\
    \ False)))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\
    \n\r\nTraceback (most recent call last):\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 937, in build_triton_ir\r\n    generator.visit(fn.parse())\r\n  File \"\
    /u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 855, in visit\r\n    return super().visit(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
    , line 418, in visit\r\n    return visitor(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 183, in visit_Module\r\n    ast.NodeVisitor.generic_visit(self, node)\r\
    \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\", line 426,\
    \ in generic_visit\r\n    self.visit(item)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 855, in visit\r\n    return super().visit(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
    , line 418, in visit\r\n    return visitor(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 252, in visit_FunctionDef\r\n    has_ret = self.visit_compound_statement(node.body)\r\
    \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 177, in visit_compound_statement\r\n    self.last_ret_type = self.visit(stmt)\r\
    \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 855, in visit\r\n    return super().visit(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
    , line 418, in visit\r\n    return visitor(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 678, in visit_For\r\n    self.visit_compound_statement(node.body)\r\n \
    \ File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 177, in visit_compound_statement\r\n    self.last_ret_type = self.visit(stmt)\r\
    \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 855, in visit\r\n    return super().visit(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
    , line 418, in visit\r\n    return visitor(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 319, in visit_AugAssign\r\n    self.visit(assign)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 855, in visit\r\n    return super().visit(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
    , line 418, in visit\r\n    return visitor(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 301, in visit_Assign\r\n    values = self.visit(node.value)\r\n  File \"\
    /u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 855, in visit\r\n    return super().visit(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
    , line 418, in visit\r\n    return visitor(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 339, in visit_BinOp\r\n    rhs = self.visit(node.right)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 855, in visit\r\n    return super().visit(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/ast.py\"\
    , line 418, in visit\r\n    return visitor(node)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 797, in visit_Call\r\n    return fn(*args, _builder=self.builder, **kws)\r\
    \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/impl/base.py\"\
    , line 22, in wrapper\r\n    return fn(*args, **kwargs)\r\nTypeError: dot() got\
    \ an unexpected keyword argument 'trans_b'\r\n\r\nThe above exception was the\
    \ direct cause of the following exception:\r\n\r\nTraceback (most recent call\
    \ last):\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
    , line 3460, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\
    \n  File \"/tmp/ipykernel_791836/924310910.py\", line 2, in <module>\r\n    res\
    \ = model.generate(inputs, max_new_tokens=20) #, do_sample=True, temperature=0.7,\
    \ top_k=0, top_p=40) #, return_full_text=False)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1437, in generate\r\n    return self.greedy_search(\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2248, in greedy_search\r\n    outputs = self(\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/modeling_mpt.py\"\
    , line 237, in forward\r\n    outputs = self.transformer(input_ids=input_ids,\
    \ past_key_values=past_key_values, attention_mask=attention_mask, prefix_mask=prefix_mask,\
    \ sequence_id=sequence_id, return_dict=return_dict, output_attentions=output_attentions,\
    \ output_hidden_states=output_hidden_states, use_cache=use_cache)\r\n  File \"\
    /u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/modeling_mpt.py\"\
    , line 183, in forward\r\n    (x, past_key_value) = block(x, past_key_value=past_key_value,\
    \ attn_bias=attn_bias, attention_mask=attention_mask, is_causal=self.is_causal)\r\
    \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/blocks.py\"\
    , line 36, in forward\r\n    (b, _, past_key_value) = self.attn(a, past_key_value=past_key_value,\
    \ attn_bias=attn_bias, attention_mask=attention_mask, is_causal=is_causal)\r\n\
    \  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/attention.py\"\
    , line 171, in forward\r\n    (context, attn_weights) = self.attn_fn(query, key,\
    \ value, self.n_heads, softmax_scale=self.softmax_scale, attn_bias=attn_bias,\
    \ key_padding_mask=key_padding_mask, is_causal=is_causal, dropout_p=self.attn_dropout_p,\
    \ training=self.training, needs_weights=needs_weights)\r\n  File \"/u/user983/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/attention.py\"\
    , line 111, in triton_flash_attn_fn\r\n    attn_output = flash_attn_triton.flash_attn_func(query,\
    \ key, value, attn_bias, reset_is_causal, softmax_scale)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/function.py\"\
    , line 506, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\
    \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/flash_attn/flash_attn_triton.py\"\
    , line 810, in forward\r\n    o, lse, ctx.softmax_scale = _flash_attn_forward(\r\
    \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/flash_attn/flash_attn_triton.py\"\
    , line 623, in _flash_attn_forward\r\n    _fwd_kernel[grid](\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/runtime/autotuner.py\"\
    , line 199, in run\r\n    return self.fn.run(*args, **kwargs)\r\n  File \"<string>\"\
    , line 41, in _fwd_kernel\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 1620, in compile\r\n    next_module = compile(module)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 1549, in <lambda>\r\n    lambda src: ast_to_ttir(src, signature, configs[0],\
    \ constants)),\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 962, in ast_to_ttir\r\n    mod, _ = build_triton_ir(fn, signature, specialization,\
    \ constants)\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/triton/compiler.py\"\
    , line 942, in build_triton_ir\r\n    raise CompilationError(fn.src, node) from\
    \ e\r\ntriton.compiler.CompilationError: at 78:24:\r\ndef _fwd_kernel(\r\n   \
    \ Q, K, V, Bias, Out,\r\n    Lse, TMP,  # NOTE: TMP is a scratchpad buffer to\
    \ workaround a compiler bug\r\n    softmax_scale,\r\n    stride_qb, stride_qh,\
    \ stride_qm,\r\n    stride_kb, stride_kh, stride_kn,\r\n    stride_vb, stride_vh,\
    \ stride_vn,\r\n    stride_bb, stride_bh, stride_bm,\r\n    stride_ob, stride_oh,\
    \ stride_om,\r\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\r\n\
    \    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K,\r\n    BIAS_TYPE: tl.constexpr,\r\
    \n    IS_CAUSAL: tl.constexpr,\r\n    BLOCK_HEADDIM: tl.constexpr,\r\n    EVEN_M:\
    \ tl.constexpr, EVEN_N: tl.constexpr, EVEN_HEADDIM: tl.constexpr,\r\n    BLOCK_M:\
    \ tl.constexpr, BLOCK_N: tl.constexpr,\r\n):\r\n    start_m = tl.program_id(0)\r\
    \n    off_hb = tl.program_id(1)\r\n    off_b = off_hb // nheads\r\n    off_h =\
    \ off_hb % nheads\r\n    # off_b = tl.program_id(1)\r\n    # off_h = tl.program_id(2)\r\
    \n    # off_hb = off_b * nheads + off_h\r\n    # initialize offsets\r\n    offs_m\
    \ = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\r\n    offs_n = tl.arange(0, BLOCK_N)\r\
    \n    offs_d = tl.arange(0, BLOCK_HEADDIM)\r\n    # Initialize pointers to Q,\
    \ K, V\r\n    # Adding parenthesis around indexing might use int32 math instead\
    \ of int64 math?\r\n    # https://github.com/openai/triton/issues/741\r\n    #\
    \ I'm seeing a tiny bit of difference (5-7us)\r\n    q_ptrs = Q + off_b * stride_qb\
    \ + off_h * stride_qh + (offs_m[:, None] * stride_qm + offs_d[None, :])\r\n  \
    \  k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] * stride_kn\
    \ + offs_d[None, :])\r\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh\
    \ + (offs_n[:, None] * stride_vn + offs_d[None, :])\r\n    if BIAS_TYPE == 'vector':\r\
    \n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\r\n \
    \   elif BIAS_TYPE == 'matrix':\r\n        b_ptrs = Bias + off_b * stride_bb +\
    \ off_h * stride_bh + (offs_m[:, None] * stride_bm + offs_n[None, :])\r\n    #\
    \ initialize pointer to m and l\r\n    t_ptrs = TMP + off_hb * seqlen_q_rounded\
    \ + offs_m\r\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\"\
    )\r\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\r\n    acc_o\
    \ = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\r\n    # load q: it will\
    \ stay in SRAM throughout\r\n    # [2022-10-30] TD: Triton bug - in the case of\
    \ EVEN_M=True and EVEN_N=False, if we just call\r\n    # tl.load(q_ptrs), we get\
    \ the wrong output!\r\n    if EVEN_M & EVEN_N:\r\n        if EVEN_HEADDIM:\r\n\
    \            q = tl.load(q_ptrs)\r\n        else:\r\n            q = tl.load(q_ptrs,\
    \ mask=offs_d[None, :] < headdim, other=0.0)\r\n    else:\r\n        if EVEN_HEADDIM:\r\
    \n            q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\r\
    \n        else:\r\n            q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q)\
    \ & (offs_d[None, :] < headdim),\r\n                        other=0.0)\r\n   \
    \ # loop over k, v and update accumulator\r\n    end_n = seqlen_k if not IS_CAUSAL\
    \ else tl.minimum((start_m + 1) * BLOCK_M, seqlen_k)\r\n    for start_n in range(0,\
    \ end_n, BLOCK_N):\r\n        start_n = tl.multiple_of(start_n, BLOCK_N)\r\n \
    \       # -- compute qk ----\r\n        if EVEN_N & EVEN_M:  # If we just do \"\
    if EVEN_N\", there seems to be some race condition\r\n            if EVEN_HEADDIM:\r\
    \n                k = tl.load(k_ptrs + start_n * stride_kn)\r\n            else:\r\
    \n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\
    \ :] < headdim, other=0.0)\r\n        else:\r\n            if EVEN_HEADDIM:\r\n\
    \                k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n + offs_n)[:,\
    \ None] < seqlen_k,\r\n                            other=0.0)\r\n            else:\r\
    \n                k = tl.load(k_ptrs + start_n * stride_kn,\r\n              \
    \              mask=((start_n + offs_n)[:, None] < seqlen_k) & (offs_d[None, :]\
    \ < headdim),\r\n                            other=0.0)\r\n        qk = tl.zeros([BLOCK_M,\
    \ BLOCK_N], dtype=tl.float32)\r\n        qk += tl.dot(q, k, trans_b=True)\r\n\
    \                        ^\r\n\r\nDuring handling of the above exception, another\
    \ exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
    , line 2057, in showtraceback\r\n    stb = self.InteractiveTB.structured_traceback(\r\
    \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
    , line 1288, in structured_traceback\r\n    return FormattedTB.structured_traceback(\r\
    \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
    , line 1177, in structured_traceback\r\n    return VerboseTB.structured_traceback(\r\
    \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
    , line 1049, in structured_traceback\r\n    formatted_exceptions += self.format_exception_as_a_whole(etype,\
    \ evalue, etb, lines_of_context,\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
    , line 935, in format_exception_as_a_whole\r\n    self.get_records(etb, number_of_lines_of_context,\
    \ tb_offset) if etb else []\r\n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
    , line 1003, in get_records\r\n    lines, first = inspect.getsourcelines(etb.tb_frame)\r\
    \n  File \"/u/user983/miniconda3/envs/llama/lib/python3.10/inspect.py\", line\
    \ 1129, in getsourcelines\r\n    lines, lnum = findsource(object)\r\n  File \"\
    /u/user983/miniconda3/envs/llama/lib/python3.10/inspect.py\", line 958, in findsource\r\
    \n    raise OSError('could not get source code')\r\nOSError: could not get source\
    \ code\r\n```\r\nIt happens only when using triton attention (torch attention\
    \ works fine), using:\r\n```\r\nflash-attn==1.0.4\r\ntriton==2.0.0\r\n```\r\n\
    Any ideas?"
  created_at: 2023-05-07 14:49:05+00:00
  edited: false
  hidden: false
  id: 6457c871de963aff4190c41d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
      fullname: Daniel King
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daking
      type: user
    createdAt: '2023-05-08T17:47:04.000Z'
    data:
      edited: false
      editors:
      - daking
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
          fullname: Daniel King
          isHf: false
          isPro: false
          name: daking
          type: user
        html: '<p>The versions we use are listed here: <a rel="nofollow" href="https://github.com/mosaicml/llm-foundry/blob/3959eaccba53c444c5705d600d333cc3d47bc06c/setup.py#L74-L78">https://github.com/mosaicml/llm-foundry/blob/3959eaccba53c444c5705d600d333cc3d47bc06c/setup.py#L74-L78</a>.
          I''d suggest trying those.</p>

          '
        raw: 'The versions we use are listed here: https://github.com/mosaicml/llm-foundry/blob/3959eaccba53c444c5705d600d333cc3d47bc06c/setup.py#L74-L78.
          I''d suggest trying those.'
        updatedAt: '2023-05-08T17:47:04.695Z'
      numEdits: 0
      reactions: []
    id: 64593598232e5f0712b7b2ab
    type: comment
  author: daking
  content: 'The versions we use are listed here: https://github.com/mosaicml/llm-foundry/blob/3959eaccba53c444c5705d600d333cc3d47bc06c/setup.py#L74-L78.
    I''d suggest trying those.'
  created_at: 2023-05-08 16:47:04+00:00
  edited: false
  hidden: false
  id: 64593598232e5f0712b7b2ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78a8e7a380f1248fcc42a0a813e202fd.svg
      fullname: Roy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: roy650
      type: user
    createdAt: '2023-05-09T14:12:06.000Z'
    data:
      edited: false
      editors:
      - roy650
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78a8e7a380f1248fcc42a0a813e202fd.svg
          fullname: Roy
          isHf: false
          isPro: false
          name: roy650
          type: user
        html: '<p>Even after I align to the above setup I still get the same error.
          See my freeze below.<br>Also - note this issue from triton: <a rel="nofollow"
          href="https://github.com/openai/triton/issues/1054">https://github.com/openai/triton/issues/1054</a>
          where one of the maintainers hints that it might not work on a100...?</p>

          <pre><code>aiofiles==22.1.0

          aiosqlite==0.19.0

          anyio==3.6.2

          argon2-cffi==21.3.0

          argon2-cffi-bindings==21.2.0

          arrow==1.2.3

          asttokens==2.2.1

          attrs==23.1.0

          Babel==2.12.1

          backcall==0.2.0

          beautifulsoup4==4.12.2

          bleach==6.0.0

          certifi==2023.5.7

          cffi==1.15.1

          charset-normalizer==3.1.0

          cmake==3.26.3

          coloredlogs==15.0.1

          comm==0.1.3

          datasets==2.10.1

          debugpy==1.6.7

          decorator==5.1.1

          defusedxml==0.7.1

          einops==0.5.0

          executing==1.2.0

          fastjsonschema==2.16.3

          filelock==3.12.0

          flash-attn==1.0.3.post0

          flatbuffers==23.3.3

          fqdn==1.5.1

          fsspec==2023.5.0

          huggingface-hub==0.14.1

          idna==3.4

          install==1.3.5

          ipykernel==6.23.0

          ipython==8.13.2

          ipython-genutils==0.2.0

          ipywidgets==8.0.6

          isoduration==20.11.0

          jedi==0.18.2

          Jinja2==3.1.2

          json5==0.9.11

          jsonpointer==2.3

          jsonschema==4.17.3

          jupyter-events==0.6.3

          jupyter-ydoc==0.2.4

          jupyter_client==8.2.0

          jupyter_core==5.3.0

          jupyter_server==2.5.0

          jupyter_server_fileid==0.9.0

          jupyter_server_terminals==0.4.4

          jupyter_server_ydoc==0.8.0

          jupyterlab==3.6.3

          jupyterlab-pygments==0.2.2

          jupyterlab-widgets==3.0.7

          jupyterlab_server==2.22.1

          llm-foundry==1.0

          MarkupSafe==2.1.2

          matplotlib-inline==0.1.6

          mistune==2.0.5

          mosaicml==0.14.1

          mosaicml-cli==0.4.0a5

          nbclassic==1.0.0

          nbclient==0.7.4

          nbconvert==7.4.0

          nbformat==5.8.0

          nest-asyncio==1.5.6

          notebook==6.5.4

          notebook_shim==0.2.3

          numpy==1.24.3

          nvidia-cublas-cu11==11.10.3.66

          nvidia-cuda-nvrtc-cu11==11.7.99

          nvidia-cuda-runtime-cu11==11.7.99

          nvidia-cudnn-cu11==8.5.0.96

          omegaconf==2.3.0

          onnx==1.13.1

          onnxruntime==1.14.1

          packaging==23.1

          pandocfilters==1.5.0

          parso==0.8.3

          pexpect==4.8.0

          pickleshare==0.7.5

          platformdirs==3.5.0

          prometheus-client==0.16.0

          prompt-toolkit==3.0.38

          protobuf==4.23.0

          psutil==5.9.5

          ptyprocess==0.7.0

          pure-eval==0.2.2

          pycparser==2.21

          Pygments==2.15.1

          pynvml==11.5.0

          pyrsistent==0.19.3

          python-dateutil==2.8.2

          python-json-logger==2.0.7

          PyYAML==6.0

          pyzmq==25.0.2

          regex==2023.5.5

          requests==2.30.0

          rfc3339-validator==0.1.4

          rfc3986-validator==0.1.1

          Send2Trash==1.8.2

          sentencepiece==0.1.97

          six==1.16.0

          slack-sdk==3.21.3

          sniffio==1.3.0

          soupsieve==2.4.1

          stack-data==0.6.2

          sympy==1.12rc1

          terminado==0.17.1

          tinycss2==1.2.1

          tokenizers==0.13.3

          tomli==2.0.1

          torch==1.13.1

          tornado==6.3.1

          tqdm==4.65.0

          traitlets==5.9.0

          transformers==4.28.1

          triton==2.0.0.dev20221202

          typing_extensions==4.5.0

          uri-template==1.2.0

          urllib3==2.0.2

          wcwidth==0.2.6

          webcolors==1.13

          webencodings==0.5.1

          websocket-client==1.5.1

          widgetsnbextension==4.0.7

          xentropy-cuda-lib @ git+https://github.com/HazyResearch/flash-attention.git@33e0860c9c5667fded5af674882e731909096a7f#subdirectory=csrc/xentropy

          y-py==0.5.9

          ypy-websocket==0.8.2

          </code></pre>

          '
        raw: 'Even after I align to the above setup I still get the same error. See
          my freeze below.

          Also - note this issue from triton: https://github.com/openai/triton/issues/1054
          where one of the maintainers hints that it might not work on a100...?

          ```

          aiofiles==22.1.0

          aiosqlite==0.19.0

          anyio==3.6.2

          argon2-cffi==21.3.0

          argon2-cffi-bindings==21.2.0

          arrow==1.2.3

          asttokens==2.2.1

          attrs==23.1.0

          Babel==2.12.1

          backcall==0.2.0

          beautifulsoup4==4.12.2

          bleach==6.0.0

          certifi==2023.5.7

          cffi==1.15.1

          charset-normalizer==3.1.0

          cmake==3.26.3

          coloredlogs==15.0.1

          comm==0.1.3

          datasets==2.10.1

          debugpy==1.6.7

          decorator==5.1.1

          defusedxml==0.7.1

          einops==0.5.0

          executing==1.2.0

          fastjsonschema==2.16.3

          filelock==3.12.0

          flash-attn==1.0.3.post0

          flatbuffers==23.3.3

          fqdn==1.5.1

          fsspec==2023.5.0

          huggingface-hub==0.14.1

          idna==3.4

          install==1.3.5

          ipykernel==6.23.0

          ipython==8.13.2

          ipython-genutils==0.2.0

          ipywidgets==8.0.6

          isoduration==20.11.0

          jedi==0.18.2

          Jinja2==3.1.2

          json5==0.9.11

          jsonpointer==2.3

          jsonschema==4.17.3

          jupyter-events==0.6.3

          jupyter-ydoc==0.2.4

          jupyter_client==8.2.0

          jupyter_core==5.3.0

          jupyter_server==2.5.0

          jupyter_server_fileid==0.9.0

          jupyter_server_terminals==0.4.4

          jupyter_server_ydoc==0.8.0

          jupyterlab==3.6.3

          jupyterlab-pygments==0.2.2

          jupyterlab-widgets==3.0.7

          jupyterlab_server==2.22.1

          llm-foundry==1.0

          MarkupSafe==2.1.2

          matplotlib-inline==0.1.6

          mistune==2.0.5

          mosaicml==0.14.1

          mosaicml-cli==0.4.0a5

          nbclassic==1.0.0

          nbclient==0.7.4

          nbconvert==7.4.0

          nbformat==5.8.0

          nest-asyncio==1.5.6

          notebook==6.5.4

          notebook_shim==0.2.3

          numpy==1.24.3

          nvidia-cublas-cu11==11.10.3.66

          nvidia-cuda-nvrtc-cu11==11.7.99

          nvidia-cuda-runtime-cu11==11.7.99

          nvidia-cudnn-cu11==8.5.0.96

          omegaconf==2.3.0

          onnx==1.13.1

          onnxruntime==1.14.1

          packaging==23.1

          pandocfilters==1.5.0

          parso==0.8.3

          pexpect==4.8.0

          pickleshare==0.7.5

          platformdirs==3.5.0

          prometheus-client==0.16.0

          prompt-toolkit==3.0.38

          protobuf==4.23.0

          psutil==5.9.5

          ptyprocess==0.7.0

          pure-eval==0.2.2

          pycparser==2.21

          Pygments==2.15.1

          pynvml==11.5.0

          pyrsistent==0.19.3

          python-dateutil==2.8.2

          python-json-logger==2.0.7

          PyYAML==6.0

          pyzmq==25.0.2

          regex==2023.5.5

          requests==2.30.0

          rfc3339-validator==0.1.4

          rfc3986-validator==0.1.1

          Send2Trash==1.8.2

          sentencepiece==0.1.97

          six==1.16.0

          slack-sdk==3.21.3

          sniffio==1.3.0

          soupsieve==2.4.1

          stack-data==0.6.2

          sympy==1.12rc1

          terminado==0.17.1

          tinycss2==1.2.1

          tokenizers==0.13.3

          tomli==2.0.1

          torch==1.13.1

          tornado==6.3.1

          tqdm==4.65.0

          traitlets==5.9.0

          transformers==4.28.1

          triton==2.0.0.dev20221202

          typing_extensions==4.5.0

          uri-template==1.2.0

          urllib3==2.0.2

          wcwidth==0.2.6

          webcolors==1.13

          webencodings==0.5.1

          websocket-client==1.5.1

          widgetsnbextension==4.0.7

          xentropy-cuda-lib @ git+https://github.com/HazyResearch/flash-attention.git@33e0860c9c5667fded5af674882e731909096a7f#subdirectory=csrc/xentropy

          y-py==0.5.9

          ypy-websocket==0.8.2

          ```'
        updatedAt: '2023-05-09T14:12:06.079Z'
      numEdits: 0
      reactions: []
    id: 645a54b699a55c57325a695e
    type: comment
  author: roy650
  content: 'Even after I align to the above setup I still get the same error. See
    my freeze below.

    Also - note this issue from triton: https://github.com/openai/triton/issues/1054
    where one of the maintainers hints that it might not work on a100...?

    ```

    aiofiles==22.1.0

    aiosqlite==0.19.0

    anyio==3.6.2

    argon2-cffi==21.3.0

    argon2-cffi-bindings==21.2.0

    arrow==1.2.3

    asttokens==2.2.1

    attrs==23.1.0

    Babel==2.12.1

    backcall==0.2.0

    beautifulsoup4==4.12.2

    bleach==6.0.0

    certifi==2023.5.7

    cffi==1.15.1

    charset-normalizer==3.1.0

    cmake==3.26.3

    coloredlogs==15.0.1

    comm==0.1.3

    datasets==2.10.1

    debugpy==1.6.7

    decorator==5.1.1

    defusedxml==0.7.1

    einops==0.5.0

    executing==1.2.0

    fastjsonschema==2.16.3

    filelock==3.12.0

    flash-attn==1.0.3.post0

    flatbuffers==23.3.3

    fqdn==1.5.1

    fsspec==2023.5.0

    huggingface-hub==0.14.1

    idna==3.4

    install==1.3.5

    ipykernel==6.23.0

    ipython==8.13.2

    ipython-genutils==0.2.0

    ipywidgets==8.0.6

    isoduration==20.11.0

    jedi==0.18.2

    Jinja2==3.1.2

    json5==0.9.11

    jsonpointer==2.3

    jsonschema==4.17.3

    jupyter-events==0.6.3

    jupyter-ydoc==0.2.4

    jupyter_client==8.2.0

    jupyter_core==5.3.0

    jupyter_server==2.5.0

    jupyter_server_fileid==0.9.0

    jupyter_server_terminals==0.4.4

    jupyter_server_ydoc==0.8.0

    jupyterlab==3.6.3

    jupyterlab-pygments==0.2.2

    jupyterlab-widgets==3.0.7

    jupyterlab_server==2.22.1

    llm-foundry==1.0

    MarkupSafe==2.1.2

    matplotlib-inline==0.1.6

    mistune==2.0.5

    mosaicml==0.14.1

    mosaicml-cli==0.4.0a5

    nbclassic==1.0.0

    nbclient==0.7.4

    nbconvert==7.4.0

    nbformat==5.8.0

    nest-asyncio==1.5.6

    notebook==6.5.4

    notebook_shim==0.2.3

    numpy==1.24.3

    nvidia-cublas-cu11==11.10.3.66

    nvidia-cuda-nvrtc-cu11==11.7.99

    nvidia-cuda-runtime-cu11==11.7.99

    nvidia-cudnn-cu11==8.5.0.96

    omegaconf==2.3.0

    onnx==1.13.1

    onnxruntime==1.14.1

    packaging==23.1

    pandocfilters==1.5.0

    parso==0.8.3

    pexpect==4.8.0

    pickleshare==0.7.5

    platformdirs==3.5.0

    prometheus-client==0.16.0

    prompt-toolkit==3.0.38

    protobuf==4.23.0

    psutil==5.9.5

    ptyprocess==0.7.0

    pure-eval==0.2.2

    pycparser==2.21

    Pygments==2.15.1

    pynvml==11.5.0

    pyrsistent==0.19.3

    python-dateutil==2.8.2

    python-json-logger==2.0.7

    PyYAML==6.0

    pyzmq==25.0.2

    regex==2023.5.5

    requests==2.30.0

    rfc3339-validator==0.1.4

    rfc3986-validator==0.1.1

    Send2Trash==1.8.2

    sentencepiece==0.1.97

    six==1.16.0

    slack-sdk==3.21.3

    sniffio==1.3.0

    soupsieve==2.4.1

    stack-data==0.6.2

    sympy==1.12rc1

    terminado==0.17.1

    tinycss2==1.2.1

    tokenizers==0.13.3

    tomli==2.0.1

    torch==1.13.1

    tornado==6.3.1

    tqdm==4.65.0

    traitlets==5.9.0

    transformers==4.28.1

    triton==2.0.0.dev20221202

    typing_extensions==4.5.0

    uri-template==1.2.0

    urllib3==2.0.2

    wcwidth==0.2.6

    webcolors==1.13

    webencodings==0.5.1

    websocket-client==1.5.1

    widgetsnbextension==4.0.7

    xentropy-cuda-lib @ git+https://github.com/HazyResearch/flash-attention.git@33e0860c9c5667fded5af674882e731909096a7f#subdirectory=csrc/xentropy

    y-py==0.5.9

    ypy-websocket==0.8.2

    ```'
  created_at: 2023-05-09 13:12:06+00:00
  edited: false
  hidden: false
  id: 645a54b699a55c57325a695e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
      fullname: Daniel King
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daking
      type: user
    createdAt: '2023-05-09T16:56:02.000Z'
    data:
      edited: false
      editors:
      - daking
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
          fullname: Daniel King
          isHf: false
          isPro: false
          name: daking
          type: user
        html: '<p>We have done all of our testing on a100s, so it is possible it only
          works there. You can always go back to using the <code>torch</code> attention
          implementation if you aren''t able to run it with triton without a100s.</p>

          '
        raw: We have done all of our testing on a100s, so it is possible it only works
          there. You can always go back to using the `torch` attention implementation
          if you aren't able to run it with triton without a100s.
        updatedAt: '2023-05-09T16:56:02.265Z'
      numEdits: 0
      reactions: []
    id: 645a7b2216dd9c07825e772c
    type: comment
  author: daking
  content: We have done all of our testing on a100s, so it is possible it only works
    there. You can always go back to using the `torch` attention implementation if
    you aren't able to run it with triton without a100s.
  created_at: 2023-05-09 15:56:02+00:00
  edited: false
  hidden: false
  id: 645a7b2216dd9c07825e772c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435b7a4f3b08e267d9b03f6/Oa8UGthAt1MLRQVSgsXcS.jpeg?w=200&h=200&f=face
      fullname: Alex Trott
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: atrott
      type: user
    createdAt: '2023-05-09T18:12:39.000Z'
    data:
      edited: false
      editors:
      - atrott
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435b7a4f3b08e267d9b03f6/Oa8UGthAt1MLRQVSgsXcS.jpeg?w=200&h=200&f=face
          fullname: Alex Trott
          isHf: false
          isPro: false
          name: atrott
          type: user
        html: '<p>I don''t spot any obvious issues with your installed packages, but
          you might have better luck starting from our recommended docker image, if
          possible: <code>mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04</code></p>

          '
        raw: 'I don''t spot any obvious issues with your installed packages, but you
          might have better luck starting from our recommended docker image, if possible:
          `mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04`'
        updatedAt: '2023-05-09T18:12:39.837Z'
      numEdits: 0
      reactions: []
    id: 645a8d17bf3f9fbb8c6997ae
    type: comment
  author: atrott
  content: 'I don''t spot any obvious issues with your installed packages, but you
    might have better luck starting from our recommended docker image, if possible:
    `mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04`'
  created_at: 2023-05-09 17:12:39+00:00
  edited: false
  hidden: false
  id: 645a8d17bf3f9fbb8c6997ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78a8e7a380f1248fcc42a0a813e202fd.svg
      fullname: Roy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: roy650
      type: user
    createdAt: '2023-05-10T05:46:38.000Z'
    data:
      edited: false
      editors:
      - roy650
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78a8e7a380f1248fcc42a0a813e202fd.svg
          fullname: Roy
          isHf: false
          isPro: false
          name: roy650
          type: user
        html: '<p>Thanks, I''ll try that.<br>FWIW - with the torch attention it''s
          functioning fine (though somewhat slow)</p>

          '
        raw: 'Thanks, I''ll try that.

          FWIW - with the torch attention it''s functioning fine (though somewhat
          slow)'
        updatedAt: '2023-05-10T05:46:38.557Z'
      numEdits: 0
      reactions: []
    id: 645b2fbe52c5b0d155998689
    type: comment
  author: roy650
  content: 'Thanks, I''ll try that.

    FWIW - with the torch attention it''s functioning fine (though somewhat slow)'
  created_at: 2023-05-10 04:46:38+00:00
  edited: false
  hidden: false
  id: 645b2fbe52c5b0d155998689
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661118445899-noauth.png?w=200&h=200&f=face
      fullname: Robert Michael Smith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saber7ooth
      type: user
    createdAt: '2023-05-10T07:25:28.000Z'
    data:
      edited: true
      editors:
      - saber7ooth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661118445899-noauth.png?w=200&h=200&f=face
          fullname: Robert Michael Smith
          isHf: false
          isPro: false
          name: saber7ooth
          type: user
        html: '<p>Managed to get it to run locally (CPU only on a sytem with 64 GB
          RAM, on torch attention with cpu_low_mem=True), can we apply transformers
          accelerators to shard to to CPU if we run out of VRAM?   I honestly wish
          to torch export as onyx so I can run CPU only but I currently struggle to
          load the model in my local jupyter notebook.</p>

          '
        raw: Managed to get it to run locally (CPU only on a sytem with 64 GB RAM,
          on torch attention with cpu_low_mem=True), can we apply transformers accelerators
          to shard to to CPU if we run out of VRAM?   I honestly wish to torch export
          as onyx so I can run CPU only but I currently struggle to load the model
          in my local jupyter notebook.
        updatedAt: '2023-05-10T07:28:35.729Z'
      numEdits: 3
      reactions: []
    id: 645b46e85bb5418fba33720e
    type: comment
  author: saber7ooth
  content: Managed to get it to run locally (CPU only on a sytem with 64 GB RAM, on
    torch attention with cpu_low_mem=True), can we apply transformers accelerators
    to shard to to CPU if we run out of VRAM?   I honestly wish to torch export as
    onyx so I can run CPU only but I currently struggle to load the model in my local
    jupyter notebook.
  created_at: 2023-05-10 06:25:28+00:00
  edited: true
  hidden: false
  id: 645b46e85bb5418fba33720e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e88ae33e3b4b8501e37817261478d03.svg
      fullname: xingxiangrui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luoji12345
      type: user
    createdAt: '2023-05-18T02:41:14.000Z'
    data:
      edited: false
      editors:
      - luoji12345
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e88ae33e3b4b8501e37817261478d03.svg
          fullname: xingxiangrui
          isHf: false
          isPro: false
          name: luoji12345
          type: user
        html: '<p>I meet the same Error with this problem when I Run it on the A100
          with triton, It seems that it is an confilct between triton and CUDA. Because
          I tried many version of  flash-attn and triton.<br>If anyone have progress,
          Please leave a comment.  Triton is faster than torch and it is worth trying.</p>

          '
        raw: 'I meet the same Error with this problem when I Run it on the A100 with
          triton, It seems that it is an confilct between triton and CUDA. Because
          I tried many version of  flash-attn and triton.

          If anyone have progress, Please leave a comment.  Triton is faster than
          torch and it is worth trying.'
        updatedAt: '2023-05-18T02:41:14.126Z'
      numEdits: 0
      reactions: []
    id: 6465904a86e668ad22ebe055
    type: comment
  author: luoji12345
  content: 'I meet the same Error with this problem when I Run it on the A100 with
    triton, It seems that it is an confilct between triton and CUDA. Because I tried
    many version of  flash-attn and triton.

    If anyone have progress, Please leave a comment.  Triton is faster than torch
    and it is worth trying.'
  created_at: 2023-05-18 01:41:14+00:00
  edited: false
  hidden: false
  id: 6465904a86e668ad22ebe055
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
      fullname: Dmytro Ishchenko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dimaischenko
      type: user
    createdAt: '2023-05-18T08:09:54.000Z'
    data:
      edited: false
      editors:
      - dimaischenko
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
          fullname: Dmytro Ishchenko
          isHf: false
          isPro: false
          name: dimaischenko
          type: user
        html: '<blockquote>

          <p>I meet the same Error with this problem when I Run it on the A100 with
          triton, It seems that it is an confilct between triton and CUDA. Because
          I tried many version of  flash-attn and triton.<br>If anyone have progress,
          Please leave a comment.  Triton is faster than torch and it is worth trying.</p>

          </blockquote>

          <p>+1, the same error, i tried <code>torch 1.13</code> and <code>torch 2.0</code>
          with <code>CUDA 11.7</code> and <code>RTX 3090</code> GPU. Without using
          <code>triton</code> everything works great</p>

          '
        raw: '> I meet the same Error with this problem when I Run it on the A100
          with triton, It seems that it is an confilct between triton and CUDA. Because
          I tried many version of  flash-attn and triton.

          > If anyone have progress, Please leave a comment.  Triton is faster than
          torch and it is worth trying.


          +1, the same error, i tried `torch 1.13` and `torch 2.0` with `CUDA 11.7`
          and `RTX 3090` GPU. Without using `triton` everything works great'
        updatedAt: '2023-05-18T08:09:54.277Z'
      numEdits: 0
      reactions: []
    id: 6465dd52119ad94383c8adf1
    type: comment
  author: dimaischenko
  content: '> I meet the same Error with this problem when I Run it on the A100 with
    triton, It seems that it is an confilct between triton and CUDA. Because I tried
    many version of  flash-attn and triton.

    > If anyone have progress, Please leave a comment.  Triton is faster than torch
    and it is worth trying.


    +1, the same error, i tried `torch 1.13` and `torch 2.0` with `CUDA 11.7` and
    `RTX 3090` GPU. Without using `triton` everything works great'
  created_at: 2023-05-18 07:09:54+00:00
  edited: false
  hidden: false
  id: 6465dd52119ad94383c8adf1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
      fullname: Dmytro Ishchenko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dimaischenko
      type: user
    createdAt: '2023-05-18T09:17:48.000Z'
    data:
      edited: true
      editors:
      - dimaischenko
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
          fullname: Dmytro Ishchenko
          isHf: false
          isPro: false
          name: dimaischenko
          type: user
        html: '<blockquote>

          <p>I don''t spot any obvious issues with your installed packages, but you
          might have better luck starting from our recommended docker image, if possible:
          <code>mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04</code></p>

          </blockquote>

          <p>Tried this docker from scratch. The same error on <code>RTX 3090</code></p>

          '
        raw: '> I don''t spot any obvious issues with your installed packages, but
          you might have better luck starting from our recommended docker image, if
          possible: `mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04`


          Tried this docker from scratch. The same error on `RTX 3090`'
        updatedAt: '2023-05-18T09:17:58.156Z'
      numEdits: 1
      reactions: []
    id: 6465ed3ce0fe831b478b503e
    type: comment
  author: dimaischenko
  content: '> I don''t spot any obvious issues with your installed packages, but you
    might have better luck starting from our recommended docker image, if possible:
    `mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04`


    Tried this docker from scratch. The same error on `RTX 3090`'
  created_at: 2023-05-18 08:17:48+00:00
  edited: true
  hidden: false
  id: 6465ed3ce0fe831b478b503e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
      fullname: Dmytro Ishchenko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dimaischenko
      type: user
    createdAt: '2023-05-18T10:00:47.000Z'
    data:
      edited: false
      editors:
      - dimaischenko
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
          fullname: Dmytro Ishchenko
          isHf: false
          isPro: false
          name: dimaischenko
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I don''t spot any obvious issues with your installed packages, but you
          might have better luck starting from our recommended docker image, if possible:
          <code>mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04</code></p>

          </blockquote>

          <p>Tried this docker from scratch. The same error on <code>RTX 3090</code></p>

          </blockquote>

          <p>Finally it works with <code>RTX 3090</code> this <code>mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04</code>
          docker container. I re-installed some python packages in container. Final
          subset of pip freeze: </p>

          <pre><code>triton==2.0.0.dev20221202

          torch==1.13.1+cu117

          transformers==4.29.2

          flash-attn==1.0.3.post0

          </code></pre>

          '
        raw: "> > I don't spot any obvious issues with your installed packages, but\
          \ you might have better luck starting from our recommended docker image,\
          \ if possible: `mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04`\n\
          > \n> Tried this docker from scratch. The same error on `RTX 3090`\n\nFinally\
          \ it works with `RTX 3090` this `mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04`\
          \ docker container. I re-installed some python packages in container. Final\
          \ subset of pip freeze: \n\n```\ntriton==2.0.0.dev20221202\ntorch==1.13.1+cu117\n\
          transformers==4.29.2\nflash-attn==1.0.3.post0\n```"
        updatedAt: '2023-05-18T10:00:47.674Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - abhi-mosaic
    id: 6465f74fe0fe831b478bffc2
    type: comment
  author: dimaischenko
  content: "> > I don't spot any obvious issues with your installed packages, but\
    \ you might have better luck starting from our recommended docker image, if possible:\
    \ `mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04`\n> \n> Tried this docker\
    \ from scratch. The same error on `RTX 3090`\n\nFinally it works with `RTX 3090`\
    \ this `mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04` docker container.\
    \ I re-installed some python packages in container. Final subset of pip freeze:\
    \ \n\n```\ntriton==2.0.0.dev20221202\ntorch==1.13.1+cu117\ntransformers==4.29.2\n\
    flash-attn==1.0.3.post0\n```"
  created_at: 2023-05-18 09:00:47+00:00
  edited: false
  hidden: false
  id: 6465f74fe0fe831b478bffc2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-18T16:58:46.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: '<p>Torch 2.0 uses a version of Triton that uses a version of MLIR which
          breaks ALiBi, so those of you who are seeing issues with Torch2... we are
          aware of this and are working on a fix. Closing this as completed because
          it does work with the correct versions of the dependencies, which can be
          found in LLM-Foundry</p>

          '
        raw: Torch 2.0 uses a version of Triton that uses a version of MLIR which
          breaks ALiBi, so those of you who are seeing issues with Torch2... we are
          aware of this and are working on a fix. Closing this as completed because
          it does work with the correct versions of the dependencies, which can be
          found in LLM-Foundry
        updatedAt: '2023-05-18T16:58:46.467Z'
      numEdits: 0
      reactions: []
      relatedEventId: 646659463b99ed9970fd3a34
    id: 646659463b99ed9970fd3a33
    type: comment
  author: sam-mosaic
  content: Torch 2.0 uses a version of Triton that uses a version of MLIR which breaks
    ALiBi, so those of you who are seeing issues with Torch2... we are aware of this
    and are working on a fix. Closing this as completed because it does work with
    the correct versions of the dependencies, which can be found in LLM-Foundry
  created_at: 2023-05-18 15:58:46+00:00
  edited: false
  hidden: false
  id: 646659463b99ed9970fd3a33
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-18T16:58:46.000Z'
    data:
      status: closed
    id: 646659463b99ed9970fd3a34
    type: status-change
  author: sam-mosaic
  created_at: 2023-05-18 15:58:46+00:00
  id: 646659463b99ed9970fd3a34
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e88ae33e3b4b8501e37817261478d03.svg
      fullname: xingxiangrui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luoji12345
      type: user
    createdAt: '2023-05-20T02:40:18.000Z'
    data:
      edited: false
      editors:
      - luoji12345
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e88ae33e3b4b8501e37817261478d03.svg
          fullname: xingxiangrui
          isHf: false
          isPro: false
          name: luoji12345
          type: user
        html: '<p>Thanks everyone, I have fix this error by install  <code>transformers==4.29.2</code>.  It''s
          a conflict between transformer and triton, finally I did not use <code>mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04</code>
          docker.  Just make sure the folling package version can solve the problem.</p>

          <p><code>triton==2.0.0.dev20221202 torch==1.13.1+cu117 transformers==4.29.2
          flash-attn==1.0.3.post0</code></p>

          <blockquote>

          <p>Torch 2.0 uses a version of Triton that uses a version of MLIR which
          breaks ALiBi, so those of you who are seeing issues with Torch2... we are
          aware of this and are working on a fix. Closing this as completed because
          it does work with the correct versions of the dependencies, which can be
          found in LLM-Foundry</p>

          </blockquote>

          <p>Not exactly, If you install flash-attn package using pip, the torch 2.0
          will be automatically uninstalled, and torch 1.13 will be installed automatically
          to match the flash-attn version. So this is a problem between transformer
          and triton, not torch 2.0.<br> You can specify the transformer version in
          the  model card to avoid other people fail into this error.</p>

          '
        raw: "Thanks everyone, I have fix this error by install  `transformers==4.29.2`.\
          \  It's a conflict between transformer and triton, finally I did not use\
          \ `mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04` docker.  Just make\
          \ sure the folling package version can solve the problem.\n\n`\ntriton==2.0.0.dev20221202\n\
          torch==1.13.1+cu117\ntransformers==4.29.2\nflash-attn==1.0.3.post0\n`\n\n\
          > Torch 2.0 uses a version of Triton that uses a version of MLIR which breaks\
          \ ALiBi, so those of you who are seeing issues with Torch2... we are aware\
          \ of this and are working on a fix. Closing this as completed because it\
          \ does work with the correct versions of the dependencies, which can be\
          \ found in LLM-Foundry\n\nNot exactly, If you install flash-attn package\
          \ using pip, the torch 2.0 will be automatically uninstalled, and torch\
          \ 1.13 will be installed automatically to match the flash-attn version.\
          \ So this is a problem between transformer and triton, not torch 2.0.\n\
          \ You can specify the transformer version in the  model card to avoid other\
          \ people fail into this error."
        updatedAt: '2023-05-20T02:40:18.133Z'
      numEdits: 0
      reactions: []
    id: 646833123a7c8dda230f87ab
    type: comment
  author: luoji12345
  content: "Thanks everyone, I have fix this error by install  `transformers==4.29.2`.\
    \  It's a conflict between transformer and triton, finally I did not use `mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04`\
    \ docker.  Just make sure the folling package version can solve the problem.\n\
    \n`\ntriton==2.0.0.dev20221202\ntorch==1.13.1+cu117\ntransformers==4.29.2\nflash-attn==1.0.3.post0\n\
    `\n\n> Torch 2.0 uses a version of Triton that uses a version of MLIR which breaks\
    \ ALiBi, so those of you who are seeing issues with Torch2... we are aware of\
    \ this and are working on a fix. Closing this as completed because it does work\
    \ with the correct versions of the dependencies, which can be found in LLM-Foundry\n\
    \nNot exactly, If you install flash-attn package using pip, the torch 2.0 will\
    \ be automatically uninstalled, and torch 1.13 will be installed automatically\
    \ to match the flash-attn version. So this is a problem between transformer and\
    \ triton, not torch 2.0.\n You can specify the transformer version in the  model\
    \ card to avoid other people fail into this error."
  created_at: 2023-05-20 01:40:18+00:00
  edited: false
  hidden: false
  id: 646833123a7c8dda230f87ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af72012bd2779e3be544d0b579c1f9ae.svg
      fullname: Yiren Liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yirenl2
      type: user
    createdAt: '2023-07-06T22:40:53.000Z'
    data:
      edited: true
      editors:
      - yirenl2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9233775734901428
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af72012bd2779e3be544d0b579c1f9ae.svg
          fullname: Yiren Liu
          isHf: false
          isPro: false
          name: yirenl2
          type: user
        html: '<p>I had the same issue when installing on my local machine --- the
          tricky part for me is that my card is running on CUDA 12.1, so the flash-attn
          throws error for Torch not compiled in the correct version when I tried
          to run the training in torch==1.13.1+cu117 ... And only the nightly version
          of torch&gt;2.0 offers CUDA 12.1 support. </p>

          <p>The cause is basically the issue mentioned here: </p>

          <blockquote>

          <p>Torch 2.0 uses a version of Triton that uses a version of MLIR which
          breaks ALiBi, so those of you who are seeing issues with Torch2... we are
          aware of this and are working on a fix. Closing this as completed because
          it does work with the correct versions of the dependencies, which can be
          found in LLM-Foundry</p>

          </blockquote>

          <p>My workaround is simply to<br><code>pip uninstall pytorch-triton</code><br>and
          then<br><code>pip install triton==2.0.0.dev20221202 --no-deps</code><br>otherwise
          pip forces reinstallation of torch. </p>

          <p>Hope this is helpful for whoever comes later...</p>

          '
        raw: "I had the same issue when installing on my local machine --- the tricky\
          \ part for me is that my card is running on CUDA 12.1, so the flash-attn\
          \ throws error for Torch not compiled in the correct version when I tried\
          \ to run the training in torch==1.13.1+cu117 ... And only the nightly version\
          \ of torch>2.0 offers CUDA 12.1 support. \n\nThe cause is basically the\
          \ issue mentioned here: \n> Torch 2.0 uses a version of Triton that uses\
          \ a version of MLIR which breaks ALiBi, so those of you who are seeing issues\
          \ with Torch2... we are aware of this and are working on a fix. Closing\
          \ this as completed because it does work with the correct versions of the\
          \ dependencies, which can be found in LLM-Foundry\n\nMy workaround is simply\
          \ to \n```pip uninstall pytorch-triton```\nand then \n```pip install triton==2.0.0.dev20221202\
          \ --no-deps```\notherwise pip forces reinstallation of torch. \n\nHope this\
          \ is helpful for whoever comes later..."
        updatedAt: '2023-07-06T22:45:20.346Z'
      numEdits: 2
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - keithnlp
        - Boss302
        - cdyzhao91
        - kimihailv
    id: 64a742f5d738314d685eed4a
    type: comment
  author: yirenl2
  content: "I had the same issue when installing on my local machine --- the tricky\
    \ part for me is that my card is running on CUDA 12.1, so the flash-attn throws\
    \ error for Torch not compiled in the correct version when I tried to run the\
    \ training in torch==1.13.1+cu117 ... And only the nightly version of torch>2.0\
    \ offers CUDA 12.1 support. \n\nThe cause is basically the issue mentioned here:\
    \ \n> Torch 2.0 uses a version of Triton that uses a version of MLIR which breaks\
    \ ALiBi, so those of you who are seeing issues with Torch2... we are aware of\
    \ this and are working on a fix. Closing this as completed because it does work\
    \ with the correct versions of the dependencies, which can be found in LLM-Foundry\n\
    \nMy workaround is simply to \n```pip uninstall pytorch-triton```\nand then \n\
    ```pip install triton==2.0.0.dev20221202 --no-deps```\notherwise pip forces reinstallation\
    \ of torch. \n\nHope this is helpful for whoever comes later..."
  created_at: 2023-07-06 21:40:53+00:00
  edited: true
  hidden: false
  id: 64a742f5d738314d685eed4a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: mosaicml/mpt-7b-storywriter
repo_type: model
status: closed
target_branch: null
title: error while generating
