!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RonanMcGovern
conflicting_files: null
created_at: 2023-05-29 09:42:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-05-29T10:42:49.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>It would be super useful to have more than 2048 sequence length
          for chat or instruct.</p>

          '
        raw: It would be super useful to have more than 2048 sequence length for chat
          or instruct.
        updatedAt: '2023-05-29T10:42:49.767Z'
      numEdits: 0
      reactions: []
    id: 647481a9d815855e4ef482a1
    type: comment
  author: RonanMcGovern
  content: It would be super useful to have more than 2048 sequence length for chat
    or instruct.
  created_at: 2023-05-29 09:42:49+00:00
  edited: false
  hidden: false
  id: 647481a9d815855e4ef482a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676407968549-63dd38e5a8877129a15a12c5.png?w=200&h=200&f=face
      fullname: Jacob Portes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jacobfulano
      type: user
    createdAt: '2023-05-31T02:15:46.000Z'
    data:
      edited: false
      editors:
      - jacobfulano
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676407968549-63dd38e5a8877129a15a12c5.png?w=200&h=200&f=face
          fullname: Jacob Portes
          isHf: false
          isPro: false
          name: jacobfulano
          type: user
        html: "<p>In the model cards we explain how to take advantage of ALiBi so\
          \ that you can increase maximum sequence length during inference:</p>\n\
          <blockquote>\n<p>Although the model was trained with a sequence length of\
          \ 2048, ALiBi enables users to increase the maximum sequence length during\
          \ finetuning and/or inference. For example:</p>\n</blockquote>\n<pre><code>config\
          \ = transformers.AutoConfig.from_pretrained(\n  'mosaicml/mpt-7b-instruct',\n\
          \  trust_remote_code=True\n)\nconfig.update({\"max_seq_len\": 4096})\nmodel\
          \ = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b-instruct',\n\
          \  config=config,\n  trust_remote_code=True\n)\n</code></pre>\n"
        raw: "In the model cards we explain how to take advantage of ALiBi so that\
          \ you can increase maximum sequence length during inference:\n\n> Although\
          \ the model was trained with a sequence length of 2048, ALiBi enables users\
          \ to increase the maximum sequence length during finetuning and/or inference.\
          \ For example:\n\n```\nconfig = transformers.AutoConfig.from_pretrained(\n\
          \  'mosaicml/mpt-7b-instruct',\n  trust_remote_code=True\n)\nconfig.update({\"\
          max_seq_len\": 4096})\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \  'mosaicml/mpt-7b-instruct',\n  config=config,\n  trust_remote_code=True\n\
          )\n```"
        updatedAt: '2023-05-31T02:15:46.866Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6476add226ec66674c6de024
    id: 6476add226ec66674c6de023
    type: comment
  author: jacobfulano
  content: "In the model cards we explain how to take advantage of ALiBi so that you\
    \ can increase maximum sequence length during inference:\n\n> Although the model\
    \ was trained with a sequence length of 2048, ALiBi enables users to increase\
    \ the maximum sequence length during finetuning and/or inference. For example:\n\
    \n```\nconfig = transformers.AutoConfig.from_pretrained(\n  'mosaicml/mpt-7b-instruct',\n\
    \  trust_remote_code=True\n)\nconfig.update({\"max_seq_len\": 4096})\nmodel =\
    \ transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b-instruct',\n\
    \  config=config,\n  trust_remote_code=True\n)\n```"
  created_at: 2023-05-31 01:15:46+00:00
  edited: false
  hidden: false
  id: 6476add226ec66674c6de023
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676407968549-63dd38e5a8877129a15a12c5.png?w=200&h=200&f=face
      fullname: Jacob Portes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jacobfulano
      type: user
    createdAt: '2023-05-31T02:15:46.000Z'
    data:
      status: closed
    id: 6476add226ec66674c6de024
    type: status-change
  author: jacobfulano
  created_at: 2023-05-31 01:15:46+00:00
  id: 6476add226ec66674c6de024
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-05-31T12:26:36.000Z'
    data:
      status: open
    id: 64773cfc242dde316303c208
    type: status-change
  author: RonanMcGovern
  created_at: 2023-05-31 11:26:36+00:00
  id: 64773cfc242dde316303c208
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-05-31T12:29:47.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Many thanks <span data-props=\"{&quot;user&quot;:&quot;jacobfulano&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jacobfulano\"\
          >@<span class=\"underline\">jacobfulano</span></a></span>\n\n\t</span></span>\
          \ .</p>\n<p>Is it possible to directly deploy with that configuration using\
          \ Sagemaker, or would I have to take a route like building a docker image\
          \ including the config update? Thanks</p>\n"
        raw: 'Many thanks @jacobfulano .


          Is it possible to directly deploy with that configuration using Sagemaker,
          or would I have to take a route like building a docker image including the
          config update? Thanks'
        updatedAt: '2023-05-31T12:29:47.619Z'
      numEdits: 0
      reactions: []
    id: 64773dbb40c99df87605187e
    type: comment
  author: RonanMcGovern
  content: 'Many thanks @jacobfulano .


    Is it possible to directly deploy with that configuration using Sagemaker, or
    would I have to take a route like building a docker image including the config
    update? Thanks'
  created_at: 2023-05-31 11:29:47+00:00
  edited: false
  hidden: false
  id: 64773dbb40c99df87605187e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-03T00:15:29.000Z'
    data:
      edited: false
      editors:
      - abhi-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9110308885574341
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: "<p>HI <span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ , I'm not sure what the limitations of Sagemaker are, but as long as you\
          \ can pass in a custom HF config, you can set <code>config.max_seq_len=4096</code>\
          \ dynamically at start time. It's just a config arg, no need for custom\
          \ Docker images or anything.</p>\n"
        raw: HI @RonanMcGovern , I'm not sure what the limitations of Sagemaker are,
          but as long as you can pass in a custom HF config, you can set `config.max_seq_len=4096`
          dynamically at start time. It's just a config arg, no need for custom Docker
          images or anything.
        updatedAt: '2023-06-03T00:15:29.806Z'
      numEdits: 0
      reactions: []
      relatedEventId: 647a862142abe277476178db
    id: 647a862142abe277476178d9
    type: comment
  author: abhi-mosaic
  content: HI @RonanMcGovern , I'm not sure what the limitations of Sagemaker are,
    but as long as you can pass in a custom HF config, you can set `config.max_seq_len=4096`
    dynamically at start time. It's just a config arg, no need for custom Docker images
    or anything.
  created_at: 2023-06-02 23:15:29+00:00
  edited: false
  hidden: false
  id: 647a862142abe277476178d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-03T00:15:29.000Z'
    data:
      status: closed
    id: 647a862142abe277476178db
    type: status-change
  author: abhi-mosaic
  created_at: 2023-06-02 23:15:29+00:00
  id: 647a862142abe277476178db
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-07-14T14:18:25.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.401769757270813
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;abhi-mosaic&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/abhi-mosaic\"\
          >@<span class=\"underline\">abhi-mosaic</span></a></span>\n\n\t</span></span>\
          \ I've managed to get MPT-7B running well on Google collab with:</p>\n<pre><code>\
          \    # use cache directory while loading model and tokenizer\n    self.model\
          \ = AutoModelForCausalLM.from_pretrained(\n        model_name,\n       \
          \ cache_dir=cache_dir,\n        torch_dtype=torch_dtype,\n        trust_remote_code=trust_remote_code,\n\
          \        use_auth_token=use_auth_token,\n    )\n</code></pre>\n<p>~~~</p>\n\
          <p>I'm then trying to add in the config for max_seq_len but not getting\
          \ any joy:</p>\n<pre><code>    # Load the configuration\n    config = transformers.AutoConfig.from_pretrained(model_name,\n\
          \                                                    trust_remote_code=trust_remote_code)\n\
          \    # Explicitly set the max_seq_len\n    config.max_seq_len = 4096\n\n\
          \    # Load the model with the updated configuration\n    self.model = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \        model_name,\n        config=config,\n        cache_dir=cache_dir,\n\
          \        torch_dtype=torch_dtype,\n        trust_remote_code=trust_remote_code,\n\
          \        use_auth_token=use_auth_token,\n    )\n</code></pre>\n"
        raw: "Hi @abhi-mosaic I've managed to get MPT-7B running well on Google collab\
          \ with:\n\n        # use cache directory while loading model and tokenizer\n\
          \        self.model = AutoModelForCausalLM.from_pretrained(\n          \
          \  model_name,\n            cache_dir=cache_dir,\n            torch_dtype=torch_dtype,\n\
          \            trust_remote_code=trust_remote_code,\n            use_auth_token=use_auth_token,\n\
          \        )\n\n~~~\n\nI'm then trying to add in the config for max_seq_len\
          \ but not getting any joy:\n\n        # Load the configuration\n       \
          \ config = transformers.AutoConfig.from_pretrained(model_name,\n       \
          \                                                 trust_remote_code=trust_remote_code)\n\
          \        # Explicitly set the max_seq_len\n        config.max_seq_len =\
          \ 4096\n\n        # Load the model with the updated configuration\n    \
          \    self.model = transformers.AutoModelForCausalLM.from_pretrained(\n \
          \           model_name,\n            config=config,\n            cache_dir=cache_dir,\n\
          \            torch_dtype=torch_dtype,\n            trust_remote_code=trust_remote_code,\n\
          \            use_auth_token=use_auth_token,\n        )"
        updatedAt: '2023-07-14T14:18:25.259Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64b15931520fa3a154a674b7
    id: 64b15931520fa3a154a674b5
    type: comment
  author: RonanMcGovern
  content: "Hi @abhi-mosaic I've managed to get MPT-7B running well on Google collab\
    \ with:\n\n        # use cache directory while loading model and tokenizer\n \
    \       self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n\
    \            cache_dir=cache_dir,\n            torch_dtype=torch_dtype,\n    \
    \        trust_remote_code=trust_remote_code,\n            use_auth_token=use_auth_token,\n\
    \        )\n\n~~~\n\nI'm then trying to add in the config for max_seq_len but\
    \ not getting any joy:\n\n        # Load the configuration\n        config = transformers.AutoConfig.from_pretrained(model_name,\n\
    \                                                        trust_remote_code=trust_remote_code)\n\
    \        # Explicitly set the max_seq_len\n        config.max_seq_len = 4096\n\
    \n        # Load the model with the updated configuration\n        self.model\
    \ = transformers.AutoModelForCausalLM.from_pretrained(\n            model_name,\n\
    \            config=config,\n            cache_dir=cache_dir,\n            torch_dtype=torch_dtype,\n\
    \            trust_remote_code=trust_remote_code,\n            use_auth_token=use_auth_token,\n\
    \        )"
  created_at: 2023-07-14 13:18:25+00:00
  edited: false
  hidden: false
  id: 64b15931520fa3a154a674b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-07-14T14:18:25.000Z'
    data:
      status: open
    id: 64b15931520fa3a154a674b7
    type: status-change
  author: RonanMcGovern
  created_at: 2023-07-14 13:18:25+00:00
  id: 64b15931520fa3a154a674b7
  new_status: open
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 29
repo_id: mosaicml/mpt-7b-storywriter
repo_type: model
status: open
target_branch: null
title: Any reason why this longer context length wasn't applied to the chat and instruct
  versions?
