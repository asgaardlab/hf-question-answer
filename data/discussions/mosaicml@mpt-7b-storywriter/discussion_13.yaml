!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mfajcik
conflicting_files: null
created_at: 2023-05-09 15:21:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ac6c76184e72534f940c9f91a814c46.svg
      fullname: Fajcik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mfajcik
      type: user
    createdAt: '2023-05-09T16:21:15.000Z'
    data:
      edited: true
      editors:
      - mfajcik
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ac6c76184e72534f940c9f91a814c46.svg
          fullname: Fajcik
          isHf: false
          isPro: false
          name: mfajcik
          type: user
        html: '<p>Hello,</p>

          <p>this model card says</p>

          <blockquote>

          <p>Training Configuration<br>This model was trained on <strong>8 A100-80GBs</strong>
          for about 2 days using the MosaicML Platform. The model was trained with
          sharded data parallelism using FSDP and used the LION optimizer.</p>

          </blockquote>

          <p>however, <a rel="nofollow" href="https://www.mosaicml.com/blog/mpt-7b">blogpost</a>
          says</p>

          <blockquote>

          <p>Most open-source language models can only handle sequences with up to
          a few thousand tokens (see Figure 1). But with the MosaicML platform and
          a single node of <strong>8xA100-40GB</strong>, you can easily finetune MPT-7B
          to handle context lengths up to 65k! </p>

          </blockquote>

          <p>So, just to avoid confusion, is it possible to finetune this model with
          ~60k context sizes on 8x40GBs A100?</p>

          <p>Cheers,<br>Martin</p>

          '
        raw: "Hello,\n\nthis model card says\n\n> Training Configuration\nThis model\
          \ was trained on __8 A100-80GBs__ for about 2 days using the MosaicML Platform.\
          \ The model was trained with sharded data parallelism using FSDP and used\
          \ the LION optimizer.\n\nhowever, [blogpost](https://www.mosaicml.com/blog/mpt-7b)\
          \ says\n> Most open-source language models can only handle sequences with\
          \ up to a few thousand tokens (see Figure 1). But with the MosaicML platform\
          \ and a single node of __8xA100-40GB__, you can easily finetune MPT-7B to\
          \ handle context lengths up to 65k! \n\nSo, just to avoid confusion, is\
          \ it possible to finetune this model with ~60k context sizes on 8x40GBs\
          \ A100?\n\nCheers,\nMartin"
        updatedAt: '2023-05-09T16:22:41.857Z'
      numEdits: 1
      reactions: []
    id: 645a72fbd8ba048d02ab36f6
    type: comment
  author: mfajcik
  content: "Hello,\n\nthis model card says\n\n> Training Configuration\nThis model\
    \ was trained on __8 A100-80GBs__ for about 2 days using the MosaicML Platform.\
    \ The model was trained with sharded data parallelism using FSDP and used the\
    \ LION optimizer.\n\nhowever, [blogpost](https://www.mosaicml.com/blog/mpt-7b)\
    \ says\n> Most open-source language models can only handle sequences with up to\
    \ a few thousand tokens (see Figure 1). But with the MosaicML platform and a single\
    \ node of __8xA100-40GB__, you can easily finetune MPT-7B to handle context lengths\
    \ up to 65k! \n\nSo, just to avoid confusion, is it possible to finetune this\
    \ model with ~60k context sizes on 8x40GBs A100?\n\nCheers,\nMartin"
  created_at: 2023-05-09 15:21:15+00:00
  edited: true
  hidden: false
  id: 645a72fbd8ba048d02ab36f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435b7a4f3b08e267d9b03f6/Oa8UGthAt1MLRQVSgsXcS.jpeg?w=200&h=200&f=face
      fullname: Alex Trott
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: atrott
      type: user
    createdAt: '2023-05-09T17:20:34.000Z'
    data:
      edited: false
      editors:
      - atrott
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435b7a4f3b08e267d9b03f6/Oa8UGthAt1MLRQVSgsXcS.jpeg?w=200&h=200&f=face
          fullname: Alex Trott
          isHf: false
          isPro: false
          name: atrott
          type: user
        html: '<p>Hi, Martin. Thanks for catching that error in our blog post! And
          sorry for the confusion!<br>The model card is correct, though. You need
          a pretty hefty GPU memory to train on 65k, and we used A100-80GB cards throughout.
          We haven''t profiled this yet, but I would expect that with 8 A100-40GB
          cards, you could finetune on closer to 30k, give or take.</p>

          '
        raw: 'Hi, Martin. Thanks for catching that error in our blog post! And sorry
          for the confusion!

          The model card is correct, though. You need a pretty hefty GPU memory to
          train on 65k, and we used A100-80GB cards throughout. We haven''t profiled
          this yet, but I would expect that with 8 A100-40GB cards, you could finetune
          on closer to 30k, give or take.'
        updatedAt: '2023-05-09T17:20:34.444Z'
      numEdits: 0
      reactions: []
    id: 645a80e2e505443f8193de2e
    type: comment
  author: atrott
  content: 'Hi, Martin. Thanks for catching that error in our blog post! And sorry
    for the confusion!

    The model card is correct, though. You need a pretty hefty GPU memory to train
    on 65k, and we used A100-80GB cards throughout. We haven''t profiled this yet,
    but I would expect that with 8 A100-40GB cards, you could finetune on closer to
    30k, give or take.'
  created_at: 2023-05-09 16:20:34+00:00
  edited: false
  hidden: false
  id: 645a80e2e505443f8193de2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435b7a4f3b08e267d9b03f6/Oa8UGthAt1MLRQVSgsXcS.jpeg?w=200&h=200&f=face
      fullname: Alex Trott
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: atrott
      type: user
    createdAt: '2023-05-09T18:08:44.000Z'
    data:
      edited: false
      editors:
      - atrott
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435b7a4f3b08e267d9b03f6/Oa8UGthAt1MLRQVSgsXcS.jpeg?w=200&h=200&f=face
          fullname: Alex Trott
          isHf: false
          isPro: false
          name: atrott
          type: user
        html: '<p>The blog has been updated. Thanks again for catching that!</p>

          '
        raw: The blog has been updated. Thanks again for catching that!
        updatedAt: '2023-05-09T18:08:44.762Z'
      numEdits: 0
      reactions: []
      relatedEventId: 645a8c2ce505443f81947289
    id: 645a8c2ce505443f81947288
    type: comment
  author: atrott
  content: The blog has been updated. Thanks again for catching that!
  created_at: 2023-05-09 17:08:44+00:00
  edited: false
  hidden: false
  id: 645a8c2ce505443f81947288
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435b7a4f3b08e267d9b03f6/Oa8UGthAt1MLRQVSgsXcS.jpeg?w=200&h=200&f=face
      fullname: Alex Trott
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: atrott
      type: user
    createdAt: '2023-05-09T18:08:44.000Z'
    data:
      status: closed
    id: 645a8c2ce505443f81947289
    type: status-change
  author: atrott
  created_at: 2023-05-09 17:08:44+00:00
  id: 645a8c2ce505443f81947289
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: mosaicml/mpt-7b-storywriter
repo_type: model
status: closed
target_branch: null
title: Discrepancy in HW description
