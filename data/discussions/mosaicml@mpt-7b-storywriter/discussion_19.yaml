!!python/object:huggingface_hub.community.DiscussionWithDetails
author: teneriffa
conflicting_files: null
created_at: 2023-05-21 09:59:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/589e3f19a6b466a6b90a1ff222e82d2d.svg
      fullname: teneriffa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teneriffa
      type: user
    createdAt: '2023-05-21T10:59:11.000Z'
    data:
      edited: false
      editors:
      - teneriffa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/589e3f19a6b466a6b90a1ff222e82d2d.svg
          fullname: teneriffa
          isHf: false
          isPro: false
          name: teneriffa
          type: user
        html: "<p>Weights looked like loaded but actual chat was failed just as described\
          \ in the title. I'm using Apple Silicon M1 computer, macOS Ventura 13.3.1(a),\
          \ and tried to load and use this weights on oobabooga's text-generation-webui,\
          \ then failed.</p>\n<p>Oobabooga's text-generation-webui uses Hugging Face's\
          \ transformers Python module for Hugging Face weights models, and installed\
          \ transformers version is 4.28.0. Torch version is 2.0.1 and torchvision\
          \ version is 0.15.2.</p>\n<p>I have used many Hugging Face weights models\
          \ but have never seen like this before. Even I tried to run with PYTORCH_ENABLE_MPS_FALLBACK=1\
          \ environment variable, this was happened.I read the error message, so I\
          \ know it is related with torch.autocast. Does anybody know how to resolve\
          \ this problem?</p>\n<pre><code>INFO:Loading mosaicml_mpt-7b-storywriter...\n\
          Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2\
          \ [00:03&lt;00:00,  1.78s/it]\nINFO:Loaded the model in 4.76 seconds.\n\n\
          Starting streaming server at ws://127.0.0.1:5005/api/v1/stream\nINFO:Loading\
          \ the extension \"gallery\"...\nINFO:server listening on 127.0.0.1:5005\n\
          Starting API at http://127.0.0.1:5000/api\nRunning on local URL:  http://127.0.0.1:8860\n\
          \nTo create a public link, set `share=True` in `launch()`.\n/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py:690:\
          \ UserWarning: MPS: no support for int64 repeats mask, casting it to int32\
          \ (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\n\
          \  input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/attention.py:263:\
          \ UserWarning: The operator 'aten::pow.Scalar_out' is not currently supported\
          \ on the MPS backend and will fall back to run on the CPU. This may have\
          \ performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n\
          \  slopes = 1.0 / torch.pow(2, m)\nTraceback (most recent call last):\n\
          \  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/text-generation-webui/modules/callbacks.py\"\
          , line 73, in gentask\n    ret = self.mfunc(callback=_callback, **self.kwargs)\n\
          \  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/text-generation-webui/modules/text_generation.py\"\
          , line 263, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1485, in generate\n    return self.sample(\n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2524, in sample\n    outputs = self(\n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/modeling_mpt.py\"\
          , line 237, in forward\n    outputs = self.transformer(input_ids=input_ids,\
          \ past_key_values=past_key_values, attention_mask=attention_mask, prefix_mask=prefix_mask,\
          \ sequence_id=sequence_id, return_dict=return_dict, output_attentions=output_attentions,\
          \ output_hidden_states=output_hidden_states, use_cache=use_cache)\n  File\
          \ \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/modeling_mpt.py\"\
          , line 183, in forward\n    (x, past_key_value) = block(x, past_key_value=past_key_value,\
          \ attn_bias=attn_bias, attention_mask=attention_mask, is_causal=self.is_causal)\n\
          \  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/blocks.py\"\
          , line 35, in forward\n    a = self.norm_1(x)\n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/norm.py\"\
          , line 24, in forward\n    with torch.autocast(enabled=False, device_type=module_device.type):\n\
          \  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/amp/autocast_mode.py\"\
          , line 201, in __init__\n    raise RuntimeError('User specified autocast\
          \ device_type must be \\'cuda\\' or \\'cpu\\'')\nRuntimeError: User specified\
          \ autocast device_type must be 'cuda' or 'cpu'\nOutput generated in 0.35\
          \ seconds (0.00 tokens/s, 0 tokens, context 72, seed 553060808)\n</code></pre>\n"
        raw: "Weights looked like loaded but actual chat was failed just as described\
          \ in the title. I'm using Apple Silicon M1 computer, macOS Ventura 13.3.1(a),\
          \ and tried to load and use this weights on oobabooga's text-generation-webui,\
          \ then failed.\r\n\r\nOobabooga's text-generation-webui uses Hugging Face's\
          \ transformers Python module for Hugging Face weights models, and installed\
          \ transformers version is 4.28.0. Torch version is 2.0.1 and torchvision\
          \ version is 0.15.2.\r\n\r\nI have used many Hugging Face weights models\
          \ but have never seen like this before. Even I tried to run with PYTORCH_ENABLE_MPS_FALLBACK=1\
          \ environment variable, this was happened.I read the error message, so I\
          \ know it is related with torch.autocast. Does anybody know how to resolve\
          \ this problem?\r\n\r\n```\r\nINFO:Loading mosaicml_mpt-7b-storywriter...\r\
          \nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2\
          \ [00:03<00:00,  1.78s/it]\r\nINFO:Loaded the model in 4.76 seconds.\r\n\
          \r\nStarting streaming server at ws://127.0.0.1:5005/api/v1/stream\r\nINFO:Loading\
          \ the extension \"gallery\"...\r\nINFO:server listening on 127.0.0.1:5005\r\
          \nStarting API at http://127.0.0.1:5000/api\r\nRunning on local URL:  http://127.0.0.1:8860\r\
          \n\r\nTo create a public link, set `share=True` in `launch()`.\r\n/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py:690:\
          \ UserWarning: MPS: no support for int64 repeats mask, casting it to int32\
          \ (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\r\
          \n  input_ids = input_ids.repeat_interleave(expand_size, dim=0)\r\n/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/attention.py:263:\
          \ UserWarning: The operator 'aten::pow.Scalar_out' is not currently supported\
          \ on the MPS backend and will fall back to run on the CPU. This may have\
          \ performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\
          \n  slopes = 1.0 / torch.pow(2, m)\r\nTraceback (most recent call last):\r\
          \n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/text-generation-webui/modules/callbacks.py\"\
          , line 73, in gentask\r\n    ret = self.mfunc(callback=_callback, **self.kwargs)\r\
          \n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/text-generation-webui/modules/text_generation.py\"\
          , line 263, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
          \n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1485, in generate\r\n    return self.sample(\r\n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2524, in sample\r\n    outputs = self(\r\n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/modeling_mpt.py\"\
          , line 237, in forward\r\n    outputs = self.transformer(input_ids=input_ids,\
          \ past_key_values=past_key_values, attention_mask=attention_mask, prefix_mask=prefix_mask,\
          \ sequence_id=sequence_id, return_dict=return_dict, output_attentions=output_attentions,\
          \ output_hidden_states=output_hidden_states, use_cache=use_cache)\r\n  File\
          \ \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/modeling_mpt.py\"\
          , line 183, in forward\r\n    (x, past_key_value) = block(x, past_key_value=past_key_value,\
          \ attn_bias=attn_bias, attention_mask=attention_mask, is_causal=self.is_causal)\r\
          \n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/blocks.py\"\
          , line 35, in forward\r\n    a = self.norm_1(x)\r\n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/norm.py\"\
          , line 24, in forward\r\n    with torch.autocast(enabled=False, device_type=module_device.type):\r\
          \n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/amp/autocast_mode.py\"\
          , line 201, in __init__\r\n    raise RuntimeError('User specified autocast\
          \ device_type must be \\'cuda\\' or \\'cpu\\'')\r\nRuntimeError: User specified\
          \ autocast device_type must be 'cuda' or 'cpu'\r\nOutput generated in 0.35\
          \ seconds (0.00 tokens/s, 0 tokens, context 72, seed 553060808)\r\n```"
        updatedAt: '2023-05-21T10:59:11.206Z'
      numEdits: 0
      reactions: []
    id: 6469f97f3eb2bab0419944e6
    type: comment
  author: teneriffa
  content: "Weights looked like loaded but actual chat was failed just as described\
    \ in the title. I'm using Apple Silicon M1 computer, macOS Ventura 13.3.1(a),\
    \ and tried to load and use this weights on oobabooga's text-generation-webui,\
    \ then failed.\r\n\r\nOobabooga's text-generation-webui uses Hugging Face's transformers\
    \ Python module for Hugging Face weights models, and installed transformers version\
    \ is 4.28.0. Torch version is 2.0.1 and torchvision version is 0.15.2.\r\n\r\n\
    I have used many Hugging Face weights models but have never seen like this before.\
    \ Even I tried to run with PYTORCH_ENABLE_MPS_FALLBACK=1 environment variable,\
    \ this was happened.I read the error message, so I know it is related with torch.autocast.\
    \ Does anybody know how to resolve this problem?\r\n\r\n```\r\nINFO:Loading mosaicml_mpt-7b-storywriter...\r\
    \nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:03<00:00,\
    \  1.78s/it]\r\nINFO:Loaded the model in 4.76 seconds.\r\n\r\nStarting streaming\
    \ server at ws://127.0.0.1:5005/api/v1/stream\r\nINFO:Loading the extension \"\
    gallery\"...\r\nINFO:server listening on 127.0.0.1:5005\r\nStarting API at http://127.0.0.1:5000/api\r\
    \nRunning on local URL:  http://127.0.0.1:8860\r\n\r\nTo create a public link,\
    \ set `share=True` in `launch()`.\r\n/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py:690:\
    \ UserWarning: MPS: no support for int64 repeats mask, casting it to int32 (Triggered\
    \ internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\r\
    \n  input_ids = input_ids.repeat_interleave(expand_size, dim=0)\r\n/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/attention.py:263:\
    \ UserWarning: The operator 'aten::pow.Scalar_out' is not currently supported\
    \ on the MPS backend and will fall back to run on the CPU. This may have performance\
    \ implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\
    \n  slopes = 1.0 / torch.pow(2, m)\r\nTraceback (most recent call last):\r\n \
    \ File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/text-generation-webui/modules/callbacks.py\"\
    , line 73, in gentask\r\n    ret = self.mfunc(callback=_callback, **self.kwargs)\r\
    \n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/text-generation-webui/modules/text_generation.py\"\
    , line 263, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
    \n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1485, in generate\r\n    return self.sample(\r\n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2524, in sample\r\n    outputs = self(\r\n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/modeling_mpt.py\"\
    , line 237, in forward\r\n    outputs = self.transformer(input_ids=input_ids,\
    \ past_key_values=past_key_values, attention_mask=attention_mask, prefix_mask=prefix_mask,\
    \ sequence_id=sequence_id, return_dict=return_dict, output_attentions=output_attentions,\
    \ output_hidden_states=output_hidden_states, use_cache=use_cache)\r\n  File \"\
    /Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/modeling_mpt.py\"\
    , line 183, in forward\r\n    (x, past_key_value) = block(x, past_key_value=past_key_value,\
    \ attn_bias=attn_bias, attention_mask=attention_mask, is_causal=self.is_causal)\r\
    \n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/blocks.py\"\
    , line 35, in forward\r\n    a = self.norm_1(x)\r\n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/Users/knut/.cache/huggingface/modules/transformers_modules/mosaicml_mpt-7b-storywriter/norm.py\"\
    , line 24, in forward\r\n    with torch.autocast(enabled=False, device_type=module_device.type):\r\
    \n  File \"/Volumes/cuttingedge/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/amp/autocast_mode.py\"\
    , line 201, in __init__\r\n    raise RuntimeError('User specified autocast device_type\
    \ must be \\'cuda\\' or \\'cpu\\'')\r\nRuntimeError: User specified autocast device_type\
    \ must be 'cuda' or 'cpu'\r\nOutput generated in 0.35 seconds (0.00 tokens/s,\
    \ 0 tokens, context 72, seed 553060808)\r\n```"
  created_at: 2023-05-21 09:59:11+00:00
  edited: false
  hidden: false
  id: 6469f97f3eb2bab0419944e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-06-14T06:34:12.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9289889335632324
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: "<p>There are some community resources for using this model in oobabooga,\
          \ such as <a rel=\"nofollow\" href=\"https://youtu.be/QVVb6Md6huA\">https://youtu.be/QVVb6Md6huA</a>\
          \ \u2014 hopefully that helps. </p>\n"
        raw: "There are some community resources for using this model in oobabooga,\
          \ such as https://youtu.be/QVVb6Md6huA \u2014 hopefully that helps. "
        updatedAt: '2023-06-14T06:34:12.465Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64895f64373ac422e0b148d0
    id: 64895f64373ac422e0b148c6
    type: comment
  author: sam-mosaic
  content: "There are some community resources for using this model in oobabooga,\
    \ such as https://youtu.be/QVVb6Md6huA \u2014 hopefully that helps. "
  created_at: 2023-06-14 05:34:12+00:00
  edited: false
  hidden: false
  id: 64895f64373ac422e0b148c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-06-14T06:34:12.000Z'
    data:
      status: closed
    id: 64895f64373ac422e0b148d0
    type: status-change
  author: sam-mosaic
  created_at: 2023-06-14 05:34:12+00:00
  id: 64895f64373ac422e0b148d0
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/589e3f19a6b466a6b90a1ff222e82d2d.svg
      fullname: teneriffa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teneriffa
      type: user
    createdAt: '2023-06-15T13:56:45.000Z'
    data:
      edited: false
      editors:
      - teneriffa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.992064356803894
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/589e3f19a6b466a6b90a1ff222e82d2d.svg
          fullname: teneriffa
          isHf: false
          isPro: false
          name: teneriffa
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sam-mosaic&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sam-mosaic\">@<span class=\"\
          underline\">sam-mosaic</span></a></span>\n\n\t</span></span> Thanks but\
          \ it didn't help at all.</p>\n"
        raw: '@sam-mosaic Thanks but it didn''t help at all.

          '
        updatedAt: '2023-06-15T13:56:45.557Z'
      numEdits: 0
      reactions: []
    id: 648b189d9b6b947fce895b92
    type: comment
  author: teneriffa
  content: '@sam-mosaic Thanks but it didn''t help at all.

    '
  created_at: 2023-06-15 12:56:45+00:00
  edited: false
  hidden: false
  id: 648b189d9b6b947fce895b92
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: mosaicml/mpt-7b-storywriter
repo_type: model
status: closed
target_branch: null
title: Running failed on oobabooga's text-generation-webui, macOS, M1
