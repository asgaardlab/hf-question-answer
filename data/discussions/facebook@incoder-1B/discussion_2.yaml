!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sh0416
conflicting_files: null
created_at: 2023-08-07 15:50:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e8c1aa005338da15ac60cf0699771b4.svg
      fullname: Seonghyeon Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sh0416
      type: user
    createdAt: '2023-08-07T16:50:12.000Z'
    data:
      edited: false
      editors:
      - sh0416
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48285847902297974
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e8c1aa005338da15ac60cf0699771b4.svg
          fullname: Seonghyeon Lee
          isHf: false
          isPro: false
          name: sh0416
          type: user
        html: '<pre><code>from transformers import AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained("facebook/incoder-1B")

          assert "from ." == tokenizer.decode(tokenizer("from .")["input_ids"], skip_special_tokens=True,
          cleanup_tokenization_spaces=False)

          </code></pre>

          <p>Raise an assertion error. I suspect that the encoding process remove
          space.. How to handle it?</p>

          '
        raw: "```\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          facebook/incoder-1B\")\r\nassert \"from .\" == tokenizer.decode(tokenizer(\"\
          from .\")[\"input_ids\"], skip_special_tokens=True, cleanup_tokenization_spaces=False)\r\
          \n```\r\nRaise an assertion error. I suspect that the encoding process remove\
          \ space.. How to handle it?"
        updatedAt: '2023-08-07T16:50:12.144Z'
      numEdits: 0
      reactions: []
    id: 64d120c47e20ec9ea0ea9e1b
    type: comment
  author: sh0416
  content: "```\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    facebook/incoder-1B\")\r\nassert \"from .\" == tokenizer.decode(tokenizer(\"from\
    \ .\")[\"input_ids\"], skip_special_tokens=True, cleanup_tokenization_spaces=False)\r\
    \n```\r\nRaise an assertion error. I suspect that the encoding process remove\
    \ space.. How to handle it?"
  created_at: 2023-08-07 15:50:12+00:00
  edited: false
  hidden: false
  id: 64d120c47e20ec9ea0ea9e1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e8c1aa005338da15ac60cf0699771b4.svg
      fullname: Seonghyeon Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sh0416
      type: user
    createdAt: '2023-08-07T16:53:41.000Z'
    data:
      edited: false
      editors:
      - sh0416
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7268882989883423
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e8c1aa005338da15ac60cf0699771b4.svg
          fullname: Seonghyeon Lee
          isHf: false
          isPro: false
          name: sh0416
          type: user
        html: '<p>Oh, there is a typo in option.. <code>clean_up_tokenization_spaces=False</code>
          works great. Sorry for the confusion.</p>

          '
        raw: Oh, there is a typo in option.. `clean_up_tokenization_spaces=False`
          works great. Sorry for the confusion.
        updatedAt: '2023-08-07T16:53:41.442Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64d12195484264a3b318ccba
    id: 64d12195484264a3b318ccb8
    type: comment
  author: sh0416
  content: Oh, there is a typo in option.. `clean_up_tokenization_spaces=False` works
    great. Sorry for the confusion.
  created_at: 2023-08-07 15:53:41+00:00
  edited: false
  hidden: false
  id: 64d12195484264a3b318ccb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3e8c1aa005338da15ac60cf0699771b4.svg
      fullname: Seonghyeon Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sh0416
      type: user
    createdAt: '2023-08-07T16:53:41.000Z'
    data:
      status: closed
    id: 64d12195484264a3b318ccba
    type: status-change
  author: sh0416
  created_at: 2023-08-07 15:53:41+00:00
  id: 64d12195484264a3b318ccba
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: facebook/incoder-1B
repo_type: model
status: closed
target_branch: null
title: Tokenization issue
