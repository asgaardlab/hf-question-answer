!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alvations
conflicting_files: null
created_at: 2023-08-17 00:35:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0e9087f2672b0e4f28d91266acf9ce57.svg
      fullname: Liling
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvations
      type: user
    createdAt: '2023-08-17T01:35:53.000Z'
    data:
      edited: true
      editors:
      - alvations
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33475789427757263
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0e9087f2672b0e4f28d91266acf9ce57.svg
          fullname: Liling
          isHf: false
          isPro: false
          name: alvations
          type: user
        html: "<p>With this input, </p>\n<blockquote>\n<p>\"\u05EA\u05D0\u05D2\u05D9\
          \u05D3 \u05D4\u05D3\u05DC\u05E7 \u05D5\u05D4\u05DB\u05D9\u05DE\u05D9\u05E7\
          \u05DC\u05D9\u05DD \u05E9\u05DC \u05E1\u05D9\u05DF (\u05D1\u05D0\u05E0\u05D2\
          \u05DC\u05D9\u05EA: China Petroleum &amp; Chemical Corporation, \u05D1\u05E1\
          \u05D9\u05E0\u05D9\u05EA: \u4E2D\u56FD\u77F3\u6CB9\u5316\u5DE5\u80A1\u4EFD\
          \u6709\u9650\u516C\u53F8) \u05D0\u05D5 \u05E1\u05D9\u05E0\u05D5\u05E4\u05E7\
          \ (\u05D0\u05E0\u05D2\u05DC\u05D9\u05EA: Sinopec Limited, \u05E1\u05D9\u05E0\
          \u05D9\u05EA: \u4E2D\u56FD\u77F3\u5316, \u05E1\u05D9\u05E0\u05D9\u05EA \u05DE\
          \u05E1\u05D5\u05E8\u05EA\u05D9\u05EA: \u4E2D\u570B\u77F3\u5316, \u05E4\u05D9\
          \u05DF-\u05D9\u05D9\u05DF: Zh\u014Dnggu\xF3 Sh\xEDhu\xE0) \u05D4\u05D9\u05D0\
          \ \u05EA\u05D0\u05D2\u05D9\u05D3 \u05E0\u05E4\u05D8 \u05D5\u05D2\u05D6 \u05DE\
          \u05DE\u05E9\u05DC\u05EA\u05D9 \u05E1\u05D9\u05E0\u05D9 \u05D4\u05DE\u05DE\
          \u05D5\u05E7\u05DD \u05D1\u05D1\u05D9\u05D9\u05D2'\u05D9\u05E0\u05D2.\"\
          </p>\n</blockquote>\n<p>the model throws the error for <code>transformers==4.31.0</code>:</p>\n\
          <pre><code class=\"language-python\">---------------------------------------------------------------------------\n\
          IndexError                                Traceback (most recent call last)\n\
          &lt;ipython-<span class=\"hljs-built_in\">input</span>-<span class=\"hljs-number\"\
          >31</span>-e2292311132f&gt; <span class=\"hljs-keyword\">in</span> &lt;module&gt;\n\
          ----&gt; <span class=\"hljs-number\">1</span> translator(s)\n\n~/Library/Python/<span\
          \ class=\"hljs-number\">3.8</span>/lib/python/site-packages/transformers/pipelines/text2text_generation.py\
          \ <span class=\"hljs-keyword\">in</span> __call__(self, *args, **kwargs)\n\
          \    <span class=\"hljs-number\">365</span>               token ids of the\
          \ translation.\n    <span class=\"hljs-number\">366</span>         <span\
          \ class=\"hljs-string\">\"\"\"</span>\n<span class=\"hljs-string\">--&gt;\
          \ 367         return super().__call__(*args, **kwargs)</span>\n<span class=\"\
          hljs-string\"></span>\n<span class=\"hljs-string\">~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/text2text_generation.py\
          \ in __call__(self, *args, **kwargs)</span>\n<span class=\"hljs-string\"\
          >    163         \"\"\"</span>\n    <span class=\"hljs-number\">164</span>\
          \ \n--&gt; <span class=\"hljs-number\">165</span>         result = <span\
          \ class=\"hljs-built_in\">super</span>().__call__(*args, **kwargs)\n   \
          \ <span class=\"hljs-number\">166</span>         <span class=\"hljs-keyword\"\
          >if</span> (\n    <span class=\"hljs-number\">167</span>             <span\
          \ class=\"hljs-built_in\">isinstance</span>(args[<span class=\"hljs-number\"\
          >0</span>], <span class=\"hljs-built_in\">list</span>)\n\n~/Library/Python/<span\
          \ class=\"hljs-number\">3.8</span>/lib/python/site-packages/transformers/pipelines/base.py\
          \ <span class=\"hljs-keyword\">in</span> __call__(self, inputs, num_workers,\
          \ batch_size, *args, **kwargs)\n   <span class=\"hljs-number\">1120</span>\
          \             )\n   <span class=\"hljs-number\">1121</span>         <span\
          \ class=\"hljs-keyword\">else</span>:\n-&gt; <span class=\"hljs-number\"\
          >1122</span>             <span class=\"hljs-keyword\">return</span> self.run_single(inputs,\
          \ preprocess_params, forward_params, postprocess_params)\n   <span class=\"\
          hljs-number\">1123</span> \n   <span class=\"hljs-number\">1124</span> \
          \    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >run_multi</span>(<span class=\"hljs-params\">self, inputs, preprocess_params,\
          \ forward_params, postprocess_params</span>):\n\n~/Library/Python/<span\
          \ class=\"hljs-number\">3.8</span>/lib/python/site-packages/transformers/pipelines/base.py\
          \ <span class=\"hljs-keyword\">in</span> run_single(self, inputs, preprocess_params,\
          \ forward_params, postprocess_params)\n   <span class=\"hljs-number\">1127</span>\
          \     <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >run_single</span>(<span class=\"hljs-params\">self, inputs, preprocess_params,\
          \ forward_params, postprocess_params</span>):\n   <span class=\"hljs-number\"\
          >1128</span>         model_inputs = self.preprocess(inputs, **preprocess_params)\n\
          -&gt; <span class=\"hljs-number\">1129</span>         model_outputs = self.forward(model_inputs,\
          \ **forward_params)\n   <span class=\"hljs-number\">1130</span>        \
          \ outputs = self.postprocess(model_outputs, **postprocess_params)\n   <span\
          \ class=\"hljs-number\">1131</span>         <span class=\"hljs-keyword\"\
          >return</span> outputs\n\n~/Library/Python/<span class=\"hljs-number\">3.8</span>/lib/python/site-packages/transformers/pipelines/base.py\
          \ <span class=\"hljs-keyword\">in</span> forward(self, model_inputs, **forward_params)\n\
          \   <span class=\"hljs-number\">1026</span>                 <span class=\"\
          hljs-keyword\">with</span> inference_context():\n   <span class=\"hljs-number\"\
          >1027</span>                     model_inputs = self._ensure_tensor_on_device(model_inputs,\
          \ device=self.device)\n-&gt; <span class=\"hljs-number\">1028</span>   \
          \                  model_outputs = self._forward(model_inputs, **forward_params)\n\
          \   <span class=\"hljs-number\">1029</span>                     model_outputs\
          \ = self._ensure_tensor_on_device(model_outputs, device=torch.device(<span\
          \ class=\"hljs-string\">\"cpu\"</span>))\n   <span class=\"hljs-number\"\
          >1030</span>             <span class=\"hljs-keyword\">else</span>:\n\n~/Library/Python/<span\
          \ class=\"hljs-number\">3.8</span>/lib/python/site-packages/transformers/pipelines/text2text_generation.py\
          \ <span class=\"hljs-keyword\">in</span> _forward(self, model_inputs, **generate_kwargs)\n\
          \    <span class=\"hljs-number\">185</span>         generate_kwargs[<span\
          \ class=\"hljs-string\">\"max_length\"</span>] = generate_kwargs.get(<span\
          \ class=\"hljs-string\">\"max_length\"</span>, self.model.config.max_length)\n\
          \    <span class=\"hljs-number\">186</span>         self.check_inputs(input_length,\
          \ generate_kwargs[<span class=\"hljs-string\">\"min_length\"</span>], generate_kwargs[<span\
          \ class=\"hljs-string\">\"max_length\"</span>])\n--&gt; <span class=\"hljs-number\"\
          >187</span>         output_ids = self.model.generate(**model_inputs, **generate_kwargs)\n\
          \    <span class=\"hljs-number\">188</span>         out_b = output_ids.shape[<span\
          \ class=\"hljs-number\">0</span>]\n    <span class=\"hljs-number\">189</span>\
          \         <span class=\"hljs-keyword\">if</span> self.framework == <span\
          \ class=\"hljs-string\">\"pt\"</span>:\n\n~/Library/Python/<span class=\"\
          hljs-number\">3.8</span>/lib/python/site-packages/torch/utils/_contextlib.py\
          \ <span class=\"hljs-keyword\">in</span> decorate_context(*args, **kwargs)\n\
          \n~/Library/Python/<span class=\"hljs-number\">3.8</span>/lib/python/site-packages/transformers/generation/utils.py\
          \ <span class=\"hljs-keyword\">in</span> generate(self, inputs, generation_config,\
          \ logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus,\
          \ assistant_model, streamer, **kwargs)\n   <span class=\"hljs-number\">1625</span>\
          \             )\n   <span class=\"hljs-number\">1626</span>            \
          \ <span class=\"hljs-comment\"># 13. run beam search</span>\n-&gt; <span\
          \ class=\"hljs-number\">1627</span>             <span class=\"hljs-keyword\"\
          >return</span> self.beam_search(\n   <span class=\"hljs-number\">1628</span>\
          \                 input_ids,\n   <span class=\"hljs-number\">1629</span>\
          \                 beam_scorer,\n\n~/Library/Python/<span class=\"hljs-number\"\
          >3.8</span>/lib/python/site-packages/transformers/generation/utils.py <span\
          \ class=\"hljs-keyword\">in</span> beam_search(self, input_ids, beam_scorer,\
          \ logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id,\
          \ output_attentions, output_hidden_states, output_scores, return_dict_in_generate,\
          \ synced_gpus, **model_kwargs)\n   <span class=\"hljs-number\">2930</span>\
          \             model_inputs = self.prepare_inputs_for_generation(input_ids,\
          \ **model_kwargs)\n   <span class=\"hljs-number\">2931</span> \n-&gt; <span\
          \ class=\"hljs-number\">2932</span>             outputs = self(\n   <span\
          \ class=\"hljs-number\">2933</span>                 **model_inputs,\n  \
          \ <span class=\"hljs-number\">2934</span>                 return_dict=<span\
          \ class=\"hljs-literal\">True</span>,\n\n~/Library/Python/<span class=\"\
          hljs-number\">3.8</span>/lib/python/site-packages/torch/nn/modules/module.py\
          \ <span class=\"hljs-keyword\">in</span> _call_impl(self, *args, **kwargs)\n\
          \   <span class=\"hljs-number\">1499</span>     <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">parameters</span>(<span\
          \ class=\"hljs-params\">self, recurse: <span class=\"hljs-built_in\">bool</span>\
          \ = <span class=\"hljs-literal\">True</span></span>) -&gt; Iterator[Parameter]:\n\
          \   <span class=\"hljs-number\">1500</span>         <span class=\"hljs-string\"\
          >r\"\"\"Returns an iterator over module parameters.</span>\n<span class=\"\
          hljs-string\">-&gt; 1501 </span>\n<span class=\"hljs-string\">   1502  \
          \       This is typically passed to an optimizer.</span>\n<span class=\"\
          hljs-string\">   1503 </span>\n<span class=\"hljs-string\"></span>\n<span\
          \ class=\"hljs-string\">~/Library/Python/3.8/lib/python/site-packages/transformers/models/marian/modeling_marian.py\
          \ in forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask,\
          \ head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values,\
          \ inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions,\
          \ output_hidden_states, return_dict)</span>\n<span class=\"hljs-string\"\
          >   1454                 )</span>\n<span class=\"hljs-string\">   1455 </span>\n\
          <span class=\"hljs-string\">-&gt; 1456         outputs = self.model(</span>\n\
          <span class=\"hljs-string\">   1457             input_ids,</span>\n<span\
          \ class=\"hljs-string\">   1458             attention_mask=attention_mask,</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *args, **kwargs)</span>\n<span class=\"hljs-string\"\
          >   1499     def parameters(self, recurse: bool = True) -&gt; Iterator[Parameter]:</span>\n\
          <span class=\"hljs-string\">   1500         r\"\"\"</span>Returns an iterator\
          \ over module parameters.\n-&gt; <span class=\"hljs-number\">1501</span>\
          \ \n   <span class=\"hljs-number\">1502</span>         This <span class=\"\
          hljs-keyword\">is</span> typically passed to an optimizer.\n   <span class=\"\
          hljs-number\">1503</span> \n\n~/Library/Python/<span class=\"hljs-number\"\
          >3.8</span>/lib/python/site-packages/transformers/models/marian/modeling_marian.py\
          \ <span class=\"hljs-keyword\">in</span> forward(self, input_ids, attention_mask,\
          \ decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask,\
          \ cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds,\
          \ decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states,\
          \ return_dict)\n   <span class=\"hljs-number\">1256</span> \n   <span class=\"\
          hljs-number\">1257</span>         <span class=\"hljs-comment\"># decoder\
          \ outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)</span>\n\
          -&gt; <span class=\"hljs-number\">1258</span>         decoder_outputs =\
          \ self.decoder(\n   <span class=\"hljs-number\">1259</span>            \
          \ input_ids=decoder_input_ids,\n   <span class=\"hljs-number\">1260</span>\
          \             attention_mask=decoder_attention_mask,\n\n~/Library/Python/<span\
          \ class=\"hljs-number\">3.8</span>/lib/python/site-packages/torch/nn/modules/module.py\
          \ <span class=\"hljs-keyword\">in</span> _call_impl(self, *args, **kwargs)\n\
          \   <span class=\"hljs-number\">1499</span>     <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">parameters</span>(<span\
          \ class=\"hljs-params\">self, recurse: <span class=\"hljs-built_in\">bool</span>\
          \ = <span class=\"hljs-literal\">True</span></span>) -&gt; Iterator[Parameter]:\n\
          \   <span class=\"hljs-number\">1500</span>         <span class=\"hljs-string\"\
          >r\"\"\"Returns an iterator over module parameters.</span>\n<span class=\"\
          hljs-string\">-&gt; 1501 </span>\n<span class=\"hljs-string\">   1502  \
          \       This is typically passed to an optimizer.</span>\n<span class=\"\
          hljs-string\">   1503 </span>\n<span class=\"hljs-string\"></span>\n<span\
          \ class=\"hljs-string\">~/Library/Python/3.8/lib/python/site-packages/transformers/models/marian/modeling_marian.py\
          \ in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask,\
          \ head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache,\
          \ output_attentions, output_hidden_states, return_dict)</span>\n<span class=\"\
          hljs-string\">    999 </span>\n<span class=\"hljs-string\">   1000     \
          \    # embed positions</span>\n<span class=\"hljs-string\">-&gt; 1001  \
          \       positions = self.embed_positions(input_shape, past_key_values_length)</span>\n\
          <span class=\"hljs-string\">   1002 </span>\n<span class=\"hljs-string\"\
          >   1003         hidden_states = inputs_embeds + positions</span>\n<span\
          \ class=\"hljs-string\"></span>\n<span class=\"hljs-string\">~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *args, **kwargs)</span>\n<span class=\"hljs-string\"\
          >   1499     def parameters(self, recurse: bool = True) -&gt; Iterator[Parameter]:</span>\n\
          <span class=\"hljs-string\">   1500         r\"\"\"</span>Returns an iterator\
          \ over module parameters.\n-&gt; <span class=\"hljs-number\">1501</span>\
          \ \n   <span class=\"hljs-number\">1502</span>         This <span class=\"\
          hljs-keyword\">is</span> typically passed to an optimizer.\n   <span class=\"\
          hljs-number\">1503</span> \n\n~/Library/Python/<span class=\"hljs-number\"\
          >3.8</span>/lib/python/site-packages/torch/utils/_contextlib.py <span class=\"\
          hljs-keyword\">in</span> decorate_context(*args, **kwargs)\n\n~/Library/Python/<span\
          \ class=\"hljs-number\">3.8</span>/lib/python/site-packages/transformers/models/marian/modeling_marian.py\
          \ <span class=\"hljs-keyword\">in</span> forward(self, input_ids_shape,\
          \ past_key_values_length)\n    <span class=\"hljs-number\">138</span>  \
          \           past_key_values_length, past_key_values_length + seq_len, dtype=torch.long,\
          \ device=self.weight.device\n    <span class=\"hljs-number\">139</span>\
          \         )\n--&gt; <span class=\"hljs-number\">140</span>         <span\
          \ class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">super</span>().forward(positions)\n\
          \    <span class=\"hljs-number\">141</span> \n    <span class=\"hljs-number\"\
          >142</span> \n\n~/Library/Python/<span class=\"hljs-number\">3.8</span>/lib/python/site-packages/torch/nn/modules/sparse.py\
          \ <span class=\"hljs-keyword\">in</span> forward(self, <span class=\"hljs-built_in\"\
          >input</span>)\n    <span class=\"hljs-number\">160</span>             self.norm_type,\
          \ self.scale_grad_by_freq, self.sparse)\n    <span class=\"hljs-number\"\
          >161</span> \n--&gt; <span class=\"hljs-number\">162</span>     <span class=\"\
          hljs-keyword\">def</span> <span class=\"hljs-title function_\">extra_repr</span>(<span\
          \ class=\"hljs-params\">self</span>) -&gt; <span class=\"hljs-built_in\"\
          >str</span>:\n    <span class=\"hljs-number\">163</span>         s = <span\
          \ class=\"hljs-string\">'{num_embeddings}, {embedding_dim}'</span>\n   \
          \ <span class=\"hljs-number\">164</span>         <span class=\"hljs-keyword\"\
          >if</span> self.padding_idx <span class=\"hljs-keyword\">is</span> <span\
          \ class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n\
          \n~/Library/Python/<span class=\"hljs-number\">3.8</span>/lib/python/site-packages/torch/nn/functional.py\
          \ <span class=\"hljs-keyword\">in</span> embedding(<span class=\"hljs-built_in\"\
          >input</span>, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq,\
          \ sparse)\n   <span class=\"hljs-number\">2208</span>     <span class=\"\
          hljs-keyword\">else</span>:\n   <span class=\"hljs-number\">2209</span>\
          \         <span class=\"hljs-keyword\">raise</span> ValueError(<span class=\"\
          hljs-string\">\"mode has to be one of sum, mean or max\"</span>)\n-&gt;\
          \ <span class=\"hljs-number\">2210</span> \n   <span class=\"hljs-number\"\
          >2211</span>     <span class=\"hljs-keyword\">if</span> max_norm <span class=\"\
          hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"\
          hljs-literal\">None</span>:\n   <span class=\"hljs-number\">2212</span>\
          \         <span class=\"hljs-comment\"># <span class=\"hljs-doctag\">XXX:</span>\
          \ equivalent to</span>\n\nIndexError: index out of <span class=\"hljs-built_in\"\
          >range</span> <span class=\"hljs-keyword\">in</span> self\n</code></pre>\n\
          <p>To replicate the error:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, AutoModelForSeq2SeqLM\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> pipeline\n\
          \n\ntokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\"\
          >\"Helsinki-NLP/opus-mt-mul-en\"</span>)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"Helsinki-NLP/opus-mt-mul-en\"</span>)\n\n\ntranslator\
          \ = pipeline(<span class=\"hljs-string\">\"translation\"</span>, tokenizer=tokenizer,\
          \ model=model, max_length=<span class=\"hljs-number\">1024</span>)\n\ns\
          \ = <span class=\"hljs-string\">\"\u05EA\u05D0\u05D2\u05D9\u05D3 \u05D4\u05D3\
          \u05DC\u05E7 \u05D5\u05D4\u05DB\u05D9\u05DE\u05D9\u05E7\u05DC\u05D9\u05DD\
          \ \u05E9\u05DC \u05E1\u05D9\u05DF (\u05D1\u05D0\u05E0\u05D2\u05DC\u05D9\u05EA\
          : China Petroleum &amp; Chemical Corporation, \u05D1\u05E1\u05D9\u05E0\u05D9\
          \u05EA: \u4E2D\u56FD\u77F3\u6CB9\u5316\u5DE5\u80A1\u4EFD\u6709\u9650\u516C\
          \u53F8) \u05D0\u05D5 \u05E1\u05D9\u05E0\u05D5\u05E4\u05E7 (\u05D0\u05E0\u05D2\
          \u05DC\u05D9\u05EA: Sinopec Limited, \u05E1\u05D9\u05E0\u05D9\u05EA: \u4E2D\
          \u56FD\u77F3\u5316, \u05E1\u05D9\u05E0\u05D9\u05EA \u05DE\u05E1\u05D5\u05E8\
          \u05EA\u05D9\u05EA: \u4E2D\u570B\u77F3\u5316, \u05E4\u05D9\u05DF-\u05D9\u05D9\
          \u05DF: Zh\u014Dnggu\xF3 Sh\xEDhu\xE0) \u05D4\u05D9\u05D0 \u05EA\u05D0\u05D2\
          \u05D9\u05D3 \u05E0\u05E4\u05D8 \u05D5\u05D2\u05D6 \u05DE\u05DE\u05E9\u05DC\
          \u05EA\u05D9 \u05E1\u05D9\u05E0\u05D9 \u05D4\u05DE\u05DE\u05D5\u05E7\u05DD\
          \ \u05D1\u05D1\u05D9\u05D9\u05D2'\u05D9\u05E0\u05D2.\"</span>\n\ntranslator(s,\
          \ src_lang=<span class=\"hljs-string\">\"he\"</span>)\n</code></pre>\n"
        raw: "With this input, \n\n> \"\u05EA\u05D0\u05D2\u05D9\u05D3 \u05D4\u05D3\
          \u05DC\u05E7 \u05D5\u05D4\u05DB\u05D9\u05DE\u05D9\u05E7\u05DC\u05D9\u05DD\
          \ \u05E9\u05DC \u05E1\u05D9\u05DF (\u05D1\u05D0\u05E0\u05D2\u05DC\u05D9\u05EA\
          : China Petroleum & Chemical Corporation, \u05D1\u05E1\u05D9\u05E0\u05D9\
          \u05EA: \u4E2D\u56FD\u77F3\u6CB9\u5316\u5DE5\u80A1\u4EFD\u6709\u9650\u516C\
          \u53F8) \u05D0\u05D5 \u05E1\u05D9\u05E0\u05D5\u05E4\u05E7 (\u05D0\u05E0\u05D2\
          \u05DC\u05D9\u05EA: Sinopec Limited, \u05E1\u05D9\u05E0\u05D9\u05EA: \u4E2D\
          \u56FD\u77F3\u5316, \u05E1\u05D9\u05E0\u05D9\u05EA \u05DE\u05E1\u05D5\u05E8\
          \u05EA\u05D9\u05EA: \u4E2D\u570B\u77F3\u5316, \u05E4\u05D9\u05DF-\u05D9\u05D9\
          \u05DF: Zh\u014Dnggu\xF3 Sh\xEDhu\xE0) \u05D4\u05D9\u05D0 \u05EA\u05D0\u05D2\
          \u05D9\u05D3 \u05E0\u05E4\u05D8 \u05D5\u05D2\u05D6 \u05DE\u05DE\u05E9\u05DC\
          \u05EA\u05D9 \u05E1\u05D9\u05E0\u05D9 \u05D4\u05DE\u05DE\u05D5\u05E7\u05DD\
          \ \u05D1\u05D1\u05D9\u05D9\u05D2'\u05D9\u05E0\u05D2.\"\n\nthe model throws\
          \ the error for `transformers==4.31.0`:\n\n```python\n---------------------------------------------------------------------------\n\
          IndexError                                Traceback (most recent call last)\n\
          <ipython-input-31-e2292311132f> in <module>\n----> 1 translator(s)\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/text2text_generation.py\
          \ in __call__(self, *args, **kwargs)\n    365               token ids of\
          \ the translation.\n    366         \"\"\"\n--> 367         return super().__call__(*args,\
          \ **kwargs)\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/text2text_generation.py\
          \ in __call__(self, *args, **kwargs)\n    163         \"\"\"\n    164 \n\
          --> 165         result = super().__call__(*args, **kwargs)\n    166    \
          \     if (\n    167             isinstance(args[0], list)\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py\
          \ in __call__(self, inputs, num_workers, batch_size, *args, **kwargs)\n\
          \   1120             )\n   1121         else:\n-> 1122             return\
          \ self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n\
          \   1123 \n   1124     def run_multi(self, inputs, preprocess_params, forward_params,\
          \ postprocess_params):\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py\
          \ in run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n\
          \   1127     def run_single(self, inputs, preprocess_params, forward_params,\
          \ postprocess_params):\n   1128         model_inputs = self.preprocess(inputs,\
          \ **preprocess_params)\n-> 1129         model_outputs = self.forward(model_inputs,\
          \ **forward_params)\n   1130         outputs = self.postprocess(model_outputs,\
          \ **postprocess_params)\n   1131         return outputs\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py\
          \ in forward(self, model_inputs, **forward_params)\n   1026            \
          \     with inference_context():\n   1027                     model_inputs\
          \ = self._ensure_tensor_on_device(model_inputs, device=self.device)\n->\
          \ 1028                     model_outputs = self._forward(model_inputs, **forward_params)\n\
          \   1029                     model_outputs = self._ensure_tensor_on_device(model_outputs,\
          \ device=torch.device(\"cpu\"))\n   1030             else:\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/text2text_generation.py\
          \ in _forward(self, model_inputs, **generate_kwargs)\n    185         generate_kwargs[\"\
          max_length\"] = generate_kwargs.get(\"max_length\", self.model.config.max_length)\n\
          \    186         self.check_inputs(input_length, generate_kwargs[\"min_length\"\
          ], generate_kwargs[\"max_length\"])\n--> 187         output_ids = self.model.generate(**model_inputs,\
          \ **generate_kwargs)\n    188         out_b = output_ids.shape[0]\n    189\
          \         if self.framework == \"pt\":\n\n~/Library/Python/3.8/lib/python/site-packages/torch/utils/_contextlib.py\
          \ in decorate_context(*args, **kwargs)\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/generation/utils.py\
          \ in generate(self, inputs, generation_config, logits_processor, stopping_criteria,\
          \ prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\n\
          \   1625             )\n   1626             # 13. run beam search\n-> 1627\
          \             return self.beam_search(\n   1628                 input_ids,\n\
          \   1629                 beam_scorer,\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/generation/utils.py\
          \ in beam_search(self, input_ids, beam_scorer, logits_processor, stopping_criteria,\
          \ max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states,\
          \ output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\n\
          \   2930             model_inputs = self.prepare_inputs_for_generation(input_ids,\
          \ **model_kwargs)\n   2931 \n-> 2932             outputs = self(\n   2933\
          \                 **model_inputs,\n   2934                 return_dict=True,\n\
          \n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *args, **kwargs)\n   1499     def parameters(self,\
          \ recurse: bool = True) -> Iterator[Parameter]:\n   1500         r\"\"\"\
          Returns an iterator over module parameters.\n-> 1501 \n   1502         This\
          \ is typically passed to an optimizer.\n   1503 \n\n~/Library/Python/3.8/lib/python/site-packages/transformers/models/marian/modeling_marian.py\
          \ in forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask,\
          \ head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values,\
          \ inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions,\
          \ output_hidden_states, return_dict)\n   1454                 )\n   1455\
          \ \n-> 1456         outputs = self.model(\n   1457             input_ids,\n\
          \   1458             attention_mask=attention_mask,\n\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *args, **kwargs)\n   1499     def parameters(self,\
          \ recurse: bool = True) -> Iterator[Parameter]:\n   1500         r\"\"\"\
          Returns an iterator over module parameters.\n-> 1501 \n   1502         This\
          \ is typically passed to an optimizer.\n   1503 \n\n~/Library/Python/3.8/lib/python/site-packages/transformers/models/marian/modeling_marian.py\
          \ in forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask,\
          \ head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values,\
          \ inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states,\
          \ return_dict)\n   1256 \n   1257         # decoder outputs consists of\
          \ (dec_features, past_key_value, dec_hidden, dec_attn)\n-> 1258        \
          \ decoder_outputs = self.decoder(\n   1259             input_ids=decoder_input_ids,\n\
          \   1260             attention_mask=decoder_attention_mask,\n\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *args, **kwargs)\n   1499     def parameters(self,\
          \ recurse: bool = True) -> Iterator[Parameter]:\n   1500         r\"\"\"\
          Returns an iterator over module parameters.\n-> 1501 \n   1502         This\
          \ is typically passed to an optimizer.\n   1503 \n\n~/Library/Python/3.8/lib/python/site-packages/transformers/models/marian/modeling_marian.py\
          \ in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask,\
          \ head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache,\
          \ output_attentions, output_hidden_states, return_dict)\n    999 \n   1000\
          \         # embed positions\n-> 1001         positions = self.embed_positions(input_shape,\
          \ past_key_values_length)\n   1002 \n   1003         hidden_states = inputs_embeds\
          \ + positions\n\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *args, **kwargs)\n   1499     def parameters(self,\
          \ recurse: bool = True) -> Iterator[Parameter]:\n   1500         r\"\"\"\
          Returns an iterator over module parameters.\n-> 1501 \n   1502         This\
          \ is typically passed to an optimizer.\n   1503 \n\n~/Library/Python/3.8/lib/python/site-packages/torch/utils/_contextlib.py\
          \ in decorate_context(*args, **kwargs)\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/models/marian/modeling_marian.py\
          \ in forward(self, input_ids_shape, past_key_values_length)\n    138   \
          \          past_key_values_length, past_key_values_length + seq_len, dtype=torch.long,\
          \ device=self.weight.device\n    139         )\n--> 140         return super().forward(positions)\n\
          \    141 \n    142 \n\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/sparse.py\
          \ in forward(self, input)\n    160             self.norm_type, self.scale_grad_by_freq,\
          \ self.sparse)\n    161 \n--> 162     def extra_repr(self) -> str:\n   \
          \ 163         s = '{num_embeddings}, {embedding_dim}'\n    164         if\
          \ self.padding_idx is not None:\n\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/functional.py\
          \ in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq,\
          \ sparse)\n   2208     else:\n   2209         raise ValueError(\"mode has\
          \ to be one of sum, mean or max\")\n-> 2210 \n   2211     if max_norm is\
          \ not None:\n   2212         # XXX: equivalent to\n\nIndexError: index out\
          \ of range in self\n```\n\nTo replicate the error:\n\n```python\nfrom transformers\
          \ import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom transformers import\
          \ pipeline\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-mul-en\"\
          )\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-mul-en\"\
          )\n\n\ntranslator = pipeline(\"translation\", tokenizer=tokenizer, model=model,\
          \ max_length=1024)\n\ns = \"\u05EA\u05D0\u05D2\u05D9\u05D3 \u05D4\u05D3\u05DC\
          \u05E7 \u05D5\u05D4\u05DB\u05D9\u05DE\u05D9\u05E7\u05DC\u05D9\u05DD \u05E9\
          \u05DC \u05E1\u05D9\u05DF (\u05D1\u05D0\u05E0\u05D2\u05DC\u05D9\u05EA: China\
          \ Petroleum & Chemical Corporation, \u05D1\u05E1\u05D9\u05E0\u05D9\u05EA\
          : \u4E2D\u56FD\u77F3\u6CB9\u5316\u5DE5\u80A1\u4EFD\u6709\u9650\u516C\u53F8\
          ) \u05D0\u05D5 \u05E1\u05D9\u05E0\u05D5\u05E4\u05E7 (\u05D0\u05E0\u05D2\u05DC\
          \u05D9\u05EA: Sinopec Limited, \u05E1\u05D9\u05E0\u05D9\u05EA: \u4E2D\u56FD\
          \u77F3\u5316, \u05E1\u05D9\u05E0\u05D9\u05EA \u05DE\u05E1\u05D5\u05E8\u05EA\
          \u05D9\u05EA: \u4E2D\u570B\u77F3\u5316, \u05E4\u05D9\u05DF-\u05D9\u05D9\u05DF\
          : Zh\u014Dnggu\xF3 Sh\xEDhu\xE0) \u05D4\u05D9\u05D0 \u05EA\u05D0\u05D2\u05D9\
          \u05D3 \u05E0\u05E4\u05D8 \u05D5\u05D2\u05D6 \u05DE\u05DE\u05E9\u05DC\u05EA\
          \u05D9 \u05E1\u05D9\u05E0\u05D9 \u05D4\u05DE\u05DE\u05D5\u05E7\u05DD \u05D1\
          \u05D1\u05D9\u05D9\u05D2'\u05D9\u05E0\u05D2.\"\n\ntranslator(s, src_lang=\"\
          he\")\n```"
        updatedAt: '2023-08-17T01:37:13.978Z'
      numEdits: 2
      reactions: []
    id: 64dd79790f469b2073fdd74c
    type: comment
  author: alvations
  content: "With this input, \n\n> \"\u05EA\u05D0\u05D2\u05D9\u05D3 \u05D4\u05D3\u05DC\
    \u05E7 \u05D5\u05D4\u05DB\u05D9\u05DE\u05D9\u05E7\u05DC\u05D9\u05DD \u05E9\u05DC\
    \ \u05E1\u05D9\u05DF (\u05D1\u05D0\u05E0\u05D2\u05DC\u05D9\u05EA: China Petroleum\
    \ & Chemical Corporation, \u05D1\u05E1\u05D9\u05E0\u05D9\u05EA: \u4E2D\u56FD\u77F3\
    \u6CB9\u5316\u5DE5\u80A1\u4EFD\u6709\u9650\u516C\u53F8) \u05D0\u05D5 \u05E1\u05D9\
    \u05E0\u05D5\u05E4\u05E7 (\u05D0\u05E0\u05D2\u05DC\u05D9\u05EA: Sinopec Limited,\
    \ \u05E1\u05D9\u05E0\u05D9\u05EA: \u4E2D\u56FD\u77F3\u5316, \u05E1\u05D9\u05E0\
    \u05D9\u05EA \u05DE\u05E1\u05D5\u05E8\u05EA\u05D9\u05EA: \u4E2D\u570B\u77F3\u5316\
    , \u05E4\u05D9\u05DF-\u05D9\u05D9\u05DF: Zh\u014Dnggu\xF3 Sh\xEDhu\xE0) \u05D4\
    \u05D9\u05D0 \u05EA\u05D0\u05D2\u05D9\u05D3 \u05E0\u05E4\u05D8 \u05D5\u05D2\u05D6\
    \ \u05DE\u05DE\u05E9\u05DC\u05EA\u05D9 \u05E1\u05D9\u05E0\u05D9 \u05D4\u05DE\u05DE\
    \u05D5\u05E7\u05DD \u05D1\u05D1\u05D9\u05D9\u05D2'\u05D9\u05E0\u05D2.\"\n\nthe\
    \ model throws the error for `transformers==4.31.0`:\n\n```python\n---------------------------------------------------------------------------\n\
    IndexError                                Traceback (most recent call last)\n\
    <ipython-input-31-e2292311132f> in <module>\n----> 1 translator(s)\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/text2text_generation.py\
    \ in __call__(self, *args, **kwargs)\n    365               token ids of the translation.\n\
    \    366         \"\"\"\n--> 367         return super().__call__(*args, **kwargs)\n\
    \n~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/text2text_generation.py\
    \ in __call__(self, *args, **kwargs)\n    163         \"\"\"\n    164 \n--> 165\
    \         result = super().__call__(*args, **kwargs)\n    166         if (\n \
    \   167             isinstance(args[0], list)\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py\
    \ in __call__(self, inputs, num_workers, batch_size, *args, **kwargs)\n   1120\
    \             )\n   1121         else:\n-> 1122             return self.run_single(inputs,\
    \ preprocess_params, forward_params, postprocess_params)\n   1123 \n   1124  \
    \   def run_multi(self, inputs, preprocess_params, forward_params, postprocess_params):\n\
    \n~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py\
    \ in run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n\
    \   1127     def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):\n\
    \   1128         model_inputs = self.preprocess(inputs, **preprocess_params)\n\
    -> 1129         model_outputs = self.forward(model_inputs, **forward_params)\n\
    \   1130         outputs = self.postprocess(model_outputs, **postprocess_params)\n\
    \   1131         return outputs\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/base.py\
    \ in forward(self, model_inputs, **forward_params)\n   1026                 with\
    \ inference_context():\n   1027                     model_inputs = self._ensure_tensor_on_device(model_inputs,\
    \ device=self.device)\n-> 1028                     model_outputs = self._forward(model_inputs,\
    \ **forward_params)\n   1029                     model_outputs = self._ensure_tensor_on_device(model_outputs,\
    \ device=torch.device(\"cpu\"))\n   1030             else:\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/pipelines/text2text_generation.py\
    \ in _forward(self, model_inputs, **generate_kwargs)\n    185         generate_kwargs[\"\
    max_length\"] = generate_kwargs.get(\"max_length\", self.model.config.max_length)\n\
    \    186         self.check_inputs(input_length, generate_kwargs[\"min_length\"\
    ], generate_kwargs[\"max_length\"])\n--> 187         output_ids = self.model.generate(**model_inputs,\
    \ **generate_kwargs)\n    188         out_b = output_ids.shape[0]\n    189   \
    \      if self.framework == \"pt\":\n\n~/Library/Python/3.8/lib/python/site-packages/torch/utils/_contextlib.py\
    \ in decorate_context(*args, **kwargs)\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/generation/utils.py\
    \ in generate(self, inputs, generation_config, logits_processor, stopping_criteria,\
    \ prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\n\
    \   1625             )\n   1626             # 13. run beam search\n-> 1627   \
    \          return self.beam_search(\n   1628                 input_ids,\n   1629\
    \                 beam_scorer,\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/generation/utils.py\
    \ in beam_search(self, input_ids, beam_scorer, logits_processor, stopping_criteria,\
    \ max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states,\
    \ output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\n   2930\
    \             model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\
    \   2931 \n-> 2932             outputs = self(\n   2933                 **model_inputs,\n\
    \   2934                 return_dict=True,\n\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py\
    \ in _call_impl(self, *args, **kwargs)\n   1499     def parameters(self, recurse:\
    \ bool = True) -> Iterator[Parameter]:\n   1500         r\"\"\"Returns an iterator\
    \ over module parameters.\n-> 1501 \n   1502         This is typically passed\
    \ to an optimizer.\n   1503 \n\n~/Library/Python/3.8/lib/python/site-packages/transformers/models/marian/modeling_marian.py\
    \ in forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask,\
    \ head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values,\
    \ inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions,\
    \ output_hidden_states, return_dict)\n   1454                 )\n   1455 \n->\
    \ 1456         outputs = self.model(\n   1457             input_ids,\n   1458\
    \             attention_mask=attention_mask,\n\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py\
    \ in _call_impl(self, *args, **kwargs)\n   1499     def parameters(self, recurse:\
    \ bool = True) -> Iterator[Parameter]:\n   1500         r\"\"\"Returns an iterator\
    \ over module parameters.\n-> 1501 \n   1502         This is typically passed\
    \ to an optimizer.\n   1503 \n\n~/Library/Python/3.8/lib/python/site-packages/transformers/models/marian/modeling_marian.py\
    \ in forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask,\
    \ head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values,\
    \ inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states,\
    \ return_dict)\n   1256 \n   1257         # decoder outputs consists of (dec_features,\
    \ past_key_value, dec_hidden, dec_attn)\n-> 1258         decoder_outputs = self.decoder(\n\
    \   1259             input_ids=decoder_input_ids,\n   1260             attention_mask=decoder_attention_mask,\n\
    \n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py in\
    \ _call_impl(self, *args, **kwargs)\n   1499     def parameters(self, recurse:\
    \ bool = True) -> Iterator[Parameter]:\n   1500         r\"\"\"Returns an iterator\
    \ over module parameters.\n-> 1501 \n   1502         This is typically passed\
    \ to an optimizer.\n   1503 \n\n~/Library/Python/3.8/lib/python/site-packages/transformers/models/marian/modeling_marian.py\
    \ in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask,\
    \ head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache,\
    \ output_attentions, output_hidden_states, return_dict)\n    999 \n   1000   \
    \      # embed positions\n-> 1001         positions = self.embed_positions(input_shape,\
    \ past_key_values_length)\n   1002 \n   1003         hidden_states = inputs_embeds\
    \ + positions\n\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py\
    \ in _call_impl(self, *args, **kwargs)\n   1499     def parameters(self, recurse:\
    \ bool = True) -> Iterator[Parameter]:\n   1500         r\"\"\"Returns an iterator\
    \ over module parameters.\n-> 1501 \n   1502         This is typically passed\
    \ to an optimizer.\n   1503 \n\n~/Library/Python/3.8/lib/python/site-packages/torch/utils/_contextlib.py\
    \ in decorate_context(*args, **kwargs)\n\n~/Library/Python/3.8/lib/python/site-packages/transformers/models/marian/modeling_marian.py\
    \ in forward(self, input_ids_shape, past_key_values_length)\n    138         \
    \    past_key_values_length, past_key_values_length + seq_len, dtype=torch.long,\
    \ device=self.weight.device\n    139         )\n--> 140         return super().forward(positions)\n\
    \    141 \n    142 \n\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/sparse.py\
    \ in forward(self, input)\n    160             self.norm_type, self.scale_grad_by_freq,\
    \ self.sparse)\n    161 \n--> 162     def extra_repr(self) -> str:\n    163  \
    \       s = '{num_embeddings}, {embedding_dim}'\n    164         if self.padding_idx\
    \ is not None:\n\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/functional.py\
    \ in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq,\
    \ sparse)\n   2208     else:\n   2209         raise ValueError(\"mode has to be\
    \ one of sum, mean or max\")\n-> 2210 \n   2211     if max_norm is not None:\n\
    \   2212         # XXX: equivalent to\n\nIndexError: index out of range in self\n\
    ```\n\nTo replicate the error:\n\n```python\nfrom transformers import AutoTokenizer,\
    \ AutoModelForSeq2SeqLM\nfrom transformers import pipeline\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
    Helsinki-NLP/opus-mt-mul-en\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"\
    Helsinki-NLP/opus-mt-mul-en\")\n\n\ntranslator = pipeline(\"translation\", tokenizer=tokenizer,\
    \ model=model, max_length=1024)\n\ns = \"\u05EA\u05D0\u05D2\u05D9\u05D3 \u05D4\
    \u05D3\u05DC\u05E7 \u05D5\u05D4\u05DB\u05D9\u05DE\u05D9\u05E7\u05DC\u05D9\u05DD\
    \ \u05E9\u05DC \u05E1\u05D9\u05DF (\u05D1\u05D0\u05E0\u05D2\u05DC\u05D9\u05EA\
    : China Petroleum & Chemical Corporation, \u05D1\u05E1\u05D9\u05E0\u05D9\u05EA\
    : \u4E2D\u56FD\u77F3\u6CB9\u5316\u5DE5\u80A1\u4EFD\u6709\u9650\u516C\u53F8) \u05D0\
    \u05D5 \u05E1\u05D9\u05E0\u05D5\u05E4\u05E7 (\u05D0\u05E0\u05D2\u05DC\u05D9\u05EA\
    : Sinopec Limited, \u05E1\u05D9\u05E0\u05D9\u05EA: \u4E2D\u56FD\u77F3\u5316, \u05E1\
    \u05D9\u05E0\u05D9\u05EA \u05DE\u05E1\u05D5\u05E8\u05EA\u05D9\u05EA: \u4E2D\u570B\
    \u77F3\u5316, \u05E4\u05D9\u05DF-\u05D9\u05D9\u05DF: Zh\u014Dnggu\xF3 Sh\xEDhu\xE0\
    ) \u05D4\u05D9\u05D0 \u05EA\u05D0\u05D2\u05D9\u05D3 \u05E0\u05E4\u05D8 \u05D5\u05D2\
    \u05D6 \u05DE\u05DE\u05E9\u05DC\u05EA\u05D9 \u05E1\u05D9\u05E0\u05D9 \u05D4\u05DE\
    \u05DE\u05D5\u05E7\u05DD \u05D1\u05D1\u05D9\u05D9\u05D2'\u05D9\u05E0\u05D2.\"\n\
    \ntranslator(s, src_lang=\"he\")\n```"
  created_at: 2023-08-17 00:35:53+00:00
  edited: true
  hidden: false
  id: 64dd79790f469b2073fdd74c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0e9087f2672b0e4f28d91266acf9ce57.svg
      fullname: Liling
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvations
      type: user
    createdAt: '2023-08-17T11:15:34.000Z'
    data:
      edited: false
      editors:
      - alvations
      hidden: false
      identifiedLanguage:
        language: he
        probability: 0.992164134979248
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0e9087f2672b0e4f28d91266acf9ce57.svg
          fullname: Liling
          isHf: false
          isPro: false
          name: alvations
          type: user
        html: "<p>Found a few more inputs that causes the same error:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer, AutoModelForSeq2SeqLM\n\
          <span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"Helsinki-NLP/opus-mt-mul-en\"</span>)\nmodel =\
          \ AutoModelForSeq2SeqLM.from_pretrained(<span class=\"hljs-string\">\"Helsinki-NLP/opus-mt-mul-en\"\
          </span>)\n\ntranslator = pipeline(<span class=\"hljs-string\">\"translation\"\
          </span>, tokenizer=tokenizer, model=model, max_length=<span class=\"hljs-number\"\
          >1024</span>)\n\ns = <span class=\"hljs-string\">\"\"\"\u05DC\u05DB\u05DC\
          \ \u05DE\u05DB\u05E8\u05D9 \u05D5\u05DE\u05D5\u05E7\u05D9\u05E8\u05D9 \u05D6\
          \u05DB\u05E8\u05D9:\u05DE\u05E6\u05D9\u05D0\u05D0\u05D5\u05EA \u05DC\u05E4\
          \u05DE\u05D9\u05DD\u05DD \u05E0\u05E1 \u05DE\u05DE\u05DE\u05E7\u05D5\u05DD\
          \ \u05D4\u05DB\u05D9 \u05E4\u05E4\u05D7\u05D5\u05EA \u05E6\u05E4\u05D5\u05D9\
          \ \u05D0\u05D1\u05DC \u05E2\u05DD \u05D2\u05D1 \u05DC\u05DC\u05E7\u05D9\u05E8\
          \ \u05D4\u05DB\u05DC \u05D0\u05E4\u05E9\u05E8\u05D9 \u05D5\u05D5\u05D0\u05E0\
          \u05D9 \u05DE\u05D1\u05D9\u05D9\u05D8 \u05D1\u05D2\u05D3\u05D3\u05E8\u05E8\
          \ \u05D4\u05D9\u05E8\u05D5\u05E7\u05D4 \u05DC\u05E9\u05D1\u05E8\u05D9 \u05DB\
          \u05DC\u05D9\u05DD \u05E9\u05D4\u05D9\u05D9\u05EA\u05D9 \u05DE\u05D9\u05D5\
          \u05E2\u05D3 \u05DC\u05E9\u05DD, \u05DB\u05D0\u05E9\u05E8 \u05E2\u05D5\"\
          \u05E1 \u05D0\u05E7\u05D9\u05DD \u05DE\u05E9\u05D5\u05DC\u05D1 \u05DE\u05E8\
          \u05E2\u05D4\u05D5 \u05D1\u05E8\u05E4\u05E9 \u05DC\u05E0\u05E4\u05E9 \u05DC\
          \u05E4\u05E7\u05D7 \u05E2\u05DC \u05DE\u05D4 \u05E9\u05EA\u05E7\u05E0\u05D5\
          \ \"\u05E4\u05D9\u05D2\u05D5\u05E8 \u05E9\u05DC \u05D2\u05D0\u05D5\u05E0\
          \u05D9\u05DD\" \u05DC\u05D8\u05D0\u05D8\u05D8\u05D0 \u05D7\u05E6\u05E8 \u05D1\
          \u05D1\u05D9\u05EA \u05DE\u05DC\u05D0\u05DB\u05D4 \u05D5\u05D0\u05D7\u05E8\
          \u05D9 \u05E9\u05E0\u05EA\u05D9\u05D9\u05D9\u05DD \u05D0\u05D5\u05DC\u05D9\
          \ \u05DC\u05EA\u05E4\u05D5\u05E8 \u05D9\u05D3\u05D9\u05E2\u05D5\u05EA \u05DC\
          \u05EA\u05D9\u05E7\u05D9 \u05D1\u05EA\u05D9 \u05E1\u05E4\u05E8,\u05D5\u05D0\
          \u05D6 \u05DB\u05D1\u05E8 \u05D4\u05EA\u05D7\u05DC\u05EA\u05D9 \u05DC\u05D2\
          \u05E9\u05E9 \u05D1\u05DE\u05E2\u05E9 \u05D0\u05D5\u05D5\u05EA\u05E0\u05D8\
          \u05D9 \u05DC\u05D0 \u05E8\u05D5\u05D5\u05D7\u05D9 \u05DC\u05D4\u05D2\u05D9\
          \u05E2 \u05DC\u05DC\u05E2\u05D9\u05E0\u05D9\u05DD \u05D0\u05D7\u05E8\u05D5\
          \u05EA \u05D5\u05D0\u05D7\u05E8\u05D9 \u05E9\u05E0\u05EA\u05D9\u05D9\u05DD\
          \ \u05D8\u05E1\u05EA\u05D9 \u05DE\u05E2\u05E2\u05D1\u05E8 \u05DC\u05D2\u05D3\
          \u05E8\u05D5\u05EA \u05D5\u05DC\u05D7\u05D5\u05DE\u05D5\u05EA,\u05D1\u05D8\
          \u05D9\u05E4\u05D5\u05DC \u05DE\u05D9\u05D8\u05D1\u05D9 \u05DC\u05D4\u05E1\
          \u05EA\u05D9\u05E8 \u05DB\u05E1\u05E4\u05D9 \u05D9\u05E8\u05D5\u05E9\u05D4\
          \ \u05E9\u05D0\u05E0\u05D9 \u05D8\u05E8\u05D7\u05EA\u05D9 \u05DC\u05D0\u05D1\
          \u05D9 \u05DB\u05D1\u05DF \u05E9\u05E0\u05D5\u05D0\u05D0\u05EA \u05D5\u05D2\
          \u05E8\u05D5\u05E9\u05EA \u05D4\u05DC\u05D1 \u05D5\u05D4\u05DB\u05DC \u05D9\
          \u05D3\u05E2\u05D5 \u05D0\u05E8\u05D1\u05E2\u05D9\u05DD \u05E9\u05E0\u05D4\
          \ \u05DC\u05E4\u05E0\u05D9 \u05D1\u05D4\u05EA\u05E2\u05DC\u05DE\u05D5 \u05D2\
          \u05DD \u05D4\u05E4\u05D3\u05D2\u05D5\u05D2\u05D9\u05D4. \u05D6\u05DB\u05D9\
          \u05EA\u05D9 \u05D1\u05D4\u05DB\u05E8\u05EA \u05E7\u05E8\u05DF \u05E0\u05E0\
          \u05D5\u05D1\u05DC \u05DC\u05E1\u05E4\u05E8\u05D5,\u05DC\u05D0 \u05EA\u05D9\
          \u05D7\u05D5\u05DD \u05D0\u05DE\u05E0\u05D5\u05EA\u05D9 \u05DC\u05E9\u05DE\
          \u05D5,\u05E2\u05D3\u05D5\u05EA \u05E7\u05DF \u05D4\u05E7\u05D5\u05E7\u05D9\
          \u05D4 \u05DB\u05DE\u05E6\u05D9\u05D0\u05D5\u05EA \u05D2\u05D4\u05D9\u05E0\
          \u05D5\u05DE\u05D9\u05EA \u05D5\u05D2\u05D3\u05E8\u05D5\u05EA \u05E0\u05E2\
          \u05D5\u05DC\u05D5\u05EA. \u05E4\u05EA\u05E8\u05D5\u05DF \u05E1\u05D5\u05E4\
          \u05D9 \u05DC\u05D0 \u05D0\u05D9\u05D7\u05E8 \u05DC\u05D1\u05D5\u05D0, \u05D0\
          \u05D9\u05E9 \u05DE\u05DB\u05DC \u05DE\u05D9\u05D5\u05D3\u05E2\u05D9 \u05D5\
          \u05D0\u05D6 \u05D3\u05D5\u05E8 \u05D4\u05D5\u05E8\u05D9 \u05DB\u05DB\u05D1\
          \u05E8 \u05D9\u05E8\u05D3 \u05DE\u05D4\u05D1\u05DE\u05D4, \u05D1\u05D9\u05D2\
          \u05D5\u05DF \u05DC\u05DE\u05E8\u05D0\u05D4 \u05D4\u05D9\u05DC\u05D3 \u05D4\
          \u05D7\u05D1\u05D9\u05D1 \u05E9\u05DB\u05D1\u05E8\"\u05DC\u05D0 \u05D9\u05E2\
          \u05E9\u05D4 \u05E6\u05D1\u05D0\" \u05E9\u05E0\u05D5\u05EA\u05E8 \u05E4\u05E1\
          \u05D9\u05E7\u05D4 \u05D0\u05E1\u05E4\u05E1\u05D5\u05E4\u05D9\u05EA \u05DC\
          \u05D4\u05E9\u05D0\u05E8 \u05D0\u05E4\u05E1. \u05E6\u05E0\u05D5\u05E2\u05D9\
          \u05DD\u05DD,\u05E1\u05D5\u05DC\u05D9\u05D3\u05D9\u05DD,\u05D0\u05DC\u05D8\
          \u05E8\u05D5\u05D0\u05D0\u05D9\u05E1\u05D8\u05D9\u05DD \u05D5\u05EA\u05E8\
          \u05D5\u05DE\u05D9\u05D9\u05DD \u05DE\u05D0\u05DB\u05DC \u05DC\u05D7\u05D9\
          \u05D9\u05EA\u05D5 \u05D4\u05DE\u05D3\u05D9\u05E0\u05D4,\u05D0\u05D5\u05D5\
          \u05D9 \u05DC\u05E9\u05D5\u05E4\u05D8\u05D5\u05EA \u05D1\u05E8\u05E6\u05E6\
          \u05D7\u05D7\u05E0\u05D5\u05EA \u05D7\u05D5\u05DB\u05DE\u05D4 \u05DC\u05D4\
          \u05E8\u05E8\u05D9 \u05D5\u05DC\u05E0\u05D8\u05D5\u05DC \u05DE\u05E9\u05E4\
          \u05D7\u05D4 \u05E9\u05D5\u05D0\u05D4 \u05D0\u05D5 \u05E2\u05E7\u05D9\u05E8\
          \u05D4,\u05D9\u05DC\u05D3\u05D9 \u05D1\u05D0\u05D9 \u05E2\u05D9\u05E8\u05E7\
          \ \u05D5\u05E6\u05E4\u05D5\u05DF \u05D0\u05E4\u05E8\u05D9\u05E7\u05D4 \u05E0\
          \u05D3\u05D5\u05E0\u05D5 \u05DC\u05D0\u05D5\u05EA\u05D4 \u05E9\u05D8\u05E0\
          \u05EA \u05D4\u05E4\u05E7\u05E8\u05D4.\u05DB\u05DC \u05DB\u05D5\u05D7\u05D5\
          \u05EA \u05D3\u05E2\u05EA \u05D1\u05D0\u05E0\u05D2\u05DC\u05D9\u05EA \u05D1\
          \u05D9\"\u05DC \u05D4\u05E9\u05E7\u05E2\u05EA\u05D9 \u05DC\u05DE\u05E0\u05D5\
          \u05E2 \u05DE\u05D2\u05D3\u05E8 \u05D9\u05E8\u05D5\u05E7\u05D4 \u05DC\u05E1\
          \u05D2\u05D5\u05E8 \u05E2\u05DC\u05D9..\u05D1\u05E9\u05E0\u05EA\u05D9 \u05D4\
          70 \"\u05DB\u05D5\u05DB\u05D1 \u05D4\u05D4\u05DC\u05DB\u05EA \u05E9\u05DC\
          \ \u05DE\u05E8 \u05E1\u05D0\u05DE\u05DC\u05E8\". \u05DC\u05E1\u05D5\u05DC\
          \ \u05D1\u05DC\u05D5 \u05D1\u05E2\u05D9\u05E8 \u05D5\u05D0\u05DD\"\"\"</span>\n\
          \ntranslator(s, src_lang=<span class=\"hljs-string\">\"he\"</span>)\n</code></pre>\n\
          <p>and </p>\n<pre><code>s = \"\"\"\n\u05D1\u05DE\u05D5\u05D3\u05DC \u05D0\
          \u05D9\u05D9\u05E0\u05E9\u05D8\u05D9\u05D9\u05DF \u05DE-1907 (\u05D1\u05E0\
          \u05D9\u05D2\u05D5\u05D3 \u05DC\u05DE\u05D5\u05D3\u05DC \u05D3\u05D1\u05D9\
          \u05D9 \u05D4\u05DE\u05D0\u05D5\u05D7\u05E8 \u05D9\u05D5\u05EA\u05E8) \u05D0\
          \u05E0\u05D5 \u05DE\u05EA\u05D7\u05E9\u05D1\u05D9\u05DD \u05E8\u05E7 \u05D1\
          \u05D2\u05D1\u05D5\u05DC \u05D4\u05D8\u05DE\u05E4\u05E8\u05D8\u05D5\u05E8\
          \u05D4 \u05D4\u05D2\u05D1\u05D5\u05D4\u05D4: k B T \u226B \u210F \u03C9\
          \ \u03B1 . {\\displaystyle k_{B}T\\gg \\hbar \\omega _{\\alpha }.\\,} \u05D0\
          \u05D6\u05D9 1 \u2212 e \u2212 \u210F \u03C9 \u03B1 / k B T \u2248 \u210F\
          \ \u03C9 \u03B1 / k B T {\\displaystyle 1-e^{-\\hbar \\omega _{\\alpha }/k_{B}T}\\\
          approx \\hbar \\omega _{\\alpha }/k_{B}T\\,} \u05D5\u05D0\u05E0\u05D5 \u05DE\
          \u05E7\u05D1\u05DC\u05D9\u05DD F = N \u03B5 0 + k B T \u2211 \u03B1 log\
          \ \u2061 ( \u210F \u03C9 \u03B1 k B T ) . {\\displaystyle F=N\\varepsilon\
          \ _{0}+k_{B}T\\sum _{\\alpha }\\log \\left({\\frac {\\hbar \\omega _{\\\
          alpha }}{k_{B}T}}\\right).} \u05E0\u05D2\u05D3\u05D9\u05E8 \u05D0\u05EA\
          \ \u05D4\u05DE\u05DE\u05D5\u05E6\u05E2 \u05D4\u05D2\u05D0\u05D5\u05DE\u05D8\
          \u05E8\u05D9 \u05E9\u05DC \u05D4\u05EA\u05D3\u05D9\u05E8\u05D5\u05EA \u05DC\
          \u05D4\u05D9\u05D5\u05EA log \u2061 \u03C9 \xAF = 1 M \u2211 \u03B1 log\
          \ \u2061 \u03C9 \u03B1 , {\\displaystyle \\log {\\bar {\\omega }}={\\frac\
          \ {1}{M}}\\sum _{\\alpha }\\log \\omega _{\\alpha },} \u05DB\u05D0\u05E9\
          \u05E8 M \u05D4\u05D5\u05D0 \u05E1\u05DA \u05DB\u05DC \u05D3\u05E8\u05D2\
          \u05D5\u05EA \u05D4\u05D7\u05D5\u05E4\u05E9 \u05E9\u05DC \u05D4\u05DE\u05E2\
          \u05E8\u05DB\u05EA.\"\"\"\n\ntranslator(s, src_lang=\"he\")\n</code></pre>\n\
          <p>Those look like long inputs but seems to be caused by long outputs generated:</p>\n\
          <pre><code>s = \"\"\"\u05D3\u05F4\u05E8 \u05E9\u05DC\u05D5\u05DE\u05D9 \u05E7\
          \u05D5\u05D3\u05E9, \u05DE\u05E0\u05D4\u05DC \u05D1\u05D9\u05D4\"\u05D7\
          \ \u05E1\u05D5\u05E8\u05D5\u05E7\u05D4 | \u05E2\u05DC \u05E4\u05E8\u05D5\
          \u05E4' \u05E9\u05D0\u05D5\u05DC \u05E1\u05D5\u05E7\u05E0\u05D9\u05E7 \u05D6\
          \"\u05DC\"\"\"\ntranslator(s, src_lang=\"he\")\n</code></pre>\n"
        raw: "Found a few more inputs that causes the same error:\n\n```python\nfrom\
          \ transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom transformers\
          \ import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-mul-en\"\
          )\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-mul-en\"\
          )\n\ntranslator = pipeline(\"translation\", tokenizer=tokenizer, model=model,\
          \ max_length=1024)\n\ns = \"\"\"\u05DC\u05DB\u05DC \u05DE\u05DB\u05E8\u05D9\
          \ \u05D5\u05DE\u05D5\u05E7\u05D9\u05E8\u05D9 \u05D6\u05DB\u05E8\u05D9:\u05DE\
          \u05E6\u05D9\u05D0\u05D0\u05D5\u05EA \u05DC\u05E4\u05DE\u05D9\u05DD\u05DD\
          \ \u05E0\u05E1 \u05DE\u05DE\u05DE\u05E7\u05D5\u05DD \u05D4\u05DB\u05D9 \u05E4\
          \u05E4\u05D7\u05D5\u05EA \u05E6\u05E4\u05D5\u05D9 \u05D0\u05D1\u05DC \u05E2\
          \u05DD \u05D2\u05D1 \u05DC\u05DC\u05E7\u05D9\u05E8 \u05D4\u05DB\u05DC \u05D0\
          \u05E4\u05E9\u05E8\u05D9 \u05D5\u05D5\u05D0\u05E0\u05D9 \u05DE\u05D1\u05D9\
          \u05D9\u05D8 \u05D1\u05D2\u05D3\u05D3\u05E8\u05E8 \u05D4\u05D9\u05E8\u05D5\
          \u05E7\u05D4 \u05DC\u05E9\u05D1\u05E8\u05D9 \u05DB\u05DC\u05D9\u05DD \u05E9\
          \u05D4\u05D9\u05D9\u05EA\u05D9 \u05DE\u05D9\u05D5\u05E2\u05D3 \u05DC\u05E9\
          \u05DD, \u05DB\u05D0\u05E9\u05E8 \u05E2\u05D5\"\u05E1 \u05D0\u05E7\u05D9\
          \u05DD \u05DE\u05E9\u05D5\u05DC\u05D1 \u05DE\u05E8\u05E2\u05D4\u05D5 \u05D1\
          \u05E8\u05E4\u05E9 \u05DC\u05E0\u05E4\u05E9 \u05DC\u05E4\u05E7\u05D7 \u05E2\
          \u05DC \u05DE\u05D4 \u05E9\u05EA\u05E7\u05E0\u05D5 \"\u05E4\u05D9\u05D2\u05D5\
          \u05E8 \u05E9\u05DC \u05D2\u05D0\u05D5\u05E0\u05D9\u05DD\" \u05DC\u05D8\u05D0\
          \u05D8\u05D8\u05D0 \u05D7\u05E6\u05E8 \u05D1\u05D1\u05D9\u05EA \u05DE\u05DC\
          \u05D0\u05DB\u05D4 \u05D5\u05D0\u05D7\u05E8\u05D9 \u05E9\u05E0\u05EA\u05D9\
          \u05D9\u05D9\u05DD \u05D0\u05D5\u05DC\u05D9 \u05DC\u05EA\u05E4\u05D5\u05E8\
          \ \u05D9\u05D3\u05D9\u05E2\u05D5\u05EA \u05DC\u05EA\u05D9\u05E7\u05D9 \u05D1\
          \u05EA\u05D9 \u05E1\u05E4\u05E8,\u05D5\u05D0\u05D6 \u05DB\u05D1\u05E8 \u05D4\
          \u05EA\u05D7\u05DC\u05EA\u05D9 \u05DC\u05D2\u05E9\u05E9 \u05D1\u05DE\u05E2\
          \u05E9 \u05D0\u05D5\u05D5\u05EA\u05E0\u05D8\u05D9 \u05DC\u05D0 \u05E8\u05D5\
          \u05D5\u05D7\u05D9 \u05DC\u05D4\u05D2\u05D9\u05E2 \u05DC\u05DC\u05E2\u05D9\
          \u05E0\u05D9\u05DD \u05D0\u05D7\u05E8\u05D5\u05EA \u05D5\u05D0\u05D7\u05E8\
          \u05D9 \u05E9\u05E0\u05EA\u05D9\u05D9\u05DD \u05D8\u05E1\u05EA\u05D9 \u05DE\
          \u05E2\u05E2\u05D1\u05E8 \u05DC\u05D2\u05D3\u05E8\u05D5\u05EA \u05D5\u05DC\
          \u05D7\u05D5\u05DE\u05D5\u05EA,\u05D1\u05D8\u05D9\u05E4\u05D5\u05DC \u05DE\
          \u05D9\u05D8\u05D1\u05D9 \u05DC\u05D4\u05E1\u05EA\u05D9\u05E8 \u05DB\u05E1\
          \u05E4\u05D9 \u05D9\u05E8\u05D5\u05E9\u05D4 \u05E9\u05D0\u05E0\u05D9 \u05D8\
          \u05E8\u05D7\u05EA\u05D9 \u05DC\u05D0\u05D1\u05D9 \u05DB\u05D1\u05DF \u05E9\
          \u05E0\u05D5\u05D0\u05D0\u05EA \u05D5\u05D2\u05E8\u05D5\u05E9\u05EA \u05D4\
          \u05DC\u05D1 \u05D5\u05D4\u05DB\u05DC \u05D9\u05D3\u05E2\u05D5 \u05D0\u05E8\
          \u05D1\u05E2\u05D9\u05DD \u05E9\u05E0\u05D4 \u05DC\u05E4\u05E0\u05D9 \u05D1\
          \u05D4\u05EA\u05E2\u05DC\u05DE\u05D5 \u05D2\u05DD \u05D4\u05E4\u05D3\u05D2\
          \u05D5\u05D2\u05D9\u05D4. \u05D6\u05DB\u05D9\u05EA\u05D9 \u05D1\u05D4\u05DB\
          \u05E8\u05EA \u05E7\u05E8\u05DF \u05E0\u05E0\u05D5\u05D1\u05DC \u05DC\u05E1\
          \u05E4\u05E8\u05D5,\u05DC\u05D0 \u05EA\u05D9\u05D7\u05D5\u05DD \u05D0\u05DE\
          \u05E0\u05D5\u05EA\u05D9 \u05DC\u05E9\u05DE\u05D5,\u05E2\u05D3\u05D5\u05EA\
          \ \u05E7\u05DF \u05D4\u05E7\u05D5\u05E7\u05D9\u05D4 \u05DB\u05DE\u05E6\u05D9\
          \u05D0\u05D5\u05EA \u05D2\u05D4\u05D9\u05E0\u05D5\u05DE\u05D9\u05EA \u05D5\
          \u05D2\u05D3\u05E8\u05D5\u05EA \u05E0\u05E2\u05D5\u05DC\u05D5\u05EA. \u05E4\
          \u05EA\u05E8\u05D5\u05DF \u05E1\u05D5\u05E4\u05D9 \u05DC\u05D0 \u05D0\u05D9\
          \u05D7\u05E8 \u05DC\u05D1\u05D5\u05D0, \u05D0\u05D9\u05E9 \u05DE\u05DB\u05DC\
          \ \u05DE\u05D9\u05D5\u05D3\u05E2\u05D9 \u05D5\u05D0\u05D6 \u05D3\u05D5\u05E8\
          \ \u05D4\u05D5\u05E8\u05D9 \u05DB\u05DB\u05D1\u05E8 \u05D9\u05E8\u05D3 \u05DE\
          \u05D4\u05D1\u05DE\u05D4, \u05D1\u05D9\u05D2\u05D5\u05DF \u05DC\u05DE\u05E8\
          \u05D0\u05D4 \u05D4\u05D9\u05DC\u05D3 \u05D4\u05D7\u05D1\u05D9\u05D1 \u05E9\
          \u05DB\u05D1\u05E8\"\u05DC\u05D0 \u05D9\u05E2\u05E9\u05D4 \u05E6\u05D1\u05D0\
          \" \u05E9\u05E0\u05D5\u05EA\u05E8 \u05E4\u05E1\u05D9\u05E7\u05D4 \u05D0\u05E1\
          \u05E4\u05E1\u05D5\u05E4\u05D9\u05EA \u05DC\u05D4\u05E9\u05D0\u05E8 \u05D0\
          \u05E4\u05E1. \u05E6\u05E0\u05D5\u05E2\u05D9\u05DD\u05DD,\u05E1\u05D5\u05DC\
          \u05D9\u05D3\u05D9\u05DD,\u05D0\u05DC\u05D8\u05E8\u05D5\u05D0\u05D0\u05D9\
          \u05E1\u05D8\u05D9\u05DD \u05D5\u05EA\u05E8\u05D5\u05DE\u05D9\u05D9\u05DD\
          \ \u05DE\u05D0\u05DB\u05DC \u05DC\u05D7\u05D9\u05D9\u05EA\u05D5 \u05D4\u05DE\
          \u05D3\u05D9\u05E0\u05D4,\u05D0\u05D5\u05D5\u05D9 \u05DC\u05E9\u05D5\u05E4\
          \u05D8\u05D5\u05EA \u05D1\u05E8\u05E6\u05E6\u05D7\u05D7\u05E0\u05D5\u05EA\
          \ \u05D7\u05D5\u05DB\u05DE\u05D4 \u05DC\u05D4\u05E8\u05E8\u05D9 \u05D5\u05DC\
          \u05E0\u05D8\u05D5\u05DC \u05DE\u05E9\u05E4\u05D7\u05D4 \u05E9\u05D5\u05D0\
          \u05D4 \u05D0\u05D5 \u05E2\u05E7\u05D9\u05E8\u05D4,\u05D9\u05DC\u05D3\u05D9\
          \ \u05D1\u05D0\u05D9 \u05E2\u05D9\u05E8\u05E7 \u05D5\u05E6\u05E4\u05D5\u05DF\
          \ \u05D0\u05E4\u05E8\u05D9\u05E7\u05D4 \u05E0\u05D3\u05D5\u05E0\u05D5 \u05DC\
          \u05D0\u05D5\u05EA\u05D4 \u05E9\u05D8\u05E0\u05EA \u05D4\u05E4\u05E7\u05E8\
          \u05D4.\u05DB\u05DC \u05DB\u05D5\u05D7\u05D5\u05EA \u05D3\u05E2\u05EA \u05D1\
          \u05D0\u05E0\u05D2\u05DC\u05D9\u05EA \u05D1\u05D9\"\u05DC \u05D4\u05E9\u05E7\
          \u05E2\u05EA\u05D9 \u05DC\u05DE\u05E0\u05D5\u05E2 \u05DE\u05D2\u05D3\u05E8\
          \ \u05D9\u05E8\u05D5\u05E7\u05D4 \u05DC\u05E1\u05D2\u05D5\u05E8 \u05E2\u05DC\
          \u05D9..\u05D1\u05E9\u05E0\u05EA\u05D9 \u05D470 \"\u05DB\u05D5\u05DB\u05D1\
          \ \u05D4\u05D4\u05DC\u05DB\u05EA \u05E9\u05DC \u05DE\u05E8 \u05E1\u05D0\u05DE\
          \u05DC\u05E8\". \u05DC\u05E1\u05D5\u05DC \u05D1\u05DC\u05D5 \u05D1\u05E2\
          \u05D9\u05E8 \u05D5\u05D0\u05DD\"\"\"\n\ntranslator(s, src_lang=\"he\")\n\
          ```\n\nand \n\n```\ns = \"\"\"\n\u05D1\u05DE\u05D5\u05D3\u05DC \u05D0\u05D9\
          \u05D9\u05E0\u05E9\u05D8\u05D9\u05D9\u05DF \u05DE-1907 (\u05D1\u05E0\u05D9\
          \u05D2\u05D5\u05D3 \u05DC\u05DE\u05D5\u05D3\u05DC \u05D3\u05D1\u05D9\u05D9\
          \ \u05D4\u05DE\u05D0\u05D5\u05D7\u05E8 \u05D9\u05D5\u05EA\u05E8) \u05D0\u05E0\
          \u05D5 \u05DE\u05EA\u05D7\u05E9\u05D1\u05D9\u05DD \u05E8\u05E7 \u05D1\u05D2\
          \u05D1\u05D5\u05DC \u05D4\u05D8\u05DE\u05E4\u05E8\u05D8\u05D5\u05E8\u05D4\
          \ \u05D4\u05D2\u05D1\u05D5\u05D4\u05D4: k B T \u226B \u210F \u03C9 \u03B1\
          \ . {\\displaystyle k_{B}T\\gg \\hbar \\omega _{\\alpha }.\\,} \u05D0\u05D6\
          \u05D9 1 \u2212 e \u2212 \u210F \u03C9 \u03B1 / k B T \u2248 \u210F \u03C9\
          \ \u03B1 / k B T {\\displaystyle 1-e^{-\\hbar \\omega _{\\alpha }/k_{B}T}\\\
          approx \\hbar \\omega _{\\alpha }/k_{B}T\\,} \u05D5\u05D0\u05E0\u05D5 \u05DE\
          \u05E7\u05D1\u05DC\u05D9\u05DD F = N \u03B5 0 + k B T \u2211 \u03B1 log\
          \ \u2061 ( \u210F \u03C9 \u03B1 k B T ) . {\\displaystyle F=N\\varepsilon\
          \ _{0}+k_{B}T\\sum _{\\alpha }\\log \\left({\\frac {\\hbar \\omega _{\\\
          alpha }}{k_{B}T}}\\right).} \u05E0\u05D2\u05D3\u05D9\u05E8 \u05D0\u05EA\
          \ \u05D4\u05DE\u05DE\u05D5\u05E6\u05E2 \u05D4\u05D2\u05D0\u05D5\u05DE\u05D8\
          \u05E8\u05D9 \u05E9\u05DC \u05D4\u05EA\u05D3\u05D9\u05E8\u05D5\u05EA \u05DC\
          \u05D4\u05D9\u05D5\u05EA log \u2061 \u03C9 \xAF = 1 M \u2211 \u03B1 log\
          \ \u2061 \u03C9 \u03B1 , {\\displaystyle \\log {\\bar {\\omega }}={\\frac\
          \ {1}{M}}\\sum _{\\alpha }\\log \\omega _{\\alpha },} \u05DB\u05D0\u05E9\
          \u05E8 M \u05D4\u05D5\u05D0 \u05E1\u05DA \u05DB\u05DC \u05D3\u05E8\u05D2\
          \u05D5\u05EA \u05D4\u05D7\u05D5\u05E4\u05E9 \u05E9\u05DC \u05D4\u05DE\u05E2\
          \u05E8\u05DB\u05EA.\"\"\"\n\ntranslator(s, src_lang=\"he\")\n```\n\n\nThose\
          \ look like long inputs but seems to be caused by long outputs generated:\n\
          \n```\ns = \"\"\"\u05D3\u05F4\u05E8 \u05E9\u05DC\u05D5\u05DE\u05D9 \u05E7\
          \u05D5\u05D3\u05E9, \u05DE\u05E0\u05D4\u05DC \u05D1\u05D9\u05D4\"\u05D7\
          \ \u05E1\u05D5\u05E8\u05D5\u05E7\u05D4 | \u05E2\u05DC \u05E4\u05E8\u05D5\
          \u05E4' \u05E9\u05D0\u05D5\u05DC \u05E1\u05D5\u05E7\u05E0\u05D9\u05E7 \u05D6\
          \"\u05DC\"\"\"\ntranslator(s, src_lang=\"he\")\n```\n\n"
        updatedAt: '2023-08-17T11:15:34.670Z'
      numEdits: 0
      reactions: []
    id: 64de015622d604b137fb74fe
    type: comment
  author: alvations
  content: "Found a few more inputs that causes the same error:\n\n```python\nfrom\
    \ transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom transformers\
    \ import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-mul-en\"\
    )\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-mul-en\"\
    )\n\ntranslator = pipeline(\"translation\", tokenizer=tokenizer, model=model,\
    \ max_length=1024)\n\ns = \"\"\"\u05DC\u05DB\u05DC \u05DE\u05DB\u05E8\u05D9 \u05D5\
    \u05DE\u05D5\u05E7\u05D9\u05E8\u05D9 \u05D6\u05DB\u05E8\u05D9:\u05DE\u05E6\u05D9\
    \u05D0\u05D0\u05D5\u05EA \u05DC\u05E4\u05DE\u05D9\u05DD\u05DD \u05E0\u05E1 \u05DE\
    \u05DE\u05DE\u05E7\u05D5\u05DD \u05D4\u05DB\u05D9 \u05E4\u05E4\u05D7\u05D5\u05EA\
    \ \u05E6\u05E4\u05D5\u05D9 \u05D0\u05D1\u05DC \u05E2\u05DD \u05D2\u05D1 \u05DC\
    \u05DC\u05E7\u05D9\u05E8 \u05D4\u05DB\u05DC \u05D0\u05E4\u05E9\u05E8\u05D9 \u05D5\
    \u05D5\u05D0\u05E0\u05D9 \u05DE\u05D1\u05D9\u05D9\u05D8 \u05D1\u05D2\u05D3\u05D3\
    \u05E8\u05E8 \u05D4\u05D9\u05E8\u05D5\u05E7\u05D4 \u05DC\u05E9\u05D1\u05E8\u05D9\
    \ \u05DB\u05DC\u05D9\u05DD \u05E9\u05D4\u05D9\u05D9\u05EA\u05D9 \u05DE\u05D9\u05D5\
    \u05E2\u05D3 \u05DC\u05E9\u05DD, \u05DB\u05D0\u05E9\u05E8 \u05E2\u05D5\"\u05E1\
    \ \u05D0\u05E7\u05D9\u05DD \u05DE\u05E9\u05D5\u05DC\u05D1 \u05DE\u05E8\u05E2\u05D4\
    \u05D5 \u05D1\u05E8\u05E4\u05E9 \u05DC\u05E0\u05E4\u05E9 \u05DC\u05E4\u05E7\u05D7\
    \ \u05E2\u05DC \u05DE\u05D4 \u05E9\u05EA\u05E7\u05E0\u05D5 \"\u05E4\u05D9\u05D2\
    \u05D5\u05E8 \u05E9\u05DC \u05D2\u05D0\u05D5\u05E0\u05D9\u05DD\" \u05DC\u05D8\u05D0\
    \u05D8\u05D8\u05D0 \u05D7\u05E6\u05E8 \u05D1\u05D1\u05D9\u05EA \u05DE\u05DC\u05D0\
    \u05DB\u05D4 \u05D5\u05D0\u05D7\u05E8\u05D9 \u05E9\u05E0\u05EA\u05D9\u05D9\u05D9\
    \u05DD \u05D0\u05D5\u05DC\u05D9 \u05DC\u05EA\u05E4\u05D5\u05E8 \u05D9\u05D3\u05D9\
    \u05E2\u05D5\u05EA \u05DC\u05EA\u05D9\u05E7\u05D9 \u05D1\u05EA\u05D9 \u05E1\u05E4\
    \u05E8,\u05D5\u05D0\u05D6 \u05DB\u05D1\u05E8 \u05D4\u05EA\u05D7\u05DC\u05EA\u05D9\
    \ \u05DC\u05D2\u05E9\u05E9 \u05D1\u05DE\u05E2\u05E9 \u05D0\u05D5\u05D5\u05EA\u05E0\
    \u05D8\u05D9 \u05DC\u05D0 \u05E8\u05D5\u05D5\u05D7\u05D9 \u05DC\u05D4\u05D2\u05D9\
    \u05E2 \u05DC\u05DC\u05E2\u05D9\u05E0\u05D9\u05DD \u05D0\u05D7\u05E8\u05D5\u05EA\
    \ \u05D5\u05D0\u05D7\u05E8\u05D9 \u05E9\u05E0\u05EA\u05D9\u05D9\u05DD \u05D8\u05E1\
    \u05EA\u05D9 \u05DE\u05E2\u05E2\u05D1\u05E8 \u05DC\u05D2\u05D3\u05E8\u05D5\u05EA\
    \ \u05D5\u05DC\u05D7\u05D5\u05DE\u05D5\u05EA,\u05D1\u05D8\u05D9\u05E4\u05D5\u05DC\
    \ \u05DE\u05D9\u05D8\u05D1\u05D9 \u05DC\u05D4\u05E1\u05EA\u05D9\u05E8 \u05DB\u05E1\
    \u05E4\u05D9 \u05D9\u05E8\u05D5\u05E9\u05D4 \u05E9\u05D0\u05E0\u05D9 \u05D8\u05E8\
    \u05D7\u05EA\u05D9 \u05DC\u05D0\u05D1\u05D9 \u05DB\u05D1\u05DF \u05E9\u05E0\u05D5\
    \u05D0\u05D0\u05EA \u05D5\u05D2\u05E8\u05D5\u05E9\u05EA \u05D4\u05DC\u05D1 \u05D5\
    \u05D4\u05DB\u05DC \u05D9\u05D3\u05E2\u05D5 \u05D0\u05E8\u05D1\u05E2\u05D9\u05DD\
    \ \u05E9\u05E0\u05D4 \u05DC\u05E4\u05E0\u05D9 \u05D1\u05D4\u05EA\u05E2\u05DC\u05DE\
    \u05D5 \u05D2\u05DD \u05D4\u05E4\u05D3\u05D2\u05D5\u05D2\u05D9\u05D4. \u05D6\u05DB\
    \u05D9\u05EA\u05D9 \u05D1\u05D4\u05DB\u05E8\u05EA \u05E7\u05E8\u05DF \u05E0\u05E0\
    \u05D5\u05D1\u05DC \u05DC\u05E1\u05E4\u05E8\u05D5,\u05DC\u05D0 \u05EA\u05D9\u05D7\
    \u05D5\u05DD \u05D0\u05DE\u05E0\u05D5\u05EA\u05D9 \u05DC\u05E9\u05DE\u05D5,\u05E2\
    \u05D3\u05D5\u05EA \u05E7\u05DF \u05D4\u05E7\u05D5\u05E7\u05D9\u05D4 \u05DB\u05DE\
    \u05E6\u05D9\u05D0\u05D5\u05EA \u05D2\u05D4\u05D9\u05E0\u05D5\u05DE\u05D9\u05EA\
    \ \u05D5\u05D2\u05D3\u05E8\u05D5\u05EA \u05E0\u05E2\u05D5\u05DC\u05D5\u05EA. \u05E4\
    \u05EA\u05E8\u05D5\u05DF \u05E1\u05D5\u05E4\u05D9 \u05DC\u05D0 \u05D0\u05D9\u05D7\
    \u05E8 \u05DC\u05D1\u05D5\u05D0, \u05D0\u05D9\u05E9 \u05DE\u05DB\u05DC \u05DE\u05D9\
    \u05D5\u05D3\u05E2\u05D9 \u05D5\u05D0\u05D6 \u05D3\u05D5\u05E8 \u05D4\u05D5\u05E8\
    \u05D9 \u05DB\u05DB\u05D1\u05E8 \u05D9\u05E8\u05D3 \u05DE\u05D4\u05D1\u05DE\u05D4\
    , \u05D1\u05D9\u05D2\u05D5\u05DF \u05DC\u05DE\u05E8\u05D0\u05D4 \u05D4\u05D9\u05DC\
    \u05D3 \u05D4\u05D7\u05D1\u05D9\u05D1 \u05E9\u05DB\u05D1\u05E8\"\u05DC\u05D0 \u05D9\
    \u05E2\u05E9\u05D4 \u05E6\u05D1\u05D0\" \u05E9\u05E0\u05D5\u05EA\u05E8 \u05E4\u05E1\
    \u05D9\u05E7\u05D4 \u05D0\u05E1\u05E4\u05E1\u05D5\u05E4\u05D9\u05EA \u05DC\u05D4\
    \u05E9\u05D0\u05E8 \u05D0\u05E4\u05E1. \u05E6\u05E0\u05D5\u05E2\u05D9\u05DD\u05DD\
    ,\u05E1\u05D5\u05DC\u05D9\u05D3\u05D9\u05DD,\u05D0\u05DC\u05D8\u05E8\u05D5\u05D0\
    \u05D0\u05D9\u05E1\u05D8\u05D9\u05DD \u05D5\u05EA\u05E8\u05D5\u05DE\u05D9\u05D9\
    \u05DD \u05DE\u05D0\u05DB\u05DC \u05DC\u05D7\u05D9\u05D9\u05EA\u05D5 \u05D4\u05DE\
    \u05D3\u05D9\u05E0\u05D4,\u05D0\u05D5\u05D5\u05D9 \u05DC\u05E9\u05D5\u05E4\u05D8\
    \u05D5\u05EA \u05D1\u05E8\u05E6\u05E6\u05D7\u05D7\u05E0\u05D5\u05EA \u05D7\u05D5\
    \u05DB\u05DE\u05D4 \u05DC\u05D4\u05E8\u05E8\u05D9 \u05D5\u05DC\u05E0\u05D8\u05D5\
    \u05DC \u05DE\u05E9\u05E4\u05D7\u05D4 \u05E9\u05D5\u05D0\u05D4 \u05D0\u05D5 \u05E2\
    \u05E7\u05D9\u05E8\u05D4,\u05D9\u05DC\u05D3\u05D9 \u05D1\u05D0\u05D9 \u05E2\u05D9\
    \u05E8\u05E7 \u05D5\u05E6\u05E4\u05D5\u05DF \u05D0\u05E4\u05E8\u05D9\u05E7\u05D4\
    \ \u05E0\u05D3\u05D5\u05E0\u05D5 \u05DC\u05D0\u05D5\u05EA\u05D4 \u05E9\u05D8\u05E0\
    \u05EA \u05D4\u05E4\u05E7\u05E8\u05D4.\u05DB\u05DC \u05DB\u05D5\u05D7\u05D5\u05EA\
    \ \u05D3\u05E2\u05EA \u05D1\u05D0\u05E0\u05D2\u05DC\u05D9\u05EA \u05D1\u05D9\"\
    \u05DC \u05D4\u05E9\u05E7\u05E2\u05EA\u05D9 \u05DC\u05DE\u05E0\u05D5\u05E2 \u05DE\
    \u05D2\u05D3\u05E8 \u05D9\u05E8\u05D5\u05E7\u05D4 \u05DC\u05E1\u05D2\u05D5\u05E8\
    \ \u05E2\u05DC\u05D9..\u05D1\u05E9\u05E0\u05EA\u05D9 \u05D470 \"\u05DB\u05D5\u05DB\
    \u05D1 \u05D4\u05D4\u05DC\u05DB\u05EA \u05E9\u05DC \u05DE\u05E8 \u05E1\u05D0\u05DE\
    \u05DC\u05E8\". \u05DC\u05E1\u05D5\u05DC \u05D1\u05DC\u05D5 \u05D1\u05E2\u05D9\
    \u05E8 \u05D5\u05D0\u05DD\"\"\"\n\ntranslator(s, src_lang=\"he\")\n```\n\nand\
    \ \n\n```\ns = \"\"\"\n\u05D1\u05DE\u05D5\u05D3\u05DC \u05D0\u05D9\u05D9\u05E0\
    \u05E9\u05D8\u05D9\u05D9\u05DF \u05DE-1907 (\u05D1\u05E0\u05D9\u05D2\u05D5\u05D3\
    \ \u05DC\u05DE\u05D5\u05D3\u05DC \u05D3\u05D1\u05D9\u05D9 \u05D4\u05DE\u05D0\u05D5\
    \u05D7\u05E8 \u05D9\u05D5\u05EA\u05E8) \u05D0\u05E0\u05D5 \u05DE\u05EA\u05D7\u05E9\
    \u05D1\u05D9\u05DD \u05E8\u05E7 \u05D1\u05D2\u05D1\u05D5\u05DC \u05D4\u05D8\u05DE\
    \u05E4\u05E8\u05D8\u05D5\u05E8\u05D4 \u05D4\u05D2\u05D1\u05D5\u05D4\u05D4: k B\
    \ T \u226B \u210F \u03C9 \u03B1 . {\\displaystyle k_{B}T\\gg \\hbar \\omega _{\\\
    alpha }.\\,} \u05D0\u05D6\u05D9 1 \u2212 e \u2212 \u210F \u03C9 \u03B1 / k B T\
    \ \u2248 \u210F \u03C9 \u03B1 / k B T {\\displaystyle 1-e^{-\\hbar \\omega _{\\\
    alpha }/k_{B}T}\\approx \\hbar \\omega _{\\alpha }/k_{B}T\\,} \u05D5\u05D0\u05E0\
    \u05D5 \u05DE\u05E7\u05D1\u05DC\u05D9\u05DD F = N \u03B5 0 + k B T \u2211 \u03B1\
    \ log \u2061 ( \u210F \u03C9 \u03B1 k B T ) . {\\displaystyle F=N\\varepsilon\
    \ _{0}+k_{B}T\\sum _{\\alpha }\\log \\left({\\frac {\\hbar \\omega _{\\alpha }}{k_{B}T}}\\\
    right).} \u05E0\u05D2\u05D3\u05D9\u05E8 \u05D0\u05EA \u05D4\u05DE\u05DE\u05D5\u05E6\
    \u05E2 \u05D4\u05D2\u05D0\u05D5\u05DE\u05D8\u05E8\u05D9 \u05E9\u05DC \u05D4\u05EA\
    \u05D3\u05D9\u05E8\u05D5\u05EA \u05DC\u05D4\u05D9\u05D5\u05EA log \u2061 \u03C9\
    \ \xAF = 1 M \u2211 \u03B1 log \u2061 \u03C9 \u03B1 , {\\displaystyle \\log {\\\
    bar {\\omega }}={\\frac {1}{M}}\\sum _{\\alpha }\\log \\omega _{\\alpha },} \u05DB\
    \u05D0\u05E9\u05E8 M \u05D4\u05D5\u05D0 \u05E1\u05DA \u05DB\u05DC \u05D3\u05E8\
    \u05D2\u05D5\u05EA \u05D4\u05D7\u05D5\u05E4\u05E9 \u05E9\u05DC \u05D4\u05DE\u05E2\
    \u05E8\u05DB\u05EA.\"\"\"\n\ntranslator(s, src_lang=\"he\")\n```\n\n\nThose look\
    \ like long inputs but seems to be caused by long outputs generated:\n\n```\n\
    s = \"\"\"\u05D3\u05F4\u05E8 \u05E9\u05DC\u05D5\u05DE\u05D9 \u05E7\u05D5\u05D3\
    \u05E9, \u05DE\u05E0\u05D4\u05DC \u05D1\u05D9\u05D4\"\u05D7 \u05E1\u05D5\u05E8\
    \u05D5\u05E7\u05D4 | \u05E2\u05DC \u05E4\u05E8\u05D5\u05E4' \u05E9\u05D0\u05D5\
    \u05DC \u05E1\u05D5\u05E7\u05E0\u05D9\u05E7 \u05D6\"\u05DC\"\"\"\ntranslator(s,\
    \ src_lang=\"he\")\n```\n\n"
  created_at: 2023-08-17 10:15:34+00:00
  edited: false
  hidden: false
  id: 64de015622d604b137fb74fe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: Helsinki-NLP/opus-mt-mul-en
repo_type: model
status: open
target_branch: null
title: Error during inference
