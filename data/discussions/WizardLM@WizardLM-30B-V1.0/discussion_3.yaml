!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Thireus
conflicting_files: null
created_at: 2023-06-07 20:36:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-06-07T21:36:44.000Z'
    data:
      edited: true
      editors:
      - Thireus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.908172607421875
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: "<p>I found that the following prompt format works better than one without\
          \ \\n:</p>\n<p>With \\n:</p>\n<pre><code>A chat between a curious user and\
          \ an artificial intelligence assistant. The assistant gives helpful, detailed,\
          \ and polite answers to the user's questions.\nUSER:\nHi\nASSISTANT:\nHello!\
          \ How can I assist you today? Do you have any specific question or topic\
          \ in mind that you would like me to help with? If not, feel free to ask\
          \ anything related to my knowledge domain, which includes general information\
          \ on various topics such as science, history, geography, culture, etc.,\
          \ as well as technical skills such as programming languages, software development,\
          \ machine learning, data analysis, and more. Please let me know how I can\
          \ be of assistance. \U0001F60A\n</code></pre>\n<p>Without \\n:</p>\n<pre><code>A\
          \ chat between a curious user and an artificial intelligence assistant.\
          \ The assistant gives helpful, detailed, and polite answers to the user's\
          \ questions. USER: Hi ASSISTANT: \u200B[06:23] &lt;Assistant&gt; Hello!\
          \ How can I assist you today?\n(Note: This is just a sample response from\
          \ me as an AI language model. In reality, there would be no\n</code></pre>\n\
          <p>The biggest issue however, is that the EOS token gets embedded in the\
          \ middle of the replies prematurely.</p>\n<p>Notable example:</p>\n<pre><code>USER:\
          \ Tell me a random fact ASSISTANT: \n</code></pre>\n<p>Will immediately\
          \ output an EOS token without producing a response.</p>\n<p>Other example:</p>\n\
          <pre><code>USER:\nThree experts with exceptional logical thinking skills\
          \ are collaboratively solving a problem, creating a breadth-first search\
          \ of the tree of probable solutions. Each expert will share their thought\
          \ process in detail, taking into account the previous thoughts of other\
          \ and admitting any errors. They will iteratively refine and expand upon\
          \ each other's ideas, giving credit where it's due, and discarding the dead\
          \ end nodes of the tree. The process continues until a conclusive answer\
          \ is found. Organize the entire response in a markdown table format. The\
          \ problem is \"Decide what is the meaning of life.\"\nASSISTANT:\n</code></pre>\n\
          <p>The above request will not complete, an EOS token will be embedded prematurely\
          \ in the middle of the markdown table.</p>\n<p>Other example:</p>\n<pre><code>USER:\n\
          Give me a list of 50 insects\nASSISTANT:\n</code></pre>\n<p>will output:</p>\n\
          <pre><code>USER:\nGive me a list of 50 insects\nASSISTANT:\n1. Ant\n2. Bee\n\
          3. Butterfly\n4. Caterpillar\n5. Cricket\n6. Dragonfly\n7. Earwig\n8. Fly\n\
          9. Grasshopper\n10. Ladybug\n11. Lice\n12. Locust\n13. Mantis\n14. Mealworm\n\
          15. Mosquito\n16. Moth\n17. Nematode\n18. Praying mantis\n1\n</code></pre>\n\
          <p>Ignoring EOS token doesn't solve the issue because the model does not\
          \ follow a rigorous conversation layout, sometimes it can talk forever without\
          \ outputting a \"USER:\" or similar pattern that could be captured instead\
          \ of the EOS to stop the completion process.</p>\n<p>I should mention that\
          \ I'm using a quantized version - TheBloke/WizardLM-30B-GPTQ - would be\
          \ interesting to know if the results of the non-quantized model differ.</p>\n"
        raw: "I found that the following prompt format works better than one without\
          \ \\n:\n\nWith \\n:\n```\nA chat between a curious user and an artificial\
          \ intelligence assistant. The assistant gives helpful, detailed, and polite\
          \ answers to the user's questions.\nUSER:\nHi\nASSISTANT:\nHello! How can\
          \ I assist you today? Do you have any specific question or topic in mind\
          \ that you would like me to help with? If not, feel free to ask anything\
          \ related to my knowledge domain, which includes general information on\
          \ various topics such as science, history, geography, culture, etc., as\
          \ well as technical skills such as programming languages, software development,\
          \ machine learning, data analysis, and more. Please let me know how I can\
          \ be of assistance. \U0001F60A\n```\n\nWithout \\n:\n```\nA chat between\
          \ a curious user and an artificial intelligence assistant. The assistant\
          \ gives helpful, detailed, and polite answers to the user's questions. USER:\
          \ Hi ASSISTANT: \u200B[06:23] <Assistant> Hello! How can I assist you today?\n\
          (Note: This is just a sample response from me as an AI language model. In\
          \ reality, there would be no\n```\n\nThe biggest issue however, is that\
          \ the EOS token gets embedded in the middle of the replies prematurely.\n\
          \nNotable example:\n```\nUSER: Tell me a random fact ASSISTANT: \n```\n\n\
          Will immediately output an EOS token without producing a response.\n\nOther\
          \ example:\n```\nUSER:\nThree experts with exceptional logical thinking\
          \ skills are collaboratively solving a problem, creating a breadth-first\
          \ search of the tree of probable solutions. Each expert will share their\
          \ thought process in detail, taking into account the previous thoughts of\
          \ other and admitting any errors. They will iteratively refine and expand\
          \ upon each other's ideas, giving credit where it's due, and discarding\
          \ the dead end nodes of the tree. The process continues until a conclusive\
          \ answer is found. Organize the entire response in a markdown table format.\
          \ The problem is \"Decide what is the meaning of life.\"\nASSISTANT:\n```\n\
          \nThe above request will not complete, an EOS token will be embedded prematurely\
          \ in the middle of the markdown table.\n\nOther example:\n```\nUSER:\nGive\
          \ me a list of 50 insects\nASSISTANT:\n```\n\nwill output:\n```\nUSER:\n\
          Give me a list of 50 insects\nASSISTANT:\n1. Ant\n2. Bee\n3. Butterfly\n\
          4. Caterpillar\n5. Cricket\n6. Dragonfly\n7. Earwig\n8. Fly\n9. Grasshopper\n\
          10. Ladybug\n11. Lice\n12. Locust\n13. Mantis\n14. Mealworm\n15. Mosquito\n\
          16. Moth\n17. Nematode\n18. Praying mantis\n1\n```\n\nIgnoring EOS token\
          \ doesn't solve the issue because the model does not follow a rigorous conversation\
          \ layout, sometimes it can talk forever without outputting a \"USER:\" or\
          \ similar pattern that could be captured instead of the EOS to stop the\
          \ completion process.\n\nI should mention that I'm using a quantized version\
          \ - TheBloke/WizardLM-30B-GPTQ - would be interesting to know if the results\
          \ of the non-quantized model differ."
        updatedAt: '2023-06-07T21:47:23.599Z'
      numEdits: 1
      reactions: []
    id: 6480f86cde559d48dbb3cfd3
    type: comment
  author: Thireus
  content: "I found that the following prompt format works better than one without\
    \ \\n:\n\nWith \\n:\n```\nA chat between a curious user and an artificial intelligence\
    \ assistant. The assistant gives helpful, detailed, and polite answers to the\
    \ user's questions.\nUSER:\nHi\nASSISTANT:\nHello! How can I assist you today?\
    \ Do you have any specific question or topic in mind that you would like me to\
    \ help with? If not, feel free to ask anything related to my knowledge domain,\
    \ which includes general information on various topics such as science, history,\
    \ geography, culture, etc., as well as technical skills such as programming languages,\
    \ software development, machine learning, data analysis, and more. Please let\
    \ me know how I can be of assistance. \U0001F60A\n```\n\nWithout \\n:\n```\nA\
    \ chat between a curious user and an artificial intelligence assistant. The assistant\
    \ gives helpful, detailed, and polite answers to the user's questions. USER: Hi\
    \ ASSISTANT: \u200B[06:23] <Assistant> Hello! How can I assist you today?\n(Note:\
    \ This is just a sample response from me as an AI language model. In reality,\
    \ there would be no\n```\n\nThe biggest issue however, is that the EOS token gets\
    \ embedded in the middle of the replies prematurely.\n\nNotable example:\n```\n\
    USER: Tell me a random fact ASSISTANT: \n```\n\nWill immediately output an EOS\
    \ token without producing a response.\n\nOther example:\n```\nUSER:\nThree experts\
    \ with exceptional logical thinking skills are collaboratively solving a problem,\
    \ creating a breadth-first search of the tree of probable solutions. Each expert\
    \ will share their thought process in detail, taking into account the previous\
    \ thoughts of other and admitting any errors. They will iteratively refine and\
    \ expand upon each other's ideas, giving credit where it's due, and discarding\
    \ the dead end nodes of the tree. The process continues until a conclusive answer\
    \ is found. Organize the entire response in a markdown table format. The problem\
    \ is \"Decide what is the meaning of life.\"\nASSISTANT:\n```\n\nThe above request\
    \ will not complete, an EOS token will be embedded prematurely in the middle of\
    \ the markdown table.\n\nOther example:\n```\nUSER:\nGive me a list of 50 insects\n\
    ASSISTANT:\n```\n\nwill output:\n```\nUSER:\nGive me a list of 50 insects\nASSISTANT:\n\
    1. Ant\n2. Bee\n3. Butterfly\n4. Caterpillar\n5. Cricket\n6. Dragonfly\n7. Earwig\n\
    8. Fly\n9. Grasshopper\n10. Ladybug\n11. Lice\n12. Locust\n13. Mantis\n14. Mealworm\n\
    15. Mosquito\n16. Moth\n17. Nematode\n18. Praying mantis\n1\n```\n\nIgnoring EOS\
    \ token doesn't solve the issue because the model does not follow a rigorous conversation\
    \ layout, sometimes it can talk forever without outputting a \"USER:\" or similar\
    \ pattern that could be captured instead of the EOS to stop the completion process.\n\
    \nI should mention that I'm using a quantized version - TheBloke/WizardLM-30B-GPTQ\
    \ - would be interesting to know if the results of the non-quantized model differ."
  created_at: 2023-06-07 20:36:44+00:00
  edited: true
  hidden: false
  id: 6480f86cde559d48dbb3cfd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-06-08T20:28:48.000Z'
    data:
      edited: false
      editors:
      - Thireus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9199114441871643
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: '<p>EOS issue can be fixed by making sure the chat prompt begins with:</p>

          <pre><code>A chat between a curious user and an artificial intelligence
          assistant. The assistant gives helpful, detailed, and polite answers to
          the user''s questions.

          </code></pre>

          <p>Without it, premature EOS tokens happen.</p>

          '
        raw: 'EOS issue can be fixed by making sure the chat prompt begins with:


          ```

          A chat between a curious user and an artificial intelligence assistant.
          The assistant gives helpful, detailed, and polite answers to the user''s
          questions.

          ```


          Without it, premature EOS tokens happen.'
        updatedAt: '2023-06-08T20:28:48.951Z'
      numEdits: 0
      reactions: []
    id: 64823a0085ce4d2973ba8504
    type: comment
  author: Thireus
  content: 'EOS issue can be fixed by making sure the chat prompt begins with:


    ```

    A chat between a curious user and an artificial intelligence assistant. The assistant
    gives helpful, detailed, and polite answers to the user''s questions.

    ```


    Without it, premature EOS tokens happen.'
  created_at: 2023-06-08 19:28:48+00:00
  edited: false
  hidden: false
  id: 64823a0085ce4d2973ba8504
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: WizardLM/WizardLM-30B-V1.0
repo_type: model
status: open
target_branch: null
title: Prompt format, Premature EOS and Empty Responses
