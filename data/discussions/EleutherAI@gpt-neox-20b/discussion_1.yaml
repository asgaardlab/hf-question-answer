!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ghpkishore
conflicting_files: null
created_at: 2022-06-13 12:30:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f41b208bbbdafebe521845140f006c68.svg
      fullname: kishore G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ghpkishore
      type: user
    createdAt: '2022-06-13T13:30:34.000Z'
    data:
      edited: false
      editors:
      - ghpkishore
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f41b208bbbdafebe521845140f006c68.svg
          fullname: kishore G
          isHf: false
          isPro: false
          name: ghpkishore
          type: user
        html: '<p>I cannot seem to locally load the model in colab using the git function.
          It shows that setup.py is missing. Also, when I try to use the normal method
          of "from transformer import", I am not able to load it cause the RAM gets
          over. I am using Google Colab pro account. Is there a way for me to resolve
          this? </p>

          '
        raw: 'I cannot seem to locally load the model in colab using the git function.
          It shows that setup.py is missing. Also, when I try to use the normal method
          of "from transformer import", I am not able to load it cause the RAM gets
          over. I am using Google Colab pro account. Is there a way for me to resolve
          this? '
        updatedAt: '2022-06-13T13:30:34.718Z'
      numEdits: 0
      reactions: []
    id: 62a73bfab8128deefcd15c0a
    type: comment
  author: ghpkishore
  content: 'I cannot seem to locally load the model in colab using the git function.
    It shows that setup.py is missing. Also, when I try to use the normal method of
    "from transformer import", I am not able to load it cause the RAM gets over. I
    am using Google Colab pro account. Is there a way for me to resolve this? '
  created_at: 2022-06-13 12:30:34+00:00
  edited: false
  hidden: false
  id: 62a73bfab8128deefcd15c0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2022-06-18T08:52:56.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Hi,</p>

          <p>Looking at the <a href="https://huggingface.co/docs/transformers/model_doc/gpt_neox">docs</a>,
          the weights are in float16 format, meaning that 16 bits or 2 bytes are used
          to store each parameter. </p>

          <p>That means that, for a 20 billion parameter model, you need 20 billion
          parameters * 2 bytes / parameter = 40 billion bytes, also known as 40 GB.
          That''s the amount of RAM required to load the model.</p>

          '
        raw: "Hi,\n\nLooking at the [docs](https://huggingface.co/docs/transformers/model_doc/gpt_neox),\
          \ the weights are in float16 format, meaning that 16 bits or 2 bytes are\
          \ used to store each parameter. \n\nThat means that, for a 20 billion parameter\
          \ model, you need 20 billion parameters * 2 bytes / parameter = 40 billion\
          \ bytes, also known as 40 GB. That's the amount of RAM required to load\
          \ the model."
        updatedAt: '2022-06-18T08:52:56.478Z'
      numEdits: 0
      reactions: []
    id: 62ad9268d2543e176d3dd612
    type: comment
  author: nielsr
  content: "Hi,\n\nLooking at the [docs](https://huggingface.co/docs/transformers/model_doc/gpt_neox),\
    \ the weights are in float16 format, meaning that 16 bits or 2 bytes are used\
    \ to store each parameter. \n\nThat means that, for a 20 billion parameter model,\
    \ you need 20 billion parameters * 2 bytes / parameter = 40 billion bytes, also\
    \ known as 40 GB. That's the amount of RAM required to load the model."
  created_at: 2022-06-18 07:52:56+00:00
  edited: false
  hidden: false
  id: 62ad9268d2543e176d3dd612
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg?w=200&h=200&f=face
      fullname: Stella Biderman
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: stellaathena
      type: user
    createdAt: '2022-06-18T12:52:17.000Z'
    data:
      edited: false
      editors:
      - stellaathena
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg?w=200&h=200&f=face
          fullname: Stella Biderman
          isHf: false
          isPro: false
          name: stellaathena
          type: user
        html: "<p>That\u2019s not quite correct. GPT-NeoX-20B was trained using mixed\
          \ precision (fp32/fp16). These weights are in fp32, which is why the docs\
          \ mention using <code>.half()</code> before loading the model onto GPU.</p>\n\
          <p>I\u2019m not sure what GPUs you are able to get via Colab but inference\
          \ with this model typically requires more then 40 GB of VRAM.</p>\n"
        raw: "That\u2019s not quite correct. GPT-NeoX-20B was trained using mixed\
          \ precision (fp32/fp16). These weights are in fp32, which is why the docs\
          \ mention using `.half()` before loading the model onto GPU.\n\nI\u2019\
          m not sure what GPUs you are able to get via Colab but inference with this\
          \ model typically requires more then 40 GB of VRAM."
        updatedAt: '2022-06-18T12:52:17.195Z'
      numEdits: 0
      reactions: []
    id: 62adca8117d21e782c6ed51b
    type: comment
  author: stellaathena
  content: "That\u2019s not quite correct. GPT-NeoX-20B was trained using mixed precision\
    \ (fp32/fp16). These weights are in fp32, which is why the docs mention using\
    \ `.half()` before loading the model onto GPU.\n\nI\u2019m not sure what GPUs\
    \ you are able to get via Colab but inference with this model typically requires\
    \ more then 40 GB of VRAM."
  created_at: 2022-06-18 11:52:17+00:00
  edited: false
  hidden: false
  id: 62adca8117d21e782c6ed51b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2022-06-20T08:20:12.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Oh my apologies. I read from the docs "GPT-NeoX-20B was trained
          with fp16", I guess this can be corrected.</p>

          <p>Also, I think it may be beneficial to add the RAM requirements to the
          docs as well, similar to the <a href="https://huggingface.co/docs/transformers/model_doc/gptj#overview">"tips"
          section</a> of GPT-J. </p>

          <p>Do you think it would be beneficial to have a separate branch on this
          repo with float16 weights?</p>

          '
        raw: "Oh my apologies. I read from the docs \"GPT-NeoX-20B was trained with\
          \ fp16\", I guess this can be corrected.\n\nAlso, I think it may be beneficial\
          \ to add the RAM requirements to the docs as well, similar to the [\"tips\"\
          \ section](https://huggingface.co/docs/transformers/model_doc/gptj#overview)\
          \ of GPT-J. \n\nDo you think it would be beneficial to have a separate branch\
          \ on this repo with float16 weights?"
        updatedAt: '2022-06-20T08:20:12.759Z'
      numEdits: 0
      reactions: []
    id: 62b02dbc205cd78bca63c2f7
    type: comment
  author: nielsr
  content: "Oh my apologies. I read from the docs \"GPT-NeoX-20B was trained with\
    \ fp16\", I guess this can be corrected.\n\nAlso, I think it may be beneficial\
    \ to add the RAM requirements to the docs as well, similar to the [\"tips\" section](https://huggingface.co/docs/transformers/model_doc/gptj#overview)\
    \ of GPT-J. \n\nDo you think it would be beneficial to have a separate branch\
    \ on this repo with float16 weights?"
  created_at: 2022-06-20 07:20:12+00:00
  edited: false
  hidden: false
  id: 62b02dbc205cd78bca63c2f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2022-06-21T15:46:24.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: "<p>cc'ing <span data-props=\"{&quot;user&quot;:&quot;sgugger&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sgugger\"\
          >@<span class=\"underline\">sgugger</span></a></span>\n\n\t</span></span>\
          \ regarding whether or not this model can be loaded into Google Colab using\
          \ Accelerate's big model inference feature.</p>\n"
        raw: cc'ing @sgugger regarding whether or not this model can be loaded into
          Google Colab using Accelerate's big model inference feature.
        updatedAt: '2022-06-21T15:46:24.114Z'
      numEdits: 0
      reactions: []
    id: 62b1e7d0f4a72794188c2cad
    type: comment
  author: nielsr
  content: cc'ing @sgugger regarding whether or not this model can be loaded into
    Google Colab using Accelerate's big model inference feature.
  created_at: 2022-06-21 14:46:24+00:00
  edited: false
  hidden: false
  id: 62b1e7d0f4a72794188c2cad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
      fullname: Sylvain Gugger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgugger
      type: user
    createdAt: '2022-06-21T21:42:28.000Z'
    data:
      edited: true
      editors:
      - sgugger
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
          fullname: Sylvain Gugger
          isHf: false
          isPro: false
          name: sgugger
          type: user
        html: "<p>Not on Colab free no, they don't provide enough disk space to even\
          \ download the weights.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;stellaathena&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/stellaathena\"\
          >@<span class=\"underline\">stellaathena</span></a></span>\n\n\t</span></span>\
          \ I'm surprised to learn the model was trained in fp16 (not bfloat16?) as\
          \ we get crappy generations in FP16 but decent ones in bfloat16 in our tests.</p>\n\
          <p><strong>Edit:</strong> Looks like it was only a bug in the Transformers\
          \ implementation. <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/pull/17811\"\
          >https://github.com/huggingface/transformers/pull/17811</a> should fix the\
          \ float16 generations.</p>\n"
        raw: 'Not on Colab free no, they don''t provide enough disk space to even
          download the weights.


          @stellaathena I''m surprised to learn the model was trained in fp16 (not
          bfloat16?) as we get crappy generations in FP16 but decent ones in bfloat16
          in our tests.


          **Edit:** Looks like it was only a bug in the Transformers implementation.
          https://github.com/huggingface/transformers/pull/17811 should fix the float16
          generations.'
        updatedAt: '2022-06-22T13:15:16.863Z'
      numEdits: 1
      reactions: []
    id: 62b23b442375526ae5120641
    type: comment
  author: sgugger
  content: 'Not on Colab free no, they don''t provide enough disk space to even download
    the weights.


    @stellaathena I''m surprised to learn the model was trained in fp16 (not bfloat16?)
    as we get crappy generations in FP16 but decent ones in bfloat16 in our tests.


    **Edit:** Looks like it was only a bug in the Transformers implementation. https://github.com/huggingface/transformers/pull/17811
    should fix the float16 generations.'
  created_at: 2022-06-21 20:42:28+00:00
  edited: true
  hidden: false
  id: 62b23b442375526ae5120641
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f41b208bbbdafebe521845140f006c68.svg
      fullname: kishore G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ghpkishore
      type: user
    createdAt: '2022-06-25T05:31:00.000Z'
    data:
      edited: false
      editors:
      - ghpkishore
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f41b208bbbdafebe521845140f006c68.svg
          fullname: kishore G
          isHf: false
          isPro: false
          name: ghpkishore
          type: user
        html: "<p>Thanks for letting me know and fixing the issue <span data-props=\"\
          {&quot;user&quot;:&quot;sgugger&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/sgugger\">@<span class=\"underline\">sgugger</span></a></span>\n\
          \n\t</span></span>. I will upgrade to Colab Pro and see if can run their.</p>\n"
        raw: Thanks for letting me know and fixing the issue @sgugger. I will upgrade
          to Colab Pro and see if can run their.
        updatedAt: '2022-06-25T05:31:00.564Z'
      numEdits: 0
      reactions: []
    id: 62b69d943797f21de0cc9b18
    type: comment
  author: ghpkishore
  content: Thanks for letting me know and fixing the issue @sgugger. I will upgrade
    to Colab Pro and see if can run their.
  created_at: 2022-06-25 04:31:00+00:00
  edited: false
  hidden: false
  id: 62b69d943797f21de0cc9b18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg?w=200&h=200&f=face
      fullname: Stella Biderman
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: stellaathena
      type: user
    createdAt: '2022-08-09T11:40:34.000Z'
    data:
      status: closed
    id: 62f247b20ac068a9d7035426
    type: status-change
  author: stellaathena
  created_at: 2022-08-09 10:40:34+00:00
  id: 62f247b20ac068a9d7035426
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/51af1070f064ed9e823c6b95125df1fd.svg
      fullname: Keith Hon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: keithhon
      type: user
    createdAt: '2023-02-10T13:02:36.000Z'
    data:
      edited: false
      editors:
      - keithhon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/51af1070f064ed9e823c6b95125df1fd.svg
          fullname: Keith Hon
          isHf: false
          isPro: false
          name: keithhon
          type: user
        html: '<p>I have tried running the model in Colab Pro but failed, as it only
          has 39GB~40GB gpu ram</p>

          '
        raw: I have tried running the model in Colab Pro but failed, as it only has
          39GB~40GB gpu ram
        updatedAt: '2023-02-10T13:02:36.627Z'
      numEdits: 0
      reactions: []
    id: 63e6406c70fa0ed02a502125
    type: comment
  author: keithhon
  content: I have tried running the model in Colab Pro but failed, as it only has
    39GB~40GB gpu ram
  created_at: 2023-02-10 13:02:36+00:00
  edited: false
  hidden: false
  id: 63e6406c70fa0ed02a502125
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff84ad12ab26260da65279ad9b84865a.svg
      fullname: HexDev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HexDev
      type: user
    createdAt: '2023-04-02T00:01:48.000Z'
    data:
      edited: false
      editors:
      - HexDev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff84ad12ab26260da65279ad9b84865a.svg
          fullname: HexDev
          isHf: false
          isPro: false
          name: HexDev
          type: user
        html: '<p>github codespaces</p>

          '
        raw: github codespaces
        updatedAt: '2023-04-02T00:01:48.512Z'
      numEdits: 0
      reactions: []
    id: 6428c5ecb443581755afd65e
    type: comment
  author: HexDev
  content: github codespaces
  created_at: 2023-04-01 23:01:48+00:00
  edited: false
  hidden: false
  id: 6428c5ecb443581755afd65e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: EleutherAI/gpt-neox-20b
repo_type: model
status: closed
target_branch: null
title: How to Load into colab
