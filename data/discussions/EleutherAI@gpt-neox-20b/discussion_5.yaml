!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sunyt32
conflicting_files: null
created_at: 2022-08-20 13:35:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661005591657-noauth.jpeg?w=200&h=200&f=face
      fullname: Yutao Sun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sunyt32
      type: user
    createdAt: '2022-08-20T14:35:34.000Z'
    data:
      edited: false
      editors:
      - sunyt32
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661005591657-noauth.jpeg?w=200&h=200&f=face
          fullname: Yutao Sun
          isHf: false
          isPro: false
          name: sunyt32
          type: user
        html: '<p>In gpt-j-6b, the code contains "parallelize" function which helps
          to parallelize the model conveniently. It seems difficult to put the entire
          model in one GPU. Or there is a more fancy way to do that?</p>

          '
        raw: In gpt-j-6b, the code contains "parallelize" function which helps to
          parallelize the model conveniently. It seems difficult to put the entire
          model in one GPU. Or there is a more fancy way to do that?
        updatedAt: '2022-08-20T14:35:34.343Z'
      numEdits: 0
      reactions: []
    id: 6300f136792a47211c336e17
    type: comment
  author: sunyt32
  content: In gpt-j-6b, the code contains "parallelize" function which helps to parallelize
    the model conveniently. It seems difficult to put the entire model in one GPU.
    Or there is a more fancy way to do that?
  created_at: 2022-08-20 13:35:34+00:00
  edited: false
  hidden: false
  id: 6300f136792a47211c336e17
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg?w=200&h=200&f=face
      fullname: Stella Biderman
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: stellaathena
      type: user
    createdAt: '2022-08-20T14:38:49.000Z'
    data:
      edited: false
      editors:
      - stellaathena
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg?w=200&h=200&f=face
          fullname: Stella Biderman
          isHf: false
          isPro: false
          name: stellaathena
          type: user
        html: '<p>This model does not fit on one GPU unless you have an 80 GB A100,
          an A40, or an A6000. It will just barely fit for running inference in 48
          GB and does not fit in 40 GB.</p>

          '
        raw: This model does not fit on one GPU unless you have an 80 GB A100, an
          A40, or an A6000. It will just barely fit for running inference in 48 GB
          and does not fit in 40 GB.
        updatedAt: '2022-08-20T14:38:49.344Z'
      numEdits: 0
      reactions: []
    id: 6300f1f9bb33e0882d2e4380
    type: comment
  author: stellaathena
  content: This model does not fit on one GPU unless you have an 80 GB A100, an A40,
    or an A6000. It will just barely fit for running inference in 48 GB and does not
    fit in 40 GB.
  created_at: 2022-08-20 13:38:49+00:00
  edited: false
  hidden: false
  id: 6300f1f9bb33e0882d2e4380
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg?w=200&h=200&f=face
      fullname: Stella Biderman
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: stellaathena
      type: user
    createdAt: '2022-08-27T12:21:38.000Z'
    data:
      status: closed
    id: 630a0c522ff113e0fb2a90df
    type: status-change
  author: stellaathena
  created_at: 2022-08-27 11:21:38+00:00
  id: 630a0c522ff113e0fb2a90df
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: EleutherAI/gpt-neox-20b
repo_type: model
status: closed
target_branch: null
title: How to implement model parallelism in gpt-neox?
