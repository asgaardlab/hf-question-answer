!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mldavid101
conflicting_files: null
created_at: 2022-12-27 12:29:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2d904e5aff2ebd35b247170f9ab271c.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mldavid101
      type: user
    createdAt: '2022-12-27T12:29:47.000Z'
    data:
      edited: false
      editors:
      - mldavid101
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2d904e5aff2ebd35b247170f9ab271c.svg
          fullname: David
          isHf: false
          isPro: false
          name: mldavid101
          type: user
        html: '<p>I''m trying to run the sample code on SageMaker but the predictor
          fails:</p>

          <p>from sagemaker.huggingface import HuggingFaceModel<br>import sagemaker</p>

          <p>role = sagemaker.get_execution_role()</p>

          <h1 id="hub-model-configuration-httpshuggingfacecomodels">Hub Model configuration.
          <a href="https://huggingface.co/models">https://huggingface.co/models</a></h1>

          <p>hub = {<br>    ''HF_MODEL_ID'':''EleutherAI/gpt-neox-20b'',<br>    ''HF_TASK'':''text-generation''<br>}</p>

          <h1 id="create-hugging-face-model-class">create Hugging Face Model Class</h1>

          <p>huggingface_model = HuggingFaceModel(<br>    transformers_version=''4.17.0'',<br>    pytorch_version=''1.10.2'',<br>    py_version=''py38'',<br>    env=hub,<br>    role=role,<br>)</p>

          <h1 id="deploy-model-to-sagemaker-inference">deploy model to SageMaker Inference</h1>

          <p>predictor = huggingface_model.deploy(<br>    initial_instance_count=1,
          # number of instances<br>    instance_type=''ml.m5.xlarge'' # ec2 instance
          type<br>)</p>

          <p>predictor.predict({<br>    ''inputs'': "Can you please let us know more
          details about your "<br>})</p>

          <p>=====================</p>

          <p>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint
          operation: Received client error (400) from primary with message "{<br>  "code":
          400,<br>  "type": "InternalServerException",<br>  "message": "\u0027gpt_neox\u0027"<br>}</p>

          <p>The corresponding logs:<br>W-EleutherAI__gpt-neox-20b-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle
          -     config = AutoConfig.from_pretrained(model, revision=revision, _from_pipeline=task,
          **model_kwargs)<br>W-EleutherAI__gpt-neox-20b-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle
          -   File "/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py",
          line 657, in from_pretrained<br>W-EleutherAI__gpt-neox-20b-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle
          -     config_class = CONFIG_MAPPING[config_dict["model_type"]]<br>W-EleutherAI__gpt-neox-20b-1-stdout
          com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File "/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py",
          line 372, in <strong>getitem</strong><br>W-EleutherAI__gpt-neox-20b-1-stdout
          com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise KeyError(key)<br>W-EleutherAI__gpt-neox-20b-1-stdout
          com.amazonaws.ml.mms.wlm.WorkerLifeCycle - KeyError: ''gpt_neox''</p>

          <p>Any idea how to solve it?</p>

          '
        raw: "I'm trying to run the sample code on SageMaker but the predictor fails:\r\
          \n\r\nfrom sagemaker.huggingface import HuggingFaceModel\r\nimport sagemaker\r\
          \n\r\nrole = sagemaker.get_execution_role()\r\n# Hub Model configuration.\
          \ https://huggingface.co/models\r\nhub = {\r\n\t'HF_MODEL_ID':'EleutherAI/gpt-neox-20b',\r\
          \n\t'HF_TASK':'text-generation'\r\n}\r\n\r\n# create Hugging Face Model\
          \ Class\r\nhuggingface_model = HuggingFaceModel(\r\n\ttransformers_version='4.17.0',\r\
          \n\tpytorch_version='1.10.2',\r\n\tpy_version='py38',\r\n\tenv=hub,\r\n\t\
          role=role, \r\n)\r\n\r\n# deploy model to SageMaker Inference\r\npredictor\
          \ = huggingface_model.deploy(\r\n\tinitial_instance_count=1, # number of\
          \ instances\r\n\tinstance_type='ml.m5.xlarge' # ec2 instance type\r\n)\r\
          \n\r\npredictor.predict({\r\n\t'inputs': \"Can you please let us know more\
          \ details about your \"\r\n})\r\n\r\n=====================\r\n\r\nModelError:\
          \ An error occurred (ModelError) when calling the InvokeEndpoint operation:\
          \ Received client error (400) from primary with message \"{\r\n  \"code\"\
          : 400,\r\n  \"type\": \"InternalServerException\",\r\n  \"message\": \"\\\
          u0027gpt_neox\\u0027\"\r\n}\r\n\r\nThe corresponding logs:\r\nW-EleutherAI__gpt-neox-20b-1-stdout\
          \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     config = AutoConfig.from_pretrained(model,\
          \ revision=revision, _from_pipeline=task, **model_kwargs)\r\nW-EleutherAI__gpt-neox-20b-1-stdout\
          \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 657, in from_pretrained\r\nW-EleutherAI__gpt-neox-20b-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
          \ -     config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\r\nW-EleutherAI__gpt-neox-20b-1-stdout\
          \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 372, in __getitem__\r\nW-EleutherAI__gpt-neox-20b-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
          \ -     raise KeyError(key)\r\nW-EleutherAI__gpt-neox-20b-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
          \ - KeyError: 'gpt_neox'\r\n\r\nAny idea how to solve it?\r\n\r\n"
        updatedAt: '2022-12-27T12:29:47.149Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - panrodrigo
        - schneiderfelipe
        - 2M1
    id: 63aae53ba4bdd629b7eaa80f
    type: comment
  author: mldavid101
  content: "I'm trying to run the sample code on SageMaker but the predictor fails:\r\
    \n\r\nfrom sagemaker.huggingface import HuggingFaceModel\r\nimport sagemaker\r\
    \n\r\nrole = sagemaker.get_execution_role()\r\n# Hub Model configuration. https://huggingface.co/models\r\
    \nhub = {\r\n\t'HF_MODEL_ID':'EleutherAI/gpt-neox-20b',\r\n\t'HF_TASK':'text-generation'\r\
    \n}\r\n\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\
    \n\ttransformers_version='4.17.0',\r\n\tpytorch_version='1.10.2',\r\n\tpy_version='py38',\r\
    \n\tenv=hub,\r\n\trole=role, \r\n)\r\n\r\n# deploy model to SageMaker Inference\r\
    \npredictor = huggingface_model.deploy(\r\n\tinitial_instance_count=1, # number\
    \ of instances\r\n\tinstance_type='ml.m5.xlarge' # ec2 instance type\r\n)\r\n\r\
    \npredictor.predict({\r\n\t'inputs': \"Can you please let us know more details\
    \ about your \"\r\n})\r\n\r\n=====================\r\n\r\nModelError: An error\
    \ occurred (ModelError) when calling the InvokeEndpoint operation: Received client\
    \ error (400) from primary with message \"{\r\n  \"code\": 400,\r\n  \"type\"\
    : \"InternalServerException\",\r\n  \"message\": \"\\u0027gpt_neox\\u0027\"\r\n\
    }\r\n\r\nThe corresponding logs:\r\nW-EleutherAI__gpt-neox-20b-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
    \ -     config = AutoConfig.from_pretrained(model, revision=revision, _from_pipeline=task,\
    \ **model_kwargs)\r\nW-EleutherAI__gpt-neox-20b-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
    \ -   File \"/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 657, in from_pretrained\r\nW-EleutherAI__gpt-neox-20b-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
    \ -     config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\r\nW-EleutherAI__gpt-neox-20b-1-stdout\
    \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 372, in __getitem__\r\nW-EleutherAI__gpt-neox-20b-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
    \ -     raise KeyError(key)\r\nW-EleutherAI__gpt-neox-20b-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
    \ - KeyError: 'gpt_neox'\r\n\r\nAny idea how to solve it?\r\n\r\n"
  created_at: 2022-12-27 12:29:47+00:00
  edited: false
  hidden: false
  id: 63aae53ba4bdd629b7eaa80f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/db3292bab93d885e51ddb511c72019e8.svg
      fullname: Rodrigo Meireles de Oliveira
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: panrodrigo
      type: user
    createdAt: '2022-12-29T17:08:21.000Z'
    data:
      edited: false
      editors:
      - panrodrigo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/db3292bab93d885e51ddb511c72019e8.svg
          fullname: Rodrigo Meireles de Oliveira
          isHf: false
          isPro: false
          name: panrodrigo
          type: user
        html: '<p>Same issue here.</p>

          '
        raw: Same issue here.
        updatedAt: '2022-12-29T17:08:21.761Z'
      numEdits: 0
      reactions: []
    id: 63adc985ba0c1d4a9a029848
    type: comment
  author: panrodrigo
  content: Same issue here.
  created_at: 2022-12-29 17:08:21+00:00
  edited: false
  hidden: false
  id: 63adc985ba0c1d4a9a029848
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c76b2516270222a4abcae35eb6799e75.svg
      fullname: Kesar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kesar
      type: user
    createdAt: '2022-12-29T19:28:34.000Z'
    data:
      edited: false
      editors:
      - kesar
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c76b2516270222a4abcae35eb6799e75.svg
          fullname: Kesar
          isHf: false
          isPro: false
          name: kesar
          type: user
        html: '<p>Same issue here. Any help?</p>

          '
        raw: Same issue here. Any help?
        updatedAt: '2022-12-29T19:28:34.192Z'
      numEdits: 0
      reactions: []
    id: 63adea62ba0c1d4a9a042e3c
    type: comment
  author: kesar
  content: Same issue here. Any help?
  created_at: 2022-12-29 19:28:34+00:00
  edited: false
  hidden: false
  id: 63adea62ba0c1d4a9a042e3c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654268089989-noauth.png?w=200&h=200&f=face
      fullname: Justlearn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Justlearn
      type: user
    createdAt: '2023-01-02T08:47:07.000Z'
    data:
      edited: false
      editors:
      - Justlearn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654268089989-noauth.png?w=200&h=200&f=face
          fullname: Justlearn
          isHf: false
          isPro: false
          name: Justlearn
          type: user
        html: '<p>Same issue on my sagemaker</p>

          '
        raw: Same issue on my sagemaker
        updatedAt: '2023-01-02T08:47:07.845Z'
      numEdits: 0
      reactions: []
    id: 63b29a0b54211fcd0624cab4
    type: comment
  author: Justlearn
  content: Same issue on my sagemaker
  created_at: 2023-01-02 08:47:07+00:00
  edited: false
  hidden: false
  id: 63b29a0b54211fcd0624cab4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/32edf6dfbc6dcd224bc2b688f6cc7a4b.svg
      fullname: Workshops
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ml-workshops
      type: user
    createdAt: '2023-01-11T22:33:46.000Z'
    data:
      edited: false
      editors:
      - ml-workshops
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/32edf6dfbc6dcd224bc2b688f6cc7a4b.svg
          fullname: Workshops
          isHf: false
          isPro: false
          name: ml-workshops
          type: user
        html: '<p>Hey folks I just saw this as well. Will triage.</p>

          '
        raw: Hey folks I just saw this as well. Will triage.
        updatedAt: '2023-01-11T22:33:46.957Z'
      numEdits: 0
      reactions: []
    id: 63bf394a82f7306d074fa7f4
    type: comment
  author: ml-workshops
  content: Hey folks I just saw this as well. Will triage.
  created_at: 2023-01-11 22:33:46+00:00
  edited: false
  hidden: false
  id: 63bf394a82f7306d074fa7f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/32edf6dfbc6dcd224bc2b688f6cc7a4b.svg
      fullname: Workshops
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ml-workshops
      type: user
    createdAt: '2023-01-12T23:06:31.000Z'
    data:
      edited: false
      editors:
      - ml-workshops
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/32edf6dfbc6dcd224bc2b688f6cc7a4b.svg
          fullname: Workshops
          isHf: false
          isPro: false
          name: ml-workshops
          type: user
        html: '<p>I am pretty sure the core issue here is transformers version. GPT-Neo
          was released in a later version of the transformers sdk. When you use the
          boilerplate provided above, which is 4.17, this doesn''t have neo. When
          I use a more recent version locally on my notebook, 4.25, I can use neo
          without an issue. </p>

          <p>To solve this on the hosting side of things, bring a requirements.txt
          file and point to a more recent version of the transformers sdk. </p>

          <p>You can build a custom image and point to it, or you can pass an entry
          point with a requirements.txt.<br><a rel="nofollow" href="https://sagemaker.readthedocs.io/en/stable/api/inference/model.html">https://sagemaker.readthedocs.io/en/stable/api/inference/model.html</a></p>

          '
        raw: "I am pretty sure the core issue here is transformers version. GPT-Neo\
          \ was released in a later version of the transformers sdk. When you use\
          \ the boilerplate provided above, which is 4.17, this doesn't have neo.\
          \ When I use a more recent version locally on my notebook, 4.25, I can use\
          \ neo without an issue. \n\nTo solve this on the hosting side of things,\
          \ bring a requirements.txt file and point to a more recent version of the\
          \ transformers sdk. \n\nYou can build a custom image and point to it, or\
          \ you can pass an entry point with a requirements.txt.\nhttps://sagemaker.readthedocs.io/en/stable/api/inference/model.html"
        updatedAt: '2023-01-12T23:06:31.911Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - panrodrigo
    id: 63c09277552b2e8b55289182
    type: comment
  author: ml-workshops
  content: "I am pretty sure the core issue here is transformers version. GPT-Neo\
    \ was released in a later version of the transformers sdk. When you use the boilerplate\
    \ provided above, which is 4.17, this doesn't have neo. When I use a more recent\
    \ version locally on my notebook, 4.25, I can use neo without an issue. \n\nTo\
    \ solve this on the hosting side of things, bring a requirements.txt file and\
    \ point to a more recent version of the transformers sdk. \n\nYou can build a\
    \ custom image and point to it, or you can pass an entry point with a requirements.txt.\n\
    https://sagemaker.readthedocs.io/en/stable/api/inference/model.html"
  created_at: 2023-01-12 23:06:31+00:00
  edited: false
  hidden: false
  id: 63c09277552b2e8b55289182
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg?w=200&h=200&f=face
      fullname: Stella Biderman
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: stellaathena
      type: user
    createdAt: '2023-02-07T06:45:47.000Z'
    data:
      status: closed
    id: 63e1f39b2667d47ebe8ee099
    type: status-change
  author: stellaathena
  created_at: 2023-02-07 06:45:47+00:00
  id: 63e1f39b2667d47ebe8ee099
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: EleutherAI/gpt-neox-20b
repo_type: model
status: closed
target_branch: null
title: 'SageMaker deploy failure - KeyError: ''gpt_neox'''
