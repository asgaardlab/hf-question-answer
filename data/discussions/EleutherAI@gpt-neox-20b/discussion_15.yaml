!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vmajor
conflicting_files: null
created_at: 2023-02-22 04:33:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-02-22T04:33:28.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: "<p>I originally posted this on the neox GitHub page was was politely\
          \ told to ask here.</p>\n<p>Describe the bug:</p>\n<p>Inference output does\
          \ not seem sensible, please see a sample output below:</p>\n<pre><code>You:\
          \ what are large language models?\nBot: You: what are large language models?\
          \ Small language models? Are one size fits all?\n&lt;dholbach&gt; https://wiki.ubuntu.com/UbuntuGlobalJam\
          \ explains the scope of it\n&lt;nigelb&gt; dholbach: oooh, right. /me is\
          \ a bit slow\n&lt;dholbach&gt; it's a place where anyone can do anything\
          \ they like (if some rules aren't broken)\n&lt;dholbach&gt; it's a place\
          \ where people can get together and work on stuff they care about\n&lt;dholbach&gt;\
          \ if you're doing app development, if you love playing with the phone or\
          \ the new tablet, or if you love doing advocacy, testing, whatever, you\
          \ can do it there\n&lt;dholbach&gt; https://wiki.ubuntu.com/UbuntuGlobalJam\
          \ has more info on how you can get involved\n&lt;dholbach&gt; https://spreadsheets.google.com/spreadsheet/ccc?key=0AkEUPNDy0YB1dDJpdE90QHVvUHZZRXBwRUhBQmdC&amp;hl=en_US#gid=1\
          \ has a list of some ideas\n&lt;dholbach&gt; a few ideas that folks have\
          \ came up with are:\n&lt;dholbach&gt;  - a quiz with 5 questions, 1 for\
          \ each day of UGJ - people can take a photo after completing the quiz and\
          \ email it to the team\nYou:\n</code></pre>\n<p>To Reproduce<br>Steps to\
          \ reproduce the behavior:<br>Run this code:</p>\n<pre><code># Import the\
          \ transformers library\nfrom transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n\
          \n# Load the tokenizer and model for gpt-neox-20b\nmodel = GPTNeoXForCausalLM.from_pretrained(\"\
          EleutherAI/gpt-neox-20b\")\ntokenizer = GPTNeoXTokenizerFast.from_pretrained(\"\
          EleutherAI/gpt-neox-20b\")\n\n# Start a loop to get user input and generate\
          \ chatbot output\nwhile True:\n    # Get user input\n    user_input = input(\"\
          You: \")\n    \n    # Break the loop if user types \"quit\"\n    if user_input.lower()\
          \ == \"quit\":\n        break\n    \n    # Add a prompt to the user input\n\
          \    prompt = \"You: \" + user_input\n    \n    # Encode the prompt using\
          \ the tokenizer\n    input_ids = tokenizer(prompt, return_tensors=\"pt\"\
          ).input_ids\n    \n    # Generate chatbot output using the model\n    bot_output_ids\
          \ = model.generate(\n        input_ids,\n        do_sample=True,\n     \
          \   temperature=0.9,\n        max_length=300,\n        pad_token_id=tokenizer.eos_token_id\n\
          \    )\n    \n    # Decode chatbot output ids as text\n    bot_output =\
          \ tokenizer.decode(bot_output_ids[0], skip_special_tokens=True)\n    \n\
          \    # Print chatbot output\n    print(\"Bot:\", bot_output)\n</code></pre>\n\
          <p>Then ask: what are large language models?</p>\n<p>Expected behavior:<br>A\
          \ sensible answer of some kind.</p>\n<p>Environment:</p>\n<pre><code>GPUs:\
          \ 0\nCPU only\n</code></pre>\n"
        raw: "I originally posted this on the neox GitHub page was was politely told\
          \ to ask here.\r\n\r\nDescribe the bug:\r\n\r\nInference output does not\
          \ seem sensible, please see a sample output below:\r\n\r\n```\r\nYou: what\
          \ are large language models?\r\nBot: You: what are large language models?\
          \ Small language models? Are one size fits all?\r\n<dholbach> https://wiki.ubuntu.com/UbuntuGlobalJam\
          \ explains the scope of it\r\n<nigelb> dholbach: oooh, right. /me is a bit\
          \ slow\r\n<dholbach> it's a place where anyone can do anything they like\
          \ (if some rules aren't broken)\r\n<dholbach> it's a place where people\
          \ can get together and work on stuff they care about\r\n<dholbach> if you're\
          \ doing app development, if you love playing with the phone or the new tablet,\
          \ or if you love doing advocacy, testing, whatever, you can do it there\r\
          \n<dholbach> https://wiki.ubuntu.com/UbuntuGlobalJam has more info on how\
          \ you can get involved\r\n<dholbach> https://spreadsheets.google.com/spreadsheet/ccc?key=0AkEUPNDy0YB1dDJpdE90QHVvUHZZRXBwRUhBQmdC&hl=en_US#gid=1\
          \ has a list of some ideas\r\n<dholbach> a few ideas that folks have came\
          \ up with are:\r\n<dholbach>  - a quiz with 5 questions, 1 for each day\
          \ of UGJ - people can take a photo after completing the quiz and email it\
          \ to the team\r\nYou:\r\n```\r\n\r\nTo Reproduce\r\nSteps to reproduce the\
          \ behavior:\r\nRun this code:\r\n```\r\n# Import the transformers library\r\
          \nfrom transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast\r\n\r\
          \n# Load the tokenizer and model for gpt-neox-20b\r\nmodel = GPTNeoXForCausalLM.from_pretrained(\"\
          EleutherAI/gpt-neox-20b\")\r\ntokenizer = GPTNeoXTokenizerFast.from_pretrained(\"\
          EleutherAI/gpt-neox-20b\")\r\n\r\n# Start a loop to get user input and generate\
          \ chatbot output\r\nwhile True:\r\n    # Get user input\r\n    user_input\
          \ = input(\"You: \")\r\n    \r\n    # Break the loop if user types \"quit\"\
          \r\n    if user_input.lower() == \"quit\":\r\n        break\r\n    \r\n\
          \    # Add a prompt to the user input\r\n    prompt = \"You: \" + user_input\r\
          \n    \r\n    # Encode the prompt using the tokenizer\r\n    input_ids =\
          \ tokenizer(prompt, return_tensors=\"pt\").input_ids\r\n    \r\n    # Generate\
          \ chatbot output using the model\r\n    bot_output_ids = model.generate(\r\
          \n        input_ids,\r\n        do_sample=True,\r\n        temperature=0.9,\r\
          \n        max_length=300,\r\n        pad_token_id=tokenizer.eos_token_id\r\
          \n    )\r\n    \r\n    # Decode chatbot output ids as text\r\n    bot_output\
          \ = tokenizer.decode(bot_output_ids[0], skip_special_tokens=True)\r\n  \
          \  \r\n    # Print chatbot output\r\n    print(\"Bot:\", bot_output)\r\n\
          ```\r\n\r\nThen ask: what are large language models?\r\n\r\nExpected behavior:\r\
          \nA sensible answer of some kind.\r\n\r\n\r\nEnvironment:\r\n\r\n    GPUs:\
          \ 0\r\n    CPU only\r\n"
        updatedAt: '2023-02-22T04:33:28.732Z'
      numEdits: 0
      reactions: []
    id: 63f59b1871a5d395c7246727
    type: comment
  author: vmajor
  content: "I originally posted this on the neox GitHub page was was politely told\
    \ to ask here.\r\n\r\nDescribe the bug:\r\n\r\nInference output does not seem\
    \ sensible, please see a sample output below:\r\n\r\n```\r\nYou: what are large\
    \ language models?\r\nBot: You: what are large language models? Small language\
    \ models? Are one size fits all?\r\n<dholbach> https://wiki.ubuntu.com/UbuntuGlobalJam\
    \ explains the scope of it\r\n<nigelb> dholbach: oooh, right. /me is a bit slow\r\
    \n<dholbach> it's a place where anyone can do anything they like (if some rules\
    \ aren't broken)\r\n<dholbach> it's a place where people can get together and\
    \ work on stuff they care about\r\n<dholbach> if you're doing app development,\
    \ if you love playing with the phone or the new tablet, or if you love doing advocacy,\
    \ testing, whatever, you can do it there\r\n<dholbach> https://wiki.ubuntu.com/UbuntuGlobalJam\
    \ has more info on how you can get involved\r\n<dholbach> https://spreadsheets.google.com/spreadsheet/ccc?key=0AkEUPNDy0YB1dDJpdE90QHVvUHZZRXBwRUhBQmdC&hl=en_US#gid=1\
    \ has a list of some ideas\r\n<dholbach> a few ideas that folks have came up with\
    \ are:\r\n<dholbach>  - a quiz with 5 questions, 1 for each day of UGJ - people\
    \ can take a photo after completing the quiz and email it to the team\r\nYou:\r\
    \n```\r\n\r\nTo Reproduce\r\nSteps to reproduce the behavior:\r\nRun this code:\r\
    \n```\r\n# Import the transformers library\r\nfrom transformers import GPTNeoXForCausalLM,\
    \ GPTNeoXTokenizerFast\r\n\r\n# Load the tokenizer and model for gpt-neox-20b\r\
    \nmodel = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\")\r\n\
    tokenizer = GPTNeoXTokenizerFast.from_pretrained(\"EleutherAI/gpt-neox-20b\")\r\
    \n\r\n# Start a loop to get user input and generate chatbot output\r\nwhile True:\r\
    \n    # Get user input\r\n    user_input = input(\"You: \")\r\n    \r\n    # Break\
    \ the loop if user types \"quit\"\r\n    if user_input.lower() == \"quit\":\r\n\
    \        break\r\n    \r\n    # Add a prompt to the user input\r\n    prompt =\
    \ \"You: \" + user_input\r\n    \r\n    # Encode the prompt using the tokenizer\r\
    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\r\n    \r\
    \n    # Generate chatbot output using the model\r\n    bot_output_ids = model.generate(\r\
    \n        input_ids,\r\n        do_sample=True,\r\n        temperature=0.9,\r\n\
    \        max_length=300,\r\n        pad_token_id=tokenizer.eos_token_id\r\n  \
    \  )\r\n    \r\n    # Decode chatbot output ids as text\r\n    bot_output = tokenizer.decode(bot_output_ids[0],\
    \ skip_special_tokens=True)\r\n    \r\n    # Print chatbot output\r\n    print(\"\
    Bot:\", bot_output)\r\n```\r\n\r\nThen ask: what are large language models?\r\n\
    \r\nExpected behavior:\r\nA sensible answer of some kind.\r\n\r\n\r\nEnvironment:\r\
    \n\r\n    GPUs: 0\r\n    CPU only\r\n"
  created_at: 2023-02-22 04:33:28+00:00
  edited: false
  hidden: false
  id: 63f59b1871a5d395c7246727
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-02-27T05:26:18.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>There is something wrong with the model. Here is it''s response
          to a stacked query:</p>

          <p>Query: "What is the highest mountain in the world? Tell me the height
          in meters."</p>

          <p>Response: "This is my code:<br>import java.io.<em>;<br>import java.util.</em>;</p>

          <p>public class Main {</p>

          <pre><code>public static void main(String[]"

          </code></pre>

          '
        raw: "There is something wrong with the model. Here is it's response to a\
          \ stacked query:\n\nQuery: \"What is the highest mountain in the world?\
          \ Tell me the height in meters.\"\n\nResponse: \"This is my code:\nimport\
          \ java.io.*;\nimport java.util.*;\n\npublic class Main {\n\n    public static\
          \ void main(String[]\""
        updatedAt: '2023-02-27T05:26:18.138Z'
      numEdits: 0
      reactions: []
    id: 63fc3efaf0818c9bfbf2af63
    type: comment
  author: vmajor
  content: "There is something wrong with the model. Here is it's response to a stacked\
    \ query:\n\nQuery: \"What is the highest mountain in the world? Tell me the height\
    \ in meters.\"\n\nResponse: \"This is my code:\nimport java.io.*;\nimport java.util.*;\n\
    \npublic class Main {\n\n    public static void main(String[]\""
  created_at: 2023-02-27 05:26:18+00:00
  edited: false
  hidden: false
  id: 63fc3efaf0818c9bfbf2af63
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: EleutherAI/gpt-neox-20b
repo_type: model
status: open
target_branch: null
title: Unusual behaviour with inference using transformers library
