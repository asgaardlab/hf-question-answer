!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kissacat
conflicting_files: null
created_at: 2023-10-28 07:17:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c6c987dab9accc326e07d9a2ec371ef.svg
      fullname: Liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kissacat
      type: user
    createdAt: '2023-10-28T08:17:32.000Z'
    data:
      edited: false
      editors:
      - Kissacat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45905643701553345
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c6c987dab9accc326e07d9a2ec371ef.svg
          fullname: Liu
          isHf: false
          isPro: false
          name: Kissacat
          type: user
        html: '<p>hello! thank you for your work.</p>

          <p>however, I cannot use your model since I got this following the error.
          Are you still maintaining it</p>

          <p>Traceback (most recent call last):<br>  File "/home/Hongwei/disk_hdd/Tiantian/LLM_Memory/MemoryBank-SiliconFriend-main/SiliconFriend-ChatGLM-BELLE/cli_demo.py",
          line 86, in <br>    tokenizer, model= load_belle_tokenizer_and_model(model_args.base_model,model_args.adapter_model)<br>  File
          "/home/Hongwei/disk_hdd/Tiantian/LLM_Memory/MemoryBank-SiliconFriend-main/SiliconFriend-ChatGLM-BELLE/../utils/model_utils.py",
          line 67, in load_belle_tokenizer_and_model<br>    tokenizer = LlamaTokenizer.from_pretrained(base_model)<br>  File
          "/home/Hongwei/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py",
          line 1825, in from_pretrained<br>    return cls._from_pretrained(<br>  File
          "/home/Hongwei/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py",
          line 1876, in _from_pretrained<br>    init_kwargs = json.load(tokenizer_config_handle)<br>  File
          "/home/Hongwei/anaconda3/lib/python3.9/json/<strong>init</strong>.py", line
          293, in load<br>    return loads(fp.read(),<br>  File "/home/Hongwei/anaconda3/lib/python3.9/codecs.py",
          line 322, in decode<br>    (result, consumed) = self._buffer_decode(data,
          self.errors, final)<br>UnicodeDecodeError: ''utf-8'' codec can''t decode
          byte 0xa6 in position 0: invalid start byte</p>

          '
        raw: "hello! thank you for your work.\r\n\r\nhowever, I cannot use your model\
          \ since I got this following the error. Are you still maintaining it\r\n\
          \r\nTraceback (most recent call last):\r\n  File \"/home/Hongwei/disk_hdd/Tiantian/LLM_Memory/MemoryBank-SiliconFriend-main/SiliconFriend-ChatGLM-BELLE/cli_demo.py\"\
          , line 86, in <module>\r\n    tokenizer, model= load_belle_tokenizer_and_model(model_args.base_model,model_args.adapter_model)\r\
          \n  File \"/home/Hongwei/disk_hdd/Tiantian/LLM_Memory/MemoryBank-SiliconFriend-main/SiliconFriend-ChatGLM-BELLE/../utils/model_utils.py\"\
          , line 67, in load_belle_tokenizer_and_model\r\n    tokenizer = LlamaTokenizer.from_pretrained(base_model)\r\
          \n  File \"/home/Hongwei/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1825, in from_pretrained\r\n    return cls._from_pretrained(\r\n\
          \  File \"/home/Hongwei/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1876, in _from_pretrained\r\n    init_kwargs = json.load(tokenizer_config_handle)\r\
          \n  File \"/home/Hongwei/anaconda3/lib/python3.9/json/__init__.py\", line\
          \ 293, in load\r\n    return loads(fp.read(),\r\n  File \"/home/Hongwei/anaconda3/lib/python3.9/codecs.py\"\
          , line 322, in decode\r\n    (result, consumed) = self._buffer_decode(data,\
          \ self.errors, final)\r\nUnicodeDecodeError: 'utf-8' codec can't decode\
          \ byte 0xa6 in position 0: invalid start byte\r\n"
        updatedAt: '2023-10-28T08:17:32.266Z'
      numEdits: 0
      reactions: []
    id: 653cc39c06a7bb4b1d5e30f5
    type: comment
  author: Kissacat
  content: "hello! thank you for your work.\r\n\r\nhowever, I cannot use your model\
    \ since I got this following the error. Are you still maintaining it\r\n\r\nTraceback\
    \ (most recent call last):\r\n  File \"/home/Hongwei/disk_hdd/Tiantian/LLM_Memory/MemoryBank-SiliconFriend-main/SiliconFriend-ChatGLM-BELLE/cli_demo.py\"\
    , line 86, in <module>\r\n    tokenizer, model= load_belle_tokenizer_and_model(model_args.base_model,model_args.adapter_model)\r\
    \n  File \"/home/Hongwei/disk_hdd/Tiantian/LLM_Memory/MemoryBank-SiliconFriend-main/SiliconFriend-ChatGLM-BELLE/../utils/model_utils.py\"\
    , line 67, in load_belle_tokenizer_and_model\r\n    tokenizer = LlamaTokenizer.from_pretrained(base_model)\r\
    \n  File \"/home/Hongwei/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1825, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File\
    \ \"/home/Hongwei/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1876, in _from_pretrained\r\n    init_kwargs = json.load(tokenizer_config_handle)\r\
    \n  File \"/home/Hongwei/anaconda3/lib/python3.9/json/__init__.py\", line 293,\
    \ in load\r\n    return loads(fp.read(),\r\n  File \"/home/Hongwei/anaconda3/lib/python3.9/codecs.py\"\
    , line 322, in decode\r\n    (result, consumed) = self._buffer_decode(data, self.errors,\
    \ final)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xa6 in position\
    \ 0: invalid start byte\r\n"
  created_at: 2023-10-28 07:17:32+00:00
  edited: false
  hidden: false
  id: 653cc39c06a7bb4b1d5e30f5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: BelleGroup/BELLE-LLaMA-7B-2M-enc
repo_type: model
status: open
target_branch: null
title: 'error: when i download your model'
