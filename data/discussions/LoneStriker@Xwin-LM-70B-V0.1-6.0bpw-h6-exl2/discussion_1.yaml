!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-10-23 16:07:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-10-23T17:07:13.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8836324214935303
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Hi,</p>

          <p>Would you elaborate more on these parameters?</p>

          <p>Thanks!</p>

          '
        raw: "Hi,\r\n\r\nWould you elaborate more on these parameters?\r\n\r\nThanks!"
        updatedAt: '2023-10-23T17:07:13.593Z'
      numEdits: 0
      reactions: []
    id: 6536a841c33220a872899639
    type: comment
  author: Yhyu13
  content: "Hi,\r\n\r\nWould you elaborate more on these parameters?\r\n\r\nThanks!"
  created_at: 2023-10-23 16:07:13+00:00
  edited: false
  hidden: false
  id: 6536a841c33220a872899639
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-10-23T19:24:24.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9503337144851685
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>6.0 bpw = bits per weight. It''s similar to llama.cpp''s q# measurement.
          In Exllama V2, it''s an average of the weights assigned to the entire model.  Layers
          can have a dynamic bits assigned to them to get this overall average.  h6
          refers to the bits allocated to the head layer itself.  The reason for providing
          multiple bpw quants is to allow users to fit different sized models on the
          GPUs they own or rent.  For a 6bpw model, you''ll need 3x 3090s or 3x 4090s
          to run.  For a 2.4 bpw model, you only need a single 3090 or 4090.</p>

          <p>From what the Exllama V2 author has stated, 4.0-6.0 bpw are very close
          in perplexity to the full fp16 model precision. So, using a quantized model
          may not lose much in terms of precision.</p>

          '
        raw: '6.0 bpw = bits per weight. It''s similar to llama.cpp''s q# measurement.
          In Exllama V2, it''s an average of the weights assigned to the entire model.  Layers
          can have a dynamic bits assigned to them to get this overall average.  h6
          refers to the bits allocated to the head layer itself.  The reason for providing
          multiple bpw quants is to allow users to fit different sized models on the
          GPUs they own or rent.  For a 6bpw model, you''ll need 3x 3090s or 3x 4090s
          to run.  For a 2.4 bpw model, you only need a single 3090 or 4090.


          From what the Exllama V2 author has stated, 4.0-6.0 bpw are very close in
          perplexity to the full fp16 model precision. So, using a quantized model
          may not lose much in terms of precision.'
        updatedAt: '2023-10-23T19:24:24.453Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - Yhyu13
    id: 6536c868fa9f02750d12d967
    type: comment
  author: LoneStriker
  content: '6.0 bpw = bits per weight. It''s similar to llama.cpp''s q# measurement.
    In Exllama V2, it''s an average of the weights assigned to the entire model.  Layers
    can have a dynamic bits assigned to them to get this overall average.  h6 refers
    to the bits allocated to the head layer itself.  The reason for providing multiple
    bpw quants is to allow users to fit different sized models on the GPUs they own
    or rent.  For a 6bpw model, you''ll need 3x 3090s or 3x 4090s to run.  For a 2.4
    bpw model, you only need a single 3090 or 4090.


    From what the Exllama V2 author has stated, 4.0-6.0 bpw are very close in perplexity
    to the full fp16 model precision. So, using a quantized model may not lose much
    in terms of precision.'
  created_at: 2023-10-23 18:24:24+00:00
  edited: false
  hidden: false
  id: 6536c868fa9f02750d12d967
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/Xwin-LM-70B-V0.1-6.0bpw-h6-exl2
repo_type: model
status: open
target_branch: null
title: What is 6.0bpw-h6-exl2?
