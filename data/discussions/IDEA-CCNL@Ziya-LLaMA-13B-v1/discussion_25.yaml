!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zjtzpanxb
conflicting_files: null
created_at: 2023-06-08 22:41:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5942d16d0e255e672583db3110d32922.svg
      fullname: pxb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zjtzpanxb
      type: user
    createdAt: '2023-06-08T23:41:02.000Z'
    data:
      edited: false
      editors:
      - zjtzpanxb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6291404962539673
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5942d16d0e255e672583db3110d32922.svg
          fullname: pxb
          isHf: false
          isPro: false
          name: zjtzpanxb
          type: user
        html: '<p>Step 2 error:The size of tensor a (32000) must match the size of
          tensor b (39424) at non-singleton dimension 0<br>How can I solve the problem,
          please?</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/648265eae4bbb1c2dd29c472/-WiQxZBwG4RtWuIyKr7s-.png"><img
          alt="WX20230609-072158@2x.png" src="https://cdn-uploads.huggingface.co/production/uploads/648265eae4bbb1c2dd29c472/-WiQxZBwG4RtWuIyKr7s-.png"></a></p>

          '
        raw: "Step 2 error:The size of tensor a (32000) must match the size of tensor\
          \ b (39424) at non-singleton dimension 0\r\nHow can I solve the problem,\
          \ please?\r\n\r\n![WX20230609-072158@2x.png](https://cdn-uploads.huggingface.co/production/uploads/648265eae4bbb1c2dd29c472/-WiQxZBwG4RtWuIyKr7s-.png)\r\
          \n"
        updatedAt: '2023-06-08T23:41:02.738Z'
      numEdits: 0
      reactions: []
    id: 6482670e954578a9d1d4aee1
    type: comment
  author: zjtzpanxb
  content: "Step 2 error:The size of tensor a (32000) must match the size of tensor\
    \ b (39424) at non-singleton dimension 0\r\nHow can I solve the problem, please?\r\
    \n\r\n![WX20230609-072158@2x.png](https://cdn-uploads.huggingface.co/production/uploads/648265eae4bbb1c2dd29c472/-WiQxZBwG4RtWuIyKr7s-.png)\r\
    \n"
  created_at: 2023-06-08 22:41:02+00:00
  edited: false
  hidden: false
  id: 6482670e954578a9d1d4aee1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668779559428-6215a6efbfcb3893344dd095.png?w=200&h=200&f=face
      fullname: Qi Yang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: qiyang
      type: user
    createdAt: '2023-06-09T03:24:43.000Z'
    data:
      edited: true
      editors:
      - qiyang
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.37019193172454834
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668779559428-6215a6efbfcb3893344dd095.png?w=200&h=200&f=face
          fullname: Qi Yang
          isHf: false
          isPro: false
          name: qiyang
          type: user
        html: "<p>We added 7000 chinese token to vocab.txt\uFF0Cwhich  leads to mismatch\
          \ error with the original apply_delta.py script from fastchat. Here is our\
          \ modified script : <a rel=\"nofollow\" href=\"https://github.com/IDEA-CCNL/Fengshenbang-LM/blob/main/fengshen/utils/apply_delta.py\"\
          >https://github.com/IDEA-CCNL/Fengshenbang-LM/blob/main/fengshen/utils/apply_delta.py</a></p>\n\
          <p>The main difference is (line 140), the weight of embedding and lm_head\
          \ doesn't need to convert. </p>\n<pre><code>        # param.data += delta.state_dict()[name]\n\
          \        if \"embed_tokens\" in name or \"lm_head.weight\" in name or \"\
          self_attn.rotary_emb.inv_freq\" in name:\n            continue\n       \
          \ else:\n            param.data += base.state_dict()[name]\n</code></pre>\n"
        raw: "We added 7000 chinese token to vocab.txt\uFF0Cwhich  leads to mismatch\
          \ error with the original apply_delta.py script from fastchat. Here is our\
          \ modified script : https://github.com/IDEA-CCNL/Fengshenbang-LM/blob/main/fengshen/utils/apply_delta.py\n\
          \nThe main difference is (line 140), the weight of embedding and lm_head\
          \ doesn't need to convert. \n```\n        # param.data += delta.state_dict()[name]\n\
          \        if \"embed_tokens\" in name or \"lm_head.weight\" in name or \"\
          self_attn.rotary_emb.inv_freq\" in name:\n            continue\n       \
          \ else:\n            param.data += base.state_dict()[name]\n```"
        updatedAt: '2023-06-09T10:35:26.957Z'
      numEdits: 4
      reactions: []
    id: 64829b7b17a7082a9944fc95
    type: comment
  author: qiyang
  content: "We added 7000 chinese token to vocab.txt\uFF0Cwhich  leads to mismatch\
    \ error with the original apply_delta.py script from fastchat. Here is our modified\
    \ script : https://github.com/IDEA-CCNL/Fengshenbang-LM/blob/main/fengshen/utils/apply_delta.py\n\
    \nThe main difference is (line 140), the weight of embedding and lm_head doesn't\
    \ need to convert. \n```\n        # param.data += delta.state_dict()[name]\n \
    \       if \"embed_tokens\" in name or \"lm_head.weight\" in name or \"self_attn.rotary_emb.inv_freq\"\
    \ in name:\n            continue\n        else:\n            param.data += base.state_dict()[name]\n\
    ```"
  created_at: 2023-06-09 02:24:43+00:00
  edited: true
  hidden: false
  id: 64829b7b17a7082a9944fc95
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: IDEA-CCNL/Ziya-LLaMA-13B-v1
repo_type: model
status: open
target_branch: null
title: Step 2 error:The size of tensor a (32000) must match the size of tensor b (39424)
  at non-singleton dimension 0
