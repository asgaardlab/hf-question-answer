!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Samvanity
conflicting_files: null
created_at: 2024-01-05 20:18:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41f136dd7933e710bea90eba83fb38d4.svg
      fullname: Sam Vance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Samvanity
      type: user
    createdAt: '2024-01-05T20:18:27.000Z'
    data:
      edited: false
      editors:
      - Samvanity
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7581762671470642
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41f136dd7933e710bea90eba83fb38d4.svg
          fullname: Sam Vance
          isHf: false
          isPro: false
          name: Samvanity
          type: user
        html: '<p>as title... Is the default context for this model 4096 like llama
          models?</p>

          '
        raw: as title... Is the default context for this model 4096 like llama models?
        updatedAt: '2024-01-05T20:18:27.624Z'
      numEdits: 0
      reactions: []
    id: 6598641317edd1f053089b68
    type: comment
  author: Samvanity
  content: as title... Is the default context for this model 4096 like llama models?
  created_at: 2024-01-05 20:18:27+00:00
  edited: false
  hidden: false
  id: 6598641317edd1f053089b68
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6376b76aa3b787faca41f8e8/-3_1Y8WSYnXQrA1jxy-q5.jpeg?w=200&h=200&f=face
      fullname: Kooten
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Kooten
      type: user
    createdAt: '2024-01-05T20:45:40.000Z'
    data:
      edited: false
      editors:
      - Kooten
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8998410105705261
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6376b76aa3b787faca41f8e8/-3_1Y8WSYnXQrA1jxy-q5.jpeg?w=200&h=200&f=face
          fullname: Kooten
          isHf: false
          isPro: false
          name: Kooten
          type: user
        html: "<p>You don\u2019t need to set compression </p>\n<p>Mistral and Mixtral\
          \ is trained at 8192 and use sliding window up to 32k,<br>I find 8k to work\
          \ best though. </p>\n"
        raw: "You don\u2019t need to set compression \n\nMistral and Mixtral is trained\
          \ at 8192 and use sliding window up to 32k,\nI find 8k to work best though. "
        updatedAt: '2024-01-05T20:45:40.612Z'
      numEdits: 0
      reactions: []
    id: 65986a7428676374f327b7af
    type: comment
  author: Kooten
  content: "You don\u2019t need to set compression \n\nMistral and Mixtral is trained\
    \ at 8192 and use sliding window up to 32k,\nI find 8k to work best though. "
  created_at: 2024-01-05 20:45:40+00:00
  edited: false
  hidden: false
  id: 65986a7428676374f327b7af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41f136dd7933e710bea90eba83fb38d4.svg
      fullname: Sam Vance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Samvanity
      type: user
    createdAt: '2024-01-05T21:45:04.000Z'
    data:
      edited: false
      editors:
      - Samvanity
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8857213258743286
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41f136dd7933e710bea90eba83fb38d4.svg
          fullname: Sam Vance
          isHf: false
          isPro: false
          name: Samvanity
          type: user
        html: '<p>Got it thanks! And use 8bit Cache if I run into OOM? or is using
          8bit cache a must? I just tested it without 8-bit cache and it seems to
          run fine...</p>

          '
        raw: Got it thanks! And use 8bit Cache if I run into OOM? or is using 8bit
          cache a must? I just tested it without 8-bit cache and it seems to run fine...
        updatedAt: '2024-01-05T21:45:04.243Z'
      numEdits: 0
      reactions: []
    id: 659878600c972f4c775cdde0
    type: comment
  author: Samvanity
  content: Got it thanks! And use 8bit Cache if I run into OOM? or is using 8bit cache
    a must? I just tested it without 8-bit cache and it seems to run fine...
  created_at: 2024-01-05 21:45:04+00:00
  edited: false
  hidden: false
  id: 659878600c972f4c775cdde0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6376b76aa3b787faca41f8e8/-3_1Y8WSYnXQrA1jxy-q5.jpeg?w=200&h=200&f=face
      fullname: Kooten
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Kooten
      type: user
    createdAt: '2024-01-06T02:29:28.000Z'
    data:
      edited: false
      editors:
      - Kooten
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9451234936714172
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6376b76aa3b787faca41f8e8/-3_1Y8WSYnXQrA1jxy-q5.jpeg?w=200&h=200&f=face
          fullname: Kooten
          isHf: false
          isPro: false
          name: Kooten
          type: user
        html: '<p>8bit cache saves 10-20% vram at no real cost, might be slightly
          slower, if you have no problem there is no need for it, but it might let
          you run a model at higher bpw at no real  cost.</p>

          '
        raw: 8bit cache saves 10-20% vram at no real cost, might be slightly slower,
          if you have no problem there is no need for it, but it might let you run
          a model at higher bpw at no real  cost.
        updatedAt: '2024-01-06T02:29:28.845Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Samvanity
    id: 6598bb08acaab7bec3f0b323
    type: comment
  author: Kooten
  content: 8bit cache saves 10-20% vram at no real cost, might be slightly slower,
    if you have no problem there is no need for it, but it might let you run a model
    at higher bpw at no real  cost.
  created_at: 2024-01-06 02:29:28+00:00
  edited: false
  hidden: false
  id: 6598bb08acaab7bec3f0b323
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41f136dd7933e710bea90eba83fb38d4.svg
      fullname: Sam Vance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Samvanity
      type: user
    createdAt: '2024-01-22T08:47:18.000Z'
    data:
      edited: false
      editors:
      - Samvanity
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8907533884048462
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41f136dd7933e710bea90eba83fb38d4.svg
          fullname: Sam Vance
          isHf: false
          isPro: false
          name: Samvanity
          type: user
        html: "<blockquote>\n<p>You don\u2019t need to set compression </p>\n<p>Mistral\
          \ and Mixtral is trained at 8192 and use sliding window up to 32k</p>\n\
          </blockquote>\n<p>Say if I want to increase the ctx to 10k, can I set compression\
          \ to 1.25 so it doesn't use sliding window between 8k-10k? or it will still\
          \ use sliding window as soon as it reaches beyond 8k regardless of settings?</p>\n\
          <p>Or that compress_pos_emb is useless unless I want to go beyond 32k?</p>\n\
          <p>I guess my question really is about whether the following 2 settings\
          \ are the same with mixtral / mistral:</p>\n<ol>\n<li>ctx = 10k, compress_pos_emb\
          \ = 1</li>\n<li>ctx = 10k, compress_pos_emb = 1.25 (this is assuming base\
          \ ctx = 8k, so 10k/8k = 1.25)</li>\n</ol>\n<p>Thanks!</p>\n"
        raw: "> You don\u2019t need to set compression \n> \n> Mistral and Mixtral\
          \ is trained at 8192 and use sliding window up to 32k\n\nSay if I want to\
          \ increase the ctx to 10k, can I set compression to 1.25 so it doesn't use\
          \ sliding window between 8k-10k? or it will still use sliding window as\
          \ soon as it reaches beyond 8k regardless of settings?\n\nOr that compress_pos_emb\
          \ is useless unless I want to go beyond 32k?\n\nI guess my question really\
          \ is about whether the following 2 settings are the same with mixtral /\
          \ mistral:\n\n1) ctx = 10k, compress_pos_emb = 1\n2) ctx = 10k, compress_pos_emb\
          \ = 1.25 (this is assuming base ctx = 8k, so 10k/8k = 1.25)\n\nThanks!"
        updatedAt: '2024-01-22T08:47:18.340Z'
      numEdits: 0
      reactions: []
    id: 65ae2b96043d53781a15c06f
    type: comment
  author: Samvanity
  content: "> You don\u2019t need to set compression \n> \n> Mistral and Mixtral is\
    \ trained at 8192 and use sliding window up to 32k\n\nSay if I want to increase\
    \ the ctx to 10k, can I set compression to 1.25 so it doesn't use sliding window\
    \ between 8k-10k? or it will still use sliding window as soon as it reaches beyond\
    \ 8k regardless of settings?\n\nOr that compress_pos_emb is useless unless I want\
    \ to go beyond 32k?\n\nI guess my question really is about whether the following\
    \ 2 settings are the same with mixtral / mistral:\n\n1) ctx = 10k, compress_pos_emb\
    \ = 1\n2) ctx = 10k, compress_pos_emb = 1.25 (this is assuming base ctx = 8k,\
    \ so 10k/8k = 1.25)\n\nThanks!"
  created_at: 2024-01-22 08:47:18+00:00
  edited: false
  hidden: false
  id: 65ae2b96043d53781a15c06f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6376b76aa3b787faca41f8e8/-3_1Y8WSYnXQrA1jxy-q5.jpeg?w=200&h=200&f=face
      fullname: Kooten
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Kooten
      type: user
    createdAt: '2024-01-22T10:03:29.000Z'
    data:
      edited: false
      editors:
      - Kooten
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9081001281738281
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6376b76aa3b787faca41f8e8/-3_1Y8WSYnXQrA1jxy-q5.jpeg?w=200&h=200&f=face
          fullname: Kooten
          isHf: false
          isPro: false
          name: Kooten
          type: user
        html: '<p>You should not have to set either compress or rope with Mistral
          and Mixtral based models at anything below 32k<br>Mixtral, this model,"<a
          rel="nofollow" href="https://mistral.ai/news/mixtral-of-experts/">gracefully
          handles a context of 32k tokens.</a>"</p>

          '
        raw: 'You should not have to set either compress or rope with Mistral and
          Mixtral based models at anything below 32k

          Mixtral, this model,"[gracefully handles a context of 32k tokens.](https://mistral.ai/news/mixtral-of-experts/)"'
        updatedAt: '2024-01-22T10:03:29.176Z'
      numEdits: 0
      reactions: []
    id: 65ae3d7150403d7f78f79a59
    type: comment
  author: Kooten
  content: 'You should not have to set either compress or rope with Mistral and Mixtral
    based models at anything below 32k

    Mixtral, this model,"[gracefully handles a context of 32k tokens.](https://mistral.ai/news/mixtral-of-experts/)"'
  created_at: 2024-01-22 10:03:29+00:00
  edited: false
  hidden: false
  id: 65ae3d7150403d7f78f79a59
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Kooten/FlatDolphinMaid-8x7B-4bpw-exl2
repo_type: model
status: open
target_branch: null
title: using context above 4096 - need to set compress_pos_emb above 1?
