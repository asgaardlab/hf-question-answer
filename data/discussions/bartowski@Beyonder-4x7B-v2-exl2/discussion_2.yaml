!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MarinaraSpaghetti
conflicting_files: null
created_at: 2024-01-15 08:28:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6550b16f7490049d6237f200/xyq2CPV73PBdZTd6uu9Uc.png?w=200&h=200&f=face
      fullname: Marianna
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MarinaraSpaghetti
      type: user
    createdAt: '2024-01-15T08:28:08.000Z'
    data:
      edited: false
      editors:
      - MarinaraSpaghetti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9040929675102234
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6550b16f7490049d6237f200/xyq2CPV73PBdZTd6uu9Uc.png?w=200&h=200&f=face
          fullname: Marianna
          isHf: false
          isPro: false
          name: MarinaraSpaghetti
          type: user
        html: "<p>This model holds so much potential, but sadly, it breaks for me.\
          \ I\u2019m using the 6.5 quant at full 32k context and it spews nonsense\
          \ (like repeating one letter, for example). This happens with basically\
          \ all MOE models aside from the basic Mixtral Instruct. Has anyone else\
          \ faced the same issue? I\u2019m using Oobabooga for loading, SillyTavern\
          \ as frontend and I only use Temperature and Min P to control the output.\
          \ Thank you in advance for help!</p>\n"
        raw: "This model holds so much potential, but sadly, it breaks for me. I\u2019\
          m using the 6.5 quant at full 32k context and it spews nonsense (like repeating\
          \ one letter, for example). This happens with basically all MOE models aside\
          \ from the basic Mixtral Instruct. Has anyone else faced the same issue?\
          \ I\u2019m using Oobabooga for loading, SillyTavern as frontend and I only\
          \ use Temperature and Min P to control the output. Thank you in advance\
          \ for help!"
        updatedAt: '2024-01-15T08:28:08.488Z'
      numEdits: 0
      reactions: []
    id: 65a4ec9841b6ef119ca01b68
    type: comment
  author: MarinaraSpaghetti
  content: "This model holds so much potential, but sadly, it breaks for me. I\u2019\
    m using the 6.5 quant at full 32k context and it spews nonsense (like repeating\
    \ one letter, for example). This happens with basically all MOE models aside from\
    \ the basic Mixtral Instruct. Has anyone else faced the same issue? I\u2019m using\
    \ Oobabooga for loading, SillyTavern as frontend and I only use Temperature and\
    \ Min P to control the output. Thank you in advance for help!"
  created_at: 2024-01-15 08:28:08+00:00
  edited: false
  hidden: false
  id: 65a4ec9841b6ef119ca01b68
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
      fullname: Bartowski
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bartowski
      type: user
    createdAt: '2024-01-15T16:38:53.000Z'
    data:
      edited: false
      editors:
      - bartowski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9834724068641663
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
          fullname: Bartowski
          isHf: false
          isPro: false
          name: bartowski
          type: user
        html: '<p>Saw I assume your post on Reddit, looks like even though they set
          the config.json value to 32k it might have been a stretch, the base models
          are all much lower so it''s odd they''d push it so far with the merge..
          shame it''s not working out for huge context! Wish I could provide further
          help but I think it''s just not meant for it</p>

          '
        raw: Saw I assume your post on Reddit, looks like even though they set the
          config.json value to 32k it might have been a stretch, the base models are
          all much lower so it's odd they'd push it so far with the merge.. shame
          it's not working out for huge context! Wish I could provide further help
          but I think it's just not meant for it
        updatedAt: '2024-01-15T16:38:53.252Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MarinaraSpaghetti
    id: 65a55f9d3e3a00a9c1646409
    type: comment
  author: bartowski
  content: Saw I assume your post on Reddit, looks like even though they set the config.json
    value to 32k it might have been a stretch, the base models are all much lower
    so it's odd they'd push it so far with the merge.. shame it's not working out
    for huge context! Wish I could provide further help but I think it's just not
    meant for it
  created_at: 2024-01-15 16:38:53+00:00
  edited: false
  hidden: false
  id: 65a55f9d3e3a00a9c1646409
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6550b16f7490049d6237f200/xyq2CPV73PBdZTd6uu9Uc.png?w=200&h=200&f=face
      fullname: Marianna
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MarinaraSpaghetti
      type: user
    createdAt: '2024-01-17T08:00:35.000Z'
    data:
      edited: false
      editors:
      - MarinaraSpaghetti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9770883917808533
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6550b16f7490049d6237f200/xyq2CPV73PBdZTd6uu9Uc.png?w=200&h=200&f=face
          fullname: Marianna
          isHf: false
          isPro: false
          name: MarinaraSpaghetti
          type: user
        html: '<p>Yes, that was my post! The authors of the model reached our to me
          on Reddit and let me know that the context for the model is 8k - they will
          add this to their model card info. Thank you for your reply regardless,
          super sweet of you!</p>

          '
        raw: Yes, that was my post! The authors of the model reached our to me on
          Reddit and let me know that the context for the model is 8k - they will
          add this to their model card info. Thank you for your reply regardless,
          super sweet of you!
        updatedAt: '2024-01-17T08:00:35.948Z'
      numEdits: 0
      reactions: []
    id: 65a789238d4eb84e0081b13b
    type: comment
  author: MarinaraSpaghetti
  content: Yes, that was my post! The authors of the model reached our to me on Reddit
    and let me know that the context for the model is 8k - they will add this to their
    model card info. Thank you for your reply regardless, super sweet of you!
  created_at: 2024-01-17 08:00:35+00:00
  edited: false
  hidden: false
  id: 65a789238d4eb84e0081b13b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: bartowski/Beyonder-4x7B-v2-exl2
repo_type: model
status: open
target_branch: null
title: Model breaks at full context
