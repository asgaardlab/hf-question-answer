!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ramzeez88
conflicting_files: null
created_at: 2024-01-13 08:21:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5186befff3a95ba33f4900421ad23b4.svg
      fullname: Adam Filipkowski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ramzeez88
      type: user
    createdAt: '2024-01-13T08:21:10.000Z'
    data:
      edited: false
      editors:
      - ramzeez88
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9418608546257019
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5186befff3a95ba33f4900421ad23b4.svg
          fullname: Adam Filipkowski
          isHf: false
          isPro: false
          name: ramzeez88
          type: user
        html: '<p>Hi, is it possible to determine when quantizing ,which bpw will
          be suitable for a given vram amount?  I was thinking if you could state
          the figures in the model card. For example 3.0 bpw fits in 8gb of vram,4.0
          bpw fits in 12gb vram,5.0 bpw fits in 16gb of vram and so on ?</p>

          '
        raw: Hi, is it possible to determine when quantizing ,which bpw will be suitable
          for a given vram amount?  I was thinking if you could state the figures
          in the model card. For example 3.0 bpw fits in 8gb of vram,4.0 bpw fits
          in 12gb vram,5.0 bpw fits in 16gb of vram and so on ?
        updatedAt: '2024-01-13T08:21:10.957Z'
      numEdits: 0
      reactions: []
    id: 65a247f68df9302d15440321
    type: comment
  author: ramzeez88
  content: Hi, is it possible to determine when quantizing ,which bpw will be suitable
    for a given vram amount?  I was thinking if you could state the figures in the
    model card. For example 3.0 bpw fits in 8gb of vram,4.0 bpw fits in 12gb vram,5.0
    bpw fits in 16gb of vram and so on ?
  created_at: 2024-01-13 08:21:10+00:00
  edited: false
  hidden: false
  id: 65a247f68df9302d15440321
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
      fullname: Bartowski
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bartowski
      type: user
    createdAt: '2024-01-13T20:48:43.000Z'
    data:
      edited: false
      editors:
      - bartowski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9921563863754272
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
          fullname: Bartowski
          isHf: false
          isPro: false
          name: bartowski
          type: user
        html: '<p>Yeah this is something I''ve wanted to do for awhile now and will
          get around to figuring out, it also does depend on context but would be
          nice to at least get a ballpark</p>

          <p>My guess for this model is that you''d be able to do about 4 bpw since
          the 8x7 I have to run at 3.5 on a 24gb card</p>

          '
        raw: 'Yeah this is something I''ve wanted to do for awhile now and will get
          around to figuring out, it also does depend on context but would be nice
          to at least get a ballpark


          My guess for this model is that you''d be able to do about 4 bpw since the
          8x7 I have to run at 3.5 on a 24gb card'
        updatedAt: '2024-01-13T20:48:43.724Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ramzeez88
    id: 65a2f72bdb5c00652e893230
    type: comment
  author: bartowski
  content: 'Yeah this is something I''ve wanted to do for awhile now and will get
    around to figuring out, it also does depend on context but would be nice to at
    least get a ballpark


    My guess for this model is that you''d be able to do about 4 bpw since the 8x7
    I have to run at 3.5 on a 24gb card'
  created_at: 2024-01-13 20:48:43+00:00
  edited: false
  hidden: false
  id: 65a2f72bdb5c00652e893230
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6550b16f7490049d6237f200/xyq2CPV73PBdZTd6uu9Uc.png?w=200&h=200&f=face
      fullname: Marianna
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MarinaraSpaghetti
      type: user
    createdAt: '2024-01-15T08:24:17.000Z'
    data:
      edited: false
      editors:
      - MarinaraSpaghetti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8839053511619568
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6550b16f7490049d6237f200/xyq2CPV73PBdZTd6uu9Uc.png?w=200&h=200&f=face
          fullname: Marianna
          isHf: false
          isPro: false
          name: MarinaraSpaghetti
          type: user
        html: '<p>I have 24GB VRAM and can easily fit in the 6.5 quants of 4x7B models
          with 49k context (so 1,5x the original context). Hope this helps!</p>

          '
        raw: I have 24GB VRAM and can easily fit in the 6.5 quants of 4x7B models
          with 49k context (so 1,5x the original context). Hope this helps!
        updatedAt: '2024-01-15T08:24:17.966Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ramzeez88
    id: 65a4ebb1c7e6b607c29c142c
    type: comment
  author: MarinaraSpaghetti
  content: I have 24GB VRAM and can easily fit in the 6.5 quants of 4x7B models with
    49k context (so 1,5x the original context). Hope this helps!
  created_at: 2024-01-15 08:24:17+00:00
  edited: false
  hidden: false
  id: 65a4ebb1c7e6b607c29c142c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: bartowski/Beyonder-4x7B-v2-exl2
repo_type: model
status: open
target_branch: null
title: Best quant for vram size
