!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DannyWu
conflicting_files: null
created_at: 2023-06-19 02:05:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ece72da1d8a25e772ee2d54bcbea4764.svg
      fullname: Danny
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DannyWu
      type: user
    createdAt: '2023-06-19T03:05:40.000Z'
    data:
      edited: false
      editors:
      - DannyWu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6572587490081787
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ece72da1d8a25e772ee2d54bcbea4764.svg
          fullname: Danny
          isHf: false
          isPro: false
          name: DannyWu
          type: user
        html: '<p>File "/home/ai/.local/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1671, in load_state_dict<br>raise RuntimeError(''Error(s) in loading
          state_dict for {}:\n\t{}''.format(<br>RuntimeError: Error(s) in loading
          state_dict for GLM130B:<br>size mismatch for transformer.layers.0.attention.query_key_value.weight:
          copying a param with shape torch.Size([4608 , 6144]) from checkpoint, the
          shape in current model is torch.Size([4608, 12288]).<br>size mismatch for
          transformer.layers.0.attention.dense.weight: copying a param with shape
          torch.Size([12288, 768]) f rom checkpoint, the shape in current model is
          torch.Size([12288, 1536]).<br>size mismatch for transformer.layers.0.mlp.dense_4h_to_h.weight:
          copying a param with shape torch.Size([12288, 2048] ) from checkpoint, the
          shape in current model is torch.Size([12288, 4096]).<br>size mismatch for
          transformer.layers.0.mlp.dense_h_to_4h.weight: copying a param with shape
          torch.Size([8192, 6144]) from checkpoint, the shape in current model is
          torch.Size([8192, 12288]).<br>size mismatch for transformer.layers.1.attention.query_key_value.weight:
          copying a param with shape torch.Size([4608 , 6144]) from checkpoint, the
          shape in current model is torch.Size([4608, 12288]).</p>

          '
        raw: "File \"/home/ai/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1671, in load_state_dict\r\nraise RuntimeError('Error(s) in loading\
          \ state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading\
          \ state_dict for GLM130B:\r\nsize mismatch for transformer.layers.0.attention.query_key_value.weight:\
          \ copying a param with shape torch.Size([4608 , 6144]) from checkpoint,\
          \ the shape in current model is torch.Size([4608, 12288]).\r\nsize mismatch\
          \ for transformer.layers.0.attention.dense.weight: copying a param with\
          \ shape torch.Size([12288, 768]) f rom checkpoint, the shape in current\
          \ model is torch.Size([12288, 1536]).\r\nsize mismatch for transformer.layers.0.mlp.dense_4h_to_h.weight:\
          \ copying a param with shape torch.Size([12288, 2048] ) from checkpoint,\
          \ the shape in current model is torch.Size([12288, 4096]).\r\nsize mismatch\
          \ for transformer.layers.0.mlp.dense_h_to_4h.weight: copying a param with\
          \ shape torch.Size([8192, 6144]) from checkpoint, the shape in current model\
          \ is torch.Size([8192, 12288]).\r\nsize mismatch for transformer.layers.1.attention.query_key_value.weight:\
          \ copying a param with shape torch.Size([4608 , 6144]) from checkpoint,\
          \ the shape in current model is torch.Size([4608, 12288]).\r\n\r\n"
        updatedAt: '2023-06-19T03:05:40.413Z'
      numEdits: 0
      reactions: []
    id: 648fc604f7f5d960da8834ab
    type: comment
  author: DannyWu
  content: "File \"/home/ai/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1671, in load_state_dict\r\nraise RuntimeError('Error(s) in loading state_dict\
    \ for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for\
    \ GLM130B:\r\nsize mismatch for transformer.layers.0.attention.query_key_value.weight:\
    \ copying a param with shape torch.Size([4608 , 6144]) from checkpoint, the shape\
    \ in current model is torch.Size([4608, 12288]).\r\nsize mismatch for transformer.layers.0.attention.dense.weight:\
    \ copying a param with shape torch.Size([12288, 768]) f rom checkpoint, the shape\
    \ in current model is torch.Size([12288, 1536]).\r\nsize mismatch for transformer.layers.0.mlp.dense_4h_to_h.weight:\
    \ copying a param with shape torch.Size([12288, 2048] ) from checkpoint, the shape\
    \ in current model is torch.Size([12288, 4096]).\r\nsize mismatch for transformer.layers.0.mlp.dense_h_to_4h.weight:\
    \ copying a param with shape torch.Size([8192, 6144]) from checkpoint, the shape\
    \ in current model is torch.Size([8192, 12288]).\r\nsize mismatch for transformer.layers.1.attention.query_key_value.weight:\
    \ copying a param with shape torch.Size([4608 , 6144]) from checkpoint, the shape\
    \ in current model is torch.Size([4608, 12288]).\r\n\r\n"
  created_at: 2023-06-19 02:05:40+00:00
  edited: false
  hidden: false
  id: 648fc604f7f5d960da8834ab
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: ianZzzzzz/GLM-130B-quant-int4-4gpu
repo_type: model
status: open
target_branch: null
title: "\u5144\u5F1F\u80FD\u8FD0\u884C\u8D77\u6765\u5417\uFF1F\u6211\u505A\u4E86\u4E2A\
  8\u5361int4\u7684\uFF0C\u7ED3\u679C\u62A5\u9519"
