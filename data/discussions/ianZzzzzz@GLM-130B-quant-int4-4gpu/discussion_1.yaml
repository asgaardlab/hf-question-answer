!!python/object:huggingface_hub.community.DiscussionWithDetails
author: suizhuluoqi
conflicting_files: null
created_at: 2023-04-21 08:14:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/931ba26983db51c983c89ccb2d1b8edd.svg
      fullname: '1'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: suizhuluoqi
      type: user
    createdAt: '2023-04-21T09:14:43.000Z'
    data:
      edited: false
      editors:
      - suizhuluoqi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/931ba26983db51c983c89ccb2d1b8edd.svg
          fullname: '1'
          isHf: false
          isPro: false
          name: suizhuluoqi
          type: user
        html: '<p>how to use the ckpt for reference</p>

          '
        raw: how to use the ckpt for reference
        updatedAt: '2023-04-21T09:14:43.947Z'
      numEdits: 0
      reactions: []
    id: 644254033bfe6b220253a108
    type: comment
  author: suizhuluoqi
  content: how to use the ckpt for reference
  created_at: 2023-04-21 08:14:43+00:00
  edited: false
  hidden: false
  id: 644254033bfe6b220253a108
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/16f17af08ef9a253c8d37808bd7da915.svg
      fullname: z
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ianZzzzzz
      type: user
    createdAt: '2023-04-28T08:38:25.000Z'
    data:
      edited: true
      editors:
      - ianZzzzzz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/16f17af08ef9a253c8d37808bd7da915.svg
          fullname: z
          isHf: false
          isPro: false
          name: ianZzzzzz
          type: user
        html: '<ol>

          <li><p>You should clone the original project "<a rel="nofollow" href="https://github.com/THUDM/GLM-130B.git&quot;">https://github.com/THUDM/GLM-130B.git"</a>
          then set all env done, and read  quantization document "<a rel="nofollow"
          href="https://github.com/THUDM/GLM-130B/blob/main/docs/quantization.md&quot;">https://github.com/THUDM/GLM-130B/blob/main/docs/quantization.md"</a>,
          my huggingface project only provide the result of quant process, in 4 GPU
          situation.</p>

          </li>

          <li><p>Change the model config file from configs/model_glm_130b.sh to configs/model_glm_130b_int4.sh
          in your scripts (e.g. scripts/generate.sh).</p>

          </li>

          <li><p>Download this project , set the path of the model to the folder of
          this project.</p>

          </li>

          <li><p>Add " --from-quantized-checkpoint " when your run scripts just as
          normal .</p>

          </li>

          </ol>

          '
        raw: '1. You should clone the original project "https://github.com/THUDM/GLM-130B.git"
          then set all env done, and read  quantization document "https://github.com/THUDM/GLM-130B/blob/main/docs/quantization.md",
          my huggingface project only provide the result of quant process, in 4 GPU
          situation.


          2. Change the model config file from configs/model_glm_130b.sh to configs/model_glm_130b_int4.sh
          in your scripts (e.g. scripts/generate.sh).

          3. Download this project , set the path of the model to the folder of this
          project.

          4. Add " --from-quantized-checkpoint " when your run scripts just as normal
          .'
        updatedAt: '2023-04-28T08:41:51.128Z'
      numEdits: 2
      reactions: []
    id: 644b8601d873cbc8cc298b13
    type: comment
  author: ianZzzzzz
  content: '1. You should clone the original project "https://github.com/THUDM/GLM-130B.git"
    then set all env done, and read  quantization document "https://github.com/THUDM/GLM-130B/blob/main/docs/quantization.md",
    my huggingface project only provide the result of quant process, in 4 GPU situation.


    2. Change the model config file from configs/model_glm_130b.sh to configs/model_glm_130b_int4.sh
    in your scripts (e.g. scripts/generate.sh).

    3. Download this project , set the path of the model to the folder of this project.

    4. Add " --from-quantized-checkpoint " when your run scripts just as normal .'
  created_at: 2023-04-28 07:38:25+00:00
  edited: true
  hidden: false
  id: 644b8601d873cbc8cc298b13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/085615818c810014fc5af74c8e6cbd94.svg
      fullname: Huang Chein
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Chein
      type: user
    createdAt: '2023-05-22T03:30:38.000Z'
    data:
      edited: false
      editors:
      - Chein
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/085615818c810014fc5af74c8e6cbd94.svg
          fullname: Huang Chein
          isHf: false
          isPro: false
          name: Chein
          type: user
        html: '<p>Is there a link to use it online?</p>

          '
        raw: Is there a link to use it online?
        updatedAt: '2023-05-22T03:30:38.233Z'
      numEdits: 0
      reactions: []
    id: 646ae1de13396ee0a56f1d0e
    type: comment
  author: Chein
  content: Is there a link to use it online?
  created_at: 2023-05-22 02:30:38+00:00
  edited: false
  hidden: false
  id: 646ae1de13396ee0a56f1d0e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ianZzzzzz/GLM-130B-quant-int4-4gpu
repo_type: model
status: open
target_branch: null
title: how to use it? can you share the process, thx
