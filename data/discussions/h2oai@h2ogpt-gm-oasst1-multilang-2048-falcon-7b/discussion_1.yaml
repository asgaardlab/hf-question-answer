!!python/object:huggingface_hub.community.DiscussionWithDetails
author: eastwind
conflicting_files: null
created_at: 2023-06-05 19:47:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-05T20:47:20.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3717101812362671
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: "<pre><code>File &lt;command-2149863258257856&gt;:12\n      2 from transformers\
          \ import pipeline\n      4 generate_text = pipeline(\n      5     model=\"\
          h2oai/h2ogpt-gm-oasst1-multilang-2048-falcon-7b\",\n      6     torch_dtype=torch.float16,\n\
          \   (...)\n      9     device_map = 'auto',\n     10 )\n---&gt; 12 res =\
          \ generate_text(\n     13     \"What is quantum tunnelling\",\n     14 \
          \    min_new_tokens=2,\n     15     max_new_tokens=1024,\n     16     do_sample=False,\n\
          \     17     num_beams=1,\n     18     temperature=float(0.3),\n     19\
          \     repetition_penalty=float(1.2),\n     20     renormalize_logits=True\n\
          \     21 )\n     22 print(res[0][\"generated_text\"])\n\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:201,\
          \ in TextGenerationPipeline.__call__(self, text_inputs, **kwargs)\n    160\
          \ def __call__(self, text_inputs, **kwargs):\n    161     \"\"\"\n    162\
          \     Complete the prompt(s) given as inputs.\n    163 \n   (...)\n    199\
          \           ids of the generated text.\n    200     \"\"\"\n--&gt; 201 \
          \    return super().__call__(text_inputs, **kwargs)\n\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/pipelines/base.py:1119,\
          \ in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\n\
          \   1111     return next(\n   1112         iter(\n   1113             self.get_iterator(\n\
          \   (...)\n   1116         )\n   1117     )\n   1118 else:\n-&gt; 1119 \
          \    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n\
          \nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/pipelines/base.py:1125,\
          \ in Pipeline.run_single(self, inputs, preprocess_params, forward_params,\
          \ postprocess_params)\n   1124 def run_single(self, inputs, preprocess_params,\
          \ forward_params, postprocess_params):\n-&gt; 1125     model_inputs = self.preprocess(inputs,\
          \ **preprocess_params)\n   1126     model_outputs = self.forward(model_inputs,\
          \ **forward_params)\n   1127     outputs = self.postprocess(model_outputs,\
          \ **postprocess_params)\n\nFile ~/.cache/huggingface/modules/transformers_modules/h2oai/h2ogpt-gm-oasst1-multilang-2048-falcon-7b/d5781acb7c1b1af6437449a28d8302acbcd08404/h2oai_pipeline.py:16,\
          \ in H2OTextGenerationPipeline.preprocess(self, prompt_text, prefix, handle_long_generation,\
          \ **generate_kwargs)\n     12 def preprocess(\n     13     self, prompt_text,\
          \ prefix=\"\", handle_long_generation=None, **generate_kwargs\n     14 ):\n\
          \     15     prompt_text = self.prompt.format(instruction=prompt_text)\n\
          ---&gt; 16     return super().preprocess(\n     17         prompt_text,\n\
          \     18         prefix=prefix,\n     19         handle_long_generation=handle_long_generation,\n\
          \     20         **generate_kwargs,\n     21     )\n\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:204,\
          \ in TextGenerationPipeline.preprocess(self, prompt_text, prefix, handle_long_generation,\
          \ **generate_kwargs)\n    203 def preprocess(self, prompt_text, prefix=\"\
          \", handle_long_generation=None, **generate_kwargs):\n--&gt; 204     inputs\
          \ = self.tokenizer(\n    205         prefix + prompt_text, padding=False,\
          \ add_special_tokens=False, return_tensors=self.framework\n    206     )\n\
          \    207     inputs[\"prompt_text\"] = prompt_text\n    209     if handle_long_generation\
          \ == \"hole\":\n\nTypeError: 'NoneType' object is not callable\n</code></pre>\n\
          <p>I thought it was sentencepiece but the error still exists </p>\n"
        raw: "```\r\nFile <command-2149863258257856>:12\r\n      2 from transformers\
          \ import pipeline\r\n      4 generate_text = pipeline(\r\n      5     model=\"\
          h2oai/h2ogpt-gm-oasst1-multilang-2048-falcon-7b\",\r\n      6     torch_dtype=torch.float16,\r\
          \n   (...)\r\n      9     device_map = 'auto',\r\n     10 )\r\n---> 12 res\
          \ = generate_text(\r\n     13     \"What is quantum tunnelling\",\r\n  \
          \   14     min_new_tokens=2,\r\n     15     max_new_tokens=1024,\r\n   \
          \  16     do_sample=False,\r\n     17     num_beams=1,\r\n     18     temperature=float(0.3),\r\
          \n     19     repetition_penalty=float(1.2),\r\n     20     renormalize_logits=True\r\
          \n     21 )\r\n     22 print(res[0][\"generated_text\"])\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:201,\
          \ in TextGenerationPipeline.__call__(self, text_inputs, **kwargs)\r\n  \
          \  160 def __call__(self, text_inputs, **kwargs):\r\n    161     \"\"\"\r\
          \n    162     Complete the prompt(s) given as inputs.\r\n    163 \r\n  \
          \ (...)\r\n    199           ids of the generated text.\r\n    200     \"\
          \"\"\r\n--> 201     return super().__call__(text_inputs, **kwargs)\r\n\r\
          \nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/pipelines/base.py:1119,\
          \ in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\r\
          \n   1111     return next(\r\n   1112         iter(\r\n   1113         \
          \    self.get_iterator(\r\n   (...)\r\n   1116         )\r\n   1117    \
          \ )\r\n   1118 else:\r\n-> 1119     return self.run_single(inputs, preprocess_params,\
          \ forward_params, postprocess_params)\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/pipelines/base.py:1125,\
          \ in Pipeline.run_single(self, inputs, preprocess_params, forward_params,\
          \ postprocess_params)\r\n   1124 def run_single(self, inputs, preprocess_params,\
          \ forward_params, postprocess_params):\r\n-> 1125     model_inputs = self.preprocess(inputs,\
          \ **preprocess_params)\r\n   1126     model_outputs = self.forward(model_inputs,\
          \ **forward_params)\r\n   1127     outputs = self.postprocess(model_outputs,\
          \ **postprocess_params)\r\n\r\nFile ~/.cache/huggingface/modules/transformers_modules/h2oai/h2ogpt-gm-oasst1-multilang-2048-falcon-7b/d5781acb7c1b1af6437449a28d8302acbcd08404/h2oai_pipeline.py:16,\
          \ in H2OTextGenerationPipeline.preprocess(self, prompt_text, prefix, handle_long_generation,\
          \ **generate_kwargs)\r\n     12 def preprocess(\r\n     13     self, prompt_text,\
          \ prefix=\"\", handle_long_generation=None, **generate_kwargs\r\n     14\
          \ ):\r\n     15     prompt_text = self.prompt.format(instruction=prompt_text)\r\
          \n---> 16     return super().preprocess(\r\n     17         prompt_text,\r\
          \n     18         prefix=prefix,\r\n     19         handle_long_generation=handle_long_generation,\r\
          \n     20         **generate_kwargs,\r\n     21     )\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:204,\
          \ in TextGenerationPipeline.preprocess(self, prompt_text, prefix, handle_long_generation,\
          \ **generate_kwargs)\r\n    203 def preprocess(self, prompt_text, prefix=\"\
          \", handle_long_generation=None, **generate_kwargs):\r\n--> 204     inputs\
          \ = self.tokenizer(\r\n    205         prefix + prompt_text, padding=False,\
          \ add_special_tokens=False, return_tensors=self.framework\r\n    206   \
          \  )\r\n    207     inputs[\"prompt_text\"] = prompt_text\r\n    209   \
          \  if handle_long_generation == \"hole\":\r\n\r\nTypeError: 'NoneType' object\
          \ is not callable\r\n```\r\n\r\nI thought it was sentencepiece but the error\
          \ still exists "
        updatedAt: '2023-06-05T20:47:20.279Z'
      numEdits: 0
      reactions: []
    id: 647e49d8becb41a27295749e
    type: comment
  author: eastwind
  content: "```\r\nFile <command-2149863258257856>:12\r\n      2 from transformers\
    \ import pipeline\r\n      4 generate_text = pipeline(\r\n      5     model=\"\
    h2oai/h2ogpt-gm-oasst1-multilang-2048-falcon-7b\",\r\n      6     torch_dtype=torch.float16,\r\
    \n   (...)\r\n      9     device_map = 'auto',\r\n     10 )\r\n---> 12 res = generate_text(\r\
    \n     13     \"What is quantum tunnelling\",\r\n     14     min_new_tokens=2,\r\
    \n     15     max_new_tokens=1024,\r\n     16     do_sample=False,\r\n     17\
    \     num_beams=1,\r\n     18     temperature=float(0.3),\r\n     19     repetition_penalty=float(1.2),\r\
    \n     20     renormalize_logits=True\r\n     21 )\r\n     22 print(res[0][\"\
    generated_text\"])\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:201,\
    \ in TextGenerationPipeline.__call__(self, text_inputs, **kwargs)\r\n    160 def\
    \ __call__(self, text_inputs, **kwargs):\r\n    161     \"\"\"\r\n    162    \
    \ Complete the prompt(s) given as inputs.\r\n    163 \r\n   (...)\r\n    199 \
    \          ids of the generated text.\r\n    200     \"\"\"\r\n--> 201     return\
    \ super().__call__(text_inputs, **kwargs)\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/pipelines/base.py:1119,\
    \ in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\r\
    \n   1111     return next(\r\n   1112         iter(\r\n   1113             self.get_iterator(\r\
    \n   (...)\r\n   1116         )\r\n   1117     )\r\n   1118 else:\r\n-> 1119 \
    \    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\r\
    \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/pipelines/base.py:1125,\
    \ in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\r\
    \n   1124 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):\r\
    \n-> 1125     model_inputs = self.preprocess(inputs, **preprocess_params)\r\n\
    \   1126     model_outputs = self.forward(model_inputs, **forward_params)\r\n\
    \   1127     outputs = self.postprocess(model_outputs, **postprocess_params)\r\
    \n\r\nFile ~/.cache/huggingface/modules/transformers_modules/h2oai/h2ogpt-gm-oasst1-multilang-2048-falcon-7b/d5781acb7c1b1af6437449a28d8302acbcd08404/h2oai_pipeline.py:16,\
    \ in H2OTextGenerationPipeline.preprocess(self, prompt_text, prefix, handle_long_generation,\
    \ **generate_kwargs)\r\n     12 def preprocess(\r\n     13     self, prompt_text,\
    \ prefix=\"\", handle_long_generation=None, **generate_kwargs\r\n     14 ):\r\n\
    \     15     prompt_text = self.prompt.format(instruction=prompt_text)\r\n--->\
    \ 16     return super().preprocess(\r\n     17         prompt_text,\r\n     18\
    \         prefix=prefix,\r\n     19         handle_long_generation=handle_long_generation,\r\
    \n     20         **generate_kwargs,\r\n     21     )\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:204,\
    \ in TextGenerationPipeline.preprocess(self, prompt_text, prefix, handle_long_generation,\
    \ **generate_kwargs)\r\n    203 def preprocess(self, prompt_text, prefix=\"\"\
    , handle_long_generation=None, **generate_kwargs):\r\n--> 204     inputs = self.tokenizer(\r\
    \n    205         prefix + prompt_text, padding=False, add_special_tokens=False,\
    \ return_tensors=self.framework\r\n    206     )\r\n    207     inputs[\"prompt_text\"\
    ] = prompt_text\r\n    209     if handle_long_generation == \"hole\":\r\n\r\n\
    TypeError: 'NoneType' object is not callable\r\n```\r\n\r\nI thought it was sentencepiece\
    \ but the error still exists "
  created_at: 2023-06-05 19:47:20+00:00
  edited: false
  hidden: false
  id: 647e49d8becb41a27295749e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676417502037-6316fc44c92fd6fee3161e9a.png?w=200&h=200&f=face
      fullname: Pascal Pfeiffer
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ilu000
      type: user
    createdAt: '2023-06-05T21:57:18.000Z'
    data:
      edited: false
      editors:
      - ilu000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8913998007774353
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676417502037-6316fc44c92fd6fee3161e9a.png?w=200&h=200&f=face
          fullname: Pascal Pfeiffer
          isHf: false
          isPro: false
          name: ilu000
          type: user
        html: "<p>Thank you for reporting <span data-props=\"{&quot;user&quot;:&quot;eastwind&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/eastwind\"\
          >@<span class=\"underline\">eastwind</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Please try the updated pipeline from the model card. Falcon needs\
          \ the tokenizer to be explicitly passed to the pipeline.</p>\n"
        raw: "Thank you for reporting @eastwind \n\nPlease try the updated pipeline\
          \ from the model card. Falcon needs the tokenizer to be explicitly passed\
          \ to the pipeline."
        updatedAt: '2023-06-05T21:57:18.709Z'
      numEdits: 0
      reactions: []
    id: 647e5a3e10b7a3b15712dcf1
    type: comment
  author: ilu000
  content: "Thank you for reporting @eastwind \n\nPlease try the updated pipeline\
    \ from the model card. Falcon needs the tokenizer to be explicitly passed to the\
    \ pipeline."
  created_at: 2023-06-05 20:57:18+00:00
  edited: false
  hidden: false
  id: 647e5a3e10b7a3b15712dcf1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-05T22:01:54.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8837133646011353
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p>Thanks so much. This fixed it</p>

          '
        raw: Thanks so much. This fixed it
        updatedAt: '2023-06-05T22:01:54.191Z'
      numEdits: 0
      reactions: []
    id: 647e5b5210b7a3b15712fc18
    type: comment
  author: eastwind
  content: Thanks so much. This fixed it
  created_at: 2023-06-05 21:01:54+00:00
  edited: false
  hidden: false
  id: 647e5b5210b7a3b15712fc18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-05T22:02:06.000Z'
    data:
      status: closed
    id: 647e5b5e32c471a7fa997fc3
    type: status-change
  author: eastwind
  created_at: 2023-06-05 21:02:06+00:00
  id: 647e5b5e32c471a7fa997fc3
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: h2oai/h2ogpt-gm-oasst1-multilang-2048-falcon-7b
repo_type: model
status: closed
target_branch: null
title: NoneType is not callable.
