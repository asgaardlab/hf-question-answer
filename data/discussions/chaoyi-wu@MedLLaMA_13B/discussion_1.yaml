!!python/object:huggingface_hub.community.DiscussionWithDetails
author: akshay1943
conflicting_files: null
created_at: 2023-09-07 06:45:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd0be5342e32ac3289c38b55031c8e35.svg
      fullname: Akshay Valsaraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akshay1943
      type: user
    createdAt: '2023-09-07T07:45:45.000Z'
    data:
      edited: false
      editors:
      - akshay1943
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9016183614730835
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd0be5342e32ac3289c38b55031c8e35.svg
          fullname: Akshay Valsaraj
          isHf: false
          isPro: false
          name: akshay1943
          type: user
        html: "<p>CODE:</p>\n<pre><code>import transformers\nimport torch\ntokenizer\
          \ = transformers.LlamaTokenizer.from_pretrained('chaoyi-wu/PMC_LLAMA_7B')\n\
          model = transformers.LlamaForCausalLM.from_pretrained('chaoyi-wu/PMC_LLAMA_7B',\n\
          \                                                        torch_dtype=torch.bfloat16,\n\
          \                                                        low_cpu_mem_usage=True,\n\
          \                                                        device_map=\"auto\"\
          ,)\nsentence = 'Hello, doctor , I have a huge pain in my chest , what could\
          \ it be , can it be cancer' \nbatch = tokenizer(\n            sentence,\n\
          \            return_tensors=\"pt\", \n            add_special_tokens=False\n\
          \        )\n\nwith torch.no_grad():\n    generated = model.generate(inputs\
          \ = batch[\"input_ids\"], max_length=200, do_sample=True, top_k=50)\n  \
          \  print('model predict: ',tokenizer.decode(generated[0]))\n</code></pre>\n\
          <p>it gave this<br>model predict: Hello, doctor , I have a huge pain in\
          \ my chest , what could it be , can it be cancer and will they need to take\
          \ a part of my heart or not ?\" [P02].</p>\n<p>Theme 2-Disease in relation\
          \ to gender</p>\n<p>In this theme, men are considered more rational and\
          \ knowledgeable about their diseases while the women are seen as irrational.\
          \ This is clearly seen from the answers:</p>\n<p>'They don't know what happens\
          \ to their heart, they just believe the patient's heart will go out of control\
          \ and they will be dead, so they cannot say anything about their heart disease\
          \ ' [P02].</p>\n<p>'I am a mother, how can I think about something serious\
          \ like my heart , I don't worry . ' [P05].</p>\n<p>It seems for women, the\
          \ disease is very severe since it kills many people in the family or close\
          \ relatives, so</p>\n<p>Along with that it gave some warnings<br>UserWarning:\
          \ You have modified the pretrained model configuration to control generation.\
          \ This is a deprecated strategy to control generation and will be removed\
          \ soon, in a future version. Please use a generation configuration file\
          \ (see <a href=\"https://huggingface.co/docs/transformers/main_classes/text_generation\"\
          >https://huggingface.co/docs/transformers/main_classes/text_generation</a>\
          \ )</p>\n"
        raw: "CODE:\r\n```\r\nimport transformers\r\nimport torch\r\ntokenizer = transformers.LlamaTokenizer.from_pretrained('chaoyi-wu/PMC_LLAMA_7B')\r\
          \nmodel = transformers.LlamaForCausalLM.from_pretrained('chaoyi-wu/PMC_LLAMA_7B',\r\
          \n                                                        torch_dtype=torch.bfloat16,\r\
          \n                                                        low_cpu_mem_usage=True,\r\
          \n                                                        device_map=\"\
          auto\",)\r\nsentence = 'Hello, doctor , I have a huge pain in my chest ,\
          \ what could it be , can it be cancer' \r\nbatch = tokenizer(\r\n      \
          \      sentence,\r\n            return_tensors=\"pt\", \r\n            add_special_tokens=False\r\
          \n        )\r\n\r\nwith torch.no_grad():\r\n    generated = model.generate(inputs\
          \ = batch[\"input_ids\"], max_length=200, do_sample=True, top_k=50)\r\n\
          \    print('model predict: ',tokenizer.decode(generated[0]))\r\n```\r\n\
          it gave this\r\nmodel predict: Hello, doctor , I have a huge pain in my\
          \ chest , what could it be , can it be cancer and will they need to take\
          \ a part of my heart or not ?\" [P02].\r\n\r\nTheme 2-Disease in relation\
          \ to gender\r\n\r\nIn this theme, men are considered more rational and knowledgeable\
          \ about their diseases while the women are seen as irrational. This is clearly\
          \ seen from the answers:\r\n\r\n'They don't know what happens to their heart,\
          \ they just believe the patient's heart will go out of control and they\
          \ will be dead, so they cannot say anything about their heart disease '\
          \ [P02].\r\n\r\n'I am a mother, how can I think about something serious\
          \ like my heart , I don't worry . ' [P05].\r\n\r\nIt seems for women, the\
          \ disease is very severe since it kills many people in the family or close\
          \ relatives, so\r\n\r\nAlong with that it gave some warnings\r\nUserWarning:\
          \ You have modified the pretrained model configuration to control generation.\
          \ This is a deprecated strategy to control generation and will be removed\
          \ soon, in a future version. Please use a generation configuration file\
          \ (see https://huggingface.co/docs/transformers/main_classes/text_generation\
          \ )"
        updatedAt: '2023-09-07T07:45:45.856Z'
      numEdits: 0
      reactions: []
    id: 64f97fa9d78a0037fec78629
    type: comment
  author: akshay1943
  content: "CODE:\r\n```\r\nimport transformers\r\nimport torch\r\ntokenizer = transformers.LlamaTokenizer.from_pretrained('chaoyi-wu/PMC_LLAMA_7B')\r\
    \nmodel = transformers.LlamaForCausalLM.from_pretrained('chaoyi-wu/PMC_LLAMA_7B',\r\
    \n                                                        torch_dtype=torch.bfloat16,\r\
    \n                                                        low_cpu_mem_usage=True,\r\
    \n                                                        device_map=\"auto\"\
    ,)\r\nsentence = 'Hello, doctor , I have a huge pain in my chest , what could\
    \ it be , can it be cancer' \r\nbatch = tokenizer(\r\n            sentence,\r\n\
    \            return_tensors=\"pt\", \r\n            add_special_tokens=False\r\
    \n        )\r\n\r\nwith torch.no_grad():\r\n    generated = model.generate(inputs\
    \ = batch[\"input_ids\"], max_length=200, do_sample=True, top_k=50)\r\n    print('model\
    \ predict: ',tokenizer.decode(generated[0]))\r\n```\r\nit gave this\r\nmodel predict:\
    \ Hello, doctor , I have a huge pain in my chest , what could it be , can it be\
    \ cancer and will they need to take a part of my heart or not ?\" [P02].\r\n\r\
    \nTheme 2-Disease in relation to gender\r\n\r\nIn this theme, men are considered\
    \ more rational and knowledgeable about their diseases while the women are seen\
    \ as irrational. This is clearly seen from the answers:\r\n\r\n'They don't know\
    \ what happens to their heart, they just believe the patient's heart will go out\
    \ of control and they will be dead, so they cannot say anything about their heart\
    \ disease ' [P02].\r\n\r\n'I am a mother, how can I think about something serious\
    \ like my heart , I don't worry . ' [P05].\r\n\r\nIt seems for women, the disease\
    \ is very severe since it kills many people in the family or close relatives,\
    \ so\r\n\r\nAlong with that it gave some warnings\r\nUserWarning: You have modified\
    \ the pretrained model configuration to control generation. This is a deprecated\
    \ strategy to control generation and will be removed soon, in a future version.\
    \ Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation\
    \ )"
  created_at: 2023-09-07 06:45:45+00:00
  edited: false
  hidden: false
  id: 64f97fa9d78a0037fec78629
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6436aaaa0c77d7c5036abdbd/C9A276yEeAPkKLEqUcjl_.jpeg?w=200&h=200&f=face
      fullname: Chaoyi Wu
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: chaoyi-wu
      type: user
    createdAt: '2023-09-07T08:50:02.000Z'
    data:
      edited: false
      editors:
      - chaoyi-wu
      hidden: false
      identifiedLanguage:
        language: id
        probability: 0.09468059986829758
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6436aaaa0c77d7c5036abdbd/C9A276yEeAPkKLEqUcjl_.jpeg?w=200&h=200&f=face
          fullname: Chaoyi Wu
          isHf: false
          isPro: false
          name: chaoyi-wu
          type: user
        html: '<p>Check github </p>

          '
        raw: 'Check github '
        updatedAt: '2023-09-07T08:50:02.712Z'
      numEdits: 0
      reactions: []
    id: 64f98eba740c911bf94c8dbe
    type: comment
  author: chaoyi-wu
  content: 'Check github '
  created_at: 2023-09-07 07:50:02+00:00
  edited: false
  hidden: false
  id: 64f98eba740c911bf94c8dbe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6436aaaa0c77d7c5036abdbd/C9A276yEeAPkKLEqUcjl_.jpeg?w=200&h=200&f=face
      fullname: Chaoyi Wu
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: chaoyi-wu
      type: user
    createdAt: '2023-10-18T08:47:47.000Z'
    data:
      status: closed
    id: 652f9bb357845998fceaf4c1
    type: status-change
  author: chaoyi-wu
  created_at: 2023-10-18 07:47:47+00:00
  id: 652f9bb357845998fceaf4c1
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: chaoyi-wu/MedLLaMA_13B
repo_type: model
status: closed
target_branch: null
title: Giving complete giberish results when i give the input to the model along with
  that it gives some warning with the tokenizer
