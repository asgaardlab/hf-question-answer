!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rocca
conflicting_files: null
created_at: 2023-11-21 08:33:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8213f0ffc9106d7663c4b5f29f4ec9e5.svg
      fullname: Joe Rocca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rocca
      type: user
    createdAt: '2023-11-21T08:33:25.000Z'
    data:
      edited: true
      editors:
      - rocca
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8557156324386597
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8213f0ffc9106d7663c4b5f29f4ec9e5.svg
          fullname: Joe Rocca
          isHf: false
          isPro: false
          name: rocca
          type: user
        html: "<p>If I use <code>ghcr.io/huggingface/text-generation-inference:1.1.1</code>\
          \ with these option: <code>--quantize gptq --model-id TheBloke/LLaMA2-13B-Tiefighter-GPTQ\
          \ --revision <a href=\"/TheBloke/LLaMA2-13B-Tiefighter-GPTQ/commit/2a797d26cdafd37714f033503fa085bb58fcd4bc\"\
          >2a797d26cdafd37714f033503fa085bb58fcd4bc</a> --max-total-tokens 4096 --port\
          \ 3000</code> then it complains that a fast tokenizer isn't found, which\
          \ means that the server isn't able to dynamically compute <code>max_new_tokens</code>\
          \ (based on length of input text + <code>--max-total-tokens</code>) and\
          \ it instead needs to be computed manually and sent along with the API request.</p>\n\
          <p>If I understand correctly, to fix this it would just require uploading\
          \ the correct <code>tokenizer.json</code>file to this repo? Would be as\
          \ easy as copying it from another llama2 repo I'm guessing? If so, would\
          \ it be possible to add that to this and future repos? \U0001F64F (or is\
          \ this perhaps a bug with TGI? If so, I can file an issue)</p>\n"
        raw: "If I use `ghcr.io/huggingface/text-generation-inference:1.1.1` with\
          \ these option: `--quantize gptq --model-id TheBloke/LLaMA2-13B-Tiefighter-GPTQ\
          \ --revision 2a797d26cdafd37714f033503fa085bb58fcd4bc --max-total-tokens\
          \ 4096 --port 3000` then it complains that a fast tokenizer isn't found,\
          \ which means that the server isn't able to dynamically compute `max_new_tokens`\
          \ (based on length of input text + `--max-total-tokens`) and it instead\
          \ needs to be computed manually and sent along with the API request.\n\n\
          If I understand correctly, to fix this it would just require uploading the\
          \ correct `tokenizer.json`file to this repo? Would be as easy as copying\
          \ it from another llama2 repo I'm guessing? If so, would it be possible\
          \ to add that to this and future repos? \U0001F64F (or is this perhaps a\
          \ bug with TGI? If so, I can file an issue)"
        updatedAt: '2023-11-21T08:46:05.645Z'
      numEdits: 2
      reactions: []
    id: 655c6b557558de19727d488c
    type: comment
  author: rocca
  content: "If I use `ghcr.io/huggingface/text-generation-inference:1.1.1` with these\
    \ option: `--quantize gptq --model-id TheBloke/LLaMA2-13B-Tiefighter-GPTQ --revision\
    \ 2a797d26cdafd37714f033503fa085bb58fcd4bc --max-total-tokens 4096 --port 3000`\
    \ then it complains that a fast tokenizer isn't found, which means that the server\
    \ isn't able to dynamically compute `max_new_tokens` (based on length of input\
    \ text + `--max-total-tokens`) and it instead needs to be computed manually and\
    \ sent along with the API request.\n\nIf I understand correctly, to fix this it\
    \ would just require uploading the correct `tokenizer.json`file to this repo?\
    \ Would be as easy as copying it from another llama2 repo I'm guessing? If so,\
    \ would it be possible to add that to this and future repos? \U0001F64F (or is\
    \ this perhaps a bug with TGI? If so, I can file an issue)"
  created_at: 2023-11-21 08:33:25+00:00
  edited: true
  hidden: false
  id: 655c6b557558de19727d488c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/LLaMA2-13B-Tiefighter-GPTQ
repo_type: model
status: open
target_branch: null
title: TGI "Fast Tokenizer" support?
