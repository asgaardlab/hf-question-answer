!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MooCow27
conflicting_files: null
created_at: 2023-04-01 04:39:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4cd714e8c86279e26b10eb9cf26f4bc7.svg
      fullname: Matt H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MooCow27
      type: user
    createdAt: '2023-04-01T05:39:34.000Z'
    data:
      edited: false
      editors:
      - MooCow27
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4cd714e8c86279e26b10eb9cf26f4bc7.svg
          fullname: Matt H
          isHf: false
          isPro: false
          name: MooCow27
          type: user
        html: '<p>I was trying to load up the model to integrate it with llama index,
          but does running this really use 26gb of vram? Is there a way to reduce
          this down? </p>

          <p>Thanks! </p>

          '
        raw: "I was trying to load up the model to integrate it with llama index,\
          \ but does running this really use 26gb of vram? Is there a way to reduce\
          \ this down? \r\n\r\nThanks! "
        updatedAt: '2023-04-01T05:39:34.940Z'
      numEdits: 0
      reactions: []
    id: 6427c396682e4152bbdfff3a
    type: comment
  author: MooCow27
  content: "I was trying to load up the model to integrate it with llama index, but\
    \ does running this really use 26gb of vram? Is there a way to reduce this down?\
    \ \r\n\r\nThanks! "
  created_at: 2023-04-01 04:39:34+00:00
  edited: false
  hidden: false
  id: 6427c396682e4152bbdfff3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
      fullname: Jonathan Yankovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tensiondriven
      type: user
    createdAt: '2023-04-04T00:58:47.000Z'
    data:
      edited: false
      editors:
      - tensiondriven
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
          fullname: Jonathan Yankovich
          isHf: false
          isPro: false
          name: tensiondriven
          type: user
        html: '<p>The model would likely need to be quantized to use less memory.  You
          could probably load it as-is with the --load-in-8bit flag when using text-generation-webui.  (The
          8bit feature is provided by the bitsandbytes python dep.)</p>

          <p>To take it down farther, it could be quantized to 4-bits.  There''s another
          discussion thread here that talks about that.</p>

          <p>For 8bit, you can run the model in its current form.  For 4-bit, you''ll
          have to run a quantization step yourself, which takes a while, but is totally
          doable on a local machine.</p>

          '
        raw: 'The model would likely need to be quantized to use less memory.  You
          could probably load it as-is with the --load-in-8bit flag when using text-generation-webui.  (The
          8bit feature is provided by the bitsandbytes python dep.)


          To take it down farther, it could be quantized to 4-bits.  There''s another
          discussion thread here that talks about that.


          For 8bit, you can run the model in its current form.  For 4-bit, you''ll
          have to run a quantization step yourself, which takes a while, but is totally
          doable on a local machine.'
        updatedAt: '2023-04-04T00:58:47.385Z'
      numEdits: 0
      reactions: []
    id: 642b7647609bb798dfb59535
    type: comment
  author: tensiondriven
  content: 'The model would likely need to be quantized to use less memory.  You could
    probably load it as-is with the --load-in-8bit flag when using text-generation-webui.  (The
    8bit feature is provided by the bitsandbytes python dep.)


    To take it down farther, it could be quantized to 4-bits.  There''s another discussion
    thread here that talks about that.


    For 8bit, you can run the model in its current form.  For 4-bit, you''ll have
    to run a quantization step yourself, which takes a while, but is totally doable
    on a local machine.'
  created_at: 2023-04-03 23:58:47+00:00
  edited: false
  hidden: false
  id: 642b7647609bb798dfb59535
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-04-23T00:47:30.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>i bet this model released as FP32 instead of FP16</p>

          '
        raw: i bet this model released as FP32 instead of FP16
        updatedAt: '2023-04-23T00:47:30.044Z'
      numEdits: 0
      reactions: []
    id: 64448022c63001ae635961d7
    type: comment
  author: autobots
  content: i bet this model released as FP32 instead of FP16
  created_at: 2023-04-22 23:47:30+00:00
  edited: false
  hidden: false
  id: 64448022c63001ae635961d7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: 8bit-coder/alpaca-7b-nativeEnhanced
repo_type: model
status: open
target_branch: null
title: 'Loading the model 26gb? '
