!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tensiondriven
conflicting_files: null
created_at: 2023-04-04 00:02:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
      fullname: Jonathan Yankovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tensiondriven
      type: user
    createdAt: '2023-04-04T01:02:57.000Z'
    data:
      edited: false
      editors:
      - tensiondriven
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
          fullname: Jonathan Yankovich
          isHf: false
          isPro: false
          name: tensiondriven
          type: user
        html: '<p>May I be the first to be "that guy" and request a 13b version and
          a 30b version?</p>

          <p>Very excited to work with the conversation aspects of this model.  Thanks
          for putting it together, it is so very appriciated.</p>

          <p>I agree with <a href="https://huggingface.co/8bit-coder/alpaca-7b-nativeEnhanced/discussions/5">this
          thread</a> where the author suggests that quantizing to 4 bits using GPTQ
          using all the latest GPTQ features would be the way to go.</p>

          '
        raw: "May I be the first to be \"that guy\" and request a 13b version and\
          \ a 30b version?\r\n\r\nVery excited to work with the conversation aspects\
          \ of this model.  Thanks for putting it together, it is so very appriciated.\r\
          \n\r\nI agree with [this thread](https://huggingface.co/8bit-coder/alpaca-7b-nativeEnhanced/discussions/5)\
          \ where the author suggests that quantizing to 4 bits using GPTQ using all\
          \ the latest GPTQ features would be the way to go."
        updatedAt: '2023-04-04T01:02:57.139Z'
      numEdits: 0
      reactions: []
    id: 642b77414f379cdc3de9928f
    type: comment
  author: tensiondriven
  content: "May I be the first to be \"that guy\" and request a 13b version and a\
    \ 30b version?\r\n\r\nVery excited to work with the conversation aspects of this\
    \ model.  Thanks for putting it together, it is so very appriciated.\r\n\r\nI\
    \ agree with [this thread](https://huggingface.co/8bit-coder/alpaca-7b-nativeEnhanced/discussions/5)\
    \ where the author suggests that quantizing to 4 bits using GPTQ using all the\
    \ latest GPTQ features would be the way to go."
  created_at: 2023-04-04 00:02:57+00:00
  edited: false
  hidden: false
  id: 642b77414f379cdc3de9928f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f10773eba515524b2c9302a12b26981.svg
      fullname: Patrog Azadfekr
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: 8bit-coder
      type: user
    createdAt: '2023-04-04T14:59:55.000Z'
    data:
      edited: false
      editors:
      - 8bit-coder
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f10773eba515524b2c9302a12b26981.svg
          fullname: Patrog Azadfekr
          isHf: false
          isPro: false
          name: 8bit-coder
          type: user
        html: '<p>We''re actually working on a 13b and 30b version. It''s taking longer
          than expected due to the hardware limitations of using only 8 A100 80GB
          GPUs.</p>

          '
        raw: We're actually working on a 13b and 30b version. It's taking longer than
          expected due to the hardware limitations of using only 8 A100 80GB GPUs.
        updatedAt: '2023-04-04T14:59:55.272Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F917"
        users:
        - tensiondriven
        - underlines
    id: 642c3b6b722bc53c7782b3bb
    type: comment
  author: 8bit-coder
  content: We're actually working on a 13b and 30b version. It's taking longer than
    expected due to the hardware limitations of using only 8 A100 80GB GPUs.
  created_at: 2023-04-04 13:59:55+00:00
  edited: false
  hidden: false
  id: 642c3b6b722bc53c7782b3bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
      fullname: Jonathan Yankovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tensiondriven
      type: user
    createdAt: '2023-04-22T13:20:25.000Z'
    data:
      edited: false
      editors:
      - tensiondriven
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
          fullname: Jonathan Yankovich
          isHf: false
          isPro: false
          name: tensiondriven
          type: user
        html: '<p>Still so stoked about this!  I think alpaca-*-nativeEnhanced may
          be a good base model to target for loras.  I''m also curious if anything
          has come along in the past 20 days that is vastly and obviously superior.  If
          not, I''m still very stoked for a 13b!</p>

          '
        raw: Still so stoked about this!  I think alpaca-*-nativeEnhanced may be a
          good base model to target for loras.  I'm also curious if anything has come
          along in the past 20 days that is vastly and obviously superior.  If not,
          I'm still very stoked for a 13b!
        updatedAt: '2023-04-22T13:20:25.982Z'
      numEdits: 0
      reactions: []
    id: 6443df198f795c936dfec620
    type: comment
  author: tensiondriven
  content: Still so stoked about this!  I think alpaca-*-nativeEnhanced may be a good
    base model to target for loras.  I'm also curious if anything has come along in
    the past 20 days that is vastly and obviously superior.  If not, I'm still very
    stoked for a 13b!
  created_at: 2023-04-22 12:20:25+00:00
  edited: false
  hidden: false
  id: 6443df198f795c936dfec620
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: 8bit-coder/alpaca-7b-nativeEnhanced
repo_type: model
status: open
target_branch: null
title: 13b, 30b?
