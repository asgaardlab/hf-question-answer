!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheYuriLover
conflicting_files: null
created_at: 2023-03-30 02:47:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-03-30T03:47:42.000Z'
    data:
      edited: true
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>Hello,</p>

          <p>I noticed that you have experimented with quantization for your model,
          but the results were not as good as expected:</p>

          <p>"When quantized to 4 bits, the model demonstrates unusual behavior, possibly
          due to its complexity. We suggest using a minimum quantization of 8 bits,
          although this has not been tested."</p>

          <p>I recommend trying the new GPTQ quantization method, which includes the
          combined options "act-order"  +  "true-sequential"  +  "groupsize 128".<br>This
          method brings the quantized model''s performance much closer to that of
          the 16-bit model.</p>

          <p>Check out the following link for more information: <a rel="nofollow"
          href="https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/triton">https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/triton</a></p>

          <p>If you decide to implement this quantization method, please consider
          uploading the resulting model to your repository. This way, users with both
          high-performance (16-bit) and lower-end computers (4-bit) can enjoy your
          models.</p>

          <p>Best regards,</p>

          '
        raw: "Hello,\n\nI noticed that you have experimented with quantization for\
          \ your model, but the results were not as good as expected:\n\n\"When quantized\
          \ to 4 bits, the model demonstrates unusual behavior, possibly due to its\
          \ complexity. We suggest using a minimum quantization of 8 bits, although\
          \ this has not been tested.\"\n\nI recommend trying the new GPTQ quantization\
          \ method, which includes the combined options \"act-order\"  +  \"true-sequential\"\
          \  +  \"groupsize 128\". \nThis method brings the quantized model's performance\
          \ much closer to that of the 16-bit model.\n\nCheck out the following link\
          \ for more information: https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/triton\n\
          \nIf you decide to implement this quantization method, please consider uploading\
          \ the resulting model to your repository. This way, users with both high-performance\
          \ (16-bit) and lower-end computers (4-bit) can enjoy your models.\n\nBest\
          \ regards,"
        updatedAt: '2023-03-30T16:16:27.789Z'
      numEdits: 3
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - addictivepixels
        - tensiondriven
        - UnderSampled
    id: 6425065ef1d18f46decdc650
    type: comment
  author: TheYuriLover
  content: "Hello,\n\nI noticed that you have experimented with quantization for your\
    \ model, but the results were not as good as expected:\n\n\"When quantized to\
    \ 4 bits, the model demonstrates unusual behavior, possibly due to its complexity.\
    \ We suggest using a minimum quantization of 8 bits, although this has not been\
    \ tested.\"\n\nI recommend trying the new GPTQ quantization method, which includes\
    \ the combined options \"act-order\"  +  \"true-sequential\"  +  \"groupsize 128\"\
    . \nThis method brings the quantized model's performance much closer to that of\
    \ the 16-bit model.\n\nCheck out the following link for more information: https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/triton\n\
    \nIf you decide to implement this quantization method, please consider uploading\
    \ the resulting model to your repository. This way, users with both high-performance\
    \ (16-bit) and lower-end computers (4-bit) can enjoy your models.\n\nBest regards,"
  created_at: 2023-03-30 02:47:42+00:00
  edited: true
  hidden: false
  id: 6425065ef1d18f46decdc650
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-03-30T04:13:03.000Z'
    data:
      from: Can we get the GPTQ quantized version?
      to: Can we get the GPTQ quantized model?
    id: 64250c4f6a04b6dd1243c6fd
    type: title-change
  author: TheYuriLover
  created_at: 2023-03-30 03:13:03+00:00
  id: 64250c4f6a04b6dd1243c6fd
  new_title: Can we get the GPTQ quantized model?
  old_title: Can we get the GPTQ quantized version?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9ddf10b2290f0e1f8b78a2ff5444605.svg
      fullname: Anton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DaveScream
      type: user
    createdAt: '2023-03-30T23:45:18.000Z'
    data:
      edited: false
      editors:
      - DaveScream
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9ddf10b2290f0e1f8b78a2ff5444605.svg
          fullname: Anton
          isHf: false
          isPro: false
          name: DaveScream
          type: user
        html: '<p>+1</p>

          '
        raw: '+1'
        updatedAt: '2023-03-30T23:45:18.771Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - addictivepixels
        - UnderSampled
    id: 64261f0eb9dfed28cf6565a8
    type: comment
  author: DaveScream
  content: '+1'
  created_at: 2023-03-30 22:45:18+00:00
  edited: false
  hidden: false
  id: 64261f0eb9dfed28cf6565a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-04-23T00:46:36.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p><a href="https://huggingface.co/autobots/alpaca-7b-native-enhanced-4bit/tree/main">https://huggingface.co/autobots/alpaca-7b-native-enhanced-4bit/tree/main</a></p>

          '
        raw: https://huggingface.co/autobots/alpaca-7b-native-enhanced-4bit/tree/main
        updatedAt: '2023-04-23T00:46:36.846Z'
      numEdits: 0
      reactions: []
    id: 64447fecc63001ae63595e97
    type: comment
  author: autobots
  content: https://huggingface.co/autobots/alpaca-7b-native-enhanced-4bit/tree/main
  created_at: 2023-04-22 23:46:36+00:00
  edited: false
  hidden: false
  id: 64447fecc63001ae63595e97
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: 8bit-coder/alpaca-7b-nativeEnhanced
repo_type: model
status: open
target_branch: null
title: Can we get the GPTQ quantized model?
