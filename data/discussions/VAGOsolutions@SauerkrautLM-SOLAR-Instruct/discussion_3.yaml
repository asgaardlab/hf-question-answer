!!python/object:huggingface_hub.community.DiscussionWithDetails
author: andreP
conflicting_files: null
created_at: 2023-12-22 08:04:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
      fullname: "Andr\xE9 Pankraz"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreP
      type: user
    createdAt: '2023-12-22T08:04:57.000Z'
    data:
      edited: true
      editors:
      - andreP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5769324898719788
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
          fullname: "Andr\xE9 Pankraz"
          isHf: false
          isPro: false
          name: andreP
          type: user
        html: "<p>Hi,</p>\n<p>i try to summarize Markdown tables like this (Markdown\
          \ table content excluded):</p>\n<p>INFO 12-22 07:50:41 async_llm_engine.py:379]\
          \ Received request cmpl-42304499db4546e9957718a83f18c31f: prompt: '&lt;s&gt;GPT4\
          \ Correct System: Du antwortest kurz und pr\xE4zise.&lt;|end_of_turn|&gt;GPT4\
          \ Correct User: Erstelle eine kurze ZUSAMMENFASSUNG (High Level \xDCberblick\
          \ in 2-3 kurzen S\xE4tzen) zur folgenden Tabelle (Markdown): .....MARKDOWN-TABLE....&lt;|end_of_turn|&gt;GPT4\
          \ Correct Assistant:', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0,\
          \ frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0,\
          \ top_k=-1, min_p=0.0, use_beam_search=False, length_penalty=1.0, early_stopping=False,\
          \ stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False,\
          \ max_tokens=3425, logprobs=None, prompt_logprobs=None, skip_special_tokens=True,\
          \ spaces_between_special_tokens=True)</p>\n<p>I go via vLLM and it applies\
          \ your tokenizer.chat_template correctly, as far i can see.<br>This LLM\
          \ just repeats the table in a more or less jumbled mess.</p>\n<p>ChatGPT\
          \ does it just fine and also the smaller and lower ranked OpenChat 3.5 1210\
          \ works perfectly for this.<br>I tried many different prompts and tables\
          \ - it just isn't doing very good on text-analyzing tasks.</p>\n<p>Best\
          \ regards,<br>Andr\xE9</p>\n"
        raw: "Hi,\n\ni try to summarize Markdown tables like this (Markdown table\
          \ content excluded):\n\nINFO 12-22 07:50:41 async_llm_engine.py:379] Received\
          \ request cmpl-42304499db4546e9957718a83f18c31f: prompt: '&lt;s>GPT4 Correct\
          \ System: Du antwortest kurz und pr\xE4zise.<|end_of_turn|>GPT4 Correct\
          \ User: Erstelle eine kurze ZUSAMMENFASSUNG (High Level \xDCberblick in\
          \ 2-3 kurzen S\xE4tzen) zur folgenden Tabelle (Markdown): .....MARKDOWN-TABLE....<|end_of_turn|>GPT4\
          \ Correct Assistant:', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0,\
          \ frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0,\
          \ top_k=-1, min_p=0.0, use_beam_search=False, length_penalty=1.0, early_stopping=False,\
          \ stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False,\
          \ max_tokens=3425, logprobs=None, prompt_logprobs=None, skip_special_tokens=True,\
          \ spaces_between_special_tokens=True)\n\nI go via vLLM and it applies your\
          \ tokenizer.chat_template correctly, as far i can see.\nThis LLM just repeats\
          \ the table in a more or less jumbled mess.\n\nChatGPT does it just fine\
          \ and also the smaller and lower ranked OpenChat 3.5 1210 works perfectly\
          \ for this.\nI tried many different prompts and tables - it just isn't doing\
          \ very good on text-analyzing tasks.\n\nBest regards,\nAndr\xE9"
        updatedAt: '2023-12-22T08:09:11.381Z'
      numEdits: 2
      reactions: []
    id: 65854329a74290979b038e5c
    type: comment
  author: andreP
  content: "Hi,\n\ni try to summarize Markdown tables like this (Markdown table content\
    \ excluded):\n\nINFO 12-22 07:50:41 async_llm_engine.py:379] Received request\
    \ cmpl-42304499db4546e9957718a83f18c31f: prompt: '&lt;s>GPT4 Correct System: Du\
    \ antwortest kurz und pr\xE4zise.<|end_of_turn|>GPT4 Correct User: Erstelle eine\
    \ kurze ZUSAMMENFASSUNG (High Level \xDCberblick in 2-3 kurzen S\xE4tzen) zur\
    \ folgenden Tabelle (Markdown): .....MARKDOWN-TABLE....<|end_of_turn|>GPT4 Correct\
    \ Assistant:', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0,\
    \ frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1,\
    \ min_p=0.0, use_beam_search=False, length_penalty=1.0, early_stopping=False,\
    \ stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False,\
    \ max_tokens=3425, logprobs=None, prompt_logprobs=None, skip_special_tokens=True,\
    \ spaces_between_special_tokens=True)\n\nI go via vLLM and it applies your tokenizer.chat_template\
    \ correctly, as far i can see.\nThis LLM just repeats the table in a more or less\
    \ jumbled mess.\n\nChatGPT does it just fine and also the smaller and lower ranked\
    \ OpenChat 3.5 1210 works perfectly for this.\nI tried many different prompts\
    \ and tables - it just isn't doing very good on text-analyzing tasks.\n\nBest\
    \ regards,\nAndr\xE9"
  created_at: 2023-12-22 08:04:57+00:00
  edited: true
  hidden: false
  id: 65854329a74290979b038e5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b999a40b24527e9c25583a/xFHCewJdf5EGn8qDPypqy.jpeg?w=200&h=200&f=face
      fullname: David Golchinfar
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: DavidGF
      type: user
    createdAt: '2023-12-22T17:38:43.000Z'
    data:
      edited: false
      editors:
      - DavidGF
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9008742570877075
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b999a40b24527e9c25583a/xFHCewJdf5EGn8qDPypqy.jpeg?w=200&h=200&f=face
          fullname: David Golchinfar
          isHf: false
          isPro: false
          name: DavidGF
          type: user
        html: '<p>Hey andreP,</p>

          <p>Thanks for testing and your feedback!<br>Your template doesn''t look
          quite like the one we provide in the model card or tokenizer_config. But
          I haven''t used vllm before, so maybe it will append the template in the
          background?<br>Regarding your parameters: please test temp = 0.3 - 0.5,
          top_p = 0.9, top_k = 20 and rep penalty to 1.15.<br>Please let us know if
          this worked for you.</p>

          '
        raw: "Hey andreP,\n\nThanks for testing and your feedback!\nYour template\
          \ doesn't look quite like the one we provide in the model card or tokenizer_config.\
          \ But I haven't used vllm before, so maybe it will append the template in\
          \ the background? \nRegarding your parameters: please test temp = 0.3 -\
          \ 0.5, top_p = 0.9, top_k = 20 and rep penalty to 1.15. \nPlease let us\
          \ know if this worked for you."
        updatedAt: '2023-12-22T17:38:43.316Z'
      numEdits: 0
      reactions: []
    id: 6585c9a3b826b4b77f6482ae
    type: comment
  author: DavidGF
  content: "Hey andreP,\n\nThanks for testing and your feedback!\nYour template doesn't\
    \ look quite like the one we provide in the model card or tokenizer_config. But\
    \ I haven't used vllm before, so maybe it will append the template in the background?\
    \ \nRegarding your parameters: please test temp = 0.3 - 0.5, top_p = 0.9, top_k\
    \ = 20 and rep penalty to 1.15. \nPlease let us know if this worked for you."
  created_at: 2023-12-22 17:38:43+00:00
  edited: false
  hidden: false
  id: 6585c9a3b826b4b77f6482ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
      fullname: "Andr\xE9 Pankraz"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreP
      type: user
    createdAt: '2023-12-23T12:08:10.000Z'
    data:
      edited: false
      editors:
      - andreP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6026923656463623
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
          fullname: "Andr\xE9 Pankraz"
          isHf: false
          isPro: false
          name: andreP
          type: user
        html: "<p>Hi,</p>\n<p>sry have send the wrong prompt log from OpenChat (I\
          \ compared your model with that one).<br>The template application of vllm\
          \ looks right:</p>\n<p>INFO 12-23 11:23:06 async_llm_engine.py:379] Received\
          \ request cmpl-c944d8ddf21a40e29a5256a785fa7f91: prompt: '### System:\\\
          nDu antwortest kurz und pr\xE4zise.\\n\\n### User:\\nErstelle eine kurze\
          \ ZUSAMMENFASSUNG (High Level \xDCberblick in 2-3 kurzen S\xE4tzen) zur\
          \ folgenden Tabelle (Markdown):  ......MARKDOWN........\\n\\n### Assistant:\\\
          n', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0,\
          \ frequency_penalty=1.15, repetition_penalty=1.0, temperature=0.4, top_p=0.9,\
          \ top_k=20, min_p=0.0, use_beam_search=False, length_penalty=1.0, early_stopping=False,\
          \ stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False,\
          \ max_tokens=5772, logprobs=None, prompt_logprobs=None, skip_special_tokens=True,\
          \ spaces_between_special_tokens=True), prompt token ids:  ....</p>\n<p>vllm\
          \ just uses the tokenizer.chat_template and offers an OpenAI compatible\
          \ chat endpoint.<br>For testing vLLM, just use this docker-compose.yml (adapt\
          \ volume and gpu-devices/mem):</p>\n<p>version: '3.8'<br>services:<br> \
          \ vllm-sauerkraut:<br>    image: vllm/vllm-openai:latest<br>    container_name:\
          \ vllm-sauerkraut<br>    environment:<br>      - HUGGING_FACE_HUB_TOKEN=<br>\
          \      - NVIDIA_VISIBLE_DEVICES=1<br>    volumes:<br>      - /mnt/sda/vllm:/root/.cache/huggingface<br>\
          \    ports:<br>      - \"8002:8000\"<br>    ipc: host<br>    deploy:<br>\
          \      resources:<br>        reservations:<br>          devices:<br>   \
          \         - driver: nvidia<br>              capabilities: [ gpu ]<br>  \
          \  restart: unless-stopped<br>    command:<br>      - --model=VAGOsolutions/SauerkrautLM-SOLAR-Instruct<br>\
          \      - --gpu-memory-utilization=0.8</p>\n<p>And this curl, sry can't provide\
          \ my MARKDOWN - private data:</p>\n<p>$ curl --location --insecure --request\
          \ POST '<a rel=\"nofollow\" href=\"http://ai1.dev.init:8002/v1/chat/completions'\"\
          >http://ai1.dev.init:8002/v1/chat/completions'</a> <br>-H \"Content-Type:\
          \ application/json\" <br>-d '{<br>\"model\": \"VAGOsolutions/SauerkrautLM-SOLAR-Instruct\"\
          ,<br>\"messages\": [{\"role\": \"system\", \"content\": \"Du antwortest\
          \ kurz und pr\xE4zise.\"},{\"role\": \"user\", \"content\": \"Erstelle eine\
          \ kurze ZUSAMMENFASSUNG (High Level \xDCberblick in 2-3 kurzen S\xE4tzen)\
          \ zur folgenden Tabelle (Markdown):  ......MARKDOWN.....\"}],<br>\"temperature\"\
          : 0.4,<br>\"top_p\": 0.9,<br>\"top_k\": 20,<br>\"frequency_penalty\": 1.15<br>}'</p>\n\
          <p>I changed the params a bit around, yes the output changes - but it doesn't\
          \ really get much better and has very unstable behaviour.<br>I havn't checked,\
          \ if the SOLAR base model also has this problem and you just inherited that.<br>OpenChat\
          \ also answers perfectly with these (same) params. In my experience, these\
          \ params can improve the model response, but if it's very bad at default,\
          \ it will not get really good with magic sampling params.</p>\n<p>May be\
          \ you haven't overtrained on Benchmark test data (like you wrote), but many\
          \ models in the leader board are just optimizing for benchmark-metrics anway.\
          \ The benchmarks just don't test all aspects (and never can).</p>\n"
        raw: "Hi,\n\nsry have send the wrong prompt log from OpenChat (I compared\
          \ your model with that one).\nThe template application of vllm looks right:\n\
          \nINFO 12-23 11:23:06 async_llm_engine.py:379] Received request cmpl-c944d8ddf21a40e29a5256a785fa7f91:\
          \ prompt: '### System:\\nDu antwortest kurz und pr\xE4zise.\\n\\n### User:\\\
          nErstelle eine kurze ZUSAMMENFASSUNG (High Level \xDCberblick in 2-3 kurzen\
          \ S\xE4tzen) zur folgenden Tabelle (Markdown):  ......MARKDOWN........\\\
          n\\n### Assistant:\\n', sampling params: SamplingParams(n=1, best_of=1,\
          \ presence_penalty=0.0, frequency_penalty=1.15, repetition_penalty=1.0,\
          \ temperature=0.4, top_p=0.9, top_k=20, min_p=0.0, use_beam_search=False,\
          \ length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[],\
          \ include_stop_str_in_output=False, ignore_eos=False, max_tokens=5772, logprobs=None,\
          \ prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True),\
          \ prompt token ids:  ....\n\n\nvllm just uses the tokenizer.chat_template\
          \ and offers an OpenAI compatible chat endpoint.\nFor testing vLLM, just\
          \ use this docker-compose.yml (adapt volume and gpu-devices/mem):\n\nversion:\
          \ '3.8'\nservices:\n  vllm-sauerkraut:\n    image: vllm/vllm-openai:latest\n\
          \    container_name: vllm-sauerkraut\n    environment:\n      - HUGGING_FACE_HUB_TOKEN=<secret>\n\
          \      - NVIDIA_VISIBLE_DEVICES=1\n    volumes:\n      - /mnt/sda/vllm:/root/.cache/huggingface\n\
          \    ports:\n      - \"8002:8000\"\n    ipc: host\n    deploy:\n      resources:\n\
          \        reservations:\n          devices:\n            - driver: nvidia\n\
          \              capabilities: [ gpu ]\n    restart: unless-stopped\n    command:\n\
          \      - --model=VAGOsolutions/SauerkrautLM-SOLAR-Instruct\n      - --gpu-memory-utilization=0.8\n\
          \nAnd this curl, sry can't provide my MARKDOWN - private data:\n\n$ curl\
          \ --location --insecure --request POST 'http://ai1.dev.init:8002/v1/chat/completions'\
          \ \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"VAGOsolutions/SauerkrautLM-SOLAR-Instruct\"\
          ,\n\"messages\": [{\"role\": \"system\", \"content\": \"Du antwortest kurz\
          \ und pr\xE4zise.\"},{\"role\": \"user\", \"content\": \"Erstelle eine kurze\
          \ ZUSAMMENFASSUNG (High Level \xDCberblick in 2-3 kurzen S\xE4tzen) zur\
          \ folgenden Tabelle (Markdown):  ......MARKDOWN.....\"}],\n\"temperature\"\
          : 0.4,\n\"top_p\": 0.9,\n\"top_k\": 20,\n\"frequency_penalty\": 1.15\n}'\n\
          \nI changed the params a bit around, yes the output changes - but it doesn't\
          \ really get much better and has very unstable behaviour.\nI havn't checked,\
          \ if the SOLAR base model also has this problem and you just inherited that.\n\
          OpenChat also answers perfectly with these (same) params. In my experience,\
          \ these params can improve the model response, but if it's very bad at default,\
          \ it will not get really good with magic sampling params.\n\nMay be you\
          \ haven't overtrained on Benchmark test data (like you wrote), but many\
          \ models in the leader board are just optimizing for benchmark-metrics anway.\
          \ The benchmarks just don't test all aspects (and never can).\n"
        updatedAt: '2023-12-23T12:08:10.993Z'
      numEdits: 0
      reactions: []
    id: 6586cdaa08f83845fc00de1d
    type: comment
  author: andreP
  content: "Hi,\n\nsry have send the wrong prompt log from OpenChat (I compared your\
    \ model with that one).\nThe template application of vllm looks right:\n\nINFO\
    \ 12-23 11:23:06 async_llm_engine.py:379] Received request cmpl-c944d8ddf21a40e29a5256a785fa7f91:\
    \ prompt: '### System:\\nDu antwortest kurz und pr\xE4zise.\\n\\n### User:\\nErstelle\
    \ eine kurze ZUSAMMENFASSUNG (High Level \xDCberblick in 2-3 kurzen S\xE4tzen)\
    \ zur folgenden Tabelle (Markdown):  ......MARKDOWN........\\n\\n### Assistant:\\\
    n', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=1.15,\
    \ repetition_penalty=1.0, temperature=0.4, top_p=0.9, top_k=20, min_p=0.0, use_beam_search=False,\
    \ length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False,\
    \ ignore_eos=False, max_tokens=5772, logprobs=None, prompt_logprobs=None, skip_special_tokens=True,\
    \ spaces_between_special_tokens=True), prompt token ids:  ....\n\n\nvllm just\
    \ uses the tokenizer.chat_template and offers an OpenAI compatible chat endpoint.\n\
    For testing vLLM, just use this docker-compose.yml (adapt volume and gpu-devices/mem):\n\
    \nversion: '3.8'\nservices:\n  vllm-sauerkraut:\n    image: vllm/vllm-openai:latest\n\
    \    container_name: vllm-sauerkraut\n    environment:\n      - HUGGING_FACE_HUB_TOKEN=<secret>\n\
    \      - NVIDIA_VISIBLE_DEVICES=1\n    volumes:\n      - /mnt/sda/vllm:/root/.cache/huggingface\n\
    \    ports:\n      - \"8002:8000\"\n    ipc: host\n    deploy:\n      resources:\n\
    \        reservations:\n          devices:\n            - driver: nvidia\n   \
    \           capabilities: [ gpu ]\n    restart: unless-stopped\n    command:\n\
    \      - --model=VAGOsolutions/SauerkrautLM-SOLAR-Instruct\n      - --gpu-memory-utilization=0.8\n\
    \nAnd this curl, sry can't provide my MARKDOWN - private data:\n\n$ curl --location\
    \ --insecure --request POST 'http://ai1.dev.init:8002/v1/chat/completions' \\\n\
    -H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"VAGOsolutions/SauerkrautLM-SOLAR-Instruct\"\
    ,\n\"messages\": [{\"role\": \"system\", \"content\": \"Du antwortest kurz und\
    \ pr\xE4zise.\"},{\"role\": \"user\", \"content\": \"Erstelle eine kurze ZUSAMMENFASSUNG\
    \ (High Level \xDCberblick in 2-3 kurzen S\xE4tzen) zur folgenden Tabelle (Markdown):\
    \  ......MARKDOWN.....\"}],\n\"temperature\": 0.4,\n\"top_p\": 0.9,\n\"top_k\"\
    : 20,\n\"frequency_penalty\": 1.15\n}'\n\nI changed the params a bit around, yes\
    \ the output changes - but it doesn't really get much better and has very unstable\
    \ behaviour.\nI havn't checked, if the SOLAR base model also has this problem\
    \ and you just inherited that.\nOpenChat also answers perfectly with these (same)\
    \ params. In my experience, these params can improve the model response, but if\
    \ it's very bad at default, it will not get really good with magic sampling params.\n\
    \nMay be you haven't overtrained on Benchmark test data (like you wrote), but\
    \ many models in the leader board are just optimizing for benchmark-metrics anway.\
    \ The benchmarks just don't test all aspects (and never can).\n"
  created_at: 2023-12-23 12:08:10+00:00
  edited: false
  hidden: false
  id: 6586cdaa08f83845fc00de1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b999a40b24527e9c25583a/xFHCewJdf5EGn8qDPypqy.jpeg?w=200&h=200&f=face
      fullname: David Golchinfar
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: DavidGF
      type: user
    createdAt: '2023-12-28T04:50:38.000Z'
    data:
      edited: false
      editors:
      - DavidGF
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8680717349052429
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b999a40b24527e9c25583a/xFHCewJdf5EGn8qDPypqy.jpeg?w=200&h=200&f=face
          fullname: David Golchinfar
          isHf: false
          isPro: false
          name: DavidGF
          type: user
        html: '<p>Hi andreP,<br>Many thanks for the constructive feedback! I personally
          noticed that the model does not perform so well with a system prompt in
          the template. Could you do me a favor and test the whole thing again without
          System prompt? But you should include the content of the system prompt in
          the user prompt part.</p>

          '
        raw: 'Hi andreP,

          Many thanks for the constructive feedback! I personally noticed that the
          model does not perform so well with a system prompt in the template. Could
          you do me a favor and test the whole thing again without System prompt?
          But you should include the content of the system prompt in the user prompt
          part.'
        updatedAt: '2023-12-28T04:50:38.577Z'
      numEdits: 0
      reactions: []
    id: 658cfe9e7f5ad56419fc35d0
    type: comment
  author: DavidGF
  content: 'Hi andreP,

    Many thanks for the constructive feedback! I personally noticed that the model
    does not perform so well with a system prompt in the template. Could you do me
    a favor and test the whole thing again without System prompt? But you should include
    the content of the system prompt in the user prompt part.'
  created_at: 2023-12-28 04:50:38+00:00
  edited: false
  hidden: false
  id: 658cfe9e7f5ad56419fc35d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
      fullname: "Andr\xE9 Pankraz"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreP
      type: user
    createdAt: '2023-12-28T09:55:48.000Z'
    data:
      edited: true
      editors:
      - andreP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9223523736000061
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
          fullname: "Andr\xE9 Pankraz"
          isHf: false
          isPro: false
          name: andreP
          type: user
        html: "<p>Hi,</p>\n<p>Thanks for your feedback. I've tested it based on your\
          \ suggestions. Indeed, if I combine the system prompt with the first user\
          \ message and use your recommended sampling parameters, it works. However,\
          \ if I alter any part of this setup\u2014for example, by reducing the frequency_penalty,\
          \ temperature, or top_p settings, or by separating the system message\u2014\
          then it doesn't work. It's extremely sensitive to these factors.</p>\n<p>When\
          \ I use your VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct, I don't encounter\
          \ this issue and can vary the sampling parameters broadly without receiving\
          \ bogus responses (though, it doesn't support separate system prompts, which\
          \ is one less variable to consider). Disadvantage is, that it's 4-5 times\
          \ slower with vLLM AWQ (Unquantized doesn't fit onto my L40).</p>\n<p>Thanks\
          \ for the feedback. I won't be using this model\u2014it's just too sensitive\
          \ to meta settings for my text analysis tasks.</p>\n<p>Looking forward to\
          \ your future work! It's impressive.</p>\n"
        raw: "Hi,\n\nThanks for your feedback. I've tested it based on your suggestions.\
          \ Indeed, if I combine the system prompt with the first user message and\
          \ use your recommended sampling parameters, it works. However, if I alter\
          \ any part of this setup\u2014for example, by reducing the frequency_penalty,\
          \ temperature, or top_p settings, or by separating the system message\u2014\
          then it doesn't work. It's extremely sensitive to these factors.\n\nWhen\
          \ I use your VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct, I don't encounter\
          \ this issue and can vary the sampling parameters broadly without receiving\
          \ bogus responses (though, it doesn't support separate system prompts, which\
          \ is one less variable to consider). Disadvantage is, that it's 4-5 times\
          \ slower with vLLM AWQ (Unquantized doesn't fit onto my L40).\n\nThanks\
          \ for the feedback. I won't be using this model\u2014it's just too sensitive\
          \ to meta settings for my text analysis tasks.\n\nLooking forward to your\
          \ future work! It's impressive."
        updatedAt: '2023-12-28T10:01:00.995Z'
      numEdits: 3
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - DaryoushV
        - DavidGF
    id: 658d4624363800354b64d5c6
    type: comment
  author: andreP
  content: "Hi,\n\nThanks for your feedback. I've tested it based on your suggestions.\
    \ Indeed, if I combine the system prompt with the first user message and use your\
    \ recommended sampling parameters, it works. However, if I alter any part of this\
    \ setup\u2014for example, by reducing the frequency_penalty, temperature, or top_p\
    \ settings, or by separating the system message\u2014then it doesn't work. It's\
    \ extremely sensitive to these factors.\n\nWhen I use your VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,\
    \ I don't encounter this issue and can vary the sampling parameters broadly without\
    \ receiving bogus responses (though, it doesn't support separate system prompts,\
    \ which is one less variable to consider). Disadvantage is, that it's 4-5 times\
    \ slower with vLLM AWQ (Unquantized doesn't fit onto my L40).\n\nThanks for the\
    \ feedback. I won't be using this model\u2014it's just too sensitive to meta settings\
    \ for my text analysis tasks.\n\nLooking forward to your future work! It's impressive."
  created_at: 2023-12-28 09:55:48+00:00
  edited: true
  hidden: false
  id: 658d4624363800354b64d5c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b999a40b24527e9c25583a/xFHCewJdf5EGn8qDPypqy.jpeg?w=200&h=200&f=face
      fullname: David Golchinfar
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: DavidGF
      type: user
    createdAt: '2023-12-29T23:24:00.000Z'
    data:
      status: closed
    id: 658f5510e7b4143ec8a5e0fa
    type: status-change
  author: DavidGF
  created_at: 2023-12-29 23:24:00+00:00
  id: 658f5510e7b4143ec8a5e0fa
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: VAGOsolutions/SauerkrautLM-SOLAR-Instruct
repo_type: model
status: closed
target_branch: null
title: Doesn't follow instruction very well
