!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YanbinZhu
conflicting_files: null
created_at: 2023-09-07 07:24:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3cb9c333cefb1ee94b25c5ee032451d7.svg
      fullname: Yanbin Zhu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YanbinZhu
      type: user
    createdAt: '2023-09-07T08:24:40.000Z'
    data:
      edited: true
      editors:
      - YanbinZhu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.512580156326294
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3cb9c333cefb1ee94b25c5ee032451d7.svg
          fullname: Yanbin Zhu
          isHf: false
          isPro: false
          name: YanbinZhu
          type: user
        html: "<p>transformers-cli env</p>\n<ul>\n<li><code>transformers</code> version:\
          \ 4.32.1</li>\n<li>Platform: macOS-12.5.1-arm64-arm-64bit</li>\n<li>Python\
          \ version: 3.9.16</li>\n<li>Huggingface_hub version: 0.16.4</li>\n<li>Safetensors\
          \ version: 0.3.3</li>\n<li>Accelerate version: not installed</li>\n<li>Accelerate\
          \ config: not found</li>\n<li>PyTorch version (GPU?): 2.0.1 (False)</li>\n\
          <li>Tensorflow version (GPU?): not installed (NA)</li>\n<li>Flax version\
          \ (CPU?/GPU?/TPU?): not installed (NA)</li>\n<li>Jax version: not installed</li>\n\
          <li>JaxLib version: not installed</li>\n<li>Using GPU in script?: </li>\n\
          <li>Using distributed or parallel set-up in script?: </li>\n</ul>\n<p>Code:</p>\n\
          <p>import torch<br>from transformers import AutoTokenizer, AutoModelForSequenceClassification,\
          \ BertConfig</p>\n<p>model_name = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\
          <br>tokenizer = AutoTokenizer.from_pretrained(model_name)</p>\n<h1 id=\"\
          initializing-the-model-with-the-torchscript-flag\">Initializing the model\
          \ with the torchscript flag</h1>\n<h1 id=\"flag-set-to-true-even-though-it-is-not-necessary-as-this-model-does-not-have-an-lm-head\"\
          >Flag set to True even though it is not necessary as this model does not\
          \ have an LM Head.</h1>\n<p>config = BertConfig(<br>    vocab_size_or_config_json_file=32000,<br>\
          \    hidden_size=768,<br>    num_hidden_layers=12,<br>    num_attention_heads=12,<br>\
          \    intermediate_size=3072,<br>    torchscript=True,<br>)</p>\n<h1 id=\"\
          instantiating-the-model\">Instantiating the model</h1>\n<p>model = AutoModelForSequenceClassification.from_config(config)</p>\n\
          <p>model.eval()<br>premise = \"Test case.\"<br>hypothesis = \"issue\"</p>\n\
          <p>inputs = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"\
          pt\")</p>\n<p>print(inputs)</p>\n<p>model = AutoModelForSequenceClassification.from_pretrained(model_name,\
          \ torchscript=True)<br>traced_model = torch.jit.trace(model, [inputs['input_ids'],\
          \ inputs['attention_mask'], inputs['token_type_ids']])</p>\n<p>torch.jit.save(traced_model,\
          \ \"DeBERTa_4.pt\")</p>\n<p>RuntimeError:<br>Could not export Python function\
          \ call 'XSoftmax'. Remove calls to Python functions before export. Did you\
          \ forget to add <span data-props=\"{&quot;user&quot;:&quot;script&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/script\"\
          >@<span class=\"underline\">script</span></a></span>\n\n\t</span></span>\
          \ or @script_method annotation? If this is a nn.ModuleList, add it to <strong>constants</strong>:</p>\n"
        raw: "transformers-cli env\n\n- `transformers` version: 4.32.1\n- Platform:\
          \ macOS-12.5.1-arm64-arm-64bit\n- Python version: 3.9.16\n- Huggingface_hub\
          \ version: 0.16.4\n- Safetensors version: 0.3.3\n- Accelerate version: not\
          \ installed\n- Accelerate config: not found\n- PyTorch version (GPU?): 2.0.1\
          \ (False)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version\
          \ (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n-\
          \ JaxLib version: not installed\n- Using GPU in script?: <fill in>\n- Using\
          \ distributed or parallel set-up in script?: <fill in>\n\n\nCode:\n\nimport\
          \ torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification,\
          \ BertConfig\n\nmodel_name = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\
          \ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Initializing\
          \ the model with the torchscript flag\n# Flag set to True even though it\
          \ is not necessary as this model does not have an LM Head.\nconfig = BertConfig(\n\
          \    vocab_size_or_config_json_file=32000,\n    hidden_size=768,\n    num_hidden_layers=12,\n\
          \    num_attention_heads=12,\n    intermediate_size=3072,\n    torchscript=True,\n\
          )\n\n# Instantiating the model\nmodel = AutoModelForSequenceClassification.from_config(config)\n\
          \nmodel.eval()\npremise = \"Test case.\"\nhypothesis = \"issue\"\n\ninputs\
          \ = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\"\
          )\n\nprint(inputs)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name,\
          \ torchscript=True)\ntraced_model = torch.jit.trace(model, [inputs['input_ids'],\
          \ inputs['attention_mask'], inputs['token_type_ids']])\n\ntorch.jit.save(traced_model,\
          \ \"DeBERTa_4.pt\")\n\n\n\nRuntimeError: \nCould not export Python function\
          \ call 'XSoftmax'. Remove calls to Python functions before export. Did you\
          \ forget to add @script or @script_method annotation? If this is a nn.ModuleList,\
          \ add it to __constants__:\n\n"
        updatedAt: '2023-09-07T08:25:57.919Z'
      numEdits: 1
      reactions: []
    id: 64f988c8ebed008ad37c47a3
    type: comment
  author: YanbinZhu
  content: "transformers-cli env\n\n- `transformers` version: 4.32.1\n- Platform:\
    \ macOS-12.5.1-arm64-arm-64bit\n- Python version: 3.9.16\n- Huggingface_hub version:\
    \ 0.16.4\n- Safetensors version: 0.3.3\n- Accelerate version: not installed\n\
    - Accelerate config: not found\n- PyTorch version (GPU?): 2.0.1 (False)\n- Tensorflow\
    \ version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed\
    \ (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using\
    \ GPU in script?: <fill in>\n- Using distributed or parallel set-up in script?:\
    \ <fill in>\n\n\nCode:\n\nimport torch\nfrom transformers import AutoTokenizer,\
    \ AutoModelForSequenceClassification, BertConfig\n\nmodel_name = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\
    \ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Initializing the\
    \ model with the torchscript flag\n# Flag set to True even though it is not necessary\
    \ as this model does not have an LM Head.\nconfig = BertConfig(\n    vocab_size_or_config_json_file=32000,\n\
    \    hidden_size=768,\n    num_hidden_layers=12,\n    num_attention_heads=12,\n\
    \    intermediate_size=3072,\n    torchscript=True,\n)\n\n# Instantiating the\
    \ model\nmodel = AutoModelForSequenceClassification.from_config(config)\n\nmodel.eval()\n\
    premise = \"Test case.\"\nhypothesis = \"issue\"\n\ninputs = tokenizer(premise,\
    \ hypothesis, truncation=True, return_tensors=\"pt\")\n\nprint(inputs)\n\nmodel\
    \ = AutoModelForSequenceClassification.from_pretrained(model_name, torchscript=True)\n\
    traced_model = torch.jit.trace(model, [inputs['input_ids'], inputs['attention_mask'],\
    \ inputs['token_type_ids']])\n\ntorch.jit.save(traced_model, \"DeBERTa_4.pt\"\
    )\n\n\n\nRuntimeError: \nCould not export Python function call 'XSoftmax'. Remove\
    \ calls to Python functions before export. Did you forget to add @script or @script_method\
    \ annotation? If this is a nn.ModuleList, add it to __constants__:\n\n"
  created_at: 2023-09-07 07:24:40+00:00
  edited: true
  hidden: false
  id: 64f988c8ebed008ad37c47a3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli
repo_type: model
status: open
target_branch: null
title: Cannot export this model to TorchScript
