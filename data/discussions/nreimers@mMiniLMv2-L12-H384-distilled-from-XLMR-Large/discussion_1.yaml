!!python/object:huggingface_hub.community.DiscussionWithDetails
author: agomberto
conflicting_files: null
created_at: 2022-11-04 11:28:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650965788157-noauth.jpeg?w=200&h=200&f=face
      fullname: Arnault Gombert
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: agomberto
      type: user
    createdAt: '2022-11-04T12:28:59.000Z'
    data:
      edited: false
      editors:
      - agomberto
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650965788157-noauth.jpeg?w=200&h=200&f=face
          fullname: Arnault Gombert
          isHf: false
          isPro: false
          name: agomberto
          type: user
        html: "<p>Hey, </p>\n<p>Just a question, can we use it to fine-tune like the\
          \ <a href=\"https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384\"\
          >miniLMv1</a> ? What is the licence ?</p>\n<p>Is it the one with this readme:</p>\n\
          <p><strong>Small and fast pre-trained models for language understanding\
          \ and generation</strong></p>\n<p><strong>***** New June 9, 2021: MiniLM\
          \ v2 release *****</strong></p>\n<p><strong>MiniLM v2</strong>: the pre-trained\
          \ models for the paper entitled \"<a rel=\"nofollow\" href=\"https://arxiv.org/abs/2012.15828\"\
          >MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing\
          \ Pretrained Transformers</a>\". We generalize deep self-attention distillation\
          \ in MiniLMv1 by using self-attention relation distillation for task-agnostic\
          \ compression of pre-trained Transformers. The proposed method eliminates\
          \ the restriction on the number of student\u2019s attention heads. Our monolingual\
          \ and multilingual small models distilled from different base and large\
          \ size teacher models achieve competitive performance. </p>\n<p><strong>[Multilingual]\
          \ Pre-trained Models</strong></p>\n<div class=\"max-w-full overflow-auto\"\
          >\n\t<table>\n\t\t<thead><tr>\n<th>Model</th>\n<th>Teacher Model</th>\n\
          <th>Speedup</th>\n<th>#Param</th>\n<th>XNLI (Acc)</th>\n<th>MLQA (F1)</th>\n\
          </tr>\n\n\t\t</thead><tbody><tr>\n<td><strong><a rel=\"nofollow\" href=\"\
          https://1drv.ms/u/s!AjHn0yEmKG8qiyGH2uvettAtQOov\">L12xH384 mMiniLMv2</a></strong></td>\n\
          <td>XLMR-Large</td>\n<td>2.7x</td>\n<td>117M</td>\n<td>72.9</td>\n<td>64.9</td>\n\
          </tr>\n<tr>\n<td><strong><a rel=\"nofollow\" href=\"https://1drv.ms/u/s!AjHn0yEmKG8qiyC4-L624EmV2i7z\"\
          >L6xH384 mMiniLMv2</a></strong></td>\n<td>XLMR-Large</td>\n<td>5.3x</td>\n\
          <td>107M</td>\n<td>69.3</td>\n<td>59.0</td>\n</tr>\n</tbody>\n\t</table>\n\
          </div>\n<p>We compress XLMR-Large into 12-layer and 6-layer models with\
          \ 384 hidden size and report the zero-shot performance on XNLI and MLQA\
          \ test set. </p>\n<p>Arnault</p>\n"
        raw: "Hey, \r\n\r\nJust a question, can we use it to fine-tune like the [miniLMv1](https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384)\
          \ ? What is the licence ?\r\n\r\nIs it the one with this readme:\r\n\r\n\
          **Small and fast pre-trained models for language understanding and generation**\r\
          \n\r\n**\\*\\*\\*\\*\\* New June 9, 2021: MiniLM v2 release \\*\\*\\*\\\
          *\\***\r\n\r\n**MiniLM v2**: the pre-trained models for the paper entitled\
          \ \"[MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing\
          \ Pretrained Transformers](https://arxiv.org/abs/2012.15828)\". We generalize\
          \ deep self-attention distillation in MiniLMv1 by using self-attention relation\
          \ distillation for task-agnostic compression of pre-trained Transformers.\
          \ The proposed method eliminates the restriction on the number of student\u2019\
          s attention heads. Our monolingual and multilingual small models distilled\
          \ from different base and large size teacher models achieve competitive\
          \ performance. \r\n\r\n**[Multilingual] Pre-trained Models**\r\n\r\n| Model\
          \                                                                      \
          \  | Teacher Model | Speedup   | #Param    | XNLI (Acc) | MLQA (F1)    |\r\
          \n|------------------------------------------------------------------------------|---------------|-----------|-----------|------------|--------------|\r\
          \n| **[L12xH384 mMiniLMv2](https://1drv.ms/u/s!AjHn0yEmKG8qiyGH2uvettAtQOov)**\
          \   | XLMR-Large    | 2.7x      | 117M      | 72.9       | 64.9        \
          \ |\r\n| **[L6xH384 mMiniLMv2](https://1drv.ms/u/s!AjHn0yEmKG8qiyC4-L624EmV2i7z)**\
          \    | XLMR-Large    | 5.3x      | 107M      | 69.3       | 59.0       \
          \  |\r\n\r\nWe compress XLMR-Large into 12-layer and 6-layer models with\
          \ 384 hidden size and report the zero-shot performance on XNLI and MLQA\
          \ test set. \r\n\r\n\r\nArnault"
        updatedAt: '2022-11-04T12:28:59.779Z'
      numEdits: 0
      reactions: []
    id: 6365058b98da81987e1e1bd0
    type: comment
  author: agomberto
  content: "Hey, \r\n\r\nJust a question, can we use it to fine-tune like the [miniLMv1](https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384)\
    \ ? What is the licence ?\r\n\r\nIs it the one with this readme:\r\n\r\n**Small\
    \ and fast pre-trained models for language understanding and generation**\r\n\r\
    \n**\\*\\*\\*\\*\\* New June 9, 2021: MiniLM v2 release \\*\\*\\*\\*\\***\r\n\r\
    \n**MiniLM v2**: the pre-trained models for the paper entitled \"[MiniLMv2: Multi-Head\
    \ Self-Attention Relation Distillation for Compressing Pretrained Transformers](https://arxiv.org/abs/2012.15828)\"\
    . We generalize deep self-attention distillation in MiniLMv1 by using self-attention\
    \ relation distillation for task-agnostic compression of pre-trained Transformers.\
    \ The proposed method eliminates the restriction on the number of student\u2019\
    s attention heads. Our monolingual and multilingual small models distilled from\
    \ different base and large size teacher models achieve competitive performance.\
    \ \r\n\r\n**[Multilingual] Pre-trained Models**\r\n\r\n| Model               \
    \                                                         | Teacher Model | Speedup\
    \   | #Param    | XNLI (Acc) | MLQA (F1)    |\r\n|------------------------------------------------------------------------------|---------------|-----------|-----------|------------|--------------|\r\
    \n| **[L12xH384 mMiniLMv2](https://1drv.ms/u/s!AjHn0yEmKG8qiyGH2uvettAtQOov)**\
    \   | XLMR-Large    | 2.7x      | 117M      | 72.9       | 64.9         |\r\n\
    | **[L6xH384 mMiniLMv2](https://1drv.ms/u/s!AjHn0yEmKG8qiyC4-L624EmV2i7z)**  \
    \  | XLMR-Large    | 5.3x      | 107M      | 69.3       | 59.0         |\r\n\r\
    \nWe compress XLMR-Large into 12-layer and 6-layer models with 384 hidden size\
    \ and report the zero-shot performance on XNLI and MLQA test set. \r\n\r\n\r\n\
    Arnault"
  created_at: 2022-11-04 11:28:59+00:00
  edited: false
  hidden: false
  id: 6365058b98da81987e1e1bd0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650965788157-noauth.jpeg?w=200&h=200&f=face
      fullname: Arnault Gombert
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: agomberto
      type: user
    createdAt: '2022-11-04T12:35:32.000Z'
    data:
      edited: false
      editors:
      - agomberto
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650965788157-noauth.jpeg?w=200&h=200&f=face
          fullname: Arnault Gombert
          isHf: false
          isPro: false
          name: agomberto
          type: user
        html: '<p>And also, is it planed to have it in tensorflow ? If not, is it
          a way I can change it from pytorch to tensorflow myself and upload it ?
          Thanks :)</p>

          '
        raw: And also, is it planed to have it in tensorflow ? If not, is it a way
          I can change it from pytorch to tensorflow myself and upload it ? Thanks
          :)
        updatedAt: '2022-11-04T12:35:32.324Z'
      numEdits: 0
      reactions: []
    id: 63650714f31ef76df4fab4b2
    type: comment
  author: agomberto
  content: And also, is it planed to have it in tensorflow ? If not, is it a way I
    can change it from pytorch to tensorflow myself and upload it ? Thanks :)
  created_at: 2022-11-04 11:35:32+00:00
  edited: false
  hidden: false
  id: 63650714f31ef76df4fab4b2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large
repo_type: model
status: open
target_branch: null
title: Language & Readme.md
