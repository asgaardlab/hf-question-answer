!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MatrixC7
conflicting_files: null
created_at: 2024-01-05 09:27:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/-OLDRiEOWDgqQUCKXigXD.jpeg?w=200&h=200&f=face
      fullname: Fangru Shao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MatrixC7
      type: user
    createdAt: '2024-01-05T09:27:45.000Z'
    data:
      edited: false
      editors:
      - MatrixC7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.42162197828292847
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/-OLDRiEOWDgqQUCKXigXD.jpeg?w=200&h=200&f=face
          fullname: Fangru Shao
          isHf: false
          isPro: false
          name: MatrixC7
          type: user
        html: "<p>Greetings!</p>\n<p>First thank you for developing and supporting\
          \ Chinese language capabilities with the Goliath model but we've encountered\
          \ several issues about quantization and functionality that we hope can be\
          \ addressed.</p>\n<ol>\n<li>Issues with Git LFS files during repository\
          \ cloning:</li>\n</ol>\n<p>We attempted to clone the repository using the\
          \ git clone command but faced issues with the Git LFS files. Each file was\
          \ only 1KB in size and contained a description rather than the expected\
          \ data.</p>\n<pre><code>PS E:\\&gt; git clone https://huggingface.co/hongyin/chat-goliath-120b-80k\n\
          Cloning into 'chat-goliath-120b-80k'...\nremote: Enumerating objects: 65,\
          \ done.\nremote: Counting objects: 100% (62/62), done.\nremote: Compressing\
          \ objects: 100% (61/61), done.\nremote: Total 65 (delta 14), reused 0 (delta\
          \ 0), pack-reused 3\nUnpacking objects: 100% (65/65), 662.65 KiB | 215.00\
          \ KiB/s, done.\nPS E:\\&gt; cd .\\chat-goliath-120b-80k\\\nPS E:\\chat-goliath-120b-80k&gt;\
          \ cat .\\pytorch_model-00001-of-00024.bin\nversion https://git-lfs.github.com/spec/v1\n\
          oid sha256:34aaef9a13c63f0fd5c14747ecab157bae1a9c948aaa60a543e27d083492f2c1\n\
          size 9666282843\n</code></pre>\n<p>We then directly downloaded the files,\
          \ ensuring their integrity through SHA256 checksums.</p>\n<ol start=\"2\"\
          >\n<li>Strange Metadata and Incoherent Responses in Kobold.cpp:</li>\n</ol>\n\
          <p>We use llama.cpp (commit b3a7c20) to quantize the model to Q2_K. </p>\n\
          <pre><code>(llamacpp) PS F:\\llama.cpp\\build\\bin\\Release&gt; .\\quantize.exe\
          \ F:\\chat-goliath-120b-80k-f16.gguf F:\\chat-goliath-120b-80k-f16-q2_k.gguf\
          \ Q2_K\nmain: build = 1768 (b3a7c20)\nmain: built with MSVC 19.38.33133.0\
          \ for x64\nmain: quantizing 'F:\\chat-goliath-120b-80k-f16.gguf' to 'F:\\\
          chat-goliath-120b-80k-f16-q2_k.gguf' as Q2_K\nllama_model_loader: loaded\
          \ meta data with 21 key-value pairs and 1236 tensors from F:\\chat-goliath-120b-80k-f16.gguf\
          \ (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values.\
          \ Note: KV overrides do not apply in this output.\nllama_model_loader: -\
          \ kv   0:                       general.architecture str              =\
          \ llama\nllama_model_loader: - kv   1:                               general.name\
          \ str              = F:\\\nllama_model_loader: - kv   2:               \
          \        llama.context_length u32              = 81920\nllama_model_loader:\
          \ - kv   3:                     llama.embedding_length u32             \
          \ = 8192\nllama_model_loader: - kv   4:                          llama.block_count\
          \ u32              = 137\nllama_model_loader: - kv   5:                \
          \  llama.feed_forward_length u32              = 28672\nllama_model_loader:\
          \ - kv   6:                 llama.rope.dimension_count u32             \
          \ = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count\
          \ u32              = 64\nllama_model_loader: - kv   8:              llama.attention.head_count_kv\
          \ u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
          \ f32              = 0.000010\nllama_model_loader: - kv  10:           \
          \            llama.rope.freq_base f32              = 10000.000000\nllama_model_loader:\
          \ - kv  11:                          general.file_type u32             \
          \ = 1\nllama_model_loader: - kv  12:                       tokenizer.ggml.model\
          \ str              = llama\nllama_model_loader: - kv  13:              \
          \        tokenizer.ggml.tokens arr[str,49300]   = [\"&lt;unk&gt;\", \"&lt;s&gt;\"\
          , \"&lt;/s&gt;\", \"&lt;0x00&gt;\", \"&lt;...\nllama_model_loader: - kv\
          \  14:                      tokenizer.ggml.scores arr[f32,49300]   = [0.000000,\
          \ 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:         \
          \         tokenizer.ggml.token_type arr[i32,49300]   = [2, 3, 3, 6, 6, 6,\
          \ 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                 \
          \     tokenizer.ggml.merges arr[str,71829]   = [\"\u6458 \u8981\", \"\u2581\
          \ \u300A\", \"\u4E2D \u56FD\", \"1 ...\nllama_model_loader: - kv  17:  \
          \              tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader:\
          \ - kv  18:                tokenizer.ggml.eos_token_id u32             \
          \ = 2\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id\
          \ u32              = 0\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id\
          \ u32              = 2\nllama_model_loader: - type  f32:  275 tensors\n\
          llama_model_loader: - type  f16:  961 tensors\nllama_model_quantize_internal:\
          \ meta size = 2310176 bytes\n...\nllama_model_quantize_internal: model size\
          \  = 225133.22 MB\nllama_model_quantize_internal: quant size  = 47484.73\
          \ MB\n</code></pre>\n<p>Upon loading the files with Kobold.cpp, we observed\
          \ oddities in the metadata. The Q2_K parameter exhibited an irregular 3.37\
          \ BPW value, whose value should usually be around 2.625 BPW.</p>\n<pre><code>llm_load_print_meta:\
          \ format           = GGUF V3 (latest)\nllm_load_print_meta: arch       \
          \      = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta:\
          \ n_vocab          = 49300\nllm_load_print_meta: n_merges         = 0\n\
          llm_load_print_meta: n_ctx_train      = 81920\nllm_load_print_meta: n_embd\
          \           = 8192\nllm_load_print_meta: n_head           = 64\nllm_load_print_meta:\
          \ n_head_kv        = 8\nllm_load_print_meta: n_layer          = 137\nllm_load_print_meta:\
          \ n_rot            = 128\nllm_load_print_meta: n_gqa            = 8\nllm_load_print_meta:\
          \ f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n\
          llm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias\
          \ = 0.0e+00\nllm_load_print_meta: n_ff             = 28672\nllm_load_print_meta:\
          \ n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta:\
          \ rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\n\
          llm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx\
          \  = 81920\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta:\
          \ model type       = ?B\nllm_load_print_meta: model ftype      = unknown,\
          \ may not work\nllm_load_print_meta: model params     = 118.03 B\nllm_load_print_meta:\
          \ model size       = 46.37 GiB (3.37 BPW)\nllm_load_print_meta: general.name\
          \     = F:\\\nllm_load_print_meta: BOS token        = 1 '&lt;s&gt;'\nllm_load_print_meta:\
          \ EOS token        = 2 '&lt;/s&gt;'\nllm_load_print_meta: UNK token    \
          \    = 0 '&lt;unk&gt;'\nllm_load_print_meta: PAD token        = 2 '&lt;/s&gt;'\n\
          llm_load_print_meta: LF token         = 13 '&lt;0x0A&gt;'\nllm_load_tensors:\
          \ ggml ctx size       =    0.47 MiB\nllm_load_tensors: using CUDA for GPU\
          \ acceleration\nllm_load_tensors: system memory used  =  126.85 MiB\nllm_load_tensors:\
          \ VRAM used           = 47358.35 MiB\nllm_load_tensors: offloading 137 repeating\
          \ layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\n\
          llm_load_tensors: offloaded 138/138 layers to GPU\n</code></pre>\n<p>The\
          \ model's responses were also inconsistent and interspersed with a mix of\
          \ Chinese and English, lacking coherence and relevance.<br><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/JgGjPStpchvv1AG1QRgOV.png\"\
          ><img alt=\"kobldcpp gguf response 1.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/JgGjPStpchvv1AG1QRgOV.png\"\
          ></a><br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/_st9FUkas4TjIhjYUxwmO.png\"\
          ><img alt=\"kobldcpp gguf response 2.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/_st9FUkas4TjIhjYUxwmO.png\"\
          ></a><br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/Xe3kj0c1LF1wB6yfegp6j.png\"\
          ><img alt=\"kobldcpp gguf response 3.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/Xe3kj0c1LF1wB6yfegp6j.png\"\
          ></a></p>\n<ol start=\"3\">\n<li>Quantization Attempt in exl2:</li>\n</ol>\n\
          <p>Moving forward, we attempted to quantize the model in exl2. We first\
          \ converted the model into .safetensors, with our SHA256 for the .safetensors\
          \ matching the expected values.</p>\n<pre><code>PS F:\\chat-goliath-120b-80k&gt;\
          \ Get-ChildItem -File | Get-FileHash -Algorithm SHA256\n\nAlgorithm    \
          \   Hash                                                               \
          \    Path\n---------       ----                                        \
          \                           ----\nSHA256          D606246B713E31B1B43DFE913554DD47819089B9E44B9EBAA294EDA2589A6B4C\
          \       F:\\chat-goliath-120b-80k\\added_tokens.json\nSHA256          EF58A0874EB14E647EF9DBF1B5A104701375FEE34F2D01BFD9795C984A07FDC0\
          \       F:\\chat-goliath-120b-80k\\config.json\nSHA256          E9ADFCFDC532213AE59497E875478D95451D063640FB51755BC63C1070B9701D\
          \       F:\\chat-goliath-120b-80k\\generation_config.json\nSHA256      \
          \    3C9006082484A4A04AD7932BCAF0FAE22948CA063B4E1EF16A74C15D6E7F72CC  \
          \     F:\\chat-goliath-120b-80k\\model-00001-of-00024.safetensors\nSHA256\
          \          60B3F61167C18CFE8968A3FC377D3DF7D14051595CD3044E48549580387B1A42\
          \       F:\\chat-goliath-120b-80k\\model-00002-of-00024.safetensors\nSHA256\
          \          358A58059687D8DEF130B2E4102DD596825255DA51F18DEAE277921EE6DA28B2\
          \       F:\\chat-goliath-120b-80k\\model-00003-of-00024.safetensors\nSHA256\
          \          7D7E35B80E4AC0B59E2BAB33F0F8E6FE5F8D2DD36983EF36BCA1E2628C20264E\
          \       F:\\chat-goliath-120b-80k\\model-00004-of-00024.safetensors\nSHA256\
          \          9CCB7212C979E15229DDBEFF6992E4A30C3D3AD1F1FFE571D14F8EB4721398EE\
          \       F:\\chat-goliath-120b-80k\\model-00005-of-00024.safetensors\nSHA256\
          \          6753B8281079AC0927C0FB4E4AD9877F57E1BE40E4B5840ABB36F95BE8DB35D7\
          \       F:\\chat-goliath-120b-80k\\model-00006-of-00024.safetensors\nSHA256\
          \          32BCF2B2BBF3E3BD052239158D14D0693142B6A3D5B014EA0C626C686C42AFA7\
          \       F:\\chat-goliath-120b-80k\\model-00007-of-00024.safetensors\nSHA256\
          \          051B77A0CD28F54B4AE6D2087257CF4D2C7198C245D9031312582B2AEEEF143C\
          \       F:\\chat-goliath-120b-80k\\model-00008-of-00024.safetensors\nSHA256\
          \          EFC8528F9FE25EAB6D72F7E84D9CADCD13CA998F179A8AFD6D7521A7F8BCAE1B\
          \       F:\\chat-goliath-120b-80k\\model-00009-of-00024.safetensors\nSHA256\
          \          109643CE4559EC438CF0D1C8326CE5B32E9DFF4A35AC61EAF149CA1C0B27731C\
          \       F:\\chat-goliath-120b-80k\\model-00010-of-00024.safetensors\nSHA256\
          \          EE9EB1BEEBC5FBA71DCF3124F82A41EB66F9B29583C4D9426A21D4367D1EF7C2\
          \       F:\\chat-goliath-120b-80k\\model-00011-of-00024.safetensors\nSHA256\
          \          EEE031C8611D3E7E4E65C22CAA18F4E27319E359A5EB215C932828CB647BB81D\
          \       F:\\chat-goliath-120b-80k\\model-00012-of-00024.safetensors\nSHA256\
          \          24B9486120B14560703E08D298A281FDD9035DE8DF3CBB528573C6C76E49B2EE\
          \       F:\\chat-goliath-120b-80k\\model-00013-of-00024.safetensors\nSHA256\
          \          A54D145D4BB95E10F0594F8FE7FE4EBC66DCC5CA3A238EDE7A4D8E33B4E16082\
          \       F:\\chat-goliath-120b-80k\\model-00014-of-00024.safetensors\nSHA256\
          \          0CAB89F83642A1553BEEB7965111C511BC05F59027C7D19DC9C853A6B422F85A\
          \       F:\\chat-goliath-120b-80k\\model-00015-of-00024.safetensors\nSHA256\
          \          CCBBDAD6FA6E95720D4B70801E38A2A04154E8787BB5D4A6024E59824EC2191D\
          \       F:\\chat-goliath-120b-80k\\model-00016-of-00024.safetensors\nSHA256\
          \          08137CCDA1B37C1EFF769EE894BB929FFD43EB7EE735386E626E88B1F187827C\
          \       F:\\chat-goliath-120b-80k\\model-00017-of-00024.safetensors\nSHA256\
          \          6ED11E748D85699980A3990658D0D3B07BD89E0B55DE755C884555BEE2C4A620\
          \       F:\\chat-goliath-120b-80k\\model-00018-of-00024.safetensors\nSHA256\
          \          39D879163BF2BF5B1A14906B707159A04C7E8381759A4E2837E517366F4B5D04\
          \       F:\\chat-goliath-120b-80k\\model-00019-of-00024.safetensors\nSHA256\
          \          4DF0A12B117B57D6D882B0C18894973474E0D7DCD46A1841E9C87E61FC94F4F4\
          \       F:\\chat-goliath-120b-80k\\model-00020-of-00024.safetensors\nSHA256\
          \          E5CAAA8E602F230CB752F0091E41B2FFCD2550B8C425325D3A3ABA2F6DC27209\
          \       F:\\chat-goliath-120b-80k\\model-00021-of-00024.safetensors\nSHA256\
          \          D41A1AF612EFC80D051A6487812327EC6D312B5BEA97310986B9AD97B70A3B2A\
          \       F:\\chat-goliath-120b-80k\\model-00022-of-00024.safetensors\nSHA256\
          \          5F9B8F2040BA5E448110BD9B4924CFAE3970309EAE32811FFB3F3641FE133A7F\
          \       F:\\chat-goliath-120b-80k\\model-00023-of-00024.safetensors\nSHA256\
          \          6BDDB8D1C02E4F406F5157BC9709D1B23AC094448C712F6EB3797DEF7ED42B9B\
          \       F:\\chat-goliath-120b-80k\\model-00024-of-00024.safetensors\nSHA256\
          \          E92E69756A90A7F7D90774BFA0A4B09D893E669BDFCFB810B5F3BC598DC52E7C\
          \       F:\\chat-goliath-120b-80k\\model.safetensors.index.json\nSHA256\
          \          8E16855E765EFB8FF6A8D05DCC1FC4E1C62C633E9503550EA2C462B336A96FC7\
          \       F:\\chat-goliath-120b-80k\\README.md\nSHA256          9921EDFD5322DD354EC44F825329E1C6C6B89B357BF3CE49377BE325B23EAA34\
          \       F:\\chat-goliath-120b-80k\\special_tokens_map.json\nSHA256     \
          \     79F2F33C690C49458F122090C1DFF223481CCF28162289407E8FA788074A330D \
          \      F:\\chat-goliath-120b-80k\\tokenizer_config.json\nSHA256        \
          \  4E28ED6CBB25942DCCBDBC000F414BC6075F580CC91E74C1AEC04D811CA60EAA    \
          \   F:\\chat-goliath-120b-80k\\tokenizer.json\nSHA256          D16FB023FB71C56B63E09C51631A3BC56297CAF04E60CEB1812614569DE3DBDD\
          \       F:\\chat-goliath-120b-80k\\tokenizer.model\n</code></pre>\n<p>However,\
          \ the quantization process also failed to yield consistent replies, like\
          \ the issues encountered in the previous steps.</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/3kAMfZdVx3MZ_V76VtXVj.png\"\
          ><img alt=\"ooba webui exl2 response.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/3kAMfZdVx3MZ_V76VtXVj.png\"\
          ></a></p>\n<p>We believe that we need your assistance. Thank you for your\
          \ time and consideration!</p>\n<p>Kind regards,<br>Fangru Shao</p>\n"
        raw: "Greetings!\r\n\r\nFirst thank you for developing and supporting Chinese\
          \ language capabilities with the Goliath model but we've encountered several\
          \ issues about quantization and functionality that we hope can be addressed.\r\
          \n\r\n1. Issues with Git LFS files during repository cloning:\r\n\r\nWe\
          \ attempted to clone the repository using the git clone command but faced\
          \ issues with the Git LFS files. Each file was only 1KB in size and contained\
          \ a description rather than the expected data.\r\n```\r\nPS E:\\> git clone\
          \ https://huggingface.co/hongyin/chat-goliath-120b-80k\r\nCloning into 'chat-goliath-120b-80k'...\r\
          \nremote: Enumerating objects: 65, done.\r\nremote: Counting objects: 100%\
          \ (62/62), done.\r\nremote: Compressing objects: 100% (61/61), done.\r\n\
          remote: Total 65 (delta 14), reused 0 (delta 0), pack-reused 3\r\nUnpacking\
          \ objects: 100% (65/65), 662.65 KiB | 215.00 KiB/s, done.\r\nPS E:\\> cd\
          \ .\\chat-goliath-120b-80k\\\r\nPS E:\\chat-goliath-120b-80k> cat .\\pytorch_model-00001-of-00024.bin\r\
          \nversion https://git-lfs.github.com/spec/v1\r\noid sha256:34aaef9a13c63f0fd5c14747ecab157bae1a9c948aaa60a543e27d083492f2c1\r\
          \nsize 9666282843\r\n```\r\nWe then directly downloaded the files, ensuring\
          \ their integrity through SHA256 checksums.\r\n\r\n2. Strange Metadata and\
          \ Incoherent Responses in Kobold.cpp:\r\n\r\nWe use llama.cpp (commit b3a7c20)\
          \ to quantize the model to Q2_K. \r\n```\r\n(llamacpp) PS F:\\llama.cpp\\\
          build\\bin\\Release> .\\quantize.exe F:\\chat-goliath-120b-80k-f16.gguf\
          \ F:\\chat-goliath-120b-80k-f16-q2_k.gguf Q2_K\r\nmain: build = 1768 (b3a7c20)\r\
          \nmain: built with MSVC 19.38.33133.0 for x64\r\nmain: quantizing 'F:\\\
          chat-goliath-120b-80k-f16.gguf' to 'F:\\chat-goliath-120b-80k-f16-q2_k.gguf'\
          \ as Q2_K\r\nllama_model_loader: loaded meta data with 21 key-value pairs\
          \ and 1236 tensors from F:\\chat-goliath-120b-80k-f16.gguf (version GGUF\
          \ V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note:\
          \ KV overrides do not apply in this output.\r\nllama_model_loader: - kv\
          \   0:                       general.architecture str              = llama\r\
          \nllama_model_loader: - kv   1:                               general.name\
          \ str              = F:\\\r\nllama_model_loader: - kv   2:             \
          \          llama.context_length u32              = 81920\r\nllama_model_loader:\
          \ - kv   3:                     llama.embedding_length u32             \
          \ = 8192\r\nllama_model_loader: - kv   4:                          llama.block_count\
          \ u32              = 137\r\nllama_model_loader: - kv   5:              \
          \    llama.feed_forward_length u32              = 28672\r\nllama_model_loader:\
          \ - kv   6:                 llama.rope.dimension_count u32             \
          \ = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count\
          \ u32              = 64\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv\
          \ u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
          \ f32              = 0.000010\r\nllama_model_loader: - kv  10:         \
          \              llama.rope.freq_base f32              = 10000.000000\r\n\
          llama_model_loader: - kv  11:                          general.file_type\
          \ u32              = 1\r\nllama_model_loader: - kv  12:                \
          \       tokenizer.ggml.model str              = llama\r\nllama_model_loader:\
          \ - kv  13:                      tokenizer.ggml.tokens arr[str,49300]  \
          \ = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader:\
          \ - kv  14:                      tokenizer.ggml.scores arr[f32,49300]  \
          \ = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv\
          \  15:                  tokenizer.ggml.token_type arr[i32,49300]   = [2,\
          \ 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:\
          \                      tokenizer.ggml.merges arr[str,71829]   = [\"\u6458\
          \ \u8981\", \"\u2581 \u300A\", \"\u4E2D \u56FD\", \"1 ...\r\nllama_model_loader:\
          \ - kv  17:                tokenizer.ggml.bos_token_id u32             \
          \ = 1\r\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id\
          \ u32              = 2\r\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id\
          \ u32              = 0\r\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id\
          \ u32              = 2\r\nllama_model_loader: - type  f32:  275 tensors\r\
          \nllama_model_loader: - type  f16:  961 tensors\r\nllama_model_quantize_internal:\
          \ meta size = 2310176 bytes\r\n...\r\nllama_model_quantize_internal: model\
          \ size  = 225133.22 MB\r\nllama_model_quantize_internal: quant size  = 47484.73\
          \ MB\r\n```\r\nUpon loading the files with Kobold.cpp, we observed oddities\
          \ in the metadata. The Q2_K parameter exhibited an irregular 3.37 BPW value,\
          \ whose value should usually be around 2.625 BPW.\r\n```\r\nllm_load_print_meta:\
          \ format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch     \
          \        = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta:\
          \ n_vocab          = 49300\r\nllm_load_print_meta: n_merges         = 0\r\
          \nllm_load_print_meta: n_ctx_train      = 81920\r\nllm_load_print_meta:\
          \ n_embd           = 8192\r\nllm_load_print_meta: n_head           = 64\r\
          \nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer\
          \          = 137\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta:\
          \ n_gqa            = 8\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\
          \nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta:\
          \ f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias =\
          \ 0.0e+00\r\nllm_load_print_meta: n_ff             = 28672\r\nllm_load_print_meta:\
          \ n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\n\
          llm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train\
          \  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta:\
          \ n_yarn_orig_ctx  = 81920\r\nllm_load_print_meta: rope_finetuned   = unknown\r\
          \nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model\
          \ ftype      = unknown, may not work\r\nllm_load_print_meta: model params\
          \     = 118.03 B\r\nllm_load_print_meta: model size       = 46.37 GiB (3.37\
          \ BPW)\r\nllm_load_print_meta: general.name     = F:\\\r\nllm_load_print_meta:\
          \ BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        =\
          \ 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta:\
          \ PAD token        = 2 '</s>'\r\nllm_load_print_meta: LF token         =\
          \ 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size       =    0.47 MiB\r\n\
          llm_load_tensors: using CUDA for GPU acceleration\r\nllm_load_tensors: system\
          \ memory used  =  126.85 MiB\r\nllm_load_tensors: VRAM used           =\
          \ 47358.35 MiB\r\nllm_load_tensors: offloading 137 repeating layers to GPU\r\
          \nllm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors:\
          \ offloaded 138/138 layers to GPU\r\n```\r\nThe model's responses were also\
          \ inconsistent and interspersed with a mix of Chinese and English, lacking\
          \ coherence and relevance.\r\n![kobldcpp gguf response 1.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/JgGjPStpchvv1AG1QRgOV.png)\r\
          \n![kobldcpp gguf response 2.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/_st9FUkas4TjIhjYUxwmO.png)\r\
          \n![kobldcpp gguf response 3.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/Xe3kj0c1LF1wB6yfegp6j.png)\r\
          \n\r\n3. Quantization Attempt in exl2:\r\n\r\nMoving forward, we attempted\
          \ to quantize the model in exl2. We first converted the model into .safetensors,\
          \ with our SHA256 for the .safetensors matching the expected values.\r\n\
          ```\r\nPS F:\\chat-goliath-120b-80k> Get-ChildItem -File | Get-FileHash\
          \ -Algorithm SHA256\r\n\r\nAlgorithm       Hash                        \
          \                                           Path\r\n---------       ----\
          \                                                                   ----\r\
          \nSHA256          D606246B713E31B1B43DFE913554DD47819089B9E44B9EBAA294EDA2589A6B4C\
          \       F:\\chat-goliath-120b-80k\\added_tokens.json\r\nSHA256         \
          \ EF58A0874EB14E647EF9DBF1B5A104701375FEE34F2D01BFD9795C984A07FDC0     \
          \  F:\\chat-goliath-120b-80k\\config.json\r\nSHA256          E9ADFCFDC532213AE59497E875478D95451D063640FB51755BC63C1070B9701D\
          \       F:\\chat-goliath-120b-80k\\generation_config.json\r\nSHA256    \
          \      3C9006082484A4A04AD7932BCAF0FAE22948CA063B4E1EF16A74C15D6E7F72CC\
          \       F:\\chat-goliath-120b-80k\\model-00001-of-00024.safetensors\r\n\
          SHA256          60B3F61167C18CFE8968A3FC377D3DF7D14051595CD3044E48549580387B1A42\
          \       F:\\chat-goliath-120b-80k\\model-00002-of-00024.safetensors\r\n\
          SHA256          358A58059687D8DEF130B2E4102DD596825255DA51F18DEAE277921EE6DA28B2\
          \       F:\\chat-goliath-120b-80k\\model-00003-of-00024.safetensors\r\n\
          SHA256          7D7E35B80E4AC0B59E2BAB33F0F8E6FE5F8D2DD36983EF36BCA1E2628C20264E\
          \       F:\\chat-goliath-120b-80k\\model-00004-of-00024.safetensors\r\n\
          SHA256          9CCB7212C979E15229DDBEFF6992E4A30C3D3AD1F1FFE571D14F8EB4721398EE\
          \       F:\\chat-goliath-120b-80k\\model-00005-of-00024.safetensors\r\n\
          SHA256          6753B8281079AC0927C0FB4E4AD9877F57E1BE40E4B5840ABB36F95BE8DB35D7\
          \       F:\\chat-goliath-120b-80k\\model-00006-of-00024.safetensors\r\n\
          SHA256          32BCF2B2BBF3E3BD052239158D14D0693142B6A3D5B014EA0C626C686C42AFA7\
          \       F:\\chat-goliath-120b-80k\\model-00007-of-00024.safetensors\r\n\
          SHA256          051B77A0CD28F54B4AE6D2087257CF4D2C7198C245D9031312582B2AEEEF143C\
          \       F:\\chat-goliath-120b-80k\\model-00008-of-00024.safetensors\r\n\
          SHA256          EFC8528F9FE25EAB6D72F7E84D9CADCD13CA998F179A8AFD6D7521A7F8BCAE1B\
          \       F:\\chat-goliath-120b-80k\\model-00009-of-00024.safetensors\r\n\
          SHA256          109643CE4559EC438CF0D1C8326CE5B32E9DFF4A35AC61EAF149CA1C0B27731C\
          \       F:\\chat-goliath-120b-80k\\model-00010-of-00024.safetensors\r\n\
          SHA256          EE9EB1BEEBC5FBA71DCF3124F82A41EB66F9B29583C4D9426A21D4367D1EF7C2\
          \       F:\\chat-goliath-120b-80k\\model-00011-of-00024.safetensors\r\n\
          SHA256          EEE031C8611D3E7E4E65C22CAA18F4E27319E359A5EB215C932828CB647BB81D\
          \       F:\\chat-goliath-120b-80k\\model-00012-of-00024.safetensors\r\n\
          SHA256          24B9486120B14560703E08D298A281FDD9035DE8DF3CBB528573C6C76E49B2EE\
          \       F:\\chat-goliath-120b-80k\\model-00013-of-00024.safetensors\r\n\
          SHA256          A54D145D4BB95E10F0594F8FE7FE4EBC66DCC5CA3A238EDE7A4D8E33B4E16082\
          \       F:\\chat-goliath-120b-80k\\model-00014-of-00024.safetensors\r\n\
          SHA256          0CAB89F83642A1553BEEB7965111C511BC05F59027C7D19DC9C853A6B422F85A\
          \       F:\\chat-goliath-120b-80k\\model-00015-of-00024.safetensors\r\n\
          SHA256          CCBBDAD6FA6E95720D4B70801E38A2A04154E8787BB5D4A6024E59824EC2191D\
          \       F:\\chat-goliath-120b-80k\\model-00016-of-00024.safetensors\r\n\
          SHA256          08137CCDA1B37C1EFF769EE894BB929FFD43EB7EE735386E626E88B1F187827C\
          \       F:\\chat-goliath-120b-80k\\model-00017-of-00024.safetensors\r\n\
          SHA256          6ED11E748D85699980A3990658D0D3B07BD89E0B55DE755C884555BEE2C4A620\
          \       F:\\chat-goliath-120b-80k\\model-00018-of-00024.safetensors\r\n\
          SHA256          39D879163BF2BF5B1A14906B707159A04C7E8381759A4E2837E517366F4B5D04\
          \       F:\\chat-goliath-120b-80k\\model-00019-of-00024.safetensors\r\n\
          SHA256          4DF0A12B117B57D6D882B0C18894973474E0D7DCD46A1841E9C87E61FC94F4F4\
          \       F:\\chat-goliath-120b-80k\\model-00020-of-00024.safetensors\r\n\
          SHA256          E5CAAA8E602F230CB752F0091E41B2FFCD2550B8C425325D3A3ABA2F6DC27209\
          \       F:\\chat-goliath-120b-80k\\model-00021-of-00024.safetensors\r\n\
          SHA256          D41A1AF612EFC80D051A6487812327EC6D312B5BEA97310986B9AD97B70A3B2A\
          \       F:\\chat-goliath-120b-80k\\model-00022-of-00024.safetensors\r\n\
          SHA256          5F9B8F2040BA5E448110BD9B4924CFAE3970309EAE32811FFB3F3641FE133A7F\
          \       F:\\chat-goliath-120b-80k\\model-00023-of-00024.safetensors\r\n\
          SHA256          6BDDB8D1C02E4F406F5157BC9709D1B23AC094448C712F6EB3797DEF7ED42B9B\
          \       F:\\chat-goliath-120b-80k\\model-00024-of-00024.safetensors\r\n\
          SHA256          E92E69756A90A7F7D90774BFA0A4B09D893E669BDFCFB810B5F3BC598DC52E7C\
          \       F:\\chat-goliath-120b-80k\\model.safetensors.index.json\r\nSHA256\
          \          8E16855E765EFB8FF6A8D05DCC1FC4E1C62C633E9503550EA2C462B336A96FC7\
          \       F:\\chat-goliath-120b-80k\\README.md\r\nSHA256          9921EDFD5322DD354EC44F825329E1C6C6B89B357BF3CE49377BE325B23EAA34\
          \       F:\\chat-goliath-120b-80k\\special_tokens_map.json\r\nSHA256   \
          \       79F2F33C690C49458F122090C1DFF223481CCF28162289407E8FA788074A330D\
          \       F:\\chat-goliath-120b-80k\\tokenizer_config.json\r\nSHA256     \
          \     4E28ED6CBB25942DCCBDBC000F414BC6075F580CC91E74C1AEC04D811CA60EAA \
          \      F:\\chat-goliath-120b-80k\\tokenizer.json\r\nSHA256          D16FB023FB71C56B63E09C51631A3BC56297CAF04E60CEB1812614569DE3DBDD\
          \       F:\\chat-goliath-120b-80k\\tokenizer.model\r\n```\r\nHowever, the\
          \ quantization process also failed to yield consistent replies, like the\
          \ issues encountered in the previous steps.\r\n\r\n![ooba webui exl2 response.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/3kAMfZdVx3MZ_V76VtXVj.png)\r\
          \n\r\nWe believe that we need your assistance. Thank you for your time and\
          \ consideration!\r\n\r\nKind regards,\r\nFangru Shao"
        updatedAt: '2024-01-05T09:27:45.247Z'
      numEdits: 0
      reactions: []
    id: 6597cb916571a30108790c57
    type: comment
  author: MatrixC7
  content: "Greetings!\r\n\r\nFirst thank you for developing and supporting Chinese\
    \ language capabilities with the Goliath model but we've encountered several issues\
    \ about quantization and functionality that we hope can be addressed.\r\n\r\n\
    1. Issues with Git LFS files during repository cloning:\r\n\r\nWe attempted to\
    \ clone the repository using the git clone command but faced issues with the Git\
    \ LFS files. Each file was only 1KB in size and contained a description rather\
    \ than the expected data.\r\n```\r\nPS E:\\> git clone https://huggingface.co/hongyin/chat-goliath-120b-80k\r\
    \nCloning into 'chat-goliath-120b-80k'...\r\nremote: Enumerating objects: 65,\
    \ done.\r\nremote: Counting objects: 100% (62/62), done.\r\nremote: Compressing\
    \ objects: 100% (61/61), done.\r\nremote: Total 65 (delta 14), reused 0 (delta\
    \ 0), pack-reused 3\r\nUnpacking objects: 100% (65/65), 662.65 KiB | 215.00 KiB/s,\
    \ done.\r\nPS E:\\> cd .\\chat-goliath-120b-80k\\\r\nPS E:\\chat-goliath-120b-80k>\
    \ cat .\\pytorch_model-00001-of-00024.bin\r\nversion https://git-lfs.github.com/spec/v1\r\
    \noid sha256:34aaef9a13c63f0fd5c14747ecab157bae1a9c948aaa60a543e27d083492f2c1\r\
    \nsize 9666282843\r\n```\r\nWe then directly downloaded the files, ensuring their\
    \ integrity through SHA256 checksums.\r\n\r\n2. Strange Metadata and Incoherent\
    \ Responses in Kobold.cpp:\r\n\r\nWe use llama.cpp (commit b3a7c20) to quantize\
    \ the model to Q2_K. \r\n```\r\n(llamacpp) PS F:\\llama.cpp\\build\\bin\\Release>\
    \ .\\quantize.exe F:\\chat-goliath-120b-80k-f16.gguf F:\\chat-goliath-120b-80k-f16-q2_k.gguf\
    \ Q2_K\r\nmain: build = 1768 (b3a7c20)\r\nmain: built with MSVC 19.38.33133.0\
    \ for x64\r\nmain: quantizing 'F:\\chat-goliath-120b-80k-f16.gguf' to 'F:\\chat-goliath-120b-80k-f16-q2_k.gguf'\
    \ as Q2_K\r\nllama_model_loader: loaded meta data with 21 key-value pairs and\
    \ 1236 tensors from F:\\chat-goliath-120b-80k-f16.gguf (version GGUF V3 (latest))\r\
    \nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not\
    \ apply in this output.\r\nllama_model_loader: - kv   0:                     \
    \  general.architecture str              = llama\r\nllama_model_loader: - kv \
    \  1:                               general.name str              = F:\\\r\nllama_model_loader:\
    \ - kv   2:                       llama.context_length u32              = 81920\r\
    \nllama_model_loader: - kv   3:                     llama.embedding_length u32\
    \              = 8192\r\nllama_model_loader: - kv   4:                       \
    \   llama.block_count u32              = 137\r\nllama_model_loader: - kv   5:\
    \                  llama.feed_forward_length u32              = 28672\r\nllama_model_loader:\
    \ - kv   6:                 llama.rope.dimension_count u32              = 128\r\
    \nllama_model_loader: - kv   7:                 llama.attention.head_count u32\
    \              = 64\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv\
    \ u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
    \ f32              = 0.000010\r\nllama_model_loader: - kv  10:               \
    \        llama.rope.freq_base f32              = 10000.000000\r\nllama_model_loader:\
    \ - kv  11:                          general.file_type u32              = 1\r\n\
    llama_model_loader: - kv  12:                       tokenizer.ggml.model str \
    \             = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens\
    \ arr[str,49300]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader:\
    \ - kv  14:                      tokenizer.ggml.scores arr[f32,49300]   = [0.000000,\
    \ 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:             \
    \     tokenizer.ggml.token_type arr[i32,49300]   = [2, 3, 3, 6, 6, 6, 6, 6, 6,\
    \ 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges\
    \ arr[str,71829]   = [\"\u6458 \u8981\", \"\u2581 \u300A\", \"\u4E2D \u56FD\"\
    , \"1 ...\r\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id\
    \ u32              = 1\r\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id\
    \ u32              = 2\r\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id\
    \ u32              = 0\r\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id\
    \ u32              = 2\r\nllama_model_loader: - type  f32:  275 tensors\r\nllama_model_loader:\
    \ - type  f16:  961 tensors\r\nllama_model_quantize_internal: meta size = 2310176\
    \ bytes\r\n...\r\nllama_model_quantize_internal: model size  = 225133.22 MB\r\n\
    llama_model_quantize_internal: quant size  = 47484.73 MB\r\n```\r\nUpon loading\
    \ the files with Kobold.cpp, we observed oddities in the metadata. The Q2_K parameter\
    \ exhibited an irregular 3.37 BPW value, whose value should usually be around\
    \ 2.625 BPW.\r\n```\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\
    \nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab\
    \ type       = SPM\r\nllm_load_print_meta: n_vocab          = 49300\r\nllm_load_print_meta:\
    \ n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 81920\r\nllm_load_print_meta:\
    \ n_embd           = 8192\r\nllm_load_print_meta: n_head           = 64\r\nllm_load_print_meta:\
    \ n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 137\r\nllm_load_print_meta:\
    \ n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 8\r\nllm_load_print_meta:\
    \ f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\
    \nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias\
    \ = 0.0e+00\r\nllm_load_print_meta: n_ff             = 28672\r\nllm_load_print_meta:\
    \ n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta:\
    \ rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\
    \nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx\
    \  = 81920\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta:\
    \ model type       = ?B\r\nllm_load_print_meta: model ftype      = unknown, may\
    \ not work\r\nllm_load_print_meta: model params     = 118.03 B\r\nllm_load_print_meta:\
    \ model size       = 46.37 GiB (3.37 BPW)\r\nllm_load_print_meta: general.name\
    \     = F:\\\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta:\
    \ EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\
    \nllm_load_print_meta: PAD token        = 2 '</s>'\r\nllm_load_print_meta: LF\
    \ token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size       =    0.47\
    \ MiB\r\nllm_load_tensors: using CUDA for GPU acceleration\r\nllm_load_tensors:\
    \ system memory used  =  126.85 MiB\r\nllm_load_tensors: VRAM used           =\
    \ 47358.35 MiB\r\nllm_load_tensors: offloading 137 repeating layers to GPU\r\n\
    llm_load_tensors: offloading non-repeating layers to GPU\r\nllm_load_tensors:\
    \ offloaded 138/138 layers to GPU\r\n```\r\nThe model's responses were also inconsistent\
    \ and interspersed with a mix of Chinese and English, lacking coherence and relevance.\r\
    \n![kobldcpp gguf response 1.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/JgGjPStpchvv1AG1QRgOV.png)\r\
    \n![kobldcpp gguf response 2.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/_st9FUkas4TjIhjYUxwmO.png)\r\
    \n![kobldcpp gguf response 3.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/Xe3kj0c1LF1wB6yfegp6j.png)\r\
    \n\r\n3. Quantization Attempt in exl2:\r\n\r\nMoving forward, we attempted to\
    \ quantize the model in exl2. We first converted the model into .safetensors,\
    \ with our SHA256 for the .safetensors matching the expected values.\r\n```\r\n\
    PS F:\\chat-goliath-120b-80k> Get-ChildItem -File | Get-FileHash -Algorithm SHA256\r\
    \n\r\nAlgorithm       Hash                                                   \
    \                Path\r\n---------       ----                                \
    \                                   ----\r\nSHA256          D606246B713E31B1B43DFE913554DD47819089B9E44B9EBAA294EDA2589A6B4C\
    \       F:\\chat-goliath-120b-80k\\added_tokens.json\r\nSHA256          EF58A0874EB14E647EF9DBF1B5A104701375FEE34F2D01BFD9795C984A07FDC0\
    \       F:\\chat-goliath-120b-80k\\config.json\r\nSHA256          E9ADFCFDC532213AE59497E875478D95451D063640FB51755BC63C1070B9701D\
    \       F:\\chat-goliath-120b-80k\\generation_config.json\r\nSHA256          3C9006082484A4A04AD7932BCAF0FAE22948CA063B4E1EF16A74C15D6E7F72CC\
    \       F:\\chat-goliath-120b-80k\\model-00001-of-00024.safetensors\r\nSHA256\
    \          60B3F61167C18CFE8968A3FC377D3DF7D14051595CD3044E48549580387B1A42  \
    \     F:\\chat-goliath-120b-80k\\model-00002-of-00024.safetensors\r\nSHA256  \
    \        358A58059687D8DEF130B2E4102DD596825255DA51F18DEAE277921EE6DA28B2    \
    \   F:\\chat-goliath-120b-80k\\model-00003-of-00024.safetensors\r\nSHA256    \
    \      7D7E35B80E4AC0B59E2BAB33F0F8E6FE5F8D2DD36983EF36BCA1E2628C20264E      \
    \ F:\\chat-goliath-120b-80k\\model-00004-of-00024.safetensors\r\nSHA256      \
    \    9CCB7212C979E15229DDBEFF6992E4A30C3D3AD1F1FFE571D14F8EB4721398EE       F:\\\
    chat-goliath-120b-80k\\model-00005-of-00024.safetensors\r\nSHA256          6753B8281079AC0927C0FB4E4AD9877F57E1BE40E4B5840ABB36F95BE8DB35D7\
    \       F:\\chat-goliath-120b-80k\\model-00006-of-00024.safetensors\r\nSHA256\
    \          32BCF2B2BBF3E3BD052239158D14D0693142B6A3D5B014EA0C626C686C42AFA7  \
    \     F:\\chat-goliath-120b-80k\\model-00007-of-00024.safetensors\r\nSHA256  \
    \        051B77A0CD28F54B4AE6D2087257CF4D2C7198C245D9031312582B2AEEEF143C    \
    \   F:\\chat-goliath-120b-80k\\model-00008-of-00024.safetensors\r\nSHA256    \
    \      EFC8528F9FE25EAB6D72F7E84D9CADCD13CA998F179A8AFD6D7521A7F8BCAE1B      \
    \ F:\\chat-goliath-120b-80k\\model-00009-of-00024.safetensors\r\nSHA256      \
    \    109643CE4559EC438CF0D1C8326CE5B32E9DFF4A35AC61EAF149CA1C0B27731C       F:\\\
    chat-goliath-120b-80k\\model-00010-of-00024.safetensors\r\nSHA256          EE9EB1BEEBC5FBA71DCF3124F82A41EB66F9B29583C4D9426A21D4367D1EF7C2\
    \       F:\\chat-goliath-120b-80k\\model-00011-of-00024.safetensors\r\nSHA256\
    \          EEE031C8611D3E7E4E65C22CAA18F4E27319E359A5EB215C932828CB647BB81D  \
    \     F:\\chat-goliath-120b-80k\\model-00012-of-00024.safetensors\r\nSHA256  \
    \        24B9486120B14560703E08D298A281FDD9035DE8DF3CBB528573C6C76E49B2EE    \
    \   F:\\chat-goliath-120b-80k\\model-00013-of-00024.safetensors\r\nSHA256    \
    \      A54D145D4BB95E10F0594F8FE7FE4EBC66DCC5CA3A238EDE7A4D8E33B4E16082      \
    \ F:\\chat-goliath-120b-80k\\model-00014-of-00024.safetensors\r\nSHA256      \
    \    0CAB89F83642A1553BEEB7965111C511BC05F59027C7D19DC9C853A6B422F85A       F:\\\
    chat-goliath-120b-80k\\model-00015-of-00024.safetensors\r\nSHA256          CCBBDAD6FA6E95720D4B70801E38A2A04154E8787BB5D4A6024E59824EC2191D\
    \       F:\\chat-goliath-120b-80k\\model-00016-of-00024.safetensors\r\nSHA256\
    \          08137CCDA1B37C1EFF769EE894BB929FFD43EB7EE735386E626E88B1F187827C  \
    \     F:\\chat-goliath-120b-80k\\model-00017-of-00024.safetensors\r\nSHA256  \
    \        6ED11E748D85699980A3990658D0D3B07BD89E0B55DE755C884555BEE2C4A620    \
    \   F:\\chat-goliath-120b-80k\\model-00018-of-00024.safetensors\r\nSHA256    \
    \      39D879163BF2BF5B1A14906B707159A04C7E8381759A4E2837E517366F4B5D04      \
    \ F:\\chat-goliath-120b-80k\\model-00019-of-00024.safetensors\r\nSHA256      \
    \    4DF0A12B117B57D6D882B0C18894973474E0D7DCD46A1841E9C87E61FC94F4F4       F:\\\
    chat-goliath-120b-80k\\model-00020-of-00024.safetensors\r\nSHA256          E5CAAA8E602F230CB752F0091E41B2FFCD2550B8C425325D3A3ABA2F6DC27209\
    \       F:\\chat-goliath-120b-80k\\model-00021-of-00024.safetensors\r\nSHA256\
    \          D41A1AF612EFC80D051A6487812327EC6D312B5BEA97310986B9AD97B70A3B2A  \
    \     F:\\chat-goliath-120b-80k\\model-00022-of-00024.safetensors\r\nSHA256  \
    \        5F9B8F2040BA5E448110BD9B4924CFAE3970309EAE32811FFB3F3641FE133A7F    \
    \   F:\\chat-goliath-120b-80k\\model-00023-of-00024.safetensors\r\nSHA256    \
    \      6BDDB8D1C02E4F406F5157BC9709D1B23AC094448C712F6EB3797DEF7ED42B9B      \
    \ F:\\chat-goliath-120b-80k\\model-00024-of-00024.safetensors\r\nSHA256      \
    \    E92E69756A90A7F7D90774BFA0A4B09D893E669BDFCFB810B5F3BC598DC52E7C       F:\\\
    chat-goliath-120b-80k\\model.safetensors.index.json\r\nSHA256          8E16855E765EFB8FF6A8D05DCC1FC4E1C62C633E9503550EA2C462B336A96FC7\
    \       F:\\chat-goliath-120b-80k\\README.md\r\nSHA256          9921EDFD5322DD354EC44F825329E1C6C6B89B357BF3CE49377BE325B23EAA34\
    \       F:\\chat-goliath-120b-80k\\special_tokens_map.json\r\nSHA256         \
    \ 79F2F33C690C49458F122090C1DFF223481CCF28162289407E8FA788074A330D       F:\\\
    chat-goliath-120b-80k\\tokenizer_config.json\r\nSHA256          4E28ED6CBB25942DCCBDBC000F414BC6075F580CC91E74C1AEC04D811CA60EAA\
    \       F:\\chat-goliath-120b-80k\\tokenizer.json\r\nSHA256          D16FB023FB71C56B63E09C51631A3BC56297CAF04E60CEB1812614569DE3DBDD\
    \       F:\\chat-goliath-120b-80k\\tokenizer.model\r\n```\r\nHowever, the quantization\
    \ process also failed to yield consistent replies, like the issues encountered\
    \ in the previous steps.\r\n\r\n![ooba webui exl2 response.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/3kAMfZdVx3MZ_V76VtXVj.png)\r\
    \n\r\nWe believe that we need your assistance. Thank you for your time and consideration!\r\
    \n\r\nKind regards,\r\nFangru Shao"
  created_at: 2024-01-05 09:27:45+00:00
  edited: false
  hidden: false
  id: 6597cb916571a30108790c57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64265252a5ec4a5cbc52fe48/M0t275nR0Vw79bTORMpu9.png?w=200&h=200&f=face
      fullname: hongyin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: hongyin
      type: user
    createdAt: '2024-01-05T16:19:49.000Z'
    data:
      edited: false
      editors:
      - hongyin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4796847701072693
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64265252a5ec4a5cbc52fe48/M0t275nR0Vw79bTORMpu9.png?w=200&h=200&f=face
          fullname: hongyin
          isHf: false
          isPro: false
          name: hongyin
          type: user
        html: "<blockquote>\n<p>Greetings!</p>\n<p>First thank you for developing\
          \ and supporting Chinese language capabilities with the Goliath model but\
          \ we've encountered several issues about quantization and functionality\
          \ that we hope can be addressed.</p>\n<ol>\n<li>Issues with Git LFS files\
          \ during repository cloning:</li>\n</ol>\n<p>We attempted to clone the repository\
          \ using the git clone command but faced issues with the Git LFS files. Each\
          \ file was only 1KB in size and contained a description rather than the\
          \ expected data.</p>\n<pre><code>PS E:\\&gt; git clone https://huggingface.co/hongyin/chat-goliath-120b-80k\n\
          Cloning into 'chat-goliath-120b-80k'...\nremote: Enumerating objects: 65,\
          \ done.\nremote: Counting objects: 100% (62/62), done.\nremote: Compressing\
          \ objects: 100% (61/61), done.\nremote: Total 65 (delta 14), reused 0 (delta\
          \ 0), pack-reused 3\nUnpacking objects: 100% (65/65), 662.65 KiB | 215.00\
          \ KiB/s, done.\nPS E:\\&gt; cd .\\chat-goliath-120b-80k\\\nPS E:\\chat-goliath-120b-80k&gt;\
          \ cat .\\pytorch_model-00001-of-00024.bin\nversion https://git-lfs.github.com/spec/v1\n\
          oid sha256:34aaef9a13c63f0fd5c14747ecab157bae1a9c948aaa60a543e27d083492f2c1\n\
          size 9666282843\n</code></pre>\n<p>We then directly downloaded the files,\
          \ ensuring their integrity through SHA256 checksums.</p>\n<ol start=\"2\"\
          >\n<li>Strange Metadata and Incoherent Responses in Kobold.cpp:</li>\n</ol>\n\
          <p>We use llama.cpp (commit b3a7c20) to quantize the model to Q2_K. </p>\n\
          <pre><code>(llamacpp) PS F:\\llama.cpp\\build\\bin\\Release&gt; .\\quantize.exe\
          \ F:\\chat-goliath-120b-80k-f16.gguf F:\\chat-goliath-120b-80k-f16-q2_k.gguf\
          \ Q2_K\nmain: build = 1768 (b3a7c20)\nmain: built with MSVC 19.38.33133.0\
          \ for x64\nmain: quantizing 'F:\\chat-goliath-120b-80k-f16.gguf' to 'F:\\\
          chat-goliath-120b-80k-f16-q2_k.gguf' as Q2_K\nllama_model_loader: loaded\
          \ meta data with 21 key-value pairs and 1236 tensors from F:\\chat-goliath-120b-80k-f16.gguf\
          \ (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values.\
          \ Note: KV overrides do not apply in this output.\nllama_model_loader: -\
          \ kv   0:                       general.architecture str              =\
          \ llama\nllama_model_loader: - kv   1:                               general.name\
          \ str              = F:\\\nllama_model_loader: - kv   2:               \
          \        llama.context_length u32              = 81920\nllama_model_loader:\
          \ - kv   3:                     llama.embedding_length u32             \
          \ = 8192\nllama_model_loader: - kv   4:                          llama.block_count\
          \ u32              = 137\nllama_model_loader: - kv   5:                \
          \  llama.feed_forward_length u32              = 28672\nllama_model_loader:\
          \ - kv   6:                 llama.rope.dimension_count u32             \
          \ = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count\
          \ u32              = 64\nllama_model_loader: - kv   8:              llama.attention.head_count_kv\
          \ u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
          \ f32              = 0.000010\nllama_model_loader: - kv  10:           \
          \            llama.rope.freq_base f32              = 10000.000000\nllama_model_loader:\
          \ - kv  11:                          general.file_type u32             \
          \ = 1\nllama_model_loader: - kv  12:                       tokenizer.ggml.model\
          \ str              = llama\nllama_model_loader: - kv  13:              \
          \        tokenizer.ggml.tokens arr[str,49300]   = [\"&lt;unk&gt;\", \"&lt;s&gt;\"\
          , \"&lt;/s&gt;\", \"&lt;0x00&gt;\", \"&lt;...\nllama_model_loader: - kv\
          \  14:                      tokenizer.ggml.scores arr[f32,49300]   = [0.000000,\
          \ 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:         \
          \         tokenizer.ggml.token_type arr[i32,49300]   = [2, 3, 3, 6, 6, 6,\
          \ 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                 \
          \     tokenizer.ggml.merges arr[str,71829]   = [\"\u6458 \u8981\", \"\u2581\
          \ \u300A\", \"\u4E2D \u56FD\", \"1 ...\nllama_model_loader: - kv  17:  \
          \              tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader:\
          \ - kv  18:                tokenizer.ggml.eos_token_id u32             \
          \ = 2\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id\
          \ u32              = 0\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id\
          \ u32              = 2\nllama_model_loader: - type  f32:  275 tensors\n\
          llama_model_loader: - type  f16:  961 tensors\nllama_model_quantize_internal:\
          \ meta size = 2310176 bytes\n...\nllama_model_quantize_internal: model size\
          \  = 225133.22 MB\nllama_model_quantize_internal: quant size  = 47484.73\
          \ MB\n</code></pre>\n<p>Upon loading the files with Kobold.cpp, we observed\
          \ oddities in the metadata. The Q2_K parameter exhibited an irregular 3.37\
          \ BPW value, whose value should usually be around 2.625 BPW.</p>\n<pre><code>llm_load_print_meta:\
          \ format           = GGUF V3 (latest)\nllm_load_print_meta: arch       \
          \      = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta:\
          \ n_vocab          = 49300\nllm_load_print_meta: n_merges         = 0\n\
          llm_load_print_meta: n_ctx_train      = 81920\nllm_load_print_meta: n_embd\
          \           = 8192\nllm_load_print_meta: n_head           = 64\nllm_load_print_meta:\
          \ n_head_kv        = 8\nllm_load_print_meta: n_layer          = 137\nllm_load_print_meta:\
          \ n_rot            = 128\nllm_load_print_meta: n_gqa            = 8\nllm_load_print_meta:\
          \ f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n\
          llm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias\
          \ = 0.0e+00\nllm_load_print_meta: n_ff             = 28672\nllm_load_print_meta:\
          \ n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta:\
          \ rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\n\
          llm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx\
          \  = 81920\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta:\
          \ model type       = ?B\nllm_load_print_meta: model ftype      = unknown,\
          \ may not work\nllm_load_print_meta: model params     = 118.03 B\nllm_load_print_meta:\
          \ model size       = 46.37 GiB (3.37 BPW)\nllm_load_print_meta: general.name\
          \     = F:\\\nllm_load_print_meta: BOS token        = 1 '&lt;s&gt;'\nllm_load_print_meta:\
          \ EOS token        = 2 '&lt;/s&gt;'\nllm_load_print_meta: UNK token    \
          \    = 0 '&lt;unk&gt;'\nllm_load_print_meta: PAD token        = 2 '&lt;/s&gt;'\n\
          llm_load_print_meta: LF token         = 13 '&lt;0x0A&gt;'\nllm_load_tensors:\
          \ ggml ctx size       =    0.47 MiB\nllm_load_tensors: using CUDA for GPU\
          \ acceleration\nllm_load_tensors: system memory used  =  126.85 MiB\nllm_load_tensors:\
          \ VRAM used           = 47358.35 MiB\nllm_load_tensors: offloading 137 repeating\
          \ layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\n\
          llm_load_tensors: offloaded 138/138 layers to GPU\n</code></pre>\n<p>The\
          \ model's responses were also inconsistent and interspersed with a mix of\
          \ Chinese and English, lacking coherence and relevance.<br><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/JgGjPStpchvv1AG1QRgOV.png\"\
          ><img alt=\"kobldcpp gguf response 1.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/JgGjPStpchvv1AG1QRgOV.png\"\
          ></a><br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/_st9FUkas4TjIhjYUxwmO.png\"\
          ><img alt=\"kobldcpp gguf response 2.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/_st9FUkas4TjIhjYUxwmO.png\"\
          ></a><br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/Xe3kj0c1LF1wB6yfegp6j.png\"\
          ><img alt=\"kobldcpp gguf response 3.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/Xe3kj0c1LF1wB6yfegp6j.png\"\
          ></a></p>\n<ol start=\"3\">\n<li>Quantization Attempt in exl2:</li>\n</ol>\n\
          <p>Moving forward, we attempted to quantize the model in exl2. We first\
          \ converted the model into .safetensors, with our SHA256 for the .safetensors\
          \ matching the expected values.</p>\n<pre><code>PS F:\\chat-goliath-120b-80k&gt;\
          \ Get-ChildItem -File | Get-FileHash -Algorithm SHA256\n\nAlgorithm    \
          \   Hash                                                               \
          \    Path\n---------       ----                                        \
          \                           ----\nSHA256          D606246B713E31B1B43DFE913554DD47819089B9E44B9EBAA294EDA2589A6B4C\
          \       F:\\chat-goliath-120b-80k\\added_tokens.json\nSHA256          EF58A0874EB14E647EF9DBF1B5A104701375FEE34F2D01BFD9795C984A07FDC0\
          \       F:\\chat-goliath-120b-80k\\config.json\nSHA256          E9ADFCFDC532213AE59497E875478D95451D063640FB51755BC63C1070B9701D\
          \       F:\\chat-goliath-120b-80k\\generation_config.json\nSHA256      \
          \    3C9006082484A4A04AD7932BCAF0FAE22948CA063B4E1EF16A74C15D6E7F72CC  \
          \     F:\\chat-goliath-120b-80k\\model-00001-of-00024.safetensors\nSHA256\
          \          60B3F61167C18CFE8968A3FC377D3DF7D14051595CD3044E48549580387B1A42\
          \       F:\\chat-goliath-120b-80k\\model-00002-of-00024.safetensors\nSHA256\
          \          358A58059687D8DEF130B2E4102DD596825255DA51F18DEAE277921EE6DA28B2\
          \       F:\\chat-goliath-120b-80k\\model-00003-of-00024.safetensors\nSHA256\
          \          7D7E35B80E4AC0B59E2BAB33F0F8E6FE5F8D2DD36983EF36BCA1E2628C20264E\
          \       F:\\chat-goliath-120b-80k\\model-00004-of-00024.safetensors\nSHA256\
          \          9CCB7212C979E15229DDBEFF6992E4A30C3D3AD1F1FFE571D14F8EB4721398EE\
          \       F:\\chat-goliath-120b-80k\\model-00005-of-00024.safetensors\nSHA256\
          \          6753B8281079AC0927C0FB4E4AD9877F57E1BE40E4B5840ABB36F95BE8DB35D7\
          \       F:\\chat-goliath-120b-80k\\model-00006-of-00024.safetensors\nSHA256\
          \          32BCF2B2BBF3E3BD052239158D14D0693142B6A3D5B014EA0C626C686C42AFA7\
          \       F:\\chat-goliath-120b-80k\\model-00007-of-00024.safetensors\nSHA256\
          \          051B77A0CD28F54B4AE6D2087257CF4D2C7198C245D9031312582B2AEEEF143C\
          \       F:\\chat-goliath-120b-80k\\model-00008-of-00024.safetensors\nSHA256\
          \          EFC8528F9FE25EAB6D72F7E84D9CADCD13CA998F179A8AFD6D7521A7F8BCAE1B\
          \       F:\\chat-goliath-120b-80k\\model-00009-of-00024.safetensors\nSHA256\
          \          109643CE4559EC438CF0D1C8326CE5B32E9DFF4A35AC61EAF149CA1C0B27731C\
          \       F:\\chat-goliath-120b-80k\\model-00010-of-00024.safetensors\nSHA256\
          \          EE9EB1BEEBC5FBA71DCF3124F82A41EB66F9B29583C4D9426A21D4367D1EF7C2\
          \       F:\\chat-goliath-120b-80k\\model-00011-of-00024.safetensors\nSHA256\
          \          EEE031C8611D3E7E4E65C22CAA18F4E27319E359A5EB215C932828CB647BB81D\
          \       F:\\chat-goliath-120b-80k\\model-00012-of-00024.safetensors\nSHA256\
          \          24B9486120B14560703E08D298A281FDD9035DE8DF3CBB528573C6C76E49B2EE\
          \       F:\\chat-goliath-120b-80k\\model-00013-of-00024.safetensors\nSHA256\
          \          A54D145D4BB95E10F0594F8FE7FE4EBC66DCC5CA3A238EDE7A4D8E33B4E16082\
          \       F:\\chat-goliath-120b-80k\\model-00014-of-00024.safetensors\nSHA256\
          \          0CAB89F83642A1553BEEB7965111C511BC05F59027C7D19DC9C853A6B422F85A\
          \       F:\\chat-goliath-120b-80k\\model-00015-of-00024.safetensors\nSHA256\
          \          CCBBDAD6FA6E95720D4B70801E38A2A04154E8787BB5D4A6024E59824EC2191D\
          \       F:\\chat-goliath-120b-80k\\model-00016-of-00024.safetensors\nSHA256\
          \          08137CCDA1B37C1EFF769EE894BB929FFD43EB7EE735386E626E88B1F187827C\
          \       F:\\chat-goliath-120b-80k\\model-00017-of-00024.safetensors\nSHA256\
          \          6ED11E748D85699980A3990658D0D3B07BD89E0B55DE755C884555BEE2C4A620\
          \       F:\\chat-goliath-120b-80k\\model-00018-of-00024.safetensors\nSHA256\
          \          39D879163BF2BF5B1A14906B707159A04C7E8381759A4E2837E517366F4B5D04\
          \       F:\\chat-goliath-120b-80k\\model-00019-of-00024.safetensors\nSHA256\
          \          4DF0A12B117B57D6D882B0C18894973474E0D7DCD46A1841E9C87E61FC94F4F4\
          \       F:\\chat-goliath-120b-80k\\model-00020-of-00024.safetensors\nSHA256\
          \          E5CAAA8E602F230CB752F0091E41B2FFCD2550B8C425325D3A3ABA2F6DC27209\
          \       F:\\chat-goliath-120b-80k\\model-00021-of-00024.safetensors\nSHA256\
          \          D41A1AF612EFC80D051A6487812327EC6D312B5BEA97310986B9AD97B70A3B2A\
          \       F:\\chat-goliath-120b-80k\\model-00022-of-00024.safetensors\nSHA256\
          \          5F9B8F2040BA5E448110BD9B4924CFAE3970309EAE32811FFB3F3641FE133A7F\
          \       F:\\chat-goliath-120b-80k\\model-00023-of-00024.safetensors\nSHA256\
          \          6BDDB8D1C02E4F406F5157BC9709D1B23AC094448C712F6EB3797DEF7ED42B9B\
          \       F:\\chat-goliath-120b-80k\\model-00024-of-00024.safetensors\nSHA256\
          \          E92E69756A90A7F7D90774BFA0A4B09D893E669BDFCFB810B5F3BC598DC52E7C\
          \       F:\\chat-goliath-120b-80k\\model.safetensors.index.json\nSHA256\
          \          8E16855E765EFB8FF6A8D05DCC1FC4E1C62C633E9503550EA2C462B336A96FC7\
          \       F:\\chat-goliath-120b-80k\\README.md\nSHA256          9921EDFD5322DD354EC44F825329E1C6C6B89B357BF3CE49377BE325B23EAA34\
          \       F:\\chat-goliath-120b-80k\\special_tokens_map.json\nSHA256     \
          \     79F2F33C690C49458F122090C1DFF223481CCF28162289407E8FA788074A330D \
          \      F:\\chat-goliath-120b-80k\\tokenizer_config.json\nSHA256        \
          \  4E28ED6CBB25942DCCBDBC000F414BC6075F580CC91E74C1AEC04D811CA60EAA    \
          \   F:\\chat-goliath-120b-80k\\tokenizer.json\nSHA256          D16FB023FB71C56B63E09C51631A3BC56297CAF04E60CEB1812614569DE3DBDD\
          \       F:\\chat-goliath-120b-80k\\tokenizer.model\n</code></pre>\n<p>However,\
          \ the quantization process also failed to yield consistent replies, like\
          \ the issues encountered in the previous steps.</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/3kAMfZdVx3MZ_V76VtXVj.png\"\
          ><img alt=\"ooba webui exl2 response.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/3kAMfZdVx3MZ_V76VtXVj.png\"\
          ></a></p>\n<p>We believe that we need your assistance. Thank you for your\
          \ time and consideration!</p>\n<p>Kind regards,<br>Fangru Shao</p>\n</blockquote>\n\
          <p>Dear Dr Shao,</p>\n<p>Thank you for your detailed test report, you did\
          \ a great job!!</p>\n<ol>\n<li>Regarding the problem that the files downloaded\
          \ by git clone are very small, because I uploaded them directly through\
          \ a web browser and VPN, I cannot upload them directly because my network\
          \ environment is very poor.</li>\n<li>About the 46.37 GiB (3.37 BPW) after\
          \ quantization is correct, this is because I expanded the Chinese vocabulary,\
          \ which resulted in the expansion of the word embedding matrix, which has\
          \ more parameters than the original Goliath model.</li>\n<li>Regarding the\
          \ problem of a mixture of Chinese and English responses generated by the\
          \ model, I guess the main reason is that I did not fully train with a large\
          \ amount of data after expanding the Chinese vocabulary. I used some low-quality\
          \ data for instruction tuning before uploading the model.</li>\n</ol>\n\
          <p>In the future, I may conduct training based on earlier checkpoints.</p>\n\
          <p>Best regards,<br>Hongyin Zhu</p>\n"
        raw: "> Greetings!\n> \n> First thank you for developing and supporting Chinese\
          \ language capabilities with the Goliath model but we've encountered several\
          \ issues about quantization and functionality that we hope can be addressed.\n\
          > \n> 1. Issues with Git LFS files during repository cloning:\n> \n> We\
          \ attempted to clone the repository using the git clone command but faced\
          \ issues with the Git LFS files. Each file was only 1KB in size and contained\
          \ a description rather than the expected data.\n> ```\n> PS E:\\> git clone\
          \ https://huggingface.co/hongyin/chat-goliath-120b-80k\n> Cloning into 'chat-goliath-120b-80k'...\n\
          > remote: Enumerating objects: 65, done.\n> remote: Counting objects: 100%\
          \ (62/62), done.\n> remote: Compressing objects: 100% (61/61), done.\n>\
          \ remote: Total 65 (delta 14), reused 0 (delta 0), pack-reused 3\n> Unpacking\
          \ objects: 100% (65/65), 662.65 KiB | 215.00 KiB/s, done.\n> PS E:\\> cd\
          \ .\\chat-goliath-120b-80k\\\n> PS E:\\chat-goliath-120b-80k> cat .\\pytorch_model-00001-of-00024.bin\n\
          > version https://git-lfs.github.com/spec/v1\n> oid sha256:34aaef9a13c63f0fd5c14747ecab157bae1a9c948aaa60a543e27d083492f2c1\n\
          > size 9666282843\n> ```\n> We then directly downloaded the files, ensuring\
          \ their integrity through SHA256 checksums.\n> \n> 2. Strange Metadata and\
          \ Incoherent Responses in Kobold.cpp:\n> \n> We use llama.cpp (commit b3a7c20)\
          \ to quantize the model to Q2_K. \n> ```\n> (llamacpp) PS F:\\llama.cpp\\\
          build\\bin\\Release> .\\quantize.exe F:\\chat-goliath-120b-80k-f16.gguf\
          \ F:\\chat-goliath-120b-80k-f16-q2_k.gguf Q2_K\n> main: build = 1768 (b3a7c20)\n\
          > main: built with MSVC 19.38.33133.0 for x64\n> main: quantizing 'F:\\\
          chat-goliath-120b-80k-f16.gguf' to 'F:\\chat-goliath-120b-80k-f16-q2_k.gguf'\
          \ as Q2_K\n> llama_model_loader: loaded meta data with 21 key-value pairs\
          \ and 1236 tensors from F:\\chat-goliath-120b-80k-f16.gguf (version GGUF\
          \ V3 (latest))\n> llama_model_loader: Dumping metadata keys/values. Note:\
          \ KV overrides do not apply in this output.\n> llama_model_loader: - kv\
          \   0:                       general.architecture str              = llama\n\
          > llama_model_loader: - kv   1:                               general.name\
          \ str              = F:\\\n> llama_model_loader: - kv   2:             \
          \          llama.context_length u32              = 81920\n> llama_model_loader:\
          \ - kv   3:                     llama.embedding_length u32             \
          \ = 8192\n> llama_model_loader: - kv   4:                          llama.block_count\
          \ u32              = 137\n> llama_model_loader: - kv   5:              \
          \    llama.feed_forward_length u32              = 28672\n> llama_model_loader:\
          \ - kv   6:                 llama.rope.dimension_count u32             \
          \ = 128\n> llama_model_loader: - kv   7:                 llama.attention.head_count\
          \ u32              = 64\n> llama_model_loader: - kv   8:              llama.attention.head_count_kv\
          \ u32              = 8\n> llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
          \ f32              = 0.000010\n> llama_model_loader: - kv  10:         \
          \              llama.rope.freq_base f32              = 10000.000000\n> llama_model_loader:\
          \ - kv  11:                          general.file_type u32             \
          \ = 1\n> llama_model_loader: - kv  12:                       tokenizer.ggml.model\
          \ str              = llama\n> llama_model_loader: - kv  13:            \
          \          tokenizer.ggml.tokens arr[str,49300]   = [\"<unk>\", \"<s>\"\
          , \"</s>\", \"<0x00>\", \"<...\n> llama_model_loader: - kv  14:        \
          \              tokenizer.ggml.scores arr[f32,49300]   = [0.000000, 0.000000,\
          \ 0.000000, 0.0000...\n> llama_model_loader: - kv  15:                 \
          \ tokenizer.ggml.token_type arr[i32,49300]   = [2, 3, 3, 6, 6, 6, 6, 6,\
          \ 6, 6, 6, 6, ...\n> llama_model_loader: - kv  16:                     \
          \ tokenizer.ggml.merges arr[str,71829]   = [\"\u6458 \u8981\", \"\u2581\
          \ \u300A\", \"\u4E2D \u56FD\", \"1 ...\n> llama_model_loader: - kv  17:\
          \                tokenizer.ggml.bos_token_id u32              = 1\n> llama_model_loader:\
          \ - kv  18:                tokenizer.ggml.eos_token_id u32             \
          \ = 2\n> llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id\
          \ u32              = 0\n> llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id\
          \ u32              = 2\n> llama_model_loader: - type  f32:  275 tensors\n\
          > llama_model_loader: - type  f16:  961 tensors\n> llama_model_quantize_internal:\
          \ meta size = 2310176 bytes\n> ...\n> llama_model_quantize_internal: model\
          \ size  = 225133.22 MB\n> llama_model_quantize_internal: quant size  = 47484.73\
          \ MB\n> ```\n> Upon loading the files with Kobold.cpp, we observed oddities\
          \ in the metadata. The Q2_K parameter exhibited an irregular 3.37 BPW value,\
          \ whose value should usually be around 2.625 BPW.\n> ```\n> llm_load_print_meta:\
          \ format           = GGUF V3 (latest)\n> llm_load_print_meta: arch     \
          \        = llama\n> llm_load_print_meta: vocab type       = SPM\n> llm_load_print_meta:\
          \ n_vocab          = 49300\n> llm_load_print_meta: n_merges         = 0\n\
          > llm_load_print_meta: n_ctx_train      = 81920\n> llm_load_print_meta:\
          \ n_embd           = 8192\n> llm_load_print_meta: n_head           = 64\n\
          > llm_load_print_meta: n_head_kv        = 8\n> llm_load_print_meta: n_layer\
          \          = 137\n> llm_load_print_meta: n_rot            = 128\n> llm_load_print_meta:\
          \ n_gqa            = 8\n> llm_load_print_meta: f_norm_eps       = 0.0e+00\n\
          > llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n> llm_load_print_meta:\
          \ f_clamp_kqv      = 0.0e+00\n> llm_load_print_meta: f_max_alibi_bias =\
          \ 0.0e+00\n> llm_load_print_meta: n_ff             = 28672\n> llm_load_print_meta:\
          \ n_expert         = 0\n> llm_load_print_meta: n_expert_used    = 0\n> llm_load_print_meta:\
          \ rope scaling     = linear\n> llm_load_print_meta: freq_base_train  = 10000.0\n\
          > llm_load_print_meta: freq_scale_train = 1\n> llm_load_print_meta: n_yarn_orig_ctx\
          \  = 81920\n> llm_load_print_meta: rope_finetuned   = unknown\n> llm_load_print_meta:\
          \ model type       = ?B\n> llm_load_print_meta: model ftype      = unknown,\
          \ may not work\n> llm_load_print_meta: model params     = 118.03 B\n> llm_load_print_meta:\
          \ model size       = 46.37 GiB (3.37 BPW)\n> llm_load_print_meta: general.name\
          \     = F:\\\n> llm_load_print_meta: BOS token        = 1 '<s>'\n> llm_load_print_meta:\
          \ EOS token        = 2 '</s>'\n> llm_load_print_meta: UNK token        =\
          \ 0 '<unk>'\n> llm_load_print_meta: PAD token        = 2 '</s>'\n> llm_load_print_meta:\
          \ LF token         = 13 '<0x0A>'\n> llm_load_tensors: ggml ctx size    \
          \   =    0.47 MiB\n> llm_load_tensors: using CUDA for GPU acceleration\n\
          > llm_load_tensors: system memory used  =  126.85 MiB\n> llm_load_tensors:\
          \ VRAM used           = 47358.35 MiB\n> llm_load_tensors: offloading 137\
          \ repeating layers to GPU\n> llm_load_tensors: offloading non-repeating\
          \ layers to GPU\n> llm_load_tensors: offloaded 138/138 layers to GPU\n>\
          \ ```\n> The model's responses were also inconsistent and interspersed with\
          \ a mix of Chinese and English, lacking coherence and relevance.\n> ![kobldcpp\
          \ gguf response 1.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/JgGjPStpchvv1AG1QRgOV.png)\n\
          > ![kobldcpp gguf response 2.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/_st9FUkas4TjIhjYUxwmO.png)\n\
          > ![kobldcpp gguf response 3.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/Xe3kj0c1LF1wB6yfegp6j.png)\n\
          > \n> 3. Quantization Attempt in exl2:\n> \n> Moving forward, we attempted\
          \ to quantize the model in exl2. We first converted the model into .safetensors,\
          \ with our SHA256 for the .safetensors matching the expected values.\n>\
          \ ```\n> PS F:\\chat-goliath-120b-80k> Get-ChildItem -File | Get-FileHash\
          \ -Algorithm SHA256\n> \n> Algorithm       Hash                        \
          \                                           Path\n> ---------       ----\
          \                                                                   ----\n\
          > SHA256          D606246B713E31B1B43DFE913554DD47819089B9E44B9EBAA294EDA2589A6B4C\
          \       F:\\chat-goliath-120b-80k\\added_tokens.json\n> SHA256         \
          \ EF58A0874EB14E647EF9DBF1B5A104701375FEE34F2D01BFD9795C984A07FDC0     \
          \  F:\\chat-goliath-120b-80k\\config.json\n> SHA256          E9ADFCFDC532213AE59497E875478D95451D063640FB51755BC63C1070B9701D\
          \       F:\\chat-goliath-120b-80k\\generation_config.json\n> SHA256    \
          \      3C9006082484A4A04AD7932BCAF0FAE22948CA063B4E1EF16A74C15D6E7F72CC\
          \       F:\\chat-goliath-120b-80k\\model-00001-of-00024.safetensors\n> SHA256\
          \          60B3F61167C18CFE8968A3FC377D3DF7D14051595CD3044E48549580387B1A42\
          \       F:\\chat-goliath-120b-80k\\model-00002-of-00024.safetensors\n> SHA256\
          \          358A58059687D8DEF130B2E4102DD596825255DA51F18DEAE277921EE6DA28B2\
          \       F:\\chat-goliath-120b-80k\\model-00003-of-00024.safetensors\n> SHA256\
          \          7D7E35B80E4AC0B59E2BAB33F0F8E6FE5F8D2DD36983EF36BCA1E2628C20264E\
          \       F:\\chat-goliath-120b-80k\\model-00004-of-00024.safetensors\n> SHA256\
          \          9CCB7212C979E15229DDBEFF6992E4A30C3D3AD1F1FFE571D14F8EB4721398EE\
          \       F:\\chat-goliath-120b-80k\\model-00005-of-00024.safetensors\n> SHA256\
          \          6753B8281079AC0927C0FB4E4AD9877F57E1BE40E4B5840ABB36F95BE8DB35D7\
          \       F:\\chat-goliath-120b-80k\\model-00006-of-00024.safetensors\n> SHA256\
          \          32BCF2B2BBF3E3BD052239158D14D0693142B6A3D5B014EA0C626C686C42AFA7\
          \       F:\\chat-goliath-120b-80k\\model-00007-of-00024.safetensors\n> SHA256\
          \          051B77A0CD28F54B4AE6D2087257CF4D2C7198C245D9031312582B2AEEEF143C\
          \       F:\\chat-goliath-120b-80k\\model-00008-of-00024.safetensors\n> SHA256\
          \          EFC8528F9FE25EAB6D72F7E84D9CADCD13CA998F179A8AFD6D7521A7F8BCAE1B\
          \       F:\\chat-goliath-120b-80k\\model-00009-of-00024.safetensors\n> SHA256\
          \          109643CE4559EC438CF0D1C8326CE5B32E9DFF4A35AC61EAF149CA1C0B27731C\
          \       F:\\chat-goliath-120b-80k\\model-00010-of-00024.safetensors\n> SHA256\
          \          EE9EB1BEEBC5FBA71DCF3124F82A41EB66F9B29583C4D9426A21D4367D1EF7C2\
          \       F:\\chat-goliath-120b-80k\\model-00011-of-00024.safetensors\n> SHA256\
          \          EEE031C8611D3E7E4E65C22CAA18F4E27319E359A5EB215C932828CB647BB81D\
          \       F:\\chat-goliath-120b-80k\\model-00012-of-00024.safetensors\n> SHA256\
          \          24B9486120B14560703E08D298A281FDD9035DE8DF3CBB528573C6C76E49B2EE\
          \       F:\\chat-goliath-120b-80k\\model-00013-of-00024.safetensors\n> SHA256\
          \          A54D145D4BB95E10F0594F8FE7FE4EBC66DCC5CA3A238EDE7A4D8E33B4E16082\
          \       F:\\chat-goliath-120b-80k\\model-00014-of-00024.safetensors\n> SHA256\
          \          0CAB89F83642A1553BEEB7965111C511BC05F59027C7D19DC9C853A6B422F85A\
          \       F:\\chat-goliath-120b-80k\\model-00015-of-00024.safetensors\n> SHA256\
          \          CCBBDAD6FA6E95720D4B70801E38A2A04154E8787BB5D4A6024E59824EC2191D\
          \       F:\\chat-goliath-120b-80k\\model-00016-of-00024.safetensors\n> SHA256\
          \          08137CCDA1B37C1EFF769EE894BB929FFD43EB7EE735386E626E88B1F187827C\
          \       F:\\chat-goliath-120b-80k\\model-00017-of-00024.safetensors\n> SHA256\
          \          6ED11E748D85699980A3990658D0D3B07BD89E0B55DE755C884555BEE2C4A620\
          \       F:\\chat-goliath-120b-80k\\model-00018-of-00024.safetensors\n> SHA256\
          \          39D879163BF2BF5B1A14906B707159A04C7E8381759A4E2837E517366F4B5D04\
          \       F:\\chat-goliath-120b-80k\\model-00019-of-00024.safetensors\n> SHA256\
          \          4DF0A12B117B57D6D882B0C18894973474E0D7DCD46A1841E9C87E61FC94F4F4\
          \       F:\\chat-goliath-120b-80k\\model-00020-of-00024.safetensors\n> SHA256\
          \          E5CAAA8E602F230CB752F0091E41B2FFCD2550B8C425325D3A3ABA2F6DC27209\
          \       F:\\chat-goliath-120b-80k\\model-00021-of-00024.safetensors\n> SHA256\
          \          D41A1AF612EFC80D051A6487812327EC6D312B5BEA97310986B9AD97B70A3B2A\
          \       F:\\chat-goliath-120b-80k\\model-00022-of-00024.safetensors\n> SHA256\
          \          5F9B8F2040BA5E448110BD9B4924CFAE3970309EAE32811FFB3F3641FE133A7F\
          \       F:\\chat-goliath-120b-80k\\model-00023-of-00024.safetensors\n> SHA256\
          \          6BDDB8D1C02E4F406F5157BC9709D1B23AC094448C712F6EB3797DEF7ED42B9B\
          \       F:\\chat-goliath-120b-80k\\model-00024-of-00024.safetensors\n> SHA256\
          \          E92E69756A90A7F7D90774BFA0A4B09D893E669BDFCFB810B5F3BC598DC52E7C\
          \       F:\\chat-goliath-120b-80k\\model.safetensors.index.json\n> SHA256\
          \          8E16855E765EFB8FF6A8D05DCC1FC4E1C62C633E9503550EA2C462B336A96FC7\
          \       F:\\chat-goliath-120b-80k\\README.md\n> SHA256          9921EDFD5322DD354EC44F825329E1C6C6B89B357BF3CE49377BE325B23EAA34\
          \       F:\\chat-goliath-120b-80k\\special_tokens_map.json\n> SHA256   \
          \       79F2F33C690C49458F122090C1DFF223481CCF28162289407E8FA788074A330D\
          \       F:\\chat-goliath-120b-80k\\tokenizer_config.json\n> SHA256     \
          \     4E28ED6CBB25942DCCBDBC000F414BC6075F580CC91E74C1AEC04D811CA60EAA \
          \      F:\\chat-goliath-120b-80k\\tokenizer.json\n> SHA256          D16FB023FB71C56B63E09C51631A3BC56297CAF04E60CEB1812614569DE3DBDD\
          \       F:\\chat-goliath-120b-80k\\tokenizer.model\n> ```\n> However, the\
          \ quantization process also failed to yield consistent replies, like the\
          \ issues encountered in the previous steps.\n> \n> ![ooba webui exl2 response.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/3kAMfZdVx3MZ_V76VtXVj.png)\n\
          > \n> We believe that we need your assistance. Thank you for your time and\
          \ consideration!\n> \n> Kind regards,\n> Fangru Shao\n\nDear Dr Shao,\n\n\
          Thank you for your detailed test report, you did a great job!!\n1. Regarding\
          \ the problem that the files downloaded by git clone are very small, because\
          \ I uploaded them directly through a web browser and VPN, I cannot upload\
          \ them directly because my network environment is very poor.\n2. About the\
          \ 46.37 GiB (3.37 BPW) after quantization is correct, this is because I\
          \ expanded the Chinese vocabulary, which resulted in the expansion of the\
          \ word embedding matrix, which has more parameters than the original Goliath\
          \ model.\n3. Regarding the problem of a mixture of Chinese and English responses\
          \ generated by the model, I guess the main reason is that I did not fully\
          \ train with a large amount of data after expanding the Chinese vocabulary.\
          \ I used some low-quality data for instruction tuning before uploading the\
          \ model.\n\nIn the future, I may conduct training based on earlier checkpoints.\n\
          \nBest regards,\nHongyin Zhu\n"
        updatedAt: '2024-01-05T16:19:49.910Z'
      numEdits: 0
      reactions: []
    id: 65982c2589cae82d1416c79b
    type: comment
  author: hongyin
  content: "> Greetings!\n> \n> First thank you for developing and supporting Chinese\
    \ language capabilities with the Goliath model but we've encountered several issues\
    \ about quantization and functionality that we hope can be addressed.\n> \n> 1.\
    \ Issues with Git LFS files during repository cloning:\n> \n> We attempted to\
    \ clone the repository using the git clone command but faced issues with the Git\
    \ LFS files. Each file was only 1KB in size and contained a description rather\
    \ than the expected data.\n> ```\n> PS E:\\> git clone https://huggingface.co/hongyin/chat-goliath-120b-80k\n\
    > Cloning into 'chat-goliath-120b-80k'...\n> remote: Enumerating objects: 65,\
    \ done.\n> remote: Counting objects: 100% (62/62), done.\n> remote: Compressing\
    \ objects: 100% (61/61), done.\n> remote: Total 65 (delta 14), reused 0 (delta\
    \ 0), pack-reused 3\n> Unpacking objects: 100% (65/65), 662.65 KiB | 215.00 KiB/s,\
    \ done.\n> PS E:\\> cd .\\chat-goliath-120b-80k\\\n> PS E:\\chat-goliath-120b-80k>\
    \ cat .\\pytorch_model-00001-of-00024.bin\n> version https://git-lfs.github.com/spec/v1\n\
    > oid sha256:34aaef9a13c63f0fd5c14747ecab157bae1a9c948aaa60a543e27d083492f2c1\n\
    > size 9666282843\n> ```\n> We then directly downloaded the files, ensuring their\
    \ integrity through SHA256 checksums.\n> \n> 2. Strange Metadata and Incoherent\
    \ Responses in Kobold.cpp:\n> \n> We use llama.cpp (commit b3a7c20) to quantize\
    \ the model to Q2_K. \n> ```\n> (llamacpp) PS F:\\llama.cpp\\build\\bin\\Release>\
    \ .\\quantize.exe F:\\chat-goliath-120b-80k-f16.gguf F:\\chat-goliath-120b-80k-f16-q2_k.gguf\
    \ Q2_K\n> main: build = 1768 (b3a7c20)\n> main: built with MSVC 19.38.33133.0\
    \ for x64\n> main: quantizing 'F:\\chat-goliath-120b-80k-f16.gguf' to 'F:\\chat-goliath-120b-80k-f16-q2_k.gguf'\
    \ as Q2_K\n> llama_model_loader: loaded meta data with 21 key-value pairs and\
    \ 1236 tensors from F:\\chat-goliath-120b-80k-f16.gguf (version GGUF V3 (latest))\n\
    > llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not\
    \ apply in this output.\n> llama_model_loader: - kv   0:                     \
    \  general.architecture str              = llama\n> llama_model_loader: - kv \
    \  1:                               general.name str              = F:\\\n> llama_model_loader:\
    \ - kv   2:                       llama.context_length u32              = 81920\n\
    > llama_model_loader: - kv   3:                     llama.embedding_length u32\
    \              = 8192\n> llama_model_loader: - kv   4:                       \
    \   llama.block_count u32              = 137\n> llama_model_loader: - kv   5:\
    \                  llama.feed_forward_length u32              = 28672\n> llama_model_loader:\
    \ - kv   6:                 llama.rope.dimension_count u32              = 128\n\
    > llama_model_loader: - kv   7:                 llama.attention.head_count u32\
    \              = 64\n> llama_model_loader: - kv   8:              llama.attention.head_count_kv\
    \ u32              = 8\n> llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
    \ f32              = 0.000010\n> llama_model_loader: - kv  10:               \
    \        llama.rope.freq_base f32              = 10000.000000\n> llama_model_loader:\
    \ - kv  11:                          general.file_type u32              = 1\n\
    > llama_model_loader: - kv  12:                       tokenizer.ggml.model str\
    \              = llama\n> llama_model_loader: - kv  13:                      tokenizer.ggml.tokens\
    \ arr[str,49300]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n> llama_model_loader:\
    \ - kv  14:                      tokenizer.ggml.scores arr[f32,49300]   = [0.000000,\
    \ 0.000000, 0.000000, 0.0000...\n> llama_model_loader: - kv  15:             \
    \     tokenizer.ggml.token_type arr[i32,49300]   = [2, 3, 3, 6, 6, 6, 6, 6, 6,\
    \ 6, 6, 6, ...\n> llama_model_loader: - kv  16:                      tokenizer.ggml.merges\
    \ arr[str,71829]   = [\"\u6458 \u8981\", \"\u2581 \u300A\", \"\u4E2D \u56FD\"\
    , \"1 ...\n> llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id\
    \ u32              = 1\n> llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id\
    \ u32              = 2\n> llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id\
    \ u32              = 0\n> llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id\
    \ u32              = 2\n> llama_model_loader: - type  f32:  275 tensors\n> llama_model_loader:\
    \ - type  f16:  961 tensors\n> llama_model_quantize_internal: meta size = 2310176\
    \ bytes\n> ...\n> llama_model_quantize_internal: model size  = 225133.22 MB\n\
    > llama_model_quantize_internal: quant size  = 47484.73 MB\n> ```\n> Upon loading\
    \ the files with Kobold.cpp, we observed oddities in the metadata. The Q2_K parameter\
    \ exhibited an irregular 3.37 BPW value, whose value should usually be around\
    \ 2.625 BPW.\n> ```\n> llm_load_print_meta: format           = GGUF V3 (latest)\n\
    > llm_load_print_meta: arch             = llama\n> llm_load_print_meta: vocab\
    \ type       = SPM\n> llm_load_print_meta: n_vocab          = 49300\n> llm_load_print_meta:\
    \ n_merges         = 0\n> llm_load_print_meta: n_ctx_train      = 81920\n> llm_load_print_meta:\
    \ n_embd           = 8192\n> llm_load_print_meta: n_head           = 64\n> llm_load_print_meta:\
    \ n_head_kv        = 8\n> llm_load_print_meta: n_layer          = 137\n> llm_load_print_meta:\
    \ n_rot            = 128\n> llm_load_print_meta: n_gqa            = 8\n> llm_load_print_meta:\
    \ f_norm_eps       = 0.0e+00\n> llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n\
    > llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n> llm_load_print_meta: f_max_alibi_bias\
    \ = 0.0e+00\n> llm_load_print_meta: n_ff             = 28672\n> llm_load_print_meta:\
    \ n_expert         = 0\n> llm_load_print_meta: n_expert_used    = 0\n> llm_load_print_meta:\
    \ rope scaling     = linear\n> llm_load_print_meta: freq_base_train  = 10000.0\n\
    > llm_load_print_meta: freq_scale_train = 1\n> llm_load_print_meta: n_yarn_orig_ctx\
    \  = 81920\n> llm_load_print_meta: rope_finetuned   = unknown\n> llm_load_print_meta:\
    \ model type       = ?B\n> llm_load_print_meta: model ftype      = unknown, may\
    \ not work\n> llm_load_print_meta: model params     = 118.03 B\n> llm_load_print_meta:\
    \ model size       = 46.37 GiB (3.37 BPW)\n> llm_load_print_meta: general.name\
    \     = F:\\\n> llm_load_print_meta: BOS token        = 1 '<s>'\n> llm_load_print_meta:\
    \ EOS token        = 2 '</s>'\n> llm_load_print_meta: UNK token        = 0 '<unk>'\n\
    > llm_load_print_meta: PAD token        = 2 '</s>'\n> llm_load_print_meta: LF\
    \ token         = 13 '<0x0A>'\n> llm_load_tensors: ggml ctx size       =    0.47\
    \ MiB\n> llm_load_tensors: using CUDA for GPU acceleration\n> llm_load_tensors:\
    \ system memory used  =  126.85 MiB\n> llm_load_tensors: VRAM used           =\
    \ 47358.35 MiB\n> llm_load_tensors: offloading 137 repeating layers to GPU\n>\
    \ llm_load_tensors: offloading non-repeating layers to GPU\n> llm_load_tensors:\
    \ offloaded 138/138 layers to GPU\n> ```\n> The model's responses were also inconsistent\
    \ and interspersed with a mix of Chinese and English, lacking coherence and relevance.\n\
    > ![kobldcpp gguf response 1.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/JgGjPStpchvv1AG1QRgOV.png)\n\
    > ![kobldcpp gguf response 2.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/_st9FUkas4TjIhjYUxwmO.png)\n\
    > ![kobldcpp gguf response 3.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/Xe3kj0c1LF1wB6yfegp6j.png)\n\
    > \n> 3. Quantization Attempt in exl2:\n> \n> Moving forward, we attempted to\
    \ quantize the model in exl2. We first converted the model into .safetensors,\
    \ with our SHA256 for the .safetensors matching the expected values.\n> ```\n\
    > PS F:\\chat-goliath-120b-80k> Get-ChildItem -File | Get-FileHash -Algorithm\
    \ SHA256\n> \n> Algorithm       Hash                                         \
    \                          Path\n> ---------       ----                      \
    \                                             ----\n> SHA256          D606246B713E31B1B43DFE913554DD47819089B9E44B9EBAA294EDA2589A6B4C\
    \       F:\\chat-goliath-120b-80k\\added_tokens.json\n> SHA256          EF58A0874EB14E647EF9DBF1B5A104701375FEE34F2D01BFD9795C984A07FDC0\
    \       F:\\chat-goliath-120b-80k\\config.json\n> SHA256          E9ADFCFDC532213AE59497E875478D95451D063640FB51755BC63C1070B9701D\
    \       F:\\chat-goliath-120b-80k\\generation_config.json\n> SHA256          3C9006082484A4A04AD7932BCAF0FAE22948CA063B4E1EF16A74C15D6E7F72CC\
    \       F:\\chat-goliath-120b-80k\\model-00001-of-00024.safetensors\n> SHA256\
    \          60B3F61167C18CFE8968A3FC377D3DF7D14051595CD3044E48549580387B1A42  \
    \     F:\\chat-goliath-120b-80k\\model-00002-of-00024.safetensors\n> SHA256  \
    \        358A58059687D8DEF130B2E4102DD596825255DA51F18DEAE277921EE6DA28B2    \
    \   F:\\chat-goliath-120b-80k\\model-00003-of-00024.safetensors\n> SHA256    \
    \      7D7E35B80E4AC0B59E2BAB33F0F8E6FE5F8D2DD36983EF36BCA1E2628C20264E      \
    \ F:\\chat-goliath-120b-80k\\model-00004-of-00024.safetensors\n> SHA256      \
    \    9CCB7212C979E15229DDBEFF6992E4A30C3D3AD1F1FFE571D14F8EB4721398EE       F:\\\
    chat-goliath-120b-80k\\model-00005-of-00024.safetensors\n> SHA256          6753B8281079AC0927C0FB4E4AD9877F57E1BE40E4B5840ABB36F95BE8DB35D7\
    \       F:\\chat-goliath-120b-80k\\model-00006-of-00024.safetensors\n> SHA256\
    \          32BCF2B2BBF3E3BD052239158D14D0693142B6A3D5B014EA0C626C686C42AFA7  \
    \     F:\\chat-goliath-120b-80k\\model-00007-of-00024.safetensors\n> SHA256  \
    \        051B77A0CD28F54B4AE6D2087257CF4D2C7198C245D9031312582B2AEEEF143C    \
    \   F:\\chat-goliath-120b-80k\\model-00008-of-00024.safetensors\n> SHA256    \
    \      EFC8528F9FE25EAB6D72F7E84D9CADCD13CA998F179A8AFD6D7521A7F8BCAE1B      \
    \ F:\\chat-goliath-120b-80k\\model-00009-of-00024.safetensors\n> SHA256      \
    \    109643CE4559EC438CF0D1C8326CE5B32E9DFF4A35AC61EAF149CA1C0B27731C       F:\\\
    chat-goliath-120b-80k\\model-00010-of-00024.safetensors\n> SHA256          EE9EB1BEEBC5FBA71DCF3124F82A41EB66F9B29583C4D9426A21D4367D1EF7C2\
    \       F:\\chat-goliath-120b-80k\\model-00011-of-00024.safetensors\n> SHA256\
    \          EEE031C8611D3E7E4E65C22CAA18F4E27319E359A5EB215C932828CB647BB81D  \
    \     F:\\chat-goliath-120b-80k\\model-00012-of-00024.safetensors\n> SHA256  \
    \        24B9486120B14560703E08D298A281FDD9035DE8DF3CBB528573C6C76E49B2EE    \
    \   F:\\chat-goliath-120b-80k\\model-00013-of-00024.safetensors\n> SHA256    \
    \      A54D145D4BB95E10F0594F8FE7FE4EBC66DCC5CA3A238EDE7A4D8E33B4E16082      \
    \ F:\\chat-goliath-120b-80k\\model-00014-of-00024.safetensors\n> SHA256      \
    \    0CAB89F83642A1553BEEB7965111C511BC05F59027C7D19DC9C853A6B422F85A       F:\\\
    chat-goliath-120b-80k\\model-00015-of-00024.safetensors\n> SHA256          CCBBDAD6FA6E95720D4B70801E38A2A04154E8787BB5D4A6024E59824EC2191D\
    \       F:\\chat-goliath-120b-80k\\model-00016-of-00024.safetensors\n> SHA256\
    \          08137CCDA1B37C1EFF769EE894BB929FFD43EB7EE735386E626E88B1F187827C  \
    \     F:\\chat-goliath-120b-80k\\model-00017-of-00024.safetensors\n> SHA256  \
    \        6ED11E748D85699980A3990658D0D3B07BD89E0B55DE755C884555BEE2C4A620    \
    \   F:\\chat-goliath-120b-80k\\model-00018-of-00024.safetensors\n> SHA256    \
    \      39D879163BF2BF5B1A14906B707159A04C7E8381759A4E2837E517366F4B5D04      \
    \ F:\\chat-goliath-120b-80k\\model-00019-of-00024.safetensors\n> SHA256      \
    \    4DF0A12B117B57D6D882B0C18894973474E0D7DCD46A1841E9C87E61FC94F4F4       F:\\\
    chat-goliath-120b-80k\\model-00020-of-00024.safetensors\n> SHA256          E5CAAA8E602F230CB752F0091E41B2FFCD2550B8C425325D3A3ABA2F6DC27209\
    \       F:\\chat-goliath-120b-80k\\model-00021-of-00024.safetensors\n> SHA256\
    \          D41A1AF612EFC80D051A6487812327EC6D312B5BEA97310986B9AD97B70A3B2A  \
    \     F:\\chat-goliath-120b-80k\\model-00022-of-00024.safetensors\n> SHA256  \
    \        5F9B8F2040BA5E448110BD9B4924CFAE3970309EAE32811FFB3F3641FE133A7F    \
    \   F:\\chat-goliath-120b-80k\\model-00023-of-00024.safetensors\n> SHA256    \
    \      6BDDB8D1C02E4F406F5157BC9709D1B23AC094448C712F6EB3797DEF7ED42B9B      \
    \ F:\\chat-goliath-120b-80k\\model-00024-of-00024.safetensors\n> SHA256      \
    \    E92E69756A90A7F7D90774BFA0A4B09D893E669BDFCFB810B5F3BC598DC52E7C       F:\\\
    chat-goliath-120b-80k\\model.safetensors.index.json\n> SHA256          8E16855E765EFB8FF6A8D05DCC1FC4E1C62C633E9503550EA2C462B336A96FC7\
    \       F:\\chat-goliath-120b-80k\\README.md\n> SHA256          9921EDFD5322DD354EC44F825329E1C6C6B89B357BF3CE49377BE325B23EAA34\
    \       F:\\chat-goliath-120b-80k\\special_tokens_map.json\n> SHA256         \
    \ 79F2F33C690C49458F122090C1DFF223481CCF28162289407E8FA788074A330D       F:\\\
    chat-goliath-120b-80k\\tokenizer_config.json\n> SHA256          4E28ED6CBB25942DCCBDBC000F414BC6075F580CC91E74C1AEC04D811CA60EAA\
    \       F:\\chat-goliath-120b-80k\\tokenizer.json\n> SHA256          D16FB023FB71C56B63E09C51631A3BC56297CAF04E60CEB1812614569DE3DBDD\
    \       F:\\chat-goliath-120b-80k\\tokenizer.model\n> ```\n> However, the quantization\
    \ process also failed to yield consistent replies, like the issues encountered\
    \ in the previous steps.\n> \n> ![ooba webui exl2 response.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/3kAMfZdVx3MZ_V76VtXVj.png)\n\
    > \n> We believe that we need your assistance. Thank you for your time and consideration!\n\
    > \n> Kind regards,\n> Fangru Shao\n\nDear Dr Shao,\n\nThank you for your detailed\
    \ test report, you did a great job!!\n1. Regarding the problem that the files\
    \ downloaded by git clone are very small, because I uploaded them directly through\
    \ a web browser and VPN, I cannot upload them directly because my network environment\
    \ is very poor.\n2. About the 46.37 GiB (3.37 BPW) after quantization is correct,\
    \ this is because I expanded the Chinese vocabulary, which resulted in the expansion\
    \ of the word embedding matrix, which has more parameters than the original Goliath\
    \ model.\n3. Regarding the problem of a mixture of Chinese and English responses\
    \ generated by the model, I guess the main reason is that I did not fully train\
    \ with a large amount of data after expanding the Chinese vocabulary. I used some\
    \ low-quality data for instruction tuning before uploading the model.\n\nIn the\
    \ future, I may conduct training based on earlier checkpoints.\n\nBest regards,\n\
    Hongyin Zhu\n"
  created_at: 2024-01-05 16:19:49+00:00
  edited: false
  hidden: false
  id: 65982c2589cae82d1416c79b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/-OLDRiEOWDgqQUCKXigXD.jpeg?w=200&h=200&f=face
      fullname: Fangru Shao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MatrixC7
      type: user
    createdAt: '2024-01-05T17:56:14.000Z'
    data:
      edited: false
      editors:
      - MatrixC7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9631867408752441
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/-OLDRiEOWDgqQUCKXigXD.jpeg?w=200&h=200&f=face
          fullname: Fangru Shao
          isHf: false
          isPro: false
          name: MatrixC7
          type: user
        html: '<p>Hi Hongyin,</p>

          <p>Thanks for your information! We then would wait for your process and
          hope you could successfully train a good model at that time.</p>

          <p>Kind regards,<br>Fangru Shao</p>

          '
        raw: 'Hi Hongyin,


          Thanks for your information! We then would wait for your process and hope
          you could successfully train a good model at that time.


          Kind regards,

          Fangru Shao'
        updatedAt: '2024-01-05T17:56:14.872Z'
      numEdits: 0
      reactions: []
    id: 659842be5f7a6d40f7c5b4b6
    type: comment
  author: MatrixC7
  content: 'Hi Hongyin,


    Thanks for your information! We then would wait for your process and hope you
    could successfully train a good model at that time.


    Kind regards,

    Fangru Shao'
  created_at: 2024-01-05 17:56:14+00:00
  edited: false
  hidden: false
  id: 659842be5f7a6d40f7c5b4b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/-OLDRiEOWDgqQUCKXigXD.jpeg?w=200&h=200&f=face
      fullname: Fangru Shao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MatrixC7
      type: user
    createdAt: '2024-01-16T04:36:30.000Z'
    data:
      edited: false
      editors:
      - MatrixC7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9789752960205078
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/-OLDRiEOWDgqQUCKXigXD.jpeg?w=200&h=200&f=face
          fullname: Fangru Shao
          isHf: false
          isPro: false
          name: MatrixC7
          type: user
        html: '<p>Greetings!</p>

          <p>I have noticed that you have re-uploaded model files but it seems that
          only several parts of them have been updated? Is the upload not complete
          or the network issue?<br>Anyway take your time and we would be waiting for
          your model.<br>Wish you a nice day! </p>

          <p>Kind regards,<br>Fangru Shao</p>

          '
        raw: "Greetings!\n\nI have noticed that you have re-uploaded model files but\
          \ it seems that only several parts of them have been updated? Is the upload\
          \ not complete or the network issue?\nAnyway take your time and we would\
          \ be waiting for your model.\nWish you a nice day! \n\nKind regards,\nFangru\
          \ Shao"
        updatedAt: '2024-01-16T04:36:30.723Z'
      numEdits: 0
      reactions: []
    id: 65a607ce7f290515fce2830f
    type: comment
  author: MatrixC7
  content: "Greetings!\n\nI have noticed that you have re-uploaded model files but\
    \ it seems that only several parts of them have been updated? Is the upload not\
    \ complete or the network issue?\nAnyway take your time and we would be waiting\
    \ for your model.\nWish you a nice day! \n\nKind regards,\nFangru Shao"
  created_at: 2024-01-16 04:36:30+00:00
  edited: false
  hidden: false
  id: 65a607ce7f290515fce2830f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64265252a5ec4a5cbc52fe48/M0t275nR0Vw79bTORMpu9.png?w=200&h=200&f=face
      fullname: hongyin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: hongyin
      type: user
    createdAt: '2024-01-16T09:30:53.000Z'
    data:
      edited: false
      editors:
      - hongyin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9610231518745422
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64265252a5ec4a5cbc52fe48/M0t275nR0Vw79bTORMpu9.png?w=200&h=200&f=face
          fullname: hongyin
          isHf: false
          isPro: false
          name: hongyin
          type: user
        html: '<blockquote>

          <p>Greetings!</p>

          <p>I have noticed that you have re-uploaded model files but it seems that
          only several parts of them have been updated? Is the upload not complete
          or the network issue?<br>Anyway take your time and we would be waiting for
          your model.<br>Wish you a nice day! </p>

          <p>Kind regards,<br>Fangru Shao</p>

          </blockquote>

          <p>HI! </p>

          <p>I suggest that you only replace the two files pytorch_model-00001-of-00024.bin
          and pytorch_model-00024-of-00024.bin, and do not need to download the others
          again. Since I only fine-tuned some of the model parameters, most of them
          were fixed. When I was uploading the model to hugginface, the website automatically
          verified the hash value and skipped files that had not changed.</p>

          <p>Best regrads,<br>Hongyin Zhu</p>

          '
        raw: "> Greetings!\n> \n> I have noticed that you have re-uploaded model files\
          \ but it seems that only several parts of them have been updated? Is the\
          \ upload not complete or the network issue?\n> Anyway take your time and\
          \ we would be waiting for your model.\n> Wish you a nice day! \n> \n> Kind\
          \ regards,\n> Fangru Shao\n\nHI! \n\nI suggest that you only replace the\
          \ two files pytorch_model-00001-of-00024.bin and pytorch_model-00024-of-00024.bin,\
          \ and do not need to download the others again. Since I only fine-tuned\
          \ some of the model parameters, most of them were fixed. When I was uploading\
          \ the model to hugginface, the website automatically verified the hash value\
          \ and skipped files that had not changed.\n\nBest regrads,\nHongyin Zhu"
        updatedAt: '2024-01-16T09:30:53.976Z'
      numEdits: 0
      reactions: []
    id: 65a64ccd0b5704678ad0b33e
    type: comment
  author: hongyin
  content: "> Greetings!\n> \n> I have noticed that you have re-uploaded model files\
    \ but it seems that only several parts of them have been updated? Is the upload\
    \ not complete or the network issue?\n> Anyway take your time and we would be\
    \ waiting for your model.\n> Wish you a nice day! \n> \n> Kind regards,\n> Fangru\
    \ Shao\n\nHI! \n\nI suggest that you only replace the two files pytorch_model-00001-of-00024.bin\
    \ and pytorch_model-00024-of-00024.bin, and do not need to download the others\
    \ again. Since I only fine-tuned some of the model parameters, most of them were\
    \ fixed. When I was uploading the model to hugginface, the website automatically\
    \ verified the hash value and skipped files that had not changed.\n\nBest regrads,\n\
    Hongyin Zhu"
  created_at: 2024-01-16 09:30:53+00:00
  edited: false
  hidden: false
  id: 65a64ccd0b5704678ad0b33e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/-OLDRiEOWDgqQUCKXigXD.jpeg?w=200&h=200&f=face
      fullname: Fangru Shao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MatrixC7
      type: user
    createdAt: '2024-01-16T19:09:26.000Z'
    data:
      edited: false
      editors:
      - MatrixC7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.23244524002075195
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/-OLDRiEOWDgqQUCKXigXD.jpeg?w=200&h=200&f=face
          fullname: Fangru Shao
          isHf: false
          isPro: false
          name: MatrixC7
          type: user
        html: '<p>Sadly it might still not be working properly. I am using the Q2_K
          quantization of the model.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/BG1-UNK_SAkUoFHLVRYfG.png"><img
          alt="Snipaste_2024-01-17_03-00-47.png" src="https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/BG1-UNK_SAkUoFHLVRYfG.png"></a><br>Here
          are the checksums of my files. </p>

          <pre><code class="language-powershell">Algorithm       Hash                                                                   Path

          <span class="hljs-literal">---------</span>       <span class="hljs-literal">----</span>                                                                   <span
          class="hljs-literal">----</span>

          SHA256          EB45E5C5BF988775AF2CEC6524040BD88EBC002A83891DBDA052AB59CBF3F628       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\added_tokens.json

          SHA256          DA1F80396552FD1AF2F8FF019F3D17FA4E2903C5127F501249FC9C9DBD4DDE64       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\config.json

          SHA256          B77BE1183605A19D3B3A115236EBEB8B15B04B784CCC386EC0B2B782D597A216       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\generation_config.json

          SHA256          <span class="hljs-number">07</span>FB40C64E2F645EBBB4450A5FB9882EDA477EC3B9DE629B422BD16557112CC8       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00001-of-00024</span>.bin

          SHA256          C6395064F3EBBF2DC4A9C5231D8FAE64E8441DEBF1BA69B5B387261AC6DB6B3F       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00002-of-00024</span>.bin

          SHA256          <span class="hljs-number">945</span>CF5F3608DE8F01E0B31F0E9CB1A1FD0A1809F371B88B24877132E75635D33       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00003-of-00024</span>.bin

          SHA256          <span class="hljs-number">800</span>A35D317B6AA9F4DE98CF0B0264FFE8D563750B0055555456F4EAFF2F9C5C3       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00004-of-00024</span>.bin

          SHA256          <span class="hljs-number">2125</span>CB4B9EA54182E6ADA059022C5FC6BFAF06F25AEBE198F6CCA1913F5C918D       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00005-of-00024</span>.bin

          SHA256          <span class="hljs-number">420</span>E6788E175CB6EB89CB57110A11A2A501EDD5F02B9FBA138B59AAB3DB51A8A       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00006-of-00024</span>.bin

          SHA256          B3322275DFFF7840BF38CAD7C26B94BB4DC32E0F308D85FED4BCC3DD74C03141       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00007-of-00024</span>.bin

          SHA256          <span class="hljs-number">4</span>C0C938DF6A934B7A08467019FBE75EBAAAA4369E9A27614A1A7B84735B171A5       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00008-of-00024</span>.bin

          SHA256          DFEBA74B2278133351B9FC210A4CFB6A13AB8C518EA8B32113B0592BA11DE533       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00009-of-00024</span>.bin

          SHA256          <span class="hljs-number">3</span>E62B8FDC48E4A0F72FB3A5F92B74E4ADCBA2F2F65F18B332EA19EAB305CC770       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00010-of-00024</span>.bin

          SHA256          <span class="hljs-number">274</span>F1B52C8DB4B2A182FDE1AF9840D94DC5C18CA30CC1DE8E2F47D15EF777A88       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00011-of-00024</span>.bin

          SHA256          <span class="hljs-number">51678</span>AF98D98DD4712DB8F14F6D9C55CC601F97A985958FFF1C4A6C77E6E38C7       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00012-of-00024</span>.bin

          SHA256          <span class="hljs-number">55</span>C2365A54055CDB2C9432A4861DDA8F8EA1B8057D098FD53297976150E482C9       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00013-of-00024</span>.bin

          SHA256          <span class="hljs-number">7</span>E9DEE41C3CDC3F7FCE9D23B5BD2D15AB94CE603CAC360E4A5E8BDCFEDCF2453       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00014-of-00024</span>.bin

          SHA256          <span class="hljs-number">431</span>BA6777E57BF184E07FBF19B19C2FF11A63F524C0F3B43B7F9A6C6AD2B7FBE       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00015-of-00024</span>.bin

          SHA256          CB2AC86E13CD3352B4299C78DC87CA7AA9C09A8EA82F2D96660BB39272A248F5       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00016-of-00024</span>.bin

          SHA256          ABC02772BC996E04BF4ED576F679A54B2E26904BC6881A74ED82A54D68799AC1       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00017-of-00024</span>.bin

          SHA256          <span class="hljs-number">22</span>EB0D7A4B0E1D430E9B208DA108521AED410C2B87EFE81F8699516DDE372716       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00018-of-00024</span>.bin

          SHA256          <span class="hljs-number">9</span>FCE38CE560E92EEECE946B60188F46B476574C1B5183513C483908739E6B7D6       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00019-of-00024</span>.bin

          SHA256          <span class="hljs-number">327</span>DEB0B6547A4B1B281188608086AC2FE5CFE5D157DA27C77A0D7EE1846647D       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00020-of-00024</span>.bin

          SHA256          E94589111171D1A634DEFD4462FD2B2B3DF6EF51C9F4D7313D0E13486D087ABE       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00021-of-00024</span>.bin

          SHA256          <span class="hljs-number">8</span>BE9A89C7FC63596C251121EF799F35F60CEBA22CBDCA8D68B0C283FE321D803       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00022-of-00024</span>.bin

          SHA256          E952F146D9BCDED86E09F935290636993A307BD4FB0F75425DC080BEAF74EC22       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00023-of-00024</span>.bin

          SHA256          C867E8E7D6C2CB8D52E55C7A82A02C2D97E48CA965C986B51813F34AA7C797E0       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00024-of-00024</span>.bin

          SHA256          <span class="hljs-number">71</span>C47CECFF8AF76F8247098000694A47000FBB0268C02CF3FA6AD8BC97A35F6A       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model.bin.index.json

          SHA256          <span class="hljs-number">44</span>D3B2B486728D1946EF5330D909EF36E36250BB7ED70164B91C9B004106DBBC       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\README.md

          SHA256          <span class="hljs-number">42</span>E10E2B1078436869C03BE50600A60ACD2C018B48224169E4F504E8382B77E1       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\special_tokens_map.json

          SHA256          <span class="hljs-number">40</span>F7BE569328E60645C00D732B7CE81E2DA455CC35145C6C1D2C94E395FF556C       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\tokenizer_config.json

          SHA256          <span class="hljs-number">7</span>BBB48CE49C00135753F4F8D093893650C3D249352B0672ED4A1B68DE236473F       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\tokenizer.json

          SHA256          D16FB023FB71C56B63E09C51631A3BC56297CAF04E60CEB1812614569DE3DBDD       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\tokenizer.model

          </code></pre>

          <p>Kind regards,<br>Fangru Shao</p>

          '
        raw: "Sadly it might still not be working properly. I am using the Q2_K quantization\
          \ of the model.\n![Snipaste_2024-01-17_03-00-47.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/BG1-UNK_SAkUoFHLVRYfG.png)\n\
          Here are the checksums of my files. \n```powershell\nAlgorithm       Hash\
          \                                                                   Path\n\
          ---------       ----                                                   \
          \                ----\nSHA256          EB45E5C5BF988775AF2CEC6524040BD88EBC002A83891DBDA052AB59CBF3F628\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\added_tokens.json\nSHA256\
          \          DA1F80396552FD1AF2F8FF019F3D17FA4E2903C5127F501249FC9C9DBD4DDE64\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\config.json\nSHA256 \
          \         B77BE1183605A19D3B3A115236EBEB8B15B04B784CCC386EC0B2B782D597A216\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\generation_config.json\n\
          SHA256          07FB40C64E2F645EBBB4450A5FB9882EDA477EC3B9DE629B422BD16557112CC8\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00001-of-00024.bin\n\
          SHA256          C6395064F3EBBF2DC4A9C5231D8FAE64E8441DEBF1BA69B5B387261AC6DB6B3F\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00002-of-00024.bin\n\
          SHA256          945CF5F3608DE8F01E0B31F0E9CB1A1FD0A1809F371B88B24877132E75635D33\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00003-of-00024.bin\n\
          SHA256          800A35D317B6AA9F4DE98CF0B0264FFE8D563750B0055555456F4EAFF2F9C5C3\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00004-of-00024.bin\n\
          SHA256          2125CB4B9EA54182E6ADA059022C5FC6BFAF06F25AEBE198F6CCA1913F5C918D\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00005-of-00024.bin\n\
          SHA256          420E6788E175CB6EB89CB57110A11A2A501EDD5F02B9FBA138B59AAB3DB51A8A\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00006-of-00024.bin\n\
          SHA256          B3322275DFFF7840BF38CAD7C26B94BB4DC32E0F308D85FED4BCC3DD74C03141\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00007-of-00024.bin\n\
          SHA256          4C0C938DF6A934B7A08467019FBE75EBAAAA4369E9A27614A1A7B84735B171A5\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00008-of-00024.bin\n\
          SHA256          DFEBA74B2278133351B9FC210A4CFB6A13AB8C518EA8B32113B0592BA11DE533\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00009-of-00024.bin\n\
          SHA256          3E62B8FDC48E4A0F72FB3A5F92B74E4ADCBA2F2F65F18B332EA19EAB305CC770\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00010-of-00024.bin\n\
          SHA256          274F1B52C8DB4B2A182FDE1AF9840D94DC5C18CA30CC1DE8E2F47D15EF777A88\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00011-of-00024.bin\n\
          SHA256          51678AF98D98DD4712DB8F14F6D9C55CC601F97A985958FFF1C4A6C77E6E38C7\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00012-of-00024.bin\n\
          SHA256          55C2365A54055CDB2C9432A4861DDA8F8EA1B8057D098FD53297976150E482C9\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00013-of-00024.bin\n\
          SHA256          7E9DEE41C3CDC3F7FCE9D23B5BD2D15AB94CE603CAC360E4A5E8BDCFEDCF2453\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00014-of-00024.bin\n\
          SHA256          431BA6777E57BF184E07FBF19B19C2FF11A63F524C0F3B43B7F9A6C6AD2B7FBE\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00015-of-00024.bin\n\
          SHA256          CB2AC86E13CD3352B4299C78DC87CA7AA9C09A8EA82F2D96660BB39272A248F5\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00016-of-00024.bin\n\
          SHA256          ABC02772BC996E04BF4ED576F679A54B2E26904BC6881A74ED82A54D68799AC1\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00017-of-00024.bin\n\
          SHA256          22EB0D7A4B0E1D430E9B208DA108521AED410C2B87EFE81F8699516DDE372716\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00018-of-00024.bin\n\
          SHA256          9FCE38CE560E92EEECE946B60188F46B476574C1B5183513C483908739E6B7D6\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00019-of-00024.bin\n\
          SHA256          327DEB0B6547A4B1B281188608086AC2FE5CFE5D157DA27C77A0D7EE1846647D\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00020-of-00024.bin\n\
          SHA256          E94589111171D1A634DEFD4462FD2B2B3DF6EF51C9F4D7313D0E13486D087ABE\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00021-of-00024.bin\n\
          SHA256          8BE9A89C7FC63596C251121EF799F35F60CEBA22CBDCA8D68B0C283FE321D803\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00022-of-00024.bin\n\
          SHA256          E952F146D9BCDED86E09F935290636993A307BD4FB0F75425DC080BEAF74EC22\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00023-of-00024.bin\n\
          SHA256          C867E8E7D6C2CB8D52E55C7A82A02C2D97E48CA965C986B51813F34AA7C797E0\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00024-of-00024.bin\n\
          SHA256          71C47CECFF8AF76F8247098000694A47000FBB0268C02CF3FA6AD8BC97A35F6A\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model.bin.index.json\n\
          SHA256          44D3B2B486728D1946EF5330D909EF36E36250BB7ED70164B91C9B004106DBBC\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\README.md\nSHA256   \
          \       42E10E2B1078436869C03BE50600A60ACD2C018B48224169E4F504E8382B77E1\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\special_tokens_map.json\n\
          SHA256          40F7BE569328E60645C00D732B7CE81E2DA455CC35145C6C1D2C94E395FF556C\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\tokenizer_config.json\n\
          SHA256          7BBB48CE49C00135753F4F8D093893650C3D249352B0672ED4A1B68DE236473F\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\tokenizer.json\nSHA256\
          \          D16FB023FB71C56B63E09C51631A3BC56297CAF04E60CEB1812614569DE3DBDD\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\tokenizer.model\n```\n\
          \nKind regards,\nFangru Shao"
        updatedAt: '2024-01-16T19:09:26.171Z'
      numEdits: 0
      reactions: []
    id: 65a6d466a57bd99a51350d50
    type: comment
  author: MatrixC7
  content: "Sadly it might still not be working properly. I am using the Q2_K quantization\
    \ of the model.\n![Snipaste_2024-01-17_03-00-47.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/BG1-UNK_SAkUoFHLVRYfG.png)\n\
    Here are the checksums of my files. \n```powershell\nAlgorithm       Hash    \
    \                                                               Path\n---------\
    \       ----                                                                 \
    \  ----\nSHA256          EB45E5C5BF988775AF2CEC6524040BD88EBC002A83891DBDA052AB59CBF3F628\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\added_tokens.json\nSHA256 \
    \         DA1F80396552FD1AF2F8FF019F3D17FA4E2903C5127F501249FC9C9DBD4DDE64   \
    \    F:\\llm-models-raw\\chat-goliath-120b-80k\\config.json\nSHA256          B77BE1183605A19D3B3A115236EBEB8B15B04B784CCC386EC0B2B782D597A216\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\generation_config.json\nSHA256\
    \          07FB40C64E2F645EBBB4450A5FB9882EDA477EC3B9DE629B422BD16557112CC8  \
    \     F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00001-of-00024.bin\n\
    SHA256          C6395064F3EBBF2DC4A9C5231D8FAE64E8441DEBF1BA69B5B387261AC6DB6B3F\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00002-of-00024.bin\n\
    SHA256          945CF5F3608DE8F01E0B31F0E9CB1A1FD0A1809F371B88B24877132E75635D33\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00003-of-00024.bin\n\
    SHA256          800A35D317B6AA9F4DE98CF0B0264FFE8D563750B0055555456F4EAFF2F9C5C3\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00004-of-00024.bin\n\
    SHA256          2125CB4B9EA54182E6ADA059022C5FC6BFAF06F25AEBE198F6CCA1913F5C918D\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00005-of-00024.bin\n\
    SHA256          420E6788E175CB6EB89CB57110A11A2A501EDD5F02B9FBA138B59AAB3DB51A8A\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00006-of-00024.bin\n\
    SHA256          B3322275DFFF7840BF38CAD7C26B94BB4DC32E0F308D85FED4BCC3DD74C03141\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00007-of-00024.bin\n\
    SHA256          4C0C938DF6A934B7A08467019FBE75EBAAAA4369E9A27614A1A7B84735B171A5\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00008-of-00024.bin\n\
    SHA256          DFEBA74B2278133351B9FC210A4CFB6A13AB8C518EA8B32113B0592BA11DE533\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00009-of-00024.bin\n\
    SHA256          3E62B8FDC48E4A0F72FB3A5F92B74E4ADCBA2F2F65F18B332EA19EAB305CC770\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00010-of-00024.bin\n\
    SHA256          274F1B52C8DB4B2A182FDE1AF9840D94DC5C18CA30CC1DE8E2F47D15EF777A88\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00011-of-00024.bin\n\
    SHA256          51678AF98D98DD4712DB8F14F6D9C55CC601F97A985958FFF1C4A6C77E6E38C7\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00012-of-00024.bin\n\
    SHA256          55C2365A54055CDB2C9432A4861DDA8F8EA1B8057D098FD53297976150E482C9\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00013-of-00024.bin\n\
    SHA256          7E9DEE41C3CDC3F7FCE9D23B5BD2D15AB94CE603CAC360E4A5E8BDCFEDCF2453\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00014-of-00024.bin\n\
    SHA256          431BA6777E57BF184E07FBF19B19C2FF11A63F524C0F3B43B7F9A6C6AD2B7FBE\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00015-of-00024.bin\n\
    SHA256          CB2AC86E13CD3352B4299C78DC87CA7AA9C09A8EA82F2D96660BB39272A248F5\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00016-of-00024.bin\n\
    SHA256          ABC02772BC996E04BF4ED576F679A54B2E26904BC6881A74ED82A54D68799AC1\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00017-of-00024.bin\n\
    SHA256          22EB0D7A4B0E1D430E9B208DA108521AED410C2B87EFE81F8699516DDE372716\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00018-of-00024.bin\n\
    SHA256          9FCE38CE560E92EEECE946B60188F46B476574C1B5183513C483908739E6B7D6\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00019-of-00024.bin\n\
    SHA256          327DEB0B6547A4B1B281188608086AC2FE5CFE5D157DA27C77A0D7EE1846647D\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00020-of-00024.bin\n\
    SHA256          E94589111171D1A634DEFD4462FD2B2B3DF6EF51C9F4D7313D0E13486D087ABE\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00021-of-00024.bin\n\
    SHA256          8BE9A89C7FC63596C251121EF799F35F60CEBA22CBDCA8D68B0C283FE321D803\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00022-of-00024.bin\n\
    SHA256          E952F146D9BCDED86E09F935290636993A307BD4FB0F75425DC080BEAF74EC22\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00023-of-00024.bin\n\
    SHA256          C867E8E7D6C2CB8D52E55C7A82A02C2D97E48CA965C986B51813F34AA7C797E0\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00024-of-00024.bin\n\
    SHA256          71C47CECFF8AF76F8247098000694A47000FBB0268C02CF3FA6AD8BC97A35F6A\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model.bin.index.json\n\
    SHA256          44D3B2B486728D1946EF5330D909EF36E36250BB7ED70164B91C9B004106DBBC\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\README.md\nSHA256         \
    \ 42E10E2B1078436869C03BE50600A60ACD2C018B48224169E4F504E8382B77E1       F:\\\
    llm-models-raw\\chat-goliath-120b-80k\\special_tokens_map.json\nSHA256       \
    \   40F7BE569328E60645C00D732B7CE81E2DA455CC35145C6C1D2C94E395FF556C       F:\\\
    llm-models-raw\\chat-goliath-120b-80k\\tokenizer_config.json\nSHA256         \
    \ 7BBB48CE49C00135753F4F8D093893650C3D249352B0672ED4A1B68DE236473F       F:\\\
    llm-models-raw\\chat-goliath-120b-80k\\tokenizer.json\nSHA256          D16FB023FB71C56B63E09C51631A3BC56297CAF04E60CEB1812614569DE3DBDD\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\tokenizer.model\n```\n\nKind\
    \ regards,\nFangru Shao"
  created_at: 2024-01-16 19:09:26+00:00
  edited: false
  hidden: false
  id: 65a6d466a57bd99a51350d50
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64265252a5ec4a5cbc52fe48/M0t275nR0Vw79bTORMpu9.png?w=200&h=200&f=face
      fullname: hongyin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: hongyin
      type: user
    createdAt: '2024-01-17T01:02:28.000Z'
    data:
      edited: false
      editors:
      - hongyin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2550750970840454
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64265252a5ec4a5cbc52fe48/M0t275nR0Vw79bTORMpu9.png?w=200&h=200&f=face
          fullname: hongyin
          isHf: false
          isPro: false
          name: hongyin
          type: user
        html: '<blockquote>

          <p>Sadly it might still not be working properly. I am using the Q2_K quantization
          of the model.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/BG1-UNK_SAkUoFHLVRYfG.png"><img
          alt="Snipaste_2024-01-17_03-00-47.png" src="https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/BG1-UNK_SAkUoFHLVRYfG.png"></a><br>Here
          are the checksums of my files. </p>

          <pre><code class="language-powershell">Algorithm       Hash                                                                   Path

          <span class="hljs-literal">---------</span>       <span class="hljs-literal">----</span>                                                                   <span
          class="hljs-literal">----</span>

          SHA256          EB45E5C5BF988775AF2CEC6524040BD88EBC002A83891DBDA052AB59CBF3F628       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\added_tokens.json

          SHA256          DA1F80396552FD1AF2F8FF019F3D17FA4E2903C5127F501249FC9C9DBD4DDE64       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\config.json

          SHA256          B77BE1183605A19D3B3A115236EBEB8B15B04B784CCC386EC0B2B782D597A216       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\generation_config.json

          SHA256          <span class="hljs-number">07</span>FB40C64E2F645EBBB4450A5FB9882EDA477EC3B9DE629B422BD16557112CC8       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00001-of-00024</span>.bin

          SHA256          C6395064F3EBBF2DC4A9C5231D8FAE64E8441DEBF1BA69B5B387261AC6DB6B3F       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00002-of-00024</span>.bin

          SHA256          <span class="hljs-number">945</span>CF5F3608DE8F01E0B31F0E9CB1A1FD0A1809F371B88B24877132E75635D33       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00003-of-00024</span>.bin

          SHA256          <span class="hljs-number">800</span>A35D317B6AA9F4DE98CF0B0264FFE8D563750B0055555456F4EAFF2F9C5C3       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00004-of-00024</span>.bin

          SHA256          <span class="hljs-number">2125</span>CB4B9EA54182E6ADA059022C5FC6BFAF06F25AEBE198F6CCA1913F5C918D       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00005-of-00024</span>.bin

          SHA256          <span class="hljs-number">420</span>E6788E175CB6EB89CB57110A11A2A501EDD5F02B9FBA138B59AAB3DB51A8A       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00006-of-00024</span>.bin

          SHA256          B3322275DFFF7840BF38CAD7C26B94BB4DC32E0F308D85FED4BCC3DD74C03141       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00007-of-00024</span>.bin

          SHA256          <span class="hljs-number">4</span>C0C938DF6A934B7A08467019FBE75EBAAAA4369E9A27614A1A7B84735B171A5       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00008-of-00024</span>.bin

          SHA256          DFEBA74B2278133351B9FC210A4CFB6A13AB8C518EA8B32113B0592BA11DE533       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00009-of-00024</span>.bin

          SHA256          <span class="hljs-number">3</span>E62B8FDC48E4A0F72FB3A5F92B74E4ADCBA2F2F65F18B332EA19EAB305CC770       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00010-of-00024</span>.bin

          SHA256          <span class="hljs-number">274</span>F1B52C8DB4B2A182FDE1AF9840D94DC5C18CA30CC1DE8E2F47D15EF777A88       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00011-of-00024</span>.bin

          SHA256          <span class="hljs-number">51678</span>AF98D98DD4712DB8F14F6D9C55CC601F97A985958FFF1C4A6C77E6E38C7       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00012-of-00024</span>.bin

          SHA256          <span class="hljs-number">55</span>C2365A54055CDB2C9432A4861DDA8F8EA1B8057D098FD53297976150E482C9       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00013-of-00024</span>.bin

          SHA256          <span class="hljs-number">7</span>E9DEE41C3CDC3F7FCE9D23B5BD2D15AB94CE603CAC360E4A5E8BDCFEDCF2453       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00014-of-00024</span>.bin

          SHA256          <span class="hljs-number">431</span>BA6777E57BF184E07FBF19B19C2FF11A63F524C0F3B43B7F9A6C6AD2B7FBE       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00015-of-00024</span>.bin

          SHA256          CB2AC86E13CD3352B4299C78DC87CA7AA9C09A8EA82F2D96660BB39272A248F5       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00016-of-00024</span>.bin

          SHA256          ABC02772BC996E04BF4ED576F679A54B2E26904BC6881A74ED82A54D68799AC1       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00017-of-00024</span>.bin

          SHA256          <span class="hljs-number">22</span>EB0D7A4B0E1D430E9B208DA108521AED410C2B87EFE81F8699516DDE372716       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00018-of-00024</span>.bin

          SHA256          <span class="hljs-number">9</span>FCE38CE560E92EEECE946B60188F46B476574C1B5183513C483908739E6B7D6       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00019-of-00024</span>.bin

          SHA256          <span class="hljs-number">327</span>DEB0B6547A4B1B281188608086AC2FE5CFE5D157DA27C77A0D7EE1846647D       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00020-of-00024</span>.bin

          SHA256          E94589111171D1A634DEFD4462FD2B2B3DF6EF51C9F4D7313D0E13486D087ABE       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00021-of-00024</span>.bin

          SHA256          <span class="hljs-number">8</span>BE9A89C7FC63596C251121EF799F35F60CEBA22CBDCA8D68B0C283FE321D803       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00022-of-00024</span>.bin

          SHA256          E952F146D9BCDED86E09F935290636993A307BD4FB0F75425DC080BEAF74EC22       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00023-of-00024</span>.bin

          SHA256          C867E8E7D6C2CB8D52E55C7A82A02C2D97E48CA965C986B51813F34AA7C797E0       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model<span
          class="hljs-literal">-00024-of-00024</span>.bin

          SHA256          <span class="hljs-number">71</span>C47CECFF8AF76F8247098000694A47000FBB0268C02CF3FA6AD8BC97A35F6A       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\pytorch_model.bin.index.json

          SHA256          <span class="hljs-number">44</span>D3B2B486728D1946EF5330D909EF36E36250BB7ED70164B91C9B004106DBBC       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\README.md

          SHA256          <span class="hljs-number">42</span>E10E2B1078436869C03BE50600A60ACD2C018B48224169E4F504E8382B77E1       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\special_tokens_map.json

          SHA256          <span class="hljs-number">40</span>F7BE569328E60645C00D732B7CE81E2DA455CC35145C6C1D2C94E395FF556C       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\tokenizer_config.json

          SHA256          <span class="hljs-number">7</span>BBB48CE49C00135753F4F8D093893650C3D249352B0672ED4A1B68DE236473F       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\tokenizer.json

          SHA256          D16FB023FB71C56B63E09C51631A3BC56297CAF04E60CEB1812614569DE3DBDD       F:\llm<span
          class="hljs-literal">-models-raw</span>\chat<span class="hljs-literal">-goliath-120b-80k</span>\tokenizer.model

          </code></pre>

          <p>Kind regards,<br>Fangru Shao</p>

          </blockquote>

          <p>I suggest you input the following template directly.</p>

          <p>input_str = "Human: How is an earthquake is measured?  \nAssistant:"</p>

          <p>"Human:" and "Assistant:" are special symbols.</p>

          '
        raw: "> Sadly it might still not be working properly. I am using the Q2_K\
          \ quantization of the model.\n> ![Snipaste_2024-01-17_03-00-47.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/BG1-UNK_SAkUoFHLVRYfG.png)\n\
          > Here are the checksums of my files. \n> ```powershell\n> Algorithm   \
          \    Hash                                                              \
          \     Path\n> ---------       ----                                     \
          \                              ----\n> SHA256          EB45E5C5BF988775AF2CEC6524040BD88EBC002A83891DBDA052AB59CBF3F628\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\added_tokens.json\n>\
          \ SHA256          DA1F80396552FD1AF2F8FF019F3D17FA4E2903C5127F501249FC9C9DBD4DDE64\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\config.json\n> SHA256\
          \          B77BE1183605A19D3B3A115236EBEB8B15B04B784CCC386EC0B2B782D597A216\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\generation_config.json\n\
          > SHA256          07FB40C64E2F645EBBB4450A5FB9882EDA477EC3B9DE629B422BD16557112CC8\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00001-of-00024.bin\n\
          > SHA256          C6395064F3EBBF2DC4A9C5231D8FAE64E8441DEBF1BA69B5B387261AC6DB6B3F\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00002-of-00024.bin\n\
          > SHA256          945CF5F3608DE8F01E0B31F0E9CB1A1FD0A1809F371B88B24877132E75635D33\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00003-of-00024.bin\n\
          > SHA256          800A35D317B6AA9F4DE98CF0B0264FFE8D563750B0055555456F4EAFF2F9C5C3\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00004-of-00024.bin\n\
          > SHA256          2125CB4B9EA54182E6ADA059022C5FC6BFAF06F25AEBE198F6CCA1913F5C918D\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00005-of-00024.bin\n\
          > SHA256          420E6788E175CB6EB89CB57110A11A2A501EDD5F02B9FBA138B59AAB3DB51A8A\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00006-of-00024.bin\n\
          > SHA256          B3322275DFFF7840BF38CAD7C26B94BB4DC32E0F308D85FED4BCC3DD74C03141\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00007-of-00024.bin\n\
          > SHA256          4C0C938DF6A934B7A08467019FBE75EBAAAA4369E9A27614A1A7B84735B171A5\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00008-of-00024.bin\n\
          > SHA256          DFEBA74B2278133351B9FC210A4CFB6A13AB8C518EA8B32113B0592BA11DE533\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00009-of-00024.bin\n\
          > SHA256          3E62B8FDC48E4A0F72FB3A5F92B74E4ADCBA2F2F65F18B332EA19EAB305CC770\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00010-of-00024.bin\n\
          > SHA256          274F1B52C8DB4B2A182FDE1AF9840D94DC5C18CA30CC1DE8E2F47D15EF777A88\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00011-of-00024.bin\n\
          > SHA256          51678AF98D98DD4712DB8F14F6D9C55CC601F97A985958FFF1C4A6C77E6E38C7\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00012-of-00024.bin\n\
          > SHA256          55C2365A54055CDB2C9432A4861DDA8F8EA1B8057D098FD53297976150E482C9\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00013-of-00024.bin\n\
          > SHA256          7E9DEE41C3CDC3F7FCE9D23B5BD2D15AB94CE603CAC360E4A5E8BDCFEDCF2453\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00014-of-00024.bin\n\
          > SHA256          431BA6777E57BF184E07FBF19B19C2FF11A63F524C0F3B43B7F9A6C6AD2B7FBE\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00015-of-00024.bin\n\
          > SHA256          CB2AC86E13CD3352B4299C78DC87CA7AA9C09A8EA82F2D96660BB39272A248F5\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00016-of-00024.bin\n\
          > SHA256          ABC02772BC996E04BF4ED576F679A54B2E26904BC6881A74ED82A54D68799AC1\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00017-of-00024.bin\n\
          > SHA256          22EB0D7A4B0E1D430E9B208DA108521AED410C2B87EFE81F8699516DDE372716\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00018-of-00024.bin\n\
          > SHA256          9FCE38CE560E92EEECE946B60188F46B476574C1B5183513C483908739E6B7D6\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00019-of-00024.bin\n\
          > SHA256          327DEB0B6547A4B1B281188608086AC2FE5CFE5D157DA27C77A0D7EE1846647D\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00020-of-00024.bin\n\
          > SHA256          E94589111171D1A634DEFD4462FD2B2B3DF6EF51C9F4D7313D0E13486D087ABE\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00021-of-00024.bin\n\
          > SHA256          8BE9A89C7FC63596C251121EF799F35F60CEBA22CBDCA8D68B0C283FE321D803\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00022-of-00024.bin\n\
          > SHA256          E952F146D9BCDED86E09F935290636993A307BD4FB0F75425DC080BEAF74EC22\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00023-of-00024.bin\n\
          > SHA256          C867E8E7D6C2CB8D52E55C7A82A02C2D97E48CA965C986B51813F34AA7C797E0\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00024-of-00024.bin\n\
          > SHA256          71C47CECFF8AF76F8247098000694A47000FBB0268C02CF3FA6AD8BC97A35F6A\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model.bin.index.json\n\
          > SHA256          44D3B2B486728D1946EF5330D909EF36E36250BB7ED70164B91C9B004106DBBC\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\README.md\n> SHA256 \
          \         42E10E2B1078436869C03BE50600A60ACD2C018B48224169E4F504E8382B77E1\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\special_tokens_map.json\n\
          > SHA256          40F7BE569328E60645C00D732B7CE81E2DA455CC35145C6C1D2C94E395FF556C\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\tokenizer_config.json\n\
          > SHA256          7BBB48CE49C00135753F4F8D093893650C3D249352B0672ED4A1B68DE236473F\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\tokenizer.json\n> SHA256\
          \          D16FB023FB71C56B63E09C51631A3BC56297CAF04E60CEB1812614569DE3DBDD\
          \       F:\\llm-models-raw\\chat-goliath-120b-80k\\tokenizer.model\n> ```\n\
          > \n> Kind regards,\n> Fangru Shao\n\nI suggest you input the following\
          \ template directly.\n\ninput_str = \"Human: How is an earthquake is measured?\
          \  \\nAssistant:\"\n\n\"Human:\" and \"Assistant:\" are special symbols.\n\
          \n\n"
        updatedAt: '2024-01-17T01:02:28.577Z'
      numEdits: 0
      reactions: []
    id: 65a727240637ea5cccd15507
    type: comment
  author: hongyin
  content: "> Sadly it might still not be working properly. I am using the Q2_K quantization\
    \ of the model.\n> ![Snipaste_2024-01-17_03-00-47.png](https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/BG1-UNK_SAkUoFHLVRYfG.png)\n\
    > Here are the checksums of my files. \n> ```powershell\n> Algorithm       Hash\
    \                                                                   Path\n> ---------\
    \       ----                                                                 \
    \  ----\n> SHA256          EB45E5C5BF988775AF2CEC6524040BD88EBC002A83891DBDA052AB59CBF3F628\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\added_tokens.json\n> SHA256\
    \          DA1F80396552FD1AF2F8FF019F3D17FA4E2903C5127F501249FC9C9DBD4DDE64  \
    \     F:\\llm-models-raw\\chat-goliath-120b-80k\\config.json\n> SHA256       \
    \   B77BE1183605A19D3B3A115236EBEB8B15B04B784CCC386EC0B2B782D597A216       F:\\\
    llm-models-raw\\chat-goliath-120b-80k\\generation_config.json\n> SHA256      \
    \    07FB40C64E2F645EBBB4450A5FB9882EDA477EC3B9DE629B422BD16557112CC8       F:\\\
    llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00001-of-00024.bin\n> SHA256\
    \          C6395064F3EBBF2DC4A9C5231D8FAE64E8441DEBF1BA69B5B387261AC6DB6B3F  \
    \     F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00002-of-00024.bin\n\
    > SHA256          945CF5F3608DE8F01E0B31F0E9CB1A1FD0A1809F371B88B24877132E75635D33\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00003-of-00024.bin\n\
    > SHA256          800A35D317B6AA9F4DE98CF0B0264FFE8D563750B0055555456F4EAFF2F9C5C3\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00004-of-00024.bin\n\
    > SHA256          2125CB4B9EA54182E6ADA059022C5FC6BFAF06F25AEBE198F6CCA1913F5C918D\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00005-of-00024.bin\n\
    > SHA256          420E6788E175CB6EB89CB57110A11A2A501EDD5F02B9FBA138B59AAB3DB51A8A\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00006-of-00024.bin\n\
    > SHA256          B3322275DFFF7840BF38CAD7C26B94BB4DC32E0F308D85FED4BCC3DD74C03141\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00007-of-00024.bin\n\
    > SHA256          4C0C938DF6A934B7A08467019FBE75EBAAAA4369E9A27614A1A7B84735B171A5\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00008-of-00024.bin\n\
    > SHA256          DFEBA74B2278133351B9FC210A4CFB6A13AB8C518EA8B32113B0592BA11DE533\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00009-of-00024.bin\n\
    > SHA256          3E62B8FDC48E4A0F72FB3A5F92B74E4ADCBA2F2F65F18B332EA19EAB305CC770\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00010-of-00024.bin\n\
    > SHA256          274F1B52C8DB4B2A182FDE1AF9840D94DC5C18CA30CC1DE8E2F47D15EF777A88\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00011-of-00024.bin\n\
    > SHA256          51678AF98D98DD4712DB8F14F6D9C55CC601F97A985958FFF1C4A6C77E6E38C7\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00012-of-00024.bin\n\
    > SHA256          55C2365A54055CDB2C9432A4861DDA8F8EA1B8057D098FD53297976150E482C9\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00013-of-00024.bin\n\
    > SHA256          7E9DEE41C3CDC3F7FCE9D23B5BD2D15AB94CE603CAC360E4A5E8BDCFEDCF2453\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00014-of-00024.bin\n\
    > SHA256          431BA6777E57BF184E07FBF19B19C2FF11A63F524C0F3B43B7F9A6C6AD2B7FBE\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00015-of-00024.bin\n\
    > SHA256          CB2AC86E13CD3352B4299C78DC87CA7AA9C09A8EA82F2D96660BB39272A248F5\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00016-of-00024.bin\n\
    > SHA256          ABC02772BC996E04BF4ED576F679A54B2E26904BC6881A74ED82A54D68799AC1\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00017-of-00024.bin\n\
    > SHA256          22EB0D7A4B0E1D430E9B208DA108521AED410C2B87EFE81F8699516DDE372716\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00018-of-00024.bin\n\
    > SHA256          9FCE38CE560E92EEECE946B60188F46B476574C1B5183513C483908739E6B7D6\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00019-of-00024.bin\n\
    > SHA256          327DEB0B6547A4B1B281188608086AC2FE5CFE5D157DA27C77A0D7EE1846647D\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00020-of-00024.bin\n\
    > SHA256          E94589111171D1A634DEFD4462FD2B2B3DF6EF51C9F4D7313D0E13486D087ABE\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00021-of-00024.bin\n\
    > SHA256          8BE9A89C7FC63596C251121EF799F35F60CEBA22CBDCA8D68B0C283FE321D803\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00022-of-00024.bin\n\
    > SHA256          E952F146D9BCDED86E09F935290636993A307BD4FB0F75425DC080BEAF74EC22\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00023-of-00024.bin\n\
    > SHA256          C867E8E7D6C2CB8D52E55C7A82A02C2D97E48CA965C986B51813F34AA7C797E0\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model-00024-of-00024.bin\n\
    > SHA256          71C47CECFF8AF76F8247098000694A47000FBB0268C02CF3FA6AD8BC97A35F6A\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\pytorch_model.bin.index.json\n\
    > SHA256          44D3B2B486728D1946EF5330D909EF36E36250BB7ED70164B91C9B004106DBBC\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\README.md\n> SHA256       \
    \   42E10E2B1078436869C03BE50600A60ACD2C018B48224169E4F504E8382B77E1       F:\\\
    llm-models-raw\\chat-goliath-120b-80k\\special_tokens_map.json\n> SHA256     \
    \     40F7BE569328E60645C00D732B7CE81E2DA455CC35145C6C1D2C94E395FF556C       F:\\\
    llm-models-raw\\chat-goliath-120b-80k\\tokenizer_config.json\n> SHA256       \
    \   7BBB48CE49C00135753F4F8D093893650C3D249352B0672ED4A1B68DE236473F       F:\\\
    llm-models-raw\\chat-goliath-120b-80k\\tokenizer.json\n> SHA256          D16FB023FB71C56B63E09C51631A3BC56297CAF04E60CEB1812614569DE3DBDD\
    \       F:\\llm-models-raw\\chat-goliath-120b-80k\\tokenizer.model\n> ```\n> \n\
    > Kind regards,\n> Fangru Shao\n\nI suggest you input the following template directly.\n\
    \ninput_str = \"Human: How is an earthquake is measured?  \\nAssistant:\"\n\n\"\
    Human:\" and \"Assistant:\" are special symbols.\n\n\n"
  created_at: 2024-01-17 01:02:28+00:00
  edited: false
  hidden: false
  id: 65a727240637ea5cccd15507
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/-OLDRiEOWDgqQUCKXigXD.jpeg?w=200&h=200&f=face
      fullname: Fangru Shao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MatrixC7
      type: user
    createdAt: '2024-01-17T06:49:13.000Z'
    data:
      edited: false
      editors:
      - MatrixC7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5628982782363892
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/658e9da1c04427eb3836833f/-OLDRiEOWDgqQUCKXigXD.jpeg?w=200&h=200&f=face
          fullname: Fangru Shao
          isHf: false
          isPro: false
          name: MatrixC7
          type: user
        html: "<p>Still no luck :(<br>To confirm, am I doing it appropriately?<br>Below\
          \ is the full code and you could also see there is an error for llm_load_vocab\
          \ as <code>llm_load_vocab: mismatch in special tokens definition ( 1151/49300\
          \ vs 259/49300 ).</code>.</p>\n<pre><code class=\"language-powershell\"\
          >PowerShell <span class=\"hljs-number\">7.4</span>.<span class=\"hljs-number\"\
          >1</span>\n<span class=\"hljs-built_in\">PS</span> F:\\llama.cpp\\build\\\
          bin\\Release&gt; .\\main.exe <span class=\"hljs-literal\">-m</span> F:\\\
          chat<span class=\"hljs-literal\">-goliath-120b-80k-q2_k</span>.gguf <span\
          \ class=\"hljs-literal\">--prompt</span> <span class=\"hljs-string\">\"\
          Human: How is an earthquake is measured? \\nAssistant: \"</span>\nLog <span\
          \ class=\"hljs-built_in\">start</span>\nmain: build = <span class=\"hljs-number\"\
          >1879</span> (<span class=\"hljs-number\">3</span>e5ca79)\nmain: built with\
          \ MSVC <span class=\"hljs-number\">19.38</span>.<span class=\"hljs-number\"\
          >33134.0</span> <span class=\"hljs-keyword\">for</span> x64\nmain: seed\
          \  = <span class=\"hljs-number\">1705473913</span>\nllama_model_loader:\
          \ loaded meta <span class=\"hljs-keyword\">data</span> with <span class=\"\
          hljs-number\">21</span> key<span class=\"hljs-literal\">-value</span> pairs\
          \ and <span class=\"hljs-number\">1236</span> tensors from F:\\chat<span\
          \ class=\"hljs-literal\">-goliath-120b-80k-q2_k</span>.gguf (version GGUF\
          \ V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note:\
          \ KV overrides <span class=\"hljs-keyword\">do</span> not apply <span class=\"\
          hljs-keyword\">in</span> this output.\nllama_model_loader: - kv   <span\
          \ class=\"hljs-number\">0</span>:                       general.architecture\
          \ str              = llama\nllama_model_loader: - kv   <span class=\"hljs-number\"\
          >1</span>:                               general.name str              =\
          \ F:\\llm<span class=\"hljs-literal\">-models-raw</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">2</span>:                       llama.context_length\
          \ u32              = <span class=\"hljs-number\">81920</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">3</span>:                     llama.embedding_length\
          \ u32              = <span class=\"hljs-number\">8192</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">4</span>:                         \
          \ llama.block_count u32              = <span class=\"hljs-number\">137</span>\n\
          llama_model_loader: - kv   <span class=\"hljs-number\">5</span>:       \
          \           llama.feed_forward_length u32              = <span class=\"\
          hljs-number\">28672</span>\nllama_model_loader: - kv   <span class=\"hljs-number\"\
          >6</span>:                 llama.rope.dimension_count u32              =\
          \ <span class=\"hljs-number\">128</span>\nllama_model_loader: - kv   <span\
          \ class=\"hljs-number\">7</span>:                 llama.attention.head_count\
          \ u32              = <span class=\"hljs-number\">64</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">8</span>:              llama.attention.head_count_kv\
          \ u32              = <span class=\"hljs-number\">8</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">9</span>:     llama.attention.layer_norm_rms_epsilon\
          \ f32              = <span class=\"hljs-number\">0.000010</span>\nllama_model_loader:\
          \ - kv  <span class=\"hljs-number\">10</span>:                       llama.rope.freq_base\
          \ f32              = <span class=\"hljs-number\">10000.000000</span>\nllama_model_loader:\
          \ - kv  <span class=\"hljs-number\">11</span>:                         \
          \ general.file_type u32              = <span class=\"hljs-number\">10</span>\n\
          llama_model_loader: - kv  <span class=\"hljs-number\">12</span>:       \
          \                tokenizer.ggml.model str              = llama\nllama_model_loader:\
          \ - kv  <span class=\"hljs-number\">13</span>:                      tokenizer.ggml.tokens\
          \ arr[<span class=\"hljs-type\">str</span>,<span class=\"hljs-number\">49300</span>]\
          \   = [<span class=\"hljs-string\">\"&lt;unk&gt;\"</span>, <span class=\"\
          hljs-string\">\"&lt;s&gt;\"</span>, <span class=\"hljs-string\">\"&lt;/s&gt;\"\
          </span>, <span class=\"hljs-string\">\"&lt;0x00&gt;\"</span>, <span class=\"\
          hljs-string\">\"&lt;...</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - kv  14:                      tokenizer.ggml.scores arr[f32,49300]  \
          \ = [0.000000, 0.000000, 0.000000, 0.0000...</span>\n<span class=\"hljs-string\"\
          >llama_model_loader: - kv  15:                  tokenizer.ggml.token_type\
          \ arr[i32,49300]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</span>\n<span\
          \ class=\"hljs-string\">llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id\
          \ u32              = 1</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - kv  17:                tokenizer.ggml.eos_token_id u32             \
          \ = 2</span>\n<span class=\"hljs-string\">llama_model_loader: - kv  18:\
          \            tokenizer.ggml.unknown_token_id u32              = 0</span>\n\
          <span class=\"hljs-string\">llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id\
          \ u32              = 2</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - kv  20:               general.quantization_version u32             \
          \ = 2</span>\n<span class=\"hljs-string\">llama_model_loader: - type  f32:\
          \  275 tensors</span>\n<span class=\"hljs-string\">llama_model_loader: -\
          \ type q2_K:  549 tensors</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - type q3_K:  411 tensors</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - type q6_K:    1 tensors</span>\n<span class=\"hljs-string\">llm_load_vocab:\
          \ mismatch in special tokens definition ( 1151/49300 vs 259/49300 ).</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: format           = GGUF\
          \ V3 (latest)</span>\n<span class=\"hljs-string\">llm_load_print_meta: arch\
          \             = llama</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ vocab type       = SPM</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_vocab          = 49300</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_merges         = 0</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_ctx_train      = 81920</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd           = 8192</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_head           = 64</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_head_kv        = 8</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_layer          = 137</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_rot            = 128</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd_head_k    = 128</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd_head_v    = 128</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_gqa            = 8</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd_k_gqa     = 1024</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd_v_gqa     = 1024</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ f_norm_eps       = 0.0e+00</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ f_norm_rms_eps   = 1.0e-05</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ f_clamp_kqv      = 0.0e+00</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ f_max_alibi_bias = 0.0e+00</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_ff             = 28672</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_expert         = 0</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_expert_used    = 0</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ rope scaling     = linear</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ freq_base_train  = 10000.0</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ freq_scale_train = 1</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_yarn_orig_ctx  = 81920</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ rope_finetuned   = unknown</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ model type       = ?B</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ model ftype      = Q2_K - Medium</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ model params     = 118.03 B</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ model size       = 40.28 GiB (2.93 BPW)</span>\n<span class=\"hljs-string\"\
          >llm_load_print_meta: general.name     = F:\\llm-models-raw</span>\n<span\
          \ class=\"hljs-string\">llm_load_print_meta: BOS token        = 1 '&lt;s&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: EOS token        = 2 '&lt;/s&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: UNK token        = 0 '&lt;unk&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: PAD token        = 2 '&lt;/s&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: LF token         = 13 '&lt;0x0A&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_tensors: ggml ctx size =    0.47 MiB</span>\n\
          <span class=\"hljs-string\">llm_load_tensors: offloading 0 repeating layers\
          \ to GPU</span>\n<span class=\"hljs-string\">llm_load_tensors: offloaded\
          \ 0/138 layers to GPU</span>\n<span class=\"hljs-string\">llm_load_tensors:\
          \        CPU buffer size = 41251.23 MiB</span>\n<span class=\"hljs-string\"\
          >....................................................................................................</span>\n\
          <span class=\"hljs-string\">llama_new_context_with_model: n_ctx      = 512</span>\n\
          <span class=\"hljs-string\">llama_new_context_with_model: freq_base  = 10000.0</span>\n\
          <span class=\"hljs-string\">llama_new_context_with_model: freq_scale = 1</span>\n\
          <span class=\"hljs-string\">llama_kv_cache_init:        CPU KV buffer size\
          \ =   274.00 MiB</span>\n<span class=\"hljs-string\">llama_new_context_with_model:\
          \ KV self size  =  274.00 MiB, K (f16):  137.00 MiB, V (f16):  137.00 MiB</span>\n\
          <span class=\"hljs-string\">llama_new_context_with_model: graph splits (measure):\
          \ 1</span>\n<span class=\"hljs-string\">llama_new_context_with_model:  \
          \      CPU compute buffer size =   145.00 MiB</span>\n<span class=\"hljs-string\"\
          ></span>\n<span class=\"hljs-string\">system_info: n_threads = 16 / 32 |\
          \ AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
          \ = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
          \ = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |</span>\n<span class=\"\
          hljs-string\">sampling:</span>\n<span class=\"hljs-string\">        repeat_last_n\
          \ = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty\
          \ = 0.000</span>\n<span class=\"hljs-string\">        top_k = 40, tfs_z\
          \ = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800</span>\n\
          <span class=\"hljs-string\">        mirostat = 0, mirostat_lr = 0.100, mirostat_ent\
          \ = 5.000</span>\n<span class=\"hljs-string\">sampling order:</span>\n<span\
          \ class=\"hljs-string\">CFG -&gt; Penalties -&gt; top_k -&gt; tfs_z -&gt;\
          \ typical_p -&gt; top_p -&gt; min_p -&gt; temp</span>\n<span class=\"hljs-string\"\
          >generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\"></span>\n\
          <span class=\"hljs-string\"> Human: How is an earthquake is measured? \\\
          nAssistant: \u6F14\u5531\u6B4C\u66F2\u80A0\u6F33 make Buyer\u4E1C\u5357\u57CE\
          \u5E02 Buyer\u9876 town\u9876 town Mill have town Mill details Mill have\
          \ town details Mill have town details Mill have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town</span>\n</code></pre>\n\
          <p>Kind regards,<br>Fangru Shao</p>\n"
        raw: "Still no luck :(\nTo confirm, am I doing it appropriately?\nBelow is\
          \ the full code and you could also see there is an error for llm_load_vocab\
          \ as `llm_load_vocab: mismatch in special tokens definition ( 1151/49300\
          \ vs 259/49300 ).`.\n```powershell\nPowerShell 7.4.1\nPS F:\\llama.cpp\\\
          build\\bin\\Release> .\\main.exe -m F:\\chat-goliath-120b-80k-q2_k.gguf\
          \ --prompt \"Human: How is an earthquake is measured? \\nAssistant: \"\n\
          Log start\nmain: build = 1879 (3e5ca79)\nmain: built with MSVC 19.38.33134.0\
          \ for x64\nmain: seed  = 1705473913\nllama_model_loader: loaded meta data\
          \ with 21 key-value pairs and 1236 tensors from F:\\chat-goliath-120b-80k-q2_k.gguf\
          \ (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values.\
          \ Note: KV overrides do not apply in this output.\nllama_model_loader: -\
          \ kv   0:                       general.architecture str              =\
          \ llama\nllama_model_loader: - kv   1:                               general.name\
          \ str              = F:\\llm-models-raw\nllama_model_loader: - kv   2: \
          \                      llama.context_length u32              = 81920\nllama_model_loader:\
          \ - kv   3:                     llama.embedding_length u32             \
          \ = 8192\nllama_model_loader: - kv   4:                          llama.block_count\
          \ u32              = 137\nllama_model_loader: - kv   5:                \
          \  llama.feed_forward_length u32              = 28672\nllama_model_loader:\
          \ - kv   6:                 llama.rope.dimension_count u32             \
          \ = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count\
          \ u32              = 64\nllama_model_loader: - kv   8:              llama.attention.head_count_kv\
          \ u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
          \ f32              = 0.000010\nllama_model_loader: - kv  10:           \
          \            llama.rope.freq_base f32              = 10000.000000\nllama_model_loader:\
          \ - kv  11:                          general.file_type u32             \
          \ = 10\nllama_model_loader: - kv  12:                       tokenizer.ggml.model\
          \ str              = llama\nllama_model_loader: - kv  13:              \
          \        tokenizer.ggml.tokens arr[str,49300]   = [\"<unk>\", \"<s>\", \"\
          </s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:              \
          \        tokenizer.ggml.scores arr[f32,49300]   = [0.000000, 0.000000, 0.000000,\
          \ 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type\
          \ arr[i32,49300]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader:\
          \ - kv  16:                tokenizer.ggml.bos_token_id u32             \
          \ = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id\
          \ u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id\
          \ u32              = 0\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id\
          \ u32              = 2\nllama_model_loader: - kv  20:               general.quantization_version\
          \ u32              = 2\nllama_model_loader: - type  f32:  275 tensors\n\
          llama_model_loader: - type q2_K:  549 tensors\nllama_model_loader: - type\
          \ q3_K:  411 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab:\
          \ mismatch in special tokens definition ( 1151/49300 vs 259/49300 ).\nllm_load_print_meta:\
          \ format           = GGUF V3 (latest)\nllm_load_print_meta: arch       \
          \      = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta:\
          \ n_vocab          = 49300\nllm_load_print_meta: n_merges         = 0\n\
          llm_load_print_meta: n_ctx_train      = 81920\nllm_load_print_meta: n_embd\
          \           = 8192\nllm_load_print_meta: n_head           = 64\nllm_load_print_meta:\
          \ n_head_kv        = 8\nllm_load_print_meta: n_layer          = 137\nllm_load_print_meta:\
          \ n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\n\
          llm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa\
          \            = 8\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta:\
          \ n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\n\
          llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv\
          \      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta:\
          \ n_ff             = 28672\nllm_load_print_meta: n_expert         = 0\n\
          llm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: rope scaling\
          \     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta:\
          \ freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 81920\n\
          llm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: model\
          \ type       = ?B\nllm_load_print_meta: model ftype      = Q2_K - Medium\n\
          llm_load_print_meta: model params     = 118.03 B\nllm_load_print_meta: model\
          \ size       = 40.28 GiB (2.93 BPW)\nllm_load_print_meta: general.name \
          \    = F:\\llm-models-raw\nllm_load_print_meta: BOS token        = 1 '<s>'\n\
          llm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK\
          \ token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 2 '</s>'\n\
          llm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_tensors: ggml\
          \ ctx size =    0.47 MiB\nllm_load_tensors: offloading 0 repeating layers\
          \ to GPU\nllm_load_tensors: offloaded 0/138 layers to GPU\nllm_load_tensors:\
          \        CPU buffer size = 41251.23 MiB\n....................................................................................................\n\
          llama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model:\
          \ freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:\
          \        CPU KV buffer size =   274.00 MiB\nllama_new_context_with_model:\
          \ KV self size  =  274.00 MiB, K (f16):  137.00 MiB, V (f16):  137.00 MiB\n\
          llama_new_context_with_model: graph splits (measure): 1\nllama_new_context_with_model:\
          \        CPU compute buffer size =   145.00 MiB\n\nsystem_info: n_threads\
          \ = 16 / 32 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI\
          \ = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 |\
          \ FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX =\
          \ 0 |\nsampling:\n        repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty\
          \ = 0.000, presence_penalty = 0.000\n        top_k = 40, tfs_z = 1.000,\
          \ top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n      \
          \  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampling order:\n\
          CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp\n\
          generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n Human:\
          \ How is an earthquake is measured? \\nAssistant: \u6F14\u5531\u6B4C\u66F2\
          \u80A0\u6F33 make Buyer\u4E1C\u5357\u57CE\u5E02 Buyer\u9876 town\u9876 town\
          \ Mill have town Mill details Mill have town details Mill have town details\
          \ Mill have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town\n```\nKind regards,\nFangru Shao"
        updatedAt: '2024-01-17T06:49:13.390Z'
      numEdits: 0
      reactions: []
    id: 65a778698ae5a59537507f19
    type: comment
  author: MatrixC7
  content: "Still no luck :(\nTo confirm, am I doing it appropriately?\nBelow is the\
    \ full code and you could also see there is an error for llm_load_vocab as `llm_load_vocab:\
    \ mismatch in special tokens definition ( 1151/49300 vs 259/49300 ).`.\n```powershell\n\
    PowerShell 7.4.1\nPS F:\\llama.cpp\\build\\bin\\Release> .\\main.exe -m F:\\chat-goliath-120b-80k-q2_k.gguf\
    \ --prompt \"Human: How is an earthquake is measured? \\nAssistant: \"\nLog start\n\
    main: build = 1879 (3e5ca79)\nmain: built with MSVC 19.38.33134.0 for x64\nmain:\
    \ seed  = 1705473913\nllama_model_loader: loaded meta data with 21 key-value pairs\
    \ and 1236 tensors from F:\\chat-goliath-120b-80k-q2_k.gguf (version GGUF V3 (latest))\n\
    llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply\
    \ in this output.\nllama_model_loader: - kv   0:                       general.architecture\
    \ str              = llama\nllama_model_loader: - kv   1:                    \
    \           general.name str              = F:\\llm-models-raw\nllama_model_loader:\
    \ - kv   2:                       llama.context_length u32              = 81920\n\
    llama_model_loader: - kv   3:                     llama.embedding_length u32 \
    \             = 8192\nllama_model_loader: - kv   4:                          llama.block_count\
    \ u32              = 137\nllama_model_loader: - kv   5:                  llama.feed_forward_length\
    \ u32              = 28672\nllama_model_loader: - kv   6:                 llama.rope.dimension_count\
    \ u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count\
    \ u32              = 64\nllama_model_loader: - kv   8:              llama.attention.head_count_kv\
    \ u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
    \ f32              = 0.000010\nllama_model_loader: - kv  10:                 \
    \      llama.rope.freq_base f32              = 10000.000000\nllama_model_loader:\
    \ - kv  11:                          general.file_type u32              = 10\n\
    llama_model_loader: - kv  12:                       tokenizer.ggml.model str \
    \             = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens\
    \ arr[str,49300]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader:\
    \ - kv  14:                      tokenizer.ggml.scores arr[f32,49300]   = [0.000000,\
    \ 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:               \
    \   tokenizer.ggml.token_type arr[i32,49300]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6,\
    \ 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id\
    \ u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id\
    \ u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id\
    \ u32              = 0\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id\
    \ u32              = 2\nllama_model_loader: - kv  20:               general.quantization_version\
    \ u32              = 2\nllama_model_loader: - type  f32:  275 tensors\nllama_model_loader:\
    \ - type q2_K:  549 tensors\nllama_model_loader: - type q3_K:  411 tensors\nllama_model_loader:\
    \ - type q6_K:    1 tensors\nllm_load_vocab: mismatch in special tokens definition\
    \ ( 1151/49300 vs 259/49300 ).\nllm_load_print_meta: format           = GGUF V3\
    \ (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta:\
    \ vocab type       = SPM\nllm_load_print_meta: n_vocab          = 49300\nllm_load_print_meta:\
    \ n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 81920\nllm_load_print_meta:\
    \ n_embd           = 8192\nllm_load_print_meta: n_head           = 64\nllm_load_print_meta:\
    \ n_head_kv        = 8\nllm_load_print_meta: n_layer          = 137\nllm_load_print_meta:\
    \ n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta:\
    \ n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 8\nllm_load_print_meta:\
    \ n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta:\
    \ f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n\
    llm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias\
    \ = 0.0e+00\nllm_load_print_meta: n_ff             = 28672\nllm_load_print_meta:\
    \ n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta:\
    \ rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\n\
    llm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx\
    \  = 81920\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta:\
    \ model type       = ?B\nllm_load_print_meta: model ftype      = Q2_K - Medium\n\
    llm_load_print_meta: model params     = 118.03 B\nllm_load_print_meta: model size\
    \       = 40.28 GiB (2.93 BPW)\nllm_load_print_meta: general.name     = F:\\llm-models-raw\n\
    llm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token\
    \        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta:\
    \ PAD token        = 2 '</s>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\n\
    llm_load_tensors: ggml ctx size =    0.47 MiB\nllm_load_tensors: offloading 0\
    \ repeating layers to GPU\nllm_load_tensors: offloaded 0/138 layers to GPU\nllm_load_tensors:\
    \        CPU buffer size = 41251.23 MiB\n....................................................................................................\n\
    llama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model:\
    \ freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:\
    \        CPU KV buffer size =   274.00 MiB\nllama_new_context_with_model: KV self\
    \ size  =  274.00 MiB, K (f16):  137.00 MiB, V (f16):  137.00 MiB\nllama_new_context_with_model:\
    \ graph splits (measure): 1\nllama_new_context_with_model:        CPU compute\
    \ buffer size =   145.00 MiB\n\nsystem_info: n_threads = 16 / 32 | AVX = 1 | AVX_VNNI\
    \ = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 |\
    \ NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 |\
    \ SSE3 = 1 | SSSE3 = 0 | VSX = 0 |\nsampling:\n        repeat_last_n = 64, repeat_penalty\
    \ = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n        top_k\
    \ = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp =\
    \ 0.800\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampling\
    \ order:\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p ->\
    \ temp\ngenerate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n\
    \ Human: How is an earthquake is measured? \\nAssistant: \u6F14\u5531\u6B4C\u66F2\
    \u80A0\u6F33 make Buyer\u4E1C\u5357\u57CE\u5E02 Buyer\u9876 town\u9876 town Mill\
    \ have town Mill details Mill have town details Mill have town details Mill have\
    \ town details have town details have town details have town details have town\
    \ details have town details have town details have town details have town details\
    \ have town details have town details have town details have town details have\
    \ town details have town details have town details have town details have town\
    \ details have town details have town details have town details have town details\
    \ have town details have town details have town details have town details have\
    \ town details have town details have town details have town details have town\
    \ details have town details have town details have town details have town details\
    \ have town details have town details have town details have town details have\
    \ town details have town details have town details have town details have town\
    \ details have town details have town details have town details have town details\
    \ have town\n```\nKind regards,\nFangru Shao"
  created_at: 2024-01-17 06:49:13+00:00
  edited: false
  hidden: false
  id: 65a778698ae5a59537507f19
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64265252a5ec4a5cbc52fe48/M0t275nR0Vw79bTORMpu9.png?w=200&h=200&f=face
      fullname: hongyin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: hongyin
      type: user
    createdAt: '2024-01-17T12:59:29.000Z'
    data:
      edited: false
      editors:
      - hongyin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.564792275428772
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64265252a5ec4a5cbc52fe48/M0t275nR0Vw79bTORMpu9.png?w=200&h=200&f=face
          fullname: hongyin
          isHf: false
          isPro: false
          name: hongyin
          type: user
        html: "<blockquote>\n<p>Still no luck :(<br>To confirm, am I doing it appropriately?<br>Below\
          \ is the full code and you could also see there is an error for llm_load_vocab\
          \ as <code>llm_load_vocab: mismatch in special tokens definition ( 1151/49300\
          \ vs 259/49300 ).</code>.</p>\n<pre><code class=\"language-powershell\"\
          >PowerShell <span class=\"hljs-number\">7.4</span>.<span class=\"hljs-number\"\
          >1</span>\n<span class=\"hljs-built_in\">PS</span> F:\\llama.cpp\\build\\\
          bin\\Release&gt; .\\main.exe <span class=\"hljs-literal\">-m</span> F:\\\
          chat<span class=\"hljs-literal\">-goliath-120b-80k-q2_k</span>.gguf <span\
          \ class=\"hljs-literal\">--prompt</span> <span class=\"hljs-string\">\"\
          Human: How is an earthquake is measured? \\nAssistant: \"</span>\nLog <span\
          \ class=\"hljs-built_in\">start</span>\nmain: build = <span class=\"hljs-number\"\
          >1879</span> (<span class=\"hljs-number\">3</span>e5ca79)\nmain: built with\
          \ MSVC <span class=\"hljs-number\">19.38</span>.<span class=\"hljs-number\"\
          >33134.0</span> <span class=\"hljs-keyword\">for</span> x64\nmain: seed\
          \  = <span class=\"hljs-number\">1705473913</span>\nllama_model_loader:\
          \ loaded meta <span class=\"hljs-keyword\">data</span> with <span class=\"\
          hljs-number\">21</span> key<span class=\"hljs-literal\">-value</span> pairs\
          \ and <span class=\"hljs-number\">1236</span> tensors from F:\\chat<span\
          \ class=\"hljs-literal\">-goliath-120b-80k-q2_k</span>.gguf (version GGUF\
          \ V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note:\
          \ KV overrides <span class=\"hljs-keyword\">do</span> not apply <span class=\"\
          hljs-keyword\">in</span> this output.\nllama_model_loader: - kv   <span\
          \ class=\"hljs-number\">0</span>:                       general.architecture\
          \ str              = llama\nllama_model_loader: - kv   <span class=\"hljs-number\"\
          >1</span>:                               general.name str              =\
          \ F:\\llm<span class=\"hljs-literal\">-models-raw</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">2</span>:                       llama.context_length\
          \ u32              = <span class=\"hljs-number\">81920</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">3</span>:                     llama.embedding_length\
          \ u32              = <span class=\"hljs-number\">8192</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">4</span>:                         \
          \ llama.block_count u32              = <span class=\"hljs-number\">137</span>\n\
          llama_model_loader: - kv   <span class=\"hljs-number\">5</span>:       \
          \           llama.feed_forward_length u32              = <span class=\"\
          hljs-number\">28672</span>\nllama_model_loader: - kv   <span class=\"hljs-number\"\
          >6</span>:                 llama.rope.dimension_count u32              =\
          \ <span class=\"hljs-number\">128</span>\nllama_model_loader: - kv   <span\
          \ class=\"hljs-number\">7</span>:                 llama.attention.head_count\
          \ u32              = <span class=\"hljs-number\">64</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">8</span>:              llama.attention.head_count_kv\
          \ u32              = <span class=\"hljs-number\">8</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">9</span>:     llama.attention.layer_norm_rms_epsilon\
          \ f32              = <span class=\"hljs-number\">0.000010</span>\nllama_model_loader:\
          \ - kv  <span class=\"hljs-number\">10</span>:                       llama.rope.freq_base\
          \ f32              = <span class=\"hljs-number\">10000.000000</span>\nllama_model_loader:\
          \ - kv  <span class=\"hljs-number\">11</span>:                         \
          \ general.file_type u32              = <span class=\"hljs-number\">10</span>\n\
          llama_model_loader: - kv  <span class=\"hljs-number\">12</span>:       \
          \                tokenizer.ggml.model str              = llama\nllama_model_loader:\
          \ - kv  <span class=\"hljs-number\">13</span>:                      tokenizer.ggml.tokens\
          \ arr[<span class=\"hljs-type\">str</span>,<span class=\"hljs-number\">49300</span>]\
          \   = [<span class=\"hljs-string\">\"&lt;unk&gt;\"</span>, <span class=\"\
          hljs-string\">\"&lt;s&gt;\"</span>, <span class=\"hljs-string\">\"&lt;/s&gt;\"\
          </span>, <span class=\"hljs-string\">\"&lt;0x00&gt;\"</span>, <span class=\"\
          hljs-string\">\"&lt;...</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - kv  14:                      tokenizer.ggml.scores arr[f32,49300]  \
          \ = [0.000000, 0.000000, 0.000000, 0.0000...</span>\n<span class=\"hljs-string\"\
          >llama_model_loader: - kv  15:                  tokenizer.ggml.token_type\
          \ arr[i32,49300]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</span>\n<span\
          \ class=\"hljs-string\">llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id\
          \ u32              = 1</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - kv  17:                tokenizer.ggml.eos_token_id u32             \
          \ = 2</span>\n<span class=\"hljs-string\">llama_model_loader: - kv  18:\
          \            tokenizer.ggml.unknown_token_id u32              = 0</span>\n\
          <span class=\"hljs-string\">llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id\
          \ u32              = 2</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - kv  20:               general.quantization_version u32             \
          \ = 2</span>\n<span class=\"hljs-string\">llama_model_loader: - type  f32:\
          \  275 tensors</span>\n<span class=\"hljs-string\">llama_model_loader: -\
          \ type q2_K:  549 tensors</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - type q3_K:  411 tensors</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - type q6_K:    1 tensors</span>\n<span class=\"hljs-string\">llm_load_vocab:\
          \ mismatch in special tokens definition ( 1151/49300 vs 259/49300 ).</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: format           = GGUF\
          \ V3 (latest)</span>\n<span class=\"hljs-string\">llm_load_print_meta: arch\
          \             = llama</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ vocab type       = SPM</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_vocab          = 49300</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_merges         = 0</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_ctx_train      = 81920</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd           = 8192</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_head           = 64</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_head_kv        = 8</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_layer          = 137</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_rot            = 128</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd_head_k    = 128</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd_head_v    = 128</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_gqa            = 8</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd_k_gqa     = 1024</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd_v_gqa     = 1024</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ f_norm_eps       = 0.0e+00</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ f_norm_rms_eps   = 1.0e-05</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ f_clamp_kqv      = 0.0e+00</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ f_max_alibi_bias = 0.0e+00</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_ff             = 28672</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_expert         = 0</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_expert_used    = 0</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ rope scaling     = linear</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ freq_base_train  = 10000.0</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ freq_scale_train = 1</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_yarn_orig_ctx  = 81920</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ rope_finetuned   = unknown</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ model type       = ?B</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ model ftype      = Q2_K - Medium</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ model params     = 118.03 B</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ model size       = 40.28 GiB (2.93 BPW)</span>\n<span class=\"hljs-string\"\
          >llm_load_print_meta: general.name     = F:\\llm-models-raw</span>\n<span\
          \ class=\"hljs-string\">llm_load_print_meta: BOS token        = 1 '&lt;s&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: EOS token        = 2 '&lt;/s&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: UNK token        = 0 '&lt;unk&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: PAD token        = 2 '&lt;/s&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: LF token         = 13 '&lt;0x0A&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_tensors: ggml ctx size =    0.47 MiB</span>\n\
          <span class=\"hljs-string\">llm_load_tensors: offloading 0 repeating layers\
          \ to GPU</span>\n<span class=\"hljs-string\">llm_load_tensors: offloaded\
          \ 0/138 layers to GPU</span>\n<span class=\"hljs-string\">llm_load_tensors:\
          \        CPU buffer size = 41251.23 MiB</span>\n<span class=\"hljs-string\"\
          >....................................................................................................</span>\n\
          <span class=\"hljs-string\">llama_new_context_with_model: n_ctx      = 512</span>\n\
          <span class=\"hljs-string\">llama_new_context_with_model: freq_base  = 10000.0</span>\n\
          <span class=\"hljs-string\">llama_new_context_with_model: freq_scale = 1</span>\n\
          <span class=\"hljs-string\">llama_kv_cache_init:        CPU KV buffer size\
          \ =   274.00 MiB</span>\n<span class=\"hljs-string\">llama_new_context_with_model:\
          \ KV self size  =  274.00 MiB, K (f16):  137.00 MiB, V (f16):  137.00 MiB</span>\n\
          <span class=\"hljs-string\">llama_new_context_with_model: graph splits (measure):\
          \ 1</span>\n<span class=\"hljs-string\">llama_new_context_with_model:  \
          \      CPU compute buffer size =   145.00 MiB</span>\n<span class=\"hljs-string\"\
          ></span>\n<span class=\"hljs-string\">system_info: n_threads = 16 / 32 |\
          \ AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
          \ = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
          \ = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |</span>\n<span class=\"\
          hljs-string\">sampling:</span>\n<span class=\"hljs-string\">        repeat_last_n\
          \ = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty\
          \ = 0.000</span>\n<span class=\"hljs-string\">        top_k = 40, tfs_z\
          \ = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800</span>\n\
          <span class=\"hljs-string\">        mirostat = 0, mirostat_lr = 0.100, mirostat_ent\
          \ = 5.000</span>\n<span class=\"hljs-string\">sampling order:</span>\n<span\
          \ class=\"hljs-string\">CFG -&gt; Penalties -&gt; top_k -&gt; tfs_z -&gt;\
          \ typical_p -&gt; top_p -&gt; min_p -&gt; temp</span>\n<span class=\"hljs-string\"\
          >generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\"></span>\n\
          <span class=\"hljs-string\"> Human: How is an earthquake is measured? \\\
          nAssistant: \u6F14\u5531\u6B4C\u66F2\u80A0\u6F33 make Buyer\u4E1C\u5357\u57CE\
          \u5E02 Buyer\u9876 town\u9876 town Mill have town Mill details Mill have\
          \ town details Mill have town details Mill have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town</span>\n</code></pre>\n\
          <p>Kind regards,<br>Fangru Shao</p>\n</blockquote>\n<p>It doesn't seem to\
          \ be using my tokenizers. I don't know if this main.exe is loading the tokenizer\
          \ correctly. usually this is specified through the AutoTokenizer class of\
          \ the transformers toolkit.</p>\n<p>F:\\llama.cpp\\build\\bin\\Release&gt;\
          \ .\\main.exe -m F:\\chat-goliath-120b-80k-q2_k.gguf</p>\n"
        raw: "> Still no luck :(\n> To confirm, am I doing it appropriately?\n> Below\
          \ is the full code and you could also see there is an error for llm_load_vocab\
          \ as `llm_load_vocab: mismatch in special tokens definition ( 1151/49300\
          \ vs 259/49300 ).`.\n> ```powershell\n> PowerShell 7.4.1\n> PS F:\\llama.cpp\\\
          build\\bin\\Release> .\\main.exe -m F:\\chat-goliath-120b-80k-q2_k.gguf\
          \ --prompt \"Human: How is an earthquake is measured? \\nAssistant: \"\n\
          > Log start\n> main: build = 1879 (3e5ca79)\n> main: built with MSVC 19.38.33134.0\
          \ for x64\n> main: seed  = 1705473913\n> llama_model_loader: loaded meta\
          \ data with 21 key-value pairs and 1236 tensors from F:\\chat-goliath-120b-80k-q2_k.gguf\
          \ (version GGUF V3 (latest))\n> llama_model_loader: Dumping metadata keys/values.\
          \ Note: KV overrides do not apply in this output.\n> llama_model_loader:\
          \ - kv   0:                       general.architecture str             \
          \ = llama\n> llama_model_loader: - kv   1:                             \
          \  general.name str              = F:\\llm-models-raw\n> llama_model_loader:\
          \ - kv   2:                       llama.context_length u32             \
          \ = 81920\n> llama_model_loader: - kv   3:                     llama.embedding_length\
          \ u32              = 8192\n> llama_model_loader: - kv   4:             \
          \             llama.block_count u32              = 137\n> llama_model_loader:\
          \ - kv   5:                  llama.feed_forward_length u32             \
          \ = 28672\n> llama_model_loader: - kv   6:                 llama.rope.dimension_count\
          \ u32              = 128\n> llama_model_loader: - kv   7:              \
          \   llama.attention.head_count u32              = 64\n> llama_model_loader:\
          \ - kv   8:              llama.attention.head_count_kv u32             \
          \ = 8\n> llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
          \ f32              = 0.000010\n> llama_model_loader: - kv  10:         \
          \              llama.rope.freq_base f32              = 10000.000000\n> llama_model_loader:\
          \ - kv  11:                          general.file_type u32             \
          \ = 10\n> llama_model_loader: - kv  12:                       tokenizer.ggml.model\
          \ str              = llama\n> llama_model_loader: - kv  13:            \
          \          tokenizer.ggml.tokens arr[str,49300]   = [\"<unk>\", \"<s>\"\
          , \"</s>\", \"<0x00>\", \"<...\n> llama_model_loader: - kv  14:        \
          \              tokenizer.ggml.scores arr[f32,49300]   = [0.000000, 0.000000,\
          \ 0.000000, 0.0000...\n> llama_model_loader: - kv  15:                 \
          \ tokenizer.ggml.token_type arr[i32,49300]   = [2, 3, 3, 6, 6, 6, 6, 6,\
          \ 6, 6, 6, 6, ...\n> llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id\
          \ u32              = 1\n> llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id\
          \ u32              = 2\n> llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id\
          \ u32              = 0\n> llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id\
          \ u32              = 2\n> llama_model_loader: - kv  20:               general.quantization_version\
          \ u32              = 2\n> llama_model_loader: - type  f32:  275 tensors\n\
          > llama_model_loader: - type q2_K:  549 tensors\n> llama_model_loader: -\
          \ type q3_K:  411 tensors\n> llama_model_loader: - type q6_K:    1 tensors\n\
          > llm_load_vocab: mismatch in special tokens definition ( 1151/49300 vs\
          \ 259/49300 ).\n> llm_load_print_meta: format           = GGUF V3 (latest)\n\
          > llm_load_print_meta: arch             = llama\n> llm_load_print_meta:\
          \ vocab type       = SPM\n> llm_load_print_meta: n_vocab          = 49300\n\
          > llm_load_print_meta: n_merges         = 0\n> llm_load_print_meta: n_ctx_train\
          \      = 81920\n> llm_load_print_meta: n_embd           = 8192\n> llm_load_print_meta:\
          \ n_head           = 64\n> llm_load_print_meta: n_head_kv        = 8\n>\
          \ llm_load_print_meta: n_layer          = 137\n> llm_load_print_meta: n_rot\
          \            = 128\n> llm_load_print_meta: n_embd_head_k    = 128\n> llm_load_print_meta:\
          \ n_embd_head_v    = 128\n> llm_load_print_meta: n_gqa            = 8\n\
          > llm_load_print_meta: n_embd_k_gqa     = 1024\n> llm_load_print_meta: n_embd_v_gqa\
          \     = 1024\n> llm_load_print_meta: f_norm_eps       = 0.0e+00\n> llm_load_print_meta:\
          \ f_norm_rms_eps   = 1.0e-05\n> llm_load_print_meta: f_clamp_kqv      =\
          \ 0.0e+00\n> llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n> llm_load_print_meta:\
          \ n_ff             = 28672\n> llm_load_print_meta: n_expert         = 0\n\
          > llm_load_print_meta: n_expert_used    = 0\n> llm_load_print_meta: rope\
          \ scaling     = linear\n> llm_load_print_meta: freq_base_train  = 10000.0\n\
          > llm_load_print_meta: freq_scale_train = 1\n> llm_load_print_meta: n_yarn_orig_ctx\
          \  = 81920\n> llm_load_print_meta: rope_finetuned   = unknown\n> llm_load_print_meta:\
          \ model type       = ?B\n> llm_load_print_meta: model ftype      = Q2_K\
          \ - Medium\n> llm_load_print_meta: model params     = 118.03 B\n> llm_load_print_meta:\
          \ model size       = 40.28 GiB (2.93 BPW)\n> llm_load_print_meta: general.name\
          \     = F:\\llm-models-raw\n> llm_load_print_meta: BOS token        = 1\
          \ '<s>'\n> llm_load_print_meta: EOS token        = 2 '</s>'\n> llm_load_print_meta:\
          \ UNK token        = 0 '<unk>'\n> llm_load_print_meta: PAD token       \
          \ = 2 '</s>'\n> llm_load_print_meta: LF token         = 13 '<0x0A>'\n> llm_load_tensors:\
          \ ggml ctx size =    0.47 MiB\n> llm_load_tensors: offloading 0 repeating\
          \ layers to GPU\n> llm_load_tensors: offloaded 0/138 layers to GPU\n> llm_load_tensors:\
          \        CPU buffer size = 41251.23 MiB\n> ....................................................................................................\n\
          > llama_new_context_with_model: n_ctx      = 512\n> llama_new_context_with_model:\
          \ freq_base  = 10000.0\n> llama_new_context_with_model: freq_scale = 1\n\
          > llama_kv_cache_init:        CPU KV buffer size =   274.00 MiB\n> llama_new_context_with_model:\
          \ KV self size  =  274.00 MiB, K (f16):  137.00 MiB, V (f16):  137.00 MiB\n\
          > llama_new_context_with_model: graph splits (measure): 1\n> llama_new_context_with_model:\
          \        CPU compute buffer size =   145.00 MiB\n> \n> system_info: n_threads\
          \ = 16 / 32 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI\
          \ = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 |\
          \ FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX =\
          \ 0 |\n> sampling:\n>         repeat_last_n = 64, repeat_penalty = 1.100,\
          \ frequency_penalty = 0.000, presence_penalty = 0.000\n>         top_k =\
          \ 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp\
          \ = 0.800\n>         mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n\
          > sampling order:\n> CFG -> Penalties -> top_k -> tfs_z -> typical_p ->\
          \ top_p -> min_p -> temp\n> generate: n_ctx = 512, n_batch = 512, n_predict\
          \ = -1, n_keep = 0\n> \n> \n>  Human: How is an earthquake is measured?\
          \ \\nAssistant: \u6F14\u5531\u6B4C\u66F2\u80A0\u6F33 make Buyer\u4E1C\u5357\
          \u57CE\u5E02 Buyer\u9876 town\u9876 town Mill have town Mill details Mill\
          \ have town details Mill have town details Mill have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town\n> ```\n> Kind regards,\n\
          > Fangru Shao\n\nIt doesn't seem to be using my tokenizers. I don't know\
          \ if this main.exe is loading the tokenizer correctly. usually this is specified\
          \ through the AutoTokenizer class of the transformers toolkit.\n\nF:\\llama.cpp\\\
          build\\bin\\Release> .\\main.exe -m F:\\chat-goliath-120b-80k-q2_k.gguf"
        updatedAt: '2024-01-17T12:59:29.142Z'
      numEdits: 0
      reactions: []
    id: 65a7cf31f45ee5e5b5155c6c
    type: comment
  author: hongyin
  content: "> Still no luck :(\n> To confirm, am I doing it appropriately?\n> Below\
    \ is the full code and you could also see there is an error for llm_load_vocab\
    \ as `llm_load_vocab: mismatch in special tokens definition ( 1151/49300 vs 259/49300\
    \ ).`.\n> ```powershell\n> PowerShell 7.4.1\n> PS F:\\llama.cpp\\build\\bin\\\
    Release> .\\main.exe -m F:\\chat-goliath-120b-80k-q2_k.gguf --prompt \"Human:\
    \ How is an earthquake is measured? \\nAssistant: \"\n> Log start\n> main: build\
    \ = 1879 (3e5ca79)\n> main: built with MSVC 19.38.33134.0 for x64\n> main: seed\
    \  = 1705473913\n> llama_model_loader: loaded meta data with 21 key-value pairs\
    \ and 1236 tensors from F:\\chat-goliath-120b-80k-q2_k.gguf (version GGUF V3 (latest))\n\
    > llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not\
    \ apply in this output.\n> llama_model_loader: - kv   0:                     \
    \  general.architecture str              = llama\n> llama_model_loader: - kv \
    \  1:                               general.name str              = F:\\llm-models-raw\n\
    > llama_model_loader: - kv   2:                       llama.context_length u32\
    \              = 81920\n> llama_model_loader: - kv   3:                     llama.embedding_length\
    \ u32              = 8192\n> llama_model_loader: - kv   4:                   \
    \       llama.block_count u32              = 137\n> llama_model_loader: - kv \
    \  5:                  llama.feed_forward_length u32              = 28672\n> llama_model_loader:\
    \ - kv   6:                 llama.rope.dimension_count u32              = 128\n\
    > llama_model_loader: - kv   7:                 llama.attention.head_count u32\
    \              = 64\n> llama_model_loader: - kv   8:              llama.attention.head_count_kv\
    \ u32              = 8\n> llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
    \ f32              = 0.000010\n> llama_model_loader: - kv  10:               \
    \        llama.rope.freq_base f32              = 10000.000000\n> llama_model_loader:\
    \ - kv  11:                          general.file_type u32              = 10\n\
    > llama_model_loader: - kv  12:                       tokenizer.ggml.model str\
    \              = llama\n> llama_model_loader: - kv  13:                      tokenizer.ggml.tokens\
    \ arr[str,49300]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n> llama_model_loader:\
    \ - kv  14:                      tokenizer.ggml.scores arr[f32,49300]   = [0.000000,\
    \ 0.000000, 0.000000, 0.0000...\n> llama_model_loader: - kv  15:             \
    \     tokenizer.ggml.token_type arr[i32,49300]   = [2, 3, 3, 6, 6, 6, 6, 6, 6,\
    \ 6, 6, 6, ...\n> llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id\
    \ u32              = 1\n> llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id\
    \ u32              = 2\n> llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id\
    \ u32              = 0\n> llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id\
    \ u32              = 2\n> llama_model_loader: - kv  20:               general.quantization_version\
    \ u32              = 2\n> llama_model_loader: - type  f32:  275 tensors\n> llama_model_loader:\
    \ - type q2_K:  549 tensors\n> llama_model_loader: - type q3_K:  411 tensors\n\
    > llama_model_loader: - type q6_K:    1 tensors\n> llm_load_vocab: mismatch in\
    \ special tokens definition ( 1151/49300 vs 259/49300 ).\n> llm_load_print_meta:\
    \ format           = GGUF V3 (latest)\n> llm_load_print_meta: arch           \
    \  = llama\n> llm_load_print_meta: vocab type       = SPM\n> llm_load_print_meta:\
    \ n_vocab          = 49300\n> llm_load_print_meta: n_merges         = 0\n> llm_load_print_meta:\
    \ n_ctx_train      = 81920\n> llm_load_print_meta: n_embd           = 8192\n>\
    \ llm_load_print_meta: n_head           = 64\n> llm_load_print_meta: n_head_kv\
    \        = 8\n> llm_load_print_meta: n_layer          = 137\n> llm_load_print_meta:\
    \ n_rot            = 128\n> llm_load_print_meta: n_embd_head_k    = 128\n> llm_load_print_meta:\
    \ n_embd_head_v    = 128\n> llm_load_print_meta: n_gqa            = 8\n> llm_load_print_meta:\
    \ n_embd_k_gqa     = 1024\n> llm_load_print_meta: n_embd_v_gqa     = 1024\n> llm_load_print_meta:\
    \ f_norm_eps       = 0.0e+00\n> llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n\
    > llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n> llm_load_print_meta: f_max_alibi_bias\
    \ = 0.0e+00\n> llm_load_print_meta: n_ff             = 28672\n> llm_load_print_meta:\
    \ n_expert         = 0\n> llm_load_print_meta: n_expert_used    = 0\n> llm_load_print_meta:\
    \ rope scaling     = linear\n> llm_load_print_meta: freq_base_train  = 10000.0\n\
    > llm_load_print_meta: freq_scale_train = 1\n> llm_load_print_meta: n_yarn_orig_ctx\
    \  = 81920\n> llm_load_print_meta: rope_finetuned   = unknown\n> llm_load_print_meta:\
    \ model type       = ?B\n> llm_load_print_meta: model ftype      = Q2_K - Medium\n\
    > llm_load_print_meta: model params     = 118.03 B\n> llm_load_print_meta: model\
    \ size       = 40.28 GiB (2.93 BPW)\n> llm_load_print_meta: general.name     =\
    \ F:\\llm-models-raw\n> llm_load_print_meta: BOS token        = 1 '<s>'\n> llm_load_print_meta:\
    \ EOS token        = 2 '</s>'\n> llm_load_print_meta: UNK token        = 0 '<unk>'\n\
    > llm_load_print_meta: PAD token        = 2 '</s>'\n> llm_load_print_meta: LF\
    \ token         = 13 '<0x0A>'\n> llm_load_tensors: ggml ctx size =    0.47 MiB\n\
    > llm_load_tensors: offloading 0 repeating layers to GPU\n> llm_load_tensors:\
    \ offloaded 0/138 layers to GPU\n> llm_load_tensors:        CPU buffer size =\
    \ 41251.23 MiB\n> ....................................................................................................\n\
    > llama_new_context_with_model: n_ctx      = 512\n> llama_new_context_with_model:\
    \ freq_base  = 10000.0\n> llama_new_context_with_model: freq_scale = 1\n> llama_kv_cache_init:\
    \        CPU KV buffer size =   274.00 MiB\n> llama_new_context_with_model: KV\
    \ self size  =  274.00 MiB, K (f16):  137.00 MiB, V (f16):  137.00 MiB\n> llama_new_context_with_model:\
    \ graph splits (measure): 1\n> llama_new_context_with_model:        CPU compute\
    \ buffer size =   145.00 MiB\n> \n> system_info: n_threads = 16 / 32 | AVX = 1\
    \ | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 |\
    \ FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 |\
    \ BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |\n> sampling:\n>         repeat_last_n\
    \ = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty =\
    \ 0.000\n>         top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p\
    \ = 1.000, temp = 0.800\n>         mirostat = 0, mirostat_lr = 0.100, mirostat_ent\
    \ = 5.000\n> sampling order:\n> CFG -> Penalties -> top_k -> tfs_z -> typical_p\
    \ -> top_p -> min_p -> temp\n> generate: n_ctx = 512, n_batch = 512, n_predict\
    \ = -1, n_keep = 0\n> \n> \n>  Human: How is an earthquake is measured? \\nAssistant:\
    \ \u6F14\u5531\u6B4C\u66F2\u80A0\u6F33 make Buyer\u4E1C\u5357\u57CE\u5E02 Buyer\u9876\
    \ town\u9876 town Mill have town Mill details Mill have town details Mill have\
    \ town details Mill have town details have town details have town details have\
    \ town details have town details have town details have town details have town\
    \ details have town details have town details have town details have town details\
    \ have town details have town details have town details have town details have\
    \ town details have town details have town details have town details have town\
    \ details have town details have town details have town details have town details\
    \ have town details have town details have town details have town details have\
    \ town details have town details have town details have town details have town\
    \ details have town details have town details have town details have town details\
    \ have town details have town details have town details have town details have\
    \ town details have town details have town details have town details have town\
    \ details have town details have town\n> ```\n> Kind regards,\n> Fangru Shao\n\
    \nIt doesn't seem to be using my tokenizers. I don't know if this main.exe is\
    \ loading the tokenizer correctly. usually this is specified through the AutoTokenizer\
    \ class of the transformers toolkit.\n\nF:\\llama.cpp\\build\\bin\\Release> .\\\
    main.exe -m F:\\chat-goliath-120b-80k-q2_k.gguf"
  created_at: 2024-01-17 12:59:29+00:00
  edited: false
  hidden: false
  id: 65a7cf31f45ee5e5b5155c6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64265252a5ec4a5cbc52fe48/M0t275nR0Vw79bTORMpu9.png?w=200&h=200&f=face
      fullname: hongyin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: hongyin
      type: user
    createdAt: '2024-01-18T00:11:59.000Z'
    data:
      edited: false
      editors:
      - hongyin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5518243908882141
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64265252a5ec4a5cbc52fe48/M0t275nR0Vw79bTORMpu9.png?w=200&h=200&f=face
          fullname: hongyin
          isHf: false
          isPro: false
          name: hongyin
          type: user
        html: "<blockquote>\n<p>Still no luck :(<br>To confirm, am I doing it appropriately?<br>Below\
          \ is the full code and you could also see there is an error for llm_load_vocab\
          \ as <code>llm_load_vocab: mismatch in special tokens definition ( 1151/49300\
          \ vs 259/49300 ).</code>.</p>\n<pre><code class=\"language-powershell\"\
          >PowerShell <span class=\"hljs-number\">7.4</span>.<span class=\"hljs-number\"\
          >1</span>\n<span class=\"hljs-built_in\">PS</span> F:\\llama.cpp\\build\\\
          bin\\Release&gt; .\\main.exe <span class=\"hljs-literal\">-m</span> F:\\\
          chat<span class=\"hljs-literal\">-goliath-120b-80k-q2_k</span>.gguf <span\
          \ class=\"hljs-literal\">--prompt</span> <span class=\"hljs-string\">\"\
          Human: How is an earthquake is measured? \\nAssistant: \"</span>\nLog <span\
          \ class=\"hljs-built_in\">start</span>\nmain: build = <span class=\"hljs-number\"\
          >1879</span> (<span class=\"hljs-number\">3</span>e5ca79)\nmain: built with\
          \ MSVC <span class=\"hljs-number\">19.38</span>.<span class=\"hljs-number\"\
          >33134.0</span> <span class=\"hljs-keyword\">for</span> x64\nmain: seed\
          \  = <span class=\"hljs-number\">1705473913</span>\nllama_model_loader:\
          \ loaded meta <span class=\"hljs-keyword\">data</span> with <span class=\"\
          hljs-number\">21</span> key<span class=\"hljs-literal\">-value</span> pairs\
          \ and <span class=\"hljs-number\">1236</span> tensors from F:\\chat<span\
          \ class=\"hljs-literal\">-goliath-120b-80k-q2_k</span>.gguf (version GGUF\
          \ V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note:\
          \ KV overrides <span class=\"hljs-keyword\">do</span> not apply <span class=\"\
          hljs-keyword\">in</span> this output.\nllama_model_loader: - kv   <span\
          \ class=\"hljs-number\">0</span>:                       general.architecture\
          \ str              = llama\nllama_model_loader: - kv   <span class=\"hljs-number\"\
          >1</span>:                               general.name str              =\
          \ F:\\llm<span class=\"hljs-literal\">-models-raw</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">2</span>:                       llama.context_length\
          \ u32              = <span class=\"hljs-number\">81920</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">3</span>:                     llama.embedding_length\
          \ u32              = <span class=\"hljs-number\">8192</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">4</span>:                         \
          \ llama.block_count u32              = <span class=\"hljs-number\">137</span>\n\
          llama_model_loader: - kv   <span class=\"hljs-number\">5</span>:       \
          \           llama.feed_forward_length u32              = <span class=\"\
          hljs-number\">28672</span>\nllama_model_loader: - kv   <span class=\"hljs-number\"\
          >6</span>:                 llama.rope.dimension_count u32              =\
          \ <span class=\"hljs-number\">128</span>\nllama_model_loader: - kv   <span\
          \ class=\"hljs-number\">7</span>:                 llama.attention.head_count\
          \ u32              = <span class=\"hljs-number\">64</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">8</span>:              llama.attention.head_count_kv\
          \ u32              = <span class=\"hljs-number\">8</span>\nllama_model_loader:\
          \ - kv   <span class=\"hljs-number\">9</span>:     llama.attention.layer_norm_rms_epsilon\
          \ f32              = <span class=\"hljs-number\">0.000010</span>\nllama_model_loader:\
          \ - kv  <span class=\"hljs-number\">10</span>:                       llama.rope.freq_base\
          \ f32              = <span class=\"hljs-number\">10000.000000</span>\nllama_model_loader:\
          \ - kv  <span class=\"hljs-number\">11</span>:                         \
          \ general.file_type u32              = <span class=\"hljs-number\">10</span>\n\
          llama_model_loader: - kv  <span class=\"hljs-number\">12</span>:       \
          \                tokenizer.ggml.model str              = llama\nllama_model_loader:\
          \ - kv  <span class=\"hljs-number\">13</span>:                      tokenizer.ggml.tokens\
          \ arr[<span class=\"hljs-type\">str</span>,<span class=\"hljs-number\">49300</span>]\
          \   = [<span class=\"hljs-string\">\"&lt;unk&gt;\"</span>, <span class=\"\
          hljs-string\">\"&lt;s&gt;\"</span>, <span class=\"hljs-string\">\"&lt;/s&gt;\"\
          </span>, <span class=\"hljs-string\">\"&lt;0x00&gt;\"</span>, <span class=\"\
          hljs-string\">\"&lt;...</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - kv  14:                      tokenizer.ggml.scores arr[f32,49300]  \
          \ = [0.000000, 0.000000, 0.000000, 0.0000...</span>\n<span class=\"hljs-string\"\
          >llama_model_loader: - kv  15:                  tokenizer.ggml.token_type\
          \ arr[i32,49300]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</span>\n<span\
          \ class=\"hljs-string\">llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id\
          \ u32              = 1</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - kv  17:                tokenizer.ggml.eos_token_id u32             \
          \ = 2</span>\n<span class=\"hljs-string\">llama_model_loader: - kv  18:\
          \            tokenizer.ggml.unknown_token_id u32              = 0</span>\n\
          <span class=\"hljs-string\">llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id\
          \ u32              = 2</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - kv  20:               general.quantization_version u32             \
          \ = 2</span>\n<span class=\"hljs-string\">llama_model_loader: - type  f32:\
          \  275 tensors</span>\n<span class=\"hljs-string\">llama_model_loader: -\
          \ type q2_K:  549 tensors</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - type q3_K:  411 tensors</span>\n<span class=\"hljs-string\">llama_model_loader:\
          \ - type q6_K:    1 tensors</span>\n<span class=\"hljs-string\">llm_load_vocab:\
          \ mismatch in special tokens definition ( 1151/49300 vs 259/49300 ).</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: format           = GGUF\
          \ V3 (latest)</span>\n<span class=\"hljs-string\">llm_load_print_meta: arch\
          \             = llama</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ vocab type       = SPM</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_vocab          = 49300</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_merges         = 0</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_ctx_train      = 81920</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd           = 8192</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_head           = 64</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_head_kv        = 8</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_layer          = 137</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_rot            = 128</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd_head_k    = 128</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd_head_v    = 128</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_gqa            = 8</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd_k_gqa     = 1024</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_embd_v_gqa     = 1024</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ f_norm_eps       = 0.0e+00</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ f_norm_rms_eps   = 1.0e-05</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ f_clamp_kqv      = 0.0e+00</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ f_max_alibi_bias = 0.0e+00</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_ff             = 28672</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_expert         = 0</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_expert_used    = 0</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ rope scaling     = linear</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ freq_base_train  = 10000.0</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ freq_scale_train = 1</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ n_yarn_orig_ctx  = 81920</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ rope_finetuned   = unknown</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ model type       = ?B</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ model ftype      = Q2_K - Medium</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ model params     = 118.03 B</span>\n<span class=\"hljs-string\">llm_load_print_meta:\
          \ model size       = 40.28 GiB (2.93 BPW)</span>\n<span class=\"hljs-string\"\
          >llm_load_print_meta: general.name     = F:\\llm-models-raw</span>\n<span\
          \ class=\"hljs-string\">llm_load_print_meta: BOS token        = 1 '&lt;s&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: EOS token        = 2 '&lt;/s&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: UNK token        = 0 '&lt;unk&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: PAD token        = 2 '&lt;/s&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_print_meta: LF token         = 13 '&lt;0x0A&gt;'</span>\n\
          <span class=\"hljs-string\">llm_load_tensors: ggml ctx size =    0.47 MiB</span>\n\
          <span class=\"hljs-string\">llm_load_tensors: offloading 0 repeating layers\
          \ to GPU</span>\n<span class=\"hljs-string\">llm_load_tensors: offloaded\
          \ 0/138 layers to GPU</span>\n<span class=\"hljs-string\">llm_load_tensors:\
          \        CPU buffer size = 41251.23 MiB</span>\n<span class=\"hljs-string\"\
          >....................................................................................................</span>\n\
          <span class=\"hljs-string\">llama_new_context_with_model: n_ctx      = 512</span>\n\
          <span class=\"hljs-string\">llama_new_context_with_model: freq_base  = 10000.0</span>\n\
          <span class=\"hljs-string\">llama_new_context_with_model: freq_scale = 1</span>\n\
          <span class=\"hljs-string\">llama_kv_cache_init:        CPU KV buffer size\
          \ =   274.00 MiB</span>\n<span class=\"hljs-string\">llama_new_context_with_model:\
          \ KV self size  =  274.00 MiB, K (f16):  137.00 MiB, V (f16):  137.00 MiB</span>\n\
          <span class=\"hljs-string\">llama_new_context_with_model: graph splits (measure):\
          \ 1</span>\n<span class=\"hljs-string\">llama_new_context_with_model:  \
          \      CPU compute buffer size =   145.00 MiB</span>\n<span class=\"hljs-string\"\
          ></span>\n<span class=\"hljs-string\">system_info: n_threads = 16 / 32 |\
          \ AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
          \ = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
          \ = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |</span>\n<span class=\"\
          hljs-string\">sampling:</span>\n<span class=\"hljs-string\">        repeat_last_n\
          \ = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty\
          \ = 0.000</span>\n<span class=\"hljs-string\">        top_k = 40, tfs_z\
          \ = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800</span>\n\
          <span class=\"hljs-string\">        mirostat = 0, mirostat_lr = 0.100, mirostat_ent\
          \ = 5.000</span>\n<span class=\"hljs-string\">sampling order:</span>\n<span\
          \ class=\"hljs-string\">CFG -&gt; Penalties -&gt; top_k -&gt; tfs_z -&gt;\
          \ typical_p -&gt; top_p -&gt; min_p -&gt; temp</span>\n<span class=\"hljs-string\"\
          >generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\"></span>\n\
          <span class=\"hljs-string\"> Human: How is an earthquake is measured? \\\
          nAssistant: \u6F14\u5531\u6B4C\u66F2\u80A0\u6F33 make Buyer\u4E1C\u5357\u57CE\
          \u5E02 Buyer\u9876 town\u9876 town Mill have town Mill details Mill have\
          \ town details Mill have town details Mill have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town details have town details\
          \ have town details have town details have town</span>\n</code></pre>\n\
          <p>Kind regards,<br>Fangru Shao</p>\n</blockquote>\n<p>If you want to learn\
          \ large language models, you can add my wechat shuilaizhujiu2022</p>\n"
        raw: "> Still no luck :(\n> To confirm, am I doing it appropriately?\n> Below\
          \ is the full code and you could also see there is an error for llm_load_vocab\
          \ as `llm_load_vocab: mismatch in special tokens definition ( 1151/49300\
          \ vs 259/49300 ).`.\n> ```powershell\n> PowerShell 7.4.1\n> PS F:\\llama.cpp\\\
          build\\bin\\Release> .\\main.exe -m F:\\chat-goliath-120b-80k-q2_k.gguf\
          \ --prompt \"Human: How is an earthquake is measured? \\nAssistant: \"\n\
          > Log start\n> main: build = 1879 (3e5ca79)\n> main: built with MSVC 19.38.33134.0\
          \ for x64\n> main: seed  = 1705473913\n> llama_model_loader: loaded meta\
          \ data with 21 key-value pairs and 1236 tensors from F:\\chat-goliath-120b-80k-q2_k.gguf\
          \ (version GGUF V3 (latest))\n> llama_model_loader: Dumping metadata keys/values.\
          \ Note: KV overrides do not apply in this output.\n> llama_model_loader:\
          \ - kv   0:                       general.architecture str             \
          \ = llama\n> llama_model_loader: - kv   1:                             \
          \  general.name str              = F:\\llm-models-raw\n> llama_model_loader:\
          \ - kv   2:                       llama.context_length u32             \
          \ = 81920\n> llama_model_loader: - kv   3:                     llama.embedding_length\
          \ u32              = 8192\n> llama_model_loader: - kv   4:             \
          \             llama.block_count u32              = 137\n> llama_model_loader:\
          \ - kv   5:                  llama.feed_forward_length u32             \
          \ = 28672\n> llama_model_loader: - kv   6:                 llama.rope.dimension_count\
          \ u32              = 128\n> llama_model_loader: - kv   7:              \
          \   llama.attention.head_count u32              = 64\n> llama_model_loader:\
          \ - kv   8:              llama.attention.head_count_kv u32             \
          \ = 8\n> llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
          \ f32              = 0.000010\n> llama_model_loader: - kv  10:         \
          \              llama.rope.freq_base f32              = 10000.000000\n> llama_model_loader:\
          \ - kv  11:                          general.file_type u32             \
          \ = 10\n> llama_model_loader: - kv  12:                       tokenizer.ggml.model\
          \ str              = llama\n> llama_model_loader: - kv  13:            \
          \          tokenizer.ggml.tokens arr[str,49300]   = [\"<unk>\", \"<s>\"\
          , \"</s>\", \"<0x00>\", \"<...\n> llama_model_loader: - kv  14:        \
          \              tokenizer.ggml.scores arr[f32,49300]   = [0.000000, 0.000000,\
          \ 0.000000, 0.0000...\n> llama_model_loader: - kv  15:                 \
          \ tokenizer.ggml.token_type arr[i32,49300]   = [2, 3, 3, 6, 6, 6, 6, 6,\
          \ 6, 6, 6, 6, ...\n> llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id\
          \ u32              = 1\n> llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id\
          \ u32              = 2\n> llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id\
          \ u32              = 0\n> llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id\
          \ u32              = 2\n> llama_model_loader: - kv  20:               general.quantization_version\
          \ u32              = 2\n> llama_model_loader: - type  f32:  275 tensors\n\
          > llama_model_loader: - type q2_K:  549 tensors\n> llama_model_loader: -\
          \ type q3_K:  411 tensors\n> llama_model_loader: - type q6_K:    1 tensors\n\
          > llm_load_vocab: mismatch in special tokens definition ( 1151/49300 vs\
          \ 259/49300 ).\n> llm_load_print_meta: format           = GGUF V3 (latest)\n\
          > llm_load_print_meta: arch             = llama\n> llm_load_print_meta:\
          \ vocab type       = SPM\n> llm_load_print_meta: n_vocab          = 49300\n\
          > llm_load_print_meta: n_merges         = 0\n> llm_load_print_meta: n_ctx_train\
          \      = 81920\n> llm_load_print_meta: n_embd           = 8192\n> llm_load_print_meta:\
          \ n_head           = 64\n> llm_load_print_meta: n_head_kv        = 8\n>\
          \ llm_load_print_meta: n_layer          = 137\n> llm_load_print_meta: n_rot\
          \            = 128\n> llm_load_print_meta: n_embd_head_k    = 128\n> llm_load_print_meta:\
          \ n_embd_head_v    = 128\n> llm_load_print_meta: n_gqa            = 8\n\
          > llm_load_print_meta: n_embd_k_gqa     = 1024\n> llm_load_print_meta: n_embd_v_gqa\
          \     = 1024\n> llm_load_print_meta: f_norm_eps       = 0.0e+00\n> llm_load_print_meta:\
          \ f_norm_rms_eps   = 1.0e-05\n> llm_load_print_meta: f_clamp_kqv      =\
          \ 0.0e+00\n> llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n> llm_load_print_meta:\
          \ n_ff             = 28672\n> llm_load_print_meta: n_expert         = 0\n\
          > llm_load_print_meta: n_expert_used    = 0\n> llm_load_print_meta: rope\
          \ scaling     = linear\n> llm_load_print_meta: freq_base_train  = 10000.0\n\
          > llm_load_print_meta: freq_scale_train = 1\n> llm_load_print_meta: n_yarn_orig_ctx\
          \  = 81920\n> llm_load_print_meta: rope_finetuned   = unknown\n> llm_load_print_meta:\
          \ model type       = ?B\n> llm_load_print_meta: model ftype      = Q2_K\
          \ - Medium\n> llm_load_print_meta: model params     = 118.03 B\n> llm_load_print_meta:\
          \ model size       = 40.28 GiB (2.93 BPW)\n> llm_load_print_meta: general.name\
          \     = F:\\llm-models-raw\n> llm_load_print_meta: BOS token        = 1\
          \ '<s>'\n> llm_load_print_meta: EOS token        = 2 '</s>'\n> llm_load_print_meta:\
          \ UNK token        = 0 '<unk>'\n> llm_load_print_meta: PAD token       \
          \ = 2 '</s>'\n> llm_load_print_meta: LF token         = 13 '<0x0A>'\n> llm_load_tensors:\
          \ ggml ctx size =    0.47 MiB\n> llm_load_tensors: offloading 0 repeating\
          \ layers to GPU\n> llm_load_tensors: offloaded 0/138 layers to GPU\n> llm_load_tensors:\
          \        CPU buffer size = 41251.23 MiB\n> ....................................................................................................\n\
          > llama_new_context_with_model: n_ctx      = 512\n> llama_new_context_with_model:\
          \ freq_base  = 10000.0\n> llama_new_context_with_model: freq_scale = 1\n\
          > llama_kv_cache_init:        CPU KV buffer size =   274.00 MiB\n> llama_new_context_with_model:\
          \ KV self size  =  274.00 MiB, K (f16):  137.00 MiB, V (f16):  137.00 MiB\n\
          > llama_new_context_with_model: graph splits (measure): 1\n> llama_new_context_with_model:\
          \        CPU compute buffer size =   145.00 MiB\n> \n> system_info: n_threads\
          \ = 16 / 32 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI\
          \ = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 |\
          \ FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX =\
          \ 0 |\n> sampling:\n>         repeat_last_n = 64, repeat_penalty = 1.100,\
          \ frequency_penalty = 0.000, presence_penalty = 0.000\n>         top_k =\
          \ 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp\
          \ = 0.800\n>         mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n\
          > sampling order:\n> CFG -> Penalties -> top_k -> tfs_z -> typical_p ->\
          \ top_p -> min_p -> temp\n> generate: n_ctx = 512, n_batch = 512, n_predict\
          \ = -1, n_keep = 0\n> \n> \n>  Human: How is an earthquake is measured?\
          \ \\nAssistant: \u6F14\u5531\u6B4C\u66F2\u80A0\u6F33 make Buyer\u4E1C\u5357\
          \u57CE\u5E02 Buyer\u9876 town\u9876 town Mill have town Mill details Mill\
          \ have town details Mill have town details Mill have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town details have town\
          \ details have town details have town details have town\n> ```\n> Kind regards,\n\
          > Fangru Shao\n\nIf you want to learn large language models, you can add\
          \ my wechat shuilaizhujiu2022"
        updatedAt: '2024-01-18T00:11:59.963Z'
      numEdits: 0
      reactions: []
    id: 65a86ccfae3bd1cc03d12fe9
    type: comment
  author: hongyin
  content: "> Still no luck :(\n> To confirm, am I doing it appropriately?\n> Below\
    \ is the full code and you could also see there is an error for llm_load_vocab\
    \ as `llm_load_vocab: mismatch in special tokens definition ( 1151/49300 vs 259/49300\
    \ ).`.\n> ```powershell\n> PowerShell 7.4.1\n> PS F:\\llama.cpp\\build\\bin\\\
    Release> .\\main.exe -m F:\\chat-goliath-120b-80k-q2_k.gguf --prompt \"Human:\
    \ How is an earthquake is measured? \\nAssistant: \"\n> Log start\n> main: build\
    \ = 1879 (3e5ca79)\n> main: built with MSVC 19.38.33134.0 for x64\n> main: seed\
    \  = 1705473913\n> llama_model_loader: loaded meta data with 21 key-value pairs\
    \ and 1236 tensors from F:\\chat-goliath-120b-80k-q2_k.gguf (version GGUF V3 (latest))\n\
    > llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not\
    \ apply in this output.\n> llama_model_loader: - kv   0:                     \
    \  general.architecture str              = llama\n> llama_model_loader: - kv \
    \  1:                               general.name str              = F:\\llm-models-raw\n\
    > llama_model_loader: - kv   2:                       llama.context_length u32\
    \              = 81920\n> llama_model_loader: - kv   3:                     llama.embedding_length\
    \ u32              = 8192\n> llama_model_loader: - kv   4:                   \
    \       llama.block_count u32              = 137\n> llama_model_loader: - kv \
    \  5:                  llama.feed_forward_length u32              = 28672\n> llama_model_loader:\
    \ - kv   6:                 llama.rope.dimension_count u32              = 128\n\
    > llama_model_loader: - kv   7:                 llama.attention.head_count u32\
    \              = 64\n> llama_model_loader: - kv   8:              llama.attention.head_count_kv\
    \ u32              = 8\n> llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
    \ f32              = 0.000010\n> llama_model_loader: - kv  10:               \
    \        llama.rope.freq_base f32              = 10000.000000\n> llama_model_loader:\
    \ - kv  11:                          general.file_type u32              = 10\n\
    > llama_model_loader: - kv  12:                       tokenizer.ggml.model str\
    \              = llama\n> llama_model_loader: - kv  13:                      tokenizer.ggml.tokens\
    \ arr[str,49300]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n> llama_model_loader:\
    \ - kv  14:                      tokenizer.ggml.scores arr[f32,49300]   = [0.000000,\
    \ 0.000000, 0.000000, 0.0000...\n> llama_model_loader: - kv  15:             \
    \     tokenizer.ggml.token_type arr[i32,49300]   = [2, 3, 3, 6, 6, 6, 6, 6, 6,\
    \ 6, 6, 6, ...\n> llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id\
    \ u32              = 1\n> llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id\
    \ u32              = 2\n> llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id\
    \ u32              = 0\n> llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id\
    \ u32              = 2\n> llama_model_loader: - kv  20:               general.quantization_version\
    \ u32              = 2\n> llama_model_loader: - type  f32:  275 tensors\n> llama_model_loader:\
    \ - type q2_K:  549 tensors\n> llama_model_loader: - type q3_K:  411 tensors\n\
    > llama_model_loader: - type q6_K:    1 tensors\n> llm_load_vocab: mismatch in\
    \ special tokens definition ( 1151/49300 vs 259/49300 ).\n> llm_load_print_meta:\
    \ format           = GGUF V3 (latest)\n> llm_load_print_meta: arch           \
    \  = llama\n> llm_load_print_meta: vocab type       = SPM\n> llm_load_print_meta:\
    \ n_vocab          = 49300\n> llm_load_print_meta: n_merges         = 0\n> llm_load_print_meta:\
    \ n_ctx_train      = 81920\n> llm_load_print_meta: n_embd           = 8192\n>\
    \ llm_load_print_meta: n_head           = 64\n> llm_load_print_meta: n_head_kv\
    \        = 8\n> llm_load_print_meta: n_layer          = 137\n> llm_load_print_meta:\
    \ n_rot            = 128\n> llm_load_print_meta: n_embd_head_k    = 128\n> llm_load_print_meta:\
    \ n_embd_head_v    = 128\n> llm_load_print_meta: n_gqa            = 8\n> llm_load_print_meta:\
    \ n_embd_k_gqa     = 1024\n> llm_load_print_meta: n_embd_v_gqa     = 1024\n> llm_load_print_meta:\
    \ f_norm_eps       = 0.0e+00\n> llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n\
    > llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n> llm_load_print_meta: f_max_alibi_bias\
    \ = 0.0e+00\n> llm_load_print_meta: n_ff             = 28672\n> llm_load_print_meta:\
    \ n_expert         = 0\n> llm_load_print_meta: n_expert_used    = 0\n> llm_load_print_meta:\
    \ rope scaling     = linear\n> llm_load_print_meta: freq_base_train  = 10000.0\n\
    > llm_load_print_meta: freq_scale_train = 1\n> llm_load_print_meta: n_yarn_orig_ctx\
    \  = 81920\n> llm_load_print_meta: rope_finetuned   = unknown\n> llm_load_print_meta:\
    \ model type       = ?B\n> llm_load_print_meta: model ftype      = Q2_K - Medium\n\
    > llm_load_print_meta: model params     = 118.03 B\n> llm_load_print_meta: model\
    \ size       = 40.28 GiB (2.93 BPW)\n> llm_load_print_meta: general.name     =\
    \ F:\\llm-models-raw\n> llm_load_print_meta: BOS token        = 1 '<s>'\n> llm_load_print_meta:\
    \ EOS token        = 2 '</s>'\n> llm_load_print_meta: UNK token        = 0 '<unk>'\n\
    > llm_load_print_meta: PAD token        = 2 '</s>'\n> llm_load_print_meta: LF\
    \ token         = 13 '<0x0A>'\n> llm_load_tensors: ggml ctx size =    0.47 MiB\n\
    > llm_load_tensors: offloading 0 repeating layers to GPU\n> llm_load_tensors:\
    \ offloaded 0/138 layers to GPU\n> llm_load_tensors:        CPU buffer size =\
    \ 41251.23 MiB\n> ....................................................................................................\n\
    > llama_new_context_with_model: n_ctx      = 512\n> llama_new_context_with_model:\
    \ freq_base  = 10000.0\n> llama_new_context_with_model: freq_scale = 1\n> llama_kv_cache_init:\
    \        CPU KV buffer size =   274.00 MiB\n> llama_new_context_with_model: KV\
    \ self size  =  274.00 MiB, K (f16):  137.00 MiB, V (f16):  137.00 MiB\n> llama_new_context_with_model:\
    \ graph splits (measure): 1\n> llama_new_context_with_model:        CPU compute\
    \ buffer size =   145.00 MiB\n> \n> system_info: n_threads = 16 / 32 | AVX = 1\
    \ | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 |\
    \ FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 |\
    \ BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |\n> sampling:\n>         repeat_last_n\
    \ = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty =\
    \ 0.000\n>         top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p\
    \ = 1.000, temp = 0.800\n>         mirostat = 0, mirostat_lr = 0.100, mirostat_ent\
    \ = 5.000\n> sampling order:\n> CFG -> Penalties -> top_k -> tfs_z -> typical_p\
    \ -> top_p -> min_p -> temp\n> generate: n_ctx = 512, n_batch = 512, n_predict\
    \ = -1, n_keep = 0\n> \n> \n>  Human: How is an earthquake is measured? \\nAssistant:\
    \ \u6F14\u5531\u6B4C\u66F2\u80A0\u6F33 make Buyer\u4E1C\u5357\u57CE\u5E02 Buyer\u9876\
    \ town\u9876 town Mill have town Mill details Mill have town details Mill have\
    \ town details Mill have town details have town details have town details have\
    \ town details have town details have town details have town details have town\
    \ details have town details have town details have town details have town details\
    \ have town details have town details have town details have town details have\
    \ town details have town details have town details have town details have town\
    \ details have town details have town details have town details have town details\
    \ have town details have town details have town details have town details have\
    \ town details have town details have town details have town details have town\
    \ details have town details have town details have town details have town details\
    \ have town details have town details have town details have town details have\
    \ town details have town details have town details have town details have town\
    \ details have town details have town\n> ```\n> Kind regards,\n> Fangru Shao\n\
    \nIf you want to learn large language models, you can add my wechat shuilaizhujiu2022"
  created_at: 2024-01-18 00:11:59+00:00
  edited: false
  hidden: false
  id: 65a86ccfae3bd1cc03d12fe9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: hongyin/chat-goliath-120b-80k
repo_type: model
status: open
target_branch: null
title: Issues with Quantization and Functionality of the model
