!!python/object:huggingface_hub.community.DiscussionWithDetails
author: paulmoonraker
conflicting_files: null
created_at: 2023-12-03 18:32:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/482d1922d433d8b76aa7aefa65e1cb4f.svg
      fullname: Paul Maker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: paulmoonraker
      type: user
    createdAt: '2023-12-03T18:32:42.000Z'
    data:
      edited: false
      editors:
      - paulmoonraker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9015684723854065
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/482d1922d433d8b76aa7aefa65e1cb4f.svg
          fullname: Paul Maker
          isHf: false
          isPro: false
          name: paulmoonraker
          type: user
        html: '<p>Hi,</p>

          <p>I see that ''By default, input text longer than 384 word pieces is truncated''.
          However, in the tokenizer config I see model_max_length is 512. Does the
          model respect this? Or do i need to set the max seq length somewhere? Thanks,</p>

          '
        raw: "Hi,\r\n\r\nI see that 'By default, input text longer than 384 word pieces\
          \ is truncated'. However, in the tokenizer config I see model_max_length\
          \ is 512. Does the model respect this? Or do i need to set the max seq length\
          \ somewhere? Thanks,"
        updatedAt: '2023-12-03T18:32:42.899Z'
      numEdits: 0
      reactions: []
    id: 656cc9ca9496f21be88c3371
    type: comment
  author: paulmoonraker
  content: "Hi,\r\n\r\nI see that 'By default, input text longer than 384 word pieces\
    \ is truncated'. However, in the tokenizer config I see model_max_length is 512.\
    \ Does the model respect this? Or do i need to set the max seq length somewhere?\
    \ Thanks,"
  created_at: 2023-12-03 18:32:42+00:00
  edited: false
  hidden: false
  id: 656cc9ca9496f21be88c3371
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
      fullname: Tom Aarsen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: tomaarsen
      type: user
    createdAt: '2023-12-03T21:12:39.000Z'
    data:
      edited: false
      editors:
      - tomaarsen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6926460862159729
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
          fullname: Tom Aarsen
          isHf: true
          isPro: false
          name: tomaarsen
          type: user
        html: '<p>Hello!</p>

          <p>The model indeed respects the token length of 384 via this configuration
          setting: <a href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2/blob/main/sentence_bert_config.json#L2">https://huggingface.co/sentence-transformers/all-mpnet-base-v2/blob/main/sentence_bert_config.json#L2</a><br>This
          parameter has priority over the tokenizer one. I do recognize that it is
          a bit confusing to have two separate values for the same setting in the
          model. </p>

          <ul>

          <li>Tom Aarsen</li>

          </ul>

          '
        raw: "Hello!\n\nThe model indeed respects the token length of 384 via this\
          \ configuration setting: https://huggingface.co/sentence-transformers/all-mpnet-base-v2/blob/main/sentence_bert_config.json#L2\n\
          This parameter has priority over the tokenizer one. I do recognize that\
          \ it is a bit confusing to have two separate values for the same setting\
          \ in the model. \n\n- Tom Aarsen"
        updatedAt: '2023-12-03T21:12:39.547Z'
      numEdits: 0
      reactions: []
    id: 656cef47665d15428ab0b7fe
    type: comment
  author: tomaarsen
  content: "Hello!\n\nThe model indeed respects the token length of 384 via this configuration\
    \ setting: https://huggingface.co/sentence-transformers/all-mpnet-base-v2/blob/main/sentence_bert_config.json#L2\n\
    This parameter has priority over the tokenizer one. I do recognize that it is\
    \ a bit confusing to have two separate values for the same setting in the model.\
    \ \n\n- Tom Aarsen"
  created_at: 2023-12-03 21:12:39+00:00
  edited: false
  hidden: false
  id: 656cef47665d15428ab0b7fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/482d1922d433d8b76aa7aefa65e1cb4f.svg
      fullname: Paul Maker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: paulmoonraker
      type: user
    createdAt: '2023-12-19T17:29:46.000Z'
    data:
      edited: false
      editors:
      - paulmoonraker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9851957559585571
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/482d1922d433d8b76aa7aefa65e1cb4f.svg
          fullname: Paul Maker
          isHf: false
          isPro: false
          name: paulmoonraker
          type: user
        html: '<p>than you for the response</p>

          '
        raw: than you for the response
        updatedAt: '2023-12-19T17:29:46.246Z'
      numEdits: 0
      reactions: []
    id: 6581d30a33e43f447c901dc9
    type: comment
  author: paulmoonraker
  content: than you for the response
  created_at: 2023-12-19 17:29:46+00:00
  edited: false
  hidden: false
  id: 6581d30a33e43f447c901dc9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/482d1922d433d8b76aa7aefa65e1cb4f.svg
      fullname: Paul Maker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: paulmoonraker
      type: user
    createdAt: '2023-12-19T17:36:04.000Z'
    data:
      edited: false
      editors:
      - paulmoonraker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8771053552627563
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/482d1922d433d8b76aa7aefa65e1cb4f.svg
          fullname: Paul Maker
          isHf: false
          isPro: false
          name: paulmoonraker
          type: user
        html: '<p>is it possible to set the max_seq_length to 512 via transformers?</p>

          '
        raw: is it possible to set the max_seq_length to 512 via transformers?
        updatedAt: '2023-12-19T17:36:04.001Z'
      numEdits: 0
      reactions: []
    id: 6581d4844867f8699d22df24
    type: comment
  author: paulmoonraker
  content: is it possible to set the max_seq_length to 512 via transformers?
  created_at: 2023-12-19 17:36:04+00:00
  edited: false
  hidden: false
  id: 6581d4844867f8699d22df24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
      fullname: Tom Aarsen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: tomaarsen
      type: user
    createdAt: '2023-12-19T23:05:53.000Z'
    data:
      edited: false
      editors:
      - tomaarsen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6177504062652588
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
          fullname: Tom Aarsen
          isHf: true
          isPro: false
          name: tomaarsen
          type: user
        html: '<p>You could, but the performance of the model will likely be worse
          than if you kept it at 384. Feel free to experiment with it:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer

          <span class="hljs-keyword">from</span> sentence_transformers.models <span
          class="hljs-keyword">import</span> Transformer, Pooling


          transformer = Transformer(<span class="hljs-string">"sentence-transformers/all-mpnet-base-v2"</span>,
          max_seq_length=<span class="hljs-number">512</span>)

          pooling = Pooling(transformer.get_word_embedding_dimension(), <span class="hljs-string">"mean"</span>)

          model = SentenceTransformer(modules=[transformer, pooling])


          embedding = model.encode(<span class="hljs-string">"My text!"</span>)

          <span class="hljs-built_in">print</span>(embedding.shape)

          </code></pre>

          '
        raw: 'You could, but the performance of the model will likely be worse than
          if you kept it at 384. Feel free to experiment with it:

          ```python

          from sentence_transformers import SentenceTransformer

          from sentence_transformers.models import Transformer, Pooling


          transformer = Transformer("sentence-transformers/all-mpnet-base-v2", max_seq_length=512)

          pooling = Pooling(transformer.get_word_embedding_dimension(), "mean")

          model = SentenceTransformer(modules=[transformer, pooling])


          embedding = model.encode("My text!")

          print(embedding.shape)

          ```'
        updatedAt: '2023-12-19T23:05:53.235Z'
      numEdits: 0
      reactions: []
    id: 658221d1aecf478be397efdd
    type: comment
  author: tomaarsen
  content: 'You could, but the performance of the model will likely be worse than
    if you kept it at 384. Feel free to experiment with it:

    ```python

    from sentence_transformers import SentenceTransformer

    from sentence_transformers.models import Transformer, Pooling


    transformer = Transformer("sentence-transformers/all-mpnet-base-v2", max_seq_length=512)

    pooling = Pooling(transformer.get_word_embedding_dimension(), "mean")

    model = SentenceTransformer(modules=[transformer, pooling])


    embedding = model.encode("My text!")

    print(embedding.shape)

    ```'
  created_at: 2023-12-19 23:05:53+00:00
  edited: false
  hidden: false
  id: 658221d1aecf478be397efdd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/482d1922d433d8b76aa7aefa65e1cb4f.svg
      fullname: Paul Maker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: paulmoonraker
      type: user
    createdAt: '2023-12-23T15:17:14.000Z'
    data:
      edited: true
      editors:
      - paulmoonraker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5259313583374023
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/482d1922d433d8b76aa7aefa65e1cb4f.svg
          fullname: Paul Maker
          isHf: false
          isPro: false
          name: paulmoonraker
          type: user
        html: '<p>Hi, is it possible to specify max_seq_length if we are using AutoTokenizer
          and AutoModel? I can pass max_length at tokenisation time, but I doubt that
          stops the model truncating at 384. Thank you</p>

          <p>from transformers import AutoTokenizer, AutoModel<br>import torch</p>

          <p>#Mean Pooling - Take attention mask into account for correct averaging<br>def
          mean_pooling(model_output, attention_mask):<br>    token_embeddings = model_output[0]
          #First element of model_output contains all token embeddings<br>    input_mask_expanded
          = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()<br>    sum_embeddings
          = torch.sum(token_embeddings * input_mask_expanded, 1)<br>    sum_mask =
          torch.clamp(input_mask_expanded.sum(1), min=1e-9)<br>    return sum_embeddings
          / sum_mask</p>

          <p>#Sentences we want sentence embeddings for<br>sentences = [''This framework
          generates embeddings for each input sentence'']</p>

          <p>#Load AutoModel from huggingface model repository<br>tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")<br>model
          = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")</p>

          <p>#Tokenize sentences<br>encoded_input = tokenizer(sentences, padding=True,
          truncation=True, max_length=128, return_tensors=''pt'')</p>

          <p>#Compute token embeddings<br>with torch.no_grad():<br>    model_output
          = model(**encoded_input)</p>

          <p>#Perform pooling. In this case, mean pooling<br>sentence_embeddings =
          mean_pooling(model_output, encoded_input[''attention_mask''])</p>

          '
        raw: "Hi, is it possible to specify max_seq_length if we are using AutoTokenizer\
          \ and AutoModel? I can pass max_length at tokenisation time, but I doubt\
          \ that stops the model truncating at 384. Thank you\n\nfrom transformers\
          \ import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take\
          \ attention mask into account for correct averaging\ndef mean_pooling(model_output,\
          \ attention_mask):\n    token_embeddings = model_output[0] #First element\
          \ of model_output contains all token embeddings\n    input_mask_expanded\
          \ = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n\
          \    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded,\
          \ 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\
          \    return sum_embeddings / sum_mask\n\n#Sentences we want sentence embeddings\
          \ for\nsentences = ['This framework generates embeddings for each input\
          \ sentence']\n\n#Load AutoModel from huggingface model repository\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\"\
          )\nmodel = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\"\
          )\n\n#Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True,\
          \ truncation=True, max_length=128, return_tensors='pt')\n\n#Compute token\
          \ embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\
          \n#Perform pooling. In this case, mean pooling\nsentence_embeddings = mean_pooling(model_output,\
          \ encoded_input['attention_mask'])"
        updatedAt: '2023-12-23T15:17:32.861Z'
      numEdits: 1
      reactions: []
    id: 6586f9fad1331d552bb67515
    type: comment
  author: paulmoonraker
  content: "Hi, is it possible to specify max_seq_length if we are using AutoTokenizer\
    \ and AutoModel? I can pass max_length at tokenisation time, but I doubt that\
    \ stops the model truncating at 384. Thank you\n\nfrom transformers import AutoTokenizer,\
    \ AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account\
    \ for correct averaging\ndef mean_pooling(model_output, attention_mask):\n   \
    \ token_embeddings = model_output[0] #First element of model_output contains all\
    \ token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n\
    \    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n \
    \   sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings\
    \ / sum_mask\n\n#Sentences we want sentence embeddings for\nsentences = ['This\
    \ framework generates embeddings for each input sentence']\n\n#Load AutoModel\
    \ from huggingface model repository\ntokenizer = AutoTokenizer.from_pretrained(\"\
    sentence-transformers/all-MiniLM-L6-v2\")\nmodel = AutoModel.from_pretrained(\"\
    sentence-transformers/all-MiniLM-L6-v2\")\n\n#Tokenize sentences\nencoded_input\
    \ = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n\
    \n#Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\
    \n#Perform pooling. In this case, mean pooling\nsentence_embeddings = mean_pooling(model_output,\
    \ encoded_input['attention_mask'])"
  created_at: 2023-12-23 15:17:14+00:00
  edited: true
  hidden: false
  id: 6586f9fad1331d552bb67515
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/482d1922d433d8b76aa7aefa65e1cb4f.svg
      fullname: Paul Maker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: paulmoonraker
      type: user
    createdAt: '2023-12-23T15:30:06.000Z'
    data:
      edited: false
      editors:
      - paulmoonraker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.904148280620575
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/482d1922d433d8b76aa7aefa65e1cb4f.svg
          fullname: Paul Maker
          isHf: false
          isPro: false
          name: paulmoonraker
          type: user
        html: '<p>Some simple testing would indicate that the model processes up to
          512. For the same text string, if I truncate at 384, and then again at greater
          than 384 but less than 512, I get a different vectors back. If I try greater
          than 512 the model throws an error.</p>

          '
        raw: Some simple testing would indicate that the model processes up to 512.
          For the same text string, if I truncate at 384, and then again at greater
          than 384 but less than 512, I get a different vectors back. If I try greater
          than 512 the model throws an error.
        updatedAt: '2023-12-23T15:30:06.734Z'
      numEdits: 0
      reactions: []
    id: 6586fcfe99ed106ac85324cf
    type: comment
  author: paulmoonraker
  content: Some simple testing would indicate that the model processes up to 512.
    For the same text string, if I truncate at 384, and then again at greater than
    384 but less than 512, I get a different vectors back. If I try greater than 512
    the model throws an error.
  created_at: 2023-12-23 15:30:06+00:00
  edited: false
  hidden: false
  id: 6586fcfe99ed106ac85324cf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: sentence-transformers/all-mpnet-base-v2
repo_type: model
status: open
target_branch: null
title: max model size / max seq length
