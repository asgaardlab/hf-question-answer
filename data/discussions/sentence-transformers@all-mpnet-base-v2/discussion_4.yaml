!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RajaRamKankipati
conflicting_files: null
created_at: 2023-04-13 09:16:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aabc67505f4a65366d253a8788d3993f.svg
      fullname: Raja Ram Kankipati
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RajaRamKankipati
      type: user
    createdAt: '2023-04-13T10:16:59.000Z'
    data:
      edited: true
      editors:
      - RajaRamKankipati
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aabc67505f4a65366d253a8788d3993f.svg
          fullname: Raja Ram Kankipati
          isHf: false
          isPro: false
          name: RajaRamKankipati
          type: user
        html: "<p>Hi Team, </p>\n<p>Implementing MPNET code for long documents which\
          \ have more than 512 tokens in the following approach: </p>\n<ol>\n<li>Get\
          \ all the tokens from the tokenizers without truncation </li>\n<li>Split\
          \ the tokens in chunks of 512 and</li>\n<li>Pass the chunks to the model\
          \ in a batch</li>\n</ol>\n<pre><code class=\"language-python\">encoded_input\
          \ = tokenizer(\n            document,\n            max_length=<span class=\"\
          hljs-literal\">None</span>,\n            padding=<span class=\"hljs-literal\"\
          >True</span>,\n            truncation=<span class=\"hljs-literal\">False</span>,\n\
          \            return_tensors=<span class=\"hljs-string\">\"pt\"</span>,\n\
          \        ).to(device)\n\nencoded_input = pre_processing_encoded_input(encoded_input,\
          \ size = <span class=\"hljs-number\">512</span>)       \n\n<span class=\"\
          hljs-comment\"># Compute token embeddings</span>\n<span class=\"hljs-keyword\"\
          >with</span> torch.no_grad():\n      model_output = self.model(**encoded_input)\n\
          </code></pre>\n<p>With a simple encoded_input of 512 tokens, the model takes\
          \ around 230ms to compute the embedding, with the array shape (2, 512) taking\
          \ 2000ms and increasing exponentially, is there any way I can achieve low\
          \ latency using the model for long documents ?</p>\n"
        raw: "Hi Team, \n\nImplementing MPNET code for long documents which have more\
          \ than 512 tokens in the following approach: \n\n1. Get all the tokens from\
          \ the tokenizers without truncation \n2. Split the tokens in chunks of 512\
          \ and\n3. Pass the chunks to the model in a batch \n\n```python\nencoded_input\
          \ = tokenizer(\n            document,\n            max_length=None,\n  \
          \          padding=True,\n            truncation=False,\n            return_tensors=\"\
          pt\",\n        ).to(device)\n\nencoded_input = pre_processing_encoded_input(encoded_input,\
          \ size = 512)       \n\n# Compute token embeddings\nwith torch.no_grad():\n\
          \      model_output = self.model(**encoded_input)\n```\n\nWith a simple\
          \ encoded_input of 512 tokens, the model takes around 230ms to compute the\
          \ embedding, with the array shape (2, 512) taking 2000ms and increasing\
          \ exponentially, is there any way I can achieve low latency using the model\
          \ for long documents ?"
        updatedAt: '2023-04-13T10:20:59.575Z'
      numEdits: 2
      reactions: []
    id: 6437d69bfac5ea753f1e13ca
    type: comment
  author: RajaRamKankipati
  content: "Hi Team, \n\nImplementing MPNET code for long documents which have more\
    \ than 512 tokens in the following approach: \n\n1. Get all the tokens from the\
    \ tokenizers without truncation \n2. Split the tokens in chunks of 512 and\n3.\
    \ Pass the chunks to the model in a batch \n\n```python\nencoded_input = tokenizer(\n\
    \            document,\n            max_length=None,\n            padding=True,\n\
    \            truncation=False,\n            return_tensors=\"pt\",\n        ).to(device)\n\
    \nencoded_input = pre_processing_encoded_input(encoded_input, size = 512)    \
    \   \n\n# Compute token embeddings\nwith torch.no_grad():\n      model_output\
    \ = self.model(**encoded_input)\n```\n\nWith a simple encoded_input of 512 tokens,\
    \ the model takes around 230ms to compute the embedding, with the array shape\
    \ (2, 512) taking 2000ms and increasing exponentially, is there any way I can\
    \ achieve low latency using the model for long documents ?"
  created_at: 2023-04-13 09:16:59+00:00
  edited: true
  hidden: false
  id: 6437d69bfac5ea753f1e13ca
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: sentence-transformers/all-mpnet-base-v2
repo_type: model
status: open
target_branch: null
title: Latency observed in Embedding computation
