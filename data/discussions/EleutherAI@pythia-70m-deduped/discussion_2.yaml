!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jacobpfau
conflicting_files: null
created_at: 2023-06-07 16:38:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8563015693f5e0e03c25d39034cae599.svg
      fullname: Jacob Pfau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jacobpfau
      type: user
    createdAt: '2023-06-07T17:38:29.000Z'
    data:
      edited: true
      editors:
      - jacobpfau
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7652974128723145
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8563015693f5e0e03c25d39034cae599.svg
          fullname: Jacob Pfau
          isHf: false
          isPro: false
          name: jacobpfau
          type: user
        html: '<p>The Eleuther trained models seem to rely on initialization schemes
          defined <a rel="nofollow" href="https://github.com/EleutherAI/gpt-neox/blob/eedf1a81e3fbac6d465121033621c84bc1432a43/megatron/model/init_functions.py#L138">here</a>.
          Meanwhile, the HF version does initialization <a rel="nofollow" href="https://github.com/huggingface/transformers/blob/v4.29.1/src/transformers/models/gpt_neox/modeling_gpt_neox.py#L66">here</a>
          -- these are very different. In practice, the Eleuther initialization scheme
          seems far better on my data. It would be great if the HF version could be
          updated to correspond to the model''s intended initialization scheme.</p>

          <p>The significance of this difference can be seen by comparing model performance
          between:<br>model = GPTNeoXForCausalLM(config).to("cuda")<br>and<br>model
          = GPTNeoXForCausalLM.from_pretrained(<br>                        ''EleutherAI/pythia-70m'',<br>                        revision="step0",<br>                    ).to("cuda")</p>

          '
        raw: "The Eleuther trained models seem to rely on initialization schemes defined\
          \ [here](https://github.com/EleutherAI/gpt-neox/blob/eedf1a81e3fbac6d465121033621c84bc1432a43/megatron/model/init_functions.py#L138).\
          \ Meanwhile, the HF version does initialization [here](https://github.com/huggingface/transformers/blob/v4.29.1/src/transformers/models/gpt_neox/modeling_gpt_neox.py#L66)\
          \ -- these are very different. In practice, the Eleuther initialization\
          \ scheme seems far better on my data. It would be great if the HF version\
          \ could be updated to correspond to the model's intended initialization\
          \ scheme.\n\nThe significance of this difference can be seen by comparing\
          \ model performance between:\nmodel = GPTNeoXForCausalLM(config).to(\"cuda\"\
          ) \nand\nmodel = GPTNeoXForCausalLM.from_pretrained(\n                 \
          \       'EleutherAI/pythia-70m',\n                        revision=\"step0\"\
          , \n                    ).to(\"cuda\")"
        updatedAt: '2023-06-07T17:39:32.307Z'
      numEdits: 1
      reactions: []
    id: 6480c0957a741c7f33f2c4dc
    type: comment
  author: jacobpfau
  content: "The Eleuther trained models seem to rely on initialization schemes defined\
    \ [here](https://github.com/EleutherAI/gpt-neox/blob/eedf1a81e3fbac6d465121033621c84bc1432a43/megatron/model/init_functions.py#L138).\
    \ Meanwhile, the HF version does initialization [here](https://github.com/huggingface/transformers/blob/v4.29.1/src/transformers/models/gpt_neox/modeling_gpt_neox.py#L66)\
    \ -- these are very different. In practice, the Eleuther initialization scheme\
    \ seems far better on my data. It would be great if the HF version could be updated\
    \ to correspond to the model's intended initialization scheme.\n\nThe significance\
    \ of this difference can be seen by comparing model performance between:\nmodel\
    \ = GPTNeoXForCausalLM(config).to(\"cuda\") \nand\nmodel = GPTNeoXForCausalLM.from_pretrained(\n\
    \                        'EleutherAI/pythia-70m',\n                        revision=\"\
    step0\", \n                    ).to(\"cuda\")"
  created_at: 2023-06-07 16:38:29+00:00
  edited: true
  hidden: false
  id: 6480c0957a741c7f33f2c4dc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: EleutherAI/pythia-70m-deduped
repo_type: model
status: open
target_branch: null
title: Model init does not correspond to paper's (and pre-trained weights') initialization
  scheme
