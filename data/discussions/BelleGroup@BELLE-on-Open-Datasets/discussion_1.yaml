!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Minami-su
conflicting_files: null
created_at: 2023-04-19 06:02:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
      fullname: "\u5357\u6816"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Minami-su
      type: user
    createdAt: '2023-04-19T07:02:50.000Z'
    data:
      edited: false
      editors:
      - Minami-su
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
          fullname: "\u5357\u6816"
          isHf: false
          isPro: false
          name: Minami-su
          type: user
        html: "<p>C\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E<br>\u2502\
          \ /root/autodl-fs/llama7bzh/1.py:7 in                                  \
          \                    \u2502<br>\u2502                                  \
          \                                                                \u2502\
          <br>\u2502    4 ckpt = './llama'                                       \
          \                                     \u2502<br>\u2502    5 device = torch.device('cuda')\
          \                                                               \u2502<br>\u2502\
          \    6 model = LlamaForCausalLM.from_pretrained(ckpt, device_map='auto',\
          \ low_cpu_mem_usage=True    \u2502<br>\u2502 \u2771  7 tokenizer = AutoTokenizer.from_pretrained(ckpt)\
          \                                             \u2502<br>\u2502    8 history\
          \ = []                                                                 \
          \               \u2502<br>\u2502    9 max_history_len=6                \
          \                                                           \u2502<br>\u2502\
          \   10 while True:                                                     \
          \                            \u2502<br>\u2502                          \
          \                                                                      \
          \  \u2502<br>\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:702\
          \   \u2502<br>\u2502 in from_pretrained                                \
          \                                               \u2502<br>\u2502       \
          \                                                                      \
          \                     \u2502<br>\u2502   699 \u2502   \u2502   \u2502  \
          \ \u2502   raise ValueError(                                           \
          \               \u2502<br>\u2502   700 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   f\"Tokenizer class {tokenizer_class_candidate} does not exist\
          \ or is n   \u2502<br>\u2502   701 \u2502   \u2502   \u2502   \u2502   )\
          \                                                                      \
          \    \u2502<br>\u2502 \u2771 702 \u2502   \u2502   \u2502   return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *input   \u2502<br>\u2502   703 \u2502   \u2502                      \
          \                                                                \u2502\
          <br>\u2502   704 \u2502   \u2502   # Otherwise we have to be creative. \
          \                                               \u2502<br>\u2502   705 \u2502\
          \   \u2502   # if model is an encoder decoder, the encoder tokenizer class\
          \ is used by default   \u2502<br>\u2502                                \
          \                                                                  \u2502\
          <br>\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1811\
          \ in     \u2502<br>\u2502 from_pretrained                              \
          \                                                    \u2502<br>\u2502  \
          \                                                                      \
          \                          \u2502<br>\u2502   1808 \u2502   \u2502   \u2502\
          \   else:                                                              \
          \           \u2502<br>\u2502   1809 \u2502   \u2502   \u2502   \u2502  \
          \ logger.info(f\"loading file {file_path} from cache at {resolved_vocab_fil\
          \  \u2502<br>\u2502   1810 \u2502   \u2502                             \
          \                                                        \u2502<br>\u2502\
          \ \u2771 1811 \u2502   \u2502   return cls._from_pretrained(           \
          \                                           \u2502<br>\u2502   1812 \u2502\
          \   \u2502   \u2502   resolved_vocab_files,                            \
          \                             \u2502<br>\u2502   1813 \u2502   \u2502  \
          \ \u2502   pretrained_model_name_or_path,                              \
          \                  \u2502<br>\u2502   1814 \u2502   \u2502   \u2502   init_configuration,\
          \                                                           \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1965\
          \ in     \u2502<br>\u2502 _from_pretrained                             \
          \                                                    \u2502<br>\u2502  \
          \                                                                      \
          \                          \u2502<br>\u2502   1962 \u2502   \u2502     \
          \                                                                      \
          \          \u2502<br>\u2502   1963 \u2502   \u2502   # Instantiate tokenizer.\
          \                                                          \u2502<br>\u2502\
          \   1964 \u2502   \u2502   try:                                        \
          \                                      \u2502<br>\u2502 \u2771 1965 \u2502\
          \   \u2502   \u2502   tokenizer = cls(*init_inputs, **init_kwargs)     \
          \                             \u2502<br>\u2502   1966 \u2502   \u2502  \
          \ except OSError:                                                      \
          \             \u2502<br>\u2502   1967 \u2502   \u2502   \u2502   raise OSError(\
          \                                                                \u2502\
          <br>\u2502   1968 \u2502   \u2502   \u2502   \u2502   \"Unable to load vocabulary\
          \ from file. \"                                   \u2502<br>\u2502     \
          \                                                                      \
          \                       \u2502<br>\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama_fast.p\
          \ \u2502<br>\u2502 y:89 in <strong>init</strong>                       \
          \                                                          \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502    86 \u2502   \u2502   eos_token=\"\
          \",                                                                  \u2502\
          <br>\u2502    87 \u2502   \u2502   **kwargs,                           \
          \                                               \u2502<br>\u2502    88 \u2502\
          \   ):                                                                 \
          \                    \u2502<br>\u2502 \u2771  89 \u2502   \u2502   super().<strong>init</strong>(\
          \                                                                  \u2502\
          <br>\u2502    90 \u2502   \u2502   \u2502   vocab_file=vocab_file,     \
          \                                                    \u2502<br>\u2502  \
          \  91 \u2502   \u2502   \u2502   tokenizer_file=tokenizer_file,        \
          \                                         \u2502<br>\u2502    92 \u2502\
          \   \u2502   \u2502   clean_up_tokenization_spaces=clean_up_tokenization_spaces,\
          \                     \u2502<br>\u2502                                 \
          \                                                                 \u2502\
          <br>\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:114\
          \ in      \u2502<br>\u2502 <strong>init</strong>                       \
          \                                                                  \u2502\
          <br>\u2502                                                             \
          \                                     \u2502<br>\u2502   111 \u2502   \u2502\
          \   \u2502   fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\
          \                  \u2502<br>\u2502   112 \u2502   \u2502   elif slow_tokenizer\
          \ is not None:                                                   \u2502\
          <br>\u2502   113 \u2502   \u2502   \u2502   # We need to convert a slow\
          \ tokenizer to build the backend                     \u2502<br>\u2502 \u2771\
          \ 114 \u2502   \u2502   \u2502   fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\
          \                        \u2502<br>\u2502   115 \u2502   \u2502   elif self.slow_tokenizer_class\
          \ is not None:                                        \u2502<br>\u2502 \
          \  116 \u2502   \u2502   \u2502   # We need to create and convert a slow\
          \ tokenizer to build the backend          \u2502<br>\u2502   117 \u2502\
          \   \u2502   \u2502   slow_tokenizer = self.slow_tokenizer_class(*args,\
          \ **kwargs)                    \u2502<br>\u2502                        \
          \                                                                      \
          \    \u2502<br>\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:1288\
          \ in      \u2502<br>\u2502 convert_slow_tokenizer                      \
          \                                                     \u2502<br>\u2502 \
          \                                                                      \
          \                           \u2502<br>\u2502   1285 \u2502             \
          \                                                                      \
          \      \u2502<br>\u2502   1286 \u2502   converter_class = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\
          \                       \u2502<br>\u2502   1287 \u2502                 \
          \                                                                      \
          \  \u2502<br>\u2502 \u2771 1288 \u2502   return converter_class(transformer_tokenizer).converted()\
          \                             \u2502<br>\u2502   1289                  \
          \                                                                      \
          \   \u2502<br>\u2502                                                   \
          \                                               \u2502<br>\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:511\
          \ in       \u2502<br>\u2502 converted                                  \
          \                                                      \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502    508 \u2502   \u2502   return\
          \ decoders.Metaspace(replacement=replacement, add_prefix_space=add_prefix_s\
          \  \u2502<br>\u2502    509 \u2502                                      \
          \                                                   \u2502<br>\u2502   \
          \ 510 \u2502   def converted(self) -&gt; Tokenizer:                    \
          \                                 \u2502<br>\u2502 \u2771  511 \u2502  \
          \ \u2502   tokenizer = self.tokenizer(self.proto)                      \
          \                      \u2502<br>\u2502    512 \u2502   \u2502         \
          \                                                                      \
          \      \u2502<br>\u2502    513 \u2502   \u2502   # Tokenizer assemble  \
          \                                                            \u2502<br>\u2502\
          \    514 \u2502   \u2502   normalizer = self.normalizer(self.proto)    \
          \                                      \u2502<br>\u2502                \
          \                                                                      \
          \            \u2502<br>\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:1130\
          \ in      \u2502<br>\u2502 tokenizer                                   \
          \                                                     \u2502<br>\u2502 \
          \                                                                      \
          \                           \u2502<br>\u2502   1127 \u2502   \u2502   if\
          \ model_type == 1:                                                     \
          \          \u2502<br>\u2502   1128 \u2502   \u2502   \u2502   raise RuntimeError(\"\
          Llama is supposed to be a BPE model!\")                    \u2502<br>\u2502\
          \   1129 \u2502   \u2502   elif model_type == 2:                       \
          \                                      \u2502<br>\u2502 \u2771 1130 \u2502\
          \   \u2502   \u2502   _, merges = SentencePieceExtractor(self.original_tokenizer.vocab_file).extra\
          \  \u2502<br>\u2502   1131 \u2502   \u2502   \u2502   bpe_vocab = {word:\
          \ i for i, (word, _score) in enumerate(vocab_scores)}        \u2502<br>\u2502\
          \   1132 \u2502   \u2502   \u2502   tokenizer = Tokenizer(             \
          \                                           \u2502<br>\u2502   1133 \u2502\
          \   \u2502   \u2502   \u2502   BPE(bpe_vocab, merges, unk_token=proto.trainer_spec.unk_piece,\
          \ fuse_unk=  \u2502<br>\u2502                                          \
          \                                                        \u2502<br>\u2502\
          \ /root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:60\
          \ in        \u2502<br>\u2502 extract                                   \
          \                                                       \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502     57 \u2502   \u2502   for\
          \ piece_l in vocab.keys():                                             \
          \         \u2502<br>\u2502     58 \u2502   \u2502   \u2502   for piece_r\
          \ in vocab.keys():                                                  \u2502\
          <br>\u2502     59 \u2502   \u2502   \u2502   \u2502   merge = f\"{piece_l}{piece_r}\"\
          \                                             \u2502<br>\u2502 \u2771  \
          \ 60 \u2502   \u2502   \u2502   \u2502   piece_score = vocab_scores.get(merge,\
          \ None)                               \u2502<br>\u2502     61 \u2502   \u2502\
          \   \u2502   \u2502   if piece_score:                                  \
          \                         \u2502<br>\u2502     62 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   merges += [(piece_l, piece_r, piece_score)]      \
          \                     \u2502<br>\u2502     63 \u2502   \u2502   merges =\
          \ sorted(merges, key=lambda val: val[2], reverse=reverse)              \
          \    \u2502</p>\n"
        raw: "C\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502\
          \ /root/autodl-fs/llama7bzh/1.py:7 in <module>                         \
          \                            \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502    4 ckpt = './llama'                              \
          \                                              \u2502\r\n\u2502    5 device\
          \ = torch.device('cuda')                                               \
          \                \u2502\r\n\u2502    6 model = LlamaForCausalLM.from_pretrained(ckpt,\
          \ device_map='auto', low_cpu_mem_usage=True    \u2502\r\n\u2502 \u2771 \
          \ 7 tokenizer = AutoTokenizer.from_pretrained(ckpt)                    \
          \                         \u2502\r\n\u2502    8 history = []           \
          \                                                                     \u2502\
          \r\n\u2502    9 max_history_len=6                                      \
          \                                     \u2502\r\n\u2502   10 while True:\
          \                                                                      \
          \           \u2502\r\n\u2502                                           \
          \                                                       \u2502\r\n\u2502\
          \ /root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:702\
          \   \u2502\r\n\u2502 in from_pretrained                                \
          \                                               \u2502\r\n\u2502       \
          \                                                                      \
          \                     \u2502\r\n\u2502   699 \u2502   \u2502   \u2502  \
          \ \u2502   raise ValueError(                                           \
          \               \u2502\r\n\u2502   700 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   f\"Tokenizer class {tokenizer_class_candidate} does not exist\
          \ or is n   \u2502\r\n\u2502   701 \u2502   \u2502   \u2502   \u2502   )\
          \                                                                      \
          \    \u2502\r\n\u2502 \u2771 702 \u2502   \u2502   \u2502   return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *input   \u2502\r\n\u2502   703 \u2502   \u2502                      \
          \                                                                \u2502\r\
          \n\u2502   704 \u2502   \u2502   # Otherwise we have to be creative.   \
          \                                             \u2502\r\n\u2502   705 \u2502\
          \   \u2502   # if model is an encoder decoder, the encoder tokenizer class\
          \ is used by default   \u2502\r\n\u2502                                \
          \                                                                  \u2502\
          \r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1811\
          \ in     \u2502\r\n\u2502 from_pretrained                              \
          \                                                    \u2502\r\n\u2502  \
          \                                                                      \
          \                          \u2502\r\n\u2502   1808 \u2502   \u2502   \u2502\
          \   else:                                                              \
          \           \u2502\r\n\u2502   1809 \u2502   \u2502   \u2502   \u2502  \
          \ logger.info(f\"loading file {file_path} from cache at {resolved_vocab_fil\
          \  \u2502\r\n\u2502   1810 \u2502   \u2502                             \
          \                                                        \u2502\r\n\u2502\
          \ \u2771 1811 \u2502   \u2502   return cls._from_pretrained(           \
          \                                           \u2502\r\n\u2502   1812 \u2502\
          \   \u2502   \u2502   resolved_vocab_files,                            \
          \                             \u2502\r\n\u2502   1813 \u2502   \u2502  \
          \ \u2502   pretrained_model_name_or_path,                              \
          \                  \u2502\r\n\u2502   1814 \u2502   \u2502   \u2502   init_configuration,\
          \                                                           \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1965\
          \ in     \u2502\r\n\u2502 _from_pretrained                             \
          \                                                    \u2502\r\n\u2502  \
          \                                                                      \
          \                          \u2502\r\n\u2502   1962 \u2502   \u2502     \
          \                                                                      \
          \          \u2502\r\n\u2502   1963 \u2502   \u2502   # Instantiate tokenizer.\
          \                                                          \u2502\r\n\u2502\
          \   1964 \u2502   \u2502   try:                                        \
          \                                      \u2502\r\n\u2502 \u2771 1965 \u2502\
          \   \u2502   \u2502   tokenizer = cls(*init_inputs, **init_kwargs)     \
          \                             \u2502\r\n\u2502   1966 \u2502   \u2502  \
          \ except OSError:                                                      \
          \             \u2502\r\n\u2502   1967 \u2502   \u2502   \u2502   raise OSError(\
          \                                                                \u2502\r\
          \n\u2502   1968 \u2502   \u2502   \u2502   \u2502   \"Unable to load vocabulary\
          \ from file. \"                                   \u2502\r\n\u2502     \
          \                                                                      \
          \                       \u2502\r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama_fast.p\
          \ \u2502\r\n\u2502 y:89 in __init__                                    \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502    86 \u2502   \u2502   eos_token=\"\
          </s>\",                                                                \
          \  \u2502\r\n\u2502    87 \u2502   \u2502   **kwargs,                  \
          \                                                        \u2502\r\n\u2502\
          \    88 \u2502   ):                                                    \
          \                                 \u2502\r\n\u2502 \u2771  89 \u2502   \u2502\
          \   super().__init__(                                                  \
          \                \u2502\r\n\u2502    90 \u2502   \u2502   \u2502   vocab_file=vocab_file,\
          \                                                         \u2502\r\n\u2502\
          \    91 \u2502   \u2502   \u2502   tokenizer_file=tokenizer_file,      \
          \                                           \u2502\r\n\u2502    92 \u2502\
          \   \u2502   \u2502   clean_up_tokenization_spaces=clean_up_tokenization_spaces,\
          \                     \u2502\r\n\u2502                                 \
          \                                                                 \u2502\
          \r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:114\
          \ in      \u2502\r\n\u2502 __init__                                    \
          \                                                     \u2502\r\n\u2502 \
          \                                                                      \
          \                           \u2502\r\n\u2502   111 \u2502   \u2502   \u2502\
          \   fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)      \
          \            \u2502\r\n\u2502   112 \u2502   \u2502   elif slow_tokenizer\
          \ is not None:                                                   \u2502\r\
          \n\u2502   113 \u2502   \u2502   \u2502   # We need to convert a slow tokenizer\
          \ to build the backend                     \u2502\r\n\u2502 \u2771 114 \u2502\
          \   \u2502   \u2502   fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\
          \                        \u2502\r\n\u2502   115 \u2502   \u2502   elif self.slow_tokenizer_class\
          \ is not None:                                        \u2502\r\n\u2502 \
          \  116 \u2502   \u2502   \u2502   # We need to create and convert a slow\
          \ tokenizer to build the backend          \u2502\r\n\u2502   117 \u2502\
          \   \u2502   \u2502   slow_tokenizer = self.slow_tokenizer_class(*args,\
          \ **kwargs)                    \u2502\r\n\u2502                        \
          \                                                                      \
          \    \u2502\r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:1288\
          \ in      \u2502\r\n\u2502 convert_slow_tokenizer                      \
          \                                                     \u2502\r\n\u2502 \
          \                                                                      \
          \                           \u2502\r\n\u2502   1285 \u2502             \
          \                                                                      \
          \      \u2502\r\n\u2502   1286 \u2502   converter_class = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\
          \                       \u2502\r\n\u2502   1287 \u2502                 \
          \                                                                      \
          \  \u2502\r\n\u2502 \u2771 1288 \u2502   return converter_class(transformer_tokenizer).converted()\
          \                             \u2502\r\n\u2502   1289                  \
          \                                                                      \
          \   \u2502\r\n\u2502                                                   \
          \                                               \u2502\r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:511\
          \ in       \u2502\r\n\u2502 converted                                  \
          \                                                      \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502    508 \u2502   \u2502   return\
          \ decoders.Metaspace(replacement=replacement, add_prefix_space=add_prefix_s\
          \  \u2502\r\n\u2502    509 \u2502                                      \
          \                                                   \u2502\r\n\u2502   \
          \ 510 \u2502   def converted(self) -> Tokenizer:                       \
          \                              \u2502\r\n\u2502 \u2771  511 \u2502   \u2502\
          \   tokenizer = self.tokenizer(self.proto)                             \
          \               \u2502\r\n\u2502    512 \u2502   \u2502                \
          \                                                                     \u2502\
          \r\n\u2502    513 \u2502   \u2502   # Tokenizer assemble               \
          \                                               \u2502\r\n\u2502    514\
          \ \u2502   \u2502   normalizer = self.normalizer(self.proto)           \
          \                               \u2502\r\n\u2502                       \
          \                                                                      \
          \     \u2502\r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:1130\
          \ in      \u2502\r\n\u2502 tokenizer                                   \
          \                                                     \u2502\r\n\u2502 \
          \                                                                      \
          \                           \u2502\r\n\u2502   1127 \u2502   \u2502   if\
          \ model_type == 1:                                                     \
          \          \u2502\r\n\u2502   1128 \u2502   \u2502   \u2502   raise RuntimeError(\"\
          Llama is supposed to be a BPE model!\")                    \u2502\r\n\u2502\
          \   1129 \u2502   \u2502   elif model_type == 2:                       \
          \                                      \u2502\r\n\u2502 \u2771 1130 \u2502\
          \   \u2502   \u2502   _, merges = SentencePieceExtractor(self.original_tokenizer.vocab_file).extra\
          \  \u2502\r\n\u2502   1131 \u2502   \u2502   \u2502   bpe_vocab = {word:\
          \ i for i, (word, _score) in enumerate(vocab_scores)}        \u2502\r\n\u2502\
          \   1132 \u2502   \u2502   \u2502   tokenizer = Tokenizer(             \
          \                                           \u2502\r\n\u2502   1133 \u2502\
          \   \u2502   \u2502   \u2502   BPE(bpe_vocab, merges, unk_token=proto.trainer_spec.unk_piece,\
          \ fuse_unk=  \u2502\r\n\u2502                                          \
          \                                                        \u2502\r\n\u2502\
          \ /root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:60\
          \ in        \u2502\r\n\u2502 extract                                   \
          \                                                       \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502     57 \u2502   \u2502   for\
          \ piece_l in vocab.keys():                                             \
          \         \u2502\r\n\u2502     58 \u2502   \u2502   \u2502   for piece_r\
          \ in vocab.keys():                                                  \u2502\
          \r\n\u2502     59 \u2502   \u2502   \u2502   \u2502   merge = f\"{piece_l}{piece_r}\"\
          \                                             \u2502\r\n\u2502 \u2771  \
          \ 60 \u2502   \u2502   \u2502   \u2502   piece_score = vocab_scores.get(merge,\
          \ None)                               \u2502\r\n\u2502     61 \u2502   \u2502\
          \   \u2502   \u2502   if piece_score:                                  \
          \                         \u2502\r\n\u2502     62 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   merges += [(piece_l, piece_r, piece_score)]      \
          \                     \u2502\r\n\u2502     63 \u2502   \u2502   merges =\
          \ sorted(merges, key=lambda val: val[2], reverse=reverse)              \
          \    \u2502"
        updatedAt: '2023-04-19T07:02:50.694Z'
      numEdits: 0
      reactions: []
    id: 643f921a4c64045dbddb5b56
    type: comment
  author: Minami-su
  content: "C\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502 /root/autodl-fs/llama7bzh/1.py:7\
    \ in <module>                                                     \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502    4 ckpt = './llama'                \
    \                                                            \u2502\r\n\u2502\
    \    5 device = torch.device('cuda')                                         \
    \                      \u2502\r\n\u2502    6 model = LlamaForCausalLM.from_pretrained(ckpt,\
    \ device_map='auto', low_cpu_mem_usage=True    \u2502\r\n\u2502 \u2771  7 tokenizer\
    \ = AutoTokenizer.from_pretrained(ckpt)                                      \
    \       \u2502\r\n\u2502    8 history = []                                   \
    \                                             \u2502\r\n\u2502    9 max_history_len=6\
    \                                                                           \u2502\
    \r\n\u2502   10 while True:                                                  \
    \                               \u2502\r\n\u2502                             \
    \                                                                     \u2502\r\
    \n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:702\
    \   \u2502\r\n\u2502 in from_pretrained                                      \
    \                                         \u2502\r\n\u2502                   \
    \                                                                            \
    \   \u2502\r\n\u2502   699 \u2502   \u2502   \u2502   \u2502   raise ValueError(\
    \                                                          \u2502\r\n\u2502  \
    \ 700 \u2502   \u2502   \u2502   \u2502   \u2502   f\"Tokenizer class {tokenizer_class_candidate}\
    \ does not exist or is n   \u2502\r\n\u2502   701 \u2502   \u2502   \u2502   \u2502\
    \   )                                                                        \
    \  \u2502\r\n\u2502 \u2771 702 \u2502   \u2502   \u2502   return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *input   \u2502\r\n\u2502   703 \u2502   \u2502                            \
    \                                                          \u2502\r\n\u2502  \
    \ 704 \u2502   \u2502   # Otherwise we have to be creative.                  \
    \                              \u2502\r\n\u2502   705 \u2502   \u2502   # if model\
    \ is an encoder decoder, the encoder tokenizer class is used by default   \u2502\
    \r\n\u2502                                                                   \
    \                               \u2502\r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1811\
    \ in     \u2502\r\n\u2502 from_pretrained                                    \
    \                                              \u2502\r\n\u2502              \
    \                                                                            \
    \        \u2502\r\n\u2502   1808 \u2502   \u2502   \u2502   else:            \
    \                                                             \u2502\r\n\u2502\
    \   1809 \u2502   \u2502   \u2502   \u2502   logger.info(f\"loading file {file_path}\
    \ from cache at {resolved_vocab_fil  \u2502\r\n\u2502   1810 \u2502   \u2502 \
    \                                                                            \
    \        \u2502\r\n\u2502 \u2771 1811 \u2502   \u2502   return cls._from_pretrained(\
    \                                                      \u2502\r\n\u2502   1812\
    \ \u2502   \u2502   \u2502   resolved_vocab_files,                           \
    \                              \u2502\r\n\u2502   1813 \u2502   \u2502   \u2502\
    \   pretrained_model_name_or_path,                                           \
    \     \u2502\r\n\u2502   1814 \u2502   \u2502   \u2502   init_configuration, \
    \                                                          \u2502\r\n\u2502  \
    \                                                                            \
    \                    \u2502\r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1965\
    \ in     \u2502\r\n\u2502 _from_pretrained                                   \
    \                                              \u2502\r\n\u2502              \
    \                                                                            \
    \        \u2502\r\n\u2502   1962 \u2502   \u2502                             \
    \                                                        \u2502\r\n\u2502   1963\
    \ \u2502   \u2502   # Instantiate tokenizer.                                 \
    \                         \u2502\r\n\u2502   1964 \u2502   \u2502   try:     \
    \                                                                         \u2502\
    \r\n\u2502 \u2771 1965 \u2502   \u2502   \u2502   tokenizer = cls(*init_inputs,\
    \ **init_kwargs)                                  \u2502\r\n\u2502   1966 \u2502\
    \   \u2502   except OSError:                                                 \
    \                  \u2502\r\n\u2502   1967 \u2502   \u2502   \u2502   raise OSError(\
    \                                                                \u2502\r\n\u2502\
    \   1968 \u2502   \u2502   \u2502   \u2502   \"Unable to load vocabulary from\
    \ file. \"                                   \u2502\r\n\u2502                \
    \                                                                            \
    \      \u2502\r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama_fast.p\
    \ \u2502\r\n\u2502 y:89 in __init__                                          \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502    86 \u2502   \u2502   eos_token=\"</s>\",               \
    \                                                   \u2502\r\n\u2502    87 \u2502\
    \   \u2502   **kwargs,                                                       \
    \                   \u2502\r\n\u2502    88 \u2502   ):                       \
    \                                                              \u2502\r\n\u2502\
    \ \u2771  89 \u2502   \u2502   super().__init__(                             \
    \                                     \u2502\r\n\u2502    90 \u2502   \u2502 \
    \  \u2502   vocab_file=vocab_file,                                           \
    \              \u2502\r\n\u2502    91 \u2502   \u2502   \u2502   tokenizer_file=tokenizer_file,\
    \                                                 \u2502\r\n\u2502    92 \u2502\
    \   \u2502   \u2502   clean_up_tokenization_spaces=clean_up_tokenization_spaces,\
    \                     \u2502\r\n\u2502                                       \
    \                                                           \u2502\r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:114\
    \ in      \u2502\r\n\u2502 __init__                                          \
    \                                               \u2502\r\n\u2502             \
    \                                                                            \
    \         \u2502\r\n\u2502   111 \u2502   \u2502   \u2502   fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\
    \                  \u2502\r\n\u2502   112 \u2502   \u2502   elif slow_tokenizer\
    \ is not None:                                                   \u2502\r\n\u2502\
    \   113 \u2502   \u2502   \u2502   # We need to convert a slow tokenizer to build\
    \ the backend                     \u2502\r\n\u2502 \u2771 114 \u2502   \u2502\
    \   \u2502   fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)         \
    \               \u2502\r\n\u2502   115 \u2502   \u2502   elif self.slow_tokenizer_class\
    \ is not None:                                        \u2502\r\n\u2502   116 \u2502\
    \   \u2502   \u2502   # We need to create and convert a slow tokenizer to build\
    \ the backend          \u2502\r\n\u2502   117 \u2502   \u2502   \u2502   slow_tokenizer\
    \ = self.slow_tokenizer_class(*args, **kwargs)                    \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:1288\
    \ in      \u2502\r\n\u2502 convert_slow_tokenizer                            \
    \                                               \u2502\r\n\u2502             \
    \                                                                            \
    \         \u2502\r\n\u2502   1285 \u2502                                     \
    \                                                    \u2502\r\n\u2502   1286 \u2502\
    \   converter_class = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]          \
    \             \u2502\r\n\u2502   1287 \u2502                                 \
    \                                                        \u2502\r\n\u2502 \u2771\
    \ 1288 \u2502   return converter_class(transformer_tokenizer).converted()    \
    \                         \u2502\r\n\u2502   1289                            \
    \                                                               \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:511\
    \ in       \u2502\r\n\u2502 converted                                        \
    \                                                \u2502\r\n\u2502            \
    \                                                                            \
    \          \u2502\r\n\u2502    508 \u2502   \u2502   return decoders.Metaspace(replacement=replacement,\
    \ add_prefix_space=add_prefix_s  \u2502\r\n\u2502    509 \u2502              \
    \                                                                           \u2502\
    \r\n\u2502    510 \u2502   def converted(self) -> Tokenizer:                 \
    \                                    \u2502\r\n\u2502 \u2771  511 \u2502   \u2502\
    \   tokenizer = self.tokenizer(self.proto)                                   \
    \         \u2502\r\n\u2502    512 \u2502   \u2502                            \
    \                                                         \u2502\r\n\u2502   \
    \ 513 \u2502   \u2502   # Tokenizer assemble                                 \
    \                             \u2502\r\n\u2502    514 \u2502   \u2502   normalizer\
    \ = self.normalizer(self.proto)                                          \u2502\
    \r\n\u2502                                                                   \
    \                               \u2502\r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:1130\
    \ in      \u2502\r\n\u2502 tokenizer                                         \
    \                                               \u2502\r\n\u2502             \
    \                                                                            \
    \         \u2502\r\n\u2502   1127 \u2502   \u2502   if model_type == 1:      \
    \                                                         \u2502\r\n\u2502   1128\
    \ \u2502   \u2502   \u2502   raise RuntimeError(\"Llama is supposed to be a BPE\
    \ model!\")                    \u2502\r\n\u2502   1129 \u2502   \u2502   elif\
    \ model_type == 2:                                                           \
    \  \u2502\r\n\u2502 \u2771 1130 \u2502   \u2502   \u2502   _, merges = SentencePieceExtractor(self.original_tokenizer.vocab_file).extra\
    \  \u2502\r\n\u2502   1131 \u2502   \u2502   \u2502   bpe_vocab = {word: i for\
    \ i, (word, _score) in enumerate(vocab_scores)}        \u2502\r\n\u2502   1132\
    \ \u2502   \u2502   \u2502   tokenizer = Tokenizer(                          \
    \                              \u2502\r\n\u2502   1133 \u2502   \u2502   \u2502\
    \   \u2502   BPE(bpe_vocab, merges, unk_token=proto.trainer_spec.unk_piece, fuse_unk=\
    \  \u2502\r\n\u2502                                                          \
    \                                        \u2502\r\n\u2502 /root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:60\
    \ in        \u2502\r\n\u2502 extract                                         \
    \                                                 \u2502\r\n\u2502           \
    \                                                                            \
    \           \u2502\r\n\u2502     57 \u2502   \u2502   for piece_l in vocab.keys():\
    \                                                      \u2502\r\n\u2502     58\
    \ \u2502   \u2502   \u2502   for piece_r in vocab.keys():                    \
    \                              \u2502\r\n\u2502     59 \u2502   \u2502   \u2502\
    \   \u2502   merge = f\"{piece_l}{piece_r}\"                                 \
    \            \u2502\r\n\u2502 \u2771   60 \u2502   \u2502   \u2502   \u2502  \
    \ piece_score = vocab_scores.get(merge, None)                               \u2502\
    \r\n\u2502     61 \u2502   \u2502   \u2502   \u2502   if piece_score:        \
    \                                                   \u2502\r\n\u2502     62 \u2502\
    \   \u2502   \u2502   \u2502   \u2502   merges += [(piece_l, piece_r, piece_score)]\
    \                           \u2502\r\n\u2502     63 \u2502   \u2502   merges =\
    \ sorted(merges, key=lambda val: val[2], reverse=reverse)                  \u2502"
  created_at: 2023-04-19 06:02:50+00:00
  edited: false
  hidden: false
  id: 643f921a4c64045dbddb5b56
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
      fullname: "\u5357\u6816"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Minami-su
      type: user
    createdAt: '2023-04-19T07:05:23.000Z'
    data:
      edited: false
      editors:
      - Minami-su
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
          fullname: "\u5357\u6816"
          isHf: false
          isPro: false
          name: Minami-su
          type: user
        html: "<p>\u5728tokenizer\u90A3\u91CC\u4E00\u76F4\u52A0\u8F7D\uFF0C\u8D85\u7EA7\
          \u6162</p>\n"
        raw: "\u5728tokenizer\u90A3\u91CC\u4E00\u76F4\u52A0\u8F7D\uFF0C\u8D85\u7EA7\
          \u6162"
        updatedAt: '2023-04-19T07:05:23.850Z'
      numEdits: 0
      reactions: []
    id: 643f92b32ab12e441fa7689b
    type: comment
  author: Minami-su
  content: "\u5728tokenizer\u90A3\u91CC\u4E00\u76F4\u52A0\u8F7D\uFF0C\u8D85\u7EA7\u6162"
  created_at: 2023-04-19 06:05:23+00:00
  edited: false
  hidden: false
  id: 643f92b32ab12e441fa7689b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
      fullname: "\u5357\u6816"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Minami-su
      type: user
    createdAt: '2023-04-19T08:23:41.000Z'
    data:
      edited: true
      editors:
      - Minami-su
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
          fullname: "\u5357\u6816"
          isHf: false
          isPro: false
          name: Minami-su
          type: user
        html: "<p>\u89E3\u51B3\u4E86\uFF0C\u4E0D\u8981\u7528</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/Ww0u60STczPhrquuhmZPF.png\"\
          ><img alt=\"dac4f592d7e64423dfc1cab3f531dea8_233014055-3756ccd6-9103-4c22-ba93-ce506e27bcca.png\"\
          \ src=\"https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/Ww0u60STczPhrquuhmZPF.png\"\
          ></a></p>\n<p>\u7528\u8FD9\u4E2A\uFF1A</p>\n<p><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/wci0zd-7_-3u5btcLVvgV.png\"\
          ><img alt=\"f4ab5cff0c7fac45e4bae6644bd3db70_233014228-6ddf1ef3-470c-4d89-9f26-f00e1d0963c4.png\"\
          \ src=\"https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/wci0zd-7_-3u5btcLVvgV.png\"\
          ></a></p>\n<p>\u79D2\u52A0\u8F7D</p>\n"
        raw: "\u89E3\u51B3\u4E86\uFF0C\u4E0D\u8981\u7528\n\n![dac4f592d7e64423dfc1cab3f531dea8_233014055-3756ccd6-9103-4c22-ba93-ce506e27bcca.png](https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/Ww0u60STczPhrquuhmZPF.png)\n\
          \n\u7528\u8FD9\u4E2A\uFF1A\n\n![f4ab5cff0c7fac45e4bae6644bd3db70_233014228-6ddf1ef3-470c-4d89-9f26-f00e1d0963c4.png](https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/wci0zd-7_-3u5btcLVvgV.png)\n\
          \n\n\u79D2\u52A0\u8F7D"
        updatedAt: '2023-04-19T08:24:43.746Z'
      numEdits: 2
      reactions: []
    id: 643fa50dba7506b57e39e64d
    type: comment
  author: Minami-su
  content: "\u89E3\u51B3\u4E86\uFF0C\u4E0D\u8981\u7528\n\n![dac4f592d7e64423dfc1cab3f531dea8_233014055-3756ccd6-9103-4c22-ba93-ce506e27bcca.png](https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/Ww0u60STczPhrquuhmZPF.png)\n\
    \n\u7528\u8FD9\u4E2A\uFF1A\n\n![f4ab5cff0c7fac45e4bae6644bd3db70_233014228-6ddf1ef3-470c-4d89-9f26-f00e1d0963c4.png](https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/wci0zd-7_-3u5btcLVvgV.png)\n\
    \n\n\u79D2\u52A0\u8F7D"
  created_at: 2023-04-19 07:23:41+00:00
  edited: true
  hidden: false
  id: 643fa50dba7506b57e39e64d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6394c673bbdacac6196a7cf3/gT4zc8jO4gwwmE2cxrAcy.jpeg?w=200&h=200&f=face
      fullname: Terrence
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: terrencefm
      type: user
    createdAt: '2023-04-19T08:31:08.000Z'
    data:
      edited: false
      editors:
      - terrencefm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6394c673bbdacac6196a7cf3/gT4zc8jO4gwwmE2cxrAcy.jpeg?w=200&h=200&f=face
          fullname: Terrence
          isHf: false
          isPro: false
          name: terrencefm
          type: user
        html: "<p>\u8BF4\u660EAutoTokenizer\u6839\u636E\u914D\u7F6E\u6587\u4EF6\uFF0C\
          \u81EA\u52A8\u9009\u62E9\u4E86 LlamaTokenizerFast\u3002</p>\n"
        raw: "\u8BF4\u660EAutoTokenizer\u6839\u636E\u914D\u7F6E\u6587\u4EF6\uFF0C\u81EA\
          \u52A8\u9009\u62E9\u4E86 LlamaTokenizerFast\u3002"
        updatedAt: '2023-04-19T08:31:08.778Z'
      numEdits: 0
      reactions: []
    id: 643fa6ccba7506b57e3a14a0
    type: comment
  author: terrencefm
  content: "\u8BF4\u660EAutoTokenizer\u6839\u636E\u914D\u7F6E\u6587\u4EF6\uFF0C\u81EA\
    \u52A8\u9009\u62E9\u4E86 LlamaTokenizerFast\u3002"
  created_at: 2023-04-19 07:31:08+00:00
  edited: false
  hidden: false
  id: 643fa6ccba7506b57e3a14a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
      fullname: "\u5357\u6816"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Minami-su
      type: user
    createdAt: '2023-06-04T22:05:50.000Z'
    data:
      status: closed
    id: 647d0abec788767ab5dbe5a2
    type: status-change
  author: Minami-su
  created_at: 2023-06-04 21:05:50+00:00
  id: 647d0abec788767ab5dbe5a2
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: BelleGroup/BELLE-on-Open-Datasets
repo_type: model
status: closed
target_branch: null
title: "tokenizer\u52A0\u8F7D\u975E\u5E38\u7684\u6162"
