!!python/object:huggingface_hub.community.DiscussionWithDetails
author: viktor-ferenczi
conflicting_files: null
created_at: 2024-01-07 03:13:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
      fullname: Viktor Ferenczi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viktor-ferenczi
      type: user
    createdAt: '2024-01-07T03:13:26.000Z'
    data:
      edited: false
      editors:
      - viktor-ferenczi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9260311126708984
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
          fullname: Viktor Ferenczi
          isHf: false
          isPro: false
          name: viktor-ferenczi
          type: user
        html: '<p>Have the Yi 6B/34B models been fine-tuned to use EOS properly up
          to the 200k context length?</p>

          <p>Did you use a fine-tuning dataset with long enough example to achieve
          that?</p>

          <p>How did you test the quality of the context and that the model properly
          using EOS up to 200k context lengths? Any results?</p>

          '
        raw: "Have the Yi 6B/34B models been fine-tuned to use EOS properly up to\
          \ the 200k context length?\r\n\r\nDid you use a fine-tuning dataset with\
          \ long enough example to achieve that?\r\n\r\nHow did you test the quality\
          \ of the context and that the model properly using EOS up to 200k context\
          \ lengths? Any results?"
        updatedAt: '2024-01-07T03:13:26.502Z'
      numEdits: 0
      reactions: []
    id: 659a16d62bc3a1e0f65252e3
    type: comment
  author: viktor-ferenczi
  content: "Have the Yi 6B/34B models been fine-tuned to use EOS properly up to the\
    \ 200k context length?\r\n\r\nDid you use a fine-tuning dataset with long enough\
    \ example to achieve that?\r\n\r\nHow did you test the quality of the context\
    \ and that the model properly using EOS up to 200k context lengths? Any results?"
  created_at: 2024-01-07 03:13:26+00:00
  edited: false
  hidden: false
  id: 659a16d62bc3a1e0f65252e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
      fullname: Viktor Ferenczi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viktor-ferenczi
      type: user
    createdAt: '2024-01-07T03:41:30.000Z'
    data:
      edited: true
      editors:
      - viktor-ferenczi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9001572728157043
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
          fullname: Viktor Ferenczi
          isHf: false
          isPro: false
          name: viktor-ferenczi
          type: user
        html: '<p>Did you use PEFT or full fine-tuning? Same for 6B and 34B?</p>

          '
        raw: Did you use PEFT or full fine-tuning? Same for 6B and 34B?
        updatedAt: '2024-01-07T03:41:50.380Z'
      numEdits: 1
      reactions: []
    id: 659a1d6a2fe7ca485fe8e4a4
    type: comment
  author: viktor-ferenczi
  content: Did you use PEFT or full fine-tuning? Same for 6B and 34B?
  created_at: 2024-01-07 03:41:30+00:00
  edited: true
  hidden: false
  id: 659a1d6a2fe7ca485fe8e4a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-07T10:13:14.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9558504223823547
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Howdy!</p>

          <p>Here is the video to check out: <a rel="nofollow" href="https://www.youtube.com/watch?v=71x8EMrB0Gc">https://www.youtube.com/watch?v=71x8EMrB0Gc</a></p>

          <p>I use PEFT training (bf16), but with the addition of making embed and
          norm modules trainable as well. This allows the model to get the chat format
          and get the EOS token correct. Alternatively, you could do full fine tuning,
          but typically that is less stable and much slower to get the same results.</p>

          <p>As you will see in the video, the 6B model does not perform well responding
          with text after about 15 to 20,000 tokens. However, the larger 34B model
          does achieve good responses - even for 100K+ contexts. This is despite the
          fine tuning I did which involved only 4000 token context.</p>

          '
        raw: 'Howdy!


          Here is the video to check out: https://www.youtube.com/watch?v=71x8EMrB0Gc


          I use PEFT training (bf16), but with the addition of making embed and norm
          modules trainable as well. This allows the model to get the chat format
          and get the EOS token correct. Alternatively, you could do full fine tuning,
          but typically that is less stable and much slower to get the same results.


          As you will see in the video, the 6B model does not perform well responding
          with text after about 15 to 20,000 tokens. However, the larger 34B model
          does achieve good responses - even for 100K+ contexts. This is despite the
          fine tuning I did which involved only 4000 token context.'
        updatedAt: '2024-01-07T10:13:14.252Z'
      numEdits: 0
      reactions: []
    id: 659a793aeff07dcf1fa63247
    type: comment
  author: RonanMcGovern
  content: 'Howdy!


    Here is the video to check out: https://www.youtube.com/watch?v=71x8EMrB0Gc


    I use PEFT training (bf16), but with the addition of making embed and norm modules
    trainable as well. This allows the model to get the chat format and get the EOS
    token correct. Alternatively, you could do full fine tuning, but typically that
    is less stable and much slower to get the same results.


    As you will see in the video, the 6B model does not perform well responding with
    text after about 15 to 20,000 tokens. However, the larger 34B model does achieve
    good responses - even for 100K+ contexts. This is despite the fine tuning I did
    which involved only 4000 token context.'
  created_at: 2024-01-07 10:13:14+00:00
  edited: false
  hidden: false
  id: 659a793aeff07dcf1fa63247
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-09T17:34:52.000Z'
    data:
      status: closed
    id: 659d83bc50c1bbee5be538c6
    type: status-change
  author: RonanMcGovern
  created_at: 2024-01-09 17:34:52+00:00
  id: 659d83bc50c1bbee5be538c6
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Trelis/Yi-6B-200K-Llamafied-chat-SFT-AWQ
repo_type: model
status: closed
target_branch: null
title: EOS fine-tuning
