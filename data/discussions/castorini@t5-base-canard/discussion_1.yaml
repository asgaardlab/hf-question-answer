!!python/object:huggingface_hub.community.DiscussionWithDetails
author: PaulBrouillette
conflicting_files: null
created_at: 2022-06-07 16:33:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff88a5654402595c4ce86e358ff0fc92.svg
      fullname: 'Paul Brouillette '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PaulBrouillette
      type: user
    createdAt: '2022-06-07T17:33:56.000Z'
    data:
      edited: false
      editors:
      - PaulBrouillette
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff88a5654402595c4ce86e358ff0fc92.svg
          fullname: 'Paul Brouillette '
          isHf: false
          isPro: false
          name: PaulBrouillette
          type: user
        html: '<p>In my Google Colab file, I have everything  set up properly like
          this: </p>

          <p>from transformers import AutoModelForSeq2SeqLM, AutoTokenizer<br>model_name
          = "castorini/t5-base-canard"<br>model = AutoModelForSeq2SeqLM.from_pretrained(model_name)<br>tokenizer
          = AutoTokenizer.from_pretrained(model_name)</p>

          <p>context = ''Frank Zappa ||| Disbandment ||| What group disbanded |||
          Zappa and the Mothers of Invention ||| When did they disband?''</p>

          <p>encoded_input = tokenizer(<br>    context,<br>    padding=''max_length'',<br>    max_length=512,<br>    truncation=True,<br>    return_tensors="pt",<br>)<br>decoder_input
          = tokenizer(<br>    context,<br>    padding=''max_length'',<br>    max_length=512,<br>    truncation=True,<br>    return_tensors="pt",<br>)</p>

          <p>encoder_output = model.generate(input_ids=encoded_input["input_ids"],
          decoder_input_ids=decoder_input["input_ids"])<br>output = tokenizer.decode(<br>    encoder_output[0],<br>    skip_special_tokens=True<br>)<br>print(output)</p>

          <p>However, my output looks like this:<br>Input length of decoder_input_ids
          is 512, but <code>max_length</code> is set to 20. This can lead to unexpected
          behavior. You should consider increasing <code>config.max_length</code>
          or <code>max_length</code>.<br>Frank Zappa ||| Disbandment ||| What group
          disbanded ||| Zappa and the Mothers of Invention ||| When did they disband?
          When</p>

          <p>I have been trying to change the max_length values to print out the fully
          rewritten query, for example in the max_length''s for encoded_input and
          decoder_input I set them to 59 and set the bottom max_length to 89, and
          the output is:<br>Frank Zappa ||| Disbandment ||| What group disbanded |||
          Zappa and the Mothers of Invention ||| When did they disband? When did Frank
          Zappa and the Mothers of Invention disband?</p>

          <p>I have to hardcode those values which isn''t convenient when there''s
          several ''context'' phrases. Does anyone know how to circumvent this problem?   </p>

          '
        raw: "In my Google Colab file, I have everything  set up properly like this:\
          \ \r\n\r\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\
          \ \r\nmodel_name = \"castorini/t5-base-canard\" \r\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\r\
          \ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\n\r\ncontext =\
          \ 'Frank Zappa ||| Disbandment ||| What group disbanded ||| Zappa and the\
          \ Mothers of Invention ||| When did they disband?'\r\n\r\nencoded_input\
          \ = tokenizer(\r\n    context,\r\n    padding='max_length',\r\n    max_length=512,\r\
          \n    truncation=True,\r\n    return_tensors=\"pt\",\r\n)\r\ndecoder_input\
          \ = tokenizer(\r\n    context,\r\n    padding='max_length',\r\n    max_length=512,\r\
          \n    truncation=True,\r\n    return_tensors=\"pt\",\r\n)\r\n\r\nencoder_output\
          \ = model.generate(input_ids=encoded_input[\"input_ids\"], decoder_input_ids=decoder_input[\"\
          input_ids\"])\r\noutput = tokenizer.decode(\r\n    encoder_output[0],\r\n\
          \    skip_special_tokens=True\r\n)\r\nprint(output)\r\n\r\nHowever, my output\
          \ looks like this:\r\nInput length of decoder_input_ids is 512, but ``max_length``\
          \ is set to 20. This can lead to unexpected behavior. You should consider\
          \ increasing ``config.max_length`` or ``max_length``.\r\nFrank Zappa |||\
          \ Disbandment ||| What group disbanded ||| Zappa and the Mothers of Invention\
          \ ||| When did they disband? When\r\n\r\nI have been trying to change the\
          \ max_length values to print out the fully rewritten query, for example\
          \ in the max_length's for encoded_input and decoder_input I set them to\
          \ 59 and set the bottom max_length to 89, and the output is:\r\nFrank Zappa\
          \ ||| Disbandment ||| What group disbanded ||| Zappa and the Mothers of\
          \ Invention ||| When did they disband? When did Frank Zappa and the Mothers\
          \ of Invention disband?\r\n\r\nI have to hardcode those values which isn't\
          \ convenient when there's several 'context' phrases. Does anyone know how\
          \ to circumvent this problem?   "
        updatedAt: '2022-06-07T17:33:56.904Z'
      numEdits: 0
      reactions: []
    id: 629f8c04a1ccdbdd4c538602
    type: comment
  author: PaulBrouillette
  content: "In my Google Colab file, I have everything  set up properly like this:\
    \ \r\n\r\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer \r\nmodel_name\
    \ = \"castorini/t5-base-canard\" \r\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\r\
    \ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\n\r\ncontext = 'Frank\
    \ Zappa ||| Disbandment ||| What group disbanded ||| Zappa and the Mothers of\
    \ Invention ||| When did they disband?'\r\n\r\nencoded_input = tokenizer(\r\n\
    \    context,\r\n    padding='max_length',\r\n    max_length=512,\r\n    truncation=True,\r\
    \n    return_tensors=\"pt\",\r\n)\r\ndecoder_input = tokenizer(\r\n    context,\r\
    \n    padding='max_length',\r\n    max_length=512,\r\n    truncation=True,\r\n\
    \    return_tensors=\"pt\",\r\n)\r\n\r\nencoder_output = model.generate(input_ids=encoded_input[\"\
    input_ids\"], decoder_input_ids=decoder_input[\"input_ids\"])\r\noutput = tokenizer.decode(\r\
    \n    encoder_output[0],\r\n    skip_special_tokens=True\r\n)\r\nprint(output)\r\
    \n\r\nHowever, my output looks like this:\r\nInput length of decoder_input_ids\
    \ is 512, but ``max_length`` is set to 20. This can lead to unexpected behavior.\
    \ You should consider increasing ``config.max_length`` or ``max_length``.\r\n\
    Frank Zappa ||| Disbandment ||| What group disbanded ||| Zappa and the Mothers\
    \ of Invention ||| When did they disband? When\r\n\r\nI have been trying to change\
    \ the max_length values to print out the fully rewritten query, for example in\
    \ the max_length's for encoded_input and decoder_input I set them to 59 and set\
    \ the bottom max_length to 89, and the output is:\r\nFrank Zappa ||| Disbandment\
    \ ||| What group disbanded ||| Zappa and the Mothers of Invention ||| When did\
    \ they disband? When did Frank Zappa and the Mothers of Invention disband?\r\n\
    \r\nI have to hardcode those values which isn't convenient when there's several\
    \ 'context' phrases. Does anyone know how to circumvent this problem?   "
  created_at: 2022-06-07 16:33:56+00:00
  edited: false
  hidden: false
  id: 629f8c04a1ccdbdd4c538602
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b847823e3f484f0072e0adb792a95ac3.svg
      fullname: Sheng-Chieh Lin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jacklin
      type: user
    createdAt: '2022-06-07T17:42:01.000Z'
    data:
      edited: false
      editors:
      - jacklin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b847823e3f484f0072e0adb792a95ac3.svg
          fullname: Sheng-Chieh Lin
          isHf: false
          isPro: false
          name: jacklin
          type: user
        html: '<p>Thanks for your question. I guess you would like to use T5ForConditionalGeneration.from_pretrained
          in this case. Here is our implementation in chattygoose for your reference.
          <a rel="nofollow" href="https://github.com/castorini/chatty-goose/blob/master/chatty_goose/cqr/ntr.py">https://github.com/castorini/chatty-goose/blob/master/chatty_goose/cqr/ntr.py</a></p>

          '
        raw: Thanks for your question. I guess you would like to use T5ForConditionalGeneration.from_pretrained
          in this case. Here is our implementation in chattygoose for your reference.
          https://github.com/castorini/chatty-goose/blob/master/chatty_goose/cqr/ntr.py
        updatedAt: '2022-06-07T17:42:01.021Z'
      numEdits: 0
      reactions: []
    id: 629f8de9e462d45c92e87429
    type: comment
  author: jacklin
  content: Thanks for your question. I guess you would like to use T5ForConditionalGeneration.from_pretrained
    in this case. Here is our implementation in chattygoose for your reference. https://github.com/castorini/chatty-goose/blob/master/chatty_goose/cqr/ntr.py
  created_at: 2022-06-07 16:42:01+00:00
  edited: false
  hidden: false
  id: 629f8de9e462d45c92e87429
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff88a5654402595c4ce86e358ff0fc92.svg
      fullname: 'Paul Brouillette '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PaulBrouillette
      type: user
    createdAt: '2022-06-07T17:56:41.000Z'
    data:
      edited: false
      editors:
      - PaulBrouillette
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff88a5654402595c4ce86e358ff0fc92.svg
          fullname: 'Paul Brouillette '
          isHf: false
          isPro: false
          name: PaulBrouillette
          type: user
        html: '<p>Thank you for your reply. I looked at the GitHub link and I did:<br>model
          = T5ForConditionalGeneration.from_pretrained(model_name)<br>tokenizer =
          T5Tokenizer.from_pretrained(model_name)</p>

          <p>However, I''m still getting the same output:<br>Input length of decoder_input_ids
          is 512, but <code>max_length</code> is set to 20. This can lead to unexpected
          behavior. You should consider increasing <code>config.max_length</code>
          or <code>max_length</code>.<br>Frank Zappa ||| Disbandment ||| What group
          disbanded ||| Zappa and the Mothers of Invention ||| When did they disband?
          When</p>

          <p>Do I need to change the values of my max_length, or do I need to change
          more things and make it more similar to the link?</p>

          '
        raw: 'Thank you for your reply. I looked at the GitHub link and I did:

          model = T5ForConditionalGeneration.from_pretrained(model_name)

          tokenizer = T5Tokenizer.from_pretrained(model_name)


          However, I''m still getting the same output:

          Input length of decoder_input_ids is 512, but ``max_length`` is set to 20.
          This can lead to unexpected behavior. You should consider increasing ``config.max_length``
          or ``max_length``.

          Frank Zappa ||| Disbandment ||| What group disbanded ||| Zappa and the Mothers
          of Invention ||| When did they disband? When


          Do I need to change the values of my max_length, or do I need to change
          more things and make it more similar to the link?'
        updatedAt: '2022-06-07T17:56:41.903Z'
      numEdits: 0
      reactions: []
    id: 629f9159a1ccdbdd4c54ac52
    type: comment
  author: PaulBrouillette
  content: 'Thank you for your reply. I looked at the GitHub link and I did:

    model = T5ForConditionalGeneration.from_pretrained(model_name)

    tokenizer = T5Tokenizer.from_pretrained(model_name)


    However, I''m still getting the same output:

    Input length of decoder_input_ids is 512, but ``max_length`` is set to 20. This
    can lead to unexpected behavior. You should consider increasing ``config.max_length``
    or ``max_length``.

    Frank Zappa ||| Disbandment ||| What group disbanded ||| Zappa and the Mothers
    of Invention ||| When did they disband? When


    Do I need to change the values of my max_length, or do I need to change more things
    and make it more similar to the link?'
  created_at: 2022-06-07 16:56:41+00:00
  edited: false
  hidden: false
  id: 629f9159a1ccdbdd4c54ac52
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b847823e3f484f0072e0adb792a95ac3.svg
      fullname: Sheng-Chieh Lin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jacklin
      type: user
    createdAt: '2022-06-07T18:08:28.000Z'
    data:
      edited: false
      editors:
      - jacklin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b847823e3f484f0072e0adb792a95ac3.svg
          fullname: Sheng-Chieh Lin
          isHf: false
          isPro: false
          name: jacklin
          type: user
        html: '<p>I think you don''t need decoder_input_ids in model.generation. Just
          use something like this: encoder_output = model.generate(input_ids=encoded_input["input_ids"],
          max_length=512, num_beams=1).</p>

          '
        raw: 'I think you don''t need decoder_input_ids in model.generation. Just
          use something like this: encoder_output = model.generate(input_ids=encoded_input["input_ids"],
          max_length=512, num_beams=1).'
        updatedAt: '2022-06-07T18:08:28.742Z'
      numEdits: 0
      reactions: []
    id: 629f941ce462d45c92e9c81b
    type: comment
  author: jacklin
  content: 'I think you don''t need decoder_input_ids in model.generation. Just use
    something like this: encoder_output = model.generate(input_ids=encoded_input["input_ids"],
    max_length=512, num_beams=1).'
  created_at: 2022-06-07 17:08:28+00:00
  edited: false
  hidden: false
  id: 629f941ce462d45c92e9c81b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff88a5654402595c4ce86e358ff0fc92.svg
      fullname: 'Paul Brouillette '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PaulBrouillette
      type: user
    createdAt: '2022-06-07T18:16:37.000Z'
    data:
      edited: false
      editors:
      - PaulBrouillette
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff88a5654402595c4ce86e358ff0fc92.svg
          fullname: 'Paul Brouillette '
          isHf: false
          isPro: false
          name: PaulBrouillette
          type: user
        html: '<p>Wow, that worked! Thank you so much! :)</p>

          '
        raw: Wow, that worked! Thank you so much! :)
        updatedAt: '2022-06-07T18:16:37.033Z'
      numEdits: 0
      reactions: []
    id: 629f9605e462d45c92ea2dea
    type: comment
  author: PaulBrouillette
  content: Wow, that worked! Thank you so much! :)
  created_at: 2022-06-07 17:16:37+00:00
  edited: false
  hidden: false
  id: 629f9605e462d45c92ea2dea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b847823e3f484f0072e0adb792a95ac3.svg
      fullname: Sheng-Chieh Lin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jacklin
      type: user
    createdAt: '2022-06-07T18:17:26.000Z'
    data:
      edited: false
      editors:
      - jacklin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b847823e3f484f0072e0adb792a95ac3.svg
          fullname: Sheng-Chieh Lin
          isHf: false
          isPro: false
          name: jacklin
          type: user
        html: '<p>You are welcome :)</p>

          '
        raw: You are welcome :)
        updatedAt: '2022-06-07T18:17:26.530Z'
      numEdits: 0
      reactions: []
    id: 629f963661e9f6be2bcd8a3c
    type: comment
  author: jacklin
  content: You are welcome :)
  created_at: 2022-06-07 17:17:26+00:00
  edited: false
  hidden: false
  id: 629f963661e9f6be2bcd8a3c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b847823e3f484f0072e0adb792a95ac3.svg
      fullname: Sheng-Chieh Lin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jacklin
      type: user
    createdAt: '2022-06-07T18:17:26.000Z'
    data:
      status: closed
    id: 629f963661e9f6be2bcd8a3d
    type: status-change
  author: jacklin
  created_at: 2022-06-07 17:17:26+00:00
  id: 629f963661e9f6be2bcd8a3d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: castorini/t5-base-canard
repo_type: model
status: closed
target_branch: null
title: Trouble generating rewritten query
