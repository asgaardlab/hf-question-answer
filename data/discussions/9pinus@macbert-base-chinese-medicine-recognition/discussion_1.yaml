!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HokyeeJau
conflicting_files: null
created_at: 2022-06-17 05:57:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4afac705fbc02f5bbbdba4d48a3b1c7c.svg
      fullname: HokyeeJau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HokyeeJau
      type: user
    createdAt: '2022-06-17T06:57:27.000Z'
    data:
      edited: false
      editors:
      - HokyeeJau
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4afac705fbc02f5bbbdba4d48a3b1c7c.svg
          fullname: HokyeeJau
          isHf: false
          isPro: false
          name: HokyeeJau
          type: user
        html: '<p>There is something wrong with the Chinese character indexes.</p>

          <p>When I first loaded the tokenizer, according to the screenshot attached,
          it seemed the index is wrong.<br>Following the tips, I changed the index
          in the added_tokens.json file, but there is another wrong information for
          the same reason that came out.</p>

          <p>I am wondering if I could do anything to avoid this kind of error.<br>Thank
          you.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1655448867288-62ac21f7de8bfbb93089216b.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/1655448867288-62ac21f7de8bfbb93089216b.png"></a></p>

          '
        raw: "There is something wrong with the Chinese character indexes.\r\n\r\n\
          When I first loaded the tokenizer, according to the screenshot attached,\
          \ it seemed the index is wrong.\r\nFollowing the tips, I changed the index\
          \ in the added_tokens.json file, but there is another wrong information\
          \ for the same reason that came out.\r\n\r\nI am wondering if I could do\
          \ anything to avoid this kind of error.\r\nThank you.\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1655448867288-62ac21f7de8bfbb93089216b.png)\r\
          \n"
        updatedAt: '2022-06-17T06:57:27.980Z'
      numEdits: 0
      reactions: []
    id: 62ac25d70dbb72be3410053b
    type: comment
  author: HokyeeJau
  content: "There is something wrong with the Chinese character indexes.\r\n\r\nWhen\
    \ I first loaded the tokenizer, according to the screenshot attached, it seemed\
    \ the index is wrong.\r\nFollowing the tips, I changed the index in the added_tokens.json\
    \ file, but there is another wrong information for the same reason that came out.\r\
    \n\r\nI am wondering if I could do anything to avoid this kind of error.\r\nThank\
    \ you.\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1655448867288-62ac21f7de8bfbb93089216b.png)\r\
    \n"
  created_at: 2022-06-17 05:57:27+00:00
  edited: false
  hidden: false
  id: 62ac25d70dbb72be3410053b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: 9pinus/macbert-base-chinese-medicine-recognition
repo_type: model
status: open
target_branch: null
title: "AssertionError: Non-consecutive added token '\u835C' found. Should have index\
  \ 21170 but has index 21128 in saved vocabulary."
