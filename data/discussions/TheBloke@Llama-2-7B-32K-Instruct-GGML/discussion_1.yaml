!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gsimard
conflicting_files: null
created_at: 2023-08-28 23:28:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf032b73af34b1906f8901b99ad239b6.svg
      fullname: G S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsimard
      type: user
    createdAt: '2023-08-29T00:28:31.000Z'
    data:
      edited: true
      editors:
      - gsimard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.38620525598526
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf032b73af34b1906f8901b99ad239b6.svg
          fullname: G S
          isHf: false
          isPro: false
          name: gsimard
          type: user
        html: '<p>I tried running the model with the following command, but only got
          repeated blank spaces or columns forever.</p>

          <p>simard@CATALYS-1:/mnt/c/git/llama.cpp$ ./main -t 15 -m /mnt/f/ai-models/llama-2-7b-32k-instruct.ggmlv3.q8_0.bin
          --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "### Instruction:
          Write a story about llamas\n### Response:"<br>main: build = 888 (e76d630)<br>main:
          seed  = 1693268931<br>llama.cpp: loading model from /mnt/f/ai-models/llama-2-7b-32k-instruct.ggmlv3.q8_0.bin<br>llama_model_load_internal:
          format     = ggjt v3 (latest)<br>llama_model_load_internal: n_vocab    =
          32000<br>llama_model_load_internal: n_ctx      = 2048<br>llama_model_load_internal:
          n_embd     = 4096<br>llama_model_load_internal: n_mult     = 5504<br>llama_model_load_internal:
          n_head     = 32<br>llama_model_load_internal: n_head_kv  = 32<br>llama_model_load_internal:
          n_layer    = 32<br>llama_model_load_internal: n_rot      = 128<br>llama_model_load_internal:
          n_gqa      = 1<br>llama_model_load_internal: n_ff       = 11008<br>llama_model_load_internal:
          freq_base  = 10000.0<br>llama_model_load_internal: freq_scale = 1<br>llama_model_load_internal:
          ftype      = 7 (mostly Q8_0)<br>llama_model_load_internal: model size =
          7B<br>llama_model_load_internal: ggml ctx size =    0.08 MB<br>llama_model_load_internal:
          mem required  = 7196.45 MB (+ 1024.00 MB per state)<br>llama_new_context_with_model:
          kv self size  = 1024.00 MB</p>

          <p>system_info: n_threads = 15 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 |
          AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C
          = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |<br>sampling:
          repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000,
          frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000,
          typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000,
          mirostat_ent = 5.000000<br>generate: n_ctx = 2048, n_batch = 512, n_predict
          = -1, n_keep = 0</p>

          <h3 id="instruction-write-a-story-about-llamasn-response">Instruction: Write
          a story about llamas\n### Response:::::::::::</h3>

          <p>CTRL+C</p>

          <p>simard@CATALYS-1:/mnt/c/git/llama.cpp$</p>

          '
        raw: "I tried running the model with the following command, but only got repeated\
          \ blank spaces or columns forever.\n\nsimard@CATALYS-1:/mnt/c/git/llama.cpp$\
          \ ./main -t 15 -m /mnt/f/ai-models/llama-2-7b-32k-instruct.ggmlv3.q8_0.bin\
          \ --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"### Instruction:\
          \ Write a story about llamas\\n### Response:\"\nmain: build = 888 (e76d630)\n\
          main: seed  = 1693268931\nllama.cpp: loading model from /mnt/f/ai-models/llama-2-7b-32k-instruct.ggmlv3.q8_0.bin\n\
          llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
          \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
          \ n_embd     = 4096\nllama_model_load_internal: n_mult     = 5504\nllama_model_load_internal:\
          \ n_head     = 32\nllama_model_load_internal: n_head_kv  = 32\nllama_model_load_internal:\
          \ n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal:\
          \ n_gqa      = 1\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal:\
          \ freq_base  = 10000.0\nllama_model_load_internal: freq_scale = 1\nllama_model_load_internal:\
          \ ftype      = 7 (mostly Q8_0)\nllama_model_load_internal: model size =\
          \ 7B\nllama_model_load_internal: ggml ctx size =    0.08 MB\nllama_model_load_internal:\
          \ mem required  = 7196.45 MB (+ 1024.00 MB per state)\nllama_new_context_with_model:\
          \ kv self size  = 1024.00 MB\n\nsystem_info: n_threads = 15 / 32 | AVX =\
          \ 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA =\
          \ 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 |\
          \ BLAS = 0 | SSE3 = 1 | VSX = 0 |\nsampling: repeat_last_n = 64, repeat_penalty\
          \ = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000,\
          \ top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000,\
          \ temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent =\
          \ 5.000000\ngenerate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep\
          \ = 0\n\n\n ### Instruction: Write a story about llamas\\n### Response:::::::::::\n\
          \n\n\n\n\n\n\n\n\n\n\n\n\nCTRL+C\n\nsimard@CATALYS-1:/mnt/c/git/llama.cpp$"
        updatedAt: '2023-08-29T00:29:24.145Z'
      numEdits: 1
      reactions: []
    id: 64ed3baf9e28bbb8995ff57a
    type: comment
  author: gsimard
  content: "I tried running the model with the following command, but only got repeated\
    \ blank spaces or columns forever.\n\nsimard@CATALYS-1:/mnt/c/git/llama.cpp$ ./main\
    \ -t 15 -m /mnt/f/ai-models/llama-2-7b-32k-instruct.ggmlv3.q8_0.bin --color -c\
    \ 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"### Instruction: Write a story\
    \ about llamas\\n### Response:\"\nmain: build = 888 (e76d630)\nmain: seed  = 1693268931\n\
    llama.cpp: loading model from /mnt/f/ai-models/llama-2-7b-32k-instruct.ggmlv3.q8_0.bin\n\
    llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
    \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
    \ n_embd     = 4096\nllama_model_load_internal: n_mult     = 5504\nllama_model_load_internal:\
    \ n_head     = 32\nllama_model_load_internal: n_head_kv  = 32\nllama_model_load_internal:\
    \ n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal:\
    \ n_gqa      = 1\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal:\
    \ freq_base  = 10000.0\nllama_model_load_internal: freq_scale = 1\nllama_model_load_internal:\
    \ ftype      = 7 (mostly Q8_0)\nllama_model_load_internal: model size = 7B\nllama_model_load_internal:\
    \ ggml ctx size =    0.08 MB\nllama_model_load_internal: mem required  = 7196.45\
    \ MB (+ 1024.00 MB per state)\nllama_new_context_with_model: kv self size  = 1024.00\
    \ MB\n\nsystem_info: n_threads = 15 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI\
    \ = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA\
    \ = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |\nsampling: repeat_last_n\
    \ = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty\
    \ = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000,\
    \ temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n\
    generate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n ### Instruction:\
    \ Write a story about llamas\\n### Response:::::::::::\n\n\n\n\n\n\n\n\n\n\n\n\
    \n\nCTRL+C\n\nsimard@CATALYS-1:/mnt/c/git/llama.cpp$"
  created_at: 2023-08-28 23:28:31+00:00
  edited: true
  hidden: false
  id: 64ed3baf9e28bbb8995ff57a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662098610747-noauth.jpeg?w=200&h=200&f=face
      fullname: Akarshan Biswas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akarshanbiswas
      type: user
    createdAt: '2023-08-29T05:19:03.000Z'
    data:
      edited: false
      editors:
      - akarshanbiswas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7448776364326477
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662098610747-noauth.jpeg?w=200&h=200&f=face
          fullname: Akarshan Biswas
          isHf: false
          isPro: false
          name: akarshanbiswas
          type: user
        html: '<blockquote>

          <p>-p "### Instruction: Write a story about llamas\n### Response:"</p>

          </blockquote>

          <p>It should be <code>-p "[INST] Write a story about llamas. [/INST] "</code>I
          think.</p>

          '
        raw: '> -p "### Instruction: Write a story about llamas\n### Response:"


          It should be `-p "[INST] Write a story about llamas. [/INST] "`I think.

          '
        updatedAt: '2023-08-29T05:19:03.604Z'
      numEdits: 0
      reactions: []
    id: 64ed7fc7d3dab90cf4eaeb99
    type: comment
  author: akarshanbiswas
  content: '> -p "### Instruction: Write a story about llamas\n### Response:"


    It should be `-p "[INST] Write a story about llamas. [/INST] "`I think.

    '
  created_at: 2023-08-29 04:19:03+00:00
  edited: false
  hidden: false
  id: 64ed7fc7d3dab90cf4eaeb99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf032b73af34b1906f8901b99ad239b6.svg
      fullname: G S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsimard
      type: user
    createdAt: '2023-08-29T16:28:01.000Z'
    data:
      edited: false
      editors:
      - gsimard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7778144478797913
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf032b73af34b1906f8901b99ad239b6.svg
          fullname: G S
          isHf: false
          isPro: false
          name: gsimard
          type: user
        html: '<p>Same issue with <em>-p "[INST] Write a story about llamas. [/INST]
          "</em></p>

          '
        raw: Same issue with *-p "[INST] Write a story about llamas. [/INST] "*
        updatedAt: '2023-08-29T16:28:01.004Z'
      numEdits: 0
      reactions: []
    id: 64ee1c9181e61fdcf756ebb9
    type: comment
  author: gsimard
  content: Same issue with *-p "[INST] Write a story about llamas. [/INST] "*
  created_at: 2023-08-29 15:28:01+00:00
  edited: false
  hidden: false
  id: 64ee1c9181e61fdcf756ebb9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a8fcbc35a348f47cd192f394aeaa72f.svg
      fullname: Tarun Mishra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tarun1986
      type: user
    createdAt: '2023-08-29T19:12:08.000Z'
    data:
      edited: false
      editors:
      - Tarun1986
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8812897801399231
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a8fcbc35a348f47cd192f394aeaa72f.svg
          fullname: Tarun Mishra
          isHf: false
          isPro: false
          name: Tarun1986
          type: user
        html: '<p>i am facing same issue</p>

          '
        raw: i am facing same issue
        updatedAt: '2023-08-29T19:12:08.583Z'
      numEdits: 0
      reactions: []
    id: 64ee4308fe7ebfcec4ade315
    type: comment
  author: Tarun1986
  content: i am facing same issue
  created_at: 2023-08-29 18:12:08+00:00
  edited: false
  hidden: false
  id: 64ee4308fe7ebfcec4ade315
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa8e2e38e07d1fa0d2dc611723bc8f4c.svg
      fullname: "\u0141ael Al-Halawani"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ljhwild
      type: user
    createdAt: '2023-08-30T16:31:18.000Z'
    data:
      edited: false
      editors:
      - ljhwild
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2991885542869568
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa8e2e38e07d1fa0d2dc611723bc8f4c.svg
          fullname: "\u0141ael Al-Halawani"
          isHf: false
          isPro: false
          name: ljhwild
          type: user
        html: '<p>How about<br>def llama_prompt(message: str, system_message: str
          = LLAMA_SYSTEM_PROMPT) -&gt; str:<br>    prompt = f"[INST] &lt;&gt;\n{system_message}\n&lt;&gt;\n\n{message}
          [/INST]"<br>    return prompt</p>

          '
        raw: "How about \ndef llama_prompt(message: str, system_message: str = LLAMA_SYSTEM_PROMPT)\
          \ -> str:\n    prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\\
          n\\n{message} [/INST]\"\n    return prompt"
        updatedAt: '2023-08-30T16:31:18.731Z'
      numEdits: 0
      reactions: []
    id: 64ef6ed677777df14e23fa11
    type: comment
  author: ljhwild
  content: "How about \ndef llama_prompt(message: str, system_message: str = LLAMA_SYSTEM_PROMPT)\
    \ -> str:\n    prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{message}\
    \ [/INST]\"\n    return prompt"
  created_at: 2023-08-30 15:31:18+00:00
  edited: false
  hidden: false
  id: 64ef6ed677777df14e23fa11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa8e2e38e07d1fa0d2dc611723bc8f4c.svg
      fullname: "\u0141ael Al-Halawani"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ljhwild
      type: user
    createdAt: '2023-08-30T16:32:40.000Z'
    data:
      edited: false
      editors:
      - ljhwild
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8327459692955017
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa8e2e38e07d1fa0d2dc611723bc8f4c.svg
          fullname: "\u0141ael Al-Halawani"
          isHf: false
          isPro: false
          name: ljhwild
          type: user
        html: '<p>It deleted system taggs form inside the brackets  and  it only left
          empty brackets. It should be double in fact so &lt; &lt; SYS &gt; &gt; and
          &lt; &lt; / SYS &gt; &gt; without spaces</p>

          '
        raw: It deleted system taggs form inside the brackets <SYS> and </SYS> it
          only left empty brackets. It should be double in fact so < < SYS > > and
          < < / SYS > > without spaces
        updatedAt: '2023-08-30T16:32:40.006Z'
      numEdits: 0
      reactions: []
    id: 64ef6f28a6a978156a96fa6d
    type: comment
  author: ljhwild
  content: It deleted system taggs form inside the brackets <SYS> and </SYS> it only
    left empty brackets. It should be double in fact so < < SYS > > and < < / SYS
    > > without spaces
  created_at: 2023-08-30 15:32:40+00:00
  edited: false
  hidden: false
  id: 64ef6f28a6a978156a96fa6d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf032b73af34b1906f8901b99ad239b6.svg
      fullname: G S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsimard
      type: user
    createdAt: '2023-08-31T19:26:26.000Z'
    data:
      edited: false
      editors:
      - gsimard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9985338449478149
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf032b73af34b1906f8901b99ad239b6.svg
          fullname: G S
          isHf: false
          isPro: false
          name: gsimard
          type: user
        html: '<p>Same issue as far as I can tell.</p>

          '
        raw: Same issue as far as I can tell.
        updatedAt: '2023-08-31T19:26:26.559Z'
      numEdits: 0
      reactions: []
    id: 64f0e9627cc645bea4984f4a
    type: comment
  author: gsimard
  content: Same issue as far as I can tell.
  created_at: 2023-08-31 18:26:26+00:00
  edited: false
  hidden: false
  id: 64f0e9627cc645bea4984f4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf032b73af34b1906f8901b99ad239b6.svg
      fullname: G S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsimard
      type: user
    createdAt: '2023-09-01T00:48:32.000Z'
    data:
      edited: false
      editors:
      - gsimard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9900436997413635
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf032b73af34b1906f8901b99ad239b6.svg
          fullname: G S
          isHf: false
          isPro: false
          name: gsimard
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> I know you must\
          \ be super busy, but would you have an idea ? I have followed the instructions\
          \ on this model's page as far as I can tell with no luck. Your other models\
          \ have been working fine.</p>\n"
        raw: '@TheBloke I know you must be super busy, but would you have an idea
          ? I have followed the instructions on this model''s page as far as I can
          tell with no luck. Your other models have been working fine.'
        updatedAt: '2023-09-01T00:48:32.533Z'
      numEdits: 0
      reactions: []
    id: 64f134e07811ae9db3924307
    type: comment
  author: gsimard
  content: '@TheBloke I know you must be super busy, but would you have an idea ?
    I have followed the instructions on this model''s page as far as I can tell with
    no luck. Your other models have been working fine.'
  created_at: 2023-08-31 23:48:32+00:00
  edited: false
  hidden: false
  id: 64f134e07811ae9db3924307
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b154a4b1dc69cfb770d5c0c9f6c94f5.svg
      fullname: Enes Jakic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: enesj
      type: user
    createdAt: '2023-09-29T19:30:32.000Z'
    data:
      edited: false
      editors:
      - enesj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.988386332988739
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b154a4b1dc69cfb770d5c0c9f6c94f5.svg
          fullname: Enes Jakic
          isHf: false
          isPro: false
          name: enesj
          type: user
        html: '<p>Same with me.</p>

          '
        raw: 'Same with me.

          '
        updatedAt: '2023-09-29T19:30:32.170Z'
      numEdits: 0
      reactions: []
    id: 651725d8684223da597c765a
    type: comment
  author: enesj
  content: 'Same with me.

    '
  created_at: 2023-09-29 18:30:32+00:00
  edited: false
  hidden: false
  id: 651725d8684223da597c765a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/55d45dc3a84542e05637d7df26d60fdc.svg
      fullname: Julian Streibel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JulianStreibel
      type: user
    createdAt: '2024-01-06T10:19:38.000Z'
    data:
      edited: false
      editors:
      - JulianStreibel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9597141146659851
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/55d45dc3a84542e05637d7df26d60fdc.svg
          fullname: Julian Streibel
          isHf: false
          isPro: false
          name: JulianStreibel
          type: user
        html: '<p>Same here.</p>

          '
        raw: Same here.
        updatedAt: '2024-01-06T10:19:38.287Z'
      numEdits: 0
      reactions: []
    id: 6599293a5f7a6d40f7064661
    type: comment
  author: JulianStreibel
  content: Same here.
  created_at: 2024-01-06 10:19:38+00:00
  edited: false
  hidden: false
  id: 6599293a5f7a6d40f7064661
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Llama-2-7B-32K-Instruct-GGML
repo_type: model
status: open
target_branch: null
title: Running  Llama-2-7B-32K-Instruct-GGML with llama.cpp ?
