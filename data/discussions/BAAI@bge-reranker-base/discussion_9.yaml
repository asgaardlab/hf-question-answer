!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lucashw
conflicting_files: null
created_at: 2023-12-11 16:19:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ca9fdcbcfc1061a8c387fb162bc2da4d.svg
      fullname: Huan Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucashw
      type: user
    createdAt: '2023-12-11T16:19:59.000Z'
    data:
      edited: true
      editors:
      - lucashw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49062463641166687
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ca9fdcbcfc1061a8c387fb162bc2da4d.svg
          fullname: Huan Wang
          isHf: false
          isPro: false
          name: lucashw
          type: user
        html: "<p>Hi, </p>\n<p>I'm trying to run command below, but I keep running\
          \ into error. </p>\n<pre><code>from transformers import AutoTokenizer\n\
          tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-base\")\n\
          </code></pre>\n<p>I tried cloning the entire repo and load the tokenizer\
          \ file manually locally, yet the same error occurs. Can anyone please help\
          \ troubleshoot? Does it require any specific \"transformers\" version? My\
          \ version is 4.34.1</p>\n<p>Here is the error. </p>\n<pre><code>---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[61], line 2\n      1 from transformers import AutoTokenizer\n----&gt;\
          \ 2 tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-base\"\
          )\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:751,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\n    747     if tokenizer_class is None:\n    748         raise\
          \ ValueError(\n    749             f\"Tokenizer class {tokenizer_class_candidate}\
          \ does not exist or is not currently imported.\"\n    750         )\n--&gt;\
          \ 751     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n    753 # Otherwise we have to be creative.\n    754\
          \ # if model is an encoder decoder, the encoder tokenizer class is used\
          \ by default\n    755 if isinstance(config, EncoderDecoderConfig):\n\nFile\
          \ ~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2017,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
          \ **kwargs)\n   2014     else:\n   2015         logger.info(f\"loading file\
          \ {file_path} from cache at {resolved_vocab_files[file_id]}\")\n-&gt; 2017\
          \ return cls._from_pretrained(\n   2018     resolved_vocab_files,\n   2019\
          \     pretrained_model_name_or_path,\n   2020     init_configuration,\n\
          \   2021     *init_inputs,\n   2022     token=token,\n   2023     cache_dir=cache_dir,\n\
          \   2024     local_files_only=local_files_only,\n   2025     _commit_hash=commit_hash,\n\
          \   2026     _is_local=is_local,\n   2027     **kwargs,\n   2028 )\n\nFile\
          \ ~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2249,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only,\
          \ _commit_hash, _is_local, *init_inputs, **kwargs)\n   2247 # Instantiate\
          \ the tokenizer.\n   2248 try:\n-&gt; 2249     tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\n   2250 except OSError:\n   2251     raise OSError(\n\
          \   2252         \"Unable to load vocabulary from file. \"\n   2253    \
          \     \"Please check that the provided vocabulary is accessible and not\
          \ corrupted.\"\n   2254     )\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:155,\
          \ in XLMRobertaTokenizerFast.__init__(self, vocab_file, tokenizer_file,\
          \ bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token,\
          \ **kwargs)\n    139 def __init__(\n    140     self,\n    141     vocab_file=None,\n\
          \   (...)\n    151 ):\n    152     # Mask token behave like a normal word,\
          \ i.e. include the space before it\n    153     mask_token = AddedToken(mask_token,\
          \ lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n\
          --&gt; 155     super().__init__(\n    156         vocab_file,\n    157 \
          \        tokenizer_file=tokenizer_file,\n    158         bos_token=bos_token,\n\
          \    159         eos_token=eos_token,\n    160         sep_token=sep_token,\n\
          \    161         cls_token=cls_token,\n    162         unk_token=unk_token,\n\
          \    163         pad_token=pad_token,\n    164         mask_token=mask_token,\n\
          \    165         **kwargs,\n    166     )\n    168     self.vocab_file =\
          \ vocab_file\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:120,\
          \ in PreTrainedTokenizerFast.__init__(self, *args, **kwargs)\n    118  \
          \   fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n    119 else:\n\
          --&gt; 120     raise ValueError(\n    121         \"Couldn't instantiate\
          \ the backend tokenizer from one of: \\n\"\n    122         \"(1) a `tokenizers`\
          \ library serialization file, \\n\"\n    123         \"(2) a slow tokenizer\
          \ instance to convert or \\n\"\n    124         \"(3) an equivalent slow\
          \ tokenizer class to instantiate and convert. \\n\"\n    125         \"\
          You need to have sentencepiece installed to convert a slow tokenizer to\
          \ a fast one.\"\n    126     )\n    128 self._tokenizer = fast_tokenizer\n\
          \    130 if slow_tokenizer is not None:\n\nValueError: Couldn't instantiate\
          \ the backend tokenizer from one of: \n(1) a `tokenizers` library serialization\
          \ file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent\
          \ slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece\
          \ installed to convert a slow tokenizer to a fast one.\n</code></pre>\n"
        raw: "Hi, \n\nI'm trying to run command below, but I keep running into error.\
          \ \n\n```\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"\
          BAAI/bge-reranker-base\")\n```\n\nI tried cloning the entire repo and load\
          \ the tokenizer file manually locally, yet the same error occurs. Can anyone\
          \ please help troubleshoot? Does it require any specific \"transformers\"\
          \ version? My version is 4.34.1\n\nHere is the error. \n\n```\n---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[61], line 2\n      1 from transformers import AutoTokenizer\n---->\
          \ 2 tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-base\"\
          )\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:751,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\n    747     if tokenizer_class is None:\n    748         raise\
          \ ValueError(\n    749             f\"Tokenizer class {tokenizer_class_candidate}\
          \ does not exist or is not currently imported.\"\n    750         )\n-->\
          \ 751     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n    753 # Otherwise we have to be creative.\n    754\
          \ # if model is an encoder decoder, the encoder tokenizer class is used\
          \ by default\n    755 if isinstance(config, EncoderDecoderConfig):\n\nFile\
          \ ~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2017,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
          \ **kwargs)\n   2014     else:\n   2015         logger.info(f\"loading file\
          \ {file_path} from cache at {resolved_vocab_files[file_id]}\")\n-> 2017\
          \ return cls._from_pretrained(\n   2018     resolved_vocab_files,\n   2019\
          \     pretrained_model_name_or_path,\n   2020     init_configuration,\n\
          \   2021     *init_inputs,\n   2022     token=token,\n   2023     cache_dir=cache_dir,\n\
          \   2024     local_files_only=local_files_only,\n   2025     _commit_hash=commit_hash,\n\
          \   2026     _is_local=is_local,\n   2027     **kwargs,\n   2028 )\n\nFile\
          \ ~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2249,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only,\
          \ _commit_hash, _is_local, *init_inputs, **kwargs)\n   2247 # Instantiate\
          \ the tokenizer.\n   2248 try:\n-> 2249     tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\n   2250 except OSError:\n   2251     raise OSError(\n\
          \   2252         \"Unable to load vocabulary from file. \"\n   2253    \
          \     \"Please check that the provided vocabulary is accessible and not\
          \ corrupted.\"\n   2254     )\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:155,\
          \ in XLMRobertaTokenizerFast.__init__(self, vocab_file, tokenizer_file,\
          \ bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token,\
          \ **kwargs)\n    139 def __init__(\n    140     self,\n    141     vocab_file=None,\n\
          \   (...)\n    151 ):\n    152     # Mask token behave like a normal word,\
          \ i.e. include the space before it\n    153     mask_token = AddedToken(mask_token,\
          \ lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n\
          --> 155     super().__init__(\n    156         vocab_file,\n    157    \
          \     tokenizer_file=tokenizer_file,\n    158         bos_token=bos_token,\n\
          \    159         eos_token=eos_token,\n    160         sep_token=sep_token,\n\
          \    161         cls_token=cls_token,\n    162         unk_token=unk_token,\n\
          \    163         pad_token=pad_token,\n    164         mask_token=mask_token,\n\
          \    165         **kwargs,\n    166     )\n    168     self.vocab_file =\
          \ vocab_file\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:120,\
          \ in PreTrainedTokenizerFast.__init__(self, *args, **kwargs)\n    118  \
          \   fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n    119 else:\n\
          --> 120     raise ValueError(\n    121         \"Couldn't instantiate the\
          \ backend tokenizer from one of: \\n\"\n    122         \"(1) a `tokenizers`\
          \ library serialization file, \\n\"\n    123         \"(2) a slow tokenizer\
          \ instance to convert or \\n\"\n    124         \"(3) an equivalent slow\
          \ tokenizer class to instantiate and convert. \\n\"\n    125         \"\
          You need to have sentencepiece installed to convert a slow tokenizer to\
          \ a fast one.\"\n    126     )\n    128 self._tokenizer = fast_tokenizer\n\
          \    130 if slow_tokenizer is not None:\n\nValueError: Couldn't instantiate\
          \ the backend tokenizer from one of: \n(1) a `tokenizers` library serialization\
          \ file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent\
          \ slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece\
          \ installed to convert a slow tokenizer to a fast one.\n```"
        updatedAt: '2023-12-11T16:21:02.146Z'
      numEdits: 2
      reactions: []
    id: 657736af5c5a6c93161b49ee
    type: comment
  author: lucashw
  content: "Hi, \n\nI'm trying to run command below, but I keep running into error.\
    \ \n\n```\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"\
    BAAI/bge-reranker-base\")\n```\n\nI tried cloning the entire repo and load the\
    \ tokenizer file manually locally, yet the same error occurs. Can anyone please\
    \ help troubleshoot? Does it require any specific \"transformers\" version? My\
    \ version is 4.34.1\n\nHere is the error. \n\n```\n---------------------------------------------------------------------------\n\
    ValueError                                Traceback (most recent call last)\n\
    Cell In[61], line 2\n      1 from transformers import AutoTokenizer\n----> 2 tokenizer\
    \ = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-base\")\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:751,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\n    747     if tokenizer_class is None:\n    748         raise ValueError(\n\
    \    749             f\"Tokenizer class {tokenizer_class_candidate} does not exist\
    \ or is not currently imported.\"\n    750         )\n--> 751     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\n    753 # Otherwise we have to be creative.\n    754 # if\
    \ model is an encoder decoder, the encoder tokenizer class is used by default\n\
    \    755 if isinstance(config, EncoderDecoderConfig):\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2017,\
    \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
    \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
    \ **kwargs)\n   2014     else:\n   2015         logger.info(f\"loading file {file_path}\
    \ from cache at {resolved_vocab_files[file_id]}\")\n-> 2017 return cls._from_pretrained(\n\
    \   2018     resolved_vocab_files,\n   2019     pretrained_model_name_or_path,\n\
    \   2020     init_configuration,\n   2021     *init_inputs,\n   2022     token=token,\n\
    \   2023     cache_dir=cache_dir,\n   2024     local_files_only=local_files_only,\n\
    \   2025     _commit_hash=commit_hash,\n   2026     _is_local=is_local,\n   2027\
    \     **kwargs,\n   2028 )\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2249,\
    \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local,\
    \ *init_inputs, **kwargs)\n   2247 # Instantiate the tokenizer.\n   2248 try:\n\
    -> 2249     tokenizer = cls(*init_inputs, **init_kwargs)\n   2250 except OSError:\n\
    \   2251     raise OSError(\n   2252         \"Unable to load vocabulary from\
    \ file. \"\n   2253         \"Please check that the provided vocabulary is accessible\
    \ and not corrupted.\"\n   2254     )\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:155,\
    \ in XLMRobertaTokenizerFast.__init__(self, vocab_file, tokenizer_file, bos_token,\
    \ eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\n\
    \    139 def __init__(\n    140     self,\n    141     vocab_file=None,\n   (...)\n\
    \    151 ):\n    152     # Mask token behave like a normal word, i.e. include\
    \ the space before it\n    153     mask_token = AddedToken(mask_token, lstrip=True,\
    \ rstrip=False) if isinstance(mask_token, str) else mask_token\n--> 155     super().__init__(\n\
    \    156         vocab_file,\n    157         tokenizer_file=tokenizer_file,\n\
    \    158         bos_token=bos_token,\n    159         eos_token=eos_token,\n\
    \    160         sep_token=sep_token,\n    161         cls_token=cls_token,\n\
    \    162         unk_token=unk_token,\n    163         pad_token=pad_token,\n\
    \    164         mask_token=mask_token,\n    165         **kwargs,\n    166  \
    \   )\n    168     self.vocab_file = vocab_file\n\nFile ~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:120,\
    \ in PreTrainedTokenizerFast.__init__(self, *args, **kwargs)\n    118     fast_tokenizer\
    \ = convert_slow_tokenizer(slow_tokenizer)\n    119 else:\n--> 120     raise ValueError(\n\
    \    121         \"Couldn't instantiate the backend tokenizer from one of: \\\
    n\"\n    122         \"(1) a `tokenizers` library serialization file, \\n\"\n\
    \    123         \"(2) a slow tokenizer instance to convert or \\n\"\n    124\
    \         \"(3) an equivalent slow tokenizer class to instantiate and convert.\
    \ \\n\"\n    125         \"You need to have sentencepiece installed to convert\
    \ a slow tokenizer to a fast one.\"\n    126     )\n    128 self._tokenizer =\
    \ fast_tokenizer\n    130 if slow_tokenizer is not None:\n\nValueError: Couldn't\
    \ instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library\
    \ serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent\
    \ slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece\
    \ installed to convert a slow tokenizer to a fast one.\n```"
  created_at: 2023-12-11 16:19:59+00:00
  edited: true
  hidden: false
  id: 657736af5c5a6c93161b49ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c0675d05a52192ee14e9ab1633353956.svg
      fullname: Xiao
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Shitao
      type: user
    createdAt: '2023-12-12T04:09:08.000Z'
    data:
      edited: false
      editors:
      - Shitao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9380658268928528
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c0675d05a52192ee14e9ab1633353956.svg
          fullname: Xiao
          isHf: false
          isPro: false
          name: Shitao
          type: user
        html: "<p>Hi, we have not encountered this error, and transformers=4.34.1\
          \ also works well.<br>According to the error information, you can try to\
          \ install the sentencepiece package and run this command again.\n </p>\n"
        raw: "Hi, we have not encountered this error, and transformers=4.34.1 also\
          \ works well. \nAccording to the error information, you can try to install\
          \ the sentencepiece package and run this command again.\n "
        updatedAt: '2023-12-12T04:09:08.831Z'
      numEdits: 0
      reactions: []
    id: 6577dce40e0d4703ffa59c6e
    type: comment
  author: Shitao
  content: "Hi, we have not encountered this error, and transformers=4.34.1 also works\
    \ well. \nAccording to the error information, you can try to install the sentencepiece\
    \ package and run this command again.\n "
  created_at: 2023-12-12 04:09:08+00:00
  edited: false
  hidden: false
  id: 6577dce40e0d4703ffa59c6e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ca9fdcbcfc1061a8c387fb162bc2da4d.svg
      fullname: Huan Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucashw
      type: user
    createdAt: '2023-12-12T05:35:43.000Z'
    data:
      edited: false
      editors:
      - lucashw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.989345908164978
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ca9fdcbcfc1061a8c387fb162bc2da4d.svg
          fullname: Huan Wang
          isHf: false
          isPro: false
          name: lucashw
          type: user
        html: '<p>I did install sentencepiece with its latest version, and it didn''t
          help. At a total loss now ...</p>

          '
        raw: I did install sentencepiece with its latest version, and it didn't help.
          At a total loss now ...
        updatedAt: '2023-12-12T05:35:43.365Z'
      numEdits: 0
      reactions: []
    id: 6577f12fdd2996f01ae7372c
    type: comment
  author: lucashw
  content: I did install sentencepiece with its latest version, and it didn't help.
    At a total loss now ...
  created_at: 2023-12-12 05:35:43+00:00
  edited: false
  hidden: false
  id: 6577f12fdd2996f01ae7372c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: BAAI/bge-reranker-base
repo_type: model
status: open
target_branch: null
title: Error " Couldn't instantiate the backend tokenizer from one of ... "
