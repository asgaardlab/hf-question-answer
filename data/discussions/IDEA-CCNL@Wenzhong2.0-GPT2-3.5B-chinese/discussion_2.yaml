!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ghosthamlet
conflicting_files: null
created_at: 2022-10-22 04:17:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7dcc2807164ad198dd515e8a48354b8f.svg
      fullname: Han
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ghosthamlet
      type: user
    createdAt: '2022-10-22T05:17:50.000Z'
    data:
      edited: true
      editors:
      - ghosthamlet
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7dcc2807164ad198dd515e8a48354b8f.svg
          fullname: Han
          isHf: false
          isPro: false
          name: ghosthamlet
          type: user
        html: '<p>Thanks for open source this large GPT2 model.<br>I found the tokenizer
          will tokenize many single word to two tokens,  then any text after tokenize
          will have double or more length .<br>don''t know why did you make this strange
          tokenizer?<br>you can see BPE tokenizer in the bloom model: <a href="https://huggingface.co/Langboat/bloom-6b4-zh">https://huggingface.co/Langboat/bloom-6b4-zh</a>,
          it has similar size vocab, but they tokenize most single word to single
          tokens.</p>

          '
        raw: 'Thanks for open source this large GPT2 model.

          I found the tokenizer will tokenize many single word to two tokens,  then
          any text after tokenize will have double or more length .

          don''t know why did you make this strange tokenizer?

          you can see BPE tokenizer in the bloom model: https://huggingface.co/Langboat/bloom-6b4-zh,
          it has similar size vocab, but they tokenize most single word to single
          tokens.'
        updatedAt: '2022-10-22T05:21:46.575Z'
      numEdits: 1
      reactions: []
    id: 63537cfea023833e59dacfe6
    type: comment
  author: ghosthamlet
  content: 'Thanks for open source this large GPT2 model.

    I found the tokenizer will tokenize many single word to two tokens,  then any
    text after tokenize will have double or more length .

    don''t know why did you make this strange tokenizer?

    you can see BPE tokenizer in the bloom model: https://huggingface.co/Langboat/bloom-6b4-zh,
    it has similar size vocab, but they tokenize most single word to single tokens.'
  created_at: 2022-10-22 04:17:50+00:00
  edited: true
  hidden: false
  id: 63537cfea023833e59dacfe6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1637806723246-612300897d70fa0063f38731.jpeg?w=200&h=200&f=face
      fullname: ganruyi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: roygan
      type: user
    createdAt: '2022-10-22T07:52:01.000Z'
    data:
      edited: false
      editors:
      - roygan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1637806723246-612300897d70fa0063f38731.jpeg?w=200&h=200&f=face
          fullname: ganruyi
          isHf: false
          isPro: false
          name: roygan
          type: user
        html: '<p>For chinese, single word to single tokens is more convenient. We
          use the original GPT2 tokenizer  for some early training reason, and look
          forward to reuse the large model in multilingual area. The model ''IDEA-CCNL/YuyuanQA-GPT2-3.5B''
          is based on it. We will release some GPT model which use ''single word to
          single tokens'' tokenizer like bert soon.</p>

          '
        raw: For chinese, single word to single tokens is more convenient. We use
          the original GPT2 tokenizer  for some early training reason, and look forward
          to reuse the large model in multilingual area. The model 'IDEA-CCNL/YuyuanQA-GPT2-3.5B'
          is based on it. We will release some GPT model which use 'single word to
          single tokens' tokenizer like bert soon.
        updatedAt: '2022-10-22T07:52:01.223Z'
      numEdits: 0
      reactions: []
    id: 6353a1213bc1819d22d2a151
    type: comment
  author: roygan
  content: For chinese, single word to single tokens is more convenient. We use the
    original GPT2 tokenizer  for some early training reason, and look forward to reuse
    the large model in multilingual area. The model 'IDEA-CCNL/YuyuanQA-GPT2-3.5B'
    is based on it. We will release some GPT model which use 'single word to single
    tokens' tokenizer like bert soon.
  created_at: 2022-10-22 06:52:01+00:00
  edited: false
  hidden: false
  id: 6353a1213bc1819d22d2a151
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7dcc2807164ad198dd515e8a48354b8f.svg
      fullname: Han
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ghosthamlet
      type: user
    createdAt: '2022-10-22T10:10:42.000Z'
    data:
      edited: false
      editors:
      - ghosthamlet
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7dcc2807164ad198dd515e8a48354b8f.svg
          fullname: Han
          isHf: false
          isPro: false
          name: ghosthamlet
          type: user
        html: '<p>Thanks for the detailed answer, Looking forward to the new GPT models.</p>

          '
        raw: Thanks for the detailed answer, Looking forward to the new GPT models.
        updatedAt: '2022-10-22T10:10:42.410Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6353c1a212edd0ed5dbf7b90
    id: 6353c1a212edd0ed5dbf7b8f
    type: comment
  author: ghosthamlet
  content: Thanks for the detailed answer, Looking forward to the new GPT models.
  created_at: 2022-10-22 09:10:42+00:00
  edited: false
  hidden: false
  id: 6353c1a212edd0ed5dbf7b8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7dcc2807164ad198dd515e8a48354b8f.svg
      fullname: Han
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ghosthamlet
      type: user
    createdAt: '2022-10-22T10:10:42.000Z'
    data:
      status: closed
    id: 6353c1a212edd0ed5dbf7b90
    type: status-change
  author: ghosthamlet
  created_at: 2022-10-22 09:10:42+00:00
  id: 6353c1a212edd0ed5dbf7b90
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese
repo_type: model
status: closed
target_branch: null
title: The tokenizer problem
