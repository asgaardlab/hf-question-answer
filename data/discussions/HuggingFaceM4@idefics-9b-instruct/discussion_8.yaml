!!python/object:huggingface_hub.community.DiscussionWithDetails
author: abalakrishnaTRI
conflicting_files: null
created_at: 2023-11-02 23:07:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd48cf05b946663dcb927545cd8edddc.svg
      fullname: Ashwin Balakrishna
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abalakrishnaTRI
      type: user
    createdAt: '2023-11-03T00:07:32.000Z'
    data:
      edited: false
      editors:
      - abalakrishnaTRI
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8862413763999939
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd48cf05b946663dcb927545cd8edddc.svg
          fullname: Ashwin Balakrishna
          isHf: false
          isPro: false
          name: abalakrishnaTRI
          type: user
        html: '<p>Hi, I''m trying to reproduce the 0-shot results in Table 2 in the
          IDEFICS paper for the VQA-v2, VizWiz, and TextVQA datasets. I have been
          starting with VQA-v2, and  It seems from the paper that idefics-9b-instruct
          is getting 50.9% accuracy on VQA-v2, but I am only able to get an accuracy
          of around 30%. I suspect there is a prompting problem. I am following the
          evaluation protocol here exactly to steer IDEFICS towards VQA: <a href="https://huggingface.co/docs/transformers/main/en/tasks/idefics#visual-question-answering">https://huggingface.co/docs/transformers/main/en/tasks/idefics#visual-question-answering</a></p>

          <p>This is an example output I am getting as a result:</p>

          <p>Prompt:<br>[''Instruction: Provide an answer to the question. Use the
          image to answer.\n'',<br>&lt;PIL.Image.Image image mode=RGB size=224x224
          at 0x7FC0940A0430&gt;,<br>''Question: what is the boy listening to? Answer:'']
          </p>

          <p>Model output:<br>''Instruction: Provide an answer to the question. Use
          the image to answer.\n Question: what is the boy listening to? Answer: music\n\nQuestion:
          what is the boy listening to? Answer: music\n\nQuestion: what is the boy
          listening to? Answer: music\n\nQuestion: what is the boy listening to? Answer:
          music\n\nQuestion: what is the boy listening to? Answer: music\n\nQuestion:
          what is the''</p>

          <p>For the VQA-v2, VizWiz, and TextVQA evals, would you be able to provide
          the exact prompt/evaluation code used for IDEFICS/point me to any necessary
          additional pre-processing beyond the example in the link above that might
          be needed to reproduce the results in the paper? Thanks a bunch for the
          help!</p>

          '
        raw: "Hi, I'm trying to reproduce the 0-shot results in Table 2 in the IDEFICS\
          \ paper for the VQA-v2, VizWiz, and TextVQA datasets. I have been starting\
          \ with VQA-v2, and  It seems from the paper that idefics-9b-instruct is\
          \ getting 50.9% accuracy on VQA-v2, but I am only able to get an accuracy\
          \ of around 30%. I suspect there is a prompting problem. I am following\
          \ the evaluation protocol here exactly to steer IDEFICS towards VQA: https://huggingface.co/docs/transformers/main/en/tasks/idefics#visual-question-answering\r\
          \n\r\nThis is an example output I am getting as a result:\r\n\r\nPrompt:\r\
          \n['Instruction: Provide an answer to the question. Use the image to answer.\\\
          n', \r\n<PIL.Image.Image image mode=RGB size=224x224 at 0x7FC0940A0430>,\
          \ \r\n'Question: what is the boy listening to? Answer:'] \r\n\r\nModel output:\
          \ \r\n'Instruction: Provide an answer to the question. Use the image to\
          \ answer.\\n Question: what is the boy listening to? Answer: music\\n\\\
          nQuestion: what is the boy listening to? Answer: music\\n\\nQuestion: what\
          \ is the boy listening to? Answer: music\\n\\nQuestion: what is the boy\
          \ listening to? Answer: music\\n\\nQuestion: what is the boy listening to?\
          \ Answer: music\\n\\nQuestion: what is the'\r\n\r\nFor the VQA-v2, VizWiz,\
          \ and TextVQA evals, would you be able to provide the exact prompt/evaluation\
          \ code used for IDEFICS/point me to any necessary additional pre-processing\
          \ beyond the example in the link above that might be needed to reproduce\
          \ the results in the paper? Thanks a bunch for the help!"
        updatedAt: '2023-11-03T00:07:32.127Z'
      numEdits: 0
      reactions: []
    id: 654439c4893aec5da95f278d
    type: comment
  author: abalakrishnaTRI
  content: "Hi, I'm trying to reproduce the 0-shot results in Table 2 in the IDEFICS\
    \ paper for the VQA-v2, VizWiz, and TextVQA datasets. I have been starting with\
    \ VQA-v2, and  It seems from the paper that idefics-9b-instruct is getting 50.9%\
    \ accuracy on VQA-v2, but I am only able to get an accuracy of around 30%. I suspect\
    \ there is a prompting problem. I am following the evaluation protocol here exactly\
    \ to steer IDEFICS towards VQA: https://huggingface.co/docs/transformers/main/en/tasks/idefics#visual-question-answering\r\
    \n\r\nThis is an example output I am getting as a result:\r\n\r\nPrompt:\r\n['Instruction:\
    \ Provide an answer to the question. Use the image to answer.\\n', \r\n<PIL.Image.Image\
    \ image mode=RGB size=224x224 at 0x7FC0940A0430>, \r\n'Question: what is the boy\
    \ listening to? Answer:'] \r\n\r\nModel output: \r\n'Instruction: Provide an answer\
    \ to the question. Use the image to answer.\\n Question: what is the boy listening\
    \ to? Answer: music\\n\\nQuestion: what is the boy listening to? Answer: music\\\
    n\\nQuestion: what is the boy listening to? Answer: music\\n\\nQuestion: what\
    \ is the boy listening to? Answer: music\\n\\nQuestion: what is the boy listening\
    \ to? Answer: music\\n\\nQuestion: what is the'\r\n\r\nFor the VQA-v2, VizWiz,\
    \ and TextVQA evals, would you be able to provide the exact prompt/evaluation\
    \ code used for IDEFICS/point me to any necessary additional pre-processing beyond\
    \ the example in the link above that might be needed to reproduce the results\
    \ in the paper? Thanks a bunch for the help!"
  created_at: 2023-11-02 23:07:32+00:00
  edited: false
  hidden: false
  id: 654439c4893aec5da95f278d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
      fullname: Victor Sanh
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: VictorSanh
      type: user
    createdAt: '2023-11-06T19:07:44.000Z'
    data:
      edited: false
      editors:
      - VictorSanh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9645730257034302
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
          fullname: Victor Sanh
          isHf: true
          isPro: true
          name: VictorSanh
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;HugoLaurencon&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/HugoLaurencon\"\
          >@<span class=\"underline\">HugoLaurencon</span></a></span>\n\n\t</span></span>\
          \ could you answer that question?</p>\n"
        raw: '@HugoLaurencon could you answer that question?'
        updatedAt: '2023-11-06T19:07:44.049Z'
      numEdits: 0
      reactions: []
    id: 65493980a80a87cccc7e60cd
    type: comment
  author: VictorSanh
  content: '@HugoLaurencon could you answer that question?'
  created_at: 2023-11-06 19:07:44+00:00
  edited: false
  hidden: false
  id: 65493980a80a87cccc7e60cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd48cf05b946663dcb927545cd8edddc.svg
      fullname: Ashwin Balakrishna
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abalakrishnaTRI
      type: user
    createdAt: '2023-11-07T13:54:44.000Z'
    data:
      edited: false
      editors:
      - abalakrishnaTRI
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9013084769248962
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd48cf05b946663dcb927545cd8edddc.svg
          fullname: Ashwin Balakrishna
          isHf: false
          isPro: false
          name: abalakrishnaTRI
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;VictorSanh&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/VictorSanh\"\
          >@<span class=\"underline\">VictorSanh</span></a></span>\n\n\t</span></span>,\
          \ <span data-props=\"{&quot;user&quot;:&quot;HugoLaurencon&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/HugoLaurencon\">@<span\
          \ class=\"underline\">HugoLaurencon</span></a></span>\n\n\t</span></span>\
          \ any thoughts you have on this would be super appreciated :) Thanks!</p>\n"
        raw: Thanks @VictorSanh, @HugoLaurencon any thoughts you have on this would
          be super appreciated :) Thanks!
        updatedAt: '2023-11-07T13:54:44.755Z'
      numEdits: 0
      reactions: []
    id: 654a41a4589f50f1b19f3a77
    type: comment
  author: abalakrishnaTRI
  content: Thanks @VictorSanh, @HugoLaurencon any thoughts you have on this would
    be super appreciated :) Thanks!
  created_at: 2023-11-07 13:54:44+00:00
  edited: false
  hidden: false
  id: 654a41a4589f50f1b19f3a77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1635201569275-noauth.jpeg?w=200&h=200&f=face
      fullname: "Hugo Lauren\xE7on"
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: HugoLaurencon
      type: user
    createdAt: '2023-11-07T14:58:23.000Z'
    data:
      edited: false
      editors:
      - HugoLaurencon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9196376204490662
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1635201569275-noauth.jpeg?w=200&h=200&f=face
          fullname: "Hugo Lauren\xE7on"
          isHf: true
          isPro: false
          name: HugoLaurencon
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;abalakrishnaTRI&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/abalakrishnaTRI\"\
          >@<span class=\"underline\">abalakrishnaTRI</span></a></span>\n\n\t</span></span>,\
          \ are you using the instruct model or the base model?<br>The number 50.9%\
          \ you are trying to reproduce is for the base model, for the instruct one\
          \ it's 65.8%.</p>\n<p>The prompts we used for the base model are in the\
          \ attached image (from the appendix of <a rel=\"nofollow\" href=\"https://arxiv.org/pdf/2306.16527.pdf\"\
          >https://arxiv.org/pdf/2306.16527.pdf</a>).<br><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/6177322d37f32ecb1e2d4cdf/bRmr9nIfcbqhQUTX9CtQL.png\"\
          ><img alt=\"screenshot.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6177322d37f32ecb1e2d4cdf/bRmr9nIfcbqhQUTX9CtQL.png\"\
          ></a></p>\n<p>For WizViz, note the prompts changes from VQAv2 or TextVQA.</p>\n\
          <p>It is slightly different from what you are using, you should an \"Image:\"\
          \ before the image.</p>\n<p>We also used stop words: whenever a stop word\
          \ is generated, we cut the generation at this place (without including the\
          \ stop word), strip the example (to remove extra white space) and use this\
          \ as the answer.<br>I believe the 0-shot, especially for VQA tasks, is not\
          \ a good way to evaluate the models because it depends too much on how you\
          \ are answering. Saying \"On the left\" when the correct answer is \"Left\"\
          \ will be scored 0. Using the stop words can help.</p>\n<p>We don't provide\
          \ access to the code base at the moment.<br>Can you try with these changes\
          \ to see if you can improve your performance? If not, can you try switching\
          \ to the base model to see if you can reproduce the result? It will help\
          \ identify the problem.</p>\n"
        raw: 'Hi @abalakrishnaTRI, are you using the instruct model or the base model?

          The number 50.9% you are trying to reproduce is for the base model, for
          the instruct one it''s 65.8%.


          The prompts we used for the base model are in the attached image (from the
          appendix of https://arxiv.org/pdf/2306.16527.pdf).

          ![screenshot.png](https://cdn-uploads.huggingface.co/production/uploads/6177322d37f32ecb1e2d4cdf/bRmr9nIfcbqhQUTX9CtQL.png)


          For WizViz, note the prompts changes from VQAv2 or TextVQA.



          It is slightly different from what you are using, you should an "Image:"
          before the image.


          We also used stop words: whenever a stop word is generated, we cut the generation
          at this place (without including the stop word), strip the example (to remove
          extra white space) and use this as the answer.

          I believe the 0-shot, especially for VQA tasks, is not a good way to evaluate
          the models because it depends too much on how you are answering. Saying
          "On the left" when the correct answer is "Left" will be scored 0. Using
          the stop words can help.


          We don''t provide access to the code base at the moment.

          Can you try with these changes to see if you can improve your performance?
          If not, can you try switching to the base model to see if you can reproduce
          the result? It will help identify the problem.'
        updatedAt: '2023-11-07T14:58:23.108Z'
      numEdits: 0
      reactions: []
    id: 654a508f7c2fd9829f438a36
    type: comment
  author: HugoLaurencon
  content: 'Hi @abalakrishnaTRI, are you using the instruct model or the base model?

    The number 50.9% you are trying to reproduce is for the base model, for the instruct
    one it''s 65.8%.


    The prompts we used for the base model are in the attached image (from the appendix
    of https://arxiv.org/pdf/2306.16527.pdf).

    ![screenshot.png](https://cdn-uploads.huggingface.co/production/uploads/6177322d37f32ecb1e2d4cdf/bRmr9nIfcbqhQUTX9CtQL.png)


    For WizViz, note the prompts changes from VQAv2 or TextVQA.



    It is slightly different from what you are using, you should an "Image:" before
    the image.


    We also used stop words: whenever a stop word is generated, we cut the generation
    at this place (without including the stop word), strip the example (to remove
    extra white space) and use this as the answer.

    I believe the 0-shot, especially for VQA tasks, is not a good way to evaluate
    the models because it depends too much on how you are answering. Saying "On the
    left" when the correct answer is "Left" will be scored 0. Using the stop words
    can help.


    We don''t provide access to the code base at the moment.

    Can you try with these changes to see if you can improve your performance? If
    not, can you try switching to the base model to see if you can reproduce the result?
    It will help identify the problem.'
  created_at: 2023-11-07 14:58:23+00:00
  edited: false
  hidden: false
  id: 654a508f7c2fd9829f438a36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/350f2048f67b452a2bb90d55662358fc.svg
      fullname: P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VishnuSuganth
      type: user
    createdAt: '2024-01-22T15:27:44.000Z'
    data:
      edited: true
      editors:
      - VishnuSuganth
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7806696891784668
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/350f2048f67b452a2bb90d55662358fc.svg
          fullname: P
          isHf: false
          isPro: false
          name: VishnuSuganth
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;HugoLaurencon&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/HugoLaurencon\"\
          >@<span class=\"underline\">HugoLaurencon</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;VictorSanh&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/VictorSanh\">@<span class=\"\
          underline\">VictorSanh</span></a></span>\n\n\t</span></span> </p>\n<p>Following\
          \ your suggestion in the previous comment, I am trying to reproduce the\
          \ VizWiz Zero Shot results on the validation set. But my zero-shot outputs\
          \ does not seem to be quite right. Quantitative numbers are not looking\
          \ good either.<br>Colab - <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1KNg3q1YFk5aLux4eii17COnUbsuYEFJ0?usp=sharing\"\
          >https://colab.research.google.com/drive/1KNg3q1YFk5aLux4eii17COnUbsuYEFJ0?usp=sharing</a></p>\n\
          <p>Can you review once and suggest any changes to the prompt ?</p>\n"
        raw: "@HugoLaurencon @VictorSanh \n\nFollowing your suggestion in the previous\
          \ comment, I am trying to reproduce the VizWiz Zero Shot results on the\
          \ validation set. But my zero-shot outputs does not seem to be quite right.\
          \ Quantitative numbers are not looking good either.\nColab - https://colab.research.google.com/drive/1KNg3q1YFk5aLux4eii17COnUbsuYEFJ0?usp=sharing\n\
          \nCan you review once and suggest any changes to the prompt ?"
        updatedAt: '2024-01-22T16:01:33.160Z'
      numEdits: 1
      reactions: []
    id: 65ae8970a134c07dde5577d6
    type: comment
  author: VishnuSuganth
  content: "@HugoLaurencon @VictorSanh \n\nFollowing your suggestion in the previous\
    \ comment, I am trying to reproduce the VizWiz Zero Shot results on the validation\
    \ set. But my zero-shot outputs does not seem to be quite right. Quantitative\
    \ numbers are not looking good either.\nColab - https://colab.research.google.com/drive/1KNg3q1YFk5aLux4eii17COnUbsuYEFJ0?usp=sharing\n\
    \nCan you review once and suggest any changes to the prompt ?"
  created_at: 2024-01-22 15:27:44+00:00
  edited: true
  hidden: false
  id: 65ae8970a134c07dde5577d6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: HuggingFaceM4/idefics-9b-instruct
repo_type: model
status: open
target_branch: null
title: 'Trying to reproduce VQA-v2 results in the Table 2 here: https://arxiv.org/pdf/2306.16527.pdf'
