!!python/object:huggingface_hub.community.DiscussionWithDetails
author: VishnuSuganth
conflicting_files: null
created_at: 2024-01-18 06:20:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/350f2048f67b452a2bb90d55662358fc.svg
      fullname: P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VishnuSuganth
      type: user
    createdAt: '2024-01-18T06:20:24.000Z'
    data:
      edited: false
      editors:
      - VishnuSuganth
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4887305200099945
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/350f2048f67b452a2bb90d55662358fc.svg
          fullname: P
          isHf: false
          isPro: false
          name: VishnuSuganth
          type: user
        html: "<p>Hi Team,</p>\n<p>I noticed an issue with attention masks returned\
          \ by processor for batched inputs sequences. For padded tokens, the attention\
          \ mask is set to 1 instead of 0. This behavior occurs when we set <code>padding='longest'</code>\
          \ and does not occur otherwise. Any thoughts ?</p>\n<p>Please find the code\
          \ below to reproduce with <code>transformers==v4.36.2</code>.</p>\n<pre><code>import\
          \ torch\nfrom transformers import AutoProcessor\n\ndevice = \"cuda:1\" if\
          \ torch.cuda.is_available() else \"cpu\"\ncheckpoint = \"HuggingFaceM4/idefics-9b-instruct\"\
          \nprocessor = AutoProcessor.from_pretrained(checkpoint)\n\n\nprompts = [\n\
          \    [\n        \"User: What is in this image?\",\n        \"https://upload.wikimedia.org/wikipedia/commons/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg\"\
          ,\n    ],   \n\n    [\n        \"User: Is there a cat in the image ? Please\
          \ answer yes or no.\",\n        \"https://upload.wikimedia.org/wikipedia/commons/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg\"\
          ,\n    ],  \n    \n]\n\nprint(prompts)\n\n# inputs = processor(prompts,\
          \ return_tensors=\"pt\", max_length=512, truncation=True, add_end_of_utterance_token=False).to(device)\n\
          inputs = processor(prompts, return_tensors=\"pt\", max_length=512, truncation=True,\
          \ padding='longest', add_end_of_utterance_token=False).to(device)\n\nprint(inputs['attention_mask'].shape)\n\
          print(inputs['attention_mask'])\n</code></pre>\n<p>Expected output:</p>\n\
          <pre><code>torch.Size([2, 20])\ntensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\
          \ 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\
          \ 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n       device='cuda:1')\n</code></pre>\n\
          <p>Actual output:</p>\n<pre><code>torch.Size([2, 20])\ntensor([[1, 1, 1,\
          \ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1,\
          \ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n       device='cuda:1')\n\
          </code></pre>\n"
        raw: "Hi Team,\r\n\r\nI noticed an issue with attention masks returned by\
          \ processor for batched inputs sequences. For padded tokens, the attention\
          \ mask is set to 1 instead of 0. This behavior occurs when we set `padding='longest'`\
          \ and does not occur otherwise. Any thoughts ?\r\n\r\nPlease find the code\
          \ below to reproduce with `transformers==v4.36.2`.\r\n\r\n```\r\nimport\
          \ torch\r\nfrom transformers import AutoProcessor\r\n\r\ndevice = \"cuda:1\"\
          \ if torch.cuda.is_available() else \"cpu\"\r\ncheckpoint = \"HuggingFaceM4/idefics-9b-instruct\"\
          \r\nprocessor = AutoProcessor.from_pretrained(checkpoint)\r\n\r\n\r\nprompts\
          \ = [\r\n    [\r\n        \"User: What is in this image?\",\r\n        \"\
          https://upload.wikimedia.org/wikipedia/commons/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg\"\
          ,\r\n    ],   \r\n\r\n    [\r\n        \"User: Is there a cat in the image\
          \ ? Please answer yes or no.\",\r\n        \"https://upload.wikimedia.org/wikipedia/commons/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg\"\
          ,\r\n    ],  \r\n    \r\n]\r\n\r\nprint(prompts)\r\n\r\n# inputs = processor(prompts,\
          \ return_tensors=\"pt\", max_length=512, truncation=True, add_end_of_utterance_token=False).to(device)\r\
          \ninputs = processor(prompts, return_tensors=\"pt\", max_length=512, truncation=True,\
          \ padding='longest', add_end_of_utterance_token=False).to(device)\r\n\r\n\
          print(inputs['attention_mask'].shape)\r\nprint(inputs['attention_mask'])\r\
          \n```\r\n\r\nExpected output:\r\n```\r\ntorch.Size([2, 20])\r\ntensor([[0,\
          \ 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\r\n        [1,\
          \ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\r\n       device='cuda:1')\r\
          \n```\r\n\r\nActual output:\r\n```\r\ntorch.Size([2, 20])\r\ntensor([[1,\
          \ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\r\n        [1,\
          \ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\r\n       device='cuda:1')\r\
          \n```"
        updatedAt: '2024-01-18T06:20:24.510Z'
      numEdits: 0
      reactions: []
    id: 65a8c32891ec5d1ec6f0740d
    type: comment
  author: VishnuSuganth
  content: "Hi Team,\r\n\r\nI noticed an issue with attention masks returned by processor\
    \ for batched inputs sequences. For padded tokens, the attention mask is set to\
    \ 1 instead of 0. This behavior occurs when we set `padding='longest'` and does\
    \ not occur otherwise. Any thoughts ?\r\n\r\nPlease find the code below to reproduce\
    \ with `transformers==v4.36.2`.\r\n\r\n```\r\nimport torch\r\nfrom transformers\
    \ import AutoProcessor\r\n\r\ndevice = \"cuda:1\" if torch.cuda.is_available()\
    \ else \"cpu\"\r\ncheckpoint = \"HuggingFaceM4/idefics-9b-instruct\"\r\nprocessor\
    \ = AutoProcessor.from_pretrained(checkpoint)\r\n\r\n\r\nprompts = [\r\n    [\r\
    \n        \"User: What is in this image?\",\r\n        \"https://upload.wikimedia.org/wikipedia/commons/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg\"\
    ,\r\n    ],   \r\n\r\n    [\r\n        \"User: Is there a cat in the image ? Please\
    \ answer yes or no.\",\r\n        \"https://upload.wikimedia.org/wikipedia/commons/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg\"\
    ,\r\n    ],  \r\n    \r\n]\r\n\r\nprint(prompts)\r\n\r\n# inputs = processor(prompts,\
    \ return_tensors=\"pt\", max_length=512, truncation=True, add_end_of_utterance_token=False).to(device)\r\
    \ninputs = processor(prompts, return_tensors=\"pt\", max_length=512, truncation=True,\
    \ padding='longest', add_end_of_utterance_token=False).to(device)\r\n\r\nprint(inputs['attention_mask'].shape)\r\
    \nprint(inputs['attention_mask'])\r\n```\r\n\r\nExpected output:\r\n```\r\ntorch.Size([2,\
    \ 20])\r\ntensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\r\
    \n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\r\n \
    \      device='cuda:1')\r\n```\r\n\r\nActual output:\r\n```\r\ntorch.Size([2,\
    \ 20])\r\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\r\
    \n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\r\n \
    \      device='cuda:1')\r\n```"
  created_at: 2024-01-18 06:20:24+00:00
  edited: false
  hidden: false
  id: 65a8c32891ec5d1ec6f0740d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
      fullname: Victor Sanh
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: VictorSanh
      type: user
    createdAt: '2024-01-18T17:30:15.000Z'
    data:
      edited: false
      editors:
      - VictorSanh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8151715397834778
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
          fullname: Victor Sanh
          isHf: true
          isPro: true
          name: VictorSanh
          type: user
        html: '<p>Hey!<br>thanks for the reproduction case, i can reproduce the problem.<br>i
          opened an issue on hf transformers: <a rel="nofollow" href="https://github.com/huggingface/transformers/issues/28591">https://github.com/huggingface/transformers/issues/28591</a></p>

          '
        raw: 'Hey!

          thanks for the reproduction case, i can reproduce the problem.

          i opened an issue on hf transformers: https://github.com/huggingface/transformers/issues/28591'
        updatedAt: '2024-01-18T17:30:15.932Z'
      numEdits: 0
      reactions: []
    id: 65a96027043d53781a5d0c35
    type: comment
  author: VictorSanh
  content: 'Hey!

    thanks for the reproduction case, i can reproduce the problem.

    i opened an issue on hf transformers: https://github.com/huggingface/transformers/issues/28591'
  created_at: 2024-01-18 17:30:15+00:00
  edited: false
  hidden: false
  id: 65a96027043d53781a5d0c35
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: HuggingFaceM4/idefics-9b-instruct
repo_type: model
status: open
target_branch: null
title: AttentionMasks wrongly set with padding='longest'
