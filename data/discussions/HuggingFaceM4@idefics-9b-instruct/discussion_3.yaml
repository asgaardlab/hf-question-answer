!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Sol-B-ML
conflicting_files: null
created_at: 2023-09-14 14:26:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/21fe1ae7203ad66e48221da8a6eea8b4.svg
      fullname: Sol Bridger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sol-B-ML
      type: user
    createdAt: '2023-09-14T15:26:58.000Z'
    data:
      edited: false
      editors:
      - Sol-B-ML
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8409257531166077
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/21fe1ae7203ad66e48221da8a6eea8b4.svg
          fullname: Sol Bridger
          isHf: false
          isPro: false
          name: Sol-B-ML
          type: user
        html: '<p>When calling the generate() method or model.logits, the logits returned
          are not the logits of the tokens predicted by the model, but rather the
          logits of prompt tokens given by the user. Is this intentional? If so, why
          has this behaviour been chosen? Also, are there plans to alter or add a
          method that can extract the logits of the tokens actually predicted by the
          model?</p>

          '
        raw: When calling the generate() method or model.logits, the logits returned
          are not the logits of the tokens predicted by the model, but rather the
          logits of prompt tokens given by the user. Is this intentional? If so, why
          has this behaviour been chosen? Also, are there plans to alter or add a
          method that can extract the logits of the tokens actually predicted by the
          model?
        updatedAt: '2023-09-14T15:26:58.885Z'
      numEdits: 0
      reactions: []
    id: 65032642e89922975fdac989
    type: comment
  author: Sol-B-ML
  content: When calling the generate() method or model.logits, the logits returned
    are not the logits of the tokens predicted by the model, but rather the logits
    of prompt tokens given by the user. Is this intentional? If so, why has this behaviour
    been chosen? Also, are there plans to alter or add a method that can extract the
    logits of the tokens actually predicted by the model?
  created_at: 2023-09-14 14:26:58+00:00
  edited: false
  hidden: false
  id: 65032642e89922975fdac989
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/21fe1ae7203ad66e48221da8a6eea8b4.svg
      fullname: Sol Bridger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sol-B-ML
      type: user
    createdAt: '2023-09-15T15:27:45.000Z'
    data:
      status: closed
    id: 650477f1586652b6f9331adf
    type: status-change
  author: Sol-B-ML
  created_at: 2023-09-15 14:27:45+00:00
  id: 650477f1586652b6f9331adf
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: HuggingFaceM4/idefics-9b-instruct
repo_type: model
status: closed
target_branch: null
title: Problem with the logits returned from the generate() method
