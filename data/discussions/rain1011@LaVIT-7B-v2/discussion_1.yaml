!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Andyrasika
conflicting_files: null
created_at: 2023-11-23 14:09:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6290ec00a29097b211b94f0f/7OYU-yroOUxJryJkl33L6.png?w=200&h=200&f=face
      fullname: Ankush Singal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Andyrasika
      type: user
    createdAt: '2023-11-23T14:09:16.000Z'
    data:
      edited: false
      editors:
      - Andyrasika
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5304301977157593
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6290ec00a29097b211b94f0f/7OYU-yroOUxJryJkl33L6.png?w=200&h=200&f=face
          fullname: Ankush Singal
          isHf: false
          isPro: false
          name: Andyrasika
          type: user
        html: "<p>Thanks for sharing with amazing model , here is some issues i am\
          \ getting running the model:</p>\n<pre><code>---------------------------------------------------------------------------\n\
          NotImplementedError                       Traceback (most recent call last)\n\
          Cell In[4], line 17\n     12 device = torch.device('cuda')\n     14 # For\
          \ Multi-modal Image Generation, must set `load_tokenizer=True` to load the\
          \ tokenizer to tokenize input image.\n     15 # If you have already install\
          \ xformers, set `use_xformers=True` to save the GPU memory (Xformers is\
          \ not supported on V100 GPU)\n     16 # If you have already download the\
          \ checkpoint, set `local_files_only=True`` to avoid auto-downloading from\
          \ remote\n---&gt; 17 model = build_model(model_path=model_path, model_dtype=model_dtype,\
          \ check_safety=False,\n     18             device_id=device_id, use_xformers=True,\
          \ understanding=False, load_tokenizer=True)\n     19 model = model.to(device)\n\
          \     20 print(\"Building Model Finsished\")\n\nFile /workspace/LaVIT/models/__init__.py:47,\
          \ in build_model(model_path, model_dtype, device_id, image_size, use_xformers,\
          \ understanding, load_tokenizer, pixel_decoding, check_safety, local_files_only)\n\
          \     44     lavit = LaVITforUnderstanding(model_path=model_path, model_dtype=model_dtype,\
          \ \n     45             device_id=device_id, use_xformers=use_xformers)\n\
          \     46 else:\n---&gt; 47     lavit = LaVITforGeneration(model_path=model_path,\
          \ model_dtype=model_dtype, device_id=device_id, \n     48             use_xformers=use_xformers,\
          \ check_safety=check_safety, load_tokenizer=load_tokenizer, pixel_decoding=pixel_decoding)\n\
          \     50 # Convert the model parameters to the defined precision\n     51\
          \ if model_dtype == 'bf16':\n\nFile /workspace/LaVIT/models/lavit_for_generation.py:121,\
          \ in LaVITforGeneration.__init__(self, model_path, model_dtype, device_id,\
          \ use_xformers, check_safety, load_tokenizer, pixel_decoding, **kwargs)\n\
          \    117     if xformers_version == version.parse(\"0.0.16\"):\n    118\
          \         print(\n    119             \"xFormers 0.0.16 cannot be used for\
          \ training in some GPUs. If you observe problems during training, please\
          \ update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers\
          \ for more details.\"\n    120         )\n--&gt; 121     self.unet.enable_xformers_memory_efficient_attention()\n\
          \    122 else:\n    123     raise ValueError(\"xformers is not available.\
          \ Make sure it is installed correctly or set `use_xformers=False`\")\n\n\
          File /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:287,\
          \ in ModelMixin.enable_xformers_memory_efficient_attention(self, attention_op)\n\
          \    253 def enable_xformers_memory_efficient_attention(self, attention_op:\
          \ Optional[Callable] = None):\n    254     r\"\"\"\n    255     Enable memory\
          \ efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).\n\
          \    256 \n   (...)\n    285     ```\n    286     \"\"\"\n--&gt; 287   \
          \  self.set_use_memory_efficient_attention_xformers(True, attention_op)\n\
          \nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:251,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers(self, valid,\
          \ attention_op)\n    249 for module in self.children():\n    250     if\
          \ isinstance(module, torch.nn.Module):\n--&gt; 251         fn_recursive_set_mem_eff(module)\n\
          \nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers.&lt;locals&gt;.fn_recursive_set_mem_eff(module)\n\
          \    244     module.set_use_memory_efficient_attention_xformers(valid, attention_op)\n\
          \    246 for child in module.children():\n--&gt; 247     fn_recursive_set_mem_eff(child)\n\
          \nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers.&lt;locals&gt;.fn_recursive_set_mem_eff(module)\n\
          \    244     module.set_use_memory_efficient_attention_xformers(valid, attention_op)\n\
          \    246 for child in module.children():\n--&gt; 247     fn_recursive_set_mem_eff(child)\n\
          \nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers.&lt;locals&gt;.fn_recursive_set_mem_eff(module)\n\
          \    244     module.set_use_memory_efficient_attention_xformers(valid, attention_op)\n\
          \    246 for child in module.children():\n--&gt; 247     fn_recursive_set_mem_eff(child)\n\
          \nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:244,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers.&lt;locals&gt;.fn_recursive_set_mem_eff(module)\n\
          \    242 def fn_recursive_set_mem_eff(module: torch.nn.Module):\n    243\
          \     if hasattr(module, \"set_use_memory_efficient_attention_xformers\"\
          ):\n--&gt; 244         module.set_use_memory_efficient_attention_xformers(valid,\
          \ attention_op)\n    246     for child in module.children():\n    247  \
          \       fn_recursive_set_mem_eff(child)\n\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:251,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers(self, valid,\
          \ attention_op)\n    249 for module in self.children():\n    250     if\
          \ isinstance(module, torch.nn.Module):\n--&gt; 251         fn_recursive_set_mem_eff(module)\n\
          \nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers.&lt;locals&gt;.fn_recursive_set_mem_eff(module)\n\
          \    244     module.set_use_memory_efficient_attention_xformers(valid, attention_op)\n\
          \    246 for child in module.children():\n--&gt; 247     fn_recursive_set_mem_eff(child)\n\
          \nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers.&lt;locals&gt;.fn_recursive_set_mem_eff(module)\n\
          \    244     module.set_use_memory_efficient_attention_xformers(valid, attention_op)\n\
          \    246 for child in module.children():\n--&gt; 247     fn_recursive_set_mem_eff(child)\n\
          \nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:244,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers.&lt;locals&gt;.fn_recursive_set_mem_eff(module)\n\
          \    242 def fn_recursive_set_mem_eff(module: torch.nn.Module):\n    243\
          \     if hasattr(module, \"set_use_memory_efficient_attention_xformers\"\
          ):\n--&gt; 244         module.set_use_memory_efficient_attention_xformers(valid,\
          \ attention_op)\n    246     for child in module.children():\n    247  \
          \       fn_recursive_set_mem_eff(child)\n\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/attention_processor.py:216,\
          \ in Attention.set_use_memory_efficient_attention_xformers(self, use_memory_efficient_attention_xformers,\
          \ attention_op)\n    210         _ = xformers.ops.memory_efficient_attention(\n\
          \    211             torch.randn((1, 2, 40), device=\"cuda\"),\n    212\
          \             torch.randn((1, 2, 40), device=\"cuda\"),\n    213       \
          \      torch.randn((1, 2, 40), device=\"cuda\"),\n    214         )\n  \
          \  215     except Exception as e:\n--&gt; 216         raise e\n    218 if\
          \ is_lora:\n    219     # TODO (sayakpaul): should we throw a warning if\
          \ someone wants to use the xformers\n    220     # variant when using PT\
          \ 2.0 now that we have LoRAAttnProcessor2_0?\n    221     processor = LoRAXFormersAttnProcessor(\n\
          \    222         hidden_size=self.processor.hidden_size,\n    223      \
          \   cross_attention_dim=self.processor.cross_attention_dim,\n    224   \
          \      rank=self.processor.rank,\n    225         attention_op=attention_op,\n\
          \    226     )\n\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/attention_processor.py:210,\
          \ in Attention.set_use_memory_efficient_attention_xformers(self, use_memory_efficient_attention_xformers,\
          \ attention_op)\n    207 else:\n    208     try:\n    209         # Make\
          \ sure we can run the memory efficient attention\n--&gt; 210         _ =\
          \ xformers.ops.memory_efficient_attention(\n    211             torch.randn((1,\
          \ 2, 40), device=\"cuda\"),\n    212             torch.randn((1, 2, 40),\
          \ device=\"cuda\"),\n    213             torch.randn((1, 2, 40), device=\"\
          cuda\"),\n    214         )\n    215     except Exception as e:\n    216\
          \         raise e\n\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py:223,\
          \ in memory_efficient_attention(query, key, value, attn_bias, p, scale,\
          \ op)\n    116 def memory_efficient_attention(\n    117     query: torch.Tensor,\n\
          \    118     key: torch.Tensor,\n   (...)\n    124     op: Optional[AttentionOp]\
          \ = None,\n    125 ) -&gt; torch.Tensor:\n    126     \"\"\"Implements the\
          \ memory-efficient attention mechanism following\n    127     `\"Self-Attention\
          \ Does Not Need O(n^2) Memory\" &lt;http://arxiv.org/abs/2112.05682&gt;`_.\n\
          \    128 \n   (...)\n    221     :return: multi-head attention Tensor with\
          \ shape ``[B, Mq, H, Kv]``\n    222     \"\"\"\n--&gt; 223     return _memory_efficient_attention(\n\
          \    224         Inputs(\n    225             query=query, key=key, value=value,\
          \ p=p, attn_bias=attn_bias, scale=scale\n    226         ),\n    227   \
          \      op=op,\n    228     )\n\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py:321,\
          \ in _memory_efficient_attention(inp, op)\n    316 def _memory_efficient_attention(\n\
          \    317     inp: Inputs, op: Optional[AttentionOp] = None\n    318 ) -&gt;\
          \ torch.Tensor:\n    319     # fast-path that doesn't require computing\
          \ the logsumexp for backward computation\n    320     if all(x.requires_grad\
          \ is False for x in [inp.query, inp.key, inp.value]):\n--&gt; 321      \
          \   return _memory_efficient_attention_forward(\n    322             inp,\
          \ op=op[0] if op is not None else None\n    323         )\n    325     output_shape\
          \ = inp.normalize_bmhk()\n    326     return _fMHA.apply(\n    327     \
          \    op, inp.query, inp.key, inp.value, inp.attn_bias, inp.p, inp.scale\n\
          \    328     ).reshape(output_shape)\n\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py:337,\
          \ in _memory_efficient_attention_forward(inp, op)\n    335 output_shape\
          \ = inp.normalize_bmhk()\n    336 if op is None:\n--&gt; 337     op = _dispatch_fw(inp,\
          \ False)\n    338 else:\n    339     _ensure_op_supports_or_raise(ValueError,\
          \ \"memory_efficient_attention\", op, inp)\n\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/dispatch.py:120,\
          \ in _dispatch_fw(inp, needs_gradient)\n    111 def _dispatch_fw(inp: Inputs,\
          \ needs_gradient: bool) -&gt; Type[AttentionFwOpBase]:\n    112     \"\"\
          \"Computes the best operator for forward\n    113 \n    114     Raises:\n\
          \   (...)\n    118         AttentionOp: The best operator for the configuration\n\
          \    119     \"\"\"\n--&gt; 120     return _run_priority_list(\n    121\
          \         \"memory_efficient_attention_forward\",\n    122         _dispatch_fw_priority_list(inp,\
          \ needs_gradient),\n    123         inp,\n    124     )\n\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/dispatch.py:63,\
          \ in _run_priority_list(name, priority_list, inp)\n     61 for op, not_supported\
          \ in zip(priority_list, not_supported_reasons):\n     62     msg += \"\\\
          n\" + _format_not_supported_reasons(op, not_supported)\n---&gt; 63 raise\
          \ NotImplementedError(msg)\n\nNotImplementedError: No operator found for\
          \ `memory_efficient_attention_forward` with inputs:\n     query       :\
          \ shape=(1, 2, 1, 40) (torch.float32)\n     key         : shape=(1, 2, 1,\
          \ 40) (torch.float32)\n     value       : shape=(1, 2, 1, 40) (torch.float32)\n\
          \     attn_bias   : &lt;class 'NoneType'&gt;\n     p           : 0.0\n`decoderF`\
          \ is not supported because:\n    xFormers wasn't build with CUDA support\n\
          \    attn_bias type is &lt;class 'NoneType'&gt;\n    operator wasn't built\
          \ - see `python -m xformers.info` for more info\n`flshattF@0.0.0` is not\
          \ supported because:\n    xFormers wasn't build with CUDA support\n    dtype=torch.float32\
          \ (supported: {torch.bfloat16, torch.float16})\n    operator wasn't built\
          \ - see `python -m xformers.info` for more info\n`tritonflashattF` is not\
          \ supported because:\n    xFormers wasn't build with CUDA support\n    dtype=torch.float32\
          \ (supported: {torch.bfloat16, torch.float16})\n    operator wasn't built\
          \ - see `python -m xformers.info` for more info\n    triton is not available\n\
          \    Only work on pre-MLIR triton for now\n`cutlassF` is not supported because:\n\
          \    xFormers wasn't build with CUDA support\n    operator wasn't built\
          \ - see `python -m xformers.info` for more info\n`smallkF` is not supported\
          \ because:\n    max(query.shape[-1] != value.shape[-1]) &gt; 32\n    xFormers\
          \ wasn't build with CUDA support\n    operator wasn't built - see `python\
          \ -m xformers.info` for more info\n    unsupported embed per head: 40\n\
          </code></pre>\n<p>Any help to resolve it would be appreciated.<br>Thanks,<br>Andy\
          \ </p>\n"
        raw: "Thanks for sharing with amazing model , here is some issues i am getting\
          \ running the model:\r\n```\r\n---------------------------------------------------------------------------\r\
          \nNotImplementedError                       Traceback (most recent call\
          \ last)\r\nCell In[4], line 17\r\n     12 device = torch.device('cuda')\r\
          \n     14 # For Multi-modal Image Generation, must set `load_tokenizer=True`\
          \ to load the tokenizer to tokenize input image.\r\n     15 # If you have\
          \ already install xformers, set `use_xformers=True` to save the GPU memory\
          \ (Xformers is not supported on V100 GPU)\r\n     16 # If you have already\
          \ download the checkpoint, set `local_files_only=True`` to avoid auto-downloading\
          \ from remote\r\n---> 17 model = build_model(model_path=model_path, model_dtype=model_dtype,\
          \ check_safety=False,\r\n     18             device_id=device_id, use_xformers=True,\
          \ understanding=False, load_tokenizer=True)\r\n     19 model = model.to(device)\r\
          \n     20 print(\"Building Model Finsished\")\r\n\r\nFile /workspace/LaVIT/models/__init__.py:47,\
          \ in build_model(model_path, model_dtype, device_id, image_size, use_xformers,\
          \ understanding, load_tokenizer, pixel_decoding, check_safety, local_files_only)\r\
          \n     44     lavit = LaVITforUnderstanding(model_path=model_path, model_dtype=model_dtype,\
          \ \r\n     45             device_id=device_id, use_xformers=use_xformers)\r\
          \n     46 else:\r\n---> 47     lavit = LaVITforGeneration(model_path=model_path,\
          \ model_dtype=model_dtype, device_id=device_id, \r\n     48            \
          \ use_xformers=use_xformers, check_safety=check_safety, load_tokenizer=load_tokenizer,\
          \ pixel_decoding=pixel_decoding)\r\n     50 # Convert the model parameters\
          \ to the defined precision\r\n     51 if model_dtype == 'bf16':\r\n\r\n\
          File /workspace/LaVIT/models/lavit_for_generation.py:121, in LaVITforGeneration.__init__(self,\
          \ model_path, model_dtype, device_id, use_xformers, check_safety, load_tokenizer,\
          \ pixel_decoding, **kwargs)\r\n    117     if xformers_version == version.parse(\"\
          0.0.16\"):\r\n    118         print(\r\n    119             \"xFormers 0.0.16\
          \ cannot be used for training in some GPUs. If you observe problems during\
          \ training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers\
          \ for more details.\"\r\n    120         )\r\n--> 121     self.unet.enable_xformers_memory_efficient_attention()\r\
          \n    122 else:\r\n    123     raise ValueError(\"xformers is not available.\
          \ Make sure it is installed correctly or set `use_xformers=False`\")\r\n\
          \r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:287,\
          \ in ModelMixin.enable_xformers_memory_efficient_attention(self, attention_op)\r\
          \n    253 def enable_xformers_memory_efficient_attention(self, attention_op:\
          \ Optional[Callable] = None):\r\n    254     r\"\"\"\r\n    255     Enable\
          \ memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).\r\
          \n    256 \r\n   (...)\r\n    285     ```\r\n    286     \"\"\"\r\n--> 287\
          \     self.set_use_memory_efficient_attention_xformers(True, attention_op)\r\
          \n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:251,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers(self, valid,\
          \ attention_op)\r\n    249 for module in self.children():\r\n    250   \
          \  if isinstance(module, torch.nn.Module):\r\n--> 251         fn_recursive_set_mem_eff(module)\r\
          \n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff(module)\r\
          \n    244     module.set_use_memory_efficient_attention_xformers(valid,\
          \ attention_op)\r\n    246 for child in module.children():\r\n--> 247  \
          \   fn_recursive_set_mem_eff(child)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff(module)\r\
          \n    244     module.set_use_memory_efficient_attention_xformers(valid,\
          \ attention_op)\r\n    246 for child in module.children():\r\n--> 247  \
          \   fn_recursive_set_mem_eff(child)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff(module)\r\
          \n    244     module.set_use_memory_efficient_attention_xformers(valid,\
          \ attention_op)\r\n    246 for child in module.children():\r\n--> 247  \
          \   fn_recursive_set_mem_eff(child)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:244,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff(module)\r\
          \n    242 def fn_recursive_set_mem_eff(module: torch.nn.Module):\r\n   \
          \ 243     if hasattr(module, \"set_use_memory_efficient_attention_xformers\"\
          ):\r\n--> 244         module.set_use_memory_efficient_attention_xformers(valid,\
          \ attention_op)\r\n    246     for child in module.children():\r\n    247\
          \         fn_recursive_set_mem_eff(child)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:251,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers(self, valid,\
          \ attention_op)\r\n    249 for module in self.children():\r\n    250   \
          \  if isinstance(module, torch.nn.Module):\r\n--> 251         fn_recursive_set_mem_eff(module)\r\
          \n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff(module)\r\
          \n    244     module.set_use_memory_efficient_attention_xformers(valid,\
          \ attention_op)\r\n    246 for child in module.children():\r\n--> 247  \
          \   fn_recursive_set_mem_eff(child)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff(module)\r\
          \n    244     module.set_use_memory_efficient_attention_xformers(valid,\
          \ attention_op)\r\n    246 for child in module.children():\r\n--> 247  \
          \   fn_recursive_set_mem_eff(child)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:244,\
          \ in ModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff(module)\r\
          \n    242 def fn_recursive_set_mem_eff(module: torch.nn.Module):\r\n   \
          \ 243     if hasattr(module, \"set_use_memory_efficient_attention_xformers\"\
          ):\r\n--> 244         module.set_use_memory_efficient_attention_xformers(valid,\
          \ attention_op)\r\n    246     for child in module.children():\r\n    247\
          \         fn_recursive_set_mem_eff(child)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/attention_processor.py:216,\
          \ in Attention.set_use_memory_efficient_attention_xformers(self, use_memory_efficient_attention_xformers,\
          \ attention_op)\r\n    210         _ = xformers.ops.memory_efficient_attention(\r\
          \n    211             torch.randn((1, 2, 40), device=\"cuda\"),\r\n    212\
          \             torch.randn((1, 2, 40), device=\"cuda\"),\r\n    213     \
          \        torch.randn((1, 2, 40), device=\"cuda\"),\r\n    214         )\r\
          \n    215     except Exception as e:\r\n--> 216         raise e\r\n    218\
          \ if is_lora:\r\n    219     # TODO (sayakpaul): should we throw a warning\
          \ if someone wants to use the xformers\r\n    220     # variant when using\
          \ PT 2.0 now that we have LoRAAttnProcessor2_0?\r\n    221     processor\
          \ = LoRAXFormersAttnProcessor(\r\n    222         hidden_size=self.processor.hidden_size,\r\
          \n    223         cross_attention_dim=self.processor.cross_attention_dim,\r\
          \n    224         rank=self.processor.rank,\r\n    225         attention_op=attention_op,\r\
          \n    226     )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/attention_processor.py:210,\
          \ in Attention.set_use_memory_efficient_attention_xformers(self, use_memory_efficient_attention_xformers,\
          \ attention_op)\r\n    207 else:\r\n    208     try:\r\n    209        \
          \ # Make sure we can run the memory efficient attention\r\n--> 210     \
          \    _ = xformers.ops.memory_efficient_attention(\r\n    211           \
          \  torch.randn((1, 2, 40), device=\"cuda\"),\r\n    212             torch.randn((1,\
          \ 2, 40), device=\"cuda\"),\r\n    213             torch.randn((1, 2, 40),\
          \ device=\"cuda\"),\r\n    214         )\r\n    215     except Exception\
          \ as e:\r\n    216         raise e\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py:223,\
          \ in memory_efficient_attention(query, key, value, attn_bias, p, scale,\
          \ op)\r\n    116 def memory_efficient_attention(\r\n    117     query: torch.Tensor,\r\
          \n    118     key: torch.Tensor,\r\n   (...)\r\n    124     op: Optional[AttentionOp]\
          \ = None,\r\n    125 ) -> torch.Tensor:\r\n    126     \"\"\"Implements\
          \ the memory-efficient attention mechanism following\r\n    127     `\"\
          Self-Attention Does Not Need O(n^2) Memory\" <http://arxiv.org/abs/2112.05682>`_.\r\
          \n    128 \r\n   (...)\r\n    221     :return: multi-head attention Tensor\
          \ with shape ``[B, Mq, H, Kv]``\r\n    222     \"\"\"\r\n--> 223     return\
          \ _memory_efficient_attention(\r\n    224         Inputs(\r\n    225   \
          \          query=query, key=key, value=value, p=p, attn_bias=attn_bias,\
          \ scale=scale\r\n    226         ),\r\n    227         op=op,\r\n    228\
          \     )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py:321,\
          \ in _memory_efficient_attention(inp, op)\r\n    316 def _memory_efficient_attention(\r\
          \n    317     inp: Inputs, op: Optional[AttentionOp] = None\r\n    318 )\
          \ -> torch.Tensor:\r\n    319     # fast-path that doesn't require computing\
          \ the logsumexp for backward computation\r\n    320     if all(x.requires_grad\
          \ is False for x in [inp.query, inp.key, inp.value]):\r\n--> 321       \
          \  return _memory_efficient_attention_forward(\r\n    322             inp,\
          \ op=op[0] if op is not None else None\r\n    323         )\r\n    325 \
          \    output_shape = inp.normalize_bmhk()\r\n    326     return _fMHA.apply(\r\
          \n    327         op, inp.query, inp.key, inp.value, inp.attn_bias, inp.p,\
          \ inp.scale\r\n    328     ).reshape(output_shape)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py:337,\
          \ in _memory_efficient_attention_forward(inp, op)\r\n    335 output_shape\
          \ = inp.normalize_bmhk()\r\n    336 if op is None:\r\n--> 337     op = _dispatch_fw(inp,\
          \ False)\r\n    338 else:\r\n    339     _ensure_op_supports_or_raise(ValueError,\
          \ \"memory_efficient_attention\", op, inp)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/dispatch.py:120,\
          \ in _dispatch_fw(inp, needs_gradient)\r\n    111 def _dispatch_fw(inp:\
          \ Inputs, needs_gradient: bool) -> Type[AttentionFwOpBase]:\r\n    112 \
          \    \"\"\"Computes the best operator for forward\r\n    113 \r\n    114\
          \     Raises:\r\n   (...)\r\n    118         AttentionOp: The best operator\
          \ for the configuration\r\n    119     \"\"\"\r\n--> 120     return _run_priority_list(\r\
          \n    121         \"memory_efficient_attention_forward\",\r\n    122   \
          \      _dispatch_fw_priority_list(inp, needs_gradient),\r\n    123     \
          \    inp,\r\n    124     )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/dispatch.py:63,\
          \ in _run_priority_list(name, priority_list, inp)\r\n     61 for op, not_supported\
          \ in zip(priority_list, not_supported_reasons):\r\n     62     msg += \"\
          \\n\" + _format_not_supported_reasons(op, not_supported)\r\n---> 63 raise\
          \ NotImplementedError(msg)\r\n\r\nNotImplementedError: No operator found\
          \ for `memory_efficient_attention_forward` with inputs:\r\n     query  \
          \     : shape=(1, 2, 1, 40) (torch.float32)\r\n     key         : shape=(1,\
          \ 2, 1, 40) (torch.float32)\r\n     value       : shape=(1, 2, 1, 40) (torch.float32)\r\
          \n     attn_bias   : <class 'NoneType'>\r\n     p           : 0.0\r\n`decoderF`\
          \ is not supported because:\r\n    xFormers wasn't build with CUDA support\r\
          \n    attn_bias type is <class 'NoneType'>\r\n    operator wasn't built\
          \ - see `python -m xformers.info` for more info\r\n`flshattF@0.0.0` is not\
          \ supported because:\r\n    xFormers wasn't build with CUDA support\r\n\
          \    dtype=torch.float32 (supported: {torch.bfloat16, torch.float16})\r\n\
          \    operator wasn't built - see `python -m xformers.info` for more info\r\
          \n`tritonflashattF` is not supported because:\r\n    xFormers wasn't build\
          \ with CUDA support\r\n    dtype=torch.float32 (supported: {torch.bfloat16,\
          \ torch.float16})\r\n    operator wasn't built - see `python -m xformers.info`\
          \ for more info\r\n    triton is not available\r\n    Only work on pre-MLIR\
          \ triton for now\r\n`cutlassF` is not supported because:\r\n    xFormers\
          \ wasn't build with CUDA support\r\n    operator wasn't built - see `python\
          \ -m xformers.info` for more info\r\n`smallkF` is not supported because:\r\
          \n    max(query.shape[-1] != value.shape[-1]) > 32\r\n    xFormers wasn't\
          \ build with CUDA support\r\n    operator wasn't built - see `python -m\
          \ xformers.info` for more info\r\n    unsupported embed per head: 40\r\n\
          ```\r\nAny help to resolve it would be appreciated.\r\nThanks,\r\nAndy "
        updatedAt: '2023-11-23T14:09:16.702Z'
      numEdits: 0
      reactions: []
    id: 655f5d0c0ec0e161854bc883
    type: comment
  author: Andyrasika
  content: "Thanks for sharing with amazing model , here is some issues i am getting\
    \ running the model:\r\n```\r\n---------------------------------------------------------------------------\r\
    \nNotImplementedError                       Traceback (most recent call last)\r\
    \nCell In[4], line 17\r\n     12 device = torch.device('cuda')\r\n     14 # For\
    \ Multi-modal Image Generation, must set `load_tokenizer=True` to load the tokenizer\
    \ to tokenize input image.\r\n     15 # If you have already install xformers,\
    \ set `use_xformers=True` to save the GPU memory (Xformers is not supported on\
    \ V100 GPU)\r\n     16 # If you have already download the checkpoint, set `local_files_only=True``\
    \ to avoid auto-downloading from remote\r\n---> 17 model = build_model(model_path=model_path,\
    \ model_dtype=model_dtype, check_safety=False,\r\n     18             device_id=device_id,\
    \ use_xformers=True, understanding=False, load_tokenizer=True)\r\n     19 model\
    \ = model.to(device)\r\n     20 print(\"Building Model Finsished\")\r\n\r\nFile\
    \ /workspace/LaVIT/models/__init__.py:47, in build_model(model_path, model_dtype,\
    \ device_id, image_size, use_xformers, understanding, load_tokenizer, pixel_decoding,\
    \ check_safety, local_files_only)\r\n     44     lavit = LaVITforUnderstanding(model_path=model_path,\
    \ model_dtype=model_dtype, \r\n     45             device_id=device_id, use_xformers=use_xformers)\r\
    \n     46 else:\r\n---> 47     lavit = LaVITforGeneration(model_path=model_path,\
    \ model_dtype=model_dtype, device_id=device_id, \r\n     48             use_xformers=use_xformers,\
    \ check_safety=check_safety, load_tokenizer=load_tokenizer, pixel_decoding=pixel_decoding)\r\
    \n     50 # Convert the model parameters to the defined precision\r\n     51 if\
    \ model_dtype == 'bf16':\r\n\r\nFile /workspace/LaVIT/models/lavit_for_generation.py:121,\
    \ in LaVITforGeneration.__init__(self, model_path, model_dtype, device_id, use_xformers,\
    \ check_safety, load_tokenizer, pixel_decoding, **kwargs)\r\n    117     if xformers_version\
    \ == version.parse(\"0.0.16\"):\r\n    118         print(\r\n    119         \
    \    \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe\
    \ problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers\
    \ for more details.\"\r\n    120         )\r\n--> 121     self.unet.enable_xformers_memory_efficient_attention()\r\
    \n    122 else:\r\n    123     raise ValueError(\"xformers is not available. Make\
    \ sure it is installed correctly or set `use_xformers=False`\")\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:287,\
    \ in ModelMixin.enable_xformers_memory_efficient_attention(self, attention_op)\r\
    \n    253 def enable_xformers_memory_efficient_attention(self, attention_op: Optional[Callable]\
    \ = None):\r\n    254     r\"\"\"\r\n    255     Enable memory efficient attention\
    \ from [xFormers](https://facebookresearch.github.io/xformers/).\r\n    256 \r\
    \n   (...)\r\n    285     ```\r\n    286     \"\"\"\r\n--> 287     self.set_use_memory_efficient_attention_xformers(True,\
    \ attention_op)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:251,\
    \ in ModelMixin.set_use_memory_efficient_attention_xformers(self, valid, attention_op)\r\
    \n    249 for module in self.children():\r\n    250     if isinstance(module,\
    \ torch.nn.Module):\r\n--> 251         fn_recursive_set_mem_eff(module)\r\n\r\n\
    File /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
    \ in ModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff(module)\r\
    \n    244     module.set_use_memory_efficient_attention_xformers(valid, attention_op)\r\
    \n    246 for child in module.children():\r\n--> 247     fn_recursive_set_mem_eff(child)\r\
    \n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
    \ in ModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff(module)\r\
    \n    244     module.set_use_memory_efficient_attention_xformers(valid, attention_op)\r\
    \n    246 for child in module.children():\r\n--> 247     fn_recursive_set_mem_eff(child)\r\
    \n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
    \ in ModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff(module)\r\
    \n    244     module.set_use_memory_efficient_attention_xformers(valid, attention_op)\r\
    \n    246 for child in module.children():\r\n--> 247     fn_recursive_set_mem_eff(child)\r\
    \n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:244,\
    \ in ModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff(module)\r\
    \n    242 def fn_recursive_set_mem_eff(module: torch.nn.Module):\r\n    243  \
    \   if hasattr(module, \"set_use_memory_efficient_attention_xformers\"):\r\n-->\
    \ 244         module.set_use_memory_efficient_attention_xformers(valid, attention_op)\r\
    \n    246     for child in module.children():\r\n    247         fn_recursive_set_mem_eff(child)\r\
    \n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:251,\
    \ in ModelMixin.set_use_memory_efficient_attention_xformers(self, valid, attention_op)\r\
    \n    249 for module in self.children():\r\n    250     if isinstance(module,\
    \ torch.nn.Module):\r\n--> 251         fn_recursive_set_mem_eff(module)\r\n\r\n\
    File /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
    \ in ModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff(module)\r\
    \n    244     module.set_use_memory_efficient_attention_xformers(valid, attention_op)\r\
    \n    246 for child in module.children():\r\n--> 247     fn_recursive_set_mem_eff(child)\r\
    \n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:247,\
    \ in ModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff(module)\r\
    \n    244     module.set_use_memory_efficient_attention_xformers(valid, attention_op)\r\
    \n    246 for child in module.children():\r\n--> 247     fn_recursive_set_mem_eff(child)\r\
    \n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py:244,\
    \ in ModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff(module)\r\
    \n    242 def fn_recursive_set_mem_eff(module: torch.nn.Module):\r\n    243  \
    \   if hasattr(module, \"set_use_memory_efficient_attention_xformers\"):\r\n-->\
    \ 244         module.set_use_memory_efficient_attention_xformers(valid, attention_op)\r\
    \n    246     for child in module.children():\r\n    247         fn_recursive_set_mem_eff(child)\r\
    \n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/attention_processor.py:216,\
    \ in Attention.set_use_memory_efficient_attention_xformers(self, use_memory_efficient_attention_xformers,\
    \ attention_op)\r\n    210         _ = xformers.ops.memory_efficient_attention(\r\
    \n    211             torch.randn((1, 2, 40), device=\"cuda\"),\r\n    212   \
    \          torch.randn((1, 2, 40), device=\"cuda\"),\r\n    213             torch.randn((1,\
    \ 2, 40), device=\"cuda\"),\r\n    214         )\r\n    215     except Exception\
    \ as e:\r\n--> 216         raise e\r\n    218 if is_lora:\r\n    219     # TODO\
    \ (sayakpaul): should we throw a warning if someone wants to use the xformers\r\
    \n    220     # variant when using PT 2.0 now that we have LoRAAttnProcessor2_0?\r\
    \n    221     processor = LoRAXFormersAttnProcessor(\r\n    222         hidden_size=self.processor.hidden_size,\r\
    \n    223         cross_attention_dim=self.processor.cross_attention_dim,\r\n\
    \    224         rank=self.processor.rank,\r\n    225         attention_op=attention_op,\r\
    \n    226     )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/diffusers/models/attention_processor.py:210,\
    \ in Attention.set_use_memory_efficient_attention_xformers(self, use_memory_efficient_attention_xformers,\
    \ attention_op)\r\n    207 else:\r\n    208     try:\r\n    209         # Make\
    \ sure we can run the memory efficient attention\r\n--> 210         _ = xformers.ops.memory_efficient_attention(\r\
    \n    211             torch.randn((1, 2, 40), device=\"cuda\"),\r\n    212   \
    \          torch.randn((1, 2, 40), device=\"cuda\"),\r\n    213             torch.randn((1,\
    \ 2, 40), device=\"cuda\"),\r\n    214         )\r\n    215     except Exception\
    \ as e:\r\n    216         raise e\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py:223,\
    \ in memory_efficient_attention(query, key, value, attn_bias, p, scale, op)\r\n\
    \    116 def memory_efficient_attention(\r\n    117     query: torch.Tensor,\r\
    \n    118     key: torch.Tensor,\r\n   (...)\r\n    124     op: Optional[AttentionOp]\
    \ = None,\r\n    125 ) -> torch.Tensor:\r\n    126     \"\"\"Implements the memory-efficient\
    \ attention mechanism following\r\n    127     `\"Self-Attention Does Not Need\
    \ O(n^2) Memory\" <http://arxiv.org/abs/2112.05682>`_.\r\n    128 \r\n   (...)\r\
    \n    221     :return: multi-head attention Tensor with shape ``[B, Mq, H, Kv]``\r\
    \n    222     \"\"\"\r\n--> 223     return _memory_efficient_attention(\r\n  \
    \  224         Inputs(\r\n    225             query=query, key=key, value=value,\
    \ p=p, attn_bias=attn_bias, scale=scale\r\n    226         ),\r\n    227     \
    \    op=op,\r\n    228     )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py:321,\
    \ in _memory_efficient_attention(inp, op)\r\n    316 def _memory_efficient_attention(\r\
    \n    317     inp: Inputs, op: Optional[AttentionOp] = None\r\n    318 ) -> torch.Tensor:\r\
    \n    319     # fast-path that doesn't require computing the logsumexp for backward\
    \ computation\r\n    320     if all(x.requires_grad is False for x in [inp.query,\
    \ inp.key, inp.value]):\r\n--> 321         return _memory_efficient_attention_forward(\r\
    \n    322             inp, op=op[0] if op is not None else None\r\n    323   \
    \      )\r\n    325     output_shape = inp.normalize_bmhk()\r\n    326     return\
    \ _fMHA.apply(\r\n    327         op, inp.query, inp.key, inp.value, inp.attn_bias,\
    \ inp.p, inp.scale\r\n    328     ).reshape(output_shape)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py:337,\
    \ in _memory_efficient_attention_forward(inp, op)\r\n    335 output_shape = inp.normalize_bmhk()\r\
    \n    336 if op is None:\r\n--> 337     op = _dispatch_fw(inp, False)\r\n    338\
    \ else:\r\n    339     _ensure_op_supports_or_raise(ValueError, \"memory_efficient_attention\"\
    , op, inp)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/dispatch.py:120,\
    \ in _dispatch_fw(inp, needs_gradient)\r\n    111 def _dispatch_fw(inp: Inputs,\
    \ needs_gradient: bool) -> Type[AttentionFwOpBase]:\r\n    112     \"\"\"Computes\
    \ the best operator for forward\r\n    113 \r\n    114     Raises:\r\n   (...)\r\
    \n    118         AttentionOp: The best operator for the configuration\r\n   \
    \ 119     \"\"\"\r\n--> 120     return _run_priority_list(\r\n    121        \
    \ \"memory_efficient_attention_forward\",\r\n    122         _dispatch_fw_priority_list(inp,\
    \ needs_gradient),\r\n    123         inp,\r\n    124     )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/dispatch.py:63,\
    \ in _run_priority_list(name, priority_list, inp)\r\n     61 for op, not_supported\
    \ in zip(priority_list, not_supported_reasons):\r\n     62     msg += \"\\n\"\
    \ + _format_not_supported_reasons(op, not_supported)\r\n---> 63 raise NotImplementedError(msg)\r\
    \n\r\nNotImplementedError: No operator found for `memory_efficient_attention_forward`\
    \ with inputs:\r\n     query       : shape=(1, 2, 1, 40) (torch.float32)\r\n \
    \    key         : shape=(1, 2, 1, 40) (torch.float32)\r\n     value       : shape=(1,\
    \ 2, 1, 40) (torch.float32)\r\n     attn_bias   : <class 'NoneType'>\r\n     p\
    \           : 0.0\r\n`decoderF` is not supported because:\r\n    xFormers wasn't\
    \ build with CUDA support\r\n    attn_bias type is <class 'NoneType'>\r\n    operator\
    \ wasn't built - see `python -m xformers.info` for more info\r\n`flshattF@0.0.0`\
    \ is not supported because:\r\n    xFormers wasn't build with CUDA support\r\n\
    \    dtype=torch.float32 (supported: {torch.bfloat16, torch.float16})\r\n    operator\
    \ wasn't built - see `python -m xformers.info` for more info\r\n`tritonflashattF`\
    \ is not supported because:\r\n    xFormers wasn't build with CUDA support\r\n\
    \    dtype=torch.float32 (supported: {torch.bfloat16, torch.float16})\r\n    operator\
    \ wasn't built - see `python -m xformers.info` for more info\r\n    triton is\
    \ not available\r\n    Only work on pre-MLIR triton for now\r\n`cutlassF` is not\
    \ supported because:\r\n    xFormers wasn't build with CUDA support\r\n    operator\
    \ wasn't built - see `python -m xformers.info` for more info\r\n`smallkF` is not\
    \ supported because:\r\n    max(query.shape[-1] != value.shape[-1]) > 32\r\n \
    \   xFormers wasn't build with CUDA support\r\n    operator wasn't built - see\
    \ `python -m xformers.info` for more info\r\n    unsupported embed per head: 40\r\
    \n```\r\nAny help to resolve it would be appreciated.\r\nThanks,\r\nAndy "
  created_at: 2023-11-23 14:09:16+00:00
  edited: false
  hidden: false
  id: 655f5d0c0ec0e161854bc883
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8339f3ab9bb7baaf69bf174eafb7282c.svg
      fullname: Yang Jin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: rain1011
      type: user
    createdAt: '2023-11-24T07:51:37.000Z'
    data:
      edited: false
      editors:
      - rain1011
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8841572403907776
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8339f3ab9bb7baaf69bf174eafb7282c.svg
          fullname: Yang Jin
          isHf: false
          isPro: false
          name: rain1011
          type: user
        html: '<p>You can set use_xformers=False, this error is raised because you
          do not install the xformers in your environment</p>

          '
        raw: You can set use_xformers=False, this error is raised because you do not
          install the xformers in your environment
        updatedAt: '2023-11-24T07:51:37.968Z'
      numEdits: 0
      reactions: []
    id: 65605609def5905d38c72afd
    type: comment
  author: rain1011
  content: You can set use_xformers=False, this error is raised because you do not
    install the xformers in your environment
  created_at: 2023-11-24 07:51:37+00:00
  edited: false
  hidden: false
  id: 65605609def5905d38c72afd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: rain1011/LaVIT-7B-v2
repo_type: model
status: open
target_branch: null
title: Re. No operator found for `memory_efficient_attention_forward` with inputs
