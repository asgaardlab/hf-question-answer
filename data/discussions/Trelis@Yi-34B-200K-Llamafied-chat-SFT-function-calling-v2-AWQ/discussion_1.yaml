!!python/object:huggingface_hub.community.DiscussionWithDetails
author: renjithwarrier
conflicting_files: null
created_at: 2023-12-12 03:33:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f13d6df121e0352faecdf55b8018f323.svg
      fullname: Renjith Warrier
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: renjithwarrier
      type: user
    createdAt: '2023-12-12T03:33:29.000Z'
    data:
      edited: false
      editors:
      - renjithwarrier
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5366392135620117
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f13d6df121e0352faecdf55b8018f323.svg
          fullname: Renjith Warrier
          isHf: false
          isPro: false
          name: renjithwarrier
          type: user
        html: '<p>I was running it in a Lambda Labs Ubuntu 22.04 machine using TGI
          and I got the following error:</p>

          <p>2023-12-12T03:24:19.350313Z ERROR text_generation_launcher: Method Warmup
          encountered an error.<br>Traceback (most recent call last):<br>  File "/opt/conda/bin/text-generation-server",
          line 8, in <br>    sys.exit(app())<br>  File "/opt/conda/lib/python3.10/site-packages/typer/main.py",
          line 311, in <strong>call</strong><br>    return get_command(self)(*args,
          **kwargs)<br>  File "/opt/conda/lib/python3.10/site-packages/click/core.py",
          line 1157, in <strong>call</strong><br>    return self.main(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.10/site-packages/typer/core.py", line 778, in main<br>    return
          _main(<br>  File "/opt/conda/lib/python3.10/site-packages/typer/core.py",
          line 216, in _main<br>    rv = self.invoke(ctx)<br>  File "/opt/conda/lib/python3.10/site-packages/click/core.py",
          line 1688, in invoke<br>    return _process_result(sub_ctx.command.invoke(sub_ctx))<br>  File
          "/opt/conda/lib/python3.10/site-packages/click/core.py", line 1434, in invoke<br>    return
          ctx.invoke(self.callback, **ctx.params)<br>  File "/opt/conda/lib/python3.10/site-packages/click/core.py",
          line 783, in invoke<br>    return __callback(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.10/site-packages/typer/main.py", line 683, in wrapper<br>    return
          callback(**use_params)  # type: ignore<br>  File "/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py",
          line 89, in serve<br>    server.serve(<br>  File "/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py",
          line 215, in serve<br>    asyncio.run(<br>  File "/opt/conda/lib/python3.10/asyncio/runners.py",
          line 44, in run<br>    return loop.run_until_complete(main)<br>  File "/opt/conda/lib/python3.10/asyncio/base_events.py",
          line 636, in run_until_complete<br>    self.run_forever()<br>  File "/opt/conda/lib/python3.10/asyncio/base_events.py",
          line 603, in run_forever<br>    self._run_once()<br>  File "/opt/conda/lib/python3.10/asyncio/base_events.py",
          line 1909, in _run_once<br>    handle._run()<br>  File "/opt/conda/lib/python3.10/asyncio/events.py",
          line 80, in _run<br>    self._context.run(self._callback, *self._args)<br>  File
          "/opt/conda/lib/python3.10/site-packages/grpc_interceptor/server.py", line
          165, in invoke_intercept_method<br>    return await self.intercept(</p>

          <blockquote>

          <p>File "/opt/conda/lib/python3.10/site-packages/text_generation_server/interceptor.py",
          line 21, in intercept<br>    return await response<br>  File "/opt/conda/lib/python3.10/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py",
          line 82, in _unary_interceptor<br>    raise error<br>  File "/opt/conda/lib/python3.10/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py",
          line 73, in _unary_interceptor<br>    return await behavior(request_or_iterator,
          context)<br>  File "/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py",
          line 72, in Warmup<br>    max_supported_total_tokens = self.model.warmup(batch)<br>  File
          "/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py",
          line 692, in warmup<br>    _, batch = self.generate_token(batch)<br>  File
          "/opt/conda/lib/python3.10/contextlib.py", line 79, in inner<br>    return
          func(*args, **kwds)<br>  File "/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py",
          line 823, in generate_token<br>    raise e<br>  File "/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py",
          line 820, in generate_token<br>    out = self.forward(batch)<br>  File "/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py",
          line 787, in forward<br>    return self.model.forward(<br>  File "/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py",
          line 431, in forward<br>    hidden_states = self.model(<br>  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line
          1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py",
          line 390, in forward<br>    hidden_states, residual = layer(<br>  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line
          1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py",
          line 317, in forward<br>    attn_output = self.self_attn(<br>  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line
          1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py",
          line 196, in forward<br>    qkv = self.query_key_value(hidden_states)<br>  File
          "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line
          1518, in _wrapped_call_impl<br>    return self._call_impl(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line
          1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/layers.py",
          line 371, in forward<br>    return self.linear.forward(x)<br>  File "/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/awq/quantize/qmodule.py",
          line 46, in forward<br>    out = awq_inference_engine.gemm_forward_cuda(<br>RuntimeError:
          CUDA error: an illegal memory access was encountered<br>CUDA kernel errors
          might be asynchronously reported at some other API call, so the stacktrace
          below might be incorrect.<br>For debugging consider passing CUDA_LAUNCH_BLOCKING=1.<br>Compile
          with <code>TORCH_USE_CUDA_DSA</code> to enable device-side assertions.</p>

          </blockquote>

          <p>2023-12-12T03:24:19.351798Z ERROR warmup{max_input_length=40000 max_prefill_tokens=40960
          max_total_tokens=40960}:warmup: text_generation_client: router/client/src/lib.rs:33:
          Server error: Unexpected &lt;class ''RuntimeError''&gt;: CUDA error: an
          illegal memory access was encountered<br>CUDA kernel errors might be asynchronously
          reported at some other API call, so the stacktrace below might be incorrect.<br>For
          debugging consider passing CUDA_LAUNCH_BLOCKING=1.<br>Compile with <code>TORCH_USE_CUDA_DSA</code>
          to enable device-side assertions.</p>

          <p>Machine config:<br>26 vCPUs, 200 GB RAM, 1 TB SSD, 1 X H100 80GB PCIe
          GPU</p>

          <p>The model runs fine on the same machine using ScaleLLM or cog. So it
          could be a TGI version issue? I would really appreciate your inputs on this.
          Happy to provide more info.</p>

          '
        raw: "I was running it in a Lambda Labs Ubuntu 22.04 machine using TGI and\
          \ I got the following error:\r\n\r\n2023-12-12T03:24:19.350313Z ERROR text_generation_launcher:\
          \ Method Warmup encountered an error.\r\nTraceback (most recent call last):\r\
          \n  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\
          \n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
          , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.10/site-packages/typer/core.py\", line 778, in main\r\
          \n    return _main(\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
          , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
          , line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.10/site-packages/typer/main.py\", line 683, in wrapper\r\
          \n    return callback(**use_params)  # type: ignore\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py\"\
          , line 89, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 215, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.10/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 636, in run_until_complete\r\
          \n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 603, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 1909, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\"\
          , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/grpc_interceptor/server.py\"\
          , line 165, in invoke_intercept_method\r\n    return await self.intercept(\r\
          \n> File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/interceptor.py\"\
          , line 21, in intercept\r\n    return await response\r\n  File \"/opt/conda/lib/python3.10/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
          , line 82, in _unary_interceptor\r\n    raise error\r\n  File \"/opt/conda/lib/python3.10/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
          , line 73, in _unary_interceptor\r\n    return await behavior(request_or_iterator,\
          \ context)\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 72, in Warmup\r\n    max_supported_total_tokens = self.model.warmup(batch)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 692, in warmup\r\n    _, batch = self.generate_token(batch)\r\n \
          \ File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\r\n\
          \    return func(*args, **kwds)\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 823, in generate_token\r\n    raise e\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 820, in generate_token\r\n    out = self.forward(batch)\r\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 787, in forward\r\n    return self.model.forward(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 431, in forward\r\n    hidden_states = self.model(\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 390, in forward\r\n    hidden_states, residual = layer(\r\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 317, in forward\r\n    attn_output = self.self_attn(\r\n  File \"\
          /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line\
          \ 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
          , line 196, in forward\r\n    qkv = self.query_key_value(hidden_states)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/layers.py\"\
          , line 371, in forward\r\n    return self.linear.forward(x)\r\n  File \"\
          /opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line\
          \ 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/awq/quantize/qmodule.py\"\
          , line 46, in forward\r\n    out = awq_inference_engine.gemm_forward_cuda(\r\
          \nRuntimeError: CUDA error: an illegal memory access was encountered\r\n\
          CUDA kernel errors might be asynchronously reported at some other API call,\
          \ so the stacktrace below might be incorrect.\r\nFor debugging consider\
          \ passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to\
          \ enable device-side assertions.\r\n\r\n\r\n2023-12-12T03:24:19.351798Z\
          \ ERROR warmup{max_input_length=40000 max_prefill_tokens=40960 max_total_tokens=40960}:warmup:\
          \ text_generation_client: router/client/src/lib.rs:33: Server error: Unexpected\
          \ <class 'RuntimeError'>: CUDA error: an illegal memory access was encountered\r\
          \nCUDA kernel errors might be asynchronously reported at some other API\
          \ call, so the stacktrace below might be incorrect.\r\nFor debugging consider\
          \ passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to\
          \ enable device-side assertions.\r\n\r\nMachine config:\r\n26 vCPUs, 200\
          \ GB RAM, 1 TB SSD, 1 X H100 80GB PCIe GPU\r\n\r\nThe model runs fine on\
          \ the same machine using ScaleLLM or cog. So it could be a TGI version issue?\
          \ I would really appreciate your inputs on this. Happy to provide more info."
        updatedAt: '2023-12-12T03:33:29.451Z'
      numEdits: 0
      reactions: []
    id: 6577d489197a6182c333c2ed
    type: comment
  author: renjithwarrier
  content: "I was running it in a Lambda Labs Ubuntu 22.04 machine using TGI and I\
    \ got the following error:\r\n\r\n2023-12-12T03:24:19.350313Z ERROR text_generation_launcher:\
    \ Method Warmup encountered an error.\r\nTraceback (most recent call last):\r\n\
    \  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\n   \
    \ sys.exit(app())\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
    , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\n \
    \ File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line 1157, in\
    \ __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
    , line 778, in main\r\n    return _main(\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
    , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
    , line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line 1434,\
    \ in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\
    /opt/conda/lib/python3.10/site-packages/click/core.py\", line 783, in invoke\r\
    \n    return __callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
    , line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n\
    \  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py\"\
    , line 89, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
    , line 215, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.10/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 636, in run_until_complete\r\n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 603, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 1909, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\"\
    , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n \
    \ File \"/opt/conda/lib/python3.10/site-packages/grpc_interceptor/server.py\"\
    , line 165, in invoke_intercept_method\r\n    return await self.intercept(\r\n\
    > File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/interceptor.py\"\
    , line 21, in intercept\r\n    return await response\r\n  File \"/opt/conda/lib/python3.10/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
    , line 82, in _unary_interceptor\r\n    raise error\r\n  File \"/opt/conda/lib/python3.10/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
    , line 73, in _unary_interceptor\r\n    return await behavior(request_or_iterator,\
    \ context)\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
    , line 72, in Warmup\r\n    max_supported_total_tokens = self.model.warmup(batch)\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py\"\
    , line 692, in warmup\r\n    _, batch = self.generate_token(batch)\r\n  File \"\
    /opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\r\n    return func(*args,\
    \ **kwds)\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py\"\
    , line 823, in generate_token\r\n    raise e\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py\"\
    , line 820, in generate_token\r\n    out = self.forward(batch)\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py\"\
    , line 787, in forward\r\n    return self.model.forward(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 431, in forward\r\n    hidden_states = self.model(\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 390, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 317, in forward\r\n    attn_output = self.self_attn(\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_llama_modeling.py\"\
    , line 196, in forward\r\n    qkv = self.query_key_value(hidden_states)\r\n  File\
    \ \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line\
    \ 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/layers.py\"\
    , line 371, in forward\r\n    return self.linear.forward(x)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/awq/quantize/qmodule.py\"\
    , line 46, in forward\r\n    out = awq_inference_engine.gemm_forward_cuda(\r\n\
    RuntimeError: CUDA error: an illegal memory access was encountered\r\nCUDA kernel\
    \ errors might be asynchronously reported at some other API call, so the stacktrace\
    \ below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\
    \nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\r\
    \n2023-12-12T03:24:19.351798Z ERROR warmup{max_input_length=40000 max_prefill_tokens=40960\
    \ max_total_tokens=40960}:warmup: text_generation_client: router/client/src/lib.rs:33:\
    \ Server error: Unexpected <class 'RuntimeError'>: CUDA error: an illegal memory\
    \ access was encountered\r\nCUDA kernel errors might be asynchronously reported\
    \ at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging\
    \ consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA`\
    \ to enable device-side assertions.\r\n\r\nMachine config:\r\n26 vCPUs, 200 GB\
    \ RAM, 1 TB SSD, 1 X H100 80GB PCIe GPU\r\n\r\nThe model runs fine on the same\
    \ machine using ScaleLLM or cog. So it could be a TGI version issue? I would really\
    \ appreciate your inputs on this. Happy to provide more info."
  created_at: 2023-12-12 03:33:29+00:00
  edited: false
  hidden: false
  id: 6577d489197a6182c333c2ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-12T10:59:34.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.869742751121521
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Howdy. Thanks, that''s good to know regarding ScaleLLM and cog.</p>

          <p>I''m able to run this on TGI with: <a rel="nofollow" href="https://runpod.io/gsc?template=dxxxl2aqgr&amp;ref=jmfkcdio">https://runpod.io/gsc?template=dxxxl2aqgr&amp;ref=jmfkcdio</a>
          - that''s a runpod template and it''s working fine on TGI 1.2</p>

          '
        raw: 'Howdy. Thanks, that''s good to know regarding ScaleLLM and cog.


          I''m able to run this on TGI with: https://runpod.io/gsc?template=dxxxl2aqgr&ref=jmfkcdio
          - that''s a runpod template and it''s working fine on TGI 1.2'
        updatedAt: '2023-12-12T10:59:34.049Z'
      numEdits: 0
      reactions: []
    id: 65783d169c5c26f96cfa371e
    type: comment
  author: RonanMcGovern
  content: 'Howdy. Thanks, that''s good to know regarding ScaleLLM and cog.


    I''m able to run this on TGI with: https://runpod.io/gsc?template=dxxxl2aqgr&ref=jmfkcdio
    - that''s a runpod template and it''s working fine on TGI 1.2'
  created_at: 2023-12-12 10:59:34+00:00
  edited: false
  hidden: false
  id: 65783d169c5c26f96cfa371e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-05T15:19:17.000Z'
    data:
      status: closed
    id: 65981df567b8fc62794e4f1c
    type: status-change
  author: RonanMcGovern
  created_at: 2024-01-05 15:19:17+00:00
  id: 65981df567b8fc62794e4f1c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2-AWQ
repo_type: model
status: closed
target_branch: null
title: 'Getting : CUDA error: an illegal memory access was encountered'
