!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-11-28 11:58:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-28T11:58:54.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9328014850616455
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>How do you manage merge two llama2 models and make then bigger?
          This is crazy</p>

          '
        raw: How do you manage merge two llama2 models and make then bigger? This
          is crazy
        updatedAt: '2023-11-28T11:58:54.346Z'
      numEdits: 0
      reactions: []
    id: 6565d5fed35fc55406f8ce62
    type: comment
  author: Yhyu13
  content: How do you manage merge two llama2 models and make then bigger? This is
    crazy
  created_at: 2023-11-28 11:58:54+00:00
  edited: false
  hidden: false
  id: 6565d5fed35fc55406f8ce62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64be6a5376a6e2efccc638c1/WeV-NfIS6SMPzTI--lNvd.jpeg?w=200&h=200&f=face
      fullname: Saofiq
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Sao10K
      type: user
    createdAt: '2023-11-28T13:35:49.000Z'
    data:
      edited: false
      editors:
      - Sao10K
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9311795234680176
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64be6a5376a6e2efccc638c1/WeV-NfIS6SMPzTI--lNvd.jpeg?w=200&h=200&f=face
          fullname: Saofiq
          isHf: false
          isPro: false
          name: Sao10K
          type: user
        html: '<p>By using <a rel="nofollow" href="https://github.com/cg123/mergekit">https://github.com/cg123/mergekit</a>
          with the passthrough method.</p>

          <p>For mistral models, its basically 32 hidden layers. Llama 2 13b has 40,
          and 70b has 80.</p>

          <p>Goliath for example, is a 120b frankenmerge made with 140 layers stacked
          on top of each other, so 80 original + 60 more, from two models.</p>

          <p>Lila is 80 + 40.</p>

          <p>Personally I don''t think it is worth the performance increase but people
          like bigger numbers.</p>

          '
        raw: 'By using https://github.com/cg123/mergekit with the passthrough method.


          For mistral models, its basically 32 hidden layers. Llama 2 13b has 40,
          and 70b has 80.


          Goliath for example, is a 120b frankenmerge made with 140 layers stacked
          on top of each other, so 80 original + 60 more, from two models.


          Lila is 80 + 40.


          Personally I don''t think it is worth the performance increase but people
          like bigger numbers.'
        updatedAt: '2023-11-28T13:35:49.170Z'
      numEdits: 0
      reactions: []
    id: 6565ecb542b1b11ecc7ac4cf
    type: comment
  author: Sao10K
  content: 'By using https://github.com/cg123/mergekit with the passthrough method.


    For mistral models, its basically 32 hidden layers. Llama 2 13b has 40, and 70b
    has 80.


    Goliath for example, is a 120b frankenmerge made with 140 layers stacked on top
    of each other, so 80 original + 60 more, from two models.


    Lila is 80 + 40.


    Personally I don''t think it is worth the performance increase but people like
    bigger numbers.'
  created_at: 2023-11-28 13:35:49+00:00
  edited: false
  hidden: false
  id: 6565ecb542b1b11ecc7ac4cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-28T15:34:36.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9287363290786743
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Does frankenmerge actually perform better? Such as emergence? Just
          speculating</p>

          '
        raw: Does frankenmerge actually perform better? Such as emergence? Just speculating
        updatedAt: '2023-11-28T15:34:36.228Z'
      numEdits: 0
      reactions: []
    id: 6566088c5ed6ca5f18c77ad4
    type: comment
  author: Yhyu13
  content: Does frankenmerge actually perform better? Such as emergence? Just speculating
  created_at: 2023-11-28 15:34:36+00:00
  edited: false
  hidden: false
  id: 6566088c5ed6ca5f18c77ad4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64be6a5376a6e2efccc638c1/WeV-NfIS6SMPzTI--lNvd.jpeg?w=200&h=200&f=face
      fullname: Saofiq
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Sao10K
      type: user
    createdAt: '2023-11-28T16:49:40.000Z'
    data:
      edited: false
      editors:
      - Sao10K
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9833037853240967
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64be6a5376a6e2efccc638c1/WeV-NfIS6SMPzTI--lNvd.jpeg?w=200&h=200&f=face
          fullname: Saofiq
          isHf: false
          isPro: false
          name: Sao10K
          type: user
        html: '<p>There are claims that is does, such as 20B models made from 13bs
          being better for roleplay, and Goliath being better than 70bs.</p>

          <p>Personally? It''s a minor improvement in terms of creativity, but thats
          it. It''s much more ''creative'' and ''sovl'' or whatever the term is for
          roleplay, that I can agree.</p>

          <p>Its kind of placebo for everything else, I haven''t noticed anything
          major, maybe minor improvements. Benchmarks suggest minor improvements too.</p>

          <p>A frankenmerge 100B would be much weaker than a true, pretrained 100B,
          in theory.</p>

          '
        raw: 'There are claims that is does, such as 20B models made from 13bs being
          better for roleplay, and Goliath being better than 70bs.


          Personally? It''s a minor improvement in terms of creativity, but thats
          it. It''s much more ''creative'' and ''sovl'' or whatever the term is for
          roleplay, that I can agree.


          Its kind of placebo for everything else, I haven''t noticed anything major,
          maybe minor improvements. Benchmarks suggest minor improvements too.


          A frankenmerge 100B would be much weaker than a true, pretrained 100B, in
          theory.'
        updatedAt: '2023-11-28T16:49:40.182Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PrimeD
    id: 65661a249450460026bb0865
    type: comment
  author: Sao10K
  content: 'There are claims that is does, such as 20B models made from 13bs being
    better for roleplay, and Goliath being better than 70bs.


    Personally? It''s a minor improvement in terms of creativity, but thats it. It''s
    much more ''creative'' and ''sovl'' or whatever the term is for roleplay, that
    I can agree.


    Its kind of placebo for everything else, I haven''t noticed anything major, maybe
    minor improvements. Benchmarks suggest minor improvements too.


    A frankenmerge 100B would be much weaker than a true, pretrained 100B, in theory.'
  created_at: 2023-11-28 16:49:40+00:00
  edited: false
  hidden: false
  id: 65661a249450460026bb0865
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8a081099e6d560e1b9016666568584e.svg
      fullname: Blair Sadewitz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tachyphylaxis
      type: user
    createdAt: '2023-11-30T16:53:32.000Z'
    data:
      edited: false
      editors:
      - tachyphylaxis
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9803147912025452
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8a081099e6d560e1b9016666568584e.svg
          fullname: Blair Sadewitz
          isHf: false
          isPro: false
          name: tachyphylaxis
          type: user
        html: '<p>I''ve tried out these bizarre 103B models you''ve cooked up, and
          they seem to occasionally have fantastic bursts of "creativity", specifically
          with regard to using relatively complex simile and metaphor.  As to whether
          it wasn''t just luck, well, obviously I have no idea.</p>

          '
        raw: I've tried out these bizarre 103B models you've cooked up, and they seem
          to occasionally have fantastic bursts of "creativity", specifically with
          regard to using relatively complex simile and metaphor.  As to whether it
          wasn't just luck, well, obviously I have no idea.
        updatedAt: '2023-11-30T16:53:32.911Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - Yhyu13
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PrimeD
    id: 6568be0c2f335c337072473b
    type: comment
  author: tachyphylaxis
  content: I've tried out these bizarre 103B models you've cooked up, and they seem
    to occasionally have fantastic bursts of "creativity", specifically with regard
    to using relatively complex simile and metaphor.  As to whether it wasn't just
    luck, well, obviously I have no idea.
  created_at: 2023-11-30 16:53:32+00:00
  edited: false
  hidden: false
  id: 6568be0c2f335c337072473b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Sao10K/Lila-103B-L2
repo_type: model
status: open
target_branch: null
title: What is this?
