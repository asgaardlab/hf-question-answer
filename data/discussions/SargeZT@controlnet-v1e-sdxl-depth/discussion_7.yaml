!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nmkd
conflicting_files: null
created_at: 2023-08-13 05:43:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/545094a9f23b840706c3b4f9be505c1b.svg
      fullname: JM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nmkd
      type: user
    createdAt: '2023-08-13T06:43:51.000Z'
    data:
      edited: false
      editors:
      - nmkd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9516507983207703
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/545094a9f23b840706c3b4f9be505c1b.svg
          fullname: JM
          isHf: false
          isPro: false
          name: nmkd
          type: user
        html: '<p>I''ve fiddled with ZoeDepth but can''t get it to give me the kind
          of depth map that this ControlNet expects.<br>The 1.5 model just uses grayscale
          maps.</p>

          '
        raw: "I've fiddled with ZoeDepth but can't get it to give me the kind of depth\
          \ map that this ControlNet expects.\r\nThe 1.5 model just uses grayscale\
          \ maps."
        updatedAt: '2023-08-13T06:43:51.676Z'
      numEdits: 0
      reactions: []
    id: 64d87ba78ebc4044384c494f
    type: comment
  author: nmkd
  content: "I've fiddled with ZoeDepth but can't get it to give me the kind of depth\
    \ map that this ControlNet expects.\r\nThe 1.5 model just uses grayscale maps."
  created_at: 2023-08-13 05:43:51+00:00
  edited: false
  hidden: false
  id: 64d87ba78ebc4044384c494f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d0b407e56ec2c8bdd945afcb349a61fe.svg
      fullname: David Burnett
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vargol
      type: user
    createdAt: '2023-08-13T10:54:42.000Z'
    data:
      edited: true
      editors:
      - Vargol
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9307019114494324
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d0b407e56ec2c8bdd945afcb349a61fe.svg
          fullname: David Burnett
          isHf: false
          isPro: false
          name: Vargol
          type: user
        html: '<p>from the examples it looks like a RGB image with the value from
          the channels in GRB  order being treating as a single 24 bit value, black
          being closest white being farthest away, value wise thats the inverse of
          the ''inferno'' depth maps that MiDaS outputs. Can''t think of a clever
          way to invert the values at the moment other than doing all the maths.</p>

          <p>Have you tried the example images, do they work ?</p>

          '
        raw: 'from the examples it looks like a RGB image with the value from the
          channels in GRB  order being treating as a single 24 bit value, black being
          closest white being farthest away, value wise thats the inverse of the ''inferno''
          depth maps that MiDaS outputs. Can''t think of a clever way to invert the
          values at the moment other than doing all the maths.


          Have you tried the example images, do they work ?


          '
        updatedAt: '2023-08-13T10:56:43.340Z'
      numEdits: 1
      reactions: []
    id: 64d8b6720f992cf1ce8836c2
    type: comment
  author: Vargol
  content: 'from the examples it looks like a RGB image with the value from the channels
    in GRB  order being treating as a single 24 bit value, black being closest white
    being farthest away, value wise thats the inverse of the ''inferno'' depth maps
    that MiDaS outputs. Can''t think of a clever way to invert the values at the moment
    other than doing all the maths.


    Have you tried the example images, do they work ?


    '
  created_at: 2023-08-13 09:54:42+00:00
  edited: true
  hidden: false
  id: 64d8b6720f992cf1ce8836c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/545094a9f23b840706c3b4f9be505c1b.svg
      fullname: JM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nmkd
      type: user
    createdAt: '2023-08-13T13:15:37.000Z'
    data:
      edited: false
      editors:
      - nmkd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9370198845863342
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/545094a9f23b840706c3b4f9be505c1b.svg
          fullname: JM
          isHf: false
          isPro: false
          name: nmkd
          type: user
        html: '<blockquote>

          <p>from the examples it looks like a RGB image with the value from the channels
          in GRB  order being treating as a single 24 bit value, black being closest
          white being farthest away, value wise thats the inverse of the ''inferno''
          depth maps that MiDaS outputs. Can''t think of a clever way to invert the
          values at the moment other than doing all the maths.</p>

          <p>Have you tried the example images, do they work ?</p>

          </blockquote>

          <p>Yes, the examples work, but I can''t get my own inputs working.</p>

          '
        raw: "> from the examples it looks like a RGB image with the value from the\
          \ channels in GRB  order being treating as a single 24 bit value, black\
          \ being closest white being farthest away, value wise thats the inverse\
          \ of the 'inferno' depth maps that MiDaS outputs. Can't think of a clever\
          \ way to invert the values at the moment other than doing all the maths.\n\
          > \n> Have you tried the example images, do they work ?\n\nYes, the examples\
          \ work, but I can't get my own inputs working."
        updatedAt: '2023-08-13T13:15:37.318Z'
      numEdits: 0
      reactions: []
    id: 64d8d7792fe2c112645a65c4
    type: comment
  author: nmkd
  content: "> from the examples it looks like a RGB image with the value from the\
    \ channels in GRB  order being treating as a single 24 bit value, black being\
    \ closest white being farthest away, value wise thats the inverse of the 'inferno'\
    \ depth maps that MiDaS outputs. Can't think of a clever way to invert the values\
    \ at the moment other than doing all the maths.\n> \n> Have you tried the example\
    \ images, do they work ?\n\nYes, the examples work, but I can't get my own inputs\
    \ working."
  created_at: 2023-08-13 12:15:37+00:00
  edited: false
  hidden: false
  id: 64d8d7792fe2c112645a65c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/545094a9f23b840706c3b4f9be505c1b.svg
      fullname: JM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nmkd
      type: user
    createdAt: '2023-08-13T13:36:31.000Z'
    data:
      edited: false
      editors:
      - nmkd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6561352610588074
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/545094a9f23b840706c3b4f9be505c1b.svg
          fullname: JM
          isHf: false
          isPro: false
          name: nmkd
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Vargol&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Vargol\">@<span class=\"\
          underline\">Vargol</span></a></span>\n\n\t</span></span> thanks, I managed\
          \ to make it work by adding the following lines to <code>ComfyUI\\custom_nodes\\\
          comfy_controlnet_preprocessors\\v11\\zoe\\__init__.py</code>:</p>\n<pre><code\
          \ class=\"language-py\">            <span class=\"hljs-keyword\">import</span>\
          \ matplotlib.pyplot <span class=\"hljs-keyword\">as</span> plt\n       \
          \     colored_depth = plt.cm.inferno(depth) <span class=\"hljs-comment\"\
          ># Get \"inferno\" map</span>\n            depth_image = (colored_depth[:,\
          \ :, :<span class=\"hljs-number\">3</span>] * <span class=\"hljs-number\"\
          >255</span>).astype(np.uint8)\n            depth_image = <span class=\"\
          hljs-number\">255</span> - depth_image <span class=\"hljs-comment\"># Invert\
          \ \"inferno\" palette</span>\n</code></pre>\n<p>right before <code>return\
          \ depth_image</code>. The preprocessor now returns an inverted inferno map,\
          \ which is what this model appears to be trained on.</p>\n<p>It works:</p>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6303f3657b50dd9d0a370126/LO9NZUBvtvFjaaFy3oo8A.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6303f3657b50dd9d0a370126/LO9NZUBvtvFjaaFy3oo8A.png\"\
          ></a></p>\n"
        raw: "@Vargol thanks, I managed to make it work by adding the following lines\
          \ to `ComfyUI\\custom_nodes\\comfy_controlnet_preprocessors\\v11\\zoe\\\
          __init__.py`:\n\n```py\n            import matplotlib.pyplot as plt\n  \
          \          colored_depth = plt.cm.inferno(depth) # Get \"inferno\" map\n\
          \            depth_image = (colored_depth[:, :, :3] * 255).astype(np.uint8)\n\
          \            depth_image = 255 - depth_image # Invert \"inferno\" palette\n\
          ```\nright before `return depth_image`. The preprocessor now returns an\
          \ inverted inferno map, which is what this model appears to be trained on.\n\
          \nIt works:\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3657b50dd9d0a370126/LO9NZUBvtvFjaaFy3oo8A.png)\n"
        updatedAt: '2023-08-13T13:36:31.088Z'
      numEdits: 0
      reactions: []
    id: 64d8dc5f3ca2924d6eb268ab
    type: comment
  author: nmkd
  content: "@Vargol thanks, I managed to make it work by adding the following lines\
    \ to `ComfyUI\\custom_nodes\\comfy_controlnet_preprocessors\\v11\\zoe\\__init__.py`:\n\
    \n```py\n            import matplotlib.pyplot as plt\n            colored_depth\
    \ = plt.cm.inferno(depth) # Get \"inferno\" map\n            depth_image = (colored_depth[:,\
    \ :, :3] * 255).astype(np.uint8)\n            depth_image = 255 - depth_image\
    \ # Invert \"inferno\" palette\n```\nright before `return depth_image`. The preprocessor\
    \ now returns an inverted inferno map, which is what this model appears to be\
    \ trained on.\n\nIt works:\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3657b50dd9d0a370126/LO9NZUBvtvFjaaFy3oo8A.png)\n"
  created_at: 2023-08-13 12:36:31+00:00
  edited: false
  hidden: false
  id: 64d8dc5f3ca2924d6eb268ab
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: SargeZT/controlnet-v1e-sdxl-depth
repo_type: model
status: open
target_branch: null
title: What kind of preprocessing is required for this? Grayscale depth maps do not
  appear to work.
