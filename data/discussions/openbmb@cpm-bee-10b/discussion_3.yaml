!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zcpp
conflicting_files: null
created_at: 2023-06-26 06:04:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bc026cb9b4b07b559b2e4a4cfc676379.svg
      fullname: zcpp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zcpp
      type: user
    createdAt: '2023-06-26T07:04:47.000Z'
    data:
      edited: true
      editors:
      - zcpp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.13539472222328186
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bc026cb9b4b07b559b2e4a4cfc676379.svg
          fullname: zcpp
          isHf: false
          isPro: false
          name: zcpp
          type: user
        html: "<h1 id=\"error--type-1---using-code-in-your-github-forked-version-to-install-transformers\"\
          >Error  type 1 - Using <a rel=\"nofollow\" href=\"https://github.com/gongbaitao/transformers/blob/main/src/transformers/models/cpmbee/modeling_cpmbee.py\"\
          >code in your github forked</a> version to install transformers</h1>\n<p>/CPMBee-fork-transformer/transformers/src/transformers/models/cpmbee/modeling_cpmbee.py:572\
          \ in forward<br>\u2502                                                 \
          \                                                 \u2502<br>\u2502    569\
          \ \u2502   \u2502   self.inv_freq = inv_freq.to(config.torch_dtype)    \
          \                               \u2502<br>\u2502    570 \u2502         \
          \                                                                      \
          \          \u2502<br>\u2502    571 \u2502   def forward(self, x: torch.Tensor,\
          \ x_pos: torch.Tensor):                              \u2502<br>\u2502 \u2771\
          \  572 \u2502   \u2502   inv_freq = self.inv_freq.to(device=x.device, dtype=self.dtype)\
          \                    \u2502<br>\u2502    573 \u2502   \u2502           \
          \                                                                      \
          \    \u2502<br>\u2502    574 \u2502   \u2502   x_pos = x_pos * self.distance_scale\
          \                                               \u2502<br>\u2502    575\
          \ \u2502   \u2502   freqs = x_pos[..., None].to(self.dtype) * inv_freq[None,\
          \ :]  # (..., dim/2)       \u2502<br>\u2570\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F<br>RuntimeError:\
          \ CUDA error: device-side assert triggered</p>\n<h1 id=\"error-type-2---using-the-code-in-this-huggingface-repo-with-setting-trust_remote_codetrue\"\
          >Error type 2 - Using <a href=\"https://huggingface.co/openbmb/cpm-bee-10b/blob/main/modeling_cpmbee.py\"\
          >the code</a> in this huggingface repo with setting trust_remote_code=True</h1>\n\
          <p>modeling_cpmbee.py:787 in forward<br>\u2502                         \
          \                                                                      \
          \   \u2502<br>\u2502    784 \u2502   \u2502   \u2502   \u2502   + segment_rel_offset[:,\
          \ :, None],                                         \u2502<br>\u2502   \
          \ 785 \u2502   \u2502   \u2502   \u2502   ~(                           \
          \                                             \u2502<br>\u2502    786 \u2502\
          \   \u2502   \u2502   \u2502   \u2502   (sample_ids[:, :, None] == sample_ids[:,\
          \ None, :])                    \u2502<br>\u2502 \u2771  787 \u2502   \u2502\
          \   \u2502   \u2502   \u2502   &amp; (span[:, None, :] == span[:, :, None])\
          \                              \u2502<br>\u2502    788 \u2502   \u2502 \
          \  \u2502   \u2502   ),  # not in the same span or sample              \
          \                        \u2502<br>\u2502    789 \u2502   \u2502   \u2502\
          \   \u2502   0,  # avoid torch.gather overflow                         \
          \                \u2502<br>\u2502    790 \u2502   \u2502   \u2502   ).view(batch,\
          \ seqlen * seqlen)                                                \u2502\
          <br>\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u256F<br>TypeError: 'NoneType' object is not subscriptable</p>\n"
        raw: "# Error  type 1 - Using [code in your github forked](https://github.com/gongbaitao/transformers/blob/main/src/transformers/models/cpmbee/modeling_cpmbee.py)\
          \ version to install transformers\n\n/CPMBee-fork-transformer/transformers/src/transformers/models/cpmbee/modeling_cpmbee.py:572\
          \ in forward\n\u2502                                                   \
          \                                               \u2502\n\u2502    569 \u2502\
          \   \u2502   self.inv_freq = inv_freq.to(config.torch_dtype)           \
          \                        \u2502\n\u2502    570 \u2502                  \
          \                                                                      \
          \ \u2502\n\u2502    571 \u2502   def forward(self, x: torch.Tensor, x_pos:\
          \ torch.Tensor):                              \u2502\n\u2502 \u2771  572\
          \ \u2502   \u2502   inv_freq = self.inv_freq.to(device=x.device, dtype=self.dtype)\
          \                    \u2502\n\u2502    573 \u2502   \u2502             \
          \                                                                      \
          \  \u2502\n\u2502    574 \u2502   \u2502   x_pos = x_pos * self.distance_scale\
          \                                               \u2502\n\u2502    575 \u2502\
          \   \u2502   freqs = x_pos[..., None].to(self.dtype) * inv_freq[None, :]\
          \  # (..., dim/2)       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nRuntimeError: CUDA\
          \ error: device-side assert triggered\n\n# Error type 2 - Using [the code](https://huggingface.co/openbmb/cpm-bee-10b/blob/main/modeling_cpmbee.py)\
          \ in this huggingface repo with setting trust_remote_code=True\n\nmodeling_cpmbee.py:787\
          \ in forward\n\u2502                                                   \
          \                                               \u2502\n\u2502    784 \u2502\
          \   \u2502   \u2502   \u2502   + segment_rel_offset[:, :, None],       \
          \                                  \u2502\n\u2502    785 \u2502   \u2502\
          \   \u2502   \u2502   ~(                                               \
          \                         \u2502\n\u2502    786 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   (sample_ids[:, :, None] == sample_ids[:, None, :])\
          \                    \u2502\n\u2502 \u2771  787 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   & (span[:, None, :] == span[:, :, None])         \
          \                     \u2502\n\u2502    788 \u2502   \u2502   \u2502   \u2502\
          \   ),  # not in the same span or sample                               \
          \       \u2502\n\u2502    789 \u2502   \u2502   \u2502   \u2502   0,  #\
          \ avoid torch.gather overflow                                         \u2502\
          \n\u2502    790 \u2502   \u2502   \u2502   ).view(batch, seqlen * seqlen)\
          \                                                \u2502\n\u2570\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u256F\nTypeError: 'NoneType' object is not subscriptable"
        updatedAt: '2023-06-26T07:54:20.225Z'
      numEdits: 1
      reactions: []
    id: 6499388f42d585137ebc5d03
    type: comment
  author: zcpp
  content: "# Error  type 1 - Using [code in your github forked](https://github.com/gongbaitao/transformers/blob/main/src/transformers/models/cpmbee/modeling_cpmbee.py)\
    \ version to install transformers\n\n/CPMBee-fork-transformer/transformers/src/transformers/models/cpmbee/modeling_cpmbee.py:572\
    \ in forward\n\u2502                                                         \
    \                                         \u2502\n\u2502    569 \u2502   \u2502\
    \   self.inv_freq = inv_freq.to(config.torch_dtype)                          \
    \         \u2502\n\u2502    570 \u2502                                       \
    \                                                  \u2502\n\u2502    571 \u2502\
    \   def forward(self, x: torch.Tensor, x_pos: torch.Tensor):                 \
    \             \u2502\n\u2502 \u2771  572 \u2502   \u2502   inv_freq = self.inv_freq.to(device=x.device,\
    \ dtype=self.dtype)                    \u2502\n\u2502    573 \u2502   \u2502 \
    \                                                                            \
    \        \u2502\n\u2502    574 \u2502   \u2502   x_pos = x_pos * self.distance_scale\
    \                                               \u2502\n\u2502    575 \u2502 \
    \  \u2502   freqs = x_pos[..., None].to(self.dtype) * inv_freq[None, :]  # (...,\
    \ dim/2)       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nRuntimeError:\
    \ CUDA error: device-side assert triggered\n\n# Error type 2 - Using [the code](https://huggingface.co/openbmb/cpm-bee-10b/blob/main/modeling_cpmbee.py)\
    \ in this huggingface repo with setting trust_remote_code=True\n\nmodeling_cpmbee.py:787\
    \ in forward\n\u2502                                                         \
    \                                         \u2502\n\u2502    784 \u2502   \u2502\
    \   \u2502   \u2502   + segment_rel_offset[:, :, None],                      \
    \                   \u2502\n\u2502    785 \u2502   \u2502   \u2502   \u2502  \
    \ ~(                                                                        \u2502\
    \n\u2502    786 \u2502   \u2502   \u2502   \u2502   \u2502   (sample_ids[:, :,\
    \ None] == sample_ids[:, None, :])                    \u2502\n\u2502 \u2771  787\
    \ \u2502   \u2502   \u2502   \u2502   \u2502   & (span[:, None, :] == span[:,\
    \ :, None])                              \u2502\n\u2502    788 \u2502   \u2502\
    \   \u2502   \u2502   ),  # not in the same span or sample                   \
    \                   \u2502\n\u2502    789 \u2502   \u2502   \u2502   \u2502  \
    \ 0,  # avoid torch.gather overflow                                         \u2502\
    \n\u2502    790 \u2502   \u2502   \u2502   ).view(batch, seqlen * seqlen)    \
    \                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u256F\nTypeError: 'NoneType' object is not subscriptable"
  created_at: 2023-06-26 06:04:47+00:00
  edited: true
  hidden: false
  id: 6499388f42d585137ebc5d03
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b5a4d84e75a9f3c52162ff277bf2a4bd.svg
      fullname: Jeffrey Gong
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jeffreygo
      type: user
    createdAt: '2023-06-27T16:00:59.000Z'
    data:
      edited: false
      editors:
      - jeffreygo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7055979371070862
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b5a4d84e75a9f3c52162ff277bf2a4bd.svg
          fullname: Jeffrey Gong
          isHf: false
          isPro: false
          name: jeffreygo
          type: user
        html: '<p>You should not use the forked github code, please follow the case
          in model card.<br>As type 2, please use <code>model.generate()</code>. When
          you use <code>model.forward()</code>, you should process the data by <code>tokenizer.prepare_for_finetune()</code>.</p>

          '
        raw: 'You should not use the forked github code, please follow the case in
          model card.

          As type 2, please use `model.generate()`. When you use `model.forward()`,
          you should process the data by `tokenizer.prepare_for_finetune()`.'
        updatedAt: '2023-06-27T16:00:59.103Z'
      numEdits: 0
      reactions: []
    id: 649b07bb00f8bbeefb87f3a3
    type: comment
  author: jeffreygo
  content: 'You should not use the forked github code, please follow the case in model
    card.

    As type 2, please use `model.generate()`. When you use `model.forward()`, you
    should process the data by `tokenizer.prepare_for_finetune()`.'
  created_at: 2023-06-27 15:00:59+00:00
  edited: false
  hidden: false
  id: 649b07bb00f8bbeefb87f3a3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: openbmb/cpm-bee-10b
repo_type: model
status: open
target_branch: null
title: Error while running it with huggingface transformers
