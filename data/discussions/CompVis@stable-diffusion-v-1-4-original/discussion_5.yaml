!!python/object:huggingface_hub.community.DiscussionWithDetails
author: huggingmaw
conflicting_files: null
created_at: 2022-08-24 13:36:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2042dae19bb5db403afb073e3cd07a8.svg
      fullname: Hugging Maw
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: huggingmaw
      type: user
    createdAt: '2022-08-24T14:36:56.000Z'
    data:
      edited: false
      editors:
      - huggingmaw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2042dae19bb5db403afb073e3cd07a8.svg
          fullname: Hugging Maw
          isHf: false
          isPro: false
          name: huggingmaw
          type: user
        html: '<p>hi, I noticed that the diffuser version of the model has <code>fp16</code>
          branch available for those who has lower VRAMs. is there a plan to release
          the CompVis version as well?<br>I haven''t read the code yet, but I have
          an impression that it''s a bit diffcult to adapt diffuser version to img2img
          purpose.</p>

          <p>p.s. thank you for releasing the model, we''ve been using it to spice
          up the table-top rpg game of ours and we''re loving it so far.</p>

          '
        raw: "hi, I noticed that the diffuser version of the model has `fp16` branch\
          \ available for those who has lower VRAMs. is there a plan to release the\
          \ CompVis version as well?\r\nI haven't read the code yet, but I have an\
          \ impression that it's a bit diffcult to adapt diffuser version to img2img\
          \ purpose.\r\n\r\np.s. thank you for releasing the model, we've been using\
          \ it to spice up the table-top rpg game of ours and we're loving it so far."
        updatedAt: '2022-08-24T14:36:56.705Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - NnmBzr
        - multimodalart
    id: 6306378814e7d8892aaa7189
    type: comment
  author: huggingmaw
  content: "hi, I noticed that the diffuser version of the model has `fp16` branch\
    \ available for those who has lower VRAMs. is there a plan to release the CompVis\
    \ version as well?\r\nI haven't read the code yet, but I have an impression that\
    \ it's a bit diffcult to adapt diffuser version to img2img purpose.\r\n\r\np.s.\
    \ thank you for releasing the model, we've been using it to spice up the table-top\
    \ rpg game of ours and we're loving it so far."
  created_at: 2022-08-24 13:36:56+00:00
  edited: false
  hidden: false
  id: 6306378814e7d8892aaa7189
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1649143001781-624bebf604abc7ebb01789af.jpeg?w=200&h=200&f=face
      fullname: "Apolin\xE1rio from multimodal AI art"
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multimodalart
      type: user
    createdAt: '2022-08-26T13:23:44.000Z'
    data:
      edited: true
      editors:
      - multimodalart
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1649143001781-624bebf604abc7ebb01789af.jpeg?w=200&h=200&f=face
          fullname: "Apolin\xE1rio from multimodal AI art"
          isHf: true
          isPro: false
          name: multimodalart
          type: user
        html: '<p>Hi, indeed we don''t have pre-converted <code>fp16</code> weights
          for the CompVis weights yet - I think this is something we''d love community
          contribution on. </p>

          <blockquote>

          <p>I haven''t read the code yet, but I have an impression that it''s a bit
          diffcult to adapt diffuser version to img2img purpose.</p>

          </blockquote>

          <p><code>diffusers</code> fully supports img2image, check out: <a rel="nofollow"
          href="https://github.com/huggingface/diffusers/tree/main/examples/inference">https://github.com/huggingface/diffusers/tree/main/examples/inference</a>
          - there''s also a Colab made for it: <a rel="nofollow" href="https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/image_2_image_using_diffusers.ipynb">https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/image_2_image_using_diffusers.ipynb</a></p>

          '
        raw: "Hi, indeed we don't have pre-converted `fp16` weights for the CompVis\
          \ weights yet - I think this is something we'd love community contribution\
          \ on. \n\n> I haven't read the code yet, but I have an impression that it's\
          \ a bit diffcult to adapt diffuser version to img2img purpose.\n\n`diffusers`\
          \ fully supports img2image, check out: https://github.com/huggingface/diffusers/tree/main/examples/inference\
          \ - there's also a Colab made for it: https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/image_2_image_using_diffusers.ipynb"
        updatedAt: '2022-08-26T13:24:04.578Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - patrickvonplaten
        - huggingmaw
    id: 6308c96037556c4ab030f4c2
    type: comment
  author: multimodalart
  content: "Hi, indeed we don't have pre-converted `fp16` weights for the CompVis\
    \ weights yet - I think this is something we'd love community contribution on.\
    \ \n\n> I haven't read the code yet, but I have an impression that it's a bit\
    \ diffcult to adapt diffuser version to img2img purpose.\n\n`diffusers` fully\
    \ supports img2image, check out: https://github.com/huggingface/diffusers/tree/main/examples/inference\
    \ - there's also a Colab made for it: https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/image_2_image_using_diffusers.ipynb"
  created_at: 2022-08-26 12:23:44+00:00
  edited: true
  hidden: false
  id: 6308c96037556c4ab030f4c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2042dae19bb5db403afb073e3cd07a8.svg
      fullname: Hugging Maw
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: huggingmaw
      type: user
    createdAt: '2022-08-27T00:58:17.000Z'
    data:
      edited: false
      editors:
      - huggingmaw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2042dae19bb5db403afb073e3cd07a8.svg
          fullname: Hugging Maw
          isHf: false
          isPro: false
          name: huggingmaw
          type: user
        html: '<p>hi, thank you for your answer! seems like I''ve completely missed
          the diffuser version of img2img. I''ll check it out.</p>

          '
        raw: hi, thank you for your answer! seems like I've completely missed the
          diffuser version of img2img. I'll check it out.
        updatedAt: '2022-08-27T00:58:17.978Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - patrickvonplaten
      relatedEventId: 63096c296e9cc2bcbd658b6a
    id: 63096c296e9cc2bcbd658b69
    type: comment
  author: huggingmaw
  content: hi, thank you for your answer! seems like I've completely missed the diffuser
    version of img2img. I'll check it out.
  created_at: 2022-08-26 23:58:17+00:00
  edited: false
  hidden: false
  id: 63096c296e9cc2bcbd658b69
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f2042dae19bb5db403afb073e3cd07a8.svg
      fullname: Hugging Maw
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: huggingmaw
      type: user
    createdAt: '2022-08-27T00:58:17.000Z'
    data:
      status: closed
    id: 63096c296e9cc2bcbd658b6a
    type: status-change
  author: huggingmaw
  created_at: 2022-08-26 23:58:17+00:00
  id: 63096c296e9cc2bcbd658b6a
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e95fac3898103393781475c4b4648adc.svg
      fullname: Sumit Chaturvedi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: suchatur
      type: user
    createdAt: '2022-08-29T17:09:32.000Z'
    data:
      edited: false
      editors:
      - suchatur
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e95fac3898103393781475c4b4648adc.svg
          fullname: Sumit Chaturvedi
          isHf: false
          isPro: false
          name: suchatur
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;multimodalart&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/multimodalart\"\
          >@<span class=\"underline\">multimodalart</span></a></span>\n\n\t</span></span>,\
          \ did you train a diffusion model from scratch for fp16 or can we simply\
          \ set a fp32 model to fp16 and expect it to work? Thanks a lot for open-sourcing\
          \ Stable Diffusion!!</p>\n"
        raw: Hi @multimodalart, did you train a diffusion model from scratch for fp16
          or can we simply set a fp32 model to fp16 and expect it to work? Thanks
          a lot for open-sourcing Stable Diffusion!!
        updatedAt: '2022-08-29T17:09:32.588Z'
      numEdits: 0
      reactions: []
    id: 630cf2cc467bc15dec84fe5c
    type: comment
  author: suchatur
  content: Hi @multimodalart, did you train a diffusion model from scratch for fp16
    or can we simply set a fp32 model to fp16 and expect it to work? Thanks a lot
    for open-sourcing Stable Diffusion!!
  created_at: 2022-08-29 16:09:32+00:00
  edited: false
  hidden: false
  id: 630cf2cc467bc15dec84fe5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e95fac3898103393781475c4b4648adc.svg
      fullname: Sumit Chaturvedi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: suchatur
      type: user
    createdAt: '2022-08-29T17:22:08.000Z'
    data:
      edited: false
      editors:
      - suchatur
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e95fac3898103393781475c4b4648adc.svg
          fullname: Sumit Chaturvedi
          isHf: false
          isPro: false
          name: suchatur
          type: user
        html: '<p>I tried it out. It works!!</p>

          '
        raw: I tried it out. It works!!
        updatedAt: '2022-08-29T17:22:08.884Z'
      numEdits: 0
      reactions: []
    id: 630cf5c054c3dbd48054dbde
    type: comment
  author: suchatur
  content: I tried it out. It works!!
  created_at: 2022-08-29 16:22:08+00:00
  edited: false
  hidden: false
  id: 630cf5c054c3dbd48054dbde
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59dd66821c9ea7ae1127670c73007a82.svg
      fullname: Niklas Arndt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: niklas2810
      type: user
    createdAt: '2022-09-06T18:16:33.000Z'
    data:
      edited: false
      editors:
      - niklas2810
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59dd66821c9ea7ae1127670c73007a82.svg
          fullname: Niklas Arndt
          isHf: false
          isPro: false
          name: niklas2810
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;suchatur&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/suchatur\">@<span class=\"\
          underline\">suchatur</span></a></span>\n\n\t</span></span> I'm not an expert\
          \ when it comes to Python, how exactly can one \"set\" a model to fp16?\
          \ Would be awesome if you could reference this for others here :)</p>\n"
        raw: '@suchatur I''m not an expert when it comes to Python, how exactly can
          one "set" a model to fp16? Would be awesome if you could reference this
          for others here :)'
        updatedAt: '2022-09-06T18:16:33.800Z'
      numEdits: 0
      reactions: []
    id: 63178e8149af4d408c2e6c13
    type: comment
  author: niklas2810
  content: '@suchatur I''m not an expert when it comes to Python, how exactly can
    one "set" a model to fp16? Would be awesome if you could reference this for others
    here :)'
  created_at: 2022-09-06 17:16:33+00:00
  edited: false
  hidden: false
  id: 63178e8149af4d408c2e6c13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e95fac3898103393781475c4b4648adc.svg
      fullname: Sumit Chaturvedi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: suchatur
      type: user
    createdAt: '2022-09-07T03:11:06.000Z'
    data:
      edited: true
      editors:
      - suchatur
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e95fac3898103393781475c4b4648adc.svg
          fullname: Sumit Chaturvedi
          isHf: false
          isPro: false
          name: suchatur
          type: user
        html: '<pre><code># load model

          # ...

          model = model.to(torch.float16)

          model = model.to(''cuda'')

          # run it

          # ...

          </code></pre>

          '
        raw: '```

          # load model

          # ...

          model = model.to(torch.float16)

          model = model.to(''cuda'')

          # run it

          # ...

          ```'
        updatedAt: '2022-09-07T03:11:22.755Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - PabloG
        - Milky80
        - rajpal
    id: 63180bca212fce5a3cdcd4d6
    type: comment
  author: suchatur
  content: '```

    # load model

    # ...

    model = model.to(torch.float16)

    model = model.to(''cuda'')

    # run it

    # ...

    ```'
  created_at: 2022-09-07 02:11:06+00:00
  edited: true
  hidden: false
  id: 63180bca212fce5a3cdcd4d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665740371407-61a3e2aa2172c41f121589fb.jpeg?w=200&h=200&f=face
      fullname: AQ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anzorq
      type: user
    createdAt: '2022-10-03T23:32:08.000Z'
    data:
      edited: false
      editors:
      - anzorq
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665740371407-61a3e2aa2172c41f121589fb.jpeg?w=200&h=200&f=face
          fullname: AQ
          isHf: false
          isPro: false
          name: anzorq
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;suchatur&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/suchatur\">@<span class=\"\
          underline\">suchatur</span></a></span>\n\n\t</span></span> does that reduce\
          \ the model's size and inference time?</p>\n"
        raw: '@suchatur does that reduce the model''s size and inference time?'
        updatedAt: '2022-10-03T23:32:08.516Z'
      numEdits: 0
      reactions: []
    id: 633b70f89fe04b13f46e602c
    type: comment
  author: anzorq
  content: '@suchatur does that reduce the model''s size and inference time?'
  created_at: 2022-10-03 22:32:08+00:00
  edited: false
  hidden: false
  id: 633b70f89fe04b13f46e602c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e95fac3898103393781475c4b4648adc.svg
      fullname: Sumit Chaturvedi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: suchatur
      type: user
    createdAt: '2022-10-04T05:25:05.000Z'
    data:
      edited: false
      editors:
      - suchatur
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e95fac3898103393781475c4b4648adc.svg
          fullname: Sumit Chaturvedi
          isHf: false
          isPro: false
          name: suchatur
          type: user
        html: '<p>It halves the model size thus making it fit in GPU memory. Can''t
          comment on inference time because I couldn''t run the full precision (float32)
          model on my GPU.</p>

          '
        raw: It halves the model size thus making it fit in GPU memory. Can't comment
          on inference time because I couldn't run the full precision (float32) model
          on my GPU.
        updatedAt: '2022-10-04T05:25:05.027Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - anzorq
    id: 633bc3b13124a257cd24dc28
    type: comment
  author: suchatur
  content: It halves the model size thus making it fit in GPU memory. Can't comment
    on inference time because I couldn't run the full precision (float32) model on
    my GPU.
  created_at: 2022-10-04 04:25:05+00:00
  edited: false
  hidden: false
  id: 633bc3b13124a257cd24dc28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1b9376c554ef01008282d017c40f7f12.svg
      fullname: Wolfgang Meyers
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfgangmeyers
      type: user
    createdAt: '2022-11-03T00:31:53.000Z'
    data:
      edited: false
      editors:
      - wolfgangmeyers
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1b9376c554ef01008282d017c40f7f12.svg
          fullname: Wolfgang Meyers
          isHf: false
          isPro: false
          name: wolfgangmeyers
          type: user
        html: '<p>Is this only possible with diffusers library? I can run fp32, and
          I''d be interested to see the difference in inference time.</p>

          '
        raw: Is this only possible with diffusers library? I can run fp32, and I'd
          be interested to see the difference in inference time.
        updatedAt: '2022-11-03T00:31:53.070Z'
      numEdits: 0
      reactions: []
    id: 63630bf9242f3b3a8597e03f
    type: comment
  author: wolfgangmeyers
  content: Is this only possible with diffusers library? I can run fp32, and I'd be
    interested to see the difference in inference time.
  created_at: 2022-11-02 23:31:53+00:00
  edited: false
  hidden: false
  id: 63630bf9242f3b3a8597e03f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e95fac3898103393781475c4b4648adc.svg
      fullname: Sumit Chaturvedi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: suchatur
      type: user
    createdAt: '2022-11-03T09:19:45.000Z'
    data:
      edited: false
      editors:
      - suchatur
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e95fac3898103393781475c4b4648adc.svg
          fullname: Sumit Chaturvedi
          isHf: false
          isPro: false
          name: suchatur
          type: user
        html: '<p>No, you can do this with the <a rel="nofollow" href="https://github.com/CompVis/stable-diffusion">github
          repository</a> also. I don''t think there will be much difference in inference
          time (in fact fp16 may be slower due to quantization overhead).</p>

          '
        raw: No, you can do this with the [github repository](https://github.com/CompVis/stable-diffusion)
          also. I don't think there will be much difference in inference time (in
          fact fp16 may be slower due to quantization overhead).
        updatedAt: '2022-11-03T09:19:45.418Z'
      numEdits: 0
      reactions: []
    id: 636387b19234172f58353140
    type: comment
  author: suchatur
  content: No, you can do this with the [github repository](https://github.com/CompVis/stable-diffusion)
    also. I don't think there will be much difference in inference time (in fact fp16
    may be slower due to quantization overhead).
  created_at: 2022-11-03 08:19:45+00:00
  edited: false
  hidden: false
  id: 636387b19234172f58353140
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1b9376c554ef01008282d017c40f7f12.svg
      fullname: Wolfgang Meyers
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfgangmeyers
      type: user
    createdAt: '2022-12-21T17:05:22.000Z'
    data:
      edited: false
      editors:
      - wolfgangmeyers
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1b9376c554ef01008282d017c40f7f12.svg
          fullname: Wolfgang Meyers
          isHf: false
          isPro: false
          name: wolfgangmeyers
          type: user
        html: '<p>Confirmed that this works with the github repo as well. I didn''t
          see any noticeable change in inference time.</p>

          '
        raw: Confirmed that this works with the github repo as well. I didn't see
          any noticeable change in inference time.
        updatedAt: '2022-12-21T17:05:22.418Z'
      numEdits: 0
      reactions: []
    id: 63a33cd2b5fc9ab9f63eca80
    type: comment
  author: wolfgangmeyers
  content: Confirmed that this works with the github repo as well. I didn't see any
    noticeable change in inference time.
  created_at: 2022-12-21 17:05:22+00:00
  edited: false
  hidden: false
  id: 63a33cd2b5fc9ab9f63eca80
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: CompVis/stable-diffusion-v-1-4-original
repo_type: model
status: closed
target_branch: null
title: half-precision (fp16) version?
