!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vdruts
conflicting_files: null
created_at: 2023-05-11 01:28:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
      fullname: Vladimir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vdruts
      type: user
    createdAt: '2023-05-11T02:28:40.000Z'
    data:
      edited: false
      editors:
      - vdruts
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
          fullname: Vladimir
          isHf: false
          isPro: false
          name: vdruts
          type: user
        html: "<p>Not sure why, im pretty sure I was opening it with latest GPTQ before\
          \ Triton.</p>\n<p>Traceback (most recent call last):<br>File \u201C/home/perplexity/text-generation-webui/server.py\u201D\
          , line 67, in load_model_wrapper<br>shared.model, shared.tokenizer = load_model(shared.model_name)<br>File\
          \ \u201C/home/perplexity/text-generation-webui/modules/models.py\u201D,\
          \ line 159, in load_model<br>model = load_quantized(model_name)<br>File\
          \ \u201C/home/perplexity/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 178, in load_quantized<br>model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)<br>File\
          \ \u201C/home/perplexity/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 84, in _load_quant<br>model.load_state_dict(safe_load(checkpoint),\
          \ strict=False)<br>File \u201C/home/perplexity/miniconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\u201D\
          , line 2041, in load_state_dict<br>raise RuntimeError(\u2018Error(s) in\
          \ loading state_dict for {}:\\n\\t{}\u2019.format(<br>RuntimeError: Error(s)\
          \ in loading state_dict for LlamaForCausalLM:<br>size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([52, 832]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 832]).<br>size mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape</p>\n"
        raw: "Not sure why, im pretty sure I was opening it with latest GPTQ before\
          \ Triton.\r\n\r\nTraceback (most recent call last):\r\nFile \u201C/home/perplexity/text-generation-webui/server.py\u201D\
          , line 67, in load_model_wrapper\r\nshared.model, shared.tokenizer = load_model(shared.model_name)\r\
          \nFile \u201C/home/perplexity/text-generation-webui/modules/models.py\u201D\
          , line 159, in load_model\r\nmodel = load_quantized(model_name)\r\nFile\
          \ \u201C/home/perplexity/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 178, in load_quantized\r\nmodel = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\r\
          \nFile \u201C/home/perplexity/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 84, in _load_quant\r\nmodel.load_state_dict(safe_load(checkpoint),\
          \ strict=False)\r\nFile \u201C/home/perplexity/miniconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\u201D\
          , line 2041, in load_state_dict\r\nraise RuntimeError(\u2018Error(s) in\
          \ loading state_dict for {}:\\n\\t{}\u2019.format(\r\nRuntimeError: Error(s)\
          \ in loading state_dict for LlamaForCausalLM:\r\nsize mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([52, 832]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 832]).\r\nsize mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape"
        updatedAt: '2023-05-11T02:28:40.427Z'
      numEdits: 0
      reactions: []
    id: 645c52d806df9aaa910f36d5
    type: comment
  author: vdruts
  content: "Not sure why, im pretty sure I was opening it with latest GPTQ before\
    \ Triton.\r\n\r\nTraceback (most recent call last):\r\nFile \u201C/home/perplexity/text-generation-webui/server.py\u201D\
    , line 67, in load_model_wrapper\r\nshared.model, shared.tokenizer = load_model(shared.model_name)\r\
    \nFile \u201C/home/perplexity/text-generation-webui/modules/models.py\u201D, line\
    \ 159, in load_model\r\nmodel = load_quantized(model_name)\r\nFile \u201C/home/perplexity/text-generation-webui/modules/GPTQ_loader.py\u201D\
    , line 178, in load_quantized\r\nmodel = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\r\
    \nFile \u201C/home/perplexity/text-generation-webui/modules/GPTQ_loader.py\u201D\
    , line 84, in _load_quant\r\nmodel.load_state_dict(safe_load(checkpoint), strict=False)\r\
    \nFile \u201C/home/perplexity/miniconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\u201D\
    , line 2041, in load_state_dict\r\nraise RuntimeError(\u2018Error(s) in loading\
    \ state_dict for {}:\\n\\t{}\u2019.format(\r\nRuntimeError: Error(s) in loading\
    \ state_dict for LlamaForCausalLM:\r\nsize mismatch for model.layers.0.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([52, 832]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 832]).\r\nsize mismatch for model.layers.0.self_attn.k_proj.scales:\
    \ copying a param with shape"
  created_at: 2023-05-11 01:28:40+00:00
  edited: false
  hidden: false
  id: 645c52d806df9aaa910f36d5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: MetaIX/OpenAssistant-Llama-30b-4bit
repo_type: model
status: open
target_branch: null
title: Can't open this model anymore
