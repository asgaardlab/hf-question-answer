!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jbgg
conflicting_files: null
created_at: 2023-07-11 15:19:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/23c69508017bee26301285d227b1ec0f.svg
      fullname: Jesse
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jbgg
      type: user
    createdAt: '2023-07-11T16:19:10.000Z'
    data:
      edited: false
      editors:
      - jbgg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9351330995559692
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/23c69508017bee26301285d227b1ec0f.svg
          fullname: Jesse
          isHf: false
          isPro: false
          name: jbgg
          type: user
        html: '<p>I am pretty new to using torch and transformers, and am trying to
          assess bias in news articles, but am often getting this error for longer
          articles (with the tensor size not necessarily being 922):<br>"RuntimeError:
          The size of tensor a (922) must match the size of tensor b (512) at non-singleton
          dimension 1"<br>In my understanding the tokenizer should have already made
          the tokens of correct dimensions, but I am obviously wrong, and am wondering
          how to solve this bug in my code</p>

          '
        raw: "I am pretty new to using torch and transformers, and am trying to assess\
          \ bias in news articles, but am often getting this error for longer articles\
          \ (with the tensor size not necessarily being 922): \r\n\"RuntimeError:\
          \ The size of tensor a (922) must match the size of tensor b (512) at non-singleton\
          \ dimension 1\"\r\nIn my understanding the tokenizer should have already\
          \ made the tokens of correct dimensions, but I am obviously wrong, and am\
          \ wondering how to solve this bug in my code"
        updatedAt: '2023-07-11T16:19:10.021Z'
      numEdits: 0
      reactions: []
    id: 64ad80fecfb9ae8971f2ba57
    type: comment
  author: jbgg
  content: "I am pretty new to using torch and transformers, and am trying to assess\
    \ bias in news articles, but am often getting this error for longer articles (with\
    \ the tensor size not necessarily being 922): \r\n\"RuntimeError: The size of\
    \ tensor a (922) must match the size of tensor b (512) at non-singleton dimension\
    \ 1\"\r\nIn my understanding the tokenizer should have already made the tokens\
    \ of correct dimensions, but I am obviously wrong, and am wondering how to solve\
    \ this bug in my code"
  created_at: 2023-07-11 15:19:10+00:00
  edited: false
  hidden: false
  id: 64ad80fecfb9ae8971f2ba57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c212996ba7f4bc1b69b715ebc6c05bc.svg
      fullname: R K
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rklra
      type: user
    createdAt: '2023-07-13T20:52:00.000Z'
    data:
      edited: false
      editors:
      - rklra
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6964024901390076
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c212996ba7f4bc1b69b715ebc6c05bc.svg
          fullname: R K
          isHf: false
          isPro: false
          name: rklra
          type: user
        html: '<p>Are you using the tokenizer code provided in the example? If so
          look at the docs here:<br><a href="https://huggingface.co/docs/transformers/main/en/pad_truncation">https://huggingface.co/docs/transformers/main/en/pad_truncation</a><br>A
          possible solution here is to add something similar to<br><code>max_length=512,
          truncation=True</code> to your tokenizer arguments.</p>

          '
        raw: "Are you using the tokenizer code provided in the example? If so look\
          \ at the docs here:\nhttps://huggingface.co/docs/transformers/main/en/pad_truncation\n\
          A possible solution here is to add something similar to \n`max_length=512,\
          \ truncation=True` to your tokenizer arguments.\n"
        updatedAt: '2023-07-13T20:52:00.071Z'
      numEdits: 0
      reactions: []
    id: 64b063f0394fd0e6fae8c5ea
    type: comment
  author: rklra
  content: "Are you using the tokenizer code provided in the example? If so look at\
    \ the docs here:\nhttps://huggingface.co/docs/transformers/main/en/pad_truncation\n\
    A possible solution here is to add something similar to \n`max_length=512, truncation=True`\
    \ to your tokenizer arguments.\n"
  created_at: 2023-07-13 19:52:00+00:00
  edited: false
  hidden: false
  id: 64b063f0394fd0e6fae8c5ea
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: bucketresearch/politicalBiasBERT
repo_type: model
status: open
target_branch: null
title: Size of token a must match the size of token b
