!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mike-ravkine
conflicting_files: []
created_at: 2023-07-08 20:58:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-07-08T21:58:38.000Z'
    data:
      edited: false
      editors:
      - mike-ravkine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.971307635307312
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
          fullname: Mike Ravkine
          isHf: false
          isPro: false
          name: mike-ravkine
          type: user
        html: '<p>thanks for this model, I spent the afternoon working with it.</p>

          <p>I''ve proposed three minor updates to the README:</p>

          <ul>

          <li>Docker compatibility (I was able to confirm it works with 12.1)</li>

          <li>I hit an issue because I build on a different machine then I run on
          that it tried to target CUDA versions the code doesn''t support, being specific
          about which CUDA versions we target fixes that</li>

          <li>The config for this model hasn''t actually got a tokenizer defined,
          it just has the same name.</li>

          </ul>

          '
        raw: 'thanks for this model, I spent the afternoon working with it.


          I''ve proposed three minor updates to the README:


          * Docker compatibility (I was able to confirm it works with 12.1)

          * I hit an issue because I build on a different machine then I run on that
          it tried to target CUDA versions the code doesn''t support, being specific
          about which CUDA versions we target fixes that

          * The config for this model hasn''t actually got a tokenizer defined, it
          just has the same name.'
        updatedAt: '2023-07-08T21:58:38.842Z'
      numEdits: 0
      reactions: []
    id: 64a9dc0eecac79c49fd57f14
    type: comment
  author: mike-ravkine
  content: 'thanks for this model, I spent the afternoon working with it.


    I''ve proposed three minor updates to the README:


    * Docker compatibility (I was able to confirm it works with 12.1)

    * I hit an issue because I build on a different machine then I run on that it
    tried to target CUDA versions the code doesn''t support, being specific about
    which CUDA versions we target fixes that

    * The config for this model hasn''t actually got a tokenizer defined, it just
    has the same name.'
  created_at: 2023-07-08 20:58:38+00:00
  edited: false
  hidden: false
  id: 64a9dc0eecac79c49fd57f14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-07-08T21:58:39.000Z'
    data:
      oid: 55dd7fe37d3e28d6e43c54358df57956bf04efc0
      parents:
      - f2ce49687ec5a968715b0e8cf803a9deda0e8402
      subject: Thanks for this model!
    id: 64a9dc0f0000000000000000
    type: commit
  author: mike-ravkine
  created_at: 2023-07-08 20:58:39+00:00
  id: 64a9dc0f0000000000000000
  oid: 55dd7fe37d3e28d6e43c54358df57956bf04efc0
  summary: Thanks for this model!
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-07-08T22:13:16.000Z'
    data:
      edited: false
      editors:
      - mike-ravkine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9468811750411987
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
          fullname: Mike Ravkine
          isHf: false
          isPro: false
          name: mike-ravkine
          type: user
        html: '<p>I''ve made one other change locally that I did not include with
          this PR because it''s changing behavior and I wanted to first confirm thats
          OK:</p>

          <pre><code>model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.float16,
          trust_remote_code=True)

          </code></pre>

          <p>Replacing <code>from_pretrained</code> with <code>from_config</code>
          like this makes it so you don''t need to have the original model weights
          downloaded and stored - what do you think?  This is an advantage on cloud
          platforms that charge for storage resources especially.</p>

          '
        raw: 'I''ve made one other change locally that I did not include with this
          PR because it''s changing behavior and I wanted to first confirm thats OK:


          ```

          model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.float16,
          trust_remote_code=True)

          ```


          Replacing `from_pretrained` with `from_config` like this makes it so you
          don''t need to have the original model weights downloaded and stored - what
          do you think?  This is an advantage on cloud platforms that charge for storage
          resources especially.'
        updatedAt: '2023-07-08T22:13:16.610Z'
      numEdits: 0
      reactions: []
    id: 64a9df7cf972a36c53c6cd6f
    type: comment
  author: mike-ravkine
  content: 'I''ve made one other change locally that I did not include with this PR
    because it''s changing behavior and I wanted to first confirm thats OK:


    ```

    model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.float16, trust_remote_code=True)

    ```


    Replacing `from_pretrained` with `from_config` like this makes it so you don''t
    need to have the original model weights downloaded and stored - what do you think?  This
    is an advantage on cloud platforms that charge for storage resources especially.'
  created_at: 2023-07-08 21:13:16+00:00
  edited: false
  hidden: false
  id: 64a9df7cf972a36c53c6cd6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
      fullname: Abhinav Kulkarni
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: abhinavkulkarni
      type: user
    createdAt: '2023-07-09T14:33:58.000Z'
    data:
      edited: true
      editors:
      - abhinavkulkarni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9235111474990845
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
          fullname: Abhinav Kulkarni
          isHf: false
          isPro: false
          name: abhinavkulkarni
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;mike-ravkine&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mike-ravkine\"\
          >@<span class=\"underline\">mike-ravkine</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>Thanks for the PR.</p>\n<blockquote>\n<p><code>export TORCH_CUDA_ARCH_LIST='8.0\
          \ 8.6 8.7 8.9 9.0'</code></p>\n</blockquote>\n<p>What if newer architectures\
          \ come up? We'll need to go back and edit this list. Also, seems like, you\
          \ may have missed an architecture version <code>9.0a</code>.</p>\n<blockquote>\n\
          <p>The config for this model hasn't actually got a tokenizer defined, it\
          \ just has the same name.</p>\n</blockquote>\n<p>I have updated all of the\
          \ AWQ models with tokenizers, so either of </p>\n<p><code>tokenizer = AutoTokenizer.from_pretrained(model_name)</code></p>\n\
          <p>or </p>\n<p><code>tokenizer = AutoTokenizer.from_pretrained(config.tokenizer)</code></p>\n\
          <p>should work. I'll gladly accept your change.</p>\n<blockquote>\n<p>Replacing\
          \ from_pretrained with from_config like this makes it so you don't need\
          \ to have the original model weights downloaded and stored - what do you\
          \ think? This is an advantage on cloud platforms that charge for storage\
          \ resources especially.</p>\n</blockquote>\n<p>Actually, it does load the\
          \ model, only in the CPU. Except for using <code>accelerate.init_empty_weights()</code>,\
          \ there is no other way to init model with empty weights on a <code>meta</code>\
          \ device.</p>\n<p>As for adding Docker image, I'd happy to include that\
          \ line in the model card.</p>\n<p>Thanks for your PR! It definitely helped\
          \ me update all the AWQ models with respective tokenizers. And let me know\
          \ what you think about <code>TORCH_CUDA_ARCH_LIST</code> - how to keep it\
          \ updated as future NVIDIA cards are released.</p>\n"
        raw: "Hey @mike-ravkine,\n\nThanks for the PR.\n\n> `export TORCH_CUDA_ARCH_LIST='8.0\
          \ 8.6 8.7 8.9 9.0'`\n\nWhat if newer architectures come up? We'll need to\
          \ go back and edit this list. Also, seems like, you may have missed an architecture\
          \ version `9.0a`.\n\n> The config for this model hasn't actually got a tokenizer\
          \ defined, it just has the same name.\n\nI have updated all of the AWQ models\
          \ with tokenizers, so either of \n\n`tokenizer = AutoTokenizer.from_pretrained(model_name)`\n\
          \nor \n\n`tokenizer = AutoTokenizer.from_pretrained(config.tokenizer)`\n\
          \nshould work. I'll gladly accept your change.\n\n> Replacing from_pretrained\
          \ with from_config like this makes it so you don't need to have the original\
          \ model weights downloaded and stored - what do you think? This is an advantage\
          \ on cloud platforms that charge for storage resources especially.\n\nActually,\
          \ it does load the model, only in the CPU. Except for using `accelerate.init_empty_weights()`,\
          \ there is no other way to init model with empty weights on a `meta` device.\n\
          \nAs for adding Docker image, I'd happy to include that line in the model\
          \ card.\n\nThanks for your PR! It definitely helped me update all the AWQ\
          \ models with respective tokenizers. And let me know what you think about\
          \ `TORCH_CUDA_ARCH_LIST` - how to keep it updated as future NVIDIA cards\
          \ are released."
        updatedAt: '2023-07-09T14:38:22.731Z'
      numEdits: 1
      reactions: []
    id: 64aac556ac49ad6717b4aa14
    type: comment
  author: abhinavkulkarni
  content: "Hey @mike-ravkine,\n\nThanks for the PR.\n\n> `export TORCH_CUDA_ARCH_LIST='8.0\
    \ 8.6 8.7 8.9 9.0'`\n\nWhat if newer architectures come up? We'll need to go back\
    \ and edit this list. Also, seems like, you may have missed an architecture version\
    \ `9.0a`.\n\n> The config for this model hasn't actually got a tokenizer defined,\
    \ it just has the same name.\n\nI have updated all of the AWQ models with tokenizers,\
    \ so either of \n\n`tokenizer = AutoTokenizer.from_pretrained(model_name)`\n\n\
    or \n\n`tokenizer = AutoTokenizer.from_pretrained(config.tokenizer)`\n\nshould\
    \ work. I'll gladly accept your change.\n\n> Replacing from_pretrained with from_config\
    \ like this makes it so you don't need to have the original model weights downloaded\
    \ and stored - what do you think? This is an advantage on cloud platforms that\
    \ charge for storage resources especially.\n\nActually, it does load the model,\
    \ only in the CPU. Except for using `accelerate.init_empty_weights()`, there is\
    \ no other way to init model with empty weights on a `meta` device.\n\nAs for\
    \ adding Docker image, I'd happy to include that line in the model card.\n\nThanks\
    \ for your PR! It definitely helped me update all the AWQ models with respective\
    \ tokenizers. And let me know what you think about `TORCH_CUDA_ARCH_LIST` - how\
    \ to keep it updated as future NVIDIA cards are released."
  created_at: 2023-07-09 13:33:58+00:00
  edited: true
  hidden: false
  id: 64aac556ac49ad6717b4aa14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-07-09T14:43:49.000Z'
    data:
      edited: false
      editors:
      - mike-ravkine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9176679849624634
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
          fullname: Mike Ravkine
          isHf: false
          isPro: false
          name: mike-ravkine
          type: user
        html: "<ul>\n<li><p><code>model_name</code> refers to the upstream model right?\
          \ In this case it's <code>tiiuae/falcon-7b</code> that did not have a <code>config.tokenizer</code></p>\n\
          </li>\n<li><p>Good point on the list of CUDA versions.  The original problem\
          \ I hit was that if you dont have any hardware at all installed during the\
          \ compile it will decide to support all previous versions but the kernels\
          \ here don't work below 8.0.  If you don't set this variable torch tries\
          \ to auto-detect based on your current hardware, so perhaps this is best\
          \ added a note rather then actually modifying the build instructions.</p>\n\
          </li>\n<li><p>To be clear I have no issue with <code>accelerate.init_empty_weights()</code>..\
          \ The question is do you want to require the weights of the original model\
          \ (even if they're not used at all)?  That's a side-effect of <code>AutoModelForCausalLM.from_pretrained</code>\
          \ it will always download the weights.  Swapping in <code>AutoModelForCausalLM.from_config</code>\
          \ makes it so you don't need to download original weights:</p>\n</li>\n\
          </ul>\n<pre><code>        with init_empty_weights():\n            model\
          \ = AutoModelForCausalLM.from_config(config, torch_dtype=torch.float16,\
          \ trust_remote_code=True)\n</code></pre>\n"
        raw: "- `model_name` refers to the upstream model right? In this case it's\
          \ `tiiuae/falcon-7b` that did not have a `config.tokenizer`\n\n- Good point\
          \ on the list of CUDA versions.  The original problem I hit was that if\
          \ you dont have any hardware at all installed during the compile it will\
          \ decide to support all previous versions but the kernels here don't work\
          \ below 8.0.  If you don't set this variable torch tries to auto-detect\
          \ based on your current hardware, so perhaps this is best added a note rather\
          \ then actually modifying the build instructions.\n\n- To be clear I have\
          \ no issue with `accelerate.init_empty_weights()`.. The question is do you\
          \ want to require the weights of the original model (even if they're not\
          \ used at all)?  That's a side-effect of `AutoModelForCausalLM.from_pretrained`\
          \ it will always download the weights.  Swapping in `AutoModelForCausalLM.from_config`\
          \ makes it so you don't need to download original weights:\n\n```\n    \
          \    with init_empty_weights():\n            model = AutoModelForCausalLM.from_config(config,\
          \ torch_dtype=torch.float16, trust_remote_code=True)\n```"
        updatedAt: '2023-07-09T14:43:49.155Z'
      numEdits: 0
      reactions: []
    id: 64aac7a5eaf3ada4ec887253
    type: comment
  author: mike-ravkine
  content: "- `model_name` refers to the upstream model right? In this case it's `tiiuae/falcon-7b`\
    \ that did not have a `config.tokenizer`\n\n- Good point on the list of CUDA versions.\
    \  The original problem I hit was that if you dont have any hardware at all installed\
    \ during the compile it will decide to support all previous versions but the kernels\
    \ here don't work below 8.0.  If you don't set this variable torch tries to auto-detect\
    \ based on your current hardware, so perhaps this is best added a note rather\
    \ then actually modifying the build instructions.\n\n- To be clear I have no issue\
    \ with `accelerate.init_empty_weights()`.. The question is do you want to require\
    \ the weights of the original model (even if they're not used at all)?  That's\
    \ a side-effect of `AutoModelForCausalLM.from_pretrained` it will always download\
    \ the weights.  Swapping in `AutoModelForCausalLM.from_config` makes it so you\
    \ don't need to download original weights:\n\n```\n        with init_empty_weights():\n\
    \            model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.float16,\
    \ trust_remote_code=True)\n```"
  created_at: 2023-07-09 13:43:49+00:00
  edited: false
  hidden: false
  id: 64aac7a5eaf3ada4ec887253
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
      fullname: Abhinav Kulkarni
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: abhinavkulkarni
      type: user
    createdAt: '2023-07-09T14:56:39.000Z'
    data:
      status: merged
    id: 64aacaa7ecac79c49fea44ed
    type: status-change
  author: abhinavkulkarni
  created_at: 2023-07-09 13:56:39+00:00
  id: 64aacaa7ecac79c49fea44ed
  new_status: merged
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
      fullname: Abhinav Kulkarni
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: abhinavkulkarni
      type: user
    createdAt: '2023-07-09T14:57:34.000Z'
    data:
      edited: false
      editors:
      - abhinavkulkarni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.918670117855072
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
          fullname: Abhinav Kulkarni
          isHf: false
          isPro: false
          name: abhinavkulkarni
          type: user
        html: '<p>Thanks for clarifying <code>init_empty_weights</code> issue! I was
          under the wrong impression that <code>from_pretrained</code> does not load
          weights in <code>init_empty_weights</code> context block.</p>

          <p>I have updated all of the AWQ model documentions to reflect this learning.</p>

          '
        raw: 'Thanks for clarifying `init_empty_weights` issue! I was under the wrong
          impression that `from_pretrained` does not load weights in `init_empty_weights`
          context block.


          I have updated all of the AWQ model documentions to reflect this learning.'
        updatedAt: '2023-07-09T14:57:34.141Z'
      numEdits: 0
      reactions: []
    id: 64aacadefb9cf6afdb644fe8
    type: comment
  author: abhinavkulkarni
  content: 'Thanks for clarifying `init_empty_weights` issue! I was under the wrong
    impression that `from_pretrained` does not load weights in `init_empty_weights`
    context block.


    I have updated all of the AWQ model documentions to reflect this learning.'
  created_at: 2023-07-09 13:57:34+00:00
  edited: false
  hidden: false
  id: 64aacadefb9cf6afdb644fe8
  type: comment
is_pull_request: true
merge_commit_oid: c0eee9725b529c544787609568b97b52b6bc5965
num: 1
repo_id: abhinavkulkarni/tiiuae-falcon-7b-instruct-w4-g64-awq
repo_type: model
status: merged
target_branch: refs/heads/main
title: Thanks for this model!
