!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Steelclaw
conflicting_files: null
created_at: 2023-11-09 03:32:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b2713642fd24f97d989e11afe7359b6.svg
      fullname: Treven Ricketts
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Steelclaw
      type: user
    createdAt: '2023-11-09T03:32:47.000Z'
    data:
      edited: true
      editors:
      - Steelclaw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9671398997306824
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8b2713642fd24f97d989e11afe7359b6.svg
          fullname: Treven Ricketts
          isHf: false
          isPro: false
          name: Steelclaw
          type: user
        html: '<p>I am loving the creativity of this model! However, there''s an issue
          I can''t seem to fix:</p>

          <p>I''m using Ooba''s Text Gen to load the model, and I''m having no issues
          with chatting there. This issue only seems to happen when I am using SillyTavern.</p>

          <p>Every character, every response, every template setting, the AI will
          output great text, but at the very end, usually on the same line it''ll
          generate &lt;|im_end|&gt;</p>

          <p>This seems to be a stopping string in some of the templates, but it appears
          even on templates that lack that string.</p>

          <p>SillyTavern ignores &lt;|im_end|&gt; in the "banned token" list for whatever
          reason. I also cannot seem to find a way in that program to automatically
          remove this unwanted stopping string after generation.</p>

          <p>It''s a minor (but annoying) issue, since it just requires editing the
          output, but why is it happening in the first place? I did not have this
          problem with your other merged models. I last used xwin-stellarbright-mythospice-70b
          with no issues like this. I even tested it to see if it was a SillyTavern
          thing, but it doesn''t appear to be.</p>

          <p>Are you able to reproduce this &lt;|im_end|&gt; issue in SillyTavern
          (or any other programs)?</p>

          <p>This new model IS extremely creative from what I''ve seen in the couple
          hours I''ve tested it in a few scenarios. I''m likely going to keep using
          it regardless, but I wondered if there was an issue with the merge that
          might have caused this.</p>

          <p>Thanks for experimenting so much with these. Keep up the good work!&lt;|im_end|&gt;</p>

          '
        raw: 'I am loving the creativity of this model! However, there''s an issue
          I can''t seem to fix:


          I''m using Ooba''s Text Gen to load the model, and I''m having no issues
          with chatting there. This issue only seems to happen when I am using SillyTavern.


          Every character, every response, every template setting, the AI will output
          great text, but at the very end, usually on the same line it''ll generate
          <|im_end|>


          This seems to be a stopping string in some of the templates, but it appears
          even on templates that lack that string.


          SillyTavern ignores <|im_end|> in the "banned token" list for whatever reason.
          I also cannot seem to find a way in that program to automatically remove
          this unwanted stopping string after generation.


          It''s a minor (but annoying) issue, since it just requires editing the output,
          but why is it happening in the first place? I did not have this problem
          with your other merged models. I last used xwin-stellarbright-mythospice-70b
          with no issues like this. I even tested it to see if it was a SillyTavern
          thing, but it doesn''t appear to be.


          Are you able to reproduce this <|im_end|> issue in SillyTavern (or any other
          programs)?


          This new model IS extremely creative from what I''ve seen in the couple
          hours I''ve tested it in a few scenarios. I''m likely going to keep using
          it regardless, but I wondered if there was an issue with the merge that
          might have caused this.


          Thanks for experimenting so much with these. Keep up the good work!<|im_end|>'
        updatedAt: '2023-11-09T03:34:48.677Z'
      numEdits: 3
      reactions: []
    id: 654c52df08031bb3deb7f536
    type: comment
  author: Steelclaw
  content: 'I am loving the creativity of this model! However, there''s an issue I
    can''t seem to fix:


    I''m using Ooba''s Text Gen to load the model, and I''m having no issues with
    chatting there. This issue only seems to happen when I am using SillyTavern.


    Every character, every response, every template setting, the AI will output great
    text, but at the very end, usually on the same line it''ll generate <|im_end|>


    This seems to be a stopping string in some of the templates, but it appears even
    on templates that lack that string.


    SillyTavern ignores <|im_end|> in the "banned token" list for whatever reason.
    I also cannot seem to find a way in that program to automatically remove this
    unwanted stopping string after generation.


    It''s a minor (but annoying) issue, since it just requires editing the output,
    but why is it happening in the first place? I did not have this problem with your
    other merged models. I last used xwin-stellarbright-mythospice-70b with no issues
    like this. I even tested it to see if it was a SillyTavern thing, but it doesn''t
    appear to be.


    Are you able to reproduce this <|im_end|> issue in SillyTavern (or any other programs)?


    This new model IS extremely creative from what I''ve seen in the couple hours
    I''ve tested it in a few scenarios. I''m likely going to keep using it regardless,
    but I wondered if there was an issue with the merge that might have caused this.


    Thanks for experimenting so much with these. Keep up the good work!<|im_end|>'
  created_at: 2023-11-09 03:32:47+00:00
  edited: true
  hidden: false
  id: 654c52df08031bb3deb7f536
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
      fullname: Sophosympatheia
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sophosympatheia
      type: user
    createdAt: '2023-11-09T08:13:39.000Z'
    data:
      edited: false
      editors:
      - sophosympatheia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9589038491249084
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b108103617b0a5b0f6f5/A5P1pKfMv5KhGvnpfo21G.png?w=200&h=200&f=face
          fullname: Sophosympatheia
          isHf: false
          isPro: false
          name: sophosympatheia
          type: user
        html: '<p>I love that you ended your commend with &lt;|im_end|&gt;. That made
          me laugh!</p>

          <p>I haven''t encountered the issue you''re describing with this merge.
          Can you share more details about your SillyTavern settings and sampler settings?</p>

          <p>In the meantime, adding &lt;|im_end|&gt; to Custom Stopping Strings should
          get SillyTavern to remove the string when it appears.</p>

          <p>I''m glad you''re enjoying this merge. Check out my xwin-stellarbright-airoboros_peft
          merge if you want something similar that still has its own flavor; I think
          I prefer that one over the blend with mythospice added into the mix. I''m
          running another experiment now replicating this merge but with limarp and
          Kimiko LoRAs applied instead of airoboros. We''ll see if that proves to
          be any good. I tried a 50-50 blend of dolphin and xwin + limarp LoRA and
          the result was so underwhelming by comparison that I''m not even going to
          upload it.</p>

          '
        raw: 'I love that you ended your commend with <|im_end|>. That made me laugh!


          I haven''t encountered the issue you''re describing with this merge. Can
          you share more details about your SillyTavern settings and sampler settings?


          In the meantime, adding <|im_end|> to Custom Stopping Strings should get
          SillyTavern to remove the string when it appears.


          I''m glad you''re enjoying this merge. Check out my xwin-stellarbright-airoboros_peft
          merge if you want something similar that still has its own flavor; I think
          I prefer that one over the blend with mythospice added into the mix. I''m
          running another experiment now replicating this merge but with limarp and
          Kimiko LoRAs applied instead of airoboros. We''ll see if that proves to
          be any good. I tried a 50-50 blend of dolphin and xwin + limarp LoRA and
          the result was so underwhelming by comparison that I''m not even going to
          upload it.'
        updatedAt: '2023-11-09T08:13:39.234Z'
      numEdits: 0
      reactions: []
    id: 654c94b3d6153dccbe4fe1f4
    type: comment
  author: sophosympatheia
  content: 'I love that you ended your commend with <|im_end|>. That made me laugh!


    I haven''t encountered the issue you''re describing with this merge. Can you share
    more details about your SillyTavern settings and sampler settings?


    In the meantime, adding <|im_end|> to Custom Stopping Strings should get SillyTavern
    to remove the string when it appears.


    I''m glad you''re enjoying this merge. Check out my xwin-stellarbright-airoboros_peft
    merge if you want something similar that still has its own flavor; I think I prefer
    that one over the blend with mythospice added into the mix. I''m running another
    experiment now replicating this merge but with limarp and Kimiko LoRAs applied
    instead of airoboros. We''ll see if that proves to be any good. I tried a 50-50
    blend of dolphin and xwin + limarp LoRA and the result was so underwhelming by
    comparison that I''m not even going to upload it.'
  created_at: 2023-11-09 08:13:39+00:00
  edited: false
  hidden: false
  id: 654c94b3d6153dccbe4fe1f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b2713642fd24f97d989e11afe7359b6.svg
      fullname: Treven Ricketts
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Steelclaw
      type: user
    createdAt: '2023-11-09T09:40:06.000Z'
    data:
      edited: false
      editors:
      - Steelclaw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8129981160163879
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8b2713642fd24f97d989e11afe7359b6.svg
          fullname: Treven Ricketts
          isHf: false
          isPro: false
          name: Steelclaw
          type: user
        html: '<p>I discovered my error in ST. I hadn''t enclosed the stop string
          in quotes, so it wasn''t being recognized. That fixed the problem, but I
          wonder why it happened in the first place. I''ve never encountered that
          issue where specific unwanted strings or instructions remained regardless
          of the prompt. Note that if the AI stops in the middle of its response and
          feels it''s unfinished, &lt;|im_end|&gt; does NOT generate. It will eventually
          generate when you can no longer "continue", however.</p>

          <p>Here are a few of my SillyTavern settings that may be relevant, or not.
          Using the main release, not staging/upcoming. Again, found the issue, so
          only read if interested:</p>

          <p>Preset: Mirostat, defaults except for Tau between 3-5, unchecked [Add
          BOS token, Ban EOS Token, Skip Special Tokens]<br>Advanced Formatting: Happened
          in all with multiple tests, but currently using:<br>"Default" Context Template
          with story string:<br>{{#if system}}{{system}}<br>{{/if}}{{#if wiBefore}}{{wiBefore}}<br>{{/if}}{{#if
          description}}{{description}}<br>{{/if}}{{#if personality}}{{char}}''s personality:
          {{personality}}<br>{{/if}}{{#if scenario}}Scenario: {{scenario}}<br>{{/if}}{{#if
          wiAfter}}{{wiAfter}}<br>{{/if}}{{#if persona}}{{persona}}<br>{{/if}}</p>

          <p>Instruct Mode Enabled: Vicuna 1.1<br>Custom Stopping Strings (this did
          fix the issue): ["USER: ", "ASSISTANT: ", "ASSANT: ", "&lt;|im_end|&gt;"]<br>Checked
          [Replace Macros in Sequences, Replace Macro in Custom Stopping Strings,
          Always add character name to prompt, Trim Spaces]<br>Input Sequence: "USER:
          " (with space)<br>Output Sequence: "ASSISTANT: " (with space)<br>System
          Sequence Prefix: BEGINNING OF CONVERSATION: (no space)<br>Separator: </p>

          <p>Most of those were set by the "default" setting up top and the Vicuna
          1.1 instruct preset.</p>

          <p>If you''re interested in poking at that, feel free to test on your end
          with those settings, though I found that there was no effect in changing
          Context Template, Instruct Mode Presets, or Text Gen WebUI (ooba/Mancer)
          presets.</p>

          <p>I''ll try out your other models soon. I usually like to test a model
          for a couple days of casual use before judging it. Just because it''s good
          in one area doesn''t mean it''s good in all.</p>

          <p>I love seeing the mix of different things. I look forward to finding
          a great a model for novel-style storytelling and RP, but one that also remembers
          what''s going on and can follow instructions.</p>

          <p>This model was able to almost perfectly summarize a 12k context (SFW)
          story with multiple things going on, so well that I was almost shocked.</p>

          <p>I never did bother uploading my 4.0bpw exl2 quant of lzlv. LoneStriker
          beat me to that anyway, and there''s no point in reinventing the wheel.</p>

          <p>Attached is an image from SillyTavern without the workaround, otherwise
          using the settings above. Note that this "character" is just an author to
          bounce ideas off of, so it likes to chat with the user.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6303d6daa362e7e8b51c1fff/_NoXfkT9t1EUeJ_Sz7ifm.png"><img
          alt="SillyTavern HF Info.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/6303d6daa362e7e8b51c1fff/_NoXfkT9t1EUeJ_Sz7ifm.png"></a></p>

          <p>Thanks again for helping me with the quantization issues I was having
          a couple weeks ago.</p>

          <p>-"Cl_Gr" (abbr.) from Reddit</p>

          '
        raw: 'I discovered my error in ST. I hadn''t enclosed the stop string in quotes,
          so it wasn''t being recognized. That fixed the problem, but I wonder why
          it happened in the first place. I''ve never encountered that issue where
          specific unwanted strings or instructions remained regardless of the prompt.
          Note that if the AI stops in the middle of its response and feels it''s
          unfinished, <|im_end|> does NOT generate. It will eventually generate when
          you can no longer "continue", however.


          Here are a few of my SillyTavern settings that may be relevant, or not.
          Using the main release, not staging/upcoming. Again, found the issue, so
          only read if interested:


          Preset: Mirostat, defaults except for Tau between 3-5, unchecked [Add BOS
          token, Ban EOS Token, Skip Special Tokens]

          Advanced Formatting: Happened in all with multiple tests, but currently
          using:

          "Default" Context Template with story string:

          {{#if system}}{{system}}

          {{/if}}{{#if wiBefore}}{{wiBefore}}

          {{/if}}{{#if description}}{{description}}

          {{/if}}{{#if personality}}{{char}}''s personality: {{personality}}

          {{/if}}{{#if scenario}}Scenario: {{scenario}}

          {{/if}}{{#if wiAfter}}{{wiAfter}}

          {{/if}}{{#if persona}}{{persona}}

          {{/if}}


          Instruct Mode Enabled: Vicuna 1.1

          Custom Stopping Strings (this did fix the issue): ["USER: ", "ASSISTANT:
          ", "ASSANT: ", "<|im_end|>"]

          Checked [Replace Macros in Sequences, Replace Macro in Custom Stopping Strings,
          Always add character name to prompt, Trim Spaces]

          Input Sequence: "USER: " (with space)

          Output Sequence: "ASSISTANT: " (with space)

          System Sequence Prefix: BEGINNING OF CONVERSATION: (no space)

          Separator: </s>


          Most of those were set by the "default" setting up top and the Vicuna 1.1
          instruct preset.




          If you''re interested in poking at that, feel free to test on your end with
          those settings, though I found that there was no effect in changing Context
          Template, Instruct Mode Presets, or Text Gen WebUI (ooba/Mancer) presets.


          I''ll try out your other models soon. I usually like to test a model for
          a couple days of casual use before judging it. Just because it''s good in
          one area doesn''t mean it''s good in all.


          I love seeing the mix of different things. I look forward to finding a great
          a model for novel-style storytelling and RP, but one that also remembers
          what''s going on and can follow instructions.


          This model was able to almost perfectly summarize a 12k context (SFW) story
          with multiple things going on, so well that I was almost shocked.


          I never did bother uploading my 4.0bpw exl2 quant of lzlv. LoneStriker beat
          me to that anyway, and there''s no point in reinventing the wheel.


          Attached is an image from SillyTavern without the workaround, otherwise
          using the settings above. Note that this "character" is just an author to
          bounce ideas off of, so it likes to chat with the user.


          ![SillyTavern HF Info.PNG](https://cdn-uploads.huggingface.co/production/uploads/6303d6daa362e7e8b51c1fff/_NoXfkT9t1EUeJ_Sz7ifm.png)


          Thanks again for helping me with the quantization issues I was having a
          couple weeks ago.


          -"Cl_Gr" (abbr.) from Reddit'
        updatedAt: '2023-11-09T09:40:06.091Z'
      numEdits: 0
      reactions: []
      relatedEventId: 654ca8f621cb7942392beb76
    id: 654ca8f621cb7942392beb72
    type: comment
  author: Steelclaw
  content: 'I discovered my error in ST. I hadn''t enclosed the stop string in quotes,
    so it wasn''t being recognized. That fixed the problem, but I wonder why it happened
    in the first place. I''ve never encountered that issue where specific unwanted
    strings or instructions remained regardless of the prompt. Note that if the AI
    stops in the middle of its response and feels it''s unfinished, <|im_end|> does
    NOT generate. It will eventually generate when you can no longer "continue", however.


    Here are a few of my SillyTavern settings that may be relevant, or not. Using
    the main release, not staging/upcoming. Again, found the issue, so only read if
    interested:


    Preset: Mirostat, defaults except for Tau between 3-5, unchecked [Add BOS token,
    Ban EOS Token, Skip Special Tokens]

    Advanced Formatting: Happened in all with multiple tests, but currently using:

    "Default" Context Template with story string:

    {{#if system}}{{system}}

    {{/if}}{{#if wiBefore}}{{wiBefore}}

    {{/if}}{{#if description}}{{description}}

    {{/if}}{{#if personality}}{{char}}''s personality: {{personality}}

    {{/if}}{{#if scenario}}Scenario: {{scenario}}

    {{/if}}{{#if wiAfter}}{{wiAfter}}

    {{/if}}{{#if persona}}{{persona}}

    {{/if}}


    Instruct Mode Enabled: Vicuna 1.1

    Custom Stopping Strings (this did fix the issue): ["USER: ", "ASSISTANT: ", "ASSANT:
    ", "<|im_end|>"]

    Checked [Replace Macros in Sequences, Replace Macro in Custom Stopping Strings,
    Always add character name to prompt, Trim Spaces]

    Input Sequence: "USER: " (with space)

    Output Sequence: "ASSISTANT: " (with space)

    System Sequence Prefix: BEGINNING OF CONVERSATION: (no space)

    Separator: </s>


    Most of those were set by the "default" setting up top and the Vicuna 1.1 instruct
    preset.




    If you''re interested in poking at that, feel free to test on your end with those
    settings, though I found that there was no effect in changing Context Template,
    Instruct Mode Presets, or Text Gen WebUI (ooba/Mancer) presets.


    I''ll try out your other models soon. I usually like to test a model for a couple
    days of casual use before judging it. Just because it''s good in one area doesn''t
    mean it''s good in all.


    I love seeing the mix of different things. I look forward to finding a great a
    model for novel-style storytelling and RP, but one that also remembers what''s
    going on and can follow instructions.


    This model was able to almost perfectly summarize a 12k context (SFW) story with
    multiple things going on, so well that I was almost shocked.


    I never did bother uploading my 4.0bpw exl2 quant of lzlv. LoneStriker beat me
    to that anyway, and there''s no point in reinventing the wheel.


    Attached is an image from SillyTavern without the workaround, otherwise using
    the settings above. Note that this "character" is just an author to bounce ideas
    off of, so it likes to chat with the user.


    ![SillyTavern HF Info.PNG](https://cdn-uploads.huggingface.co/production/uploads/6303d6daa362e7e8b51c1fff/_NoXfkT9t1EUeJ_Sz7ifm.png)


    Thanks again for helping me with the quantization issues I was having a couple
    weeks ago.


    -"Cl_Gr" (abbr.) from Reddit'
  created_at: 2023-11-09 09:40:06+00:00
  edited: false
  hidden: false
  id: 654ca8f621cb7942392beb72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/8b2713642fd24f97d989e11afe7359b6.svg
      fullname: Treven Ricketts
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Steelclaw
      type: user
    createdAt: '2023-11-09T09:40:06.000Z'
    data:
      status: closed
    id: 654ca8f621cb7942392beb76
    type: status-change
  author: Steelclaw
  created_at: 2023-11-09 09:40:06+00:00
  id: 654ca8f621cb7942392beb76
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: sophosympatheia/dolphin-xwin-airoboros_peft-70b
repo_type: model
status: closed
target_branch: null
title: Keeps generating <|im_end|> at the end of its output.
