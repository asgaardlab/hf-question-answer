!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nafnlaus
conflicting_files: null
created_at: 2023-09-29 19:56:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-09-29T20:56:21.000Z'
    data:
      edited: true
      editors:
      - Nafnlaus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7141331434249878
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: '<p>Using text-generation-webui with the llamacpp loader, these are
          the parameters I''ve discovered  for the best balance between performance
          and quality (results from hours of running on random inputs):</p>

          <p>** NVidia RTX 3090 + Intel i7-7800X @ 3.50GHz ***</p>

          <p>python server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q3_K_S.gguf
          --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000  --loader
          llamacpp --n_batch 2048 --threads 3 --n_ctx 2048<br>Tokens/s: 11.40 (Note:
          advise trying 4 threads, seems to work better with heavier weight models)</p>

          <p>python server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q3_K_M.gguf
          --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 60  --loader
          llamacpp --n_batch 2048 --threads 12 --n_ctx 2048<br>Tokens/s: 6.77</p>

          <p>** NVidia RTX 3090 + NVidia RTX 3060 + Intel i7-7800X @ 3.50GHz ***</p>

          <p>python server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q4_K_M.gguf
          --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000  --loader
          llamacpp --n_batch 2048 --threads 12 --n_ctx 2048  --tensor_split 47,13<br>Tokens/s:
          9.63</p>

          <p>python server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q4_K_M.gguf
          --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000  --loader
          llamacpp --n_batch 2048 --threads 4 --n_ctx 2048  --tensor_split 47,13<br>Tokens/s:
          10.93</p>

          <p>python server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf
          --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000  --loader
          llamacpp --n_batch 2048 --threads 2 --n_ctx 2048  --tensor_split 43,17<br>Tokens/s:
          7.65</p>

          <p>python server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf
          --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000  --loader
          llamacpp --n_batch 2048 --threads 3 --n_ctx 2048  --tensor_split 43,17<br>Tokens/s:
          7.97</p>

          <p>python server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf
          --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000  --loader
          llamacpp --n_batch 2048 --threads 4 --n_ctx 2048  --tensor_split 43,17<br>Tokens/s:
          8.21</p>

          <p>python server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf
          --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000  --loader
          llamacpp --n_batch 2048 --threads 5 --n_ctx 2048  --tensor_split 43,17<br>Tokens/s:
          7.82</p>

          <p>python server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf
          --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000  --loader
          llamacpp --n_batch 2048 --threads 12 --n_ctx 2048  --tensor_split 43,17<br>Tokens/s:
          7.60</p>

          <p>python server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_M.gguf
          --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 59  --loader
          llamacpp --n_batch 2048 --threads 12 --n_ctx 2048<br>Tokens/s: 4.57 (I think
          it was 12 threads, but I can''t be positive)</p>

          '
        raw: "Using text-generation-webui with the llamacpp loader, these are the\
          \ parameters I've discovered  for the best balance between performance and\
          \ quality (results from hours of running on random inputs):\n\n** NVidia\
          \ RTX 3090 + Intel i7-7800X @ 3.50GHz ***\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q3_K_S.gguf\
          \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers\
          \ 1000000000  --loader llamacpp --n_batch 2048 --threads 3 --n_ctx 2048\n\
          Tokens/s: 11.40 (Note: advise trying 4 threads, seems to work better with\
          \ heavier weight models)\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q3_K_M.gguf\
          \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers\
          \ 60  --loader llamacpp --n_batch 2048 --threads 12 --n_ctx 2048\nTokens/s:\
          \ 6.77\n\n** NVidia RTX 3090 + NVidia RTX 3060 + Intel i7-7800X @ 3.50GHz\
          \ ***\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q4_K_M.gguf\
          \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers\
          \ 1000000000  --loader llamacpp --n_batch 2048 --threads 12 --n_ctx 2048\
          \  --tensor_split 47,13 \nTokens/s: 9.63\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q4_K_M.gguf\
          \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers\
          \ 1000000000  --loader llamacpp --n_batch 2048 --threads 4 --n_ctx 2048\
          \  --tensor_split 47,13 \nTokens/s: 10.93\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf\
          \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers\
          \ 1000000000  --loader llamacpp --n_batch 2048 --threads 2 --n_ctx 2048\
          \  --tensor_split 43,17 \nTokens/s: 7.65\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf\
          \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers\
          \ 1000000000  --loader llamacpp --n_batch 2048 --threads 3 --n_ctx 2048\
          \  --tensor_split 43,17 \nTokens/s: 7.97\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf\
          \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers\
          \ 1000000000  --loader llamacpp --n_batch 2048 --threads 4 --n_ctx 2048\
          \  --tensor_split 43,17 \nTokens/s: 8.21\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf\
          \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers\
          \ 1000000000  --loader llamacpp --n_batch 2048 --threads 5 --n_ctx 2048\
          \  --tensor_split 43,17 \nTokens/s: 7.82\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf\
          \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers\
          \ 1000000000  --loader llamacpp --n_batch 2048 --threads 12 --n_ctx 2048\
          \  --tensor_split 43,17 \nTokens/s: 7.60\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_M.gguf\
          \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers\
          \ 59  --loader llamacpp --n_batch 2048 --threads 12 --n_ctx 2048 \nTokens/s:\
          \ 4.57 (I think it was 12 threads, but I can't be positive)\n"
        updatedAt: '2023-10-04T20:59:20.536Z'
      numEdits: 3
      reactions: []
    id: 651739f5bd0ed1f7dda1f68e
    type: comment
  author: Nafnlaus
  content: "Using text-generation-webui with the llamacpp loader, these are the parameters\
    \ I've discovered  for the best balance between performance and quality (results\
    \ from hours of running on random inputs):\n\n** NVidia RTX 3090 + Intel i7-7800X\
    \ @ 3.50GHz ***\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q3_K_S.gguf\
    \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000\
    \  --loader llamacpp --n_batch 2048 --threads 3 --n_ctx 2048\nTokens/s: 11.40\
    \ (Note: advise trying 4 threads, seems to work better with heavier weight models)\n\
    \npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q3_K_M.gguf --listen\
    \ --listen-port 5678 --verbose --api --xformers --n-gpu-layers 60  --loader llamacpp\
    \ --n_batch 2048 --threads 12 --n_ctx 2048\nTokens/s: 6.77\n\n** NVidia RTX 3090\
    \ + NVidia RTX 3060 + Intel i7-7800X @ 3.50GHz ***\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q4_K_M.gguf\
    \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000\
    \  --loader llamacpp --n_batch 2048 --threads 12 --n_ctx 2048  --tensor_split\
    \ 47,13 \nTokens/s: 9.63\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q4_K_M.gguf\
    \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000\
    \  --loader llamacpp --n_batch 2048 --threads 4 --n_ctx 2048  --tensor_split 47,13\
    \ \nTokens/s: 10.93\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf\
    \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000\
    \  --loader llamacpp --n_batch 2048 --threads 2 --n_ctx 2048  --tensor_split 43,17\
    \ \nTokens/s: 7.65\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf\
    \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000\
    \  --loader llamacpp --n_batch 2048 --threads 3 --n_ctx 2048  --tensor_split 43,17\
    \ \nTokens/s: 7.97\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf\
    \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000\
    \  --loader llamacpp --n_batch 2048 --threads 4 --n_ctx 2048  --tensor_split 43,17\
    \ \nTokens/s: 8.21\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf\
    \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000\
    \  --loader llamacpp --n_batch 2048 --threads 5 --n_ctx 2048  --tensor_split 43,17\
    \ \nTokens/s: 7.82\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_S.gguf\
    \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 1000000000\
    \  --loader llamacpp --n_batch 2048 --threads 12 --n_ctx 2048  --tensor_split\
    \ 43,17 \nTokens/s: 7.60\n\npython server.py ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q5_K_M.gguf\
    \ --listen --listen-port 5678 --verbose --api --xformers --n-gpu-layers 59  --loader\
    \ llamacpp --n_batch 2048 --threads 12 --n_ctx 2048 \nTokens/s: 4.57 (I think\
    \ it was 12 threads, but I can't be positive)\n"
  created_at: 2023-09-29 19:56:21+00:00
  edited: true
  hidden: false
  id: 651739f5bd0ed1f7dda1f68e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-11-17T23:06:38.000Z'
    data:
      edited: true
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9302070140838623
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>Just as a general note:<br>On single 3090 GPU you should get 20-25
          tokens/second for Falcon 40B, anything below 20 means something is going
          wrong<br>Your speed is less than half of where it should be, indicating
          something is wrong. I''d do a console client test as a start.</p>

          <p>When using llama.cpp (in 2023, this hopefully improves): Do not use multi
          GPU in any scenario you can avoid. Falcon 40B fits into single GPU at moderate
          quantization, use that.<br>Multi GPU comes with extreme performance losses
          currently. Use it for Falcon 180B, there you have no choice.</p>

          <p>Sidenotes: using 100000000 gpu layers is useless. There are less than
          100 layers, so you can make it a smaller number for convenience in reading.<br>Batch
          size of 2048 is too high, that''s something you could try on a H100 but
          not on a consumer GPU. Try 128,256,512 (512 is default)</p>

          '
        raw: 'Just as a general note:

          On single 3090 GPU you should get 20-25 tokens/second for Falcon 40B, anything
          below 20 means something is going wrong

          Your speed is less than half of where it should be, indicating something
          is wrong. I''d do a console client test as a start.


          When using llama.cpp (in 2023, this hopefully improves): Do not use multi
          GPU in any scenario you can avoid. Falcon 40B fits into single GPU at moderate
          quantization, use that.

          Multi GPU comes with extreme performance losses currently. Use it for Falcon
          180B, there you have no choice.


          Sidenotes: using 100000000 gpu layers is useless. There are less than 100
          layers, so you can make it a smaller number for convenience in reading.

          Batch size of 2048 is too high, that''s something you could try on a H100
          but not on a consumer GPU. Try 128,256,512 (512 is default)'
        updatedAt: '2023-11-17T23:08:58.668Z'
      numEdits: 1
      reactions: []
    id: 6557f1fe43c6fb21e46e9103
    type: comment
  author: cmp-nct
  content: 'Just as a general note:

    On single 3090 GPU you should get 20-25 tokens/second for Falcon 40B, anything
    below 20 means something is going wrong

    Your speed is less than half of where it should be, indicating something is wrong.
    I''d do a console client test as a start.


    When using llama.cpp (in 2023, this hopefully improves): Do not use multi GPU
    in any scenario you can avoid. Falcon 40B fits into single GPU at moderate quantization,
    use that.

    Multi GPU comes with extreme performance losses currently. Use it for Falcon 180B,
    there you have no choice.


    Sidenotes: using 100000000 gpu layers is useless. There are less than 100 layers,
    so you can make it a smaller number for convenience in reading.

    Batch size of 2048 is too high, that''s something you could try on a H100 but
    not on a consumer GPU. Try 128,256,512 (512 is default)'
  created_at: 2023-11-17 23:06:38+00:00
  edited: true
  hidden: false
  id: 6557f1fe43c6fb21e46e9103
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-11-18T18:46:55.000Z'
    data:
      edited: true
      editors:
      - Nafnlaus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9701256155967712
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: '<p>I''ve love a command line that would give me 20-25 tokens/sec if
          you have one.</p>

          <p>Note that I''ve since switched to the oasst version of this model, so
          I can only comment on that, but new flags have come out since I last wrote
          that, and adding in --numa --mlock --mul_mat_q boosted up my performance
          to 16-17 tokens/s on a single 3090.</p>

          <p>I could add a note that my 3090 has its power cut from 350W to 300W,
          but my understanding from CUDA benchmarks is that the performance hit from
          this is quite small relative to the power savings, just a couple percent,
          and it''s been my general experience that the impact seems unnoticeable.  And
          when running on two cards, the limit is inter-card bandwidth, not compute,
          anyway. Though I should probably measure it at some point and verify that
          I''m not shooting myself in the foot. [ED: my card is currently doing training,
          but going back up to 350W only boosts performance by 5%... which would equate
          to less than 1 extra token per second in inference)</p>

          <p>Re, running on multiple cards, my experience (without NVLink) has been
          that the hit is <em>usually</em> not that huge IF one of your cards is only
          running a small number of layers.  But the more layers are run on the second
          card, the more you get bandwidth bottlenecked.  And I can confirm that llama.cpp''s
          load balancing algorithm across multiple cards is garbage - some times it''ll
          do great and sometimes poorly.  The fewer layers you have on the second
          card, the less likely it''ll get locked into its poor balancing mode.  </p>

          <p>Anyway, I''ll be upgrading my second card (the 3060) to a 3090 shortly
          and adding NVLink, so it''ll probably worth a second round of benchmarks
          at that point.</p>

          '
        raw: "I've love a command line that would give me 20-25 tokens/sec if you\
          \ have one.\n\nNote that I've since switched to the oasst version of this\
          \ model, so I can only comment on that, but new flags have come out since\
          \ I last wrote that, and adding in --numa --mlock --mul_mat_q boosted up\
          \ my performance to 16-17 tokens/s on a single 3090.\n\nI could add a note\
          \ that my 3090 has its power cut from 350W to 300W, but my understanding\
          \ from CUDA benchmarks is that the performance hit from this is quite small\
          \ relative to the power savings, just a couple percent, and it's been my\
          \ general experience that the impact seems unnoticeable.  And when running\
          \ on two cards, the limit is inter-card bandwidth, not compute, anyway.\
          \ Though I should probably measure it at some point and verify that I'm\
          \ not shooting myself in the foot. [ED: my card is currently doing training,\
          \ but going back up to 350W only boosts performance by 5%... which would\
          \ equate to less than 1 extra token per second in inference)\n\nRe, running\
          \ on multiple cards, my experience (without NVLink) has been that the hit\
          \ is *usually* not that huge IF one of your cards is only running a small\
          \ number of layers.  But the more layers are run on the second card, the\
          \ more you get bandwidth bottlenecked.  And I can confirm that llama.cpp's\
          \ load balancing algorithm across multiple cards is garbage - some times\
          \ it'll do great and sometimes poorly.  The fewer layers you have on the\
          \ second card, the less likely it'll get locked into its poor balancing\
          \ mode.  \n\nAnyway, I'll be upgrading my second card (the 3060) to a 3090\
          \ shortly and adding NVLink, so it'll probably worth a second round of benchmarks\
          \ at that point."
        updatedAt: '2023-11-18T23:51:21.118Z'
      numEdits: 2
      reactions: []
    id: 6559069f029b03128bd93c3b
    type: comment
  author: Nafnlaus
  content: "I've love a command line that would give me 20-25 tokens/sec if you have\
    \ one.\n\nNote that I've since switched to the oasst version of this model, so\
    \ I can only comment on that, but new flags have come out since I last wrote that,\
    \ and adding in --numa --mlock --mul_mat_q boosted up my performance to 16-17\
    \ tokens/s on a single 3090.\n\nI could add a note that my 3090 has its power\
    \ cut from 350W to 300W, but my understanding from CUDA benchmarks is that the\
    \ performance hit from this is quite small relative to the power savings, just\
    \ a couple percent, and it's been my general experience that the impact seems\
    \ unnoticeable.  And when running on two cards, the limit is inter-card bandwidth,\
    \ not compute, anyway. Though I should probably measure it at some point and verify\
    \ that I'm not shooting myself in the foot. [ED: my card is currently doing training,\
    \ but going back up to 350W only boosts performance by 5%... which would equate\
    \ to less than 1 extra token per second in inference)\n\nRe, running on multiple\
    \ cards, my experience (without NVLink) has been that the hit is *usually* not\
    \ that huge IF one of your cards is only running a small number of layers.  But\
    \ the more layers are run on the second card, the more you get bandwidth bottlenecked.\
    \  And I can confirm that llama.cpp's load balancing algorithm across multiple\
    \ cards is garbage - some times it'll do great and sometimes poorly.  The fewer\
    \ layers you have on the second card, the less likely it'll get locked into its\
    \ poor balancing mode.  \n\nAnyway, I'll be upgrading my second card (the 3060)\
    \ to a 3090 shortly and adding NVLink, so it'll probably worth a second round\
    \ of benchmarks at that point."
  created_at: 2023-11-18 18:46:55+00:00
  edited: true
  hidden: false
  id: 6559069f029b03128bd93c3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-11-19T01:03:07.000Z'
    data:
      edited: true
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8935099244117737
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>Start with -b 256<br>Prepend $env:CUDA_VISIBLE_DEVICES = "0"; (or
          an export on linux) to ensure it''s single GPU</p>

          <p>Though given you cut your power target, that might be the reason. Imho
          that''s only useful for mining or 24/7 usage<br>I''d not give too much hopes
          on nvlink, stay on single GPU if performance matters to you</p>

          '
        raw: 'Start with -b 256

          Prepend $env:CUDA_VISIBLE_DEVICES = "0"; (or an export on linux) to ensure
          it''s single GPU


          Though given you cut your power target, that might be the reason. Imho that''s
          only useful for mining or 24/7 usage

          I''d not give too much hopes on nvlink, stay on single GPU if performance
          matters to you'
        updatedAt: '2023-11-19T01:03:47.617Z'
      numEdits: 1
      reactions: []
    id: 65595ecb9e9dfc1089e9ea10
    type: comment
  author: cmp-nct
  content: 'Start with -b 256

    Prepend $env:CUDA_VISIBLE_DEVICES = "0"; (or an export on linux) to ensure it''s
    single GPU


    Though given you cut your power target, that might be the reason. Imho that''s
    only useful for mining or 24/7 usage

    I''d not give too much hopes on nvlink, stay on single GPU if performance matters
    to you'
  created_at: 2023-11-19 01:03:07+00:00
  edited: true
  hidden: false
  id: 65595ecb9e9dfc1089e9ea10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-11-19T03:17:08.000Z'
    data:
      edited: true
      editors:
      - Nafnlaus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9280983209609985
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: '<blockquote>

          <p>Start with -b 256</p>

          </blockquote>

          <p>You can see in my command line above that I''m doing n_batch (equivalent
          to -b) 2048. Have also done low batches.</p>

          <blockquote>

          <p>Prepend $env:CUDA_VISIBLE_DEVICES = "0"; (or an export on linux) to ensure
          it''s single GPU</p>

          </blockquote>

          <p>Obviously I do this on my single-GPU tests   :)</p>

          <blockquote>

          <p>Though given you cut your power target, that might be the reason.</p>

          </blockquote>

          <p>As mentioned, at least with training, it''s only a 5% performance penalty,
          to save 17% on the GPU''s power consumption.  5% &lt;= 1 token per second.</p>

          <blockquote>

          <p>Imho that''s only useful for mining </p>

          </blockquote>

          <p>Not that...</p>

          <blockquote>

          <p>or 24/7 usage</p>

          </blockquote>

          <p>Bingo.  :)  I (A) train models, and (B) run inference to generate data
          to use to train models. Both mean 24/7 GPU usage.    The very reason why
          I use Falcon-40B is because they don''t lay any claim in their license to
          your generations like a lot of models (including Llama) do.  I''m training
          an ultrafast summarization model at the moment, just got started on a dataset
          for a series of probabilistic state machine models (0-shot or multi-shot
          analysis of probabilities or relation strengths, returning one or more floating
          point responses), and in general look to build a suite of fully open models
          for use in multi-agent systems.</p>

          <p>Thusfar, the only thing I''ve seen that we''re doing different (apart
          from me trying every single command line option under the sun) is that you''re
          running direct from llama.cpp, while I''m using the llama.cpp loader from
          text-generation-webui.</p>

          '
        raw: "> Start with -b 256\n\nYou can see in my command line above that I'm\
          \ doing n_batch (equivalent to -b) 2048. Have also done low batches.\n\n\
          > Prepend $env:CUDA_VISIBLE_DEVICES = \"0\"; (or an export on linux) to\
          \ ensure it's single GPU\n\nObviously I do this on my single-GPU tests \
          \  :)\n\n> Though given you cut your power target, that might be the reason.\n\
          \nAs mentioned, at least with training, it's only a 5% performance penalty,\
          \ to save 17% on the GPU's power consumption.  5% <= 1 token per second.\n\
          \n> Imho that's only useful for mining \n\nNot that...\n\n> or 24/7 usage\n\
          \nBingo.  :)  I (A) train models, and (B) run inference to generate data\
          \ to use to train models. Both mean 24/7 GPU usage.    The very reason why\
          \ I use Falcon-40B is because they don't lay any claim in their license\
          \ to your generations like a lot of models (including Llama) do.  I'm training\
          \ an ultrafast summarization model at the moment, just got started on a\
          \ dataset for a series of probabilistic state machine models (0-shot or\
          \ multi-shot analysis of probabilities or relation strengths, returning\
          \ one or more floating point responses), and in general look to build a\
          \ suite of fully open models for use in multi-agent systems.\n\nThusfar,\
          \ the only thing I've seen that we're doing different (apart from me trying\
          \ every single command line option under the sun) is that you're running\
          \ direct from llama.cpp, while I'm using the llama.cpp loader from text-generation-webui."
        updatedAt: '2023-11-19T03:22:14.657Z'
      numEdits: 1
      reactions: []
    id: 65597e346f460bf329e95bd0
    type: comment
  author: Nafnlaus
  content: "> Start with -b 256\n\nYou can see in my command line above that I'm doing\
    \ n_batch (equivalent to -b) 2048. Have also done low batches.\n\n> Prepend $env:CUDA_VISIBLE_DEVICES\
    \ = \"0\"; (or an export on linux) to ensure it's single GPU\n\nObviously I do\
    \ this on my single-GPU tests   :)\n\n> Though given you cut your power target,\
    \ that might be the reason.\n\nAs mentioned, at least with training, it's only\
    \ a 5% performance penalty, to save 17% on the GPU's power consumption.  5% <=\
    \ 1 token per second.\n\n> Imho that's only useful for mining \n\nNot that...\n\
    \n> or 24/7 usage\n\nBingo.  :)  I (A) train models, and (B) run inference to\
    \ generate data to use to train models. Both mean 24/7 GPU usage.    The very\
    \ reason why I use Falcon-40B is because they don't lay any claim in their license\
    \ to your generations like a lot of models (including Llama) do.  I'm training\
    \ an ultrafast summarization model at the moment, just got started on a dataset\
    \ for a series of probabilistic state machine models (0-shot or multi-shot analysis\
    \ of probabilities or relation strengths, returning one or more floating point\
    \ responses), and in general look to build a suite of fully open models for use\
    \ in multi-agent systems.\n\nThusfar, the only thing I've seen that we're doing\
    \ different (apart from me trying every single command line option under the sun)\
    \ is that you're running direct from llama.cpp, while I'm using the llama.cpp\
    \ loader from text-generation-webui."
  created_at: 2023-11-19 03:17:08+00:00
  edited: true
  hidden: false
  id: 65597e346f460bf329e95bd0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-11-19T04:37:37.000Z'
    data:
      edited: true
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9614080190658569
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>In my tests anything above -b 512 causes slowdowns (on my 4090 too)
          so I stick to smaller batches.<br>Though given you do 2k batches, I missed
          the obvious: you have a very large context. My context typically is not
          much larger than 2048.<br>Falcon models suffer from a moderate slowdown
          with context size, so if you run with large prompts I would assume that''s
          the main reason.<br>I did not dig into the reason what is going on in llama.cpp,
          my guess is an inefficiency in handling the KV cache and broadcasting.<br>This
          can get better over time, if Falcon stays relevant.</p>

          <p>In general Falcon 40B is a very good model, I prefer it over llama-2
          (which is also fully permissive licensed)</p>

          <p>P.S.<br>I also considered upgrades, I had a 3070 + 3090. I decided to
          go for the 4090 and would recommend that.<br>It''s considerably faster than
          the 3090 and it supports the modern APIs of Nvidia, including FP8 which
          is much faster than FP16 or FP32. So prompt ingestion speed could benefit
          it llama.cpp integrates it which isn''t that big a development. The lack
          of NVlink is there, sadly that''s deprecated on consumer hardware now. Nvidia
          protecting their endgame.</p>

          '
        raw: 'In my tests anything above -b 512 causes slowdowns (on my 4090 too)
          so I stick to smaller batches.

          Though given you do 2k batches, I missed the obvious: you have a very large
          context. My context typically is not much larger than 2048.

          Falcon models suffer from a moderate slowdown with context size, so if you
          run with large prompts I would assume that''s the main reason.

          I did not dig into the reason what is going on in llama.cpp, my guess is
          an inefficiency in handling the KV cache and broadcasting.

          This can get better over time, if Falcon stays relevant.


          In general Falcon 40B is a very good model, I prefer it over llama-2 (which
          is also fully permissive licensed)



          P.S.

          I also considered upgrades, I had a 3070 + 3090. I decided to go for the
          4090 and would recommend that.

          It''s considerably faster than the 3090 and it supports the modern APIs
          of Nvidia, including FP8 which is much faster than FP16 or FP32. So prompt
          ingestion speed could benefit it llama.cpp integrates it which isn''t that
          big a development. The lack of NVlink is there, sadly that''s deprecated
          on consumer hardware now. Nvidia protecting their endgame.'
        updatedAt: '2023-11-19T04:40:20.011Z'
      numEdits: 2
      reactions: []
    id: 6559911130ad83ad6b47872f
    type: comment
  author: cmp-nct
  content: 'In my tests anything above -b 512 causes slowdowns (on my 4090 too) so
    I stick to smaller batches.

    Though given you do 2k batches, I missed the obvious: you have a very large context.
    My context typically is not much larger than 2048.

    Falcon models suffer from a moderate slowdown with context size, so if you run
    with large prompts I would assume that''s the main reason.

    I did not dig into the reason what is going on in llama.cpp, my guess is an inefficiency
    in handling the KV cache and broadcasting.

    This can get better over time, if Falcon stays relevant.


    In general Falcon 40B is a very good model, I prefer it over llama-2 (which is
    also fully permissive licensed)



    P.S.

    I also considered upgrades, I had a 3070 + 3090. I decided to go for the 4090
    and would recommend that.

    It''s considerably faster than the 3090 and it supports the modern APIs of Nvidia,
    including FP8 which is much faster than FP16 or FP32. So prompt ingestion speed
    could benefit it llama.cpp integrates it which isn''t that big a development.
    The lack of NVlink is there, sadly that''s deprecated on consumer hardware now.
    Nvidia protecting their endgame.'
  created_at: 2023-11-19 04:37:37+00:00
  edited: true
  hidden: false
  id: 6559911130ad83ad6b47872f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-11-19T11:47:32.000Z'
    data:
      edited: false
      editors:
      - Nafnlaus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.939244270324707
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: '<p>LLaMA 2 is "relatively" permissively licensed but they ban use of
          outputs to train anything other than other LLaMA models, so it''s viral,
          which leads to it dominating usage among people training their own models,
          iteratively. Which is where the other part of the license hits: if any tool
          ever DOES make it big - no matter how many iterations down the road - Meta
          retains commercial rights to that.  Very clever strategy, IMHO....</p>

          <p>I''m not going to use anything to generate outputs where they impose
          license restrictions on my outputs .  Hence, Falcon 40B   :) I want my training
          datasets to be fully free and not infect anyone else''s projects.</p>

          '
        raw: 'LLaMA 2 is "relatively" permissively licensed but they ban use of outputs
          to train anything other than other LLaMA models, so it''s viral, which leads
          to it dominating usage among people training their own models, iteratively.
          Which is where the other part of the license hits: if any tool ever DOES
          make it big - no matter how many iterations down the road - Meta retains
          commercial rights to that.  Very clever strategy, IMHO....


          I''m not going to use anything to generate outputs where they impose license
          restrictions on my outputs .  Hence, Falcon 40B   :) I want my training
          datasets to be fully free and not infect anyone else''s projects.'
        updatedAt: '2023-11-19T11:47:32.766Z'
      numEdits: 0
      reactions: []
    id: 6559f5d4180749bf32ecc6e4
    type: comment
  author: Nafnlaus
  content: 'LLaMA 2 is "relatively" permissively licensed but they ban use of outputs
    to train anything other than other LLaMA models, so it''s viral, which leads to
    it dominating usage among people training their own models, iteratively. Which
    is where the other part of the license hits: if any tool ever DOES make it big
    - no matter how many iterations down the road - Meta retains commercial rights
    to that.  Very clever strategy, IMHO....


    I''m not going to use anything to generate outputs where they impose license restrictions
    on my outputs .  Hence, Falcon 40B   :) I want my training datasets to be fully
    free and not infect anyone else''s projects.'
  created_at: 2023-11-19 11:47:32+00:00
  edited: false
  hidden: false
  id: 6559f5d4180749bf32ecc6e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-11-19T12:59:14.000Z'
    data:
      edited: true
      editors:
      - Nafnlaus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7819488644599915
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: '<p>I think I''ve discovered the difference between our experiences.</p>

          <p>When you look at something like this (this is running the related oasst
          Falcon 40B model''s Q3_K_M, the largest model that I can fit into one card,
          and using the --numa --mlock --mul_mat_q flags that as mentioned increase
          performance over my initial results):</p>

          <p>llama_print_timings:        load time =   522.32 ms<br>llama_print_timings:      sample
          time =   282.65 ms /   287 runs   (    0.98 ms per token,  1015.40 tokens
          per second)<br>llama_print_timings: prompt eval time =   529.75 ms /   107
          tokens (    4.95 ms per token,   201.98 tokens per second)<br>llama_print_timings:        eval
          time = 14150.93 ms /   286 runs   (   49.48 ms per token,    20.21 tokens
          per second)<br>llama_print_timings:       total time = 16421.51 ms<br>127.0.0.1
          - - [19/Nov/2023 12:13:14] "POST /api/v1/chat HTTP/1.1" 200 -<br>Llama.generate:
          prefix-match hit</p>

          <p>Looking at the above, would you read that as "20.21 tokens per second"?  And
          then add in 5% performance for the cut power on the card, and get 21,2 tokens
          per second, right in your 20-25 tokens/s?</p>

          <p>I don''t.  I say =14150.93 / 49.48 = 286 tokens ("286 runs"); 286 tokens
          / 16,421ms total time = 17,42 tokens/ms.</p>

          <p>Furthermore, when averaging net performance, I don''t just average the
          above (like "17,42") - I add together total elapsed time, and total tokens
          generated.  So that quick generations aren''t overweighted vs. long-generations
          where most of the time is actually spent.</p>

          <p>Basically, my interest is in real-world performance, how many tokens
          you <em>actually</em> get per second of GPU usage.  Not just some number
          that only describes one part of the run.</p>

          '
        raw: 'I think I''ve discovered the difference between our experiences.


          When you look at something like this (this is running the related oasst
          Falcon 40B model''s Q3_K_M, the largest model that I can fit into one card,
          and using the --numa --mlock --mul_mat_q flags that as mentioned increase
          performance over my initial results):


          llama_print_timings:        load time =   522.32 ms

          llama_print_timings:      sample time =   282.65 ms /   287 runs   (    0.98
          ms per token,  1015.40 tokens per second)

          llama_print_timings: prompt eval time =   529.75 ms /   107 tokens (    4.95
          ms per token,   201.98 tokens per second)

          llama_print_timings:        eval time = 14150.93 ms /   286 runs   (   49.48
          ms per token,    20.21 tokens per second)

          llama_print_timings:       total time = 16421.51 ms

          127.0.0.1 - - [19/Nov/2023 12:13:14] "POST /api/v1/chat HTTP/1.1" 200 -

          Llama.generate: prefix-match hit


          Looking at the above, would you read that as "20.21 tokens per second"?  And
          then add in 5% performance for the cut power on the card, and get 21,2 tokens
          per second, right in your 20-25 tokens/s?


          I don''t.  I say =14150.93 / 49.48 = 286 tokens ("286 runs"); 286 tokens
          / 16,421ms total time = 17,42 tokens/ms.


          Furthermore, when averaging net performance, I don''t just average the above
          (like "17,42") - I add together total elapsed time, and total tokens generated.  So
          that quick generations aren''t overweighted vs. long-generations where most
          of the time is actually spent.


          Basically, my interest is in real-world performance, how many tokens you
          *actually* get per second of GPU usage.  Not just some number that only
          describes one part of the run.'
        updatedAt: '2023-11-19T13:00:07.315Z'
      numEdits: 1
      reactions: []
    id: 655a06a2bf604a773258998a
    type: comment
  author: Nafnlaus
  content: 'I think I''ve discovered the difference between our experiences.


    When you look at something like this (this is running the related oasst Falcon
    40B model''s Q3_K_M, the largest model that I can fit into one card, and using
    the --numa --mlock --mul_mat_q flags that as mentioned increase performance over
    my initial results):


    llama_print_timings:        load time =   522.32 ms

    llama_print_timings:      sample time =   282.65 ms /   287 runs   (    0.98 ms
    per token,  1015.40 tokens per second)

    llama_print_timings: prompt eval time =   529.75 ms /   107 tokens (    4.95 ms
    per token,   201.98 tokens per second)

    llama_print_timings:        eval time = 14150.93 ms /   286 runs   (   49.48 ms
    per token,    20.21 tokens per second)

    llama_print_timings:       total time = 16421.51 ms

    127.0.0.1 - - [19/Nov/2023 12:13:14] "POST /api/v1/chat HTTP/1.1" 200 -

    Llama.generate: prefix-match hit


    Looking at the above, would you read that as "20.21 tokens per second"?  And then
    add in 5% performance for the cut power on the card, and get 21,2 tokens per second,
    right in your 20-25 tokens/s?


    I don''t.  I say =14150.93 / 49.48 = 286 tokens ("286 runs"); 286 tokens / 16,421ms
    total time = 17,42 tokens/ms.


    Furthermore, when averaging net performance, I don''t just average the above (like
    "17,42") - I add together total elapsed time, and total tokens generated.  So
    that quick generations aren''t overweighted vs. long-generations where most of
    the time is actually spent.


    Basically, my interest is in real-world performance, how many tokens you *actually*
    get per second of GPU usage.  Not just some number that only describes one part
    of the run.'
  created_at: 2023-11-19 12:59:14+00:00
  edited: true
  hidden: false
  id: 655a06a2bf604a773258998a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: maddes8cht/ehartford-WizardLM-Uncensored-Falcon-40b-gguf
repo_type: model
status: open
target_branch: null
title: Some experience with using this model
