!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nafnlaus
conflicting_files: null
created_at: 2023-09-26 00:32:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-09-26T01:32:58.000Z'
    data:
      edited: false
      editors:
      - Nafnlaus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9653210043907166
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: '<p>I''m running ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q3_K_S.gguf
          (as it''s the largest I can fit into a 24GB card, so I don''t have to also
          occupy my secondary card), and it''s running great!</p>

          <p>I had previously run a task on TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ.
          I had to modify it a good bit as this model tends to want to "elaborate
          on its input using its knowledge of the topic" when I wanted it to only
          act on what the input text said.  And a couple other issues, like it tending
          to only focus on the end of the input text.  But I was largely able to fix
          them via prompt adjustments.  Now it''s making nice, neatly-formatted, consistent
          responses, and overall the output is better than TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ.  Very
          nice!</p>

          <p>BTW, if you ever feel fit to do any "lightweight" models (e.g. just a
          couple billion parameters), that would be appreciated.  :)  They''re nice
          to use as a base for training new, highly-specialized models. You could
          even just take a heavyweight model and strip out deep layers then retrain
          for a bit - I''ve seen this done with a (non-open) model and it worked well.</p>

          '
        raw: "I'm running ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q3_K_S.gguf\
          \ (as it's the largest I can fit into a 24GB card, so I don't have to also\
          \ occupy my secondary card), and it's running great!\r\n\r\nI had previously\
          \ run a task on TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ. I had to modify\
          \ it a good bit as this model tends to want to \"elaborate on its input\
          \ using its knowledge of the topic\" when I wanted it to only act on what\
          \ the input text said.  And a couple other issues, like it tending to only\
          \ focus on the end of the input text.  But I was largely able to fix them\
          \ via prompt adjustments.  Now it's making nice, neatly-formatted, consistent\
          \ responses, and overall the output is better than TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ.\
          \  Very nice!\r\n\r\nBTW, if you ever feel fit to do any \"lightweight\"\
          \ models (e.g. just a couple billion parameters), that would be appreciated.\
          \  :)  They're nice to use as a base for training new, highly-specialized\
          \ models. You could even just take a heavyweight model and strip out deep\
          \ layers then retrain for a bit - I've seen this done with a (non-open)\
          \ model and it worked well."
        updatedAt: '2023-09-26T01:32:58.408Z'
      numEdits: 0
      reactions: []
    id: 651234ca301678dc97cbfada
    type: comment
  author: Nafnlaus
  content: "I'm running ggml-ehartford-WizardLM-Uncensored-Falcon-40b-Q3_K_S.gguf\
    \ (as it's the largest I can fit into a 24GB card, so I don't have to also occupy\
    \ my secondary card), and it's running great!\r\n\r\nI had previously run a task\
    \ on TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ. I had to modify it a good bit\
    \ as this model tends to want to \"elaborate on its input using its knowledge\
    \ of the topic\" when I wanted it to only act on what the input text said.  And\
    \ a couple other issues, like it tending to only focus on the end of the input\
    \ text.  But I was largely able to fix them via prompt adjustments.  Now it's\
    \ making nice, neatly-formatted, consistent responses, and overall the output\
    \ is better than TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ.  Very nice!\r\n\r\
    \nBTW, if you ever feel fit to do any \"lightweight\" models (e.g. just a couple\
    \ billion parameters), that would be appreciated.  :)  They're nice to use as\
    \ a base for training new, highly-specialized models. You could even just take\
    \ a heavyweight model and strip out deep layers then retrain for a bit - I've\
    \ seen this done with a (non-open) model and it worked well."
  created_at: 2023-09-26 00:32:58+00:00
  edited: false
  hidden: false
  id: 651234ca301678dc97cbfada
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64314d56dd466752c73bce33/Hny4H26P7Zg0PbET-iYH-.jpeg?w=200&h=200&f=face
      fullname: Mathias Bachmann
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: maddes8cht
      type: user
    createdAt: '2023-09-26T05:24:26.000Z'
    data:
      edited: false
      editors:
      - maddes8cht
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9652589559555054
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64314d56dd466752c73bce33/Hny4H26P7Zg0PbET-iYH-.jpeg?w=200&h=200&f=face
          fullname: Mathias Bachmann
          isHf: false
          isPro: false
          name: maddes8cht
          type: user
        html: '<blockquote>

          <p>BTW, if you ever feel fit to do any "lightweight" models (e.g. just a
          couple billion parameters), that would be appreciated. :) </p>

          </blockquote>

          <p>I asked for support of the falcon 1B and 7b RW variants in Llama.cpp,
          ut they seem to bee too different  from the standard Falcon to be easily
          converted to gguf right now. I know there are other "small" models and will
          have a look soon.<br>:)</p>

          '
        raw: "> BTW, if you ever feel fit to do any \"lightweight\" models (e.g. just\
          \ a couple billion parameters), that would be appreciated. :) \n\nI asked\
          \ for support of the falcon 1B and 7b RW variants in Llama.cpp, ut they\
          \ seem to bee too different  from the standard Falcon to be easily converted\
          \ to gguf right now. I know there are other \"small\" models and will have\
          \ a look soon.\n:)"
        updatedAt: '2023-09-26T05:24:26.485Z'
      numEdits: 0
      reactions: []
    id: 65126b0aec3eef908a98da97
    type: comment
  author: maddes8cht
  content: "> BTW, if you ever feel fit to do any \"lightweight\" models (e.g. just\
    \ a couple billion parameters), that would be appreciated. :) \n\nI asked for\
    \ support of the falcon 1B and 7b RW variants in Llama.cpp, ut they seem to bee\
    \ too different  from the standard Falcon to be easily converted to gguf right\
    \ now. I know there are other \"small\" models and will have a look soon.\n:)"
  created_at: 2023-09-26 04:24:26+00:00
  edited: false
  hidden: false
  id: 65126b0aec3eef908a98da97
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: maddes8cht/ehartford-WizardLM-Uncensored-Falcon-40b-gguf
repo_type: model
status: open
target_branch: null
title: Thanks for this!
