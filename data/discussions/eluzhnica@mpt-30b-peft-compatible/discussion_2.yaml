!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jdm959
conflicting_files: null
created_at: 2023-07-07 15:24:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/92dc12f005b74bb0aa37e18fdc6752f1.svg
      fullname: Jon Matthews
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdm959
      type: user
    createdAt: '2023-07-07T16:24:39.000Z'
    data:
      edited: false
      editors:
      - jdm959
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44480177760124207
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/92dc12f005b74bb0aa37e18fdc6752f1.svg
          fullname: Jon Matthews
          isHf: false
          isPro: false
          name: jdm959
          type: user
        html: '<p>I''m using peft and trainer to fine tune the model and got the following
          error:</p>

          <blockquote>

          <blockquote>

          <blockquote>

          <p>trainer.train()<br>  0%|                                                                                                                                                           |
          0/10 [20:45:22&lt;?, ?it/s]<br>  0%|                                                                                                                                                              |
          0/10 [00:00&lt;?, ?it/s]Traceback (most recent call last):<br>  File "",
          line 1, in <br>  File "/transformers/trainer.py", line 1537, in train<br>    return
          inner_training_loop(<br>  File "/transformers/trainer.py", line 1811, in
          _inner_training_loop<br>    tr_loss_step = self.training_step(model, inputs)<br>  File
          "/transformers/trainer.py", line 2632, in training_step<br>    loss = self.compute_loss(model,
          inputs)<br>  File "/transformers/trainer.py", line 2657, in compute_loss<br>    outputs
          = model(**inputs)<br>  File "/torch/nn/modules/module.py", line 1502, in
          _wrapped_call_impl<br>    return self._call_impl(*args, **kwargs)<br>  File
          "/torch/nn/modules/module.py", line 1511, in _call_impl<br>    return forward_call(*args,
          **kwargs)<br>  File "/torch/nn/parallel/data_parallel.py", line 181, in
          forward<br>    outputs = self.parallel_apply(replicas, inputs, module_kwargs)<br>  File
          "/torch/nn/parallel/data_parallel.py", line 196, in parallel_apply<br>    return
          parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])<br>  File
          "/torch/nn/parallel/parallel_apply.py", line 110, in parallel_apply<br>    output.reraise()<br>  File
          "/torch/_utils.py", line 658, in reraise<br>    raise exception<br>TypeError:
          Caught TypeError in replica 0 on device 0.<br>Original Traceback (most recent
          call last):<br>  File "/torch/nn/parallel/parallel_apply.py", line 85, in
          _worker<br>    output = module(*input, **kwargs)<br>  File "/torch/nn/modules/module.py",
          line 1502, in _wrapped_call_impl<br>    return self._call_impl(*args, **kwargs)<br>  File
          "/torch/nn/modules/module.py", line 1511, in _call_impl<br>    return forward_call(*args,
          **kwargs)<br>  File "/peft/peft_model.py", line 739, in forward<br>    return
          self.base_model(<br>  File "/torch/nn/modules/module.py", line 1502, in
          _wrapped_call_impl<br>    return self._call_impl(*args, **kwargs)<br>  File
          "/torch/nn/modules/module.py", line 1511, in _call_impl<br>    return forward_call(*args,
          **kwargs)<br>  File "/peft/peft_model.py", line 739, in forward<br>    return
          self.base_model(<br>  File "/torch/nn/modules/module.py", line 1502, in
          _wrapped_call_impl<br>    return self._call_impl(*args, **kwargs)<br>  File
          "/torch/nn/modules/module.py", line 1511, in _call_impl<br>    return forward_call(*args,
          **kwargs)<br>  File "/accelerate/hooks.py", line 165, in new_forward<br>    output
          = old_forward(*args, **kwargs)<br>TypeError: MPTForCausalLM.forward() got
          an unexpected keyword argument ''inputs_embeds''</p>

          </blockquote>

          </blockquote>

          </blockquote>

          <p>Versions:<br>transformers @ git+<a rel="nofollow" href="https://github.com/huggingface/transformers.git@70c79940957fb25b54bd1b106935c756b90345eb">https://github.com/huggingface/transformers.git@70c79940957fb25b54bd1b106935c756b90345eb</a><br>torch==2.1.0.dev20230601<br>peft
          @ git+<a rel="nofollow" href="https://github.com/huggingface/peft.git@189a6b8e357ecda05ccde13999e4c35759596a67">https://github.com/huggingface/peft.git@189a6b8e357ecda05ccde13999e4c35759596a67</a></p>

          '
        raw: "I'm using peft and trainer to fine tune the model and got the following\
          \ error:\r\n\r\n>>> trainer.train()\r\n  0%|                           \
          \                                                                      \
          \                                                          | 0/10 [20:45:22<?,\
          \ ?it/s]\r\n  0%|                                                      \
          \                                                                      \
          \                                  | 0/10 [00:00<?, ?it/s]Traceback (most\
          \ recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File\
          \ \"<redacted>/transformers/trainer.py\", line 1537, in train\r\n    return\
          \ inner_training_loop(\r\n  File \"<redacted>/transformers/trainer.py\"\
          , line 1811, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model,\
          \ inputs)\r\n  File \"<redacted>/transformers/trainer.py\", line 2632, in\
          \ training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File\
          \ \"<redacted>/transformers/trainer.py\", line 2657, in compute_loss\r\n\
          \    outputs = model(**inputs)\r\n  File \"<redacted>/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"<redacted>/torch/nn/modules/module.py\", line 1511,\
          \ in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"\
          <redacted>/torch/nn/parallel/data_parallel.py\", line 181, in forward\r\n\
          \    outputs = self.parallel_apply(replicas, inputs, module_kwargs)\r\n\
          \  File \"<redacted>/torch/nn/parallel/data_parallel.py\", line 196, in\
          \ parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs,\
          \ self.device_ids[:len(replicas)])\r\n  File \"<redacted>/torch/nn/parallel/parallel_apply.py\"\
          , line 110, in parallel_apply\r\n    output.reraise()\r\n  File \"<redacted>/torch/_utils.py\"\
          , line 658, in reraise\r\n    raise exception\r\nTypeError: Caught TypeError\
          \ in replica 0 on device 0.\r\nOriginal Traceback (most recent call last):\r\
          \n  File \"<redacted>/torch/nn/parallel/parallel_apply.py\", line 85, in\
          \ _worker\r\n    output = module(*input, **kwargs)\r\n  File \"<redacted>/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"<redacted>/torch/nn/modules/module.py\", line 1511,\
          \ in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"\
          <redacted>/peft/peft_model.py\", line 739, in forward\r\n    return self.base_model(\r\
          \n  File \"<redacted>/torch/nn/modules/module.py\", line 1502, in _wrapped_call_impl\r\
          \n    return self._call_impl(*args, **kwargs)\r\n  File \"<redacted>/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"<redacted>/peft/peft_model.py\", line 739, in forward\r\n   \
          \ return self.base_model(\r\n  File \"<redacted>/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"<redacted>/torch/nn/modules/module.py\", line 1511,\
          \ in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"\
          <redacted>/accelerate/hooks.py\", line 165, in new_forward\r\n    output\
          \ = old_forward(*args, **kwargs)\r\nTypeError: MPTForCausalLM.forward()\
          \ got an unexpected keyword argument 'inputs_embeds'\r\n\r\n\r\nVersions:\
          \ \r\ntransformers @ git+https://github.com/huggingface/transformers.git@70c79940957fb25b54bd1b106935c756b90345eb\r\
          \ntorch==2.1.0.dev20230601\r\npeft @ git+https://github.com/huggingface/peft.git@189a6b8e357ecda05ccde13999e4c35759596a67\r\
          \n"
        updatedAt: '2023-07-07T16:24:39.937Z'
      numEdits: 0
      reactions: []
    id: 64a83c4703835e13f956b1c3
    type: comment
  author: jdm959
  content: "I'm using peft and trainer to fine tune the model and got the following\
    \ error:\r\n\r\n>>> trainer.train()\r\n  0%|                                 \
    \                                                                            \
    \                                              | 0/10 [20:45:22<?, ?it/s]\r\n\
    \  0%|                                                                       \
    \                                                                            \
    \           | 0/10 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File\
    \ \"<stdin>\", line 1, in <module>\r\n  File \"<redacted>/transformers/trainer.py\"\
    , line 1537, in train\r\n    return inner_training_loop(\r\n  File \"<redacted>/transformers/trainer.py\"\
    , line 1811, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model,\
    \ inputs)\r\n  File \"<redacted>/transformers/trainer.py\", line 2632, in training_step\r\
    \n    loss = self.compute_loss(model, inputs)\r\n  File \"<redacted>/transformers/trainer.py\"\
    , line 2657, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"<redacted>/torch/nn/modules/module.py\"\
    , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"<redacted>/torch/nn/modules/module.py\", line 1511, in _call_impl\r\
    \n    return forward_call(*args, **kwargs)\r\n  File \"<redacted>/torch/nn/parallel/data_parallel.py\"\
    , line 181, in forward\r\n    outputs = self.parallel_apply(replicas, inputs,\
    \ module_kwargs)\r\n  File \"<redacted>/torch/nn/parallel/data_parallel.py\",\
    \ line 196, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs,\
    \ self.device_ids[:len(replicas)])\r\n  File \"<redacted>/torch/nn/parallel/parallel_apply.py\"\
    , line 110, in parallel_apply\r\n    output.reraise()\r\n  File \"<redacted>/torch/_utils.py\"\
    , line 658, in reraise\r\n    raise exception\r\nTypeError: Caught TypeError in\
    \ replica 0 on device 0.\r\nOriginal Traceback (most recent call last):\r\n  File\
    \ \"<redacted>/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\r\n\
    \    output = module(*input, **kwargs)\r\n  File \"<redacted>/torch/nn/modules/module.py\"\
    , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"<redacted>/torch/nn/modules/module.py\", line 1511, in _call_impl\r\
    \n    return forward_call(*args, **kwargs)\r\n  File \"<redacted>/peft/peft_model.py\"\
    , line 739, in forward\r\n    return self.base_model(\r\n  File \"<redacted>/torch/nn/modules/module.py\"\
    , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"<redacted>/torch/nn/modules/module.py\", line 1511, in _call_impl\r\
    \n    return forward_call(*args, **kwargs)\r\n  File \"<redacted>/peft/peft_model.py\"\
    , line 739, in forward\r\n    return self.base_model(\r\n  File \"<redacted>/torch/nn/modules/module.py\"\
    , line 1502, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"<redacted>/torch/nn/modules/module.py\", line 1511, in _call_impl\r\
    \n    return forward_call(*args, **kwargs)\r\n  File \"<redacted>/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\nTypeError:\
    \ MPTForCausalLM.forward() got an unexpected keyword argument 'inputs_embeds'\r\
    \n\r\n\r\nVersions: \r\ntransformers @ git+https://github.com/huggingface/transformers.git@70c79940957fb25b54bd1b106935c756b90345eb\r\
    \ntorch==2.1.0.dev20230601\r\npeft @ git+https://github.com/huggingface/peft.git@189a6b8e357ecda05ccde13999e4c35759596a67\r\
    \n"
  created_at: 2023-07-07 15:24:39+00:00
  edited: false
  hidden: false
  id: 64a83c4703835e13f956b1c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/92dc12f005b74bb0aa37e18fdc6752f1.svg
      fullname: Jon Matthews
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdm959
      type: user
    createdAt: '2023-07-07T17:15:43.000Z'
    data:
      edited: false
      editors:
      - jdm959
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8916844725608826
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/92dc12f005b74bb0aa37e18fdc6752f1.svg
          fullname: Jon Matthews
          isHf: false
          isPro: false
          name: jdm959
          type: user
        html: '<p>Turns out that uninstalling peft and installing the latest commit
          worked</p>

          '
        raw: Turns out that uninstalling peft and installing the latest commit worked
        updatedAt: '2023-07-07T17:15:43.136Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - eluzhnica
      relatedEventId: 64a8483fa347b9571974f084
    id: 64a8483fa347b9571974f083
    type: comment
  author: jdm959
  content: Turns out that uninstalling peft and installing the latest commit worked
  created_at: 2023-07-07 16:15:43+00:00
  edited: false
  hidden: false
  id: 64a8483fa347b9571974f083
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/92dc12f005b74bb0aa37e18fdc6752f1.svg
      fullname: Jon Matthews
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdm959
      type: user
    createdAt: '2023-07-07T17:15:43.000Z'
    data:
      status: closed
    id: 64a8483fa347b9571974f084
    type: status-change
  author: jdm959
  created_at: 2023-07-07 16:15:43+00:00
  id: 64a8483fa347b9571974f084
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: eluzhnica/mpt-30b-peft-compatible
repo_type: model
status: closed
target_branch: null
title: 'TypeError: MPTForCausalLM.forward() got an unexpected keyword argument ''inputs_embeds'''
