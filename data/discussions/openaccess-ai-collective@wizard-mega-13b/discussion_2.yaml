!!python/object:huggingface_hub.community.DiscussionWithDetails
author: polymer
conflicting_files: null
created_at: 2023-05-15 12:25:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
      fullname: polymer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: polymer
      type: user
    createdAt: '2023-05-15T13:25:16.000Z'
    data:
      edited: true
      editors:
      - polymer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
          fullname: polymer
          isHf: false
          isPro: false
          name: polymer
          type: user
        html: "<p>Reading the configuration files and provided examples, this model\
          \ seems to be the result of a rank-8 LoRA with the given datasets. However,\
          \ it isn\u2019t clear what the fine-tuned formatting is supposed to be,\
          \ seeing as how even the provided examples are a mix of the current formats.</p>\n\
          <p>Axolotl appears to <em>ingest</em> all these dataset types differently,\
          \ but was the training done with a unified format, or separate formats?\
          \ (Vicuna V0? V1.1? Alpaca?)</p>\n<p>Starting a discussion for hopefully\
          \ some clarification or a correction of my understanding.</p>\n<p>Edit:\
          \ The \u201Cadapter\u201D parameter seems to have been left empty, so I\
          \ presume this means that it wasn\u2019t actually a LoRA, but a native fine-tune.</p>\n"
        raw: "Reading the configuration files and provided examples, this model seems\
          \ to be the result of a rank-8 LoRA with the given datasets. However, it\
          \ isn\u2019t clear what the fine-tuned formatting is supposed to be, seeing\
          \ as how even the provided examples are a mix of the current formats.\n\n\
          Axolotl appears to *ingest* all these dataset types differently, but was\
          \ the training done with a unified format, or separate formats? (Vicuna\
          \ V0? V1.1? Alpaca?)\n\nStarting a discussion for hopefully some clarification\
          \ or a correction of my understanding.\n\nEdit: The \u201Cadapter\u201D\
          \ parameter seems to have been left empty, so I presume this means that\
          \ it wasn\u2019t actually a LoRA, but a native fine-tune."
        updatedAt: '2023-05-15T13:31:53.826Z'
      numEdits: 1
      reactions: []
    id: 646232bc3c6fd465dc0dcae8
    type: comment
  author: polymer
  content: "Reading the configuration files and provided examples, this model seems\
    \ to be the result of a rank-8 LoRA with the given datasets. However, it isn\u2019\
    t clear what the fine-tuned formatting is supposed to be, seeing as how even the\
    \ provided examples are a mix of the current formats.\n\nAxolotl appears to *ingest*\
    \ all these dataset types differently, but was the training done with a unified\
    \ format, or separate formats? (Vicuna V0? V1.1? Alpaca?)\n\nStarting a discussion\
    \ for hopefully some clarification or a correction of my understanding.\n\nEdit:\
    \ The \u201Cadapter\u201D parameter seems to have been left empty, so I presume\
    \ this means that it wasn\u2019t actually a LoRA, but a native fine-tune."
  created_at: 2023-05-15 12:25:16+00:00
  edited: true
  hidden: false
  id: 646232bc3c6fd465dc0dcae8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-05-15T13:43:39.000Z'
    data:
      edited: false
      editors:
      - winglian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: '<p>This was a native fine tune of all the weights. the lora parameters
          were left in the config just as a copy paste. One could easily recreate
          a lora version of this simply by setting <code>adapter: lora</code>. Correct,
          we use all the dataset types, and train according to each dataset''s original
          training prompt format. A plan is in the works to try to force them all
          into a format like open-assistant''s proposal here: <a rel="nofollow" href="https://github.com/LAION-AI/Open-Assistant/blob/main/model/MESSAGE_AND_TOKEN_FORMAT.md">https://github.com/LAION-AI/Open-Assistant/blob/main/model/MESSAGE_AND_TOKEN_FORMAT.md</a></p>

          '
        raw: 'This was a native fine tune of all the weights. the lora parameters
          were left in the config just as a copy paste. One could easily recreate
          a lora version of this simply by setting `adapter: lora`. Correct, we use
          all the dataset types, and train according to each dataset''s original training
          prompt format. A plan is in the works to try to force them all into a format
          like open-assistant''s proposal here: https://github.com/LAION-AI/Open-Assistant/blob/main/model/MESSAGE_AND_TOKEN_FORMAT.md'
        updatedAt: '2023-05-15T13:43:39.736Z'
      numEdits: 0
      reactions: []
    id: 6462370bcce92c7d882b1db7
    type: comment
  author: winglian
  content: 'This was a native fine tune of all the weights. the lora parameters were
    left in the config just as a copy paste. One could easily recreate a lora version
    of this simply by setting `adapter: lora`. Correct, we use all the dataset types,
    and train according to each dataset''s original training prompt format. A plan
    is in the works to try to force them all into a format like open-assistant''s
    proposal here: https://github.com/LAION-AI/Open-Assistant/blob/main/model/MESSAGE_AND_TOKEN_FORMAT.md'
  created_at: 2023-05-15 12:43:39+00:00
  edited: false
  hidden: false
  id: 6462370bcce92c7d882b1db7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
      fullname: polymer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: polymer
      type: user
    createdAt: '2023-05-15T15:03:53.000Z'
    data:
      edited: true
      editors:
      - polymer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
          fullname: polymer
          isHf: false
          isPro: false
          name: polymer
          type: user
        html: "<p>I see. If there are indeed plans to unifying the prompting, there\
          \ may be more value in finding a more plain conversational format (like\
          \ Vicuna V1.1), as the special added tokens seem to cause accessibility\
          \ issues for users (see llama.cpp, where it isn\u2019t a simple task for\
          \ average users to add special tokens).</p>\n<p>For example, the internal\
          \ (raw) style in Vicuna V1.1 is as follows:</p>\n<p>A chat between \u2026\
          \ answers questions. USER: Hello! ASSISTANT: Hi, how can I help you today?USER:\
          \ What is 2 + 2 equal to? ASSISTANT: 2 + 2 is equal to 4.</p>\n<p>On a side\
          \ note, when using said V1.1 formatting on wizard-mega-13b, I find that\
          \ the model currently generates \u201C&lt;/s&gt;\u201D as literal text tokens\
          \ (sometimes alongside the correct end-of-stream token) instead of just\
          \ a single end-of-stream token. Tokenization issues may be present in the\
          \ Axolotl code.</p>\n"
        raw: "I see. If there are indeed plans to unifying the prompting, there may\
          \ be more value in finding a more plain conversational format (like Vicuna\
          \ V1.1), as the special added tokens seem to cause accessibility issues\
          \ for users (see llama.cpp, where it isn\u2019t a simple task for average\
          \ users to add special tokens).\n\nFor example, the internal (raw) style\
          \ in Vicuna V1.1 is as follows:\n\nA chat between \u2026 answers questions.\
          \ USER: Hello! ASSISTANT: Hi, how can I help you today?</s>USER: What is\
          \ 2 + 2 equal to? ASSISTANT: 2 + 2 is equal to 4.</s>\n\nOn a side note,\
          \ when using said V1.1 formatting on wizard-mega-13b, I find that the model\
          \ currently generates \u201C\\</s>\u201D as literal text tokens (sometimes\
          \ alongside the correct end-of-stream token) instead of just a single end-of-stream\
          \ token. Tokenization issues may be present in the Axolotl code."
        updatedAt: '2023-05-15T15:23:53.187Z'
      numEdits: 1
      reactions: []
    id: 646249d948e13890ea524e76
    type: comment
  author: polymer
  content: "I see. If there are indeed plans to unifying the prompting, there may\
    \ be more value in finding a more plain conversational format (like Vicuna V1.1),\
    \ as the special added tokens seem to cause accessibility issues for users (see\
    \ llama.cpp, where it isn\u2019t a simple task for average users to add special\
    \ tokens).\n\nFor example, the internal (raw) style in Vicuna V1.1 is as follows:\n\
    \nA chat between \u2026 answers questions. USER: Hello! ASSISTANT: Hi, how can\
    \ I help you today?</s>USER: What is 2 + 2 equal to? ASSISTANT: 2 + 2 is equal\
    \ to 4.</s>\n\nOn a side note, when using said V1.1 formatting on wizard-mega-13b,\
    \ I find that the model currently generates \u201C\\</s>\u201D as literal text\
    \ tokens (sometimes alongside the correct end-of-stream token) instead of just\
    \ a single end-of-stream token. Tokenization issues may be present in the Axolotl\
    \ code."
  created_at: 2023-05-15 14:03:53+00:00
  edited: true
  hidden: false
  id: 646249d948e13890ea524e76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-05-15T20:51:33.000Z'
    data:
      edited: false
      editors:
      - winglian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: '<p>Thanks for the feedback. I''ll probably have to dig deeper to figure
          out why the eos token is getting generated as literal text, esp as its defined
          in the file as a special token. <a href="https://huggingface.co/openaccess-ai-collective/wizard-mega-13b/blob/main/special_tokens_map.json#L9-L15">https://huggingface.co/openaccess-ai-collective/wizard-mega-13b/blob/main/special_tokens_map.json#L9-L15</a></p>

          <p>One of the things I actually want to eventually try is to mix and match
          all the user and assistant tokens. The model should really be capable of
          generalizing the meaning of each, and since we always provide for example
          <code>&lt;|prompter|&gt;</code> and <code>&lt;|assistant|&gt;</code> in
          the inputs, it doesn''t need to know to generate it one way or another,
          and it can be pretty robust in dealing with various differences in training
          prompts vs inference prompts.  Thoughts?</p>

          '
        raw: 'Thanks for the feedback. I''ll probably have to dig deeper to figure
          out why the eos token is getting generated as literal text, esp as its defined
          in the file as a special token. https://huggingface.co/openaccess-ai-collective/wizard-mega-13b/blob/main/special_tokens_map.json#L9-L15


          One of the things I actually want to eventually try is to mix and match
          all the user and assistant tokens. The model should really be capable of
          generalizing the meaning of each, and since we always provide for example
          `<|prompter|>` and `<|assistant|>` in the inputs, it doesn''t need to know
          to generate it one way or another, and it can be pretty robust in dealing
          with various differences in training prompts vs inference prompts.  Thoughts?'
        updatedAt: '2023-05-15T20:51:33.180Z'
      numEdits: 0
      reactions: []
    id: 64629b55c87d09bb4a8e4d06
    type: comment
  author: winglian
  content: 'Thanks for the feedback. I''ll probably have to dig deeper to figure out
    why the eos token is getting generated as literal text, esp as its defined in
    the file as a special token. https://huggingface.co/openaccess-ai-collective/wizard-mega-13b/blob/main/special_tokens_map.json#L9-L15


    One of the things I actually want to eventually try is to mix and match all the
    user and assistant tokens. The model should really be capable of generalizing
    the meaning of each, and since we always provide for example `<|prompter|>` and
    `<|assistant|>` in the inputs, it doesn''t need to know to generate it one way
    or another, and it can be pretty robust in dealing with various differences in
    training prompts vs inference prompts.  Thoughts?'
  created_at: 2023-05-15 19:51:33+00:00
  edited: false
  hidden: false
  id: 64629b55c87d09bb4a8e4d06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
      fullname: polymer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: polymer
      type: user
    createdAt: '2023-05-15T22:01:18.000Z'
    data:
      edited: true
      editors:
      - polymer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
          fullname: polymer
          isHf: false
          isPro: false
          name: polymer
          type: user
        html: "<p>I\u2019d check the tokenizer used and its list of accepted tokens\
          \ in your training program. If you were simply adding the text \u201C&lt;/s&gt;\u201D\
          \ and tokenizing as done in FastChat, the text sequences were not being\
          \ tokenized properly. If nothing else works, adding EOS using the ID could\
          \ work, although not ideal (unrelated, but also check if the text \u201C\
          &lt;/s&gt;\u201D is in the actual dataset, it shouldn\u2019t be present).\
          \ I noticed that it also generated both of them sometimes (the text \u201C\
          &lt;/s&gt;\u201D and then an actual EOS).</p>\n<p>I saw discussions here\
          \ about getting these models to generalize better (in terms of prompt formatting),\
          \ but I\u2019m not sure whether this should be prioritized with our goals\
          \ of reaching a size-efficient model using a diverse (both instructional\
          \ and conversational) dataset. If we want to give it the best chance for\
          \ high quality responses (by using those weights instead for better output),\
          \ I think at least the \u201Cuser\u201D marker/markers should stay consistent\
          \ to help the attention mechanism reliably find where the human sequences\
          \ are.</p>\n<p>No matter what, the one thing that should probably be kept\
          \ constant is the <em>conversational</em> style, as a chatty model can take\
          \ instructions but an instructional model will not converse well. Maybe\
          \ the \u201Cassistant\u201D token/tokens could be varied per chat with different\
          \ token combinations to hopefully allow custom identities (like using \u201C\
          BING:\u201D or \u201CGPT-4:\u201D).</p>\n"
        raw: "I\u2019d check the tokenizer used and its list of accepted tokens in\
          \ your training program. If you were simply adding the text \u201C\\</s>\u201D\
          \ and tokenizing as done in FastChat, the text sequences were not being\
          \ tokenized properly. If nothing else works, adding EOS using the ID could\
          \ work, although not ideal (unrelated, but also check if the text \u201C\
          \\</s>\u201D is in the actual dataset, it shouldn\u2019t be present). I\
          \ noticed that it also generated both of them sometimes (the text \u201C\
          \\</s>\u201D and then an actual EOS).\n\nI saw discussions here about getting\
          \ these models to generalize better (in terms of prompt formatting), but\
          \ I\u2019m not sure whether this should be prioritized with our goals of\
          \ reaching a size-efficient model using a diverse (both instructional and\
          \ conversational) dataset. If we want to give it the best chance for high\
          \ quality responses (by using those weights instead for better output),\
          \ I think at least the \u201Cuser\u201D marker/markers should stay consistent\
          \ to help the attention mechanism reliably find where the human sequences\
          \ are.\n\nNo matter what, the one thing that should probably be kept constant\
          \ is the *conversational* style, as a chatty model can take instructions\
          \ but an instructional model will not converse well. Maybe the \u201Cassistant\u201D\
          \ token/tokens could be varied per chat with different token combinations\
          \ to hopefully allow custom identities (like using \u201CBING:\u201D or\
          \ \u201CGPT-4:\u201D)."
        updatedAt: '2023-05-15T22:06:56.375Z'
      numEdits: 2
      reactions: []
    id: 6462abae904bbc4cf2e2c2df
    type: comment
  author: polymer
  content: "I\u2019d check the tokenizer used and its list of accepted tokens in your\
    \ training program. If you were simply adding the text \u201C\\</s>\u201D and\
    \ tokenizing as done in FastChat, the text sequences were not being tokenized\
    \ properly. If nothing else works, adding EOS using the ID could work, although\
    \ not ideal (unrelated, but also check if the text \u201C\\</s>\u201D is in the\
    \ actual dataset, it shouldn\u2019t be present). I noticed that it also generated\
    \ both of them sometimes (the text \u201C\\</s>\u201D and then an actual EOS).\n\
    \nI saw discussions here about getting these models to generalize better (in terms\
    \ of prompt formatting), but I\u2019m not sure whether this should be prioritized\
    \ with our goals of reaching a size-efficient model using a diverse (both instructional\
    \ and conversational) dataset. If we want to give it the best chance for high\
    \ quality responses (by using those weights instead for better output), I think\
    \ at least the \u201Cuser\u201D marker/markers should stay consistent to help\
    \ the attention mechanism reliably find where the human sequences are.\n\nNo matter\
    \ what, the one thing that should probably be kept constant is the *conversational*\
    \ style, as a chatty model can take instructions but an instructional model will\
    \ not converse well. Maybe the \u201Cassistant\u201D token/tokens could be varied\
    \ per chat with different token combinations to hopefully allow custom identities\
    \ (like using \u201CBING:\u201D or \u201CGPT-4:\u201D)."
  created_at: 2023-05-15 21:01:18+00:00
  edited: true
  hidden: false
  id: 6462abae904bbc4cf2e2c2df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-05-16T00:24:45.000Z'
    data:
      edited: false
      editors:
      - winglian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: "<p>I think the eos tokens have something to do with the change to \"\
          Fast\" tokenizers in a recent release. I spent a lot of time getting it\
          \ right, and I guess they broke it with Fast Tokenizers \U0001F92C</p>\n"
        raw: "I think the eos tokens have something to do with the change to \"Fast\"\
          \ tokenizers in a recent release. I spent a lot of time getting it right,\
          \ and I guess they broke it with Fast Tokenizers \U0001F92C"
        updatedAt: '2023-05-16T00:24:45.815Z'
      numEdits: 0
      reactions: []
    id: 6462cd4d02de01c83c0f23c7
    type: comment
  author: winglian
  content: "I think the eos tokens have something to do with the change to \"Fast\"\
    \ tokenizers in a recent release. I spent a lot of time getting it right, and\
    \ I guess they broke it with Fast Tokenizers \U0001F92C"
  created_at: 2023-05-15 23:24:45+00:00
  edited: false
  hidden: false
  id: 6462cd4d02de01c83c0f23c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
      fullname: polymer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: polymer
      type: user
    createdAt: '2023-05-16T09:30:58.000Z'
    data:
      edited: false
      editors:
      - polymer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
          fullname: polymer
          isHf: false
          isPro: false
          name: polymer
          type: user
        html: "<p>Haha, yes. It\u2019s always the \u201Cupgrades\u201D that break\
          \ everything. \U0001F644</p>\n"
        raw: "Haha, yes. It\u2019s always the \u201Cupgrades\u201D that break everything.\
          \ \U0001F644"
        updatedAt: '2023-05-16T09:30:58.811Z'
      numEdits: 0
      reactions: []
    id: 64634d5212814d754177d232
    type: comment
  author: polymer
  content: "Haha, yes. It\u2019s always the \u201Cupgrades\u201D that break everything.\
    \ \U0001F644"
  created_at: 2023-05-16 08:30:58+00:00
  edited: false
  hidden: false
  id: 64634d5212814d754177d232
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: openaccess-ai-collective/wizard-mega-13b
repo_type: model
status: open
target_branch: null
title: Fine-tune specific details
