!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 2EyeGuy
conflicting_files: null
created_at: 2023-05-18 03:32:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5e847bf628189128ba05f29601fc063.svg
      fullname: Carl Kenner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 2EyeGuy
      type: user
    createdAt: '2023-05-18T04:32:26.000Z'
    data:
      edited: true
      editors:
      - 2EyeGuy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5e847bf628189128ba05f29601fc063.svg
          fullname: Carl Kenner
          isHf: false
          isPro: false
          name: 2EyeGuy
          type: user
        html: '<p>You give two examples of the following prompt format:<br><code>###
          Instruction:  &lt;question&gt;\n\n### Assistant: &lt;answer&gt;\n\n</code></p>

          <p>But your hugging face space uses this version of the Alpaca prompt format:<br><code>###
          Instruction: \n&lt;question&gt;\n\n### Response:\n&lt;answer&gt;\n\n</code></p>

          <p>And this version of the Vicuna 1.1 prompt format:<br><code>USER: &lt;question&gt;\nASSISTANT:
          &lt;answer&gt;\n</code></p>

          <p>But reading the axolotl source code and looking at your config file,
          shows that you trained two of the models with a different version of the
          Vicuna 1.1 instruction format, and one of the models with the Alpaca instruction
          format. Which is a weird way of doing it.</p>

          <p>Two of the datasets:<br><code>USER: &lt;question&gt; ASSISTANT: &lt;answer&gt;&lt;/s&gt;</code></p>

          <p>The other dataset:<br><code>Below is an instruction that describes a
          task. Write a response that appropriately completes the request.\n\n###
          Instruction:\n&lt;question&gt;\n\n### Response:\n&lt;answer&gt;</code></p>

          <p>So what format should I actually use?</p>

          <p>EDIT: fixed my mistake about the hugging face space</p>

          '
        raw: 'You give two examples of the following prompt format:

          `### Instruction:  <question>\n\n### Assistant: <answer>\n\n`


          But your hugging face space uses this version of the Alpaca prompt format:

          `### Instruction: \n<question>\n\n### Response:\n<answer>\n\n`


          And this version of the Vicuna 1.1 prompt format:

          `USER: <question>\nASSISTANT: <answer>\n`


          But reading the axolotl source code and looking at your config file, shows
          that you trained two of the models with a different version of the Vicuna
          1.1 instruction format, and one of the models with the Alpaca instruction
          format. Which is a weird way of doing it.


          Two of the datasets:

          `USER: <question> ASSISTANT: <answer></s>`


          The other dataset:

          `Below is an instruction that describes a task. Write a response that appropriately
          completes the request.\n\n### Instruction:\n<question>\n\n### Response:\n<answer>`


          So what format should I actually use?


          EDIT: fixed my mistake about the hugging face space'
        updatedAt: '2023-05-18T05:22:37.086Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - atulika
    id: 6465aa5a200b583e1e5389a4
    type: comment
  author: 2EyeGuy
  content: 'You give two examples of the following prompt format:

    `### Instruction:  <question>\n\n### Assistant: <answer>\n\n`


    But your hugging face space uses this version of the Alpaca prompt format:

    `### Instruction: \n<question>\n\n### Response:\n<answer>\n\n`


    And this version of the Vicuna 1.1 prompt format:

    `USER: <question>\nASSISTANT: <answer>\n`


    But reading the axolotl source code and looking at your config file, shows that
    you trained two of the models with a different version of the Vicuna 1.1 instruction
    format, and one of the models with the Alpaca instruction format. Which is a weird
    way of doing it.


    Two of the datasets:

    `USER: <question> ASSISTANT: <answer></s>`


    The other dataset:

    `Below is an instruction that describes a task. Write a response that appropriately
    completes the request.\n\n### Instruction:\n<question>\n\n### Response:\n<answer>`


    So what format should I actually use?


    EDIT: fixed my mistake about the hugging face space'
  created_at: 2023-05-18 03:32:26+00:00
  edited: true
  hidden: false
  id: 6465aa5a200b583e1e5389a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-05-19T01:30:36.000Z'
    data:
      edited: false
      editors:
      - winglian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: '<p>My goal is to give the model enough examples that it is capable
          of handling any prompt format and could infer the meaning.</p>

          '
        raw: My goal is to give the model enough examples that it is capable of handling
          any prompt format and could infer the meaning.
        updatedAt: '2023-05-19T01:30:36.418Z'
      numEdits: 0
      reactions: []
    id: 6466d13c80e48bb90c37afe1
    type: comment
  author: winglian
  content: My goal is to give the model enough examples that it is capable of handling
    any prompt format and could infer the meaning.
  created_at: 2023-05-19 00:30:36+00:00
  edited: false
  hidden: false
  id: 6466d13c80e48bb90c37afe1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd1af2d47fb0b4c3608f4552668e311e.svg
      fullname: Ali Abuharb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Aillian
      type: user
    createdAt: '2023-10-17T13:51:51.000Z'
    data:
      edited: false
      editors:
      - Aillian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9932788610458374
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd1af2d47fb0b4c3608f4552668e311e.svg
          fullname: Ali Abuharb
          isHf: false
          isPro: false
          name: Aillian
          type: user
        html: '<p>have you figured it out?</p>

          '
        raw: have you figured it out?
        updatedAt: '2023-10-17T13:51:51.553Z'
      numEdits: 0
      reactions: []
    id: 652e9177d1796ce8215e636c
    type: comment
  author: Aillian
  content: have you figured it out?
  created_at: 2023-10-17 12:51:51+00:00
  edited: false
  hidden: false
  id: 652e9177d1796ce8215e636c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: openaccess-ai-collective/wizard-mega-13b
repo_type: model
status: open
target_branch: null
title: Prompt format contradiction
