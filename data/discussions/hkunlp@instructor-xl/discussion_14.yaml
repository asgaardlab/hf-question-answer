!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Boman
conflicting_files: null
created_at: 2023-05-31 00:11:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bde330681424e6128ef495dc549a1c17.svg
      fullname: Boman Ng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boman
      type: user
    createdAt: '2023-05-31T01:11:20.000Z'
    data:
      edited: false
      editors:
      - Boman
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bde330681424e6128ef495dc549a1c17.svg
          fullname: Boman Ng
          isHf: false
          isPro: false
          name: Boman
          type: user
        html: "<p>Hi, dear developer!</p>\n<p>I'm using instructor-xl for embedding\
          \ inference and have encountered some issues. </p>\n<p>I have millions of\
          \ data with lengths ranging from 10 to 1000 tokens (using the instructor-large\
          \ tokenizer). I'm performing inference on two 3090Ti (24GB each) with a\
          \ batch size of 128, which just fits the model and data into the GPU memory.\
          \ When monitoring the GPU memory usage with nvtop, it reaching around 23.7GB\
          \ out of 23.98GB, which seems work.</p>\n<p>However, after running for a\
          \ while, I encounter out-of-memory (OOM) errors:</p>\n<pre><code class=\"\
          language-shell\">File \"/home/be/miniconda3/envs/amar/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"\
          , line 600, in forward\n    attention_output = self.SelfAttention(\n  File\
          \ \"/home/be/miniconda3/envs/amar/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/be/miniconda3/envs/amar/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"\
          , line 560, in forward\n    attn_weights = nn.functional.softmax(scores.float(),\
          \ dim=-1).type_as(\n  File \"/home/be/miniconda3/envs/amar/lib/python3.10/site-packages/torch/nn/functional.py\"\
          , line 1843, in softmax\n    ret = input.softmax(dim)\ntorch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 23.68 GiB total\
          \ capacity; 16.30 GiB already allocated; 3.47 GiB free; 19.90 GiB reserved\
          \ in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try\
          \ setting max_split_size_mb to avoid fragmentation.  See documentation for\
          \ Memory Management and PYTORCH_CUDA_ALLOC_CONF\n</code></pre>\n<p> I'm\
          \ not sure why the GPU memory requirements suddenly increase.</p>\n<p>I\
          \ suspect that it might be due to data longer than the model's max_length\
          \ (which is set to the default value of 512) getting truncated into multiple\
          \ entries, resulting in individual batch sizes larger than 128 and leading\
          \ to OOM errors.<br>Later, I strictly limited the input data to a length\
          \ of 512 tokens, but still encountered the same OOM errors.<br>I have no\
          \ more idea about the possible reasons.</p>\n<p>I would appreciate some\
          \ tips or help regarding this matter.</p>\n<p>Thank you !</p>\n"
        raw: "Hi, dear developer!\r\n\r\nI'm using instructor-xl for embedding inference\
          \ and have encountered some issues. \r\n\r\nI have millions of data with\
          \ lengths ranging from 10 to 1000 tokens (using the instructor-large tokenizer).\
          \ I'm performing inference on two 3090Ti (24GB each) with a batch size of\
          \ 128, which just fits the model and data into the GPU memory. When monitoring\
          \ the GPU memory usage with nvtop, it reaching around 23.7GB out of 23.98GB,\
          \ which seems work.\r\n\r\nHowever, after running for a while, I encounter\
          \ out-of-memory (OOM) errors:\r\n```shell\r\nFile \"/home/be/miniconda3/envs/amar/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"\
          , line 600, in forward\r\n    attention_output = self.SelfAttention(\r\n\
          \  File \"/home/be/miniconda3/envs/amar/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/be/miniconda3/envs/amar/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"\
          , line 560, in forward\r\n    attn_weights = nn.functional.softmax(scores.float(),\
          \ dim=-1).type_as(\r\n  File \"/home/be/miniconda3/envs/amar/lib/python3.10/site-packages/torch/nn/functional.py\"\
          , line 1843, in softmax\r\n    ret = input.softmax(dim)\r\ntorch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 23.68 GiB total\
          \ capacity; 16.30 GiB already allocated; 3.47 GiB free; 19.90 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\n I'm not sure why the\
          \ GPU memory requirements suddenly increase.\r\n\r\nI suspect that it might\
          \ be due to data longer than the model's max_length (which is set to the\
          \ default value of 512) getting truncated into multiple entries, resulting\
          \ in individual batch sizes larger than 128 and leading to OOM errors. \r\
          \nLater, I strictly limited the input data to a length of 512 tokens, but\
          \ still encountered the same OOM errors. \r\nI have no more idea about the\
          \ possible reasons.\r\n\r\nI would appreciate some tips or help regarding\
          \ this matter.\r\n\r\nThank you !"
        updatedAt: '2023-05-31T01:11:20.617Z'
      numEdits: 0
      reactions: []
    id: 64769eb8b9c742c511181845
    type: comment
  author: Boman
  content: "Hi, dear developer!\r\n\r\nI'm using instructor-xl for embedding inference\
    \ and have encountered some issues. \r\n\r\nI have millions of data with lengths\
    \ ranging from 10 to 1000 tokens (using the instructor-large tokenizer). I'm performing\
    \ inference on two 3090Ti (24GB each) with a batch size of 128, which just fits\
    \ the model and data into the GPU memory. When monitoring the GPU memory usage\
    \ with nvtop, it reaching around 23.7GB out of 23.98GB, which seems work.\r\n\r\
    \nHowever, after running for a while, I encounter out-of-memory (OOM) errors:\r\
    \n```shell\r\nFile \"/home/be/miniconda3/envs/amar/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"\
    , line 600, in forward\r\n    attention_output = self.SelfAttention(\r\n  File\
    \ \"/home/be/miniconda3/envs/amar/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/be/miniconda3/envs/amar/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"\
    , line 560, in forward\r\n    attn_weights = nn.functional.softmax(scores.float(),\
    \ dim=-1).type_as(\r\n  File \"/home/be/miniconda3/envs/amar/lib/python3.10/site-packages/torch/nn/functional.py\"\
    , line 1843, in softmax\r\n    ret = input.softmax(dim)\r\ntorch.cuda.OutOfMemoryError:\
    \ CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 23.68 GiB total capacity;\
    \ 16.30 GiB already allocated; 3.47 GiB free; 19.90 GiB reserved in total by PyTorch)\
    \ If reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
    \n```\r\n I'm not sure why the GPU memory requirements suddenly increase.\r\n\r\
    \nI suspect that it might be due to data longer than the model's max_length (which\
    \ is set to the default value of 512) getting truncated into multiple entries,\
    \ resulting in individual batch sizes larger than 128 and leading to OOM errors.\
    \ \r\nLater, I strictly limited the input data to a length of 512 tokens, but\
    \ still encountered the same OOM errors. \r\nI have no more idea about the possible\
    \ reasons.\r\n\r\nI would appreciate some tips or help regarding this matter.\r\
    \n\r\nThank you !"
  created_at: 2023-05-31 00:11:20+00:00
  edited: false
  hidden: false
  id: 64769eb8b9c742c511181845
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/bde330681424e6128ef495dc549a1c17.svg
      fullname: Boman Ng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boman
      type: user
    createdAt: '2023-05-31T01:11:44.000Z'
    data:
      from: ' OOM erroe: GPU memory requirements suddenly increase'
      to: 'OOM error: GPU memory requirements suddenly increase'
    id: 64769ed057108da17602a492
    type: title-change
  author: Boman
  created_at: 2023-05-31 00:11:44+00:00
  id: 64769ed057108da17602a492
  new_title: 'OOM error: GPU memory requirements suddenly increase'
  old_title: ' OOM erroe: GPU memory requirements suddenly increase'
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2023-06-05T11:59:21.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9157467484474182
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>Hi, Thanks a lot for your interest in the INSTRUCTOR model!</p>

          <p>You might try to reduce the batch size in the inference, because some
          data sequences may be shorter, which fits into the 24GB memory. For other
          longer sequences, they may require more memory for a single batch.</p>

          <p>Feel free to add any further questions or comments!</p>

          '
        raw: 'Hi, Thanks a lot for your interest in the INSTRUCTOR model!


          You might try to reduce the batch size in the inference, because some data
          sequences may be shorter, which fits into the 24GB memory. For other longer
          sequences, they may require more memory for a single batch.


          Feel free to add any further questions or comments!'
        updatedAt: '2023-06-05T11:59:21.147Z'
      numEdits: 0
      reactions: []
    id: 647dce195214d172cbb5848e
    type: comment
  author: multi-train
  content: 'Hi, Thanks a lot for your interest in the INSTRUCTOR model!


    You might try to reduce the batch size in the inference, because some data sequences
    may be shorter, which fits into the 24GB memory. For other longer sequences, they
    may require more memory for a single batch.


    Feel free to add any further questions or comments!'
  created_at: 2023-06-05 10:59:21+00:00
  edited: false
  hidden: false
  id: 647dce195214d172cbb5848e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bde330681424e6128ef495dc549a1c17.svg
      fullname: Boman Ng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boman
      type: user
    createdAt: '2023-06-05T12:30:59.000Z'
    data:
      edited: false
      editors:
      - Boman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9694610238075256
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bde330681424e6128ef495dc549a1c17.svg
          fullname: Boman Ng
          isHf: false
          isPro: false
          name: Boman
          type: user
        html: '<p>This is exactly how I''m currently addressing this issue. I reduced
          the batch size by 20% to limit its memory usage to only 85%. However, I
          later noticed that the memory usage of some individual GPUs reached around
          95%.</p>

          <p>But I''m curious because I strictly limit the length of all my sequences,
          and no single sequence can exceed the max length. Therefore, I shouldn''t
          be experiencing the phenomenon you mentioned about "For other longer sequences,
          they may require more memory for a single batch."</p>

          <p>Thank you</p>

          '
        raw: 'This is exactly how I''m currently addressing this issue. I reduced
          the batch size by 20% to limit its memory usage to only 85%. However, I
          later noticed that the memory usage of some individual GPUs reached around
          95%.


          But I''m curious because I strictly limit the length of all my sequences,
          and no single sequence can exceed the max length. Therefore, I shouldn''t
          be experiencing the phenomenon you mentioned about "For other longer sequences,
          they may require more memory for a single batch."


          Thank you'
        updatedAt: '2023-06-05T12:30:59.255Z'
      numEdits: 0
      reactions: []
    id: 647dd58310b7a3b157020339
    type: comment
  author: Boman
  content: 'This is exactly how I''m currently addressing this issue. I reduced the
    batch size by 20% to limit its memory usage to only 85%. However, I later noticed
    that the memory usage of some individual GPUs reached around 95%.


    But I''m curious because I strictly limit the length of all my sequences, and
    no single sequence can exceed the max length. Therefore, I shouldn''t be experiencing
    the phenomenon you mentioned about "For other longer sequences, they may require
    more memory for a single batch."


    Thank you'
  created_at: 2023-06-05 11:30:59+00:00
  edited: false
  hidden: false
  id: 647dd58310b7a3b157020339
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2023-06-06T02:02:32.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.952081024646759
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>By longer sequences, I mean those with lengths approximately the
          maximum length, while for other sequences, they may have lengths smaller
          than the maximum length, e.g., 200 or 300.</p>

          '
        raw: By longer sequences, I mean those with lengths approximately the maximum
          length, while for other sequences, they may have lengths smaller than the
          maximum length, e.g., 200 or 300.
        updatedAt: '2023-06-06T02:02:32.696Z'
      numEdits: 0
      reactions: []
    id: 647e93b832c471a7fa9f24e4
    type: comment
  author: multi-train
  content: By longer sequences, I mean those with lengths approximately the maximum
    length, while for other sequences, they may have lengths smaller than the maximum
    length, e.g., 200 or 300.
  created_at: 2023-06-06 01:02:32+00:00
  edited: false
  hidden: false
  id: 647e93b832c471a7fa9f24e4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: hkunlp/instructor-xl
repo_type: model
status: open
target_branch: null
title: 'OOM error: GPU memory requirements suddenly increase'
