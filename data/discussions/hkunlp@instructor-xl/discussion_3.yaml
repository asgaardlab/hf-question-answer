!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gromag
conflicting_files: null
created_at: 2023-03-22 16:24:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6364f96f29941f95ad717388/LE12cOzX7lloiQwssvsGU.jpeg?w=200&h=200&f=face
      fullname: Giuseppe Romagnuolo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gromag
      type: user
    createdAt: '2023-03-22T17:24:49.000Z'
    data:
      edited: true
      editors:
      - gromag
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6364f96f29941f95ad717388/LE12cOzX7lloiQwssvsGU.jpeg?w=200&h=200&f=face
          fullname: Giuseppe Romagnuolo
          isHf: false
          isPro: false
          name: gromag
          type: user
        html: '<p>Thank you for sharing this model and paper.<br>I''m investigating
          what would take to further fine tune Instructor-XL to a legal domain for
          retrival tasks.<br>I''m trying to assess what could be a good starting training
          set size, loss temperature and what could be a good k of negative pairs
          per positive pairs.<br>I welcome any other heads-ups.</p>

          <p>PS. with hindsight I feel a little daft asking about finetuning when
          the model card explicitly say <em>"embeddings tailored to any task and domains
          [...] by simply providing the task instruction, without any finetuning.
          "</em> , please let me know if it is a stupid idea.</p>

          '
        raw: "Thank you for sharing this model and paper.\nI'm investigating what\
          \ would take to further fine tune Instructor-XL to a legal domain for retrival\
          \ tasks. \nI'm trying to assess what could be a good starting training set\
          \ size, loss temperature and what could be a good k of negative pairs per\
          \ positive pairs.\nI welcome any other heads-ups.\n\nPS. with hindsight\
          \ I feel a little daft asking about finetuning when the model card explicitly\
          \ say _\"embeddings tailored to any task and domains [...] by simply providing\
          \ the task instruction, without any finetuning. \"_ , please let me know\
          \ if it is a stupid idea."
        updatedAt: '2023-03-23T11:52:21.942Z'
      numEdits: 1
      reactions: []
    id: 641b39e1ec5b871c0bcd9d8a
    type: comment
  author: gromag
  content: "Thank you for sharing this model and paper.\nI'm investigating what would\
    \ take to further fine tune Instructor-XL to a legal domain for retrival tasks.\
    \ \nI'm trying to assess what could be a good starting training set size, loss\
    \ temperature and what could be a good k of negative pairs per positive pairs.\n\
    I welcome any other heads-ups.\n\nPS. with hindsight I feel a little daft asking\
    \ about finetuning when the model card explicitly say _\"embeddings tailored to\
    \ any task and domains [...] by simply providing the task instruction, without\
    \ any finetuning. \"_ , please let me know if it is a stupid idea."
  created_at: 2023-03-22 16:24:49+00:00
  edited: true
  hidden: false
  id: 641b39e1ec5b871c0bcd9d8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2023-03-24T03:21:35.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>Thank you very much for your interest in INSTRUCTOR!</p>

          <p>The instruction serves as an efficient option for adapting embeddings
          to specific domains, but you can also further enhance the model ability
          through finetuning. At the start, you may use all the available training
          data (probably training for a maximum of 40K steps). For other hyper-parameters,
          you may adopt our default setting (e.g., loss_temperature=0.01, k=4, etc.)</p>

          <p>Hope this helps! Feel free to add any further questions or comments!</p>

          '
        raw: 'Thank you very much for your interest in INSTRUCTOR!


          The instruction serves as an efficient option for adapting embeddings to
          specific domains, but you can also further enhance the model ability through
          finetuning. At the start, you may use all the available training data (probably
          training for a maximum of 40K steps). For other hyper-parameters, you may
          adopt our default setting (e.g., loss_temperature=0.01, k=4, etc.)


          Hope this helps! Feel free to add any further questions or comments!'
        updatedAt: '2023-03-24T03:21:35.149Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - gromag
    id: 641d173fa073e0d29324db13
    type: comment
  author: multi-train
  content: 'Thank you very much for your interest in INSTRUCTOR!


    The instruction serves as an efficient option for adapting embeddings to specific
    domains, but you can also further enhance the model ability through finetuning.
    At the start, you may use all the available training data (probably training for
    a maximum of 40K steps). For other hyper-parameters, you may adopt our default
    setting (e.g., loss_temperature=0.01, k=4, etc.)


    Hope this helps! Feel free to add any further questions or comments!'
  created_at: 2023-03-24 02:21:35+00:00
  edited: false
  hidden: false
  id: 641d173fa073e0d29324db13
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: hkunlp/instructor-xl
repo_type: model
status: open
target_branch: null
title: Fine tuning
