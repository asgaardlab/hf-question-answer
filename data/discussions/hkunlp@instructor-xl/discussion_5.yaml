!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hiranya911
conflicting_files: null
created_at: 2023-04-12 23:05:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43879c82b86659e5298bec3837d9d038.svg
      fullname: Hiranya Jayathilaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiranya911
      type: user
    createdAt: '2023-04-13T00:05:57.000Z'
    data:
      edited: false
      editors:
      - hiranya911
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43879c82b86659e5298bec3837d9d038.svg
          fullname: Hiranya Jayathilaka
          isHf: false
          isPro: false
          name: hiranya911
          type: user
        html: '<p>Thanks for sharing this model with the community. </p>

          <p>What''s the max number of tokens that can be embedded with this? I noticed
          that it logs "max_seq_length  512" every time the model is loaded. Is that
          512 characters?</p>

          '
        raw: "Thanks for sharing this model with the community. \r\n\r\nWhat's the\
          \ max number of tokens that can be embedded with this? I noticed that it\
          \ logs \"max_seq_length  512\" every time the model is loaded. Is that 512\
          \ characters?"
        updatedAt: '2023-04-13T00:05:57.485Z'
      numEdits: 0
      reactions: []
    id: 643747659dec089097c433bd
    type: comment
  author: hiranya911
  content: "Thanks for sharing this model with the community. \r\n\r\nWhat's the max\
    \ number of tokens that can be embedded with this? I noticed that it logs \"max_seq_length\
    \  512\" every time the model is loaded. Is that 512 characters?"
  created_at: 2023-04-12 23:05:57+00:00
  edited: false
  hidden: false
  id: 643747659dec089097c433bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2023-04-14T03:07:00.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>Thanks a lot for your interests in our INSTRUCTOR model!</p>

          <p>Your understanding is correct! By default, the maximum sequence length
          is 512. For changing the maximum sequence length, you may refer to <a rel="nofollow"
          href="https://github.com/HKUNLP/instructor-embedding/issues/12">https://github.com/HKUNLP/instructor-embedding/issues/12</a>.</p>

          <p>Hope this helps! Feel free to add any further questions or comments!</p>

          '
        raw: 'Thanks a lot for your interests in our INSTRUCTOR model!


          Your understanding is correct! By default, the maximum sequence length is
          512. For changing the maximum sequence length, you may refer to https://github.com/HKUNLP/instructor-embedding/issues/12.


          Hope this helps! Feel free to add any further questions or comments!'
        updatedAt: '2023-04-14T03:07:00.481Z'
      numEdits: 0
      reactions: []
    id: 6438c3546c8841ba74e9a713
    type: comment
  author: multi-train
  content: 'Thanks a lot for your interests in our INSTRUCTOR model!


    Your understanding is correct! By default, the maximum sequence length is 512.
    For changing the maximum sequence length, you may refer to https://github.com/HKUNLP/instructor-embedding/issues/12.


    Hope this helps! Feel free to add any further questions or comments!'
  created_at: 2023-04-14 02:07:00+00:00
  edited: false
  hidden: false
  id: 6438c3546c8841ba74e9a713
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43879c82b86659e5298bec3837d9d038.svg
      fullname: Hiranya Jayathilaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiranya911
      type: user
    createdAt: '2023-04-14T16:33:31.000Z'
    data:
      edited: false
      editors:
      - hiranya911
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43879c82b86659e5298bec3837d9d038.svg
          fullname: Hiranya Jayathilaka
          isHf: false
          isPro: false
          name: hiranya911
          type: user
        html: '<p>Thanks for the link. That helped answer a number of questions I
          had.</p>

          <p>What''s the tokenizer I should use if I were to chunk a long text before
          generating embeddings? I skimmed through the code, and found references
          to <code>AutoTransformer</code> and T5. So will something like the following
          work?</p>

          <pre><code>TOKENIZER = T5Tokenizer.from_pretrained(''t5-large'', model_max_length=512)

          SPLITTER = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(TOKENIZER,
          chunk_size=512, chunk_overlap=0)

          </code></pre>

          '
        raw: 'Thanks for the link. That helped answer a number of questions I had.


          What''s the tokenizer I should use if I were to chunk a long text before
          generating embeddings? I skimmed through the code, and found references
          to `AutoTransformer` and T5. So will something like the following work?


          ```

          TOKENIZER = T5Tokenizer.from_pretrained(''t5-large'', model_max_length=512)

          SPLITTER = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(TOKENIZER,
          chunk_size=512, chunk_overlap=0)

          ```'
        updatedAt: '2023-04-14T16:33:31.779Z'
      numEdits: 0
      reactions: []
    id: 6439805b1eddadc9404b7405
    type: comment
  author: hiranya911
  content: 'Thanks for the link. That helped answer a number of questions I had.


    What''s the tokenizer I should use if I were to chunk a long text before generating
    embeddings? I skimmed through the code, and found references to `AutoTransformer`
    and T5. So will something like the following work?


    ```

    TOKENIZER = T5Tokenizer.from_pretrained(''t5-large'', model_max_length=512)

    SPLITTER = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(TOKENIZER,
    chunk_size=512, chunk_overlap=0)

    ```'
  created_at: 2023-04-14 15:33:31+00:00
  edited: false
  hidden: false
  id: 6439805b1eddadc9404b7405
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2023-04-14T17:33:32.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>Hi, Thanks a lot for your comments!</p>

          <p>The recommended tokenizer for calculating the sequence length would be
          the INSTRUCTOR tokenizer. For example:</p>

          <pre><code>from transformers import AutoTokenizer

          tokenizer = AutoTokenizer.from_pretrained(''hkunlp/instructor-large'') #
          initialize the INSTRUCTOR tokenizer

          text = "Hello, world!"

          text_length = len(tokenizer(text))

          print(text_length)

          </code></pre>

          <p>Hope this helps! Feel free to add any further questions or comments!</p>

          '
        raw: 'Hi, Thanks a lot for your comments!


          The recommended tokenizer for calculating the sequence length would be the
          INSTRUCTOR tokenizer. For example:

          ```

          from transformers import AutoTokenizer

          tokenizer = AutoTokenizer.from_pretrained(''hkunlp/instructor-large'') #
          initialize the INSTRUCTOR tokenizer

          text = "Hello, world!"

          text_length = len(tokenizer(text))

          print(text_length)

          ```


          Hope this helps! Feel free to add any further questions or comments!'
        updatedAt: '2023-04-14T17:33:32.326Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - evan-oman
    id: 64398e6c1eddadc9404bd542
    type: comment
  author: multi-train
  content: 'Hi, Thanks a lot for your comments!


    The recommended tokenizer for calculating the sequence length would be the INSTRUCTOR
    tokenizer. For example:

    ```

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained(''hkunlp/instructor-large'') # initialize
    the INSTRUCTOR tokenizer

    text = "Hello, world!"

    text_length = len(tokenizer(text))

    print(text_length)

    ```


    Hope this helps! Feel free to add any further questions or comments!'
  created_at: 2023-04-14 16:33:32+00:00
  edited: false
  hidden: false
  id: 64398e6c1eddadc9404bd542
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56a8158b228595ed056737b9fda0660c.svg
      fullname: Hector Lopez Hernandez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HectorL
      type: user
    createdAt: '2023-05-06T17:49:08.000Z'
    data:
      edited: false
      editors:
      - HectorL
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56a8158b228595ed056737b9fda0660c.svg
          fullname: Hector Lopez Hernandez
          isHf: false
          isPro: false
          name: HectorL
          type: user
        html: '<p>Very glad I found this thread. Is there any way to easily turn on
          a truncation warning? I have text that I''m chunking but can have large
          variations in token length.</p>

          '
        raw: Very glad I found this thread. Is there any way to easily turn on a truncation
          warning? I have text that I'm chunking but can have large variations in
          token length.
        updatedAt: '2023-05-06T17:49:08.770Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - evan-oman
    id: 64569314cd6567f52fb86579
    type: comment
  author: HectorL
  content: Very glad I found this thread. Is there any way to easily turn on a truncation
    warning? I have text that I'm chunking but can have large variations in token
    length.
  created_at: 2023-05-06 16:49:08+00:00
  edited: false
  hidden: false
  id: 64569314cd6567f52fb86579
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56a8158b228595ed056737b9fda0660c.svg
      fullname: Hector Lopez Hernandez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HectorL
      type: user
    createdAt: '2023-05-06T17:59:14.000Z'
    data:
      edited: false
      editors:
      - HectorL
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56a8158b228595ed056737b9fda0660c.svg
          fullname: Hector Lopez Hernandez
          isHf: false
          isPro: false
          name: HectorL
          type: user
        html: '<blockquote>

          <p>Hi, Thanks a lot for your comments!</p>

          <p>The recommended tokenizer for calculating the sequence length would be
          the INSTRUCTOR tokenizer. For example:</p>

          <pre><code>from transformers import AutoTokenizer

          tokenizer = AutoTokenizer.from_pretrained(''hkunlp/instructor-large'') #
          initialize the INSTRUCTOR tokenizer

          text = "Hello, world!"

          text_length = len(tokenizer(text))

          print(text_length)

          </code></pre>

          <p>Hope this helps! Feel free to add any further questions or comments!</p>

          </blockquote>

          <p>Small fix: </p>

          <pre><code>text_length = len(tokenizer(text)[''input_ids''])

          </code></pre>

          '
        raw: "> Hi, Thanks a lot for your comments!\n> \n> The recommended tokenizer\
          \ for calculating the sequence length would be the INSTRUCTOR tokenizer.\
          \ For example:\n> ```\n> from transformers import AutoTokenizer\n> tokenizer\
          \ = AutoTokenizer.from_pretrained('hkunlp/instructor-large') # initialize\
          \ the INSTRUCTOR tokenizer\n> text = \"Hello, world!\"\n> text_length =\
          \ len(tokenizer(text))\n> print(text_length)\n> ```\n> \n> Hope this helps!\
          \ Feel free to add any further questions or comments!\n\nSmall fix: \n```\n\
          text_length = len(tokenizer(text)['input_ids'])\n```"
        updatedAt: '2023-05-06T17:59:14.880Z'
      numEdits: 0
      reactions: []
    id: 64569572cd6567f52fb8843b
    type: comment
  author: HectorL
  content: "> Hi, Thanks a lot for your comments!\n> \n> The recommended tokenizer\
    \ for calculating the sequence length would be the INSTRUCTOR tokenizer. For example:\n\
    > ```\n> from transformers import AutoTokenizer\n> tokenizer = AutoTokenizer.from_pretrained('hkunlp/instructor-large')\
    \ # initialize the INSTRUCTOR tokenizer\n> text = \"Hello, world!\"\n> text_length\
    \ = len(tokenizer(text))\n> print(text_length)\n> ```\n> \n> Hope this helps!\
    \ Feel free to add any further questions or comments!\n\nSmall fix: \n```\ntext_length\
    \ = len(tokenizer(text)['input_ids'])\n```"
  created_at: 2023-05-06 16:59:14+00:00
  edited: false
  hidden: false
  id: 64569572cd6567f52fb8843b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: hkunlp/instructor-xl
repo_type: model
status: open
target_branch: null
title: Max tokens
