!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Boman
conflicting_files: null
created_at: 2023-06-05 15:34:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bde330681424e6128ef495dc549a1c17.svg
      fullname: Boman Ng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boman
      type: user
    createdAt: '2023-06-05T16:34:43.000Z'
    data:
      edited: false
      editors:
      - Boman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9342999458312988
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bde330681424e6128ef495dc549a1c17.svg
          fullname: Boman Ng
          isHf: false
          isPro: false
          name: Boman
          type: user
        html: '<p>The default input length for instructor-xl is 512. However, when
          analyzing my own dataset (which consists of approximately 10 million entries),
          I found that the average length of my strings is around 160 tokens. So,
          when I perform custom vectorization on my dataset, a significant portion
          of GPU memory is actually wasted. Am I correct in my understanding?</p>

          <p>In order to improve GPU  utilization, if I change the model''s input
          length to 256, in theory, I should be able to use a larger batch size, thereby
          enhancing the efficiency of vectorization overall.</p>

          <p>However, inevitably, there is a question:<br>Will reducing the input
          length of the model by half affect the quality of the output vectors? In
          other words, may I ask what is the average length of strings in your dataset?</p>

          <p>Thank you</p>

          '
        raw: "The default input length for instructor-xl is 512. However, when analyzing\
          \ my own dataset (which consists of approximately 10 million entries), I\
          \ found that the average length of my strings is around 160 tokens. So,\
          \ when I perform custom vectorization on my dataset, a significant portion\
          \ of GPU memory is actually wasted. Am I correct in my understanding?\r\n\
          \r\nIn order to improve GPU  utilization, if I change the model's input\
          \ length to 256, in theory, I should be able to use a larger batch size,\
          \ thereby enhancing the efficiency of vectorization overall.\r\n\r\nHowever,\
          \ inevitably, there is a question: \r\nWill reducing the input length of\
          \ the model by half affect the quality of the output vectors? In other words,\
          \ may I ask what is the average length of strings in your dataset?\r\n\r\
          \nThank you"
        updatedAt: '2023-06-05T16:34:43.427Z'
      numEdits: 0
      reactions: []
    id: 647e0ea311084fb5831b2a8f
    type: comment
  author: Boman
  content: "The default input length for instructor-xl is 512. However, when analyzing\
    \ my own dataset (which consists of approximately 10 million entries), I found\
    \ that the average length of my strings is around 160 tokens. So, when I perform\
    \ custom vectorization on my dataset, a significant portion of GPU memory is actually\
    \ wasted. Am I correct in my understanding?\r\n\r\nIn order to improve GPU  utilization,\
    \ if I change the model's input length to 256, in theory, I should be able to\
    \ use a larger batch size, thereby enhancing the efficiency of vectorization overall.\r\
    \n\r\nHowever, inevitably, there is a question: \r\nWill reducing the input length\
    \ of the model by half affect the quality of the output vectors? In other words,\
    \ may I ask what is the average length of strings in your dataset?\r\n\r\nThank\
    \ you"
  created_at: 2023-06-05 15:34:43+00:00
  edited: false
  hidden: false
  id: 647e0ea311084fb5831b2a8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2023-06-06T02:10:12.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9356775879859924
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>Hi, Thanks a lot for your interest in the INSTRUCTOR!</p>

          <p>As we trained the model with the maximum length 512 with many long retrieval
          tasks, it may be best to have consistent settings in the inference. However,
          it is reasonable to reduce the input length by half, and I guess there should
          not be significant performance drop.</p>

          <p>Feel free to add any further questions or comments!</p>

          '
        raw: 'Hi, Thanks a lot for your interest in the INSTRUCTOR!


          As we trained the model with the maximum length 512 with many long retrieval
          tasks, it may be best to have consistent settings in the inference. However,
          it is reasonable to reduce the input length by half, and I guess there should
          not be significant performance drop.


          Feel free to add any further questions or comments!'
        updatedAt: '2023-06-06T02:10:12.904Z'
      numEdits: 0
      reactions: []
    id: 647e958432c471a7fa9f5232
    type: comment
  author: multi-train
  content: 'Hi, Thanks a lot for your interest in the INSTRUCTOR!


    As we trained the model with the maximum length 512 with many long retrieval tasks,
    it may be best to have consistent settings in the inference. However, it is reasonable
    to reduce the input length by half, and I guess there should not be significant
    performance drop.


    Feel free to add any further questions or comments!'
  created_at: 2023-06-06 01:10:12+00:00
  edited: false
  hidden: false
  id: 647e958432c471a7fa9f5232
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: hkunlp/instructor-xl
repo_type: model
status: open
target_branch: null
title: Can the Performance of the Model be Maintained by Shortening the Max Input
  Length?
