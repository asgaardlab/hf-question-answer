!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ishaan812
conflicting_files: null
created_at: 2023-06-05 12:01:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5542fbbf9d0f46948ccd5ca13396b651.svg
      fullname: Ishaan Shah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ishaan812
      type: user
    createdAt: '2023-06-05T13:01:01.000Z'
    data:
      edited: false
      editors:
      - ishaan812
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9424662590026855
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5542fbbf9d0f46948ccd5ca13396b651.svg
          fullname: Ishaan Shah
          isHf: false
          isPro: false
          name: ishaan812
          type: user
        html: '<p>Hey I''ve been running the XL mostly on the CPU. Just wanted to
          know the exact tech specs if there are any for running it on a GPU since
          I wanted to upgrade. Also the GPU version will be faster right if Im not
          misunderstanding. Thanks for the help!</p>

          '
        raw: Hey I've been running the XL mostly on the CPU. Just wanted to know the
          exact tech specs if there are any for running it on a GPU since I wanted
          to upgrade. Also the GPU version will be faster right if Im not misunderstanding.
          Thanks for the help!
        updatedAt: '2023-06-05T13:01:01.588Z'
      numEdits: 0
      reactions: []
    id: 647ddc8d0ed7d0c8760a3bcf
    type: comment
  author: ishaan812
  content: Hey I've been running the XL mostly on the CPU. Just wanted to know the
    exact tech specs if there are any for running it on a GPU since I wanted to upgrade.
    Also the GPU version will be faster right if Im not misunderstanding. Thanks for
    the help!
  created_at: 2023-06-05 12:01:01+00:00
  edited: false
  hidden: false
  id: 647ddc8d0ed7d0c8760a3bcf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2023-06-06T02:06:03.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9230919480323792
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>Hi, Thanks a lot for your interest in the INSTRUCTOR!</p>

          <p>It is possible to run the XL model on GPU devices. Others have tried
          with 24GB memory. It should only consume reasonable spaces with controlled
          batch sizes.</p>

          <p>Feel free to add any further questions or comments!</p>

          '
        raw: 'Hi, Thanks a lot for your interest in the INSTRUCTOR!


          It is possible to run the XL model on GPU devices. Others have tried with
          24GB memory. It should only consume reasonable spaces with controlled batch
          sizes.


          Feel free to add any further questions or comments!'
        updatedAt: '2023-06-06T02:06:03.688Z'
      numEdits: 0
      reactions: []
    id: 647e948bf14eafc3b4652a6f
    type: comment
  author: multi-train
  content: 'Hi, Thanks a lot for your interest in the INSTRUCTOR!


    It is possible to run the XL model on GPU devices. Others have tried with 24GB
    memory. It should only consume reasonable spaces with controlled batch sizes.


    Feel free to add any further questions or comments!'
  created_at: 2023-06-06 01:06:03+00:00
  edited: false
  hidden: false
  id: 647e948bf14eafc3b4652a6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8f2e887f147a0cd3c464441d284f875.svg
      fullname: Nitin Kohli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lonetrader
      type: user
    createdAt: '2023-07-05T17:04:52.000Z'
    data:
      edited: false
      editors:
      - lonetrader
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9315614104270935
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8f2e887f147a0cd3c464441d284f875.svg
          fullname: Nitin Kohli
          isHf: false
          isPro: false
          name: lonetrader
          type: user
        html: '<p>Hi, can you please tell me what is the minimum GPU required to run
          this model? Actually, I''m doing research for which I am using this model.
          I have various documents, and the total length of chunks is 27700, and each
          chunk is 1000. I have a laptop with  16gb ram, 2070 max-q design GPU of
          8GB + intel optane memory of 8gb, and  i7 11th gen. If my specs are not
          proper then can I run the embedding process on collab pro?<br>It would be
          great if you can help me out by answering the query.</p>

          '
        raw: 'Hi, can you please tell me what is the minimum GPU required to run this
          model? Actually, I''m doing research for which I am using this model. I
          have various documents, and the total length of chunks is 27700, and each
          chunk is 1000. I have a laptop with  16gb ram, 2070 max-q design GPU of
          8GB + intel optane memory of 8gb, and  i7 11th gen. If my specs are not
          proper then can I run the embedding process on collab pro?

          It would be great if you can help me out by answering the query.'
        updatedAt: '2023-07-05T17:04:52.238Z'
      numEdits: 0
      reactions: []
    id: 64a5a2b4f271bca65cbb9031
    type: comment
  author: lonetrader
  content: 'Hi, can you please tell me what is the minimum GPU required to run this
    model? Actually, I''m doing research for which I am using this model. I have various
    documents, and the total length of chunks is 27700, and each chunk is 1000. I
    have a laptop with  16gb ram, 2070 max-q design GPU of 8GB + intel optane memory
    of 8gb, and  i7 11th gen. If my specs are not proper then can I run the embedding
    process on collab pro?

    It would be great if you can help me out by answering the query.'
  created_at: 2023-07-05 16:04:52+00:00
  edited: false
  hidden: false
  id: 64a5a2b4f271bca65cbb9031
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d27189ace3689c1f826c2360fb72ba29.svg
      fullname: Wouter Spekkink
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wahsp
      type: user
    createdAt: '2023-08-08T06:38:08.000Z'
    data:
      edited: false
      editors:
      - wahsp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9441739916801453
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d27189ace3689c1f826c2360fb72ba29.svg
          fullname: Wouter Spekkink
          isHf: false
          isPro: false
          name: wahsp
          type: user
        html: '<blockquote>

          <p>Hi, can you please tell me what is the minimum GPU required to run this
          model? Actually, I''m doing research for which I am using this model. I
          have various documents, and the total length of chunks is 27700, and each
          chunk is 1000. I have a laptop with  16gb ram, 2070 max-q design GPU of
          8GB + intel optane memory of 8gb, and  i7 11th gen. If my specs are not
          proper then can I run the embedding process on collab pro?<br>It would be
          great if you can help me out by answering the query.</p>

          </blockquote>

          <p>Long story short: 8GB will usually not be enough. I''ve tried and I ran
          into errors related to memory allocation many times. Colab pro will work
          with the appropriate run environment, but don''t forget that (if you use
          the embeddings for information retrieval), you need to run the model during
          information retrieval as well, because you need to embed your questions.
          You''ll run into the same memory-related problems as you''d have when making
          the embeddings.</p>

          <p>I tried out the instructor-xl model on colab pro and it can indeed be
          done if you use a run environment with high memory and a premium GPU (I
          think the one assigned to me had 16GB VRAM). I also tried with the free
          version of colab and then it runs out of memory at some point, crashing
          the environment.</p>

          <p>Assuming you are indexing the embeddings in a vector store for information
          retrieval: You''ll of course need to load the model for information retrieval
          too. If I use my laptop''s GPU (2070 max-q with 8GB), I run out of VRAM
          quickly when running queries. It is possible to use the CPU instead, but
          it is a bit slower.  You can of course also run the queries in google colab.
          </p>

          '
        raw: '> Hi, can you please tell me what is the minimum GPU required to run
          this model? Actually, I''m doing research for which I am using this model.
          I have various documents, and the total length of chunks is 27700, and each
          chunk is 1000. I have a laptop with  16gb ram, 2070 max-q design GPU of
          8GB + intel optane memory of 8gb, and  i7 11th gen. If my specs are not
          proper then can I run the embedding process on collab pro?

          > It would be great if you can help me out by answering the query.


          Long story short: 8GB will usually not be enough. I''ve tried and I ran
          into errors related to memory allocation many times. Colab pro will work
          with the appropriate run environment, but don''t forget that (if you use
          the embeddings for information retrieval), you need to run the model during
          information retrieval as well, because you need to embed your questions.
          You''ll run into the same memory-related problems as you''d have when making
          the embeddings.


          I tried out the instructor-xl model on colab pro and it can indeed be done
          if you use a run environment with high memory and a premium GPU (I think
          the one assigned to me had 16GB VRAM). I also tried with the free version
          of colab and then it runs out of memory at some point, crashing the environment.


          Assuming you are indexing the embeddings in a vector store for information
          retrieval: You''ll of course need to load the model for information retrieval
          too. If I use my laptop''s GPU (2070 max-q with 8GB), I run out of VRAM
          quickly when running queries. It is possible to use the CPU instead, but
          it is a bit slower.  You can of course also run the queries in google colab. '
        updatedAt: '2023-08-08T06:38:08.141Z'
      numEdits: 0
      reactions: []
    id: 64d1e2d084f205869016e122
    type: comment
  author: wahsp
  content: '> Hi, can you please tell me what is the minimum GPU required to run this
    model? Actually, I''m doing research for which I am using this model. I have various
    documents, and the total length of chunks is 27700, and each chunk is 1000. I
    have a laptop with  16gb ram, 2070 max-q design GPU of 8GB + intel optane memory
    of 8gb, and  i7 11th gen. If my specs are not proper then can I run the embedding
    process on collab pro?

    > It would be great if you can help me out by answering the query.


    Long story short: 8GB will usually not be enough. I''ve tried and I ran into errors
    related to memory allocation many times. Colab pro will work with the appropriate
    run environment, but don''t forget that (if you use the embeddings for information
    retrieval), you need to run the model during information retrieval as well, because
    you need to embed your questions. You''ll run into the same memory-related problems
    as you''d have when making the embeddings.


    I tried out the instructor-xl model on colab pro and it can indeed be done if
    you use a run environment with high memory and a premium GPU (I think the one
    assigned to me had 16GB VRAM). I also tried with the free version of colab and
    then it runs out of memory at some point, crashing the environment.


    Assuming you are indexing the embeddings in a vector store for information retrieval:
    You''ll of course need to load the model for information retrieval too. If I use
    my laptop''s GPU (2070 max-q with 8GB), I run out of VRAM quickly when running
    queries. It is possible to use the CPU instead, but it is a bit slower.  You can
    of course also run the queries in google colab. '
  created_at: 2023-08-08 05:38:08+00:00
  edited: false
  hidden: false
  id: 64d1e2d084f205869016e122
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0abf50a01b2ce58f65fd92cf3b0c5198.svg
      fullname: Armen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: armenabnousi
      type: user
    createdAt: '2023-08-25T05:23:17.000Z'
    data:
      edited: false
      editors:
      - armenabnousi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.955540120601654
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0abf50a01b2ce58f65fd92cf3b0c5198.svg
          fullname: Armen
          isHf: false
          isPro: false
          name: armenabnousi
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ishaan812&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ishaan812\"\
          >@<span class=\"underline\">ishaan812</span></a></span>\n\n\t</span></span>\
          \ , can you please share the CPU device specs you have successfully ran\
          \ the XL model on.</p>\n"
        raw: Hi @ishaan812 , can you please share the CPU device specs you have successfully
          ran the XL model on.
        updatedAt: '2023-08-25T05:23:17.953Z'
      numEdits: 0
      reactions: []
    id: 64e83ac52c506d28d483aee4
    type: comment
  author: armenabnousi
  content: Hi @ishaan812 , can you please share the CPU device specs you have successfully
    ran the XL model on.
  created_at: 2023-08-25 04:23:17+00:00
  edited: false
  hidden: false
  id: 64e83ac52c506d28d483aee4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: hkunlp/instructor-xl
repo_type: model
status: open
target_branch: null
title: Technical Specs required to run the XL
