!!python/object:huggingface_hub.community.DiscussionWithDetails
author: WasamiKirua
conflicting_files: null
created_at: 2023-10-10 12:00:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/651d625aff8745149ae562aa/2FNNEU-UlPVYMmhyfNuCp.jpeg?w=200&h=200&f=face
      fullname: Wasami
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WasamiKirua
      type: user
    createdAt: '2023-10-10T13:00:42.000Z'
    data:
      edited: false
      editors:
      - WasamiKirua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9267251491546631
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/651d625aff8745149ae562aa/2FNNEU-UlPVYMmhyfNuCp.jpeg?w=200&h=200&f=face
          fullname: Wasami
          isHf: false
          isPro: false
          name: WasamiKirua
          type: user
        html: '<p>HI, </p>

          <p>how this model differs from the Ministral 7B in terms of overall output
          quality ? I''m freaking out (because noob) to understand how shall I train
          Samantha on my own language but I have troubles understanding the starting
          point. for the ministral has been used the ChatML format but I did not understand
          if for both the ministral as well the llama the same dataset has been used
          (ehartford/samantha-data). I have been told to translate the samantha-1.1.json
          but at the same time i am wondering if:</p>

          <ol>

          <li>do i need to translate all the json/jsonl file under the folder "data"
          as well </li>

          <li>how to fine-tune Samantha using the translated dataset(s)</li>

          </ol>

          <p>do you have any suggestion or some channel/ml/whatever place where could
          I ask ? many thanks, appreciated</p>

          '
        raw: "HI, \r\n\r\nhow this model differs from the Ministral 7B in terms of\
          \ overall output quality ? I'm freaking out (because noob) to understand\
          \ how shall I train Samantha on my own language but I have troubles understanding\
          \ the starting point. for the ministral has been used the ChatML format\
          \ but I did not understand if for both the ministral as well the llama the\
          \ same dataset has been used (ehartford/samantha-data). I have been told\
          \ to translate the samantha-1.1.json but at the same time i am wondering\
          \ if:\r\n\r\n1) do i need to translate all the json/jsonl file under the\
          \ folder \"data\" as well \r\n2) how to fine-tune Samantha using the translated\
          \ dataset(s) \r\n\r\ndo you have any suggestion or some channel/ml/whatever\
          \ place where could I ask ? many thanks, appreciated"
        updatedAt: '2023-10-10T13:00:42.832Z'
      numEdits: 0
      reactions: []
    id: 65254afab6972a1a339badd2
    type: comment
  author: WasamiKirua
  content: "HI, \r\n\r\nhow this model differs from the Ministral 7B in terms of overall\
    \ output quality ? I'm freaking out (because noob) to understand how shall I train\
    \ Samantha on my own language but I have troubles understanding the starting point.\
    \ for the ministral has been used the ChatML format but I did not understand if\
    \ for both the ministral as well the llama the same dataset has been used (ehartford/samantha-data).\
    \ I have been told to translate the samantha-1.1.json but at the same time i am\
    \ wondering if:\r\n\r\n1) do i need to translate all the json/jsonl file under\
    \ the folder \"data\" as well \r\n2) how to fine-tune Samantha using the translated\
    \ dataset(s) \r\n\r\ndo you have any suggestion or some channel/ml/whatever place\
    \ where could I ask ? many thanks, appreciated"
  created_at: 2023-10-10 12:00:42+00:00
  edited: false
  hidden: false
  id: 65254afab6972a1a339badd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/783c1b9f5c63ea0f0d607b8de0003131.svg
      fullname: Ian Dennis Miller
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: iandennismiller
      type: user
    createdAt: '2023-10-10T20:11:07.000Z'
    data:
      edited: false
      editors:
      - iandennismiller
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9532443284988403
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/783c1b9f5c63ea0f0d607b8de0003131.svg
          fullname: Ian Dennis Miller
          isHf: false
          isPro: false
          name: iandennismiller
          type: user
        html: '<p>It sounds like you want to fine-tune a model by training it on a
          new language. Just know this is computationally expensive and highly technical.
          Also, the bigger the model, the more resources it requires.  Changing the
          .json is not sufficient.</p>

          <p>There are lots of learning resources for this but it really depends on
          the hardware you will use. I suggest searching YouTube for an introduction
          to LLM training, then going from there.</p>

          <p>Regarding these two specific models, there are quite a few differences:</p>

          <ul>

          <li>Samantha 33b is a fine-tune based on LLaMa. It was fine-tuned by ehartford
          by training the base LLaMa model on new inputs.</li>

          <li>Mistral 7b is a base model, just like LLaMa. There are now fine-tunes
          based on Mistral, including Mistral-Samantha.</li>

          <li>The other big difference is the number of parameters: 33b vs 7b. Samantha
          33b is "smarter" than Mistral 7b, both in terms of the size of the model
          and in terms of the quality of the output.</li>

          </ul>

          '
        raw: 'It sounds like you want to fine-tune a model by training it on a new
          language. Just know this is computationally expensive and highly technical.
          Also, the bigger the model, the more resources it requires.  Changing the
          .json is not sufficient.


          There are lots of learning resources for this but it really depends on the
          hardware you will use. I suggest searching YouTube for an introduction to
          LLM training, then going from there.


          Regarding these two specific models, there are quite a few differences:


          - Samantha 33b is a fine-tune based on LLaMa. It was fine-tuned by ehartford
          by training the base LLaMa model on new inputs.

          - Mistral 7b is a base model, just like LLaMa. There are now fine-tunes
          based on Mistral, including Mistral-Samantha.

          - The other big difference is the number of parameters: 33b vs 7b. Samantha
          33b is "smarter" than Mistral 7b, both in terms of the size of the model
          and in terms of the quality of the output.'
        updatedAt: '2023-10-10T20:11:07.429Z'
      numEdits: 0
      reactions: []
    id: 6525afdbf8db96cffc9f3c4e
    type: comment
  author: iandennismiller
  content: 'It sounds like you want to fine-tune a model by training it on a new language.
    Just know this is computationally expensive and highly technical. Also, the bigger
    the model, the more resources it requires.  Changing the .json is not sufficient.


    There are lots of learning resources for this but it really depends on the hardware
    you will use. I suggest searching YouTube for an introduction to LLM training,
    then going from there.


    Regarding these two specific models, there are quite a few differences:


    - Samantha 33b is a fine-tune based on LLaMa. It was fine-tuned by ehartford by
    training the base LLaMa model on new inputs.

    - Mistral 7b is a base model, just like LLaMa. There are now fine-tunes based
    on Mistral, including Mistral-Samantha.

    - The other big difference is the number of parameters: 33b vs 7b. Samantha 33b
    is "smarter" than Mistral 7b, both in terms of the size of the model and in terms
    of the quality of the output.'
  created_at: 2023-10-10 19:11:07+00:00
  edited: false
  hidden: false
  id: 6525afdbf8db96cffc9f3c4e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/651d625aff8745149ae562aa/2FNNEU-UlPVYMmhyfNuCp.jpeg?w=200&h=200&f=face
      fullname: Wasami
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WasamiKirua
      type: user
    createdAt: '2023-10-10T20:55:24.000Z'
    data:
      edited: true
      editors:
      - WasamiKirua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9627802968025208
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/651d625aff8745149ae562aa/2FNNEU-UlPVYMmhyfNuCp.jpeg?w=200&h=200&f=face
          fullname: Wasami
          isHf: false
          isPro: false
          name: WasamiKirua
          type: user
        html: '<p>Yes exactly, that''s the idea, train one of the two Ministral or
          LLama with the translated dataset. Since the Ministral 7B seems to be very
          promising I thought to start with it, simply because it would be much more
          "light" to run. The most powerful GPU available to me is a RTX 3080,I could
          try to run the Samantha 33B if I find ( is available if i am not mistaken
          ) a quantized version of it.  </p>

          <p>Anyway regarding the training yes there are a lot of video, unfortunately
          find a good one is not easy as find it they tend to be pretty confusing
          and skipping important details.</p>

          <p>i was triggered by this guy: <a rel="nofollow" href="https://www.youtube.com/watch?v=DhUsZ40jQb0">https://www.youtube.com/watch?v=DhUsZ40jQb0</a>
          it seems that he trained samantha so be able to speak in spanish</p>

          '
        raw: "Yes exactly, that's the idea, train one of the two Ministral or LLama\
          \ with the translated dataset. Since the Ministral 7B seems to be very promising\
          \ I thought to start with it, simply because it would be much more \"light\"\
          \ to run. The most powerful GPU available to me is a RTX 3080,I could try\
          \ to run the Samantha 33B if I find ( is available if i am not mistaken\
          \ ) a quantized version of it.  \n\nAnyway regarding the training yes there\
          \ are a lot of video, unfortunately find a good one is not easy as find\
          \ it they tend to be pretty confusing and skipping important details.\n\n\
          i was triggered by this guy: https://www.youtube.com/watch?v=DhUsZ40jQb0\
          \ it seems that he trained samantha so be able to speak in spanish"
        updatedAt: '2023-10-10T20:58:48.410Z'
      numEdits: 2
      reactions: []
    id: 6525ba3cfb1678bd6c4d4d48
    type: comment
  author: WasamiKirua
  content: "Yes exactly, that's the idea, train one of the two Ministral or LLama\
    \ with the translated dataset. Since the Ministral 7B seems to be very promising\
    \ I thought to start with it, simply because it would be much more \"light\" to\
    \ run. The most powerful GPU available to me is a RTX 3080,I could try to run\
    \ the Samantha 33B if I find ( is available if i am not mistaken ) a quantized\
    \ version of it.  \n\nAnyway regarding the training yes there are a lot of video,\
    \ unfortunately find a good one is not easy as find it they tend to be pretty\
    \ confusing and skipping important details.\n\ni was triggered by this guy: https://www.youtube.com/watch?v=DhUsZ40jQb0\
    \ it seems that he trained samantha so be able to speak in spanish"
  created_at: 2023-10-10 19:55:24+00:00
  edited: true
  hidden: false
  id: 6525ba3cfb1678bd6c4d4d48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/783c1b9f5c63ea0f0d607b8de0003131.svg
      fullname: Ian Dennis Miller
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: iandennismiller
      type: user
    createdAt: '2023-10-11T20:02:18.000Z'
    data:
      edited: false
      editors:
      - iandennismiller
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.849753737449646
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/783c1b9f5c63ea0f0d607b8de0003131.svg
          fullname: Ian Dennis Miller
          isHf: false
          isPro: false
          name: iandennismiller
          type: user
        html: '<p>Perhaps start by training a small model from scratch, then learning
          how to finetune the small model.  Once you''ve done that, you can apply
          what you learned to Mistral or Samantha.</p>

          <p>Here''s a neat tutorial I found on the /r/LocalLlama subreddit: <a rel="nofollow"
          href="https://old.reddit.com/r/LocalLLaMA/comments/14dstqm/tutorial_train_your_own_llamacpp_miniggmlmodel/">https://old.reddit.com/r/LocalLLaMA/comments/14dstqm/tutorial_train_your_own_llamacpp_miniggmlmodel/</a></p>

          <p>That should be small enough to run on most systems.</p>

          <p>Then, fine-tune your small model.  Since it''s small, it will be easier
          to run - and therefore easier to debug and learn from.</p>

          <p>Here''s a good example of fine-tuning: <a rel="nofollow" href="https://github.com/mzbac/qlora-fine-tune">https://github.com/mzbac/qlora-fine-tune</a></p>

          <p>Another resource for finetuning is the llama.cpp example: <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/tree/master/examples/finetune">https://github.com/ggerganov/llama.cpp/tree/master/examples/finetune</a></p>

          '
        raw: 'Perhaps start by training a small model from scratch, then learning
          how to finetune the small model.  Once you''ve done that, you can apply
          what you learned to Mistral or Samantha.


          Here''s a neat tutorial I found on the /r/LocalLlama subreddit: https://old.reddit.com/r/LocalLLaMA/comments/14dstqm/tutorial_train_your_own_llamacpp_miniggmlmodel/


          That should be small enough to run on most systems.


          Then, fine-tune your small model.  Since it''s small, it will be easier
          to run - and therefore easier to debug and learn from.


          Here''s a good example of fine-tuning: https://github.com/mzbac/qlora-fine-tune


          Another resource for finetuning is the llama.cpp example: https://github.com/ggerganov/llama.cpp/tree/master/examples/finetune'
        updatedAt: '2023-10-11T20:02:18.756Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - WasamiKirua
    id: 6526ff4a218b15a0e1e5e32a
    type: comment
  author: iandennismiller
  content: 'Perhaps start by training a small model from scratch, then learning how
    to finetune the small model.  Once you''ve done that, you can apply what you learned
    to Mistral or Samantha.


    Here''s a neat tutorial I found on the /r/LocalLlama subreddit: https://old.reddit.com/r/LocalLLaMA/comments/14dstqm/tutorial_train_your_own_llamacpp_miniggmlmodel/


    That should be small enough to run on most systems.


    Then, fine-tune your small model.  Since it''s small, it will be easier to run
    - and therefore easier to debug and learn from.


    Here''s a good example of fine-tuning: https://github.com/mzbac/qlora-fine-tune


    Another resource for finetuning is the llama.cpp example: https://github.com/ggerganov/llama.cpp/tree/master/examples/finetune'
  created_at: 2023-10-11 19:02:18+00:00
  edited: false
  hidden: false
  id: 6526ff4a218b15a0e1e5e32a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/651d625aff8745149ae562aa/2FNNEU-UlPVYMmhyfNuCp.jpeg?w=200&h=200&f=face
      fullname: Wasami
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WasamiKirua
      type: user
    createdAt: '2023-10-11T20:48:49.000Z'
    data:
      edited: false
      editors:
      - WasamiKirua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9866719245910645
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/651d625aff8745149ae562aa/2FNNEU-UlPVYMmhyfNuCp.jpeg?w=200&h=200&f=face
          fullname: Wasami
          isHf: false
          isPro: false
          name: WasamiKirua
          type: user
        html: '<p>thank you man, very appreciated. you took the time and the patience
          nobody else took. I will study further. cheers and take care</p>

          '
        raw: thank you man, very appreciated. you took the time and the patience nobody
          else took. I will study further. cheers and take care
        updatedAt: '2023-10-11T20:48:49.919Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - iandennismiller
    id: 65270a31f4fafd2386d77519
    type: comment
  author: WasamiKirua
  content: thank you man, very appreciated. you took the time and the patience nobody
    else took. I will study further. cheers and take care
  created_at: 2023-10-11 19:48:49+00:00
  edited: false
  hidden: false
  id: 65270a31f4fafd2386d77519
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/783c1b9f5c63ea0f0d607b8de0003131.svg
      fullname: Ian Dennis Miller
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: iandennismiller
      type: user
    createdAt: '2023-11-12T14:50:14.000Z'
    data:
      status: closed
    id: 6550e6263fe6c0b1f8b1f05b
    type: status-change
  author: iandennismiller
  created_at: 2023-11-12 14:50:14+00:00
  id: 6550e6263fe6c0b1f8b1f05b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: iandennismiller/samantha-1.1-llama-33b-GGUF
repo_type: model
status: closed
target_branch: null
title: questions
