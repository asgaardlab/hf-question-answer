!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Hoioi
conflicting_files: null
created_at: 2024-01-19 12:14:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
      fullname: Hut hiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hoioi
      type: user
    createdAt: '2024-01-19T12:14:51.000Z'
    data:
      edited: false
      editors:
      - Hoioi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9666350483894348
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
          fullname: Hut hiu
          isHf: false
          isPro: false
          name: Hoioi
          type: user
        html: '<p>I have tested the q4_k_m version and it works perfectly fine. Thank
          you so much for this great model.<br>I only have an issue not only with
          your model but with all models (more than 150 models) i have tested so far
          but as you are a helpful and knowledge person in AI, I asked you this issue
          and i hope that you guide me through this. </p>

          <p>The issue is that when I put a full article as input for example an article
          with 1500 words and I want the AI model to edit it or rewrite it somehow,
          the output will become  so much smaller than the input for example the output
          will be 900 words and it seems that the AI model forgets some parts of the
          input.<br>I have tested many 32k models or even 200k models  and i tested
          it with different tools like oobabooga, lmstudio, gpt4all, koboldcpp and
          changed their settings to overcome this issue, but the issue remains the
          same. </p>

          <p> I would like to ask you to help me regarding this issue and tell me
          how can I put a full article as input and expect a model to rewrite it for
          me as I want without losing any details of it and without making in making
          it much smaller. </p>

          <p> the only way I I know so far is to split the article into i.e 10 parts
          manually and put each part separately in input, and get output and finally
          combine the whole outputs together to make a full article with a tedious
          job.<br>I would be so grateful if you could help me regarding this problem.
          </p>

          '
        raw: "I have tested the q4_k_m version and it works perfectly fine. Thank\
          \ you so much for this great model. \r\nI only have an issue not only with\
          \ your model but with all models (more than 150 models) i have tested so\
          \ far but as you are a helpful and knowledge person in AI, I asked you this\
          \ issue and i hope that you guide me through this. \r\n\r\nThe issue is\
          \ that when I put a full article as input for example an article with 1500\
          \ words and I want the AI model to edit it or rewrite it somehow, the output\
          \ will become  so much smaller than the input for example the output will\
          \ be 900 words and it seems that the AI model forgets some parts of the\
          \ input.\r\nI have tested many 32k models or even 200k models  and i tested\
          \ it with different tools like oobabooga, lmstudio, gpt4all, koboldcpp and\
          \ changed their settings to overcome this issue, but the issue remains the\
          \ same. \r\n\r\n I would like to ask you to help me regarding this issue\
          \ and tell me how can I put a full article as input and expect a model to\
          \ rewrite it for me as I want without losing any details of it and without\
          \ making in making it much smaller. \r\n\r\n the only way I I know so far\
          \ is to split the article into i.e 10 parts manually and put each part separately\
          \ in input, and get output and finally combine the whole outputs together\
          \ to make a full article with a tedious job. \r\nI would be so grateful\
          \ if you could help me regarding this problem. \r\n\r\n"
        updatedAt: '2024-01-19T12:14:51.516Z'
      numEdits: 0
      reactions: []
    id: 65aa67bbc8903e28ae9cce91
    type: comment
  author: Hoioi
  content: "I have tested the q4_k_m version and it works perfectly fine. Thank you\
    \ so much for this great model. \r\nI only have an issue not only with your model\
    \ but with all models (more than 150 models) i have tested so far but as you are\
    \ a helpful and knowledge person in AI, I asked you this issue and i hope that\
    \ you guide me through this. \r\n\r\nThe issue is that when I put a full article\
    \ as input for example an article with 1500 words and I want the AI model to edit\
    \ it or rewrite it somehow, the output will become  so much smaller than the input\
    \ for example the output will be 900 words and it seems that the AI model forgets\
    \ some parts of the input.\r\nI have tested many 32k models or even 200k models\
    \  and i tested it with different tools like oobabooga, lmstudio, gpt4all, koboldcpp\
    \ and changed their settings to overcome this issue, but the issue remains the\
    \ same. \r\n\r\n I would like to ask you to help me regarding this issue and tell\
    \ me how can I put a full article as input and expect a model to rewrite it for\
    \ me as I want without losing any details of it and without making in making it\
    \ much smaller. \r\n\r\n the only way I I know so far is to split the article\
    \ into i.e 10 parts manually and put each part separately in input, and get output\
    \ and finally combine the whole outputs together to make a full article with a\
    \ tedious job. \r\nI would be so grateful if you could help me regarding this\
    \ problem. \r\n\r\n"
  created_at: 2024-01-19 12:14:51+00:00
  edited: false
  hidden: false
  id: 65aa67bbc8903e28ae9cce91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
      fullname: Tim Dolan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: macadeliccc
      type: user
    createdAt: '2024-01-20T05:54:06.000Z'
    data:
      edited: false
      editors:
      - macadeliccc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.740425169467926
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
          fullname: Tim Dolan
          isHf: false
          isPro: false
          name: macadeliccc
          type: user
        html: '<p>The new quants are available here <a href="https://huggingface.co/macadeliccc/laser-dolphin-mixtral-2x7b-GGUF">https://huggingface.co/macadeliccc/laser-dolphin-mixtral-2x7b-GGUF</a>.<br>There
          is also now a GGUF chat in spaces for the model: <a href="https://huggingface.co/spaces/macadeliccc/laser-dolphin-mixtral-chat-GGUF">https://huggingface.co/spaces/macadeliccc/laser-dolphin-mixtral-chat-GGUF</a>.</p>

          <p>To fix the issue regarding you problem the answer is context.  Run the
          llama.cpp <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md">server</a>
          and make sure <code>n_ctx</code> is set to something like 32xxx by default</p>

          '
        raw: "The new quants are available here https://huggingface.co/macadeliccc/laser-dolphin-mixtral-2x7b-GGUF.\
          \ \nThere is also now a GGUF chat in spaces for the model: https://huggingface.co/spaces/macadeliccc/laser-dolphin-mixtral-chat-GGUF.\n\
          \nTo fix the issue regarding you problem the answer is context.  Run the\
          \ llama.cpp [server](https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md)\
          \ and make sure `n_ctx` is set to something like 32xxx by default"
        updatedAt: '2024-01-20T05:54:06.759Z'
      numEdits: 0
      reactions: []
    id: 65ab5ffe5860f06ff2edf957
    type: comment
  author: macadeliccc
  content: "The new quants are available here https://huggingface.co/macadeliccc/laser-dolphin-mixtral-2x7b-GGUF.\
    \ \nThere is also now a GGUF chat in spaces for the model: https://huggingface.co/spaces/macadeliccc/laser-dolphin-mixtral-chat-GGUF.\n\
    \nTo fix the issue regarding you problem the answer is context.  Run the llama.cpp\
    \ [server](https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md)\
    \ and make sure `n_ctx` is set to something like 32xxx by default"
  created_at: 2024-01-20 05:54:06+00:00
  edited: false
  hidden: false
  id: 65ab5ffe5860f06ff2edf957
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
      fullname: H R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HR1777
      type: user
    createdAt: '2024-01-20T06:50:39.000Z'
    data:
      edited: true
      editors:
      - HR1777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9650471806526184
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
          fullname: H R
          isHf: false
          isPro: false
          name: HR1777
          type: user
        html: '<p>Thanks for the new model. What''s different in this model compared
          to the previous one?</p>

          '
        raw: Thanks for the new model. What's different in this model compared to
          the previous one?
        updatedAt: '2024-01-20T06:51:00.633Z'
      numEdits: 1
      reactions: []
    id: 65ab6d3f48c718a574596c25
    type: comment
  author: HR1777
  content: Thanks for the new model. What's different in this model compared to the
    previous one?
  created_at: 2024-01-20 06:50:39+00:00
  edited: true
  hidden: false
  id: 65ab6d3f48c718a574596c25
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
      fullname: H R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HR1777
      type: user
    createdAt: '2024-01-20T06:51:27.000Z'
    data:
      edited: false
      editors:
      - HR1777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9723122119903564
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
          fullname: H R
          isHf: false
          isPro: false
          name: HR1777
          type: user
        html: '<p>I have the same issue. I set n_ctx to the maximum which is 32768,
          but the issue persists. I don''t know how to put a full article in the input
          and ask the model to expand it without loosing details of it. Any other
          solution?</p>

          '
        raw: I have the same issue. I set n_ctx to the maximum which is 32768, but
          the issue persists. I don't know how to put a full article in the input
          and ask the model to expand it without loosing details of it. Any other
          solution?
        updatedAt: '2024-01-20T06:51:27.837Z'
      numEdits: 0
      reactions: []
    id: 65ab6d6f0150f64adf12e390
    type: comment
  author: HR1777
  content: I have the same issue. I set n_ctx to the maximum which is 32768, but the
    issue persists. I don't know how to put a full article in the input and ask the
    model to expand it without loosing details of it. Any other solution?
  created_at: 2024-01-20 06:51:27+00:00
  edited: false
  hidden: false
  id: 65ab6d6f0150f64adf12e390
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
      fullname: Tim Dolan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: macadeliccc
      type: user
    createdAt: '2024-01-20T07:05:36.000Z'
    data:
      edited: true
      editors:
      - macadeliccc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9447638988494873
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
          fullname: Tim Dolan
          isHf: false
          isPro: false
          name: macadeliccc
          type: user
        html: "<p>This version is better at math, has better prompt alignment, and\
          \ it fixes a chat template error from the previous merge.</p>\n<p>As for\
          \ the context issue, language modeling is fairly experimental in general\
          \ even though its become so common place. Aside from implementing a custom\
          \ logits processor and controlling how abstractive/extractive the generation\
          \ is, I don't see another way to control whether the output has the exact\
          \ same tokens as the input.</p>\n<p>Short answer: you very well may have\
          \ difficulty retaining the information you want in your article. I would\
          \ experiment with prompts and try shorter articles first. <span data-props=\"\
          {&quot;user&quot;:&quot;HR1777&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/HR1777\">@<span class=\"underline\">HR1777</span></a></span>\n\
          \n\t</span></span> </p>\n<p>Also I would try the new quants in the link\
          \ above. You will likely have a better experience</p>\n"
        raw: "This version is better at math, has better prompt alignment, and it\
          \ fixes a chat template error from the previous merge.\n\nAs for the context\
          \ issue, language modeling is fairly experimental in general even though\
          \ its become so common place. Aside from implementing a custom logits processor\
          \ and controlling how abstractive/extractive the generation is, I don't\
          \ see another way to control whether the output has the exact same tokens\
          \ as the input.\n\nShort answer: you very well may have difficulty retaining\
          \ the information you want in your article. I would experiment with prompts\
          \ and try shorter articles first. @HR1777 \n\nAlso I would try the new quants\
          \ in the link above. You will likely have a better experience\n"
        updatedAt: '2024-01-20T07:06:27.431Z'
      numEdits: 1
      reactions: []
    id: 65ab70c0fd4261f531283c24
    type: comment
  author: macadeliccc
  content: "This version is better at math, has better prompt alignment, and it fixes\
    \ a chat template error from the previous merge.\n\nAs for the context issue,\
    \ language modeling is fairly experimental in general even though its become so\
    \ common place. Aside from implementing a custom logits processor and controlling\
    \ how abstractive/extractive the generation is, I don't see another way to control\
    \ whether the output has the exact same tokens as the input.\n\nShort answer:\
    \ you very well may have difficulty retaining the information you want in your\
    \ article. I would experiment with prompts and try shorter articles first. @HR1777\
    \ \n\nAlso I would try the new quants in the link above. You will likely have\
    \ a better experience\n"
  created_at: 2024-01-20 07:05:36+00:00
  edited: true
  hidden: false
  id: 65ab70c0fd4261f531283c24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
      fullname: H R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HR1777
      type: user
    createdAt: '2024-01-20T07:40:53.000Z'
    data:
      edited: false
      editors:
      - HR1777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9532141089439392
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
          fullname: H R
          isHf: false
          isPro: false
          name: HR1777
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;macadeliccc&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/macadeliccc\"\
          >@<span class=\"underline\">macadeliccc</span></a></span>\n\n\t</span></span>,\
          \ thanks for the improvement of the model and your explanations. When i\
          \ give the model i.e. 2 paragraphs, it works pretty well, but when i put\
          \ a full article which is i.e. 1700 words, it can't handle it properly.\
          \ I even put a 1700 words article in here and asked it to expand it <a href=\"\
          https://huggingface.co/spaces/macadeliccc/laser-dolphin-mixtral-chat-GGUF\"\
          >https://huggingface.co/spaces/macadeliccc/laser-dolphin-mixtral-chat-GGUF</a>\
          \ and although it took more than 20 minutes. the output was less than 600\
          \ words!<br>I believe its a big issue with all models i have tested so far.\
          \ I hope you can find a solution or develop a model for better handling\
          \ long inputs.</p>\n"
        raw: '@macadeliccc, thanks for the improvement of the model and your explanations.
          When i give the model i.e. 2 paragraphs, it works pretty well, but when
          i put a full article which is i.e. 1700 words, it can''t handle it properly.
          I even put a 1700 words article in here and asked it to expand it https://huggingface.co/spaces/macadeliccc/laser-dolphin-mixtral-chat-GGUF
          and although it took more than 20 minutes. the output was less than 600
          words!

          I believe its a big issue with all models i have tested so far. I hope you
          can find a solution or develop a model for better handling long inputs.'
        updatedAt: '2024-01-20T07:40:53.141Z'
      numEdits: 0
      reactions: []
    id: 65ab7905d6b4c93ba6c8e76d
    type: comment
  author: HR1777
  content: '@macadeliccc, thanks for the improvement of the model and your explanations.
    When i give the model i.e. 2 paragraphs, it works pretty well, but when i put
    a full article which is i.e. 1700 words, it can''t handle it properly. I even
    put a 1700 words article in here and asked it to expand it https://huggingface.co/spaces/macadeliccc/laser-dolphin-mixtral-chat-GGUF
    and although it took more than 20 minutes. the output was less than 600 words!

    I believe its a big issue with all models i have tested so far. I hope you can
    find a solution or develop a model for better handling long inputs.'
  created_at: 2024-01-20 07:40:53+00:00
  edited: false
  hidden: false
  id: 65ab7905d6b4c93ba6c8e76d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: macadeliccc/laser-dolphin-mixtral-2x7b-dpo-GGUF
repo_type: model
status: open
target_branch: null
title: Please help me with this issue
