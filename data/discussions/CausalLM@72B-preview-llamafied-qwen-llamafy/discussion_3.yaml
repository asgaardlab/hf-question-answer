!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ehartford
conflicting_files: null
created_at: 2023-12-02 00:28:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-12-02T00:28:14.000Z'
    data:
      edited: true
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8575240969657898
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>Can we have just a llama-compatible base model please?  (as opposed
          to a chat model)</p>

          '
        raw: Can we have just a llama-compatible base model please?  (as opposed to
          a chat model)
        updatedAt: '2023-12-02T00:29:44.451Z'
      numEdits: 1
      reactions: []
    id: 656a7a1eaf6d3c4129ee87b0
    type: comment
  author: ehartford
  content: Can we have just a llama-compatible base model please?  (as opposed to
    a chat model)
  created_at: 2023-12-02 00:28:14+00:00
  edited: true
  hidden: false
  id: 656a7a1eaf6d3c4129ee87b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
      fullname: "Jos\xE9phus Cheung"
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: JosephusCheung
      type: user
    createdAt: '2023-12-02T03:34:56.000Z'
    data:
      edited: true
      editors:
      - JosephusCheung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9511446356773376
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
          fullname: "Jos\xE9phus Cheung"
          isHf: false
          isPro: false
          name: JosephusCheung
          type: user
        html: '<p>The implementations of Qwen and LLaMA-2 are slightly different except
          for the different tensor names, so some modifications were made in addition
          to renaming. And I decide remove the QKV attention bias, as I have done
          these operations on 14B and 7B, but there are still some issues being dealt
          with on 72B, which is why it''s still in preview. Attention bias was once
          considered to optimize the model''s extrapolation performance, but it in
          fact introduced some problems. And recently, the author of RoPE updated
          his <a rel="nofollow" href="https://kexue.fm/archives/9577">blog post</a>
          about the instability of reproducibility in length extrapolation results.</p>

          <p>The following plot shows the loss during calibration, compared to the
          original Qwen implementation.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63468a143ea42ee2cb49ddd1/LY8T0erqhhoDej97hQyW8.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/63468a143ea42ee2cb49ddd1/LY8T0erqhhoDej97hQyW8.png"></a></p>

          <p>Since this is still a work in progress, I will share more details once
          the matter at hand is completed.</p>

          '
        raw: 'The implementations of Qwen and LLaMA-2 are slightly different except
          for the different tensor names, so some modifications were made in addition
          to renaming. And I decide remove the QKV attention bias, as I have done
          these operations on 14B and 7B, but there are still some issues being dealt
          with on 72B, which is why it''s still in preview. Attention bias was once
          considered to optimize the model''s extrapolation performance, but it in
          fact introduced some problems. And recently, the author of RoPE updated
          his [blog post](https://kexue.fm/archives/9577) about the instability of
          reproducibility in length extrapolation results.


          The following plot shows the loss during calibration, compared to the original
          Qwen implementation.


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63468a143ea42ee2cb49ddd1/LY8T0erqhhoDej97hQyW8.png)


          Since this is still a work in progress, I will share more details once the
          matter at hand is completed.'
        updatedAt: '2023-12-02T03:35:38.965Z'
      numEdits: 1
      reactions: []
      relatedEventId: 656aa5e0ede71189adeb3a56
    id: 656aa5e0ede71189adeb3a53
    type: comment
  author: JosephusCheung
  content: 'The implementations of Qwen and LLaMA-2 are slightly different except
    for the different tensor names, so some modifications were made in addition to
    renaming. And I decide remove the QKV attention bias, as I have done these operations
    on 14B and 7B, but there are still some issues being dealt with on 72B, which
    is why it''s still in preview. Attention bias was once considered to optimize
    the model''s extrapolation performance, but it in fact introduced some problems.
    And recently, the author of RoPE updated his [blog post](https://kexue.fm/archives/9577)
    about the instability of reproducibility in length extrapolation results.


    The following plot shows the loss during calibration, compared to the original
    Qwen implementation.


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63468a143ea42ee2cb49ddd1/LY8T0erqhhoDej97hQyW8.png)


    Since this is still a work in progress, I will share more details once the matter
    at hand is completed.'
  created_at: 2023-12-02 03:34:56+00:00
  edited: true
  hidden: false
  id: 656aa5e0ede71189adeb3a53
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
      fullname: "Jos\xE9phus Cheung"
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: JosephusCheung
      type: user
    createdAt: '2023-12-02T03:34:56.000Z'
    data:
      status: closed
    id: 656aa5e0ede71189adeb3a56
    type: status-change
  author: JosephusCheung
  created_at: 2023-12-02 03:34:56+00:00
  id: 656aa5e0ede71189adeb3a56
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: CausalLM/72B-preview-llamafied-qwen-llamafy
repo_type: model
status: closed
target_branch: null
title: Base model
