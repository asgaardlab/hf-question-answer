!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lucazug
conflicting_files: null
created_at: 2022-12-20 18:12:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0298cf701b644a72ea957e59406627f.svg
      fullname: Luca Zug
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucazug
      type: user
    createdAt: '2022-12-20T18:12:39.000Z'
    data:
      edited: true
      editors:
      - lucazug
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0298cf701b644a72ea957e59406627f.svg
          fullname: Luca Zug
          isHf: false
          isPro: false
          name: lucazug
          type: user
        html: "<p>Hi!<br>Is it possible to use a websocket in python to obtain word\
          \ embeddings through the Inference API for the GBERT-base model?</p>\n<p>I\
          \ have been able to collect word embeddings through individual HTTP requests\
          \ for every sentence. However, that is very time consuming. I've been struggling\
          \ with implementing a websocket (as in <a href=\"https://huggingface.co/docs/api-inference/parallelism\"\
          >https://huggingface.co/docs/api-inference/parallelism</a>) to obtain word\
          \ embeddings \u2013 the websocket only works for multiple sentences with\
          \ a [MASK] token (wss://api-inference.huggingface.co/bulk/stream/cpu/deepset/gbert-base).\
          \  The websocket wss for GBERT-base does not seem to work, though (wss://api-inference.huggingface.co/pipeline/feature-extraction/deepset/gbert-base).</p>\n\
          <p>Any ideas? Thank you for your help in advance! (Response in German would\
          \ also be fine with me)</p>\n"
        raw: "Hi!\nIs it possible to use a websocket in python to obtain word embeddings\
          \ through the Inference API for the GBERT-base model?\n\nI have been able\
          \ to collect word embeddings through individual HTTP requests for every\
          \ sentence. However, that is very time consuming. I've been struggling with\
          \ implementing a websocket (as in https://huggingface.co/docs/api-inference/parallelism)\
          \ to obtain word embeddings \u2013 the websocket only works for multiple\
          \ sentences with a [MASK] token (wss://api-inference.huggingface.co/bulk/stream/cpu/deepset/gbert-base).\
          \  The websocket wss for GBERT-base does not seem to work, though (wss://api-inference.huggingface.co/pipeline/feature-extraction/deepset/gbert-base).\n\
          \nAny ideas? Thank you for your help in advance! (Response in German would\
          \ also be fine with me)"
        updatedAt: '2022-12-20T18:13:13.234Z'
      numEdits: 2
      reactions: []
    id: 63a1fb17e36f2e4d5b137301
    type: comment
  author: lucazug
  content: "Hi!\nIs it possible to use a websocket in python to obtain word embeddings\
    \ through the Inference API for the GBERT-base model?\n\nI have been able to collect\
    \ word embeddings through individual HTTP requests for every sentence. However,\
    \ that is very time consuming. I've been struggling with implementing a websocket\
    \ (as in https://huggingface.co/docs/api-inference/parallelism) to obtain word\
    \ embeddings \u2013 the websocket only works for multiple sentences with a [MASK]\
    \ token (wss://api-inference.huggingface.co/bulk/stream/cpu/deepset/gbert-base).\
    \  The websocket wss for GBERT-base does not seem to work, though (wss://api-inference.huggingface.co/pipeline/feature-extraction/deepset/gbert-base).\n\
    \nAny ideas? Thank you for your help in advance! (Response in German would also\
    \ be fine with me)"
  created_at: 2022-12-20 18:12:39+00:00
  edited: true
  hidden: false
  id: 63a1fb17e36f2e4d5b137301
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648966381588-6064e095abd8d3692e3e2ed6.jpeg?w=200&h=200&f=face
      fullname: "Radam\xE9s Ajna"
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: radames
      type: user
    createdAt: '2022-12-20T20:17:06.000Z'
    data:
      edited: true
      editors:
      - radames
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648966381588-6064e095abd8d3692e3e2ed6.jpeg?w=200&h=200&f=face
          fullname: "Radam\xE9s Ajna"
          isHf: true
          isPro: false
          name: radames
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;lucazug&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lucazug\">@<span class=\"\
          underline\">lucazug</span></a></span>\n\n\t</span></span>,</p>\n<p>Just\
          \ tested with the following python code using websockets and worked fine<br>But\
          \ I'd recommend you to look into our <a href=\"https://huggingface.co/inference-endpoints\"\
          >Inference Endpoints Solution</a><br><a rel=\"nofollow\" href=\"https://ui.endpoints.huggingface.co/new?repository=deepset/gbert-base\"\
          >https://ui.endpoints.huggingface.co/new?repository=deepset/gbert-base</a>\
          \ </p>\n<p>Here <code>HUGGING_FACE_HUB_TOKEN</code> is your access token</p>\n\
          <pre><code class=\"language-bash\">HUGGING_FACE_HUB_TOKEN=hf_XXXXXXXXXX\
          \ python app.py\n</code></pre>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\"\
          >import</span> asyncio\n<span class=\"hljs-keyword\">import</span> json\n\
          <span class=\"hljs-keyword\">import</span> uuid\n<span class=\"hljs-keyword\"\
          >import</span> websockets\nMODEL_ID = <span class=\"hljs-string\">\"deepset/gbert-base\"\
          </span>\nCOMPUTE_TYPE = <span class=\"hljs-string\">\"cpu\"</span>  <span\
          \ class=\"hljs-comment\"># or \"gpu\"</span>\nAPI_TOKEN = os.environ[<span\
          \ class=\"hljs-string\">\"HUGGING_FACE_HUB_TOKEN\"</span>]\n\n<span class=\"\
          hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span\
          \ class=\"hljs-title function_\">send</span>(<span class=\"hljs-params\"\
          >websocket, payloads</span>):\n    <span class=\"hljs-comment\"># You need\
          \ to login with a first message as headers are not forwarded</span>\n  \
          \  <span class=\"hljs-comment\"># for websockets</span>\n    <span class=\"\
          hljs-keyword\">await</span> websocket.send(<span class=\"hljs-string\">f\"\
          Bearer <span class=\"hljs-subst\">{API_TOKEN}</span>\"</span>.encode(<span\
          \ class=\"hljs-string\">\"utf-8\"</span>))\n    <span class=\"hljs-keyword\"\
          >for</span> payload <span class=\"hljs-keyword\">in</span> payloads:\n \
          \       <span class=\"hljs-keyword\">await</span> websocket.send(json.dumps(payload).encode(<span\
          \ class=\"hljs-string\">\"utf-8\"</span>))\n        <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"Sent \"</span>)\n<span class=\"\
          hljs-keyword\">async</span> <span class=\"hljs-keyword\">def</span> <span\
          \ class=\"hljs-title function_\">recv</span>(<span class=\"hljs-params\"\
          >websocket, last_id</span>):\n    outputs = []\n    <span class=\"hljs-keyword\"\
          >while</span> <span class=\"hljs-literal\">True</span>:\n        data =\
          \ <span class=\"hljs-keyword\">await</span> websocket.recv()\n        payload\
          \ = json.loads(data)\n        <span class=\"hljs-keyword\">if</span> payload[<span\
          \ class=\"hljs-string\">\"type\"</span>] == <span class=\"hljs-string\"\
          >\"results\"</span>:\n            <span class=\"hljs-comment\"># {\"type\"\
          : \"results\", \"outputs\": JSONFormatted results, \"id\": the id we sent}</span>\n\
          \            <span class=\"hljs-built_in\">print</span>(payload[<span class=\"\
          hljs-string\">\"outputs\"</span>])\n            outputs.append(payload[<span\
          \ class=\"hljs-string\">\"outputs\"</span>])\n            <span class=\"\
          hljs-keyword\">if</span> payload[<span class=\"hljs-string\">\"id\"</span>]\
          \ == last_id:\n                <span class=\"hljs-keyword\">return</span>\
          \ outputs\n        <span class=\"hljs-keyword\">else</span>:\n         \
          \   <span class=\"hljs-comment\"># {\"type\": \"status\", \"message\": \"\
          Some information about the queue\"}</span>\n            <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">f\"&lt; <span class=\"hljs-subst\"\
          >{payload[<span class=\"hljs-string\">'message'</span>]}</span>\"</span>)\n\
          \            <span class=\"hljs-keyword\">pass</span>\n<span class=\"hljs-keyword\"\
          >async</span> <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\
          \ function_\">main</span>():\n    uri = <span class=\"hljs-string\">f\"\
          wss://api-inference.huggingface.co/bulk/stream/<span class=\"hljs-subst\"\
          >{COMPUTE_TYPE}</span>/<span class=\"hljs-subst\">{MODEL_ID}</span>\"</span>\n\
          \    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"Connecting to <span class=\"hljs-subst\">{uri}</span>\"</span>)\n  \
          \  <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\"\
          >with</span> websockets.connect(uri) <span class=\"hljs-keyword\">as</span>\
          \ websocket:\n        <span class=\"hljs-comment\"># inputs and parameters\
          \ are classic, \"id\" is a way to track that query</span>\n        payloads\
          \ = [\n            {\n                <span class=\"hljs-string\">\"id\"\
          </span>: <span class=\"hljs-built_in\">str</span>(uuid.uuid4()),\n     \
          \           <span class=\"hljs-string\">\"inputs\"</span>: <span class=\"\
          hljs-string\">\"Das Ziel des Lebens ist [MASK].\"</span>,\n            }\n\
          \            <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\"\
          >in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\"\
          >10</span>)\n        ]\n        last_id = payloads[-<span class=\"hljs-number\"\
          >1</span>][<span class=\"hljs-string\">\"id\"</span>]\n        future =\
          \ send(websocket, payloads)\n        future_r = recv(websocket, last_id)\n\
          \        _, outputs = <span class=\"hljs-keyword\">await</span> asyncio.gather(future,\
          \ future_r)\n    results = [out[<span class=\"hljs-string\">\"labels\"</span>][<span\
          \ class=\"hljs-number\">0</span>] <span class=\"hljs-keyword\">for</span>\
          \ out <span class=\"hljs-keyword\">in</span> outputs]\n    <span class=\"\
          hljs-keyword\">return</span> results\nloop = asyncio.get_event_loop()\n\
          <span class=\"hljs-keyword\">if</span> loop.is_running():\n    <span class=\"\
          hljs-comment\"># When running in notebooks</span>\n    <span class=\"hljs-keyword\"\
          >import</span> nest_asyncio\n    nest_asyncio.apply()\nresults = loop.run_until_complete(main())\n\
          </code></pre>\n<pre><code class=\"language-json\"><span class=\"hljs-punctuation\"\
          >[</span>\n    <span class=\"hljs-punctuation\">{</span>'score'<span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-number\">0.25408321619033813</span><span\
          \ class=\"hljs-punctuation\">,</span> 'token'<span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">3531</span><span class=\"hljs-punctuation\"\
          >,</span> 'token_str'<span class=\"hljs-punctuation\">:</span> 'erreicht'<span\
          \ class=\"hljs-punctuation\">,</span> 'sequence'<span class=\"hljs-punctuation\"\
          >:</span> 'Das Ziel des Lebens ist erreicht.'\n    <span class=\"hljs-punctuation\"\
          >}</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-punctuation\"\
          >{</span>'score'<span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-number\">0.04424438253045082</span><span class=\"hljs-punctuation\"\
          >,</span> 'token'<span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-number\">7939</span><span class=\"hljs-punctuation\">,</span> 'token_str'<span\
          \ class=\"hljs-punctuation\">:</span> 'erf\xFCllt'<span class=\"hljs-punctuation\"\
          >,</span> 'sequence'<span class=\"hljs-punctuation\">:</span> 'Das Ziel\
          \ des Lebens ist erf\xFCllt.'\n    <span class=\"hljs-punctuation\">}</span><span\
          \ class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-punctuation\"\
          >{</span>'score'<span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-number\">0.031005313619971275</span><span class=\"hljs-punctuation\"\
          >,</span> 'token'<span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-number\">199</span><span class=\"hljs-punctuation\">,</span> 'token_str'<span\
          \ class=\"hljs-punctuation\">:</span> 'das'<span class=\"hljs-punctuation\"\
          >,</span> 'sequence'<span class=\"hljs-punctuation\">:</span> 'Das Ziel\
          \ des Lebens ist das.'\n    <span class=\"hljs-punctuation\">}</span><span\
          \ class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-punctuation\"\
          >{</span>'score'<span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-number\">0.029783131554722786</span><span class=\"hljs-punctuation\"\
          >,</span> 'token'<span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-number\">288</span><span class=\"hljs-punctuation\">,</span> 'token_str'<span\
          \ class=\"hljs-punctuation\">:</span> 'es'<span class=\"hljs-punctuation\"\
          >,</span> 'sequence'<span class=\"hljs-punctuation\">:</span> 'Das Ziel\
          \ des Lebens ist es.'\n    <span class=\"hljs-punctuation\">}</span><span\
          \ class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-punctuation\"\
          >{</span>'score'<span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-number\">0.021744653582572937</span><span class=\"hljs-punctuation\"\
          >,</span> 'token'<span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-number\">2458</span><span class=\"hljs-punctuation\">,</span> 'token_str'<span\
          \ class=\"hljs-punctuation\">:</span> 'klar'<span class=\"hljs-punctuation\"\
          >,</span> 'sequence'<span class=\"hljs-punctuation\">:</span> 'Das Ziel\
          \ des Lebens ist klar.'\n    <span class=\"hljs-punctuation\">}</span>\n\
          <span class=\"hljs-punctuation\">]</span>\n</code></pre>\n"
        raw: "Hi @lucazug,\n\nJust tested with the following python code using websockets\
          \ and worked fine\nBut I'd recommend you to look into our [Inference Endpoints\
          \ Solution](https://huggingface.co/inference-endpoints)\nhttps://ui.endpoints.huggingface.co/new?repository=deepset/gbert-base\
          \ \n\nHere `HUGGING_FACE_HUB_TOKEN` is your access token\n\n```bash\nHUGGING_FACE_HUB_TOKEN=hf_XXXXXXXXXX\
          \ python app.py\n```\n\n```python\nimport os\nimport asyncio\nimport json\n\
          import uuid\nimport websockets\nMODEL_ID = \"deepset/gbert-base\"\nCOMPUTE_TYPE\
          \ = \"cpu\"  # or \"gpu\"\nAPI_TOKEN = os.environ[\"HUGGING_FACE_HUB_TOKEN\"\
          ]\n\nasync def send(websocket, payloads):\n    # You need to login with\
          \ a first message as headers are not forwarded\n    # for websockets\n \
          \   await websocket.send(f\"Bearer {API_TOKEN}\".encode(\"utf-8\"))\n  \
          \  for payload in payloads:\n        await websocket.send(json.dumps(payload).encode(\"\
          utf-8\"))\n        print(\"Sent \")\nasync def recv(websocket, last_id):\n\
          \    outputs = []\n    while True:\n        data = await websocket.recv()\n\
          \        payload = json.loads(data)\n        if payload[\"type\"] == \"\
          results\":\n            # {\"type\": \"results\", \"outputs\": JSONFormatted\
          \ results, \"id\": the id we sent}\n            print(payload[\"outputs\"\
          ])\n            outputs.append(payload[\"outputs\"])\n            if payload[\"\
          id\"] == last_id:\n                return outputs\n        else:\n     \
          \       # {\"type\": \"status\", \"message\": \"Some information about the\
          \ queue\"}\n            print(f\"< {payload['message']}\")\n           \
          \ pass\nasync def main():\n    uri = f\"wss://api-inference.huggingface.co/bulk/stream/{COMPUTE_TYPE}/{MODEL_ID}\"\
          \n    print(f\"Connecting to {uri}\")\n    async with websockets.connect(uri)\
          \ as websocket:\n        # inputs and parameters are classic, \"id\" is\
          \ a way to track that query\n        payloads = [\n            {\n     \
          \           \"id\": str(uuid.uuid4()),\n                \"inputs\": \"Das\
          \ Ziel des Lebens ist [MASK].\",\n            }\n            for i in range(10)\n\
          \        ]\n        last_id = payloads[-1][\"id\"]\n        future = send(websocket,\
          \ payloads)\n        future_r = recv(websocket, last_id)\n        _, outputs\
          \ = await asyncio.gather(future, future_r)\n    results = [out[\"labels\"\
          ][0] for out in outputs]\n    return results\nloop = asyncio.get_event_loop()\n\
          if loop.is_running():\n    # When running in notebooks\n    import nest_asyncio\n\
          \    nest_asyncio.apply()\nresults = loop.run_until_complete(main())\n```\n\
          \n```json\n[\n    {'score': 0.25408321619033813, 'token': 3531, 'token_str':\
          \ 'erreicht', 'sequence': 'Das Ziel des Lebens ist erreicht.'\n    },\n\
          \    {'score': 0.04424438253045082, 'token': 7939, 'token_str': 'erf\xFC\
          llt', 'sequence': 'Das Ziel des Lebens ist erf\xFCllt.'\n    },\n    {'score':\
          \ 0.031005313619971275, 'token': 199, 'token_str': 'das', 'sequence': 'Das\
          \ Ziel des Lebens ist das.'\n    },\n    {'score': 0.029783131554722786,\
          \ 'token': 288, 'token_str': 'es', 'sequence': 'Das Ziel des Lebens ist\
          \ es.'\n    },\n    {'score': 0.021744653582572937, 'token': 2458, 'token_str':\
          \ 'klar', 'sequence': 'Das Ziel des Lebens ist klar.'\n    }\n]\n```"
        updatedAt: '2022-12-20T20:18:38.974Z'
      numEdits: 1
      reactions: []
    id: 63a21842931d2c13ad7eb1f2
    type: comment
  author: radames
  content: "Hi @lucazug,\n\nJust tested with the following python code using websockets\
    \ and worked fine\nBut I'd recommend you to look into our [Inference Endpoints\
    \ Solution](https://huggingface.co/inference-endpoints)\nhttps://ui.endpoints.huggingface.co/new?repository=deepset/gbert-base\
    \ \n\nHere `HUGGING_FACE_HUB_TOKEN` is your access token\n\n```bash\nHUGGING_FACE_HUB_TOKEN=hf_XXXXXXXXXX\
    \ python app.py\n```\n\n```python\nimport os\nimport asyncio\nimport json\nimport\
    \ uuid\nimport websockets\nMODEL_ID = \"deepset/gbert-base\"\nCOMPUTE_TYPE = \"\
    cpu\"  # or \"gpu\"\nAPI_TOKEN = os.environ[\"HUGGING_FACE_HUB_TOKEN\"]\n\nasync\
    \ def send(websocket, payloads):\n    # You need to login with a first message\
    \ as headers are not forwarded\n    # for websockets\n    await websocket.send(f\"\
    Bearer {API_TOKEN}\".encode(\"utf-8\"))\n    for payload in payloads:\n      \
    \  await websocket.send(json.dumps(payload).encode(\"utf-8\"))\n        print(\"\
    Sent \")\nasync def recv(websocket, last_id):\n    outputs = []\n    while True:\n\
    \        data = await websocket.recv()\n        payload = json.loads(data)\n \
    \       if payload[\"type\"] == \"results\":\n            # {\"type\": \"results\"\
    , \"outputs\": JSONFormatted results, \"id\": the id we sent}\n            print(payload[\"\
    outputs\"])\n            outputs.append(payload[\"outputs\"])\n            if\
    \ payload[\"id\"] == last_id:\n                return outputs\n        else:\n\
    \            # {\"type\": \"status\", \"message\": \"Some information about the\
    \ queue\"}\n            print(f\"< {payload['message']}\")\n            pass\n\
    async def main():\n    uri = f\"wss://api-inference.huggingface.co/bulk/stream/{COMPUTE_TYPE}/{MODEL_ID}\"\
    \n    print(f\"Connecting to {uri}\")\n    async with websockets.connect(uri)\
    \ as websocket:\n        # inputs and parameters are classic, \"id\" is a way\
    \ to track that query\n        payloads = [\n            {\n                \"\
    id\": str(uuid.uuid4()),\n                \"inputs\": \"Das Ziel des Lebens ist\
    \ [MASK].\",\n            }\n            for i in range(10)\n        ]\n     \
    \   last_id = payloads[-1][\"id\"]\n        future = send(websocket, payloads)\n\
    \        future_r = recv(websocket, last_id)\n        _, outputs = await asyncio.gather(future,\
    \ future_r)\n    results = [out[\"labels\"][0] for out in outputs]\n    return\
    \ results\nloop = asyncio.get_event_loop()\nif loop.is_running():\n    # When\
    \ running in notebooks\n    import nest_asyncio\n    nest_asyncio.apply()\nresults\
    \ = loop.run_until_complete(main())\n```\n\n```json\n[\n    {'score': 0.25408321619033813,\
    \ 'token': 3531, 'token_str': 'erreicht', 'sequence': 'Das Ziel des Lebens ist\
    \ erreicht.'\n    },\n    {'score': 0.04424438253045082, 'token': 7939, 'token_str':\
    \ 'erf\xFCllt', 'sequence': 'Das Ziel des Lebens ist erf\xFCllt.'\n    },\n  \
    \  {'score': 0.031005313619971275, 'token': 199, 'token_str': 'das', 'sequence':\
    \ 'Das Ziel des Lebens ist das.'\n    },\n    {'score': 0.029783131554722786,\
    \ 'token': 288, 'token_str': 'es', 'sequence': 'Das Ziel des Lebens ist es.'\n\
    \    },\n    {'score': 0.021744653582572937, 'token': 2458, 'token_str': 'klar',\
    \ 'sequence': 'Das Ziel des Lebens ist klar.'\n    }\n]\n```"
  created_at: 2022-12-20 20:17:06+00:00
  edited: true
  hidden: false
  id: 63a21842931d2c13ad7eb1f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0298cf701b644a72ea957e59406627f.svg
      fullname: Luca Zug
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucazug
      type: user
    createdAt: '2022-12-20T23:04:43.000Z'
    data:
      edited: false
      editors:
      - lucazug
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0298cf701b644a72ea957e59406627f.svg
          fullname: Luca Zug
          isHf: false
          isPro: false
          name: lucazug
          type: user
        html: '<p>Hi Radames,<br>thank you for your quick reply. The websocket solution
          works fine for the input with a sentence and a masked token and the output
          you inserted above for me too. However, I want to use a websocket to ''upload''
          a list of sentences (without masked tokens) and have the Inference API and
          the GBERT model return word embeddings (list of list of 768 elements). I
          have successfully implemented this using HTTP requests like this:</p>

          <p>for text in X_train:<br>        response = requests.post(f"<a rel="nofollow"
          href="https://api-inference.huggingface.co/pipeline/feature-extraction/deepset/gbert-base&quot;">https://api-inference.huggingface.co/pipeline/feature-extraction/deepset/gbert-base"</a>,
          headers=headers, json={"inputs": text, "options":{"wait_for_model":True}})<br>        embeddings
          = response.json()[0]<br>        inputs_train.append(embeddings)</p>

          <p>Sadly, this is slow and expensive. Now I''m looking for a websocket solution
          for this problem and have been unable to find one online. Is there a GBERT-base
          URI for a websocket application that would let me call word embeddings trough
          the Inference API?</p>

          <p>Sorry for not being more specific in the first place!</p>

          '
        raw: "Hi Radames,\nthank you for your quick reply. The websocket solution\
          \ works fine for the input with a sentence and a masked token and the output\
          \ you inserted above for me too. However, I want to use a websocket to 'upload'\
          \ a list of sentences (without masked tokens) and have the Inference API\
          \ and the GBERT model return word embeddings (list of list of 768 elements).\
          \ I have successfully implemented this using HTTP requests like this:\n\n\
          for text in X_train:\n        response = requests.post(f\"https://api-inference.huggingface.co/pipeline/feature-extraction/deepset/gbert-base\"\
          , headers=headers, json={\"inputs\": text, \"options\":{\"wait_for_model\"\
          :True}})\n        embeddings = response.json()[0]\n        inputs_train.append(embeddings)\n\
          \nSadly, this is slow and expensive. Now I'm looking for a websocket solution\
          \ for this problem and have been unable to find one online. Is there a GBERT-base\
          \ URI for a websocket application that would let me call word embeddings\
          \ trough the Inference API?\n\nSorry for not being more specific in the\
          \ first place!"
        updatedAt: '2022-12-20T23:04:43.042Z'
      numEdits: 0
      reactions: []
    id: 63a23f8b3c003e40931efd56
    type: comment
  author: lucazug
  content: "Hi Radames,\nthank you for your quick reply. The websocket solution works\
    \ fine for the input with a sentence and a masked token and the output you inserted\
    \ above for me too. However, I want to use a websocket to 'upload' a list of sentences\
    \ (without masked tokens) and have the Inference API and the GBERT model return\
    \ word embeddings (list of list of 768 elements). I have successfully implemented\
    \ this using HTTP requests like this:\n\nfor text in X_train:\n        response\
    \ = requests.post(f\"https://api-inference.huggingface.co/pipeline/feature-extraction/deepset/gbert-base\"\
    , headers=headers, json={\"inputs\": text, \"options\":{\"wait_for_model\":True}})\n\
    \        embeddings = response.json()[0]\n        inputs_train.append(embeddings)\n\
    \nSadly, this is slow and expensive. Now I'm looking for a websocket solution\
    \ for this problem and have been unable to find one online. Is there a GBERT-base\
    \ URI for a websocket application that would let me call word embeddings trough\
    \ the Inference API?\n\nSorry for not being more specific in the first place!"
  created_at: 2022-12-20 23:04:43+00:00
  edited: false
  hidden: false
  id: 63a23f8b3c003e40931efd56
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648966381588-6064e095abd8d3692e3e2ed6.jpeg?w=200&h=200&f=face
      fullname: "Radam\xE9s Ajna"
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: radames
      type: user
    createdAt: '2022-12-22T16:34:38.000Z'
    data:
      edited: true
      editors:
      - radames
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648966381588-6064e095abd8d3692e3e2ed6.jpeg?w=200&h=200&f=face
          fullname: "Radam\xE9s Ajna"
          isHf: true
          isPro: false
          name: radames
          type: user
        html: "<p>hi <span data-props=\"{&quot;user&quot;:&quot;lucazug&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lucazug\">@<span class=\"\
          underline\">lucazug</span></a></span>\n\n\t</span></span> , sorry I didn't\
          \ understand your request first. Indeed you can't override the the task\
          \ via the websocket endpoint like you're doing via HTTP.<br>The recommended\
          \ way is to clone the model and set a new <code>pipeline_tag: feature-extraction</code>\
          \ on the <code>README.md</code> metadata headers.<br>I've tested this approach\
          \ with the code above and I got the embeddings.</p>\n<pre><code class=\"\
          language-yaml\"><span class=\"hljs-meta\">---</span>\n<span class=\"hljs-attr\"\
          >language:</span> <span class=\"hljs-string\">de</span>\n<span class=\"\
          hljs-attr\">license:</span> <span class=\"hljs-string\">mit</span>\n<span\
          \ class=\"hljs-attr\">tags:</span>\n  <span class=\"hljs-bullet\">-</span>\
          \ <span class=\"hljs-string\">feature-extraction</span>\n<span class=\"\
          hljs-attr\">pipeline_tag:</span> <span class=\"hljs-string\">feature-extraction</span>\n\
          <span class=\"hljs-attr\">datasets:</span>\n<span class=\"hljs-bullet\"\
          >-</span> <span class=\"hljs-string\">wikipedia</span>\n<span class=\"hljs-bullet\"\
          >-</span> <span class=\"hljs-string\">OPUS</span>\n<span class=\"hljs-bullet\"\
          >-</span> <span class=\"hljs-string\">OpenLegalData</span>\n<span class=\"\
          hljs-meta\">--- </span>\n</code></pre>\n"
        raw: "hi @lucazug , sorry I didn't understand your request first. Indeed you\
          \ can't override the the task via the websocket endpoint like you're doing\
          \ via HTTP. \nThe recommended way is to clone the model and set a new `pipeline_tag:\
          \ feature-extraction` on the `README.md` metadata headers. \nI've tested\
          \ this approach with the code above and I got the embeddings.\n\n```yaml\n\
          ---\nlanguage: de\nlicense: mit\ntags:\n  - feature-extraction\npipeline_tag:\
          \ feature-extraction\ndatasets:\n- wikipedia\n- OPUS\n- OpenLegalData\n\
          --- \n```"
        updatedAt: '2022-12-22T16:38:59.466Z'
      numEdits: 1
      reactions: []
    id: 63a4871e658851481f7560cf
    type: comment
  author: radames
  content: "hi @lucazug , sorry I didn't understand your request first. Indeed you\
    \ can't override the the task via the websocket endpoint like you're doing via\
    \ HTTP. \nThe recommended way is to clone the model and set a new `pipeline_tag:\
    \ feature-extraction` on the `README.md` metadata headers. \nI've tested this\
    \ approach with the code above and I got the embeddings.\n\n```yaml\n---\nlanguage:\
    \ de\nlicense: mit\ntags:\n  - feature-extraction\npipeline_tag: feature-extraction\n\
    datasets:\n- wikipedia\n- OPUS\n- OpenLegalData\n--- \n```"
  created_at: 2022-12-22 16:34:38+00:00
  edited: true
  hidden: false
  id: 63a4871e658851481f7560cf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: deepset/gbert-base
repo_type: model
status: open
target_branch: null
title: Websocket for GBERT
