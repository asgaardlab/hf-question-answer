!!python/object:huggingface_hub.community.DiscussionWithDetails
author: eeyrw
conflicting_files: null
created_at: 2022-12-02 06:25:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/13efc01665a3155eed454cac2e62fad9.svg
      fullname: Yuan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eeyrw
      type: user
    createdAt: '2022-12-02T06:25:31.000Z'
    data:
      edited: false
      editors:
      - eeyrw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/13efc01665a3155eed454cac2e62fad9.svg
          fullname: Yuan
          isHf: false
          isPro: false
          name: eeyrw
          type: user
        html: '<p>Traceback (most recent call last):<br>  File "F:\diffusers-test\translate.py",
          line 3, in <br>    tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-one-mmt",cache_dir=''.'')<br>  File
          "F:\diffusers-test\diffusers_venv\lib\site-packages\transformers\models\auto\tokenization_auto.py",
          line 619, in from_pretrained<br>    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,
          *inputs, **kwargs)<br>  File "F:\diffusers-test\diffusers_venv\lib\site-packages\transformers\tokenization_utils_base.py",
          line 1777, in from_pretrained<br>    return cls._from_pretrained(<br>  File
          "F:\diffusers-test\diffusers_venv\lib\site-packages\transformers\tokenization_utils_base.py",
          line 1932, in _from_pretrained<br>    tokenizer = cls(*init_inputs, **init_kwargs)<br>  File
          "F:\diffusers-test\diffusers_venv\lib\site-packages\transformers\models\mbart50\tokenization_mbart50_fast.py",
          line 135, in <strong>init</strong><br>    super().<strong>init</strong>(<br>  File
          "F:\diffusers-test\diffusers_venv\lib\site-packages\transformers\tokenization_utils_fast.py",
          line 120, in <strong>init</strong><br>    raise ValueError(<br>ValueError:
          Couldn''t instantiate the backend tokenizer from one of:<br>(1) a <code>tokenizers</code>
          library serialization file,<br>(2) a slow tokenizer instance to convert
          or<br>(3) an equivalent slow tokenizer class to instantiate and convert.<br>You
          need to have sentencepiece installed to convert a slow tokenizer to a fast
          one.</p>

          '
        raw: "Traceback (most recent call last):\r\n  File \"F:\\diffusers-test\\\
          translate.py\", line 3, in <module>\r\n    tokenizer = AutoTokenizer.from_pretrained(\"\
          facebook/mbart-large-50-many-to-one-mmt\",cache_dir='.')\r\n  File \"F:\\\
          diffusers-test\\diffusers_venv\\lib\\site-packages\\transformers\\models\\\
          auto\\tokenization_auto.py\", line 619, in from_pretrained   \r\n    return\
          \ tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\r\n  File \"F:\\diffusers-test\\diffusers_venv\\lib\\site-packages\\\
          transformers\\tokenization_utils_base.py\", line 1777, in from_pretrained\
          \        \r\n    return cls._from_pretrained(\r\n  File \"F:\\diffusers-test\\\
          diffusers_venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py\"\
          , line 1932, in _from_pretrained       \r\n    tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\r\n  File \"F:\\diffusers-test\\diffusers_venv\\lib\\site-packages\\\
          transformers\\models\\mbart50\\tokenization_mbart50_fast.py\", line 135,\
          \ in __init__\r\n    super().__init__(\r\n  File \"F:\\diffusers-test\\\
          diffusers_venv\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\"\
          , line 120, in __init__\r\n    raise ValueError(\r\nValueError: Couldn't\
          \ instantiate the backend tokenizer from one of:\r\n(1) a `tokenizers` library\
          \ serialization file,\r\n(2) a slow tokenizer instance to convert or\r\n\
          (3) an equivalent slow tokenizer class to instantiate and convert.\r\nYou\
          \ need to have sentencepiece installed to convert a slow tokenizer to a\
          \ fast one."
        updatedAt: '2022-12-02T06:25:31.950Z'
      numEdits: 0
      reactions: []
    id: 63899a5b863e8c5b0f493bb6
    type: comment
  author: eeyrw
  content: "Traceback (most recent call last):\r\n  File \"F:\\diffusers-test\\translate.py\"\
    , line 3, in <module>\r\n    tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\"\
    ,cache_dir='.')\r\n  File \"F:\\diffusers-test\\diffusers_venv\\lib\\site-packages\\\
    transformers\\models\\auto\\tokenization_auto.py\", line 619, in from_pretrained\
    \   \r\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\r\n  File \"F:\\diffusers-test\\diffusers_venv\\lib\\site-packages\\\
    transformers\\tokenization_utils_base.py\", line 1777, in from_pretrained    \
    \    \r\n    return cls._from_pretrained(\r\n  File \"F:\\diffusers-test\\diffusers_venv\\\
    lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 1932, in\
    \ _from_pretrained       \r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n  File \"F:\\diffusers-test\\diffusers_venv\\lib\\site-packages\\transformers\\\
    models\\mbart50\\tokenization_mbart50_fast.py\", line 135, in __init__\r\n   \
    \ super().__init__(\r\n  File \"F:\\diffusers-test\\diffusers_venv\\lib\\site-packages\\\
    transformers\\tokenization_utils_fast.py\", line 120, in __init__\r\n    raise\
    \ ValueError(\r\nValueError: Couldn't instantiate the backend tokenizer from one\
    \ of:\r\n(1) a `tokenizers` library serialization file,\r\n(2) a slow tokenizer\
    \ instance to convert or\r\n(3) an equivalent slow tokenizer class to instantiate\
    \ and convert.\r\nYou need to have sentencepiece installed to convert a slow tokenizer\
    \ to a fast one."
  created_at: 2022-12-02 06:25:31+00:00
  edited: false
  hidden: false
  id: 63899a5b863e8c5b0f493bb6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/13efc01665a3155eed454cac2e62fad9.svg
      fullname: Yuan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eeyrw
      type: user
    createdAt: '2022-12-02T07:31:52.000Z'
    data:
      edited: false
      editors:
      - eeyrw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/13efc01665a3155eed454cac2e62fad9.svg
          fullname: Yuan
          isHf: false
          isPro: false
          name: eeyrw
          type: user
        html: '<p>To fix this is quite trick...<br>I installed sentencepiece then
          get protobuf version incompatible error...</p>

          <p>TypeError: Descriptors cannot not be created directly.<br>If this call
          came from a _pb2.py file, your generated code is out of date and must be
          regenerated with protoc &gt;= 3.19.0.<br>If you cannot immediately regenerate
          your protos, some other possible workarounds are:</p>

          <ol>

          <li>Downgrade the protobuf package to 3.20.x or lower.</li>

          </ol>

          <p>Then install protobuf==3.20.3<br>Finally get it worked.</p>

          '
        raw: "To fix this is quite trick...\nI installed sentencepiece then get protobuf\
          \ version incompatible error...\n\nTypeError: Descriptors cannot not be\
          \ created directly.\nIf this call came from a _pb2.py file, your generated\
          \ code is out of date and must be regenerated with protoc >= 3.19.0.\nIf\
          \ you cannot immediately regenerate your protos, some other possible workarounds\
          \ are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n\nThen install\
          \ protobuf==3.20.3\nFinally get it worked."
        updatedAt: '2022-12-02T07:31:52.469Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6389a9e856f8a656a3c5f72c
    id: 6389a9e856f8a656a3c5f72b
    type: comment
  author: eeyrw
  content: "To fix this is quite trick...\nI installed sentencepiece then get protobuf\
    \ version incompatible error...\n\nTypeError: Descriptors cannot not be created\
    \ directly.\nIf this call came from a _pb2.py file, your generated code is out\
    \ of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately\
    \ regenerate your protos, some other possible workarounds are:\n 1. Downgrade\
    \ the protobuf package to 3.20.x or lower.\n\nThen install protobuf==3.20.3\n\
    Finally get it worked."
  created_at: 2022-12-02 07:31:52+00:00
  edited: false
  hidden: false
  id: 6389a9e856f8a656a3c5f72b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/13efc01665a3155eed454cac2e62fad9.svg
      fullname: Yuan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eeyrw
      type: user
    createdAt: '2022-12-02T07:31:52.000Z'
    data:
      status: closed
    id: 6389a9e856f8a656a3c5f72c
    type: status-change
  author: eeyrw
  created_at: 2022-12-02 07:31:52+00:00
  id: 6389a9e856f8a656a3c5f72c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: facebook/mbart-large-50-many-to-one-mmt
repo_type: model
status: closed
target_branch: null
title: Fail to use tokenzier
