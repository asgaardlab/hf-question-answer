!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vinnitu
conflicting_files: null
created_at: 2023-05-02 20:16:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9167f9f0abda20ccacf5bfcadb55fafa.svg
      fullname: Victor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vinnitu
      type: user
    createdAt: '2023-05-02T21:16:40.000Z'
    data:
      edited: false
      editors:
      - vinnitu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9167f9f0abda20ccacf5bfcadb55fafa.svg
          fullname: Victor
          isHf: false
          isPro: false
          name: vinnitu
          type: user
        html: "<p>How to prevent repetitions like \"It's working\"?</p>\n<p>from transformers\
          \ import MBartForConditionalGeneration, MBart50TokenizerFast<br>import torch</p>\n\
          <p>device = torch.device('cuda')</p>\n<p>max_new_tokens = 200<br>model_name\
          \ = \"facebook/mbart-large-50-many-to-one-mmt\"<br>model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)<br>tokenizer\
          \ = MBart50TokenizerFast.from_pretrained(model_name)<br>tokenizer.src_lang\
          \ = 'ko_KR'<br>input = '\uC11C\uBE44\uC2A4 \uC911\uC9C0\uAC00 \uACC4\uC18D\
          \ \uB728\uB294\uB370 \uC798 \uB41C\uAC70 \uB9DE\uB098\uC694?' # google translation\
          \ is: 'The service stop keeps popping up, is it okay?'<br>encoded = tokenizer(input,\
          \ return_tensors=\"pt\").to(device)<br>generated_tokens = model.generate(**encoded)<br>result\
          \ = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)<br>print(return\
          \ result[0])</p>\n<h1 id=\"and-its-working-right-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working-its-working\"\
          >And it's working, right? It's working. It's working. It's working. It's\
          \ working. It's working. It's working. It's working. It's working. It's\
          \ working. It's working. It's working. It's working. It's working. It's\
          \ working. It's working. It's working. It's working. It's working. It's\
          \ working. It's working. It's working. It's working. It's working. It's\
          \ working. It's working. It's working. It's working. It's working. It's\
          \ working. It's working. It's working. It's working. It's working. It's\
          \ working. It's working. It's working. It's working. It's working</h1>\n"
        raw: "How to prevent repetitions like \"It's working\"?\r\n\r\nfrom transformers\
          \ import MBartForConditionalGeneration, MBart50TokenizerFast\r\nimport torch\r\
          \n\r\ndevice = torch.device('cuda')\r\n\r\nmax_new_tokens = 200\r\nmodel_name\
          \ = \"facebook/mbart-large-50-many-to-one-mmt\"\r\nmodel = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\r\
          \ntokenizer = MBart50TokenizerFast.from_pretrained(model_name)\r\ntokenizer.src_lang\
          \ = 'ko_KR'\r\ninput = '\uC11C\uBE44\uC2A4 \uC911\uC9C0\uAC00 \uACC4\uC18D\
          \ \uB728\uB294\uB370 \uC798 \uB41C\uAC70 \uB9DE\uB098\uC694?' # google translation\
          \ is: 'The service stop keeps popping up, is it okay?'\r\nencoded = tokenizer(input,\
          \ return_tensors=\"pt\").to(device)\r\ngenerated_tokens = model.generate(**encoded)\r\
          \nresult = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\r\
          \nprint(return result[0])\r\n# And it's working, right? It's working. It's\
          \ working. It's working. It's working. It's working. It's working. It's\
          \ working. It's working. It's working. It's working. It's working. It's\
          \ working. It's working. It's working. It's working. It's working. It's\
          \ working. It's working. It's working. It's working. It's working. It's\
          \ working. It's working. It's working. It's working. It's working. It's\
          \ working. It's working. It's working. It's working. It's working. It's\
          \ working. It's working. It's working. It's working. It's working. It's\
          \ working. It's working"
        updatedAt: '2023-05-02T21:16:40.132Z'
      numEdits: 0
      reactions: []
    id: 64517db867703e98a3cca9a3
    type: comment
  author: vinnitu
  content: "How to prevent repetitions like \"It's working\"?\r\n\r\nfrom transformers\
    \ import MBartForConditionalGeneration, MBart50TokenizerFast\r\nimport torch\r\
    \n\r\ndevice = torch.device('cuda')\r\n\r\nmax_new_tokens = 200\r\nmodel_name\
    \ = \"facebook/mbart-large-50-many-to-one-mmt\"\r\nmodel = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\r\
    \ntokenizer = MBart50TokenizerFast.from_pretrained(model_name)\r\ntokenizer.src_lang\
    \ = 'ko_KR'\r\ninput = '\uC11C\uBE44\uC2A4 \uC911\uC9C0\uAC00 \uACC4\uC18D \uB728\
    \uB294\uB370 \uC798 \uB41C\uAC70 \uB9DE\uB098\uC694?' # google translation is:\
    \ 'The service stop keeps popping up, is it okay?'\r\nencoded = tokenizer(input,\
    \ return_tensors=\"pt\").to(device)\r\ngenerated_tokens = model.generate(**encoded)\r\
    \nresult = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\r\
    \nprint(return result[0])\r\n# And it's working, right? It's working. It's working.\
    \ It's working. It's working. It's working. It's working. It's working. It's working.\
    \ It's working. It's working. It's working. It's working. It's working. It's working.\
    \ It's working. It's working. It's working. It's working. It's working. It's working.\
    \ It's working. It's working. It's working. It's working. It's working. It's working.\
    \ It's working. It's working. It's working. It's working. It's working. It's working.\
    \ It's working. It's working. It's working. It's working. It's working. It's working"
  created_at: 2023-05-02 20:16:40+00:00
  edited: false
  hidden: false
  id: 64517db867703e98a3cca9a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1637711517996-6039478ab3ecf716b1a5fd4d.jpeg?w=200&h=200&f=face
      fullname: taesiri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: taesiri
      type: user
    createdAt: '2023-05-03T01:17:05.000Z'
    data:
      edited: false
      editors:
      - taesiri
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1637711517996-6039478ab3ecf716b1a5fd4d.jpeg?w=200&h=200&f=face
          fullname: taesiri
          isHf: false
          isPro: true
          name: taesiri
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;vinnitu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/vinnitu\"\
          >@<span class=\"underline\">vinnitu</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>You can use <code>no_repeat_ngram_size</code> (<a href=\"https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.no_repeat_ngram_size\"\
          >doc</a>) to prevent such repetition. </p>\n<p>Code:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> MBartForConditionalGeneration,\
          \ MBart50TokenizerFast\n<span class=\"hljs-keyword\">import</span> torch\n\
          \ndevice = torch.device(<span class=\"hljs-string\">'cuda'</span>)\n\nmax_new_tokens\
          \ = <span class=\"hljs-number\">200</span>\nmodel_name = <span class=\"\
          hljs-string\">\"facebook/mbart-large-50-many-to-one-mmt\"</span>\nmodel\
          \ = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n\
          tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\ntokenizer.src_lang\
          \ = <span class=\"hljs-string\">'ko_KR'</span>\n\n<span class=\"hljs-built_in\"\
          >input</span> = <span class=\"hljs-string\">'\uC11C\uBE44\uC2A4 \uC911\uC9C0\
          \uAC00 \uACC4\uC18D \uB728\uB294\uB370 \uC798 \uB41C\uAC70 \uB9DE\uB098\uC694\
          ?'</span> \nencoded = tokenizer(<span class=\"hljs-built_in\">input</span>,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>).to(device)\n\
          \n<span class=\"hljs-comment\"># Adjust the num_beams and no_repeat_ngram_size\
          \ parameters</span>\ngenerated_tokens = model.generate(\n    **encoded,\n\
          \    num_beams=<span class=\"hljs-number\">5</span>,\n    no_repeat_ngram_size=<span\
          \ class=\"hljs-number\">2</span>,\n    max_length=max_new_tokens,\n)\n\n\
          result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=<span\
          \ class=\"hljs-literal\">True</span>)\n<span class=\"hljs-built_in\">print</span>(result[<span\
          \ class=\"hljs-number\">0</span>])\n</code></pre>\n<p>Output:</p>\n<pre><code>And\
          \ it's working, right?\n</code></pre>\n"
        raw: "Hello @vinnitu \n\nYou can use `no_repeat_ngram_size` ([doc](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.no_repeat_ngram_size))\
          \ to prevent such repetition. \n\nCode:\n\n```python\nfrom transformers\
          \ import MBartForConditionalGeneration, MBart50TokenizerFast\nimport torch\n\
          \ndevice = torch.device('cuda')\n\nmax_new_tokens = 200\nmodel_name = \"\
          facebook/mbart-large-50-many-to-one-mmt\"\nmodel = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n\
          tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\ntokenizer.src_lang\
          \ = 'ko_KR'\n\ninput = '\uC11C\uBE44\uC2A4 \uC911\uC9C0\uAC00 \uACC4\uC18D\
          \ \uB728\uB294\uB370 \uC798 \uB41C\uAC70 \uB9DE\uB098\uC694?' \nencoded\
          \ = tokenizer(input, return_tensors=\"pt\").to(device)\n\n# Adjust the num_beams\
          \ and no_repeat_ngram_size parameters\ngenerated_tokens = model.generate(\n\
          \    **encoded,\n    num_beams=5,\n    no_repeat_ngram_size=2,\n    max_length=max_new_tokens,\n\
          )\n\nresult = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\
          print(result[0])\n```\n\nOutput:\n\n```\nAnd it's working, right?\n```"
        updatedAt: '2023-05-03T01:17:05.263Z'
      numEdits: 0
      reactions: []
    id: 6451b611b3f75261a7e51043
    type: comment
  author: taesiri
  content: "Hello @vinnitu \n\nYou can use `no_repeat_ngram_size` ([doc](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.no_repeat_ngram_size))\
    \ to prevent such repetition. \n\nCode:\n\n```python\nfrom transformers import\
    \ MBartForConditionalGeneration, MBart50TokenizerFast\nimport torch\n\ndevice\
    \ = torch.device('cuda')\n\nmax_new_tokens = 200\nmodel_name = \"facebook/mbart-large-50-many-to-one-mmt\"\
    \nmodel = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n\
    tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\ntokenizer.src_lang\
    \ = 'ko_KR'\n\ninput = '\uC11C\uBE44\uC2A4 \uC911\uC9C0\uAC00 \uACC4\uC18D \uB728\
    \uB294\uB370 \uC798 \uB41C\uAC70 \uB9DE\uB098\uC694?' \nencoded = tokenizer(input,\
    \ return_tensors=\"pt\").to(device)\n\n# Adjust the num_beams and no_repeat_ngram_size\
    \ parameters\ngenerated_tokens = model.generate(\n    **encoded,\n    num_beams=5,\n\
    \    no_repeat_ngram_size=2,\n    max_length=max_new_tokens,\n)\n\nresult = tokenizer.batch_decode(generated_tokens,\
    \ skip_special_tokens=True)\nprint(result[0])\n```\n\nOutput:\n\n```\nAnd it's\
    \ working, right?\n```"
  created_at: 2023-05-03 00:17:05+00:00
  edited: false
  hidden: false
  id: 6451b611b3f75261a7e51043
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9167f9f0abda20ccacf5bfcadb55fafa.svg
      fullname: Victor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vinnitu
      type: user
    createdAt: '2023-05-03T06:38:11.000Z'
    data:
      edited: false
      editors:
      - vinnitu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9167f9f0abda20ccacf5bfcadb55fafa.svg
          fullname: Victor
          isHf: false
          isPro: false
          name: vinnitu
          type: user
        html: '<p>thanks</p>

          '
        raw: thanks
        updatedAt: '2023-05-03T06:38:11.286Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64520153f65cf6a58c4566cb
    id: 64520153f65cf6a58c4566ca
    type: comment
  author: vinnitu
  content: thanks
  created_at: 2023-05-03 05:38:11+00:00
  edited: false
  hidden: false
  id: 64520153f65cf6a58c4566ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9167f9f0abda20ccacf5bfcadb55fafa.svg
      fullname: Victor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vinnitu
      type: user
    createdAt: '2023-05-03T06:38:11.000Z'
    data:
      status: closed
    id: 64520153f65cf6a58c4566cb
    type: status-change
  author: vinnitu
  created_at: 2023-05-03 05:38:11+00:00
  id: 64520153f65cf6a58c4566cb
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: facebook/mbart-large-50-many-to-one-mmt
repo_type: model
status: closed
target_branch: null
title: prevent repetitions
