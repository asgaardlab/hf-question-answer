!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Skepsun
conflicting_files: null
created_at: 2023-07-25 01:06:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/059efc11e231b625c97ab437cc18ac85.svg
      fullname: Skepsun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Skepsun
      type: user
    createdAt: '2023-07-25T02:06:07.000Z'
    data:
      edited: false
      editors:
      - Skepsun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.19385743141174316
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/059efc11e231b625c97ab437cc18ac85.svg
          fullname: Skepsun
          isHf: false
          isPro: false
          name: Skepsun
          type: user
        html: "<pre><code>{\"bos_token\": \"\", \"eos_token\": \"\", \"model_max_length\"\
          : 1000000000000000019884624838656, \"tokenizer_class\": \"LlamaTokenizer\"\
          , \"unk_token\": \"\"}\n</code></pre>\n<p>\u8FD9\u4F1A\u4E0D\u4F1A\u5F71\
          \u54CD\u6A21\u578B\u7684\u540E\u7EED\u8BAD\u7EC3\u548C\u4F7F\u7528</p>\n"
        raw: "```\r\n{\"bos_token\": \"\", \"eos_token\": \"\", \"model_max_length\"\
          : 1000000000000000019884624838656, \"tokenizer_class\": \"LlamaTokenizer\"\
          , \"unk_token\": \"\"}\r\n```\r\n\u8FD9\u4F1A\u4E0D\u4F1A\u5F71\u54CD\u6A21\
          \u578B\u7684\u540E\u7EED\u8BAD\u7EC3\u548C\u4F7F\u7528"
        updatedAt: '2023-07-25T02:06:07.144Z'
      numEdits: 0
      reactions: []
    id: 64bf2e0f891751ce9b512b22
    type: comment
  author: Skepsun
  content: "```\r\n{\"bos_token\": \"\", \"eos_token\": \"\", \"model_max_length\"\
    : 1000000000000000019884624838656, \"tokenizer_class\": \"LlamaTokenizer\", \"\
    unk_token\": \"\"}\r\n```\r\n\u8FD9\u4F1A\u4E0D\u4F1A\u5F71\u54CD\u6A21\u578B\u7684\
    \u540E\u7EED\u8BAD\u7EC3\u548C\u4F7F\u7528"
  created_at: 2023-07-25 01:06:07+00:00
  edited: false
  hidden: false
  id: 64bf2e0f891751ce9b512b22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0f1786978317835cc6e45628fd7a6c2c.svg
      fullname: Kaeli
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wmpscc
      type: user
    createdAt: '2023-07-25T02:22:00.000Z'
    data:
      edited: false
      editors:
      - wmpscc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3518275022506714
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0f1786978317835cc6e45628fd7a6c2c.svg
          fullname: Kaeli
          isHf: false
          isPro: false
          name: wmpscc
          type: user
        html: "<p>\u63A8\u7406\u53EF\u4EE5\u53C2\u8003 <a href=\"https://huggingface.co/spaces/Linly-AI/Linly-ChatFlow/blob/main/app.py\"\
          >https://huggingface.co/spaces/Linly-AI/Linly-ChatFlow/blob/main/app.py</a></p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span>\
          \ os\nos.environ[<span class=\"hljs-string\">'CUDA_LAUNCH_BLOCKING'</span>]\
          \ = <span class=\"hljs-string\">'1'</span>\n\n<span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">import</span> gradio\
          \ <span class=\"hljs-keyword\">as</span> gr\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ AutoTokenizer\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">init_model</span>():\n    model = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"Linly-AI/Chinese-LLaMA-2-7B-hf\"</span>, device_map=<span\
          \ class=\"hljs-string\">\"cuda:0\"</span>, torch_dtype=torch.float16, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n    tokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"Linly-AI/Chinese-LLaMA-2-7B-hf\"</span>, use_fast=<span\
          \ class=\"hljs-literal\">False</span>, trust_remote_code=<span class=\"\
          hljs-literal\">True</span>)\n    <span class=\"hljs-keyword\">return</span>\
          \ model, tokenizer\n    \n\n\n<span class=\"hljs-keyword\">def</span> <span\
          \ class=\"hljs-title function_\">chat</span>(<span class=\"hljs-params\"\
          >prompt, top_k, temperature</span>):\n    prompt = <span class=\"hljs-string\"\
          >f\"### Instruction:<span class=\"hljs-subst\">{prompt.strip()}</span> \
          \ ### Response:\"</span>\n    inputs = tokenizer(prompt, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>).to(<span class=\"hljs-string\">\"\
          cuda:0\"</span>)\n    generate_ids = model.generate(inputs.input_ids, do_sample=<span\
          \ class=\"hljs-literal\">True</span>, max_new_tokens=<span class=\"hljs-number\"\
          >2048</span>, top_k=<span class=\"hljs-built_in\">int</span>(top_k), top_p=<span\
          \ class=\"hljs-number\">0.84</span>, temperature=<span class=\"hljs-built_in\"\
          >float</span>(temperature), repetition_penalty=<span class=\"hljs-number\"\
          >1.15</span>, eos_token_id=<span class=\"hljs-number\">2</span>, bos_token_id=<span\
          \ class=\"hljs-number\">1</span>, pad_token_id=<span class=\"hljs-number\"\
          >0</span>)\n    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=<span\
          \ class=\"hljs-literal\">True</span>, clean_up_tokenization_spaces=<span\
          \ class=\"hljs-literal\">False</span>)[<span class=\"hljs-number\">0</span>]\n\
          \    response = response.lstrip(prompt)\n    <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">'-log: '</span>,prompt, response)\n\
          \    <span class=\"hljs-keyword\">return</span> response\n\n\n<span class=\"\
          hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">'__main__'</span>:\n\
          \    model, tokenizer = init_model()\n    demo = gr.Interface(\n       \
          \ fn=chat,\n        inputs=[<span class=\"hljs-string\">\"text\"</span>,\
          \ gr.Slider(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\"\
          >60</span>, value=<span class=\"hljs-number\">10</span>, step=<span class=\"\
          hljs-number\">1</span>), gr.Slider(<span class=\"hljs-number\">0.1</span>,\
          \ <span class=\"hljs-number\">2.0</span>, value=<span class=\"hljs-number\"\
          >1.0</span>, step=<span class=\"hljs-number\">0.1</span>)],\n        outputs=<span\
          \ class=\"hljs-string\">\"text\"</span>,\n    )\n    demo.launch()\n</code></pre>\n"
        raw: "\u63A8\u7406\u53EF\u4EE5\u53C2\u8003 https://huggingface.co/spaces/Linly-AI/Linly-ChatFlow/blob/main/app.py\n\
          \n``` python\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nimport\
          \ torch\nimport gradio as gr\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\n\ndef init_model():\n    model = AutoModelForCausalLM.from_pretrained(\"\
          Linly-AI/Chinese-LLaMA-2-7B-hf\", device_map=\"cuda:0\", torch_dtype=torch.float16,\
          \ trust_remote_code=True)\n    tokenizer = AutoTokenizer.from_pretrained(\"\
          Linly-AI/Chinese-LLaMA-2-7B-hf\", use_fast=False, trust_remote_code=True)\n\
          \    return model, tokenizer\n    \n\n\ndef chat(prompt, top_k, temperature):\n\
          \    prompt = f\"### Instruction:{prompt.strip()}  ### Response:\"\n   \
          \ inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n   \
          \ generate_ids = model.generate(inputs.input_ids, do_sample=True, max_new_tokens=2048,\
          \ top_k=int(top_k), top_p=0.84, temperature=float(temperature), repetition_penalty=1.15,\
          \ eos_token_id=2, bos_token_id=1, pad_token_id=0)\n    response = tokenizer.batch_decode(generate_ids,\
          \ skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n   \
          \ response = response.lstrip(prompt)\n    print('-log: ',prompt, response)\n\
          \    return response\n\n\nif __name__ == '__main__':\n    model, tokenizer\
          \ = init_model()\n    demo = gr.Interface(\n        fn=chat,\n        inputs=[\"\
          text\", gr.Slider(1, 60, value=10, step=1), gr.Slider(0.1, 2.0, value=1.0,\
          \ step=0.1)],\n        outputs=\"text\",\n    )\n    demo.launch()\n```"
        updatedAt: '2023-07-25T02:22:00.115Z'
      numEdits: 0
      reactions: []
    id: 64bf31c8979949d2e24dccde
    type: comment
  author: wmpscc
  content: "\u63A8\u7406\u53EF\u4EE5\u53C2\u8003 https://huggingface.co/spaces/Linly-AI/Linly-ChatFlow/blob/main/app.py\n\
    \n``` python\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nimport torch\n\
    import gradio as gr\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
    \n\ndef init_model():\n    model = AutoModelForCausalLM.from_pretrained(\"Linly-AI/Chinese-LLaMA-2-7B-hf\"\
    , device_map=\"cuda:0\", torch_dtype=torch.float16, trust_remote_code=True)\n\
    \    tokenizer = AutoTokenizer.from_pretrained(\"Linly-AI/Chinese-LLaMA-2-7B-hf\"\
    , use_fast=False, trust_remote_code=True)\n    return model, tokenizer\n    \n\
    \n\ndef chat(prompt, top_k, temperature):\n    prompt = f\"### Instruction:{prompt.strip()}\
    \  ### Response:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"\
    cuda:0\")\n    generate_ids = model.generate(inputs.input_ids, do_sample=True,\
    \ max_new_tokens=2048, top_k=int(top_k), top_p=0.84, temperature=float(temperature),\
    \ repetition_penalty=1.15, eos_token_id=2, bos_token_id=1, pad_token_id=0)\n \
    \   response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True,\
    \ clean_up_tokenization_spaces=False)[0]\n    response = response.lstrip(prompt)\n\
    \    print('-log: ',prompt, response)\n    return response\n\n\nif __name__ ==\
    \ '__main__':\n    model, tokenizer = init_model()\n    demo = gr.Interface(\n\
    \        fn=chat,\n        inputs=[\"text\", gr.Slider(1, 60, value=10, step=1),\
    \ gr.Slider(0.1, 2.0, value=1.0, step=0.1)],\n        outputs=\"text\",\n    )\n\
    \    demo.launch()\n```"
  created_at: 2023-07-25 01:22:00+00:00
  edited: false
  hidden: false
  id: 64bf31c8979949d2e24dccde
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6034058de716c63b4c65b8e2652c9e32.svg
      fullname: JaheimLee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JaheimLee
      type: user
    createdAt: '2023-07-30T15:17:21.000Z'
    data:
      edited: true
      editors:
      - JaheimLee
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.998637855052948
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6034058de716c63b4c65b8e2652c9e32.svg
          fullname: JaheimLee
          isHf: false
          isPro: false
          name: JaheimLee
          type: user
        html: "<p>\u5EFA\u8BAE\u8FD8\u662F\u66F4\u65B0\u4E00\u4E0B\u6587\u4EF6\uFF0C\
          \u5982\u679C\u7528\u7B2C\u4E09\u65B9\u5E93\u52A0\u8F7D\uFF08\u4F8B\u5982\
          vllm\uFF09\uFF0Ctokenizer\u6587\u4EF6\u6709\u95EE\u9898\u4F1A\u5F71\u54CD\
          \u6587\u672C\u7F16\u7801\u3002\u6BD5\u7ADF\u8FD9\u6839\u672C\u4E0D\u8D39\
          \u4E8B\u3002</p>\n"
        raw: "\u5EFA\u8BAE\u8FD8\u662F\u66F4\u65B0\u4E00\u4E0B\u6587\u4EF6\uFF0C\u5982\
          \u679C\u7528\u7B2C\u4E09\u65B9\u5E93\u52A0\u8F7D\uFF08\u4F8B\u5982vllm\uFF09\
          \uFF0Ctokenizer\u6587\u4EF6\u6709\u95EE\u9898\u4F1A\u5F71\u54CD\u6587\u672C\
          \u7F16\u7801\u3002\u6BD5\u7ADF\u8FD9\u6839\u672C\u4E0D\u8D39\u4E8B\u3002"
        updatedAt: '2023-07-30T15:18:26.820Z'
      numEdits: 1
      reactions: []
    id: 64c67f01513a7fa7c31aac21
    type: comment
  author: JaheimLee
  content: "\u5EFA\u8BAE\u8FD8\u662F\u66F4\u65B0\u4E00\u4E0B\u6587\u4EF6\uFF0C\u5982\
    \u679C\u7528\u7B2C\u4E09\u65B9\u5E93\u52A0\u8F7D\uFF08\u4F8B\u5982vllm\uFF09\uFF0C\
    tokenizer\u6587\u4EF6\u6709\u95EE\u9898\u4F1A\u5F71\u54CD\u6587\u672C\u7F16\u7801\
    \u3002\u6BD5\u7ADF\u8FD9\u6839\u672C\u4E0D\u8D39\u4E8B\u3002"
  created_at: 2023-07-30 14:17:21+00:00
  edited: true
  hidden: false
  id: 64c67f01513a7fa7c31aac21
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b73e76468d5e7512791de769aebffd71.svg
      fullname: ydli-ai
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: P01son
      type: user
    createdAt: '2023-07-31T02:14:57.000Z'
    data:
      edited: false
      editors:
      - P01son
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 1.000059962272644
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b73e76468d5e7512791de769aebffd71.svg
          fullname: ydli-ai
          isHf: false
          isPro: false
          name: P01son
          type: user
        html: "<p>\u597D\u7684</p>\n"
        raw: "\u597D\u7684"
        updatedAt: '2023-07-31T02:14:57.860Z'
      numEdits: 0
      reactions: []
    id: 64c7192171947b03ff25c970
    type: comment
  author: P01son
  content: "\u597D\u7684"
  created_at: 2023-07-31 01:14:57+00:00
  edited: false
  hidden: false
  id: 64c7192171947b03ff25c970
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Linly-AI/Chinese-LLaMA-2-7B-hf
repo_type: model
status: open
target_branch: null
title: "tokenizer config\u597D\u50CF\u6709\u4E9B\u95EE\u9898"
