!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mmitchell
conflicting_files: null
created_at: 2023-02-20 18:29:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676916898409-noauth.jpeg?w=200&h=200&f=face
      fullname: Margaret Mitchell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mmitchell
      type: user
    createdAt: '2023-02-20T18:29:14.000Z'
    data:
      edited: true
      editors:
      - mmitchell
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676916898409-noauth.jpeg?w=200&h=200&f=face
          fullname: Margaret Mitchell
          isHf: false
          isPro: false
          name: mmitchell
          type: user
        html: "<p>Were these models trained with te reo M\u0101ori ?<br>If so, I propose:<br>(1)\
          \ Working with M\u0101ori to clarify how their data can/cannot be used<br>(2)\
          \ Re-training and re-releasing this model (and the other versions of it)\
          \ with the M\u0101ori language removed from the training set</p>\n<p>For\
          \ reference on concerns with Whisper, see <a rel=\"nofollow\" href=\"https://blog.papareo.nz/whisper-is-another-case-study-in-colonisation/\"\
          >https://blog.papareo.nz/whisper-is-another-case-study-in-colonisation/</a>\
          \ , which notes \"The Whisper model was trained with 1381 hours of te reo\
          \ M\u0101ori\" without consent.</p>\n<p>This request is in (my likely botched\
          \ interpretation of) the spirit of 'kaitiakitanga' (<a rel=\"nofollow\"\
          \ href=\"https://papareo.nz/#kaitiakitanga\">https://papareo.nz/#kaitiakitanga</a>)\
          \ for using te reo M\u0101ori in technology.</p>\n"
        raw: "Were these models trained with te reo M\u0101ori ?\nIf so, I propose:\n\
          (1) Working with M\u0101ori to clarify how their data can/cannot be used\n\
          (2) Re-training and re-releasing this model (and the other versions of it)\
          \ with the M\u0101ori language removed from the training set\n\nFor reference\
          \ on concerns with Whisper, see https://blog.papareo.nz/whisper-is-another-case-study-in-colonisation/\
          \ , which notes \"The Whisper model was trained with 1381 hours of te reo\
          \ M\u0101ori\" without consent.\n\nThis request is in (my likely botched\
          \ interpretation of) the spirit of 'kaitiakitanga' (https://papareo.nz/#kaitiakitanga)\
          \ for using te reo M\u0101ori in technology."
        updatedAt: '2023-02-23T02:52:23.767Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - NimaBoscarino
        - Ezi
    id: 63f3bbfa0be81bdc5d9619b9
    type: comment
  author: mmitchell
  content: "Were these models trained with te reo M\u0101ori ?\nIf so, I propose:\n\
    (1) Working with M\u0101ori to clarify how their data can/cannot be used\n(2)\
    \ Re-training and re-releasing this model (and the other versions of it) with\
    \ the M\u0101ori language removed from the training set\n\nFor reference on concerns\
    \ with Whisper, see https://blog.papareo.nz/whisper-is-another-case-study-in-colonisation/\
    \ , which notes \"The Whisper model was trained with 1381 hours of te reo M\u0101\
    ori\" without consent.\n\nThis request is in (my likely botched interpretation\
    \ of) the spirit of 'kaitiakitanga' (https://papareo.nz/#kaitiakitanga) for using\
    \ te reo M\u0101ori in technology."
  created_at: 2023-02-20 18:29:14+00:00
  edited: true
  hidden: false
  id: 63f3bbfa0be81bdc5d9619b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-02-22T14:52:05.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;mmitchell&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mmitchell\"\
          >@<span class=\"underline\">mmitchell</span></a></span>\n\n\t</span></span>!\
          \ This is an English-only version of the Whisper model (<code>tiny.en</code>).\
          \ The multilingual version of the model can be found at <a href=\"https://huggingface.co/openai/whisper-tiny\"\
          >openai/whisper-tiny</a>. The model card describes the various checkpoints\
          \ and whether they're English-only / multilingual: <a href=\"https://huggingface.co/openai/whisper-tiny.en#model-details\"\
          >openai/whisper-tiny.en#model-details</a></p>\n<p>The multilingual checkpoints\
          \ are indeed trained on ~1.4k hours of Maori data (see page 27 of the <a\
          \ rel=\"nofollow\" href=\"https://arxiv.org/pdf/2212.04356.pdf\">Whisper\
          \ paper</a>). The data was obtained in the same way as the other 679k hours\
          \ of speech data (by scraping the web). Unfortunately, we don't have access\
          \ to the training dataset, so re-training the model is not possible.</p>\n"
        raw: 'Hey @mmitchell! This is an English-only version of the Whisper model
          (`tiny.en`). The multilingual version of the model can be found at [openai/whisper-tiny](https://huggingface.co/openai/whisper-tiny).
          The model card describes the various checkpoints and whether they''re English-only
          / multilingual: [openai/whisper-tiny.en#model-details](https://huggingface.co/openai/whisper-tiny.en#model-details)


          The multilingual checkpoints are indeed trained on ~1.4k hours of Maori
          data (see page 27 of the [Whisper paper](https://arxiv.org/pdf/2212.04356.pdf)).
          The data was obtained in the same way as the other 679k hours of speech
          data (by scraping the web). Unfortunately, we don''t have access to the
          training dataset, so re-training the model is not possible.'
        updatedAt: '2023-02-22T14:52:53.100Z'
      numEdits: 2
      reactions: []
    id: 63f62c1552799101f3ce4189
    type: comment
  author: sanchit-gandhi
  content: 'Hey @mmitchell! This is an English-only version of the Whisper model (`tiny.en`).
    The multilingual version of the model can be found at [openai/whisper-tiny](https://huggingface.co/openai/whisper-tiny).
    The model card describes the various checkpoints and whether they''re English-only
    / multilingual: [openai/whisper-tiny.en#model-details](https://huggingface.co/openai/whisper-tiny.en#model-details)


    The multilingual checkpoints are indeed trained on ~1.4k hours of Maori data (see
    page 27 of the [Whisper paper](https://arxiv.org/pdf/2212.04356.pdf)). The data
    was obtained in the same way as the other 679k hours of speech data (by scraping
    the web). Unfortunately, we don''t have access to the training dataset, so re-training
    the model is not possible.'
  created_at: 2023-02-22 14:52:05+00:00
  edited: true
  hidden: false
  id: 63f62c1552799101f3ce4189
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676916898409-noauth.jpeg?w=200&h=200&f=face
      fullname: Margaret Mitchell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mmitchell
      type: user
    createdAt: '2023-02-23T02:50:44.000Z'
    data:
      edited: true
      editors:
      - mmitchell
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676916898409-noauth.jpeg?w=200&h=200&f=face
          fullname: Margaret Mitchell
          isHf: false
          isPro: false
          name: mmitchell
          type: user
        html: '<p>Hi Sanchit! Indeed, this particular (sub-)model is English, apologies,
          I should have clarified: This (sub-)model is the most popular download of
          Whisper and hence the most likely one to get the attention needed for the
          multiple relevant models being shared. Let me know if it would be helpful
          to link to all of them here. I opted not to write the same message for each
          (sub-)model this is specifically relevant for,  and unfortunately I would
          be blocked from doing so due to Spam filters, but also let me know if it
          would help, and I can do it over several days.</p>

          <p>More important on my end is your answer at the end: If this is a trained
          model that you are sharing, then who is the "we" here you refer to, where
          is the training dataset, and who does have access to it?</p>

          <p>Thanks!</p>

          '
        raw: 'Hi Sanchit! Indeed, this particular (sub-)model is English, apologies,
          I should have clarified: This (sub-)model is the most popular download of
          Whisper and hence the most likely one to get the attention needed for the
          multiple relevant models being shared. Let me know if it would be helpful
          to link to all of them here. I opted not to write the same message for each
          (sub-)model this is specifically relevant for,  and unfortunately I would
          be blocked from doing so due to Spam filters, but also let me know if it
          would help, and I can do it over several days.


          More important on my end is your answer at the end: If this is a trained
          model that you are sharing, then who is the "we" here you refer to, where
          is the training dataset, and who does have access to it?


          Thanks!'
        updatedAt: '2023-02-23T02:56:23.464Z'
      numEdits: 6
      reactions: []
    id: 63f6d484d0b7e4a8f8e278f0
    type: comment
  author: mmitchell
  content: 'Hi Sanchit! Indeed, this particular (sub-)model is English, apologies,
    I should have clarified: This (sub-)model is the most popular download of Whisper
    and hence the most likely one to get the attention needed for the multiple relevant
    models being shared. Let me know if it would be helpful to link to all of them
    here. I opted not to write the same message for each (sub-)model this is specifically
    relevant for,  and unfortunately I would be blocked from doing so due to Spam
    filters, but also let me know if it would help, and I can do it over several days.


    More important on my end is your answer at the end: If this is a trained model
    that you are sharing, then who is the "we" here you refer to, where is the training
    dataset, and who does have access to it?


    Thanks!'
  created_at: 2023-02-23 02:50:44+00:00
  edited: true
  hidden: false
  id: 63f6d484d0b7e4a8f8e278f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-02-23T17:47:57.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;mmitchell&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mmitchell\"\
          >@<span class=\"underline\">mmitchell</span></a></span>\n\n\t</span></span>!\
          \ That makes sense, thanks for clarifying! No need to copy across to other\
          \ checkpoints - I just wanted to flag this to make sure that it was indeed\
          \ the multilingual checkpoints we were referring to \U0001F917</p>\n<p>OpenAI\
          \ curated the dataset by web scraping the internet for audio data. Unfortunately,\
          \ these are <em>all</em> the details we know about it from the <a rel=\"\
          nofollow\" href=\"https://arxiv.org/pdf/2212.04356.pdf\">Whisper paper</a>.\
          \ They subsequently trained 11 speech recognition models on this data. They\
          \ released all of the models, but not the dataset. The dataset remains private\
          \ and AFAIK is not going to be made open-source. So OpenAI are the only\
          \ ones with access to it and the only ones who can re-train the model.</p>\n\
          <p>There might be another way to silence the model making Maori predictions.\
          \ The format of Whisper transcriptions is as follows:</p>\n<pre><code>&lt;|startoftranscript|&gt;\
          \ &lt;|en|&gt; &lt;|transcribe|&gt; &lt;|notimestamps|&gt; Hey this is some\
          \ transcribed text! &lt;|endoftranscript|&gt;\n</code></pre>\n<p>The second\
          \ token (<code>&lt;|en|&gt;</code>) is the language id token. This token\
          \ informs Whisper which model to make its predictions in. If the second\
          \ predicted token is <code>&lt;|en|&gt;</code>, Whisper will transcribe\
          \ in English. Likewise if the second predicted token is <code>&lt;|es|&gt;</code>,\
          \ the model will transcribe in Spanish, and so on...</p>\n<p>If you want\
          \ to stop the model predicting Maori transcriptions, you could explore <em>forcing</em>\
          \ the log probability for the Maori language id token to -inf. This way,\
          \ the model will never predict Maori as the transcription language, and\
          \ thus will not generate Maori transcriptions.</p>\n<p>You can achieve this\
          \ by passing the Maori language id as <code>bad_word_ids</code> to the generate\
          \ function: <a href=\"https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.bad_words_ids\"\
          >https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.bad_words_ids</a>(List[List[int]],</p>\n\
          <p>The <code>.generate</code> method will then take care of setting the\
          \ log prob for Maori to -inf as required</p>\n"
        raw: "Hey @mmitchell! That makes sense, thanks for clarifying! No need to\
          \ copy across to other checkpoints - I just wanted to flag this to make\
          \ sure that it was indeed the multilingual checkpoints we were referring\
          \ to \U0001F917\n\nOpenAI curated the dataset by web scraping the internet\
          \ for audio data. Unfortunately, these are _all_ the details we know about\
          \ it from the [Whisper paper](https://arxiv.org/pdf/2212.04356.pdf). They\
          \ subsequently trained 11 speech recognition models on this data. They released\
          \ all of the models, but not the dataset. The dataset remains private and\
          \ AFAIK is not going to be made open-source. So OpenAI are the only ones\
          \ with access to it and the only ones who can re-train the model.\n\nThere\
          \ might be another way to silence the model making Maori predictions. The\
          \ format of Whisper transcriptions is as follows:\n```\n<|startoftranscript|>\
          \ <|en|> <|transcribe|> <|notimestamps|> Hey this is some transcribed text!\
          \ <|endoftranscript|>\n```\n\nThe second token (`<|en|>`) is the language\
          \ id token. This token informs Whisper which model to make its predictions\
          \ in. If the second predicted token is `<|en|>`, Whisper will transcribe\
          \ in English. Likewise if the second predicted token is `<|es|>`, the model\
          \ will transcribe in Spanish, and so on...\n\nIf you want to stop the model\
          \ predicting Maori transcriptions, you could explore _forcing_ the log probability\
          \ for the Maori language id token to -inf. This way, the model will never\
          \ predict Maori as the transcription language, and thus will not generate\
          \ Maori transcriptions.\n\nYou can achieve this by passing the Maori language\
          \ id as `bad_word_ids` to the generate function: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.bad_words_ids(List[List[int]],\n\
          \nThe `.generate` method will then take care of setting the log prob for\
          \ Maori to -inf as required"
        updatedAt: '2023-02-23T17:50:20.577Z'
      numEdits: 2
      reactions: []
    id: 63f7a6cdc9095c7b5afb29cc
    type: comment
  author: sanchit-gandhi
  content: "Hey @mmitchell! That makes sense, thanks for clarifying! No need to copy\
    \ across to other checkpoints - I just wanted to flag this to make sure that it\
    \ was indeed the multilingual checkpoints we were referring to \U0001F917\n\n\
    OpenAI curated the dataset by web scraping the internet for audio data. Unfortunately,\
    \ these are _all_ the details we know about it from the [Whisper paper](https://arxiv.org/pdf/2212.04356.pdf).\
    \ They subsequently trained 11 speech recognition models on this data. They released\
    \ all of the models, but not the dataset. The dataset remains private and AFAIK\
    \ is not going to be made open-source. So OpenAI are the only ones with access\
    \ to it and the only ones who can re-train the model.\n\nThere might be another\
    \ way to silence the model making Maori predictions. The format of Whisper transcriptions\
    \ is as follows:\n```\n<|startoftranscript|> <|en|> <|transcribe|> <|notimestamps|>\
    \ Hey this is some transcribed text! <|endoftranscript|>\n```\n\nThe second token\
    \ (`<|en|>`) is the language id token. This token informs Whisper which model\
    \ to make its predictions in. If the second predicted token is `<|en|>`, Whisper\
    \ will transcribe in English. Likewise if the second predicted token is `<|es|>`,\
    \ the model will transcribe in Spanish, and so on...\n\nIf you want to stop the\
    \ model predicting Maori transcriptions, you could explore _forcing_ the log probability\
    \ for the Maori language id token to -inf. This way, the model will never predict\
    \ Maori as the transcription language, and thus will not generate Maori transcriptions.\n\
    \nYou can achieve this by passing the Maori language id as `bad_word_ids` to the\
    \ generate function: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.bad_words_ids(List[List[int]],\n\
    \nThe `.generate` method will then take care of setting the log prob for Maori\
    \ to -inf as required"
  created_at: 2023-02-23 17:47:57+00:00
  edited: true
  hidden: false
  id: 63f7a6cdc9095c7b5afb29cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2b2acd13dfa0485e85f640b41b20a53d.svg
      fullname: mahelona
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mahelona
      type: user
    createdAt: '2023-03-02T00:47:29.000Z'
    data:
      edited: false
      editors:
      - mahelona
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2b2acd13dfa0485e85f640b41b20a53d.svg
          fullname: mahelona
          isHf: false
          isPro: false
          name: mahelona
          type: user
        html: "<p>Aloha <span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>.\
          \ I do appreciate your responses and consideration for our concerns.</p>\n\
          <p>Do you know much about how <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2205.12446\"\
          >FLEURS</a> was obtained? This is a key dataset used in Whisper with similar\
          \ issues around the lack of transparency on how the data was obtained.</p>\n\
          <p>It's not that we want access to the data used, we want to know what data\
          \ was used, where it came from, and how it was obtained.</p>\n"
        raw: 'Aloha @sanchit-gandhi. I do appreciate your responses and consideration
          for our concerns.


          Do you know much about how [FLEURS](https://arxiv.org/abs/2205.12446) was
          obtained? This is a key dataset used in Whisper with similar issues around
          the lack of transparency on how the data was obtained.


          It''s not that we want access to the data used, we want to know what data
          was used, where it came from, and how it was obtained.'
        updatedAt: '2023-03-02T00:47:29.531Z'
      numEdits: 0
      reactions: []
    id: 63fff2212e30e7993eecdfa3
    type: comment
  author: mahelona
  content: 'Aloha @sanchit-gandhi. I do appreciate your responses and consideration
    for our concerns.


    Do you know much about how [FLEURS](https://arxiv.org/abs/2205.12446) was obtained?
    This is a key dataset used in Whisper with similar issues around the lack of transparency
    on how the data was obtained.


    It''s not that we want access to the data used, we want to know what data was
    used, where it came from, and how it was obtained.'
  created_at: 2023-03-02 00:47:29+00:00
  edited: false
  hidden: false
  id: 63fff2212e30e7993eecdfa3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-03-06T15:39:12.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;mahelona&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mahelona\"\
          >@<span class=\"underline\">mahelona</span></a></span>\n\n\t</span></span>!\
          \ There's a very transparent paper on the FLEURS dataset that you can check-out:\
          \ <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2205.12446\">https://arxiv.org/abs/2205.12446</a></p>\n\
          <p>In summary, the data is derived from the <a rel=\"nofollow\" href=\"\
          https://arxiv.org/abs/2106.03193\">FLoRes-101 dataset</a>, a machine translation\
          \ corpus with 3001 sentence translations from English to 101 other languages.\
          \ Native speakers are recorded narrating the sentence transcriptions in\
          \ their native language. The recorded audio data is paired with the sentence\
          \ transcriptions to yield multilingual speech recognition over all 101 languages.\
          \ This corpus was used for Whisper evaluation (but not training). It's available\
          \ on the HuggingFace Hub here: <a href=\"https://huggingface.co/datasets/google/fleurs\"\
          >https://huggingface.co/datasets/google/fleurs</a></p>\n"
        raw: 'Hey @mahelona! There''s a very transparent paper on the FLEURS dataset
          that you can check-out: https://arxiv.org/abs/2205.12446


          In summary, the data is derived from the [FLoRes-101 dataset](https://arxiv.org/abs/2106.03193),
          a machine translation corpus with 3001 sentence translations from English
          to 101 other languages. Native speakers are recorded narrating the sentence
          transcriptions in their native language. The recorded audio data is paired
          with the sentence transcriptions to yield multilingual speech recognition
          over all 101 languages. This corpus was used for Whisper evaluation (but
          not training). It''s available on the HuggingFace Hub here: https://huggingface.co/datasets/google/fleurs'
        updatedAt: '2023-03-06T15:39:12.040Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ArthurZ
      - count: 1
        reaction: "\U0001F917"
        users:
        - ArthurZ
    id: 6406092024da26506ff1e88b
    type: comment
  author: sanchit-gandhi
  content: 'Hey @mahelona! There''s a very transparent paper on the FLEURS dataset
    that you can check-out: https://arxiv.org/abs/2205.12446


    In summary, the data is derived from the [FLoRes-101 dataset](https://arxiv.org/abs/2106.03193),
    a machine translation corpus with 3001 sentence translations from English to 101
    other languages. Native speakers are recorded narrating the sentence transcriptions
    in their native language. The recorded audio data is paired with the sentence
    transcriptions to yield multilingual speech recognition over all 101 languages.
    This corpus was used for Whisper evaluation (but not training). It''s available
    on the HuggingFace Hub here: https://huggingface.co/datasets/google/fleurs'
  created_at: 2023-03-06 15:39:12+00:00
  edited: false
  hidden: false
  id: 6406092024da26506ff1e88b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: openai/whisper-tiny.en
repo_type: model
status: open
target_branch: null
title: "M\u0101ori language governance & removal"
