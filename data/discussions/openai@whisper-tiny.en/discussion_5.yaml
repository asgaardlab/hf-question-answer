!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SuperXXX
conflicting_files: null
created_at: 2022-11-29 23:26:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef1763b4d7b6c44933d8addc45ab92c6.svg
      fullname: SuperXXX
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SuperXXX
      type: user
    createdAt: '2022-11-29T23:26:42.000Z'
    data:
      edited: false
      editors:
      - SuperXXX
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef1763b4d7b6c44933d8addc45ab92c6.svg
          fullname: SuperXXX
          isHf: false
          isPro: false
          name: SuperXXX
          type: user
        html: "<p>When i use the example </p>\n<pre><code class=\"language-{Python}\"\
          >from transformers import WhisperProcessor, WhisperForConditionalGeneration\n\
          from datasets import load_dataset\nimport torch\n\n# load model and processor\n\
          processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\"\
          )\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\"\
          )\n\n# load dummy dataset and read soundfiles\nds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\"\
          , \"clean\", split=\"validation\")\ninput_features = processor(ds[0][\"\
          audio\"][\"array\"], return_tensors=\"pt\").input_features \n\n# Generate\
          \ logits\nlogits = model(input_features, decoder_input_ids = torch.tensor([[50258]])).logits\
          \ \n# take argmax and decode\npredicted_ids = torch.argmax(logits, dim=-1)\n\
          transcription = processor.batch_decode(predicted_ids)\n</code></pre>\n<p>I\
          \ get result <code>['&lt;|startoftranscript|&gt;']</code></p>\n<p>However,\
          \ if I do</p>\n<pre><code class=\"language-{Python}\">generated_ids = model.generate(inputs=input_features)\n\
          transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\
          print(transcription)\n</code></pre>\n<p>I get result <code> Mr. Quilter\
          \ is the apostle of the middle classes, and we are glad to welcome his gospel.</code><br>Is\
          \ this expected? or how to I modify <code>decoder_input_ids</code> to get\
          \ te equal result?</p>\n"
        raw: "When i use the example \r\n```{Python}\r\nfrom transformers import WhisperProcessor,\
          \ WhisperForConditionalGeneration\r\nfrom datasets import load_dataset\r\
          \nimport torch\r\n\r\n# load model and processor\r\nprocessor = WhisperProcessor.from_pretrained(\"\
          openai/whisper-tiny.en\")\r\nmodel = WhisperForConditionalGeneration.from_pretrained(\"\
          openai/whisper-tiny.en\")\r\n\r\n# load dummy dataset and read soundfiles\r\
          \nds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\"\
          , split=\"validation\")\r\ninput_features = processor(ds[0][\"audio\"][\"\
          array\"], return_tensors=\"pt\").input_features \r\n\r\n# Generate logits\r\
          \nlogits = model(input_features, decoder_input_ids = torch.tensor([[50258]])).logits\
          \ \r\n# take argmax and decode\r\npredicted_ids = torch.argmax(logits, dim=-1)\r\
          \ntranscription = processor.batch_decode(predicted_ids)\r\n```\r\n\r\nI\
          \ get result `['<|startoftranscript|>']`\r\n\r\nHowever, if I do\r\n```{Python}\r\
          \ngenerated_ids = model.generate(inputs=input_features)\r\ntranscription\
          \ = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\r\
          \nprint(transcription)\r\n```\r\n\r\nI get result ` Mr. Quilter is the apostle\
          \ of the middle classes, and we are glad to welcome his gospel.`\r\nIs this\
          \ expected? or how to I modify `decoder_input_ids` to get te equal result?"
        updatedAt: '2022-11-29T23:26:42.025Z'
      numEdits: 0
      reactions: []
    id: 6386953206858a85f5953708
    type: comment
  author: SuperXXX
  content: "When i use the example \r\n```{Python}\r\nfrom transformers import WhisperProcessor,\
    \ WhisperForConditionalGeneration\r\nfrom datasets import load_dataset\r\nimport\
    \ torch\r\n\r\n# load model and processor\r\nprocessor = WhisperProcessor.from_pretrained(\"\
    openai/whisper-tiny.en\")\r\nmodel = WhisperForConditionalGeneration.from_pretrained(\"\
    openai/whisper-tiny.en\")\r\n\r\n# load dummy dataset and read soundfiles\r\n\
    ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"\
    validation\")\r\ninput_features = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"\
    pt\").input_features \r\n\r\n# Generate logits\r\nlogits = model(input_features,\
    \ decoder_input_ids = torch.tensor([[50258]])).logits \r\n# take argmax and decode\r\
    \npredicted_ids = torch.argmax(logits, dim=-1)\r\ntranscription = processor.batch_decode(predicted_ids)\r\
    \n```\r\n\r\nI get result `['<|startoftranscript|>']`\r\n\r\nHowever, if I do\r\
    \n```{Python}\r\ngenerated_ids = model.generate(inputs=input_features)\r\ntranscription\
    \ = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\r\nprint(transcription)\r\
    \n```\r\n\r\nI get result ` Mr. Quilter is the apostle of the middle classes,\
    \ and we are glad to welcome his gospel.`\r\nIs this expected? or how to I modify\
    \ `decoder_input_ids` to get te equal result?"
  created_at: 2022-11-29 23:26:42+00:00
  edited: false
  hidden: false
  id: 6386953206858a85f5953708
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-12-01T11:44:58.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>The example on the README is for one forward pass. Your code snippet
          is correct for auto regressive generation!</p>

          '
        raw: The example on the README is for one forward pass. Your code snippet
          is correct for auto regressive generation!
        updatedAt: '2022-12-01T11:44:58.050Z'
      numEdits: 0
      reactions: []
    id: 638893ba43d8b0797a06e30d
    type: comment
  author: sanchit-gandhi
  content: The example on the README is for one forward pass. Your code snippet is
    correct for auto regressive generation!
  created_at: 2022-12-01 11:44:58+00:00
  edited: false
  hidden: false
  id: 638893ba43d8b0797a06e30d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: openai/whisper-tiny.en
repo_type: model
status: open
target_branch: null
title: Question about decoder_input_ids
