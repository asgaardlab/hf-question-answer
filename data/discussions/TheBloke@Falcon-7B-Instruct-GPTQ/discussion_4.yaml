!!python/object:huggingface_hub.community.DiscussionWithDetails
author: avatar8875
conflicting_files: null
created_at: 2023-05-30 06:52:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/72fb33b24f4bbab5aeb4859ca5bdb778.svg
      fullname: tensax
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: avatar8875
      type: user
    createdAt: '2023-05-30T07:52:37.000Z'
    data:
      edited: false
      editors:
      - avatar8875
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/72fb33b24f4bbab5aeb4859ca5bdb778.svg
          fullname: tensax
          isHf: false
          isPro: false
          name: avatar8875
          type: user
        html: '<p>error-<br>INFO:Loading TheBloke_falcon-7b-instruct-GPTQ...<br>INFO:Found
          the following quantized model: models/TheBloke_falcon-7b-instruct-GPTQ/gptq_model-4bit-64g.safetensors<br>INFO:Using
          the following device map for the quantized model:<br>INFO:Loaded the model
          in 27.56 seconds.</p>

          <p>INFO:HTTP Request: POST <a rel="nofollow" href="http://127.0.0.1:7860/api/predict">http://127.0.0.1:7860/api/predict</a>
          "HTTP/1.1 200 OK"<br>INFO:HTTP Request: POST <a rel="nofollow" href="http://127.0.0.1:7860/api/predict">http://127.0.0.1:7860/api/predict</a>
          "HTTP/1.1 200 OK"<br>INFO:HTTP Request: POST <a rel="nofollow" href="http://127.0.0.1:7860/reset">http://127.0.0.1:7860/reset</a>
          "HTTP/1.1 200 OK"<br>INFO:HTTP Request: POST <a rel="nofollow" href="http://127.0.0.1:7860/api/predict">http://127.0.0.1:7860/api/predict</a>
          "HTTP/1.1 200 OK"<br>INFO:HTTP Request: POST <a rel="nofollow" href="http://127.0.0.1:7860/reset">http://127.0.0.1:7860/reset</a>
          "HTTP/1.1 200 OK"<br>INFO:HTTP Request: POST <a rel="nofollow" href="http://127.0.0.1:7860/api/predict">http://127.0.0.1:7860/api/predict</a>
          "HTTP/1.1 200 OK"<br>INFO:HTTP Request: POST <a rel="nofollow" href="http://127.0.0.1:7860/reset">http://127.0.0.1:7860/reset</a>
          "HTTP/1.1 200 OK"<br>INFO:HTTP Request: POST <a rel="nofollow" href="http://127.0.0.1:7860/api/predict">http://127.0.0.1:7860/api/predict</a>
          "HTTP/1.1 200 OK"<br>Traceback (most recent call last):<br>  File "/content/drive/MyDrive/text-generation-webui/modules/callbacks.py",
          line 73, in gentask<br>    ret = self.mfunc(callback=_callback, **self.kwargs)<br>  File
          "/content/drive/MyDrive/text-generation-webui/modules/text_generation.py",
          line 263, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line
          115, in decorate_context<br>    return func(*args, **kwargs)<br>  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py",
          line 1568, in generate<br>    return self.sample(<br>  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py",
          line 2615, in sample<br>    outputs = self(<br>  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py", line 165,
          in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File "/root/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-7b-instruct-GPTQ/modelling_RW.py",
          line 753, in forward<br>    transformer_outputs = self.transformer(<br>  File
          "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line
          1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py", line 165,
          in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File "/root/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-7b-instruct-GPTQ/modelling_RW.py",
          line 648, in forward<br>    outputs = block(<br>  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py", line 165,
          in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File "/root/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-7b-instruct-GPTQ/modelling_RW.py",
          line 385, in forward<br>    attn_outputs = self.self_attention(<br>  File
          "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line
          1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py", line 165,
          in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File "/root/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-7b-instruct-GPTQ/modelling_RW.py",
          line 279, in forward<br>    attn_output = F.scaled_dot_product_attention(<br>RuntimeError:
          Expected query, key, and value to have the same dtype, but got query.dtype:
          float key.dtype: float and value.dtype: c10::Half instead.<br>Output generated
          in 0.69 seconds (0.00 tokens/s, 0 tokens, context 2, seed 440280390)</p>

          '
        raw: "error-\r\nINFO:Loading TheBloke_falcon-7b-instruct-GPTQ...\r\nINFO:Found\
          \ the following quantized model: models/TheBloke_falcon-7b-instruct-GPTQ/gptq_model-4bit-64g.safetensors\r\
          \nINFO:Using the following device map for the quantized model:\r\nINFO:Loaded\
          \ the model in 27.56 seconds.\r\n\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/api/predict\
          \ \"HTTP/1.1 200 OK\"\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/api/predict\
          \ \"HTTP/1.1 200 OK\"\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/reset\
          \ \"HTTP/1.1 200 OK\"\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/api/predict\
          \ \"HTTP/1.1 200 OK\"\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/reset\
          \ \"HTTP/1.1 200 OK\"\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/api/predict\
          \ \"HTTP/1.1 200 OK\"\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/reset\
          \ \"HTTP/1.1 200 OK\"\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/api/predict\
          \ \"HTTP/1.1 200 OK\"\r\nTraceback (most recent call last):\r\n  File \"\
          /content/drive/MyDrive/text-generation-webui/modules/callbacks.py\", line\
          \ 73, in gentask\r\n    ret = self.mfunc(callback=_callback, **self.kwargs)\r\
          \n  File \"/content/drive/MyDrive/text-generation-webui/modules/text_generation.py\"\
          , line 263, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 1568, in generate\r\n    return self.sample(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
          , line 2615, in sample\r\n    outputs = self(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/root/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-7b-instruct-GPTQ/modelling_RW.py\"\
          , line 753, in forward\r\n    transformer_outputs = self.transformer(\r\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/root/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-7b-instruct-GPTQ/modelling_RW.py\"\
          , line 648, in forward\r\n    outputs = block(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/root/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-7b-instruct-GPTQ/modelling_RW.py\"\
          , line 385, in forward\r\n    attn_outputs = self.self_attention(\r\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/root/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-7b-instruct-GPTQ/modelling_RW.py\"\
          , line 279, in forward\r\n    attn_output = F.scaled_dot_product_attention(\r\
          \nRuntimeError: Expected query, key, and value to have the same dtype, but\
          \ got query.dtype: float key.dtype: float and value.dtype: c10::Half instead.\r\
          \nOutput generated in 0.69 seconds (0.00 tokens/s, 0 tokens, context 2,\
          \ seed 440280390)"
        updatedAt: '2023-05-30T07:52:37.318Z'
      numEdits: 0
      reactions: []
    id: 6475ab45b6cf74b6ed4b6c4d
    type: comment
  author: avatar8875
  content: "error-\r\nINFO:Loading TheBloke_falcon-7b-instruct-GPTQ...\r\nINFO:Found\
    \ the following quantized model: models/TheBloke_falcon-7b-instruct-GPTQ/gptq_model-4bit-64g.safetensors\r\
    \nINFO:Using the following device map for the quantized model:\r\nINFO:Loaded\
    \ the model in 27.56 seconds.\r\n\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/api/predict\
    \ \"HTTP/1.1 200 OK\"\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/api/predict\
    \ \"HTTP/1.1 200 OK\"\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/reset \"\
    HTTP/1.1 200 OK\"\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/api/predict\
    \ \"HTTP/1.1 200 OK\"\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/reset \"\
    HTTP/1.1 200 OK\"\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/api/predict\
    \ \"HTTP/1.1 200 OK\"\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/reset \"\
    HTTP/1.1 200 OK\"\r\nINFO:HTTP Request: POST http://127.0.0.1:7860/api/predict\
    \ \"HTTP/1.1 200 OK\"\r\nTraceback (most recent call last):\r\n  File \"/content/drive/MyDrive/text-generation-webui/modules/callbacks.py\"\
    , line 73, in gentask\r\n    ret = self.mfunc(callback=_callback, **self.kwargs)\r\
    \n  File \"/content/drive/MyDrive/text-generation-webui/modules/text_generation.py\"\
    , line 263, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
    , line 1568, in generate\r\n    return self.sample(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\"\
    , line 2615, in sample\r\n    outputs = self(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in\
    \ new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-7b-instruct-GPTQ/modelling_RW.py\"\
    , line 753, in forward\r\n    transformer_outputs = self.transformer(\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line\
    \ 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"\
    /usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\r\
    \n    output = old_forward(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-7b-instruct-GPTQ/modelling_RW.py\"\
    , line 648, in forward\r\n    outputs = block(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in\
    \ new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-7b-instruct-GPTQ/modelling_RW.py\"\
    , line 385, in forward\r\n    attn_outputs = self.self_attention(\r\n  File \"\
    /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501,\
    \ in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/root/.cache/huggingface/modules/transformers_modules/TheBloke_falcon-7b-instruct-GPTQ/modelling_RW.py\"\
    , line 279, in forward\r\n    attn_output = F.scaled_dot_product_attention(\r\n\
    RuntimeError: Expected query, key, and value to have the same dtype, but got query.dtype:\
    \ float key.dtype: float and value.dtype: c10::Half instead.\r\nOutput generated\
    \ in 0.69 seconds (0.00 tokens/s, 0 tokens, context 2, seed 440280390)"
  created_at: 2023-05-30 06:52:37+00:00
  edited: false
  hidden: false
  id: 6475ab45b6cf74b6ed4b6c4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-30T10:29:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Are you loading this with AutoGPTQ?  passing <code>--autogptq</code>
          to text-generation-webui ?</p>

          '
        raw: Are you loading this with AutoGPTQ?  passing `--autogptq` to text-generation-webui
          ?
        updatedAt: '2023-05-30T10:29:51.147Z'
      numEdits: 0
      reactions: []
    id: 6475d01fe9b57ce0caa5e436
    type: comment
  author: TheBloke
  content: Are you loading this with AutoGPTQ?  passing `--autogptq` to text-generation-webui
    ?
  created_at: 2023-05-30 09:29:51+00:00
  edited: false
  hidden: false
  id: 6475d01fe9b57ce0caa5e436
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/72fb33b24f4bbab5aeb4859ca5bdb778.svg
      fullname: tensax
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: avatar8875
      type: user
    createdAt: '2023-06-06T07:01:34.000Z'
    data:
      edited: false
      editors:
      - avatar8875
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40810105204582214
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/72fb33b24f4bbab5aeb4859ca5bdb778.svg
          fullname: tensax
          isHf: false
          isPro: false
          name: avatar8875
          type: user
        html: "<p>getting this after using this flag<br>Traceback (most recent call\
          \ last): File \u201C/home/tensax/Downloads/projects/text-generation-webui/server.py\u201D\
          , line 74, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
          \ File \u201C/home/tensax/Downloads/projects/text-generation-webui/modules/models.py\u201D\
          , line 95, in load_model output = load_func(model_name) File \u201C/home/tensax/Downloads/projects/text-generation-webui/modules/models.py\u201D\
          , line 278, in AutoGPTQ_loader import modules.AutoGPTQ_loader File \u201C\
          /home/tensax/Downloads/projects/text-generation-webui/modules/AutoGPTQ_loader.py\u201D\
          , line 30 params = { ^ IndentationError: unindent does not match any outer\
          \ indentation level</p>\n"
        raw: "getting this after using this flag \nTraceback (most recent call last):\
          \ File \u201C/home/tensax/Downloads/projects/text-generation-webui/server.py\u201D\
          , line 74, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
          \ File \u201C/home/tensax/Downloads/projects/text-generation-webui/modules/models.py\u201D\
          , line 95, in load_model output = load_func(model_name) File \u201C/home/tensax/Downloads/projects/text-generation-webui/modules/models.py\u201D\
          , line 278, in AutoGPTQ_loader import modules.AutoGPTQ_loader File \u201C\
          /home/tensax/Downloads/projects/text-generation-webui/modules/AutoGPTQ_loader.py\u201D\
          , line 30 params = { ^ IndentationError: unindent does not match any outer\
          \ indentation level"
        updatedAt: '2023-06-06T07:01:34.211Z'
      numEdits: 0
      reactions: []
    id: 647ed9ce2a7bcaa30794ded1
    type: comment
  author: avatar8875
  content: "getting this after using this flag \nTraceback (most recent call last):\
    \ File \u201C/home/tensax/Downloads/projects/text-generation-webui/server.py\u201D\
    , line 74, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
    \ File \u201C/home/tensax/Downloads/projects/text-generation-webui/modules/models.py\u201D\
    , line 95, in load_model output = load_func(model_name) File \u201C/home/tensax/Downloads/projects/text-generation-webui/modules/models.py\u201D\
    , line 278, in AutoGPTQ_loader import modules.AutoGPTQ_loader File \u201C/home/tensax/Downloads/projects/text-generation-webui/modules/AutoGPTQ_loader.py\u201D\
    , line 30 params = { ^ IndentationError: unindent does not match any outer indentation\
    \ level"
  created_at: 2023-06-06 06:01:34+00:00
  edited: false
  hidden: false
  id: 647ed9ce2a7bcaa30794ded1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-07T00:39:44.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8149063587188721
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;avatar8875&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/avatar8875\">@<span class=\"\
          underline\">avatar8875</span></a></span>\n\n\t</span></span> that's a formatting\
          \ error in the code, I think if you pull the latest text-gen-webui it's\
          \ fixed in there.</p>\n<p>Also, --autogptq is not necessary anymore, it\
          \ is on by default. Optionally you can specify --triton, which I find faster\
          \ than the default CUDA.</p>\n"
        raw: '@avatar8875 that''s a formatting error in the code, I think if you pull
          the latest text-gen-webui it''s fixed in there.


          Also, --autogptq is not necessary anymore, it is on by default. Optionally
          you can specify --triton, which I find faster than the default CUDA.'
        updatedAt: '2023-06-07T00:39:44.718Z'
      numEdits: 0
      reactions: []
    id: 647fd1d0cbb8294ed80d7bf5
    type: comment
  author: mancub
  content: '@avatar8875 that''s a formatting error in the code, I think if you pull
    the latest text-gen-webui it''s fixed in there.


    Also, --autogptq is not necessary anymore, it is on by default. Optionally you
    can specify --triton, which I find faster than the default CUDA.'
  created_at: 2023-06-06 23:39:44+00:00
  edited: false
  hidden: false
  id: 647fd1d0cbb8294ed80d7bf5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-08T08:56:34.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.82191401720047
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Also, --autogptq is not necessary anymore, it is on by default. Optionally
          you can specify --triton, which I find faster than the default CUDA.</p>

          </blockquote>

          <p><code>--triton</code> works with AutoGPTQ on Falcon? I thought Falcon
          only worked in AutoGPTQ CUDA at the moment</p>

          '
        raw: '> Also, --autogptq is not necessary anymore, it is on by default. Optionally
          you can specify --triton, which I find faster than the default CUDA.


          `--triton` works with AutoGPTQ on Falcon? I thought Falcon only worked in
          AutoGPTQ CUDA at the moment'
        updatedAt: '2023-06-08T08:56:34.382Z'
      numEdits: 0
      reactions: []
    id: 648197c215c5dc5290644f12
    type: comment
  author: TheBloke
  content: '> Also, --autogptq is not necessary anymore, it is on by default. Optionally
    you can specify --triton, which I find faster than the default CUDA.


    `--triton` works with AutoGPTQ on Falcon? I thought Falcon only worked in AutoGPTQ
    CUDA at the moment'
  created_at: 2023-06-08 07:56:34+00:00
  edited: false
  hidden: false
  id: 648197c215c5dc5290644f12
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-09T00:01:24.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2520645260810852
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I run WizardLM-Uncensored-Falcon-40B-GPTQ with --triton and autogptq.</p>

          <p>INFO:Loading thebloke_wizardlm-uncensored-falcon-40b-gptq...<br>INFO:The
          AutoGPTQ params are: {''model_basename'': ''gptq_model-4bit--1g'', ''device'':
          ''cuda:0'', ''use_triton'': True, ''use_safetensors'': True, ''trust_remote_code'':
          True, ''max_memory'': None, ''quantize_config'': None}</p>

          <p>Getting ~2 t/s or so.</p>

          <p>Without --triton I get ~0.82 t/s.</p>

          '
        raw: 'I run WizardLM-Uncensored-Falcon-40B-GPTQ with --triton and autogptq.


          INFO:Loading thebloke_wizardlm-uncensored-falcon-40b-gptq...

          INFO:The AutoGPTQ params are: {''model_basename'': ''gptq_model-4bit--1g'',
          ''device'': ''cuda:0'', ''use_triton'': True, ''use_safetensors'': True,
          ''trust_remote_code'': True, ''max_memory'': None, ''quantize_config'':
          None}


          Getting ~2 t/s or so.


          Without --triton I get ~0.82 t/s.'
        updatedAt: '2023-06-09T00:01:24.903Z'
      numEdits: 0
      reactions: []
    id: 64826bd43f1579bddc2e73da
    type: comment
  author: mancub
  content: 'I run WizardLM-Uncensored-Falcon-40B-GPTQ with --triton and autogptq.


    INFO:Loading thebloke_wizardlm-uncensored-falcon-40b-gptq...

    INFO:The AutoGPTQ params are: {''model_basename'': ''gptq_model-4bit--1g'', ''device'':
    ''cuda:0'', ''use_triton'': True, ''use_safetensors'': True, ''trust_remote_code'':
    True, ''max_memory'': None, ''quantize_config'': None}


    Getting ~2 t/s or so.


    Without --triton I get ~0.82 t/s.'
  created_at: 2023-06-08 23:01:24+00:00
  edited: false
  hidden: false
  id: 64826bd43f1579bddc2e73da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-20T10:14:45.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.92154860496521
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK, good to know!</p>

          <p>For now though I''d recommend people use the new GGMLs. I get 8 t/s when
          fully loaded on GPU.</p>

          '
        raw: 'OK, good to know!


          For now though I''d recommend people use the new GGMLs. I get 8 t/s when
          fully loaded on GPU.'
        updatedAt: '2023-06-20T10:14:45.229Z'
      numEdits: 0
      reactions: []
    id: 64917c1509d6848c5ef50077
    type: comment
  author: TheBloke
  content: 'OK, good to know!


    For now though I''d recommend people use the new GGMLs. I get 8 t/s when fully
    loaded on GPU.'
  created_at: 2023-06-20 09:14:45+00:00
  edited: false
  hidden: false
  id: 64917c1509d6848c5ef50077
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Falcon-7B-Instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: Getting 0 tokens while running using text-generation -webui
