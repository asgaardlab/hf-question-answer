!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-05-27 12:23:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-27T13:23:14.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Would 128 groups size, the suggested one, be better?</p>

          '
        raw: Would 128 groups size, the suggested one, be better?
        updatedAt: '2023-05-27T13:23:14.479Z'
      numEdits: 0
      reactions: []
    id: 64720442ba726cc401bd563f
    type: comment
  author: Yhyu13
  content: Would 128 groups size, the suggested one, be better?
  created_at: 2023-05-27 12:23:14+00:00
  edited: false
  hidden: false
  id: 64720442ba726cc401bd563f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-27T13:43:45.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>There isn''t really a ''suggested'' groupsize.  128 has tended to
          be default for Llama models under 30B in size, but I believe that''s more
          from convention than based on any hard testing. It is possible that for
          Llama models a groupsize of &lt;128 could increase VRAM usage too much for
          certain GPUs, eg going over an 8GB or 12Gb threshold, but I''m not certain
          that''s the case.</p>

          <p>However Falcon 7B is behaving differently. VRAM usage doesn''t seem to
          be an issue as I was able to return 2000 tokens in under 8GB VRAM usage.  VRAM
          usage seems to grow very little as context increases on this model, which
          is quite different to Llama.  I''m not yet sure why that is. </p>

          <p>So as group_size = 64 improves inference quality a little, I have gone
          for that.  Maybe I could have done 32 even.</p>

          '
        raw: "There isn't really a 'suggested' groupsize.  128 has tended to be default\
          \ for Llama models under 30B in size, but I believe that's more from convention\
          \ than based on any hard testing. It is possible that for Llama models a\
          \ groupsize of <128 could increase VRAM usage too much for certain GPUs,\
          \ eg going over an 8GB or 12Gb threshold, but I'm not certain that's the\
          \ case.\n\nHowever Falcon 7B is behaving differently. VRAM usage doesn't\
          \ seem to be an issue as I was able to return 2000 tokens in under 8GB VRAM\
          \ usage.  VRAM usage seems to grow very little as context increases on this\
          \ model, which is quite different to Llama.  I'm not yet sure why that is.\
          \ \n\nSo as group_size = 64 improves inference quality a little, I have\
          \ gone for that.  Maybe I could have done 32 even."
        updatedAt: '2023-05-27T13:44:45.382Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - LoneStriker
        - Yhyu13
    id: 6472091197a75cc77aac3a85
    type: comment
  author: TheBloke
  content: "There isn't really a 'suggested' groupsize.  128 has tended to be default\
    \ for Llama models under 30B in size, but I believe that's more from convention\
    \ than based on any hard testing. It is possible that for Llama models a groupsize\
    \ of <128 could increase VRAM usage too much for certain GPUs, eg going over an\
    \ 8GB or 12Gb threshold, but I'm not certain that's the case.\n\nHowever Falcon\
    \ 7B is behaving differently. VRAM usage doesn't seem to be an issue as I was\
    \ able to return 2000 tokens in under 8GB VRAM usage.  VRAM usage seems to grow\
    \ very little as context increases on this model, which is quite different to\
    \ Llama.  I'm not yet sure why that is. \n\nSo as group_size = 64 improves inference\
    \ quality a little, I have gone for that.  Maybe I could have done 32 even."
  created_at: 2023-05-27 12:43:45+00:00
  edited: true
  hidden: false
  id: 6472091197a75cc77aac3a85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-27T16:19:43.000Z'
    data:
      status: closed
    id: 64722d9fc27f74a0eba5f72e
    type: status-change
  author: Yhyu13
  created_at: 2023-05-27 15:19:43+00:00
  id: 64722d9fc27f74a0eba5f72e
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-27T16:20:22.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<blockquote>\n<p>There isn't really a 'suggested' groupsize.  128 has\
          \ tended to be default for Llama models under 30B in size, but I believe\
          \ that's more from convention than based on any hard testing. It is possible\
          \ that for Llama models a groupsize of &lt;128 could increase VRAM usage\
          \ too much for certain GPUs, eg going over an 8GB or 12Gb threshold, but\
          \ I'm not certain that's the case.</p>\n<p>However Falcon 7B is behaving\
          \ differently. VRAM usage doesn't seem to be an issue as I was able to return\
          \ 2000 tokens in under 8GB VRAM usage.  VRAM usage seems to grow very little\
          \ as context increases on this model, which is quite different to Llama.\
          \  I'm not yet sure why that is. </p>\n<p>So as group_size = 64 improves\
          \ inference quality a little, I have gone for that.  Maybe I could have\
          \ done 32 even.</p>\n</blockquote>\n<p>Thank\uFF01 Valuable observation\
          \ there\uFF01</p>\n"
        raw: "> There isn't really a 'suggested' groupsize.  128 has tended to be\
          \ default for Llama models under 30B in size, but I believe that's more\
          \ from convention than based on any hard testing. It is possible that for\
          \ Llama models a groupsize of <128 could increase VRAM usage too much for\
          \ certain GPUs, eg going over an 8GB or 12Gb threshold, but I'm not certain\
          \ that's the case.\n> \n> However Falcon 7B is behaving differently. VRAM\
          \ usage doesn't seem to be an issue as I was able to return 2000 tokens\
          \ in under 8GB VRAM usage.  VRAM usage seems to grow very little as context\
          \ increases on this model, which is quite different to Llama.  I'm not yet\
          \ sure why that is. \n> \n> So as group_size = 64 improves inference quality\
          \ a little, I have gone for that.  Maybe I could have done 32 even.\n\n\
          Thank\uFF01 Valuable observation there\uFF01"
        updatedAt: '2023-05-27T16:20:22.220Z'
      numEdits: 0
      reactions: []
    id: 64722dc65afd6a696586d9b4
    type: comment
  author: Yhyu13
  content: "> There isn't really a 'suggested' groupsize.  128 has tended to be default\
    \ for Llama models under 30B in size, but I believe that's more from convention\
    \ than based on any hard testing. It is possible that for Llama models a groupsize\
    \ of <128 could increase VRAM usage too much for certain GPUs, eg going over an\
    \ 8GB or 12Gb threshold, but I'm not certain that's the case.\n> \n> However Falcon\
    \ 7B is behaving differently. VRAM usage doesn't seem to be an issue as I was\
    \ able to return 2000 tokens in under 8GB VRAM usage.  VRAM usage seems to grow\
    \ very little as context increases on this model, which is quite different to\
    \ Llama.  I'm not yet sure why that is. \n> \n> So as group_size = 64 improves\
    \ inference quality a little, I have gone for that.  Maybe I could have done 32\
    \ even.\n\nThank\uFF01 Valuable observation there\uFF01"
  created_at: 2023-05-27 15:20:22+00:00
  edited: false
  hidden: false
  id: 64722dc65afd6a696586d9b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654022901910-61a8d1aac664736898ffc84f.jpeg?w=200&h=200&f=face
      fullname: Daniel Hesslow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DanielHesslow
      type: user
    createdAt: '2023-05-28T16:38:35.000Z'
    data:
      edited: false
      editors:
      - DanielHesslow
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654022901910-61a8d1aac664736898ffc84f.jpeg?w=200&h=200&f=face
          fullname: Daniel Hesslow
          isHf: false
          isPro: false
          name: DanielHesslow
          type: user
        html: '<p>Due to multi-query attention the model only needs to save one k,v
          per position instead of num_heads, for the 7b this corresponds to a factor
          of 71 :)</p>

          '
        raw: Due to multi-query attention the model only needs to save one k,v per
          position instead of num_heads, for the 7b this corresponds to a factor of
          71 :)
        updatedAt: '2023-05-28T16:38:35.553Z'
      numEdits: 0
      reactions: []
    id: 6473838b2a74fb43ccdfe3e9
    type: comment
  author: DanielHesslow
  content: Due to multi-query attention the model only needs to save one k,v per position
    instead of num_heads, for the 7b this corresponds to a factor of 71 :)
  created_at: 2023-05-28 15:38:35+00:00
  edited: false
  hidden: false
  id: 6473838b2a74fb43ccdfe3e9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-28T16:42:43.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh thanks @Seledorn that''s really interesting !</p>

          '
        raw: Oh thanks @Seledorn that's really interesting !
        updatedAt: '2023-05-28T16:42:43.038Z'
      numEdits: 0
      reactions: []
    id: 647384832a74fb43ccdff733
    type: comment
  author: TheBloke
  content: Oh thanks @Seledorn that's really interesting !
  created_at: 2023-05-28 15:42:43+00:00
  edited: false
  hidden: false
  id: 647384832a74fb43ccdff733
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Falcon-7B-Instruct-GPTQ
repo_type: model
status: closed
target_branch: null
title: Thanks for sharing! Just notice that you have uploaded the 64 group size
