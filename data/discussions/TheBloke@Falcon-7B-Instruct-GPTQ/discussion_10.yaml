!!python/object:huggingface_hub.community.DiscussionWithDetails
author: clementdesroches
conflicting_files: null
created_at: 2023-06-09 12:25:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/106d30a576b0fb58118ac4333b17260b.svg
      fullname: Clement Desroches
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: clementdesroches
      type: user
    createdAt: '2023-06-09T13:25:45.000Z'
    data:
      edited: false
      editors:
      - clementdesroches
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6206503510475159
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/106d30a576b0fb58118ac4333b17260b.svg
          fullname: Clement Desroches
          isHf: false
          isPro: false
          name: clementdesroches
          type: user
        html: '<p>Hello,<br>I''m trying to use this model with a transformers pipeline
          in order to integrate it with a langchain agent afterwards.<br>My code is.
          :</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=False)</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0",
          use_triton=False,<br>                                           use_safetensors=True,
          torch_dtype=torch.float32, trust_remote_code=True)</p>

          <p>quantized_pipeline = transformers.pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    device_map="cuda:0",<br>    max_length=512,<br>)</p>

          <p>I get this error : The model ''RWGPTQForCausalLM'' is not supported for
          text-generation.</p>

          <p>Do you have an idea ?</p>

          '
        raw: "Hello,\r\nI'm trying to use this model with a transformers pipeline\
          \ in order to integrate it with a langchain agent afterwards. \r\nMy code\
          \ is. :\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=False)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\
          \ device=\"cuda:0\", use_triton=False, \r\n                            \
          \               use_safetensors=True, torch_dtype=torch.float32, trust_remote_code=True)\r\
          \n\r\nquantized_pipeline = transformers.pipeline(\r\n    \"text-generation\"\
          ,\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n    device_map=\"cuda:0\"\
          ,\r\n    max_length=512,\r\n)\r\n\r\nI get this error : The model 'RWGPTQForCausalLM'\
          \ is not supported for text-generation.\r\n\r\nDo you have an idea ?"
        updatedAt: '2023-06-09T13:25:45.100Z'
      numEdits: 0
      reactions: []
    id: 648328597b7195e9d24e1e95
    type: comment
  author: clementdesroches
  content: "Hello,\r\nI'm trying to use this model with a transformers pipeline in\
    \ order to integrate it with a langchain agent afterwards. \r\nMy code is. :\r\
    \n\r\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=False)\r\
    \n\r\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=\"\
    cuda:0\", use_triton=False, \r\n                                           use_safetensors=True,\
    \ torch_dtype=torch.float32, trust_remote_code=True)\r\n\r\nquantized_pipeline\
    \ = transformers.pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\n\
    \    tokenizer=tokenizer,\r\n    device_map=\"cuda:0\",\r\n    max_length=512,\r\
    \n)\r\n\r\nI get this error : The model 'RWGPTQForCausalLM' is not supported for\
    \ text-generation.\r\n\r\nDo you have an idea ?"
  created_at: 2023-06-09 12:25:45+00:00
  edited: false
  hidden: false
  id: 648328597b7195e9d24e1e95
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1a1992146da14299cdaf10221e71955.svg
      fullname: Mohith Kune
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mohith7548
      type: user
    createdAt: '2023-06-16T08:30:24.000Z'
    data:
      edited: false
      editors:
      - Mohith7548
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.840583086013794
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1a1992146da14299cdaf10221e71955.svg
          fullname: Mohith Kune
          isHf: false
          isPro: false
          name: Mohith7548
          type: user
        html: '<p>Me too</p>

          '
        raw: Me too
        updatedAt: '2023-06-16T08:30:24.158Z'
      numEdits: 0
      reactions: []
    id: 648c1da0e360ee9b7881892d
    type: comment
  author: Mohith7548
  content: Me too
  created_at: 2023-06-16 07:30:24+00:00
  edited: false
  hidden: false
  id: 648c1da0e360ee9b7881892d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-16T11:26:58.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.593769371509552
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>This is not actually an error, or a problem. It is a wrong error\
          \ message printed by transformers.  You can just ignore the 'error' message\
          \ and it will work fine. </p>\n<p>I have just updated the README to add\
          \ a pipeline example, and to mention the error message</p>\n<p>Here is working\
          \ example code that includes pipeline:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, pipeline, logging\n<span class=\"hljs-keyword\"\
          >from</span> auto_gptq <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\n<span class=\"hljs-keyword\">import</span> argparse\n\
          \nmodel_name_or_path = <span class=\"hljs-string\">\"TheBloke/falcon-7b-instruct-GPTQ\"\
          </span>\n<span class=\"hljs-comment\"># You could also download the model\
          \ locally, and access it there</span>\n<span class=\"hljs-comment\"># model_name_or_path\
          \ = \"/path/to/TheBloke_falcon-7b-instruct-GPTQ\"</span>\n\nmodel_basename\
          \ = <span class=\"hljs-string\">\"gptq_model-4bit-64g\"</span>\n\nuse_triton\
          \ = <span class=\"hljs-literal\">False</span>\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=<span class=\"hljs-literal\">True</span>)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=<span class=\"\
          hljs-literal\">True</span>,\n        trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>,\n        device=<span class=\"hljs-string\">\"cuda:0\"</span>,\n\
          \        use_triton=use_triton,\n        quantize_config=<span class=\"\
          hljs-literal\">None</span>)\n\nprompt = <span class=\"hljs-string\">\"Tell\
          \ me about AI\"</span>\nprompt_template=<span class=\"hljs-string\">f'''###\
          \ Human: <span class=\"hljs-subst\">{prompt}</span></span>\n<span class=\"\
          hljs-string\">### Assistant:'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n\n<span class=\"hljs-comment\"># Inference\
          \ can also be done using transformers' pipeline</span>\n<span class=\"hljs-comment\"\
          ># Note that if you use pipeline, you will see a spurious error message\
          \ saying the model type is not supported</span>\n<span class=\"hljs-comment\"\
          ># This can be ignored!  Or you can hide it with the following logging line:</span>\n\
          <span class=\"hljs-comment\"># Prevent printing spurious transformers error\
          \ when using pipeline with AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\
          \n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"*** Pipeline:\"</span>)\npipe = pipeline(\n    <span class=\"hljs-string\"\
          >\"text-generation\"</span>,\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=<span class=\"hljs-number\">512</span>,\n    temperature=<span\
          \ class=\"hljs-number\">0.7</span>,\n    top_p=<span class=\"hljs-number\"\
          >0.95</span>,\n    repetition_penalty=<span class=\"hljs-number\">1.15</span>\n\
          )\n\n<span class=\"hljs-built_in\">print</span>(pipe(prompt_template)[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'generated_text'</span>])\n\
          </code></pre>\n"
        raw: "This is not actually an error, or a problem. It is a wrong error message\
          \ printed by transformers.  You can just ignore the 'error' message and\
          \ it will work fine. \n\nI have just updated the README to add a pipeline\
          \ example, and to mention the error message\n\nHere is working example code\
          \ that includes pipeline:\n```python\nfrom transformers import AutoTokenizer,\
          \ pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          import argparse\n\nmodel_name_or_path = \"TheBloke/falcon-7b-instruct-GPTQ\"\
          \n# You could also download the model locally, and access it there\n# model_name_or_path\
          \ = \"/path/to/TheBloke_falcon-7b-instruct-GPTQ\"\n\nmodel_basename = \"\
          gptq_model-4bit-64g\"\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=True,\n\
          \        trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''###\
          \ Human: {prompt}\n### Assistant:'''\n\nprint(\"\\n\\n*** Generate:\")\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n\n# Inference can also be done using\
          \ transformers' pipeline\n# Note that if you use pipeline, you will see\
          \ a spurious error message saying the model type is not supported\n# This\
          \ can be ignored!  Or you can hide it with the following logging line:\n\
          # Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\nprint(\"*** Pipeline:\"\
          )\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n\
          )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n```\n"
        updatedAt: '2023-06-16T11:27:39.560Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - flux-equals-rad
    id: 648c47022dd7247ce6f8618b
    type: comment
  author: TheBloke
  content: "This is not actually an error, or a problem. It is a wrong error message\
    \ printed by transformers.  You can just ignore the 'error' message and it will\
    \ work fine. \n\nI have just updated the README to add a pipeline example, and\
    \ to mention the error message\n\nHere is working example code that includes pipeline:\n\
    ```python\nfrom transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
    \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\nmodel_name_or_path\
    \ = \"TheBloke/falcon-7b-instruct-GPTQ\"\n# You could also download the model\
    \ locally, and access it there\n# model_name_or_path = \"/path/to/TheBloke_falcon-7b-instruct-GPTQ\"\
    \n\nmodel_basename = \"gptq_model-4bit-64g\"\n\nuse_triton = False\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nmodel\
    \ = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n        model_basename=model_basename,\n\
    \        use_safetensors=True,\n        trust_remote_code=True,\n        device=\"\
    cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\nprompt\
    \ = \"Tell me about AI\"\nprompt_template=f'''### Human: {prompt}\n### Assistant:'''\n\
    \nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers'\
    \ pipeline\n# Note that if you use pipeline, you will see a spurious error message\
    \ saying the model type is not supported\n# This can be ignored!  Or you can hide\
    \ it with the following logging line:\n# Prevent printing spurious transformers\
    \ error when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
    \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n\
    \    tokenizer=tokenizer,\n    max_new_tokens=512,\n    temperature=0.7,\n   \
    \ top_p=0.95,\n    repetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    ```\n"
  created_at: 2023-06-16 10:26:58+00:00
  edited: true
  hidden: false
  id: 648c47022dd7247ce6f8618b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
      fullname: Deepak  Kaura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkaura26
      type: user
    createdAt: '2023-07-06T18:27:32.000Z'
    data:
      edited: false
      editors:
      - deepakkaura26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5296397805213928
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
          fullname: Deepak  Kaura
          isHf: false
          isPro: false
          name: deepakkaura26
          type: user
        html: "<p>from transformers import AutoTokenizer, pipeline, logging<br>from\
          \ auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig<br>import argparse</p>\n\
          <p>model_name_or_path = \"TheBloke/falcon-7b-instruct-GPTQ\"</p>\n<h1 id=\"\
          you-could-also-download-the-model-locally-and-access-it-there\">You could\
          \ also download the model locally, and access it there</h1>\n<h1 id=\"model_name_or_path--pathtothebloke_falcon-7b-instruct-gptq\"\
          >model_name_or_path = \"/path/to/TheBloke_falcon-7b-instruct-GPTQ\"</h1>\n\
          <p>model_basename = \"gptq_model-4bit-64g\"</p>\n<p>use_triton = False</p>\n\
          <p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)</p>\n\
          <p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>  \
          \      model_basename=model_basename,<br>        use_safetensors=True,<br>\
          \        trust_remote_code=True,<br>        device=\"cuda:0\",<br>     \
          \   use_triton=use_triton,<br>        quantize_config=None)</p>\n<p>prompt\
          \ = \"Tell me about AI\"<br>prompt_template=f'''### Human: {prompt}</p>\n\
          <h3 id=\"assistant\">Assistant:'''</h3>\n<p>print(\"\\n\\n*** Generate:\"\
          )</p>\n<p>input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()<br>output\
          \ = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)<br>print(tokenizer.decode(output[0]))</p>\n\
          <h1 id=\"inference-can-also-be-done-using-transformers-pipeline\">Inference\
          \ can also be done using transformers' pipeline</h1>\n<h1 id=\"note-that-if-you-use-pipeline-you-will-see-a-spurious-error-message-saying-the-model-type-is-not-supported\"\
          >Note that if you use pipeline, you will see a spurious error message saying\
          \ the model type is not supported</h1>\n<h1 id=\"this-can-be-ignored--or-you-can-hide-it-with-the-following-logging-line\"\
          >This can be ignored!  Or you can hide it with the following logging line:</h1>\n\
          <h1 id=\"prevent-printing-spurious-transformers-error-when-using-pipeline-with-autogptq\"\
          >Prevent printing spurious transformers error when using pipeline with AutoGPTQ</h1>\n\
          <p>logging.set_verbosity(logging.CRITICAL)</p>\n<p>print(\"*** Pipeline:\"\
          )<br>pipe = pipeline(<br>    \"text-generation\",<br>    model=model,<br>\
          \    tokenizer=tokenizer,<br>    max_new_tokens=512,<br>    temperature=0.7,<br>\
          \    top_p=0.95,<br>    repetition_penalty=1.15<br>)</p>\n<p>print(pipe(prompt_template)[0]['generated_text'])</p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> how can I run\
          \ above model with CPU ? </p>\n"
        raw: "from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\nmodel_name_or_path\
          \ = \"TheBloke/falcon-7b-instruct-GPTQ\"\n# You could also download the\
          \ model locally, and access it there\n# model_name_or_path = \"/path/to/TheBloke_falcon-7b-instruct-GPTQ\"\
          \n\nmodel_basename = \"gptq_model-4bit-64g\"\n\nuse_triton = False\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\n\
          model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n       \
          \ model_basename=model_basename,\n        use_safetensors=True,\n      \
          \  trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''###\
          \ Human: {prompt}\n### Assistant:'''\n\nprint(\"\\n\\n*** Generate:\")\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n\n# Inference can also be done using\
          \ transformers' pipeline\n# Note that if you use pipeline, you will see\
          \ a spurious error message saying the model type is not supported\n# This\
          \ can be ignored!  Or you can hide it with the following logging line:\n\
          # Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\nprint(\"*** Pipeline:\"\
          )\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n\
          )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\n\n\n@TheBloke\
          \ how can I run above model with CPU ? "
        updatedAt: '2023-07-06T18:27:32.509Z'
      numEdits: 0
      reactions: []
    id: 64a707943987f4dd3c3c8c95
    type: comment
  author: deepakkaura26
  content: "from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
    \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\nmodel_name_or_path\
    \ = \"TheBloke/falcon-7b-instruct-GPTQ\"\n# You could also download the model\
    \ locally, and access it there\n# model_name_or_path = \"/path/to/TheBloke_falcon-7b-instruct-GPTQ\"\
    \n\nmodel_basename = \"gptq_model-4bit-64g\"\n\nuse_triton = False\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nmodel\
    \ = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n        model_basename=model_basename,\n\
    \        use_safetensors=True,\n        trust_remote_code=True,\n        device=\"\
    cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\nprompt\
    \ = \"Tell me about AI\"\nprompt_template=f'''### Human: {prompt}\n### Assistant:'''\n\
    \nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers'\
    \ pipeline\n# Note that if you use pipeline, you will see a spurious error message\
    \ saying the model type is not supported\n# This can be ignored!  Or you can hide\
    \ it with the following logging line:\n# Prevent printing spurious transformers\
    \ error when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
    \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n\
    \    tokenizer=tokenizer,\n    max_new_tokens=512,\n    temperature=0.7,\n   \
    \ top_p=0.95,\n    repetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    \n\n\n@TheBloke how can I run above model with CPU ? "
  created_at: 2023-07-06 17:27:32+00:00
  edited: false
  hidden: false
  id: 64a707943987f4dd3c3c8c95
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/be2b0931c2ca511d1edf6528cd9c74d7.svg
      fullname: n
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: potatodream
      type: user
    createdAt: '2023-07-07T05:43:37.000Z'
    data:
      edited: false
      editors:
      - potatodream
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45361441373825073
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/be2b0931c2ca511d1edf6528cd9c74d7.svg
          fullname: n
          isHf: false
          isPro: false
          name: potatodream
          type: user
        html: "<p>from langchain import HuggingFacePipeline<br>from transformers import\
          \ AutoTokenizer, pipeline, logging<br>from auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig</p>\n<pre><code>print(f\"Opening with AutoGPTQ: {model_name_or_path},\
          \ {model_basename}\")\n    \nuse_triton = False\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        use_safetensors=True,\n        model_basename=model_basename,\n\
          \        device=\"cuda:0\",\n        use_triton=use_triton,\n        trust_remote_code=True,\n\
          \        quantize_config=None)\n\n# Prevent printing spurious transformers\
          \ error when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
          \n#print(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n \
          \   temperature=0.2,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\
          llm = HuggingFacePipeline(pipeline=pipe)\n</code></pre>\n"
        raw: "from langchain import HuggingFacePipeline\nfrom transformers import\
          \ AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\n\n    print(f\"Opening with AutoGPTQ: {model_name_or_path},\
          \ {model_basename}\")\n        \n    use_triton = False\n    tokenizer =\
          \ AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n   \
          \ model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n     \
          \       use_safetensors=True,\n            model_basename=model_basename,\n\
          \            device=\"cuda:0\",\n            use_triton=use_triton,\n  \
          \          trust_remote_code=True,\n            quantize_config=None)\n\n\
          \    # Prevent printing spurious transformers error when using pipeline\
          \ with AutoGPTQ\n    logging.set_verbosity(logging.CRITICAL)\n\n    #print(\"\
          *** Pipeline:\")\n    pipe = pipeline(\n        \"text-generation\",\n \
          \       model=model,\n        tokenizer=tokenizer,\n        max_new_tokens=512,\n\
          \        temperature=0.2,\n        top_p=0.95,\n        repetition_penalty=1.15\n\
          \    )\n    llm = HuggingFacePipeline(pipeline=pipe)\n"
        updatedAt: '2023-07-07T05:43:37.427Z'
      numEdits: 0
      reactions: []
    id: 64a7a609d738314d6869bd35
    type: comment
  author: potatodream
  content: "from langchain import HuggingFacePipeline\nfrom transformers import AutoTokenizer,\
    \ pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
    \n    print(f\"Opening with AutoGPTQ: {model_name_or_path}, {model_basename}\"\
    )\n        \n    use_triton = False\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\n    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \            use_safetensors=True,\n            model_basename=model_basename,\n\
    \            device=\"cuda:0\",\n            use_triton=use_triton,\n        \
    \    trust_remote_code=True,\n            quantize_config=None)\n\n    # Prevent\
    \ printing spurious transformers error when using pipeline with AutoGPTQ\n   \
    \ logging.set_verbosity(logging.CRITICAL)\n\n    #print(\"*** Pipeline:\")\n \
    \   pipe = pipeline(\n        \"text-generation\",\n        model=model,\n   \
    \     tokenizer=tokenizer,\n        max_new_tokens=512,\n        temperature=0.2,\n\
    \        top_p=0.95,\n        repetition_penalty=1.15\n    )\n    llm = HuggingFacePipeline(pipeline=pipe)\n"
  created_at: 2023-07-07 04:43:37+00:00
  edited: false
  hidden: false
  id: 64a7a609d738314d6869bd35
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-07T09:10:11.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8230558037757874
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;deepakkaura26&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/deepakkaura26\"\
          >@<span class=\"underline\">deepakkaura26</span></a></span>\n\n\t</span></span>\
          \ change <code>device=\"cuda:0\"</code> to <code>device=\"cpu\"</code>.\
          \  It will be slow as hell though.</p>\n"
        raw: '@deepakkaura26 change `device="cuda:0"` to `device="cpu"`.  It will
          be slow as hell though.'
        updatedAt: '2023-07-07T09:10:11.713Z'
      numEdits: 0
      reactions: []
    id: 64a7d67372e39badd0214f2e
    type: comment
  author: TheBloke
  content: '@deepakkaura26 change `device="cuda:0"` to `device="cpu"`.  It will be
    slow as hell though.'
  created_at: 2023-07-07 08:10:11+00:00
  edited: false
  hidden: false
  id: 64a7d67372e39badd0214f2e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: TheBloke/Falcon-7B-Instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: Integration to transformers pipeline
