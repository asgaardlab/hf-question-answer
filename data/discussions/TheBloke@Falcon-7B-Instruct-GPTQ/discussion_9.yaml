!!python/object:huggingface_hub.community.DiscussionWithDetails
author: thefaheem
conflicting_files: null
created_at: 2023-06-02 05:34:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T06:34:31.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: "<p>This is what my code is </p>\n<pre><code class=\"language-import\"\
          >from transformers import AutoTokenizer\nfrom auto_gptq import AutoGPTQForCausalLM\n\
          \n# Download the model from HF and store it locally, then reference its\
          \ location here:\n#quantized_model_dir = model_path\n\nfrom transformers\
          \ import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"TheBloke/falcon-7b-instruct-GPTQ\"\
          , use_fast=False)\n\nmodel = AutoGPTQForCausalLM.from_quantized(\"TheBloke/falcon-7b-instruct-GPTQ\"\
          , device=\"cuda:1\", use_triton=False, use_safetensors=True, trust_remote_code=True)\n\
          </code></pre>\n<p>When I Run This, i Got These:</p>\n<pre><code class=\"\
          language-A\">- configuration_RW.py\n. Make sure to double-check they do\
          \ not contain any added malicious code. To avoid downloading new versions\
          \ of the code file, you can pin a revision.\n\u256D\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u256E\n\u2502 in &lt;cell line: 11&gt;:11     \
          \                                                                      \
          \ \u2502\n\u2502                                                       \
          \                                           \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:62\
          \ in from_quantized          \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502   59 \u2502   \u2502   model_basename: Optional[str] = None,  \
          \                                             \u2502\n\u2502   60 \u2502\
          \   \u2502   trust_remote_code: bool = False                           \
          \                          \u2502\n\u2502   61 \u2502   ) -&gt; BaseGPTQForCausalLM:\
          \                                                               \u2502\n\
          \u2502 \u2771 62 \u2502   \u2502   model_type = check_and_get_model_type(save_dir)\
          \                                     \u2502\n\u2502   63 \u2502   \u2502\
          \   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(        \
          \                 \u2502\n\u2502   64 \u2502   \u2502   \u2502   save_dir=save_dir,\
          \                                                              \u2502\n\u2502\
          \   65 \u2502   \u2502   \u2502   device=device,                       \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_utils.py:124\
          \ in                      \u2502\n\u2502 check_and_get_model_type      \
          \                                                                   \u2502\
          \n\u2502                                                               \
          \                                   \u2502\n\u2502   121 def check_and_get_model_type(model_dir):\
          \                                                   \u2502\n\u2502   122\
          \ \u2502   config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)\
          \                 \u2502\n\u2502   123 \u2502   if config.model_type not\
          \ in SUPPORTED_MODELS:                                          \u2502\n\
          \u2502 \u2771 124 \u2502   \u2502   raise TypeError(f\"{config.model_type}\
          \ isn't supported yet.\")                       \u2502\n\u2502   125 \u2502\
          \   model_type = config.model_type                                     \
          \                    \u2502\n\u2502   126 \u2502   return model_type   \
          \                                                                   \u2502\
          \n\u2502   127                                                         \
          \                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nTypeError:\
          \ RefinedWebModel isn't supported yet.\n</code></pre>\n<p>How Should I Get\
          \ Rid of These?</p>\n<p>Can SomeBody Help?</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \  can you please chime in. Thanks!</p>\n"
        raw: "This is what my code is \r\n\r\n```import torch\r\nfrom transformers\
          \ import AutoTokenizer\r\nfrom auto_gptq import AutoGPTQForCausalLM\r\n\r\
          \n# Download the model from HF and store it locally, then reference its\
          \ location here:\r\n#quantized_model_dir = model_path\r\n\r\nfrom transformers\
          \ import AutoTokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(\"TheBloke/falcon-7b-instruct-GPTQ\"\
          , use_fast=False)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(\"TheBloke/falcon-7b-instruct-GPTQ\"\
          , device=\"cuda:1\", use_triton=False, use_safetensors=True, trust_remote_code=True)\r\
          \n```\r\n\r\nWhen I Run This, i Got These:\r\n\r\n```A new version of the\
          \ following files was downloaded from https://huggingface.co/TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ:\r\
          \n- configuration_RW.py\r\n. Make sure to double-check they do not contain\
          \ any added malicious code. To avoid downloading new versions of the code\
          \ file, you can pin a revision.\r\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256E\r\n\u2502 in <cell line: 11>:11                     \
          \                                                       \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:62\
          \ in from_quantized          \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502   59 \u2502   \u2502   model_basename: Optional[str]\
          \ = None,                                               \u2502\r\n\u2502\
          \   60 \u2502   \u2502   trust_remote_code: bool = False               \
          \                                      \u2502\r\n\u2502   61 \u2502   )\
          \ -> BaseGPTQForCausalLM:                                              \
          \                 \u2502\r\n\u2502 \u2771 62 \u2502   \u2502   model_type\
          \ = check_and_get_model_type(save_dir)                                 \
          \    \u2502\r\n\u2502   63 \u2502   \u2502   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\
          \                         \u2502\r\n\u2502   64 \u2502   \u2502   \u2502\
          \   save_dir=save_dir,                                                 \
          \             \u2502\r\n\u2502   65 \u2502   \u2502   \u2502   device=device,\
          \                                                                  \u2502\
          \r\n\u2502                                                             \
          \                                     \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_utils.py:124\
          \ in                      \u2502\r\n\u2502 check_and_get_model_type    \
          \                                                                     \u2502\
          \r\n\u2502                                                             \
          \                                     \u2502\r\n\u2502   121 def check_and_get_model_type(model_dir):\
          \                                                   \u2502\r\n\u2502   122\
          \ \u2502   config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)\
          \                 \u2502\r\n\u2502   123 \u2502   if config.model_type not\
          \ in SUPPORTED_MODELS:                                          \u2502\r\
          \n\u2502 \u2771 124 \u2502   \u2502   raise TypeError(f\"{config.model_type}\
          \ isn't supported yet.\")                       \u2502\r\n\u2502   125 \u2502\
          \   model_type = config.model_type                                     \
          \                    \u2502\r\n\u2502   126 \u2502   return model_type \
          \                                                                     \u2502\
          \r\n\u2502   127                                                       \
          \                                     \u2502\r\n\u2570\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\
          \r\nTypeError: RefinedWebModel isn't supported yet.\r\n```\r\n\r\nHow Should\
          \ I Get Rid of These?\r\n\r\nCan SomeBody Help?\r\n\r\n@TheBloke  can you\
          \ please chime in. Thanks!"
        updatedAt: '2023-06-02T06:34:31.355Z'
      numEdits: 0
      reactions: []
    id: 64798d77587656b38e8d5f37
    type: comment
  author: thefaheem
  content: "This is what my code is \r\n\r\n```import torch\r\nfrom transformers import\
    \ AutoTokenizer\r\nfrom auto_gptq import AutoGPTQForCausalLM\r\n\r\n# Download\
    \ the model from HF and store it locally, then reference its location here:\r\n\
    #quantized_model_dir = model_path\r\n\r\nfrom transformers import AutoTokenizer\r\
    \ntokenizer = AutoTokenizer.from_pretrained(\"TheBloke/falcon-7b-instruct-GPTQ\"\
    , use_fast=False)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(\"TheBloke/falcon-7b-instruct-GPTQ\"\
    , device=\"cuda:1\", use_triton=False, use_safetensors=True, trust_remote_code=True)\r\
    \n```\r\n\r\nWhen I Run This, i Got These:\r\n\r\n```A new version of the following\
    \ files was downloaded from https://huggingface.co/TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ:\r\
    \n- configuration_RW.py\r\n. Make sure to double-check they do not contain any\
    \ added malicious code. To avoid downloading new versions of the code file, you\
    \ can pin a revision.\r\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
    \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502 in <cell line:\
    \ 11>:11                                                                     \
    \       \u2502\r\n\u2502                                                     \
    \                                             \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:62\
    \ in from_quantized          \u2502\r\n\u2502                                \
    \                                                                  \u2502\r\n\u2502\
    \   59 \u2502   \u2502   model_basename: Optional[str] = None,               \
    \                                \u2502\r\n\u2502   60 \u2502   \u2502   trust_remote_code:\
    \ bool = False                                                     \u2502\r\n\u2502\
    \   61 \u2502   ) -> BaseGPTQForCausalLM:                                    \
    \                           \u2502\r\n\u2502 \u2771 62 \u2502   \u2502   model_type\
    \ = check_and_get_model_type(save_dir)                                     \u2502\
    \r\n\u2502   63 \u2502   \u2502   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\
    \                         \u2502\r\n\u2502   64 \u2502   \u2502   \u2502   save_dir=save_dir,\
    \                                                              \u2502\r\n\u2502\
    \   65 \u2502   \u2502   \u2502   device=device,                             \
    \                                     \u2502\r\n\u2502                       \
    \                                                                           \u2502\
    \r\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_utils.py:124\
    \ in                      \u2502\r\n\u2502 check_and_get_model_type          \
    \                                                               \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502   121 def check_and_get_model_type(model_dir):\
    \                                                   \u2502\r\n\u2502   122 \u2502\
    \   config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)   \
    \              \u2502\r\n\u2502   123 \u2502   if config.model_type not in SUPPORTED_MODELS:\
    \                                          \u2502\r\n\u2502 \u2771 124 \u2502\
    \   \u2502   raise TypeError(f\"{config.model_type} isn't supported yet.\")  \
    \                     \u2502\r\n\u2502   125 \u2502   model_type = config.model_type\
    \                                                         \u2502\r\n\u2502   126\
    \ \u2502   return model_type                                                 \
    \                     \u2502\r\n\u2502   127                                 \
    \                                                           \u2502\r\n\u2570\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u256F\r\nTypeError: RefinedWebModel isn't\
    \ supported yet.\r\n```\r\n\r\nHow Should I Get Rid of These?\r\n\r\nCan SomeBody\
    \ Help?\r\n\r\n@TheBloke  can you please chime in. Thanks!"
  created_at: 2023-06-02 05:34:31+00:00
  edited: false
  hidden: false
  id: 64798d77587656b38e8d5f37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9e7a5ce25531f7a3d2cdc100401883d.svg
      fullname: Rishav Dash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RishuD7
      type: user
    createdAt: '2023-06-02T08:44:59.000Z'
    data:
      edited: false
      editors:
      - RishuD7
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9e7a5ce25531f7a3d2cdc100401883d.svg
          fullname: Rishav Dash
          isHf: false
          isPro: false
          name: RishuD7
          type: user
        html: '<p>Same issue...Please helppppp....</p>

          '
        raw: Same issue...Please helppppp....
        updatedAt: '2023-06-02T08:44:59.001Z'
      numEdits: 0
      reactions: []
    id: 6479ac0ba84498f2af441646
    type: comment
  author: RishuD7
  content: Same issue...Please helppppp....
  created_at: 2023-06-02 07:44:59+00:00
  edited: false
  hidden: false
  id: 6479ac0ba84498f2af441646
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-02T08:52:15.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You must not be using the latest version of AutoGPTQ.  Please ensure
          you''re running version 0.2.0, released yesterday.</p>

          <p><code>pip install auto-gptq --upgrade --upgrade-strategy only-if-needed</code></p>

          '
        raw: 'You must not be using the latest version of AutoGPTQ.  Please ensure
          you''re running version 0.2.0, released yesterday.


          `pip install auto-gptq --upgrade --upgrade-strategy only-if-needed`'
        updatedAt: '2023-06-02T08:52:15.810Z'
      numEdits: 0
      reactions: []
    id: 6479adbff518a860fbc2e827
    type: comment
  author: TheBloke
  content: 'You must not be using the latest version of AutoGPTQ.  Please ensure you''re
    running version 0.2.0, released yesterday.


    `pip install auto-gptq --upgrade --upgrade-strategy only-if-needed`'
  created_at: 2023-06-02 07:52:15+00:00
  edited: false
  hidden: false
  id: 6479adbff518a860fbc2e827
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:12:13.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<blockquote>

          <p>You must not be using the latest version of AutoGPTQ.  Please ensure
          you''re running version 0.2.0, released yesterday.</p>

          <p><code>pip install auto-gptq --upgrade --upgrade-strategy only-if-needed</code></p>

          </blockquote>

          <p>Not Worked. Still Getting The Same Issue</p>

          '
        raw: "> You must not be using the latest version of AutoGPTQ.  Please ensure\
          \ you're running version 0.2.0, released yesterday.\n> \n> `pip install\
          \ auto-gptq --upgrade --upgrade-strategy only-if-needed`\n\nNot Worked.\
          \ Still Getting The Same Issue"
        updatedAt: '2023-06-02T09:12:13.448Z'
      numEdits: 0
      reactions: []
    id: 6479b26d1a2aefceecd323f7
    type: comment
  author: thefaheem
  content: "> You must not be using the latest version of AutoGPTQ.  Please ensure\
    \ you're running version 0.2.0, released yesterday.\n> \n> `pip install auto-gptq\
    \ --upgrade --upgrade-strategy only-if-needed`\n\nNot Worked. Still Getting The\
    \ Same Issue"
  created_at: 2023-06-02 08:12:13+00:00
  edited: false
  hidden: false
  id: 6479b26d1a2aefceecd323f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-02T09:13:06.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK I am checking it</p>

          '
        raw: OK I am checking it
        updatedAt: '2023-06-02T09:13:06.641Z'
      numEdits: 0
      reactions: []
    id: 6479b2a2a84498f2af44bc59
    type: comment
  author: TheBloke
  content: OK I am checking it
  created_at: 2023-06-02 08:13:06+00:00
  edited: false
  hidden: false
  id: 6479b2a2a84498f2af44bc59
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:18:44.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<blockquote>

          <p>You must not be using the latest version of AutoGPTQ.  Please ensure
          you''re running version 0.2.0, released yesterday.</p>

          <p><code>pip install auto-gptq --upgrade --upgrade-strategy only-if-needed</code></p>

          </blockquote>

          <p>I Upgraded the Package using that command.</p>

          <p>After that i Checked the version using <code>pip show auto-gptq</code></p>

          <p>Output: ```Name: auto-gptq<br>Version: 0.1.0<br>Summary:<br>Home-page:<br>Author:<br>Author-email:<br>License:<br>Location:
          /usr/local/lib/python3.10/dist-packages<br>Requires: accelerate, datasets,
          numpy, rouge, safetensors, torch, transformers<br>Required-by:</p>

          <p>```</p>

          <p>The Version is still 0.1.0.</p>

          <p>Someone Please Help me Make This!</p>

          '
        raw: "> You must not be using the latest version of AutoGPTQ.  Please ensure\
          \ you're running version 0.2.0, released yesterday.\n> \n> `pip install\
          \ auto-gptq --upgrade --upgrade-strategy only-if-needed`\n\nI Upgraded the\
          \ Package using that command.\n\nAfter that i Checked the version using\
          \ ```pip show auto-gptq```\n\nOutput: ```Name: auto-gptq\nVersion: 0.1.0\n\
          Summary: \nHome-page: \nAuthor: \nAuthor-email: \nLicense: \nLocation: /usr/local/lib/python3.10/dist-packages\n\
          Requires: accelerate, datasets, numpy, rouge, safetensors, torch, transformers\n\
          Required-by:\n```\n\nThe Version is still 0.1.0.\n\nSomeone Please Help\
          \ me Make This!"
        updatedAt: '2023-06-02T09:18:44.962Z'
      numEdits: 0
      reactions: []
    id: 6479b3f41a2aefceecd34782
    type: comment
  author: thefaheem
  content: "> You must not be using the latest version of AutoGPTQ.  Please ensure\
    \ you're running version 0.2.0, released yesterday.\n> \n> `pip install auto-gptq\
    \ --upgrade --upgrade-strategy only-if-needed`\n\nI Upgraded the Package using\
    \ that command.\n\nAfter that i Checked the version using ```pip show auto-gptq```\n\
    \nOutput: ```Name: auto-gptq\nVersion: 0.1.0\nSummary: \nHome-page: \nAuthor:\
    \ \nAuthor-email: \nLicense: \nLocation: /usr/local/lib/python3.10/dist-packages\n\
    Requires: accelerate, datasets, numpy, rouge, safetensors, torch, transformers\n\
    Required-by:\n```\n\nThe Version is still 0.1.0.\n\nSomeone Please Help me Make\
    \ This!"
  created_at: 2023-06-02 08:18:44+00:00
  edited: false
  hidden: false
  id: 6479b3f41a2aefceecd34782
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:20:01.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<p>i used Eric''s model too... its too Showing the Same RefinedWeb
          Model Not Supported Error</p>

          '
        raw: i used Eric's model too... its too Showing the Same RefinedWeb Model
          Not Supported Error
        updatedAt: '2023-06-02T09:20:01.811Z'
      numEdits: 0
      reactions: []
    id: 6479b441f518a860fbc3850d
    type: comment
  author: thefaheem
  content: i used Eric's model too... its too Showing the Same RefinedWeb Model Not
    Supported Error
  created_at: 2023-06-02 08:20:01+00:00
  edited: false
  hidden: false
  id: 6479b441f518a860fbc3850d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-02T09:20:23.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It works for me. </p>

          <p>Please try:</p>

          <pre><code>pip uninstall auto-gptq

          pip install -v auto-gptq==0.2.0

          </code></pre>

          <p>Copy all the output, then test again. And if it doesn''t work, please
          paste the output here.  When pasting, type '''''', hit enter, paste your
          log, hit enter, then type '''''' again.  This will enclose the log in a
          code block so it''s easier to read.</p>

          '
        raw: "It works for me. \n\nPlease try:\n```\npip uninstall auto-gptq\npip\
          \ install -v auto-gptq==0.2.0\n```\n\nCopy all the output, then test again.\
          \ And if it doesn't work, please paste the output here.  When pasting, type\
          \ \\'\\'\\', hit enter, paste your log, hit enter, then type \\'\\'\\' again.\
          \  This will enclose the log in a code block so it's easier to read."
        updatedAt: '2023-06-02T09:24:23.354Z'
      numEdits: 1
      reactions: []
    id: 6479b4573a17d5e00aca796a
    type: comment
  author: TheBloke
  content: "It works for me. \n\nPlease try:\n```\npip uninstall auto-gptq\npip install\
    \ -v auto-gptq==0.2.0\n```\n\nCopy all the output, then test again. And if it\
    \ doesn't work, please paste the output here.  When pasting, type \\'\\'\\', hit\
    \ enter, paste your log, hit enter, then type \\'\\'\\' again.  This will enclose\
    \ the log in a code block so it's easier to read."
  created_at: 2023-06-02 08:20:23+00:00
  edited: true
  hidden: false
  id: 6479b4573a17d5e00aca796a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-02T09:20:46.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>If you have version 0.1.0 then that''s definitely the problem. It
          should be version 0.2.0</p>

          '
        raw: If you have version 0.1.0 then that's definitely the problem. It should
          be version 0.2.0
        updatedAt: '2023-06-02T09:20:46.555Z'
      numEdits: 0
      reactions: []
    id: 6479b46e1a2aefceecd355b0
    type: comment
  author: TheBloke
  content: If you have version 0.1.0 then that's definitely the problem. It should
    be version 0.2.0
  created_at: 2023-06-02 08:20:46+00:00
  edited: false
  hidden: false
  id: 6479b46e1a2aefceecd355b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:23:33.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: "<p>Lemme Try. btw, Thanks <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ For Spending Your Precious Time To Solve This... mow u</p>\n"
        raw: Lemme Try. btw, Thanks @TheBloke For Spending Your Precious Time To Solve
          This... mow u
        updatedAt: '2023-06-02T09:23:33.455Z'
      numEdits: 0
      reactions: []
    id: 6479b5151a2aefceecd3642f
    type: comment
  author: thefaheem
  content: Lemme Try. btw, Thanks @TheBloke For Spending Your Precious Time To Solve
    This... mow u
  created_at: 2023-06-02 08:23:33+00:00
  edited: false
  hidden: false
  id: 6479b5151a2aefceecd3642f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:32:44.000Z'
    data:
      edited: true
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: "<blockquote>\n<p>It works for me. </p>\n<p>Please try:</p>\n<pre><code>pip\
          \ uninstall auto-gptq\npip install -v auto-gptq==0.2.0\n</code></pre>\n\
          <p>Copy all the output, then test again. And if it doesn't work, please\
          \ paste the output here.  When pasting, type ''', hit enter, paste your\
          \ log, hit enter, then type ''' again.  This will enclose the log in a code\
          \ block so it's easier to read.</p>\n</blockquote>\n<p>After Running this\
          \ It Says <code>No matching distribution found for auto-gptq==0.2.0</code></p>\n\
          <p>Log:</p>\n<pre><code>Found existing installation: auto-gptq 0.1.0\nUninstalling\
          \ auto-gptq-0.1.0:\n  Successfully uninstalled auto-gptq-0.1.0\nUsing pip\
          \ 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n\
          Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n\
          Collecting auto-gptq==0.2.0\n  Using cached auto_gptq-0.2.0.tar.gz (47 kB)\n\
          \  Running command python setup.py egg_info\n  running egg_info\n  creating\
          \ /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info\n  writing /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/PKG-INFO\n\
          \  writing dependency_links to /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/dependency_links.txt\n\
          \  writing requirements to /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/requires.txt\n\
          \  writing top-level names to /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/top_level.txt\n\
          \  writing manifest file '/tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/SOURCES.txt'\n\
          \  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:476:\
          \ UserWarning: Attempted to use ninja as the BuildExtension backend but\
          \ we could not find ninja.. Falling back to using the slow distutils backend.\n\
          \    warnings.warn(msg.format('we could not find ninja.'))\n  reading manifest\
          \ file '/tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/SOURCES.txt'\n\
          \  adding license file 'LICENSE'\n  writing manifest file '/tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/SOURCES.txt'\n\
          \  Preparing metadata (setup.py) ... done\nDiscarding https://files.pythonhosted.org/packages/b1/f9/97153ae5cf926f96fd37e61424a1bb58e0c9991cc220b2e17390fb8bde97/auto_gptq-0.2.0.tar.gz\
          \ (from https://pypi.org/simple/auto-gptq/) (requires-python:&gt;=3.8.0):\
          \ Requested auto-gptq==0.2.0 from https://files.pythonhosted.org/packages/b1/f9/97153ae5cf926f96fd37e61424a1bb58e0c9991cc220b2e17390fb8bde97/auto_gptq-0.2.0.tar.gz\
          \ has inconsistent version: expected '0.2.0', but metadata has '0.2.0+cu1180'\n\
          ERROR: Could not find a version that satisfies the requirement auto-gptq==0.2.0\
          \ (from versions: 0.0.4, 0.0.5, 0.1.0, 0.2.0)\nERROR: No matching distribution\
          \ found for auto-gptq==0.2.0\n</code></pre>\n"
        raw: "> It works for me. \n> \n> Please try:\n> ```\n> pip uninstall auto-gptq\n\
          > pip install -v auto-gptq==0.2.0\n> ```\n> \n> Copy all the output, then\
          \ test again. And if it doesn't work, please paste the output here.  When\
          \ pasting, type \\'\\'\\', hit enter, paste your log, hit enter, then type\
          \ \\'\\'\\' again.  This will enclose the log in a code block so it's easier\
          \ to read.\n\nAfter Running this It Says ```No matching distribution found\
          \ for auto-gptq==0.2.0```\n\nLog:\n```\nFound existing installation: auto-gptq\
          \ 0.1.0\nUninstalling auto-gptq-0.1.0:\n  Successfully uninstalled auto-gptq-0.1.0\n\
          Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python\
          \ 3.10)\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n\
          Collecting auto-gptq==0.2.0\n  Using cached auto_gptq-0.2.0.tar.gz (47 kB)\n\
          \  Running command python setup.py egg_info\n  running egg_info\n  creating\
          \ /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info\n  writing /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/PKG-INFO\n\
          \  writing dependency_links to /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/dependency_links.txt\n\
          \  writing requirements to /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/requires.txt\n\
          \  writing top-level names to /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/top_level.txt\n\
          \  writing manifest file '/tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/SOURCES.txt'\n\
          \  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:476:\
          \ UserWarning: Attempted to use ninja as the BuildExtension backend but\
          \ we could not find ninja.. Falling back to using the slow distutils backend.\n\
          \    warnings.warn(msg.format('we could not find ninja.'))\n  reading manifest\
          \ file '/tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/SOURCES.txt'\n\
          \  adding license file 'LICENSE'\n  writing manifest file '/tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/SOURCES.txt'\n\
          \  Preparing metadata (setup.py) ... done\nDiscarding https://files.pythonhosted.org/packages/b1/f9/97153ae5cf926f96fd37e61424a1bb58e0c9991cc220b2e17390fb8bde97/auto_gptq-0.2.0.tar.gz\
          \ (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0): Requested\
          \ auto-gptq==0.2.0 from https://files.pythonhosted.org/packages/b1/f9/97153ae5cf926f96fd37e61424a1bb58e0c9991cc220b2e17390fb8bde97/auto_gptq-0.2.0.tar.gz\
          \ has inconsistent version: expected '0.2.0', but metadata has '0.2.0+cu1180'\n\
          ERROR: Could not find a version that satisfies the requirement auto-gptq==0.2.0\
          \ (from versions: 0.0.4, 0.0.5, 0.1.0, 0.2.0)\nERROR: No matching distribution\
          \ found for auto-gptq==0.2.0\n```"
        updatedAt: '2023-06-02T09:33:07.536Z'
      numEdits: 1
      reactions: []
    id: 6479b73ca84498f2af452e66
    type: comment
  author: thefaheem
  content: "> It works for me. \n> \n> Please try:\n> ```\n> pip uninstall auto-gptq\n\
    > pip install -v auto-gptq==0.2.0\n> ```\n> \n> Copy all the output, then test\
    \ again. And if it doesn't work, please paste the output here.  When pasting,\
    \ type \\'\\'\\', hit enter, paste your log, hit enter, then type \\'\\'\\' again.\
    \  This will enclose the log in a code block so it's easier to read.\n\nAfter\
    \ Running this It Says ```No matching distribution found for auto-gptq==0.2.0```\n\
    \nLog:\n```\nFound existing installation: auto-gptq 0.1.0\nUninstalling auto-gptq-0.1.0:\n\
    \  Successfully uninstalled auto-gptq-0.1.0\nUsing pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip\
    \ (python 3.10)\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n\
    Collecting auto-gptq==0.2.0\n  Using cached auto_gptq-0.2.0.tar.gz (47 kB)\n \
    \ Running command python setup.py egg_info\n  running egg_info\n  creating /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info\n\
    \  writing /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/PKG-INFO\n  writing\
    \ dependency_links to /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/dependency_links.txt\n\
    \  writing requirements to /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/requires.txt\n\
    \  writing top-level names to /tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/top_level.txt\n\
    \  writing manifest file '/tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/SOURCES.txt'\n\
    \  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:476: UserWarning:\
    \ Attempted to use ninja as the BuildExtension backend but we could not find ninja..\
    \ Falling back to using the slow distutils backend.\n    warnings.warn(msg.format('we\
    \ could not find ninja.'))\n  reading manifest file '/tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/SOURCES.txt'\n\
    \  adding license file 'LICENSE'\n  writing manifest file '/tmp/pip-pip-egg-info-d0sklosj/auto_gptq.egg-info/SOURCES.txt'\n\
    \  Preparing metadata (setup.py) ... done\nDiscarding https://files.pythonhosted.org/packages/b1/f9/97153ae5cf926f96fd37e61424a1bb58e0c9991cc220b2e17390fb8bde97/auto_gptq-0.2.0.tar.gz\
    \ (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0): Requested\
    \ auto-gptq==0.2.0 from https://files.pythonhosted.org/packages/b1/f9/97153ae5cf926f96fd37e61424a1bb58e0c9991cc220b2e17390fb8bde97/auto_gptq-0.2.0.tar.gz\
    \ has inconsistent version: expected '0.2.0', but metadata has '0.2.0+cu1180'\n\
    ERROR: Could not find a version that satisfies the requirement auto-gptq==0.2.0\
    \ (from versions: 0.0.4, 0.0.5, 0.1.0, 0.2.0)\nERROR: No matching distribution\
    \ found for auto-gptq==0.2.0\n```"
  created_at: 2023-06-02 08:32:44+00:00
  edited: true
  hidden: false
  id: 6479b73ca84498f2af452e66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-02T09:40:01.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>What OS are you using?</p>

          '
        raw: What OS are you using?
        updatedAt: '2023-06-02T09:40:01.140Z'
      numEdits: 0
      reactions: []
    id: 6479b8f11a2aefceecd3c9a8
    type: comment
  author: TheBloke
  content: What OS are you using?
  created_at: 2023-06-02 08:40:01+00:00
  edited: false
  hidden: false
  id: 6479b8f11a2aefceecd3c9a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:40:19.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<blockquote>

          <p>What OS are you using?</p>

          </blockquote>

          <p>colab sir!</p>

          '
        raw: '> What OS are you using?


          colab sir!'
        updatedAt: '2023-06-02T09:40:19.859Z'
      numEdits: 0
      reactions: []
    id: 6479b903f518a860fbc400ac
    type: comment
  author: thefaheem
  content: '> What OS are you using?


    colab sir!'
  created_at: 2023-06-02 08:40:19+00:00
  edited: false
  hidden: false
  id: 6479b903f518a860fbc400ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:41:14.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<p><code>Could not find a version that satisfies the requirement auto-gptq==0.2.0
          (from versions: 0.0.4, 0.0.5, 0.1.0, 0.2.0)</code><br>: |</p>

          '
        raw: "```Could not find a version that satisfies the requirement auto-gptq==0.2.0\
          \ (from versions: 0.0.4, 0.0.5, 0.1.0, 0.2.0)``` \n: |"
        updatedAt: '2023-06-02T09:41:14.503Z'
      numEdits: 0
      reactions: []
    id: 6479b93a3a17d5e00acaf4dc
    type: comment
  author: thefaheem
  content: "```Could not find a version that satisfies the requirement auto-gptq==0.2.0\
    \ (from versions: 0.0.4, 0.0.5, 0.1.0, 0.2.0)``` \n: |"
  created_at: 2023-06-02 08:41:14+00:00
  edited: false
  hidden: false
  id: 6479b93a3a17d5e00acaf4dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-02T09:41:45.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>All right I think this is a problem in the AutoGPTQ repo that will
          need to be fixed there.  Please build it from source:</p>

          <pre><code>git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install .

          </code></pre>

          '
        raw: 'All right I think this is a problem in the AutoGPTQ repo that will need
          to be fixed there.  Please build it from source:


          ```

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install .

          ```'
        updatedAt: '2023-06-02T09:41:45.986Z'
      numEdits: 0
      reactions: []
    id: 6479b9593a17d5e00acaf7ed
    type: comment
  author: TheBloke
  content: 'All right I think this is a problem in the AutoGPTQ repo that will need
    to be fixed there.  Please build it from source:


    ```

    git clone https://github.com/PanQiWei/AutoGPTQ

    cd AutoGPTQ

    pip install .

    ```'
  created_at: 2023-06-02 08:41:45+00:00
  edited: false
  hidden: false
  id: 6479b9593a17d5e00acaf7ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:41:53.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<p>I think i Should Compile From the Source?</p>

          '
        raw: I think i Should Compile From the Source?
        updatedAt: '2023-06-02T09:41:53.700Z'
      numEdits: 0
      reactions: []
    id: 6479b961f518a860fbc409f1
    type: comment
  author: thefaheem
  content: I think i Should Compile From the Source?
  created_at: 2023-06-02 08:41:53+00:00
  edited: false
  hidden: false
  id: 6479b961f518a860fbc409f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:42:48.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<blockquote>

          <p>All right I think this is a problem in the AutoGPTQ repo that will need
          to be fixed there.  Please build it from source:</p>

          <pre><code>git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install .

          </code></pre>

          </blockquote>

          <p>This Need Cuda Developer Pack, Right?</p>

          '
        raw: "> All right I think this is a problem in the AutoGPTQ repo that will\
          \ need to be fixed there.  Please build it from source:\n> \n> ```\n> git\
          \ clone https://github.com/PanQiWei/AutoGPTQ\n> cd AutoGPTQ\n> pip install\
          \ .\n> ```\n\nThis Need Cuda Developer Pack, Right?"
        updatedAt: '2023-06-02T09:42:48.114Z'
      numEdits: 0
      reactions: []
    id: 6479b9983a17d5e00acaff55
    type: comment
  author: thefaheem
  content: "> All right I think this is a problem in the AutoGPTQ repo that will need\
    \ to be fixed there.  Please build it from source:\n> \n> ```\n> git clone https://github.com/PanQiWei/AutoGPTQ\n\
    > cd AutoGPTQ\n> pip install .\n> ```\n\nThis Need Cuda Developer Pack, Right?"
  created_at: 2023-06-02 08:42:48+00:00
  edited: false
  hidden: false
  id: 6479b9983a17d5e00acaff55
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:47:48.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<p>The cuda version on colab gpu is 12.0. AutoGPTQ has Precompiled
          Lib for only cuda11.7 and 11.8. For 12.0 it should be compiled from the
          source. This Might be a Problem</p>

          '
        raw: The cuda version on colab gpu is 12.0. AutoGPTQ has Precompiled Lib for
          only cuda11.7 and 11.8. For 12.0 it should be compiled from the source.
          This Might be a Problem
        updatedAt: '2023-06-02T09:47:48.427Z'
      numEdits: 0
      reactions: []
    id: 6479bac43a17d5e00acb1a73
    type: comment
  author: thefaheem
  content: The cuda version on colab gpu is 12.0. AutoGPTQ has Precompiled Lib for
    only cuda11.7 and 11.8. For 12.0 it should be compiled from the source. This Might
    be a Problem
  created_at: 2023-06-02 08:47:48+00:00
  edited: false
  hidden: false
  id: 6479bac43a17d5e00acb1a73
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:50:23.000Z'
    data:
      edited: true
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: "<p>Ok. Now I Have auto-gptq version 0.2.0 for cuda+118.</p>\n<pre><code\
          \ class=\"language-Name:\">Version: 0.2.0+cu1180\nSummary: An easy-to-use\
          \ LLMs quantization package with user-friendly apis, based on GPTQ algorithm.\n\
          Home-page: https://github.com/PanQiWei/AutoGPTQ\nAuthor: PanQiWei\nAuthor-email:\
          \ \nLicense: \nLocation: /usr/local/lib/python3.10/dist-packages\nRequires:\
          \ accelerate, datasets, numpy, rouge, safetensors, torch, transformers\n\
          Required-by: \n</code></pre>\n<p>But colab cuda version is 12.0. I don't\
          \ this will work or not.</p>\n<p>Let Me Try and come here.</p>\n"
        raw: "Ok. Now I Have auto-gptq version 0.2.0 for cuda+118.\n```Name: auto-gptq\n\
          Version: 0.2.0+cu1180\nSummary: An easy-to-use LLMs quantization package\
          \ with user-friendly apis, based on GPTQ algorithm.\nHome-page: https://github.com/PanQiWei/AutoGPTQ\n\
          Author: PanQiWei\nAuthor-email: \nLicense: \nLocation: /usr/local/lib/python3.10/dist-packages\n\
          Requires: accelerate, datasets, numpy, rouge, safetensors, torch, transformers\n\
          Required-by: \n```\n\nBut colab cuda version is 12.0. I don't this will\
          \ work or not.\n\nLet Me Try and come here."
        updatedAt: '2023-06-02T09:50:42.466Z'
      numEdits: 1
      reactions: []
    id: 6479bb5ff518a860fbc437ed
    type: comment
  author: thefaheem
  content: "Ok. Now I Have auto-gptq version 0.2.0 for cuda+118.\n```Name: auto-gptq\n\
    Version: 0.2.0+cu1180\nSummary: An easy-to-use LLMs quantization package with\
    \ user-friendly apis, based on GPTQ algorithm.\nHome-page: https://github.com/PanQiWei/AutoGPTQ\n\
    Author: PanQiWei\nAuthor-email: \nLicense: \nLocation: /usr/local/lib/python3.10/dist-packages\n\
    Requires: accelerate, datasets, numpy, rouge, safetensors, torch, transformers\n\
    Required-by: \n```\n\nBut colab cuda version is 12.0. I don't this will work or\
    \ not.\n\nLet Me Try and come here."
  created_at: 2023-06-02 08:50:23+00:00
  edited: true
  hidden: false
  id: 6479bb5ff518a860fbc437ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-02T09:52:02.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You don''t have CUDA toolkit 12.0 installed, you have 11.8.  And
          compiling with source will work fine</p>

          <p>PS. I have raised this problem as an Issue on the AutoGPTQ repo: <a rel="nofollow"
          href="https://github.com/PanQiWei/AutoGPTQ/issues/124">https://github.com/PanQiWei/AutoGPTQ/issues/124</a></p>

          <p>Please track it for future</p>

          '
        raw: 'You don''t have CUDA toolkit 12.0 installed, you have 11.8.  And compiling
          with source will work fine


          PS. I have raised this problem as an Issue on the AutoGPTQ repo: https://github.com/PanQiWei/AutoGPTQ/issues/124


          Please track it for future'
        updatedAt: '2023-06-02T09:52:02.177Z'
      numEdits: 0
      reactions: []
    id: 6479bbc21a2aefceecd40de3
    type: comment
  author: TheBloke
  content: 'You don''t have CUDA toolkit 12.0 installed, you have 11.8.  And compiling
    with source will work fine


    PS. I have raised this problem as an Issue on the AutoGPTQ repo: https://github.com/PanQiWei/AutoGPTQ/issues/124


    Please track it for future'
  created_at: 2023-06-02 08:52:02+00:00
  edited: false
  hidden: false
  id: 6479bbc21a2aefceecd40de3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T11:02:22.000Z'
    data:
      status: closed
    id: 6479cc3ea84498f2af472c38
    type: status-change
  author: thefaheem
  created_at: 2023-06-02 10:02:22+00:00
  id: 6479cc3ea84498f2af472c38
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e0e3c2d95e0221e4b68a9d574a61958.svg
      fullname: 'Rohit '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: roh006
      type: user
    createdAt: '2023-11-09T08:56:44.000Z'
    data:
      edited: false
      editors:
      - roh006
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6007064580917358
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e0e3c2d95e0221e4b68a9d574a61958.svg
          fullname: 'Rohit '
          isHf: false
          isPro: false
          name: roh006
          type: user
        html: '<p>I am facing the below error while using the ToRA. any support or
          guidance would be great<br>ImportError: Found an incompatible version of
          auto-gptq. Found version 0.4.2+cu1180, but only version above 0.4.99 are
          supported</p>

          '
        raw: 'I am facing the below error while using the ToRA. any support or guidance
          would be great

          ImportError: Found an incompatible version of auto-gptq. Found version 0.4.2+cu1180,
          but only version above 0.4.99 are supported'
        updatedAt: '2023-11-09T08:56:44.036Z'
      numEdits: 0
      reactions: []
    id: 654c9ecc48b47412027e983a
    type: comment
  author: roh006
  content: 'I am facing the below error while using the ToRA. any support or guidance
    would be great

    ImportError: Found an incompatible version of auto-gptq. Found version 0.4.2+cu1180,
    but only version above 0.4.99 are supported'
  created_at: 2023-11-09 08:56:44+00:00
  edited: false
  hidden: false
  id: 654c9ecc48b47412027e983a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6996f108358686a2990b7725560919a2.svg
      fullname: Dudu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DrmemoryFish
      type: user
    createdAt: '2023-11-12T03:20:32.000Z'
    data:
      edited: true
      editors:
      - DrmemoryFish
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5900511741638184
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6996f108358686a2990b7725560919a2.svg
          fullname: Dudu
          isHf: false
          isPro: false
          name: DrmemoryFish
          type: user
        html: '<blockquote>

          <p>I am facing the below error while using the ToRA. any support or guidance
          would be great<br>ImportError: Found an incompatible version of auto-gptq.
          Found version 0.4.2+cu1180, but only version above 0.4.99 are supported</p>

          </blockquote>

          <p>same</p>

          <pre><code>ImportError: Found an incompatible version of auto-gptq. Found
          version 0.4.2+cu121, but only version above 0.4.99 are supported

          </code></pre>

          '
        raw: '> I am facing the below error while using the ToRA. any support or guidance
          would be great

          > ImportError: Found an incompatible version of auto-gptq. Found version
          0.4.2+cu1180, but only version above 0.4.99 are supported


          same


          ```

          ImportError: Found an incompatible version of auto-gptq. Found version 0.4.2+cu121,
          but only version above 0.4.99 are supported

          ```'
        updatedAt: '2023-11-12T03:21:04.631Z'
      numEdits: 1
      reactions: []
    id: 6550448029c67086150b6700
    type: comment
  author: DrmemoryFish
  content: '> I am facing the below error while using the ToRA. any support or guidance
    would be great

    > ImportError: Found an incompatible version of auto-gptq. Found version 0.4.2+cu1180,
    but only version above 0.4.99 are supported


    same


    ```

    ImportError: Found an incompatible version of auto-gptq. Found version 0.4.2+cu121,
    but only version above 0.4.99 are supported

    ```'
  created_at: 2023-11-12 03:20:32+00:00
  edited: true
  hidden: false
  id: 6550448029c67086150b6700
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: TheBloke/Falcon-7B-Instruct-GPTQ
repo_type: model
status: closed
target_branch: null
title: 'TypeError: RefinedWebModel isn''t supported yet.'
