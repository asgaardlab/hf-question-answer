!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aldennX
conflicting_files: null
created_at: 2023-08-05 11:50:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
      fullname: aldenn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aldennX
      type: user
    createdAt: '2023-08-05T12:50:47.000Z'
    data:
      edited: false
      editors:
      - aldennX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6518882513046265
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
          fullname: aldenn
          isHf: false
          isPro: false
          name: aldennX
          type: user
        html: '<p>bin C:\Users\Administrator\Desktop\oobabooga_windows\installer_files\env\lib\site-packages\bitsandbytes\libbitsandbytes_cpu.so<br>C:\Users\Administrator\Desktop\oobabooga_windows\installer_files\env\lib\site-packages\bitsandbytes\cextension.py:34:
          UserWarning: The installed version of bitsandbytes was compiled without
          GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization
          are unavailable.<br>  warn("The installed version of bitsandbytes was compiled
          without GPU support. "<br>function ''cadam32bit_grad_fp32'' not found<br>2023-08-05
          20:46:42 INFO:Loading the extension "gallery"...<br>Running on local URL:  <a
          rel="nofollow" href="http://127.0.0.1:7860">http://127.0.0.1:7860</a></p>

          <p>To create a public link, set <code>share=True</code> in <code>launch()</code>.</p>

          '
        raw: "bin C:\\Users\\Administrator\\Desktop\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so\r\nC:\\Users\\\
          Administrator\\Desktop\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes\
          \ was compiled without GPU support. 8-bit optimizers, 8-bit multiplication,\
          \ and GPU quantization are unavailable.\r\n  warn(\"The installed version\
          \ of bitsandbytes was compiled without GPU support. \"\r\nfunction 'cadam32bit_grad_fp32'\
          \ not found\r\n2023-08-05 20:46:42 INFO:Loading the extension \"gallery\"\
          ...\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public\
          \ link, set `share=True` in `launch()`.\r\n\r\n\r\n\r\n"
        updatedAt: '2023-08-05T12:50:47.949Z'
      numEdits: 0
      reactions: []
    id: 64ce45a701931c6016e89007
    type: comment
  author: aldennX
  content: "bin C:\\Users\\Administrator\\Desktop\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so\r\nC:\\Users\\Administrator\\\
    Desktop\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\bitsandbytes\\\
    cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled\
    \ without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization\
    \ are unavailable.\r\n  warn(\"The installed version of bitsandbytes was compiled\
    \ without GPU support. \"\r\nfunction 'cadam32bit_grad_fp32' not found\r\n2023-08-05\
    \ 20:46:42 INFO:Loading the extension \"gallery\"...\r\nRunning on local URL:\
    \  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\
    \n\r\n\r\n\r\n"
  created_at: 2023-08-05 11:50:47+00:00
  edited: false
  hidden: false
  id: 64ce45a701931c6016e89007
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-05T20:54:56.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.956311821937561
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>There''s no errors there, that''s all fine.  You can ignore the
          bitsandbytes messages as you''re not using it.</p>

          '
        raw: There's no errors there, that's all fine.  You can ignore the bitsandbytes
          messages as you're not using it.
        updatedAt: '2023-08-05T20:54:56.300Z'
      numEdits: 0
      reactions: []
    id: 64ceb7202f1f9578a0024213
    type: comment
  author: TheBloke
  content: There's no errors there, that's all fine.  You can ignore the bitsandbytes
    messages as you're not using it.
  created_at: 2023-08-05 19:54:56+00:00
  edited: false
  hidden: false
  id: 64ceb7202f1f9578a0024213
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
      fullname: aldenn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aldennX
      type: user
    createdAt: '2023-08-06T05:07:20.000Z'
    data:
      edited: false
      editors:
      - aldennX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3042549788951874
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
          fullname: aldenn
          isHf: false
          isPro: false
          name: aldennX
          type: user
        html: '<p>Now there is a new problem, which I have solved.</p>

          <p>windows10-CPU</p>

          <p>Traceback (most recent call last):<br>  File "C:\Users\Administrator\text-generation-webui\server.py",
          line 68, in load_model_wrapper<br>    shared.model, shared.tokenizer = load_model(shared.model_name,
          loader)<br>  File "C:\Users\Administrator\text-generation-webui\modules\models.py",
          line 78, in load_model<br>    output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "C:\Users\Administrator\text-generation-webui\modules\models.py", line 232,
          in llamacpp_loader<br>    from modules.llamacpp_model import LlamaCppModel<br>  File
          "C:\Users\Administrator\text-generation-webui\modules\llamacpp_model.py",
          line 11, in <br>    import llama_cpp<br>  File "C:\Users\Administrator.conda\envs\textgen\lib\site-packages\llama_cpp_<em>init</em>_.py",
          line 1, in <br>    from .llama_cpp import *<br>  File "C:\Users\Administrator.conda\envs\textgen\lib\site-packages\llama_cpp\llama_cpp.py",
          line 1292, in <br>    llama_backend_init(c_bool(False))<br>  File "C:\Users\Administrator.conda\envs\textgen\lib\site-packages\llama_cpp\llama_cpp.py",
          line 403, in llama_backend_init<br>    return _lib.llama_backend_init(numa)<br>OSError:
          [WinError -1073741795] Windows Error 0xc000001d</p>

          '
        raw: "Now there is a new problem, which I have solved.\n\nwindows10-CPU\n\n\
          \n\n\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Administrator\\\
          text-generation-webui\\server.py\", line 68, in load_model_wrapper\n   \
          \ shared.model, shared.tokenizer = load_model(shared.model_name, loader)\n\
          \  File \"C:\\Users\\Administrator\\text-generation-webui\\modules\\models.py\"\
          , line 78, in load_model\n    output = load_func_map[loader](model_name)\n\
          \  File \"C:\\Users\\Administrator\\text-generation-webui\\modules\\models.py\"\
          , line 232, in llamacpp_loader\n    from modules.llamacpp_model import LlamaCppModel\n\
          \  File \"C:\\Users\\Administrator\\text-generation-webui\\modules\\llamacpp_model.py\"\
          , line 11, in <module>\n    import llama_cpp\n  File \"C:\\Users\\Administrator\\\
          .conda\\envs\\textgen\\lib\\site-packages\\llama_cpp\\__init__.py\", line\
          \ 1, in <module>\n    from .llama_cpp import *\n  File \"C:\\Users\\Administrator\\\
          .conda\\envs\\textgen\\lib\\site-packages\\llama_cpp\\llama_cpp.py\", line\
          \ 1292, in <module>\n    llama_backend_init(c_bool(False))\n  File \"C:\\\
          Users\\Administrator\\.conda\\envs\\textgen\\lib\\site-packages\\llama_cpp\\\
          llama_cpp.py\", line 403, in llama_backend_init\n    return _lib.llama_backend_init(numa)\n\
          OSError: [WinError -1073741795] Windows Error 0xc000001d"
        updatedAt: '2023-08-06T05:07:20.792Z'
      numEdits: 0
      reactions: []
    id: 64cf2a88995a0b5d5952859f
    type: comment
  author: aldennX
  content: "Now there is a new problem, which I have solved.\n\nwindows10-CPU\n\n\n\
    \n\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Administrator\\\
    text-generation-webui\\server.py\", line 68, in load_model_wrapper\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name, loader)\n  File \"C:\\Users\\\
    Administrator\\text-generation-webui\\modules\\models.py\", line 78, in load_model\n\
    \    output = load_func_map[loader](model_name)\n  File \"C:\\Users\\Administrator\\\
    text-generation-webui\\modules\\models.py\", line 232, in llamacpp_loader\n  \
    \  from modules.llamacpp_model import LlamaCppModel\n  File \"C:\\Users\\Administrator\\\
    text-generation-webui\\modules\\llamacpp_model.py\", line 11, in <module>\n  \
    \  import llama_cpp\n  File \"C:\\Users\\Administrator\\.conda\\envs\\textgen\\\
    lib\\site-packages\\llama_cpp\\__init__.py\", line 1, in <module>\n    from .llama_cpp\
    \ import *\n  File \"C:\\Users\\Administrator\\.conda\\envs\\textgen\\lib\\site-packages\\\
    llama_cpp\\llama_cpp.py\", line 1292, in <module>\n    llama_backend_init(c_bool(False))\n\
    \  File \"C:\\Users\\Administrator\\.conda\\envs\\textgen\\lib\\site-packages\\\
    llama_cpp\\llama_cpp.py\", line 403, in llama_backend_init\n    return _lib.llama_backend_init(numa)\n\
    OSError: [WinError -1073741795] Windows Error 0xc000001d"
  created_at: 2023-08-06 04:07:20+00:00
  edited: false
  hidden: false
  id: 64cf2a88995a0b5d5952859f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
      fullname: aldenn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aldennX
      type: user
    createdAt: '2023-08-06T08:13:37.000Z'
    data:
      status: closed
    id: 64cf56314a204a4d120a7c20
    type: status-change
  author: aldennX
  created_at: 2023-08-06 07:13:37+00:00
  id: 64cf56314a204a4d120a7c20
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: TheBloke/Falcon-7B-Instruct-GPTQ
repo_type: model
status: closed
target_branch: null
title: I am using GPU and the following is an error prompt. May I know how to resolve
  it
