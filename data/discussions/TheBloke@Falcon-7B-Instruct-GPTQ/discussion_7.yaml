!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mancub
conflicting_files: null
created_at: 2023-06-01 00:38:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-01T01:38:07.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>WARNING:The AutoGPTQ params are: {''model_basename'': ''gptq_model-4bit-64g'',
          ''device'': ''cuda:0'', ''use_triton'': False, ''use_safetensors'': True,
          ''trust_remote_code'': True, ''max_memory'': None}<br>WARNING:The safetensors
          archive passed at models/thebloke_falcon-7b-instruct-gptq/gptq_model-4bit-64g.safetensors
          does not contain metadata. Make sure to save your model with the <code>save_pretrained</code>
          method. Defaulting to ''pt'' metadata.<br>WARNING:can''t get model''s sequence
          length from model config, will set to 4096.<br>WARNING:RWGPTQForCausalLM
          hasn''t fused attention module yet, will skip inject fused attention.<br>WARNING:RWGPTQForCausalLM
          hasn''t fused mlp module yet, will skip inject fused mlp.</p>

          <p>Do I need to specify more params in when starting text_generation_webui,
          like group size and should it be using Triton or not (right now it says:
          False)?</p>

          <p>I did compile the CUDA extension from AutoGPTQ and it seems be working
          ok on my WSL2/3090 setup:</p>

          <pre><code>Output generated in 42.66 seconds (3.09 tokens/s, 132 tokens,
          context 65, seed 201216109)

          </code></pre>

          <p>Of course nothing to write home at 3 t/s but it''s a start.</p>

          '
        raw: "WARNING:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit-64g',\
          \ 'device': 'cuda:0', 'use_triton': False, 'use_safetensors': True, 'trust_remote_code':\
          \ True, 'max_memory': None}\r\nWARNING:The safetensors archive passed at\
          \ models/thebloke_falcon-7b-instruct-gptq/gptq_model-4bit-64g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\r\nWARNING:can't get model's sequence\
          \ length from model config, will set to 4096.\r\nWARNING:RWGPTQForCausalLM\
          \ hasn't fused attention module yet, will skip inject fused attention.\r\
          \nWARNING:RWGPTQForCausalLM hasn't fused mlp module yet, will skip inject\
          \ fused mlp.\r\n\r\nDo I need to specify more params in when starting text_generation_webui,\
          \ like group size and should it be using Triton or not (right now it says:\
          \ False)?\r\n\r\nI did compile the CUDA extension from AutoGPTQ and it seems\
          \ be working ok on my WSL2/3090 setup:\r\n\r\n    Output generated in 42.66\
          \ seconds (3.09 tokens/s, 132 tokens, context 65, seed 201216109)\r\n\r\n\
          Of course nothing to write home at 3 t/s but it's a start."
        updatedAt: '2023-06-01T01:38:07.869Z'
      numEdits: 0
      reactions: []
    id: 6477f67fe5ddd7d2a1739457
    type: comment
  author: mancub
  content: "WARNING:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit-64g',\
    \ 'device': 'cuda:0', 'use_triton': False, 'use_safetensors': True, 'trust_remote_code':\
    \ True, 'max_memory': None}\r\nWARNING:The safetensors archive passed at models/thebloke_falcon-7b-instruct-gptq/gptq_model-4bit-64g.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\r\nWARNING:can't get model's sequence length\
    \ from model config, will set to 4096.\r\nWARNING:RWGPTQForCausalLM hasn't fused\
    \ attention module yet, will skip inject fused attention.\r\nWARNING:RWGPTQForCausalLM\
    \ hasn't fused mlp module yet, will skip inject fused mlp.\r\n\r\nDo I need to\
    \ specify more params in when starting text_generation_webui, like group size\
    \ and should it be using Triton or not (right now it says: False)?\r\n\r\nI did\
    \ compile the CUDA extension from AutoGPTQ and it seems be working ok on my WSL2/3090\
    \ setup:\r\n\r\n    Output generated in 42.66 seconds (3.09 tokens/s, 132 tokens,\
    \ context 65, seed 201216109)\r\n\r\nOf course nothing to write home at 3 t/s\
    \ but it's a start."
  created_at: 2023-06-01 00:38:07+00:00
  edited: false
  hidden: false
  id: 6477f67fe5ddd7d2a1739457
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-02T08:57:07.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Those warnings are completely normal and expected. But I agree they''re
          not ideal. They cause confusion.</p>

          <p>You don''t need to specify group_size because that comes from quantize_config.json.  Specifying
          group_size etc would only be needed for models that don''t have quantize_config.json.  But
          all my recent ones do, and I''ll add it to all old models soon.</p>

          <p>The first line is printed by text-gen-ui to show what GPTQ params are.
          I don''t know why ooba is printing it as a warning. It''s purely informational.</p>

          <pre><code>WARNING:RWGPTQForCausalLM hasn''t fused attention module yet,
          will skip inject fused attention.

          WARNING:RWGPTQForCausalLM hasn''t fused mlp module yet, will skip inject
          fused mlp.

          </code></pre>

          <p>These lines are because AutoGPTQ tries by default to turn on two performance
          enhancing features, fused attention and fused mlp. But only Llama currently
          supports them.  So it''s telling you that those features were automatically
          enabled, but aren''t available for this model type.  Again ideally they
          shouldn''t be printed unless the user actually requested those features</p>

          <pre><code>WARNING:The safetensors archive passed at models/thebloke_falcon-7b-instruct-gptq/gptq_model-4bit-64g.safetensors
          does not contain metadata. Make sure to save your model with the save_pretrained
          method. Defaulting to ''pt'' metadata.

          </code></pre>

          <p>This comes automatically from the safetensors library.  Ideally AutoGPTQ
          should suppress this message I guess.</p>

          <pre><code>WARNING:can''t get model''s sequence length from model config,
          will set to 4096.

          </code></pre>

          <p>This is unique to Falcon. It might be a bug in AutoGPTQ''s Falcon support
          code. It should probably default Falcon to 2048 as that''s the correct max
          sequence length.  But it won''t affect text-gen will which limit output
          to 2048 anyway.</p>

          '
        raw: 'Those warnings are completely normal and expected. But I agree they''re
          not ideal. They cause confusion.


          You don''t need to specify group_size because that comes from quantize_config.json.  Specifying
          group_size etc would only be needed for models that don''t have quantize_config.json.  But
          all my recent ones do, and I''ll add it to all old models soon.


          The first line is printed by text-gen-ui to show what GPTQ params are. I
          don''t know why ooba is printing it as a warning. It''s purely informational.


          ```

          WARNING:RWGPTQForCausalLM hasn''t fused attention module yet, will skip
          inject fused attention.

          WARNING:RWGPTQForCausalLM hasn''t fused mlp module yet, will skip inject
          fused mlp.

          ```


          These lines are because AutoGPTQ tries by default to turn on two performance
          enhancing features, fused attention and fused mlp. But only Llama currently
          supports them.  So it''s telling you that those features were automatically
          enabled, but aren''t available for this model type.  Again ideally they
          shouldn''t be printed unless the user actually requested those features


          ```

          WARNING:The safetensors archive passed at models/thebloke_falcon-7b-instruct-gptq/gptq_model-4bit-64g.safetensors
          does not contain metadata. Make sure to save your model with the save_pretrained
          method. Defaulting to ''pt'' metadata.

          ```


          This comes automatically from the safetensors library.  Ideally AutoGPTQ
          should suppress this message I guess.


          ```

          WARNING:can''t get model''s sequence length from model config, will set
          to 4096.

          ```


          This is unique to Falcon. It might be a bug in AutoGPTQ''s Falcon support
          code. It should probably default Falcon to 2048 as that''s the correct max
          sequence length.  But it won''t affect text-gen will which limit output
          to 2048 anyway.'
        updatedAt: '2023-06-02T08:57:07.741Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - mancub
        - BrianTin
        - Chr-Hau
        - Kalcifer
    id: 6479aee31a2aefceecd2cb44
    type: comment
  author: TheBloke
  content: 'Those warnings are completely normal and expected. But I agree they''re
    not ideal. They cause confusion.


    You don''t need to specify group_size because that comes from quantize_config.json.  Specifying
    group_size etc would only be needed for models that don''t have quantize_config.json.  But
    all my recent ones do, and I''ll add it to all old models soon.


    The first line is printed by text-gen-ui to show what GPTQ params are. I don''t
    know why ooba is printing it as a warning. It''s purely informational.


    ```

    WARNING:RWGPTQForCausalLM hasn''t fused attention module yet, will skip inject
    fused attention.

    WARNING:RWGPTQForCausalLM hasn''t fused mlp module yet, will skip inject fused
    mlp.

    ```


    These lines are because AutoGPTQ tries by default to turn on two performance enhancing
    features, fused attention and fused mlp. But only Llama currently supports them.  So
    it''s telling you that those features were automatically enabled, but aren''t
    available for this model type.  Again ideally they shouldn''t be printed unless
    the user actually requested those features


    ```

    WARNING:The safetensors archive passed at models/thebloke_falcon-7b-instruct-gptq/gptq_model-4bit-64g.safetensors
    does not contain metadata. Make sure to save your model with the save_pretrained
    method. Defaulting to ''pt'' metadata.

    ```


    This comes automatically from the safetensors library.  Ideally AutoGPTQ should
    suppress this message I guess.


    ```

    WARNING:can''t get model''s sequence length from model config, will set to 4096.

    ```


    This is unique to Falcon. It might be a bug in AutoGPTQ''s Falcon support code.
    It should probably default Falcon to 2048 as that''s the correct max sequence
    length.  But it won''t affect text-gen will which limit output to 2048 anyway.'
  created_at: 2023-06-02 07:57:07+00:00
  edited: false
  hidden: false
  id: 6479aee31a2aefceecd2cb44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-03T15:31:51.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.865746259689331
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Thanks again, learning new stuff all the time :)</p>

          '
        raw: Thanks again, learning new stuff all the time :)
        updatedAt: '2023-06-03T15:31:51.394Z'
      numEdits: 0
      reactions: []
      relatedEventId: 647b5ce7e8b7333058a6b30f
    id: 647b5ce7e8b7333058a6b30e
    type: comment
  author: mancub
  content: Thanks again, learning new stuff all the time :)
  created_at: 2023-06-03 14:31:51+00:00
  edited: false
  hidden: false
  id: 647b5ce7e8b7333058a6b30e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-03T15:31:51.000Z'
    data:
      status: closed
    id: 647b5ce7e8b7333058a6b30f
    type: status-change
  author: mancub
  created_at: 2023-06-03 14:31:51+00:00
  id: 647b5ce7e8b7333058a6b30f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/Falcon-7B-Instruct-GPTQ
repo_type: model
status: closed
target_branch: null
title: About these warnings...
