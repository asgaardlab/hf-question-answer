!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aldennX
conflicting_files: null
created_at: 2023-08-06 07:17:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
      fullname: aldenn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aldennX
      type: user
    createdAt: '2023-08-06T08:17:04.000Z'
    data:
      edited: false
      editors:
      - aldennX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.24009597301483154
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
          fullname: aldenn
          isHf: false
          isPro: false
          name: aldennX
          type: user
        html: '<p>Traceback (most recent call last):<br>  File "C:\Users\Administrator\text-generation-webui\server.py",
          line 68, in load_model_wrapper<br>    shared.model, shared.tokenizer = load_model(shared.model_name,
          loader)<br>  File "C:\Users\Administrator\text-generation-webui\modules\models.py",
          line 78, in load_model<br>    output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "C:\Users\Administrator\text-generation-webui\modules\models.py", line 232,
          in llamacpp_loader<br>    from modules.llamacpp_model import LlamaCppModel<br>  File
          "C:\Users\Administrator\text-generation-webui\modules\llamacpp_model.py",
          line 11, in <br>    import llama_cpp<br>  File "C:\Users\Administrator.conda\envs\textgen\lib\site-packages\llama_cpp_<em>init</em>_.py",
          line 1, in <br>    from .llama_cpp import *<br>  File "C:\Users\Administrator.conda\envs\textgen\lib\site-packages\llama_cpp\llama_cpp.py",
          line 1292, in <br>    llama_backend_init(c_bool(False))<br>  File "C:\Users\Administrator.conda\envs\textgen\lib\site-packages\llama_cpp\llama_cpp.py",
          line 403, in llama_backend_init<br>    return _lib.llama_backend_init(numa)<br>OSError:
          [WinError -1073741795] Windows Error 0xc000001d</p>

          '
        raw: "Traceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\\
          text-generation-webui\\server.py\", line 68, in load_model_wrapper\r\n \
          \   shared.model, shared.tokenizer = load_model(shared.model_name, loader)\r\
          \n  File \"C:\\Users\\Administrator\\text-generation-webui\\modules\\models.py\"\
          , line 78, in load_model\r\n    output = load_func_map[loader](model_name)\r\
          \n  File \"C:\\Users\\Administrator\\text-generation-webui\\modules\\models.py\"\
          , line 232, in llamacpp_loader\r\n    from modules.llamacpp_model import\
          \ LlamaCppModel\r\n  File \"C:\\Users\\Administrator\\text-generation-webui\\\
          modules\\llamacpp_model.py\", line 11, in <module>\r\n    import llama_cpp\r\
          \n  File \"C:\\Users\\Administrator\\.conda\\envs\\textgen\\lib\\site-packages\\\
          llama_cpp\\__init__.py\", line 1, in <module>\r\n    from .llama_cpp import\
          \ *\r\n  File \"C:\\Users\\Administrator\\.conda\\envs\\textgen\\lib\\site-packages\\\
          llama_cpp\\llama_cpp.py\", line 1292, in <module>\r\n    llama_backend_init(c_bool(False))\r\
          \n  File \"C:\\Users\\Administrator\\.conda\\envs\\textgen\\lib\\site-packages\\\
          llama_cpp\\llama_cpp.py\", line 403, in llama_backend_init\r\n    return\
          \ _lib.llama_backend_init(numa)\r\nOSError: [WinError -1073741795] Windows\
          \ Error 0xc000001d"
        updatedAt: '2023-08-06T08:17:04.657Z'
      numEdits: 0
      reactions: []
    id: 64cf57002f1f9578a014ae5c
    type: comment
  author: aldennX
  content: "Traceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\\
    text-generation-webui\\server.py\", line 68, in load_model_wrapper\r\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name, loader)\r\n  File \"C:\\Users\\\
    Administrator\\text-generation-webui\\modules\\models.py\", line 78, in load_model\r\
    \n    output = load_func_map[loader](model_name)\r\n  File \"C:\\Users\\Administrator\\\
    text-generation-webui\\modules\\models.py\", line 232, in llamacpp_loader\r\n\
    \    from modules.llamacpp_model import LlamaCppModel\r\n  File \"C:\\Users\\\
    Administrator\\text-generation-webui\\modules\\llamacpp_model.py\", line 11, in\
    \ <module>\r\n    import llama_cpp\r\n  File \"C:\\Users\\Administrator\\.conda\\\
    envs\\textgen\\lib\\site-packages\\llama_cpp\\__init__.py\", line 1, in <module>\r\
    \n    from .llama_cpp import *\r\n  File \"C:\\Users\\Administrator\\.conda\\\
    envs\\textgen\\lib\\site-packages\\llama_cpp\\llama_cpp.py\", line 1292, in <module>\r\
    \n    llama_backend_init(c_bool(False))\r\n  File \"C:\\Users\\Administrator\\\
    .conda\\envs\\textgen\\lib\\site-packages\\llama_cpp\\llama_cpp.py\", line 403,\
    \ in llama_backend_init\r\n    return _lib.llama_backend_init(numa)\r\nOSError:\
    \ [WinError -1073741795] Windows Error 0xc000001d"
  created_at: 2023-08-06 07:17:04+00:00
  edited: false
  hidden: false
  id: 64cf57002f1f9578a014ae5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
      fullname: aldenn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aldennX
      type: user
    createdAt: '2023-08-06T08:18:25.000Z'
    data:
      edited: false
      editors:
      - aldennX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6419487595558167
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
          fullname: aldenn
          isHf: false
          isPro: false
          name: aldennX
          type: user
        html: '<p>windo10 -  CPU</p>

          '
        raw: windo10 -  CPU
        updatedAt: '2023-08-06T08:18:25.357Z'
      numEdits: 0
      reactions: []
    id: 64cf5751228324a28bc4d19e
    type: comment
  author: aldennX
  content: windo10 -  CPU
  created_at: 2023-08-06 07:18:25+00:00
  edited: false
  hidden: false
  id: 64cf5751228324a28bc4d19e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-06T08:19:31.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9524224400520325
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This is a GPTQ model and it looks like you''re trying to load it
          with Loader = llama.cpp.</p>

          <p>Please use the AutoGPTQ loader.</p>

          '
        raw: 'This is a GPTQ model and it looks like you''re trying to load it with
          Loader = llama.cpp.


          Please use the AutoGPTQ loader.'
        updatedAt: '2023-08-06T08:19:31.366Z'
      numEdits: 0
      reactions: []
    id: 64cf57937d8cc9f0700de61a
    type: comment
  author: TheBloke
  content: 'This is a GPTQ model and it looks like you''re trying to load it with
    Loader = llama.cpp.


    Please use the AutoGPTQ loader.'
  created_at: 2023-08-06 07:19:31+00:00
  edited: false
  hidden: false
  id: 64cf57937d8cc9f0700de61a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
      fullname: aldenn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aldennX
      type: user
    createdAt: '2023-08-06T09:08:07.000Z'
    data:
      edited: false
      editors:
      - aldennX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9346094131469727
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
          fullname: aldenn
          isHf: false
          isPro: false
          name: aldennX
          type: user
        html: '<p>Error using AutoGPTQ</p>

          <p>ERROR:The model could not be loaded because its checkpoint file in .bin/.pt/.safetensors
          format could not be located.</p>

          '
        raw: "Error using AutoGPTQ\n \nERROR:The model could not be loaded because\
          \ its checkpoint file in .bin/.pt/.safetensors format could not be located."
        updatedAt: '2023-08-06T09:08:07.377Z'
      numEdits: 0
      reactions: []
    id: 64cf62f7abc3308f0510b6ac
    type: comment
  author: aldennX
  content: "Error using AutoGPTQ\n \nERROR:The model could not be loaded because its\
    \ checkpoint file in .bin/.pt/.safetensors format could not be located."
  created_at: 2023-08-06 08:08:07+00:00
  edited: false
  hidden: false
  id: 64cf62f7abc3308f0510b6ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
      fullname: aldenn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aldennX
      type: user
    createdAt: '2023-08-06T09:10:50.000Z'
    data:
      edited: false
      editors:
      - aldennX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9518751502037048
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
          fullname: aldenn
          isHf: false
          isPro: false
          name: aldennX
          type: user
        html: '<p>ERROR:The model could not be loaded because its checkpoint file
          in .bin/.pt/.safetensors format could not be located.<br>ERROR:No model
          is loaded! Select one in the Model tab.</p>

          '
        raw: 'ERROR:The model could not be loaded because its checkpoint file in .bin/.pt/.safetensors
          format could not be located.

          ERROR:No model is loaded! Select one in the Model tab.'
        updatedAt: '2023-08-06T09:10:50.306Z'
      numEdits: 0
      reactions: []
    id: 64cf639a9e9ca8123da1a4e5
    type: comment
  author: aldennX
  content: 'ERROR:The model could not be loaded because its checkpoint file in .bin/.pt/.safetensors
    format could not be located.

    ERROR:No model is loaded! Select one in the Model tab.'
  created_at: 2023-08-06 08:10:50+00:00
  edited: false
  hidden: false
  id: 64cf639a9e9ca8123da1a4e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
      fullname: aldenn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aldennX
      type: user
    createdAt: '2023-08-06T09:11:17.000Z'
    data:
      edited: true
      editors:
      - aldennX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9494337439537048
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
          fullname: aldenn
          isHf: false
          isPro: false
          name: aldennX
          type: user
        html: '<blockquote>

          <p>This is a GPTQ model and it looks like you''re trying to load it with
          Loader = llama.cpp.</p>

          <p>Please use the AutoGPTQ loader.</p>

          </blockquote>

          <p>ERROR:The model could not be loaded because its checkpoint file in .bin/.pt/.safetensors
          format could not be located.<br>ERROR:No model is loaded! Select one in
          the Model tab.</p>

          <p>models:<br>chinese-alpaca-2-7b.ggmlv3.q4_0.bin<br>Chinese-Llama-2-7b.ggmlv3.q4_0.bin</p>

          '
        raw: "> This is a GPTQ model and it looks like you're trying to load it with\
          \ Loader = llama.cpp.\n> \n> Please use the AutoGPTQ loader.\n\nERROR:The\
          \ model could not be loaded because its checkpoint file in .bin/.pt/.safetensors\
          \ format could not be located.\nERROR:No model is loaded! Select one in\
          \ the Model tab.\n\n\n\n\n\n\nmodels:\nchinese-alpaca-2-7b.ggmlv3.q4_0.bin\n\
          Chinese-Llama-2-7b.ggmlv3.q4_0.bin"
        updatedAt: '2023-08-06T09:16:32.485Z'
      numEdits: 1
      reactions: []
    id: 64cf63b5c0c627dfa7ca0f1c
    type: comment
  author: aldennX
  content: "> This is a GPTQ model and it looks like you're trying to load it with\
    \ Loader = llama.cpp.\n> \n> Please use the AutoGPTQ loader.\n\nERROR:The model\
    \ could not be loaded because its checkpoint file in .bin/.pt/.safetensors format\
    \ could not be located.\nERROR:No model is loaded! Select one in the Model tab.\n\
    \n\n\n\n\n\nmodels:\nchinese-alpaca-2-7b.ggmlv3.q4_0.bin\nChinese-Llama-2-7b.ggmlv3.q4_0.bin"
  created_at: 2023-08-06 08:11:17+00:00
  edited: true
  hidden: false
  id: 64cf63b5c0c627dfa7ca0f1c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
      fullname: aldenn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aldennX
      type: user
    createdAt: '2023-08-06T09:15:46.000Z'
    data:
      edited: false
      editors:
      - aldennX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.20205684006214142
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
          fullname: aldenn
          isHf: false
          isPro: false
          name: aldennX
          type: user
        html: '<p> models:<br>chinese-alpaca-2-7b.ggmlv3.q4_0.bin<br>Chinese-Llama-2-7b.ggmlv3.q4_0.bin</p>

          '
        raw: ' models:

          chinese-alpaca-2-7b.ggmlv3.q4_0.bin

          Chinese-Llama-2-7b.ggmlv3.q4_0.bin'
        updatedAt: '2023-08-06T09:15:46.080Z'
      numEdits: 0
      reactions: []
    id: 64cf64c21ed6649d707cd6cb
    type: comment
  author: aldennX
  content: ' models:

    chinese-alpaca-2-7b.ggmlv3.q4_0.bin

    Chinese-Llama-2-7b.ggmlv3.q4_0.bin'
  created_at: 2023-08-06 08:15:46+00:00
  edited: false
  hidden: false
  id: 64cf64c21ed6649d707cd6cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-06T09:27:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7936792373657227
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>What do you mean, models: chinese-alpaca GGML?  This is the Falcon
          7B Instruct GPTQ model.</p>

          <p>Please follow the instructions in the README for using this Falcon 7B
          Instruct GPTQ model.</p>

          '
        raw: 'What do you mean, models: chinese-alpaca GGML?  This is the Falcon 7B
          Instruct GPTQ model.


          Please follow the instructions in the README for using this Falcon 7B Instruct
          GPTQ model.'
        updatedAt: '2023-08-06T09:27:52.372Z'
      numEdits: 0
      reactions: []
    id: 64cf6798bc6c9c8bc02e3b10
    type: comment
  author: TheBloke
  content: 'What do you mean, models: chinese-alpaca GGML?  This is the Falcon 7B
    Instruct GPTQ model.


    Please follow the instructions in the README for using this Falcon 7B Instruct
    GPTQ model.'
  created_at: 2023-08-06 08:27:52+00:00
  edited: false
  hidden: false
  id: 64cf6798bc6c9c8bc02e3b10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/5300a80e7f03a5ae00ede64d11492d9c.svg
      fullname: aldenn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aldennX
      type: user
    createdAt: '2023-08-06T09:35:30.000Z'
    data:
      status: closed
    id: 64cf69629c245c6ba72e4f85
    type: status-change
  author: aldennX
  created_at: 2023-08-06 08:35:30+00:00
  id: 64cf69629c245c6ba72e4f85
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: TheBloke/Falcon-7B-Instruct-GPTQ
repo_type: model
status: closed
target_branch: null
title: How can I solve this problem?
