!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kllisre
conflicting_files: null
created_at: 2023-05-29 12:34:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab46adee8dfe0dc9315e2f01cb591d8b.svg
      fullname: Dmitry Dronov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kllisre
      type: user
    createdAt: '2023-05-29T13:34:21.000Z'
    data:
      edited: false
      editors:
      - kllisre
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab46adee8dfe0dc9315e2f01cb591d8b.svg
          fullname: Dmitry Dronov
          isHf: false
          isPro: false
          name: kllisre
          type: user
        html: "<p>Thanks for your work.<br>When I am running </p>\n<pre><code>tokenizer\
          \ = AutoTokenizer.from_pretrained(local_dir, use_fast=False)\nmodel = AutoGPTQForCausalLM.from_quantized(local_dir,\
          \ device=\"cuda:0\", use_triton=False, use_safetensors=True, torch_dtype=torch.float32,\
          \ trust_remote_code=True)\n</code></pre>\n<p>I get these warnings:</p>\n\
          <pre><code>CUDA extension not installed.\nRWGPTQForCausalLM hasn't fused\
          \ attention module yet, will skip inject fused attention.\nRWGPTQForCausalLM\
          \ hasn't fused mlp module yet, will skip inject fused mlp.\n</code></pre>\n\
          <p>I'm wondering if <code>CUDA extension not installed</code> affects model\
          \ performance. I can't figure out if it uses my GPU. It seems that I see\
          \ a load on 6gb vram, but I don\u2019t see PID of the task that would work\
          \ during inference. Maybe it works on the CPU? At times, inference can take\
          \ a very long time.<br>My env:</p>\n<pre><code>Collecting environment information...\n\
          PyTorch version: 2.0.1+cu117\nIs debug build: False\nCUDA used to build\
          \ PyTorch: 11.7\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.5 LTS\
          \ (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\nClang version:\
          \ Could not collect\nCMake version: version 3.26.3\nLibc version: glibc-2.31\n\
          \nPython version: 3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0] (64-bit\
          \ runtime)\nPython platform: Linux-5.15.0-67-generic-x86_64-with-glibc2.31\n\
          Is CUDA available: True\nCUDA runtime version: 11.2.152\nCUDA_MODULE_LOADING\
          \ set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce GTX\
          \ 1080 Ti\nNvidia driver version: 515.86.01\ncuDNN version: Probably one\
          \ of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.0\n\
          /usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.0\n\
          /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.0\n\
          /usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.0\nHIP runtime version:\
          \ N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\n\
          Model name:    Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz\n\nVersions of relevant\
          \ libraries:\n[pip3] numpy==1.24.3\n[pip3] torch==2.0.1\n[conda] numpy \
          \       1.24.3                   pypi_0    pypi\n[conda] torch        2.0.1\
          \                    pypi_0    pypi\n</code></pre>\n"
        raw: "Thanks for your work.\r\nWhen I am running \r\n```\r\ntokenizer = AutoTokenizer.from_pretrained(local_dir,\
          \ use_fast=False)\r\nmodel = AutoGPTQForCausalLM.from_quantized(local_dir,\
          \ device=\"cuda:0\", use_triton=False, use_safetensors=True, torch_dtype=torch.float32,\
          \ trust_remote_code=True)\r\n```\r\nI get these warnings:\r\n```\r\nCUDA\
          \ extension not installed.\r\nRWGPTQForCausalLM hasn't fused attention module\
          \ yet, will skip inject fused attention.\r\nRWGPTQForCausalLM hasn't fused\
          \ mlp module yet, will skip inject fused mlp.\r\n```\r\nI'm wondering if\
          \ `CUDA extension not installed` affects model performance. I can't figure\
          \ out if it uses my GPU. It seems that I see a load on 6gb vram, but I don\u2019\
          t see PID of the task that would work during inference. Maybe it works on\
          \ the CPU? At times, inference can take a very long time.\r\nMy env:\r\n\
          ```\r\nCollecting environment information...\r\nPyTorch version: 2.0.1+cu117\r\
          \nIs debug build: False\r\nCUDA used to build PyTorch: 11.7\r\nROCM used\
          \ to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.5 LTS (x86_64)\r\nGCC version:\
          \ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\nClang version: Could not collect\r\
          \nCMake version: version 3.26.3\r\nLibc version: glibc-2.31\r\n\r\nPython\
          \ version: 3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0] (64-bit runtime)\r\
          \nPython platform: Linux-5.15.0-67-generic-x86_64-with-glibc2.31\r\nIs CUDA\
          \ available: True\r\nCUDA runtime version: 11.2.152\r\nCUDA_MODULE_LOADING\
          \ set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce GTX\
          \ 1080 Ti\r\nNvidia driver version: 515.86.01\r\ncuDNN version: Probably\
          \ one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.0\r\
          \n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.0\r\
          \n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.0\r\
          \n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.0\r\
          \nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK\
          \ available: True\r\n\r\nCPU:\r\nModel name:    Intel(R) Core(TM) i7-8700K\
          \ CPU @ 3.70GHz\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.24.3\r\
          \n[pip3] torch==2.0.1\r\n[conda] numpy        1.24.3                   pypi_0\
          \    pypi\r\n[conda] torch        2.0.1                    pypi_0    pypi\r\
          \n```"
        updatedAt: '2023-05-29T13:34:21.903Z'
      numEdits: 0
      reactions: []
    id: 6474a9dda855203d8feb2f06
    type: comment
  author: kllisre
  content: "Thanks for your work.\r\nWhen I am running \r\n```\r\ntokenizer = AutoTokenizer.from_pretrained(local_dir,\
    \ use_fast=False)\r\nmodel = AutoGPTQForCausalLM.from_quantized(local_dir, device=\"\
    cuda:0\", use_triton=False, use_safetensors=True, torch_dtype=torch.float32, trust_remote_code=True)\r\
    \n```\r\nI get these warnings:\r\n```\r\nCUDA extension not installed.\r\nRWGPTQForCausalLM\
    \ hasn't fused attention module yet, will skip inject fused attention.\r\nRWGPTQForCausalLM\
    \ hasn't fused mlp module yet, will skip inject fused mlp.\r\n```\r\nI'm wondering\
    \ if `CUDA extension not installed` affects model performance. I can't figure\
    \ out if it uses my GPU. It seems that I see a load on 6gb vram, but I don\u2019\
    t see PID of the task that would work during inference. Maybe it works on the\
    \ CPU? At times, inference can take a very long time.\r\nMy env:\r\n```\r\nCollecting\
    \ environment information...\r\nPyTorch version: 2.0.1+cu117\r\nIs debug build:\
    \ False\r\nCUDA used to build PyTorch: 11.7\r\nROCM used to build PyTorch: N/A\r\
    \n\r\nOS: Ubuntu 20.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1)\
    \ 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.3\r\
    \nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.11 (main, Apr 20 2023,\
    \ 19:02:41) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-67-generic-x86_64-with-glibc2.31\r\
    \nIs CUDA available: True\r\nCUDA runtime version: 11.2.152\r\nCUDA_MODULE_LOADING\
    \ set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce GTX 1080\
    \ Ti\r\nNvidia driver version: 515.86.01\r\ncuDNN version: Probably one of the\
    \ following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.0\r\
    \n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.0\r\
    \n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.0\r\
    \n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.0\r\nHIP runtime version:\
    \ N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\
    \nModel name:    Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz\r\n\r\nVersions of relevant\
    \ libraries:\r\n[pip3] numpy==1.24.3\r\n[pip3] torch==2.0.1\r\n[conda] numpy \
    \       1.24.3                   pypi_0    pypi\r\n[conda] torch        2.0.1\
    \                    pypi_0    pypi\r\n```"
  created_at: 2023-05-29 12:34:21+00:00
  edited: false
  hidden: false
  id: 6474a9dda855203d8feb2f06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-29T14:34:48.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Did you build Autogptq with CUDA locally?</p>

          '
        raw: Did you build Autogptq with CUDA locally?
        updatedAt: '2023-05-29T14:34:48.707Z'
      numEdits: 0
      reactions: []
    id: 6474b80882907acdddeeab4d
    type: comment
  author: Yhyu13
  content: Did you build Autogptq with CUDA locally?
  created_at: 2023-05-29 13:34:48+00:00
  edited: false
  hidden: false
  id: 6474b80882907acdddeeab4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab46adee8dfe0dc9315e2f01cb591d8b.svg
      fullname: Dmitry Dronov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kllisre
      type: user
    createdAt: '2023-05-29T14:46:13.000Z'
    data:
      edited: false
      editors:
      - kllisre
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab46adee8dfe0dc9315e2f01cb591d8b.svg
          fullname: Dmitry Dronov
          isHf: false
          isPro: false
          name: kllisre
          type: user
        html: '<blockquote>

          <p>Did you build Autogptq with CUDA locally?</p>

          </blockquote>

          <p>Yes, I did that on my local machine:</p>

          <pre><code>git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install .

          pip install einops

          </code></pre>

          '
        raw: '> Did you build Autogptq with CUDA locally?


          Yes, I did that on my local machine:

          ```

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install .

          pip install einops

          ```'
        updatedAt: '2023-05-29T14:46:13.784Z'
      numEdits: 0
      reactions: []
    id: 6474bab582907acdddeedc45
    type: comment
  author: kllisre
  content: '> Did you build Autogptq with CUDA locally?


    Yes, I did that on my local machine:

    ```

    git clone https://github.com/PanQiWei/AutoGPTQ

    cd AutoGPTQ

    pip install .

    pip install einops

    ```'
  created_at: 2023-05-29 13:46:13+00:00
  edited: false
  hidden: false
  id: 6474bab582907acdddeedc45
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7babdd7d55559aa9a235b43e06488c07.svg
      fullname: Kazzoul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Youness007
      type: user
    createdAt: '2023-10-05T15:14:58.000Z'
    data:
      edited: false
      editors:
      - Youness007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6809996366500854
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7babdd7d55559aa9a235b43e06488c07.svg
          fullname: Kazzoul
          isHf: false
          isPro: false
          name: Youness007
          type: user
        html: "<blockquote>\n<blockquote>\n<p>Did you build Autogptq with CUDA locally?</p>\n\
          </blockquote>\n<p>Yes, I did that on my local machine:</p>\n<pre><code>git\
          \ clone https://github.com/PanQiWei/AutoGPTQ\ncd AutoGPTQ\npip install .\n\
          pip install einops\n</code></pre>\n</blockquote>\n<p>I have this errors\
          \ :<br>(localGPT) [dg@localhost AutoGPTQ]$ pip install .<br>Processing /home/dg-linc/AutoGPTQ<br>\
          \  Preparing metadata (setup.py) ... error<br>  error: subprocess-exited-with-error</p>\n\
          <p>  \xD7 python setup.py egg_info did not run successfully.<br>  \u2502\
          \ exit code: 1<br>  \u2570\u2500&gt; [6 lines of output]<br>      Traceback\
          \ (most recent call last):<br>        File \"\", line 2, in <br>       \
          \ File \"\", line 34, in <br>        File \"/home/dg-linc/AutoGPTQ/setup.py\"\
          , line 58, in <br>          CUDA_VERSION = \"\".join(os.environ.get(\"CUDA_VERSION\"\
          , default_cuda_version).split(\".\"))<br>      AttributeError: 'NoneType'\
          \ object has no attribute 'split'<br>      [end of output]</p>\n<p>  note:\
          \ This error originates from a subprocess, and is likely not a problem with\
          \ pip.<br>error: metadata-generation-failed</p>\n<p>\xD7 Encountered error\
          \ while generating package metadata.<br>\u2570\u2500&gt; See above for output.</p>\n\
          <p>note: This is an issue with the package mentioned above, not pip.<br>hint:\
          \ See above for details.</p>\n<p>someone have an idea ? </p>\n"
        raw: "\n> > Did you build Autogptq with CUDA locally?\n> \n> Yes, I did that\
          \ on my local machine:\n> ```\n> git clone https://github.com/PanQiWei/AutoGPTQ\n\
          > cd AutoGPTQ\n> pip install .\n> pip install einops\n> ```\n\nI have this\
          \ errors : \n(localGPT) [dg@localhost AutoGPTQ]$ pip install .\nProcessing\
          \ /home/dg-linc/AutoGPTQ\n  Preparing metadata (setup.py) ... error\n  error:\
          \ subprocess-exited-with-error\n  \n  \xD7 python setup.py egg_info did\
          \ not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [6 lines\
          \ of output]\n      Traceback (most recent call last):\n        File \"\
          <string>\", line 2, in <module>\n        File \"<pip-setuptools-caller>\"\
          , line 34, in <module>\n        File \"/home/dg-linc/AutoGPTQ/setup.py\"\
          , line 58, in <module>\n          CUDA_VERSION = \"\".join(os.environ.get(\"\
          CUDA_VERSION\", default_cuda_version).split(\".\"))\n      AttributeError:\
          \ 'NoneType' object has no attribute 'split'\n      [end of output]\n  \n\
          \  note: This error originates from a subprocess, and is likely not a problem\
          \ with pip.\nerror: metadata-generation-failed\n\n\xD7 Encountered error\
          \ while generating package metadata.\n\u2570\u2500> See above for output.\n\
          \nnote: This is an issue with the package mentioned above, not pip.\nhint:\
          \ See above for details.\n\nsomeone have an idea ? "
        updatedAt: '2023-10-05T15:14:58.433Z'
      numEdits: 0
      reactions: []
    id: 651ed2f2c2f92699e28b8f66
    type: comment
  author: Youness007
  content: "\n> > Did you build Autogptq with CUDA locally?\n> \n> Yes, I did that\
    \ on my local machine:\n> ```\n> git clone https://github.com/PanQiWei/AutoGPTQ\n\
    > cd AutoGPTQ\n> pip install .\n> pip install einops\n> ```\n\nI have this errors\
    \ : \n(localGPT) [dg@localhost AutoGPTQ]$ pip install .\nProcessing /home/dg-linc/AutoGPTQ\n\
    \  Preparing metadata (setup.py) ... error\n  error: subprocess-exited-with-error\n\
    \  \n  \xD7 python setup.py egg_info did not run successfully.\n  \u2502 exit\
    \ code: 1\n  \u2570\u2500> [6 lines of output]\n      Traceback (most recent call\
    \ last):\n        File \"<string>\", line 2, in <module>\n        File \"<pip-setuptools-caller>\"\
    , line 34, in <module>\n        File \"/home/dg-linc/AutoGPTQ/setup.py\", line\
    \ 58, in <module>\n          CUDA_VERSION = \"\".join(os.environ.get(\"CUDA_VERSION\"\
    , default_cuda_version).split(\".\"))\n      AttributeError: 'NoneType' object\
    \ has no attribute 'split'\n      [end of output]\n  \n  note: This error originates\
    \ from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\
    \n\xD7 Encountered error while generating package metadata.\n\u2570\u2500> See\
    \ above for output.\n\nnote: This is an issue with the package mentioned above,\
    \ not pip.\nhint: See above for details.\n\nsomeone have an idea ? "
  created_at: 2023-10-05 14:14:58+00:00
  edited: false
  hidden: false
  id: 651ed2f2c2f92699e28b8f66
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Falcon-7B-Instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: CUDA extension not installed
