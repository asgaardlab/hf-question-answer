!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Melbourne
conflicting_files: null
created_at: 2023-04-17 12:21:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e30f67adbbf2377a9e4d672127313a84.svg
      fullname: M M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Melbourne
      type: user
    createdAt: '2023-04-17T13:21:35.000Z'
    data:
      edited: true
      editors:
      - Melbourne
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e30f67adbbf2377a9e4d672127313a84.svg
          fullname: M M
          isHf: false
          isPro: false
          name: Melbourne
          type: user
        html: '<p>Would you consider doing the same thing with the 30B version from
          here? <a href="https://huggingface.co/digitous/Alpacino30b/tree/main">https://huggingface.co/digitous/Alpacino30b/tree/main</a></p>

          <p>I can possibly help, I''ve got a 5900X/64GB/3090 24GB but I don''t know
          what I''m doing. For example, do I download the 33 little .bin files from
          the huggingface repo linked above, or do I just need the .safetensors?</p>

          <p>Thank you.</p>

          '
        raw: 'Would you consider doing the same thing with the 30B version from here?
          https://huggingface.co/digitous/Alpacino30b/tree/main


          I can possibly help, I''ve got a 5900X/64GB/3090 24GB but I don''t know
          what I''m doing. For example, do I download the 33 little .bin files from
          the huggingface repo linked above, or do I just need the .safetensors?


          Thank you.'
        updatedAt: '2023-04-17T13:24:58.427Z'
      numEdits: 2
      reactions: []
    id: 643d47df9347842571bda865
    type: comment
  author: Melbourne
  content: 'Would you consider doing the same thing with the 30B version from here?
    https://huggingface.co/digitous/Alpacino30b/tree/main


    I can possibly help, I''ve got a 5900X/64GB/3090 24GB but I don''t know what I''m
    doing. For example, do I download the 33 little .bin files from the huggingface
    repo linked above, or do I just need the .safetensors?


    Thank you.'
  created_at: 2023-04-17 12:21:35+00:00
  edited: true
  hidden: false
  id: 643d47df9347842571bda865
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c84200d82b2a5c0729007e5a991fe16.svg
      fullname: zatochu
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: zatochu
      type: user
    createdAt: '2023-04-17T13:33:29.000Z'
    data:
      edited: false
      editors:
      - zatochu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c84200d82b2a5c0729007e5a991fe16.svg
          fullname: zatochu
          isHf: false
          isPro: false
          name: zatochu
          type: user
        html: '<p>I don''t have enough RAM to convert 30B+. Here''s some pretty simple
          steps to convert the weights yourself for use with llama.cpp.</p>

          <ol>

          <li>git clone or download the llama.cpp source and compile with make or
          cmake as usual if you haven''t already, or if on Windows, download the prebuilt
          binaries from releases and at least get convert.py from source</li>

          <li>git clone <a href="https://huggingface.co/digitous/Alpacino30b/">https://huggingface.co/digitous/Alpacino30b/</a>
          with git-lfs installed or manually download all the files from Alpacino30b
          into a folder</li>

          <li>cd into the llama.cpp directory and do <code>python convert.py Alpacino30b/</code>
          (you''ll need python installed + numpy and sentencepiece installed with
          pip)</li>

          <li>while still in the llama.cpp directory run <code>./quantize Alpacino30b/ggml-model-f16.bin
          ggml-model-q4_0.bin 2</code></li>

          </ol>

          '
        raw: 'I don''t have enough RAM to convert 30B+. Here''s some pretty simple
          steps to convert the weights yourself for use with llama.cpp.

          1. git clone or download the llama.cpp source and compile with make or cmake
          as usual if you haven''t already, or if on Windows, download the prebuilt
          binaries from releases and at least get convert.py from source

          2. git clone https://huggingface.co/digitous/Alpacino30b/ with git-lfs installed
          or manually download all the files from Alpacino30b into a folder

          2. cd into the llama.cpp directory and do `python convert.py Alpacino30b/`
          (you''ll need python installed + numpy and sentencepiece installed with
          pip)

          3. while still in the llama.cpp directory run `./quantize Alpacino30b/ggml-model-f16.bin
          ggml-model-q4_0.bin 2`'
        updatedAt: '2023-04-17T13:33:29.227Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Ware
        - PrimeD
    id: 643d4aa97ede13bc7b56dd53
    type: comment
  author: zatochu
  content: 'I don''t have enough RAM to convert 30B+. Here''s some pretty simple steps
    to convert the weights yourself for use with llama.cpp.

    1. git clone or download the llama.cpp source and compile with make or cmake as
    usual if you haven''t already, or if on Windows, download the prebuilt binaries
    from releases and at least get convert.py from source

    2. git clone https://huggingface.co/digitous/Alpacino30b/ with git-lfs installed
    or manually download all the files from Alpacino30b into a folder

    2. cd into the llama.cpp directory and do `python convert.py Alpacino30b/` (you''ll
    need python installed + numpy and sentencepiece installed with pip)

    3. while still in the llama.cpp directory run `./quantize Alpacino30b/ggml-model-f16.bin
    ggml-model-q4_0.bin 2`'
  created_at: 2023-04-17 12:33:29+00:00
  edited: false
  hidden: false
  id: 643d4aa97ede13bc7b56dd53
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e30f67adbbf2377a9e4d672127313a84.svg
      fullname: M M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Melbourne
      type: user
    createdAt: '2023-04-17T16:22:46.000Z'
    data:
      edited: true
      editors:
      - Melbourne
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e30f67adbbf2377a9e4d672127313a84.svg
          fullname: M M
          isHf: false
          isPro: false
          name: Melbourne
          type: user
        html: '<blockquote>

          <p>I don''t have enough RAM to convert 30B+. Here''s some pretty simple
          steps to convert the weights yourself for use with llama.cpp.</p>

          <ol>

          <li>git clone or download the llama.cpp source and compile with make or
          cmake as usual if you haven''t already, or if on Windows, download the prebuilt
          binaries from releases and at least get convert.py from source</li>

          <li>git clone <a href="https://huggingface.co/digitous/Alpacino30b/">https://huggingface.co/digitous/Alpacino30b/</a>
          with git-lfs installed or manually download all the files from Alpacino30b
          into a folder</li>

          <li>cd into the llama.cpp directory and do <code>python convert.py Alpacino30b/</code>
          (you''ll need python installed + numpy and sentencepiece installed with
          pip)</li>

          <li>while still in the llama.cpp directory run <code>./quantize Alpacino30b/ggml-model-f16.bin
          ggml-model-q4_0.bin 2</code></li>

          </ol>

          </blockquote>

          <p>Thanks. I was able to do it. I thought that would be harder, but your
          instructions were very precise and easy.</p>

          '
        raw: '> I don''t have enough RAM to convert 30B+. Here''s some pretty simple
          steps to convert the weights yourself for use with llama.cpp.

          > 1. git clone or download the llama.cpp source and compile with make or
          cmake as usual if you haven''t already, or if on Windows, download the prebuilt
          binaries from releases and at least get convert.py from source

          > 2. git clone https://huggingface.co/digitous/Alpacino30b/ with git-lfs
          installed or manually download all the files from Alpacino30b into a folder

          > 2. cd into the llama.cpp directory and do `python convert.py Alpacino30b/`
          (you''ll need python installed + numpy and sentencepiece installed with
          pip)

          > 3. while still in the llama.cpp directory run `./quantize Alpacino30b/ggml-model-f16.bin
          ggml-model-q4_0.bin 2`


          Thanks. I was able to do it. I thought that would be harder, but your instructions
          were very precise and easy.'
        updatedAt: '2023-04-17T16:43:20.870Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Ware
    id: 643d725651e2958ef6ccb972
    type: comment
  author: Melbourne
  content: '> I don''t have enough RAM to convert 30B+. Here''s some pretty simple
    steps to convert the weights yourself for use with llama.cpp.

    > 1. git clone or download the llama.cpp source and compile with make or cmake
    as usual if you haven''t already, or if on Windows, download the prebuilt binaries
    from releases and at least get convert.py from source

    > 2. git clone https://huggingface.co/digitous/Alpacino30b/ with git-lfs installed
    or manually download all the files from Alpacino30b into a folder

    > 2. cd into the llama.cpp directory and do `python convert.py Alpacino30b/` (you''ll
    need python installed + numpy and sentencepiece installed with pip)

    > 3. while still in the llama.cpp directory run `./quantize Alpacino30b/ggml-model-f16.bin
    ggml-model-q4_0.bin 2`


    Thanks. I was able to do it. I thought that would be harder, but your instructions
    were very precise and easy.'
  created_at: 2023-04-17 15:22:46+00:00
  edited: true
  hidden: false
  id: 643d725651e2958ef6ccb972
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: zatochu/Alpacino-13b-ggml
repo_type: model
status: open
target_branch: null
title: 30B version.
