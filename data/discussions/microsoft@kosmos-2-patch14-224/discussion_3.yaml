!!python/object:huggingface_hub.community.DiscussionWithDetails
author: panopstor
conflicting_files: null
created_at: 2023-11-03 02:31:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/18df5db7781e49c30af73445f5fef724.svg
      fullname: Victor Hall
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: panopstor
      type: user
    createdAt: '2023-11-03T03:31:13.000Z'
    data:
      edited: false
      editors:
      - panopstor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.16424109041690826
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/18df5db7781e49c30af73445f5fef724.svg
          fullname: Victor Hall
          isHf: false
          isPro: false
          name: panopstor
          type: user
        html: "<p>This should enable using torch.float16 or torch.bfloat16 (snippet\
          \ extracted from larger script based on example):</p>\n<pre><code>parser.add_argument(\"\
          --dtype\", type=str, default=\"fp16\", help=\"force a different dtype if\
          \ using GPU (fp16, bf16, fp32) (default: fp16)\")\n</code></pre>\n<p>...</p>\n\
          <pre><code>    dtype=torch.float32\n    if args.dtype == \"fp16\":\n   \
          \     dtype=torch.float16\n    elif args.dtype == \"bf16\":\n        dtype=torch.bfloat16\n\
          \    model = model.to(dtype=dtype).cuda()\n</code></pre>\n<p>...</p>\n<pre><code>\
          \                with torch.cuda.amp.autocast(enabled=args.dtype != \"fp32\"\
          , dtype=dtype):\n                    generated_ids = model.generate(\n \
          \                       pixel_values=inputs[\"pixel_values\"].cuda() if\
          \ not args.cpu else inputs[\"pixel_values\"],\n                        input_ids=inputs[\"\
          input_ids\"].cuda() if not args.cpu else inputs[\"input_ids\"],\n      \
          \                  attention_mask=inputs[\"attention_mask\"].cuda() if not\
          \ args.cpu else inputs[\"attention_mask\"],\n                        image_embeds=None,\n\
          \                        image_embeds_position_mask=inputs[\"image_embeds_position_mask\"\
          ].cuda() if not args.cpu else inputs[\"image_embeds_position_mask\"],\n\
          \                        use_cache=True,\n                        max_new_tokens=args.max_new_tokens,\n\
          \                    )\n</code></pre>\n<p>Seems to work fine with float16\
          \ or bfloat16</p>\n"
        raw: "This should enable using torch.float16 or torch.bfloat16 (snippet extracted\
          \ from larger script based on example):\r\n\r\n```\r\nparser.add_argument(\"\
          --dtype\", type=str, default=\"fp16\", help=\"force a different dtype if\
          \ using GPU (fp16, bf16, fp32) (default: fp16)\")\r\n```\r\n...\r\n```\r\
          \n    dtype=torch.float32\r\n    if args.dtype == \"fp16\":\r\n        dtype=torch.float16\r\
          \n    elif args.dtype == \"bf16\":\r\n        dtype=torch.bfloat16\r\n \
          \   model = model.to(dtype=dtype).cuda()\r\n```\r\n...\r\n```\r\n      \
          \          with torch.cuda.amp.autocast(enabled=args.dtype != \"fp32\",\
          \ dtype=dtype):\r\n                    generated_ids = model.generate(\r\
          \n                        pixel_values=inputs[\"pixel_values\"].cuda() if\
          \ not args.cpu else inputs[\"pixel_values\"],\r\n                      \
          \  input_ids=inputs[\"input_ids\"].cuda() if not args.cpu else inputs[\"\
          input_ids\"],\r\n                        attention_mask=inputs[\"attention_mask\"\
          ].cuda() if not args.cpu else inputs[\"attention_mask\"],\r\n          \
          \              image_embeds=None,\r\n                        image_embeds_position_mask=inputs[\"\
          image_embeds_position_mask\"].cuda() if not args.cpu else inputs[\"image_embeds_position_mask\"\
          ],\r\n                        use_cache=True,\r\n                      \
          \  max_new_tokens=args.max_new_tokens,\r\n                    )\r\n```\r\
          \n\r\nSeems to work fine with float16 or bfloat16"
        updatedAt: '2023-11-03T03:31:13.732Z'
      numEdits: 0
      reactions: []
    id: 654469815b5d9185ba2c64b0
    type: comment
  author: panopstor
  content: "This should enable using torch.float16 or torch.bfloat16 (snippet extracted\
    \ from larger script based on example):\r\n\r\n```\r\nparser.add_argument(\"--dtype\"\
    , type=str, default=\"fp16\", help=\"force a different dtype if using GPU (fp16,\
    \ bf16, fp32) (default: fp16)\")\r\n```\r\n...\r\n```\r\n    dtype=torch.float32\r\
    \n    if args.dtype == \"fp16\":\r\n        dtype=torch.float16\r\n    elif args.dtype\
    \ == \"bf16\":\r\n        dtype=torch.bfloat16\r\n    model = model.to(dtype=dtype).cuda()\r\
    \n```\r\n...\r\n```\r\n                with torch.cuda.amp.autocast(enabled=args.dtype\
    \ != \"fp32\", dtype=dtype):\r\n                    generated_ids = model.generate(\r\
    \n                        pixel_values=inputs[\"pixel_values\"].cuda() if not\
    \ args.cpu else inputs[\"pixel_values\"],\r\n                        input_ids=inputs[\"\
    input_ids\"].cuda() if not args.cpu else inputs[\"input_ids\"],\r\n          \
    \              attention_mask=inputs[\"attention_mask\"].cuda() if not args.cpu\
    \ else inputs[\"attention_mask\"],\r\n                        image_embeds=None,\r\
    \n                        image_embeds_position_mask=inputs[\"image_embeds_position_mask\"\
    ].cuda() if not args.cpu else inputs[\"image_embeds_position_mask\"],\r\n    \
    \                    use_cache=True,\r\n                        max_new_tokens=args.max_new_tokens,\r\
    \n                    )\r\n```\r\n\r\nSeems to work fine with float16 or bfloat16"
  created_at: 2023-11-03 02:31:13+00:00
  edited: false
  hidden: false
  id: 654469815b5d9185ba2c64b0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: microsoft/kosmos-2-patch14-224
repo_type: model
status: open
target_branch: null
title: Mixed precision on Nvidia/CUDA device snippet
