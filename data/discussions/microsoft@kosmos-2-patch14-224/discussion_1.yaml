!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yingss
conflicting_files: null
created_at: 2023-10-30 23:02:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ffcece29f45101f81c760d49c7b4038.svg
      fullname: Ying Shen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yingss
      type: user
    createdAt: '2023-10-31T00:02:19.000Z'
    data:
      edited: false
      editors:
      - yingss
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.842732846736908
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ffcece29f45101f81c760d49c7b4038.svg
          fullname: Ying Shen
          isHf: false
          isPro: false
          name: yingss
          type: user
        html: '<p>Thank you for the great work!</p>

          <p>I am trying to run the example in README. However, I got  <code>KeyError:
          ''kosmos-2''</code> after running <code>model = AutoModelForVision2Seq.from_pretrained("microsoft/kosmos-2-patch14-224")</code>.</p>

          <p>The version of my <code>transformers </code> is <code>4.33.3</code>.  Is
          this an issue of the version? If so, which version should I install?</p>

          <p>Also, could you provide an input example with interleaved text and multiple
          images? I am confused how to construct the input for an input sequence with
          text and multiple images. </p>

          <p>Thank you for the help!</p>

          '
        raw: "Thank you for the great work!\r\n\r\nI am trying to run the example\
          \ in README. However, I got  `KeyError: 'kosmos-2'` after running `model\
          \ = AutoModelForVision2Seq.from_pretrained(\"microsoft/kosmos-2-patch14-224\"\
          )`.\r\n\r\nThe version of my `transformers ` is `4.33.3`.  Is this an issue\
          \ of the version? If so, which version should I install?\r\n\r\nAlso, could\
          \ you provide an input example with interleaved text and multiple images?\
          \ I am confused how to construct the input for an input sequence with text\
          \ and multiple images. \r\n\r\nThank you for the help!\r\n\r\n"
        updatedAt: '2023-10-31T00:02:19.020Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - unilm
        - artur454324372662
    id: 6540440b4a4828900083d223
    type: comment
  author: yingss
  content: "Thank you for the great work!\r\n\r\nI am trying to run the example in\
    \ README. However, I got  `KeyError: 'kosmos-2'` after running `model = AutoModelForVision2Seq.from_pretrained(\"\
    microsoft/kosmos-2-patch14-224\")`.\r\n\r\nThe version of my `transformers ` is\
    \ `4.33.3`.  Is this an issue of the version? If so, which version should I install?\r\
    \n\r\nAlso, could you provide an input example with interleaved text and multiple\
    \ images? I am confused how to construct the input for an input sequence with\
    \ text and multiple images. \r\n\r\nThank you for the help!\r\n\r\n"
  created_at: 2023-10-30 23:02:19+00:00
  edited: false
  hidden: false
  id: 6540440b4a4828900083d223
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2023-10-31T08:07:13.000Z'
    data:
      edited: false
      editors:
      - ydshieh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9467117786407471
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
          fullname: Yih-Dar SHIEH
          isHf: true
          isPro: false
          name: ydshieh
          type: user
        html: '<p>Hi, you have to use the latest dev version (from the main branch).
          There will be a release this week if you could wait.</p>

          '
        raw: Hi, you have to use the latest dev version (from the main branch). There
          will be a release this week if you could wait.
        updatedAt: '2023-10-31T08:07:13.585Z'
      numEdits: 0
      reactions: []
    id: 6540b5b1eb904cbf23c9e4fc
    type: comment
  author: ydshieh
  content: Hi, you have to use the latest dev version (from the main branch). There
    will be a release this week if you could wait.
  created_at: 2023-10-31 07:07:13+00:00
  edited: false
  hidden: false
  id: 6540b5b1eb904cbf23c9e4fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2023-10-31T08:19:07.000Z'
    data:
      edited: false
      editors:
      - ydshieh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8949811458587646
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
          fullname: Yih-Dar SHIEH
          isHf: true
          isPro: false
          name: ydshieh
          type: user
        html: '<blockquote>

          <p>Also, could you provide an input example with interleaved text and multiple
          images? I am confused how to construct the input for an input sequence with
          text and multiple images. </p>

          </blockquote>

          <p>This is not shown explicitly in the paper IIRC. In the original Microsoft
          GitHub repository, there is some code regarding this, but not easy to run
          it to see what the format it has.<br>The current <code>Kosmos2Processor</code>
          is designed to handle <code>a text</code> or <code>an image with a text</code>,
          but not interleaved data.</p>

          <p>I will try to see what I can provide regarding part in the next few days.</p>

          '
        raw: "> Also, could you provide an input example with interleaved text and\
          \ multiple images? I am confused how to construct the input for an input\
          \ sequence with text and multiple images. \n\nThis is not shown explicitly\
          \ in the paper IIRC. In the original Microsoft GitHub repository, there\
          \ is some code regarding this, but not easy to run it to see what the format\
          \ it has.\nThe current `Kosmos2Processor` is designed to handle `a text`\
          \ or `an image with a text`, but not interleaved data.\n\nI will try to\
          \ see what I can provide regarding part in the next few days.\n\n"
        updatedAt: '2023-10-31T08:19:07.388Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - yingss
    id: 6540b87b1579bd6b0985fbfd
    type: comment
  author: ydshieh
  content: "> Also, could you provide an input example with interleaved text and multiple\
    \ images? I am confused how to construct the input for an input sequence with\
    \ text and multiple images. \n\nThis is not shown explicitly in the paper IIRC.\
    \ In the original Microsoft GitHub repository, there is some code regarding this,\
    \ but not easy to run it to see what the format it has.\nThe current `Kosmos2Processor`\
    \ is designed to handle `a text` or `an image with a text`, but not interleaved\
    \ data.\n\nI will try to see what I can provide regarding part in the next few\
    \ days.\n\n"
  created_at: 2023-10-31 07:19:07+00:00
  edited: false
  hidden: false
  id: 6540b87b1579bd6b0985fbfd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-10-31T13:12:47.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8405600786209106
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>The release will take place on Thursday <span data-props=\"{&quot;user&quot;:&quot;yingss&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/yingss\"\
          >@<span class=\"underline\">yingss</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'The release will take place on Thursday @yingss '
        updatedAt: '2023-10-31T13:12:47.283Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - yingss
    id: 6540fd4f99e887c0406a1e65
    type: comment
  author: lysandre
  content: 'The release will take place on Thursday @yingss '
  created_at: 2023-10-31 12:12:47+00:00
  edited: false
  hidden: false
  id: 6540fd4f99e887c0406a1e65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ffcece29f45101f81c760d49c7b4038.svg
      fullname: Ying Shen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yingss
      type: user
    createdAt: '2023-10-31T23:36:14.000Z'
    data:
      edited: false
      editors:
      - yingss
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8330053687095642
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ffcece29f45101f81c760d49c7b4038.svg
          fullname: Ying Shen
          isHf: false
          isPro: false
          name: yingss
          type: user
        html: "<blockquote>\n<p>The release will take place on Thursday <span data-props=\"\
          {&quot;user&quot;:&quot;yingss&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/yingss\">@<span class=\"underline\">yingss</span></a></span>\n\
          \n\t</span></span></p>\n</blockquote>\n<p>Will this release support <code>interleaved\
          \ text and multiple images</code>? </p>\n<p>I am mostly interested in the\
          \ capability of accepting <code>interleaved text and multiple images</code>\
          \ showcased in kosmos-1. However, I could not find the checkpoint for kosmos-1\
          \ in the <a rel=\"nofollow\" href=\"https://github.com/microsoft/unilm/blob/7ae2ee53bf7fff85e730c72083b7e999b0b9ba44/kosmos-1/README.md\"\
          >official repo</a> or huggingface. I am assuming kosmos-2 will have similar\
          \ capabilities in terms of handling interleaved text and multiple images?</p>\n"
        raw: "> The release will take place on Thursday @yingss\n\nWill this release\
          \ support `interleaved text and multiple images`? \n\nI am mostly interested\
          \ in the capability of accepting `interleaved text and multiple images`\
          \ showcased in kosmos-1. However, I could not find the checkpoint for kosmos-1\
          \ in the [official repo](https://github.com/microsoft/unilm/blob/7ae2ee53bf7fff85e730c72083b7e999b0b9ba44/kosmos-1/README.md)\
          \ or huggingface. I am assuming kosmos-2 will have similar capabilities\
          \ in terms of handling interleaved text and multiple images?"
        updatedAt: '2023-10-31T23:36:14.141Z'
      numEdits: 0
      reactions: []
    id: 65418f6e3c9ae87408480b54
    type: comment
  author: yingss
  content: "> The release will take place on Thursday @yingss\n\nWill this release\
    \ support `interleaved text and multiple images`? \n\nI am mostly interested in\
    \ the capability of accepting `interleaved text and multiple images` showcased\
    \ in kosmos-1. However, I could not find the checkpoint for kosmos-1 in the [official\
    \ repo](https://github.com/microsoft/unilm/blob/7ae2ee53bf7fff85e730c72083b7e999b0b9ba44/kosmos-1/README.md)\
    \ or huggingface. I am assuming kosmos-2 will have similar capabilities in terms\
    \ of handling interleaved text and multiple images?"
  created_at: 2023-10-31 22:36:14+00:00
  edited: false
  hidden: false
  id: 65418f6e3c9ae87408480b54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2023-11-01T16:50:14.000Z'
    data:
      edited: false
      editors:
      - ydshieh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8104070425033569
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
          fullname: Yih-Dar SHIEH
          isHf: true
          isPro: false
          name: ydshieh
          type: user
        html: '<blockquote>

          <p>I am assuming kosmos-2 will have similar capabilities in terms of handling
          interleaved text and multiple images?</p>

          </blockquote>

          <p><code>Kosmos-2</code> indeed is also trained on the interleaved data,
          but the official demo never shows how this is used, as you can see </p>

          <p><a rel="nofollow" href="https://github.com/microsoft/unilm/blob/7ae2ee53bf7fff85e730c72083b7e999b0b9ba44/kosmos-2/demo/gradio_app.py#L100C8-L100C9">https://github.com/microsoft/unilm/blob/7ae2ee53bf7fff85e730c72083b7e999b0b9ba44/kosmos-2/demo/gradio_app.py#L100C8-L100C9</a></p>

          <p>In <code>Kosmos-1</code> paper, they mentioned</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/OUDSINiow9uecsAm5DGyC.png"><img
          alt="1.png" src="https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/OUDSINiow9uecsAm5DGyC.png"></a></p>

          <p>and </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/Rwf1cFQTOyarHPEMYOT0Z.png"><img
          alt="2.png" src="https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/Rwf1cFQTOyarHPEMYOT0Z.png"></a></p>

          <p>I can provide a helper method to deal with this case, but so far it won''t
          be in the official release: I will post here in the next comment.</p>

          '
        raw: "> I am assuming kosmos-2 will have similar capabilities in terms of\
          \ handling interleaved text and multiple images?\n\n`Kosmos-2` indeed is\
          \ also trained on the interleaved data, but the official demo never shows\
          \ how this is used, as you can see \n\nhttps://github.com/microsoft/unilm/blob/7ae2ee53bf7fff85e730c72083b7e999b0b9ba44/kosmos-2/demo/gradio_app.py#L100C8-L100C9\n\
          \nIn `Kosmos-1` paper, they mentioned\n\n\n![1.png](https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/OUDSINiow9uecsAm5DGyC.png)\n\
          \nand \n\n\n![2.png](https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/Rwf1cFQTOyarHPEMYOT0Z.png)\n\
          \nI can provide a helper method to deal with this case, but so far it won't\
          \ be in the official release: I will post here in the next comment."
        updatedAt: '2023-11-01T16:50:14.865Z'
      numEdits: 0
      reactions: []
    id: 654281c62cfe8660a379ac5b
    type: comment
  author: ydshieh
  content: "> I am assuming kosmos-2 will have similar capabilities in terms of handling\
    \ interleaved text and multiple images?\n\n`Kosmos-2` indeed is also trained on\
    \ the interleaved data, but the official demo never shows how this is used, as\
    \ you can see \n\nhttps://github.com/microsoft/unilm/blob/7ae2ee53bf7fff85e730c72083b7e999b0b9ba44/kosmos-2/demo/gradio_app.py#L100C8-L100C9\n\
    \nIn `Kosmos-1` paper, they mentioned\n\n\n![1.png](https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/OUDSINiow9uecsAm5DGyC.png)\n\
    \nand \n\n\n![2.png](https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/Rwf1cFQTOyarHPEMYOT0Z.png)\n\
    \nI can provide a helper method to deal with this case, but so far it won't be\
    \ in the official release: I will post here in the next comment."
  created_at: 2023-11-01 15:50:14+00:00
  edited: false
  hidden: false
  id: 654281c62cfe8660a379ac5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ffcece29f45101f81c760d49c7b4038.svg
      fullname: Ying Shen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yingss
      type: user
    createdAt: '2023-11-01T16:59:46.000Z'
    data:
      edited: false
      editors:
      - yingss
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7873565554618835
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ffcece29f45101f81c760d49c7b4038.svg
          fullname: Ying Shen
          isHf: false
          isPro: false
          name: yingss
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I am assuming kosmos-2 will have similar capabilities in terms of handling
          interleaved text and multiple images?</p>

          </blockquote>

          <p><code>Kosmos-2</code> indeed is also trained on the interleaved data,
          but the official demo never shows how this is used, as you can see </p>

          <p><a rel="nofollow" href="https://github.com/microsoft/unilm/blob/7ae2ee53bf7fff85e730c72083b7e999b0b9ba44/kosmos-2/demo/gradio_app.py#L100C8-L100C9">https://github.com/microsoft/unilm/blob/7ae2ee53bf7fff85e730c72083b7e999b0b9ba44/kosmos-2/demo/gradio_app.py#L100C8-L100C9</a></p>

          <p>In <code>Kosmos-1</code> paper, they mentioned</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/OUDSINiow9uecsAm5DGyC.png"><img
          alt="1.png" src="https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/OUDSINiow9uecsAm5DGyC.png"></a></p>

          <p>and </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/Rwf1cFQTOyarHPEMYOT0Z.png"><img
          alt="2.png" src="https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/Rwf1cFQTOyarHPEMYOT0Z.png"></a></p>

          <p>I can provide a helper method to deal with this case, but so far it won''t
          be in the official release: I will post here in the next comment.</p>

          </blockquote>

          <p>Thank you so much!</p>

          '
        raw: "> > I am assuming kosmos-2 will have similar capabilities in terms of\
          \ handling interleaved text and multiple images?\n> \n> `Kosmos-2` indeed\
          \ is also trained on the interleaved data, but the official demo never shows\
          \ how this is used, as you can see \n> \n> https://github.com/microsoft/unilm/blob/7ae2ee53bf7fff85e730c72083b7e999b0b9ba44/kosmos-2/demo/gradio_app.py#L100C8-L100C9\n\
          > \n> In `Kosmos-1` paper, they mentioned\n> \n> \n> ![1.png](https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/OUDSINiow9uecsAm5DGyC.png)\n\
          > \n> and \n> \n> \n> ![2.png](https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/Rwf1cFQTOyarHPEMYOT0Z.png)\n\
          > \n> I can provide a helper method to deal with this case, but so far it\
          \ won't be in the official release: I will post here in the next comment.\n\
          \nThank you so much!"
        updatedAt: '2023-11-01T16:59:46.447Z'
      numEdits: 0
      reactions: []
    id: 65428402d1386fcfca332bf8
    type: comment
  author: yingss
  content: "> > I am assuming kosmos-2 will have similar capabilities in terms of\
    \ handling interleaved text and multiple images?\n> \n> `Kosmos-2` indeed is also\
    \ trained on the interleaved data, but the official demo never shows how this\
    \ is used, as you can see \n> \n> https://github.com/microsoft/unilm/blob/7ae2ee53bf7fff85e730c72083b7e999b0b9ba44/kosmos-2/demo/gradio_app.py#L100C8-L100C9\n\
    > \n> In `Kosmos-1` paper, they mentioned\n> \n> \n> ![1.png](https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/OUDSINiow9uecsAm5DGyC.png)\n\
    > \n> and \n> \n> \n> ![2.png](https://cdn-uploads.huggingface.co/production/uploads/6058c8cbcbe9c7542f3501ff/Rwf1cFQTOyarHPEMYOT0Z.png)\n\
    > \n> I can provide a helper method to deal with this case, but so far it won't\
    \ be in the official release: I will post here in the next comment.\n\nThank you\
    \ so much!"
  created_at: 2023-11-01 15:59:46+00:00
  edited: false
  hidden: false
  id: 65428402d1386fcfca332bf8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/18df5db7781e49c30af73445f5fef724.svg
      fullname: Victor Hall
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: panopstor
      type: user
    createdAt: '2023-11-03T03:39:23.000Z'
    data:
      edited: false
      editors:
      - panopstor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9603540897369385
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/18df5db7781e49c30af73445f5fef724.svg
          fullname: Victor Hall
          isHf: false
          isPro: false
          name: panopstor
          type: user
        html: '<p>Transformers 4.35.0 is released on pypi and works with kosmos-2.</p>

          '
        raw: Transformers 4.35.0 is released on pypi and works with kosmos-2.
        updatedAt: '2023-11-03T03:39:23.944Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - yingss
        - unilm
        - lysandre
        - zhaominxiao
    id: 65446b6b326cb9a32bf29184
    type: comment
  author: panopstor
  content: Transformers 4.35.0 is released on pypi and works with kosmos-2.
  created_at: 2023-11-03 02:39:23+00:00
  edited: false
  hidden: false
  id: 65446b6b326cb9a32bf29184
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2023-11-03T11:29:10.000Z'
    data:
      edited: false
      editors:
      - ydshieh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.36424872279167175
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
          fullname: Yih-Dar SHIEH
          isHf: true
          isPro: false
          name: ydshieh
          type: user
        html: "<p>I need to take a final look again, but the following should work<br>(remember\
          \ that this implementation is based what I see in the paper instead of an\
          \ original implementation!)</p>\n<p>The helper function</p>\n<pre><code>from\
          \ transformers import BatchFeature\n\n\ndef process_interleaved_example(processor,\
          \ prompt, images, placeholder=\"&lt;i&gt;\", num_image_tokens=64, add_special_tokens=True,\
          \ add_eos_token=False, return_tensors=None):\n\n    first_image_token_id\
          \ = processor.tokenizer.unk_token_id + 1\n\n    image_input_ids = [processor.tokenizer.convert_tokens_to_ids(processor.boi_token)]\
          \ + list(range(first_image_token_id, num_image_tokens + first_image_token_id))\
          \ + [processor.tokenizer.convert_tokens_to_ids(processor.eoi_token)]\n \
          \   image_attention_mask = [1] * len(image_input_ids)\n    # `-2`: not including\
          \ `boi` and `eoi`\n    image_embeds_position_mask = [0] + [1] * (len(image_input_ids)\
          \ - 2) + [0]\n\n    import re\n    components = re.split(rf\"({placeholder})\"\
          , prompt)\n\n    outputs = {\"input_ids\": [], \"attention_mask\": [], \"\
          image_embeds_position_mask\": []}\n    for component in components:\n  \
          \      if component != \"&lt;i&gt;\":\n            # add text tokens: no\
          \ special tokens -&gt; add them at the end\n            encoded = processor(text=component,\
          \ add_special_tokens=False)\n            for key in [\"input_ids\", \"attention_mask\"\
          ]:\n                outputs[key].extend(encoded[key])\n            outputs[\"\
          image_embeds_position_mask\"].extend([0] * len(encoded[\"input_ids\"]))\n\
          \        else:\n            # add tokens to indicate image placeholder\n\
          \            outputs[\"input_ids\"].extend(image_input_ids)\n          \
          \  outputs[\"attention_mask\"].extend(image_attention_mask)\n          \
          \  outputs[\"image_embeds_position_mask\"].extend(image_embeds_position_mask)\n\
          \n    if add_special_tokens:\n        outputs[\"input_ids\"] = [processor.tokenizer.bos_token_id]\
          \ + outputs[\"input_ids\"] + ([processor.tokenizer.eos_token_id] if add_eos_token\
          \ else [])\n        outputs[\"attention_mask\"] = [1] + outputs[\"attention_mask\"\
          ] + ([1] if add_eos_token else [])\n        outputs[\"image_embeds_position_mask\"\
          ] = [0] + outputs[\"image_embeds_position_mask\"] + ([0] if add_eos_token\
          \  else [])\n\n    outputs[\"pixel_values\"] = processor.image_processor(images).pixel_values\n\
          \n    for k in [\"input_ids\", \"attention_mask\", \"image_embeds_position_mask\"\
          ]:\n        outputs[k] = [outputs[k]]\n    outputs = BatchFeature(data=outputs,tensor_type=return_tensors)\n\
          \n    return outputs\n</code></pre>\n<p>An example use it:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">import</span> requests\n\
          <span class=\"hljs-keyword\">from</span> PIL <span class=\"hljs-keyword\"\
          >import</span> Image\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoProcessor, AutoModelForVision2Seq\n\
          \n\nurl_1 = <span class=\"hljs-string\">\"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\
          </span>\nimage_1 = Image.<span class=\"hljs-built_in\">open</span>(requests.get(url_1,\
          \ stream=<span class=\"hljs-literal\">True</span>).raw)\n\nurl_2 = <span\
          \ class=\"hljs-string\">\"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/two_dogs.jpg\"\
          </span>\nimage_2 = Image.<span class=\"hljs-built_in\">open</span>(requests.get(url_2,\
          \ stream=<span class=\"hljs-literal\">True</span>).raw)\n\nprocessor = AutoProcessor.from_pretrained(<span\
          \ class=\"hljs-string\">\"microsoft/kosmos-2-patch14-224\"</span>)\n\nprompt\
          \ = <span class=\"hljs-string\">\"&lt;grounding&gt; There are &lt;i&gt;\
          \ two dogs want to play with &lt;i&gt; this lonely snowman.\"</span>\ninputs\
          \ = process_interleaved_example(processor, prompt, images=[image_1, image_2],\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n<span class=\"\
          hljs-built_in\">print</span>(inputs)\n\ninputs = process_interleaved_example(processor,\
          \ prompt, images=[image_1, image_2], add_eos_token=<span class=\"hljs-literal\"\
          >True</span>, return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n\
          <span class=\"hljs-built_in\">print</span>(inputs)\n\nmodel = AutoModelForVision2Seq.from_pretrained(<span\
          \ class=\"hljs-string\">\"microsoft/kosmos-2-patch14-224\"</span>)\noutputs\
          \ = model(**inputs)\n<span class=\"hljs-built_in\">print</span>(outputs[<span\
          \ class=\"hljs-number\">0</span>].shape)\n</code></pre>\n"
        raw: "I need to take a final look again, but the following should work\n(remember\
          \ that this implementation is based what I see in the paper instead of an\
          \ original implementation!)\n\nThe helper function\n```\nfrom transformers\
          \ import BatchFeature\n\n\ndef process_interleaved_example(processor, prompt,\
          \ images, placeholder=\"<i>\", num_image_tokens=64, add_special_tokens=True,\
          \ add_eos_token=False, return_tensors=None):\n\n    first_image_token_id\
          \ = processor.tokenizer.unk_token_id + 1\n\n    image_input_ids = [processor.tokenizer.convert_tokens_to_ids(processor.boi_token)]\
          \ + list(range(first_image_token_id, num_image_tokens + first_image_token_id))\
          \ + [processor.tokenizer.convert_tokens_to_ids(processor.eoi_token)]\n \
          \   image_attention_mask = [1] * len(image_input_ids)\n    # `-2`: not including\
          \ `boi` and `eoi`\n    image_embeds_position_mask = [0] + [1] * (len(image_input_ids)\
          \ - 2) + [0]\n\n    import re\n    components = re.split(rf\"({placeholder})\"\
          , prompt)\n\n    outputs = {\"input_ids\": [], \"attention_mask\": [], \"\
          image_embeds_position_mask\": []}\n    for component in components:\n  \
          \      if component != \"<i>\":\n            # add text tokens: no special\
          \ tokens -> add them at the end\n            encoded = processor(text=component,\
          \ add_special_tokens=False)\n            for key in [\"input_ids\", \"attention_mask\"\
          ]:\n                outputs[key].extend(encoded[key])\n            outputs[\"\
          image_embeds_position_mask\"].extend([0] * len(encoded[\"input_ids\"]))\n\
          \        else:\n            # add tokens to indicate image placeholder\n\
          \            outputs[\"input_ids\"].extend(image_input_ids)\n          \
          \  outputs[\"attention_mask\"].extend(image_attention_mask)\n          \
          \  outputs[\"image_embeds_position_mask\"].extend(image_embeds_position_mask)\n\
          \n    if add_special_tokens:\n        outputs[\"input_ids\"] = [processor.tokenizer.bos_token_id]\
          \ + outputs[\"input_ids\"] + ([processor.tokenizer.eos_token_id] if add_eos_token\
          \ else [])\n        outputs[\"attention_mask\"] = [1] + outputs[\"attention_mask\"\
          ] + ([1] if add_eos_token else [])\n        outputs[\"image_embeds_position_mask\"\
          ] = [0] + outputs[\"image_embeds_position_mask\"] + ([0] if add_eos_token\
          \  else [])\n\n    outputs[\"pixel_values\"] = processor.image_processor(images).pixel_values\n\
          \n    for k in [\"input_ids\", \"attention_mask\", \"image_embeds_position_mask\"\
          ]:\n        outputs[k] = [outputs[k]]\n    outputs = BatchFeature(data=outputs,tensor_type=return_tensors)\n\
          \n    return outputs\n```\n\nAn example use it:\n```python\nimport requests\n\
          from PIL import Image\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\n\
          \n\nurl_1 = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\
          \nimage_1 = Image.open(requests.get(url_1, stream=True).raw)\n\nurl_2 =\
          \ \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/two_dogs.jpg\"\
          \nimage_2 = Image.open(requests.get(url_2, stream=True).raw)\n\nprocessor\
          \ = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n\n\
          prompt = \"<grounding> There are <i> two dogs want to play with <i> this\
          \ lonely snowman.\"\ninputs = process_interleaved_example(processor, prompt,\
          \ images=[image_1, image_2], return_tensors=\"pt\")\nprint(inputs)\n\ninputs\
          \ = process_interleaved_example(processor, prompt, images=[image_1, image_2],\
          \ add_eos_token=True, return_tensors=\"pt\")\nprint(inputs)\n\nmodel = AutoModelForVision2Seq.from_pretrained(\"\
          microsoft/kosmos-2-patch14-224\")\noutputs = model(**inputs)\nprint(outputs[0].shape)\n\
          ```"
        updatedAt: '2023-11-03T11:29:10.148Z'
      numEdits: 0
      reactions: []
    id: 6544d9867ffa6819df28164a
    type: comment
  author: ydshieh
  content: "I need to take a final look again, but the following should work\n(remember\
    \ that this implementation is based what I see in the paper instead of an original\
    \ implementation!)\n\nThe helper function\n```\nfrom transformers import BatchFeature\n\
    \n\ndef process_interleaved_example(processor, prompt, images, placeholder=\"\
    <i>\", num_image_tokens=64, add_special_tokens=True, add_eos_token=False, return_tensors=None):\n\
    \n    first_image_token_id = processor.tokenizer.unk_token_id + 1\n\n    image_input_ids\
    \ = [processor.tokenizer.convert_tokens_to_ids(processor.boi_token)] + list(range(first_image_token_id,\
    \ num_image_tokens + first_image_token_id)) + [processor.tokenizer.convert_tokens_to_ids(processor.eoi_token)]\n\
    \    image_attention_mask = [1] * len(image_input_ids)\n    # `-2`: not including\
    \ `boi` and `eoi`\n    image_embeds_position_mask = [0] + [1] * (len(image_input_ids)\
    \ - 2) + [0]\n\n    import re\n    components = re.split(rf\"({placeholder})\"\
    , prompt)\n\n    outputs = {\"input_ids\": [], \"attention_mask\": [], \"image_embeds_position_mask\"\
    : []}\n    for component in components:\n        if component != \"<i>\":\n  \
    \          # add text tokens: no special tokens -> add them at the end\n     \
    \       encoded = processor(text=component, add_special_tokens=False)\n      \
    \      for key in [\"input_ids\", \"attention_mask\"]:\n                outputs[key].extend(encoded[key])\n\
    \            outputs[\"image_embeds_position_mask\"].extend([0] * len(encoded[\"\
    input_ids\"]))\n        else:\n            # add tokens to indicate image placeholder\n\
    \            outputs[\"input_ids\"].extend(image_input_ids)\n            outputs[\"\
    attention_mask\"].extend(image_attention_mask)\n            outputs[\"image_embeds_position_mask\"\
    ].extend(image_embeds_position_mask)\n\n    if add_special_tokens:\n        outputs[\"\
    input_ids\"] = [processor.tokenizer.bos_token_id] + outputs[\"input_ids\"] + ([processor.tokenizer.eos_token_id]\
    \ if add_eos_token else [])\n        outputs[\"attention_mask\"] = [1] + outputs[\"\
    attention_mask\"] + ([1] if add_eos_token else [])\n        outputs[\"image_embeds_position_mask\"\
    ] = [0] + outputs[\"image_embeds_position_mask\"] + ([0] if add_eos_token  else\
    \ [])\n\n    outputs[\"pixel_values\"] = processor.image_processor(images).pixel_values\n\
    \n    for k in [\"input_ids\", \"attention_mask\", \"image_embeds_position_mask\"\
    ]:\n        outputs[k] = [outputs[k]]\n    outputs = BatchFeature(data=outputs,tensor_type=return_tensors)\n\
    \n    return outputs\n```\n\nAn example use it:\n```python\nimport requests\n\
    from PIL import Image\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\n\
    \n\nurl_1 = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\
    \nimage_1 = Image.open(requests.get(url_1, stream=True).raw)\n\nurl_2 = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/two_dogs.jpg\"\
    \nimage_2 = Image.open(requests.get(url_2, stream=True).raw)\n\nprocessor = AutoProcessor.from_pretrained(\"\
    microsoft/kosmos-2-patch14-224\")\n\nprompt = \"<grounding> There are <i> two\
    \ dogs want to play with <i> this lonely snowman.\"\ninputs = process_interleaved_example(processor,\
    \ prompt, images=[image_1, image_2], return_tensors=\"pt\")\nprint(inputs)\n\n\
    inputs = process_interleaved_example(processor, prompt, images=[image_1, image_2],\
    \ add_eos_token=True, return_tensors=\"pt\")\nprint(inputs)\n\nmodel = AutoModelForVision2Seq.from_pretrained(\"\
    microsoft/kosmos-2-patch14-224\")\noutputs = model(**inputs)\nprint(outputs[0].shape)\n\
    ```"
  created_at: 2023-11-03 10:29:10+00:00
  edited: false
  hidden: false
  id: 6544d9867ffa6819df28164a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ffcece29f45101f81c760d49c7b4038.svg
      fullname: Ying Shen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yingss
      type: user
    createdAt: '2023-11-03T16:58:48.000Z'
    data:
      edited: false
      editors:
      - yingss
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3528430759906769
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ffcece29f45101f81c760d49c7b4038.svg
          fullname: Ying Shen
          isHf: false
          isPro: false
          name: yingss
          type: user
        html: "<blockquote>\n<p>I need to take a final look again, but the following\
          \ should work<br>(remember that this implementation is based what I see\
          \ in the paper instead of an original implementation!)</p>\n<p>The helper\
          \ function</p>\n<pre><code>from transformers import BatchFeature\n\n\ndef\
          \ process_interleaved_example(processor, prompt, images, placeholder=\"\
          &lt;i&gt;\", num_image_tokens=64, add_special_tokens=True, add_eos_token=False,\
          \ return_tensors=None):\n\n    first_image_token_id = processor.tokenizer.unk_token_id\
          \ + 1\n\n    image_input_ids = [processor.tokenizer.convert_tokens_to_ids(processor.boi_token)]\
          \ + list(range(first_image_token_id, num_image_tokens + first_image_token_id))\
          \ + [processor.tokenizer.convert_tokens_to_ids(processor.eoi_token)]\n \
          \   image_attention_mask = [1] * len(image_input_ids)\n    # `-2`: not including\
          \ `boi` and `eoi`\n    image_embeds_position_mask = [0] + [1] * (len(image_input_ids)\
          \ - 2) + [0]\n\n    import re\n    components = re.split(rf\"({placeholder})\"\
          , prompt)\n\n    outputs = {\"input_ids\": [], \"attention_mask\": [], \"\
          image_embeds_position_mask\": []}\n    for component in components:\n  \
          \      if component != \"&lt;i&gt;\":\n            # add text tokens: no\
          \ special tokens -&gt; add them at the end\n            encoded = processor(text=component,\
          \ add_special_tokens=False)\n            for key in [\"input_ids\", \"attention_mask\"\
          ]:\n                outputs[key].extend(encoded[key])\n            outputs[\"\
          image_embeds_position_mask\"].extend([0] * len(encoded[\"input_ids\"]))\n\
          \        else:\n            # add tokens to indicate image placeholder\n\
          \            outputs[\"input_ids\"].extend(image_input_ids)\n          \
          \  outputs[\"attention_mask\"].extend(image_attention_mask)\n          \
          \  outputs[\"image_embeds_position_mask\"].extend(image_embeds_position_mask)\n\
          \n    if add_special_tokens:\n        outputs[\"input_ids\"] = [processor.tokenizer.bos_token_id]\
          \ + outputs[\"input_ids\"] + ([processor.tokenizer.eos_token_id] if add_eos_token\
          \ else [])\n        outputs[\"attention_mask\"] = [1] + outputs[\"attention_mask\"\
          ] + ([1] if add_eos_token else [])\n        outputs[\"image_embeds_position_mask\"\
          ] = [0] + outputs[\"image_embeds_position_mask\"] + ([0] if add_eos_token\
          \  else [])\n\n    outputs[\"pixel_values\"] = processor.image_processor(images).pixel_values\n\
          \n    for k in [\"input_ids\", \"attention_mask\", \"image_embeds_position_mask\"\
          ]:\n        outputs[k] = [outputs[k]]\n    outputs = BatchFeature(data=outputs,tensor_type=return_tensors)\n\
          \n    return outputs\n</code></pre>\n<p>An example use it:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">import</span> requests\n\
          <span class=\"hljs-keyword\">from</span> PIL <span class=\"hljs-keyword\"\
          >import</span> Image\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoProcessor, AutoModelForVision2Seq\n\
          \n\nurl_1 = <span class=\"hljs-string\">\"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\
          </span>\nimage_1 = Image.<span class=\"hljs-built_in\">open</span>(requests.get(url_1,\
          \ stream=<span class=\"hljs-literal\">True</span>).raw)\n\nurl_2 = <span\
          \ class=\"hljs-string\">\"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/two_dogs.jpg\"\
          </span>\nimage_2 = Image.<span class=\"hljs-built_in\">open</span>(requests.get(url_2,\
          \ stream=<span class=\"hljs-literal\">True</span>).raw)\n\nprocessor = AutoProcessor.from_pretrained(<span\
          \ class=\"hljs-string\">\"microsoft/kosmos-2-patch14-224\"</span>)\n\nprompt\
          \ = <span class=\"hljs-string\">\"&lt;grounding&gt; There are &lt;i&gt;\
          \ two dogs want to play with &lt;i&gt; this lonely snowman.\"</span>\ninputs\
          \ = process_interleaved_example(processor, prompt, images=[image_1, image_2],\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n<span class=\"\
          hljs-built_in\">print</span>(inputs)\n\ninputs = process_interleaved_example(processor,\
          \ prompt, images=[image_1, image_2], add_eos_token=<span class=\"hljs-literal\"\
          >True</span>, return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n\
          <span class=\"hljs-built_in\">print</span>(inputs)\n\nmodel = AutoModelForVision2Seq.from_pretrained(<span\
          \ class=\"hljs-string\">\"microsoft/kosmos-2-patch14-224\"</span>)\noutputs\
          \ = model(**inputs)\n<span class=\"hljs-built_in\">print</span>(outputs[<span\
          \ class=\"hljs-number\">0</span>].shape)\n</code></pre>\n</blockquote>\n\
          <p>Thank you so much!!</p>\n"
        raw: "> I need to take a final look again, but the following should work\n\
          > (remember that this implementation is based what I see in the paper instead\
          \ of an original implementation!)\n> \n> The helper function\n> ```\n> from\
          \ transformers import BatchFeature\n> \n> \n> def process_interleaved_example(processor,\
          \ prompt, images, placeholder=\"<i>\", num_image_tokens=64, add_special_tokens=True,\
          \ add_eos_token=False, return_tensors=None):\n> \n>     first_image_token_id\
          \ = processor.tokenizer.unk_token_id + 1\n> \n>     image_input_ids = [processor.tokenizer.convert_tokens_to_ids(processor.boi_token)]\
          \ + list(range(first_image_token_id, num_image_tokens + first_image_token_id))\
          \ + [processor.tokenizer.convert_tokens_to_ids(processor.eoi_token)]\n>\
          \     image_attention_mask = [1] * len(image_input_ids)\n>     # `-2`: not\
          \ including `boi` and `eoi`\n>     image_embeds_position_mask = [0] + [1]\
          \ * (len(image_input_ids) - 2) + [0]\n> \n>     import re\n>     components\
          \ = re.split(rf\"({placeholder})\", prompt)\n> \n>     outputs = {\"input_ids\"\
          : [], \"attention_mask\": [], \"image_embeds_position_mask\": []}\n>   \
          \  for component in components:\n>         if component != \"<i>\":\n> \
          \            # add text tokens: no special tokens -> add them at the end\n\
          >             encoded = processor(text=component, add_special_tokens=False)\n\
          >             for key in [\"input_ids\", \"attention_mask\"]:\n>       \
          \          outputs[key].extend(encoded[key])\n>             outputs[\"image_embeds_position_mask\"\
          ].extend([0] * len(encoded[\"input_ids\"]))\n>         else:\n>        \
          \     # add tokens to indicate image placeholder\n>             outputs[\"\
          input_ids\"].extend(image_input_ids)\n>             outputs[\"attention_mask\"\
          ].extend(image_attention_mask)\n>             outputs[\"image_embeds_position_mask\"\
          ].extend(image_embeds_position_mask)\n> \n>     if add_special_tokens:\n\
          >         outputs[\"input_ids\"] = [processor.tokenizer.bos_token_id] +\
          \ outputs[\"input_ids\"] + ([processor.tokenizer.eos_token_id] if add_eos_token\
          \ else [])\n>         outputs[\"attention_mask\"] = [1] + outputs[\"attention_mask\"\
          ] + ([1] if add_eos_token else [])\n>         outputs[\"image_embeds_position_mask\"\
          ] = [0] + outputs[\"image_embeds_position_mask\"] + ([0] if add_eos_token\
          \  else [])\n> \n>     outputs[\"pixel_values\"] = processor.image_processor(images).pixel_values\n\
          > \n>     for k in [\"input_ids\", \"attention_mask\", \"image_embeds_position_mask\"\
          ]:\n>         outputs[k] = [outputs[k]]\n>     outputs = BatchFeature(data=outputs,tensor_type=return_tensors)\n\
          > \n>     return outputs\n> ```\n> \n> An example use it:\n> ```python\n\
          > import requests\n> from PIL import Image\n> from transformers import AutoProcessor,\
          \ AutoModelForVision2Seq\n> \n> \n> url_1 = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\
          \n> image_1 = Image.open(requests.get(url_1, stream=True).raw)\n> \n> url_2\
          \ = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/two_dogs.jpg\"\
          \n> image_2 = Image.open(requests.get(url_2, stream=True).raw)\n> \n> processor\
          \ = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n\
          > \n> prompt = \"<grounding> There are <i> two dogs want to play with <i>\
          \ this lonely snowman.\"\n> inputs = process_interleaved_example(processor,\
          \ prompt, images=[image_1, image_2], return_tensors=\"pt\")\n> print(inputs)\n\
          > \n> inputs = process_interleaved_example(processor, prompt, images=[image_1,\
          \ image_2], add_eos_token=True, return_tensors=\"pt\")\n> print(inputs)\n\
          > \n> model = AutoModelForVision2Seq.from_pretrained(\"microsoft/kosmos-2-patch14-224\"\
          )\n> outputs = model(**inputs)\n> print(outputs[0].shape)\n> ```\n\nThank\
          \ you so much!!"
        updatedAt: '2023-11-03T16:58:48.872Z'
      numEdits: 0
      reactions: []
    id: 654526c8f2b9cff9e96eb46a
    type: comment
  author: yingss
  content: "> I need to take a final look again, but the following should work\n>\
    \ (remember that this implementation is based what I see in the paper instead\
    \ of an original implementation!)\n> \n> The helper function\n> ```\n> from transformers\
    \ import BatchFeature\n> \n> \n> def process_interleaved_example(processor, prompt,\
    \ images, placeholder=\"<i>\", num_image_tokens=64, add_special_tokens=True, add_eos_token=False,\
    \ return_tensors=None):\n> \n>     first_image_token_id = processor.tokenizer.unk_token_id\
    \ + 1\n> \n>     image_input_ids = [processor.tokenizer.convert_tokens_to_ids(processor.boi_token)]\
    \ + list(range(first_image_token_id, num_image_tokens + first_image_token_id))\
    \ + [processor.tokenizer.convert_tokens_to_ids(processor.eoi_token)]\n>     image_attention_mask\
    \ = [1] * len(image_input_ids)\n>     # `-2`: not including `boi` and `eoi`\n\
    >     image_embeds_position_mask = [0] + [1] * (len(image_input_ids) - 2) + [0]\n\
    > \n>     import re\n>     components = re.split(rf\"({placeholder})\", prompt)\n\
    > \n>     outputs = {\"input_ids\": [], \"attention_mask\": [], \"image_embeds_position_mask\"\
    : []}\n>     for component in components:\n>         if component != \"<i>\":\n\
    >             # add text tokens: no special tokens -> add them at the end\n> \
    \            encoded = processor(text=component, add_special_tokens=False)\n>\
    \             for key in [\"input_ids\", \"attention_mask\"]:\n>             \
    \    outputs[key].extend(encoded[key])\n>             outputs[\"image_embeds_position_mask\"\
    ].extend([0] * len(encoded[\"input_ids\"]))\n>         else:\n>             #\
    \ add tokens to indicate image placeholder\n>             outputs[\"input_ids\"\
    ].extend(image_input_ids)\n>             outputs[\"attention_mask\"].extend(image_attention_mask)\n\
    >             outputs[\"image_embeds_position_mask\"].extend(image_embeds_position_mask)\n\
    > \n>     if add_special_tokens:\n>         outputs[\"input_ids\"] = [processor.tokenizer.bos_token_id]\
    \ + outputs[\"input_ids\"] + ([processor.tokenizer.eos_token_id] if add_eos_token\
    \ else [])\n>         outputs[\"attention_mask\"] = [1] + outputs[\"attention_mask\"\
    ] + ([1] if add_eos_token else [])\n>         outputs[\"image_embeds_position_mask\"\
    ] = [0] + outputs[\"image_embeds_position_mask\"] + ([0] if add_eos_token  else\
    \ [])\n> \n>     outputs[\"pixel_values\"] = processor.image_processor(images).pixel_values\n\
    > \n>     for k in [\"input_ids\", \"attention_mask\", \"image_embeds_position_mask\"\
    ]:\n>         outputs[k] = [outputs[k]]\n>     outputs = BatchFeature(data=outputs,tensor_type=return_tensors)\n\
    > \n>     return outputs\n> ```\n> \n> An example use it:\n> ```python\n> import\
    \ requests\n> from PIL import Image\n> from transformers import AutoProcessor,\
    \ AutoModelForVision2Seq\n> \n> \n> url_1 = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\
    \n> image_1 = Image.open(requests.get(url_1, stream=True).raw)\n> \n> url_2 =\
    \ \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/two_dogs.jpg\"\
    \n> image_2 = Image.open(requests.get(url_2, stream=True).raw)\n> \n> processor\
    \ = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n> \n> prompt\
    \ = \"<grounding> There are <i> two dogs want to play with <i> this lonely snowman.\"\
    \n> inputs = process_interleaved_example(processor, prompt, images=[image_1, image_2],\
    \ return_tensors=\"pt\")\n> print(inputs)\n> \n> inputs = process_interleaved_example(processor,\
    \ prompt, images=[image_1, image_2], add_eos_token=True, return_tensors=\"pt\"\
    )\n> print(inputs)\n> \n> model = AutoModelForVision2Seq.from_pretrained(\"microsoft/kosmos-2-patch14-224\"\
    )\n> outputs = model(**inputs)\n> print(outputs[0].shape)\n> ```\n\nThank you\
    \ so much!!"
  created_at: 2023-11-03 15:58:48+00:00
  edited: false
  hidden: false
  id: 654526c8f2b9cff9e96eb46a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f9e667280458fae9a58b84ac236af9fb.svg
      fullname: zhaomin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhaominxiao
      type: user
    createdAt: '2023-11-19T19:04:01.000Z'
    data:
      edited: false
      editors:
      - zhaominxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8786781430244446
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f9e667280458fae9a58b84ac236af9fb.svg
          fullname: zhaomin
          isHf: false
          isPro: false
          name: zhaominxiao
          type: user
        html: '<p>Could you please provide the implementation for decoding?</p>

          '
        raw: Could you please provide the implementation for decoding?
        updatedAt: '2023-11-19T19:04:01.711Z'
      numEdits: 0
      reactions: []
    id: 655a5c21ed8df8312844cc75
    type: comment
  author: zhaominxiao
  content: Could you please provide the implementation for decoding?
  created_at: 2023-11-19 19:04:01+00:00
  edited: false
  hidden: false
  id: 655a5c21ed8df8312844cc75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2023-11-19T19:05:34.000Z'
    data:
      edited: false
      editors:
      - ydshieh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8332387208938599
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
          fullname: Yih-Dar SHIEH
          isHf: true
          isPro: false
          name: ydshieh
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;zhaominxiao&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/zhaominxiao\"\
          >@<span class=\"underline\">zhaominxiao</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>The code example in the model card should work well</p>\n<p><a\
          \ href=\"https://huggingface.co/microsoft/kosmos-2-patch14-224\">https://huggingface.co/microsoft/kosmos-2-patch14-224</a></p>\n\
          <p>but let me know if there is anything missing</p>\n"
        raw: "@zhaominxiao \n\nThe code example in the model card should work well\n\
          \nhttps://huggingface.co/microsoft/kosmos-2-patch14-224\n\nbut let me know\
          \ if there is anything missing"
        updatedAt: '2023-11-19T19:05:34.580Z'
      numEdits: 0
      reactions: []
    id: 655a5c7e3ff5ba1b1b29c191
    type: comment
  author: ydshieh
  content: "@zhaominxiao \n\nThe code example in the model card should work well\n\
    \nhttps://huggingface.co/microsoft/kosmos-2-patch14-224\n\nbut let me know if\
    \ there is anything missing"
  created_at: 2023-11-19 19:05:34+00:00
  edited: false
  hidden: false
  id: 655a5c7e3ff5ba1b1b29c191
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f9e667280458fae9a58b84ac236af9fb.svg
      fullname: zhaomin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhaominxiao
      type: user
    createdAt: '2023-11-19T19:13:09.000Z'
    data:
      edited: false
      editors:
      - zhaominxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6005192399024963
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f9e667280458fae9a58b84ac236af9fb.svg
          fullname: zhaomin
          isHf: false
          isPro: false
          name: zhaominxiao
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;zhaominxiao&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/zhaominxiao\"\
          >@<span class=\"underline\">zhaominxiao</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>The code example in the model card should work well</p>\n<p><a\
          \ href=\"https://huggingface.co/microsoft/kosmos-2-patch14-224\">https://huggingface.co/microsoft/kosmos-2-patch14-224</a></p>\n\
          <p>but let me know if there is anything missing</p>\n</blockquote>\n<p>Thanks\
          \ for your prompt response. Yes, the code can generate the output tensor.\
          \ But when I tried to use the method in the README file to decode the output\
          \ tensor, it returned the error saying \"TypeError: PreTrainedTokenizerBase.decode()\
          \ missing 1 required positional argument: 'token_ids'.\"</p>\n<p>The decoding\
          \ method I used is as follows.</p>\n<pre><code>import requests\nfrom PIL\
          \ import Image\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\n\
          \n\nurl_1 = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\
          \nimage_1 = Image.open(requests.get(url_1, stream=True).raw)\n\nurl_2 =\
          \ \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/two_dogs.jpg\"\
          \nimage_2 = Image.open(requests.get(url_2, stream=True).raw)\n\nprompt =\
          \ \"&lt;grounding&gt; There are &lt;i&gt; two dogs want to play with &lt;i&gt;\
          \ this lonely snowman.\"\ninputs = process_interleaved_example(processor,\
          \ prompt, images=[image_1, image_2], return_tensors=\"pt\")\nprint(inputs)\n\
          \ninputs = process_interleaved_example(processor, prompt, images=[image_1,\
          \ image_2], add_eos_token=True, return_tensors=\"pt\").to('cuda')\nprint(inputs)\n\
          \noutputs = model(**inputs)\nprint(outputs[0].shape)\n\ngenerated_text =\
          \ processor.decode(**outputs, skip_special_tokens=True)[0]\nprocessed_text\
          \ = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n\
          processed_text, _ = processor.post_process_generation(generated_text)\n\
          </code></pre>\n"
        raw: "> @zhaominxiao \n> \n> The code example in the model card should work\
          \ well\n> \n> https://huggingface.co/microsoft/kosmos-2-patch14-224\n> \n\
          > but let me know if there is anything missing\n\nThanks for your prompt\
          \ response. Yes, the code can generate the output tensor. But when I tried\
          \ to use the method in the README file to decode the output tensor, it returned\
          \ the error saying \"TypeError: PreTrainedTokenizerBase.decode() missing\
          \ 1 required positional argument: 'token_ids'.\"\n\nThe decoding method\
          \ I used is as follows.\n\n```\nimport requests\nfrom PIL import Image\n\
          from transformers import AutoProcessor, AutoModelForVision2Seq\n\n\nurl_1\
          \ = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\
          \nimage_1 = Image.open(requests.get(url_1, stream=True).raw)\n\nurl_2 =\
          \ \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/two_dogs.jpg\"\
          \nimage_2 = Image.open(requests.get(url_2, stream=True).raw)\n\nprompt =\
          \ \"<grounding> There are <i> two dogs want to play with <i> this lonely\
          \ snowman.\"\ninputs = process_interleaved_example(processor, prompt, images=[image_1,\
          \ image_2], return_tensors=\"pt\")\nprint(inputs)\n\ninputs = process_interleaved_example(processor,\
          \ prompt, images=[image_1, image_2], add_eos_token=True, return_tensors=\"\
          pt\").to('cuda')\nprint(inputs)\n\noutputs = model(**inputs)\nprint(outputs[0].shape)\n\
          \ngenerated_text = processor.decode(**outputs, skip_special_tokens=True)[0]\n\
          processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n\
          processed_text, _ = processor.post_process_generation(generated_text)\n\
          ```"
        updatedAt: '2023-11-19T19:13:09.981Z'
      numEdits: 0
      reactions: []
    id: 655a5e457f246643397aa486
    type: comment
  author: zhaominxiao
  content: "> @zhaominxiao \n> \n> The code example in the model card should work\
    \ well\n> \n> https://huggingface.co/microsoft/kosmos-2-patch14-224\n> \n> but\
    \ let me know if there is anything missing\n\nThanks for your prompt response.\
    \ Yes, the code can generate the output tensor. But when I tried to use the method\
    \ in the README file to decode the output tensor, it returned the error saying\
    \ \"TypeError: PreTrainedTokenizerBase.decode() missing 1 required positional\
    \ argument: 'token_ids'.\"\n\nThe decoding method I used is as follows.\n\n```\n\
    import requests\nfrom PIL import Image\nfrom transformers import AutoProcessor,\
    \ AutoModelForVision2Seq\n\n\nurl_1 = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\
    \nimage_1 = Image.open(requests.get(url_1, stream=True).raw)\n\nurl_2 = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/two_dogs.jpg\"\
    \nimage_2 = Image.open(requests.get(url_2, stream=True).raw)\n\nprompt = \"<grounding>\
    \ There are <i> two dogs want to play with <i> this lonely snowman.\"\ninputs\
    \ = process_interleaved_example(processor, prompt, images=[image_1, image_2],\
    \ return_tensors=\"pt\")\nprint(inputs)\n\ninputs = process_interleaved_example(processor,\
    \ prompt, images=[image_1, image_2], add_eos_token=True, return_tensors=\"pt\"\
    ).to('cuda')\nprint(inputs)\n\noutputs = model(**inputs)\nprint(outputs[0].shape)\n\
    \ngenerated_text = processor.decode(**outputs, skip_special_tokens=True)[0]\n\
    processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n\
    processed_text, _ = processor.post_process_generation(generated_text)\n```"
  created_at: 2023-11-19 19:13:09+00:00
  edited: false
  hidden: false
  id: 655a5e457f246643397aa486
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2023-11-19T20:30:26.000Z'
    data:
      edited: false
      editors:
      - ydshieh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8948469758033752
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
          fullname: Yih-Dar SHIEH
          isHf: true
          isPro: false
          name: ydshieh
          type: user
        html: '<p>Hi, first : I am not sure if you really intend to use <code>model(**inputs)</code>
          instead of <code>model.generate</code>.</p>

          <p><code>model(**inputs)</code> gives <code>outputs</code> as something
          like dictionary, and <code>processor.decode</code> expect a list of token
          ids.</p>

          <p>I would suggest you follow the code example to use  <code>model.generate</code>
          and see how it use its <code>outputs</code> to decode.</p>

          <p>If you intend to use <code>model(**inputs)</code>, you will have to do
          extra work to make it work, for which I won''t have the bandwidth to help.</p>

          '
        raw: 'Hi, first : I am not sure if you really intend to use `model(**inputs)`
          instead of `model.generate`.


          `model(**inputs)` gives `outputs` as something like dictionary, and `processor.decode`
          expect a list of token ids.


          I would suggest you follow the code example to use  `model.generate` and
          see how it use its `outputs` to decode.


          If you intend to use `model(**inputs)`, you will have to do extra work to
          make it work, for which I won''t have the bandwidth to help.'
        updatedAt: '2023-11-19T20:30:26.276Z'
      numEdits: 0
      reactions: []
    id: 655a7062c68499a1b97fcd22
    type: comment
  author: ydshieh
  content: 'Hi, first : I am not sure if you really intend to use `model(**inputs)`
    instead of `model.generate`.


    `model(**inputs)` gives `outputs` as something like dictionary, and `processor.decode`
    expect a list of token ids.


    I would suggest you follow the code example to use  `model.generate` and see how
    it use its `outputs` to decode.


    If you intend to use `model(**inputs)`, you will have to do extra work to make
    it work, for which I won''t have the bandwidth to help.'
  created_at: 2023-11-19 20:30:26+00:00
  edited: false
  hidden: false
  id: 655a7062c68499a1b97fcd22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f9e667280458fae9a58b84ac236af9fb.svg
      fullname: zhaomin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhaominxiao
      type: user
    createdAt: '2023-11-19T20:53:56.000Z'
    data:
      edited: false
      editors:
      - zhaominxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9051306247711182
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f9e667280458fae9a58b84ac236af9fb.svg
          fullname: zhaomin
          isHf: false
          isPro: false
          name: zhaominxiao
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ydshieh&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ydshieh\">@<span class=\"\
          underline\">ydshieh</span></a></span>\n\n\t</span></span><br>Thanks for\
          \ your help. I am totally ok with using <code>model.generate</code>. I checked\
          \ the code example in the model card. It is about how to use one image and\
          \ one text sequence to do visual question answering (or image captioning).\
          \ My use case needs the model to consume interleaved text-and-image sequences.\
          \ For example, to do the few-shot learning, I need to provide multiple examples\
          \ before asking the \"real\" question. In this case, I need to make sure\
          \ the text and images are presented to KOSMOS in order. I went through your\
          \ helper function implementation and my understanding is that with the dictionary\
          \ returned by your helper function, I can use its <code>input_ids</code>,\
          \ <code>attention_mask</code>, and <code>image_embeds_position_mask</code>\
          \ as the value of the parameters in <code>model.generate</code>. But I am\
          \ not sure how I can set the <code>pixel_values</code> and <code>image_embeds</code>.\
          \ Should I set them as <code>None</code>?</p>\n"
        raw: "@ydshieh \nThanks for your help. I am totally ok with using `model.generate`.\
          \ I checked the code example in the model card. It is about how to use one\
          \ image and one text sequence to do visual question answering (or image\
          \ captioning). My use case needs the model to consume interleaved text-and-image\
          \ sequences. For example, to do the few-shot learning, I need to provide\
          \ multiple examples before asking the \"real\" question. In this case, I\
          \ need to make sure the text and images are presented to KOSMOS in order.\
          \ I went through your helper function implementation and my understanding\
          \ is that with the dictionary returned by your helper function, I can use\
          \ its `input_ids`, `attention_mask`, and `image_embeds_position_mask` as\
          \ the value of the parameters in `model.generate`. But I am not sure how\
          \ I can set the `pixel_values` and `image_embeds`. Should I set them as\
          \ `None`?"
        updatedAt: '2023-11-19T20:53:56.091Z'
      numEdits: 0
      reactions: []
    id: 655a75e4c1f2352d23c60897
    type: comment
  author: zhaominxiao
  content: "@ydshieh \nThanks for your help. I am totally ok with using `model.generate`.\
    \ I checked the code example in the model card. It is about how to use one image\
    \ and one text sequence to do visual question answering (or image captioning).\
    \ My use case needs the model to consume interleaved text-and-image sequences.\
    \ For example, to do the few-shot learning, I need to provide multiple examples\
    \ before asking the \"real\" question. In this case, I need to make sure the text\
    \ and images are presented to KOSMOS in order. I went through your helper function\
    \ implementation and my understanding is that with the dictionary returned by\
    \ your helper function, I can use its `input_ids`, `attention_mask`, and `image_embeds_position_mask`\
    \ as the value of the parameters in `model.generate`. But I am not sure how I\
    \ can set the `pixel_values` and `image_embeds`. Should I set them as `None`?"
  created_at: 2023-11-19 20:53:56+00:00
  edited: false
  hidden: false
  id: 655a75e4c1f2352d23c60897
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f9e667280458fae9a58b84ac236af9fb.svg
      fullname: zhaomin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhaominxiao
      type: user
    createdAt: '2023-11-19T20:58:42.000Z'
    data:
      edited: false
      editors:
      - zhaominxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9533159732818604
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f9e667280458fae9a58b84ac236af9fb.svg
          fullname: zhaomin
          isHf: false
          isPro: false
          name: zhaominxiao
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ydshieh&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ydshieh\">@<span class=\"\
          underline\">ydshieh</span></a></span>\n\n\t</span></span><br>Oh, I think\
          \ I got the answer. The <code>pixel_values</code> was assigned some values\
          \ in the helper function. Regarding the <code>image_embeds</code>, I think\
          \ I should leave it as <code>None</code>.</p>\n<p>Thank you very much!</p>\n"
        raw: "@ydshieh \nOh, I think I got the answer. The `pixel_values` was assigned\
          \ some values in the helper function. Regarding the `image_embeds`, I think\
          \ I should leave it as `None`.\n\nThank you very much!"
        updatedAt: '2023-11-19T20:58:42.451Z'
      numEdits: 0
      reactions: []
    id: 655a770230ad83ad6b662e66
    type: comment
  author: zhaominxiao
  content: "@ydshieh \nOh, I think I got the answer. The `pixel_values` was assigned\
    \ some values in the helper function. Regarding the `image_embeds`, I think I\
    \ should leave it as `None`.\n\nThank you very much!"
  created_at: 2023-11-19 20:58:42+00:00
  edited: false
  hidden: false
  id: 655a770230ad83ad6b662e66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
      fullname: Yih-Dar SHIEH
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ydshieh
      type: user
    createdAt: '2023-11-20T18:17:13.000Z'
    data:
      edited: false
      editors:
      - ydshieh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.919855535030365
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625150194719-6058c8cbcbe9c7542f3501ff.png?w=200&h=200&f=face
          fullname: Yih-Dar SHIEH
          isHf: true
          isPro: false
          name: ydshieh
          type: user
        html: '<p>Yes, usually you don''t need <code>image_embeds</code>. Passing
          <code>pixel_values</code> is the usual case.</p>

          '
        raw: Yes, usually you don't need `image_embeds`. Passing `pixel_values` is
          the usual case.
        updatedAt: '2023-11-20T18:17:13.504Z'
      numEdits: 0
      reactions: []
    id: 655ba2a9b426ec8f4b429e01
    type: comment
  author: ydshieh
  content: Yes, usually you don't need `image_embeds`. Passing `pixel_values` is the
    usual case.
  created_at: 2023-11-20 18:17:13+00:00
  edited: false
  hidden: false
  id: 655ba2a9b426ec8f4b429e01
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: microsoft/kosmos-2-patch14-224
repo_type: model
status: open
target_branch: null
title: 'KeyError: ''kosmos-2'''
