!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LorenzoCevolaniAXA
conflicting_files: null
created_at: 2023-08-01 09:53:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/dWy8pkzRWllMotX8WHVOf.jpeg?w=200&h=200&f=face
      fullname: Lorenzo Cevolani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LorenzoCevolaniAXA
      type: user
    createdAt: '2023-08-01T10:53:33.000Z'
    data:
      edited: false
      editors:
      - LorenzoCevolaniAXA
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6194480657577515
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/dWy8pkzRWllMotX8WHVOf.jpeg?w=200&h=200&f=face
          fullname: Lorenzo Cevolani
          isHf: false
          isPro: false
          name: LorenzoCevolaniAXA
          type: user
        html: "<p>Hello,</p>\n<p>I wanted to try this model into one of our SageMaker\
          \ instances.<br>I tried the code to deploy the model you present in the\
          \ deploy button but the instance proposed (\"ml.g5.2xlarge\") is too small\
          \ so it get a memory error while converting the pytorch weights to safetensors,\
          \ the problem is solved when using the following instance: \"ml.g5.8xlarge\"\
          .<br>I am using the huggingface llm image version 0.8.2 (latest available).<br>After\
          \ this I encountered another problem which I do not know how to solve it.<br>The\
          \ problem is the following:</p>\n<pre><code class=\"language-bash\"> Shard\
          \ 0 failed to start:\nTraceback (most recent call last):\nError: ShardCannotStart\n\
          \  File <span class=\"hljs-string\">\"/opt/conda/bin/text-generation-server\"\
          </span>, line 8, <span class=\"hljs-keyword\">in</span> &lt;module&gt;\n\
          \    sys.exit(app())\n  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          </span>, line 67, <span class=\"hljs-keyword\">in</span> serve\n    server.serve(model_id,\
          \ revision, sharded, quantize, trust_remote_code, uds_path)\n  File <span\
          \ class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          </span>, line 155, <span class=\"hljs-keyword\">in</span> serve\n    asyncio.run(serve_inner(model_id,\
          \ revision, sharded, quantize, trust_remote_code))\n  File <span class=\"\
          hljs-string\">\"/opt/conda/lib/python3.9/asyncio/runners.py\"</span>, line\
          \ 44, <span class=\"hljs-keyword\">in</span> run\n    <span class=\"hljs-built_in\"\
          >return</span> loop.run_until_complete(main)\n  File <span class=\"hljs-string\"\
          >\"/opt/conda/lib/python3.9/asyncio/base_events.py\"</span>, line 647, <span\
          \ class=\"hljs-keyword\">in</span> run_until_complete\n    <span class=\"\
          hljs-built_in\">return</span> future.result()\n  File <span class=\"hljs-string\"\
          >\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          </span>, line 124, <span class=\"hljs-keyword\">in</span> serve_inner\n\
          \    model = get_model(model_id, revision, sharded, quantize, trust_remote_code)\n\
          \  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          </span>, line 289, <span class=\"hljs-keyword\">in</span> get_model\n  \
          \  <span class=\"hljs-built_in\">return</span> CausalLM(\n  File <span class=\"\
          hljs-string\">\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/causal_lm.py\"\
          </span>, line 469, <span class=\"hljs-keyword\">in</span> __init__\n   \
          \ tokenizer = AutoTokenizer.from_pretrained(\n  File <span class=\"hljs-string\"\
          >\"/usr/src/transformers/src/transformers/models/auto/tokenization_auto.py\"\
          </span>, line 692, <span class=\"hljs-keyword\">in</span> from_pretrained\n\
          \    <span class=\"hljs-built_in\">return</span> tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n  File <span class=\"hljs-string\">\"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          </span>, line 1812, <span class=\"hljs-keyword\">in</span> from_pretrained\n\
          \    <span class=\"hljs-built_in\">return</span> cls._from_pretrained(\n\
          \  File <span class=\"hljs-string\">\"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          </span>, line 1975, <span class=\"hljs-keyword\">in</span> _from_pretrained\n\
          \    tokenizer = cls(*init_inputs, **init_kwargs)\n  File <span class=\"\
          hljs-string\">\"/usr/src/transformers/src/transformers/models/llama/tokenization_llama_fast.py\"\
          </span>, line 89, <span class=\"hljs-keyword\">in</span> __init__\n    super().__init__(\n\
          \  File <span class=\"hljs-string\">\"/usr/src/transformers/src/transformers/tokenization_utils_fast.py\"\
          </span>, line 114, <span class=\"hljs-keyword\">in</span> __init__\n   \
          \ fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n  File <span\
          \ class=\"hljs-string\">\"/usr/src/transformers/src/transformers/convert_slow_tokenizer.py\"\
          </span>, line 1303, <span class=\"hljs-keyword\">in</span> convert_slow_tokenizer\n\
          \    <span class=\"hljs-built_in\">return</span> converter_class(transformer_tokenizer).converted()\n\
          \  File <span class=\"hljs-string\">\"/usr/src/transformers/src/transformers/convert_slow_tokenizer.py\"\
          </span>, line 445, <span class=\"hljs-keyword\">in</span> __init__\n   \
          \ from .utils import sentencepiece_model_pb2 as model_pb2\n  File <span\
          \ class=\"hljs-string\">\"/usr/src/transformers/src/transformers/utils/sentencepiece_model_pb2.py\"\
          </span>, line 91, <span class=\"hljs-keyword\">in</span> &lt;module&gt;\n\
          \    _descriptor.EnumValueDescriptor(\n  File <span class=\"hljs-string\"\
          >\"/opt/conda/lib/python3.9/site-packages/google/protobuf/descriptor.py\"\
          </span>, line 796, <span class=\"hljs-keyword\">in</span> __new__\n    _message.Message._CheckCalledFromGeneratedFile()\n\
          TypeError: Descriptors cannot not be created directly.\nIf this call came\
          \ from a _pb2.py file, your generated code is out of <span class=\"hljs-built_in\"\
          >date</span> and must be regenerated with protoc &gt;= 3.19.0.\nIf you cannot\
          \ immediately regenerate your protos, some other possible workarounds are:\n\
          \ 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\
          \ (but this will use pure-Python parsing and will be much slower).\nMore\
          \ information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06<span\
          \ class=\"hljs-comment\">#python-updates</span>\n</code></pre>\n<p>I tried\
          \ to set the PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python in the environmental\
          \ variable and the error I get is the following:</p>\n<pre><code class=\"\
          language-bash\">RuntimeError: Llama is supposed to be a BPE model!\n</code></pre>\n\
          <p>Can you give some hints about what is not right here?<br>Is it an instance\
          \ problem?<br>Is it a package problem?</p>\n<p>Thanks in advance</p>\n"
        raw: "Hello,\r\n\r\nI wanted to try this model into one of our SageMaker instances.\r\
          \nI tried the code to deploy the model you present in the deploy button\
          \ but the instance proposed (\"ml.g5.2xlarge\") is too small so it get a\
          \ memory error while converting the pytorch weights to safetensors, the\
          \ problem is solved when using the following instance: \"ml.g5.8xlarge\"\
          .\r\nI am using the huggingface llm image version 0.8.2 (latest available).\r\
          \nAfter this I encountered another problem which I do not know how to solve\
          \ it.\r\nThe problem is the following:\r\n```bash\r\n Shard 0 failed to\
          \ start:\r\nTraceback (most recent call last):\r\nError: ShardCannotStart\r\
          \n  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\
          \n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 67, in serve\r\n    server.serve(model_id, revision, sharded, quantize,\
          \ trust_remote_code, uds_path)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 155, in serve\r\n    asyncio.run(serve_inner(model_id, revision,\
          \ sharded, quantize, trust_remote_code))\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\r\
          \n    return future.result()\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 124, in serve_inner\r\n    model = get_model(model_id, revision,\
          \ sharded, quantize, trust_remote_code)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 289, in get_model\r\n    return CausalLM(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/causal_lm.py\"\
          , line 469, in __init__\r\n    tokenizer = AutoTokenizer.from_pretrained(\r\
          \n  File \"/usr/src/transformers/src/transformers/models/auto/tokenization_auto.py\"\
          , line 692, in from_pretrained\r\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n  File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          , line 1812, in from_pretrained\r\n    return cls._from_pretrained(\r\n\
          \  File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          , line 1975, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
          \n  File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama_fast.py\"\
          , line 89, in __init__\r\n    super().__init__(\r\n  File \"/usr/src/transformers/src/transformers/tokenization_utils_fast.py\"\
          , line 114, in __init__\r\n    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\r\
          \n  File \"/usr/src/transformers/src/transformers/convert_slow_tokenizer.py\"\
          , line 1303, in convert_slow_tokenizer\r\n    return converter_class(transformer_tokenizer).converted()\r\
          \n  File \"/usr/src/transformers/src/transformers/convert_slow_tokenizer.py\"\
          , line 445, in __init__\r\n    from .utils import sentencepiece_model_pb2\
          \ as model_pb2\r\n  File \"/usr/src/transformers/src/transformers/utils/sentencepiece_model_pb2.py\"\
          , line 91, in <module>\r\n    _descriptor.EnumValueDescriptor(\r\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/google/protobuf/descriptor.py\"\
          , line 796, in __new__\r\n    _message.Message._CheckCalledFromGeneratedFile()\r\
          \nTypeError: Descriptors cannot not be created directly.\r\nIf this call\
          \ came from a _pb2.py file, your generated code is out of date and must\
          \ be regenerated with protoc >= 3.19.0.\r\nIf you cannot immediately regenerate\
          \ your protos, some other possible workarounds are:\r\n 1. Downgrade the\
          \ protobuf package to 3.20.x or lower.\r\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\
          \ (but this will use pure-Python parsing and will be much slower).\r\nMore\
          \ information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\r\
          \n```\r\n\r\nI tried to set the PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\
          \ in the environmental variable and the error I get is the following:\r\n\
          \r\n```bash\r\nRuntimeError: Llama is supposed to be a BPE model!\r\n```\r\
          \n\r\nCan you give some hints about what is not right here?\r\nIs it an\
          \ instance problem?\r\nIs it a package problem?\r\n\r\n\r\nThanks in advance"
        updatedAt: '2023-08-01T10:53:33.315Z'
      numEdits: 0
      reactions: []
    id: 64c8e42d4515835c4d970ad9
    type: comment
  author: LorenzoCevolaniAXA
  content: "Hello,\r\n\r\nI wanted to try this model into one of our SageMaker instances.\r\
    \nI tried the code to deploy the model you present in the deploy button but the\
    \ instance proposed (\"ml.g5.2xlarge\") is too small so it get a memory error\
    \ while converting the pytorch weights to safetensors, the problem is solved when\
    \ using the following instance: \"ml.g5.8xlarge\".\r\nI am using the huggingface\
    \ llm image version 0.8.2 (latest available).\r\nAfter this I encountered another\
    \ problem which I do not know how to solve it.\r\nThe problem is the following:\r\
    \n```bash\r\n Shard 0 failed to start:\r\nTraceback (most recent call last):\r\
    \nError: ShardCannotStart\r\n  File \"/opt/conda/bin/text-generation-server\"\
    , line 8, in <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 67, in serve\r\n    server.serve(model_id, revision, sharded, quantize,\
    \ trust_remote_code, uds_path)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 155, in serve\r\n    asyncio.run(serve_inner(model_id, revision, sharded,\
    \ quantize, trust_remote_code))\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 647, in run_until_complete\r\n    return future.result()\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 124, in serve_inner\r\n    model = get_model(model_id, revision, sharded,\
    \ quantize, trust_remote_code)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 289, in get_model\r\n    return CausalLM(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/causal_lm.py\"\
    , line 469, in __init__\r\n    tokenizer = AutoTokenizer.from_pretrained(\r\n\
    \  File \"/usr/src/transformers/src/transformers/models/auto/tokenization_auto.py\"\
    , line 692, in from_pretrained\r\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\r\n  File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
    , line 1812, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File\
    \ \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\", line\
    \ 1975, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n  File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama_fast.py\"\
    , line 89, in __init__\r\n    super().__init__(\r\n  File \"/usr/src/transformers/src/transformers/tokenization_utils_fast.py\"\
    , line 114, in __init__\r\n    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\r\
    \n  File \"/usr/src/transformers/src/transformers/convert_slow_tokenizer.py\"\
    , line 1303, in convert_slow_tokenizer\r\n    return converter_class(transformer_tokenizer).converted()\r\
    \n  File \"/usr/src/transformers/src/transformers/convert_slow_tokenizer.py\"\
    , line 445, in __init__\r\n    from .utils import sentencepiece_model_pb2 as model_pb2\r\
    \n  File \"/usr/src/transformers/src/transformers/utils/sentencepiece_model_pb2.py\"\
    , line 91, in <module>\r\n    _descriptor.EnumValueDescriptor(\r\n  File \"/opt/conda/lib/python3.9/site-packages/google/protobuf/descriptor.py\"\
    , line 796, in __new__\r\n    _message.Message._CheckCalledFromGeneratedFile()\r\
    \nTypeError: Descriptors cannot not be created directly.\r\nIf this call came\
    \ from a _pb2.py file, your generated code is out of date and must be regenerated\
    \ with protoc >= 3.19.0.\r\nIf you cannot immediately regenerate your protos,\
    \ some other possible workarounds are:\r\n 1. Downgrade the protobuf package to\
    \ 3.20.x or lower.\r\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but\
    \ this will use pure-Python parsing and will be much slower).\r\nMore information:\
    \ https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\r\
    \n```\r\n\r\nI tried to set the PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\
    \ in the environmental variable and the error I get is the following:\r\n\r\n\
    ```bash\r\nRuntimeError: Llama is supposed to be a BPE model!\r\n```\r\n\r\nCan\
    \ you give some hints about what is not right here?\r\nIs it an instance problem?\r\
    \nIs it a package problem?\r\n\r\n\r\nThanks in advance"
  created_at: 2023-08-01 09:53:33+00:00
  edited: false
  hidden: false
  id: 64c8e42d4515835c4d970ad9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7702da77650df0be9f21cc300f761975.svg
      fullname: Xiangpeng Wei
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pemywei
      type: user
    createdAt: '2023-08-10T01:43:10.000Z'
    data:
      edited: false
      editors:
      - pemywei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9041550755500793
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7702da77650df0be9f21cc300f761975.svg
          fullname: Xiangpeng Wei
          isHf: false
          isPro: false
          name: pemywei
          type: user
        html: '<p>Actually, I am not very familiar with SageMaker, so I am not entirely
          clear about the specific reasons. However, you can try cloning the latest
          model files and installing the latest version of Transformers (v4.31.0).
          If any problems persist, we can further investigate the reasons together.</p>

          '
        raw: Actually, I am not very familiar with SageMaker, so I am not entirely
          clear about the specific reasons. However, you can try cloning the latest
          model files and installing the latest version of Transformers (v4.31.0).
          If any problems persist, we can further investigate the reasons together.
        updatedAt: '2023-08-10T01:43:10.496Z'
      numEdits: 0
      reactions: []
    id: 64d440aed0cfd876d93b56b1
    type: comment
  author: pemywei
  content: Actually, I am not very familiar with SageMaker, so I am not entirely clear
    about the specific reasons. However, you can try cloning the latest model files
    and installing the latest version of Transformers (v4.31.0). If any problems persist,
    we can further investigate the reasons together.
  created_at: 2023-08-10 00:43:10+00:00
  edited: false
  hidden: false
  id: 64d440aed0cfd876d93b56b1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: DAMO-NLP-MT/polylm-chat-13b
repo_type: model
status: open
target_branch: null
title: protoc  error while deploying the model into SM instance
