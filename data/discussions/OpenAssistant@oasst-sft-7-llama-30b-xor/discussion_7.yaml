!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pevogam
conflicting_files: null
created_at: 2023-05-18 05:41:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
      fullname: Plamen Dimitrov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pevogam
      type: user
    createdAt: '2023-05-18T06:41:39.000Z'
    data:
      edited: false
      editors:
      - pevogam
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
          fullname: Plamen Dimitrov
          isHf: false
          isPro: false
          name: pevogam
          type: user
        html: "<p>Hi all,</p>\n<p>While the XOR conversion process is fully described\
          \ and one can obtain the weights this way and it is possible to run the\
          \ model with various minimalistic setup, how can we actually do so with\
          \ OpenAssistant?</p>\n<p>I have placed the converted weights in <code>open-assistant-repo/data/models--OpenAssistant--oasst-sft-7-llama-30b</code>\
          \ and tried to export <code>MODEL_CONFIG_NAME=OA_SFT_Llama_30B_7</code>\
          \ but I guess I should either edit the models config file to include a XOR\
          \ version or do something else since I get the rather expected error</p>\n\
          <pre><code>open-assistant-inference-worker-1  | /bin/bash: /opt/miniconda/envs/text-generation/lib/libtinfo.so.6:\
          \ no version information available (required by /bin/bash)\nopen-assistant-inference-worker-1\
          \  | Downloading model OpenAssistant/oasst-sft-7-llama-30b\nopen-assistant-inference-worker-1\
          \  | Traceback (most recent call last):\nopen-assistant-inference-worker-1\
          \  |   File \"/worker/download_model.py\", line 17, in &lt;module&gt;\n\
          open-assistant-inference-worker-1  |     transformers.LlamaTokenizer.from_pretrained(model_id)\n\
          open-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/utils/import_utils.py\"\
          , line 1112, in __getattr__\nopen-assistant-inference-worker-1  |     raise\
          \ AttributeError(f\"module {self.__name__} has no attribute {name}\")\n\
          open-assistant-inference-worker-1  | AttributeError: module transformers\
          \ has no attribute LlamaTokenizer\nopen-assistant-inference-worker-1  |\
          \ Starting worker server on port 8300\nopen-assistant-inference-worker-1\
          \  | Starting worker\nopen-assistant-inference-worker-1  | 2023-05-18T06:35:01.641548Z\
          \  INFO text_generation_launcher: Args { model_id: \"OpenAssistant/oasst-sft-7-llama-30b\"\
          , revision: None, sharded: None, num_shard: Some(1), quantize: false, max_concurrent_requests:\
          \ 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 4096, max_total_tokens:\
          \ 8192, max_batch_size: 32, max_waiting_tokens: 20, port: 8300, shard_uds_path:\
          \ \"/tmp/text-generation-server\", master_addr: \"localhost\", master_port:\
          \ 29500, huggingface_hub_cache: Some(\"/data\"), weights_cache_override:\
          \ None, disable_custom_kernels: false, json_output: false, otlp_endpoint:\
          \ None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None\
          \ }\nopen-assistant-inference-worker-1  | 2023-05-18T06:35:01.641844Z  INFO\
          \ text_generation_launcher: Starting shard 0\nopen-assistant-inference-worker-1\
          \  | None of PyTorch, TensorFlow &gt;= 2.0, or Flax have been found. Models\
          \ won't be available and only tokenizers, configuration and file/data utilities\
          \ can be used.\nopen-assistant-inference-worker-1  | 2023-05-18 06:35:02.671\
          \ | INFO     | __main__:main:25 - Inference protocol version: 1\nopen-assistant-inference-worker-1\
          \  | 2023-05-18 06:35:02.671 | WARNING  | __main__:main:28 - Model config:\
          \ model_id='OpenAssistant/oasst-sft-7-llama-30b' max_input_length=1024 max_total_length=1792\
          \ quantized=False\nopen-assistant-inference-worker-1  | Traceback (most\
          \ recent call last):\nopen-assistant-inference-server-1  | INFO:     Will\
          \ watch for changes in these directories: ['/opt/inference/server']\nopen-assistant-inference-server-1\
          \  | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n\
          open-assistant-inference-server-1  | INFO:     Started reloader process\
          \ [7] using WatchFiles\nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\"\
          , line 259, in hf_raise_for_status\nopen-assistant-inference-worker-1  |\
          \     response.raise_for_status()\nopen-assistant-inference-worker-1  |\
          \   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/requests/models.py\"\
          , line 1021, in raise_for_status\nopen-assistant-inference-worker-1  | \
          \    raise HTTPError(http_error_msg, response=self)\nopen-assistant-inference-worker-1\
          \  | requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url:\
          \ https://huggingface.co/OpenAssistant/oasst-sft-7-llama-30b/resolve/main/tokenizer_config.json\n\
          open-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
          \  | The above exception was the direct cause of the following exception:\n\
          open-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
          \  | Traceback (most recent call last):\nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/transformers/utils/hub.py\"\
          , line 417, in cached_file\nopen-assistant-inference-worker-1  |     resolved_file\
          \ = hf_hub_download(\nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\"\
          , line 120, in _inner_fn\nopen-assistant-inference-worker-1  |     return\
          \ fn(*args, **kwargs)\nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/file_download.py\"\
          , line 1195, in hf_hub_download\nopen-assistant-inference-worker-1  |  \
          \   metadata = get_hf_file_metadata(\nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\"\
          , line 120, in _inner_fn\nopen-assistant-inference-worker-1  |     return\
          \ fn(*args, **kwargs)\nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/file_download.py\"\
          , line 1541, in get_hf_file_metadata\nopen-assistant-inference-worker-1\
          \  |     hf_raise_for_status(r)\nopen-assistant-inference-worker-1  |  \
          \ File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\"\
          , line 291, in hf_raise_for_status\nopen-assistant-inference-worker-1  |\
          \     raise RepositoryNotFoundError(message, response) from e\nopen-assistant-inference-worker-1\
          \  | huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error.\
          \ (Request ID: Root=1-6465c717-62334eb312470f792fff2835)\nopen-assistant-inference-worker-1\
          \  | \nopen-assistant-inference-worker-1  | Repository Not Found for url:\
          \ https://huggingface.co/OpenAssistant/oasst-sft-7-llama-30b/resolve/main/tokenizer_config.json.\n\
          open-assistant-inference-worker-1  | Please make sure you specified the\
          \ correct `repo_id` and `repo_type`.\nopen-assistant-inference-worker-1\
          \  | If you are trying to access a private or gated repo, make sure you\
          \ are authenticated.\nopen-assistant-inference-worker-1  | Invalid username\
          \ or password.\nopen-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
          \  | During handling of the above exception, another exception occurred:\n\
          open-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
          \  | Traceback (most recent call last):\nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/worker/lib/python3.10/runpy.py\", line\
          \ 196, in _run_module_as_main\nopen-assistant-inference-worker-1  |    \
          \ return _run_code(code, main_globals, None,\nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/worker/lib/python3.10/runpy.py\", line\
          \ 86, in _run_code\nopen-assistant-inference-worker-1  |     exec(code,\
          \ run_globals)\nopen-assistant-inference-worker-1  |   File \"/worker/__main__.py\"\
          , line 132, in &lt;module&gt;\nopen-assistant-inference-worker-1  |    \
          \ main()\nopen-assistant-inference-worker-1  |   File \"/worker/__main__.py\"\
          , line 36, in main\nopen-assistant-inference-worker-1  |     tokenizer:\
          \ transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(model_config.model_id)\n\
          open-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 643, in from_pretrained\nopen-assistant-inference-worker-1  |   \
          \  tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path,\
          \ **kwargs)\nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 487, in get_tokenizer_config\nopen-assistant-inference-worker-1 \
          \ |     resolved_config_file = cached_file(\nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/transformers/utils/hub.py\"\
          , line 433, in cached_file\nopen-assistant-inference-worker-1  |     raise\
          \ EnvironmentError(\nopen-assistant-inference-worker-1  | OSError: OpenAssistant/oasst-sft-7-llama-30b\
          \ is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n\
          open-assistant-inference-worker-1  | If this is a private repository, make\
          \ sure to pass a token having permission to this repo with `use_auth_token`\
          \ or log in with `huggingface-cli login` and pass `use_auth_token=True`.\n\
          </code></pre>\n"
        raw: "Hi all,\r\n\r\nWhile the XOR conversion process is fully described and\
          \ one can obtain the weights this way and it is possible to run the model\
          \ with various minimalistic setup, how can we actually do so with OpenAssistant?\r\
          \n\r\nI have placed the converted weights in `open-assistant-repo/data/models--OpenAssistant--oasst-sft-7-llama-30b`\
          \ and tried to export `MODEL_CONFIG_NAME=OA_SFT_Llama_30B_7` but I guess\
          \ I should either edit the models config file to include a XOR version or\
          \ do something else since I get the rather expected error\r\n```\r\nopen-assistant-inference-worker-1\
          \  | /bin/bash: /opt/miniconda/envs/text-generation/lib/libtinfo.so.6: no\
          \ version information available (required by /bin/bash)\r\nopen-assistant-inference-worker-1\
          \  | Downloading model OpenAssistant/oasst-sft-7-llama-30b\r\nopen-assistant-inference-worker-1\
          \  | Traceback (most recent call last):\r\nopen-assistant-inference-worker-1\
          \  |   File \"/worker/download_model.py\", line 17, in <module>\r\nopen-assistant-inference-worker-1\
          \  |     transformers.LlamaTokenizer.from_pretrained(model_id)\r\nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/utils/import_utils.py\"\
          , line 1112, in __getattr__\r\nopen-assistant-inference-worker-1  |    \
          \ raise AttributeError(f\"module {self.__name__} has no attribute {name}\"\
          )\r\nopen-assistant-inference-worker-1  | AttributeError: module transformers\
          \ has no attribute LlamaTokenizer\r\nopen-assistant-inference-worker-1 \
          \ | Starting worker server on port 8300\r\nopen-assistant-inference-worker-1\
          \  | Starting worker\r\nopen-assistant-inference-worker-1  | 2023-05-18T06:35:01.641548Z\
          \  INFO text_generation_launcher: Args { model_id: \"OpenAssistant/oasst-sft-7-llama-30b\"\
          , revision: None, sharded: None, num_shard: Some(1), quantize: false, max_concurrent_requests:\
          \ 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 4096, max_total_tokens:\
          \ 8192, max_batch_size: 32, max_waiting_tokens: 20, port: 8300, shard_uds_path:\
          \ \"/tmp/text-generation-server\", master_addr: \"localhost\", master_port:\
          \ 29500, huggingface_hub_cache: Some(\"/data\"), weights_cache_override:\
          \ None, disable_custom_kernels: false, json_output: false, otlp_endpoint:\
          \ None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None\
          \ }\r\nopen-assistant-inference-worker-1  | 2023-05-18T06:35:01.641844Z\
          \  INFO text_generation_launcher: Starting shard 0\r\nopen-assistant-inference-worker-1\
          \  | None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models\
          \ won't be available and only tokenizers, configuration and file/data utilities\
          \ can be used.\r\nopen-assistant-inference-worker-1  | 2023-05-18 06:35:02.671\
          \ | INFO     | __main__:main:25 - Inference protocol version: 1\r\nopen-assistant-inference-worker-1\
          \  | 2023-05-18 06:35:02.671 | WARNING  | __main__:main:28 - Model config:\
          \ model_id='OpenAssistant/oasst-sft-7-llama-30b' max_input_length=1024 max_total_length=1792\
          \ quantized=False\r\nopen-assistant-inference-worker-1  | Traceback (most\
          \ recent call last):\r\nopen-assistant-inference-server-1  | INFO:     Will\
          \ watch for changes in these directories: ['/opt/inference/server']\r\n\
          open-assistant-inference-server-1  | INFO:     Uvicorn running on http://0.0.0.0:8000\
          \ (Press CTRL+C to quit)\r\nopen-assistant-inference-server-1  | INFO: \
          \    Started reloader process [7] using WatchFiles\r\nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\"\
          , line 259, in hf_raise_for_status\r\nopen-assistant-inference-worker-1\
          \  |     response.raise_for_status()\r\nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/requests/models.py\"\
          , line 1021, in raise_for_status\r\nopen-assistant-inference-worker-1  |\
          \     raise HTTPError(http_error_msg, response=self)\r\nopen-assistant-inference-worker-1\
          \  | requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url:\
          \ https://huggingface.co/OpenAssistant/oasst-sft-7-llama-30b/resolve/main/tokenizer_config.json\r\
          \nopen-assistant-inference-worker-1  | \r\nopen-assistant-inference-worker-1\
          \  | The above exception was the direct cause of the following exception:\r\
          \nopen-assistant-inference-worker-1  | \r\nopen-assistant-inference-worker-1\
          \  | Traceback (most recent call last):\r\nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/transformers/utils/hub.py\"\
          , line 417, in cached_file\r\nopen-assistant-inference-worker-1  |     resolved_file\
          \ = hf_hub_download(\r\nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\"\
          , line 120, in _inner_fn\r\nopen-assistant-inference-worker-1  |     return\
          \ fn(*args, **kwargs)\r\nopen-assistant-inference-worker-1  |   File \"\
          /opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/file_download.py\"\
          , line 1195, in hf_hub_download\r\nopen-assistant-inference-worker-1  |\
          \     metadata = get_hf_file_metadata(\r\nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\"\
          , line 120, in _inner_fn\r\nopen-assistant-inference-worker-1  |     return\
          \ fn(*args, **kwargs)\r\nopen-assistant-inference-worker-1  |   File \"\
          /opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/file_download.py\"\
          , line 1541, in get_hf_file_metadata\r\nopen-assistant-inference-worker-1\
          \  |     hf_raise_for_status(r)\r\nopen-assistant-inference-worker-1  |\
          \   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\"\
          , line 291, in hf_raise_for_status\r\nopen-assistant-inference-worker-1\
          \  |     raise RepositoryNotFoundError(message, response) from e\r\nopen-assistant-inference-worker-1\
          \  | huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error.\
          \ (Request ID: Root=1-6465c717-62334eb312470f792fff2835)\r\nopen-assistant-inference-worker-1\
          \  | \r\nopen-assistant-inference-worker-1  | Repository Not Found for url:\
          \ https://huggingface.co/OpenAssistant/oasst-sft-7-llama-30b/resolve/main/tokenizer_config.json.\r\
          \nopen-assistant-inference-worker-1  | Please make sure you specified the\
          \ correct `repo_id` and `repo_type`.\r\nopen-assistant-inference-worker-1\
          \  | If you are trying to access a private or gated repo, make sure you\
          \ are authenticated.\r\nopen-assistant-inference-worker-1  | Invalid username\
          \ or password.\r\nopen-assistant-inference-worker-1  | \r\nopen-assistant-inference-worker-1\
          \  | During handling of the above exception, another exception occurred:\r\
          \nopen-assistant-inference-worker-1  | \r\nopen-assistant-inference-worker-1\
          \  | Traceback (most recent call last):\r\nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/worker/lib/python3.10/runpy.py\", line\
          \ 196, in _run_module_as_main\r\nopen-assistant-inference-worker-1  |  \
          \   return _run_code(code, main_globals, None,\r\nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/worker/lib/python3.10/runpy.py\", line\
          \ 86, in _run_code\r\nopen-assistant-inference-worker-1  |     exec(code,\
          \ run_globals)\r\nopen-assistant-inference-worker-1  |   File \"/worker/__main__.py\"\
          , line 132, in <module>\r\nopen-assistant-inference-worker-1  |     main()\r\
          \nopen-assistant-inference-worker-1  |   File \"/worker/__main__.py\", line\
          \ 36, in main\r\nopen-assistant-inference-worker-1  |     tokenizer: transformers.PreTrainedTokenizer\
          \ = transformers.AutoTokenizer.from_pretrained(model_config.model_id)\r\n\
          open-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 643, in from_pretrained\r\nopen-assistant-inference-worker-1  | \
          \    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path,\
          \ **kwargs)\r\nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 487, in get_tokenizer_config\r\nopen-assistant-inference-worker-1\
          \  |     resolved_config_file = cached_file(\r\nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/transformers/utils/hub.py\"\
          , line 433, in cached_file\r\nopen-assistant-inference-worker-1  |     raise\
          \ EnvironmentError(\r\nopen-assistant-inference-worker-1  | OSError: OpenAssistant/oasst-sft-7-llama-30b\
          \ is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\r\
          \nopen-assistant-inference-worker-1  | If this is a private repository,\
          \ make sure to pass a token having permission to this repo with `use_auth_token`\
          \ or log in with `huggingface-cli login` and pass `use_auth_token=True`.\r\
          \n```"
        updatedAt: '2023-05-18T06:41:39.433Z'
      numEdits: 0
      reactions: []
    id: 6465c8a37ff8fcbef7d1126e
    type: comment
  author: pevogam
  content: "Hi all,\r\n\r\nWhile the XOR conversion process is fully described and\
    \ one can obtain the weights this way and it is possible to run the model with\
    \ various minimalistic setup, how can we actually do so with OpenAssistant?\r\n\
    \r\nI have placed the converted weights in `open-assistant-repo/data/models--OpenAssistant--oasst-sft-7-llama-30b`\
    \ and tried to export `MODEL_CONFIG_NAME=OA_SFT_Llama_30B_7` but I guess I should\
    \ either edit the models config file to include a XOR version or do something\
    \ else since I get the rather expected error\r\n```\r\nopen-assistant-inference-worker-1\
    \  | /bin/bash: /opt/miniconda/envs/text-generation/lib/libtinfo.so.6: no version\
    \ information available (required by /bin/bash)\r\nopen-assistant-inference-worker-1\
    \  | Downloading model OpenAssistant/oasst-sft-7-llama-30b\r\nopen-assistant-inference-worker-1\
    \  | Traceback (most recent call last):\r\nopen-assistant-inference-worker-1 \
    \ |   File \"/worker/download_model.py\", line 17, in <module>\r\nopen-assistant-inference-worker-1\
    \  |     transformers.LlamaTokenizer.from_pretrained(model_id)\r\nopen-assistant-inference-worker-1\
    \  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/utils/import_utils.py\"\
    , line 1112, in __getattr__\r\nopen-assistant-inference-worker-1  |     raise\
    \ AttributeError(f\"module {self.__name__} has no attribute {name}\")\r\nopen-assistant-inference-worker-1\
    \  | AttributeError: module transformers has no attribute LlamaTokenizer\r\nopen-assistant-inference-worker-1\
    \  | Starting worker server on port 8300\r\nopen-assistant-inference-worker-1\
    \  | Starting worker\r\nopen-assistant-inference-worker-1  | 2023-05-18T06:35:01.641548Z\
    \  INFO text_generation_launcher: Args { model_id: \"OpenAssistant/oasst-sft-7-llama-30b\"\
    , revision: None, sharded: None, num_shard: Some(1), quantize: false, max_concurrent_requests:\
    \ 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 4096, max_total_tokens:\
    \ 8192, max_batch_size: 32, max_waiting_tokens: 20, port: 8300, shard_uds_path:\
    \ \"/tmp/text-generation-server\", master_addr: \"localhost\", master_port: 29500,\
    \ huggingface_hub_cache: Some(\"/data\"), weights_cache_override: None, disable_custom_kernels:\
    \ false, json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma:\
    \ None, watermark_delta: None }\r\nopen-assistant-inference-worker-1  | 2023-05-18T06:35:01.641844Z\
    \  INFO text_generation_launcher: Starting shard 0\r\nopen-assistant-inference-worker-1\
    \  | None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't\
    \ be available and only tokenizers, configuration and file/data utilities can\
    \ be used.\r\nopen-assistant-inference-worker-1  | 2023-05-18 06:35:02.671 | INFO\
    \     | __main__:main:25 - Inference protocol version: 1\r\nopen-assistant-inference-worker-1\
    \  | 2023-05-18 06:35:02.671 | WARNING  | __main__:main:28 - Model config: model_id='OpenAssistant/oasst-sft-7-llama-30b'\
    \ max_input_length=1024 max_total_length=1792 quantized=False\r\nopen-assistant-inference-worker-1\
    \  | Traceback (most recent call last):\r\nopen-assistant-inference-server-1 \
    \ | INFO:     Will watch for changes in these directories: ['/opt/inference/server']\r\
    \nopen-assistant-inference-server-1  | INFO:     Uvicorn running on http://0.0.0.0:8000\
    \ (Press CTRL+C to quit)\r\nopen-assistant-inference-server-1  | INFO:     Started\
    \ reloader process [7] using WatchFiles\r\nopen-assistant-inference-worker-1 \
    \ |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\"\
    , line 259, in hf_raise_for_status\r\nopen-assistant-inference-worker-1  |   \
    \  response.raise_for_status()\r\nopen-assistant-inference-worker-1  |   File\
    \ \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/requests/models.py\"\
    , line 1021, in raise_for_status\r\nopen-assistant-inference-worker-1  |     raise\
    \ HTTPError(http_error_msg, response=self)\r\nopen-assistant-inference-worker-1\
    \  | requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/OpenAssistant/oasst-sft-7-llama-30b/resolve/main/tokenizer_config.json\r\
    \nopen-assistant-inference-worker-1  | \r\nopen-assistant-inference-worker-1 \
    \ | The above exception was the direct cause of the following exception:\r\nopen-assistant-inference-worker-1\
    \  | \r\nopen-assistant-inference-worker-1  | Traceback (most recent call last):\r\
    \nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/transformers/utils/hub.py\"\
    , line 417, in cached_file\r\nopen-assistant-inference-worker-1  |     resolved_file\
    \ = hf_hub_download(\r\nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\"\
    , line 120, in _inner_fn\r\nopen-assistant-inference-worker-1  |     return fn(*args,\
    \ **kwargs)\r\nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/file_download.py\"\
    , line 1195, in hf_hub_download\r\nopen-assistant-inference-worker-1  |     metadata\
    \ = get_hf_file_metadata(\r\nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\"\
    , line 120, in _inner_fn\r\nopen-assistant-inference-worker-1  |     return fn(*args,\
    \ **kwargs)\r\nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/file_download.py\"\
    , line 1541, in get_hf_file_metadata\r\nopen-assistant-inference-worker-1  | \
    \    hf_raise_for_status(r)\r\nopen-assistant-inference-worker-1  |   File \"\
    /opt/miniconda/envs/worker/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\"\
    , line 291, in hf_raise_for_status\r\nopen-assistant-inference-worker-1  |   \
    \  raise RepositoryNotFoundError(message, response) from e\r\nopen-assistant-inference-worker-1\
    \  | huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error.\
    \ (Request ID: Root=1-6465c717-62334eb312470f792fff2835)\r\nopen-assistant-inference-worker-1\
    \  | \r\nopen-assistant-inference-worker-1  | Repository Not Found for url: https://huggingface.co/OpenAssistant/oasst-sft-7-llama-30b/resolve/main/tokenizer_config.json.\r\
    \nopen-assistant-inference-worker-1  | Please make sure you specified the correct\
    \ `repo_id` and `repo_type`.\r\nopen-assistant-inference-worker-1  | If you are\
    \ trying to access a private or gated repo, make sure you are authenticated.\r\
    \nopen-assistant-inference-worker-1  | Invalid username or password.\r\nopen-assistant-inference-worker-1\
    \  | \r\nopen-assistant-inference-worker-1  | During handling of the above exception,\
    \ another exception occurred:\r\nopen-assistant-inference-worker-1  | \r\nopen-assistant-inference-worker-1\
    \  | Traceback (most recent call last):\r\nopen-assistant-inference-worker-1 \
    \ |   File \"/opt/miniconda/envs/worker/lib/python3.10/runpy.py\", line 196, in\
    \ _run_module_as_main\r\nopen-assistant-inference-worker-1  |     return _run_code(code,\
    \ main_globals, None,\r\nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/worker/lib/python3.10/runpy.py\"\
    , line 86, in _run_code\r\nopen-assistant-inference-worker-1  |     exec(code,\
    \ run_globals)\r\nopen-assistant-inference-worker-1  |   File \"/worker/__main__.py\"\
    , line 132, in <module>\r\nopen-assistant-inference-worker-1  |     main()\r\n\
    open-assistant-inference-worker-1  |   File \"/worker/__main__.py\", line 36,\
    \ in main\r\nopen-assistant-inference-worker-1  |     tokenizer: transformers.PreTrainedTokenizer\
    \ = transformers.AutoTokenizer.from_pretrained(model_config.model_id)\r\nopen-assistant-inference-worker-1\
    \  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 643, in from_pretrained\r\nopen-assistant-inference-worker-1  |     tokenizer_config\
    \ = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\r\nopen-assistant-inference-worker-1\
    \  |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 487, in get_tokenizer_config\r\nopen-assistant-inference-worker-1  |  \
    \   resolved_config_file = cached_file(\r\nopen-assistant-inference-worker-1 \
    \ |   File \"/opt/miniconda/envs/worker/lib/python3.10/site-packages/transformers/utils/hub.py\"\
    , line 433, in cached_file\r\nopen-assistant-inference-worker-1  |     raise EnvironmentError(\r\
    \nopen-assistant-inference-worker-1  | OSError: OpenAssistant/oasst-sft-7-llama-30b\
    \ is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\r\
    \nopen-assistant-inference-worker-1  | If this is a private repository, make sure\
    \ to pass a token having permission to this repo with `use_auth_token` or log\
    \ in with `huggingface-cli login` and pass `use_auth_token=True`.\r\n```"
  created_at: 2023-05-18 05:41:39+00:00
  edited: false
  hidden: false
  id: 6465c8a37ff8fcbef7d1126e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: OpenAssistant/oasst-sft-7-llama-30b-xor
repo_type: model
status: open
target_branch: null
title: 'Could we run a XOR converted model using OpenAssistant? '
