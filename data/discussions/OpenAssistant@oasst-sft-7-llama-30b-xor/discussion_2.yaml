!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheBloke
conflicting_files: null
created_at: 2023-04-29 14:39:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T15:39:33.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<ul>

          <li>XORs merged to produce fp16 HF repo: <a href="https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-HF">https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-HF</a></li>

          <li>Quantised to 4bit GPTQ for GPU inference: <a href="https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ">https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ</a></li>

          <li>Quantised to 4bit and 5bit GGML for CPU inference: <a href="https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GGML">https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GGML</a></li>

          </ul>

          '
        raw: "* XORs merged to produce fp16 HF repo: https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-HF\r\
          \n* Quantised to 4bit GPTQ for GPU inference: https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\r\
          \n* Quantised to 4bit and 5bit GGML for CPU inference: https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GGML"
        updatedAt: '2023-04-29T15:39:33.557Z'
      numEdits: 0
      reactions:
      - count: 15
        reaction: "\u2764\uFE0F"
        users:
        - bvictor1503
        - Aloento
        - OllieStanley
        - 0x22almostEvil
        - tsumeone
        - dduval
        - amitj
        - jocastroc
        - hanifabdlh
        - captainst
        - zymsun
        - Chengyu
        - btseytlin
        - lrq3000
        - rainbowflesh
      - count: 1
        reaction: "\U0001F44D"
        users:
        - XelotX
    id: 644d3a355c1b4e14d0c0bb13
    type: comment
  author: TheBloke
  content: "* XORs merged to produce fp16 HF repo: https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-HF\r\
    \n* Quantised to 4bit GPTQ for GPU inference: https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\r\
    \n* Quantised to 4bit and 5bit GGML for CPU inference: https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GGML"
  created_at: 2023-04-29 14:39:33+00:00
  edited: false
  hidden: false
  id: 644d3a355c1b4e14d0c0bb13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
      fullname: qc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captainst
      type: user
    createdAt: '2023-05-04T07:19:06.000Z'
    data:
      edited: false
      editors:
      - captainst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
          fullname: qc
          isHf: false
          isPro: false
          name: captainst
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span>  Really cool!\
          \ The q4_0 model runs on a AMD Ryzen 9 3950X 16-Core Processor, takes about\
          \ 20GB memory, and generate a token every ~0.5sec.<br>I am wondering if\
          \ you could make a 4bit HF compliant model so I can run it on 2 x RTX 3060\
          \ GPU :D</p>\n"
        raw: '@TheBloke  Really cool! The q4_0 model runs on a AMD Ryzen 9 3950X 16-Core
          Processor, takes about 20GB memory, and generate a token every ~0.5sec.

          I am wondering if you could make a 4bit HF compliant model so I can run
          it on 2 x RTX 3060 GPU :D'
        updatedAt: '2023-05-04T07:19:06.577Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - joostshao1989
    id: 64535c6a794fdf08450dbe32
    type: comment
  author: captainst
  content: '@TheBloke  Really cool! The q4_0 model runs on a AMD Ryzen 9 3950X 16-Core
    Processor, takes about 20GB memory, and generate a token every ~0.5sec.

    I am wondering if you could make a 4bit HF compliant model so I can run it on
    2 x RTX 3060 GPU :D'
  created_at: 2023-05-04 06:19:06+00:00
  edited: false
  hidden: false
  id: 64535c6a794fdf08450dbe32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-04T07:23:17.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I did make 4bit - that''s the GPTQ model! I believe GPTQ supports
          splitting across two GPUs, though I''ve not done it personally.</p>

          <p>HF doesn''t support 4bit, only 16bit and 8bit (with <code>load_in_8bit=True</code>
          and with <code>bitsandbytes</code> installed). But GPTQ models are for GPU
          inference like HF models.</p>

          <p>You can look at text-generation-webui as a way to load GPTQs with, I
          think, multi-GPU split.  Or for Python inference look at <code>llama_inference.py</code>
          in the GPTQ-for-LLaMa repo. The code is a bit complex and messy but it should
          be possible to use it for any general inference.</p>

          <p>In the future the way to use GPTQ will be a new, better repo called AutoGPTQ.
          That allows loading models in a very similar way to standard transformers/HF
          code.  However it  definitely doesn''t support multi-GPU yet. But that should
          be coming quite soon.</p>

          '
        raw: 'I did make 4bit - that''s the GPTQ model! I believe GPTQ supports splitting
          across two GPUs, though I''ve not done it personally.


          HF doesn''t support 4bit, only 16bit and 8bit (with `load_in_8bit=True`
          and with `bitsandbytes` installed). But GPTQ models are for GPU inference
          like HF models.


          You can look at text-generation-webui as a way to load GPTQs with, I think,
          multi-GPU split.  Or for Python inference look at `llama_inference.py` in
          the GPTQ-for-LLaMa repo. The code is a bit complex and messy but it should
          be possible to use it for any general inference.


          In the future the way to use GPTQ will be a new, better repo called AutoGPTQ.
          That allows loading models in a very similar way to standard transformers/HF
          code.  However it  definitely doesn''t support multi-GPU yet. But that should
          be coming quite soon.'
        updatedAt: '2023-05-04T07:24:08.421Z'
      numEdits: 2
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - captainst
        - joostshao1989
        - Chengyu
        - rainbowflesh
    id: 64535d650afb444bbee712b5
    type: comment
  author: TheBloke
  content: 'I did make 4bit - that''s the GPTQ model! I believe GPTQ supports splitting
    across two GPUs, though I''ve not done it personally.


    HF doesn''t support 4bit, only 16bit and 8bit (with `load_in_8bit=True` and with
    `bitsandbytes` installed). But GPTQ models are for GPU inference like HF models.


    You can look at text-generation-webui as a way to load GPTQs with, I think, multi-GPU
    split.  Or for Python inference look at `llama_inference.py` in the GPTQ-for-LLaMa
    repo. The code is a bit complex and messy but it should be possible to use it
    for any general inference.


    In the future the way to use GPTQ will be a new, better repo called AutoGPTQ.
    That allows loading models in a very similar way to standard transformers/HF code.  However
    it  definitely doesn''t support multi-GPU yet. But that should be coming quite
    soon.'
  created_at: 2023-05-04 06:23:17+00:00
  edited: true
  hidden: false
  id: 64535d650afb444bbee712b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
      fullname: Plamen Dimitrov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pevogam
      type: user
    createdAt: '2023-05-18T06:22:37.000Z'
    data:
      edited: false
      editors:
      - pevogam
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
          fullname: Plamen Dimitrov
          isHf: false
          isPro: false
          name: pevogam
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ have you managed to run these models with OpenAssistant's repo? I was\
          \ hoping that the following will be sufficient</p>\n<pre><code class=\"\
          language-diff\"><span class=\"hljs-comment\">diff --git a/oasst-shared/oasst_shared/model_configs.py\
          \ b/oasst-shared/oasst_shared/model_configs</span>\n.py\n<span class=\"\
          hljs-comment\">index 13b78e17..e062d411 100644</span>\n<span class=\"hljs-comment\"\
          >--- a/oasst-shared/oasst_shared/model_configs.py</span>\n<span class=\"\
          hljs-comment\">+++ b/oasst-shared/oasst_shared/model_configs.py</span>\n\
          <span class=\"hljs-meta\">@@ -124,6 +124,12 @@</span> MODEL_CONFIGS = {\n\
          \         max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n\
          \         quantized=True,\n     ),\n<span class=\"hljs-addition\">+    \"\
          OA_SFT_Llama_30Bq_7_Bloke\": ModelConfig(</span>\n<span class=\"hljs-addition\"\
          >+        model_id=\"TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\",</span>\n\
          <span class=\"hljs-addition\">+        max_input_length=1024,</span>\n<span\
          \ class=\"hljs-addition\">+        max_total_length=1792,  # seeing OOMs\
          \ on 2048 on an A100 80GB</span>\n<span class=\"hljs-addition\">+      \
          \  quantized=True,</span>\n<span class=\"hljs-addition\">+    ),</span>\n\
          \     \"OA_SFT_Llama_30B_7e3\": ModelConfig(\n         model_id=\"OpenAssistant/oasst-sft-7e3-llama-30b\"\
          ,\n         max_input_length=1024,\n</code></pre>\n<p>but as I haven't added\
          \ custom models there before perhaps someone else has tried with the models\
          \ provided here.</p>\n"
        raw: "Hi @TheBloke have you managed to run these models with OpenAssistant's\
          \ repo? I was hoping that the following will be sufficient\n```diff\ndiff\
          \ --git a/oasst-shared/oasst_shared/model_configs.py b/oasst-shared/oasst_shared/model_configs\n\
          .py\nindex 13b78e17..e062d411 100644\n--- a/oasst-shared/oasst_shared/model_configs.py\n\
          +++ b/oasst-shared/oasst_shared/model_configs.py\n@@ -124,6 +124,12 @@ MODEL_CONFIGS\
          \ = {\n         max_total_length=1792,  # seeing OOMs on 2048 on an A100\
          \ 80GB\n         quantized=True,\n     ),\n+    \"OA_SFT_Llama_30Bq_7_Bloke\"\
          : ModelConfig(\n+        model_id=\"TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\"\
          ,\n+        max_input_length=1024,\n+        max_total_length=1792,  # seeing\
          \ OOMs on 2048 on an A100 80GB\n+        quantized=True,\n+    ),\n    \
          \ \"OA_SFT_Llama_30B_7e3\": ModelConfig(\n         model_id=\"OpenAssistant/oasst-sft-7e3-llama-30b\"\
          ,\n         max_input_length=1024,\n```\nbut as I haven't added custom models\
          \ there before perhaps someone else has tried with the models provided here."
        updatedAt: '2023-05-18T06:22:37.063Z'
      numEdits: 0
      reactions: []
    id: 6465c42d9c627c78f860e9ab
    type: comment
  author: pevogam
  content: "Hi @TheBloke have you managed to run these models with OpenAssistant's\
    \ repo? I was hoping that the following will be sufficient\n```diff\ndiff --git\
    \ a/oasst-shared/oasst_shared/model_configs.py b/oasst-shared/oasst_shared/model_configs\n\
    .py\nindex 13b78e17..e062d411 100644\n--- a/oasst-shared/oasst_shared/model_configs.py\n\
    +++ b/oasst-shared/oasst_shared/model_configs.py\n@@ -124,6 +124,12 @@ MODEL_CONFIGS\
    \ = {\n         max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n\
    \         quantized=True,\n     ),\n+    \"OA_SFT_Llama_30Bq_7_Bloke\": ModelConfig(\n\
    +        model_id=\"TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\",\n+        max_input_length=1024,\n\
    +        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n+    \
    \    quantized=True,\n+    ),\n     \"OA_SFT_Llama_30B_7e3\": ModelConfig(\n \
    \        model_id=\"OpenAssistant/oasst-sft-7e3-llama-30b\",\n         max_input_length=1024,\n\
    ```\nbut as I haven't added custom models there before perhaps someone else has\
    \ tried with the models provided here."
  created_at: 2023-05-18 05:22:37+00:00
  edited: false
  hidden: false
  id: 6465c42d9c627c78f860e9ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
      fullname: Plamen Dimitrov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pevogam
      type: user
    createdAt: '2023-05-18T06:26:09.000Z'
    data:
      edited: false
      editors:
      - pevogam
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
          fullname: Plamen Dimitrov
          isHf: false
          isPro: false
          name: pevogam
          type: user
        html: "<p>I did try to draw inspiration from some commits that have previously\
          \ added models there like <a rel=\"nofollow\" href=\"https://github.com/LAION-AI/Open-Assistant/commit/6c3519ba558e4f1aa75b859357e6ceb56eac0429\"\
          >https://github.com/LAION-AI/Open-Assistant/commit/6c3519ba558e4f1aa75b859357e6ceb56eac0429</a>\
          \ but it seems this is all they are doing. However, with the above diff\
          \ I get</p>\n<pre><code>open-assistant-inference-worker-1  | 2023-05-18\
          \ 06:17:30.477 | WARNING  | __main__:main:28 - Model config: model_id='TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ'\
          \ max_input_length=1024 max_total_length=1792 quantized=True\nopen-assistant-inference-worker-1\
          \  | 2023-05-18T06:17:31.692411Z ERROR shard-manager: text_generation_launcher:\
          \ \"Error when initializing model\nopen-assistant-inference-worker-1  |\
          \ Traceback (most recent call last):\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/bin/text-generation-server\\\
          \", line 8, in &lt;module&gt;\nopen-assistant-inference-worker-1  |    \
          \ sys.exit(app())\nopen-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/typer/main.py\\\
          \", line 311, in __call__\nopen-assistant-inference-worker-1  |     return\
          \ get_command(self)(*args, **kwargs)\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/click/core.py\\\
          \", line 1130, in __call__\nopen-assistant-inference-worker-1  |     return\
          \ self.main(*args, **kwargs)\nopen-assistant-inference-worker-1  |   File\
          \ \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/typer/core.py\\\
          \", line 778, in main\nopen-assistant-inference-worker-1  |     return _main(\n\
          open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/typer/core.py\\\
          \", line 216, in _main\nopen-assistant-inference-worker-1  |     rv = self.invoke(ctx)\n\
          open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/click/core.py\\\
          \", line 1657, in invoke\nopen-assistant-inference-worker-1  |     return\
          \ _process_result(sub_ctx.command.invoke(sub_ctx))\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/click/core.py\\\
          \", line 1404, in invoke\nopen-assistant-inference-worker-1  |     return\
          \ ctx.invoke(self.callback, **ctx.params)\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/click/core.py\\\
          \", line 760, in invoke\nopen-assistant-inference-worker-1  |     return\
          \ __callback(*args, **kwargs)\nopen-assistant-inference-worker-1  |   File\
          \ \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/typer/main.py\\\
          \", line 683, in wrapper\nopen-assistant-inference-worker-1  |     return\
          \ callback(**use_params)  # type: ignore\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/cli.py\\\
          \", line 55, in serve\nopen-assistant-inference-worker-1  |     server.serve(model_id,\
          \ revision, sharded, quantize, uds_path)\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 130, in serve\nopen-assistant-inference-worker-1  |     asyncio.run(serve_inner(model_id,\
          \ revision, sharded, quantize))\nopen-assistant-inference-worker-1  |  \
          \ File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/runners.py\\\
          \", line 44, in run\nopen-assistant-inference-worker-1  |     return loop.run_until_complete(main)\n\
          open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/base_events.py\\\
          \", line 634, in run_until_complete\nopen-assistant-inference-worker-1 \
          \ |     self.run_forever()\nopen-assistant-inference-worker-1  |   File\
          \ \\\"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/base_events.py\\\
          \", line 601, in run_forever\nopen-assistant-inference-worker-1  |     self._run_once()\n\
          open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/base_events.py\\\
          \", line 1905, in _run_once\nopen-assistant-inference-worker-1  |     handle._run()\n\
          open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/events.py\\\
          \", line 80, in _run\nopen-assistant-inference-worker-1  |     self._context.run(self._callback,\
          \ *self._args)\nopen-assistant-inference-worker-1  | &gt; File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 99, in serve_inner\nopen-assistant-inference-worker-1  |     model\
          \ = get_model(model_id, revision, sharded, quantize)\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
          \", line 52, in get_model\nopen-assistant-inference-worker-1  |     config\
          \ = AutoConfig.from_pretrained(model_id, revision=revision)\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/models/auto/configuration_auto.py\\\
          \", line 882, in from_pretrained\nopen-assistant-inference-worker-1  | \
          \    config_class = CONFIG_MAPPING[config_dict[\\\"model_type\\\"]]\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/models/auto/configuration_auto.py\\\
          \", line 588, in __getitem__\nopen-assistant-inference-worker-1  |     raise\
          \ KeyError(key)\nopen-assistant-inference-worker-1  | KeyError: 'llama'\n\
          open-assistant-inference-worker-1  | \" rank=0\nopen-assistant-inference-worker-1\
          \  | 2023-05-18T06:17:32.251412Z ERROR text_generation_launcher: Shard 0\
          \ failed to start:\nopen-assistant-inference-worker-1  | Traceback (most\
          \ recent call last):\nopen-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/text-generation/bin/text-generation-server\"\
          , line 8, in &lt;module&gt;\nopen-assistant-inference-worker-1  |     sys.exit(app())\n\
          open-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 55, in serve\nopen-assistant-inference-worker-1  |     server.serve(model_id,\
          \ revision, sharded, quantize, uds_path)\nopen-assistant-inference-worker-1\
          \  | \nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 130, in serve\nopen-assistant-inference-worker-1  |     asyncio.run(serve_inner(model_id,\
          \ revision, sharded, quantize))\nopen-assistant-inference-worker-1  | \n\
          open-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\nopen-assistant-inference-worker-1  |     return loop.run_until_complete(main)\n\
          open-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/base_events.py\"\
          , line 647, in run_until_complete\nopen-assistant-inference-worker-1  |\
          \     return future.result()\nopen-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 99, in serve_inner\nopen-assistant-inference-worker-1  |     model\
          \ = get_model(model_id, revision, sharded, quantize)\nopen-assistant-inference-worker-1\
          \  | \nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 52, in get_model\nopen-assistant-inference-worker-1  |     config\
          \ = AutoConfig.from_pretrained(model_id, revision=revision)\nopen-assistant-inference-worker-1\
          \  | \nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/models/auto/configuration_auto.py\"\
          , line 882, in from_pretrained\nopen-assistant-inference-worker-1  |   \
          \  config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\nopen-assistant-inference-worker-1\
          \  | \nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/models/auto/configuration_auto.py\"\
          , line 588, in __getitem__\nopen-assistant-inference-worker-1  |     raise\
          \ KeyError(key)\nopen-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
          \  | KeyError: 'llama'\n</code></pre>\n"
        raw: "I did try to draw inspiration from some commits that have previously\
          \ added models there like https://github.com/LAION-AI/Open-Assistant/commit/6c3519ba558e4f1aa75b859357e6ceb56eac0429\
          \ but it seems this is all they are doing. However, with the above diff\
          \ I get\n```\nopen-assistant-inference-worker-1  | 2023-05-18 06:17:30.477\
          \ | WARNING  | __main__:main:28 - Model config: model_id='TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ'\
          \ max_input_length=1024 max_total_length=1792 quantized=True\nopen-assistant-inference-worker-1\
          \  | 2023-05-18T06:17:31.692411Z ERROR shard-manager: text_generation_launcher:\
          \ \"Error when initializing model\nopen-assistant-inference-worker-1  |\
          \ Traceback (most recent call last):\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/bin/text-generation-server\\\
          \", line 8, in <module>\nopen-assistant-inference-worker-1  |     sys.exit(app())\n\
          open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/typer/main.py\\\
          \", line 311, in __call__\nopen-assistant-inference-worker-1  |     return\
          \ get_command(self)(*args, **kwargs)\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/click/core.py\\\
          \", line 1130, in __call__\nopen-assistant-inference-worker-1  |     return\
          \ self.main(*args, **kwargs)\nopen-assistant-inference-worker-1  |   File\
          \ \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/typer/core.py\\\
          \", line 778, in main\nopen-assistant-inference-worker-1  |     return _main(\n\
          open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/typer/core.py\\\
          \", line 216, in _main\nopen-assistant-inference-worker-1  |     rv = self.invoke(ctx)\n\
          open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/click/core.py\\\
          \", line 1657, in invoke\nopen-assistant-inference-worker-1  |     return\
          \ _process_result(sub_ctx.command.invoke(sub_ctx))\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/click/core.py\\\
          \", line 1404, in invoke\nopen-assistant-inference-worker-1  |     return\
          \ ctx.invoke(self.callback, **ctx.params)\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/click/core.py\\\
          \", line 760, in invoke\nopen-assistant-inference-worker-1  |     return\
          \ __callback(*args, **kwargs)\nopen-assistant-inference-worker-1  |   File\
          \ \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/typer/main.py\\\
          \", line 683, in wrapper\nopen-assistant-inference-worker-1  |     return\
          \ callback(**use_params)  # type: ignore\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/cli.py\\\
          \", line 55, in serve\nopen-assistant-inference-worker-1  |     server.serve(model_id,\
          \ revision, sharded, quantize, uds_path)\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 130, in serve\nopen-assistant-inference-worker-1  |     asyncio.run(serve_inner(model_id,\
          \ revision, sharded, quantize))\nopen-assistant-inference-worker-1  |  \
          \ File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/runners.py\\\
          \", line 44, in run\nopen-assistant-inference-worker-1  |     return loop.run_until_complete(main)\n\
          open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/base_events.py\\\
          \", line 634, in run_until_complete\nopen-assistant-inference-worker-1 \
          \ |     self.run_forever()\nopen-assistant-inference-worker-1  |   File\
          \ \\\"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/base_events.py\\\
          \", line 601, in run_forever\nopen-assistant-inference-worker-1  |     self._run_once()\n\
          open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/base_events.py\\\
          \", line 1905, in _run_once\nopen-assistant-inference-worker-1  |     handle._run()\n\
          open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/events.py\\\
          \", line 80, in _run\nopen-assistant-inference-worker-1  |     self._context.run(self._callback,\
          \ *self._args)\nopen-assistant-inference-worker-1  | > File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 99, in serve_inner\nopen-assistant-inference-worker-1  |     model\
          \ = get_model(model_id, revision, sharded, quantize)\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
          \", line 52, in get_model\nopen-assistant-inference-worker-1  |     config\
          \ = AutoConfig.from_pretrained(model_id, revision=revision)\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/models/auto/configuration_auto.py\\\
          \", line 882, in from_pretrained\nopen-assistant-inference-worker-1  | \
          \    config_class = CONFIG_MAPPING[config_dict[\\\"model_type\\\"]]\nopen-assistant-inference-worker-1\
          \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/models/auto/configuration_auto.py\\\
          \", line 588, in __getitem__\nopen-assistant-inference-worker-1  |     raise\
          \ KeyError(key)\nopen-assistant-inference-worker-1  | KeyError: 'llama'\n\
          open-assistant-inference-worker-1  | \" rank=0\nopen-assistant-inference-worker-1\
          \  | 2023-05-18T06:17:32.251412Z ERROR text_generation_launcher: Shard 0\
          \ failed to start:\nopen-assistant-inference-worker-1  | Traceback (most\
          \ recent call last):\nopen-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/text-generation/bin/text-generation-server\"\
          , line 8, in <module>\nopen-assistant-inference-worker-1  |     sys.exit(app())\n\
          open-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 55, in serve\nopen-assistant-inference-worker-1  |     server.serve(model_id,\
          \ revision, sharded, quantize, uds_path)\nopen-assistant-inference-worker-1\
          \  | \nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 130, in serve\nopen-assistant-inference-worker-1  |     asyncio.run(serve_inner(model_id,\
          \ revision, sharded, quantize))\nopen-assistant-inference-worker-1  | \n\
          open-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\nopen-assistant-inference-worker-1  |     return loop.run_until_complete(main)\n\
          open-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/base_events.py\"\
          , line 647, in run_until_complete\nopen-assistant-inference-worker-1  |\
          \     return future.result()\nopen-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
          \  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 99, in serve_inner\nopen-assistant-inference-worker-1  |     model\
          \ = get_model(model_id, revision, sharded, quantize)\nopen-assistant-inference-worker-1\
          \  | \nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 52, in get_model\nopen-assistant-inference-worker-1  |     config\
          \ = AutoConfig.from_pretrained(model_id, revision=revision)\nopen-assistant-inference-worker-1\
          \  | \nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/models/auto/configuration_auto.py\"\
          , line 882, in from_pretrained\nopen-assistant-inference-worker-1  |   \
          \  config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\nopen-assistant-inference-worker-1\
          \  | \nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/models/auto/configuration_auto.py\"\
          , line 588, in __getitem__\nopen-assistant-inference-worker-1  |     raise\
          \ KeyError(key)\nopen-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
          \  | KeyError: 'llama'\n```"
        updatedAt: '2023-05-18T06:26:09.359Z'
      numEdits: 0
      reactions: []
    id: 6465c5019c627c78f860f92d
    type: comment
  author: pevogam
  content: "I did try to draw inspiration from some commits that have previously added\
    \ models there like https://github.com/LAION-AI/Open-Assistant/commit/6c3519ba558e4f1aa75b859357e6ceb56eac0429\
    \ but it seems this is all they are doing. However, with the above diff I get\n\
    ```\nopen-assistant-inference-worker-1  | 2023-05-18 06:17:30.477 | WARNING  |\
    \ __main__:main:28 - Model config: model_id='TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ'\
    \ max_input_length=1024 max_total_length=1792 quantized=True\nopen-assistant-inference-worker-1\
    \  | 2023-05-18T06:17:31.692411Z ERROR shard-manager: text_generation_launcher:\
    \ \"Error when initializing model\nopen-assistant-inference-worker-1  | Traceback\
    \ (most recent call last):\nopen-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/bin/text-generation-server\\\
    \", line 8, in <module>\nopen-assistant-inference-worker-1  |     sys.exit(app())\n\
    open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/typer/main.py\\\
    \", line 311, in __call__\nopen-assistant-inference-worker-1  |     return get_command(self)(*args,\
    \ **kwargs)\nopen-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/click/core.py\\\
    \", line 1130, in __call__\nopen-assistant-inference-worker-1  |     return self.main(*args,\
    \ **kwargs)\nopen-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/typer/core.py\\\
    \", line 778, in main\nopen-assistant-inference-worker-1  |     return _main(\n\
    open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/typer/core.py\\\
    \", line 216, in _main\nopen-assistant-inference-worker-1  |     rv = self.invoke(ctx)\n\
    open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/click/core.py\\\
    \", line 1657, in invoke\nopen-assistant-inference-worker-1  |     return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
    open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/click/core.py\\\
    \", line 1404, in invoke\nopen-assistant-inference-worker-1  |     return ctx.invoke(self.callback,\
    \ **ctx.params)\nopen-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/click/core.py\\\
    \", line 760, in invoke\nopen-assistant-inference-worker-1  |     return __callback(*args,\
    \ **kwargs)\nopen-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/typer/main.py\\\
    \", line 683, in wrapper\nopen-assistant-inference-worker-1  |     return callback(**use_params)\
    \  # type: ignore\nopen-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/cli.py\\\
    \", line 55, in serve\nopen-assistant-inference-worker-1  |     server.serve(model_id,\
    \ revision, sharded, quantize, uds_path)\nopen-assistant-inference-worker-1  |\
    \   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 130, in serve\nopen-assistant-inference-worker-1  |     asyncio.run(serve_inner(model_id,\
    \ revision, sharded, quantize))\nopen-assistant-inference-worker-1  |   File \\\
    \"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/runners.py\\\", line\
    \ 44, in run\nopen-assistant-inference-worker-1  |     return loop.run_until_complete(main)\n\
    open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/base_events.py\\\
    \", line 634, in run_until_complete\nopen-assistant-inference-worker-1  |    \
    \ self.run_forever()\nopen-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/base_events.py\\\
    \", line 601, in run_forever\nopen-assistant-inference-worker-1  |     self._run_once()\n\
    open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/base_events.py\\\
    \", line 1905, in _run_once\nopen-assistant-inference-worker-1  |     handle._run()\n\
    open-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/events.py\\\
    \", line 80, in _run\nopen-assistant-inference-worker-1  |     self._context.run(self._callback,\
    \ *self._args)\nopen-assistant-inference-worker-1  | > File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 99, in serve_inner\nopen-assistant-inference-worker-1  |     model =\
    \ get_model(model_id, revision, sharded, quantize)\nopen-assistant-inference-worker-1\
    \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
    \", line 52, in get_model\nopen-assistant-inference-worker-1  |     config = AutoConfig.from_pretrained(model_id,\
    \ revision=revision)\nopen-assistant-inference-worker-1  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/models/auto/configuration_auto.py\\\
    \", line 882, in from_pretrained\nopen-assistant-inference-worker-1  |     config_class\
    \ = CONFIG_MAPPING[config_dict[\\\"model_type\\\"]]\nopen-assistant-inference-worker-1\
    \  |   File \\\"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/models/auto/configuration_auto.py\\\
    \", line 588, in __getitem__\nopen-assistant-inference-worker-1  |     raise KeyError(key)\n\
    open-assistant-inference-worker-1  | KeyError: 'llama'\nopen-assistant-inference-worker-1\
    \  | \" rank=0\nopen-assistant-inference-worker-1  | 2023-05-18T06:17:32.251412Z\
    \ ERROR text_generation_launcher: Shard 0 failed to start:\nopen-assistant-inference-worker-1\
    \  | Traceback (most recent call last):\nopen-assistant-inference-worker-1  |\
    \ \nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/text-generation/bin/text-generation-server\"\
    , line 8, in <module>\nopen-assistant-inference-worker-1  |     sys.exit(app())\n\
    open-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1  |  \
    \ File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 55, in serve\nopen-assistant-inference-worker-1  |     server.serve(model_id,\
    \ revision, sharded, quantize, uds_path)\nopen-assistant-inference-worker-1  |\
    \ \nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 130, in serve\nopen-assistant-inference-worker-1  |     asyncio.run(serve_inner(model_id,\
    \ revision, sharded, quantize))\nopen-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
    \  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\nopen-assistant-inference-worker-1  |     return loop.run_until_complete(main)\n\
    open-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1  |  \
    \ File \"/opt/miniconda/envs/text-generation/lib/python3.9/asyncio/base_events.py\"\
    , line 647, in run_until_complete\nopen-assistant-inference-worker-1  |     return\
    \ future.result()\nopen-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
    \  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 99, in serve_inner\nopen-assistant-inference-worker-1  |     model = get_model(model_id,\
    \ revision, sharded, quantize)\nopen-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
    \  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 52, in get_model\nopen-assistant-inference-worker-1  |     config = AutoConfig.from_pretrained(model_id,\
    \ revision=revision)\nopen-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1\
    \  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/models/auto/configuration_auto.py\"\
    , line 882, in from_pretrained\nopen-assistant-inference-worker-1  |     config_class\
    \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\nopen-assistant-inference-worker-1\
    \  | \nopen-assistant-inference-worker-1  |   File \"/opt/miniconda/envs/text-generation/lib/python3.9/site-packages/transformers-4.27.0.dev0-py3.9.egg/transformers/models/auto/configuration_auto.py\"\
    , line 588, in __getitem__\nopen-assistant-inference-worker-1  |     raise KeyError(key)\n\
    open-assistant-inference-worker-1  | \nopen-assistant-inference-worker-1  | KeyError:\
    \ 'llama'\n```"
  created_at: 2023-05-18 05:26:09+00:00
  edited: false
  hidden: false
  id: 6465c5019c627c78f860f92d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-18T21:14:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;pevogam&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pevogam\">@<span class=\"\
          underline\">pevogam</span></a></span>\n\n\t</span></span> I'm afraid you\
          \ can't use GPTQ models with OpenAssistant's repo. That will only work with\
          \ models that can be loaded natively with transformers. That means float16\
          \ repos, like my repo here:  <a href=\"https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-HF\"\
          >https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-HF</a></p>\n\
          <p>But for that you will need a ton of VRAM - 60+ GB for fp16, or ~30GB\
          \ for int8 if you use <code>bitsandbytes</code> (pass <code>load_in_8bit=True</code>\
          \ to the <code>from_pretrained()</code> call).</p>\n<p>You could look into\
          \ modifying the OpenAssistant code to use <a rel=\"nofollow\" href=\"https://github.com/PanQiWei/AutoGPTQ\"\
          >AutoGPTQ</a>. That shouldn't be too hard if you know a bit of Python.</p>\n\
          <p>And/or, wait for the new 4bit version of bitsandbytes to come out, which\
          \ will support easy 4bit quantisation in native transformers. It's still\
          \ in private beta and I've not tried it yet so can't comment on how well\
          \ it works. But in theory it sounds like it'll make it very easy to use\
          \ float16 models in 4bit in any existing Python code.</p>\n"
        raw: '@pevogam I''m afraid you can''t use GPTQ models with OpenAssistant''s
          repo. That will only work with models that can be loaded natively with transformers.
          That means float16 repos, like my repo here:  https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-HF


          But for that you will need a ton of VRAM - 60+ GB for fp16, or ~30GB for
          int8 if you use `bitsandbytes` (pass `load_in_8bit=True` to the `from_pretrained()`
          call).


          You could look into modifying the OpenAssistant code to use [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ).
          That shouldn''t be too hard if you know a bit of Python.


          And/or, wait for the new 4bit version of bitsandbytes to come out, which
          will support easy 4bit quantisation in native transformers. It''s still
          in private beta and I''ve not tried it yet so can''t comment on how well
          it works. But in theory it sounds like it''ll make it very easy to use float16
          models in 4bit in any existing Python code.'
        updatedAt: '2023-05-18T21:14:51.382Z'
      numEdits: 0
      reactions: []
    id: 6466954be0fe831b47975bf7
    type: comment
  author: TheBloke
  content: '@pevogam I''m afraid you can''t use GPTQ models with OpenAssistant''s
    repo. That will only work with models that can be loaded natively with transformers.
    That means float16 repos, like my repo here:  https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-HF


    But for that you will need a ton of VRAM - 60+ GB for fp16, or ~30GB for int8
    if you use `bitsandbytes` (pass `load_in_8bit=True` to the `from_pretrained()`
    call).


    You could look into modifying the OpenAssistant code to use [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ).
    That shouldn''t be too hard if you know a bit of Python.


    And/or, wait for the new 4bit version of bitsandbytes to come out, which will
    support easy 4bit quantisation in native transformers. It''s still in private
    beta and I''ve not tried it yet so can''t comment on how well it works. But in
    theory it sounds like it''ll make it very easy to use float16 models in 4bit in
    any existing Python code.'
  created_at: 2023-05-18 20:14:51+00:00
  edited: false
  hidden: false
  id: 6466954be0fe831b47975bf7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: OpenAssistant/oasst-sft-7-llama-30b-xor
repo_type: model
status: open
target_branch: null
title: I've done the merge and made GGML and GPTQ quantisations for anyone interested
