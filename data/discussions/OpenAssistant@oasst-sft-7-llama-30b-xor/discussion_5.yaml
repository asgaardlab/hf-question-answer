!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pevogam
conflicting_files: null
created_at: 2023-05-04 02:36:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
      fullname: Plamen Dimitrov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pevogam
      type: user
    createdAt: '2023-05-04T03:36:15.000Z'
    data:
      edited: false
      editors:
      - pevogam
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
          fullname: Plamen Dimitrov
          isHf: false
          isPro: false
          name: pevogam
          type: user
        html: "<p>I wanted to use a docker command inspired from <a href=\"https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5/discussions/2#643e891852885f3d3f54743e\"\
          >here</a> like</p>\n<p><code> docker run --gpus \"device=0\" -p 8080:80\
          \ -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:sha-7a1ba58\
          \  --model-id OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5</code><br>but\
          \ there is no <code>OpenAssistant/oasst-sft-7-llama-30b</code> and we have\
          \ to use the cached model instead. Is it possible to simply symlink the\
          \ model into the data folder and use a tag that would map to the folder\
          \ name?</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span>\
          \ Have you tried anything like this?</p>\n"
        raw: "I wanted to use a docker command inspired from [here](https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5/discussions/2#643e891852885f3d3f54743e)\
          \ like\r\n```\r\ndocker run --gpus \"device=0\" -p 8080:80 -v $PWD/data:/data\
          \ ghcr.io/huggingface/text-generation-inference:sha-7a1ba58  --model-id\
          \ OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5```\r\nbut there is no `OpenAssistant/oasst-sft-7-llama-30b`\
          \ and we have to use the cached model instead. Is it possible to simply\
          \ symlink the model into the data folder and use a tag that would map to\
          \ the folder name?\r\n\r\n@olivierdehaene Have you tried anything like this?"
        updatedAt: '2023-05-04T03:36:15.883Z'
      numEdits: 0
      reactions: []
    id: 6453282f3f80ad88c77d0699
    type: comment
  author: pevogam
  content: "I wanted to use a docker command inspired from [here](https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5/discussions/2#643e891852885f3d3f54743e)\
    \ like\r\n```\r\ndocker run --gpus \"device=0\" -p 8080:80 -v $PWD/data:/data\
    \ ghcr.io/huggingface/text-generation-inference:sha-7a1ba58  --model-id OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5```\r\
    \nbut there is no `OpenAssistant/oasst-sft-7-llama-30b` and we have to use the\
    \ cached model instead. Is it possible to simply symlink the model into the data\
    \ folder and use a tag that would map to the folder name?\r\n\r\n@olivierdehaene\
    \ Have you tried anything like this?"
  created_at: 2023-05-04 02:36:15+00:00
  edited: false
  hidden: false
  id: 6453282f3f80ad88c77d0699
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f752ced32077b8ccf36621d574d07fd3.svg
      fullname: Gregory
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kronosprime
      type: user
    createdAt: '2023-05-04T03:40:44.000Z'
    data:
      edited: false
      editors:
      - kronosprime
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f752ced32077b8ccf36621d574d07fd3.svg
          fullname: Gregory
          isHf: false
          isPro: false
          name: kronosprime
          type: user
        html: '<p>I''m literally one step behind you at this very moment, was just
          reading the details of huggingface/text-generation-inference and thinking
          about what I needed to do to run it on the MPS device rather than CUDA.  </p>

          <p>I guess you saw the end of this page:<br><a href="https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/README.md">https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/README.md</a>
          where it talks about running local inference</p>

          '
        raw: "I'm literally one step behind you at this very moment, was just reading\
          \ the details of huggingface/text-generation-inference and thinking about\
          \ what I needed to do to run it on the MPS device rather than CUDA.  \n\n\
          I guess you saw the end of this page:\nhttps://huggingface.co/spaces/huggingchat/chat-ui/blob/main/README.md\
          \ where it talks about running local inference"
        updatedAt: '2023-05-04T03:40:44.770Z'
      numEdits: 0
      reactions: []
    id: 6453293ca0c0a664a253de36
    type: comment
  author: kronosprime
  content: "I'm literally one step behind you at this very moment, was just reading\
    \ the details of huggingface/text-generation-inference and thinking about what\
    \ I needed to do to run it on the MPS device rather than CUDA.  \n\nI guess you\
    \ saw the end of this page:\nhttps://huggingface.co/spaces/huggingchat/chat-ui/blob/main/README.md\
    \ where it talks about running local inference"
  created_at: 2023-05-04 02:40:44+00:00
  edited: false
  hidden: false
  id: 6453293ca0c0a664a253de36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
      fullname: Plamen Dimitrov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pevogam
      type: user
    createdAt: '2023-05-04T04:10:19.000Z'
    data:
      edited: false
      editors:
      - pevogam
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
          fullname: Plamen Dimitrov
          isHf: false
          isPro: false
          name: pevogam
          type: user
        html: '<p>I think it is possible to do it in a simpler way without adapters
          and additional dependencies and I have managed to do so with existing pythia
          models just fine. Upon further inspection of the situation here, I try <code>docker
          run --gpus "device=0" -p 8080:80 -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:sha-7a1ba58
          --model-id OpenAssistant/oasst-sft-7-llama-30b</code> and get</p>

          <pre><code>Repository Not Found for url: https://huggingface.co/OpenAssistant/oasst-sft-7-llama-30b/resolve/main/config.json.

          Please make sure you specified the correct `repo_id` and `repo_type`.

          If you are trying to access a private or gated repo, make sure you are authenticated.

          </code></pre>

          <p>in particular I chose the name of the folder from another part of the
          same error message</p>

          <pre><code>OSError: OpenAssistant/oasst-sft-7-llama-30b is not a local folder
          and is not a valid model identifier listed on ''https://huggingface.co/models''

          </code></pre>

          <p>which told me that the text inference server would expect <code>OpenAssistant/oasst-sft-7-llama-30b</code>
          folder or symlink in its data directory, the latter extracted from</p>

          <pre><code>volume=$PWD/data # share a volume with the Docker container to
          avoid downloading weights every run

          </code></pre>

          <p>in <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference#docker">https://github.com/huggingface/text-generation-inference#docker</a>.</p>

          <p>Alas, this is still not enough and perhaps we need a different reference
          hash or more tweaks.</p>

          '
        raw: 'I think it is possible to do it in a simpler way without adapters and
          additional dependencies and I have managed to do so with existing pythia
          models just fine. Upon further inspection of the situation here, I try `docker
          run --gpus "device=0" -p 8080:80 -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:sha-7a1ba58
          --model-id OpenAssistant/oasst-sft-7-llama-30b` and get

          ```

          Repository Not Found for url: https://huggingface.co/OpenAssistant/oasst-sft-7-llama-30b/resolve/main/config.json.

          Please make sure you specified the correct `repo_id` and `repo_type`.

          If you are trying to access a private or gated repo, make sure you are authenticated.

          ```

          in particular I chose the name of the folder from another part of the same
          error message

          ```

          OSError: OpenAssistant/oasst-sft-7-llama-30b is not a local folder and is
          not a valid model identifier listed on ''https://huggingface.co/models''

          ```

          which told me that the text inference server would expect `OpenAssistant/oasst-sft-7-llama-30b`
          folder or symlink in its data directory, the latter extracted from

          ```

          volume=$PWD/data # share a volume with the Docker container to avoid downloading
          weights every run

          ```

          in https://github.com/huggingface/text-generation-inference#docker.


          Alas, this is still not enough and perhaps we need a different reference
          hash or more tweaks.'
        updatedAt: '2023-05-04T04:10:19.463Z'
      numEdits: 0
      reactions: []
    id: 6453302b3a794b2d9b20c501
    type: comment
  author: pevogam
  content: 'I think it is possible to do it in a simpler way without adapters and
    additional dependencies and I have managed to do so with existing pythia models
    just fine. Upon further inspection of the situation here, I try `docker run --gpus
    "device=0" -p 8080:80 -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:sha-7a1ba58
    --model-id OpenAssistant/oasst-sft-7-llama-30b` and get

    ```

    Repository Not Found for url: https://huggingface.co/OpenAssistant/oasst-sft-7-llama-30b/resolve/main/config.json.

    Please make sure you specified the correct `repo_id` and `repo_type`.

    If you are trying to access a private or gated repo, make sure you are authenticated.

    ```

    in particular I chose the name of the folder from another part of the same error
    message

    ```

    OSError: OpenAssistant/oasst-sft-7-llama-30b is not a local folder and is not
    a valid model identifier listed on ''https://huggingface.co/models''

    ```

    which told me that the text inference server would expect `OpenAssistant/oasst-sft-7-llama-30b`
    folder or symlink in its data directory, the latter extracted from

    ```

    volume=$PWD/data # share a volume with the Docker container to avoid downloading
    weights every run

    ```

    in https://github.com/huggingface/text-generation-inference#docker.


    Alas, this is still not enough and perhaps we need a different reference hash
    or more tweaks.'
  created_at: 2023-05-04 03:10:19+00:00
  edited: false
  hidden: false
  id: 6453302b3a794b2d9b20c501
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f752ced32077b8ccf36621d574d07fd3.svg
      fullname: Gregory
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kronosprime
      type: user
    createdAt: '2023-05-04T05:07:05.000Z'
    data:
      edited: false
      editors:
      - kronosprime
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f752ced32077b8ccf36621d574d07fd3.svg
          fullname: Gregory
          isHf: false
          isPro: false
          name: kronosprime
          type: user
        html: "<p>If anyone has an idea, I would love to hear it.  But one issue I\
          \ noticed is that oasst-sft-7-llama-30b and text-generation-inference require\
          \ different versions of the transformers package.  Most notably, the text-generation-inference\
          \ requires the transformers library to have a section for 'bloom'.  When\
          \ I ran the inference with the version required by oasst the error was:</p>\n\
          <pre><code>ModuleNotFoundError: No module named 'transformers.models.bloom.parallel_layers'\n\
          </code></pre>\n<p>And here's the error log from where I installed <code>text-generation-inference</code>\
          \ locally, with a virtual environment rather than docker.  I built, installed\
          \ it with 'make install', and ran the following using :<br><code>text-generation-launcher\
          \ --model-id ~/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b\
          \ </code></p>\n<p>It was looking good, it found the model and converted\
          \ it to safetensors, but then...</p>\n<pre><code>2023-05-04T04:44:17.786682Z\
          \  INFO text_generation_launcher: Args { model_id: \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b\"\
          , revision: None, sharded: None, num_shard: None, quantize: false, max_concurrent_requests:\
          \ 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 1000, max_total_tokens:\
          \ 1512, max_batch_size: None, waiting_served_ratio: 1.2, max_batch_total_tokens:\
          \ 32000, max_waiting_tokens: 20, port: 3000, shard_uds_path: \"/tmp/text-generation-server\"\
          , master_addr: \"localhost\", master_port: 29500, huggingface_hub_cache:\
          \ None, weights_cache_override: None, disable_custom_kernels: false, json_output:\
          \ false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None,\
          \ watermark_delta: None, env: false }\n2023-05-04T04:44:17.786799Z  INFO\
          \ text_generation_launcher: Starting download process.\n2023-05-04T04:44:19.031428Z\
          \  WARN download: text_generation_launcher: No safetensors weights found\
          \ for model /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b\
          \ at revision None. Converting PyTorch weights to safetensors.\n\n2023-05-04T04:44:19.031609Z\
          \  INFO download: text_generation_launcher: Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00007-of-00007.bin\
          \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00007-of-00007.safetensors.\n\
          \n2023-05-04T04:44:19.031778Z  INFO download: text_generation_launcher:\
          \ Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00006-of-00007.bin\
          \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00006-of-00007.safetensors.\n\
          \n2023-05-04T04:44:19.031871Z  INFO download: text_generation_launcher:\
          \ Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00001-of-00007.bin\
          \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00001-of-00007.safetensors.\n\
          \n2023-05-04T04:44:19.031959Z  INFO download: text_generation_launcher:\
          \ Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00003-of-00007.bin\
          \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00003-of-00007.safetensors.\n\
          \n2023-05-04T04:44:19.032117Z  INFO download: text_generation_launcher:\
          \ Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00004-of-00007.bin\
          \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00004-of-00007.safetensors.\n\
          \n2023-05-04T04:45:14.801473Z  INFO download: text_generation_launcher:\
          \ Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00005-of-00007.bin\
          \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00005-of-00007.safetensors.\n\
          \n2023-05-04T04:45:14.801596Z  INFO download: text_generation_launcher:\
          \ Convert: [1/7] -- ETA: 0:05:30\n\n2023-05-04T04:46:09.529107Z  INFO download:\
          \ text_generation_launcher: Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00002-of-00007.bin\
          \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00002-of-00007.safetensors.\n\
          \n2023-05-04T04:46:09.529708Z  INFO download: text_generation_launcher:\
          \ Convert: [2/7] -- ETA: 0:04:35\n\n2023-05-04T04:46:09.706104Z  INFO download:\
          \ text_generation_launcher: Convert: [3/7] -- ETA: 0:02:26.666668\n\n2023-05-04T04:46:11.292773Z\
          \  INFO download: text_generation_launcher: Convert: [4/7] -- ETA: 0:01:24\n\
          \n2023-05-04T04:46:11.419401Z  INFO download: text_generation_launcher:\
          \ Convert: [5/7] -- ETA: 0:00:44.800000\n\n2023-05-04T04:46:11.762799Z \
          \ INFO download: text_generation_launcher: Convert: [6/7] -- ETA: 0:00:18.666667\n\
          \n2023-05-04T04:46:19.904766Z  INFO download: text_generation_launcher:\
          \ Convert: [7/7] -- ETA: 0\n\n2023-05-04T04:46:20.296744Z  INFO text_generation_launcher:\
          \ Successfully downloaded weights.\n2023-05-04T04:46:20.297360Z  INFO text_generation_launcher:\
          \ Starting shard 0\n2023-05-04T04:46:30.316717Z  INFO text_generation_launcher:\
          \ Waiting for shard 0 to be ready...\n2023-05-04T04:46:40.352174Z  INFO\
          \ text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:46:50.377670Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:00.402327Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:10.440521Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:20.505151Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:30.597273Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:40.630386Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:50.674882Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:00.677545Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:10.684952Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:20.794592Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:30.798020Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:40.848794Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:50.913512Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:00.971170Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:11.051961Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:21.139972Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:31.189893Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:41.243413Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:51.329588Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:50:01.396801Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:50:04.841977Z\
          \ ERROR shard-manager: text_generation_launcher: Error when initializing\
          \ model\nTraceback (most recent call last):\n  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/bin/text-generation-server\"\
          , line 8, in &lt;module&gt;\n    sys.exit(app())\n  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/click/core.py\"\
          , line 1130, in __call__\n    return self.main(*args, **kwargs)\n  File\
          \ \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/click/core.py\"\
          , line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/click/core.py\"\
          , line 760, in invoke\n    return __callback(*args, **kwargs)\n  File \"\
          /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/cli.py\"\
          , line 58, in serve\n    server.serve(model_id, revision, sharded, quantize,\
          \ uds_path)\n  File \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/server.py\"\
          , line 155, in serve\n    asyncio.run(serve_inner(model_id, revision, sharded,\
          \ quantize))\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\"\
          , line 629, in run_until_complete\n    self.run_forever()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\"\
          , line 596, in run_forever\n    self._run_once()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\"\
          , line 1890, in _run_once\n    handle._run()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          &gt; File \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/server.py\"\
          , line 124, in serve_inner\n    model = get_model(model_id, revision, sharded,\
          \ quantize)\n  File \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/models/__init__.py\"\
          , line 137, in get_model\n    return llama_cls(model_id, revision, quantize=quantize)\n\
          \  File \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/models/causal_lm.py\"\
          , line 479, in __init__\n    super(CausalLM, self).__init__(\n  File \"\
          /Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/models/model.py\"\
          , line 26, in __init__\n    self.all_special_ids = set(tokenizer.all_special_ids)\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_base.py\"\
          , line 1299, in all_special_ids\n    all_ids = self.convert_tokens_to_ids(all_toks)\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_fast.py\"\
          , line 254, in convert_tokens_to_ids\n    ids.append(self._convert_token_to_id_with_added_voc(token))\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_fast.py\"\
          , line 260, in _convert_token_to_id_with_added_voc\n    return self.unk_token_id\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_base.py\"\
          , line 1142, in unk_token_id\n    return self.convert_tokens_to_ids(self.unk_token)\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_fast.py\"\
          , line 250, in convert_tokens_to_ids\n</code></pre>\n"
        raw: "If anyone has an idea, I would love to hear it.  But one issue I noticed\
          \ is that oasst-sft-7-llama-30b and text-generation-inference require different\
          \ versions of the transformers package.  Most notably, the text-generation-inference\
          \ requires the transformers library to have a section for 'bloom'.  When\
          \ I ran the inference with the version required by oasst the error was:\n\
          ```\nModuleNotFoundError: No module named 'transformers.models.bloom.parallel_layers'\n\
          ```\n\nAnd here's the error log from where I installed `text-generation-inference`\
          \ locally, with a virtual environment rather than docker.  I built, installed\
          \ it with 'make install', and ran the following using :\n`text-generation-launcher\
          \ --model-id ~/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b\
          \ `\n\nIt was looking good, it found the model and converted it to safetensors,\
          \ but then...\n```\n2023-05-04T04:44:17.786682Z  INFO text_generation_launcher:\
          \ Args { model_id: \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b\"\
          , revision: None, sharded: None, num_shard: None, quantize: false, max_concurrent_requests:\
          \ 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 1000, max_total_tokens:\
          \ 1512, max_batch_size: None, waiting_served_ratio: 1.2, max_batch_total_tokens:\
          \ 32000, max_waiting_tokens: 20, port: 3000, shard_uds_path: \"/tmp/text-generation-server\"\
          , master_addr: \"localhost\", master_port: 29500, huggingface_hub_cache:\
          \ None, weights_cache_override: None, disable_custom_kernels: false, json_output:\
          \ false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None,\
          \ watermark_delta: None, env: false }\n2023-05-04T04:44:17.786799Z  INFO\
          \ text_generation_launcher: Starting download process.\n2023-05-04T04:44:19.031428Z\
          \  WARN download: text_generation_launcher: No safetensors weights found\
          \ for model /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b\
          \ at revision None. Converting PyTorch weights to safetensors.\n\n2023-05-04T04:44:19.031609Z\
          \  INFO download: text_generation_launcher: Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00007-of-00007.bin\
          \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00007-of-00007.safetensors.\n\
          \n2023-05-04T04:44:19.031778Z  INFO download: text_generation_launcher:\
          \ Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00006-of-00007.bin\
          \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00006-of-00007.safetensors.\n\
          \n2023-05-04T04:44:19.031871Z  INFO download: text_generation_launcher:\
          \ Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00001-of-00007.bin\
          \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00001-of-00007.safetensors.\n\
          \n2023-05-04T04:44:19.031959Z  INFO download: text_generation_launcher:\
          \ Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00003-of-00007.bin\
          \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00003-of-00007.safetensors.\n\
          \n2023-05-04T04:44:19.032117Z  INFO download: text_generation_launcher:\
          \ Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00004-of-00007.bin\
          \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00004-of-00007.safetensors.\n\
          \n2023-05-04T04:45:14.801473Z  INFO download: text_generation_launcher:\
          \ Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00005-of-00007.bin\
          \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00005-of-00007.safetensors.\n\
          \n2023-05-04T04:45:14.801596Z  INFO download: text_generation_launcher:\
          \ Convert: [1/7] -- ETA: 0:05:30\n\n2023-05-04T04:46:09.529107Z  INFO download:\
          \ text_generation_launcher: Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00002-of-00007.bin\
          \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00002-of-00007.safetensors.\n\
          \n2023-05-04T04:46:09.529708Z  INFO download: text_generation_launcher:\
          \ Convert: [2/7] -- ETA: 0:04:35\n\n2023-05-04T04:46:09.706104Z  INFO download:\
          \ text_generation_launcher: Convert: [3/7] -- ETA: 0:02:26.666668\n\n2023-05-04T04:46:11.292773Z\
          \  INFO download: text_generation_launcher: Convert: [4/7] -- ETA: 0:01:24\n\
          \n2023-05-04T04:46:11.419401Z  INFO download: text_generation_launcher:\
          \ Convert: [5/7] -- ETA: 0:00:44.800000\n\n2023-05-04T04:46:11.762799Z \
          \ INFO download: text_generation_launcher: Convert: [6/7] -- ETA: 0:00:18.666667\n\
          \n2023-05-04T04:46:19.904766Z  INFO download: text_generation_launcher:\
          \ Convert: [7/7] -- ETA: 0\n\n2023-05-04T04:46:20.296744Z  INFO text_generation_launcher:\
          \ Successfully downloaded weights.\n2023-05-04T04:46:20.297360Z  INFO text_generation_launcher:\
          \ Starting shard 0\n2023-05-04T04:46:30.316717Z  INFO text_generation_launcher:\
          \ Waiting for shard 0 to be ready...\n2023-05-04T04:46:40.352174Z  INFO\
          \ text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:46:50.377670Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:00.402327Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:10.440521Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:20.505151Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:30.597273Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:40.630386Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:50.674882Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:00.677545Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:10.684952Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:20.794592Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:30.798020Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:40.848794Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:50.913512Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:00.971170Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:11.051961Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:21.139972Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:31.189893Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:41.243413Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:51.329588Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:50:01.396801Z\
          \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:50:04.841977Z\
          \ ERROR shard-manager: text_generation_launcher: Error when initializing\
          \ model\nTraceback (most recent call last):\n  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/bin/text-generation-server\"\
          , line 8, in <module>\n    sys.exit(app())\n  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/click/core.py\"\
          , line 1130, in __call__\n    return self.main(*args, **kwargs)\n  File\
          \ \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/click/core.py\"\
          , line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/click/core.py\"\
          , line 760, in invoke\n    return __callback(*args, **kwargs)\n  File \"\
          /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/cli.py\"\
          , line 58, in serve\n    server.serve(model_id, revision, sharded, quantize,\
          \ uds_path)\n  File \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/server.py\"\
          , line 155, in serve\n    asyncio.run(serve_inner(model_id, revision, sharded,\
          \ quantize))\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\"\
          , line 629, in run_until_complete\n    self.run_forever()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\"\
          , line 596, in run_forever\n    self._run_once()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\"\
          , line 1890, in _run_once\n    handle._run()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          > File \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/server.py\"\
          , line 124, in serve_inner\n    model = get_model(model_id, revision, sharded,\
          \ quantize)\n  File \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/models/__init__.py\"\
          , line 137, in get_model\n    return llama_cls(model_id, revision, quantize=quantize)\n\
          \  File \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/models/causal_lm.py\"\
          , line 479, in __init__\n    super(CausalLM, self).__init__(\n  File \"\
          /Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/models/model.py\"\
          , line 26, in __init__\n    self.all_special_ids = set(tokenizer.all_special_ids)\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_base.py\"\
          , line 1299, in all_special_ids\n    all_ids = self.convert_tokens_to_ids(all_toks)\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_fast.py\"\
          , line 254, in convert_tokens_to_ids\n    ids.append(self._convert_token_to_id_with_added_voc(token))\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_fast.py\"\
          , line 260, in _convert_token_to_id_with_added_voc\n    return self.unk_token_id\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_base.py\"\
          , line 1142, in unk_token_id\n    return self.convert_tokens_to_ids(self.unk_token)\n\
          \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_fast.py\"\
          , line 250, in convert_tokens_to_ids\n```"
        updatedAt: '2023-05-04T05:07:05.715Z'
      numEdits: 0
      reactions: []
    id: 64533d793a794b2d9b21edc7
    type: comment
  author: kronosprime
  content: "If anyone has an idea, I would love to hear it.  But one issue I noticed\
    \ is that oasst-sft-7-llama-30b and text-generation-inference require different\
    \ versions of the transformers package.  Most notably, the text-generation-inference\
    \ requires the transformers library to have a section for 'bloom'.  When I ran\
    \ the inference with the version required by oasst the error was:\n```\nModuleNotFoundError:\
    \ No module named 'transformers.models.bloom.parallel_layers'\n```\n\nAnd here's\
    \ the error log from where I installed `text-generation-inference` locally, with\
    \ a virtual environment rather than docker.  I built, installed it with 'make\
    \ install', and ran the following using :\n`text-generation-launcher --model-id\
    \ ~/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b `\n\nIt was looking\
    \ good, it found the model and converted it to safetensors, but then...\n```\n\
    2023-05-04T04:44:17.786682Z  INFO text_generation_launcher: Args { model_id: \"\
    /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b\"\
    , revision: None, sharded: None, num_shard: None, quantize: false, max_concurrent_requests:\
    \ 128, max_best_of: 2, max_stop_sequences: 4, max_input_length: 1000, max_total_tokens:\
    \ 1512, max_batch_size: None, waiting_served_ratio: 1.2, max_batch_total_tokens:\
    \ 32000, max_waiting_tokens: 20, port: 3000, shard_uds_path: \"/tmp/text-generation-server\"\
    , master_addr: \"localhost\", master_port: 29500, huggingface_hub_cache: None,\
    \ weights_cache_override: None, disable_custom_kernels: false, json_output: false,\
    \ otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None, watermark_delta:\
    \ None, env: false }\n2023-05-04T04:44:17.786799Z  INFO text_generation_launcher:\
    \ Starting download process.\n2023-05-04T04:44:19.031428Z  WARN download: text_generation_launcher:\
    \ No safetensors weights found for model /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b\
    \ at revision None. Converting PyTorch weights to safetensors.\n\n2023-05-04T04:44:19.031609Z\
    \  INFO download: text_generation_launcher: Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00007-of-00007.bin\
    \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00007-of-00007.safetensors.\n\
    \n2023-05-04T04:44:19.031778Z  INFO download: text_generation_launcher: Convert\
    \ /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00006-of-00007.bin\
    \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00006-of-00007.safetensors.\n\
    \n2023-05-04T04:44:19.031871Z  INFO download: text_generation_launcher: Convert\
    \ /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00001-of-00007.bin\
    \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00001-of-00007.safetensors.\n\
    \n2023-05-04T04:44:19.031959Z  INFO download: text_generation_launcher: Convert\
    \ /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00003-of-00007.bin\
    \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00003-of-00007.safetensors.\n\
    \n2023-05-04T04:44:19.032117Z  INFO download: text_generation_launcher: Convert\
    \ /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00004-of-00007.bin\
    \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00004-of-00007.safetensors.\n\
    \n2023-05-04T04:45:14.801473Z  INFO download: text_generation_launcher: Convert\
    \ /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00005-of-00007.bin\
    \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00005-of-00007.safetensors.\n\
    \n2023-05-04T04:45:14.801596Z  INFO download: text_generation_launcher: Convert:\
    \ [1/7] -- ETA: 0:05:30\n\n2023-05-04T04:46:09.529107Z  INFO download: text_generation_launcher:\
    \ Convert /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/pytorch_model-00002-of-00007.bin\
    \ to /Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/oasst-sft-7-llama-30b/model-00002-of-00007.safetensors.\n\
    \n2023-05-04T04:46:09.529708Z  INFO download: text_generation_launcher: Convert:\
    \ [2/7] -- ETA: 0:04:35\n\n2023-05-04T04:46:09.706104Z  INFO download: text_generation_launcher:\
    \ Convert: [3/7] -- ETA: 0:02:26.666668\n\n2023-05-04T04:46:11.292773Z  INFO download:\
    \ text_generation_launcher: Convert: [4/7] -- ETA: 0:01:24\n\n2023-05-04T04:46:11.419401Z\
    \  INFO download: text_generation_launcher: Convert: [5/7] -- ETA: 0:00:44.800000\n\
    \n2023-05-04T04:46:11.762799Z  INFO download: text_generation_launcher: Convert:\
    \ [6/7] -- ETA: 0:00:18.666667\n\n2023-05-04T04:46:19.904766Z  INFO download:\
    \ text_generation_launcher: Convert: [7/7] -- ETA: 0\n\n2023-05-04T04:46:20.296744Z\
    \  INFO text_generation_launcher: Successfully downloaded weights.\n2023-05-04T04:46:20.297360Z\
    \  INFO text_generation_launcher: Starting shard 0\n2023-05-04T04:46:30.316717Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:46:40.352174Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:46:50.377670Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:00.402327Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:10.440521Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:20.505151Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:30.597273Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:40.630386Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:47:50.674882Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:00.677545Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:10.684952Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:20.794592Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:30.798020Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:40.848794Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:48:50.913512Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:00.971170Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:11.051961Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:21.139972Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:31.189893Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:41.243413Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:49:51.329588Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:50:01.396801Z\
    \  INFO text_generation_launcher: Waiting for shard 0 to be ready...\n2023-05-04T04:50:04.841977Z\
    \ ERROR shard-manager: text_generation_launcher: Error when initializing model\n\
    Traceback (most recent call last):\n  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/bin/text-generation-server\"\
    , line 8, in <module>\n    sys.exit(app())\n  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/typer/main.py\"\
    , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n  File\
    \ \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/click/core.py\"\
    , line 1130, in __call__\n    return self.main(*args, **kwargs)\n  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/typer/core.py\"\
    , line 778, in main\n    return _main(\n  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/click/core.py\"\
    , line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
    \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/click/core.py\"\
    , line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n \
    \ File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/click/core.py\"\
    , line 760, in invoke\n    return __callback(*args, **kwargs)\n  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n  File\
    \ \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/cli.py\"\
    , line 58, in serve\n    server.serve(model_id, revision, sharded, quantize, uds_path)\n\
    \  File \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/server.py\"\
    , line 155, in serve\n    asyncio.run(serve_inner(model_id, revision, sharded,\
    \ quantize))\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\"\
    , line 629, in run_until_complete\n    self.run_forever()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\"\
    , line 596, in run_forever\n    self._run_once()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\"\
    , line 1890, in _run_once\n    handle._run()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run\n    self._context.run(self._callback, *self._args)\n> File\
    \ \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/server.py\"\
    , line 124, in serve_inner\n    model = get_model(model_id, revision, sharded,\
    \ quantize)\n  File \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/models/__init__.py\"\
    , line 137, in get_model\n    return llama_cls(model_id, revision, quantize=quantize)\n\
    \  File \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/models/causal_lm.py\"\
    , line 479, in __init__\n    super(CausalLM, self).__init__(\n  File \"/Users/kronosprime/Workspace/LLM/text-generation-inference/server/text_generation_server/models/model.py\"\
    , line 26, in __init__\n    self.all_special_ids = set(tokenizer.all_special_ids)\n\
    \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_base.py\"\
    , line 1299, in all_special_ids\n    all_ids = self.convert_tokens_to_ids(all_toks)\n\
    \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_fast.py\"\
    , line 254, in convert_tokens_to_ids\n    ids.append(self._convert_token_to_id_with_added_voc(token))\n\
    \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_fast.py\"\
    , line 260, in _convert_token_to_id_with_added_voc\n    return self.unk_token_id\n\
    \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_base.py\"\
    , line 1142, in unk_token_id\n    return self.convert_tokens_to_ids(self.unk_token)\n\
    \  File \"/Users/kronosprime/Workspace/oasst-sft-7-llama-30b-xor/xor_venv/lib/python3.9/site-packages/transformers-4.29.0.dev0-py3.9.egg/transformers/tokenization_utils_fast.py\"\
    , line 250, in convert_tokens_to_ids\n```"
  created_at: 2023-05-04 04:07:05+00:00
  edited: false
  hidden: false
  id: 64533d793a794b2d9b21edc7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: OpenAssistant/oasst-sft-7-llama-30b-xor
repo_type: model
status: open
target_branch: null
title: 'Could we run a XOR converted model using docker + huggingface/text-generation-inference? '
