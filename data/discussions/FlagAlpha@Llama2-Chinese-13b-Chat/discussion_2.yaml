!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vtiyyal1
conflicting_files: null
created_at: 2023-07-30 20:07:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d782569b2e4c0805079f566b64c4b95b.svg
      fullname: Vijay Murari Tiyyala
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vtiyyal1
      type: user
    createdAt: '2023-07-30T21:07:59.000Z'
    data:
      edited: false
      editors:
      - vtiyyal1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9669445753097534
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d782569b2e4c0805079f566b64c4b95b.svg
          fullname: Vijay Murari Tiyyala
          isHf: false
          isPro: false
          name: vtiyyal1
          type: user
        html: '<p>I tried the online chat llama.family it''s generating english sentences
          as a reply even after modifying the system prompt. Is this because the model
          is not really aligned for QA task? because of the architecture ?<br>I am
          asking this because I fine-tuned on QA task but with very lengthy contexts,
          but looks like the model is not learning much, it doesn''t generate anything
          when asked something in lengthy after fine-tuning. It generates responses
          to small length questions.  I''m confused if whether I did something wrong
          while fine-tuning or it''s just the model architecture. Also I would like
          to know more about your approach of training it for QA task for translation.
          Would appreciate any suggestions.</p>

          '
        raw: "I tried the online chat llama.family it's generating english sentences\
          \ as a reply even after modifying the system prompt. Is this because the\
          \ model is not really aligned for QA task? because of the architecture ?\r\
          \nI am asking this because I fine-tuned on QA task but with very lengthy\
          \ contexts, but looks like the model is not learning much, it doesn't generate\
          \ anything when asked something in lengthy after fine-tuning. It generates\
          \ responses to small length questions.  I'm confused if whether I did something\
          \ wrong while fine-tuning or it's just the model architecture. Also I would\
          \ like to know more about your approach of training it for QA task for translation.\
          \ Would appreciate any suggestions."
        updatedAt: '2023-07-30T21:07:59.741Z'
      numEdits: 0
      reactions: []
    id: 64c6d12f5e5bc55a922997f2
    type: comment
  author: vtiyyal1
  content: "I tried the online chat llama.family it's generating english sentences\
    \ as a reply even after modifying the system prompt. Is this because the model\
    \ is not really aligned for QA task? because of the architecture ?\r\nI am asking\
    \ this because I fine-tuned on QA task but with very lengthy contexts, but looks\
    \ like the model is not learning much, it doesn't generate anything when asked\
    \ something in lengthy after fine-tuning. It generates responses to small length\
    \ questions.  I'm confused if whether I did something wrong while fine-tuning\
    \ or it's just the model architecture. Also I would like to know more about your\
    \ approach of training it for QA task for translation. Would appreciate any suggestions."
  created_at: 2023-07-30 20:07:59+00:00
  edited: false
  hidden: false
  id: 64c6d12f5e5bc55a922997f2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: FlagAlpha/Llama2-Chinese-13b-Chat
repo_type: model
status: open
target_branch: null
title: fine-tuning and model capability
