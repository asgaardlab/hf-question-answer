!!python/object:huggingface_hub.community.DiscussionWithDetails
author: stevensu
conflicting_files: null
created_at: 2023-08-07 13:07:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/47cc656377b913a10583811fbd295ad6.svg
      fullname: wei su
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: stevensu
      type: user
    createdAt: '2023-08-07T14:07:54.000Z'
    data:
      edited: false
      editors:
      - stevensu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5135547518730164
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/47cc656377b913a10583811fbd295ad6.svg
          fullname: wei su
          isHf: false
          isPro: false
          name: stevensu
          type: user
        html: "<p>A10 ,  \u6D4B\u8BD5\u4E86meta \u5B98\u65B9\u7684llama2-13b-chat\
          \ \u52A0\u8F7D\u6B63\u5E38\uFF0C\u4F46\u662F\u52A0\u8F7DLlama2-Chinese-13b-Chat\
          \ \u51FA\u73B0CUDA out of memory<br>from vllm import LLM, SamplingParams<br>prompts\
          \ = [<br>    \"Hello, my name is\",<br>    \"The president of the United\
          \ States is\",<br>    \"The capital of France is\",<br>    \"The future\
          \ of AI is\",<br>]<br>sampling_params = SamplingParams(temperature=0.8,\
          \ top_p=0.95)<br>llm = LLM(model=\"./Llama2-Chinese-13b-Chat\")<br> outputs\
          \ = llm.generate(prompts, sampling_params)</p>\n<h1 id=\"-print-the-outputs\"\
          ># Print the outputs.</h1>\n<p> for output in outputs:<br>     prompt =\
          \ output.prompt<br>     generated_text = output.outputs[0].text<br>    \
          \ print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")</p>\n"
        raw: "A10 ,  \u6D4B\u8BD5\u4E86meta \u5B98\u65B9\u7684llama2-13b-chat \u52A0\
          \u8F7D\u6B63\u5E38\uFF0C\u4F46\u662F\u52A0\u8F7DLlama2-Chinese-13b-Chat\
          \ \u51FA\u73B0CUDA out of memory \r\nfrom vllm import LLM, SamplingParams\r\
          \nprompts = [\r\n    \"Hello, my name is\",\r\n    \"The president of the\
          \ United States is\",\r\n    \"The capital of France is\",\r\n    \"The\
          \ future of AI is\",\r\n]\r\nsampling_params = SamplingParams(temperature=0.8,\
          \ top_p=0.95)\r\nllm = LLM(model=\"./Llama2-Chinese-13b-Chat\")\r\n outputs\
          \ = llm.generate(prompts, sampling_params)\r\n\r\n# # Print the outputs.\r\
          \n for output in outputs:\r\n     prompt = output.prompt\r\n     generated_text\
          \ = output.outputs[0].text\r\n     print(f\"Prompt: {prompt!r}, Generated\
          \ text: {generated_text!r}\")\r\n"
        updatedAt: '2023-08-07T14:07:54.841Z'
      numEdits: 0
      reactions: []
    id: 64d0faba7c24890fb4f39b0a
    type: comment
  author: stevensu
  content: "A10 ,  \u6D4B\u8BD5\u4E86meta \u5B98\u65B9\u7684llama2-13b-chat \u52A0\
    \u8F7D\u6B63\u5E38\uFF0C\u4F46\u662F\u52A0\u8F7DLlama2-Chinese-13b-Chat \u51FA\
    \u73B0CUDA out of memory \r\nfrom vllm import LLM, SamplingParams\r\nprompts =\
    \ [\r\n    \"Hello, my name is\",\r\n    \"The president of the United States\
    \ is\",\r\n    \"The capital of France is\",\r\n    \"The future of AI is\",\r\
    \n]\r\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\r\nllm =\
    \ LLM(model=\"./Llama2-Chinese-13b-Chat\")\r\n outputs = llm.generate(prompts,\
    \ sampling_params)\r\n\r\n# # Print the outputs.\r\n for output in outputs:\r\n\
    \     prompt = output.prompt\r\n     generated_text = output.outputs[0].text\r\
    \n     print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\n"
  created_at: 2023-08-07 13:07:54+00:00
  edited: false
  hidden: false
  id: 64d0faba7c24890fb4f39b0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/47cc656377b913a10583811fbd295ad6.svg
      fullname: wei su
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: stevensu
      type: user
    createdAt: '2023-08-10T01:48:22.000Z'
    data:
      edited: false
      editors:
      - stevensu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8387818336486816
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/47cc656377b913a10583811fbd295ad6.svg
          fullname: wei su
          isHf: false
          isPro: false
          name: stevensu
          type: user
        html: '<p>I found reason , because I set load_in_8bit=True with HF transformers
          , but vLLM not support 8bit yet , </p>

          '
        raw: 'I found reason , because I set load_in_8bit=True with HF transformers
          , but vLLM not support 8bit yet , '
        updatedAt: '2023-08-10T01:48:22.596Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64d441e6c1f4edb3fafa2c8e
    id: 64d441e6c1f4edb3fafa2c8c
    type: comment
  author: stevensu
  content: 'I found reason , because I set load_in_8bit=True with HF transformers
    , but vLLM not support 8bit yet , '
  created_at: 2023-08-10 00:48:22+00:00
  edited: false
  hidden: false
  id: 64d441e6c1f4edb3fafa2c8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/47cc656377b913a10583811fbd295ad6.svg
      fullname: wei su
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: stevensu
      type: user
    createdAt: '2023-08-10T01:48:22.000Z'
    data:
      status: closed
    id: 64d441e6c1f4edb3fafa2c8e
    type: status-change
  author: stevensu
  created_at: 2023-08-10 00:48:22+00:00
  id: 64d441e6c1f4edb3fafa2c8e
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: FlagAlpha/Llama2-Chinese-13b-Chat
repo_type: model
status: closed
target_branch: null
title: "vLLM 0.1.3 \u8FD0\u884C CUDA out of memory "
