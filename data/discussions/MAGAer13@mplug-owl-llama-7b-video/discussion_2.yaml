!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LeafXR
conflicting_files: null
created_at: 2023-07-14 02:49:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8cb82ecff62e54abb6059b0068a5e4c8.svg
      fullname: Xurong Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LeafXR
      type: user
    createdAt: '2023-07-14T03:49:37.000Z'
    data:
      edited: false
      editors:
      - LeafXR
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5341346263885498
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8cb82ecff62e54abb6059b0068a5e4c8.svg
          fullname: Xurong Li
          isHf: false
          isPro: false
          name: LeafXR
          type: user
        html: '<p>I got a problem when I try to use this model for inference. I ran
          the inference service in a GPU of A10. Below is the error stack</p>

          <p>s/py_inference/serving/module_executor.py:exec_json():87][WARNING] exception:
          Traceback (most recent call last):<br>  File "/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/python_worker/python_user_base/lib/python3.8/site-packages/py_inference/serving/module_executor.py",
          line 81, in exec_json<br>    result = self.do_exec(args_json)<br>  File
          "/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/python_worker/python_user_base/lib/python3.8/site-packages/py_inference/serving/module_executor.py",
          line 98, in do_exec<br>    result = self.inference_worker.inference(**args)<br>  File
          "/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/lzd_llm_inference/llm_video_understanding.py",
          line 83, in inference<br>    response_text = self.inference_service.generate(input_text=text,
          video=video)[0]<br>  File "/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/lzd_llm_inference/llm_video_understanding.py",
          line 71, in generate<br>    outputs = self.base_model.generate(**request,
          max_new_tokens=max_new_tokens,<br>  File "/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/python_worker/python_user_base/lib/python3.8/site-packages/torch/utils/_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/mPLUG-Owl/mplug_owl_video/modeling_mplug_owl.py",
          line 1752, in generate<br>    outputs = self.language_model.generate(<br>  File
          "/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/python_worker/python_user_base/lib/python3.8/site-packages/torch/utils/_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/conda_new/lib/python3.8/site-packages/transformers/generation/utils.py",
          line 1485, in generate<br>    return self.sample(<br>  File "/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/conda_new/lib/python3.8/site-packages/transformers/generation/utils.py",
          line 2560, in sample<br>    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)<br>RuntimeError:
          probability tensor contains either <code>inf</code>, <code>nan</code> or
          element &lt; 0</p>

          '
        raw: "I got a problem when I try to use this model for inference. I ran the\
          \ inference service in a GPU of A10. Below is the error stack\r\n\r\ns/py_inference/serving/module_executor.py:exec_json():87][WARNING]\
          \ exception: Traceback (most recent call last):\r\n  File \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/python_worker/python_user_base/lib/python3.8/site-packages/py_inference/serving/module_executor.py\"\
          , line 81, in exec_json\r\n    result = self.do_exec(args_json)\r\n  File\
          \ \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/python_worker/python_user_base/lib/python3.8/site-packages/py_inference/serving/module_executor.py\"\
          , line 98, in do_exec\r\n    result = self.inference_worker.inference(**args)\r\
          \n  File \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/lzd_llm_inference/llm_video_understanding.py\"\
          , line 83, in inference\r\n    response_text = self.inference_service.generate(input_text=text,\
          \ video=video)[0]\r\n  File \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/lzd_llm_inference/llm_video_understanding.py\"\
          , line 71, in generate\r\n    outputs = self.base_model.generate(**request,\
          \ max_new_tokens=max_new_tokens,\r\n  File \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/python_worker/python_user_base/lib/python3.8/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/mPLUG-Owl/mplug_owl_video/modeling_mplug_owl.py\"\
          , line 1752, in generate\r\n    outputs = self.language_model.generate(\r\
          \n  File \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/python_worker/python_user_base/lib/python3.8/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/conda_new/lib/python3.8/site-packages/transformers/generation/utils.py\"\
          , line 1485, in generate\r\n    return self.sample(\r\n  File \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/conda_new/lib/python3.8/site-packages/transformers/generation/utils.py\"\
          , line 2560, in sample\r\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\
          \nRuntimeError: probability tensor contains either `inf`, `nan` or element\
          \ < 0"
        updatedAt: '2023-07-14T03:49:37.630Z'
      numEdits: 0
      reactions: []
    id: 64b0c5d1cf34be6bd056d023
    type: comment
  author: LeafXR
  content: "I got a problem when I try to use this model for inference. I ran the\
    \ inference service in a GPU of A10. Below is the error stack\r\n\r\ns/py_inference/serving/module_executor.py:exec_json():87][WARNING]\
    \ exception: Traceback (most recent call last):\r\n  File \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/python_worker/python_user_base/lib/python3.8/site-packages/py_inference/serving/module_executor.py\"\
    , line 81, in exec_json\r\n    result = self.do_exec(args_json)\r\n  File \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/python_worker/python_user_base/lib/python3.8/site-packages/py_inference/serving/module_executor.py\"\
    , line 98, in do_exec\r\n    result = self.inference_worker.inference(**args)\r\
    \n  File \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/lzd_llm_inference/llm_video_understanding.py\"\
    , line 83, in inference\r\n    response_text = self.inference_service.generate(input_text=text,\
    \ video=video)[0]\r\n  File \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/lzd_llm_inference/llm_video_understanding.py\"\
    , line 71, in generate\r\n    outputs = self.base_model.generate(**request, max_new_tokens=max_new_tokens,\r\
    \n  File \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/python_worker/python_user_base/lib/python3.8/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/mPLUG-Owl/mplug_owl_video/modeling_mplug_owl.py\"\
    , line 1752, in generate\r\n    outputs = self.language_model.generate(\r\n  File\
    \ \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/python_worker/python_user_base/lib/python3.8/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/conda_new/lib/python3.8/site-packages/transformers/generation/utils.py\"\
    , line 1485, in generate\r\n    return self.sample(\r\n  File \"/home/admin/hippo/worker/slave/suezops_c2_prod_mplug_owl_video.mplug_owl_video_15_31/binary/conda_new/lib/python3.8/site-packages/transformers/generation/utils.py\"\
    , line 2560, in sample\r\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\
    \nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0"
  created_at: 2023-07-14 02:49:37+00:00
  edited: false
  hidden: false
  id: 64b0c5d1cf34be6bd056d023
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0lncTpIHXn6suB0p-oSma.jpeg?w=200&h=200&f=face
      fullname: QinghaoYe
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: MAGAer13
      type: user
    createdAt: '2023-07-14T05:33:46.000Z'
    data:
      edited: false
      editors:
      - MAGAer13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9034203290939331
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0lncTpIHXn6suB0p-oSma.jpeg?w=200&h=200&f=face
          fullname: QinghaoYe
          isHf: false
          isPro: false
          name: MAGAer13
          type: user
        html: '<p>Please refer to <a rel="nofollow" href="https://github.com/X-PLUG/mPLUG-Owl/issues/101">https://github.com/X-PLUG/mPLUG-Owl/issues/101</a></p>

          '
        raw: Please refer to https://github.com/X-PLUG/mPLUG-Owl/issues/101
        updatedAt: '2023-07-14T05:33:46.668Z'
      numEdits: 0
      reactions: []
    id: 64b0de3a66a078bc7b03b33e
    type: comment
  author: MAGAer13
  content: Please refer to https://github.com/X-PLUG/mPLUG-Owl/issues/101
  created_at: 2023-07-14 04:33:46+00:00
  edited: false
  hidden: false
  id: 64b0de3a66a078bc7b03b33e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c218095b329de1b1e09712/N0haZSqU7S8WvyJYjaNCD.png?w=200&h=200&f=face
      fullname: Yuan Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zy0422
      type: user
    createdAt: '2023-08-02T09:38:02.000Z'
    data:
      edited: false
      editors:
      - Zy0422
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8851616382598877
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c218095b329de1b1e09712/N0haZSqU7S8WvyJYjaNCD.png?w=200&h=200&f=face
          fullname: Yuan Zhang
          isHf: false
          isPro: false
          name: Zy0422
          type: user
        html: '<p>Hello, how should I use this model, can you give some code examples?
          thank you</p>

          '
        raw: Hello, how should I use this model, can you give some code examples?
          thank you
        updatedAt: '2023-08-02T09:38:02.062Z'
      numEdits: 0
      reactions: []
    id: 64ca23fac58bea735b3f57de
    type: comment
  author: Zy0422
  content: Hello, how should I use this model, can you give some code examples? thank
    you
  created_at: 2023-08-02 08:38:02+00:00
  edited: false
  hidden: false
  id: 64ca23fac58bea735b3f57de
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: MAGAer13/mplug-owl-llama-7b-video
repo_type: model
status: open
target_branch: null
title: Inference error probably related to bf16 quantization
