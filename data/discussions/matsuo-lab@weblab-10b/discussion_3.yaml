!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jforan
conflicting_files: null
created_at: 2023-08-21 00:07:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/568589148f7339a646775d2b5122c31c.svg
      fullname: Joe Foran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jforan
      type: user
    createdAt: '2023-08-21T01:07:59.000Z'
    data:
      edited: true
      editors:
      - jforan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9895192980766296
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/568589148f7339a646775d2b5122c31c.svg
          fullname: Joe Foran
          isHf: false
          isPro: false
          name: jforan
          type: user
        html: '<p>The tokenizer.json seems to be the same as for the original GPT-NeoX
          model.<br>Is there a reason you didn''t retrain the vocabulary so as to
          have more Japanese subtokens? I would have guessed that this would give
          even better performance in Japanese.</p>

          '
        raw: 'The tokenizer.json seems to be the same as for the original GPT-NeoX
          model.

          Is there a reason you didn''t retrain the vocabulary so as to have more
          Japanese subtokens? I would have guessed that this would give even better
          performance in Japanese.

          '
        updatedAt: '2023-08-21T01:32:17.770Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - gojiteji
        - mm
    id: 64e2b8ef384b85306bc1ae71
    type: comment
  author: jforan
  content: 'The tokenizer.json seems to be the same as for the original GPT-NeoX model.

    Is there a reason you didn''t retrain the vocabulary so as to have more Japanese
    subtokens? I would have guessed that this would give even better performance in
    Japanese.

    '
  created_at: 2023-08-21 00:07:59+00:00
  edited: true
  hidden: false
  id: 64e2b8ef384b85306bc1ae71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60799266921db717010c7c86/_m1zygokEBC-4pD7vuw0l.jpeg?w=200&h=200&f=face
      fullname: Koki Tanaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: gojiteji
      type: user
    createdAt: '2023-08-21T06:16:31.000Z'
    data:
      edited: true
      editors:
      - gojiteji
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5688025951385498
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60799266921db717010c7c86/_m1zygokEBC-4pD7vuw0l.jpeg?w=200&h=200&f=face
          fullname: Koki Tanaka
          isHf: false
          isPro: true
          name: gojiteji
          type: user
        html: "<p>I checked the vocabulary that has more than 3 bytes chars (because\
          \ <a rel=\"nofollow\" href=\"https://itpfdoc.hitachi.co.jp/manuals/3000/30003D0820/GD080450.HTM\"\
          >most Japanese characters are longer than 3 bytes</a>) and the result was\
          \ 0.  I also want to know how they train the tokenizer.\U0001F9D0</p>\n\
          <pre><code>Token with more than 3 bytes chars\n matsuo-lab/weblab-10b: 0\
          \ / 50254\n rinna/bilingual-gpt-neox-4b 41599 / 65536\n</code></pre>\n<p>The\
          \ code to count above is as follows.</p>\n<pre><code class=\"language-Python\"\
          >modelnames =[<span class=\"hljs-string\">\"matsuo-lab/weblab-10b\"</span>,<span\
          \ class=\"hljs-string\">\"rinna/bilingual-gpt-neox-4b\"</span>]\nmodel_dict\
          \ = {}\n<span class=\"hljs-keyword\">for</span> name <span class=\"hljs-keyword\"\
          >in</span> modelnames:\n  tokenizer = AutoTokenizer.from_pretrained(name)\n\
          \  vocab = tokenizer.convert_ids_to_tokens(<span class=\"hljs-built_in\"\
          >range</span>(tokenizer.vocab_size))\n  model_dict[name]=vocab\n\n<span\
          \ class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >has_multibyte_chars</span>(<span class=\"hljs-params\">input_str</span>):\n\
          \    <span class=\"hljs-keyword\">for</span> char <span class=\"hljs-keyword\"\
          >in</span> input_str:\n        <span class=\"hljs-keyword\">return</span>\
          \ <span class=\"hljs-built_in\">len</span>(char.encode(<span class=\"hljs-string\"\
          >'utf-8'</span>)) &gt; <span class=\"hljs-number\">2</span>\n<span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"Token with more\
          \ than 3 bytes chars\"</span>)\n\ncnt =<span class=\"hljs-number\">0</span>\n\
          <span class=\"hljs-keyword\">for</span> modelname <span class=\"hljs-keyword\"\
          >in</span> modelnames:\n  <span class=\"hljs-keyword\">for</span> t <span\
          \ class=\"hljs-keyword\">in</span> model_dict[modelname]:\n    <span class=\"\
          hljs-keyword\">if</span> has_multibyte_chars(t) &gt;<span class=\"hljs-number\"\
          >0</span>:\n      cnt +=<span class=\"hljs-number\">1</span>\n  <span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\" \"</span>+modelname,cnt,<span\
          \ class=\"hljs-string\">\"/\"</span>,<span class=\"hljs-built_in\">len</span>(model_dict[modelname]))\n\
          </code></pre>\n"
        raw: "I checked the vocabulary that has more than 3 bytes chars (because [most\
          \ Japanese characters are longer than 3 bytes](https://itpfdoc.hitachi.co.jp/manuals/3000/30003D0820/GD080450.HTM))\
          \ and the result was 0.  I also want to know how they train the tokenizer.\U0001F9D0\
          \n```\nToken with more than 3 bytes chars\n matsuo-lab/weblab-10b: 0 / 50254\n\
          \ rinna/bilingual-gpt-neox-4b 41599 / 65536\n```\n\nThe code to count above\
          \ is as follows.\n```Python\nmodelnames =[\"matsuo-lab/weblab-10b\",\"rinna/bilingual-gpt-neox-4b\"\
          ]\nmodel_dict = {}\nfor name in modelnames:\n  tokenizer = AutoTokenizer.from_pretrained(name)\n\
          \  vocab = tokenizer.convert_ids_to_tokens(range(tokenizer.vocab_size))\n\
          \  model_dict[name]=vocab\n\ndef has_multibyte_chars(input_str):\n    for\
          \ char in input_str:\n        return len(char.encode('utf-8')) > 2\nprint(\"\
          Token with more than 3 bytes chars\")\n\ncnt =0\nfor modelname in modelnames:\n\
          \  for t in model_dict[modelname]:\n    if has_multibyte_chars(t) >0:\n\
          \      cnt +=1\n  print(\" \"+modelname,cnt,\"/\",len(model_dict[modelname]))\n\
          ```\n"
        updatedAt: '2023-08-21T06:40:19.059Z'
      numEdits: 2
      reactions:
      - count: 3
        reaction: "\U0001F92F"
        users:
        - kaisugi
        - leonardlin
        - mm
    id: 64e3013f618cd90997ccf6f6
    type: comment
  author: gojiteji
  content: "I checked the vocabulary that has more than 3 bytes chars (because [most\
    \ Japanese characters are longer than 3 bytes](https://itpfdoc.hitachi.co.jp/manuals/3000/30003D0820/GD080450.HTM))\
    \ and the result was 0.  I also want to know how they train the tokenizer.\U0001F9D0\
    \n```\nToken with more than 3 bytes chars\n matsuo-lab/weblab-10b: 0 / 50254\n\
    \ rinna/bilingual-gpt-neox-4b 41599 / 65536\n```\n\nThe code to count above is\
    \ as follows.\n```Python\nmodelnames =[\"matsuo-lab/weblab-10b\",\"rinna/bilingual-gpt-neox-4b\"\
    ]\nmodel_dict = {}\nfor name in modelnames:\n  tokenizer = AutoTokenizer.from_pretrained(name)\n\
    \  vocab = tokenizer.convert_ids_to_tokens(range(tokenizer.vocab_size))\n  model_dict[name]=vocab\n\
    \ndef has_multibyte_chars(input_str):\n    for char in input_str:\n        return\
    \ len(char.encode('utf-8')) > 2\nprint(\"Token with more than 3 bytes chars\"\
    )\n\ncnt =0\nfor modelname in modelnames:\n  for t in model_dict[modelname]:\n\
    \    if has_multibyte_chars(t) >0:\n      cnt +=1\n  print(\" \"+modelname,cnt,\"\
    /\",len(model_dict[modelname]))\n```\n"
  created_at: 2023-08-21 05:16:31+00:00
  edited: true
  hidden: false
  id: 64e3013f618cd90997ccf6f6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: matsuo-lab/weblab-10b
repo_type: model
status: open
target_branch: null
title: Vocabulary seems to be mostly English
