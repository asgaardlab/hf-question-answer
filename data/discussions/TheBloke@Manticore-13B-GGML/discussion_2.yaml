!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mancub
conflicting_files: null
created_at: 2023-05-20 03:06:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-20T04:06:50.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Using llama.cpp in WSL...been playing with this model and for the
          most part it works as expected when I run it with ### Instruction: ### Response:
          </p>

          <p>Although on some questions it just goes ballistic and won''t stop. It
          keeps repeating the answer, just differently each time, but unless I CTRL+C
          out, it would keep going until the world ends.</p>

          <p>Also, is there a way to run in llama.cpp interactive mode (for example
          with Wizard-Mega model it was pretty neat as it remembered the prior conversation)
          ?</p>

          <p>I tried with --interactive-first and using -r "user " as a prompt for
          it to stop, but it only works the first time. After that it keeps repeating
          the "user" with some random question (though it does not answer it). Plain
          -i(nteractive) does not work and it just takes off here writing code in
          various languages. It''s amusing but not useful. :)</p>

          <p>I also noticed that if I ask the same question in the --ins(truction)
          mode it gives me a different (and apparently longer) answer than with the
          --interactive-first mode.</p>

          '
        raw: "Using llama.cpp in WSL...been playing with this model and for the most\
          \ part it works as expected when I run it with ### Instruction: ### Response:\
          \ \r\n\r\nAlthough on some questions it just goes ballistic and won't stop.\
          \ It keeps repeating the answer, just differently each time, but unless\
          \ I CTRL+C out, it would keep going until the world ends.\r\n\r\nAlso, is\
          \ there a way to run in llama.cpp interactive mode (for example with Wizard-Mega\
          \ model it was pretty neat as it remembered the prior conversation) ?\r\n\
          \r\nI tried with --interactive-first and using -r \"user \" as a prompt\
          \ for it to stop, but it only works the first time. After that it keeps\
          \ repeating the \"user\" with some random question (though it does not answer\
          \ it). Plain -i(nteractive) does not work and it just takes off here writing\
          \ code in various languages. It's amusing but not useful. :)\r\n\r\nI also\
          \ noticed that if I ask the same question in the --ins(truction) mode it\
          \ gives me a different (and apparently longer) answer than with the --interactive-first\
          \ mode."
        updatedAt: '2023-05-20T04:06:50.929Z'
      numEdits: 0
      reactions: []
    id: 6468475ae13ff05d618b317e
    type: comment
  author: mancub
  content: "Using llama.cpp in WSL...been playing with this model and for the most\
    \ part it works as expected when I run it with ### Instruction: ### Response:\
    \ \r\n\r\nAlthough on some questions it just goes ballistic and won't stop. It\
    \ keeps repeating the answer, just differently each time, but unless I CTRL+C\
    \ out, it would keep going until the world ends.\r\n\r\nAlso, is there a way to\
    \ run in llama.cpp interactive mode (for example with Wizard-Mega model it was\
    \ pretty neat as it remembered the prior conversation) ?\r\n\r\nI tried with --interactive-first\
    \ and using -r \"user \" as a prompt for it to stop, but it only works the first\
    \ time. After that it keeps repeating the \"user\" with some random question (though\
    \ it does not answer it). Plain -i(nteractive) does not work and it just takes\
    \ off here writing code in various languages. It's amusing but not useful. :)\r\
    \n\r\nI also noticed that if I ask the same question in the --ins(truction) mode\
    \ it gives me a different (and apparently longer) answer than with the --interactive-first\
    \ mode."
  created_at: 2023-05-20 03:06:50+00:00
  edited: false
  hidden: false
  id: 6468475ae13ff05d618b317e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1633430555230-noauth.png?w=200&h=200&f=face
      fullname: Alexander Kindziora
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AkindOf
      type: user
    createdAt: '2023-05-20T12:05:07.000Z'
    data:
      edited: false
      editors:
      - AkindOf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1633430555230-noauth.png?w=200&h=200&f=face
          fullname: Alexander Kindziora
          isHf: false
          isPro: false
          name: AkindOf
          type: user
        html: '<p>Hi mancub, </p>

          <p>did you already play around with "temperature" and "repeat_penalty" ?</p>

          <p>kind regards</p>

          '
        raw: "Hi mancub, \n\ndid you already play around with \"temperature\" and\
          \ \"repeat_penalty\" ?\n\nkind regards"
        updatedAt: '2023-05-20T12:05:07.922Z'
      numEdits: 0
      reactions: []
    id: 6468b77397ffc33d43c51553
    type: comment
  author: AkindOf
  content: "Hi mancub, \n\ndid you already play around with \"temperature\" and \"\
    repeat_penalty\" ?\n\nkind regards"
  created_at: 2023-05-20 11:05:07+00:00
  edited: false
  hidden: false
  id: 6468b77397ffc33d43c51553
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff6724414ebf009fed55e158270e75d5.svg
      fullname: Edwin Tam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edwios
      type: user
    createdAt: '2023-05-20T14:28:22.000Z'
    data:
      edited: true
      editors:
      - edwios
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff6724414ebf009fed55e158270e75d5.svg
          fullname: Edwin Tam
          isHf: false
          isPro: false
          name: edwios
          type: user
        html: '<p>With <code>--temp 0.7</code>, <code>--repeat_penalty 1.7</code>
          and <code>--top_p 0.7</code>, I have no problem using it with <code>--interactive-first</code>
          mode with <code>-r "User:"</code> (Note the colon here).</p>

          '
        raw: With `--temp 0.7`, `--repeat_penalty 1.7` and `--top_p 0.7`, I have no
          problem using it with `--interactive-first` mode with `-r "User:"` (Note
          the colon here).
        updatedAt: '2023-05-20T14:29:30.429Z'
      numEdits: 1
      reactions: []
    id: 6468d90699182de17848c3d5
    type: comment
  author: edwios
  content: With `--temp 0.7`, `--repeat_penalty 1.7` and `--top_p 0.7`, I have no
    problem using it with `--interactive-first` mode with `-r "User:"` (Note the colon
    here).
  created_at: 2023-05-20 13:28:22+00:00
  edited: true
  hidden: false
  id: 6468d90699182de17848c3d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-20T15:14:56.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Yeah many people have been reporting this follow-on-answer issue.</p>\n\
          <p>I've just updated the main branch with new ggmlv3 models from the latest\
          \ version of Manticore, epoch 3</p>\n<p>I have tested 10 prompts and haven't\
          \ got a single follow-on answer</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;mancub&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mancub\"\
          >@<span class=\"underline\">mancub</span></a></span>\n\n\t</span></span>\
          \ and others, please re-test and let me know if you spot a difference.</p>\n"
        raw: 'Yeah many people have been reporting this follow-on-answer issue.


          I''ve just updated the main branch with new ggmlv3 models from the latest
          version of Manticore, epoch 3


          I have tested 10 prompts and haven''t got a single follow-on answer


          @mancub and others, please re-test and let me know if you spot a difference.'
        updatedAt: '2023-05-20T15:14:56.588Z'
      numEdits: 0
      reactions: []
    id: 6468e3f097ffc33d43c7b9b7
    type: comment
  author: TheBloke
  content: 'Yeah many people have been reporting this follow-on-answer issue.


    I''ve just updated the main branch with new ggmlv3 models from the latest version
    of Manticore, epoch 3


    I have tested 10 prompts and haven''t got a single follow-on answer


    @mancub and others, please re-test and let me know if you spot a difference.'
  created_at: 2023-05-20 14:14:56+00:00
  edited: false
  hidden: false
  id: 6468e3f097ffc33d43c7b9b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-20T15:15:21.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Hi,</p>

          <p>This is my command line:</p>

          <p>GGML_CUDA_NO_PINNED=1 ./main -t 10 -ngl 40 --color -c 2048 --temp 0.7
          --repeat_penalty 1.1 -m models/thebloke_manticore-13b.ggml.v3.q5_1.bin --interactive-first
          -p "User "</p>

          <p>I don''t have a column in the prompt User, but does it really matter,
          it''s just a stopword?</p>

          '
        raw: 'Hi,


          This is my command line:


          GGML_CUDA_NO_PINNED=1 ./main -t 10 -ngl 40 --color -c 2048 --temp 0.7 --repeat_penalty
          1.1 -m models/thebloke_manticore-13b.ggml.v3.q5_1.bin --interactive-first
          -p "User "


          I don''t have a column in the prompt User, but does it really matter, it''s
          just a stopword?'
        updatedAt: '2023-05-20T15:15:21.022Z'
      numEdits: 0
      reactions: []
    id: 6468e40999182de17849699c
    type: comment
  author: mancub
  content: 'Hi,


    This is my command line:


    GGML_CUDA_NO_PINNED=1 ./main -t 10 -ngl 40 --color -c 2048 --temp 0.7 --repeat_penalty
    1.1 -m models/thebloke_manticore-13b.ggml.v3.q5_1.bin --interactive-first -p "User
    "


    I don''t have a column in the prompt User, but does it really matter, it''s just
    a stopword?'
  created_at: 2023-05-20 14:15:21+00:00
  edited: false
  hidden: false
  id: 6468e40999182de17849699c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-20T15:19:21.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<blockquote>\n<p>Yeah many people have been reporting this follow-on-answer\
          \ issue.</p>\n<p>I've just updated the main branch with new ggmlv3 models\
          \ from the latest version of Manticore, epoch 3</p>\n<p>I have tested 10\
          \ prompts and haven't got a single follow-on answer</p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;mancub&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/mancub\">@<span class=\"underline\">mancub</span></a></span>\n\
          \n\t</span></span> and others, please re-test and let me know if you spot\
          \ a difference.</p>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Should that be the version number though, because yesterday it\
          \ was v3 when I downloaded it ( 12 hours ago, and now I see the files has\
          \ been updated 1 hr ago)?</p>\n<p>Perhaps make v3.x, or add a date e.g.\
          \ Manticore-13B.ggmlv3.20230520.q5_1.bin</p>\n"
        raw: "> Yeah many people have been reporting this follow-on-answer issue.\n\
          > \n> I've just updated the main branch with new ggmlv3 models from the\
          \ latest version of Manticore, epoch 3\n> \n> I have tested 10 prompts and\
          \ haven't got a single follow-on answer\n> \n> @mancub and others, please\
          \ re-test and let me know if you spot a difference.\n\n@TheBloke \n\nShould\
          \ that be the version number though, because yesterday it was v3 when I\
          \ downloaded it ( 12 hours ago, and now I see the files has been updated\
          \ 1 hr ago)?\n\nPerhaps make v3.x, or add a date e.g. Manticore-13B.ggmlv3.20230520.q5_1.bin"
        updatedAt: '2023-05-20T15:19:21.363Z'
      numEdits: 0
      reactions: []
    id: 6468e4f97407ab1cff34ae11
    type: comment
  author: mancub
  content: "> Yeah many people have been reporting this follow-on-answer issue.\n\
    > \n> I've just updated the main branch with new ggmlv3 models from the latest\
    \ version of Manticore, epoch 3\n> \n> I have tested 10 prompts and haven't got\
    \ a single follow-on answer\n> \n> @mancub and others, please re-test and let\
    \ me know if you spot a difference.\n\n@TheBloke \n\nShould that be the version\
    \ number though, because yesterday it was v3 when I downloaded it ( 12 hours ago,\
    \ and now I see the files has been updated 1 hr ago)?\n\nPerhaps make v3.x, or\
    \ add a date e.g. Manticore-13B.ggmlv3.20230520.q5_1.bin"
  created_at: 2023-05-20 14:19:21+00:00
  edited: false
  hidden: false
  id: 6468e4f97407ab1cff34ae11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-20T15:27:31.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Firstly, I now see the issue described when I use your command line.  Using
          <code>--interactive-first-p User</code> it won''t shut up.</p>

          <p>I was testing with single prompts, like:</p>

          <pre><code>-p "###Instruction: explain in detail the differences between
          C, C++ and Objective C\n### Response:"

          -p "###Instruction: what is pythagorus theorem? Give some examples\n###
          Response:"

          -p "###Instruction: Write an essay comparing France and Germany\n### Response:

          </code></pre>

          <p>And it works perfectly with those.</p>

          <p>So it can work with the right prompt template, but there does seem to
          be an issue with the USER: prompt.  </p>

          <p>Secondly, the <code>ggmlv3</code> refers to the GGML version number,
          ie indicates its compatibility with llama.cpp.  It''s not the version of
          the model specifically.   When I did the new GGMLv3 models last night I
          forgot to update the base Manticore model to its latest version, so this
          latest push fixes this.</p>

          <p>I''d recommend re-downloading the latest pushed files for general quality,
          but it won''t fix this prompt template issue.</p>

          <p>I''ll update the README to mention this is now epoch 3.</p>

          '
        raw: "Firstly, I now see the issue described when I use your command line.\
          \  Using `--interactive-first-p User` it won't shut up.\n\nI was testing\
          \ with single prompts, like:\n```\n-p \"###Instruction: explain in detail\
          \ the differences between C, C++ and Objective C\\n### Response:\"\n-p \"\
          ###Instruction: what is pythagorus theorem? Give some examples\\n### Response:\"\
          \n-p \"###Instruction: Write an essay comparing France and Germany\\n###\
          \ Response:\n```\n\nAnd it works perfectly with those.\n\nSo it can work\
          \ with the right prompt template, but there does seem to be an issue with\
          \ the USER: prompt.  \n\nSecondly, the `ggmlv3` refers to the GGML version\
          \ number, ie indicates its compatibility with llama.cpp.  It's not the version\
          \ of the model specifically.   When I did the new GGMLv3 models last night\
          \ I forgot to update the base Manticore model to its latest version, so\
          \ this latest push fixes this.\n\nI'd recommend re-downloading the latest\
          \ pushed files for general quality, but it won't fix this prompt template\
          \ issue.\n\nI'll update the README to mention this is now epoch 3."
        updatedAt: '2023-05-20T15:27:31.847Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - mancub
        - AkindOf
    id: 6468e6e399182de1784994af
    type: comment
  author: TheBloke
  content: "Firstly, I now see the issue described when I use your command line. \
    \ Using `--interactive-first-p User` it won't shut up.\n\nI was testing with single\
    \ prompts, like:\n```\n-p \"###Instruction: explain in detail the differences\
    \ between C, C++ and Objective C\\n### Response:\"\n-p \"###Instruction: what\
    \ is pythagorus theorem? Give some examples\\n### Response:\"\n-p \"###Instruction:\
    \ Write an essay comparing France and Germany\\n### Response:\n```\n\nAnd it works\
    \ perfectly with those.\n\nSo it can work with the right prompt template, but\
    \ there does seem to be an issue with the USER: prompt.  \n\nSecondly, the `ggmlv3`\
    \ refers to the GGML version number, ie indicates its compatibility with llama.cpp.\
    \  It's not the version of the model specifically.   When I did the new GGMLv3\
    \ models last night I forgot to update the base Manticore model to its latest\
    \ version, so this latest push fixes this.\n\nI'd recommend re-downloading the\
    \ latest pushed files for general quality, but it won't fix this prompt template\
    \ issue.\n\nI'll update the README to mention this is now epoch 3."
  created_at: 2023-05-20 14:27:31+00:00
  edited: false
  hidden: false
  id: 6468e6e399182de1784994af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-20T15:51:56.000Z'
    data:
      status: closed
    id: 6468ec9c7407ab1cff352a0c
    type: status-change
  author: mancub
  created_at: 2023-05-20 14:51:56+00:00
  id: 6468ec9c7407ab1cff352a0c
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-20T15:51:58.000Z'
    data:
      status: open
    id: 6468ec9e7407ab1cff352a2f
    type: status-change
  author: mancub
  created_at: 2023-05-20 14:51:58+00:00
  id: 6468ec9e7407ab1cff352a2f
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-20T16:06:57.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>@)#(%#$ fingers faster than the brains sometimes.</p>

          <p>I re-downloaded the model, though the filesize is the same as what I
          had prior.</p>

          <p>Not sure what it is, but this version goes all biased and restricted
          on me, more often than before. The previous one did not make an issue of
          me asking it anything. It still goes on rambling at the end first with some
          fluff self-chatter (overly cordial) and then just keeps going on repeating
          what it previously wrote. This is in the interactive mode.</p>

          <p>With Instruction/Response it is concise, but totally incorrect. Matter
          a fact it''s restricted and refuses the provide a real response like it
          did previously.</p>

          <p>it also seems slower by 10ms or so than the previous one, according the
          to figures I get after run completes (looking at eval time, 100ms/t now
          vs 90ms/t prior).</p>

          '
        raw: '@)#(%#$ fingers faster than the brains sometimes.


          I re-downloaded the model, though the filesize is the same as what I had
          prior.


          Not sure what it is, but this version goes all biased and restricted on
          me, more often than before. The previous one did not make an issue of me
          asking it anything. It still goes on rambling at the end first with some
          fluff self-chatter (overly cordial) and then just keeps going on repeating
          what it previously wrote. This is in the interactive mode.


          With Instruction/Response it is concise, but totally incorrect. Matter a
          fact it''s restricted and refuses the provide a real response like it did
          previously.


          it also seems slower by 10ms or so than the previous one, according the
          to figures I get after run completes (looking at eval time, 100ms/t now
          vs 90ms/t prior).'
        updatedAt: '2023-05-20T16:06:57.859Z'
      numEdits: 0
      reactions: []
    id: 6468f02199182de1784a2b1b
    type: comment
  author: mancub
  content: '@)#(%#$ fingers faster than the brains sometimes.


    I re-downloaded the model, though the filesize is the same as what I had prior.


    Not sure what it is, but this version goes all biased and restricted on me, more
    often than before. The previous one did not make an issue of me asking it anything.
    It still goes on rambling at the end first with some fluff self-chatter (overly
    cordial) and then just keeps going on repeating what it previously wrote. This
    is in the interactive mode.


    With Instruction/Response it is concise, but totally incorrect. Matter a fact
    it''s restricted and refuses the provide a real response like it did previously.


    it also seems slower by 10ms or so than the previous one, according the to figures
    I get after run completes (looking at eval time, 100ms/t now vs 90ms/t prior).'
  created_at: 2023-05-20 15:06:57+00:00
  edited: false
  hidden: false
  id: 6468f02199182de1784a2b1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/505272aff6b1fadda28b97b650454aab.svg
      fullname: Leonardo da Silva
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MatsuCA
      type: user
    createdAt: '2023-05-20T16:23:44.000Z'
    data:
      edited: false
      editors:
      - MatsuCA
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/505272aff6b1fadda28b97b650454aab.svg
          fullname: Leonardo da Silva
          isHf: false
          isPro: false
          name: MatsuCA
          type: user
        html: '<p>I can''t load the new model with the updated llama.cpp:</p>

          <p>(base) :~/llama.cpp$ ./main -t 4 -m ./models/Manticore-13B.ggmlv3.q4_0.bin
          --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -ngl 40 -p "### Instruction:
          ### Response:"<br>main: build = 552 (b5c9295)<br>main: seed  = 1684599696<br>llama.cpp:
          loading model from ./models/Manticore-13B.ggmlv3.q4_0.bin<br>error loading
          model: unknown (magic, version) combination: 67676a74, 00000003; is this
          really a GGML file?<br>llama_init_from_file: failed to load model<br>llama_init_from_gpt_params:
          error: failed to load model ''./models/Manticore-13B.ggmlv3.q4_0.bin''<br>main:
          error: unable to load model</p>

          '
        raw: 'I can''t load the new model with the updated llama.cpp:


          (base) :~/llama.cpp$ ./main -t 4 -m ./models/Manticore-13B.ggmlv3.q4_0.bin
          --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -ngl 40 -p "### Instruction:
          ### Response:"

          main: build = 552 (b5c9295)

          main: seed  = 1684599696

          llama.cpp: loading model from ./models/Manticore-13B.ggmlv3.q4_0.bin

          error loading model: unknown (magic, version) combination: 67676a74, 00000003;
          is this really a GGML file?

          llama_init_from_file: failed to load model

          llama_init_from_gpt_params: error: failed to load model ''./models/Manticore-13B.ggmlv3.q4_0.bin''

          main: error: unable to load model'
        updatedAt: '2023-05-20T16:23:44.854Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mrangels
    id: 6468f41097ffc33d43c8bd7e
    type: comment
  author: MatsuCA
  content: 'I can''t load the new model with the updated llama.cpp:


    (base) :~/llama.cpp$ ./main -t 4 -m ./models/Manticore-13B.ggmlv3.q4_0.bin --color
    -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -ngl 40 -p "### Instruction: ###
    Response:"

    main: build = 552 (b5c9295)

    main: seed  = 1684599696

    llama.cpp: loading model from ./models/Manticore-13B.ggmlv3.q4_0.bin

    error loading model: unknown (magic, version) combination: 67676a74, 00000003;
    is this really a GGML file?

    llama_init_from_file: failed to load model

    llama_init_from_gpt_params: error: failed to load model ''./models/Manticore-13B.ggmlv3.q4_0.bin''

    main: error: unable to load model'
  created_at: 2023-05-20 15:23:44+00:00
  edited: false
  hidden: false
  id: 6468f41097ffc33d43c8bd7e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-20T17:24:38.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;MatsuCA&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/MatsuCA\">@<span class=\"\
          underline\">MatsuCA</span></a></span>\n\n\t</span></span>  Firstly please\
          \ double check the SHA256SUM matches what's shown on the file page - maybe\
          \ the file download didn't complete, or the file got corrupted.</p>\n<p>If\
          \ that matches, please double check you're definitely using the latest llama.cpp,\
          \ recompiled.  I've tested the models on Linux and macOS and they work fine.</p>\n"
        raw: '@MatsuCA  Firstly please double check the SHA256SUM matches what''s
          shown on the file page - maybe the file download didn''t complete, or the
          file got corrupted.


          If that matches, please double check you''re definitely using the latest
          llama.cpp, recompiled.  I''ve tested the models on Linux and macOS and they
          work fine.'
        updatedAt: '2023-05-20T17:24:50.515Z'
      numEdits: 1
      reactions: []
    id: 646902567407ab1cff36c310
    type: comment
  author: TheBloke
  content: '@MatsuCA  Firstly please double check the SHA256SUM matches what''s shown
    on the file page - maybe the file download didn''t complete, or the file got corrupted.


    If that matches, please double check you''re definitely using the latest llama.cpp,
    recompiled.  I''ve tested the models on Linux and macOS and they work fine.'
  created_at: 2023-05-20 16:24:38+00:00
  edited: true
  hidden: false
  id: 646902567407ab1cff36c310
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-20T18:52:10.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I downloaded both q5_1 and q8_0 now and I notice a slight difference
          in speed.</p>

          <p>For example using:<br>-p "###Instruction: explain in detail the differences
          between C, C++ and Objective C\n### Response:"</p>

          <p>q5_1:<br>llama_print_timings:        load time =  6902.60 ms<br>llama_print_timings:      sample
          time =   500.14 ms /   805 runs   (    0.62 ms per token)<br>llama_print_timings:
          prompt eval time =   638.39 ms /    25 tokens (   25.54 ms per token)<br>llama_print_timings:        eval
          time = 89991.04 ms /   804 runs   (  111.93 ms per token)<br>llama_print_timings:       total
          time = 97673.19 ms</p>

          <p>q8_0:<br>llama_print_timings:        load time =  9330.53 ms<br>llama_print_timings:      sample
          time =   397.94 ms /   641 runs   (    0.62 ms per token)<br>llama_print_timings:
          prompt eval time =   633.99 ms /    25 tokens (   25.36 ms per token)<br>llama_print_timings:        eval
          time = 67060.31 ms /   640 runs   (  104.78 ms per token)<br>llama_print_timings:       total
          time = 77010.65 ms</p>

          <p>This is somewhat surprising to me because a larger model appears faster,
          but I don''t know enough about all of this to make any determination. Besides,
          I am not complaining 10t/s is great!</p>

          '
        raw: "I downloaded both q5_1 and q8_0 now and I notice a slight difference\
          \ in speed.\n\nFor example using: \n-p \"###Instruction: explain in detail\
          \ the differences between C, C++ and Objective C\\n### Response:\"\n\nq5_1:\n\
          llama_print_timings:        load time =  6902.60 ms\nllama_print_timings:\
          \      sample time =   500.14 ms /   805 runs   (    0.62 ms per token)\n\
          llama_print_timings: prompt eval time =   638.39 ms /    25 tokens (   25.54\
          \ ms per token)\nllama_print_timings:        eval time = 89991.04 ms / \
          \  804 runs   (  111.93 ms per token)\nllama_print_timings:       total\
          \ time = 97673.19 ms\n\nq8_0:\nllama_print_timings:        load time = \
          \ 9330.53 ms\nllama_print_timings:      sample time =   397.94 ms /   641\
          \ runs   (    0.62 ms per token)\nllama_print_timings: prompt eval time\
          \ =   633.99 ms /    25 tokens (   25.36 ms per token)\nllama_print_timings:\
          \        eval time = 67060.31 ms /   640 runs   (  104.78 ms per token)\n\
          llama_print_timings:       total time = 77010.65 ms\n\nThis is somewhat\
          \ surprising to me because a larger model appears faster, but I don't know\
          \ enough about all of this to make any determination. Besides, I am not\
          \ complaining 10t/s is great!"
        updatedAt: '2023-05-20T18:52:10.157Z'
      numEdits: 0
      reactions: []
    id: 646916da7407ab1cff388cf1
    type: comment
  author: mancub
  content: "I downloaded both q5_1 and q8_0 now and I notice a slight difference in\
    \ speed.\n\nFor example using: \n-p \"###Instruction: explain in detail the differences\
    \ between C, C++ and Objective C\\n### Response:\"\n\nq5_1:\nllama_print_timings:\
    \        load time =  6902.60 ms\nllama_print_timings:      sample time =   500.14\
    \ ms /   805 runs   (    0.62 ms per token)\nllama_print_timings: prompt eval\
    \ time =   638.39 ms /    25 tokens (   25.54 ms per token)\nllama_print_timings:\
    \        eval time = 89991.04 ms /   804 runs   (  111.93 ms per token)\nllama_print_timings:\
    \       total time = 97673.19 ms\n\nq8_0:\nllama_print_timings:        load time\
    \ =  9330.53 ms\nllama_print_timings:      sample time =   397.94 ms /   641 runs\
    \   (    0.62 ms per token)\nllama_print_timings: prompt eval time =   633.99\
    \ ms /    25 tokens (   25.36 ms per token)\nllama_print_timings:        eval\
    \ time = 67060.31 ms /   640 runs   (  104.78 ms per token)\nllama_print_timings:\
    \       total time = 77010.65 ms\n\nThis is somewhat surprising to me because\
    \ a larger model appears faster, but I don't know enough about all of this to\
    \ make any determination. Besides, I am not complaining 10t/s is great!"
  created_at: 2023-05-20 17:52:10+00:00
  edited: false
  hidden: false
  id: 646916da7407ab1cff388cf1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/505272aff6b1fadda28b97b650454aab.svg
      fullname: Leonardo da Silva
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MatsuCA
      type: user
    createdAt: '2023-05-21T17:53:10.000Z'
    data:
      edited: false
      editors:
      - MatsuCA
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/505272aff6b1fadda28b97b650454aab.svg
          fullname: Leonardo da Silva
          isHf: false
          isPro: false
          name: MatsuCA
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;MatsuCA&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MatsuCA\"\
          >@<span class=\"underline\">MatsuCA</span></a></span>\n\n\t</span></span>\
          \  Firstly please double check the SHA256SUM matches what's shown on the\
          \ file page - maybe the file download didn't complete, or the file got corrupted.</p>\n\
          <p>If that matches, please double check you're definitely using the latest\
          \ llama.cpp, recompiled.  I've tested the models on Linux and macOS and\
          \ they work fine.</p>\n</blockquote>\n<p>I could not find the source of\
          \ the issue. So I reseted my WSL instance and installed llama.cpp from zero.\
          \ All working now, thank you.</p>\n"
        raw: "> @MatsuCA  Firstly please double check the SHA256SUM matches what's\
          \ shown on the file page - maybe the file download didn't complete, or the\
          \ file got corrupted.\n> \n> If that matches, please double check you're\
          \ definitely using the latest llama.cpp, recompiled.  I've tested the models\
          \ on Linux and macOS and they work fine.\n\nI could not find the source\
          \ of the issue. So I reseted my WSL instance and installed llama.cpp from\
          \ zero. All working now, thank you."
        updatedAt: '2023-05-21T17:53:10.926Z'
      numEdits: 0
      reactions: []
    id: 646a5a8696cfe72aef7c20ff
    type: comment
  author: MatsuCA
  content: "> @MatsuCA  Firstly please double check the SHA256SUM matches what's shown\
    \ on the file page - maybe the file download didn't complete, or the file got\
    \ corrupted.\n> \n> If that matches, please double check you're definitely using\
    \ the latest llama.cpp, recompiled.  I've tested the models on Linux and macOS\
    \ and they work fine.\n\nI could not find the source of the issue. So I reseted\
    \ my WSL instance and installed llama.cpp from zero. All working now, thank you."
  created_at: 2023-05-21 16:53:10+00:00
  edited: false
  hidden: false
  id: 646a5a8696cfe72aef7c20ff
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Manticore-13B-GGML
repo_type: model
status: open
target_branch: null
title: It's a runaway train at times...
