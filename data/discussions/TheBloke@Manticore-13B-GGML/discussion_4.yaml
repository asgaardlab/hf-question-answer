!!python/object:huggingface_hub.community.DiscussionWithDetails
author: coilpaint
conflicting_files: null
created_at: 2023-05-21 03:01:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9731f23f9d1dd57747eeec12aaee3a7a.svg
      fullname: J W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: coilpaint
      type: user
    createdAt: '2023-05-21T04:01:24.000Z'
    data:
      edited: true
      editors:
      - coilpaint
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9731f23f9d1dd57747eeec12aaee3a7a.svg
          fullname: J W
          isHf: false
          isPro: false
          name: coilpaint
          type: user
        html: '<p>Manticore-13B-GPTQ-4bit-128g.no-act-order.safetensors is in my models
          directory. I get the following error. I updated oogabooga before i installed
          this, as of posting this.</p>

          <p>INFO:Loading TheBloke_Manticore-13B-GPTQ...<br>Traceback (most recent
          call last):<br>  File "C:\LLMs\oobabooga_windows\text-generation-webui\server.py",
          line 1033, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "C:\LLMs\oobabooga_windows\text-generation-webui\modules\models.py", line
          95, in load_model<br>    output = load_func(model_name)<br>  File "C:\LLMs\oobabooga_windows\text-generation-webui\modules\models.py",
          line 209, in huggingface_loader<br>    model = LoaderClass.from_pretrained(checkpoint,
          **params)<br>  File "C:\LLMs\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\auto\auto_factory.py",
          line 471, in from_pretrained<br>    return model_class.from_pretrained(<br>  File
          "C:\LLMs\oobabooga_windows\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 2405, in from_pretrained<br>    raise EnvironmentError(<br>OSError:
          Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or
          flax_model.msgpack found in directory models\TheBloke_Manticore-13B-GPTQ.</p>

          '
        raw: "Manticore-13B-GPTQ-4bit-128g.no-act-order.safetensors is in my models\
          \ directory. I get the following error. I updated oogabooga before i installed\
          \ this, as of posting this.\n\nINFO:Loading TheBloke_Manticore-13B-GPTQ...\n\
          Traceback (most recent call last):\n  File \"C:\\LLMs\\oobabooga_windows\\\
          text-generation-webui\\server.py\", line 1033, in <module>\n    shared.model,\
          \ shared.tokenizer = load_model(shared.model_name)\n  File \"C:\\LLMs\\\
          oobabooga_windows\\text-generation-webui\\modules\\models.py\", line 95,\
          \ in load_model\n    output = load_func(model_name)\n  File \"C:\\LLMs\\\
          oobabooga_windows\\text-generation-webui\\modules\\models.py\", line 209,\
          \ in huggingface_loader\n    model = LoaderClass.from_pretrained(checkpoint,\
          \ **params)\n  File \"C:\\LLMs\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line\
          \ 471, in from_pretrained\n    return model_class.from_pretrained(\n  File\
          \ \"C:\\LLMs\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\", line 2405, in from_pretrained\n    raise\
          \ EnvironmentError(\nOSError: Error no file named pytorch_model.bin, tf_model.h5,\
          \ model.ckpt.index or flax_model.msgpack found in directory models\\TheBloke_Manticore-13B-GPTQ."
        updatedAt: '2023-05-21T04:03:23.459Z'
      numEdits: 1
      reactions: []
    id: 64699794b2321e47d32ac51b
    type: comment
  author: coilpaint
  content: "Manticore-13B-GPTQ-4bit-128g.no-act-order.safetensors is in my models\
    \ directory. I get the following error. I updated oogabooga before i installed\
    \ this, as of posting this.\n\nINFO:Loading TheBloke_Manticore-13B-GPTQ...\nTraceback\
    \ (most recent call last):\n  File \"C:\\LLMs\\oobabooga_windows\\text-generation-webui\\\
    server.py\", line 1033, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    \  File \"C:\\LLMs\\oobabooga_windows\\text-generation-webui\\modules\\models.py\"\
    , line 95, in load_model\n    output = load_func(model_name)\n  File \"C:\\LLMs\\\
    oobabooga_windows\\text-generation-webui\\modules\\models.py\", line 209, in huggingface_loader\n\
    \    model = LoaderClass.from_pretrained(checkpoint, **params)\n  File \"C:\\\
    LLMs\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\auto\\auto_factory.py\", line 471, in from_pretrained\n    return model_class.from_pretrained(\n\
    \  File \"C:\\LLMs\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\modeling_utils.py\", line 2405, in from_pretrained\n    raise EnvironmentError(\n\
    OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index\
    \ or flax_model.msgpack found in directory models\\TheBloke_Manticore-13B-GPTQ."
  created_at: 2023-05-21 03:01:24+00:00
  edited: true
  hidden: false
  id: 64699794b2321e47d32ac51b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9731f23f9d1dd57747eeec12aaee3a7a.svg
      fullname: J W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: coilpaint
      type: user
    createdAt: '2023-05-21T04:18:26.000Z'
    data:
      status: closed
    id: 64699b92511e5cdeb0f36a1e
    type: status-change
  author: coilpaint
  created_at: 2023-05-21 03:18:26+00:00
  id: 64699b92511e5cdeb0f36a1e
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-21T09:00:07.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I take it you figured it out. This error comes when GPTQ parameters
          are not correctly defined for a model.</p>

          '
        raw: I take it you figured it out. This error comes when GPTQ parameters are
          not correctly defined for a model.
        updatedAt: '2023-05-21T09:00:07.843Z'
      numEdits: 0
      reactions: []
    id: 6469dd9796cfe72aef74c640
    type: comment
  author: TheBloke
  content: I take it you figured it out. This error comes when GPTQ parameters are
    not correctly defined for a model.
  created_at: 2023-05-21 08:00:07+00:00
  edited: false
  hidden: false
  id: 6469dd9796cfe72aef74c640
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9731f23f9d1dd57747eeec12aaee3a7a.svg
      fullname: J W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: coilpaint
      type: user
    createdAt: '2023-05-21T10:56:27.000Z'
    data:
      edited: false
      editors:
      - coilpaint
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9731f23f9d1dd57747eeec12aaee3a7a.svg
          fullname: J W
          isHf: false
          isPro: false
          name: coilpaint
          type: user
        html: "<p>I didn\u2019t figure it out but I read readme and it says it works\
          \ with oogabooga. I also read somewhere that GPTQ doesn\u2019t work with\
          \ windows without WSL. I don\u2019t want to use WSL so I\u2019m going to\
          \ try ggml or wait for the 7b version. Is that a good approach? I have a\
          \ 3080 GPU but the default 13b version is too slow.</p>\n"
        raw: "I didn\u2019t figure it out but I read readme and it says it works with\
          \ oogabooga. I also read somewhere that GPTQ doesn\u2019t work with windows\
          \ without WSL. I don\u2019t want to use WSL so I\u2019m going to try ggml\
          \ or wait for the 7b version. Is that a good approach? I have a 3080 GPU\
          \ but the default 13b version is too slow."
        updatedAt: '2023-05-21T10:56:27.517Z'
      numEdits: 0
      reactions: []
    id: 6469f8db96cfe72aef766e37
    type: comment
  author: coilpaint
  content: "I didn\u2019t figure it out but I read readme and it says it works with\
    \ oogabooga. I also read somewhere that GPTQ doesn\u2019t work with windows without\
    \ WSL. I don\u2019t want to use WSL so I\u2019m going to try ggml or wait for\
    \ the 7b version. Is that a good approach? I have a 3080 GPU but the default 13b\
    \ version is too slow."
  created_at: 2023-05-21 09:56:27+00:00
  edited: false
  hidden: false
  id: 6469f8db96cfe72aef766e37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-21T10:58:41.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes the GPTQ works fine with oobabooga, and it works fine in Windows.
          You can also use WSL if you want, but don''t have to</p>

          <p>Just follow the instructions on the text-generation-webui repo for installing
          their one-click installer, then follow the instructions in my Manticore
          13B GPTQ README for "easily installing with text-generation-webui".  Follow
          all steps and it will work fine. Last time it looks like you missed out
          the step for setting the GPTQ parameters.</p>

          '
        raw: 'Yes the GPTQ works fine with oobabooga, and it works fine in Windows.
          You can also use WSL if you want, but don''t have to


          Just follow the instructions on the text-generation-webui repo for installing
          their one-click installer, then follow the instructions in my Manticore
          13B GPTQ README for "easily installing with text-generation-webui".  Follow
          all steps and it will work fine. Last time it looks like you missed out
          the step for setting the GPTQ parameters.'
        updatedAt: '2023-05-21T10:58:41.998Z'
      numEdits: 0
      reactions: []
    id: 6469f96196cfe72aef7677cc
    type: comment
  author: TheBloke
  content: 'Yes the GPTQ works fine with oobabooga, and it works fine in Windows.
    You can also use WSL if you want, but don''t have to


    Just follow the instructions on the text-generation-webui repo for installing
    their one-click installer, then follow the instructions in my Manticore 13B GPTQ
    README for "easily installing with text-generation-webui".  Follow all steps and
    it will work fine. Last time it looks like you missed out the step for setting
    the GPTQ parameters.'
  created_at: 2023-05-21 09:58:41+00:00
  edited: false
  hidden: false
  id: 6469f96196cfe72aef7677cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9731f23f9d1dd57747eeec12aaee3a7a.svg
      fullname: J W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: coilpaint
      type: user
    createdAt: '2023-05-21T20:51:35.000Z'
    data:
      edited: false
      editors:
      - coilpaint
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9731f23f9d1dd57747eeec12aaee3a7a.svg
          fullname: J W
          isHf: false
          isPro: false
          name: coilpaint
          type: user
        html: '<p>Thanks that worked. Do you recommend any other settings for this
          model for GPTQ?</p>

          '
        raw: Thanks that worked. Do you recommend any other settings for this model
          for GPTQ?
        updatedAt: '2023-05-21T20:51:35.655Z'
      numEdits: 0
      reactions: []
    id: 646a8457f51b56a285c38474
    type: comment
  author: coilpaint
  content: Thanks that worked. Do you recommend any other settings for this model
    for GPTQ?
  created_at: 2023-05-21 19:51:35+00:00
  edited: false
  hidden: false
  id: 646a8457f51b56a285c38474
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Manticore-13B-GGML
repo_type: model
status: closed
target_branch: null
title: Does this work with oogabooga?
