!!python/object:huggingface_hub.community.DiscussionWithDetails
author: batawfic
conflicting_files: null
created_at: 2023-10-17 18:53:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c4473bf6b3cbd1ebb48b6e9ca9dc7a49.svg
      fullname: Basem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: batawfic
      type: user
    createdAt: '2023-10-17T19:53:12.000Z'
    data:
      edited: false
      editors:
      - batawfic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5430889129638672
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c4473bf6b3cbd1ebb48b6e9ca9dc7a49.svg
          fullname: Basem
          isHf: false
          isPro: false
          name: batawfic
          type: user
        html: '<p>I hope someone can help me with this, I''m trying to do inference
          with 2GPU using vLLM</p>

          <p>I''m using:<br>llm = LLM(model=MODEL_NAME, quantization="awq", dtype="half",
          tensor_parallel_size=2)</p>

          <p>But it failed with assertion error. My question is does that model run
          inference on multiple GPU. If yes what below error mean:<br> 559     return
          ignored<br>    561 # Execute the model.<br>--&gt; 562 output = self._run_workers(<br>    563     "execute_model",<br>    564     seq_group_metadata_list=seq_group_metadata_list,<br>    565     blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,<br>    566     blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,<br>    567     blocks_to_copy=scheduler_outputs.blocks_to_copy,<br>    568
          )<br>    570 return self._process_model_outputs(output, scheduler_outputs)
          + ignored</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/vllm/engine/llm_engine.py:712,
          in LLMEngine._run_workers(self, method, get_all_outputs, *args, **kwargs)<br>    710
          output = all_outputs[0]<br>    711 for other_output in all_outputs[1:]:<br>--&gt;
          712     assert output == other_output<br>    713 return output</p>

          <p>AssertionError: </p>

          '
        raw: "I hope someone can help me with this, I'm trying to do inference with\
          \ 2GPU using vLLM\r\n\r\nI'm using:\r\nllm = LLM(model=MODEL_NAME, quantization=\"\
          awq\", dtype=\"half\", tensor_parallel_size=2)\r\n\r\nBut it failed with\
          \ assertion error. My question is does that model run inference on multiple\
          \ GPU. If yes what below error mean:\r\n 559     return ignored\r\n    561\
          \ # Execute the model.\r\n--> 562 output = self._run_workers(\r\n    563\
          \     \"execute_model\",\r\n    564     seq_group_metadata_list=seq_group_metadata_list,\r\
          \n    565     blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\r\n\
          \    566     blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\r\n\
          \    567     blocks_to_copy=scheduler_outputs.blocks_to_copy,\r\n    568\
          \ )\r\n    570 return self._process_model_outputs(output, scheduler_outputs)\
          \ + ignored\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/vllm/engine/llm_engine.py:712,\
          \ in LLMEngine._run_workers(self, method, get_all_outputs, *args, **kwargs)\r\
          \n    710 output = all_outputs[0]\r\n    711 for other_output in all_outputs[1:]:\r\
          \n--> 712     assert output == other_output\r\n    713 return output\r\n\
          \r\nAssertionError: "
        updatedAt: '2023-10-17T19:53:12.581Z'
      numEdits: 0
      reactions: []
    id: 652ee628d7808dbb9d0641f2
    type: comment
  author: batawfic
  content: "I hope someone can help me with this, I'm trying to do inference with\
    \ 2GPU using vLLM\r\n\r\nI'm using:\r\nllm = LLM(model=MODEL_NAME, quantization=\"\
    awq\", dtype=\"half\", tensor_parallel_size=2)\r\n\r\nBut it failed with assertion\
    \ error. My question is does that model run inference on multiple GPU. If yes\
    \ what below error mean:\r\n 559     return ignored\r\n    561 # Execute the model.\r\
    \n--> 562 output = self._run_workers(\r\n    563     \"execute_model\",\r\n  \
    \  564     seq_group_metadata_list=seq_group_metadata_list,\r\n    565     blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\r\
    \n    566     blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\r\n   \
    \ 567     blocks_to_copy=scheduler_outputs.blocks_to_copy,\r\n    568 )\r\n  \
    \  570 return self._process_model_outputs(output, scheduler_outputs) + ignored\r\
    \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/vllm/engine/llm_engine.py:712,\
    \ in LLMEngine._run_workers(self, method, get_all_outputs, *args, **kwargs)\r\n\
    \    710 output = all_outputs[0]\r\n    711 for other_output in all_outputs[1:]:\r\
    \n--> 712     assert output == other_output\r\n    713 return output\r\n\r\nAssertionError: "
  created_at: 2023-10-17 18:53:12+00:00
  edited: false
  hidden: false
  id: 652ee628d7808dbb9d0641f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c4473bf6b3cbd1ebb48b6e9ca9dc7a49.svg
      fullname: Basem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: batawfic
      type: user
    createdAt: '2023-10-18T15:56:02.000Z'
    data:
      edited: false
      editors:
      - batawfic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9988880753517151
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c4473bf6b3cbd1ebb48b6e9ca9dc7a49.svg
          fullname: Basem
          isHf: false
          isPro: false
          name: batawfic
          type: user
        html: '<p>This is was solved with fix in vLLM and had nothing to do with model</p>

          '
        raw: This is was solved with fix in vLLM and had nothing to do with model
        updatedAt: '2023-10-18T15:56:02.908Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6530001224994878728ce254
    id: 6530001224994878728ce250
    type: comment
  author: batawfic
  content: This is was solved with fix in vLLM and had nothing to do with model
  created_at: 2023-10-18 14:56:02+00:00
  edited: false
  hidden: false
  id: 6530001224994878728ce250
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/c4473bf6b3cbd1ebb48b6e9ca9dc7a49.svg
      fullname: Basem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: batawfic
      type: user
    createdAt: '2023-10-18T15:56:02.000Z'
    data:
      status: closed
    id: 6530001224994878728ce254
    type: status-change
  author: batawfic
  created_at: 2023-10-18 14:56:02+00:00
  id: 6530001224994878728ce254
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-18T16:10:15.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9190576076507568
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thanks for letting us know</p>

          <p>Out of interest, what two GPUs are you using and how is performance compared
          to one GPU?</p>

          <p>With unquantised, my experience is that servers with tensor parallelism
          like TGI and vLLM do not scale great with multiple GPUs.  Like the second
          GPU adds only 60-80% more performance, not 100%.  So I prefer to scale across
          multiple separate instances (with a load balancer).  But I''ve not yet tried
          it with AWQ models.</p>

          '
        raw: 'Thanks for letting us know


          Out of interest, what two GPUs are you using and how is performance compared
          to one GPU?


          With unquantised, my experience is that servers with tensor parallelism
          like TGI and vLLM do not scale great with multiple GPUs.  Like the second
          GPU adds only 60-80% more performance, not 100%.  So I prefer to scale across
          multiple separate instances (with a load balancer).  But I''ve not yet tried
          it with AWQ models.'
        updatedAt: '2023-10-18T16:10:15.170Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - onealeph0cc
    id: 653003673fd15ee0a66a1b11
    type: comment
  author: TheBloke
  content: 'Thanks for letting us know


    Out of interest, what two GPUs are you using and how is performance compared to
    one GPU?


    With unquantised, my experience is that servers with tensor parallelism like TGI
    and vLLM do not scale great with multiple GPUs.  Like the second GPU adds only
    60-80% more performance, not 100%.  So I prefer to scale across multiple separate
    instances (with a load balancer).  But I''ve not yet tried it with AWQ models.'
  created_at: 2023-10-18 15:10:15+00:00
  edited: false
  hidden: false
  id: 653003673fd15ee0a66a1b11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c8a92a47a1b64da6ed01ec2e0ef27226.svg
      fullname: DS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: onealeph0cc
      type: user
    createdAt: '2023-10-29T21:35:19.000Z'
    data:
      edited: false
      editors:
      - onealeph0cc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9396065473556519
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c8a92a47a1b64da6ed01ec2e0ef27226.svg
          fullname: DS
          isHf: false
          isPro: true
          name: onealeph0cc
          type: user
        html: '<blockquote>

          <p>Thanks for letting us know</p>

          <p>Out of interest, what two GPUs are you using and how is performance compared
          to one GPU?</p>

          <p>With unquantised, my experience is that servers with tensor parallelism
          like TGI and vLLM do not scale great with multiple GPUs.  Like the second
          GPU adds only 60-80% more performance, not 100%.  So I prefer to scale across
          multiple separate instances (with a load balancer).  But I''ve not yet tried
          it with AWQ models.</p>

          </blockquote>

          <p>thanks for the insight, have you had a chance to try since?</p>

          '
        raw: "> Thanks for letting us know\n> \n> Out of interest, what two GPUs are\
          \ you using and how is performance compared to one GPU?\n> \n> With unquantised,\
          \ my experience is that servers with tensor parallelism like TGI and vLLM\
          \ do not scale great with multiple GPUs.  Like the second GPU adds only\
          \ 60-80% more performance, not 100%.  So I prefer to scale across multiple\
          \ separate instances (with a load balancer).  But I've not yet tried it\
          \ with AWQ models.\n\nthanks for the insight, have you had a chance to try\
          \ since?"
        updatedAt: '2023-10-29T21:35:19.052Z'
      numEdits: 0
      reactions: []
    id: 653ed01706a7bb4b1da0141a
    type: comment
  author: onealeph0cc
  content: "> Thanks for letting us know\n> \n> Out of interest, what two GPUs are\
    \ you using and how is performance compared to one GPU?\n> \n> With unquantised,\
    \ my experience is that servers with tensor parallelism like TGI and vLLM do not\
    \ scale great with multiple GPUs.  Like the second GPU adds only 60-80% more performance,\
    \ not 100%.  So I prefer to scale across multiple separate instances (with a load\
    \ balancer).  But I've not yet tried it with AWQ models.\n\nthanks for the insight,\
    \ have you had a chance to try since?"
  created_at: 2023-10-29 20:35:19+00:00
  edited: false
  hidden: false
  id: 653ed01706a7bb4b1da0141a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Xwin-LM-70B-V0.1-AWQ
repo_type: model
status: closed
target_branch: null
title: Run TheBloke/Xwin-LM-70B-V0.1-AWQ on Multiple GPU
