!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Zekri123
conflicting_files: null
created_at: 2023-11-23 10:16:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65168144639340e445d0fa6c/ZhDdVG_UvGG3S8-Oc-sp_.jpeg?w=200&h=200&f=face
      fullname: Sidi Mohamed Hicham Zekri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zekri123
      type: user
    createdAt: '2023-11-23T10:16:50.000Z'
    data:
      edited: false
      editors:
      - Zekri123
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5661382675170898
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65168144639340e445d0fa6c/ZhDdVG_UvGG3S8-Oc-sp_.jpeg?w=200&h=200&f=face
          fullname: Sidi Mohamed Hicham Zekri
          isHf: false
          isPro: false
          name: Zekri123
          type: user
        html: '<p>I have identified a discrepancy in the token count when generating
          responses using the Zephyr  model. When providing a question with a specific
          max_tokens value,  when attempting to count tokens in the text provided
          in the response using the Hugging Face tokenizer (len(pipe.tokenizer.encode(response))),
          the result exceeds the specified max_new_tokens (Sometimes, we observe an
          addition of more than 80 tokens.). This behavior is consistent across both
          LLama 2 and Zephyr models</p>

          <p>Code :<br>import torch<br>from transformers import pipeline</p>

          <p>pipe = pipeline("text-generation", model="HuggingFaceH4/zephyr-7b-alpha",
          torch_dtype=torch.bfloat16, device_map="auto")</p>

          <h1 id="we-use-the-tokenizers-chat-template-to-format-each-message---see-httpshuggingfacecodocstransformersmainenchat_templating">We
          use the tokenizer''s chat template to format each message - see <a href="https://huggingface.co/docs/transformers/main/en/chat_templating">https://huggingface.co/docs/transformers/main/en/chat_templating</a></h1>

          <p>messages = [<br>    {<br>        "role": "system",<br>        "content":
          "You are a friendly chatbot who always responds in the style of a pirate",<br>    },<br>    {"role":
          "user", "content": "How many helicopters can a human eat in one sitting?"},<br>]<br>prompt
          = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)<br>outputs
          = pipe(prompt, max_new_tokens=100, do_sample=True, temperature=0.7, top_k=50,
          top_p=0.95)<br>print(len(pipe.tokenizer.encode(outputs[0]["generated_text"])))
          # i get 116</p>

          '
        raw: "I have identified a discrepancy in the token count when generating responses\
          \ using the Zephyr  model. When providing a question with a specific max_tokens\
          \ value,  when attempting to count tokens in the text provided in the response\
          \ using the Hugging Face tokenizer (len(pipe.tokenizer.encode(response))),\
          \ the result exceeds the specified max_new_tokens (Sometimes, we observe\
          \ an addition of more than 80 tokens.). This behavior is consistent across\
          \ both LLama 2 and Zephyr models\r\n\r\nCode : \r\nimport torch\r\nfrom\
          \ transformers import pipeline\r\n\r\npipe = pipeline(\"text-generation\"\
          , model=\"HuggingFaceH4/zephyr-7b-alpha\", torch_dtype=torch.bfloat16, device_map=\"\
          auto\")\r\n\r\n# We use the tokenizer's chat template to format each message\
          \ - see https://huggingface.co/docs/transformers/main/en/chat_templating\r\
          \nmessages = [\r\n    {\r\n        \"role\": \"system\",\r\n        \"content\"\
          : \"You are a friendly chatbot who always responds in the style of a pirate\"\
          ,\r\n    },\r\n    {\"role\": \"user\", \"content\": \"How many helicopters\
          \ can a human eat in one sitting?\"},\r\n]\r\nprompt = pipe.tokenizer.apply_chat_template(messages,\
          \ tokenize=False, add_generation_prompt=True)\r\noutputs = pipe(prompt,\
          \ max_new_tokens=100, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\r\
          \nprint(len(pipe.tokenizer.encode(outputs[0][\"generated_text\"]))) # i\
          \ get 116"
        updatedAt: '2023-11-23T10:16:50.572Z'
      numEdits: 0
      reactions: []
    id: 655f2692324de022a3945a4b
    type: comment
  author: Zekri123
  content: "I have identified a discrepancy in the token count when generating responses\
    \ using the Zephyr  model. When providing a question with a specific max_tokens\
    \ value,  when attempting to count tokens in the text provided in the response\
    \ using the Hugging Face tokenizer (len(pipe.tokenizer.encode(response))), the\
    \ result exceeds the specified max_new_tokens (Sometimes, we observe an addition\
    \ of more than 80 tokens.). This behavior is consistent across both LLama 2 and\
    \ Zephyr models\r\n\r\nCode : \r\nimport torch\r\nfrom transformers import pipeline\r\
    \n\r\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-alpha\"\
    , torch_dtype=torch.bfloat16, device_map=\"auto\")\r\n\r\n# We use the tokenizer's\
    \ chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\r\
    \nmessages = [\r\n    {\r\n        \"role\": \"system\",\r\n        \"content\"\
    : \"You are a friendly chatbot who always responds in the style of a pirate\"\
    ,\r\n    },\r\n    {\"role\": \"user\", \"content\": \"How many helicopters can\
    \ a human eat in one sitting?\"},\r\n]\r\nprompt = pipe.tokenizer.apply_chat_template(messages,\
    \ tokenize=False, add_generation_prompt=True)\r\noutputs = pipe(prompt, max_new_tokens=100,\
    \ do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\r\nprint(len(pipe.tokenizer.encode(outputs[0][\"\
    generated_text\"]))) # i get 116"
  created_at: 2023-11-23 10:16:50+00:00
  edited: false
  hidden: false
  id: 655f2692324de022a3945a4b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 33
repo_id: HuggingFaceH4/zephyr-7b-alpha
repo_type: model
status: open
target_branch: null
title: Incorrect Token Count in Generated Response
