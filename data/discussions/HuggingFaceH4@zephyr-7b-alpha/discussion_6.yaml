!!python/object:huggingface_hub.community.DiscussionWithDetails
author: darkandpure
conflicting_files: null
created_at: 2023-10-12 07:23:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1da4e659eaa48701a6fff126aad2d086.svg
      fullname: Vivekjyoti bhowmik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darkandpure
      type: user
    createdAt: '2023-10-12T08:23:37.000Z'
    data:
      edited: false
      editors:
      - darkandpure
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9883172512054443
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1da4e659eaa48701a6fff126aad2d086.svg
          fullname: Vivekjyoti bhowmik
          isHf: false
          isPro: false
          name: darkandpure
          type: user
        html: '<p>I was trying with a prompt where Im feeding codebase in that , what
          /I found Is that it is taking hell lot of time for generation with 22gb
          of gpu where as mistral was taking very less time, Can I know the reason
          behind and what will be solution for better latency ?</p>

          '
        raw: "I was trying with a prompt where Im feeding codebase in that , what\
          \ /I found Is that it is taking hell lot of time for generation with 22gb\
          \ of gpu where as mistral was taking very less time, Can I know the reason\
          \ behind and what will be solution for better latency ?\r\n"
        updatedAt: '2023-10-12T08:23:37.463Z'
      numEdits: 0
      reactions: []
    id: 6527ad091eb78901534c4062
    type: comment
  author: darkandpure
  content: "I was trying with a prompt where Im feeding codebase in that , what /I\
    \ found Is that it is taking hell lot of time for generation with 22gb of gpu\
    \ where as mistral was taking very less time, Can I know the reason behind and\
    \ what will be solution for better latency ?\r\n"
  created_at: 2023-10-12 07:23:37+00:00
  edited: false
  hidden: false
  id: 6527ad091eb78901534c4062
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
      fullname: Lewis Tunstall
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lewtun
      type: user
    createdAt: '2023-10-13T14:55:37.000Z'
    data:
      edited: false
      editors:
      - lewtun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9754428863525391
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
          fullname: Lewis Tunstall
          isHf: true
          isPro: false
          name: lewtun
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;darkandpure&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/darkandpure\"\
          >@<span class=\"underline\">darkandpure</span></a></span>\n\n\t</span></span>\
          \ can you please share a code snippet of what you're running for inference,\
          \ along with the tokens / s you're getting? It would also be useful to know\
          \ what hardware you're running on. Thank you!</p>\n"
        raw: Hello @darkandpure can you please share a code snippet of what you're
          running for inference, along with the tokens / s you're getting? It would
          also be useful to know what hardware you're running on. Thank you!
        updatedAt: '2023-10-13T14:55:37.125Z'
      numEdits: 0
      reactions: []
    id: 65295a69b118f26df7067786
    type: comment
  author: lewtun
  content: Hello @darkandpure can you please share a code snippet of what you're running
    for inference, along with the tokens / s you're getting? It would also be useful
    to know what hardware you're running on. Thank you!
  created_at: 2023-10-13 13:55:37+00:00
  edited: false
  hidden: false
  id: 65295a69b118f26df7067786
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: HuggingFaceH4/zephyr-7b-alpha
repo_type: model
status: open
target_branch: null
title: Slow generation
