!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ebowwa
conflicting_files: null
created_at: 2023-10-14 01:15:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/366416f57bcaac5872132560dc34c10d.svg
      fullname: Ebowwa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ebowwa
      type: user
    createdAt: '2023-10-14T02:15:46.000Z'
    data:
      edited: false
      editors:
      - ebowwa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3876284956932068
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/366416f57bcaac5872132560dc34c10d.svg
          fullname: Ebowwa
          isHf: false
          isPro: false
          name: ebowwa
          type: user
        html: "<p>add Codeadd Markdown</p>\n<p>import pandas as pd<br>from transformers\
          \ import AutoTokenizer, AutoModelForCausalLM<br>\u200B</p>\n<h1 id=\"load-model-direc\"\
          >Load model direc</h1>\n<p>tokenizer = AutoTokenizer.from_pretrained(\"\
          HuggingFaceH4/zephyr-7b-alpha\")<br>model = AutoModelForCausalLM.from_pretrained(\"\
          HuggingFaceH4/zephyr-7b-alpha\")<br>\u200B</p>\n<h1 id=\"if-you-have-a-gpu-available-use-it-for-faster-processing\"\
          >If you have a GPU available, use it for faster processing</h1>\n<p>device\
          \ = \"cuda\" if torch.cuda.is_available() else \"cpu\"<br>model.to(device)<br>\u200B\
          </p>\n<h1 id=\"process-titles-load-the-dataset-and-extract-all-video-titles\"\
          >Process Titles: Load the dataset and extract all video titles</h1>\n<p>dataset_path\
          \ = \"/kaggle/input/youtube-watching/video_metadata.csv\"<br>video_data\
          \ = pd.read_csv(dataset_path)<br>video_titles = video_data['title'].tolist()<br>\u200B\
          <br>enhanced_titles = []<br>\u200B<br>for title in video_titles:<br>   \
          \ model_inputs = tokenizer([title], return_tensors=\"pt\").to(device)<br>\
          \    generated_ids = model.generate(**model_inputs, max_length=100, do_sample=True)<br>\
          \    generated_text = tokenizer.batch_decode(generated_ids)[0]<br>    enhanced_titles.append(generated_text)<br>\u200B\
          </p>\n<h1 id=\"print-or-save-the-enhanced-titles-as-needed\">Print or save\
          \ the enhanced titles as needed</h1>\n<p>print(enhanced_titles)<br>Downloading\
          \ (\u2026)okenizer_config.json: 100%<br>1.43k/1.43k [00:00&lt;00:00, 124kB/s]<br>Downloading\
          \ tokenizer.model: 100%<br>493k/493k [00:00&lt;00:00, 8.68MB/s]<br>Downloading\
          \ (\u2026)/main/tokenizer.json: 100%<br>1.80M/1.80M [00:00&lt;00:00, 17.2MB/s]<br>Downloading\
          \ (\u2026)in/added_tokens.json: 100%<br>42.0/42.0 [00:00&lt;00:00, 3.88kB/s]<br>Downloading\
          \ (\u2026)cial_tokens_map.json: 100%<br>168/168 [00:00&lt;00:00, 15.6kB/s]<br>Downloading\
          \ (\u2026)lve/main/config.json: 100%<br>639/639 [00:00&lt;00:00, 58.5kB/s]</p>\n\
          <hr>\n<p>KeyError                                  Traceback (most recent\
          \ call last)<br>Cell In[10], line 6<br>      4 # Load model direc<br>  \
          \    5 tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\"\
          )<br>----&gt; 6 model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\"\
          )<br>      8 # If you have a GPU available, use it for faster processing<br>\
          \      9 device = \"cuda\" if torch.cuda.is_available() else \"cpu\"</p>\n\
          <p>File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:527,\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)<br>\
          \    522 if kwargs.get(\"quantization_config\", None) is not None:<br> \
          \   523     _ = kwargs.pop(\"quantization_config\")<br>    525 config, kwargs\
          \ = AutoConfig.from_pretrained(<br>    526     pretrained_model_name_or_path,<br>--&gt;\
          \ 527     return_unused_kwargs=True,<br>    528     trust_remote_code=trust_remote_code,<br>\
          \    529     code_revision=code_revision,<br>    530     _commit_hash=commit_hash,<br>\
          \    531     **hub_kwargs,<br>    532     **kwargs,<br>    533 )<br>   \
          \ 535 # if torch_dtype=auto was passed here, ensure to pass it on<br>  \
          \  536 if kwargs_orig.get(\"torch_dtype\", None) == \"auto\":</p>\n<p>File\
          \ /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1041,\
          \ in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)<br> \
          \  1036 has_local_code = \"model_type\" in config_dict and config_dict[\"\
          model_type\"] in CONFIG_MAPPING<br>   1037 trust_remote_code = resolve_trust_remote_code(<br>\
          \   1038     trust_remote_code, pretrained_model_name_or_path, has_local_code,\
          \ has_remote_code<br>   1039 )<br>-&gt; 1041 if has_remote_code and trust_remote_code:<br>\
          \   1042     class_ref = config_dict[\"auto_map\"][\"AutoConfig\"]<br> \
          \  1043     config_class = get_class_from_dynamic_module(<br>   1044   \
          \      class_ref, pretrained_model_name_or_path, code_revision=code_revision,\
          \ **kwargs<br>   1045     )</p>\n<p>File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:734,\
          \ in <strong>getitem</strong>(self, key)<br>    730             return key<br>\
          \    731     return None<br>--&gt; 734 class _LazyConfigMapping(OrderedDict):<br>\
          \    735     \"\"\"<br>    736     A dictionary that lazily load its values\
          \ when they are requested.<br>    737     \"\"\"<br>    739     def <strong>init</strong>(self,\
          \ mapping):</p>\n<p>KeyError: 'mistral'</p>\n"
        raw: "add Codeadd Markdown\r\n\r\nimport pandas as pd\r\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\r\n\u200B\r\n# Load model direc\r\
          \ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\"\
          )\r\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\"\
          )\r\n\u200B\r\n# If you have a GPU available, use it for faster processing\r\
          \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\nmodel.to(device)\r\
          \n\u200B\r\n# Process Titles: Load the dataset and extract all video titles\r\
          \ndataset_path = \"/kaggle/input/youtube-watching/video_metadata.csv\"\r\
          \nvideo_data = pd.read_csv(dataset_path)\r\nvideo_titles = video_data['title'].tolist()\r\
          \n\u200B\r\nenhanced_titles = []\r\n\u200B\r\nfor title in video_titles:\r\
          \n    model_inputs = tokenizer([title], return_tensors=\"pt\").to(device)\r\
          \n    generated_ids = model.generate(**model_inputs, max_length=100, do_sample=True)\r\
          \n    generated_text = tokenizer.batch_decode(generated_ids)[0]\r\n    enhanced_titles.append(generated_text)\r\
          \n\u200B\r\n# Print or save the enhanced titles as needed\r\nprint(enhanced_titles)\r\
          \nDownloading (\u2026)okenizer_config.json: 100%\r\n1.43k/1.43k [00:00<00:00,\
          \ 124kB/s]\r\nDownloading tokenizer.model: 100%\r\n493k/493k [00:00<00:00,\
          \ 8.68MB/s]\r\nDownloading (\u2026)/main/tokenizer.json: 100%\r\n1.80M/1.80M\
          \ [00:00<00:00, 17.2MB/s]\r\nDownloading (\u2026)in/added_tokens.json: 100%\r\
          \n42.0/42.0 [00:00<00:00, 3.88kB/s]\r\nDownloading (\u2026)cial_tokens_map.json:\
          \ 100%\r\n168/168 [00:00<00:00, 15.6kB/s]\r\nDownloading (\u2026)lve/main/config.json:\
          \ 100%\r\n639/639 [00:00<00:00, 58.5kB/s]\r\n---------------------------------------------------------------------------\r\
          \nKeyError                                  Traceback (most recent call\
          \ last)\r\nCell In[10], line 6\r\n      4 # Load model direc\r\n      5\
          \ tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\"\
          )\r\n----> 6 model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\"\
          )\r\n      8 # If you have a GPU available, use it for faster processing\r\
          \n      9 device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n\
          \r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:527,\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\
          \n    522 if kwargs.get(\"quantization_config\", None) is not None:\r\n\
          \    523     _ = kwargs.pop(\"quantization_config\")\r\n    525 config,\
          \ kwargs = AutoConfig.from_pretrained(\r\n    526     pretrained_model_name_or_path,\r\
          \n--> 527     return_unused_kwargs=True,\r\n    528     trust_remote_code=trust_remote_code,\r\
          \n    529     code_revision=code_revision,\r\n    530     _commit_hash=commit_hash,\r\
          \n    531     **hub_kwargs,\r\n    532     **kwargs,\r\n    533 )\r\n  \
          \  535 # if torch_dtype=auto was passed here, ensure to pass it on\r\n \
          \   536 if kwargs_orig.get(\"torch_dtype\", None) == \"auto\":\r\n\r\nFile\
          \ /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1041,\
          \ in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\n \
          \  1036 has_local_code = \"model_type\" in config_dict and config_dict[\"\
          model_type\"] in CONFIG_MAPPING\r\n   1037 trust_remote_code = resolve_trust_remote_code(\r\
          \n   1038     trust_remote_code, pretrained_model_name_or_path, has_local_code,\
          \ has_remote_code\r\n   1039 )\r\n-> 1041 if has_remote_code and trust_remote_code:\r\
          \n   1042     class_ref = config_dict[\"auto_map\"][\"AutoConfig\"]\r\n\
          \   1043     config_class = get_class_from_dynamic_module(\r\n   1044  \
          \       class_ref, pretrained_model_name_or_path, code_revision=code_revision,\
          \ **kwargs\r\n   1045     )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:734,\
          \ in __getitem__(self, key)\r\n    730             return key\r\n    731\
          \     return None\r\n--> 734 class _LazyConfigMapping(OrderedDict):\r\n\
          \    735     \"\"\"\r\n    736     A dictionary that lazily load its values\
          \ when they are requested.\r\n    737     \"\"\"\r\n    739     def __init__(self,\
          \ mapping):\r\n\r\nKeyError: 'mistral'"
        updatedAt: '2023-10-14T02:15:46.492Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Pipboyguy
        - GustavoContreirasHAH
        - HealthTransformers
    id: 6529f9d20ab8936887770d92
    type: comment
  author: ebowwa
  content: "add Codeadd Markdown\r\n\r\nimport pandas as pd\r\nfrom transformers import\
    \ AutoTokenizer, AutoModelForCausalLM\r\n\u200B\r\n# Load model direc\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\r\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\r\n\
    \u200B\r\n# If you have a GPU available, use it for faster processing\r\ndevice\
    \ = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\nmodel.to(device)\r\n\
    \u200B\r\n# Process Titles: Load the dataset and extract all video titles\r\n\
    dataset_path = \"/kaggle/input/youtube-watching/video_metadata.csv\"\r\nvideo_data\
    \ = pd.read_csv(dataset_path)\r\nvideo_titles = video_data['title'].tolist()\r\
    \n\u200B\r\nenhanced_titles = []\r\n\u200B\r\nfor title in video_titles:\r\n \
    \   model_inputs = tokenizer([title], return_tensors=\"pt\").to(device)\r\n  \
    \  generated_ids = model.generate(**model_inputs, max_length=100, do_sample=True)\r\
    \n    generated_text = tokenizer.batch_decode(generated_ids)[0]\r\n    enhanced_titles.append(generated_text)\r\
    \n\u200B\r\n# Print or save the enhanced titles as needed\r\nprint(enhanced_titles)\r\
    \nDownloading (\u2026)okenizer_config.json: 100%\r\n1.43k/1.43k [00:00<00:00,\
    \ 124kB/s]\r\nDownloading tokenizer.model: 100%\r\n493k/493k [00:00<00:00, 8.68MB/s]\r\
    \nDownloading (\u2026)/main/tokenizer.json: 100%\r\n1.80M/1.80M [00:00<00:00,\
    \ 17.2MB/s]\r\nDownloading (\u2026)in/added_tokens.json: 100%\r\n42.0/42.0 [00:00<00:00,\
    \ 3.88kB/s]\r\nDownloading (\u2026)cial_tokens_map.json: 100%\r\n168/168 [00:00<00:00,\
    \ 15.6kB/s]\r\nDownloading (\u2026)lve/main/config.json: 100%\r\n639/639 [00:00<00:00,\
    \ 58.5kB/s]\r\n---------------------------------------------------------------------------\r\
    \nKeyError                                  Traceback (most recent call last)\r\
    \nCell In[10], line 6\r\n      4 # Load model direc\r\n      5 tokenizer = AutoTokenizer.from_pretrained(\"\
    HuggingFaceH4/zephyr-7b-alpha\")\r\n----> 6 model = AutoModelForCausalLM.from_pretrained(\"\
    HuggingFaceH4/zephyr-7b-alpha\")\r\n      8 # If you have a GPU available, use\
    \ it for faster processing\r\n      9 device = \"cuda\" if torch.cuda.is_available()\
    \ else \"cpu\"\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:527,\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\
    \n    522 if kwargs.get(\"quantization_config\", None) is not None:\r\n    523\
    \     _ = kwargs.pop(\"quantization_config\")\r\n    525 config, kwargs = AutoConfig.from_pretrained(\r\
    \n    526     pretrained_model_name_or_path,\r\n--> 527     return_unused_kwargs=True,\r\
    \n    528     trust_remote_code=trust_remote_code,\r\n    529     code_revision=code_revision,\r\
    \n    530     _commit_hash=commit_hash,\r\n    531     **hub_kwargs,\r\n    532\
    \     **kwargs,\r\n    533 )\r\n    535 # if torch_dtype=auto was passed here,\
    \ ensure to pass it on\r\n    536 if kwargs_orig.get(\"torch_dtype\", None) ==\
    \ \"auto\":\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1041,\
    \ in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\n   1036\
    \ has_local_code = \"model_type\" in config_dict and config_dict[\"model_type\"\
    ] in CONFIG_MAPPING\r\n   1037 trust_remote_code = resolve_trust_remote_code(\r\
    \n   1038     trust_remote_code, pretrained_model_name_or_path, has_local_code,\
    \ has_remote_code\r\n   1039 )\r\n-> 1041 if has_remote_code and trust_remote_code:\r\
    \n   1042     class_ref = config_dict[\"auto_map\"][\"AutoConfig\"]\r\n   1043\
    \     config_class = get_class_from_dynamic_module(\r\n   1044         class_ref,\
    \ pretrained_model_name_or_path, code_revision=code_revision, **kwargs\r\n   1045\
    \     )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:734,\
    \ in __getitem__(self, key)\r\n    730             return key\r\n    731     return\
    \ None\r\n--> 734 class _LazyConfigMapping(OrderedDict):\r\n    735     \"\"\"\
    \r\n    736     A dictionary that lazily load its values when they are requested.\r\
    \n    737     \"\"\"\r\n    739     def __init__(self, mapping):\r\n\r\nKeyError:\
    \ 'mistral'"
  created_at: 2023-10-14 01:15:46+00:00
  edited: false
  hidden: false
  id: 6529f9d20ab8936887770d92
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a7497c7c91ac9b0067a1d3a457a7a18.svg
      fullname: Ish Girwan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ish
      type: user
    createdAt: '2023-10-14T03:27:13.000Z'
    data:
      edited: false
      editors:
      - Ish
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9463493824005127
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a7497c7c91ac9b0067a1d3a457a7a18.svg
          fullname: Ish Girwan
          isHf: false
          isPro: false
          name: Ish
          type: user
        html: '<p>I am also getting the same error.</p>

          '
        raw: I am also getting the same error.
        updatedAt: '2023-10-14T03:27:13.590Z'
      numEdits: 0
      reactions: []
    id: 652a0a9195f4676fe1ee08b4
    type: comment
  author: Ish
  content: I am also getting the same error.
  created_at: 2023-10-14 02:27:13+00:00
  edited: false
  hidden: false
  id: 652a0a9195f4676fe1ee08b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
      fullname: Lewis Tunstall
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lewtun
      type: user
    createdAt: '2023-10-14T09:17:58.000Z'
    data:
      edited: false
      editors:
      - lewtun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8242733478546143
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
          fullname: Lewis Tunstall
          isHf: true
          isPro: false
          name: lewtun
          type: user
        html: '<p>You need the latest version of <code>transformers</code> to load
          this model, so I think if you run <code>pip install -U transformers</code>
          the error should be resolved. Please let me know if the error persists though!</p>

          '
        raw: You need the latest version of `transformers` to load this model, so
          I think if you run `pip install -U transformers` the error should be resolved.
          Please let me know if the error persists though!
        updatedAt: '2023-10-14T09:17:58.580Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - anakin87
        - nps798
        - lumiseven
        - marawanxmamdouh
        - vwxyzjn
        - drewd789
    id: 652a5cc6375b3a8bc158a6af
    type: comment
  author: lewtun
  content: You need the latest version of `transformers` to load this model, so I
    think if you run `pip install -U transformers` the error should be resolved. Please
    let me know if the error persists though!
  created_at: 2023-10-14 08:17:58+00:00
  edited: false
  hidden: false
  id: 652a5cc6375b3a8bc158a6af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59bd54cd8f1af28d96331c02802e565c.svg
      fullname: patrick
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cosmopax
      type: user
    createdAt: '2023-10-23T18:06:05.000Z'
    data:
      edited: true
      editors:
      - cosmopax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8225562572479248
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59bd54cd8f1af28d96331c02802e565c.svg
          fullname: patrick
          isHf: false
          isPro: false
          name: cosmopax
          type: user
        html: '<p>i have the latest version of transformers, yet still getting the
          KeyError ''mistral''. I try to run it on oracle linux server</p>

          <p>Traceback (most recent call last):<br>  File "load_model_and_generate.py",
          line 6, in <br>    model = AutoModelForCausalLM.from_pretrained("./zephyr-7b-alpha")<br>  File
          "/usr/local/lib/python3.6/site-packages/transformers/models/auto/auto_factory.py",
          line 424, in from_pretrained<br>    pretrained_model_name_or_path, return_unused_kwargs=True,
          trust_remote_code=trust_remote_code, **kwargs<br>  File "/usr/local/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py",
          line 672, in from_pretrained<br>    config_class = CONFIG_MAPPING[config_dict["model_type"]]<br>  File
          "/usr/local/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py",
          line 387, in <strong>getitem</strong><br>    raise KeyError(key)<br>KeyError:
          ''mistral''</p>

          '
        raw: "i have the latest version of transformers, yet still getting the KeyError\
          \ 'mistral'. I try to run it on oracle linux server\n\nTraceback (most recent\
          \ call last):\n  File \"load_model_and_generate.py\", line 6, in <module>\n\
          \    model = AutoModelForCausalLM.from_pretrained(\"./zephyr-7b-alpha\"\
          )\n  File \"/usr/local/lib/python3.6/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 424, in from_pretrained\n    pretrained_model_name_or_path, return_unused_kwargs=True,\
          \ trust_remote_code=trust_remote_code, **kwargs\n  File \"/usr/local/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 672, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]\n  File \"/usr/local/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 387, in __getitem__\n    raise KeyError(key)\nKeyError: 'mistral'"
        updatedAt: '2023-10-23T18:09:24.552Z'
      numEdits: 1
      reactions: []
    id: 6536b60d92e283bfff07da42
    type: comment
  author: cosmopax
  content: "i have the latest version of transformers, yet still getting the KeyError\
    \ 'mistral'. I try to run it on oracle linux server\n\nTraceback (most recent\
    \ call last):\n  File \"load_model_and_generate.py\", line 6, in <module>\n  \
    \  model = AutoModelForCausalLM.from_pretrained(\"./zephyr-7b-alpha\")\n  File\
    \ \"/usr/local/lib/python3.6/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 424, in from_pretrained\n    pretrained_model_name_or_path, return_unused_kwargs=True,\
    \ trust_remote_code=trust_remote_code, **kwargs\n  File \"/usr/local/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 672, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
    model_type\"]]\n  File \"/usr/local/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 387, in __getitem__\n    raise KeyError(key)\nKeyError: 'mistral'"
  created_at: 2023-10-23 17:06:05+00:00
  edited: true
  hidden: false
  id: 6536b60d92e283bfff07da42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6e61c4a0d65f220061137f75cdaaeab.svg
      fullname: Mingkun Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vermilion3154
      type: user
    createdAt: '2023-10-29T15:06:30.000Z'
    data:
      edited: false
      editors:
      - Vermilion3154
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5061950087547302
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6e61c4a0d65f220061137f75cdaaeab.svg
          fullname: Mingkun Yu
          isHf: false
          isPro: false
          name: Vermilion3154
          type: user
        html: '<blockquote>

          <p>i have the latest version of transformers, yet still getting the KeyError
          ''mistral''. I try to run it on oracle linux server</p>

          <p>Traceback (most recent call last):<br>  File "load_model_and_generate.py",
          line 6, in <br>    model = AutoModelForCausalLM.from_pretrained("./zephyr-7b-alpha")<br>  File
          "/usr/local/lib/python3.6/site-packages/transformers/models/auto/auto_factory.py",
          line 424, in from_pretrained<br>    pretrained_model_name_or_path, return_unused_kwargs=True,
          trust_remote_code=trust_remote_code, **kwargs<br>  File "/usr/local/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py",
          line 672, in from_pretrained<br>    config_class = CONFIG_MAPPING[config_dict["model_type"]]<br>  File
          "/usr/local/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py",
          line 387, in <strong>getitem</strong><br>    raise KeyError(key)<br>KeyError:
          ''mistral''</p>

          </blockquote>

          <p>print your transformers version please, is it above 4.33?</p>

          '
        raw: "> i have the latest version of transformers, yet still getting the KeyError\
          \ 'mistral'. I try to run it on oracle linux server\n> \n> Traceback (most\
          \ recent call last):\n>   File \"load_model_and_generate.py\", line 6, in\
          \ <module>\n>     model = AutoModelForCausalLM.from_pretrained(\"./zephyr-7b-alpha\"\
          )\n>   File \"/usr/local/lib/python3.6/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 424, in from_pretrained\n>     pretrained_model_name_or_path, return_unused_kwargs=True,\
          \ trust_remote_code=trust_remote_code, **kwargs\n>   File \"/usr/local/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 672, in from_pretrained\n>     config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]\n>   File \"/usr/local/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 387, in __getitem__\n>     raise KeyError(key)\n> KeyError: 'mistral'\n\
          \nprint your transformers version please, is it above 4.33?\n"
        updatedAt: '2023-10-29T15:06:30.226Z'
      numEdits: 0
      reactions: []
    id: 653e74f67ef643534b434f10
    type: comment
  author: Vermilion3154
  content: "> i have the latest version of transformers, yet still getting the KeyError\
    \ 'mistral'. I try to run it on oracle linux server\n> \n> Traceback (most recent\
    \ call last):\n>   File \"load_model_and_generate.py\", line 6, in <module>\n\
    >     model = AutoModelForCausalLM.from_pretrained(\"./zephyr-7b-alpha\")\n> \
    \  File \"/usr/local/lib/python3.6/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 424, in from_pretrained\n>     pretrained_model_name_or_path, return_unused_kwargs=True,\
    \ trust_remote_code=trust_remote_code, **kwargs\n>   File \"/usr/local/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 672, in from_pretrained\n>     config_class = CONFIG_MAPPING[config_dict[\"\
    model_type\"]]\n>   File \"/usr/local/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 387, in __getitem__\n>     raise KeyError(key)\n> KeyError: 'mistral'\n\
    \nprint your transformers version please, is it above 4.33?\n"
  created_at: 2023-10-29 14:06:30+00:00
  edited: false
  hidden: false
  id: 653e74f67ef643534b434f10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7b178c35f2187eb7b602d755e2c74ad3.svg
      fullname: Tanmey Rawal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: taurasAI
      type: user
    createdAt: '2023-10-30T11:33:39.000Z'
    data:
      edited: false
      editors:
      - taurasAI
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8526631593704224
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7b178c35f2187eb7b602d755e2c74ad3.svg
          fullname: Tanmey Rawal
          isHf: false
          isPro: false
          name: taurasAI
          type: user
        html: '<p>It is working for me , using transformers version 4.34.1, so you
          need to upgrade your transformers version</p>

          <p>!pip install --upgrade transformers accelerate</p>

          '
        raw: 'It is working for me , using transformers version 4.34.1, so you need
          to upgrade your transformers version


          !pip install --upgrade transformers accelerate'
        updatedAt: '2023-10-30T11:33:39.821Z'
      numEdits: 0
      reactions: []
    id: 653f9493de25f95f64cdf30f
    type: comment
  author: taurasAI
  content: 'It is working for me , using transformers version 4.34.1, so you need
    to upgrade your transformers version


    !pip install --upgrade transformers accelerate'
  created_at: 2023-10-30 10:33:39+00:00
  edited: false
  hidden: false
  id: 653f9493de25f95f64cdf30f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: HuggingFaceH4/zephyr-7b-alpha
repo_type: model
status: open
target_branch: null
title: 'KeyError: ''mistral'''
