!!python/object:huggingface_hub.community.DiscussionWithDetails
author: arogov
conflicting_files: null
created_at: 2023-10-11 17:59:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d79398592f3cbc9b78f644cea2bc9cf0.svg
      fullname: Alexey Rogov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arogov
      type: user
    createdAt: '2023-10-11T18:59:51.000Z'
    data:
      edited: false
      editors:
      - arogov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.665031373500824
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d79398592f3cbc9b78f644cea2bc9cf0.svg
          fullname: Alexey Rogov
          isHf: false
          isPro: false
          name: arogov
          type: user
        html: '<p><code>mistralai/Mistral-7B-Instruct-v0.1</code> can translate to
          Croatian easily, but <code>zephyr-7b-alpha</code> respond with: <code>Unfortunately,
          I am not capable of translating the summary into Croatian</code></p>

          '
        raw: '`mistralai/Mistral-7B-Instruct-v0.1` can translate to Croatian easily,
          but `zephyr-7b-alpha` respond with: `Unfortunately, I am not capable of
          translating the summary into Croatian`'
        updatedAt: '2023-10-11T18:59:51.183Z'
      numEdits: 0
      reactions: []
    id: 6526f0a762abbd50885a1bbd
    type: comment
  author: arogov
  content: '`mistralai/Mistral-7B-Instruct-v0.1` can translate to Croatian easily,
    but `zephyr-7b-alpha` respond with: `Unfortunately, I am not capable of translating
    the summary into Croatian`'
  created_at: 2023-10-11 17:59:51+00:00
  edited: false
  hidden: false
  id: 6526f0a762abbd50885a1bbd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
      fullname: Lewis Tunstall
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lewtun
      type: user
    createdAt: '2023-10-14T09:20:18.000Z'
    data:
      edited: false
      editors:
      - lewtun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.952205240726471
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
          fullname: Lewis Tunstall
          isHf: true
          isPro: false
          name: lewtun
          type: user
        html: '<p>Can you please provide an input example where this happens so I
          can also test it? </p>

          <p>For context, Zephyr was trained on English datasets (UltraChat and UltraFeedback)
          and Mistral''s Instruct model potentially had some multilingual data that
          makes it better suited for translation tasks.</p>

          '
        raw: "Can you please provide an input example where this happens so I can\
          \ also test it? \n\nFor context, Zephyr was trained on English datasets\
          \ (UltraChat and UltraFeedback) and Mistral's Instruct model potentially\
          \ had some multilingual data that makes it better suited for translation\
          \ tasks."
        updatedAt: '2023-10-14T09:20:18.419Z'
      numEdits: 0
      reactions: []
    id: 652a5d522aa5b27c77c01222
    type: comment
  author: lewtun
  content: "Can you please provide an input example where this happens so I can also\
    \ test it? \n\nFor context, Zephyr was trained on English datasets (UltraChat\
    \ and UltraFeedback) and Mistral's Instruct model potentially had some multilingual\
    \ data that makes it better suited for translation tasks."
  created_at: 2023-10-14 08:20:18+00:00
  edited: false
  hidden: false
  id: 652a5d522aa5b27c77c01222
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d79398592f3cbc9b78f644cea2bc9cf0.svg
      fullname: Alexey Rogov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arogov
      type: user
    createdAt: '2023-10-14T21:54:18.000Z'
    data:
      edited: true
      editors:
      - arogov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8241337537765503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d79398592f3cbc9b78f644cea2bc9cf0.svg
          fullname: Alexey Rogov
          isHf: false
          isPro: false
          name: arogov
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;lewtun&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lewtun\">@<span class=\"\
          underline\">lewtun</span></a></span>\n\n\t</span></span><br>Sure, here is\
          \ the exact prompt I used:</p>\n<pre><code>system_prompt = \"You are a friendly\
          \ assistant who knows all the books in the world. Respond to user queries\
          \ clearly, strictly following the context of the question, and provide an\
          \ answer within the requested number of words\"\nuser_message = \"summarize\
          \ the book Master and Margarita in just 1000 words\"\nuser_message2 = \"\
          then translate it to Croatian\"\nmessages=f\"\"\"\"&lt;s&gt;[INST] {system_prompt}\
          \ [/INST]\n[INST] {user_message} [/INST]\n[INST] {user_message2} [/INST]&lt;/s&gt;\"\
          \"\"\n</code></pre>\n"
        raw: "@lewtun \nSure, here is the exact prompt I used:\n```\nsystem_prompt\
          \ = \"You are a friendly assistant who knows all the books in the world.\
          \ Respond to user queries clearly, strictly following the context of the\
          \ question, and provide an answer within the requested number of words\"\
          \nuser_message = \"summarize the book Master and Margarita in just 1000\
          \ words\"\nuser_message2 = \"then translate it to Croatian\"\nmessages=f\"\
          \"\"\"<s>[INST] {system_prompt} [/INST]\n[INST] {user_message} [/INST]\n\
          [INST] {user_message2} [/INST]</s>\"\"\"\n```"
        updatedAt: '2023-10-14T21:55:36.183Z'
      numEdits: 1
      reactions: []
    id: 652b0e0a756a15d750f0a827
    type: comment
  author: arogov
  content: "@lewtun \nSure, here is the exact prompt I used:\n```\nsystem_prompt =\
    \ \"You are a friendly assistant who knows all the books in the world. Respond\
    \ to user queries clearly, strictly following the context of the question, and\
    \ provide an answer within the requested number of words\"\nuser_message = \"\
    summarize the book Master and Margarita in just 1000 words\"\nuser_message2 =\
    \ \"then translate it to Croatian\"\nmessages=f\"\"\"\"<s>[INST] {system_prompt}\
    \ [/INST]\n[INST] {user_message} [/INST]\n[INST] {user_message2} [/INST]</s>\"\
    \"\"\n```"
  created_at: 2023-10-14 20:54:18+00:00
  edited: true
  hidden: false
  id: 652b0e0a756a15d750f0a827
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
      fullname: Lewis Tunstall
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lewtun
      type: user
    createdAt: '2023-10-18T12:37:55.000Z'
    data:
      edited: false
      editors:
      - lewtun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4916347861289978
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
          fullname: Lewis Tunstall
          isHf: true
          isPro: false
          name: lewtun
          type: user
        html: "<p>Ah, we use a different prompt template which might explain the issue.\
          \ Can you try with this:</p>\n<pre><code class=\"language-python\">pipe\
          \ = pipeline(<span class=\"hljs-string\">\"text-generation\"</span>, model=<span\
          \ class=\"hljs-string\">\"HuggingFaceH4/zephyr-7b-alpha\"</span>, torch_dtype=torch.bfloat16,\
          \ device_map=<span class=\"hljs-string\">\"auto\"</span>)\n\n<span class=\"\
          hljs-comment\"># We use the tokenizer's chat template to format each message\
          \ - see https://huggingface.co/docs/transformers/main/en/chat_templating</span>\n\
          messages = [\n    {\n        <span class=\"hljs-string\">\"role\"</span>:\
          \ <span class=\"hljs-string\">\"system\"</span>,\n        <span class=\"\
          hljs-string\">\"content\"</span>: <span class=\"hljs-string\">\"You are\
          \ a friendly assistant who knows all the books in the world. Respond to\
          \ user queries clearly, strictly following the context of the question,\
          \ and provide an answer within the requested number of words\"</span>,\n\
          \    },\n    {<span class=\"hljs-string\">\"role\"</span>: <span class=\"\
          hljs-string\">\"user\"</span>, <span class=\"hljs-string\">\"content\"</span>:\
          \ <span class=\"hljs-string\">\"summarize the book Master and Margarita\
          \ in just 1000 words\"</span>},\n]\n<span class=\"hljs-comment\"># Generate\
          \ 1st turn</span>\nprompt = pipe.tokenizer.apply_chat_template(messages,\
          \ tokenize=<span class=\"hljs-literal\">False</span>, add_generation_prompt=<span\
          \ class=\"hljs-literal\">True</span>)\noutputs = pipe(prompt, max_new_tokens=<span\
          \ class=\"hljs-number\">256</span>, do_sample=<span class=\"hljs-literal\"\
          >True</span>, temperature=<span class=\"hljs-number\">0.7</span>, top_k=<span\
          \ class=\"hljs-number\">50</span>, top_p=<span class=\"hljs-number\">0.95</span>,\
          \ return_full_text=<span class=\"hljs-literal\">False</span>)\n<span class=\"\
          hljs-built_in\">print</span>(outputs[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">\"generated_text\"</span>])\nmessages.append({<span\
          \ class=\"hljs-string\">\"role\"</span>: <span class=\"hljs-string\">\"\
          assistant\"</span>, <span class=\"hljs-string\">\"content\"</span>: outputs[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">\"generated_text\"\
          </span>]})\nmessages.append({<span class=\"hljs-string\">\"role\"</span>:\
          \ <span class=\"hljs-string\">\"user\"</span>, <span class=\"hljs-string\"\
          >\"content\"</span>: <span class=\"hljs-string\">\"then translate it to\
          \ Croatian\"</span>})\n<span class=\"hljs-comment\"># Generate 2nd turn</span>\n\
          prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=<span class=\"\
          hljs-literal\">False</span>, add_generation_prompt=<span class=\"hljs-literal\"\
          >True</span>)\noutputs = pipe(prompt, max_new_tokens=<span class=\"hljs-number\"\
          >256</span>, do_sample=<span class=\"hljs-literal\">True</span>, temperature=<span\
          \ class=\"hljs-number\">0.7</span>, top_k=<span class=\"hljs-number\">50</span>,\
          \ top_p=<span class=\"hljs-number\">0.95</span>, return_full_text=<span\
          \ class=\"hljs-literal\">False</span>)\n<span class=\"hljs-built_in\">print</span>(outputs[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">\"generated_text\"\
          </span>])\n</code></pre>\n"
        raw: "Ah, we use a different prompt template which might explain the issue.\
          \ Can you try with this:\n\n```python\npipe = pipeline(\"text-generation\"\
          , model=\"HuggingFaceH4/zephyr-7b-alpha\", torch_dtype=torch.bfloat16, device_map=\"\
          auto\")\n\n# We use the tokenizer's chat template to format each message\
          \ - see https://huggingface.co/docs/transformers/main/en/chat_templating\n\
          messages = [\n    {\n        \"role\": \"system\",\n        \"content\"\
          : \"You are a friendly assistant who knows all the books in the world. Respond\
          \ to user queries clearly, strictly following the context of the question,\
          \ and provide an answer within the requested number of words\",\n    },\n\
          \    {\"role\": \"user\", \"content\": \"summarize the book Master and Margarita\
          \ in just 1000 words\"},\n]\n# Generate 1st turn\nprompt = pipe.tokenizer.apply_chat_template(messages,\
          \ tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256,\
          \ do_sample=True, temperature=0.7, top_k=50, top_p=0.95, return_full_text=False)\n\
          print(outputs[0][\"generated_text\"])\nmessages.append({\"role\": \"assistant\"\
          , \"content\": outputs[0][\"generated_text\"]})\nmessages.append({\"role\"\
          : \"user\", \"content\": \"then translate it to Croatian\"})\n# Generate\
          \ 2nd turn\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False,\
          \ add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256,\
          \ do_sample=True, temperature=0.7, top_k=50, top_p=0.95, return_full_text=False)\n\
          print(outputs[0][\"generated_text\"])\n```"
        updatedAt: '2023-10-18T12:37:55.870Z'
      numEdits: 0
      reactions: []
    id: 652fd1a3c4da780d124ab5c9
    type: comment
  author: lewtun
  content: "Ah, we use a different prompt template which might explain the issue.\
    \ Can you try with this:\n\n```python\npipe = pipeline(\"text-generation\", model=\"\
    HuggingFaceH4/zephyr-7b-alpha\", torch_dtype=torch.bfloat16, device_map=\"auto\"\
    )\n\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n\
    messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You\
    \ are a friendly assistant who knows all the books in the world. Respond to user\
    \ queries clearly, strictly following the context of the question, and provide\
    \ an answer within the requested number of words\",\n    },\n    {\"role\": \"\
    user\", \"content\": \"summarize the book Master and Margarita in just 1000 words\"\
    },\n]\n# Generate 1st turn\nprompt = pipe.tokenizer.apply_chat_template(messages,\
    \ tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256,\
    \ do_sample=True, temperature=0.7, top_k=50, top_p=0.95, return_full_text=False)\n\
    print(outputs[0][\"generated_text\"])\nmessages.append({\"role\": \"assistant\"\
    , \"content\": outputs[0][\"generated_text\"]})\nmessages.append({\"role\": \"\
    user\", \"content\": \"then translate it to Croatian\"})\n# Generate 2nd turn\n\
    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\
    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50,\
    \ top_p=0.95, return_full_text=False)\nprint(outputs[0][\"generated_text\"])\n\
    ```"
  created_at: 2023-10-18 11:37:55+00:00
  edited: false
  hidden: false
  id: 652fd1a3c4da780d124ab5c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
      fullname: Lewis Tunstall
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lewtun
      type: user
    createdAt: '2023-10-18T12:40:15.000Z'
    data:
      edited: false
      editors:
      - lewtun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6427314877510071
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
          fullname: Lewis Tunstall
          isHf: true
          isPro: false
          name: lewtun
          type: user
        html: '<p>Btw the demo seems to roughly understand the instruction: <a rel="nofollow"
          href="https://huggingfaceh4-zephyr-chat.hf.space">https://huggingfaceh4-zephyr-chat.hf.space</a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/5f0c746619cb630495b814fd/ynE6raJKr_n_yp9ZkQsCn.png"><img
          alt="Screenshot 2023-10-18 at 14.39.01.png" src="https://cdn-uploads.huggingface.co/production/uploads/5f0c746619cb630495b814fd/ynE6raJKr_n_yp9ZkQsCn.png"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/5f0c746619cb630495b814fd/KZZbca0sR_YH42mEC0aKD.png"><img
          alt="Screenshot 2023-10-18 at 14.39.05.png" src="https://cdn-uploads.huggingface.co/production/uploads/5f0c746619cb630495b814fd/KZZbca0sR_YH42mEC0aKD.png"></a></p>

          '
        raw: 'Btw the demo seems to roughly understand the instruction: https://huggingfaceh4-zephyr-chat.hf.space


          ![Screenshot 2023-10-18 at 14.39.01.png](https://cdn-uploads.huggingface.co/production/uploads/5f0c746619cb630495b814fd/ynE6raJKr_n_yp9ZkQsCn.png)

          ![Screenshot 2023-10-18 at 14.39.05.png](https://cdn-uploads.huggingface.co/production/uploads/5f0c746619cb630495b814fd/KZZbca0sR_YH42mEC0aKD.png)


          '
        updatedAt: '2023-10-18T12:40:15.352Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - arogov
    id: 652fd22fc4b3aa82cea01f2b
    type: comment
  author: lewtun
  content: 'Btw the demo seems to roughly understand the instruction: https://huggingfaceh4-zephyr-chat.hf.space


    ![Screenshot 2023-10-18 at 14.39.01.png](https://cdn-uploads.huggingface.co/production/uploads/5f0c746619cb630495b814fd/ynE6raJKr_n_yp9ZkQsCn.png)

    ![Screenshot 2023-10-18 at 14.39.05.png](https://cdn-uploads.huggingface.co/production/uploads/5f0c746619cb630495b814fd/KZZbca0sR_YH42mEC0aKD.png)


    '
  created_at: 2023-10-18 11:40:15+00:00
  edited: false
  hidden: false
  id: 652fd22fc4b3aa82cea01f2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d79398592f3cbc9b78f644cea2bc9cf0.svg
      fullname: Alexey Rogov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arogov
      type: user
    createdAt: '2023-10-19T11:56:12.000Z'
    data:
      edited: true
      editors:
      - arogov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9210611581802368
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d79398592f3cbc9b78f644cea2bc9cf0.svg
          fullname: Alexey Rogov
          isHf: false
          isPro: false
          name: arogov
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;lewtun&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lewtun\">@<span class=\"\
          underline\">lewtun</span></a></span>\n\n\t</span></span> thanks for your\
          \ checking!<br>I tried using your template but got exactly the same result\
          \ as before.<br>But as inference engine I use vLLM, which uses FastChat\
          \ for templates and, apparently, the template for Zephyr is incorrect and\
          \ the problem with incorrect generation lies in it.<br>Btw, template for\
          \ Mistral is also contains some garbage in it, that's why I used Mistralai's\
          \ string template instead of array.</p>\n<p>When I used the Mistral template\
          \ and the same approach as you (added the generated answer in English to\
          \ the context), then with this context Zephyr was able to generate the correct\
          \ translation into Croatian.</p>\n<p>I found the right template to try with\
          \ string prompt:</p>\n<pre><code class=\"language-python\">system_prompt\
          \ = <span class=\"hljs-string\">\"You are a friendly assistant who knows\
          \ all the books in the world. Respond to user queries clearly, strictly\
          \ following the context of the question, and provide an answer within the\
          \ requested number of words\"</span>\nuser_message = <span class=\"hljs-string\"\
          >\"summarize the book Master and Margarita in just 1000 words\"</span>\n\
          user_message2 = <span class=\"hljs-string\">\"then translate it to Croatian\"\
          </span>\nmessages = <span class=\"hljs-string\">f\"\"\"&lt;|system|&gt;<span\
          \ class=\"hljs-subst\">{system_prompt}</span>&lt;/s&gt;</span>\n<span class=\"\
          hljs-string\">&lt;|user|&gt;<span class=\"hljs-subst\">{user_message}</span>\
          \ <span class=\"hljs-subst\">{user_message2}</span>&lt;/s&gt;</span>\n<span\
          \ class=\"hljs-string\">&lt;|assistant|&gt;\"\"\"</span>\n</code></pre>\n\
          <p>An seems Zephyr can't translate without adding context in english, but\
          \ Mistral can do it.</p>\n<p>Zephyr can provide with just croatian translation\
          \ when I modify prompt like this:</p>\n<pre><code class=\"language-python\"\
          >messages = <span class=\"hljs-string\">f\"\"\"&lt;|system|&gt;<span class=\"\
          hljs-subst\">{system_prompt}</span>&lt;/s&gt;</span>\n<span class=\"hljs-string\"\
          >&lt;|user|&gt;<span class=\"hljs-subst\">{user_message}</span>&lt;/s&gt;</span>\n\
          <span class=\"hljs-string\">&lt;|assistant|&gt;&lt;/s&gt;</span>\n<span\
          \ class=\"hljs-string\">&lt;|user|&gt;<span class=\"hljs-subst\">{user_message2}</span>&lt;/s&gt;</span>\n\
          <span class=\"hljs-string\">&lt;|assistant|&gt;\"\"\"</span>\n</code></pre>\n\
          <p>So the idea is to get both english and croatian response at once from\
          \ the very single prompt and request.</p>\n"
        raw: '@lewtun thanks for your checking!

          I tried using your template but got exactly the same result as before.

          But as inference engine I use vLLM, which uses FastChat for templates and,
          apparently, the template for Zephyr is incorrect and the problem with incorrect
          generation lies in it.

          Btw, template for Mistral is also contains some garbage in it, that''s why
          I used Mistralai''s string template instead of array.


          When I used the Mistral template and the same approach as you (added the
          generated answer in English to the context), then with this context Zephyr
          was able to generate the correct translation into Croatian.


          I found the right template to try with string prompt:

          ```python

          system_prompt = "You are a friendly assistant who knows all the books in
          the world. Respond to user queries clearly, strictly following the context
          of the question, and provide an answer within the requested number of words"

          user_message = "summarize the book Master and Margarita in just 1000 words"

          user_message2 = "then translate it to Croatian"

          messages = f"""<|system|>{system_prompt}</s>

          <|user|>{user_message} {user_message2}</s>

          <|assistant|>"""

          ```


          An seems Zephyr can''t translate without adding context in english, but
          Mistral can do it.


          Zephyr can provide with just croatian translation when I modify prompt like
          this:

          ```python

          messages = f"""<|system|>{system_prompt}</s>

          <|user|>{user_message}</s>

          <|assistant|></s>

          <|user|>{user_message2}</s>

          <|assistant|>"""

          ```


          So the idea is to get both english and croatian response at once from the
          very single prompt and request.'
        updatedAt: '2023-10-19T12:40:06.866Z'
      numEdits: 2
      reactions: []
    id: 6531195c86b57032d7542332
    type: comment
  author: arogov
  content: '@lewtun thanks for your checking!

    I tried using your template but got exactly the same result as before.

    But as inference engine I use vLLM, which uses FastChat for templates and, apparently,
    the template for Zephyr is incorrect and the problem with incorrect generation
    lies in it.

    Btw, template for Mistral is also contains some garbage in it, that''s why I used
    Mistralai''s string template instead of array.


    When I used the Mistral template and the same approach as you (added the generated
    answer in English to the context), then with this context Zephyr was able to generate
    the correct translation into Croatian.


    I found the right template to try with string prompt:

    ```python

    system_prompt = "You are a friendly assistant who knows all the books in the world.
    Respond to user queries clearly, strictly following the context of the question,
    and provide an answer within the requested number of words"

    user_message = "summarize the book Master and Margarita in just 1000 words"

    user_message2 = "then translate it to Croatian"

    messages = f"""<|system|>{system_prompt}</s>

    <|user|>{user_message} {user_message2}</s>

    <|assistant|>"""

    ```


    An seems Zephyr can''t translate without adding context in english, but Mistral
    can do it.


    Zephyr can provide with just croatian translation when I modify prompt like this:

    ```python

    messages = f"""<|system|>{system_prompt}</s>

    <|user|>{user_message}</s>

    <|assistant|></s>

    <|user|>{user_message2}</s>

    <|assistant|>"""

    ```


    So the idea is to get both english and croatian response at once from the very
    single prompt and request.'
  created_at: 2023-10-19 10:56:12+00:00
  edited: true
  hidden: false
  id: 6531195c86b57032d7542332
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: HuggingFaceH4/zephyr-7b-alpha
repo_type: model
status: open
target_branch: null
title: Translation capability broken
