!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vivkhandelwal
conflicting_files: null
created_at: 2023-12-10 10:57:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/624d29e8d42648a64925916063554e26.svg
      fullname: Vivek Khandelwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vivkhandelwal
      type: user
    createdAt: '2023-12-10T10:57:16.000Z'
    data:
      edited: false
      editors:
      - vivkhandelwal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.41484522819519043
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/624d29e8d42648a64925916063554e26.svg
          fullname: Vivek Khandelwal
          isHf: false
          isPro: false
          name: vivkhandelwal
          type: user
        html: '<p>I am trying to train with my dataset but I am getting below error.
          Somone please help.<br>"---------------------------------------------------------------------------<br>TypeError                                 Traceback
          (most recent call last)<br>Input In [10], in &lt;cell line: 9&gt;()<br>      2
          peft_trainer = Trainer(<br>      3     model=peft_model,<br>      4     args=peft_training_args,<br>      5     train_dataset=training_dataset,<br>      6
          )<br>      8 # Start PEFT training<br>----&gt; 9 peft_trainer.train()<br>     11
          # Save the PEFT-trained model<br>     12 peft_model.save_pretrained("C:/D
          drive/vivek data/Mtech/Semester4/FbtExamples/peft_fine_tuned_model")</p>

          <p>File ~\Anaconda3\lib\site-packages\transformers\trainer.py:1530, in Trainer.train(self,
          resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)<br>   1528         hf_hub_utils.enable_progress_bars()<br>   1529
          else:<br>-&gt; 1530     return inner_training_loop(<br>   1531         args=args,<br>   1532         resume_from_checkpoint=resume_from_checkpoint,<br>   1533         trial=trial,<br>   1534         ignore_keys_for_eval=ignore_keys_for_eval,<br>   1535     )</p>

          <p>File ~\Anaconda3\lib\site-packages\accelerate\utils\memory.py:136, in
          find_executable_batch_size..decorator(*args, **kwargs)<br>    134     raise
          RuntimeError("No executable batch size found, reached zero.")<br>    135
          try:<br>--&gt; 136     return function(batch_size, *args, **kwargs)<br>    137
          except Exception as e:<br>    138     if should_reduce_batch_size(e):</p>

          <p>File ~\Anaconda3\lib\site-packages\transformers\trainer.py:1844, in Trainer._inner_training_loop(self,
          batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)<br>   1841     self.control
          = self.callback_handler.on_step_begin(args, self.state, self.control)<br>   1843
          with self.accelerator.accumulate(model):<br>-&gt; 1844     tr_loss_step
          = self.training_step(model, inputs)<br>   1846 if (<br>   1847     args.logging_nan_inf_filter<br>   1848     and
          not is_torch_tpu_available()<br>   1849     and (torch.isnan(tr_loss_step)
          or torch.isinf(tr_loss_step))<br>   1850 ):<br>   1851     # if loss is
          nan or inf simply add the average of previous logged losses<br>   1852     tr_loss
          += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)</p>

          <p>File ~\Anaconda3\lib\site-packages\transformers\trainer.py:2701, in Trainer.training_step(self,
          model, inputs)<br>   2698     return loss_mb.reduce_mean().detach().to(self.args.device)<br>   2700
          with self.compute_loss_context_manager():<br>-&gt; 2701     loss = self.compute_loss(model,
          inputs)<br>   2703 if self.args.n_gpu &gt; 1:<br>   2704     loss = loss.mean()  #
          mean() to average on multi-gpu parallel training</p>

          <p>File ~\Anaconda3\lib\site-packages\transformers\trainer.py:2724, in Trainer.compute_loss(self,
          model, inputs, return_outputs)<br>   2722 else:<br>   2723     labels =
          None<br>-&gt; 2724 outputs = model(**inputs)<br>   2725 # Save past state
          if it exists<br>   2726 # TODO: this needs to be fixed and made cleaner
          later.<br>   2727 if self.args.past_index &gt;= 0:</p>

          <p>File ~\Anaconda3\lib\site-packages\torch\nn\modules\module.py:1194, in
          Module._call_impl(self, *input, **kwargs)<br>   1190 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1191 # this function,
          and just call forward.<br>   1192 if not (self._backward_hooks or self._forward_hooks
          or self._forward_pre_hooks or _global_backward_hooks<br>   1193         or
          _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt; 1194     return
          forward_call(*input, **kwargs)<br>   1195 # Do not call functions when jit
          is used<br>   1196 full_backward_hooks, non_full_backward_hooks = [], []</p>

          <p>File ~\Anaconda3\lib\site-packages\peft\peft_model.py:1232, in PeftModelForSeq2SeqLM.forward(self,
          input_ids, attention_mask, inputs_embeds, decoder_input_ids, decoder_attention_mask,
          decoder_inputs_embeds, labels, output_attentions, output_hidden_states,
          return_dict, task_ids, **kwargs)<br>   1230 peft_config = self.active_peft_config<br>   1231
          if not peft_config.is_prompt_learning:<br>-&gt; 1232     return self.base_model(<br>   1233         input_ids=input_ids,<br>   1234         attention_mask=attention_mask,<br>   1235         inputs_embeds=inputs_embeds,<br>   1236         decoder_input_ids=decoder_input_ids,<br>   1237         decoder_attention_mask=decoder_attention_mask,<br>   1238         decoder_inputs_embeds=decoder_inputs_embeds,<br>   1239         labels=labels,<br>   1240         output_attentions=output_attentions,<br>   1241         output_hidden_states=output_hidden_states,<br>   1242         return_dict=return_dict,<br>   1243         **kwargs,<br>   1244     )<br>   1246
          batch_size = _get_batch_size(input_ids, inputs_embeds)<br>   1247 if decoder_attention_mask
          is not None:<br>   1248     # concat prompt attention mask</p>

          <p>File ~\Anaconda3\lib\site-packages\torch\nn\modules\module.py:1194, in
          Module._call_impl(self, *input, **kwargs)<br>   1190 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1191 # this function,
          and just call forward.<br>   1192 if not (self._backward_hooks or self._forward_hooks
          or self._forward_pre_hooks or _global_backward_hooks<br>   1193         or
          _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt; 1194     return
          forward_call(*input, **kwargs)<br>   1195 # Do not call functions when jit
          is used<br>   1196 full_backward_hooks, non_full_backward_hooks = [], []</p>

          <p>File ~\Anaconda3\lib\site-packages\peft\tuners\tuners_utils.py:108, in
          BaseTuner.forward(self, *args, **kwargs)<br>    107 def forward(self, *args:
          Any, **kwargs: Any):<br>--&gt; 108     return self.model.forward(*args,
          **kwargs)</p>

          <p>TypeError: forward() got an unexpected keyword argument ''decoder_input_ids''"</p>

          '
        raw: "I am trying to train with my dataset but I am getting below error. Somone\
          \ please help.\r\n\"---------------------------------------------------------------------------\r\
          \nTypeError                                 Traceback (most recent call\
          \ last)\r\nInput In [10], in <cell line: 9>()\r\n      2 peft_trainer =\
          \ Trainer(\r\n      3     model=peft_model,\r\n      4     args=peft_training_args,\r\
          \n      5     train_dataset=training_dataset,\r\n      6 )\r\n      8 #\
          \ Start PEFT training\r\n----> 9 peft_trainer.train()\r\n     11 # Save\
          \ the PEFT-trained model\r\n     12 peft_model.save_pretrained(\"C:/D drive/vivek\
          \ data/Mtech/Semester4/FbtExamples/peft_fine_tuned_model\")\r\n\r\nFile\
          \ ~\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py:1530, in Trainer.train(self,\
          \ resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\r\n   1528\
          \         hf_hub_utils.enable_progress_bars()\r\n   1529 else:\r\n-> 1530\
          \     return inner_training_loop(\r\n   1531         args=args,\r\n   1532\
          \         resume_from_checkpoint=resume_from_checkpoint,\r\n   1533    \
          \     trial=trial,\r\n   1534         ignore_keys_for_eval=ignore_keys_for_eval,\r\
          \n   1535     )\r\n\r\nFile ~\\Anaconda3\\lib\\site-packages\\accelerate\\\
          utils\\memory.py:136, in find_executable_batch_size.<locals>.decorator(*args,\
          \ **kwargs)\r\n    134     raise RuntimeError(\"No executable batch size\
          \ found, reached zero.\")\r\n    135 try:\r\n--> 136     return function(batch_size,\
          \ *args, **kwargs)\r\n    137 except Exception as e:\r\n    138     if should_reduce_batch_size(e):\r\
          \n\r\nFile ~\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py:1844,\
          \ in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint,\
          \ trial, ignore_keys_for_eval)\r\n   1841     self.control = self.callback_handler.on_step_begin(args,\
          \ self.state, self.control)\r\n   1843 with self.accelerator.accumulate(model):\r\
          \n-> 1844     tr_loss_step = self.training_step(model, inputs)\r\n   1846\
          \ if (\r\n   1847     args.logging_nan_inf_filter\r\n   1848     and not\
          \ is_torch_tpu_available()\r\n   1849     and (torch.isnan(tr_loss_step)\
          \ or torch.isinf(tr_loss_step))\r\n   1850 ):\r\n   1851     # if loss is\
          \ nan or inf simply add the average of previous logged losses\r\n   1852\
          \     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\r\
          \n\r\nFile ~\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py:2701,\
          \ in Trainer.training_step(self, model, inputs)\r\n   2698     return loss_mb.reduce_mean().detach().to(self.args.device)\r\
          \n   2700 with self.compute_loss_context_manager():\r\n-> 2701     loss\
          \ = self.compute_loss(model, inputs)\r\n   2703 if self.args.n_gpu > 1:\r\
          \n   2704     loss = loss.mean()  # mean() to average on multi-gpu parallel\
          \ training\r\n\r\nFile ~\\Anaconda3\\lib\\site-packages\\transformers\\\
          trainer.py:2724, in Trainer.compute_loss(self, model, inputs, return_outputs)\r\
          \n   2722 else:\r\n   2723     labels = None\r\n-> 2724 outputs = model(**inputs)\r\
          \n   2725 # Save past state if it exists\r\n   2726 # TODO: this needs to\
          \ be fixed and made cleaner later.\r\n   2727 if self.args.past_index >=\
          \ 0:\r\n\r\nFile ~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\\
          module.py:1194, in Module._call_impl(self, *input, **kwargs)\r\n   1190\
          \ # If we don't have any hooks, we want to skip the rest of the logic in\r\
          \n   1191 # this function, and just call forward.\r\n   1192 if not (self._backward_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\r\
          \n   1193         or _global_forward_hooks or _global_forward_pre_hooks):\r\
          \n-> 1194     return forward_call(*input, **kwargs)\r\n   1195 # Do not\
          \ call functions when jit is used\r\n   1196 full_backward_hooks, non_full_backward_hooks\
          \ = [], []\r\n\r\nFile ~\\Anaconda3\\lib\\site-packages\\peft\\peft_model.py:1232,\
          \ in PeftModelForSeq2SeqLM.forward(self, input_ids, attention_mask, inputs_embeds,\
          \ decoder_input_ids, decoder_attention_mask, decoder_inputs_embeds, labels,\
          \ output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\r\
          \n   1230 peft_config = self.active_peft_config\r\n   1231 if not peft_config.is_prompt_learning:\r\
          \n-> 1232     return self.base_model(\r\n   1233         input_ids=input_ids,\r\
          \n   1234         attention_mask=attention_mask,\r\n   1235         inputs_embeds=inputs_embeds,\r\
          \n   1236         decoder_input_ids=decoder_input_ids,\r\n   1237      \
          \   decoder_attention_mask=decoder_attention_mask,\r\n   1238         decoder_inputs_embeds=decoder_inputs_embeds,\r\
          \n   1239         labels=labels,\r\n   1240         output_attentions=output_attentions,\r\
          \n   1241         output_hidden_states=output_hidden_states,\r\n   1242\
          \         return_dict=return_dict,\r\n   1243         **kwargs,\r\n   1244\
          \     )\r\n   1246 batch_size = _get_batch_size(input_ids, inputs_embeds)\r\
          \n   1247 if decoder_attention_mask is not None:\r\n   1248     # concat\
          \ prompt attention mask\r\n\r\nFile ~\\Anaconda3\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py:1194, in Module._call_impl(self, *input, **kwargs)\r\
          \n   1190 # If we don't have any hooks, we want to skip the rest of the\
          \ logic in\r\n   1191 # this function, and just call forward.\r\n   1192\
          \ if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks\
          \ or _global_backward_hooks\r\n   1193         or _global_forward_hooks\
          \ or _global_forward_pre_hooks):\r\n-> 1194     return forward_call(*input,\
          \ **kwargs)\r\n   1195 # Do not call functions when jit is used\r\n   1196\
          \ full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\nFile ~\\\
          Anaconda3\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:108, in BaseTuner.forward(self,\
          \ *args, **kwargs)\r\n    107 def forward(self, *args: Any, **kwargs: Any):\r\
          \n--> 108     return self.model.forward(*args, **kwargs)\r\n\r\nTypeError:\
          \ forward() got an unexpected keyword argument 'decoder_input_ids'\""
        updatedAt: '2023-12-10T10:57:16.711Z'
      numEdits: 0
      reactions: []
    id: 6575998cec3bf96e4364f286
    type: comment
  author: vivkhandelwal
  content: "I am trying to train with my dataset but I am getting below error. Somone\
    \ please help.\r\n\"---------------------------------------------------------------------------\r\
    \nTypeError                                 Traceback (most recent call last)\r\
    \nInput In [10], in <cell line: 9>()\r\n      2 peft_trainer = Trainer(\r\n  \
    \    3     model=peft_model,\r\n      4     args=peft_training_args,\r\n     \
    \ 5     train_dataset=training_dataset,\r\n      6 )\r\n      8 # Start PEFT training\r\
    \n----> 9 peft_trainer.train()\r\n     11 # Save the PEFT-trained model\r\n  \
    \   12 peft_model.save_pretrained(\"C:/D drive/vivek data/Mtech/Semester4/FbtExamples/peft_fine_tuned_model\"\
    )\r\n\r\nFile ~\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py:1530,\
    \ in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval,\
    \ **kwargs)\r\n   1528         hf_hub_utils.enable_progress_bars()\r\n   1529\
    \ else:\r\n-> 1530     return inner_training_loop(\r\n   1531         args=args,\r\
    \n   1532         resume_from_checkpoint=resume_from_checkpoint,\r\n   1533  \
    \       trial=trial,\r\n   1534         ignore_keys_for_eval=ignore_keys_for_eval,\r\
    \n   1535     )\r\n\r\nFile ~\\Anaconda3\\lib\\site-packages\\accelerate\\utils\\\
    memory.py:136, in find_executable_batch_size.<locals>.decorator(*args, **kwargs)\r\
    \n    134     raise RuntimeError(\"No executable batch size found, reached zero.\"\
    )\r\n    135 try:\r\n--> 136     return function(batch_size, *args, **kwargs)\r\
    \n    137 except Exception as e:\r\n    138     if should_reduce_batch_size(e):\r\
    \n\r\nFile ~\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py:1844, in\
    \ Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint,\
    \ trial, ignore_keys_for_eval)\r\n   1841     self.control = self.callback_handler.on_step_begin(args,\
    \ self.state, self.control)\r\n   1843 with self.accelerator.accumulate(model):\r\
    \n-> 1844     tr_loss_step = self.training_step(model, inputs)\r\n   1846 if (\r\
    \n   1847     args.logging_nan_inf_filter\r\n   1848     and not is_torch_tpu_available()\r\
    \n   1849     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\r\n\
    \   1850 ):\r\n   1851     # if loss is nan or inf simply add the average of previous\
    \ logged losses\r\n   1852     tr_loss += tr_loss / (1 + self.state.global_step\
    \ - self._globalstep_last_logged)\r\n\r\nFile ~\\Anaconda3\\lib\\site-packages\\\
    transformers\\trainer.py:2701, in Trainer.training_step(self, model, inputs)\r\
    \n   2698     return loss_mb.reduce_mean().detach().to(self.args.device)\r\n \
    \  2700 with self.compute_loss_context_manager():\r\n-> 2701     loss = self.compute_loss(model,\
    \ inputs)\r\n   2703 if self.args.n_gpu > 1:\r\n   2704     loss = loss.mean()\
    \  # mean() to average on multi-gpu parallel training\r\n\r\nFile ~\\Anaconda3\\\
    lib\\site-packages\\transformers\\trainer.py:2724, in Trainer.compute_loss(self,\
    \ model, inputs, return_outputs)\r\n   2722 else:\r\n   2723     labels = None\r\
    \n-> 2724 outputs = model(**inputs)\r\n   2725 # Save past state if it exists\r\
    \n   2726 # TODO: this needs to be fixed and made cleaner later.\r\n   2727 if\
    \ self.args.past_index >= 0:\r\n\r\nFile ~\\Anaconda3\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py:1194, in Module._call_impl(self, *input, **kwargs)\r\n\
    \   1190 # If we don't have any hooks, we want to skip the rest of the logic in\r\
    \n   1191 # this function, and just call forward.\r\n   1192 if not (self._backward_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\r\
    \n   1193         or _global_forward_hooks or _global_forward_pre_hooks):\r\n\
    -> 1194     return forward_call(*input, **kwargs)\r\n   1195 # Do not call functions\
    \ when jit is used\r\n   1196 full_backward_hooks, non_full_backward_hooks = [],\
    \ []\r\n\r\nFile ~\\Anaconda3\\lib\\site-packages\\peft\\peft_model.py:1232, in\
    \ PeftModelForSeq2SeqLM.forward(self, input_ids, attention_mask, inputs_embeds,\
    \ decoder_input_ids, decoder_attention_mask, decoder_inputs_embeds, labels, output_attentions,\
    \ output_hidden_states, return_dict, task_ids, **kwargs)\r\n   1230 peft_config\
    \ = self.active_peft_config\r\n   1231 if not peft_config.is_prompt_learning:\r\
    \n-> 1232     return self.base_model(\r\n   1233         input_ids=input_ids,\r\
    \n   1234         attention_mask=attention_mask,\r\n   1235         inputs_embeds=inputs_embeds,\r\
    \n   1236         decoder_input_ids=decoder_input_ids,\r\n   1237         decoder_attention_mask=decoder_attention_mask,\r\
    \n   1238         decoder_inputs_embeds=decoder_inputs_embeds,\r\n   1239    \
    \     labels=labels,\r\n   1240         output_attentions=output_attentions,\r\
    \n   1241         output_hidden_states=output_hidden_states,\r\n   1242      \
    \   return_dict=return_dict,\r\n   1243         **kwargs,\r\n   1244     )\r\n\
    \   1246 batch_size = _get_batch_size(input_ids, inputs_embeds)\r\n   1247 if\
    \ decoder_attention_mask is not None:\r\n   1248     # concat prompt attention\
    \ mask\r\n\r\nFile ~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194,\
    \ in Module._call_impl(self, *input, **kwargs)\r\n   1190 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\r\n   1191 # this function,\
    \ and just call forward.\r\n   1192 if not (self._backward_hooks or self._forward_hooks\
    \ or self._forward_pre_hooks or _global_backward_hooks\r\n   1193         or _global_forward_hooks\
    \ or _global_forward_pre_hooks):\r\n-> 1194     return forward_call(*input, **kwargs)\r\
    \n   1195 # Do not call functions when jit is used\r\n   1196 full_backward_hooks,\
    \ non_full_backward_hooks = [], []\r\n\r\nFile ~\\Anaconda3\\lib\\site-packages\\\
    peft\\tuners\\tuners_utils.py:108, in BaseTuner.forward(self, *args, **kwargs)\r\
    \n    107 def forward(self, *args: Any, **kwargs: Any):\r\n--> 108     return\
    \ self.model.forward(*args, **kwargs)\r\n\r\nTypeError: forward() got an unexpected\
    \ keyword argument 'decoder_input_ids'\""
  created_at: 2023-12-10 10:57:16+00:00
  edited: false
  hidden: false
  id: 6575998cec3bf96e4364f286
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-10T11:11:36.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9367279410362244
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;vivkhandelwal&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/vivkhandelwal\"\
          >@<span class=\"underline\">vivkhandelwal</span></a></span>\n\n\t</span></span>\
          \ thanks for the issue, can you share the full script you are using?</p>\n"
        raw: Hi @vivkhandelwal thanks for the issue, can you share the full script
          you are using?
        updatedAt: '2023-12-10T11:11:36.940Z'
      numEdits: 0
      reactions: []
    id: 65759ce8a90ae2daaee4e8c4
    type: comment
  author: ybelkada
  content: Hi @vivkhandelwal thanks for the issue, can you share the full script you
    are using?
  created_at: 2023-12-10 11:11:36+00:00
  edited: false
  hidden: false
  id: 65759ce8a90ae2daaee4e8c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/624d29e8d42648a64925916063554e26.svg
      fullname: Vivek Khandelwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vivkhandelwal
      type: user
    createdAt: '2023-12-11T11:37:41.000Z'
    data:
      edited: false
      editors:
      - vivkhandelwal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.41089141368865967
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/624d29e8d42648a64925916063554e26.svg
          fullname: Vivek Khandelwal
          isHf: false
          isPro: false
          name: vivkhandelwal
          type: user
        html: "<p>Hi , Here is script. Please keep in mind , i am very new to to LLM\
          \ and can do some mistake. Your help is highly appreciated.<br>import pandas\
          \ as pd<br>import os<br>from torch.utils.data import Dataset, DataLoader<br>from\
          \ transformers import AutoTokenizer, AutoModelForCausalLM, AdamW, Trainer,\
          \ TrainingArguments<br>from peft import LoraConfig, get_peft_model, TaskType</p>\n\
          <p>def print_number_of_trainable_model_parameters(model):<br>    trainable_model_params\
          \ = 0<br>    all_model_params = 0<br>    for _, param in model.named_parameters():<br>\
          \        all_model_params += param.numel()<br>        if param.requires_grad:<br>\
          \            trainable_model_params += param.numel()<br>    return f\"Trainable\
          \ model parameters: {trainable_model_params}\\nAll model parameters: {all_model_params}\\\
          nPercentage of trainable model parameters: {100 * trainable_model_params\
          \ / all_model_params:.2f}%\"<br>tokenizer = AutoTokenizer.from_pretrained(\"\
          HuggingFaceH4/zephyr-7b-alpha\")<br>original_model = AutoModelForCausalLM.from_pretrained(\"\
          HuggingFaceH4/zephyr-7b-alpha\",device_map=\"auto\",load_in_8bit=True)</p>\n\
          <h1 id=\"print-the-number-of-trainable-parameters-in-the-original-model\"\
          >Print the number of trainable parameters in the original model</h1>\n<p>print(print_number_of_trainable_model_parameters(original_model))</p>\n\
          <h1 id=\"load-prompts-and-file-names-from-excel-file\">Load prompts and\
          \ file names from Excel file</h1>\n<p>excel_file_path = \"/content/FbtExamples/Fbtprompt.xlsx\"\
          \  # Change this to the appropriate file path<br>df = pd.read_excel(excel_file_path)</p>\n\
          <h1 id=\"specify-the-folder-where-xml-files-are-located\">Specify the folder\
          \ where XML files are located</h1>\n<p>xml_folder = \"/content/FbtExamples\"\
          \  # Change this to the appropriate folder path<br>df.head()<br>class CustomDataset(Dataset):<br>\
          \    def <strong>init</strong>(self, dataframe, tokenizer, xml_folder, max_length=512):<br>\
          \        self.data = dataframe<br>        self.tokenizer = tokenizer<br>\
          \        self.xml_folder = xml_folder<br>        self.max_length = max_length</p>\n\
          <pre><code>def __len__(self):\n    return len(self.data)\n\ndef __getitem__(self,\
          \ idx):\n    prompt = self.data['prompt'].iloc[idx]\n    xml_filename =\
          \ self.data['file_name'].iloc[idx]\n    xml_filepath = os.path.join(self.xml_folder,\
          \ xml_filename)\n\n    with open(xml_filepath, 'r') as file:\n        xml_code\
          \ = file.read()\n\n    inputs = self.tokenizer(prompt, return_tensors=\"\
          pt\", max_length=2048, truncation=True,padding=\"max_length\")\n    labels\
          \ = self.tokenizer(xml_code, return_tensors=\"pt\", max_length=2048, truncation=True,padding=\"\
          max_length\")\n    # Remove 'decoder_input_ids' from inputs\n   # inputs.pop(\"\
          decoder_input_ids\", None)\n\n    # Remove 'input_ids' from labels (PEFT\
          \ model may not need it)\n    # labels.pop(\"input_ids\", None)\n    #return\
          \ {\"input_ids\": inputs[\"input_ids\"].squeeze(), \"labels\": labels[\"\
          input_ids\"].squeeze()}\n    #return {\"input_ids\": inputs[\"input_ids\"\
          ], \"labels\": labels[\"input_ids\"]}\n    return {\n        \"input_ids\"\
          : inputs[\"input_ids\"].squeeze(),  # Squeeze the dimension for input_ids\n\
          \        \"attention_mask\": inputs[\"attention_mask\"].squeeze(),  # If\
          \ attention_mask is used\n        \"labels\": labels[\"input_ids\"].squeeze()\
          \  # Squeeze the dimension for labels\n    }\n\n# Configure PEFT\n</code></pre>\n\
          <p>lora_config = LoraConfig(<br>    r=16,  # Rank<br>    lora_alpha=16,<br>\
          \    target_modules=[\"q_proj\", \"k_proj\"],<br>    lora_dropout=0.05,<br>\
          \    bias=\"none\",<br>    task_type=TaskType.CAUSAL_LM</p>\n<h1 id=\"wrap-the-model-with-peft\"\
          >Wrap the model with PEFT</h1>\n<p>peft_model = get_peft_model(original_model,\
          \ lora_config)</p>\n<h1 id=\"print-the-number-of-trainable-parameters\"\
          >Print the number of trainable parameters</h1>\n<p>print(print_number_of_trainable_model_parameters(peft_model))</p>\n\
          <h1 id=\"wrap-the-model-with-peft-1\">Wrap the model with PEFT</h1>\n<p>peft_model\
          \ = get_peft_model(original_model, lora_config)</p>\n<h1 id=\"print-the-number-of-trainable-parameters-in-the-peft-model\"\
          >Print the number of trainable parameters in the PEFT model</h1>\n<p>print(print_number_of_trainable_model_parameters(peft_model))</p>\n\
          <h1 id=\"training-dataset\">Training dataset</h1>\n<p>training_dataset =\
          \ CustomDataset(df, tokenizer, xml_folder)</p>\n<h1 id=\"define-the-output-directory-for-peft-training\"\
          >Define the output directory for PEFT training</h1>\n<p>peft_output_dir\
          \ = \"/content/peft_output\"  # Change this to the appropriate folder path</p>\n\
          <h1 id=\"define-peft-training-arguments\">Define PEFT training arguments</h1>\n\
          <p>peft_training_args = TrainingArguments(<br>    output_dir=peft_output_dir,<br>\
          \    auto_find_batch_size=True,<br>    learning_rate=1e-3,  # Higher learning\
          \ rate than full fine-tuning.<br>    num_train_epochs=1,<br>    logging_steps=1,<br>\
          \    max_steps=1,<br>    per_device_train_batch_size=4,  # Reduce batch\
          \ size to 8<br>)</p>\n<h1 id=\"create-the-trainer-for-peft\">Create the\
          \ Trainer for PEFT</h1>\n<p>peft_trainer = Trainer(<br>    model=peft_model,<br>\
          \    args=peft_training_args,<br>    train_dataset=training_dataset,<br>)</p>\n\
          <h1 id=\"start-peft-training\">Start PEFT training</h1>\n<p>peft_trainer.train()</p>\n\
          <h1 id=\"save-the-peft-trained-model\">Save the PEFT-trained model</h1>\n\
          <p>peft_model.save_pretrained(\"/content/peft_fine_tuned_model\")  # Change\
          \ this to the appropriate folder path</p>\n"
        raw: "Hi , Here is script. Please keep in mind , i am very new to to LLM and\
          \ can do some mistake. Your help is highly appreciated.\nimport pandas as\
          \ pd\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom\
          \ transformers import AutoTokenizer, AutoModelForCausalLM, AdamW, Trainer,\
          \ TrainingArguments\nfrom peft import LoraConfig, get_peft_model, TaskType\n\
          \ndef print_number_of_trainable_model_parameters(model):\n    trainable_model_params\
          \ = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n\
          \        all_model_params += param.numel()\n        if param.requires_grad:\n\
          \            trainable_model_params += param.numel()\n    return f\"Trainable\
          \ model parameters: {trainable_model_params}\\nAll model parameters: {all_model_params}\\\
          nPercentage of trainable model parameters: {100 * trainable_model_params\
          \ / all_model_params:.2f}%\"\ntokenizer = AutoTokenizer.from_pretrained(\"\
          HuggingFaceH4/zephyr-7b-alpha\")\noriginal_model = AutoModelForCausalLM.from_pretrained(\"\
          HuggingFaceH4/zephyr-7b-alpha\",device_map=\"auto\",load_in_8bit=True)\n\
          # Print the number of trainable parameters in the original model\nprint(print_number_of_trainable_model_parameters(original_model))\n\
          \n# Load prompts and file names from Excel file\nexcel_file_path = \"/content/FbtExamples/Fbtprompt.xlsx\"\
          \  # Change this to the appropriate file path\ndf = pd.read_excel(excel_file_path)\n\
          # Specify the folder where XML files are located\nxml_folder = \"/content/FbtExamples\"\
          \  # Change this to the appropriate folder path\ndf.head()\nclass CustomDataset(Dataset):\n\
          \    def __init__(self, dataframe, tokenizer, xml_folder, max_length=512):\n\
          \        self.data = dataframe\n        self.tokenizer = tokenizer\n   \
          \     self.xml_folder = xml_folder\n        self.max_length = max_length\n\
          \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self,\
          \ idx):\n        prompt = self.data['prompt'].iloc[idx]\n        xml_filename\
          \ = self.data['file_name'].iloc[idx]\n        xml_filepath = os.path.join(self.xml_folder,\
          \ xml_filename)\n\n        with open(xml_filepath, 'r') as file:\n     \
          \       xml_code = file.read()\n\n        inputs = self.tokenizer(prompt,\
          \ return_tensors=\"pt\", max_length=2048, truncation=True,padding=\"max_length\"\
          )\n        labels = self.tokenizer(xml_code, return_tensors=\"pt\", max_length=2048,\
          \ truncation=True,padding=\"max_length\")\n        # Remove 'decoder_input_ids'\
          \ from inputs\n       # inputs.pop(\"decoder_input_ids\", None)\n\n    \
          \    # Remove 'input_ids' from labels (PEFT model may not need it)\n   \
          \     # labels.pop(\"input_ids\", None)\n        #return {\"input_ids\"\
          : inputs[\"input_ids\"].squeeze(), \"labels\": labels[\"input_ids\"].squeeze()}\n\
          \        #return {\"input_ids\": inputs[\"input_ids\"], \"labels\": labels[\"\
          input_ids\"]}\n        return {\n            \"input_ids\": inputs[\"input_ids\"\
          ].squeeze(),  # Squeeze the dimension for input_ids\n            \"attention_mask\"\
          : inputs[\"attention_mask\"].squeeze(),  # If attention_mask is used\n \
          \           \"labels\": labels[\"input_ids\"].squeeze()  # Squeeze the dimension\
          \ for labels\n        }\n    \n    # Configure PEFT\nlora_config = LoraConfig(\n\
          \    r=16,  # Rank\n    lora_alpha=16,\n    target_modules=[\"q_proj\",\
          \ \"k_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n\
          \    \n    \n# Wrap the model with PEFT\npeft_model = get_peft_model(original_model,\
          \ lora_config)\n\n# Print the number of trainable parameters\nprint(print_number_of_trainable_model_parameters(peft_model))\n\
          # Wrap the model with PEFT\npeft_model = get_peft_model(original_model,\
          \ lora_config)\n\n# Print the number of trainable parameters in the PEFT\
          \ model\nprint(print_number_of_trainable_model_parameters(peft_model))\n\
          \n# Training dataset\ntraining_dataset = CustomDataset(df, tokenizer, xml_folder)\n\
          \n# Define the output directory for PEFT training\npeft_output_dir = \"\
          /content/peft_output\"  # Change this to the appropriate folder path\n\n\
          # Define PEFT training arguments\npeft_training_args = TrainingArguments(\n\
          \    output_dir=peft_output_dir,\n    auto_find_batch_size=True,\n    learning_rate=1e-3,\
          \  # Higher learning rate than full fine-tuning.\n    num_train_epochs=1,\n\
          \    logging_steps=1,\n    max_steps=1,\n    per_device_train_batch_size=4,\
          \  # Reduce batch size to 8\n)\n\n# Create the Trainer for PEFT\npeft_trainer\
          \ = Trainer(\n    model=peft_model,\n    args=peft_training_args,\n    train_dataset=training_dataset,\n\
          )\n\n# Start PEFT training\npeft_trainer.train()\n\n# Save the PEFT-trained\
          \ model\npeft_model.save_pretrained(\"/content/peft_fine_tuned_model\")\
          \  # Change this to the appropriate folder path\n"
        updatedAt: '2023-12-11T11:37:41.139Z'
      numEdits: 0
      reactions: []
    id: 6576f485c793e5b72b079863
    type: comment
  author: vivkhandelwal
  content: "Hi , Here is script. Please keep in mind , i am very new to to LLM and\
    \ can do some mistake. Your help is highly appreciated.\nimport pandas as pd\n\
    import os\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM, AdamW, Trainer, TrainingArguments\n\
    from peft import LoraConfig, get_peft_model, TaskType\n\ndef print_number_of_trainable_model_parameters(model):\n\
    \    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in\
    \ model.named_parameters():\n        all_model_params += param.numel()\n     \
    \   if param.requires_grad:\n            trainable_model_params += param.numel()\n\
    \    return f\"Trainable model parameters: {trainable_model_params}\\nAll model\
    \ parameters: {all_model_params}\\nPercentage of trainable model parameters: {100\
    \ * trainable_model_params / all_model_params:.2f}%\"\ntokenizer = AutoTokenizer.from_pretrained(\"\
    HuggingFaceH4/zephyr-7b-alpha\")\noriginal_model = AutoModelForCausalLM.from_pretrained(\"\
    HuggingFaceH4/zephyr-7b-alpha\",device_map=\"auto\",load_in_8bit=True)\n# Print\
    \ the number of trainable parameters in the original model\nprint(print_number_of_trainable_model_parameters(original_model))\n\
    \n# Load prompts and file names from Excel file\nexcel_file_path = \"/content/FbtExamples/Fbtprompt.xlsx\"\
    \  # Change this to the appropriate file path\ndf = pd.read_excel(excel_file_path)\n\
    # Specify the folder where XML files are located\nxml_folder = \"/content/FbtExamples\"\
    \  # Change this to the appropriate folder path\ndf.head()\nclass CustomDataset(Dataset):\n\
    \    def __init__(self, dataframe, tokenizer, xml_folder, max_length=512):\n \
    \       self.data = dataframe\n        self.tokenizer = tokenizer\n        self.xml_folder\
    \ = xml_folder\n        self.max_length = max_length\n\n    def __len__(self):\n\
    \        return len(self.data)\n\n    def __getitem__(self, idx):\n        prompt\
    \ = self.data['prompt'].iloc[idx]\n        xml_filename = self.data['file_name'].iloc[idx]\n\
    \        xml_filepath = os.path.join(self.xml_folder, xml_filename)\n\n      \
    \  with open(xml_filepath, 'r') as file:\n            xml_code = file.read()\n\
    \n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=2048,\
    \ truncation=True,padding=\"max_length\")\n        labels = self.tokenizer(xml_code,\
    \ return_tensors=\"pt\", max_length=2048, truncation=True,padding=\"max_length\"\
    )\n        # Remove 'decoder_input_ids' from inputs\n       # inputs.pop(\"decoder_input_ids\"\
    , None)\n\n        # Remove 'input_ids' from labels (PEFT model may not need it)\n\
    \        # labels.pop(\"input_ids\", None)\n        #return {\"input_ids\": inputs[\"\
    input_ids\"].squeeze(), \"labels\": labels[\"input_ids\"].squeeze()}\n       \
    \ #return {\"input_ids\": inputs[\"input_ids\"], \"labels\": labels[\"input_ids\"\
    ]}\n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\
    \  # Squeeze the dimension for input_ids\n            \"attention_mask\": inputs[\"\
    attention_mask\"].squeeze(),  # If attention_mask is used\n            \"labels\"\
    : labels[\"input_ids\"].squeeze()  # Squeeze the dimension for labels\n      \
    \  }\n    \n    # Configure PEFT\nlora_config = LoraConfig(\n    r=16,  # Rank\n\
    \    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\"],\n    lora_dropout=0.05,\n\
    \    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n    \n    \n# Wrap the\
    \ model with PEFT\npeft_model = get_peft_model(original_model, lora_config)\n\n\
    # Print the number of trainable parameters\nprint(print_number_of_trainable_model_parameters(peft_model))\n\
    # Wrap the model with PEFT\npeft_model = get_peft_model(original_model, lora_config)\n\
    \n# Print the number of trainable parameters in the PEFT model\nprint(print_number_of_trainable_model_parameters(peft_model))\n\
    \n# Training dataset\ntraining_dataset = CustomDataset(df, tokenizer, xml_folder)\n\
    \n# Define the output directory for PEFT training\npeft_output_dir = \"/content/peft_output\"\
    \  # Change this to the appropriate folder path\n\n# Define PEFT training arguments\n\
    peft_training_args = TrainingArguments(\n    output_dir=peft_output_dir,\n   \
    \ auto_find_batch_size=True,\n    learning_rate=1e-3,  # Higher learning rate\
    \ than full fine-tuning.\n    num_train_epochs=1,\n    logging_steps=1,\n    max_steps=1,\n\
    \    per_device_train_batch_size=4,  # Reduce batch size to 8\n)\n\n# Create the\
    \ Trainer for PEFT\npeft_trainer = Trainer(\n    model=peft_model,\n    args=peft_training_args,\n\
    \    train_dataset=training_dataset,\n)\n\n# Start PEFT training\npeft_trainer.train()\n\
    \n# Save the PEFT-trained model\npeft_model.save_pretrained(\"/content/peft_fine_tuned_model\"\
    )  # Change this to the appropriate folder path\n"
  created_at: 2023-12-11 11:37:41+00:00
  edited: false
  hidden: false
  id: 6576f485c793e5b72b079863
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 35
repo_id: HuggingFaceH4/zephyr-7b-alpha
repo_type: model
status: open
target_branch: null
title: How to apply PEFT in this model
