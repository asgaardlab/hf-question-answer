!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GeneZC
conflicting_files: null
created_at: 2023-11-24 09:23:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ebc0d783e75e1faf867e1a300e03226.svg
      fullname: Chen Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GeneZC
      type: user
    createdAt: '2023-11-24T09:23:21.000Z'
    data:
      edited: false
      editors:
      - GeneZC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9195610880851746
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ebc0d783e75e1faf867e1a300e03226.svg
          fullname: Chen Zhang
          isHf: false
          isPro: false
          name: GeneZC
          type: user
        html: '<p>Why both the chosen rewards and rejected rewards are negative, though
          the reward margins are positive.<br>The negative chosen rewards essentially
          indicate that the optimized model does not assign higher probabilities for
          chosen examples than the reference one, which is not that reasonable.</p>

          <p>A potential explanation is that the optimized model will emphasize on
          some tokens instead of all tokens while the reference model equally emphasize
          all tokens. The same may apply with the rejected examples. Therefore, the
          reward margins are still positive.</p>

          '
        raw: "Why both the chosen rewards and rejected rewards are negative, though\
          \ the reward margins are positive.\r\nThe negative chosen rewards essentially\
          \ indicate that the optimized model does not assign higher probabilities\
          \ for chosen examples than the reference one, which is not that reasonable.\r\
          \n\r\nA potential explanation is that the optimized model will emphasize\
          \ on some tokens instead of all tokens while the reference model equally\
          \ emphasize all tokens. The same may apply with the rejected examples. Therefore,\
          \ the reward margins are still positive."
        updatedAt: '2023-11-24T09:23:21.965Z'
      numEdits: 0
      reactions: []
    id: 65606b8901913a2276404c7e
    type: comment
  author: GeneZC
  content: "Why both the chosen rewards and rejected rewards are negative, though\
    \ the reward margins are positive.\r\nThe negative chosen rewards essentially\
    \ indicate that the optimized model does not assign higher probabilities for chosen\
    \ examples than the reference one, which is not that reasonable.\r\n\r\nA potential\
    \ explanation is that the optimized model will emphasize on some tokens instead\
    \ of all tokens while the reference model equally emphasize all tokens. The same\
    \ may apply with the rejected examples. Therefore, the reward margins are still\
    \ positive."
  created_at: 2023-11-24 09:23:21+00:00
  edited: false
  hidden: false
  id: 65606b8901913a2276404c7e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 34
repo_id: HuggingFaceH4/zephyr-7b-alpha
repo_type: model
status: open
target_branch: null
title: Why the chosen rewards are negative?
