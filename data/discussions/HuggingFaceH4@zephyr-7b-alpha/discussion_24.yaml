!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Neulich
conflicting_files: null
created_at: 2023-10-19 20:37:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f030286ff0afdae54635b5e0c573bd4e.svg
      fullname: Manfred Girmes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Neulich
      type: user
    createdAt: '2023-10-19T21:37:40.000Z'
    data:
      edited: false
      editors:
      - Neulich
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9458324909210205
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f030286ff0afdae54635b5e0c573bd4e.svg
          fullname: Manfred Girmes
          isHf: false
          isPro: false
          name: Neulich
          type: user
        html: '<p>Hi there anyone,<br>I would like to know if there is a way to train
          zephyr-7b on a local machine with more or less simple data.<br>For example
          if there is a website with a store (2k - 10k Links (inc. sublinks) I would
          like to soak the knowledge up and put it inside this AI to make it to an
          expert for this Store - is there any way to archive this?</p>

          <p>At the moment it feels VERY difficult to do that.<br>Can someone pls
          gimme a hug to get a blink of the knowledge I need to get closer to a solution
          here?<br>Would be a bless &lt;3</p>

          <p>Regards<br>Manni</p>

          '
        raw: "Hi there anyone,\r\nI would like to know if there is a way to train\
          \ zephyr-7b on a local machine with more or less simple data.\r\nFor example\
          \ if there is a website with a store (2k - 10k Links (inc. sublinks) I would\
          \ like to soak the knowledge up and put it inside this AI to make it to\
          \ an expert for this Store - is there any way to archive this?\r\n\r\nAt\
          \ the moment it feels VERY difficult to do that.\r\nCan someone pls gimme\
          \ a hug to get a blink of the knowledge I need to get closer to a solution\
          \ here?\r\nWould be a bless <3\r\n\r\nRegards\r\nManni"
        updatedAt: '2023-10-19T21:37:40.657Z'
      numEdits: 0
      reactions: []
    id: 6531a1a4005c969984d16657
    type: comment
  author: Neulich
  content: "Hi there anyone,\r\nI would like to know if there is a way to train zephyr-7b\
    \ on a local machine with more or less simple data.\r\nFor example if there is\
    \ a website with a store (2k - 10k Links (inc. sublinks) I would like to soak\
    \ the knowledge up and put it inside this AI to make it to an expert for this\
    \ Store - is there any way to archive this?\r\n\r\nAt the moment it feels VERY\
    \ difficult to do that.\r\nCan someone pls gimme a hug to get a blink of the knowledge\
    \ I need to get closer to a solution here?\r\nWould be a bless <3\r\n\r\nRegards\r\
    \nManni"
  created_at: 2023-10-19 20:37:40+00:00
  edited: false
  hidden: false
  id: 6531a1a4005c969984d16657
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9f7937ed2dfff1799329147d28391044.svg
      fullname: Jan Leyva
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JanLilan
      type: user
    createdAt: '2023-10-26T15:11:45.000Z'
    data:
      edited: false
      editors:
      - JanLilan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7797614336013794
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9f7937ed2dfff1799329147d28391044.svg
          fullname: Jan Leyva
          isHf: false
          isPro: false
          name: JanLilan
          type: user
        html: '<p>Check it out this <a rel="nofollow" href="https://learn.deeplearning.ai/finetuning-large-language-models/lesson/1/introduction">course</a>.</p>

          <p>Regarding train it in a local machine you''ll need a good GPU with a
          minimum of VRAM (just charge the model for inference requires almost 14
          GB of VRAM in  bfloat16, althugh you can downgrade the precision and train
          with less memory. This huggingface <a href="https://huggingface.co/spaces/Vokturz/can-it-run-llm">space</a>
          is really nice to check the hardware requirements). Perhaps it is a good
          idea train in google <a rel="nofollow" href="https://colab.research.google.com/?hl=es">colab</a>
          or <a rel="nofollow" href="https://www.kaggle.com/docs/notebooks">kaggle
          notebook</a>. I hope it helps!</p>

          '
        raw: 'Check it out this [course](https://learn.deeplearning.ai/finetuning-large-language-models/lesson/1/introduction).


          Regarding train it in a local machine you''ll need a good GPU with a minimum
          of VRAM (just charge the model for inference requires almost 14 GB of VRAM
          in  bfloat16, althugh you can downgrade the precision and train with less
          memory. This huggingface [space](https://huggingface.co/spaces/Vokturz/can-it-run-llm)
          is really nice to check the hardware requirements). Perhaps it is a good
          idea train in google [colab](https://colab.research.google.com/?hl=es) or
          [kaggle notebook](https://www.kaggle.com/docs/notebooks). I hope it helps!'
        updatedAt: '2023-10-26T15:11:45.605Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Sigmally
    id: 653a81b10b5b891a003431df
    type: comment
  author: JanLilan
  content: 'Check it out this [course](https://learn.deeplearning.ai/finetuning-large-language-models/lesson/1/introduction).


    Regarding train it in a local machine you''ll need a good GPU with a minimum of
    VRAM (just charge the model for inference requires almost 14 GB of VRAM in  bfloat16,
    althugh you can downgrade the precision and train with less memory. This huggingface
    [space](https://huggingface.co/spaces/Vokturz/can-it-run-llm) is really nice to
    check the hardware requirements). Perhaps it is a good idea train in google [colab](https://colab.research.google.com/?hl=es)
    or [kaggle notebook](https://www.kaggle.com/docs/notebooks). I hope it helps!'
  created_at: 2023-10-26 14:11:45+00:00
  edited: false
  hidden: false
  id: 653a81b10b5b891a003431df
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: HuggingFaceH4/zephyr-7b-alpha
repo_type: model
status: open
target_branch: null
title: Train it with custom data
