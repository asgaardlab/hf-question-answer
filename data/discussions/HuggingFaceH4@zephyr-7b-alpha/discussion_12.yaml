!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KrishnaKaasyap
conflicting_files: null
created_at: 2023-10-15 09:24:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/236c771e6c5a25ef6ed5e1bc061e30b8.svg
      fullname: Krishna Kaasyap
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KrishnaKaasyap
      type: user
    createdAt: '2023-10-15T10:24:52.000Z'
    data:
      edited: false
      editors:
      - KrishnaKaasyap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9328526854515076
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/236c771e6c5a25ef6ed5e1bc061e30b8.svg
          fullname: Krishna Kaasyap
          isHf: false
          isPro: false
          name: KrishnaKaasyap
          type: user
        html: "<p>I love this model. This is probably the best 7B model out there\
          \ (as on October 2023). Thanks to Mistral team &amp; \U0001F917 team. Infact\
          \ Zephyr-7b-alpha is a bit better than Mistral Instruct and the UI in the\
          \ spaces that hosted Zephyr is awesome. And thanks a lot for open sourcing\
          \ the UI.</p>\n<p>But I cannot understand one thing! Almost every cloud\
          \ provider, from huge to big to small offer models on $/GPU/Hour basis!</p>\n\
          <p>As far as I know - only Together.ai offers OPEN models via API at $/1000\
          \ token pricing format. Not only that - they have standard rates per 1000\
          \ tokens based on models sizes (billions of parameters). Any model that\
          \ has 7 billion parameters costs 0.0002$/1k tokens on their cloud. You don't\
          \ have to pay for hosting, you don't have to painstakingly select GPU CPU\
          \ instances, you don't have to consider your demand spikes, and so on. You\
          \ only pay for what you use - whenever you use it and whichever model you\
          \ use! That is a sheer brilliant pricing strategy.</p>\n<p>But...... even\
          \ they don't have fine-tuning (which includes IMHO - both Domain Adaptation\
          \ and instruction fine-tuning) on $/1000 token basis. </p>\n<p>Open AI (The\
          \ most closed company in the world) offers not just inference but also even\
          \ fine tuning on $/1000 token basis!</p>\n<p><a rel=\"nofollow\" href=\"\
          https://openai.com/pricing\">https://openai.com/pricing</a></p>\n<p>If there\
          \ is one thing that \U0001F917 can do to improve their stake in the cloud\
          \ business and AI future in general - it is to offer dozens of awesome open\
          \ models (including and starting with Zephyr-7b-alpha) that it already possess\
          \ via API on $/1000 token pricing format and also please please offer fine-tuning\
          \ on $/1000 token pricing format.</p>\n<p>You could charge a bit premium\
          \ for fine-tuning if you offer services on $/1000 token pricing format because\
          \ all the user has to do is have the dataset in the system - prompt - output\
          \ format! Open AI charges a lot of premium for fine-tuning and inference\
          \ of even the basic Babbage model (which is estimated to have 3B parameters\
          \ - not confirmed tho!)</p>\n<p>I know it is a huge challenge to offer multiple\
          \ models for fine-tuning in $/1000 token pricing format - but you can offer\
          \ atleast one model, right? </p>\n<p>Like Together AI you can offer many\
          \ models via API and atleast offer Mistral (base and instruct) &amp;<br>Zephyr-7b-alpha\
          \ (chat) for fine-tuning in $/1000 token pricing format.</p>\n<p>The two\
          \ defining features and huge advantages of open models are - customisation\
          \ + ease of use and security. Since the code and sometimes the datasets\
          \ are in the open - the security aspect of open source is perfectly achieved.\
          \ The only pending aspect is customisation + ease of use.</p>\n<p>Trust\
          \ me - there are millions of people who don't have the technical expertise,\
          \ but the need to use Gen AI for their businesses - and the current methodologies\
          \ are in no way creating ease of use for those people without technical\
          \ expertise!</p>\n<p>If the closed source models are moving very fastly\
          \ in supporting fine tuning - why can't the open community with much larger\
          \ support base and much bigger wiggle room can do the same?</p>\n<p>More\
          \ power to the open source community. Let the wisdom of commons win.</p>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64aed48cfec303c461d06242/0CKx8FW3W5YoULpivAXKz.jpeg\"\
          ><img alt=\"IMG_20231015_153139.jpg\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64aed48cfec303c461d06242/0CKx8FW3W5YoULpivAXKz.jpeg\"\
          ></a></p>\n"
        raw: "I love this model. This is probably the best 7B model out there (as\
          \ on October 2023). Thanks to Mistral team & \U0001F917 team. Infact Zephyr-7b-alpha\
          \ is a bit better than Mistral Instruct and the UI in the spaces that hosted\
          \ Zephyr is awesome. And thanks a lot for open sourcing the UI.\r\n\r\n\
          But I cannot understand one thing! Almost every cloud provider, from huge\
          \ to big to small offer models on $/GPU/Hour basis!\r\n\r\nAs far as I know\
          \ - only Together.ai offers OPEN models via API at $/1000 token pricing\
          \ format. Not only that - they have standard rates per 1000 tokens based\
          \ on models sizes (billions of parameters). Any model that has 7 billion\
          \ parameters costs 0.0002$/1k tokens on their cloud. You don't have to pay\
          \ for hosting, you don't have to painstakingly select GPU CPU instances,\
          \ you don't have to consider your demand spikes, and so on. You only pay\
          \ for what you use - whenever you use it and whichever model you use! That\
          \ is a sheer brilliant pricing strategy.\r\n\r\nBut...... even they don't\
          \ have fine-tuning (which includes IMHO - both Domain Adaptation and instruction\
          \ fine-tuning) on $/1000 token basis. \r\n\r\nOpen AI (The most closed company\
          \ in the world) offers not just inference but also even fine tuning on $/1000\
          \ token basis!\r\n\r\nhttps://openai.com/pricing\r\n\r\nIf there is one\
          \ thing that \U0001F917 can do to improve their stake in the cloud business\
          \ and AI future in general - it is to offer dozens of awesome open models\
          \ (including and starting with Zephyr-7b-alpha) that it already possess\
          \ via API on $/1000 token pricing format and also please please offer fine-tuning\
          \ on $/1000 token pricing format.\r\n\r\nYou could charge a bit premium\
          \ for fine-tuning if you offer services on $/1000 token pricing format because\
          \ all the user has to do is have the dataset in the system - prompt - output\
          \ format! Open AI charges a lot of premium for fine-tuning and inference\
          \ of even the basic Babbage model (which is estimated to have 3B parameters\
          \ - not confirmed tho!)\r\n\r\nI know it is a huge challenge to offer multiple\
          \ models for fine-tuning in $/1000 token pricing format - but you can offer\
          \ atleast one model, right? \r\n\r\nLike Together AI you can offer many\
          \ models via API and atleast offer Mistral (base and instruct) &\r\nZephyr-7b-alpha\
          \ (chat) for fine-tuning in $/1000 token pricing format.\r\n\r\nThe two\
          \ defining features and huge advantages of open models are - customisation\
          \ + ease of use and security. Since the code and sometimes the datasets\
          \ are in the open - the security aspect of open source is perfectly achieved.\
          \ The only pending aspect is customisation + ease of use.\r\n\r\nTrust me\
          \ - there are millions of people who don't have the technical expertise,\
          \ but the need to use Gen AI for their businesses - and the current methodologies\
          \ are in no way creating ease of use for those people without technical\
          \ expertise!\r\n\r\nIf the closed source models are moving very fastly in\
          \ supporting fine tuning - why can't the open community with much larger\
          \ support base and much bigger wiggle room can do the same?\r\n\r\nMore\
          \ power to the open source community. Let the wisdom of commons win.\r\n\
          \r\n![IMG_20231015_153139.jpg](https://cdn-uploads.huggingface.co/production/uploads/64aed48cfec303c461d06242/0CKx8FW3W5YoULpivAXKz.jpeg)\r\
          \n"
        updatedAt: '2023-10-15T10:24:52.023Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - lewtun
    id: 652bbdf4caddc9e6080e4dbe
    type: comment
  author: KrishnaKaasyap
  content: "I love this model. This is probably the best 7B model out there (as on\
    \ October 2023). Thanks to Mistral team & \U0001F917 team. Infact Zephyr-7b-alpha\
    \ is a bit better than Mistral Instruct and the UI in the spaces that hosted Zephyr\
    \ is awesome. And thanks a lot for open sourcing the UI.\r\n\r\nBut I cannot understand\
    \ one thing! Almost every cloud provider, from huge to big to small offer models\
    \ on $/GPU/Hour basis!\r\n\r\nAs far as I know - only Together.ai offers OPEN\
    \ models via API at $/1000 token pricing format. Not only that - they have standard\
    \ rates per 1000 tokens based on models sizes (billions of parameters). Any model\
    \ that has 7 billion parameters costs 0.0002$/1k tokens on their cloud. You don't\
    \ have to pay for hosting, you don't have to painstakingly select GPU CPU instances,\
    \ you don't have to consider your demand spikes, and so on. You only pay for what\
    \ you use - whenever you use it and whichever model you use! That is a sheer brilliant\
    \ pricing strategy.\r\n\r\nBut...... even they don't have fine-tuning (which includes\
    \ IMHO - both Domain Adaptation and instruction fine-tuning) on $/1000 token basis.\
    \ \r\n\r\nOpen AI (The most closed company in the world) offers not just inference\
    \ but also even fine tuning on $/1000 token basis!\r\n\r\nhttps://openai.com/pricing\r\
    \n\r\nIf there is one thing that \U0001F917 can do to improve their stake in the\
    \ cloud business and AI future in general - it is to offer dozens of awesome open\
    \ models (including and starting with Zephyr-7b-alpha) that it already possess\
    \ via API on $/1000 token pricing format and also please please offer fine-tuning\
    \ on $/1000 token pricing format.\r\n\r\nYou could charge a bit premium for fine-tuning\
    \ if you offer services on $/1000 token pricing format because all the user has\
    \ to do is have the dataset in the system - prompt - output format! Open AI charges\
    \ a lot of premium for fine-tuning and inference of even the basic Babbage model\
    \ (which is estimated to have 3B parameters - not confirmed tho!)\r\n\r\nI know\
    \ it is a huge challenge to offer multiple models for fine-tuning in $/1000 token\
    \ pricing format - but you can offer atleast one model, right? \r\n\r\nLike Together\
    \ AI you can offer many models via API and atleast offer Mistral (base and instruct)\
    \ &\r\nZephyr-7b-alpha (chat) for fine-tuning in $/1000 token pricing format.\r\
    \n\r\nThe two defining features and huge advantages of open models are - customisation\
    \ + ease of use and security. Since the code and sometimes the datasets are in\
    \ the open - the security aspect of open source is perfectly achieved. The only\
    \ pending aspect is customisation + ease of use.\r\n\r\nTrust me - there are millions\
    \ of people who don't have the technical expertise, but the need to use Gen AI\
    \ for their businesses - and the current methodologies are in no way creating\
    \ ease of use for those people without technical expertise!\r\n\r\nIf the closed\
    \ source models are moving very fastly in supporting fine tuning - why can't the\
    \ open community with much larger support base and much bigger wiggle room can\
    \ do the same?\r\n\r\nMore power to the open source community. Let the wisdom\
    \ of commons win.\r\n\r\n![IMG_20231015_153139.jpg](https://cdn-uploads.huggingface.co/production/uploads/64aed48cfec303c461d06242/0CKx8FW3W5YoULpivAXKz.jpeg)\r\
    \n"
  created_at: 2023-10-15 09:24:52+00:00
  edited: false
  hidden: false
  id: 652bbdf4caddc9e6080e4dbe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/236c771e6c5a25ef6ed5e1bc061e30b8.svg
      fullname: Krishna Kaasyap
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KrishnaKaasyap
      type: user
    createdAt: '2023-10-15T10:49:28.000Z'
    data:
      edited: false
      editors:
      - KrishnaKaasyap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8907263278961182
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/236c771e6c5a25ef6ed5e1bc061e30b8.svg
          fullname: Krishna Kaasyap
          isHf: false
          isPro: false
          name: KrishnaKaasyap
          type: user
        html: "<p>I would love to see Mistral AI providing fine-tuning (both Domain\
          \ Adaptation and instruction fine-tuning) on $/1000 token pricing format!\
          \ Since you created this legendary 7B model - you'll know how to effectively\
          \ and efficiently host, fine-tune and infer the model in the bestest way\
          \ possible. I bet CoreWeave folks would be interested in this!</p>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;glample&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/glample\">@<span class=\"underline\"\
          >glample</span></a></span>\n\n\t</span></span><br><span data-props=\"{&quot;user&quot;:&quot;arthurmensch&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/arthurmensch\"\
          >@<span class=\"underline\">arthurmensch</span></a></span>\n\n\t</span></span><br><span\
          \ data-props=\"{&quot;user&quot;:&quot;devendrachaplot&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/devendrachaplot\">@<span\
          \ class=\"underline\">devendrachaplot</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: "I would love to see Mistral AI providing fine-tuning (both Domain Adaptation\
          \ and instruction fine-tuning) on $/1000 token pricing format! Since you\
          \ created this legendary 7B model - you'll know how to effectively and efficiently\
          \ host, fine-tune and infer the model in the bestest way possible. I bet\
          \ CoreWeave folks would be interested in this!\n\n@glample \n@arthurmensch\
          \ \n@devendrachaplot "
        updatedAt: '2023-10-15T10:49:28.642Z'
      numEdits: 0
      reactions: []
    id: 652bc3b83a416e1f21bd3daf
    type: comment
  author: KrishnaKaasyap
  content: "I would love to see Mistral AI providing fine-tuning (both Domain Adaptation\
    \ and instruction fine-tuning) on $/1000 token pricing format! Since you created\
    \ this legendary 7B model - you'll know how to effectively and efficiently host,\
    \ fine-tune and infer the model in the bestest way possible. I bet CoreWeave folks\
    \ would be interested in this!\n\n@glample \n@arthurmensch \n@devendrachaplot "
  created_at: 2023-10-15 09:49:28+00:00
  edited: false
  hidden: false
  id: 652bc3b83a416e1f21bd3daf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/236c771e6c5a25ef6ed5e1bc061e30b8.svg
      fullname: Krishna Kaasyap
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KrishnaKaasyap
      type: user
    createdAt: '2023-10-15T10:58:55.000Z'
    data:
      edited: false
      editors:
      - KrishnaKaasyap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8818573355674744
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/236c771e6c5a25ef6ed5e1bc061e30b8.svg
          fullname: Krishna Kaasyap
          isHf: false
          isPro: false
          name: KrishnaKaasyap
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;vipul&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/vipul\">@<span class=\"\
          underline\">vipul</span></a></span>\n\n\t</span></span><br><span data-props=\"\
          {&quot;user&quot;:&quot;orangetin&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/orangetin\">@<span class=\"underline\">orangetin</span></a></span>\n\
          \n\t</span></span><br><span data-props=\"{&quot;user&quot;:&quot;mauriceweber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mauriceweber\"\
          >@<span class=\"underline\">mauriceweber</span></a></span>\n\n\t</span></span></p>\n\
          <p>Tagging y'all just to show my appreciation for what you did to open source\
          \ community. </p>\n<p>And also to truly understand why none of the cloud\
          \ providers are providing  Fine-tuning ( both Domain Adaptation and instruction\
          \ fine-tuning) on $/1000 token basis.</p>\n<p>BTW, I adore the 32k LLAMA\
          \ 7B. You guys did it way before Long LLAMA project from Meta AI!</p>\n"
        raw: "@vipul \n@orangetin \n@mauriceweber\n\nTagging y'all just to show my\
          \ appreciation for what you did to open source community. \n\nAnd also to\
          \ truly understand why none of the cloud providers are providing  Fine-tuning\
          \ ( both Domain Adaptation and instruction fine-tuning) on $/1000 token\
          \ basis.\n\nBTW, I adore the 32k LLAMA 7B. You guys did it way before Long\
          \ LLAMA project from Meta AI!"
        updatedAt: '2023-10-15T10:58:55.770Z'
      numEdits: 0
      reactions: []
    id: 652bc5efa21958f7db6aba40
    type: comment
  author: KrishnaKaasyap
  content: "@vipul \n@orangetin \n@mauriceweber\n\nTagging y'all just to show my appreciation\
    \ for what you did to open source community. \n\nAnd also to truly understand\
    \ why none of the cloud providers are providing  Fine-tuning ( both Domain Adaptation\
    \ and instruction fine-tuning) on $/1000 token basis.\n\nBTW, I adore the 32k\
    \ LLAMA 7B. You guys did it way before Long LLAMA project from Meta AI!"
  created_at: 2023-10-15 09:58:55+00:00
  edited: false
  hidden: false
  id: 652bc5efa21958f7db6aba40
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
      fullname: Lewis Tunstall
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lewtun
      type: user
    createdAt: '2023-10-18T12:49:14.000Z'
    data:
      edited: false
      editors:
      - lewtun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8885326981544495
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
          fullname: Lewis Tunstall
          isHf: true
          isPro: false
          name: lewtun
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;KrishnaKaasyap&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/KrishnaKaasyap\"\
          >@<span class=\"underline\">KrishnaKaasyap</span></a></span>\n\n\t</span></span>\
          \ thanks for the super detailed and useful feedback! Regarding fine-tuning,\
          \ have you tried AutoTrain (<a href=\"https://huggingface.co/autotrain\"\
          >https://huggingface.co/autotrain</a>)? It doesn't offer a $/1000 token\
          \ pricing format because it supports many modalities beyond text, but it\
          \ certainly does support LLM fine-tuning :)</p>\n<p>cc <span data-props=\"\
          {&quot;user&quot;:&quot;abhishek&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/abhishek\">@<span class=\"underline\">abhishek</span></a></span>\n\
          \n\t</span></span> for vis</p>\n"
        raw: 'Hi @KrishnaKaasyap thanks for the super detailed and useful feedback!
          Regarding fine-tuning, have you tried AutoTrain (https://huggingface.co/autotrain)?
          It doesn''t offer a $/1000 token pricing format because it supports many
          modalities beyond text, but it certainly does support LLM fine-tuning :)


          cc @abhishek for vis'
        updatedAt: '2023-10-18T12:49:14.172Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - KrishnaKaasyap
    id: 652fd44ab2acab0b82fb2fad
    type: comment
  author: lewtun
  content: 'Hi @KrishnaKaasyap thanks for the super detailed and useful feedback!
    Regarding fine-tuning, have you tried AutoTrain (https://huggingface.co/autotrain)?
    It doesn''t offer a $/1000 token pricing format because it supports many modalities
    beyond text, but it certainly does support LLM fine-tuning :)


    cc @abhishek for vis'
  created_at: 2023-10-18 11:49:14+00:00
  edited: false
  hidden: false
  id: 652fd44ab2acab0b82fb2fad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/236c771e6c5a25ef6ed5e1bc061e30b8.svg
      fullname: Krishna Kaasyap
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KrishnaKaasyap
      type: user
    createdAt: '2023-10-19T06:00:15.000Z'
    data:
      edited: false
      editors:
      - KrishnaKaasyap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8590148091316223
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/236c771e6c5a25ef6ed5e1bc061e30b8.svg
          fullname: Krishna Kaasyap
          isHf: false
          isPro: false
          name: KrishnaKaasyap
          type: user
        html: "<blockquote>\n<p>have you tried AutoTrain (<a href=\"https://huggingface.co/autotrain\"\
          >https://huggingface.co/autotrain</a>)? It doesn't offer a $/1000 token\
          \ pricing format because it supports many modalities beyond text, but it\
          \ certainly does support LLM fine-tuning :)</p>\n</blockquote>\n<p>Yes,\
          \ but that is not letting me select my model of choice (in my case, either\
          \ Llama 7B or Mistral 7B) via \"manual\" selection. And even after selecting\
          \ \"automatic\" option - I am unable to proceed to the next stage and an\
          \ \"internal server error\" message is appearing!</p>\n<p>And thanks for\
          \ the response and suggestions dude, much appreciated. \U0001F64F\U0001F3FC\
          </p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64aed48cfec303c461d06242/QEZTAx4SYmLYXgj5MgbwC.jpeg\"\
          ><img alt=\"Screenshot_20231019-112728_Chrome.jpg\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64aed48cfec303c461d06242/QEZTAx4SYmLYXgj5MgbwC.jpeg\"\
          ></a></p>\n"
        raw: "> have you tried AutoTrain (https://huggingface.co/autotrain)? It doesn't\
          \ offer a $/1000 token pricing format because it supports many modalities\
          \ beyond text, but it certainly does support LLM fine-tuning :)\n> \n\n\
          Yes, but that is not letting me select my model of choice (in my case, either\
          \ Llama 7B or Mistral 7B) via \"manual\" selection. And even after selecting\
          \ \"automatic\" option - I am unable to proceed to the next stage and an\
          \ \"internal server error\" message is appearing!\n\nAnd thanks for the\
          \ response and suggestions dude, much appreciated. \U0001F64F\U0001F3FC\n\
          \n![Screenshot_20231019-112728_Chrome.jpg](https://cdn-uploads.huggingface.co/production/uploads/64aed48cfec303c461d06242/QEZTAx4SYmLYXgj5MgbwC.jpeg)\n\
          \n"
        updatedAt: '2023-10-19T06:00:15.105Z'
      numEdits: 0
      reactions: []
    id: 6530c5efbce21215c39d9d42
    type: comment
  author: KrishnaKaasyap
  content: "> have you tried AutoTrain (https://huggingface.co/autotrain)? It doesn't\
    \ offer a $/1000 token pricing format because it supports many modalities beyond\
    \ text, but it certainly does support LLM fine-tuning :)\n> \n\nYes, but that\
    \ is not letting me select my model of choice (in my case, either Llama 7B or\
    \ Mistral 7B) via \"manual\" selection. And even after selecting \"automatic\"\
    \ option - I am unable to proceed to the next stage and an \"internal server error\"\
    \ message is appearing!\n\nAnd thanks for the response and suggestions dude, much\
    \ appreciated. \U0001F64F\U0001F3FC\n\n![Screenshot_20231019-112728_Chrome.jpg](https://cdn-uploads.huggingface.co/production/uploads/64aed48cfec303c461d06242/QEZTAx4SYmLYXgj5MgbwC.jpeg)\n\
    \n"
  created_at: 2023-10-19 05:00:15+00:00
  edited: false
  hidden: false
  id: 6530c5efbce21215c39d9d42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1604427578240-noauth.png?w=200&h=200&f=face
      fullname: Abhishek Thakur
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhishek
      type: user
    createdAt: '2023-10-19T06:18:36.000Z'
    data:
      edited: true
      editors:
      - abhishek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7237915396690369
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1604427578240-noauth.png?w=200&h=200&f=face
          fullname: Abhishek Thakur
          isHf: true
          isPro: false
          name: abhishek
          type: user
        html: '<p>For LLM finetuning, you should use AutoTrain Advanced. The old UI
          is deprecated.<br>docs: <a rel="nofollow" href="https://hf.co/docs/autotrain">https://hf.co/docs/autotrain</a></p>

          '
        raw: "For LLM finetuning, you should use AutoTrain Advanced. The old UI is\
          \ deprecated. \ndocs: https://hf.co/docs/autotrain"
        updatedAt: '2023-10-19T06:18:52.876Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - KrishnaKaasyap
    id: 6530ca3c70cebad89be573a8
    type: comment
  author: abhishek
  content: "For LLM finetuning, you should use AutoTrain Advanced. The old UI is deprecated.\
    \ \ndocs: https://hf.co/docs/autotrain"
  created_at: 2023-10-19 05:18:36+00:00
  edited: true
  hidden: false
  id: 6530ca3c70cebad89be573a8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: HuggingFaceH4/zephyr-7b-alpha
repo_type: model
status: open
target_branch: null
title: APIs & Fine-tuning ( both Domain Adaptation and instruction fine-tuning) on
  $/1000 token basis
