!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ibestvina
conflicting_files: null
created_at: 2023-10-13 10:48:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/daa842b8bb7cf0bb2b86b96ca050cac4.svg
      fullname: Ivan Bestvina
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ibestvina
      type: user
    createdAt: '2023-10-13T11:48:47.000Z'
    data:
      edited: false
      editors:
      - ibestvina
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9789094924926758
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/daa842b8bb7cf0bb2b86b96ca050cac4.svg
          fullname: Ivan Bestvina
          isHf: false
          isPro: false
          name: ibestvina
          type: user
        html: '<p>I might be missing something very obvious here, but it seems that
          the length of the embedding is always equal to <code>bottleneck_scale</code>,
          instead of 1. I guess it doesn''t make any difference, since all vectors
          are scaled equally?</p>

          '
        raw: I might be missing something very obvious here, but it seems that the
          length of the embedding is always equal to `bottleneck_scale`, instead of
          1. I guess it doesn't make any difference, since all vectors are scaled
          equally?
        updatedAt: '2023-10-13T11:48:47.236Z'
      numEdits: 0
      reactions: []
    id: 65292e9fc8103ca0041e797d
    type: comment
  author: ibestvina
  content: I might be missing something very obvious here, but it seems that the length
    of the embedding is always equal to `bottleneck_scale`, instead of 1. I guess
    it doesn't make any difference, since all vectors are scaled equally?
  created_at: 2023-10-13 10:48:47+00:00
  edited: false
  hidden: false
  id: 65292e9fc8103ca0041e797d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660065734419-619993bf90692f0d9238846b.jpeg?w=200&h=200&f=face
      fullname: Linus Lee
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: thesephist
      type: user
    createdAt: '2023-11-09T02:55:26.000Z'
    data:
      edited: false
      editors:
      - thesephist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9684272408485413
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660065734419-619993bf90692f0d9238846b.jpeg?w=200&h=200&f=face
          fullname: Linus Lee
          isHf: false
          isPro: false
          name: thesephist
          type: user
        html: '<p>Ah, good observation! That''s technically true at the model architecture
          level, but for the model weights I looked at, <code>bottleneck_scale</code>
          was 1 or very very close to 1. From that I made an educated guess that this
          particular parameter doesn''t actually receive any nonzero gradients during
          training.</p>

          <p>In practice when I use the model at inference time I always normalize
          vectors to 1 for both the encoder output and decoder input, and haven''t
          had any issues.</p>

          '
        raw: 'Ah, good observation! That''s technically true at the model architecture
          level, but for the model weights I looked at, `bottleneck_scale` was 1 or
          very very close to 1. From that I made an educated guess that this particular
          parameter doesn''t actually receive any nonzero gradients during training.


          In practice when I use the model at inference time I always normalize vectors
          to 1 for both the encoder output and decoder input, and haven''t had any
          issues.'
        updatedAt: '2023-11-09T02:55:26.963Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - ibestvina
        - xddtc48jo
    id: 654c4a1e5255ee86710b61e3
    type: comment
  author: thesephist
  content: 'Ah, good observation! That''s technically true at the model architecture
    level, but for the model weights I looked at, `bottleneck_scale` was 1 or very
    very close to 1. From that I made an educated guess that this particular parameter
    doesn''t actually receive any nonzero gradients during training.


    In practice when I use the model at inference time I always normalize vectors
    to 1 for both the encoder output and decoder input, and haven''t had any issues.'
  created_at: 2023-11-09 02:55:26+00:00
  edited: false
  hidden: false
  id: 654c4a1e5255ee86710b61e3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: thesephist/contra-bottleneck-t5-small-wikipedia
repo_type: model
status: open
target_branch: null
title: Embedding length is not 1, but equal to bottleneck scale?
