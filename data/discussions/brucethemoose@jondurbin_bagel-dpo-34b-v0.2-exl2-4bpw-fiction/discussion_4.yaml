!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Trangle
conflicting_files: null
created_at: 2024-01-19 13:22:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af0820f65b8db3180075bbdcb15624e0.svg
      fullname: Trangle Heshvp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Trangle
      type: user
    createdAt: '2024-01-19T13:22:56.000Z'
    data:
      edited: false
      editors:
      - Trangle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8442254662513733
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af0820f65b8db3180075bbdcb15624e0.svg
          fullname: Trangle Heshvp
          isHf: false
          isPro: false
          name: Trangle
          type: user
        html: '<p>Quantized on 300K tokens of two Vicuna format chats, a sci fi story
          and a fiction story at a long context. This should yield better storywriting
          performance than the default exl2 quantization.</p>

          '
        raw: Quantized on 300K tokens of two Vicuna format chats, a sci fi story and
          a fiction story at a long context. This should yield better storywriting
          performance than the default exl2 quantization.
        updatedAt: '2024-01-19T13:22:56.681Z'
      numEdits: 0
      reactions: []
    id: 65aa77b0a92a64ef5b46d185
    type: comment
  author: Trangle
  content: Quantized on 300K tokens of two Vicuna format chats, a sci fi story and
    a fiction story at a long context. This should yield better storywriting performance
    than the default exl2 quantization.
  created_at: 2024-01-19 13:22:56+00:00
  edited: false
  hidden: false
  id: 65aa77b0a92a64ef5b46d185
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-19T16:50:10.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9680420160293579
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>You posted this at an interesting time. There is a discussion of
          just what it useful calibration data: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/discussions/5006">https://github.com/ggerganov/llama.cpp/discussions/5006</a></p>

          <p>As well as parallel discussions on Reddit and Discord.</p>

          <p>In a nutshell, it appears that my strategy of "quantize on a lot of fiction"
          is possibly useless. Its not really worth documenting what I did because,
          as it turns out, its particularly bad for exllama below 4bpw. I would not
          recommend using this quantization, and instead lonestriker''s generic quantixations
          for now.</p>

          '
        raw: 'You posted this at an interesting time. There is a discussion of just
          what it useful calibration data: https://github.com/ggerganov/llama.cpp/discussions/5006


          As well as parallel discussions on Reddit and Discord.


          In a nutshell, it appears that my strategy of "quantize on a lot of fiction"
          is possibly useless. Its not really worth documenting what I did because,
          as it turns out, its particularly bad for exllama below 4bpw. I would not
          recommend using this quantization, and instead lonestriker''s generic quantixations
          for now.'
        updatedAt: '2024-01-19T16:50:57.365Z'
      numEdits: 1
      reactions: []
    id: 65aaa84257f263e3d04462e1
    type: comment
  author: brucethemoose
  content: 'You posted this at an interesting time. There is a discussion of just
    what it useful calibration data: https://github.com/ggerganov/llama.cpp/discussions/5006


    As well as parallel discussions on Reddit and Discord.


    In a nutshell, it appears that my strategy of "quantize on a lot of fiction" is
    possibly useless. Its not really worth documenting what I did because, as it turns
    out, its particularly bad for exllama below 4bpw. I would not recommend using
    this quantization, and instead lonestriker''s generic quantixations for now.'
  created_at: 2024-01-19 16:50:10+00:00
  edited: true
  hidden: false
  id: 65aaa84257f263e3d04462e1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: brucethemoose/jondurbin_bagel-dpo-34b-v0.2-exl2-4bpw-fiction
repo_type: model
status: open
target_branch: null
title: Is there documentation for quantization alignment in long text?
