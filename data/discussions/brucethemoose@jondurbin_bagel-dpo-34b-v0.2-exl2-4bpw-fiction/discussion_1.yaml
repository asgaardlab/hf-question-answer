!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ParasiticRogue
conflicting_files: null
created_at: 2024-01-05 02:18:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-05T02:18:18.000Z'
    data:
      edited: false
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9528838396072388
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>I''ve found that most of the time models which have a similar template
          do better when merging just from private testing, and your original Cabybara-Tess
          model was one of the better ones, even compared to your newest merges like
          v5. Now that there''s another decent model using that format and context
          I was wondering how a 3-way merge with Tess-1.4, Nous-Capybara, and Nontoxic-Bagel
          might turn out given the synergy, with the nontoxic variant seeming to be
          the best bagel flavor atm given it''s ranking position and some testing
          on my end. I''d do it myself but I''m not sure what the best approach is
          given my inexperience, so I wanted ask if you have any interest before I
          start bashing my head trying to get it right, lol.</p>

          '
        raw: I've found that most of the time models which have a similar template
          do better when merging just from private testing, and your original Cabybara-Tess
          model was one of the better ones, even compared to your newest merges like
          v5. Now that there's another decent model using that format and context
          I was wondering how a 3-way merge with Tess-1.4, Nous-Capybara, and Nontoxic-Bagel
          might turn out given the synergy, with the nontoxic variant seeming to be
          the best bagel flavor atm given it's ranking position and some testing on
          my end. I'd do it myself but I'm not sure what the best approach is given
          my inexperience, so I wanted ask if you have any interest before I start
          bashing my head trying to get it right, lol.
        updatedAt: '2024-01-05T02:18:18.858Z'
      numEdits: 0
      reactions: []
    id: 659766ea5ddc4d0123aac253
    type: comment
  author: ParasiticRogue
  content: I've found that most of the time models which have a similar template do
    better when merging just from private testing, and your original Cabybara-Tess
    model was one of the better ones, even compared to your newest merges like v5.
    Now that there's another decent model using that format and context I was wondering
    how a 3-way merge with Tess-1.4, Nous-Capybara, and Nontoxic-Bagel might turn
    out given the synergy, with the nontoxic variant seeming to be the best bagel
    flavor atm given it's ranking position and some testing on my end. I'd do it myself
    but I'm not sure what the best approach is given my inexperience, so I wanted
    ask if you have any interest before I start bashing my head trying to get it right,
    lol.
  created_at: 2024-01-05 02:18:18+00:00
  edited: false
  hidden: false
  id: 659766ea5ddc4d0123aac253
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-05T03:37:30.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9623919129371643
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Yeah I have considered this, and ran into issues with <code>&lt;im_end&gt;</code>
          appearing in Vicuna prompt in a testing v6 merge. </p>

          <p>One major issue is that DPO-bagel completely falls apart after 8K-16K,
          and the goal of my merges is to stay coherent at high context. The SFT version
          is less severe but still problematic. So it needs other 200K models to "extend"
          its context length if its included.</p>

          <p>Another issue is that Bagel has mixed prompts anyway...</p>

          <p>I may do what you suggested though, and if I do make a bigger merge,
          Capybara, Tess and Pallas will all have higher weights to try and emphasize
          the Vicuna prompt. I will include a little Tess 1.0 as well since some people
          tell me they prefer the original Tess 1.0-Capybara merge.</p>

          '
        raw: "Yeah I have considered this, and ran into issues with `<im_end>` appearing\
          \ in Vicuna prompt in a testing v6 merge. \n\nOne major issue is that DPO-bagel\
          \ completely falls apart after 8K-16K, and the goal of my merges is to stay\
          \ coherent at high context. The SFT version is less severe but still problematic.\
          \ So it needs other 200K models to \"extend\" its context length if its\
          \ included.\n\nAnother issue is that Bagel has mixed prompts anyway...\n\
          \nI may do what you suggested though, and if I do make a bigger merge, Capybara,\
          \ Tess and Pallas will all have higher weights to try and emphasize the\
          \ Vicuna prompt. I will include a little Tess 1.0 as well since some people\
          \ tell me they prefer the original Tess 1.0-Capybara merge.\n"
        updatedAt: '2024-01-05T03:55:50.477Z'
      numEdits: 7
      reactions: []
    id: 6597797aa2659adbf9f54f8c
    type: comment
  author: brucethemoose
  content: "Yeah I have considered this, and ran into issues with `<im_end>` appearing\
    \ in Vicuna prompt in a testing v6 merge. \n\nOne major issue is that DPO-bagel\
    \ completely falls apart after 8K-16K, and the goal of my merges is to stay coherent\
    \ at high context. The SFT version is less severe but still problematic. So it\
    \ needs other 200K models to \"extend\" its context length if its included.\n\n\
    Another issue is that Bagel has mixed prompts anyway...\n\nI may do what you suggested\
    \ though, and if I do make a bigger merge, Capybara, Tess and Pallas will all\
    \ have higher weights to try and emphasize the Vicuna prompt. I will include a\
    \ little Tess 1.0 as well since some people tell me they prefer the original Tess\
    \ 1.0-Capybara merge.\n"
  created_at: 2024-01-05 03:37:30+00:00
  edited: true
  hidden: false
  id: 6597797aa2659adbf9f54f8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-05T03:57:32.000Z'
    data:
      edited: false
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.939184308052063
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>One solution I''ve seen people do to try and normalize prompts when
          doing bigger merges if one of the models has a different prompt is to use
          SLERP with one that has the correct template first, and then using DARE/TIES
          afterwards for the bigger merge. Considering Pallas takes after Tess already,
          and Bagel has Capybara in the data, perhaps something like this would be
          optimal to avoid too much samey overlap?</p>

          <p>Nontoxix-Bagel-V0.2 + Pallas-v4 using SLERP.<br>Pallas-Bagel + Tess-1.4
          + Nous-Capybara combined at 33% each using DARE/TIES.</p>

          <p>This is just speculation on my part given some of the other merges I''ve
          seen at the 7b range, so it''s entirely possible I''m off base.</p>

          '
        raw: 'One solution I''ve seen people do to try and normalize prompts when
          doing bigger merges if one of the models has a different prompt is to use
          SLERP with one that has the correct template first, and then using DARE/TIES
          afterwards for the bigger merge. Considering Pallas takes after Tess already,
          and Bagel has Capybara in the data, perhaps something like this would be
          optimal to avoid too much samey overlap?


          Nontoxix-Bagel-V0.2 + Pallas-v4 using SLERP.

          Pallas-Bagel + Tess-1.4 + Nous-Capybara combined at 33% each using DARE/TIES.


          This is just speculation on my part given some of the other merges I''ve
          seen at the 7b range, so it''s entirely possible I''m off base.'
        updatedAt: '2024-01-05T03:57:32.854Z'
      numEdits: 0
      reactions: []
    id: 65977e2cccd854bca55dee04
    type: comment
  author: ParasiticRogue
  content: 'One solution I''ve seen people do to try and normalize prompts when doing
    bigger merges if one of the models has a different prompt is to use SLERP with
    one that has the correct template first, and then using DARE/TIES afterwards for
    the bigger merge. Considering Pallas takes after Tess already, and Bagel has Capybara
    in the data, perhaps something like this would be optimal to avoid too much samey
    overlap?


    Nontoxix-Bagel-V0.2 + Pallas-v4 using SLERP.

    Pallas-Bagel + Tess-1.4 + Nous-Capybara combined at 33% each using DARE/TIES.


    This is just speculation on my part given some of the other merges I''ve seen
    at the 7b range, so it''s entirely possible I''m off base.'
  created_at: 2024-01-05 03:57:32+00:00
  edited: false
  hidden: false
  id: 65977e2cccd854bca55dee04
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-05T04:31:15.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9233095645904541
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>That''s an interesting idea.</p>

          <p>Capybara + Bagel + SUS-Hermes + Tess 1.0 SLERP. Maybe some of the other
          non Vicuna models in there as well at low weight?</p>

          <p>Then Pallas + Tess 1.4  + very low density bagel +  low weight capybara
          (again) + merge?</p>

          <p>One other factor is that the low context models like bagel should ideally
          be "culled" with a very low DARE density to preserve the 200K context, at
          least according to some unscientific testing of mine. SLERP unfortunately
          doesn''t do this, but task arithmetic can.</p>

          '
        raw: 'That''s an interesting idea.


          Capybara + Bagel + SUS-Hermes + Tess 1.0 SLERP. Maybe some of the other
          non Vicuna models in there as well at low weight?


          Then Pallas + Tess 1.4  + very low density bagel +  low weight capybara
          (again) + merge?



          One other factor is that the low context models like bagel should ideally
          be "culled" with a very low DARE density to preserve the 200K context, at
          least according to some unscientific testing of mine. SLERP unfortunately
          doesn''t do this, but task arithmetic can.






          '
        updatedAt: '2024-01-05T04:55:16.805Z'
      numEdits: 6
      reactions: []
    id: 65978613c7a30c638b234e0e
    type: comment
  author: brucethemoose
  content: 'That''s an interesting idea.


    Capybara + Bagel + SUS-Hermes + Tess 1.0 SLERP. Maybe some of the other non Vicuna
    models in there as well at low weight?


    Then Pallas + Tess 1.4  + very low density bagel +  low weight capybara (again)
    + merge?



    One other factor is that the low context models like bagel should ideally be "culled"
    with a very low DARE density to preserve the 200K context, at least according
    to some unscientific testing of mine. SLERP unfortunately doesn''t do this, but
    task arithmetic can.






    '
  created_at: 2024-01-05 04:31:15+00:00
  edited: true
  hidden: false
  id: 65978613c7a30c638b234e0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-05T05:27:39.000Z'
    data:
      edited: false
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9581750631332397
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>Possibly, given what I''ve heard and talked to with others before.
          But for the first merge to optimize prompting I wouldn''t cast too wide
          of a net with different prompts simply because it might dilute it.</p>

          <p>SUS-Hermes is probably fine since the original SUS-Chat model is some
          weird hybrid of Alpaca and Vicuna, so it would probably understand the Vicuna
          prompt to an extant? But idk if I''d add anything else like AEZAKMI or PlatYi
          to the mix personally. Plus most of AEZAKMI''s data is sourced from Spicy/Airboros
          which is already contained in Capybara and Bagel to an extant.</p>

          <p>Other then that I''d agree with the mix and slerp you proposed. Maybe
          swap the Tess versions around, with the Tess-1.0 at a slightly smaller mix
          compared to Pallas, so the final mix has a better diversity considering
          Pallas uses 1.4. So something at around this for the final mix below would
          be my best guess.</p>

          <p>Tess-1.0 at 20%<br>Pallas (Either version 3 or 4 just from the rankings)
          at 30%<br>Capybara at 30%<br>Bagel-SLERP at 20%</p>

          <p>Course those are just my estimates. Raise or lower depending on what
          you find best, but it''s probably best to have the original Capybara have
          a slightly higher digit once it''s all said and done just because it''s
          still seems to be the best atm. BTW I''ve seen you on the regular and DPO
          variants of Bagel, but what do you think of the third Nontoxic version and
          have you tried it?</p>

          <p><a href="https://huggingface.co/jondurbin/nontoxic-bagel-34b-v0.2">https://huggingface.co/jondurbin/nontoxic-bagel-34b-v0.2</a></p>

          '
        raw: 'Possibly, given what I''ve heard and talked to with others before. But
          for the first merge to optimize prompting I wouldn''t cast too wide of a
          net with different prompts simply because it might dilute it.


          SUS-Hermes is probably fine since the original SUS-Chat model is some weird
          hybrid of Alpaca and Vicuna, so it would probably understand the Vicuna
          prompt to an extant? But idk if I''d add anything else like AEZAKMI or PlatYi
          to the mix personally. Plus most of AEZAKMI''s data is sourced from Spicy/Airboros
          which is already contained in Capybara and Bagel to an extant.


          Other then that I''d agree with the mix and slerp you proposed. Maybe swap
          the Tess versions around, with the Tess-1.0 at a slightly smaller mix compared
          to Pallas, so the final mix has a better diversity considering Pallas uses
          1.4. So something at around this for the final mix below would be my best
          guess.


          Tess-1.0 at 20%

          Pallas (Either version 3 or 4 just from the rankings) at 30%

          Capybara at 30%

          Bagel-SLERP at 20%


          Course those are just my estimates. Raise or lower depending on what you
          find best, but it''s probably best to have the original Capybara have a
          slightly higher digit once it''s all said and done just because it''s still
          seems to be the best atm. BTW I''ve seen you on the regular and DPO variants
          of Bagel, but what do you think of the third Nontoxic version and have you
          tried it?


          https://huggingface.co/jondurbin/nontoxic-bagel-34b-v0.2'
        updatedAt: '2024-01-05T05:27:39.007Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - brucethemoose
    id: 6597934b2235d4056bb29dc8
    type: comment
  author: ParasiticRogue
  content: 'Possibly, given what I''ve heard and talked to with others before. But
    for the first merge to optimize prompting I wouldn''t cast too wide of a net with
    different prompts simply because it might dilute it.


    SUS-Hermes is probably fine since the original SUS-Chat model is some weird hybrid
    of Alpaca and Vicuna, so it would probably understand the Vicuna prompt to an
    extant? But idk if I''d add anything else like AEZAKMI or PlatYi to the mix personally.
    Plus most of AEZAKMI''s data is sourced from Spicy/Airboros which is already contained
    in Capybara and Bagel to an extant.


    Other then that I''d agree with the mix and slerp you proposed. Maybe swap the
    Tess versions around, with the Tess-1.0 at a slightly smaller mix compared to
    Pallas, so the final mix has a better diversity considering Pallas uses 1.4. So
    something at around this for the final mix below would be my best guess.


    Tess-1.0 at 20%

    Pallas (Either version 3 or 4 just from the rankings) at 30%

    Capybara at 30%

    Bagel-SLERP at 20%


    Course those are just my estimates. Raise or lower depending on what you find
    best, but it''s probably best to have the original Capybara have a slightly higher
    digit once it''s all said and done just because it''s still seems to be the best
    atm. BTW I''ve seen you on the regular and DPO variants of Bagel, but what do
    you think of the third Nontoxic version and have you tried it?


    https://huggingface.co/jondurbin/nontoxic-bagel-34b-v0.2'
  created_at: 2024-01-05 05:27:39+00:00
  edited: false
  hidden: false
  id: 6597934b2235d4056bb29dc8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-05T06:56:46.000Z'
    data:
      edited: false
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7666565179824829
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>As an aside you could also consider making 2 separate SLERPS using
          one of the Tess variants and/or Capybara to possibly strengthen both Bagel
          and  SUS-Hermes'' individual strengths for the final merge. As an example:</p>

          <p>Suss-Hermes + Capybara<br>Bagel + Tess-v1<br>SLEEP-Bagel + SLERP-Hermes
          + Pallas-v4 + Capybara</p>

          '
        raw: 'As an aside you could also consider making 2 separate SLERPS using one
          of the Tess variants and/or Capybara to possibly strengthen both Bagel and  SUS-Hermes''
          individual strengths for the final merge. As an example:


          Suss-Hermes + Capybara

          Bagel + Tess-v1

          SLEEP-Bagel + SLERP-Hermes + Pallas-v4 + Capybara'
        updatedAt: '2024-01-05T06:56:46.716Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - brucethemoose
    id: 6597a82e5fc65052a9e968d4
    type: comment
  author: ParasiticRogue
  content: 'As an aside you could also consider making 2 separate SLERPS using one
    of the Tess variants and/or Capybara to possibly strengthen both Bagel and  SUS-Hermes''
    individual strengths for the final merge. As an example:


    Suss-Hermes + Capybara

    Bagel + Tess-v1

    SLEEP-Bagel + SLERP-Hermes + Pallas-v4 + Capybara'
  created_at: 2024-01-05 06:56:46+00:00
  edited: false
  hidden: false
  id: 6597a82e5fc65052a9e968d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-05T07:00:21.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9587008953094482
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>I haven''t yet. TBH I haven''t really touched DPO anymore once I
          found out the context length was so short, but if nontoxic bagel is a better
          finetune that could be interesting...</p>

          <p>Also, what do you think about gradients? EG a non Vicuna model could
          have a gradient like:</p>

          <p>weight: [0, 0.2, 0.2, 0.2, 0.2, 0.2, 0]</p>

          <p>So that it would it would have no influence at the input and output.
          Does that even make any sense? Would it help suppress the prompt syntax?</p>

          '
        raw: 'I haven''t yet. TBH I haven''t really touched DPO anymore once I found
          out the context length was so short, but if nontoxic bagel is a better finetune
          that could be interesting...


          Also, what do you think about gradients? EG a non Vicuna model could have
          a gradient like:


          weight: [0, 0.2, 0.2, 0.2, 0.2, 0.2, 0]


          So that it would it would have no influence at the input and output. Does
          that even make any sense? Would it help suppress the prompt syntax?'
        updatedAt: '2024-01-05T07:00:21.420Z'
      numEdits: 0
      reactions: []
    id: 6597a90592afb150dd0d83ff
    type: comment
  author: brucethemoose
  content: 'I haven''t yet. TBH I haven''t really touched DPO anymore once I found
    out the context length was so short, but if nontoxic bagel is a better finetune
    that could be interesting...


    Also, what do you think about gradients? EG a non Vicuna model could have a gradient
    like:


    weight: [0, 0.2, 0.2, 0.2, 0.2, 0.2, 0]


    So that it would it would have no influence at the input and output. Does that
    even make any sense? Would it help suppress the prompt syntax?'
  created_at: 2024-01-05 07:00:21+00:00
  edited: false
  hidden: false
  id: 6597a90592afb150dd0d83ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-05T07:19:05.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9000091552734375
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Also, another idea bouncing around my head is using a YI 4K as a
          base in a ties model to "extend" the 4K models like SUS. For instance: </p>

          <p>Yi 4K base -&gt; Capybara high density + SUS low density</p>

          <p>Yi 200K is technically a finetune of Yi 4k I believe, so this would more
          directly merge the 200K training + prompt syntax into other models.</p>

          '
        raw: "Also, another idea bouncing around my head is using a YI 4K as a base\
          \ in a ties model to \"extend\" the 4K models like SUS. For instance: \n\
          \nYi 4K base -> Capybara high density + SUS low density\n\nYi 200K is technically\
          \ a finetune of Yi 4k I believe, so this would more directly merge the 200K\
          \ training + prompt syntax into other models."
        updatedAt: '2024-01-05T07:19:05.877Z'
      numEdits: 0
      reactions: []
    id: 6597ad69a2659adbf900d56e
    type: comment
  author: brucethemoose
  content: "Also, another idea bouncing around my head is using a YI 4K as a base\
    \ in a ties model to \"extend\" the 4K models like SUS. For instance: \n\nYi 4K\
    \ base -> Capybara high density + SUS low density\n\nYi 200K is technically a\
    \ finetune of Yi 4k I believe, so this would more directly merge the 200K training\
    \ + prompt syntax into other models."
  created_at: 2024-01-05 07:19:05+00:00
  edited: false
  hidden: false
  id: 6597ad69a2659adbf900d56e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-05T08:20:38.000Z'
    data:
      edited: false
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9693419933319092
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>I don''t recall seeing any merges using that specif method, only
          the standard 0.0&gt;1.0/1.0&gt;0.0 method like from here:</p>

          <p><a rel="nofollow" href="https://github.com/Gryphe/BlockMerge_Gradient">https://github.com/Gryphe/BlockMerge_Gradient</a></p>

          <p>The only thing i can say is do a test run to see if it works or not,
          maybe using a smaller 7b model and comparing with another established one
          like OpenHermes-2.5-neural-chat-v3-3-Slerp in the same mold. I could probably
          make a Hermes-Chat version later this weak to compare formats and overall
          chatting prowess if you don''t want to.</p>

          <p>As for the model base idea...I''m not sure it would make too much of
          a difference. Context training seems to be tied to the individual models
          themselves, rather than the base, since I''ve tested someone doing something
          similar with Mistral, with a guy taking a couple models from v1 over to
          v2 and the context only worked up to 8k instead of 32k without rope scaling.
          No idea if the reverse works though.<br>To get another idea from the Mixtral
          frankenmerges, it seems people chose bases that would better fit the prompting
          so that the model would understand better what is going on underneath the
          hood. It''s why Bagel 7b was picked at first as the base for the 8 separate
          MoEs, since you could throw different prompts style models in and the base
          would "sorta" understand them all to a degree as long as the base was trained
          on it. Course a standard format works better then throwing a whole bunch
          of different clowns into the clowncar, as I''ve discovered when trying to
          do so on my own and gave up. No idea if this translates to regular models
          though, or if choosing a fine-tune with similar prompting as a base would
          be better for it. idk.<br>It''s something that might be worth testing on
          a smaller model if nothing else, but raising context is something I''ve
          been banging my head against when doing research and testing, and it''s
          not something I''ve seen to work fully aside from merging a longer model
          as a band-aid. Even those so called 16k Mistral extension models start to
          putter out before hitting 12k.<br>So best bet from my inexperienced advice
          for your merge atm, without further testing ofc, is keep it simple and by
          the books in continuing to do what you know best with the model''s parameters.</p>

          '
        raw: 'I don''t recall seeing any merges using that specif method, only the
          standard 0.0>1.0/1.0>0.0 method like from here:


          https://github.com/Gryphe/BlockMerge_Gradient


          The only thing i can say is do a test run to see if it works or not, maybe
          using a smaller 7b model and comparing with another established one like
          OpenHermes-2.5-neural-chat-v3-3-Slerp in the same mold. I could probably
          make a Hermes-Chat version later this weak to compare formats and overall
          chatting prowess if you don''t want to.


          As for the model base idea...I''m not sure it would make too much of a difference.
          Context training seems to be tied to the individual models themselves, rather
          than the base, since I''ve tested someone doing something similar with Mistral,
          with a guy taking a couple models from v1 over to v2 and the context only
          worked up to 8k instead of 32k without rope scaling. No idea if the reverse
          works though.

          To get another idea from the Mixtral frankenmerges, it seems people chose
          bases that would better fit the prompting so that the model would understand
          better what is going on underneath the hood. It''s why Bagel 7b was picked
          at first as the base for the 8 separate MoEs, since you could throw different
          prompts style models in and the base would "sorta" understand them all to
          a degree as long as the base was trained on it. Course a standard format
          works better then throwing a whole bunch of different clowns into the clowncar,
          as I''ve discovered when trying to do so on my own and gave up. No idea
          if this translates to regular models though, or if choosing a fine-tune
          with similar prompting as a base would be better for it. idk.

          It''s something that might be worth testing on a smaller model if nothing
          else, but raising context is something I''ve been banging my head against
          when doing research and testing, and it''s not something I''ve seen to work
          fully aside from merging a longer model as a band-aid. Even those so called
          16k Mistral extension models start to putter out before hitting 12k.

          So best bet from my inexperienced advice for your merge atm, without further
          testing ofc, is keep it simple and by the books in continuing to do what
          you know best with the model''s parameters.'
        updatedAt: '2024-01-05T08:20:38.933Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - brucethemoose
        - Trangle
    id: 6597bbd6092b4516b8877edb
    type: comment
  author: ParasiticRogue
  content: 'I don''t recall seeing any merges using that specif method, only the standard
    0.0>1.0/1.0>0.0 method like from here:


    https://github.com/Gryphe/BlockMerge_Gradient


    The only thing i can say is do a test run to see if it works or not, maybe using
    a smaller 7b model and comparing with another established one like OpenHermes-2.5-neural-chat-v3-3-Slerp
    in the same mold. I could probably make a Hermes-Chat version later this weak
    to compare formats and overall chatting prowess if you don''t want to.


    As for the model base idea...I''m not sure it would make too much of a difference.
    Context training seems to be tied to the individual models themselves, rather
    than the base, since I''ve tested someone doing something similar with Mistral,
    with a guy taking a couple models from v1 over to v2 and the context only worked
    up to 8k instead of 32k without rope scaling. No idea if the reverse works though.

    To get another idea from the Mixtral frankenmerges, it seems people chose bases
    that would better fit the prompting so that the model would understand better
    what is going on underneath the hood. It''s why Bagel 7b was picked at first as
    the base for the 8 separate MoEs, since you could throw different prompts style
    models in and the base would "sorta" understand them all to a degree as long as
    the base was trained on it. Course a standard format works better then throwing
    a whole bunch of different clowns into the clowncar, as I''ve discovered when
    trying to do so on my own and gave up. No idea if this translates to regular models
    though, or if choosing a fine-tune with similar prompting as a base would be better
    for it. idk.

    It''s something that might be worth testing on a smaller model if nothing else,
    but raising context is something I''ve been banging my head against when doing
    research and testing, and it''s not something I''ve seen to work fully aside from
    merging a longer model as a band-aid. Even those so called 16k Mistral extension
    models start to putter out before hitting 12k.

    So best bet from my inexperienced advice for your merge atm, without further testing
    ofc, is keep it simple and by the books in continuing to do what you know best
    with the model''s parameters.'
  created_at: 2024-01-05 08:20:38+00:00
  edited: false
  hidden: false
  id: 6597bbd6092b4516b8877edb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-06T06:28:12.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9616749286651611
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Also just to revisit this, I am thinking about doing a Vicuna centric
          merge with SLERP leftovers just as you said, but its on the backburner atm.
          I may not get to it this weekend, so you should totally investigate doing
          it yourself if you wish. I can even post the mergekit configs to do it.</p>

          '
        raw: Also just to revisit this, I am thinking about doing a Vicuna centric
          merge with SLERP leftovers just as you said, but its on the backburner atm.
          I may not get to it this weekend, so you should totally investigate doing
          it yourself if you wish. I can even post the mergekit configs to do it.
        updatedAt: '2024-01-06T06:28:12.530Z'
      numEdits: 0
      reactions: []
    id: 6598f2fc0ae723e79c566139
    type: comment
  author: brucethemoose
  content: Also just to revisit this, I am thinking about doing a Vicuna centric merge
    with SLERP leftovers just as you said, but its on the backburner atm. I may not
    get to it this weekend, so you should totally investigate doing it yourself if
    you wish. I can even post the mergekit configs to do it.
  created_at: 2024-01-06 06:28:12+00:00
  edited: false
  hidden: false
  id: 6598f2fc0ae723e79c566139
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-06T23:59:56.000Z'
    data:
      edited: false
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.975933313369751
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>I might do that. Perhaps I''ll focus on doing a ChatML 34B version
          later using Dolphin as the main to compare later with yours, so I''d be
          happy to have some configs as a guideline to look at. How much V/RAM does
          34B merges take up? I have 24gb of VRAM and 32gb of regular ram atm. Do
          i have to set up an environment with swap/virtual memory first with a decent
          chunk set, or do you typically use a secondary service like runpod for bigger
          models?</p>

          '
        raw: I might do that. Perhaps I'll focus on doing a ChatML 34B version later
          using Dolphin as the main to compare later with yours, so I'd be happy to
          have some configs as a guideline to look at. How much V/RAM does 34B merges
          take up? I have 24gb of VRAM and 32gb of regular ram atm. Do i have to set
          up an environment with swap/virtual memory first with a decent chunk set,
          or do you typically use a secondary service like runpod for bigger models?
        updatedAt: '2024-01-06T23:59:56.954Z'
      numEdits: 0
      reactions: []
    id: 6599e97c654fe4eb0accd431
    type: comment
  author: ParasiticRogue
  content: I might do that. Perhaps I'll focus on doing a ChatML 34B version later
    using Dolphin as the main to compare later with yours, so I'd be happy to have
    some configs as a guideline to look at. How much V/RAM does 34B merges take up?
    I have 24gb of VRAM and 32gb of regular ram atm. Do i have to set up an environment
    with swap/virtual memory first with a decent chunk set, or do you typically use
    a secondary service like runpod for bigger models?
  created_at: 2024-01-06 23:59:56+00:00
  edited: false
  hidden: false
  id: 6599e97c654fe4eb0accd431
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-07T03:07:57.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9601954817771912
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<blockquote>

          <p>I might do that. Perhaps I''ll focus on doing a ChatML 34B version later
          using Dolphin as the main to compare later with yours, so I''d be happy
          to have some configs as a guideline to look at. How much V/RAM does 34B
          merges take up? I have 24gb of VRAM and 32gb of regular ram atm. Do i have
          to set up an environment with swap/virtual memory first with a decent chunk
          set, or do you typically use a secondary service like runpod for bigger
          models?</p>

          </blockquote>

          <p>I have the exact same setup, 24GB/32GB. I do it all locally.</p>

          <p>You can merge 4 34B models (+ the base model) in 24GB VRAM + 32GB CPU
          RAM. Maybe 5? You still need swap space, but just a little. Its pretty fast!</p>

          <p>Past that, you need to do a cpu only merge and set up a lot of swap on
          an ssd. Probably at least 64GB. This merge is slower but still gets done
          in a reasonable amount of time.</p>

          '
        raw: '> I might do that. Perhaps I''ll focus on doing a ChatML 34B version
          later using Dolphin as the main to compare later with yours, so I''d be
          happy to have some configs as a guideline to look at. How much V/RAM does
          34B merges take up? I have 24gb of VRAM and 32gb of regular ram atm. Do
          i have to set up an environment with swap/virtual memory first with a decent
          chunk set, or do you typically use a secondary service like runpod for bigger
          models?


          I have the exact same setup, 24GB/32GB. I do it all locally.


          You can merge 4 34B models (+ the base model) in 24GB VRAM + 32GB CPU RAM.
          Maybe 5? You still need swap space, but just a little. Its pretty fast!


          Past that, you need to do a cpu only merge and set up a lot of swap on an
          ssd. Probably at least 64GB. This merge is slower but still gets done in
          a reasonable amount of time.'
        updatedAt: '2024-01-07T03:11:21.463Z'
      numEdits: 4
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ParasiticRogue
    id: 659a158d816bb94a4f6f90d3
    type: comment
  author: brucethemoose
  content: '> I might do that. Perhaps I''ll focus on doing a ChatML 34B version later
    using Dolphin as the main to compare later with yours, so I''d be happy to have
    some configs as a guideline to look at. How much V/RAM does 34B merges take up?
    I have 24gb of VRAM and 32gb of regular ram atm. Do i have to set up an environment
    with swap/virtual memory first with a decent chunk set, or do you typically use
    a secondary service like runpod for bigger models?


    I have the exact same setup, 24GB/32GB. I do it all locally.


    You can merge 4 34B models (+ the base model) in 24GB VRAM + 32GB CPU RAM. Maybe
    5? You still need swap space, but just a little. Its pretty fast!


    Past that, you need to do a cpu only merge and set up a lot of swap on an ssd.
    Probably at least 64GB. This merge is slower but still gets done in a reasonable
    amount of time.'
  created_at: 2024-01-07 03:07:57+00:00
  edited: true
  hidden: false
  id: 659a158d816bb94a4f6f90d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-07T03:43:07.000Z'
    data:
      edited: false
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9520081281661987
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I might do that. Perhaps I''ll focus on doing a ChatML 34B version later
          using Dolphin as the main to compare later with yours, so I''d be happy
          to have some configs as a guideline to look at. How much V/RAM does 34B
          merges take up? I have 24gb of VRAM and 32gb of regular ram atm. Do i have
          to set up an environment with swap/virtual memory first with a decent chunk
          set, or do you typically use a secondary service like runpod for bigger
          models?</p>

          </blockquote>

          <p>I have the exact same setup, 24GB/32GB. I do it all locally.</p>

          <p>You can merge 4 34B models (+ the base model) in 24GB VRAM + 32GB CPU
          RAM. Maybe 5? You still need swap space, but just a little. Its pretty fast!</p>

          <p>Past that, you need to do a cpu only merge and set up a lot of swap on
          an ssd. Probably at least 64GB. This merge is slower but still gets done
          in a reasonable amount of time.</p>

          </blockquote>

          <p>Alright then. Would I still need to muck about with swap if it''s only
          3 models (+base) for the merge? I''m thinking of just sticking to Bagel+Dolphin+Hermes
          for now. Maybe using AEZAKMI to slerp with Bagel first for hopefully better
          prompting on the ChatML format, idk. I''ll see how the first merge works
          first. Thanks for the insight thus far!</p>

          '
        raw: "> > I might do that. Perhaps I'll focus on doing a ChatML 34B version\
          \ later using Dolphin as the main to compare later with yours, so I'd be\
          \ happy to have some configs as a guideline to look at. How much V/RAM does\
          \ 34B merges take up? I have 24gb of VRAM and 32gb of regular ram atm. Do\
          \ i have to set up an environment with swap/virtual memory first with a\
          \ decent chunk set, or do you typically use a secondary service like runpod\
          \ for bigger models?\n> \n> I have the exact same setup, 24GB/32GB. I do\
          \ it all locally.\n> \n> You can merge 4 34B models (+ the base model) in\
          \ 24GB VRAM + 32GB CPU RAM. Maybe 5? You still need swap space, but just\
          \ a little. Its pretty fast!\n> \n> Past that, you need to do a cpu only\
          \ merge and set up a lot of swap on an ssd. Probably at least 64GB. This\
          \ merge is slower but still gets done in a reasonable amount of time.\n\n\
          Alright then. Would I still need to muck about with swap if it's only 3\
          \ models (+base) for the merge? I'm thinking of just sticking to Bagel+Dolphin+Hermes\
          \ for now. Maybe using AEZAKMI to slerp with Bagel first for hopefully better\
          \ prompting on the ChatML format, idk. I'll see how the first merge works\
          \ first. Thanks for the insight thus far!"
        updatedAt: '2024-01-07T03:43:07.120Z'
      numEdits: 0
      reactions: []
    id: 659a1dcba3259bc417f4b548
    type: comment
  author: ParasiticRogue
  content: "> > I might do that. Perhaps I'll focus on doing a ChatML 34B version\
    \ later using Dolphin as the main to compare later with yours, so I'd be happy\
    \ to have some configs as a guideline to look at. How much V/RAM does 34B merges\
    \ take up? I have 24gb of VRAM and 32gb of regular ram atm. Do i have to set up\
    \ an environment with swap/virtual memory first with a decent chunk set, or do\
    \ you typically use a secondary service like runpod for bigger models?\n> \n>\
    \ I have the exact same setup, 24GB/32GB. I do it all locally.\n> \n> You can\
    \ merge 4 34B models (+ the base model) in 24GB VRAM + 32GB CPU RAM. Maybe 5?\
    \ You still need swap space, but just a little. Its pretty fast!\n> \n> Past that,\
    \ you need to do a cpu only merge and set up a lot of swap on an ssd. Probably\
    \ at least 64GB. This merge is slower but still gets done in a reasonable amount\
    \ of time.\n\nAlright then. Would I still need to muck about with swap if it's\
    \ only 3 models (+base) for the merge? I'm thinking of just sticking to Bagel+Dolphin+Hermes\
    \ for now. Maybe using AEZAKMI to slerp with Bagel first for hopefully better\
    \ prompting on the ChatML format, idk. I'll see how the first merge works first.\
    \ Thanks for the insight thus far!"
  created_at: 2024-01-07 03:43:07+00:00
  edited: false
  hidden: false
  id: 659a1dcba3259bc417f4b548
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-08T01:37:16.000Z'
    data:
      edited: false
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9577350616455078
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>Quick update on the HermesChat 7b test using similar gradient methods
          you suggested; the model seems to work after some brief testing, but idk
          if it''s better then the regular merge methods with similar settings tbh.
          Comparing it to the other version listed mine seemed to do worse personally.
          Perhaps adjusting it slightly might help, but I''d say leave it for now.</p>

          '
        raw: Quick update on the HermesChat 7b test using similar gradient methods
          you suggested; the model seems to work after some brief testing, but idk
          if it's better then the regular merge methods with similar settings tbh.
          Comparing it to the other version listed mine seemed to do worse personally.
          Perhaps adjusting it slightly might help, but I'd say leave it for now.
        updatedAt: '2024-01-08T01:37:16.412Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - brucethemoose
    id: 659b51cc17edd1f053d71fd7
    type: comment
  author: ParasiticRogue
  content: Quick update on the HermesChat 7b test using similar gradient methods you
    suggested; the model seems to work after some brief testing, but idk if it's better
    then the regular merge methods with similar settings tbh. Comparing it to the
    other version listed mine seemed to do worse personally. Perhaps adjusting it
    slightly might help, but I'd say leave it for now.
  created_at: 2024-01-08 01:37:16+00:00
  edited: false
  hidden: false
  id: 659b51cc17edd1f053d71fd7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-08T16:37:25.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9673330187797546
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Yeah, I briefly tested it locally and am unsure myself. The perplexity
          is not that different though.</p>

          '
        raw: Yeah, I briefly tested it locally and am unsure myself. The perplexity
          is not that different though.
        updatedAt: '2024-01-08T16:37:25.198Z'
      numEdits: 0
      reactions: []
    id: 659c24c579badc9e683cd685
    type: comment
  author: brucethemoose
  content: Yeah, I briefly tested it locally and am unsure myself. The perplexity
    is not that different though.
  created_at: 2024-01-08 16:37:25+00:00
  edited: false
  hidden: false
  id: 659c24c579badc9e683cd685
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-09T00:43:52.000Z'
    data:
      edited: false
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9213055372238159
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>The prompting methods discussed should be good enough for your merge
          later, so don''t sweat it too much. I was looking at potential models to
          experiment with Yi on my end and I came across this which might interest
          you in the Vicuna merge.</p>

          <p><a href="https://huggingface.co/bhenrym14/platypus-yi-34b">https://huggingface.co/bhenrym14/platypus-yi-34b</a></p>

          <p>It''s not 200K, but it might be useful to slerp with Sus-Hermes since
          they are both 32k. Therefore it would leave more room to add a second Tess
          variant for the final merge later to both extend context and not be too
          samey in structure. What do you think about this for the mix?</p>

          <p>Platypus + SUS-Hermes = Plat-Hermes<br>Capybara + Nontoxix-Bagel = Capy-Bagel<br>Tess-v1
          + Pallas-v4 + Capybara + Plat-Hermes + Capy-Bagel.</p>

          '
        raw: 'The prompting methods discussed should be good enough for your merge
          later, so don''t sweat it too much. I was looking at potential models to
          experiment with Yi on my end and I came across this which might interest
          you in the Vicuna merge.


          https://huggingface.co/bhenrym14/platypus-yi-34b


          It''s not 200K, but it might be useful to slerp with Sus-Hermes since they
          are both 32k. Therefore it would leave more room to add a second Tess variant
          for the final merge later to both extend context and not be too samey in
          structure. What do you think about this for the mix?


          Platypus + SUS-Hermes = Plat-Hermes

          Capybara + Nontoxix-Bagel = Capy-Bagel

          Tess-v1 + Pallas-v4 + Capybara + Plat-Hermes + Capy-Bagel.'
        updatedAt: '2024-01-09T00:43:52.189Z'
      numEdits: 0
      reactions: []
    id: 659c96c81d398a23815b071b
    type: comment
  author: ParasiticRogue
  content: 'The prompting methods discussed should be good enough for your merge later,
    so don''t sweat it too much. I was looking at potential models to experiment with
    Yi on my end and I came across this which might interest you in the Vicuna merge.


    https://huggingface.co/bhenrym14/platypus-yi-34b


    It''s not 200K, but it might be useful to slerp with Sus-Hermes since they are
    both 32k. Therefore it would leave more room to add a second Tess variant for
    the final merge later to both extend context and not be too samey in structure.
    What do you think about this for the mix?


    Platypus + SUS-Hermes = Plat-Hermes

    Capybara + Nontoxix-Bagel = Capy-Bagel

    Tess-v1 + Pallas-v4 + Capybara + Plat-Hermes + Capy-Bagel.'
  created_at: 2024-01-09 00:43:52+00:00
  edited: false
  hidden: false
  id: 659c96c81d398a23815b071b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-09T16:28:43.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9534087777137756
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<blockquote>

          <p>The prompting methods discussed should be good enough for your merge
          later, so don''t sweat it too much. I was looking at potential models to
          experiment with Yi on my end and I came across this which might interest
          you in the Vicuna merge.</p>

          <p><a href="https://huggingface.co/bhenrym14/platypus-yi-34b">https://huggingface.co/bhenrym14/platypus-yi-34b</a></p>

          <p>It''s not 200K, but it might be useful to slerp with Sus-Hermes since
          they are both 32k. Therefore it would leave more room to add a second Tess
          variant for the final merge later to both extend context and not be too
          samey in structure. What do you think about this for the mix?</p>

          <p>Platypus + SUS-Hermes = Plat-Hermes<br>Capybara + Nontoxix-Bagel = Capy-Bagel<br>Tess-v1
          + Pallas-v4 + Capybara + Plat-Hermes + Capy-Bagel.</p>

          </blockquote>

          <p>Oh yeah, I didn''t realize Platypus Yi was Vicuna format. Yeah I think
          that is a good idea, to "Vicunafy" Sus Hermes.</p>

          <p>That looks like a excellent recipe.</p>

          <p>What I would also suggest is to merge some Yi 200K base to "200kify"
          plat-hermes without affecting the prompt format. That actually seems to
          work in my tests, and it makes sense because Yi 200K seems to be a Yi 4K
          finetune.</p>

          '
        raw: "\n\n> The prompting methods discussed should be good enough for your\
          \ merge later, so don't sweat it too much. I was looking at potential models\
          \ to experiment with Yi on my end and I came across this which might interest\
          \ you in the Vicuna merge.\n> \n> https://huggingface.co/bhenrym14/platypus-yi-34b\n\
          > \n> It's not 200K, but it might be useful to slerp with Sus-Hermes since\
          \ they are both 32k. Therefore it would leave more room to add a second\
          \ Tess variant for the final merge later to both extend context and not\
          \ be too samey in structure. What do you think about this for the mix?\n\
          > \n> Platypus + SUS-Hermes = Plat-Hermes\n> Capybara + Nontoxix-Bagel =\
          \ Capy-Bagel\n> Tess-v1 + Pallas-v4 + Capybara + Plat-Hermes + Capy-Bagel.\n\
          \nOh yeah, I didn't realize Platypus Yi was Vicuna format. Yeah I think\
          \ that is a good idea, to \"Vicunafy\" Sus Hermes.\n\nThat looks like a\
          \ excellent recipe.\n\nWhat I would also suggest is to merge some Yi 200K\
          \ base to \"200kify\" plat-hermes without affecting the prompt format. That\
          \ actually seems to work in my tests, and it makes sense because Yi 200K\
          \ seems to be a Yi 4K finetune.\n\n"
        updatedAt: '2024-01-09T16:29:51.409Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ParasiticRogue
    id: 659d743bd54013d8f615f60a
    type: comment
  author: brucethemoose
  content: "\n\n> The prompting methods discussed should be good enough for your merge\
    \ later, so don't sweat it too much. I was looking at potential models to experiment\
    \ with Yi on my end and I came across this which might interest you in the Vicuna\
    \ merge.\n> \n> https://huggingface.co/bhenrym14/platypus-yi-34b\n> \n> It's not\
    \ 200K, but it might be useful to slerp with Sus-Hermes since they are both 32k.\
    \ Therefore it would leave more room to add a second Tess variant for the final\
    \ merge later to both extend context and not be too samey in structure. What do\
    \ you think about this for the mix?\n> \n> Platypus + SUS-Hermes = Plat-Hermes\n\
    > Capybara + Nontoxix-Bagel = Capy-Bagel\n> Tess-v1 + Pallas-v4 + Capybara + Plat-Hermes\
    \ + Capy-Bagel.\n\nOh yeah, I didn't realize Platypus Yi was Vicuna format. Yeah\
    \ I think that is a good idea, to \"Vicunafy\" Sus Hermes.\n\nThat looks like\
    \ a excellent recipe.\n\nWhat I would also suggest is to merge some Yi 200K base\
    \ to \"200kify\" plat-hermes without affecting the prompt format. That actually\
    \ seems to work in my tests, and it makes sense because Yi 200K seems to be a\
    \ Yi 4K finetune.\n\n"
  created_at: 2024-01-09 16:28:43+00:00
  edited: true
  hidden: false
  id: 659d743bd54013d8f615f60a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-09T18:50:10.000Z'
    data:
      edited: true
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8990496397018433
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>If that method works for you then go for it! I see no logical reason
          not to, So long as it doesn''t effect prompting ofc.</p>

          '
        raw: If that method works for you then go for it! I see no logical reason
          not to, So long as it doesn't effect prompting ofc.
        updatedAt: '2024-01-09T18:55:04.841Z'
      numEdits: 1
      reactions: []
    id: 659d9562b3e1294d58a11b1c
    type: comment
  author: ParasiticRogue
  content: If that method works for you then go for it! I see no logical reason not
    to, So long as it doesn't effect prompting ofc.
  created_at: 2024-01-09 18:50:10+00:00
  edited: true
  hidden: false
  id: 659d9562b3e1294d58a11b1c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-16T02:14:34.000Z'
    data:
      edited: true
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9313302040100098
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>Quick question: Do you get any errors when merging any of the pure
          Vicuna models on your end with Mergekit? I''ve been trying to see how using
          the custom versions of SUS-Chat-RP and UNA-Bagel might mix with Vicuna,
          but every time I use Tess or Pallas I get this.</p>

          <p>(WARNING:root:Using submatrix of /home/oem/Desktop/merge/migtissera_Tess-34B-v1.4:model.embed_tokens.weight)</p>

          <p>Meanwhile Capybara just gives me a "killed" message at around 13% of
          the merging process.</p>

          <p>I''ve tried updating Mergekit and using different merge methods, but
          none of them seem to work. Is there something I''m missing when doing these
          Yi models? Cause I''ve been able to merge the others just fine in the 34b
          range like Nous-Hermes and the other 2 models. Is their some unique settings
          you use when doing this stuff? Cause I''ve been bashing my head for a good
          week now trying to do this, and I didn''t see anyone having these issues
          on the github either...</p>

          '
        raw: 'Quick question: Do you get any errors when merging any of the pure Vicuna
          models on your end with Mergekit? I''ve been trying to see how using the
          custom versions of SUS-Chat-RP and UNA-Bagel might mix with Vicuna, but
          every time I use Tess or Pallas I get this.


          (WARNING:root:Using submatrix of /home/oem/Desktop/merge/migtissera_Tess-34B-v1.4:model.embed_tokens.weight)


          Meanwhile Capybara just gives me a "killed" message at around 13% of the
          merging process.


          I''ve tried updating Mergekit and using different merge methods, but none
          of them seem to work. Is there something I''m missing when doing these Yi
          models? Cause I''ve been able to merge the others just fine in the 34b range
          like Nous-Hermes and the other 2 models. Is their some unique settings you
          use when doing this stuff? Cause I''ve been bashing my head for a good week
          now trying to do this, and I didn''t see anyone having these issues on the
          github either...'
        updatedAt: '2024-01-16T02:16:13.808Z'
      numEdits: 1
      reactions: []
    id: 65a5e68ab26c0f0134f6ad21
    type: comment
  author: ParasiticRogue
  content: 'Quick question: Do you get any errors when merging any of the pure Vicuna
    models on your end with Mergekit? I''ve been trying to see how using the custom
    versions of SUS-Chat-RP and UNA-Bagel might mix with Vicuna, but every time I
    use Tess or Pallas I get this.


    (WARNING:root:Using submatrix of /home/oem/Desktop/merge/migtissera_Tess-34B-v1.4:model.embed_tokens.weight)


    Meanwhile Capybara just gives me a "killed" message at around 13% of the merging
    process.


    I''ve tried updating Mergekit and using different merge methods, but none of them
    seem to work. Is there something I''m missing when doing these Yi models? Cause
    I''ve been able to merge the others just fine in the 34b range like Nous-Hermes
    and the other 2 models. Is their some unique settings you use when doing this
    stuff? Cause I''ve been bashing my head for a good week now trying to do this,
    and I didn''t see anyone having these issues on the github either...'
  created_at: 2024-01-16 02:14:34+00:00
  edited: true
  hidden: false
  id: 65a5e68ab26c0f0134f6ad21
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-16T02:20:28.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9103763699531555
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<blockquote>

          <p>(WARNING:root:Using submatrix of /home/oem/Desktop/merge/migtissera_Tess-34B-v1.4:model.embed_tokens.weight)</p>

          </blockquote>

          <p>This is just a warning, but technically you need to use the "union" tokenizer
          merge to get it to go away and merge the vocabularies of all the models.</p>

          <p>You also need to start mergekit with --lazy-unpickle to get it to merge
          with a reasonable amount of RAM, and --cuda if you are merging 4 or fewer
          models. If the process just shuts down, that means its out of memory and
          you need to add more swap.</p>

          <p>I haven''t run into any issues with the Vicuna models specifically.</p>

          <p><em>Some</em> models are funky though, yeah. For instance, Nous Hermes
          and Yi Chat have some kind of misconfigured tokenizer/vocab that breaks
          mergekit and probably leaves the models themselves slightly broken.</p>

          <p>Good luck, I''m happy to help if stuff still isn''t working.</p>

          '
        raw: '> (WARNING:root:Using submatrix of /home/oem/Desktop/merge/migtissera_Tess-34B-v1.4:model.embed_tokens.weight)


          This is just a warning, but technically you need to use the "union" tokenizer
          merge to get it to go away and merge the vocabularies of all the models.


          You also need to start mergekit with --lazy-unpickle to get it to merge
          with a reasonable amount of RAM, and --cuda if you are merging 4 or fewer
          models. If the process just shuts down, that means its out of memory and
          you need to add more swap.


          I haven''t run into any issues with the Vicuna models specifically.


          *Some* models are funky though, yeah. For instance, Nous Hermes and Yi Chat
          have some kind of misconfigured tokenizer/vocab that breaks mergekit and
          probably leaves the models themselves slightly broken.


          Good luck, I''m happy to help if stuff still isn''t working.'
        updatedAt: '2024-01-16T02:27:26.474Z'
      numEdits: 2
      reactions: []
    id: 65a5e7ecfdc98e08a5e8925d
    type: comment
  author: brucethemoose
  content: '> (WARNING:root:Using submatrix of /home/oem/Desktop/merge/migtissera_Tess-34B-v1.4:model.embed_tokens.weight)


    This is just a warning, but technically you need to use the "union" tokenizer
    merge to get it to go away and merge the vocabularies of all the models.


    You also need to start mergekit with --lazy-unpickle to get it to merge with a
    reasonable amount of RAM, and --cuda if you are merging 4 or fewer models. If
    the process just shuts down, that means its out of memory and you need to add
    more swap.


    I haven''t run into any issues with the Vicuna models specifically.


    *Some* models are funky though, yeah. For instance, Nous Hermes and Yi Chat have
    some kind of misconfigured tokenizer/vocab that breaks mergekit and probably leaves
    the models themselves slightly broken.


    Good luck, I''m happy to help if stuff still isn''t working.'
  created_at: 2024-01-16 02:20:28+00:00
  edited: true
  hidden: false
  id: 65a5e7ecfdc98e08a5e8925d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-16T02:49:05.000Z'
    data:
      edited: false
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9838146567344666
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>I guess that solves one problem with the tokenizer. I assume I take
          out the "Base model" path and just use an empty "Union" folder for it to
          generate, yeah?</p>

          <p>Still, even when using lazy unpickle and cuda it still kills the model
          at 13%, even if I''m only merging two models. And I''ve been able to merge
          3 34B models before just fine, so I don''t think it''s a memory issue...
          The onle thing different I can find is Capybara has some extra files inside
          like tokenization_yi.py which is different from the others, and coincidentally
          Dolphin has something similar in it as well and I''ve been having trouble
          doing it as well (Killed at 13%). I think it''s because they were the first
          proper models Yi made I guess? None of the others have that problem for
          me.</p>

          <p>Also I''m surprised you''ve been having the exact opposite problem with
          Hemes and SUS-chat, lol. You want me to merge them on my end and then you
          can use them for your stuff later? It might iron out your problems.</p>

          '
        raw: 'I guess that solves one problem with the tokenizer. I assume I take
          out the "Base model" path and just use an empty "Union" folder for it to
          generate, yeah?


          Still, even when using lazy unpickle and cuda it still kills the model at
          13%, even if I''m only merging two models. And I''ve been able to merge
          3 34B models before just fine, so I don''t think it''s a memory issue...
          The onle thing different I can find is Capybara has some extra files inside
          like tokenization_yi.py which is different from the others, and coincidentally
          Dolphin has something similar in it as well and I''ve been having trouble
          doing it as well (Killed at 13%). I think it''s because they were the first
          proper models Yi made I guess? None of the others have that problem for
          me.


          Also I''m surprised you''ve been having the exact opposite problem with
          Hemes and SUS-chat, lol. You want me to merge them on my end and then you
          can use them for your stuff later? It might iron out your problems.'
        updatedAt: '2024-01-16T02:49:05.257Z'
      numEdits: 0
      reactions: []
    id: 65a5eea1668b3906b84ee266
    type: comment
  author: ParasiticRogue
  content: 'I guess that solves one problem with the tokenizer. I assume I take out
    the "Base model" path and just use an empty "Union" folder for it to generate,
    yeah?


    Still, even when using lazy unpickle and cuda it still kills the model at 13%,
    even if I''m only merging two models. And I''ve been able to merge 3 34B models
    before just fine, so I don''t think it''s a memory issue... The onle thing different
    I can find is Capybara has some extra files inside like tokenization_yi.py which
    is different from the others, and coincidentally Dolphin has something similar
    in it as well and I''ve been having trouble doing it as well (Killed at 13%).
    I think it''s because they were the first proper models Yi made I guess? None
    of the others have that problem for me.


    Also I''m surprised you''ve been having the exact opposite problem with Hemes
    and SUS-chat, lol. You want me to merge them on my end and then you can use them
    for your stuff later? It might iron out your problems.'
  created_at: 2024-01-16 02:49:05+00:00
  edited: false
  hidden: false
  id: 65a5eea1668b3906b84ee266
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-16T04:03:16.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9408931732177734
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Oh yeah. I thought Capybara was llamafied, if the Yi tokenizer is
          in there, that''s trouble. </p>

          <p>I... may have fixed it locally by just slapping in another tokenizer.
          It was so long ago I can''t remember TBH. </p>

          <p>Charles did post code to officially "llamafy" the base weights of old
          Yi models. I sucessfully used it on Deepsex, but unfortunately it doesn''t
          do anything to the tokenizer. For that I just copied Yi''s new tokenizer
          in.</p>

          <p><a href="https://huggingface.co/chargoddard/Yi-34B-Llama/discussions/7">https://huggingface.co/chargoddard/Yi-34B-Llama/discussions/7</a></p>

          <p>As for where to put the "union," see the config here: <a href="https://huggingface.co/brucethemoose/Yi-34B-200K-DARE-megamerge-v8#configuration">https://huggingface.co/brucethemoose/Yi-34B-200K-DARE-megamerge-v8#configuration</a></p>

          '
        raw: "Oh yeah. I thought Capybara was llamafied, if the Yi tokenizer is in\
          \ there, that's trouble. \n\nI... may have fixed it locally by just slapping\
          \ in another tokenizer. It was so long ago I can't remember TBH. \n\nCharles\
          \ did post code to officially \"llamafy\" the base weights of old Yi models.\
          \ I sucessfully used it on Deepsex, but unfortunately it doesn't do anything\
          \ to the tokenizer. For that I just copied Yi's new tokenizer in.\n\nhttps://huggingface.co/chargoddard/Yi-34B-Llama/discussions/7\n\
          \n\nAs for where to put the \"union,\" see the config here: https://huggingface.co/brucethemoose/Yi-34B-200K-DARE-megamerge-v8#configuration\n\
          \n\n"
        updatedAt: '2024-01-16T04:03:16.903Z'
      numEdits: 0
      reactions: []
    id: 65a60004810edff295f24d81
    type: comment
  author: brucethemoose
  content: "Oh yeah. I thought Capybara was llamafied, if the Yi tokenizer is in there,\
    \ that's trouble. \n\nI... may have fixed it locally by just slapping in another\
    \ tokenizer. It was so long ago I can't remember TBH. \n\nCharles did post code\
    \ to officially \"llamafy\" the base weights of old Yi models. I sucessfully used\
    \ it on Deepsex, but unfortunately it doesn't do anything to the tokenizer. For\
    \ that I just copied Yi's new tokenizer in.\n\nhttps://huggingface.co/chargoddard/Yi-34B-Llama/discussions/7\n\
    \n\nAs for where to put the \"union,\" see the config here: https://huggingface.co/brucethemoose/Yi-34B-200K-DARE-megamerge-v8#configuration\n\
    \n\n"
  created_at: 2024-01-16 04:03:16+00:00
  edited: false
  hidden: false
  id: 65a60004810edff295f24d81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-16T06:12:35.000Z'
    data:
      edited: false
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9745352268218994
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>I did a quick test using the methods you recommended. Good news?
          I''m not having problems with the Tess models being fussy with token weights
          anymore! Bad news? Still can''t get Capybara to go past 13% at all... Tried
          using Yi''s updated one, but still nothing... Even tried jamming Doctor
          Shotgun''s Capybara-RP''s entire folder in minus the pytorch/safetensors
          files, since that seemed to be the closest link I could find, but still
          stops at 13% even then, and I had no issues with his model to merge with
          others. Last ditch effort and used the script you provided, and...nothing
          but more errors. <em>Face desk</em></p>

          <p>I hate to ask this, but would it be too much trouble if you upload your
          Capybara version to huggingface for others looking to merge like me? I''d
          really be grateful.</p>

          '
        raw: 'I did a quick test using the methods you recommended. Good news? I''m
          not having problems with the Tess models being fussy with token weights
          anymore! Bad news? Still can''t get Capybara to go past 13% at all... Tried
          using Yi''s updated one, but still nothing... Even tried jamming Doctor
          Shotgun''s Capybara-RP''s entire folder in minus the pytorch/safetensors
          files, since that seemed to be the closest link I could find, but still
          stops at 13% even then, and I had no issues with his model to merge with
          others. Last ditch effort and used the script you provided, and...nothing
          but more errors. *Face desk*


          I hate to ask this, but would it be too much trouble if you upload your
          Capybara version to huggingface for others looking to merge like me? I''d
          really be grateful.'
        updatedAt: '2024-01-16T06:12:35.026Z'
      numEdits: 0
      reactions: []
    id: 65a61e530a0bcd589642015b
    type: comment
  author: ParasiticRogue
  content: 'I did a quick test using the methods you recommended. Good news? I''m
    not having problems with the Tess models being fussy with token weights anymore!
    Bad news? Still can''t get Capybara to go past 13% at all... Tried using Yi''s
    updated one, but still nothing... Even tried jamming Doctor Shotgun''s Capybara-RP''s
    entire folder in minus the pytorch/safetensors files, since that seemed to be
    the closest link I could find, but still stops at 13% even then, and I had no
    issues with his model to merge with others. Last ditch effort and used the script
    you provided, and...nothing but more errors. *Face desk*


    I hate to ask this, but would it be too much trouble if you upload your Capybara
    version to huggingface for others looking to merge like me? I''d really be grateful.'
  created_at: 2024-01-16 06:12:35+00:00
  edited: false
  hidden: false
  id: 65a61e530a0bcd589642015b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-16T06:14:55.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.986039936542511
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Yeah sure, uploading in a sec.</p>

          <p>If its crashing at 13%, it does make the think its running out of memory
          though. That is coincidentally about when memory usage peaks for me.</p>

          '
        raw: 'Yeah sure, uploading in a sec.


          If its crashing at 13%, it does make the think its running out of memory
          though. That is coincidentally about when memory usage peaks for me.'
        updatedAt: '2024-01-16T06:14:55.884Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ParasiticRogue
    id: 65a61edfd81b6fa6c8b979b3
    type: comment
  author: brucethemoose
  content: 'Yeah sure, uploading in a sec.


    If its crashing at 13%, it does make the think its running out of memory though.
    That is coincidentally about when memory usage peaks for me.'
  created_at: 2024-01-16 06:14:55+00:00
  edited: false
  hidden: false
  id: 65a61edfd81b6fa6c8b979b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-16T06:46:29.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8348824381828308
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>It will take awhile, but be here when it''s done: <a href="https://huggingface.co/brucethemoose/Capybara-Fixed-Temp">https://huggingface.co/brucethemoose/Capybara-Fixed-Temp</a></p>

          '
        raw: 'It will take awhile, but be here when it''s done: https://huggingface.co/brucethemoose/Capybara-Fixed-Temp'
        updatedAt: '2024-01-16T06:46:29.813Z'
      numEdits: 0
      reactions: []
    id: 65a62645f85e73493e3e052e
    type: comment
  author: brucethemoose
  content: 'It will take awhile, but be here when it''s done: https://huggingface.co/brucethemoose/Capybara-Fixed-Temp'
  created_at: 2024-01-16 06:46:29+00:00
  edited: false
  hidden: false
  id: 65a62645f85e73493e3e052e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-16T06:54:34.000Z'
    data:
      edited: true
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9715409278869629
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>I''ll check it out later, thanks! Still... I don''t understand why
          the original would OOM for me though, considering I''m able to merge 3 models
          just fine, and that''s without applying lazy pickle...</p>

          '
        raw: I'll check it out later, thanks! Still... I don't understand why the
          original would OOM for me though, considering I'm able to merge 3 models
          just fine, and that's without applying lazy pickle...
        updatedAt: '2024-01-16T07:19:51.397Z'
      numEdits: 3
      reactions: []
    id: 65a6282a8a0485c12d01dcd2
    type: comment
  author: ParasiticRogue
  content: I'll check it out later, thanks! Still... I don't understand why the original
    would OOM for me though, considering I'm able to merge 3 models just fine, and
    that's without applying lazy pickle...
  created_at: 2024-01-16 06:54:34+00:00
  edited: true
  hidden: false
  id: 65a6282a8a0485c12d01dcd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-17T19:33:32.000Z'
    data:
      edited: false
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.964489758014679
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>Back with some feed...sorry, meant to say here''s some feedback.
          Also some findings which may or may not interest you in your merges.</p>

          <p>Was able to use the Capybara model without hanging on 13% but it also
          gave some some warnings about using another tokenizer. However,  just making
          a copy of it and then slerping the 2 into chargod''s Yi base solved the
          issue, and now I use that no problem. A separate issue with Nontoxic-Bagel
          getting killed at around 58% when doing wider merges was also happening,
          yet somehow being able to finish when there were only 2 models peasant.
          Like before, slerping the copies together fixed it and now it merges fine
          on a wider scale. It doesn''t seem to fix everything, since the Tess variants
          still produced the warning even when slerped together, unless union is used.
          Maybe the issues you had with Hermes could be fixed in this manner? It''s
          a technique I like to call self-slerp, not to be confused with self-su...actually,
          never mind.</p>

          <p>There''s also 2 extra smaller Vicuna models you could potentially use
          if you want.</p>

          <p>NobodyExistsOnTheInternet/Yi-34B-GiftedConvo-merged</p>

          <p>Sao10K/NyakuraV2-34B-Yi-Llama</p>

          <p>GiftedConvo doesn''t have anything too unique about it, since half of
          it''s data is already stuff Capybara uses. But the model is at least competent
          from some brief testing. Nya on the other hand is unique, but much like
          regular Capybara, it has some minor issues, mainly that it likes to use
          dollar $igns at the end for some reason, and I''ve encountered it 3 times
          thus far. So if you use it you might have to add "$" as a an extra custom
          stopping string. Hope you don''t use your models for economical purposes!
          So if you wanted some more slerp fodder for one of the SUS models, or even
          Hermes, you have options now.</p>

          <p>For now I''m tinkering with just a simple merge of 4 models, but do continue
          with your mega merge and trying to extend the conga-line and seeing where
          that goes.</p>

          '
        raw: 'Back with some feed...sorry, meant to say here''s some feedback. Also
          some findings which may or may not interest you in your merges.


          Was able to use the Capybara model without hanging on 13% but it also gave
          some some warnings about using another tokenizer. However,  just making
          a copy of it and then slerping the 2 into chargod''s Yi base solved the
          issue, and now I use that no problem. A separate issue with Nontoxic-Bagel
          getting killed at around 58% when doing wider merges was also happening,
          yet somehow being able to finish when there were only 2 models peasant.
          Like before, slerping the copies together fixed it and now it merges fine
          on a wider scale. It doesn''t seem to fix everything, since the Tess variants
          still produced the warning even when slerped together, unless union is used.
          Maybe the issues you had with Hermes could be fixed in this manner? It''s
          a technique I like to call self-slerp, not to be confused with self-su...actually,
          never mind.


          There''s also 2 extra smaller Vicuna models you could potentially use if
          you want.


          NobodyExistsOnTheInternet/Yi-34B-GiftedConvo-merged


          Sao10K/NyakuraV2-34B-Yi-Llama


          GiftedConvo doesn''t have anything too unique about it, since half of it''s
          data is already stuff Capybara uses. But the model is at least competent
          from some brief testing. Nya on the other hand is unique, but much like
          regular Capybara, it has some minor issues, mainly that it likes to use
          dollar $igns at the end for some reason, and I''ve encountered it 3 times
          thus far. So if you use it you might have to add "$" as a an extra custom
          stopping string. Hope you don''t use your models for economical purposes!
          So if you wanted some more slerp fodder for one of the SUS models, or even
          Hermes, you have options now.


          For now I''m tinkering with just a simple merge of 4 models, but do continue
          with your mega merge and trying to extend the conga-line and seeing where
          that goes.'
        updatedAt: '2024-01-17T19:33:32.924Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - brucethemoose
    id: 65a82b8c0237119dc5e83be2
    type: comment
  author: ParasiticRogue
  content: 'Back with some feed...sorry, meant to say here''s some feedback. Also
    some findings which may or may not interest you in your merges.


    Was able to use the Capybara model without hanging on 13% but it also gave some
    some warnings about using another tokenizer. However,  just making a copy of it
    and then slerping the 2 into chargod''s Yi base solved the issue, and now I use
    that no problem. A separate issue with Nontoxic-Bagel getting killed at around
    58% when doing wider merges was also happening, yet somehow being able to finish
    when there were only 2 models peasant. Like before, slerping the copies together
    fixed it and now it merges fine on a wider scale. It doesn''t seem to fix everything,
    since the Tess variants still produced the warning even when slerped together,
    unless union is used. Maybe the issues you had with Hermes could be fixed in this
    manner? It''s a technique I like to call self-slerp, not to be confused with self-su...actually,
    never mind.


    There''s also 2 extra smaller Vicuna models you could potentially use if you want.


    NobodyExistsOnTheInternet/Yi-34B-GiftedConvo-merged


    Sao10K/NyakuraV2-34B-Yi-Llama


    GiftedConvo doesn''t have anything too unique about it, since half of it''s data
    is already stuff Capybara uses. But the model is at least competent from some
    brief testing. Nya on the other hand is unique, but much like regular Capybara,
    it has some minor issues, mainly that it likes to use dollar $igns at the end
    for some reason, and I''ve encountered it 3 times thus far. So if you use it you
    might have to add "$" as a an extra custom stopping string. Hope you don''t use
    your models for economical purposes! So if you wanted some more slerp fodder for
    one of the SUS models, or even Hermes, you have options now.


    For now I''m tinkering with just a simple merge of 4 models, but do continue with
    your mega merge and trying to extend the conga-line and seeing where that goes.'
  created_at: 2024-01-17 19:33:32+00:00
  edited: false
  hidden: false
  id: 65a82b8c0237119dc5e83be2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-17T20:01:44.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9471059441566467
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Thanks, I will look into these.</p>

          <p>And I am sorry about the hanging in the middle of the merge. You can
          start a GH issue on mergekit or just chat with Charles on the Kobold AI
          discord to try and figure that out.</p>

          '
        raw: 'Thanks, I will look into these.


          And I am sorry about the hanging in the middle of the merge. You can start
          a GH issue on mergekit or just chat with Charles on the Kobold AI discord
          to try and figure that out.'
        updatedAt: '2024-01-17T20:03:16.487Z'
      numEdits: 1
      reactions: []
    id: 65a83228f84e0455900e80d5
    type: comment
  author: brucethemoose
  content: 'Thanks, I will look into these.


    And I am sorry about the hanging in the middle of the merge. You can start a GH
    issue on mergekit or just chat with Charles on the Kobold AI discord to try and
    figure that out.'
  created_at: 2024-01-17 20:01:44+00:00
  edited: true
  hidden: false
  id: 65a83228f84e0455900e80d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
      fullname: PB & J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParasiticRogue
      type: user
    createdAt: '2024-01-17T20:11:57.000Z'
    data:
      edited: false
      editors:
      - ParasiticRogue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9959411025047302
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/657ac4daf10a984150b1a679/ynZtPbuL3bUC73eC3k70e.png?w=200&h=200&f=face
          fullname: PB & J
          isHf: false
          isPro: false
          name: ParasiticRogue
          type: user
        html: '<p>It''s not that big of a deal honestly, since I was able to fix it,
          even if the method was kinda scuffed, lol.</p>

          '
        raw: It's not that big of a deal honestly, since I was able to fix it, even
          if the method was kinda scuffed, lol.
        updatedAt: '2024-01-17T20:11:57.724Z'
      numEdits: 0
      reactions: []
    id: 65a8348dc0e51d82e69518fb
    type: comment
  author: ParasiticRogue
  content: It's not that big of a deal honestly, since I was able to fix it, even
    if the method was kinda scuffed, lol.
  created_at: 2024-01-17 20:11:57+00:00
  edited: false
  hidden: false
  id: 65a8348dc0e51d82e69518fb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: brucethemoose/jondurbin_bagel-dpo-34b-v0.2-exl2-4bpw-fiction
repo_type: model
status: open
target_branch: null
title: Have you considered revisiting another pure Vicuna merge using nontoxic-bagel?
