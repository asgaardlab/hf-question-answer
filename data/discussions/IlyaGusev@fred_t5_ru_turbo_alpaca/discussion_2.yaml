!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Superchik
conflicting_files: null
created_at: 2023-10-31 16:23:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7602462dd75b22ff3a6ed4feb98c225.svg
      fullname: Alexander Klochay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Superchik
      type: user
    createdAt: '2023-10-31T17:23:25.000Z'
    data:
      edited: false
      editors:
      - Superchik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.836881160736084
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7602462dd75b22ff3a6ed4feb98c225.svg
          fullname: Alexander Klochay
          isHf: false
          isPro: false
          name: Superchik
          type: user
        html: "<p>from transformers import AutoTokenizer<br>from transformers import\
          \ GenerationConfig</p>\n<p>model_answer_name = \"IlyaGusev/fred_t5_ru_turbo_alpaca\"\
          <br>generation_config_answer = GenerationConfig.from_pretrained(model_answer_name)</p>\n\
          <h1 id=\"userwarning-do_sample-is-set-to-false-however-top_p-is-set-to-09----this-flag-is-only-used-in-sample-based-generation-modes-you-should-set-do_sampletrue-or-unset-top_p-this-was-detected-when-initializing-the-generation-config-instance-which-means-the-corresponding-file-may-hold-incorrect-parameterization-and-should-be-fixed\"\
          >UserWarning: <code>do_sample</code> is set to <code>False</code>. However,\
          \ <code>top_p</code> is set to <code>0.9</code> -- this flag is only used\
          \ in sample-based generation modes. You should set <code>do_sample=True</code>\
          \ or unset <code>top_p</code>. This was detected when initializing the generation\
          \ config instance, which means the corresponding file may hold incorrect\
          \ parameterization and should be fixed.</h1>\n<h1 id=\"warningswarn\">warnings.warn(</h1>\n\
          <p>tokenizer_answer = AutoTokenizer.from_pretrained(model_answer_name)</p>\n\
          <h1 id=\"special-tokens-have-been-added-in-the-vocabulary-make-sure-the-associated-word-embeddings-are-fine-tuned-or-trained\"\
          >Special tokens have been added in the vocabulary, make sure the associated\
          \ word embeddings are fine-tuned or trained.</h1>\n<p>Console output:</p>\n\
          <pre><code class=\"language-python\">...\\venv\\lib\\site-packages\\transformers\\\
          generation\\configuration_utils.py:<span class=\"hljs-number\">367</span>:\
          \ UserWarning: `do_sample` <span class=\"hljs-keyword\">is</span> <span\
          \ class=\"hljs-built_in\">set</span> to `<span class=\"hljs-literal\">False</span>`.\
          \ However, `top_p` <span class=\"hljs-keyword\">is</span> <span class=\"\
          hljs-built_in\">set</span> to `<span class=\"hljs-number\">0.9</span>` --\
          \ this flag <span class=\"hljs-keyword\">is</span> only used <span class=\"\
          hljs-keyword\">in</span> sample-based generation modes. You should <span\
          \ class=\"hljs-built_in\">set</span> `do_sample=<span class=\"hljs-literal\"\
          >True</span>` <span class=\"hljs-keyword\">or</span> unset `top_p`. This\
          \ was detected when initializing the generation config instance, which means\
          \ the corresponding file may hold incorrect parameterization <span class=\"\
          hljs-keyword\">and</span> should be fixed.\n  warnings.warn(\nSpecial tokens\
          \ have been added <span class=\"hljs-keyword\">in</span> the vocabulary,\
          \ make sure the associated word embeddings are fine-tuned <span class=\"\
          hljs-keyword\">or</span> trained.\n</code></pre>\n"
        raw: "from transformers import AutoTokenizer\r\nfrom transformers import GenerationConfig\r\
          \n\r\nmodel_answer_name = \"IlyaGusev/fred_t5_ru_turbo_alpaca\"\r\ngeneration_config_answer\
          \ = GenerationConfig.from_pretrained(model_answer_name)\r\n# UserWarning:\
          \ `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this\
          \ flag is only used in sample-based generation modes. You should set `do_sample=True`\
          \ or unset `top_p`. This was detected when initializing the generation config\
          \ instance, which means the corresponding file may hold incorrect parameterization\
          \ and should be fixed.\r\n#   warnings.warn(\r\n\r\ntokenizer_answer = AutoTokenizer.from_pretrained(model_answer_name)\r\
          \n# Special tokens have been added in the vocabulary, make sure the associated\
          \ word embeddings are fine-tuned or trained.\r\n\r\nConsole output:\r\n\
          ```python\r\n...\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367:\
          \ UserWarning: `do_sample` is set to `False`. However, `top_p` is set to\
          \ `0.9` -- this flag is only used in sample-based generation modes. You\
          \ should set `do_sample=True` or unset `top_p`. This was detected when initializing\
          \ the generation config instance, which means the corresponding file may\
          \ hold incorrect parameterization and should be fixed.\r\n  warnings.warn(\r\
          \nSpecial tokens have been added in the vocabulary, make sure the associated\
          \ word embeddings are fine-tuned or trained.\r\n```"
        updatedAt: '2023-10-31T17:23:25.470Z'
      numEdits: 0
      reactions: []
    id: 6541380d4cb81ed6dea5d1fb
    type: comment
  author: Superchik
  content: "from transformers import AutoTokenizer\r\nfrom transformers import GenerationConfig\r\
    \n\r\nmodel_answer_name = \"IlyaGusev/fred_t5_ru_turbo_alpaca\"\r\ngeneration_config_answer\
    \ = GenerationConfig.from_pretrained(model_answer_name)\r\n# UserWarning: `do_sample`\
    \ is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used\
    \ in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\
    \ This was detected when initializing the generation config instance, which means\
    \ the corresponding file may hold incorrect parameterization and should be fixed.\r\
    \n#   warnings.warn(\r\n\r\ntokenizer_answer = AutoTokenizer.from_pretrained(model_answer_name)\r\
    \n# Special tokens have been added in the vocabulary, make sure the associated\
    \ word embeddings are fine-tuned or trained.\r\n\r\nConsole output:\r\n```python\r\
    \n...\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367:\
    \ UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9`\
    \ -- this flag is only used in sample-based generation modes. You should set `do_sample=True`\
    \ or unset `top_p`. This was detected when initializing the generation config\
    \ instance, which means the corresponding file may hold incorrect parameterization\
    \ and should be fixed.\r\n  warnings.warn(\r\nSpecial tokens have been added in\
    \ the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\
    \n```"
  created_at: 2023-10-31 16:23:25+00:00
  edited: false
  hidden: false
  id: 6541380d4cb81ed6dea5d1fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d7602462dd75b22ff3a6ed4feb98c225.svg
      fullname: Alexander Klochay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Superchik
      type: user
    createdAt: '2023-11-17T00:00:22.000Z'
    data:
      status: closed
    id: 6556ad169dfd3ef873434ad5
    type: status-change
  author: Superchik
  created_at: 2023-11-17 00:00:22+00:00
  id: 6556ad169dfd3ef873434ad5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: IlyaGusev/fred_t5_ru_turbo_alpaca
repo_type: model
status: closed
target_branch: null
title: Some warnings
