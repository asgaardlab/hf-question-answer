!!python/object:huggingface_hub.community.DiscussionWithDetails
author: appleatiger
conflicting_files: null
created_at: 2023-09-24 01:32:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b017885cd56110e7f72a509bda6d82ab.svg
      fullname: tiger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appleatiger
      type: user
    createdAt: '2023-09-24T02:32:33.000Z'
    data:
      edited: true
      editors:
      - appleatiger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.39201152324676514
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b017885cd56110e7f72a509bda6d82ab.svg
          fullname: tiger
          isHf: false
          isPro: false
          name: appleatiger
          type: user
        html: "<p>\u90E8\u7F72\u65B9\u6CD5\u5982\u4E0B\uFF1A<br>1.\u521B\u5EFAdockerfile\uFF1A\
          <br>FROM nvcr.io/nvidia/pytorch:22.12-py3<br>WORKDIR /workspace/<br>ENV\
          \ PYTHONPATH /workspace/<br>RUN pip uninstall -y opencv-python<br>RUN pip\
          \ install opencv-python==4.8.0.74 -i <a rel=\"nofollow\" href=\"https://pypi.tuna.tsinghua.edu.cn/simple\"\
          >https://pypi.tuna.tsinghua.edu.cn/simple</a><br>RUN pip install lmdeploy&gt;=0.0.9\
          \ -i <a rel=\"nofollow\" href=\"https://pypi.tuna.tsinghua.edu.cn/simple\"\
          >https://pypi.tuna.tsinghua.edu.cn/simple</a></p>\n<p>2.\u4E0B\u8F7Dmodel<br>git\
          \ clone <a href=\"https://huggingface.co/lmdeploy/turbomind-internlm-chat-20b-w4\"\
          >https://huggingface.co/lmdeploy/turbomind-internlm-chat-20b-w4</a><br>\u628A\
          \u76EE\u5F55turbomind-internlm-chat-20b-w4\u6539\u4E3Aworkspace<br>\u6216\
          \u8005<br>git clone <a href=\"https://huggingface.co/internlm/internlm-chat-20b-4bit\"\
          >https://huggingface.co/internlm/internlm-chat-20b-4bit</a><br>python3 -m\
          \ lmdeploy.serve.turbomind.deploy <br>    --model-name internlm-chat-20b\
          \ <br>    --model-path ./internlm-chat-20b-4bit <br>    --model-format awq\
          \ <br>    --group-size 128</p>\n<p>3.\u8FD0\u884Cdocker\u5BB9\u5668\uFF1A\
          <br>docker build -f docker/Dockerfile -t internlm .<br>docker run -it -d\
          \ --gpus all --ipc=host -p 7891:8000 --name=internlm --ulimit memlock=-1\
          \ --ulimit stack=67108864 -v D:\\ai\\internlm:/workspace internlm</p>\n\
          <p>4.\u5728\u5BB9\u5668\u4E2D\u6267\u884C\u547D\u4EE4\u542F\u52A8lmdeploy\uFF1A\
          <br>python3 -m lmdeploy.serve.gradio.app ./workspace 0.0.0.0 8000</p>\n\
          <p>5.\u8FD0\u884C\u7ED3\u679C\uFF1A<br>root@ec0690884cd4:/workspace# python3\
          \ -m lmdeploy.serve.gradio.app ./workspace 0.0.0.0 8000<br>WARNING: Can\
          \ not find tokenizer.json. It may take long time to initialize the tokenizer.<br>[TM][INFO]\
          \ Barrier(1)<br>[TM][INFO] Barrier(1)<br>[WARNING] gemm_config.in is not\
          \ found; using default GEMM algo<br>[TM][INFO] NCCL group_id = 0<br>[TM][INFO]\
          \ [LlamaCacheManager] max_entry_count = 48<br>[TM][INFO] [LlamaCacheManager]\
          \ chunk_size = 1<br>[TM][INFO] [LlamaCacheManager][allocate]<br>[TM][INFO]\
          \ [LlamaCacheManager][allocate] malloc 1<br>[TM][INFO] [LlamaCacheManager][allocate]\
          \ count = 1<br>[TM][INFO] [LlamaCacheManager][allocate] free = 1<br>[TM][INFO]\
          \ [internalThreadEntry] 0<br>[TM][INFO] Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO]\
          \ Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO]\
          \ Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO]\
          \ Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO]\
          \ Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO]\
          \ Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO]\
          \ Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO]\
          \ Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO]\
          \ Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO]\
          \ Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO] Barrier(1)<br>[TM][INFO]\
          \ Barrier(1)<br>[TM][INFO] Barrier(1)<br>server is gonna mount on: <a rel=\"\
          nofollow\" href=\"http://0.0.0.0:8000\">http://0.0.0.0:8000</a><br>Running\
          \ on local URL:  <a rel=\"nofollow\" href=\"http://0.0.0.0:8000\">http://0.0.0.0:8000</a></p>\n\
          <p>6.\u6253\u5F00\u6D4F\u89C8\u5668\u8BBF\u95EELMDeploy Playground\uFF1A\
          <br><a rel=\"nofollow\" href=\"http://127.0.0.1:7891/\">http://127.0.0.1:7891/</a></p>\n\
          <p>7.\u4EFB\u610F\u95EE\u9898\u8BBF\u95EE\u7ED3\u679C\uFF1A<br>[TM][INFO]\
          \ [forward] Enqueue requests<br>[TM][INFO] [forward] Wait for requests to\
          \ complete ...<br>[TM][WARNING] [verifyRequests] Skipping invalid infer\
          \ request for id 1721701, code = 1<br>[TM][INFO] [forward] Enqueue requests<br>[TM][INFO]\
          \ [forward] Wait for requests to complete ...<br>[TM][INFO] [synchronize]\
          \ batch_size = 0<br>[TM][INFO] [LlamaCacheManager][create] 1721701<br>[TM][INFO]\
          \ [LlamaCacheManager][allocate]<br>[TM][INFO] [LlamaCacheManager][allocate]\
          \ free = 0<br>[TM][INFO] [init] infer_request_count = 1<br>[TM][INFO] [init]\
          \ batch_size = 1<br>[TM][INFO] [init] session_len = 2056<br>[TM][INFO] [init]\
          \ max_input_length = 14<br>[TM][INFO] [init] max_context_len = 14<br>[TM][INFO]\
          \ [init] slot  sequence_id  history_len  input_len  context_len  tmp_input_len\
          \  token_ids.size  cache_len<br>[TM][INFO] [init]    0      1721701    \
          \        0         14           14             14               0      \
          \    0<br>[TM][INFO] [decodeContext] base = 0, count = 1<br>[TM][INFO] [decodeContext]\
          \ offset = 0, batch_size = 1, token_num = 13, max_input_len = 13, max_context_len\
          \ = 13<br>[TM][INFO] context decoding start</p>\n<p>8.\u6700\u7EC8\u72B6\
          \u6001\u63CF\u8FF0\uFF1A<br>\u7F51\u9875\u65E0\u4EFB\u4F55\u54CD\u5E94\uFF0C\
          \u5361\u6B7B\u3002\u6B64\u65F6\u663E\u5B58\u5360\u7528\u4E3A16G\u3002</p>\n"
        raw: "\u90E8\u7F72\u65B9\u6CD5\u5982\u4E0B\uFF1A\n1.\u521B\u5EFAdockerfile\uFF1A\
          \nFROM nvcr.io/nvidia/pytorch:22.12-py3\nWORKDIR /workspace/\nENV PYTHONPATH\
          \ /workspace/\nRUN pip uninstall -y opencv-python\nRUN pip install opencv-python==4.8.0.74\
          \ -i https://pypi.tuna.tsinghua.edu.cn/simple\nRUN pip install lmdeploy>=0.0.9\
          \ -i https://pypi.tuna.tsinghua.edu.cn/simple\n\n2.\u4E0B\u8F7Dmodel\ngit\
          \ clone https://huggingface.co/lmdeploy/turbomind-internlm-chat-20b-w4\n\
          \u628A\u76EE\u5F55turbomind-internlm-chat-20b-w4\u6539\u4E3Aworkspace\n\u6216\
          \u8005\ngit clone https://huggingface.co/internlm/internlm-chat-20b-4bit\n\
          python3 -m lmdeploy.serve.turbomind.deploy \\\n    --model-name internlm-chat-20b\
          \ \\\n    --model-path ./internlm-chat-20b-4bit \\\n    --model-format awq\
          \ \\\n    --group-size 128\n\n3.\u8FD0\u884Cdocker\u5BB9\u5668\uFF1A\ndocker\
          \ build -f docker/Dockerfile -t internlm .\ndocker run -it -d --gpus all\
          \ --ipc=host -p 7891:8000 --name=internlm --ulimit memlock=-1 --ulimit stack=67108864\
          \ -v D:\\ai\\internlm:/workspace internlm\n\n4.\u5728\u5BB9\u5668\u4E2D\u6267\
          \u884C\u547D\u4EE4\u542F\u52A8lmdeploy\uFF1A\npython3 -m lmdeploy.serve.gradio.app\
          \ ./workspace 0.0.0.0 8000\n\n5.\u8FD0\u884C\u7ED3\u679C\uFF1A\nroot@ec0690884cd4:/workspace#\
          \ python3 -m lmdeploy.serve.gradio.app ./workspace 0.0.0.0 8000\nWARNING:\
          \ Can not find tokenizer.json. It may take long time to initialize the tokenizer.\n\
          [TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[WARNING] gemm_config.in is\
          \ not found; using default GEMM algo\n[TM][INFO] NCCL group_id = 0\n[TM][INFO]\
          \ [LlamaCacheManager] max_entry_count = 48\n[TM][INFO] [LlamaCacheManager]\
          \ chunk_size = 1\n[TM][INFO] [LlamaCacheManager][allocate]\n[TM][INFO] [LlamaCacheManager][allocate]\
          \ malloc 1\n[TM][INFO] [LlamaCacheManager][allocate] count = 1\n[TM][INFO]\
          \ [LlamaCacheManager][allocate] free = 1\n[TM][INFO] [internalThreadEntry]\
          \ 0\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n\
          [TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO]\
          \ Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n\
          [TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO]\
          \ Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n\
          [TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO]\
          \ Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n\
          [TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO]\
          \ Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n\
          server is gonna mount on: http://0.0.0.0:8000\nRunning on local URL:  http://0.0.0.0:8000\n\
          \n6.\u6253\u5F00\u6D4F\u89C8\u5668\u8BBF\u95EELMDeploy Playground\uFF1A\n\
          http://127.0.0.1:7891/\n\n7.\u4EFB\u610F\u95EE\u9898\u8BBF\u95EE\u7ED3\u679C\
          \uFF1A\n[TM][INFO] [forward] Enqueue requests\n[TM][INFO] [forward] Wait\
          \ for requests to complete ...\n[TM][WARNING] [verifyRequests] Skipping\
          \ invalid infer request for id 1721701, code = 1\n[TM][INFO] [forward] Enqueue\
          \ requests\n[TM][INFO] [forward] Wait for requests to complete ...\n[TM][INFO]\
          \ [synchronize] batch_size = 0\n[TM][INFO] [LlamaCacheManager][create] 1721701\n\
          [TM][INFO] [LlamaCacheManager][allocate]\n[TM][INFO] [LlamaCacheManager][allocate]\
          \ free = 0\n[TM][INFO] [init] infer_request_count = 1\n[TM][INFO] [init]\
          \ batch_size = 1\n[TM][INFO] [init] session_len = 2056\n[TM][INFO] [init]\
          \ max_input_length = 14\n[TM][INFO] [init] max_context_len = 14\n[TM][INFO]\
          \ [init] slot  sequence_id  history_len  input_len  context_len  tmp_input_len\
          \  token_ids.size  cache_len\n[TM][INFO] [init]    0      1721701      \
          \      0         14           14             14               0        \
          \  0\n[TM][INFO] [decodeContext] base = 0, count = 1\n[TM][INFO] [decodeContext]\
          \ offset = 0, batch_size = 1, token_num = 13, max_input_len = 13, max_context_len\
          \ = 13\n[TM][INFO] context decoding start\n\n8.\u6700\u7EC8\u72B6\u6001\u63CF\
          \u8FF0\uFF1A\n\u7F51\u9875\u65E0\u4EFB\u4F55\u54CD\u5E94\uFF0C\u5361\u6B7B\
          \u3002\u6B64\u65F6\u663E\u5B58\u5360\u7528\u4E3A16G\u3002\n\n"
        updatedAt: '2023-09-24T02:52:46.518Z'
      numEdits: 2
      reactions: []
    id: 650f9fc1b63668f44825b5a4
    type: comment
  author: appleatiger
  content: "\u90E8\u7F72\u65B9\u6CD5\u5982\u4E0B\uFF1A\n1.\u521B\u5EFAdockerfile\uFF1A\
    \nFROM nvcr.io/nvidia/pytorch:22.12-py3\nWORKDIR /workspace/\nENV PYTHONPATH /workspace/\n\
    RUN pip uninstall -y opencv-python\nRUN pip install opencv-python==4.8.0.74 -i\
    \ https://pypi.tuna.tsinghua.edu.cn/simple\nRUN pip install lmdeploy>=0.0.9 -i\
    \ https://pypi.tuna.tsinghua.edu.cn/simple\n\n2.\u4E0B\u8F7Dmodel\ngit clone https://huggingface.co/lmdeploy/turbomind-internlm-chat-20b-w4\n\
    \u628A\u76EE\u5F55turbomind-internlm-chat-20b-w4\u6539\u4E3Aworkspace\n\u6216\u8005\
    \ngit clone https://huggingface.co/internlm/internlm-chat-20b-4bit\npython3 -m\
    \ lmdeploy.serve.turbomind.deploy \\\n    --model-name internlm-chat-20b \\\n\
    \    --model-path ./internlm-chat-20b-4bit \\\n    --model-format awq \\\n   \
    \ --group-size 128\n\n3.\u8FD0\u884Cdocker\u5BB9\u5668\uFF1A\ndocker build -f\
    \ docker/Dockerfile -t internlm .\ndocker run -it -d --gpus all --ipc=host -p\
    \ 7891:8000 --name=internlm --ulimit memlock=-1 --ulimit stack=67108864 -v D:\\\
    ai\\internlm:/workspace internlm\n\n4.\u5728\u5BB9\u5668\u4E2D\u6267\u884C\u547D\
    \u4EE4\u542F\u52A8lmdeploy\uFF1A\npython3 -m lmdeploy.serve.gradio.app ./workspace\
    \ 0.0.0.0 8000\n\n5.\u8FD0\u884C\u7ED3\u679C\uFF1A\nroot@ec0690884cd4:/workspace#\
    \ python3 -m lmdeploy.serve.gradio.app ./workspace 0.0.0.0 8000\nWARNING: Can\
    \ not find tokenizer.json. It may take long time to initialize the tokenizer.\n\
    [TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[WARNING] gemm_config.in is not\
    \ found; using default GEMM algo\n[TM][INFO] NCCL group_id = 0\n[TM][INFO] [LlamaCacheManager]\
    \ max_entry_count = 48\n[TM][INFO] [LlamaCacheManager] chunk_size = 1\n[TM][INFO]\
    \ [LlamaCacheManager][allocate]\n[TM][INFO] [LlamaCacheManager][allocate] malloc\
    \ 1\n[TM][INFO] [LlamaCacheManager][allocate] count = 1\n[TM][INFO] [LlamaCacheManager][allocate]\
    \ free = 1\n[TM][INFO] [internalThreadEntry] 0\n[TM][INFO] Barrier(1)\n[TM][INFO]\
    \ Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n\
    [TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO]\
    \ Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n\
    [TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO]\
    \ Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n\
    [TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO]\
    \ Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n\
    [TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO] Barrier(1)\n[TM][INFO]\
    \ Barrier(1)\n[TM][INFO] Barrier(1)\nserver is gonna mount on: http://0.0.0.0:8000\n\
    Running on local URL:  http://0.0.0.0:8000\n\n6.\u6253\u5F00\u6D4F\u89C8\u5668\
    \u8BBF\u95EELMDeploy Playground\uFF1A\nhttp://127.0.0.1:7891/\n\n7.\u4EFB\u610F\
    \u95EE\u9898\u8BBF\u95EE\u7ED3\u679C\uFF1A\n[TM][INFO] [forward] Enqueue requests\n\
    [TM][INFO] [forward] Wait for requests to complete ...\n[TM][WARNING] [verifyRequests]\
    \ Skipping invalid infer request for id 1721701, code = 1\n[TM][INFO] [forward]\
    \ Enqueue requests\n[TM][INFO] [forward] Wait for requests to complete ...\n[TM][INFO]\
    \ [synchronize] batch_size = 0\n[TM][INFO] [LlamaCacheManager][create] 1721701\n\
    [TM][INFO] [LlamaCacheManager][allocate]\n[TM][INFO] [LlamaCacheManager][allocate]\
    \ free = 0\n[TM][INFO] [init] infer_request_count = 1\n[TM][INFO] [init] batch_size\
    \ = 1\n[TM][INFO] [init] session_len = 2056\n[TM][INFO] [init] max_input_length\
    \ = 14\n[TM][INFO] [init] max_context_len = 14\n[TM][INFO] [init] slot  sequence_id\
    \  history_len  input_len  context_len  tmp_input_len  token_ids.size  cache_len\n\
    [TM][INFO] [init]    0      1721701            0         14           14     \
    \        14               0          0\n[TM][INFO] [decodeContext] base = 0, count\
    \ = 1\n[TM][INFO] [decodeContext] offset = 0, batch_size = 1, token_num = 13,\
    \ max_input_len = 13, max_context_len = 13\n[TM][INFO] context decoding start\n\
    \n8.\u6700\u7EC8\u72B6\u6001\u63CF\u8FF0\uFF1A\n\u7F51\u9875\u65E0\u4EFB\u4F55\
    \u54CD\u5E94\uFF0C\u5361\u6B7B\u3002\u6B64\u65F6\u663E\u5B58\u5360\u7528\u4E3A\
    16G\u3002\n\n"
  created_at: 2023-09-24 01:32:33+00:00
  edited: true
  hidden: false
  id: 650f9fc1b63668f44825b5a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/b017885cd56110e7f72a509bda6d82ab.svg
      fullname: tiger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appleatiger
      type: user
    createdAt: '2023-09-24T02:51:19.000Z'
    data:
      from: "[Bug] win11\u4E0B\u4F7F\u7528docker\u90E8\u7F72internlm-chat-20b-4bit\uFF0C\
        \u663E\u793AWARNING: Can not find tokenizer.json. It may take long time to\
        \ initialize the tokenizer.\u63D0\u95EE\u65F6\u65E0\u53CD\u9988\u5361\u6B7B\
        \u3002"
      to: "[Bug] win11\u4E0Bgpu4090\u4F7F\u7528docker\u90E8\u7F72internlm-chat-20b-4bit\uFF0C\
        \u663E\u793AWARNING: Can not find tokenizer.json. It may take long time to\
        \ initialize the tokenizer.\u63D0\u95EE\u65F6\u65E0\u53CD\u9988\u5361\u6B7B\
        \u3002"
    id: 650fa427df31b1809f48118a
    type: title-change
  author: appleatiger
  created_at: 2023-09-24 01:51:19+00:00
  id: 650fa427df31b1809f48118a
  new_title: "[Bug] win11\u4E0Bgpu4090\u4F7F\u7528docker\u90E8\u7F72internlm-chat-20b-4bit\uFF0C\
    \u663E\u793AWARNING: Can not find tokenizer.json. It may take long time to initialize\
    \ the tokenizer.\u63D0\u95EE\u65F6\u65E0\u53CD\u9988\u5361\u6B7B\u3002"
  old_title: "[Bug] win11\u4E0B\u4F7F\u7528docker\u90E8\u7F72internlm-chat-20b-4bit\uFF0C\
    \u663E\u793AWARNING: Can not find tokenizer.json. It may take long time to initialize\
    \ the tokenizer.\u63D0\u95EE\u65F6\u65E0\u53CD\u9988\u5361\u6B7B\u3002"
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/b017885cd56110e7f72a509bda6d82ab.svg
      fullname: tiger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appleatiger
      type: user
    createdAt: '2023-09-24T02:51:29.000Z'
    data:
      from: "[Bug] win11\u4E0Bgpu4090\u4F7F\u7528docker\u90E8\u7F72internlm-chat-20b-4bit\uFF0C\
        \u663E\u793AWARNING: Can not find tokenizer.json. It may take long time to\
        \ initialize the tokenizer.\u63D0\u95EE\u65F6\u65E0\u53CD\u9988\u5361\u6B7B\
        \u3002"
      to: "[Bug] win11\u4E0Bgpu\uFF1A4090\u4F7F\u7528docker\u90E8\u7F72internlm-chat-20b-4bit\uFF0C\
        \u663E\u793AWARNING: Can not find tokenizer.json. It may take long time to\
        \ initialize the tokenizer.\u63D0\u95EE\u65F6\u65E0\u53CD\u9988\u5361\u6B7B\
        \u3002"
    id: 650fa431871fed2307d970db
    type: title-change
  author: appleatiger
  created_at: 2023-09-24 01:51:29+00:00
  id: 650fa431871fed2307d970db
  new_title: "[Bug] win11\u4E0Bgpu\uFF1A4090\u4F7F\u7528docker\u90E8\u7F72internlm-chat-20b-4bit\uFF0C\
    \u663E\u793AWARNING: Can not find tokenizer.json. It may take long time to initialize\
    \ the tokenizer.\u63D0\u95EE\u65F6\u65E0\u53CD\u9988\u5361\u6B7B\u3002"
  old_title: "[Bug] win11\u4E0Bgpu4090\u4F7F\u7528docker\u90E8\u7F72internlm-chat-20b-4bit\uFF0C\
    \u663E\u793AWARNING: Can not find tokenizer.json. It may take long time to initialize\
    \ the tokenizer.\u63D0\u95EE\u65F6\u65E0\u53CD\u9988\u5361\u6B7B\u3002"
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f6279ee9e9f6a5b2f6794c5a394ef09.svg
      fullname: lmdeploy
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: unsubscribe
      type: user
    createdAt: '2023-10-09T03:06:11.000Z'
    data:
      edited: false
      editors:
      - unsubscribe
      hidden: false
      identifiedLanguage:
        language: ja
        probability: 0.4313032627105713
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f6279ee9e9f6a5b2f6794c5a394ef09.svg
          fullname: lmdeploy
          isHf: false
          isPro: false
          name: unsubscribe
          type: user
        html: "<p><em>lmdeploy/turbomind/chat.py</em> \u811A\u672C\u8DD1\u5462\uFF1F\
          </p>\n"
        raw: "*lmdeploy/turbomind/chat.py* \u811A\u672C\u8DD1\u5462\uFF1F"
        updatedAt: '2023-10-09T03:06:11.330Z'
      numEdits: 0
      reactions: []
    id: 65236e23a7716548c5eba2f5
    type: comment
  author: unsubscribe
  content: "*lmdeploy/turbomind/chat.py* \u811A\u672C\u8DD1\u5462\uFF1F"
  created_at: 2023-10-09 02:06:11+00:00
  edited: false
  hidden: false
  id: 65236e23a7716548c5eba2f5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: internlm/internlm-chat-20b-4bit
repo_type: model
status: open
target_branch: null
title: "[Bug] win11\u4E0Bgpu\uFF1A4090\u4F7F\u7528docker\u90E8\u7F72internlm-chat-20b-4bit\uFF0C\
  \u663E\u793AWARNING: Can not find tokenizer.json. It may take long time to initialize\
  \ the tokenizer.\u63D0\u95EE\u65F6\u65E0\u53CD\u9988\u5361\u6B7B\u3002"
