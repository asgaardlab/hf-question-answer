!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KeepKool
conflicting_files: null
created_at: 2022-07-25 18:17:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e39ec06d143c7c2886e3f9298b232db.svg
      fullname: Laurent
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KeepKool
      type: user
    createdAt: '2022-07-25T19:17:34.000Z'
    data:
      edited: false
      editors:
      - KeepKool
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e39ec06d143c7c2886e3f9298b232db.svg
          fullname: Laurent
          isHf: false
          isPro: false
          name: KeepKool
          type: user
        html: '<p>How to use it ? All my trials lead to : out = in.   Does it need
          a prompt with an example ? </p>

          '
        raw: "How to use it ? All my trials lead to : out = in.   Does it need a prompt\
          \ with an example ? \r\n"
        updatedAt: '2022-07-25T19:17:34.722Z'
      numEdits: 0
      reactions: []
    id: 62deec4ea8ccfacec7128c72
    type: comment
  author: KeepKool
  content: "How to use it ? All my trials lead to : out = in.   Does it need a prompt\
    \ with an example ? \r\n"
  created_at: 2022-07-25 18:17:34+00:00
  edited: false
  hidden: false
  id: 62deec4ea8ccfacec7128c72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb9a49571a978c1ec45e939504c81c2d.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: yelpfeast
      type: user
    createdAt: '2022-07-25T19:46:22.000Z'
    data:
      edited: false
      editors:
      - yelpfeast
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb9a49571a978c1ec45e939504c81c2d.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: yelpfeast
          type: user
        html: "<p>The model should work when loaded with T5ForConditionalGeneration.</p>\n\
          <p>Are you using this example code in the readme? And what inputs did you\
          \ use?</p>\n<pre><code class=\"language-python\">\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> T5ForConditionalGeneration,\
          \ AutoTokenizer\n\n\nmodel = T5ForConditionalGeneration.from_pretrained(<span\
          \ class=\"hljs-string\">'yelpfeast/byt5-base-english-ocr-correction'</span>)\n\
          tokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">\"\
          yelpfeast/byt5-base-english-ocr-correction\"</span>)\n\ninputs = tokenizer(<span\
          \ class=\"hljs-string\">\"I l0ve anima1s\"</span>, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>, padding=<span class=\"hljs-literal\"\
          >True</span>)\n\noutput_sequences = model.generate(\n\n    input_ids=inputs[<span\
          \ class=\"hljs-string\">\"input_ids\"</span>],\n\n    attention_mask=inputs[<span\
          \ class=\"hljs-string\">\"attention_mask\"</span>],\n\n    do_sample=<span\
          \ class=\"hljs-literal\">False</span>,  <span class=\"hljs-comment\"># disable\
          \ sampling to test if batching affects output</span>\n\n)\n\n<span class=\"\
          hljs-built_in\">print</span>(tokenizer.batch_decode(output_sequences, skip_special_tokens=<span\
          \ class=\"hljs-literal\">True</span>))\n</code></pre>\n<p>Also not all inputs\
          \ may be corrected. Since the model was trained on synthetically corrupted\
          \ data I assume that if the input text is very different to the training\
          \ the data the model might just output the same text unaltered. I aim to\
          \ improve the model at some point by training on a larger dataset with different\
          \ levels of corrupted text to make it work on a wider variety of input text.</p>\n"
        raw: "The model should work when loaded with T5ForConditionalGeneration.\n\
          \nAre you using this example code in the readme? And what inputs did you\
          \ use?\n\n```python\n\nfrom transformers import T5ForConditionalGeneration,\
          \ AutoTokenizer\n\n\nmodel = T5ForConditionalGeneration.from_pretrained('yelpfeast/byt5-base-english-ocr-correction')\n\
          tokenizer = AutoTokenizer.from_pretrained(\"yelpfeast/byt5-base-english-ocr-correction\"\
          )\n\ninputs = tokenizer(\"I l0ve anima1s\", return_tensors=\"pt\", padding=True)\n\
          \noutput_sequences = model.generate(\n\n    input_ids=inputs[\"input_ids\"\
          ],\n\n    attention_mask=inputs[\"attention_mask\"],\n\n    do_sample=False,\
          \  # disable sampling to test if batching affects output\n\n)\n\nprint(tokenizer.batch_decode(output_sequences,\
          \ skip_special_tokens=True))\n```\nAlso not all inputs may be corrected.\
          \ Since the model was trained on synthetically corrupted data I assume that\
          \ if the input text is very different to the training the data the model\
          \ might just output the same text unaltered. I aim to improve the model\
          \ at some point by training on a larger dataset with different levels of\
          \ corrupted text to make it work on a wider variety of input text."
        updatedAt: '2022-07-25T19:46:22.294Z'
      numEdits: 0
      reactions: []
    id: 62def30ee89c1e5797b4b938
    type: comment
  author: yelpfeast
  content: "The model should work when loaded with T5ForConditionalGeneration.\n\n\
    Are you using this example code in the readme? And what inputs did you use?\n\n\
    ```python\n\nfrom transformers import T5ForConditionalGeneration, AutoTokenizer\n\
    \n\nmodel = T5ForConditionalGeneration.from_pretrained('yelpfeast/byt5-base-english-ocr-correction')\n\
    tokenizer = AutoTokenizer.from_pretrained(\"yelpfeast/byt5-base-english-ocr-correction\"\
    )\n\ninputs = tokenizer(\"I l0ve anima1s\", return_tensors=\"pt\", padding=True)\n\
    \noutput_sequences = model.generate(\n\n    input_ids=inputs[\"input_ids\"],\n\
    \n    attention_mask=inputs[\"attention_mask\"],\n\n    do_sample=False,  # disable\
    \ sampling to test if batching affects output\n\n)\n\nprint(tokenizer.batch_decode(output_sequences,\
    \ skip_special_tokens=True))\n```\nAlso not all inputs may be corrected. Since\
    \ the model was trained on synthetically corrupted data I assume that if the input\
    \ text is very different to the training the data the model might just output\
    \ the same text unaltered. I aim to improve the model at some point by training\
    \ on a larger dataset with different levels of corrupted text to make it work\
    \ on a wider variety of input text."
  created_at: 2022-07-25 18:46:22+00:00
  edited: false
  hidden: false
  id: 62def30ee89c1e5797b4b938
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e39ec06d143c7c2886e3f9298b232db.svg
      fullname: Laurent
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KeepKool
      type: user
    createdAt: '2022-07-25T21:01:05.000Z'
    data:
      edited: false
      editors:
      - KeepKool
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e39ec06d143c7c2886e3f9298b232db.svg
          fullname: Laurent
          isHf: false
          isPro: false
          name: KeepKool
          type: user
        html: "<p>I tried it using the Hosted API , manually (no code).<br>With your\
          \ example \"I l0ve anima1s\", it works. But not with my test sentences.<br>This\
          \ has certainly to do with your training set.<br>Here is the type of OCR\
          \ issues I have (here in french)  (On your model, I tried in english of\
          \ course)</p>\n<p>Lorsque ces sciences sont employ\xE9es .\xE0 pr\xE9dire\
          \ l'avenir de .l'individu,<br>:lles offrent de nombreux dangers. Il est\
          \ risqu\xE9 d'indiquer \xE0 quelqu'un un .<br>-v\xE8nement \xB7fi tur probable.\
          \ En effet, le\xB7 fait\xB7 de cannai tre cet \xE9v\xE8nement in;.. .:</p>\n"
        raw: "I tried it using the Hosted API , manually (no code). \nWith your example\
          \ \"I l0ve anima1s\", it works. But not with my test sentences.\nThis has\
          \ certainly to do with your training set. \nHere is the type of OCR issues\
          \ I have (here in french)  (On your model, I tried in english of course)\n\
          \nLorsque ces sciences sont employ\xE9es .\xE0 pr\xE9dire l'avenir de .l'individu,\n\
          :lles offrent de nombreux dangers. Il est risqu\xE9 d'indiquer \xE0 quelqu'un\
          \ un .\n-v\xE8nement \xB7fi tur probable. En effet, le\xB7 fait\xB7 de cannai\
          \ tre cet \xE9v\xE8nement in;.. .:"
        updatedAt: '2022-07-25T21:01:05.805Z'
      numEdits: 0
      reactions: []
    id: 62df0491eee79ca5ef92ca5c
    type: comment
  author: KeepKool
  content: "I tried it using the Hosted API , manually (no code). \nWith your example\
    \ \"I l0ve anima1s\", it works. But not with my test sentences.\nThis has certainly\
    \ to do with your training set. \nHere is the type of OCR issues I have (here\
    \ in french)  (On your model, I tried in english of course)\n\nLorsque ces sciences\
    \ sont employ\xE9es .\xE0 pr\xE9dire l'avenir de .l'individu,\n:lles offrent de\
    \ nombreux dangers. Il est risqu\xE9 d'indiquer \xE0 quelqu'un un .\n-v\xE8nement\
    \ \xB7fi tur probable. En effet, le\xB7 fait\xB7 de cannai tre cet \xE9v\xE8nement\
    \ in;.. .:"
  created_at: 2022-07-25 20:01:05+00:00
  edited: false
  hidden: false
  id: 62df0491eee79ca5ef92ca5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb9a49571a978c1ec45e939504c81c2d.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: yelpfeast
      type: user
    createdAt: '2022-07-26T17:35:15.000Z'
    data:
      edited: true
      editors:
      - yelpfeast
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb9a49571a978c1ec45e939504c81c2d.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: yelpfeast
          type: user
        html: '<p>I used the nlpaug library for creating the synthetic ocr errors.
          Looking at the source code <a rel="nofollow" href="https://github.com/makcedward/nlpaug/blob/master/nlpaug/model/char/ocr.py">here</a>
          they have mappings of common OCR errors e.g. 0 -&gt; o. However these are
          quite limited so you could fine tune the model on some OCR errors you have
          in your data to make the model perform better.</p>

          <p>I also plan on fine tuning the model on some OCR correction datasets
          as well as the synthetic data to make it better. For example the <a rel="nofollow"
          href="https://www.kaggle.com/datasets/dmollaaliod/correct-ocr-errors?select=train_output.csv">ALTA
          2017</a>. I can upload my training code to github as well.</p>

          '
        raw: 'I used the nlpaug library for creating the synthetic ocr errors. Looking
          at the source code [here](https://github.com/makcedward/nlpaug/blob/master/nlpaug/model/char/ocr.py)
          they have mappings of common OCR errors e.g. 0 -> o. However these are quite
          limited so you could fine tune the model on some OCR errors you have in
          your data to make the model perform better.


          I also plan on fine tuning the model on some OCR correction datasets as
          well as the synthetic data to make it better. For example the [ALTA 2017](https://www.kaggle.com/datasets/dmollaaliod/correct-ocr-errors?select=train_output.csv).
          I can upload my training code to github as well.'
        updatedAt: '2022-07-26T17:35:59.902Z'
      numEdits: 1
      reactions: []
    id: 62e025d34736b5a41566aeaa
    type: comment
  author: yelpfeast
  content: 'I used the nlpaug library for creating the synthetic ocr errors. Looking
    at the source code [here](https://github.com/makcedward/nlpaug/blob/master/nlpaug/model/char/ocr.py)
    they have mappings of common OCR errors e.g. 0 -> o. However these are quite limited
    so you could fine tune the model on some OCR errors you have in your data to make
    the model perform better.


    I also plan on fine tuning the model on some OCR correction datasets as well as
    the synthetic data to make it better. For example the [ALTA 2017](https://www.kaggle.com/datasets/dmollaaliod/correct-ocr-errors?select=train_output.csv).
    I can upload my training code to github as well.'
  created_at: 2022-07-26 16:35:15+00:00
  edited: true
  hidden: false
  id: 62e025d34736b5a41566aeaa
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: yelpfeast/byt5-base-english-ocr-correction
repo_type: model
status: open
target_branch: null
title: 'Doing nothing ? '
