!!python/object:huggingface_hub.community.DiscussionWithDetails
author: samitizerxu
conflicting_files: null
created_at: 2023-04-20 13:03:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644267603320-60c95d5df2627ab18cb456b2.png?w=200&h=200&f=face
      fullname: Samuel Xu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: samitizerxu
      type: user
    createdAt: '2023-04-20T14:03:32.000Z'
    data:
      edited: true
      editors:
      - samitizerxu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644267603320-60c95d5df2627ab18cb456b2.png?w=200&h=200&f=face
          fullname: Samuel Xu
          isHf: false
          isPro: false
          name: samitizerxu
          type: user
        html: "<p>Hi! I'm getting the following error when I try to run the pipeline:</p>\n\
          <pre><code>ValueError: Could not load model h2oai/h2ogpt-oasst1-512-20b\
          \ with any of the following classes: (&lt;class \n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,\
          \ &lt;class \n'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'&gt;).\n\
          </code></pre>\n<p>with this code:</p>\n<pre><code>!pip install transformers==4.28.1\n\
          !pip install accelerate==0.18.0\n</code></pre>\n<pre><code>import torch\n\
          from transformers import pipeline\n\ngenerate_text = pipeline(model=\"h2oai/h2ogpt-oasst1-512-20b\"\
          , torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\"\
          )\n\nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=3000)\n\
          print(res[0][\"generated_text\"])\n</code></pre>\n"
        raw: "Hi! I'm getting the following error when I try to run the pipeline:\n\
          \n```\nValueError: Could not load model h2oai/h2ogpt-oasst1-512-20b with\
          \ any of the following classes: (<class \n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class \n'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>).\n\
          ```\n\nwith this code:\n```\n!pip install transformers==4.28.1\n!pip install\
          \ accelerate==0.18.0\n```\n```\nimport torch\nfrom transformers import pipeline\n\
          \ngenerate_text = pipeline(model=\"h2oai/h2ogpt-oasst1-512-20b\", torch_dtype=torch.bfloat16,\
          \ trust_remote_code=True, device_map=\"auto\")\n\nres = generate_text(\"\
          Why is drinking water so healthy?\", max_new_tokens=3000)\nprint(res[0][\"\
          generated_text\"])\n```"
        updatedAt: '2023-04-20T19:52:03.720Z'
      numEdits: 2
      reactions: []
    id: 64414634ad24e9b2cfbd1ae8
    type: comment
  author: samitizerxu
  content: "Hi! I'm getting the following error when I try to run the pipeline:\n\n\
    ```\nValueError: Could not load model h2oai/h2ogpt-oasst1-512-20b with any of\
    \ the following classes: (<class \n'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
    \ <class \n'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>).\n\
    ```\n\nwith this code:\n```\n!pip install transformers==4.28.1\n!pip install accelerate==0.18.0\n\
    ```\n```\nimport torch\nfrom transformers import pipeline\n\ngenerate_text = pipeline(model=\"\
    h2oai/h2ogpt-oasst1-512-20b\", torch_dtype=torch.bfloat16, trust_remote_code=True,\
    \ device_map=\"auto\")\n\nres = generate_text(\"Why is drinking water so healthy?\"\
    , max_new_tokens=3000)\nprint(res[0][\"generated_text\"])\n```"
  created_at: 2023-04-20 13:03:32+00:00
  edited: true
  hidden: false
  id: 64414634ad24e9b2cfbd1ae8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6308791ac038bf42d568153f/z9TovAddXU3OQR9N_2KFP.jpeg?w=200&h=200&f=face
      fullname: Jonathan McKinney
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pseudotensor
      type: user
    createdAt: '2023-04-21T00:07:55.000Z'
    data:
      edited: false
      editors:
      - pseudotensor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6308791ac038bf42d568153f/z9TovAddXU3OQR9N_2KFP.jpeg?w=200&h=200&f=face
          fullname: Jonathan McKinney
          isHf: false
          isPro: false
          name: pseudotensor
          type: user
        html: "<p>Hi, thanks for trying out the model.  Unfortunately, I was unable\
          \ to reproduce the error on a fresh env using conda, e.g.:</p>\n<pre><code>conda\
          \ create -n test\nconda activate test\nconda install python=3.10\npip install\
          \ transformers==4.28.1\npip install accelerate==0.18.0\n\n$ python\n&gt;&gt;&gt;\
          \ import torch\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt;\
          \ generate_text = pipeline(model=\"h2oai/h2ogpt-oig-oasst1-512-6.9b\", torch_dtype=torch.bfloat16,\
          \ trust_remote_code=True, device_map=\"auto\")\n&gt;&gt;&gt; res = generate_text(\"\
          Why is drinking water so healthy?\")\nSetting `pad_token_id` to `eos_token_id`:0\
          \ for open-end generation.\n/home/jon/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/utils.py:1313:\
          \ UserWarning: Using `max_length`'s default (20) to control the generation\
          \ length. This behaviour is deprecated and will be removed from the config\
          \ in v5 of Transformers -- we recommend using `max_new_tokens` to control\
          \ the maximum length of the generation.\n  warnings.warn(\n&gt;&gt;&gt;\
          \ print(res[0][\"generated_text\"])\nWhy is drinking water so healthy?\n\
          \nWater is the most abundant substance in the world. It\n</code></pre>\n\
          <p>However, we have seen this too when starting the demo instance using\
          \ the \"pipeline\" from transformers.  Instead we had to use the other example\
          \ given in the model card that uses H2OTextGenerationPipeline.  That should\
          \ work, please try that way.  E.g. </p>\n<pre><code>import torch\nfrom h2oai_pipeline\
          \ import H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-oasst1-512-20b\"\
          , padding_side=\"left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          h2oai/h2ogpt-oasst1-512-20b\", torch_dtype=torch.bfloat16, device_map=\"\
          auto\")\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer)\n\
          \nres = generate_text(\"Why is drinking water so healthy?\", max_new_tokens=100)\n\
          print(res[0][\"generated_text\"])\n</code></pre>\n"
        raw: "Hi, thanks for trying out the model.  Unfortunately, I was unable to\
          \ reproduce the error on a fresh env using conda, e.g.:\n\n```\nconda create\
          \ -n test\nconda activate test\nconda install python=3.10\npip install transformers==4.28.1\n\
          pip install accelerate==0.18.0\n\n$ python\n>>> import torch\n>>> from transformers\
          \ import pipeline\n>>> generate_text = pipeline(model=\"h2oai/h2ogpt-oig-oasst1-512-6.9b\"\
          , torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\"\
          )\n>>> res = generate_text(\"Why is drinking water so healthy?\")\nSetting\
          \ `pad_token_id` to `eos_token_id`:0 for open-end generation.\n/home/jon/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/utils.py:1313:\
          \ UserWarning: Using `max_length`'s default (20) to control the generation\
          \ length. This behaviour is deprecated and will be removed from the config\
          \ in v5 of Transformers -- we recommend using `max_new_tokens` to control\
          \ the maximum length of the generation.\n  warnings.warn(\n>>> print(res[0][\"\
          generated_text\"])\nWhy is drinking water so healthy?\n\nWater is the most\
          \ abundant substance in the world. It\n```\n\nHowever, we have seen this\
          \ too when starting the demo instance using the \"pipeline\" from transformers.\
          \  Instead we had to use the other example given in the model card that\
          \ uses H2OTextGenerationPipeline.  That should work, please try that way.\
          \  E.g. \n\n```\nimport torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\n\
          from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-oasst1-512-20b\", padding_side=\"\
          left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"h2oai/h2ogpt-oasst1-512-20b\"\
          , torch_dtype=torch.bfloat16, device_map=\"auto\")\ngenerate_text = H2OTextGenerationPipeline(model=model,\
          \ tokenizer=tokenizer)\n\nres = generate_text(\"Why is drinking water so\
          \ healthy?\", max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n```"
        updatedAt: '2023-04-21T00:07:55.018Z'
      numEdits: 0
      reactions: []
    id: 6441d3dba839ee80331fa864
    type: comment
  author: pseudotensor
  content: "Hi, thanks for trying out the model.  Unfortunately, I was unable to reproduce\
    \ the error on a fresh env using conda, e.g.:\n\n```\nconda create -n test\nconda\
    \ activate test\nconda install python=3.10\npip install transformers==4.28.1\n\
    pip install accelerate==0.18.0\n\n$ python\n>>> import torch\n>>> from transformers\
    \ import pipeline\n>>> generate_text = pipeline(model=\"h2oai/h2ogpt-oig-oasst1-512-6.9b\"\
    , torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n>>>\
    \ res = generate_text(\"Why is drinking water so healthy?\")\nSetting `pad_token_id`\
    \ to `eos_token_id`:0 for open-end generation.\n/home/jon/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/utils.py:1313:\
    \ UserWarning: Using `max_length`'s default (20) to control the generation length.\
    \ This behaviour is deprecated and will be removed from the config in v5 of Transformers\
    \ -- we recommend using `max_new_tokens` to control the maximum length of the\
    \ generation.\n  warnings.warn(\n>>> print(res[0][\"generated_text\"])\nWhy is\
    \ drinking water so healthy?\n\nWater is the most abundant substance in the world.\
    \ It\n```\n\nHowever, we have seen this too when starting the demo instance using\
    \ the \"pipeline\" from transformers.  Instead we had to use the other example\
    \ given in the model card that uses H2OTextGenerationPipeline.  That should work,\
    \ please try that way.  E.g. \n\n```\nimport torch\nfrom h2oai_pipeline import\
    \ H2OTextGenerationPipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
    \ntokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-oasst1-512-20b\", padding_side=\"\
    left\")\nmodel = AutoModelForCausalLM.from_pretrained(\"h2oai/h2ogpt-oasst1-512-20b\"\
    , torch_dtype=torch.bfloat16, device_map=\"auto\")\ngenerate_text = H2OTextGenerationPipeline(model=model,\
    \ tokenizer=tokenizer)\n\nres = generate_text(\"Why is drinking water so healthy?\"\
    , max_new_tokens=100)\nprint(res[0][\"generated_text\"])\n```"
  created_at: 2023-04-20 23:07:55+00:00
  edited: false
  hidden: false
  id: 6441d3dba839ee80331fa864
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644267603320-60c95d5df2627ab18cb456b2.png?w=200&h=200&f=face
      fullname: Samuel Xu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: samitizerxu
      type: user
    createdAt: '2023-04-22T03:44:19.000Z'
    data:
      edited: true
      editors:
      - samitizerxu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644267603320-60c95d5df2627ab18cb456b2.png?w=200&h=200&f=face
          fullname: Samuel Xu
          isHf: false
          isPro: false
          name: samitizerxu
          type: user
        html: '<p>Ah, I see. I ran my code on a google colab, however I did install
          the required version of transformers and accelerate<br>.</p>

          '
        raw: 'Ah, I see. I ran my code on a google colab, however I did install the
          required version of transformers and accelerate

          .'
        updatedAt: '2023-04-22T03:44:39.450Z'
      numEdits: 1
      reactions: []
    id: 64435813dad68e008d124ba9
    type: comment
  author: samitizerxu
  content: 'Ah, I see. I ran my code on a google colab, however I did install the
    required version of transformers and accelerate

    .'
  created_at: 2023-04-22 02:44:19+00:00
  edited: true
  hidden: false
  id: 64435813dad68e008d124ba9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644267603320-60c95d5df2627ab18cb456b2.png?w=200&h=200&f=face
      fullname: Samuel Xu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: samitizerxu
      type: user
    createdAt: '2023-04-22T03:44:48.000Z'
    data:
      status: closed
    id: '644358308569978432045890'
    type: status-change
  author: samitizerxu
  created_at: 2023-04-22 02:44:48+00:00
  id: '644358308569978432045890'
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: h2oai/h2ogpt-oasst1-512-20b
repo_type: model
status: closed
target_branch: null
title: Pipeline not working
