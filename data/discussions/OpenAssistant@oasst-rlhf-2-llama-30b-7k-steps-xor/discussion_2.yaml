!!python/object:huggingface_hub.community.DiscussionWithDetails
author: acheong08
conflicting_files: null
created_at: 2023-05-09 14:06:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f5192e05fdd34b0aa915b99bb36eab1.svg
      fullname: Antonio Cheong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: acheong08
      type: user
    createdAt: '2023-05-09T15:06:41.000Z'
    data:
      edited: false
      editors:
      - acheong08
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f5192e05fdd34b0aa915b99bb36eab1.svg
          fullname: Antonio Cheong
          isHf: false
          isPro: false
          name: acheong08
          type: user
        html: '<p>Will the XOR be compatible or must it be trained from scratch again?</p>

          <p>Reference: <a rel="nofollow" href="https://github.com/openlm-research/open_llama">https://github.com/openlm-research/open_llama</a></p>

          '
        raw: "Will the XOR be compatible or must it be trained from scratch again?\r\
          \n\r\nReference: https://github.com/openlm-research/open_llama"
        updatedAt: '2023-05-09T15:06:41.742Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Kagerage
    id: 645a61815e6871b4b2d441db
    type: comment
  author: acheong08
  content: "Will the XOR be compatible or must it be trained from scratch again?\r\
    \n\r\nReference: https://github.com/openlm-research/open_llama"
  created_at: 2023-05-09 14:06:41+00:00
  edited: false
  hidden: false
  id: 645a61815e6871b4b2d441db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6449d9c56fa5e1ba98a442c1/zl5fC_x1Rh_Df85zHU76A.jpeg?w=200&h=200&f=face
      fullname: Byron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: byroneverson
      type: user
    createdAt: '2023-05-09T19:35:57.000Z'
    data:
      edited: false
      editors:
      - byroneverson
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6449d9c56fa5e1ba98a442c1/zl5fC_x1Rh_Df85zHU76A.jpeg?w=200&h=200&f=face
          fullname: Byron
          isHf: false
          isPro: false
          name: byroneverson
          type: user
        html: '<p>Open LLaMa appears to be a reproduction of the 7B param model, while
          this xor is for the 30B param model. But even if Open LLaMa offered a 30B
          param model, the xor would be different.</p>

          <p>However, it would be fairly trivial for someone to convert this xor into
          one needed for a theoretical Open LLaMa 30B model by simply following the
          steps outlined by this xor with the original LLaMa 30B, then calculating
          a new xor based on the differences between this xor patched model and said
          theoretical Open LLaMa 30B model. In theory you could create a completely
          untrained "Open LLaMa 30B model" and perform the process I have outlined
          but there would be some implications regarding licensing, as it is hard
          to say exactly what portion of the learning came from the original LLaMa
          model.</p>

          <p>Most likely, Open Assistant would have to perform an entirely new batch
          of training on an Open LLaMa 30B model which would result in a model completely
          different than this one even if it may achieve similar results and accuracies.
          And assuming Open LLaMa''s licensing is actually open (the ethical choice),
          there would be no reason to release an xor, Open Assistant could in theory
          just release the model itself (or a LoRA adapter) without any of this superfluous
          patchwork. Hope this helps!</p>

          '
        raw: 'Open LLaMa appears to be a reproduction of the 7B param model, while
          this xor is for the 30B param model. But even if Open LLaMa offered a 30B
          param model, the xor would be different.


          However, it would be fairly trivial for someone to convert this xor into
          one needed for a theoretical Open LLaMa 30B model by simply following the
          steps outlined by this xor with the original LLaMa 30B, then calculating
          a new xor based on the differences between this xor patched model and said
          theoretical Open LLaMa 30B model. In theory you could create a completely
          untrained "Open LLaMa 30B model" and perform the process I have outlined
          but there would be some implications regarding licensing, as it is hard
          to say exactly what portion of the learning came from the original LLaMa
          model.


          Most likely, Open Assistant would have to perform an entirely new batch
          of training on an Open LLaMa 30B model which would result in a model completely
          different than this one even if it may achieve similar results and accuracies.
          And assuming Open LLaMa''s licensing is actually open (the ethical choice),
          there would be no reason to release an xor, Open Assistant could in theory
          just release the model itself (or a LoRA adapter) without any of this superfluous
          patchwork. Hope this helps!'
        updatedAt: '2023-05-09T19:35:57.040Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Kagerage
        - Yhyu13
    id: 645aa09ddbf60d37336189d2
    type: comment
  author: byroneverson
  content: 'Open LLaMa appears to be a reproduction of the 7B param model, while this
    xor is for the 30B param model. But even if Open LLaMa offered a 30B param model,
    the xor would be different.


    However, it would be fairly trivial for someone to convert this xor into one needed
    for a theoretical Open LLaMa 30B model by simply following the steps outlined
    by this xor with the original LLaMa 30B, then calculating a new xor based on the
    differences between this xor patched model and said theoretical Open LLaMa 30B
    model. In theory you could create a completely untrained "Open LLaMa 30B model"
    and perform the process I have outlined but there would be some implications regarding
    licensing, as it is hard to say exactly what portion of the learning came from
    the original LLaMa model.


    Most likely, Open Assistant would have to perform an entirely new batch of training
    on an Open LLaMa 30B model which would result in a model completely different
    than this one even if it may achieve similar results and accuracies. And assuming
    Open LLaMa''s licensing is actually open (the ethical choice), there would be
    no reason to release an xor, Open Assistant could in theory just release the model
    itself (or a LoRA adapter) without any of this superfluous patchwork. Hope this
    helps!'
  created_at: 2023-05-09 18:35:57+00:00
  edited: false
  hidden: false
  id: 645aa09ddbf60d37336189d2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: OpenAssistant/oasst-rlhf-2-llama-30b-7k-steps-xor
repo_type: model
status: open
target_branch: null
title: Open LLaMA
