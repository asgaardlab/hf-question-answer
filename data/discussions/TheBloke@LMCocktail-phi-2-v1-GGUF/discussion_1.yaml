!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dyoung
conflicting_files: null
created_at: 2024-01-03 06:08:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2024-01-03T06:08:40.000Z'
    data:
      edited: false
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6232580542564392
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: "<p>Thought I'd give it a try. Didn't expect much. But I'm finding that\
          \ ctransformers is not able to load the 8-bit GGUF of this model. I don't\
          \ recall what is the family for phi. LLama? It's been a while since I last\
          \ looked at it.</p>\n<p>While in google colab:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">import</span> time\n<span\
          \ class=\"hljs-keyword\">from</span> ctransformers <span class=\"hljs-keyword\"\
          >import</span> AutoModelForCausalLM\n\nload_tm = time.time()\n\nllm = AutoModelForCausalLM.from_pretrained(\n\
          \n    <span class=\"hljs-string\">\"TheBloke/LMCocktail-phi-2-v1-GGUF\"\
          </span>,\n    model_file=<span class=\"hljs-string\">\"lmcocktail-phi-2-v1.Q8_0.gguf\"\
          </span>,\n\n    model_type=<span class=\"hljs-string\">\"llama\"</span>,\
          \ <span class=\"hljs-comment\"># ?</span>\n    )\n\nload_rt_sec = time.time()\
          \ - load_tm\n<span class=\"hljs-keyword\">if</span> load_rt_sec &gt; <span\
          \ class=\"hljs-number\">60</span>:\n  <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"\\nDownload and/or Loading runtime (min): <span\
          \ class=\"hljs-subst\">{load_rt_sec / <span class=\"hljs-number\">60</span>}</span>\"\
          </span>)\n<span class=\"hljs-keyword\">else</span>:\n  <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">f\"\\nDownload and/or Loading\
          \ runtime (sec): <span class=\"hljs-subst\">{load_rt_sec}</span>\"</span>)\n\
          </code></pre>\n<p>error:</p>\n<pre><code>---------------------------------------------------------------------------\n\
          RuntimeError                              Traceback (most recent call last)\n\
          &lt;ipython-input-12-d12e60371a8d&gt; in &lt;cell line: 3&gt;()\n      1\
          \ load_tm = time.time()\n      2 \n----&gt; 3 llm = AutoModelForCausalLM.from_pretrained(\n\
          \      4 \n      5     \"TheBloke/LMCocktail-phi-2-v1-GGUF\",\n\n1 frames\n\
          /usr/local/lib/python3.10/dist-packages/ctransformers/llm.py in __init__(self,\
          \ model_path, model_type, config, lib)\n    251         )\n    252     \
          \    if self._llm is None:\n--&gt; 253             raise RuntimeError(\n\
          \    254                 f\"Failed to create LLM '{model_type}' from '{model_path}'.\"\
          \n    255             )\n\nRuntimeError: Failed to create LLM 'llama' from\
          \ '/root/.cache/huggingface/hub/models--TheBloke--LMCocktail-phi-2-v1-GGUF/blobs/05d6680da2235732940781679c7925e140fad0ff087cbf10961942a644dca7b6'.\n\
          </code></pre>\n<p>Maybe a conversation for marella on github.</p>\n"
        raw: "Thought I'd give it a try. Didn't expect much. But I'm finding that\
          \ ctransformers is not able to load the 8-bit GGUF of this model. I don't\
          \ recall what is the family for phi. LLama? It's been a while since I last\
          \ looked at it.\r\n\r\nWhile in google colab:\r\n\r\n```python\r\nimport\
          \ time\r\nfrom ctransformers import AutoModelForCausalLM\r\n\r\nload_tm\
          \ = time.time()\r\n\r\nllm = AutoModelForCausalLM.from_pretrained(\r\n\r\
          \n    \"TheBloke/LMCocktail-phi-2-v1-GGUF\",\r\n    model_file=\"lmcocktail-phi-2-v1.Q8_0.gguf\"\
          ,\r\n\r\n    model_type=\"llama\", # ?\r\n    )\r\n\r\nload_rt_sec = time.time()\
          \ - load_tm\r\nif load_rt_sec > 60:\r\n  print(f\"\\nDownload and/or Loading\
          \ runtime (min): {load_rt_sec / 60}\")\r\nelse:\r\n  print(f\"\\nDownload\
          \ and/or Loading runtime (sec): {load_rt_sec}\")\r\n```\r\n\r\nerror:\r\n\
          ```\r\n---------------------------------------------------------------------------\r\
          \nRuntimeError                              Traceback (most recent call\
          \ last)\r\n<ipython-input-12-d12e60371a8d> in <cell line: 3>()\r\n     \
          \ 1 load_tm = time.time()\r\n      2 \r\n----> 3 llm = AutoModelForCausalLM.from_pretrained(\r\
          \n      4 \r\n      5     \"TheBloke/LMCocktail-phi-2-v1-GGUF\",\r\n\r\n\
          1 frames\r\n/usr/local/lib/python3.10/dist-packages/ctransformers/llm.py\
          \ in __init__(self, model_path, model_type, config, lib)\r\n    251    \
          \     )\r\n    252         if self._llm is None:\r\n--> 253            \
          \ raise RuntimeError(\r\n    254                 f\"Failed to create LLM\
          \ '{model_type}' from '{model_path}'.\"\r\n    255             )\r\n\r\n\
          RuntimeError: Failed to create LLM 'llama' from '/root/.cache/huggingface/hub/models--TheBloke--LMCocktail-phi-2-v1-GGUF/blobs/05d6680da2235732940781679c7925e140fad0ff087cbf10961942a644dca7b6'.\r\
          \n```\r\n\r\nMaybe a conversation for marella on github."
        updatedAt: '2024-01-03T06:08:40.782Z'
      numEdits: 0
      reactions: []
    id: 6594f9e89e16fa7510200ffd
    type: comment
  author: dyoung
  content: "Thought I'd give it a try. Didn't expect much. But I'm finding that ctransformers\
    \ is not able to load the 8-bit GGUF of this model. I don't recall what is the\
    \ family for phi. LLama? It's been a while since I last looked at it.\r\n\r\n\
    While in google colab:\r\n\r\n```python\r\nimport time\r\nfrom ctransformers import\
    \ AutoModelForCausalLM\r\n\r\nload_tm = time.time()\r\n\r\nllm = AutoModelForCausalLM.from_pretrained(\r\
    \n\r\n    \"TheBloke/LMCocktail-phi-2-v1-GGUF\",\r\n    model_file=\"lmcocktail-phi-2-v1.Q8_0.gguf\"\
    ,\r\n\r\n    model_type=\"llama\", # ?\r\n    )\r\n\r\nload_rt_sec = time.time()\
    \ - load_tm\r\nif load_rt_sec > 60:\r\n  print(f\"\\nDownload and/or Loading runtime\
    \ (min): {load_rt_sec / 60}\")\r\nelse:\r\n  print(f\"\\nDownload and/or Loading\
    \ runtime (sec): {load_rt_sec}\")\r\n```\r\n\r\nerror:\r\n```\r\n---------------------------------------------------------------------------\r\
    \nRuntimeError                              Traceback (most recent call last)\r\
    \n<ipython-input-12-d12e60371a8d> in <cell line: 3>()\r\n      1 load_tm = time.time()\r\
    \n      2 \r\n----> 3 llm = AutoModelForCausalLM.from_pretrained(\r\n      4 \r\
    \n      5     \"TheBloke/LMCocktail-phi-2-v1-GGUF\",\r\n\r\n1 frames\r\n/usr/local/lib/python3.10/dist-packages/ctransformers/llm.py\
    \ in __init__(self, model_path, model_type, config, lib)\r\n    251         )\r\
    \n    252         if self._llm is None:\r\n--> 253             raise RuntimeError(\r\
    \n    254                 f\"Failed to create LLM '{model_type}' from '{model_path}'.\"\
    \r\n    255             )\r\n\r\nRuntimeError: Failed to create LLM 'llama' from\
    \ '/root/.cache/huggingface/hub/models--TheBloke--LMCocktail-phi-2-v1-GGUF/blobs/05d6680da2235732940781679c7925e140fad0ff087cbf10961942a644dca7b6'.\r\
    \n```\r\n\r\nMaybe a conversation for marella on github."
  created_at: 2024-01-03 06:08:40+00:00
  edited: false
  hidden: false
  id: 6594f9e89e16fa7510200ffd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2024-01-05T17:08:37.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8884957432746887
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;dyoung&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/dyoung\">@<span class=\"\
          underline\">dyoung</span></a></span>\n\n\t</span></span> I believe ctransformers\
          \ does not support phi currently. Your best bet is to use llama cpp python\
          \ if you want a ctransformers like experience. Also it should be considerably\
          \ faster then ctransformers.</p>\n"
        raw: '@dyoung I believe ctransformers does not support phi currently. Your
          best bet is to use llama cpp python if you want a ctransformers like experience.
          Also it should be considerably faster then ctransformers.'
        updatedAt: '2024-01-05T17:08:37.534Z'
      numEdits: 0
      reactions: []
    id: 659837958982abaa5c6a3ccd
    type: comment
  author: YaTharThShaRma999
  content: '@dyoung I believe ctransformers does not support phi currently. Your best
    bet is to use llama cpp python if you want a ctransformers like experience. Also
    it should be considerably faster then ctransformers.'
  created_at: 2024-01-05 17:08:37+00:00
  edited: false
  hidden: false
  id: 659837958982abaa5c6a3ccd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2024-01-05T20:31:48.000Z'
    data:
      edited: true
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.980935275554657
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;YaTharThShaRma999&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/YaTharThShaRma999\"\
          >@<span class=\"underline\">YaTharThShaRma999</span></a></span>\n\n\t</span></span>\
          \ I found that the phi-2 model is close to the gpt-j/neo family. Also, I've\
          \ been able to get llama-cpp and llama-cpp-python working on my end. Due\
          \ to my setup, I was dreading having to do what was needed to get GPU inference\
          \ with llama-cpp. But because ctransformers is running a bit behind, I bit\
          \ the bullet and figured out how to get llama-cpp going with my GPU hardware.\
          \ Which was painful! Even with experience doing setups like this. lol. I'm\
          \ glad I did it though. It's been fun playing with some of the new models.\
          \ And it brushed up on my skills a bit. And I learned some new things.<br>Thanks\
          \ for taking the time to reply.</p>\n"
        raw: '@YaTharThShaRma999 I found that the phi-2 model is close to the gpt-j/neo
          family. Also, I''ve been able to get llama-cpp and llama-cpp-python working
          on my end. Due to my setup, I was dreading having to do what was needed
          to get GPU inference with llama-cpp. But because ctransformers is running
          a bit behind, I bit the bullet and figured out how to get llama-cpp going
          with my GPU hardware. Which was painful! Even with experience doing setups
          like this. lol. I''m glad I did it though. It''s been fun playing with some
          of the new models. And it brushed up on my skills a bit. And I learned some
          new things.

          Thanks for taking the time to reply.'
        updatedAt: '2024-01-05T20:32:53.255Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - YaTharThShaRma999
      relatedEventId: 65986735eff07dcf1f0a004e
    id: 65986734eff07dcf1f09fff9
    type: comment
  author: dyoung
  content: '@YaTharThShaRma999 I found that the phi-2 model is close to the gpt-j/neo
    family. Also, I''ve been able to get llama-cpp and llama-cpp-python working on
    my end. Due to my setup, I was dreading having to do what was needed to get GPU
    inference with llama-cpp. But because ctransformers is running a bit behind, I
    bit the bullet and figured out how to get llama-cpp going with my GPU hardware.
    Which was painful! Even with experience doing setups like this. lol. I''m glad
    I did it though. It''s been fun playing with some of the new models. And it brushed
    up on my skills a bit. And I learned some new things.

    Thanks for taking the time to reply.'
  created_at: 2024-01-05 20:31:48+00:00
  edited: true
  hidden: false
  id: 65986734eff07dcf1f09fff9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2024-01-05T20:31:49.000Z'
    data:
      status: closed
    id: 65986735eff07dcf1f0a004e
    type: status-change
  author: dyoung
  created_at: 2024-01-05 20:31:49+00:00
  id: 65986735eff07dcf1f0a004e
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/LMCocktail-phi-2-v1-GGUF
repo_type: model
status: closed
target_branch: null
title: llama model type loading in marella's ctransformers
