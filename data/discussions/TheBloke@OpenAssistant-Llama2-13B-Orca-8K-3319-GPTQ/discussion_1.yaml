!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gsaivinay
conflicting_files: null
created_at: 2023-07-25 13:55:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-07-25T14:55:12.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9660940170288086
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<p>Hello,</p>

          <p>As always, thanks for providing this quantized model, awesome as always.</p>

          <p>Could you provide with which dataset and sequence length, this model
          is quantized.</p>

          <p>My question is, if we are quantizing 8k model with only 2k sequence length,
          are we going to lose performance?</p>

          '
        raw: "Hello,\r\n\r\nAs always, thanks for providing this quantized model,\
          \ awesome as always.\r\n\r\nCould you provide with which dataset and sequence\
          \ length, this model is quantized.\r\n\r\nMy question is, if we are quantizing\
          \ 8k model with only 2k sequence length, are we going to lose performance?"
        updatedAt: '2023-07-25T14:55:12.431Z'
      numEdits: 0
      reactions: []
    id: 64bfe250b619223fb2dedda4
    type: comment
  author: gsaivinay
  content: "Hello,\r\n\r\nAs always, thanks for providing this quantized model, awesome\
    \ as always.\r\n\r\nCould you provide with which dataset and sequence length,\
    \ this model is quantized.\r\n\r\nMy question is, if we are quantizing 8k model\
    \ with only 2k sequence length, are we going to lose performance?"
  created_at: 2023-07-25 13:55:12+00:00
  edited: false
  hidden: false
  id: 64bfe250b619223fb2dedda4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T14:57:38.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9652016758918762
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I used wikitext2 at 4096 sequence length.  I tried at 8192 but I
          ran out of VRAM even on a 48GB card.</p>

          <p>It''s a good question and I don''t know the answer yet. Since Llama 2
          came out I have been quantising all Llama 2 models at 4096 sequence length,
          which takes longer and uses way more VRAM.  But I haven''t actually analysed
          the results to see how much difference it makes.  That''s something I plan
          to do when I have time.  If I find it makes little or no difference, I''ll
          go back to 2048 because it''s much easier to do.</p>

          <p>If I find that it does make a useful difference, I might come back to
          this model and try making an 8192 sequence length quant using an 80GB GPU.  But
          for now I think 4096 will still be fine.</p>

          '
        raw: 'I used wikitext2 at 4096 sequence length.  I tried at 8192 but I ran
          out of VRAM even on a 48GB card.


          It''s a good question and I don''t know the answer yet. Since Llama 2 came
          out I have been quantising all Llama 2 models at 4096 sequence length, which
          takes longer and uses way more VRAM.  But I haven''t actually analysed the
          results to see how much difference it makes.  That''s something I plan to
          do when I have time.  If I find it makes little or no difference, I''ll
          go back to 2048 because it''s much easier to do.


          If I find that it does make a useful difference, I might come back to this
          model and try making an 8192 sequence length quant using an 80GB GPU.  But
          for now I think 4096 will still be fine.'
        updatedAt: '2023-07-25T14:57:38.753Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - gsaivinay
        - mirek190
        - MB7977
        - victor
        - jlzhou
    id: 64bfe2e2f1f2496251c28513
    type: comment
  author: TheBloke
  content: 'I used wikitext2 at 4096 sequence length.  I tried at 8192 but I ran out
    of VRAM even on a 48GB card.


    It''s a good question and I don''t know the answer yet. Since Llama 2 came out
    I have been quantising all Llama 2 models at 4096 sequence length, which takes
    longer and uses way more VRAM.  But I haven''t actually analysed the results to
    see how much difference it makes.  That''s something I plan to do when I have
    time.  If I find it makes little or no difference, I''ll go back to 2048 because
    it''s much easier to do.


    If I find that it does make a useful difference, I might come back to this model
    and try making an 8192 sequence length quant using an 80GB GPU.  But for now I
    think 4096 will still be fine.'
  created_at: 2023-07-25 13:57:38+00:00
  edited: false
  hidden: false
  id: 64bfe2e2f1f2496251c28513
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-07-25T15:04:00.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9292808175086975
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<p>Agreed. If you are able to run the evaluation script used by open
          llm leaderboard, then there might be better idea whether performance is
          okay or degraded. But if it is difficult to run such huge evaluation, then
          we need to find better way to quickly evaluate performance.</p>

          '
        raw: Agreed. If you are able to run the evaluation script used by open llm
          leaderboard, then there might be better idea whether performance is okay
          or degraded. But if it is difficult to run such huge evaluation, then we
          need to find better way to quickly evaluate performance.
        updatedAt: '2023-07-25T15:04:00.289Z'
      numEdits: 0
      reactions: []
    id: 64bfe460ccd15f9941145c8c
    type: comment
  author: gsaivinay
  content: Agreed. If you are able to run the evaluation script used by open llm leaderboard,
    then there might be better idea whether performance is okay or degraded. But if
    it is difficult to run such huge evaluation, then we need to find better way to
    quickly evaluate performance.
  created_at: 2023-07-25 14:04:00+00:00
  edited: false
  hidden: false
  id: 64bfe460ccd15f9941145c8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T15:05:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9759186506271362
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Actually! I think I just figured out how to do it at 8192. It''s
          running now.  I will compare the perplexity so I can do that analysis I
          mentioned above, and see if it actually makes a difference. </p>

          <p>If it does, I will re-do them all at 8K</p>

          '
        raw: "Actually! I think I just figured out how to do it at 8192. It's running\
          \ now.  I will compare the perplexity so I can do that analysis I mentioned\
          \ above, and see if it actually makes a difference. \n\nIf it does, I will\
          \ re-do them all at 8K"
        updatedAt: '2023-07-25T15:05:27.579Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F92F"
        users:
        - gsaivinay
        - mirek190
        - victor
    id: 64bfe4b7c44c8e29e5fde041
    type: comment
  author: TheBloke
  content: "Actually! I think I just figured out how to do it at 8192. It's running\
    \ now.  I will compare the perplexity so I can do that analysis I mentioned above,\
    \ and see if it actually makes a difference. \n\nIf it does, I will re-do them\
    \ all at 8K"
  created_at: 2023-07-25 14:05:27+00:00
  edited: false
  hidden: false
  id: 64bfe4b7c44c8e29e5fde041
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-07-25T15:06:52.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.984880268573761
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<blockquote>

          <p>Actually! I think I just figured out how to do it at 8192. It''s running
          now.</p>

          </blockquote>

          <p>That is awesome. If you could, please let us know how you did it once
          it is successful.</p>

          '
        raw: '> Actually! I think I just figured out how to do it at 8192. It''s running
          now.


          That is awesome. If you could, please let us know how you did it once it
          is successful.


          '
        updatedAt: '2023-07-25T15:06:52.951Z'
      numEdits: 0
      reactions: []
    id: 64bfe50cd824ac47a13a35ac
    type: comment
  author: gsaivinay
  content: '> Actually! I think I just figured out how to do it at 8192. It''s running
    now.


    That is awesome. If you could, please let us know how you did it once it is successful.


    '
  created_at: 2023-07-25 14:06:52+00:00
  edited: false
  hidden: false
  id: 64bfe50cd824ac47a13a35ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T15:06:54.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9669477939605713
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''m not going to do the open eval, it takes forever. I''ll just
          do perplexity.  If that shows no difference we know there is no difference.  If
          it shows a medium sized difference, we still don''t know how much ''real''
          difference there is and then maybe eval would be appropriate, but a small
          perplexity difference usually means it will be indistinguishable.  If it''s
          a larger difference then I''ll definitely try to do the full context in
          all cases</p>

          '
        raw: I'm not going to do the open eval, it takes forever. I'll just do perplexity.  If
          that shows no difference we know there is no difference.  If it shows a
          medium sized difference, we still don't know how much 'real' difference
          there is and then maybe eval would be appropriate, but a small perplexity
          difference usually means it will be indistinguishable.  If it's a larger
          difference then I'll definitely try to do the full context in all cases
        updatedAt: '2023-07-25T15:06:54.107Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - gsaivinay
        - mirek190
        - victor
    id: 64bfe50ea55a5e0a0e38010c
    type: comment
  author: TheBloke
  content: I'm not going to do the open eval, it takes forever. I'll just do perplexity.  If
    that shows no difference we know there is no difference.  If it shows a medium
    sized difference, we still don't know how much 'real' difference there is and
    then maybe eval would be appropriate, but a small perplexity difference usually
    means it will be indistinguishable.  If it's a larger difference then I'll definitely
    try to do the full context in all cases
  created_at: 2023-07-25 14:06:54+00:00
  edited: false
  hidden: false
  id: 64bfe50ea55a5e0a0e38010c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T15:11:39.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9354866743087769
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I changed two things but I think only one matters.  The one that
          matters is to set <code>cache_examples_on_gpu=False</code> in the AutoGPTQ
          <code>.quantize()</code>call.  As the name suggests, this does not store
          the quantisation samples (the 128 x 8192 token strings taken at random from
          wikitext) on the GPU. Instead it stores them in RAM.  I guess it slows it
          down even more, but also appears to make a big difference to VRAM usage.</p>

          <p>The thing I changed that I think is having little or no effect is adding
          environment variable <code>PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32</code>.  This
          changes how VRAM is allocated by PyTorch. With this set, it stores the data
          in smaller chunks.  This helps prevent running out of VRAM in situations
          where you are very close to 100% used.  </p>

          <p>But with the change to no longer cache examples, I am now only using
          50-60% of the 48GB VRAM I have, so I don''t think that CUDA_ALLOC change
          is affecting anything.  In fact I''m now using less VRAM at 8192 context
          than I was at 4096, when the examples were cached on GPU.</p>

          '
        raw: "I changed two things but I think only one matters.  The one that matters\
          \ is to set `cache_examples_on_gpu=False` in the AutoGPTQ `.quantize()`call.\
          \  As the name suggests, this does not store the quantisation samples (the\
          \ 128 x 8192 token strings taken at random from wikitext) on the GPU. Instead\
          \ it stores them in RAM.  I guess it slows it down even more, but also appears\
          \ to make a big difference to VRAM usage.\n\nThe thing I changed that I\
          \ think is having little or no effect is adding environment variable `PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32`.\
          \  This changes how VRAM is allocated by PyTorch. With this set, it stores\
          \ the data in smaller chunks.  This helps prevent running out of VRAM in\
          \ situations where you are very close to 100% used.  \n\nBut with the change\
          \ to no longer cache examples, I am now only using 50-60% of the 48GB VRAM\
          \ I have, so I don't think that CUDA_ALLOC change is affecting anything.\
          \  In fact I'm now using less VRAM at 8192 context than I was at 4096, when\
          \ the examples were cached on GPU."
        updatedAt: '2023-07-25T15:11:39.819Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - gsaivinay
        - mirek190
    id: 64bfe62b377d044c884d962a
    type: comment
  author: TheBloke
  content: "I changed two things but I think only one matters.  The one that matters\
    \ is to set `cache_examples_on_gpu=False` in the AutoGPTQ `.quantize()`call. \
    \ As the name suggests, this does not store the quantisation samples (the 128\
    \ x 8192 token strings taken at random from wikitext) on the GPU. Instead it stores\
    \ them in RAM.  I guess it slows it down even more, but also appears to make a\
    \ big difference to VRAM usage.\n\nThe thing I changed that I think is having\
    \ little or no effect is adding environment variable `PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32`.\
    \  This changes how VRAM is allocated by PyTorch. With this set, it stores the\
    \ data in smaller chunks.  This helps prevent running out of VRAM in situations\
    \ where you are very close to 100% used.  \n\nBut with the change to no longer\
    \ cache examples, I am now only using 50-60% of the 48GB VRAM I have, so I don't\
    \ think that CUDA_ALLOC change is affecting anything.  In fact I'm now using less\
    \ VRAM at 8192 context than I was at 4096, when the examples were cached on GPU."
  created_at: 2023-07-25 14:11:39+00:00
  edited: false
  hidden: false
  id: 64bfe62b377d044c884d962a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T22:15:15.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9516978859901428
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I did the perplexity calculations. I could only do them at 2K because
          it failed at longer lengths - probably a limitation in the PPL code.  But
          even at 2K it showed a detectable difference.  </p>

          <p>Testing the best quant - the 32g + act-order file:  testing PPL with
          wikitext, the same dataset I used for quant, it''s about 1.5%. Not much.  With
          PTB it''s 7%, which is more significant.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/nR-7k8huMR-lGjqzTm7KW.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/nR-7k8huMR-lGjqzTm7KW.png"></a></p>

          <p>How this translates into real life results, if at all, I do not know.  But
          for now I guess I will be doing 8K models at 8K context.  It took 2.5 hours
          to do using A6000s!</p>

          <p>The re-done GPTQs with 8k context are now uploaded, or will be shortly</p>

          '
        raw: "I did the perplexity calculations. I could only do them at 2K because\
          \ it failed at longer lengths - probably a limitation in the PPL code. \
          \ But even at 2K it showed a detectable difference.  \n\nTesting the best\
          \ quant - the 32g + act-order file:  testing PPL with wikitext, the same\
          \ dataset I used for quant, it's about 1.5%. Not much.  With PTB it's 7%,\
          \ which is more significant.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/nR-7k8huMR-lGjqzTm7KW.png)\n\
          \nHow this translates into real life results, if at all, I do not know.\
          \  But for now I guess I will be doing 8K models at 8K context.  It took\
          \ 2.5 hours to do using A6000s!\n\nThe re-done GPTQs with 8k context are\
          \ now uploaded, or will be shortly\n"
        updatedAt: '2023-07-25T22:16:36.888Z'
      numEdits: 1
      reactions: []
    id: 64c04973a8a2dcaa166ca75f
    type: comment
  author: TheBloke
  content: "I did the perplexity calculations. I could only do them at 2K because\
    \ it failed at longer lengths - probably a limitation in the PPL code.  But even\
    \ at 2K it showed a detectable difference.  \n\nTesting the best quant - the 32g\
    \ + act-order file:  testing PPL with wikitext, the same dataset I used for quant,\
    \ it's about 1.5%. Not much.  With PTB it's 7%, which is more significant.\n\n\
    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/nR-7k8huMR-lGjqzTm7KW.png)\n\
    \nHow this translates into real life results, if at all, I do not know.  But for\
    \ now I guess I will be doing 8K models at 8K context.  It took 2.5 hours to do\
    \ using A6000s!\n\nThe re-done GPTQs with 8k context are now uploaded, or will\
    \ be shortly\n"
  created_at: 2023-07-25 21:15:15+00:00
  edited: true
  hidden: false
  id: 64c04973a8a2dcaa166ca75f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666635591305-noauth.jpeg?w=200&h=200&f=face
      fullname: Khalil
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Abdelhak
      type: user
    createdAt: '2023-07-25T22:39:56.000Z'
    data:
      edited: false
      editors:
      - Abdelhak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9314455389976501
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666635591305-noauth.jpeg?w=200&h=200&f=face
          fullname: Khalil
          isHf: false
          isPro: false
          name: Abdelhak
          type: user
        html: '<p>Thanks! How can I directly download the gptq-8bit-64g-actorder_True
          version without going through the webui. I''d like to use Internet Download
          Manager as I have control over the speed of the internet and auto resumability.
          Thank you.</p>

          '
        raw: Thanks! How can I directly download the gptq-8bit-64g-actorder_True version
          without going through the webui. I'd like to use Internet Download Manager
          as I have control over the speed of the internet and auto resumability.
          Thank you.
        updatedAt: '2023-07-25T22:39:56.107Z'
      numEdits: 0
      reactions: []
    id: 64c04f3ce8e1818a3671ac6c
    type: comment
  author: Abdelhak
  content: Thanks! How can I directly download the gptq-8bit-64g-actorder_True version
    without going through the webui. I'd like to use Internet Download Manager as
    I have control over the speed of the internet and auto resumability. Thank you.
  created_at: 2023-07-25 21:39:56+00:00
  edited: false
  hidden: false
  id: 64c04f3ce8e1818a3671ac6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T22:47:50.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7633292078971863
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>If you want to use a download manager, I think you''ll need to manually
          switch to the desired branch in the <strong>Files and Version</strong> tab,
          and copy the download link for each file (there''s a little icon for it,
          next to each file).  Bit tedious.</p>

          <p>Generally there are three options for automatic download of branches:</p>

          <ol>

          <li><p><code>git clone -b branch-name https://huggingface.co/TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ</code>  -
          not recommended because Git stores twice as much data, taking double disk
          space. And you need to install Git LFS first. Also it''s usually really
          slow.</p>

          </li>

          <li><p>Run <code>download-model.py</code> provided with text-generation-webui,
          with the <code>--branch</code> parameter.  That runs the same code as the
          UI does, but from the command line.  eg <code>python3 download-model.py
          TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ --branch gptq-4bit-128g-actorder_True</code>
          which will download the model to <code>models/TheBloke_OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ_gptq-4bit-128g-actorder_True</code>.   You
          can use the <code>--threads X</code> parameter to use X threads at once,
          which can help to maximise your download speed.  eg add <code>--threads
          4</code></p>

          </li>

          <li><p>Python code like the following:</p>

          </li>

          </ol>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          huggingface_hub <span class="hljs-keyword">import</span> snapshot_download


          model_name = <span class="hljs-string">"TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ"</span>

          branch = <span class="hljs-string">"gptq-4bit-32g-actorder_True"</span>

          local_base_folder = <span class="hljs-string">"/workspace"</span>

          local_folder = os.path.join(local_base_folder, <span class="hljs-string">f"<span
          class="hljs-subst">{model_name.replace(<span class="hljs-string">''/''</span>,
          <span class="hljs-string">''_''</span>)}</span>_<span class="hljs-subst">{branch}</span>"</span>)


          snapshot_download(repo_id=model_name, local_dir=local_folder, revision=branch,
          local_dir_use_symlinks=<span class="hljs-literal">False</span>)

          </code></pre>

          <p>That will download branch <code>gptq-4bit-32g-actorder_True</code> to
          a folder under <code>/workspace</code> - change /workspace to an appropriate
          local folder for you.</p>

          <p>To maximise your download speed with the Python option, do <code>pip3
          install hf_transfer</code> and then set environment variable <code>HF_HUB_ENABLE_HF_TRANSFER=1</code>
          while running your script.  That will usually max out any internet connection,
          even 10Gbit/s.</p>

          <p>Eg if you saved the above script to <code>download.py</code>, then in
          Linux you can do: <code>HF_HUB_ENABLE_HF_TRANSFER=1 python3 download.py</code>
          to get that maximised download.</p>

          '
        raw: 'If you want to use a download manager, I think you''ll need to manually
          switch to the desired branch in the **Files and Version** tab, and copy
          the download link for each file (there''s a little icon for it, next to
          each file).  Bit tedious.


          Generally there are three options for automatic download of branches:


          1. `git clone -b branch-name https://huggingface.co/TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ`  -
          not recommended because Git stores twice as much data, taking double disk
          space. And you need to install Git LFS first. Also it''s usually really
          slow.


          2. Run `download-model.py` provided with text-generation-webui, with the
          `--branch` parameter.  That runs the same code as the UI does, but from
          the command line.  eg `python3 download-model.py TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ
          --branch gptq-4bit-128g-actorder_True` which will download the model to
          `models/TheBloke_OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ_gptq-4bit-128g-actorder_True`.   You
          can use the `--threads X` parameter to use X threads at once, which can
          help to maximise your download speed.  eg add `--threads 4`


          3. Python code like the following:

          ```python

          from huggingface_hub import snapshot_download


          model_name = "TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ"

          branch = "gptq-4bit-32g-actorder_True"

          local_base_folder = "/workspace"

          local_folder = os.path.join(local_base_folder, f"{model_name.replace(''/'',
          ''_'')}_{branch}")


          snapshot_download(repo_id=model_name, local_dir=local_folder, revision=branch,
          local_dir_use_symlinks=False)

          ```


          That will download branch `gptq-4bit-32g-actorder_True` to a folder under
          `/workspace` - change /workspace to an appropriate local folder for you.


          To maximise your download speed with the Python option, do `pip3 install
          hf_transfer` and then set environment variable `HF_HUB_ENABLE_HF_TRANSFER=1`
          while running your script.  That will usually max out any internet connection,
          even 10Gbit/s.


          Eg if you saved the above script to `download.py`, then in Linux you can
          do: `HF_HUB_ENABLE_HF_TRANSFER=1 python3 download.py` to get that maximised
          download.'
        updatedAt: '2023-07-25T22:48:32.135Z'
      numEdits: 1
      reactions: []
    id: 64c051165e03515457ee32fe
    type: comment
  author: TheBloke
  content: 'If you want to use a download manager, I think you''ll need to manually
    switch to the desired branch in the **Files and Version** tab, and copy the download
    link for each file (there''s a little icon for it, next to each file).  Bit tedious.


    Generally there are three options for automatic download of branches:


    1. `git clone -b branch-name https://huggingface.co/TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ`  -
    not recommended because Git stores twice as much data, taking double disk space.
    And you need to install Git LFS first. Also it''s usually really slow.


    2. Run `download-model.py` provided with text-generation-webui, with the `--branch`
    parameter.  That runs the same code as the UI does, but from the command line.  eg
    `python3 download-model.py TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ
    --branch gptq-4bit-128g-actorder_True` which will download the model to `models/TheBloke_OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ_gptq-4bit-128g-actorder_True`.   You
    can use the `--threads X` parameter to use X threads at once, which can help to
    maximise your download speed.  eg add `--threads 4`


    3. Python code like the following:

    ```python

    from huggingface_hub import snapshot_download


    model_name = "TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ"

    branch = "gptq-4bit-32g-actorder_True"

    local_base_folder = "/workspace"

    local_folder = os.path.join(local_base_folder, f"{model_name.replace(''/'', ''_'')}_{branch}")


    snapshot_download(repo_id=model_name, local_dir=local_folder, revision=branch,
    local_dir_use_symlinks=False)

    ```


    That will download branch `gptq-4bit-32g-actorder_True` to a folder under `/workspace`
    - change /workspace to an appropriate local folder for you.


    To maximise your download speed with the Python option, do `pip3 install hf_transfer`
    and then set environment variable `HF_HUB_ENABLE_HF_TRANSFER=1` while running
    your script.  That will usually max out any internet connection, even 10Gbit/s.


    Eg if you saved the above script to `download.py`, then in Linux you can do: `HF_HUB_ENABLE_HF_TRANSFER=1
    python3 download.py` to get that maximised download.'
  created_at: 2023-07-25 21:47:50+00:00
  edited: true
  hidden: false
  id: 64c051165e03515457ee32fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666635591305-noauth.jpeg?w=200&h=200&f=face
      fullname: Khalil
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Abdelhak
      type: user
    createdAt: '2023-07-25T22:57:38.000Z'
    data:
      edited: false
      editors:
      - Abdelhak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9656558036804199
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666635591305-noauth.jpeg?w=200&h=200&f=face
          fullname: Khalil
          isHf: false
          isPro: false
          name: Abdelhak
          type: user
        html: '<p>Thank for your quick reply Tom. But, I always use IDM to download
          models, but there is usually one file: the gptq_model-4bit-128g.safetensors.
          However, I''d like to download the 8-bit model (gptq-8bit-64g-actorder_True),
          which is not present in the Files and Version tab. I would appreciate if
          you could direct me at the other quantized versions as well.</p>

          '
        raw: 'Thank for your quick reply Tom. But, I always use IDM to download models,
          but there is usually one file: the gptq_model-4bit-128g.safetensors. However,
          I''d like to download the 8-bit model (gptq-8bit-64g-actorder_True), which
          is not present in the Files and Version tab. I would appreciate if you could
          direct me at the other quantized versions as well.

          '
        updatedAt: '2023-07-25T22:57:38.430Z'
      numEdits: 0
      reactions: []
    id: 64c05362e8e1818a36723e14
    type: comment
  author: Abdelhak
  content: 'Thank for your quick reply Tom. But, I always use IDM to download models,
    but there is usually one file: the gptq_model-4bit-128g.safetensors. However,
    I''d like to download the 8-bit model (gptq-8bit-64g-actorder_True), which is
    not present in the Files and Version tab. I would appreciate if you could direct
    me at the other quantized versions as well.

    '
  created_at: 2023-07-25 21:57:38+00:00
  edited: false
  hidden: false
  id: 64c05362e8e1818a36723e14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T22:58:46.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7027025818824768
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry not sure what you mean. 8bit-64g is here: <a href="https://huggingface.co/TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ/tree/gptq-8bit-64g-actorder_True">https://huggingface.co/TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ/tree/gptq-8bit-64g-actorder_True</a></p>

          '
        raw: 'Sorry not sure what you mean. 8bit-64g is here: https://huggingface.co/TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ/tree/gptq-8bit-64g-actorder_True'
        updatedAt: '2023-07-25T22:58:46.367Z'
      numEdits: 0
      reactions: []
    id: 64c053a662983511b9482ee6
    type: comment
  author: TheBloke
  content: 'Sorry not sure what you mean. 8bit-64g is here: https://huggingface.co/TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ/tree/gptq-8bit-64g-actorder_True'
  created_at: 2023-07-25 21:58:46+00:00
  edited: false
  hidden: false
  id: 64c053a662983511b9482ee6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666635591305-noauth.jpeg?w=200&h=200&f=face
      fullname: Khalil
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Abdelhak
      type: user
    createdAt: '2023-07-25T23:01:14.000Z'
    data:
      edited: false
      editors:
      - Abdelhak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9061679840087891
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666635591305-noauth.jpeg?w=200&h=200&f=face
          fullname: Khalil
          isHf: false
          isPro: false
          name: Abdelhak
          type: user
        html: '<p>Apologies, I didn''t notice that I can select the models individually.
          I was still thinking like the GMLL way where all the models are in one folder
          :) My bad, and you rock.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6356d7653c32f2c90f4e72a8/oTVuETxJbbdcJ2Yi0uKwQ.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6356d7653c32f2c90f4e72a8/oTVuETxJbbdcJ2Yi0uKwQ.png"></a></p>

          '
        raw: 'Apologies, I didn''t notice that I can select the models individually.
          I was still thinking like the GMLL way where all the models are in one folder
          :) My bad, and you rock.


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6356d7653c32f2c90f4e72a8/oTVuETxJbbdcJ2Yi0uKwQ.png)

          '
        updatedAt: '2023-07-25T23:01:14.430Z'
      numEdits: 0
      reactions: []
    id: 64c0543ab85ee9e42219d2ef
    type: comment
  author: Abdelhak
  content: 'Apologies, I didn''t notice that I can select the models individually.
    I was still thinking like the GMLL way where all the models are in one folder
    :) My bad, and you rock.


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6356d7653c32f2c90f4e72a8/oTVuETxJbbdcJ2Yi0uKwQ.png)

    '
  created_at: 2023-07-25 22:01:14+00:00
  edited: false
  hidden: false
  id: 64c0543ab85ee9e42219d2ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T23:07:18.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8315651416778564
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>No worries! Actually that reminded me to roll out a new template
          feature I''d been meaning to do.  From now on, all the branch names under
          Provided Files are clickable URLs, linking to the appropriate Files and
          Folders section:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/60rxp5WkUM2lGtaqSicZX.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/60rxp5WkUM2lGtaqSicZX.png"></a></p>

          '
        raw: 'No worries! Actually that reminded me to roll out a new template feature
          I''d been meaning to do.  From now on, all the branch names under Provided
          Files are clickable URLs, linking to the appropriate Files and Folders section:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/60rxp5WkUM2lGtaqSicZX.png)

          '
        updatedAt: '2023-07-25T23:07:18.710Z'
      numEdits: 0
      reactions: []
    id: 64c055a6a8a2dcaa166e8fea
    type: comment
  author: TheBloke
  content: 'No worries! Actually that reminded me to roll out a new template feature
    I''d been meaning to do.  From now on, all the branch names under Provided Files
    are clickable URLs, linking to the appropriate Files and Folders section:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/60rxp5WkUM2lGtaqSicZX.png)

    '
  created_at: 2023-07-25 22:07:18+00:00
  edited: false
  hidden: false
  id: 64c055a6a8a2dcaa166e8fea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666635591305-noauth.jpeg?w=200&h=200&f=face
      fullname: Khalil
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Abdelhak
      type: user
    createdAt: '2023-07-25T23:23:36.000Z'
    data:
      edited: false
      editors:
      - Abdelhak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9935272932052612
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666635591305-noauth.jpeg?w=200&h=200&f=face
          fullname: Khalil
          isHf: false
          isPro: false
          name: Abdelhak
          type: user
        html: '<p>I noticed that. It''s much better and practical. Thank you very
          much.</p>

          '
        raw: I noticed that. It's much better and practical. Thank you very much.
        updatedAt: '2023-07-25T23:23:36.769Z'
      numEdits: 0
      reactions: []
    id: 64c05978f070a750cdae5dbf
    type: comment
  author: Abdelhak
  content: I noticed that. It's much better and practical. Thank you very much.
  created_at: 2023-07-25 22:23:36+00:00
  edited: false
  hidden: false
  id: 64c05978f070a750cdae5dbf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-07-26T07:59:25.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9264917373657227
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<blockquote>

          <p>I did the perplexity calculations. I could only do them at 2K because
          it failed at longer lengths - probably a limitation in the PPL code.  But
          even at 2K it showed a detectable difference.  </p>

          <p>Testing the best quant - the 32g + act-order file:  testing PPL with
          wikitext, the same dataset I used for quant, it''s about 1.5%. Not much.  With
          PTB it''s 7%, which is more significant.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/nR-7k8huMR-lGjqzTm7KW.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/nR-7k8huMR-lGjqzTm7KW.png"></a></p>

          <p>How this translates into real life results, if at all, I do not know.  But
          for now I guess I will be doing 8K models at 8K context.  It took 2.5 hours
          to do using A6000s!</p>

          <p>The re-done GPTQs with 8k context are now uploaded, or will be shortly</p>

          </blockquote>

          <p>This is excellent. Thanks for this </p>

          '
        raw: "> I did the perplexity calculations. I could only do them at 2K because\
          \ it failed at longer lengths - probably a limitation in the PPL code. \
          \ But even at 2K it showed a detectable difference.  \n> \n> Testing the\
          \ best quant - the 32g + act-order file:  testing PPL with wikitext, the\
          \ same dataset I used for quant, it's about 1.5%. Not much.  With PTB it's\
          \ 7%, which is more significant.\n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/nR-7k8huMR-lGjqzTm7KW.png)\n\
          > \n> How this translates into real life results, if at all, I do not know.\
          \  But for now I guess I will be doing 8K models at 8K context.  It took\
          \ 2.5 hours to do using A6000s!\n> \n> The re-done GPTQs with 8k context\
          \ are now uploaded, or will be shortly\n\nThis is excellent. Thanks for\
          \ this "
        updatedAt: '2023-07-26T07:59:25.179Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64c0d25df613170e7bf2e0dd
    id: 64c0d25df613170e7bf2e0db
    type: comment
  author: gsaivinay
  content: "> I did the perplexity calculations. I could only do them at 2K because\
    \ it failed at longer lengths - probably a limitation in the PPL code.  But even\
    \ at 2K it showed a detectable difference.  \n> \n> Testing the best quant - the\
    \ 32g + act-order file:  testing PPL with wikitext, the same dataset I used for\
    \ quant, it's about 1.5%. Not much.  With PTB it's 7%, which is more significant.\n\
    > \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/nR-7k8huMR-lGjqzTm7KW.png)\n\
    > \n> How this translates into real life results, if at all, I do not know.  But\
    \ for now I guess I will be doing 8K models at 8K context.  It took 2.5 hours\
    \ to do using A6000s!\n> \n> The re-done GPTQs with 8k context are now uploaded,\
    \ or will be shortly\n\nThis is excellent. Thanks for this "
  created_at: 2023-07-26 06:59:25+00:00
  edited: false
  hidden: false
  id: 64c0d25df613170e7bf2e0db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-07-26T07:59:25.000Z'
    data:
      status: closed
    id: 64c0d25df613170e7bf2e0dd
    type: status-change
  author: gsaivinay
  created_at: 2023-07-26 06:59:25+00:00
  id: 64c0d25df613170e7bf2e0dd
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ
repo_type: model
status: closed
target_branch: null
title: Many thanks
