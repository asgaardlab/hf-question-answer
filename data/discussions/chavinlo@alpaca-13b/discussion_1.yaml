!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Derg
conflicting_files: null
created_at: 2023-03-31 12:53:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ddf7d8e6475e45f19258938442475cdf.svg
      fullname: Marlorn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Derg
      type: user
    createdAt: '2023-03-31T13:53:54.000Z'
    data:
      edited: true
      editors:
      - Derg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ddf7d8e6475e45f19258938442475cdf.svg
          fullname: Marlorn
          isHf: false
          isPro: false
          name: Derg
          type: user
        html: '<p>I have tried converting the model using some older methods floating
          around and it seems to not be converting them to the correct ggml format.
          What is the current methodology or scripts for this? Specifically regarding
          converting the .bin to .pth EDIT: Nevermind, I have it working in llama.cpp
          now. Great job!</p>

          '
        raw: 'I have tried converting the model using some older methods floating
          around and it seems to not be converting them to the correct ggml format.
          What is the current methodology or scripts for this? Specifically regarding
          converting the .bin to .pth EDIT: Nevermind, I have it working in llama.cpp
          now. Great job!'
        updatedAt: '2023-03-31T15:20:12.640Z'
      numEdits: 2
      reactions: []
    id: 6426e5f216b6cce053e4b031
    type: comment
  author: Derg
  content: 'I have tried converting the model using some older methods floating around
    and it seems to not be converting them to the correct ggml format. What is the
    current methodology or scripts for this? Specifically regarding converting the
    .bin to .pth EDIT: Nevermind, I have it working in llama.cpp now. Great job!'
  created_at: 2023-03-31 12:53:54+00:00
  edited: true
  hidden: false
  id: 6426e5f216b6cce053e4b031
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a84447d462bae24c4cc32fe797f49a3b.svg
      fullname: Kris Tanto
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kaz9112
      type: user
    createdAt: '2023-03-31T15:28:59.000Z'
    data:
      edited: false
      editors:
      - kaz9112
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a84447d462bae24c4cc32fe797f49a3b.svg
          fullname: Kris Tanto
          isHf: false
          isPro: false
          name: kaz9112
          type: user
        html: '<blockquote>

          <p>I have tried converting the model using some older methods floating around
          and it seems to not be converting them to the correct ggml format. What
          is the current methodology or scripts for this? Specifically regarding converting
          the .bin to .pth EDIT: Nevermind, I have it working in llama.cpp now. Great
          job!</p>

          </blockquote>

          <p>how??</p>

          '
        raw: '> I have tried converting the model using some older methods floating
          around and it seems to not be converting them to the correct ggml format.
          What is the current methodology or scripts for this? Specifically regarding
          converting the .bin to .pth EDIT: Nevermind, I have it working in llama.cpp
          now. Great job!


          how??'
        updatedAt: '2023-03-31T15:28:59.654Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - lucasjin
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - lucasjin
      - count: 1
        reaction: "\U0001F614"
        users:
        - lucasjin
      - count: 1
        reaction: "\U0001F92F"
        users:
        - lucasjin
      - count: 1
        reaction: "\U0001F91D"
        users:
        - lucasjin
    id: 6426fc3b188b9f01b179e930
    type: comment
  author: kaz9112
  content: '> I have tried converting the model using some older methods floating
    around and it seems to not be converting them to the correct ggml format. What
    is the current methodology or scripts for this? Specifically regarding converting
    the .bin to .pth EDIT: Nevermind, I have it working in llama.cpp now. Great job!


    how??'
  created_at: 2023-03-31 14:28:59+00:00
  edited: false
  hidden: false
  id: 6426fc3b188b9f01b179e930
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/740e53fd63d0cfda723793c5b338192f.svg
      fullname: Elie Tsai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Revert4865
      type: user
    createdAt: '2023-04-01T05:11:38.000Z'
    data:
      edited: false
      editors:
      - Revert4865
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/740e53fd63d0cfda723793c5b338192f.svg
          fullname: Elie Tsai
          isHf: false
          isPro: false
          name: Revert4865
          type: user
        html: '<p>Could a kind soul provide a magnet of the ggml quantitized binaries
          for llama.cpp?</p>

          '
        raw: Could a kind soul provide a magnet of the ggml quantitized binaries for
          llama.cpp?
        updatedAt: '2023-04-01T05:11:38.920Z'
      numEdits: 0
      reactions: []
    id: 6427bd0abf11884bbfec265f
    type: comment
  author: Revert4865
  content: Could a kind soul provide a magnet of the ggml quantitized binaries for
    llama.cpp?
  created_at: 2023-04-01 04:11:38+00:00
  edited: false
  hidden: false
  id: 6427bd0abf11884bbfec265f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef4aa134664f17673c4d9480bfaeff67.svg
      fullname: Diego Amicabile
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: diegoami
      type: user
    createdAt: '2023-05-08T20:04:42.000Z'
    data:
      edited: false
      editors:
      - diegoami
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef4aa134664f17673c4d9480bfaeff67.svg
          fullname: Diego Amicabile
          isHf: false
          isPro: false
          name: diegoami
          type: user
        html: '<p>Yeah, how ?</p>

          '
        raw: Yeah, how ?
        updatedAt: '2023-05-08T20:04:42.948Z'
      numEdits: 0
      reactions: []
    id: 645955da39e6aea69cc44523
    type: comment
  author: diegoami
  content: Yeah, how ?
  created_at: 2023-05-08 19:04:42+00:00
  edited: false
  hidden: false
  id: 645955da39e6aea69cc44523
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/248fd83199b95d35b5cce33e65ab3d5c.svg
      fullname: Ivan Stepanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ivanstepanovftw
      type: user
    createdAt: '2023-05-08T20:37:02.000Z'
    data:
      edited: false
      editors:
      - ivanstepanovftw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/248fd83199b95d35b5cce33e65ab3d5c.svg
          fullname: Ivan Stepanov
          isHf: false
          isPro: false
          name: ivanstepanovftw
          type: user
        html: '<p>Just use convert.py script in the root of llama.cpp</p>

          '
        raw: Just use convert.py script in the root of llama.cpp
        updatedAt: '2023-05-08T20:37:02.996Z'
      numEdits: 0
      reactions: []
    id: 64595d6e39e6aea69cc4c065
    type: comment
  author: ivanstepanovftw
  content: Just use convert.py script in the root of llama.cpp
  created_at: 2023-05-08 19:37:02+00:00
  edited: false
  hidden: false
  id: 64595d6e39e6aea69cc4c065
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: chavinlo/alpaca-13b
repo_type: model
status: open
target_branch: null
title: What is the current method to convert these models to work with Llama.cpp?
