!!python/object:huggingface_hub.community.DiscussionWithDetails
author: notbdq
conflicting_files: null
created_at: 2024-01-06 16:25:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6566b0e05710d2cdddf92a6e/HdDePP6Ofqth41TwLkq8R.png?w=200&h=200&f=face
      fullname: notbdq
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: notbdq
      type: user
    createdAt: '2024-01-06T16:25:19.000Z'
    data:
      edited: true
      editors:
      - notbdq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9356344938278198
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6566b0e05710d2cdddf92a6e/HdDePP6Ofqth41TwLkq8R.png?w=200&h=200&f=face
          fullname: notbdq
          isHf: false
          isPro: false
          name: notbdq
          type: user
        html: '<p>your models are not the actual model, they are adapter models, they
          are only weights of fine tuned part of the actual model. and you cant directly
          use it because its a part of model. you have to merge main model with your
          adapter model. i had the same issue too and i can help you merge your adapter
          weights with main model. you can check my model in my profile, which is
          a fine tuned mistral-7b-v0.2 which is fine tuned by me with these steps
          : fine tuned using qlora, merged adapter weights with main model and quantized
          it in gguf. now it can answer my questions in english and turkish as you
          can see;<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6566b0e05710d2cdddf92a6e/zooQDD6GeNlZ68vUG-yHU.png"><img
          alt="Screenshot 2024-01-05 at 08.37.59.png" src="https://cdn-uploads.huggingface.co/production/uploads/6566b0e05710d2cdddf92a6e/zooQDD6GeNlZ68vUG-yHU.png"></a></p>

          '
        raw: 'your models are not the actual model, they are adapter models, they
          are only weights of fine tuned part of the actual model. and you cant directly
          use it because its a part of model. you have to merge main model with your
          adapter model. i had the same issue too and i can help you merge your adapter
          weights with main model. you can check my model in my profile, which is
          a fine tuned mistral-7b-v0.2 which is fine tuned by me with these steps
          : fine tuned using qlora, merged adapter weights with main model and quantized
          it in gguf. now it can answer my questions in english and turkish as you
          can see;

          ![Screenshot 2024-01-05 at 08.37.59.png](https://cdn-uploads.huggingface.co/production/uploads/6566b0e05710d2cdddf92a6e/zooQDD6GeNlZ68vUG-yHU.png)

          '
        updatedAt: '2024-01-06T16:46:04.605Z'
      numEdits: 1
      reactions: []
    id: 65997eefe6df49a09db6336e
    type: comment
  author: notbdq
  content: 'your models are not the actual model, they are adapter models, they are
    only weights of fine tuned part of the actual model. and you cant directly use
    it because its a part of model. you have to merge main model with your adapter
    model. i had the same issue too and i can help you merge your adapter weights
    with main model. you can check my model in my profile, which is a fine tuned mistral-7b-v0.2
    which is fine tuned by me with these steps : fine tuned using qlora, merged adapter
    weights with main model and quantized it in gguf. now it can answer my questions
    in english and turkish as you can see;

    ![Screenshot 2024-01-05 at 08.37.59.png](https://cdn-uploads.huggingface.co/production/uploads/6566b0e05710d2cdddf92a6e/zooQDD6GeNlZ68vUG-yHU.png)

    '
  created_at: 2024-01-06 16:25:19+00:00
  edited: true
  hidden: false
  id: 65997eefe6df49a09db6336e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6566b0e05710d2cdddf92a6e/HdDePP6Ofqth41TwLkq8R.png?w=200&h=200&f=face
      fullname: notbdq
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: notbdq
      type: user
    createdAt: '2024-01-06T16:37:13.000Z'
    data:
      from: hello, why not merging merging model with your adapter model?
      to: hello, why not merging model with your adapter model?
    id: 659981b907198ffcf7adb883
    type: title-change
  author: notbdq
  created_at: 2024-01-06 16:37:13+00:00
  id: 659981b907198ffcf7adb883
  new_title: hello, why not merging model with your adapter model?
  old_title: hello, why not merging merging model with your adapter model?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aab8bd6d1f64852f4f81a3a4ffa326a.svg
      fullname: Ramchandra Vikas Chamarthi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RVikas
      type: user
    createdAt: '2024-01-07T01:59:17.000Z'
    data:
      edited: false
      editors:
      - RVikas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9471194744110107
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aab8bd6d1f64852f4f81a3a4ffa326a.svg
          fullname: Ramchandra Vikas Chamarthi
          isHf: false
          isPro: false
          name: RVikas
          type: user
        html: '<p>Hello, We have a new service that lets you deploy adapters along
          with baseModels. We don''t merge them as it is done dynamically during runtime.
          This also helps us to prevent bloating weight file size. </p>

          <p>Check out our service here: <a rel="nofollow" href="https://developer.monsterapi.ai/docs/monster-deploy-beta">https://developer.monsterapi.ai/docs/monster-deploy-beta</a>
          and apply for the beta here, we provide free credits for people to try out
          and provide us feedback. Let us know if we can help with using this tool.</p>

          '
        raw: "Hello, We have a new service that lets you deploy adapters along with\
          \ baseModels. We don't merge them as it is done dynamically during runtime.\
          \ This also helps us to prevent bloating weight file size. \n\nCheck out\
          \ our service here: https://developer.monsterapi.ai/docs/monster-deploy-beta\
          \ and apply for the beta here, we provide free credits for people to try\
          \ out and provide us feedback. Let us know if we can help with using this\
          \ tool.\n\n"
        updatedAt: '2024-01-07T01:59:17.351Z'
      numEdits: 0
      reactions: []
    id: 659a057511b48706bab223a0
    type: comment
  author: RVikas
  content: "Hello, We have a new service that lets you deploy adapters along with\
    \ baseModels. We don't merge them as it is done dynamically during runtime. This\
    \ also helps us to prevent bloating weight file size. \n\nCheck out our service\
    \ here: https://developer.monsterapi.ai/docs/monster-deploy-beta and apply for\
    \ the beta here, we provide free credits for people to try out and provide us\
    \ feedback. Let us know if we can help with using this tool.\n\n"
  created_at: 2024-01-07 01:59:17+00:00
  edited: false
  hidden: false
  id: 659a057511b48706bab223a0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: qblocks/codellama_7b_DolphinCoder
repo_type: model
status: open
target_branch: null
title: hello, why not merging model with your adapter model?
