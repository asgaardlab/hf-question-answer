!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mhyatt000
conflicting_files: null
created_at: 2022-09-01 13:51:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658862186149-62b09fe1a14cbd64386c042d.jpeg?w=200&h=200&f=face
      fullname: Matt Hyatt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mhyatt000
      type: user
    createdAt: '2022-09-01T14:51:43.000Z'
    data:
      edited: false
      editors:
      - mhyatt000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658862186149-62b09fe1a14cbd64386c042d.jpeg?w=200&h=200&f=face
          fullname: Matt Hyatt
          isHf: false
          isPro: false
          name: mhyatt000
          type: user
        html: "<p>I tried to reproduce the results mentioned on this model card. Seems\
          \ like my results do not match the claimed mAP in the model card. I cannot\
          \ figure out how to get the correct numbers, can you help me find my mistake?</p>\n\
          <ul>\n<li>Claimed mAP: 42.0</li>\n<li>Recieved mAP: 36.6</li>\n</ul>\n<p>Here\
          \ are the details for my validation:</p>\n<ul>\n<li>I instantiate pre-trained\
          \ model with <code>transformers.pipeline()</code> and use COCO API to calculate\
          \ AP from detection bboxes. </li>\n<li>Evaluation was performed on macOS\
          \ CPU.</li>\n<li>Dataset was downloaded from cocodataset.org</li>\n</ul>\n\
          <hr>\n<pre><code> Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all\
          \ | maxDets=100 ] = 0.366\n Average Precision  (AP) @[ IoU=0.50      | area=\
          \   all | maxDets=100 ] = 0.567\n Average Precision  (AP) @[ IoU=0.75  \
          \    | area=   all | maxDets=100 ] = 0.379\n Average Precision  (AP) @[\
          \ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.144\n Average Precision\
          \  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.397\n Average\
          \ Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.585\n\
          \ Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ]\
          \ = 0.303\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=\
          \ 10 ] = 0.441\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all\
          \ | maxDets=100 ] = 0.452\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=\
          \ small | maxDets=100 ] = 0.193\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area=medium | maxDets=100 ] = 0.491\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area= large | maxDets=100 ] = 0.687\n</code></pre>\n"
        raw: "I tried to reproduce the results mentioned on this model card. Seems\
          \ like my results do not match the claimed mAP in the model card. I cannot\
          \ figure out how to get the correct numbers, can you help me find my mistake?\r\
          \n\r\n- Claimed mAP: 42.0\r\n- Recieved mAP: 36.6\r\n\r\nHere are the details\
          \ for my validation:\r\n\r\n- I instantiate pre-trained model with `transformers.pipeline()`\
          \ and use COCO API to calculate AP from detection bboxes. \r\n- Evaluation\
          \ was performed on macOS CPU.\r\n- Dataset was downloaded from cocodataset.org\r\
          \n---\r\n\r\n```\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=  \
          \ all | maxDets=100 ] = 0.366\r\n Average Precision  (AP) @[ IoU=0.50  \
          \    | area=   all | maxDets=100 ] = 0.567\r\n Average Precision  (AP) @[\
          \ IoU=0.75      | area=   all | maxDets=100 ] = 0.379\r\n Average Precision\
          \  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.144\r\n Average\
          \ Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.397\r\
          \n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100\
          \ ] = 0.585\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all |\
          \ maxDets=  1 ] = 0.303\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=\
          \   all | maxDets= 10 ] = 0.441\r\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area=   all | maxDets=100 ] = 0.452\r\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area= small | maxDets=100 ] = 0.193\r\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area=medium | maxDets=100 ] = 0.491\r\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area= large | maxDets=100 ] = 0.687\r\n```"
        updatedAt: '2022-09-01T14:51:43.575Z'
      numEdits: 0
      reactions: []
    id: 6310c6ff34c7d77420b359ff
    type: comment
  author: mhyatt000
  content: "I tried to reproduce the results mentioned on this model card. Seems like\
    \ my results do not match the claimed mAP in the model card. I cannot figure out\
    \ how to get the correct numbers, can you help me find my mistake?\r\n\r\n- Claimed\
    \ mAP: 42.0\r\n- Recieved mAP: 36.6\r\n\r\nHere are the details for my validation:\r\
    \n\r\n- I instantiate pre-trained model with `transformers.pipeline()` and use\
    \ COCO API to calculate AP from detection bboxes. \r\n- Evaluation was performed\
    \ on macOS CPU.\r\n- Dataset was downloaded from cocodataset.org\r\n---\r\n\r\n\
    ```\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ]\
    \ = 0.366\r\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100\
    \ ] = 0.567\r\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100\
    \ ] = 0.379\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100\
    \ ] = 0.144\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100\
    \ ] = 0.397\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100\
    \ ] = 0.585\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=\
    \  1 ] = 0.303\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=\
    \ 10 ] = 0.441\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100\
    \ ] = 0.452\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100\
    \ ] = 0.193\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100\
    \ ] = 0.491\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100\
    \ ] = 0.687\r\n```"
  created_at: 2022-09-01 13:51:43+00:00
  edited: false
  hidden: false
  id: 6310c6ff34c7d77420b359ff
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: hustvl/yolos-base
repo_type: model
status: open
target_branch: null
title: mAP Drop
