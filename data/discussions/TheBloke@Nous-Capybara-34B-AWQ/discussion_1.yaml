!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wangdafa
conflicting_files: null
created_at: 2023-11-21 03:01:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/35beb07ac59d619b90140e20e2a8fe3c.svg
      fullname: shi guoqing
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wangdafa
      type: user
    createdAt: '2023-11-21T03:01:58.000Z'
    data:
      edited: false
      editors:
      - wangdafa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.35915952920913696
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/35beb07ac59d619b90140e20e2a8fe3c.svg
          fullname: shi guoqing
          isHf: false
          isPro: false
          name: wangdafa
          type: user
        html: "<p>I used two 4090 to run this model and still OOM\u3002</p>\n<pre><code>python\
          \ -m fastchat.serve.vllm_worker --model-path TheBloke/Nous-Capybara-34B-AWQ\
          \ --trust-remote-code --tensor-parallel-size 2 --quantization awq  --dtype\
          \ auto\n</code></pre>\n"
        raw: "I used two 4090 to run this model and still OOM\u3002\r\n\r\n```\r\n\
          python -m fastchat.serve.vllm_worker --model-path TheBloke/Nous-Capybara-34B-AWQ\
          \ --trust-remote-code --tensor-parallel-size 2 --quantization awq  --dtype\
          \ auto\r\n```"
        updatedAt: '2023-11-21T03:01:58.140Z'
      numEdits: 0
      reactions: []
    id: 655c1da6b426ec8f4b582e68
    type: comment
  author: wangdafa
  content: "I used two 4090 to run this model and still OOM\u3002\r\n\r\n```\r\npython\
    \ -m fastchat.serve.vllm_worker --model-path TheBloke/Nous-Capybara-34B-AWQ --trust-remote-code\
    \ --tensor-parallel-size 2 --quantization awq  --dtype auto\r\n```"
  created_at: 2023-11-21 03:01:58+00:00
  edited: false
  hidden: false
  id: 655c1da6b426ec8f4b582e68
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-21T04:45:07.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8379786610603333
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>You should limit the context length to 8192, otherwise it will try
          to load 200K context length which roughly need extra 40G vram</p>

          '
        raw: You should limit the context length to 8192, otherwise it will try to
          load 200K context length which roughly need extra 40G vram
        updatedAt: '2023-11-21T04:45:07.951Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - wangdafa
    id: 655c35d321a128df0acc1275
    type: comment
  author: Yhyu13
  content: You should limit the context length to 8192, otherwise it will try to load
    200K context length which roughly need extra 40G vram
  created_at: 2023-11-21 04:45:07+00:00
  edited: false
  hidden: false
  id: 655c35d321a128df0acc1275
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/35beb07ac59d619b90140e20e2a8fe3c.svg
      fullname: shi guoqing
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wangdafa
      type: user
    createdAt: '2023-11-21T05:37:27.000Z'
    data:
      edited: false
      editors:
      - wangdafa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8243684768676758
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/35beb07ac59d619b90140e20e2a8fe3c.svg
          fullname: shi guoqing
          isHf: false
          isPro: false
          name: wangdafa
          type: user
        html: '<blockquote>

          <p>You should limit the context length to 8192, otherwise it will try to
          load 200K context length which roughly need extra 40G vram</p>

          </blockquote>

          <p>Thank you for your answer. The problem has been resolved</p>

          <p>But I met a new problem, that is, vllm seems to not support the template
          of Nous-Capybara-34B, even if i specify --conv-template manticore. Is this
          a bug of vllm?</p>

          '
        raw: '

          > You should limit the context length to 8192, otherwise it will try to
          load 200K context length which roughly need extra 40G vram



          Thank you for your answer. The problem has been resolved


          But I met a new problem, that is, vllm seems to not support the template
          of Nous-Capybara-34B, even if i specify --conv-template manticore. Is this
          a bug of vllm?


          '
        updatedAt: '2023-11-21T05:37:27.831Z'
      numEdits: 0
      reactions: []
    id: 655c4217b426ec8f4b5e1c9e
    type: comment
  author: wangdafa
  content: '

    > You should limit the context length to 8192, otherwise it will try to load 200K
    context length which roughly need extra 40G vram



    Thank you for your answer. The problem has been resolved


    But I met a new problem, that is, vllm seems to not support the template of Nous-Capybara-34B,
    even if i specify --conv-template manticore. Is this a bug of vllm?


    '
  created_at: 2023-11-21 05:37:27+00:00
  edited: false
  hidden: false
  id: 655c4217b426ec8f4b5e1c9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-21T06:10:25.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9085434079170227
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;wangdafa&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/wangdafa\">@<span class=\"\
          underline\">wangdafa</span></a></span>\n\n\t</span></span> </p>\n<p><a rel=\"\
          nofollow\" href=\"https://github.com/lm-sys/FastChat/blob/e53c73f22efa9a37bf76af8783c96049276a2e98/fastchat/conversation.py#L789\"\
          >https://github.com/lm-sys/FastChat/blob/e53c73f22efa9a37bf76af8783c96049276a2e98/fastchat/conversation.py#L789</a></p>\n\
          <p>I don't think manticore has system prompt, maybe that was the cause.\
          \ Try Vicuna or airoboros?</p>\n"
        raw: "@wangdafa \n\nhttps://github.com/lm-sys/FastChat/blob/e53c73f22efa9a37bf76af8783c96049276a2e98/fastchat/conversation.py#L789\n\
          \nI don't think manticore has system prompt, maybe that was the cause. Try\
          \ Vicuna or airoboros?"
        updatedAt: '2023-11-21T06:10:25.589Z'
      numEdits: 0
      reactions: []
    id: 655c49d160b44278af706b1e
    type: comment
  author: Yhyu13
  content: "@wangdafa \n\nhttps://github.com/lm-sys/FastChat/blob/e53c73f22efa9a37bf76af8783c96049276a2e98/fastchat/conversation.py#L789\n\
    \nI don't think manticore has system prompt, maybe that was the cause. Try Vicuna\
    \ or airoboros?"
  created_at: 2023-11-21 06:10:25+00:00
  edited: false
  hidden: false
  id: 655c49d160b44278af706b1e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/35beb07ac59d619b90140e20e2a8fe3c.svg
      fullname: shi guoqing
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wangdafa
      type: user
    createdAt: '2023-12-18T03:04:01.000Z'
    data:
      status: closed
    id: 657fb6a1647c0211e7db25a0
    type: status-change
  author: wangdafa
  created_at: 2023-12-18 03:04:01+00:00
  id: 657fb6a1647c0211e7db25a0
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Nous-Capybara-34B-AWQ
repo_type: model
status: closed
target_branch: null
title: How much memory does this model require?
