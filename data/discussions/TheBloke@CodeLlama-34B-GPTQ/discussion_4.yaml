!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Onix22
conflicting_files: null
created_at: 2023-12-24 01:36:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04297acee24206d3972d73a2bc960ee8.svg
      fullname: tp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Onix22
      type: user
    createdAt: '2023-12-24T01:36:36.000Z'
    data:
      edited: false
      editors:
      - Onix22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9697455763816833
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04297acee24206d3972d73a2bc960ee8.svg
          fullname: tp
          isHf: false
          isPro: false
          name: Onix22
          type: user
        html: '<p>I have no clue how to explain this situation:<br>This model is supposed
          to be 34B so it should take lots of  VRAM and leave very little memory for
          context, however in some unknown way it manages to fit 16k tokens into 24gb
          vram   when even 20B models will only fit 8k<br>and in fact it can remember
          what was said in the very beginning of the text if asked the question.</p>

          <p>This is happening on exllamav2 newest version probably with flash attention,
          but other models still can''t match this absurd context size.</p>

          '
        raw: "I have no clue how to explain this situation:\r\nThis model is supposed\
          \ to be 34B so it should take lots of  VRAM and leave very little memory\
          \ for context, however in some unknown way it manages to fit 16k tokens\
          \ into 24gb vram   when even 20B models will only fit 8k\r\nand in fact\
          \ it can remember what was said in the very beginning of the text if asked\
          \ the question.\r\n\r\nThis is happening on exllamav2 newest version probably\
          \ with flash attention, but other models still can't match this absurd context\
          \ size."
        updatedAt: '2023-12-24T01:36:36.763Z'
      numEdits: 0
      reactions: []
    id: 65878b2435f23c0f1cf79cc1
    type: comment
  author: Onix22
  content: "I have no clue how to explain this situation:\r\nThis model is supposed\
    \ to be 34B so it should take lots of  VRAM and leave very little memory for context,\
    \ however in some unknown way it manages to fit 16k tokens into 24gb vram   when\
    \ even 20B models will only fit 8k\r\nand in fact it can remember what was said\
    \ in the very beginning of the text if asked the question.\r\n\r\nThis is happening\
    \ on exllamav2 newest version probably with flash attention, but other models\
    \ still can't match this absurd context size."
  created_at: 2023-12-24 01:36:36+00:00
  edited: false
  hidden: false
  id: 65878b2435f23c0f1cf79cc1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-12-25T20:42:52.000Z'
    data:
      edited: true
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.900985598564148
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Onix22&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Onix22\">@<span class=\"\
          underline\">Onix22</span></a></span>\n\n\t</span></span> the reason is the\
          \ codellama models, 70b llama 2 model, mistral models, mixtral models have\
          \ gqa(grouped query attention) instead of mqa. gqa basically allows the\
          \ model to use much less vram with less context so thats why its happening.\
          \ </p>\n<p>The 13b, 7b llama 2 and llama 1 models do not have gqa. 20b models\
          \ are usually merges of 13b so it will also not have gqa</p>\n"
        raw: "@Onix22 the reason is the codellama models, 70b llama 2 model, mistral\
          \ models, mixtral models have gqa(grouped query attention) instead of mqa.\
          \ gqa basically allows the model to use much less vram with less context\
          \ so thats why its happening. \n\nThe 13b, 7b llama 2 and llama 1 models\
          \ do not have gqa. 20b models are usually merges of 13b so it will also\
          \ not have gqa"
        updatedAt: '2023-12-25T20:43:15.764Z'
      numEdits: 1
      reactions: []
    id: 6589e94c61f2dd8f66a7328f
    type: comment
  author: YaTharThShaRma999
  content: "@Onix22 the reason is the codellama models, 70b llama 2 model, mistral\
    \ models, mixtral models have gqa(grouped query attention) instead of mqa. gqa\
    \ basically allows the model to use much less vram with less context so thats\
    \ why its happening. \n\nThe 13b, 7b llama 2 and llama 1 models do not have gqa.\
    \ 20b models are usually merges of 13b so it will also not have gqa"
  created_at: 2023-12-25 20:42:52+00:00
  edited: true
  hidden: false
  id: 6589e94c61f2dd8f66a7328f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04297acee24206d3972d73a2bc960ee8.svg
      fullname: tp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Onix22
      type: user
    createdAt: '2023-12-26T00:02:33.000Z'
    data:
      edited: false
      editors:
      - Onix22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9542672634124756
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04297acee24206d3972d73a2bc960ee8.svg
          fullname: tp
          isHf: false
          isPro: false
          name: Onix22
          type: user
        html: '<p>This was useful information  It should be mentioned somewhere more.<br>Because
          for this fact, 34 B model managed to fit more context than 13B model<br>sadly
          there are no other models of the same size to use</p>

          '
        raw: 'This was useful information  It should be mentioned somewhere more.

          Because for this fact, 34 B model managed to fit more context than 13B model

          sadly there are no other models of the same size to use'
        updatedAt: '2023-12-26T00:02:33.249Z'
      numEdits: 0
      reactions: []
    id: 658a1819bbb04840e3c98440
    type: comment
  author: Onix22
  content: 'This was useful information  It should be mentioned somewhere more.

    Because for this fact, 34 B model managed to fit more context than 13B model

    sadly there are no other models of the same size to use'
  created_at: 2023-12-26 00:02:33+00:00
  edited: false
  hidden: false
  id: 658a1819bbb04840e3c98440
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/CodeLlama-34B-GPTQ
repo_type: model
status: open
target_branch: null
title: This model breaks laws of physic.
