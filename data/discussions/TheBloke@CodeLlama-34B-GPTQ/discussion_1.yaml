!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HAvietisov
conflicting_files: null
created_at: 2023-08-25 14:39:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
      fullname: Hlib Avietisov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HAvietisov
      type: user
    createdAt: '2023-08-25T15:39:22.000Z'
    data:
      edited: false
      editors:
      - HAvietisov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4260087013244629
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
          fullname: Hlib Avietisov
          isHf: false
          isPro: false
          name: HAvietisov
          type: user
        html: "<p>Tried to run this using sample code from the description:</p>\n\
          <pre><code class=\"language-from\">from auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\n\nmodel_name_or_path = \"TheBloke/CodeLlama-34B-GPTQ\"\
          \n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        use_safetensors=True,\n        trust_remote_code=False,\n     \
          \   device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          \n\"\"\"\n# To download from a specific branch, use the revision parameter,\
          \ as in this example:\n# Note that `revision` requires AutoGPTQ 0.3.1 or\
          \ later!\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        revision=\"gptq-4bit-32g-actorder_True\",\n        use_safetensors=True,\n\
          \        trust_remote_code=False,\n        device=\"cuda:0\",\n        quantize_config=None)\n\
          \"\"\"\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''Info on prompt\
          \ template will be added shortly.\n'''\n\nprint(\"\\n\\n*** Generate:\"\
          )\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n\n# Inference can also be done using\
          \ transformers' pipeline\n\n# Prevent printing spurious transformers error\
          \ when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
          \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n \
          \   temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\n\
          print(pipe(prompt_template)[0]['generated_text'])\n</code></pre>\n<p>Got\
          \ this error : </p>\n<pre><code>Cell In[1], line 36\n     33 print(\"\\\
          n\\n*** Generate:\")\n     35 input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          ---&gt; 36 output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          \     37 print(tokenizer.decode(output[0]))\n     39 # Inference can also\
          \ be done using transformers' pipeline\n     40 \n     41 # Prevent printing\
          \ spurious transformers error when using pipeline with AutoGPTQ\n\nFile\
          \ /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:443,\
          \ in BaseGPTQForCausalLM.generate(self, **kwargs)\n    441 \"\"\"shortcut\
          \ for model.generate\"\"\"\n    442 with torch.inference_mode(), torch.amp.autocast(device_type=self.device.type):\n\
          --&gt; 443     return self.model.generate(**kwargs)\n\nFile ~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115,\
          \ in context_decorator..decorate_context(*args, **kwargs)\n    112 @functools.wraps(func)\n\
          \    113 def decorate_context(*args, **kwargs):\n    114     with ctx_factory():\n\
          --&gt; 115         return func(*args, **kwargs)\n\nFile ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1596,\
          \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\n\
          \   1579     return self.assisted_decoding(\n   1580         input_ids,\n\
          ...\n---&gt; 54 query_states, key_states, value_states = torch.split(qkv_states,\
          \ self.hidden_size, dim=2)\n     56 query_states = query_states.view(bsz,\
          \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\n     57 key_states\
          \ = key_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1,\
          \ 2)\n\nValueError: not enough values to unpack (expected 3, got 2)\n</code></pre>\n\
          <p>Package versions :<br>Auto-gptq : Version: 0.4.1+cu117<br>torch version\
          \ : Version: 2.0.1<br>bitsandbytes : Version: 0.41.1<br>transformers : Version:\
          \ 4.32.0</p>\n"
        raw: "Tried to run this using sample code from the description:\r\n\r\n```from\
          \ transformers import AutoTokenizer, pipeline, logging\r\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\r\n\r\nmodel_name_or_path\
          \ = \"TheBloke/CodeLlama-34B-GPTQ\"\r\n\r\nuse_triton = False\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\r\n\
          \r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\n \
          \       use_safetensors=True,\r\n        trust_remote_code=False,\r\n  \
          \      device=\"cuda:0\",\r\n        use_triton=use_triton,\r\n        quantize_config=None)\r\
          \n\r\n\"\"\"\r\n# To download from a specific branch, use the revision parameter,\
          \ as in this example:\r\n# Note that `revision` requires AutoGPTQ 0.3.1\
          \ or later!\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
          \n        revision=\"gptq-4bit-32g-actorder_True\",\r\n        use_safetensors=True,\r\
          \n        trust_remote_code=False,\r\n        device=\"cuda:0\",\r\n   \
          \     quantize_config=None)\r\n\"\"\"\r\n\r\nprompt = \"Tell me about AI\"\
          \r\nprompt_template=f'''Info on prompt template will be added shortly.\r\
          \n'''\r\n\r\nprint(\"\\n\\n*** Generate:\")\r\n\r\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\r\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, max_new_tokens=512)\r\nprint(tokenizer.decode(output[0]))\r\
          \n\r\n# Inference can also be done using transformers' pipeline\r\n\r\n\
          # Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ\r\nlogging.set_verbosity(logging.CRITICAL)\r\n\r\nprint(\"***\
          \ Pipeline:\")\r\npipe = pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
          \n    tokenizer=tokenizer,\r\n    max_new_tokens=512,\r\n    temperature=0.7,\r\
          \n    top_p=0.95,\r\n    repetition_penalty=1.15\r\n)\r\n\r\nprint(pipe(prompt_template)[0]['generated_text'])\r\
          \n```\r\nGot this error : \r\n\r\n```\r\nCell In[1], line 36\r\n     33\
          \ print(\"\\n\\n*** Generate:\")\r\n     35 input_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\r\n---> 36 output = model.generate(inputs=input_ids,\
          \ temperature=0.7, max_new_tokens=512)\r\n     37 print(tokenizer.decode(output[0]))\r\
          \n     39 # Inference can also be done using transformers' pipeline\r\n\
          \     40 \r\n     41 # Prevent printing spurious transformers error when\
          \ using pipeline with AutoGPTQ\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:443,\
          \ in BaseGPTQForCausalLM.generate(self, **kwargs)\r\n    441 \"\"\"shortcut\
          \ for model.generate\"\"\"\r\n    442 with torch.inference_mode(), torch.amp.autocast(device_type=self.device.type):\r\
          \n--> 443     return self.model.generate(**kwargs)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115,\
          \ in context_decorator..decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\
          \n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\
          \n--> 115         return func(*args, **kwargs)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1596,\
          \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\r\
          \n   1579     return self.assisted_decoding(\r\n   1580         input_ids,\r\
          \n...\r\n---> 54 query_states, key_states, value_states = torch.split(qkv_states,\
          \ self.hidden_size, dim=2)\r\n     56 query_states = query_states.view(bsz,\
          \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\r\n     57 key_states\
          \ = key_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1,\
          \ 2)\r\n\r\nValueError: not enough values to unpack (expected 3, got 2)\r\
          \n```\r\nPackage versions : \r\nAuto-gptq : Version: 0.4.1+cu117\r\ntorch\
          \ version : Version: 2.0.1\r\nbitsandbytes : Version: 0.41.1\r\ntransformers\
          \ : Version: 4.32.0\r\n"
        updatedAt: '2023-08-25T15:39:22.083Z'
      numEdits: 0
      reactions: []
    id: 64e8cb2a748eb958106efdc7
    type: comment
  author: HAvietisov
  content: "Tried to run this using sample code from the description:\r\n\r\n```from\
    \ transformers import AutoTokenizer, pipeline, logging\r\nfrom auto_gptq import\
    \ AutoGPTQForCausalLM, BaseQuantizeConfig\r\n\r\nmodel_name_or_path = \"TheBloke/CodeLlama-34B-GPTQ\"\
    \r\n\r\nuse_triton = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
    \n        use_safetensors=True,\r\n        trust_remote_code=False,\r\n      \
    \  device=\"cuda:0\",\r\n        use_triton=use_triton,\r\n        quantize_config=None)\r\
    \n\r\n\"\"\"\r\n# To download from a specific branch, use the revision parameter,\
    \ as in this example:\r\n# Note that `revision` requires AutoGPTQ 0.3.1 or later!\r\
    \n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\n     \
    \   revision=\"gptq-4bit-32g-actorder_True\",\r\n        use_safetensors=True,\r\
    \n        trust_remote_code=False,\r\n        device=\"cuda:0\",\r\n        quantize_config=None)\r\
    \n\"\"\"\r\n\r\nprompt = \"Tell me about AI\"\r\nprompt_template=f'''Info on prompt\
    \ template will be added shortly.\r\n'''\r\n\r\nprint(\"\\n\\n*** Generate:\"\
    )\r\n\r\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\r\
    \noutput = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\r\
    \nprint(tokenizer.decode(output[0]))\r\n\r\n# Inference can also be done using\
    \ transformers' pipeline\r\n\r\n# Prevent printing spurious transformers error\
    \ when using pipeline with AutoGPTQ\r\nlogging.set_verbosity(logging.CRITICAL)\r\
    \n\r\nprint(\"*** Pipeline:\")\r\npipe = pipeline(\r\n    \"text-generation\"\
    ,\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n    max_new_tokens=512,\r\
    \n    temperature=0.7,\r\n    top_p=0.95,\r\n    repetition_penalty=1.15\r\n)\r\
    \n\r\nprint(pipe(prompt_template)[0]['generated_text'])\r\n```\r\nGot this error\
    \ : \r\n\r\n```\r\nCell In[1], line 36\r\n     33 print(\"\\n\\n*** Generate:\"\
    )\r\n     35 input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\r\
    \n---> 36 output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\r\
    \n     37 print(tokenizer.decode(output[0]))\r\n     39 # Inference can also be\
    \ done using transformers' pipeline\r\n     40 \r\n     41 # Prevent printing\
    \ spurious transformers error when using pipeline with AutoGPTQ\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:443,\
    \ in BaseGPTQForCausalLM.generate(self, **kwargs)\r\n    441 \"\"\"shortcut for\
    \ model.generate\"\"\"\r\n    442 with torch.inference_mode(), torch.amp.autocast(device_type=self.device.type):\r\
    \n--> 443     return self.model.generate(**kwargs)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115,\
    \ in context_decorator..decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\
    \n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\
    \n--> 115         return func(*args, **kwargs)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1596,\
    \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
    \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer,\
    \ negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\r\n   1579  \
    \   return self.assisted_decoding(\r\n   1580         input_ids,\r\n...\r\n--->\
    \ 54 query_states, key_states, value_states = torch.split(qkv_states, self.hidden_size,\
    \ dim=2)\r\n     56 query_states = query_states.view(bsz, q_len, self.num_heads,\
    \ self.head_dim).transpose(1, 2)\r\n     57 key_states = key_states.view(bsz,\
    \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\r\n\r\nValueError: not\
    \ enough values to unpack (expected 3, got 2)\r\n```\r\nPackage versions : \r\n\
    Auto-gptq : Version: 0.4.1+cu117\r\ntorch version : Version: 2.0.1\r\nbitsandbytes\
    \ : Version: 0.41.1\r\ntransformers : Version: 4.32.0\r\n"
  created_at: 2023-08-25 14:39:22+00:00
  edited: false
  hidden: false
  id: 64e8cb2a748eb958106efdc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-25T15:41:05.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.89361172914505
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ah, please add <code>inject_fused_attention=False</code> to the
          <code>from_quantized()</code> call.  34B has the same new GQA feature is
          the same as Llama 70B. I''ll update that in the README for future</p>

          '
        raw: Ah, please add `inject_fused_attention=False` to the `from_quantized()`
          call.  34B has the same new GQA feature is the same as Llama 70B. I'll update
          that in the README for future
        updatedAt: '2023-08-25T15:41:05.195Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - HAvietisov
        - Yangdf
    id: 64e8cb910c2413c3572c0439
    type: comment
  author: TheBloke
  content: Ah, please add `inject_fused_attention=False` to the `from_quantized()`
    call.  34B has the same new GQA feature is the same as Llama 70B. I'll update
    that in the README for future
  created_at: 2023-08-25 14:41:05+00:00
  edited: false
  hidden: false
  id: 64e8cb910c2413c3572c0439
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
      fullname: Hlib Avietisov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HAvietisov
      type: user
    createdAt: '2023-08-25T17:02:01.000Z'
    data:
      edited: false
      editors:
      - HAvietisov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9648851752281189
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
          fullname: Hlib Avietisov
          isHf: false
          isPro: false
          name: HAvietisov
          type: user
        html: '<p>Yep, that works. thanks!</p>

          '
        raw: Yep, that works. thanks!
        updatedAt: '2023-08-25T17:02:01.858Z'
      numEdits: 0
      reactions: []
    id: 64e8de898c523cced2e6f984
    type: comment
  author: HAvietisov
  content: Yep, that works. thanks!
  created_at: 2023-08-25 16:02:01+00:00
  edited: false
  hidden: false
  id: 64e8de898c523cced2e6f984
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/CodeLlama-34B-GPTQ
repo_type: model
status: open
target_branch: null
title: 'Error running the example code '
