!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jonasmockzf
conflicting_files: null
created_at: 2023-11-03 09:08:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7f44167646349f2755224f01753f383.svg
      fullname: Jonas Mock
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jonasmockzf
      type: user
    createdAt: '2023-11-03T10:08:23.000Z'
    data:
      edited: true
      editors:
      - jonasmockzf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8262438178062439
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7f44167646349f2755224f01753f383.svg
          fullname: Jonas Mock
          isHf: false
          isPro: false
          name: jonasmockzf
          type: user
        html: '<p>Hello, I try to load your model with the transformers module. But
          it wont allocate the memory to my GPUs. It tries to allocate to CPU RAM
          and fails.</p>

          <p>CPU RAM: 8GB<br>GPU: 2 x Nvidia Tesla V100 32GB</p>

          <p>There is enough space on my GPUs.. Could you may help with that issue?
          How do I force to load the model only to GPU or does is first load it to
          CPU RAM and then too GPU VRAM?</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/652d3580f14b3a0fda95bacb/AOIRtus0GleMF8sVkcPIM.png"><img
          alt="Error.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/652d3580f14b3a0fda95bacb/AOIRtus0GleMF8sVkcPIM.png"></a></p>

          '
        raw: 'Hello, I try to load your model with the transformers module. But it
          wont allocate the memory to my GPUs. It tries to allocate to CPU RAM and
          fails.


          CPU RAM: 8GB

          GPU: 2 x Nvidia Tesla V100 32GB


          There is enough space on my GPUs.. Could you may help with that issue? How
          do I force to load the model only to GPU or does is first load it to CPU
          RAM and then too GPU VRAM?


          ![Error.PNG](https://cdn-uploads.huggingface.co/production/uploads/652d3580f14b3a0fda95bacb/AOIRtus0GleMF8sVkcPIM.png)

          '
        updatedAt: '2023-11-03T10:10:45.877Z'
      numEdits: 2
      reactions: []
    id: 6544c6970f9ab48ad7aa0d09
    type: comment
  author: jonasmockzf
  content: 'Hello, I try to load your model with the transformers module. But it wont
    allocate the memory to my GPUs. It tries to allocate to CPU RAM and fails.


    CPU RAM: 8GB

    GPU: 2 x Nvidia Tesla V100 32GB


    There is enough space on my GPUs.. Could you may help with that issue? How do
    I force to load the model only to GPU or does is first load it to CPU RAM and
    then too GPU VRAM?


    ![Error.PNG](https://cdn-uploads.huggingface.co/production/uploads/652d3580f14b3a0fda95bacb/AOIRtus0GleMF8sVkcPIM.png)

    '
  created_at: 2023-11-03 09:08:23+00:00
  edited: true
  hidden: false
  id: 6544c6970f9ab48ad7aa0d09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7f44167646349f2755224f01753f383.svg
      fullname: Jonas Mock
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jonasmockzf
      type: user
    createdAt: '2023-11-03T11:14:30.000Z'
    data:
      edited: false
      editors:
      - jonasmockzf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8076580762863159
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7f44167646349f2755224f01753f383.svg
          fullname: Jonas Mock
          isHf: false
          isPro: false
          name: jonasmockzf
          type: user
        html: '<p>If I upgrade the VM to 32GB CPU RAM it works fine. It allocates
          all to GPU VRAM... Why doesnt it work with 8GB CPU RAM if it wont be used
          lol</p>

          '
        raw: If I upgrade the VM to 32GB CPU RAM it works fine. It allocates all to
          GPU VRAM... Why doesnt it work with 8GB CPU RAM if it wont be used lol
        updatedAt: '2023-11-03T11:14:30.740Z'
      numEdits: 0
      reactions: []
    id: 6544d616027281a960f14fc4
    type: comment
  author: jonasmockzf
  content: If I upgrade the VM to 32GB CPU RAM it works fine. It allocates all to
    GPU VRAM... Why doesnt it work with 8GB CPU RAM if it wont be used lol
  created_at: 2023-11-03 10:14:30+00:00
  edited: false
  hidden: false
  id: 6544d616027281a960f14fc4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/CodeLlama-34B-GPTQ
repo_type: model
status: open
target_branch: null
title: 'unable to mmap: AutoModelForCausalLM.from_pretrained / from safetensors import
  safe_open'
