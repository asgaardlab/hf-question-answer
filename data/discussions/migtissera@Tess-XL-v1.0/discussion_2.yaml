!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wolfram
conflicting_files: null
created_at: 2023-11-19 18:39:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-11-19T18:39:13.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.943781852722168
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;Panchovix&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Panchovix\">@<span class=\"\
          underline\">Panchovix</span></a></span>\n\n\t</span></span> quantized EXL2\
          \ versions of Goliath 120B. Any chance we could get an EXL2 version of this\
          \ model, too? It's tuned on top of Goliath and in my tests it's also at\
          \ the top of my rankings!</p>\n"
        raw: '@LoneStriker and @Panchovix quantized EXL2 versions of Goliath 120B.
          Any chance we could get an EXL2 version of this model, too? It''s tuned
          on top of Goliath and in my tests it''s also at the top of my rankings!'
        updatedAt: '2023-11-19T18:39:13.639Z'
      numEdits: 0
      reactions: []
    id: 655a5651c68499a1b97b181f
    type: comment
  author: wolfram
  content: '@LoneStriker and @Panchovix quantized EXL2 versions of Goliath 120B. Any
    chance we could get an EXL2 version of this model, too? It''s tuned on top of
    Goliath and in my tests it''s also at the top of my rankings!'
  created_at: 2023-11-19 18:39:13+00:00
  edited: false
  hidden: false
  id: 655a5651c68499a1b97b181f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-19T18:43:41.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9598430395126343
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;Panchovix&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Panchovix\">@<span class=\"\
          underline\">Panchovix</span></a></span>\n\n\t</span></span> quantized EXL2\
          \ versions of Goliath 120B. Any chance we could get an EXL2 version of this\
          \ model, too? It's tuned on top of Goliath and in my tests it's also at\
          \ the top of my rankings!</p>\n</blockquote>\n<p>I can put it in the queue.\
          \ 120Bs are monsters to chew through though.</p>\n"
        raw: '> @LoneStriker and @Panchovix quantized EXL2 versions of Goliath 120B.
          Any chance we could get an EXL2 version of this model, too? It''s tuned
          on top of Goliath and in my tests it''s also at the top of my rankings!


          I can put it in the queue. 120Bs are monsters to chew through though.'
        updatedAt: '2023-11-19T18:43:41.997Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Panchovix
        - wolfram
    id: 655a575dab0644b5319e0d12
    type: comment
  author: LoneStriker
  content: '> @LoneStriker and @Panchovix quantized EXL2 versions of Goliath 120B.
    Any chance we could get an EXL2 version of this model, too? It''s tuned on top
    of Goliath and in my tests it''s also at the top of my rankings!


    I can put it in the queue. 120Bs are monsters to chew through though.'
  created_at: 2023-11-19 18:43:41+00:00
  edited: false
  hidden: false
  id: 655a575dab0644b5319e0d12
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-11-19T18:53:48.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8955856561660767
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>If LoneStriker takes in his queue then nice! And I can confirm,
          it takes a good while to do a quant exl2 of a 120B model.</p>

          '
        raw: If LoneStriker takes in his queue then nice! And I can confirm, it takes
          a good while to do a quant exl2 of a 120B model.
        updatedAt: '2023-11-19T18:53:48.370Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - wolfram
    id: 655a59bc930a0e1b0a9f9d7b
    type: comment
  author: Panchovix
  content: If LoneStriker takes in his queue then nice! And I can confirm, it takes
    a good while to do a quant exl2 of a 120B model.
  created_at: 2023-11-19 18:53:48+00:00
  edited: false
  hidden: false
  id: 655a59bc930a0e1b0a9f9d7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-11-19T19:58:30.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9817399978637695
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: '<p>Thanks to both of you! 120B is definitely a beast, but even down
          to 3-bit it''s still beating all the 70Bs and with EXL2 it even runs nicely
          fast. So looking forward to this, thanks a lot!</p>

          '
        raw: Thanks to both of you! 120B is definitely a beast, but even down to 3-bit
          it's still beating all the 70Bs and with EXL2 it even runs nicely fast.
          So looking forward to this, thanks a lot!
        updatedAt: '2023-11-19T19:58:30.406Z'
      numEdits: 0
      reactions: []
    id: 655a68e6cbbaec115c397f69
    type: comment
  author: wolfram
  content: Thanks to both of you! 120B is definitely a beast, but even down to 3-bit
    it's still beating all the 70Bs and with EXL2 it even runs nicely fast. So looking
    forward to this, thanks a lot!
  created_at: 2023-11-19 19:58:30+00:00
  edited: false
  hidden: false
  id: 655a68e6cbbaec115c397f69
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-20T03:54:06.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8524802923202515
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Various bit rates should appear here (3.0 and 4.5 are done, other
          variants still quantizing):<br><a href="https://huggingface.co/models?search=lonestriker%20tess-xl">https://huggingface.co/models?search=lonestriker%20tess-xl</a></p>

          '
        raw: 'Various bit rates should appear here (3.0 and 4.5 are done, other variants
          still quantizing):

          https://huggingface.co/models?search=lonestriker%20tess-xl'
        updatedAt: '2023-11-20T03:54:06.474Z'
      numEdits: 0
      reactions: []
    id: 655ad85e6f460bf3291e9a78
    type: comment
  author: LoneStriker
  content: 'Various bit rates should appear here (3.0 and 4.5 are done, other variants
    still quantizing):

    https://huggingface.co/models?search=lonestriker%20tess-xl'
  created_at: 2023-11-20 03:54:06+00:00
  edited: false
  hidden: false
  id: 655ad85e6f460bf3291e9a78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-11-20T04:26:43.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.96303391456604
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: '<p>Awesome job! Can we also get a sample script on how to run it with
          EXL2?</p>

          '
        raw: Awesome job! Can we also get a sample script on how to run it with EXL2?
        updatedAt: '2023-11-20T04:26:43.856Z'
      numEdits: 0
      reactions: []
    id: 655ae00386fbe7506e16854c
    type: comment
  author: migtissera
  content: Awesome job! Can we also get a sample script on how to run it with EXL2?
  created_at: 2023-11-20 04:26:43+00:00
  edited: false
  hidden: false
  id: 655ae00386fbe7506e16854c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-20T04:47:29.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8221147060394287
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<blockquote>

          <p>Awesome job! Can we also get a sample script on how to run it with EXL2?</p>

          </blockquote>

          <p>Most people will just run it under ooba text gen webui using the <code>exllamav2</code>
          loader.  The one setting you may need, however, is to unset this option:<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/2_M0sdA6R6e-nU88pprKn.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/2_M0sdA6R6e-nU88pprKn.png"></a></p>

          <p>For low-bit-rate quants, the ooba web interface settings will spit out
          gibberish if left checked (it''s on by default.)</p>

          <p>If you want to use it programmatically in Python, you can use Turboderp''s
          exllamav2 project.  Script here:<br><a rel="nofollow" href="https://github.com/turboderp/exllamav2/blob/master/examples/chat.py">https://github.com/turboderp/exllamav2/blob/master/examples/chat.py</a></p>

          <p>You can run the 3.0bpw quant on 2x 3090s/4090s at a reasonably fast inference
          speed.</p>

          '
        raw: '> Awesome job! Can we also get a sample script on how to run it with
          EXL2?


          Most people will just run it under ooba text gen webui using the `exllamav2`
          loader.  The one setting you may need, however, is to unset this option:

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/2_M0sdA6R6e-nU88pprKn.png)


          For low-bit-rate quants, the ooba web interface settings will spit out gibberish
          if left checked (it''s on by default.)


          If you want to use it programmatically in Python, you can use Turboderp''s
          exllamav2 project.  Script here:

          https://github.com/turboderp/exllamav2/blob/master/examples/chat.py


          You can run the 3.0bpw quant on 2x 3090s/4090s at a reasonably fast inference
          speed.'
        updatedAt: '2023-11-20T04:47:29.629Z'
      numEdits: 0
      reactions: []
    id: 655ae4e17f24664339903624
    type: comment
  author: LoneStriker
  content: '> Awesome job! Can we also get a sample script on how to run it with EXL2?


    Most people will just run it under ooba text gen webui using the `exllamav2` loader.  The
    one setting you may need, however, is to unset this option:

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/2_M0sdA6R6e-nU88pprKn.png)


    For low-bit-rate quants, the ooba web interface settings will spit out gibberish
    if left checked (it''s on by default.)


    If you want to use it programmatically in Python, you can use Turboderp''s exllamav2
    project.  Script here:

    https://github.com/turboderp/exllamav2/blob/master/examples/chat.py


    You can run the 3.0bpw quant on 2x 3090s/4090s at a reasonably fast inference
    speed.'
  created_at: 2023-11-20 04:47:29+00:00
  edited: false
  hidden: false
  id: 655ae4e17f24664339903624
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-11-20T04:52:39.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.731923520565033
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>At least what I do, for easier testing is, using exui (same developer
          of exllama) and after getting the model directly, loading it with that UI.</p>

          <p><a rel="nofollow" href="https://github.com/turboderp/exui">https://github.com/turboderp/exui</a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/649608ca0b01497fb78e2e5c/Zavo-BM50Na3kxqa_OeEv.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/649608ca0b01497fb78e2e5c/Zavo-BM50Na3kxqa_OeEv.png"></a></p>

          <p>The original exllamav2 project to do some benchmarks for example, or
          to install from source to use in other backends: <a rel="nofollow" href="https://github.com/turboderp/exllamav2">https://github.com/turboderp/exllamav2</a></p>

          '
        raw: 'At least what I do, for easier testing is, using exui (same developer
          of exllama) and after getting the model directly, loading it with that UI.


          https://github.com/turboderp/exui



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/649608ca0b01497fb78e2e5c/Zavo-BM50Na3kxqa_OeEv.png)


          The original exllamav2 project to do some benchmarks for example, or to
          install from source to use in other backends: https://github.com/turboderp/exllamav2

          '
        updatedAt: '2023-11-20T04:52:39.148Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - LoneStriker
    id: 655ae617c692310739ea42ff
    type: comment
  author: Panchovix
  content: 'At least what I do, for easier testing is, using exui (same developer
    of exllama) and after getting the model directly, loading it with that UI.


    https://github.com/turboderp/exui



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/649608ca0b01497fb78e2e5c/Zavo-BM50Na3kxqa_OeEv.png)


    The original exllamav2 project to do some benchmarks for example, or to install
    from source to use in other backends: https://github.com/turboderp/exllamav2

    '
  created_at: 2023-11-20 04:52:39+00:00
  edited: false
  hidden: false
  id: 655ae617c692310739ea42ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-20T04:59:54.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.964697003364563
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: "<p>exui is recommended (though very few people know about it yet.).\
          \ One benefit is that you can use speculative decoding as <span data-props=\"\
          {&quot;user&quot;:&quot;Panchovix&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Panchovix\">@<span class=\"underline\">Panchovix</span></a></span>\n\
          \n\t</span></span> shows above. You basically get 50-100% speedup for a\
          \ small bit of VRAM to run the draft model.</p>\n"
        raw: exui is recommended (though very few people know about it yet.). One
          benefit is that you can use speculative decoding as @Panchovix shows above.
          You basically get 50-100% speedup for a small bit of VRAM to run the draft
          model.
        updatedAt: '2023-11-20T04:59:54.119Z'
      numEdits: 0
      reactions: []
    id: 655ae7ca7f2466433990b68f
    type: comment
  author: LoneStriker
  content: exui is recommended (though very few people know about it yet.). One benefit
    is that you can use speculative decoding as @Panchovix shows above. You basically
    get 50-100% speedup for a small bit of VRAM to run the draft model.
  created_at: 2023-11-20 04:59:54+00:00
  edited: false
  hidden: false
  id: 655ae7ca7f2466433990b68f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
      fullname: Jack Boot
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jackboot
      type: user
    createdAt: '2023-11-20T10:21:45.000Z'
    data:
      edited: false
      editors:
      - jackboot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9100506901741028
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
          fullname: Jack Boot
          isHf: false
          isPro: false
          name: jackboot
          type: user
        html: '<p>The 3.0 has been saving my butt.  They give normal 70b speeds and
          what feels like 80-90% of the quality of 4KS/4KM GGUF. Only 3500 or so context
          though.</p>

          '
        raw: The 3.0 has been saving my butt.  They give normal 70b speeds and what
          feels like 80-90% of the quality of 4KS/4KM GGUF. Only 3500 or so context
          though.
        updatedAt: '2023-11-20T10:21:45.308Z'
      numEdits: 0
      reactions: []
    id: 655b3339deee83130a58b082
    type: comment
  author: jackboot
  content: The 3.0 has been saving my butt.  They give normal 70b speeds and what
    feels like 80-90% of the quality of 4KS/4KM GGUF. Only 3500 or so context though.
  created_at: 2023-11-20 10:21:45+00:00
  edited: false
  hidden: false
  id: 655b3339deee83130a58b082
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-11-20T18:48:12.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9268617630004883
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: "<p>Wow, that was faster than I expected. Thanks a lot, <span data-props=\"\
          {&quot;user&quot;:&quot;LoneStriker&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/LoneStriker\">@<span class=\"underline\"\
          >LoneStriker</span></a></span>\n\n\t</span></span> !</p>\n<p>And thanks\
          \ for the recommendation of exui, <span data-props=\"{&quot;user&quot;:&quot;Panchovix&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Panchovix\"\
          >@<span class=\"underline\">Panchovix</span></a></span>\n\n\t</span></span>\
          \ - speculative decoding sounds interesting and useful. But that UI doesn't\
          \ have an API, does it? My frontend is SillyTavern so I need a backend that\
          \ can be used with it, either OpenAI API-compatible or e. g. ooba text gen\
          \ webui (which is now OpenAI API-compatible, too).</p>\n"
        raw: 'Wow, that was faster than I expected. Thanks a lot, @LoneStriker !


          And thanks for the recommendation of exui, @Panchovix - speculative decoding
          sounds interesting and useful. But that UI doesn''t have an API, does it?
          My frontend is SillyTavern so I need a backend that can be used with it,
          either OpenAI API-compatible or e. g. ooba text gen webui (which is now
          OpenAI API-compatible, too).'
        updatedAt: '2023-11-20T18:48:12.781Z'
      numEdits: 0
      reactions: []
    id: 655ba9ec973f30a0a57d679a
    type: comment
  author: wolfram
  content: 'Wow, that was faster than I expected. Thanks a lot, @LoneStriker !


    And thanks for the recommendation of exui, @Panchovix - speculative decoding sounds
    interesting and useful. But that UI doesn''t have an API, does it? My frontend
    is SillyTavern so I need a backend that can be used with it, either OpenAI API-compatible
    or e. g. ooba text gen webui (which is now OpenAI API-compatible, too).'
  created_at: 2023-11-20 18:48:12+00:00
  edited: false
  hidden: false
  id: 655ba9ec973f30a0a57d679a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-11-20T18:56:05.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9656736254692078
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;wolfram&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/wolfram\">@<span class=\"\
          underline\">wolfram</span></a></span>\n\n\t</span></span> in that case I\
          \ suggest to use tabbyAPI <a rel=\"nofollow\" href=\"https://github.com/theroyallab/tabbyAPI\"\
          >https://github.com/theroyallab/tabbyAPI</a></p>\n<p>It is a very lightweight\
          \ API loader for exllamav2/gptq models, and it will work with ST.</p>\n\
          <p>Though it isn't as easy as ooba.</p>\n"
        raw: '@wolfram in that case I suggest to use tabbyAPI https://github.com/theroyallab/tabbyAPI


          It is a very lightweight API loader for exllamav2/gptq models, and it will
          work with ST.


          Though it isn''t as easy as ooba.'
        updatedAt: '2023-11-20T18:56:05.084Z'
      numEdits: 0
      reactions: []
    id: 655babc5935d0f9a75d087b9
    type: comment
  author: Panchovix
  content: '@wolfram in that case I suggest to use tabbyAPI https://github.com/theroyallab/tabbyAPI


    It is a very lightweight API loader for exllamav2/gptq models, and it will work
    with ST.


    Though it isn''t as easy as ooba.'
  created_at: 2023-11-20 18:56:05+00:00
  edited: false
  hidden: false
  id: 655babc5935d0f9a75d087b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-11-20T19:20:55.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8676433563232422
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Panchovix&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Panchovix\">@<span class=\"\
          underline\">Panchovix</span></a></span>\n\n\t</span></span> Why not ooba?\
          \ Does tabbyAPI support speculative decoding or what would be the advantage?</p>\n"
        raw: '@Panchovix Why not ooba? Does tabbyAPI support speculative decoding
          or what would be the advantage?'
        updatedAt: '2023-11-20T19:20:55.507Z'
      numEdits: 0
      reactions: []
    id: 655bb19795628a14533b1bc2
    type: comment
  author: wolfram
  content: '@Panchovix Why not ooba? Does tabbyAPI support speculative decoding or
    what would be the advantage?'
  created_at: 2023-11-20 19:20:55+00:00
  edited: false
  hidden: false
  id: 655bb19795628a14533b1bc2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-20T19:26:34.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9513288736343384
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>ooba does not support exl2 speculative decoding. exui and tabbyAPI
          both support it.</p>

          '
        raw: ooba does not support exl2 speculative decoding. exui and tabbyAPI both
          support it.
        updatedAt: '2023-11-20T19:26:34.559Z'
      numEdits: 0
      reactions: []
    id: 655bb2ea7558de19725d542c
    type: comment
  author: LoneStriker
  content: ooba does not support exl2 speculative decoding. exui and tabbyAPI both
    support it.
  created_at: 2023-11-20 19:26:34+00:00
  edited: false
  hidden: false
  id: 655bb2ea7558de19725d542c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
      fullname: Jack Boot
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jackboot
      type: user
    createdAt: '2023-11-21T15:30:02.000Z'
    data:
      edited: false
      editors:
      - jackboot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.974566638469696
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
          fullname: Jack Boot
          isHf: false
          isPro: false
          name: jackboot
          type: user
        html: '<p>3.0bpw barely fits in 48gb. Even the tiniest model won''t work with
          that.</p>

          '
        raw: 3.0bpw barely fits in 48gb. Even the tiniest model won't work with that.
        updatedAt: '2023-11-21T15:30:02.263Z'
      numEdits: 0
      reactions: []
    id: 655cccfa2735108d4976ded2
    type: comment
  author: jackboot
  content: 3.0bpw barely fits in 48gb. Even the tiniest model won't work with that.
  created_at: 2023-11-21 15:30:02+00:00
  edited: false
  hidden: false
  id: 655cccfa2735108d4976ded2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-21T15:38:21.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.736261785030365
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Yup, 3.0 is a tight fit, even with 8-bit cache enabled.  If you
          want to go lower, grab a 2.18, 2.4 or 2.85 bpw model:<br><a href="https://huggingface.co/LoneStriker?search_models=tess-xl">https://huggingface.co/LoneStriker?search_models=tess-xl</a></p>

          '
        raw: 'Yup, 3.0 is a tight fit, even with 8-bit cache enabled.  If you want
          to go lower, grab a 2.18, 2.4 or 2.85 bpw model:

          https://huggingface.co/LoneStriker?search_models=tess-xl'
        updatedAt: '2023-11-21T15:38:21.179Z'
      numEdits: 0
      reactions: []
    id: 655cceedddf3d8a6916f13cd
    type: comment
  author: LoneStriker
  content: 'Yup, 3.0 is a tight fit, even with 8-bit cache enabled.  If you want to
    go lower, grab a 2.18, 2.4 or 2.85 bpw model:

    https://huggingface.co/LoneStriker?search_models=tess-xl'
  created_at: 2023-11-21 15:38:21+00:00
  edited: false
  hidden: false
  id: 655cceedddf3d8a6916f13cd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: migtissera/Tess-XL-v1.0
repo_type: model
status: open
target_branch: null
title: Any chance for an EXL2 version?
