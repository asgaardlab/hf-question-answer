!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ZeroCool22
conflicting_files: null
created_at: 2022-11-28 07:40:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661906612980-630db5d3a16f7b175da7c284.jpeg?w=200&h=200&f=face
      fullname: Emilio
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZeroCool22
      type: user
    createdAt: '2022-11-28T07:40:34.000Z'
    data:
      edited: false
      editors:
      - ZeroCool22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661906612980-630db5d3a16f7b175da7c284.jpeg?w=200&h=200&f=face
          fullname: Emilio
          isHf: false
          isPro: false
          name: ZeroCool22
          type: user
        html: '<p>When try to use 512-base-ema.ckpt</p>

          <p>Traceback (most recent call last):<br>  File "C:\Users\ZeroCool22\Desktop\Auto\modules\call_queue.py",
          line 45, in f<br>    res = list(func(*args, **kwargs))<br>  File "C:\Users\ZeroCool22\Desktop\Auto\modules\call_queue.py",
          line 28, in f<br>    res = func(*args, **kwargs)<br>  File "C:\Users\ZeroCool22\Desktop\Auto\modules\txt2img.py",
          line 49, in txt2img<br>    processed = process_images(p)<br>  File "C:\Users\ZeroCool22\Desktop\Auto\modules\processing.py",
          line 430, in process_images<br>    res = process_images_inner(p)<br>  File
          "C:\Users\ZeroCool22\Desktop\Auto\modules\processing.py", line 520, in process_images_inner<br>    uc
          = prompt_parser.get_learned_conditioning(shared.sd_model, negative_prompts,
          p.steps)<br>  File "C:\Users\ZeroCool22\Desktop\Auto\modules\prompt_parser.py",
          line 138, in get_learned_conditioning<br>    conds = model.get_learned_conditioning(texts)<br>  File
          "C:\Users\ZeroCool22\Desktop\Auto\repositories\stable-diffusion-stability-ai\ldm\models\diffusion\ddpm.py",
          line 665, in get_learned_conditioning<br>    c = self.cond_stage_model.encode(c)<br>  File
          "C:\Users\ZeroCool22\Desktop\Auto\repositories\stable-diffusion-stability-ai\ldm\modules\encoders\modules.py",
          line 131, in encode<br>    return self(text)<br>  File "C:\Users\ZeroCool22\Desktop\Auto\venv\lib\site-packages\torch\nn\modules\module.py",
          line 1130, in _call_impl<br>    return forward_call(*input, **kwargs)<br>  File
          "C:\Users\ZeroCool22\Desktop\Auto\repositories\stable-diffusion-stability-ai\ldm\modules\encoders\modules.py",
          line 121, in forward<br>    outputs = self.transformer(input_ids=tokens,
          output_hidden_states=self.layer=="hidden")<br>  File "C:\Users\ZeroCool22\Desktop\Auto\venv\lib\site-packages\torch\nn\modules\module.py",
          line 1130, in _call_impl<br>    return forward_call(*input, **kwargs)<br>  File
          "C:\Users\ZeroCool22\Desktop\Auto\venv\lib\site-packages\transformers\models\clip\modeling_clip.py",
          line 722, in forward<br>    return self.text_model(<br>  File "C:\Users\ZeroCool22\Desktop\Auto\venv\lib\site-packages\torch\nn\modules\module.py",
          line 1130, in _call_impl<br>    return forward_call(*input, **kwargs)<br>  File
          "C:\Users\ZeroCool22\Desktop\Auto\venv\lib\site-packages\transformers\models\clip\modeling_clip.py",
          line 632, in forward<br>    hidden_states = self.embeddings(input_ids=input_ids,
          position_ids=position_ids)<br>  File "C:\Users\ZeroCool22\Desktop\Auto\venv\lib\site-packages\torch\nn\modules\module.py",
          line 1130, in _call_impl<br>    return forward_call(*input, **kwargs)<br>  File
          "C:\Users\ZeroCool22\Desktop\Auto\venv\lib\site-packages\transformers\models\clip\modeling_clip.py",
          line 165, in forward<br>    inputs_embeds = self.token_embedding(input_ids)<br>  File
          "C:\Users\ZeroCool22\Desktop\Auto\venv\lib\site-packages\torch\nn\modules\module.py",
          line 1130, in _call_impl<br>    return forward_call(*input, **kwargs)<br>  File
          "C:\Users\ZeroCool22\Desktop\Auto\venv\lib\site-packages\torch\nn\modules\sparse.py",
          line 158, in forward<br>    return F.embedding(<br>  File "C:\Users\ZeroCool22\Desktop\Auto\venv\lib\site-packages\torch\nn\functional.py",
          line 2199, in embedding<br>    return torch.embedding(weight, input, padding_idx,
          scale_grad_by_freq, sparse)<br>RuntimeError: Expected all tensors to be
          on the same device, but found at least two devices, cpu and cuda:0! (when
          checking argument for argument index in method wrapper__index_select)</p>

          '
        raw: "When try to use 512-base-ema.ckpt\r\n\r\n\r\nTraceback (most recent\
          \ call last):\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\modules\\\
          call_queue.py\", line 45, in f\r\n    res = list(func(*args, **kwargs))\r\
          \n  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\modules\\call_queue.py\"\
          , line 28, in f\r\n    res = func(*args, **kwargs)\r\n  File \"C:\\Users\\\
          ZeroCool22\\Desktop\\Auto\\modules\\txt2img.py\", line 49, in txt2img\r\n\
          \    processed = process_images(p)\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\\
          Auto\\modules\\processing.py\", line 430, in process_images\r\n    res =\
          \ process_images_inner(p)\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\\
          modules\\processing.py\", line 520, in process_images_inner\r\n    uc =\
          \ prompt_parser.get_learned_conditioning(shared.sd_model, negative_prompts,\
          \ p.steps)\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\modules\\prompt_parser.py\"\
          , line 138, in get_learned_conditioning\r\n    conds = model.get_learned_conditioning(texts)\r\
          \n  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\repositories\\stable-diffusion-stability-ai\\\
          ldm\\models\\diffusion\\ddpm.py\", line 665, in get_learned_conditioning\r\
          \n    c = self.cond_stage_model.encode(c)\r\n  File \"C:\\Users\\ZeroCool22\\\
          Desktop\\Auto\\repositories\\stable-diffusion-stability-ai\\ldm\\modules\\\
          encoders\\modules.py\", line 131, in encode\r\n    return self(text)\r\n\
          \  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\venv\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return\
          \ forward_call(*input, **kwargs)\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\\
          Auto\\repositories\\stable-diffusion-stability-ai\\ldm\\modules\\encoders\\\
          modules.py\", line 121, in forward\r\n    outputs = self.transformer(input_ids=tokens,\
          \ output_hidden_states=self.layer==\"hidden\")\r\n  File \"C:\\Users\\ZeroCool22\\\
          Desktop\\Auto\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\
          \n  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\venv\\lib\\site-packages\\\
          transformers\\models\\clip\\modeling_clip.py\", line 722, in forward\r\n\
          \    return self.text_model(\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\\
          Auto\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130,\
          \ in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"\
          C:\\Users\\ZeroCool22\\Desktop\\Auto\\venv\\lib\\site-packages\\transformers\\\
          models\\clip\\modeling_clip.py\", line 632, in forward\r\n    hidden_states\
          \ = self.embeddings(input_ids=input_ids, position_ids=position_ids)\r\n\
          \  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\venv\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return\
          \ forward_call(*input, **kwargs)\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\\
          Auto\\venv\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py\"\
          , line 165, in forward\r\n    inputs_embeds = self.token_embedding(input_ids)\r\
          \n  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\venv\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return\
          \ forward_call(*input, **kwargs)\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\\
          Auto\\venv\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\", line 158,\
          \ in forward\r\n    return F.embedding(\r\n  File \"C:\\Users\\ZeroCool22\\\
          Desktop\\Auto\\venv\\lib\\site-packages\\torch\\nn\\functional.py\", line\
          \ 2199, in embedding\r\n    return torch.embedding(weight, input, padding_idx,\
          \ scale_grad_by_freq, sparse)\r\nRuntimeError: Expected all tensors to be\
          \ on the same device, but found at least two devices, cpu and cuda:0! (when\
          \ checking argument for argument index in method wrapper__index_select)"
        updatedAt: '2022-11-28T07:40:34.722Z'
      numEdits: 0
      reactions: []
    id: 638465f20947447a358084a5
    type: comment
  author: ZeroCool22
  content: "When try to use 512-base-ema.ckpt\r\n\r\n\r\nTraceback (most recent call\
    \ last):\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\modules\\call_queue.py\"\
    , line 45, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"C:\\Users\\\
    ZeroCool22\\Desktop\\Auto\\modules\\call_queue.py\", line 28, in f\r\n    res\
    \ = func(*args, **kwargs)\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\modules\\\
    txt2img.py\", line 49, in txt2img\r\n    processed = process_images(p)\r\n  File\
    \ \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\modules\\processing.py\", line 430,\
    \ in process_images\r\n    res = process_images_inner(p)\r\n  File \"C:\\Users\\\
    ZeroCool22\\Desktop\\Auto\\modules\\processing.py\", line 520, in process_images_inner\r\
    \n    uc = prompt_parser.get_learned_conditioning(shared.sd_model, negative_prompts,\
    \ p.steps)\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\modules\\prompt_parser.py\"\
    , line 138, in get_learned_conditioning\r\n    conds = model.get_learned_conditioning(texts)\r\
    \n  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\repositories\\stable-diffusion-stability-ai\\\
    ldm\\models\\diffusion\\ddpm.py\", line 665, in get_learned_conditioning\r\n \
    \   c = self.cond_stage_model.encode(c)\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\\
    Auto\\repositories\\stable-diffusion-stability-ai\\ldm\\modules\\encoders\\modules.py\"\
    , line 131, in encode\r\n    return self(text)\r\n  File \"C:\\Users\\ZeroCool22\\\
    Desktop\\Auto\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"\
    C:\\Users\\ZeroCool22\\Desktop\\Auto\\repositories\\stable-diffusion-stability-ai\\\
    ldm\\modules\\encoders\\modules.py\", line 121, in forward\r\n    outputs = self.transformer(input_ids=tokens,\
    \ output_hidden_states=self.layer==\"hidden\")\r\n  File \"C:\\Users\\ZeroCool22\\\
    Desktop\\Auto\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"\
    C:\\Users\\ZeroCool22\\Desktop\\Auto\\venv\\lib\\site-packages\\transformers\\\
    models\\clip\\modeling_clip.py\", line 722, in forward\r\n    return self.text_model(\r\
    \n  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\venv\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input,\
    \ **kwargs)\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\venv\\lib\\site-packages\\\
    transformers\\models\\clip\\modeling_clip.py\", line 632, in forward\r\n    hidden_states\
    \ = self.embeddings(input_ids=input_ids, position_ids=position_ids)\r\n  File\
    \ \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\venv\\lib\\site-packages\\torch\\nn\\\
    modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input,\
    \ **kwargs)\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\Auto\\venv\\lib\\site-packages\\\
    transformers\\models\\clip\\modeling_clip.py\", line 165, in forward\r\n    inputs_embeds\
    \ = self.token_embedding(input_ids)\r\n  File \"C:\\Users\\ZeroCool22\\Desktop\\\
    Auto\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in\
    \ _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"C:\\Users\\\
    ZeroCool22\\Desktop\\Auto\\venv\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\"\
    , line 158, in forward\r\n    return F.embedding(\r\n  File \"C:\\Users\\ZeroCool22\\\
    Desktop\\Auto\\venv\\lib\\site-packages\\torch\\nn\\functional.py\", line 2199,\
    \ in embedding\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq,\
    \ sparse)\r\nRuntimeError: Expected all tensors to be on the same device, but\
    \ found at least two devices, cpu and cuda:0! (when checking argument for argument\
    \ index in method wrapper__index_select)"
  created_at: 2022-11-28 07:40:34+00:00
  edited: false
  hidden: false
  id: 638465f20947447a358084a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661906612980-630db5d3a16f7b175da7c284.jpeg?w=200&h=200&f=face
      fullname: Emilio
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZeroCool22
      type: user
    createdAt: '2022-11-28T08:19:51.000Z'
    data:
      status: closed
    id: 63846f2783ef71b9328c3a16
    type: status-change
  author: ZeroCool22
  created_at: 2022-11-28 08:19:51+00:00
  id: 63846f2783ef71b9328c3a16
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: stabilityai/stable-diffusion-2-base
repo_type: model
status: closed
target_branch: null
title: 'RuntimeError: Expected all tensors to be on the same device, but found at
  least two devices, cpu and cuda:0! (when checking argument for argument index in
  method wrapper__index_select)'
