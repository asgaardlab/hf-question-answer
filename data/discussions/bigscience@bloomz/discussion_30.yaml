!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yahma
conflicting_files: null
created_at: 2023-01-23 19:50:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672330023435-62c6faed53c7156f5bf767ed.png?w=200&h=200&f=face
      fullname: Gene Ruebsamen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yahma
      type: user
    createdAt: '2023-01-23T19:50:49.000Z'
    data:
      edited: false
      editors:
      - yahma
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672330023435-62c6faed53c7156f5bf767ed.png?w=200&h=200&f=face
          fullname: Gene Ruebsamen
          isHf: false
          isPro: false
          name: yahma
          type: user
        html: '<p>My understanding is mT0 is based on an encoder-decoder transformer
          similar to BERT/BART, while BLOOMZ is a decoder-only model like GPT3. Are
          there any performance/output differences to be aware of between these two
          models?</p>

          <p>You kindly provide a 13B mt0-xxl model and a 7B BLOOMZ model. I tried
          comparing the output of these models on a variety of tasks and did not notice
          much difference between the two.</p>

          '
        raw: "My understanding is mT0 is based on an encoder-decoder transformer similar\
          \ to BERT/BART, while BLOOMZ is a decoder-only model like GPT3. Are there\
          \ any performance/output differences to be aware of between these two models?\r\
          \n\r\nYou kindly provide a 13B mt0-xxl model and a 7B BLOOMZ model. I tried\
          \ comparing the output of these models on a variety of tasks and did not\
          \ notice much difference between the two."
        updatedAt: '2023-01-23T19:50:49.513Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - julien-c
    id: 63cee5195c1d61bb23d9f8b3
    type: comment
  author: yahma
  content: "My understanding is mT0 is based on an encoder-decoder transformer similar\
    \ to BERT/BART, while BLOOMZ is a decoder-only model like GPT3. Are there any\
    \ performance/output differences to be aware of between these two models?\r\n\r\
    \nYou kindly provide a 13B mt0-xxl model and a 7B BLOOMZ model. I tried comparing\
    \ the output of these models on a variety of tasks and did not notice much difference\
    \ between the two."
  created_at: 2023-01-23 19:50:49+00:00
  edited: false
  hidden: false
  id: 63cee5195c1d61bb23d9f8b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-01-25T09:22:37.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<p>Yes that's correct. </p>\n<p>There are output/performance differences\
          \ between the models - You may want to take a look at the paper <a rel=\"\
          nofollow\" href=\"https://arxiv.org/abs/2211.01786\">Crosslingual Generalization\
          \ through Multitask Finetuning</a>, where the graphs/tables compare performance\
          \ of mT0 &amp; BLOOMZ models. Generally, </p>\n<ul>\n<li>mT0 models are\
          \ stronger on most benchmarks if we compare models with ~the same parameters.</li>\n\
          <li>mT0 models are biased towards shorter answers, you can check the examples\
          \ in the appendix of the paper for that if you want \U0001F607</li>\n<li>Performance\
          \ by language will vary. E.g. mT0 models are much stronger on many languages\
          \ BLOOMZ has not extensively seen, such as Russian or Japanese.</li>\n</ul>\n\
          <p>In addition, various specs are different like precision &amp; various\
          \ architecture components.</p>\n<p>Let us know if you notice any other differences!</p>\n"
        raw: "Yes that's correct. \n\nThere are output/performance differences between\
          \ the models - You may want to take a look at the paper [Crosslingual Generalization\
          \ through Multitask Finetuning](https://arxiv.org/abs/2211.01786), where\
          \ the graphs/tables compare performance of mT0 & BLOOMZ models. Generally,\
          \ \n- mT0 models are stronger on most benchmarks if we compare models with\
          \ ~the same parameters.\n- mT0 models are biased towards shorter answers,\
          \ you can check the examples in the appendix of the paper for that if you\
          \ want \U0001F607\n- Performance by language will vary. E.g. mT0 models\
          \ are much stronger on many languages BLOOMZ has not extensively seen, such\
          \ as Russian or Japanese.\n\nIn addition, various specs are different like\
          \ precision & various architecture components.\n\nLet us know if you notice\
          \ any other differences!"
        updatedAt: '2023-01-25T09:22:37.418Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - yahma
        - Liyager
    id: 63d0f4dd119416cdbe0b2e9f
    type: comment
  author: Muennighoff
  content: "Yes that's correct. \n\nThere are output/performance differences between\
    \ the models - You may want to take a look at the paper [Crosslingual Generalization\
    \ through Multitask Finetuning](https://arxiv.org/abs/2211.01786), where the graphs/tables\
    \ compare performance of mT0 & BLOOMZ models. Generally, \n- mT0 models are stronger\
    \ on most benchmarks if we compare models with ~the same parameters.\n- mT0 models\
    \ are biased towards shorter answers, you can check the examples in the appendix\
    \ of the paper for that if you want \U0001F607\n- Performance by language will\
    \ vary. E.g. mT0 models are much stronger on many languages BLOOMZ has not extensively\
    \ seen, such as Russian or Japanese.\n\nIn addition, various specs are different\
    \ like precision & various architecture components.\n\nLet us know if you notice\
    \ any other differences!"
  created_at: 2023-01-25 09:22:37+00:00
  edited: false
  hidden: false
  id: 63d0f4dd119416cdbe0b2e9f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 30
repo_id: bigscience/bloomz
repo_type: model
status: open
target_branch: null
title: Difference between MT0 and BLOOMZ
