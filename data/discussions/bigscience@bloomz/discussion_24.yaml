!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kmdanikowski
conflicting_files: null
created_at: 2023-01-09 16:29:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b398e1df417aa512343a4be7f4faaed2.svg
      fullname: Kevin Danikowski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kmdanikowski
      type: user
    createdAt: '2023-01-09T16:29:05.000Z'
    data:
      edited: true
      editors:
      - kmdanikowski
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b398e1df417aa512343a4be7f4faaed2.svg
          fullname: Kevin Danikowski
          isHf: false
          isPro: false
          name: kmdanikowski
          type: user
        html: '<p>Has anyone trained the model for a language not currently supported?
          I have a few (german, Russian, polish, ukranian, greek, korean) I''d like
          to train it for (3B model) and would like to know what it would take. </p>

          <p>I was thinking:<br>Option 1: large corpus + translated prompt xP3<br>Option
          2: translated prompt xP3</p>

          <p>I''d prefer just to do option 2.... but that might not be sufficient.
          Would love any insights others have had</p>

          <p>PS: In the xP3 paper <a rel="nofollow" href="https://arxiv.org/pdf/2211.01786v1.pdf">https://arxiv.org/pdf/2211.01786v1.pdf</a>
          it specifically says there is a minor amount of non-specified langs in the
          ROOTS dataset BLOOM was trained on. I have observed it knows some german
          (not so good tho...).</p>

          '
        raw: "Has anyone trained the model for a language not currently supported?\
          \ I have a few (german, Russian, polish, ukranian, greek, korean) I'd like\
          \ to train it for (3B model) and would like to know what it would take.\
          \ \n\nI was thinking:\nOption 1: large corpus + translated prompt xP3\n\
          Option 2: translated prompt xP3\n\nI'd prefer just to do option 2.... but\
          \ that might not be sufficient. Would love any insights others have had\n\
          \nPS: In the xP3 paper https://arxiv.org/pdf/2211.01786v1.pdf it specifically\
          \ says there is a minor amount of non-specified langs in the ROOTS dataset\
          \ BLOOM was trained on. I have observed it knows some german (not so good\
          \ tho...)."
        updatedAt: '2023-01-09T16:40:16.285Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - yahma
    id: 63bc40d1d8d676a229a08cfa
    type: comment
  author: kmdanikowski
  content: "Has anyone trained the model for a language not currently supported? I\
    \ have a few (german, Russian, polish, ukranian, greek, korean) I'd like to train\
    \ it for (3B model) and would like to know what it would take. \n\nI was thinking:\n\
    Option 1: large corpus + translated prompt xP3\nOption 2: translated prompt xP3\n\
    \nI'd prefer just to do option 2.... but that might not be sufficient. Would love\
    \ any insights others have had\n\nPS: In the xP3 paper https://arxiv.org/pdf/2211.01786v1.pdf\
    \ it specifically says there is a minor amount of non-specified langs in the ROOTS\
    \ dataset BLOOM was trained on. I have observed it knows some german (not so good\
    \ tho...)."
  created_at: 2023-01-09 16:29:05+00:00
  edited: true
  hidden: false
  id: 63bc40d1d8d676a229a08cfa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-01-09T18:15:03.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<blockquote>\n<p>Has anyone trained the model for a language not currently\
          \ supported? I have a few (german, Russian, polish, ukranian, greek, korean)\
          \ I'd like to train it for (3B model) and would like to know what it would\
          \ take. </p>\n<p>I was thinking:<br>Option 1: large corpus + translated\
          \ prompt xP3<br>Option 2: translated prompt xP3</p>\n<p>I'd prefer just\
          \ to do option 2.... but that might not be sufficient. Would love any insights\
          \ others have had</p>\n<p>PS: In the xP3 paper <a rel=\"nofollow\" href=\"\
          https://arxiv.org/pdf/2211.01786v1.pdf\">https://arxiv.org/pdf/2211.01786v1.pdf</a>\
          \ it specifically says there is a minor amount of non-specified langs in\
          \ the ROOTS dataset BLOOM was trained on. I have observed it knows some\
          \ german (not so good tho...).</p>\n</blockquote>\n<p>Hey we recently tried\
          \ adding a language to BLOOMZ in this paper: <a rel=\"nofollow\" href=\"\
          https://arxiv.org/abs/2212.09535\">https://arxiv.org/abs/2212.09535</a><br>The\
          \ best way to go would be to construct an xP3 corpus for the languages you\
          \ would like (similar to <a href=\"https://huggingface.co/datasets/bs-la/xP3ru\"\
          >xP3ru</a> from the paper) &amp; then finetune BLOOM on them or continue\
          \ finetuning of BLOOMZ.<br>cc <span data-props=\"{&quot;user&quot;:&quot;yongzx&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/yongzx\"\
          >@<span class=\"underline\">yongzx</span></a></span>\n\n\t</span></span></p>\n"
        raw: "> Has anyone trained the model for a language not currently supported?\
          \ I have a few (german, Russian, polish, ukranian, greek, korean) I'd like\
          \ to train it for (3B model) and would like to know what it would take.\
          \ \n> \n> I was thinking:\n> Option 1: large corpus + translated prompt\
          \ xP3\n> Option 2: translated prompt xP3\n> \n> I'd prefer just to do option\
          \ 2.... but that might not be sufficient. Would love any insights others\
          \ have had\n> \n> PS: In the xP3 paper https://arxiv.org/pdf/2211.01786v1.pdf\
          \ it specifically says there is a minor amount of non-specified langs in\
          \ the ROOTS dataset BLOOM was trained on. I have observed it knows some\
          \ german (not so good tho...).\n\nHey we recently tried adding a language\
          \ to BLOOMZ in this paper: https://arxiv.org/abs/2212.09535\nThe best way\
          \ to go would be to construct an xP3 corpus for the languages you would\
          \ like (similar to [xP3ru](https://huggingface.co/datasets/bs-la/xP3ru)\
          \ from the paper) & then finetune BLOOM on them or continue finetuning of\
          \ BLOOMZ. \ncc @yongzx"
        updatedAt: '2023-01-09T18:15:03.119Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - yongzx
    id: 63bc59a70718d5f0c1ee3527
    type: comment
  author: Muennighoff
  content: "> Has anyone trained the model for a language not currently supported?\
    \ I have a few (german, Russian, polish, ukranian, greek, korean) I'd like to\
    \ train it for (3B model) and would like to know what it would take. \n> \n> I\
    \ was thinking:\n> Option 1: large corpus + translated prompt xP3\n> Option 2:\
    \ translated prompt xP3\n> \n> I'd prefer just to do option 2.... but that might\
    \ not be sufficient. Would love any insights others have had\n> \n> PS: In the\
    \ xP3 paper https://arxiv.org/pdf/2211.01786v1.pdf it specifically says there\
    \ is a minor amount of non-specified langs in the ROOTS dataset BLOOM was trained\
    \ on. I have observed it knows some german (not so good tho...).\n\nHey we recently\
    \ tried adding a language to BLOOMZ in this paper: https://arxiv.org/abs/2212.09535\n\
    The best way to go would be to construct an xP3 corpus for the languages you would\
    \ like (similar to [xP3ru](https://huggingface.co/datasets/bs-la/xP3ru) from the\
    \ paper) & then finetune BLOOM on them or continue finetuning of BLOOMZ. \ncc\
    \ @yongzx"
  created_at: 2023-01-09 18:15:03+00:00
  edited: false
  hidden: false
  id: 63bc59a70718d5f0c1ee3527
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b398e1df417aa512343a4be7f4faaed2.svg
      fullname: Kevin Danikowski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kmdanikowski
      type: user
    createdAt: '2023-01-09T19:56:39.000Z'
    data:
      edited: false
      editors:
      - kmdanikowski
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b398e1df417aa512343a4be7f4faaed2.svg
          fullname: Kevin Danikowski
          isHf: false
          isPro: false
          name: kmdanikowski
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \ , this is excellent, I love this paper! I can't seem to find the models\
          \ used in the experiments or the weights. Would you happen to have those\
          \ available anywhere by chance? </p>\n<p>Based on the paper, I think the\
          \ xP3 would be a great solution for my use with the 3B model. For the 560m\
          \ one, I'm trying to construct a multi-lingual summarizer that would work\
          \ in a few more untrained languages. Based on the paper, would xP3 work\
          \ for this? Because it seems to suggest pre-training would be the way to\
          \ go (a main language of interest here is Romanian.)</p>\n"
        raw: "Hello @Muennighoff , this is excellent, I love this paper! I can't seem\
          \ to find the models used in the experiments or the weights. Would you happen\
          \ to have those available anywhere by chance? \n\nBased on the paper, I\
          \ think the xP3 would be a great solution for my use with the 3B model.\
          \ For the 560m one, I'm trying to construct a multi-lingual summarizer that\
          \ would work in a few more untrained languages. Based on the paper, would\
          \ xP3 work for this? Because it seems to suggest pre-training would be the\
          \ way to go (a main language of interest here is Romanian.)"
        updatedAt: '2023-01-09T19:56:39.191Z'
      numEdits: 0
      reactions: []
    id: 63bc71770718d5f0c1f03b4c
    type: comment
  author: kmdanikowski
  content: "Hello @Muennighoff , this is excellent, I love this paper! I can't seem\
    \ to find the models used in the experiments or the weights. Would you happen\
    \ to have those available anywhere by chance? \n\nBased on the paper, I think\
    \ the xP3 would be a great solution for my use with the 3B model. For the 560m\
    \ one, I'm trying to construct a multi-lingual summarizer that would work in a\
    \ few more untrained languages. Based on the paper, would xP3 work for this? Because\
    \ it seems to suggest pre-training would be the way to go (a main language of\
    \ interest here is Romanian.)"
  created_at: 2023-01-09 19:56:39+00:00
  edited: false
  hidden: false
  id: 63bc71770718d5f0c1f03b4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-01-09T20:19:21.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<blockquote>\n<p>Hello <span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \ , this is excellent, I love this paper! I can't seem to find the models\
          \ used in the experiments or the weights. Would you happen to have those\
          \ available anywhere by chance? </p>\n<p>Based on the paper, I think the\
          \ xP3 would be a great solution for my use with the 3B model. For the 560m\
          \ one, I'm trying to construct a multi-lingual summarizer that would work\
          \ in a few more untrained languages. Based on the paper, would xP3 work\
          \ for this? Because it seems to suggest pre-training would be the way to\
          \ go (a main language of interest here is Romanian.)</p>\n</blockquote>\n\
          <p>Sure, the models are all on the hub under either <code>bigscience</code>\
          \ or <code>bs-la</code>, e.g. <a href=\"https://huggingface.co/bs-la/bloomz-7b1-4b-xp3ru\"\
          >https://huggingface.co/bs-la/bloomz-7b1-4b-xp3ru</a><br>Let me know if\
          \ you don't find a specific one.</p>\n<p>xP3 with the languages you desire\
          \ should work well. I would recommend just adding your languages to the\
          \ existing xP3 mixture (i.e. keep the other languages even if you don't\
          \ need them), as this was better than e.g. single language xP3 with only\
          \ Russian as shown in the paper.</p>\n"
        raw: "> Hello @Muennighoff , this is excellent, I love this paper! I can't\
          \ seem to find the models used in the experiments or the weights. Would\
          \ you happen to have those available anywhere by chance? \n> \n> Based on\
          \ the paper, I think the xP3 would be a great solution for my use with the\
          \ 3B model. For the 560m one, I'm trying to construct a multi-lingual summarizer\
          \ that would work in a few more untrained languages. Based on the paper,\
          \ would xP3 work for this? Because it seems to suggest pre-training would\
          \ be the way to go (a main language of interest here is Romanian.)\n\nSure,\
          \ the models are all on the hub under either `bigscience` or `bs-la`, e.g.\
          \ https://huggingface.co/bs-la/bloomz-7b1-4b-xp3ru \nLet me know if you\
          \ don't find a specific one.\n\nxP3 with the languages you desire should\
          \ work well. I would recommend just adding your languages to the existing\
          \ xP3 mixture (i.e. keep the other languages even if you don't need them),\
          \ as this was better than e.g. single language xP3 with only Russian as\
          \ shown in the paper."
        updatedAt: '2023-01-09T20:19:21.752Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - yongzx
    id: 63bc76c918e2f66c5ea4bb57
    type: comment
  author: Muennighoff
  content: "> Hello @Muennighoff , this is excellent, I love this paper! I can't seem\
    \ to find the models used in the experiments or the weights. Would you happen\
    \ to have those available anywhere by chance? \n> \n> Based on the paper, I think\
    \ the xP3 would be a great solution for my use with the 3B model. For the 560m\
    \ one, I'm trying to construct a multi-lingual summarizer that would work in a\
    \ few more untrained languages. Based on the paper, would xP3 work for this? Because\
    \ it seems to suggest pre-training would be the way to go (a main language of\
    \ interest here is Romanian.)\n\nSure, the models are all on the hub under either\
    \ `bigscience` or `bs-la`, e.g. https://huggingface.co/bs-la/bloomz-7b1-4b-xp3ru\
    \ \nLet me know if you don't find a specific one.\n\nxP3 with the languages you\
    \ desire should work well. I would recommend just adding your languages to the\
    \ existing xP3 mixture (i.e. keep the other languages even if you don't need them),\
    \ as this was better than e.g. single language xP3 with only Russian as shown\
    \ in the paper."
  created_at: 2023-01-09 20:19:21+00:00
  edited: false
  hidden: false
  id: 63bc76c918e2f66c5ea4bb57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7425d978d3c64972b01cfe9e060e3f43.svg
      fullname: Yong Zheng Xin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yongzx
      type: user
    createdAt: '2023-01-10T03:18:43.000Z'
    data:
      edited: false
      editors:
      - yongzx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7425d978d3c64972b01cfe9e060e3f43.svg
          fullname: Yong Zheng Xin
          isHf: false
          isPro: false
          name: yongzx
          type: user
        html: "<blockquote>\n<p>I can't seem to find the models used in the experiments\
          \ or the weights. Would you happen to have those available anywhere by chance?</p>\n\
          </blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;kmdanikowski&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kmdanikowski\"\
          >@<span class=\"underline\">kmdanikowski</span></a></span>\n\n\t</span></span>,\
          \ I think <span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \  has helped answered all your questions. Models can be found on <a href=\"\
          https://huggingface.co/bs-la/\">https://huggingface.co/bs-la/</a>, and we\
          \ are still uploading the rest of the models . Our codebase can be found\
          \ on <a rel=\"nofollow\" href=\"https://github.com/bigscience-workshop/multilingual-modeling\"\
          >https://github.com/bigscience-workshop/multilingual-modeling</a>. </p>\n\
          <p>Let us know if you have any other questions!</p>\n"
        raw: "> I can't seem to find the models used in the experiments or the weights.\
          \ Would you happen to have those available anywhere by chance?\n\nHi @kmdanikowski,\
          \ I think @Muennighoff  has helped answered all your questions. Models can\
          \ be found on https://huggingface.co/bs-la/, and we are still uploading\
          \ the rest of the models . Our codebase can be found on https://github.com/bigscience-workshop/multilingual-modeling.\
          \ \n\nLet us know if you have any other questions!"
        updatedAt: '2023-01-10T03:18:43.993Z'
      numEdits: 0
      reactions: []
    id: 63bcd9133c229a497bb795d7
    type: comment
  author: yongzx
  content: "> I can't seem to find the models used in the experiments or the weights.\
    \ Would you happen to have those available anywhere by chance?\n\nHi @kmdanikowski,\
    \ I think @Muennighoff  has helped answered all your questions. Models can be\
    \ found on https://huggingface.co/bs-la/, and we are still uploading the rest\
    \ of the models . Our codebase can be found on https://github.com/bigscience-workshop/multilingual-modeling.\
    \ \n\nLet us know if you have any other questions!"
  created_at: 2023-01-10 03:18:43+00:00
  edited: false
  hidden: false
  id: 63bcd9133c229a497bb795d7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: bigscience/bloomz
repo_type: model
status: open
target_branch: null
title: Train on new un-supported language, i.e. German?
