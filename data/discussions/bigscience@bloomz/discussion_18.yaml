!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pai4451
conflicting_files: null
created_at: 2022-11-09 08:54:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-11-09T08:54:49.000Z'
    data:
      edited: true
      editors:
      - pai4451
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
          fullname: Pai Yao Wei
          isHf: false
          isPro: false
          name: pai4451
          type: user
        html: '<p>Just wondering if the safetensors checkpoints are the same as pickle
          (.bin)  files. </p>

          <p>I want to double-check if both are the latest checkpoints because, from
          the commit history, the pickle files are ready about 2 months ago. Now I
          am trying to use the <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference">HuggingFace
          text-generation-inference server solution</a>, which also uses the safetensors
          package.</p>

          '
        raw: "Just wondering if the safetensors checkpoints are the same as pickle\
          \ (.bin)  files. \n\nI want to double-check if both are the latest checkpoints\
          \ because, from the commit history, the pickle files are ready about 2 months\
          \ ago. Now I am trying to use the [HuggingFace text-generation-inference\
          \ server solution](https://github.com/huggingface/text-generation-inference),\
          \ which also uses the safetensors package."
        updatedAt: '2022-11-18T02:00:55.365Z'
      numEdits: 11
      reactions: []
    id: 636b6ad958a8f9348d0ab82c
    type: comment
  author: pai4451
  content: "Just wondering if the safetensors checkpoints are the same as pickle (.bin)\
    \  files. \n\nI want to double-check if both are the latest checkpoints because,\
    \ from the commit history, the pickle files are ready about 2 months ago. Now\
    \ I am trying to use the [HuggingFace text-generation-inference server solution](https://github.com/huggingface/text-generation-inference),\
    \ which also uses the safetensors package."
  created_at: 2022-11-09 08:54:49+00:00
  edited: true
  hidden: false
  id: 636b6ad958a8f9348d0ab82c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2022-11-09T10:26:17.000Z'
    data:
      edited: false
      editors:
      - julien-c
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\"\
          >@<span class=\"underline\">TimeRobber</span></a></span>\n\n\t</span></span>\
          \ =)</p>\n"
        raw: cc @TimeRobber =)
        updatedAt: '2022-11-09T10:26:17.732Z'
      numEdits: 0
      reactions: []
    id: 636b8049cde3707d109968fc
    type: comment
  author: julien-c
  content: cc @TimeRobber =)
  created_at: 2022-11-09 10:26:17+00:00
  edited: false
  hidden: false
  id: 636b8049cde3707d109968fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-09T13:28:34.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;pai4451&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pai4451\">@<span class=\"\
          underline\">pai4451</span></a></span>\n\n\t</span></span> ! Yes they are\
          \ the same. Essentially we need the <code>safetensors</code> weights do\
          \ make the current inference deployment work, it makes loading in sharded\
          \ fashion easier without any preprocessing. Glad you're using <code>safetensors</code>,\
          \ could you describe your usecase a bit?</p>\n"
        raw: Hi @pai4451 ! Yes they are the same. Essentially we need the `safetensors`
          weights do make the current inference deployment work, it makes loading
          in sharded fashion easier without any preprocessing. Glad you're using `safetensors`,
          could you describe your usecase a bit?
        updatedAt: '2022-11-09T13:28:34.743Z'
      numEdits: 0
      reactions: []
    id: 636bab022dea76f9b79e6a4f
    type: comment
  author: TimeRobber
  content: Hi @pai4451 ! Yes they are the same. Essentially we need the `safetensors`
    weights do make the current inference deployment work, it makes loading in sharded
    fashion easier without any preprocessing. Glad you're using `safetensors`, could
    you describe your usecase a bit?
  created_at: 2022-11-09 13:28:34+00:00
  edited: false
  hidden: false
  id: 636bab022dea76f9b79e6a4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-11-09T13:46:40.000Z'
    data:
      edited: true
      editors:
      - pai4451
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
          fullname: Pai Yao Wei
          isHf: false
          isPro: false
          name: pai4451
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\">@<span class=\"\
          underline\">TimeRobber</span></a></span>\n\n\t</span></span> Thanks for\
          \ your reply. Actually I looking for a fast and stable serving solution\
          \ for bloom model. One of the candidate I am trying to use is the <a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/text-generation-inference\"\
          >HuggingFace server solution</a>, which also uses the safetensors package.\
          \ I used to serve bloom by the DeepSpeed framework to load the MP sharded\
          \ checkpoints but I ran into some instability issues. I will try BLOOMZ\
          \ with HuggingFace serving frameworks in the next few days. Hope the service\
          \ will become much stable :)</p>\n"
        raw: '@TimeRobber Thanks for your reply. Actually I looking for a fast and
          stable serving solution for bloom model. One of the candidate I am trying
          to use is the [HuggingFace server solution](https://github.com/huggingface/text-generation-inference),
          which also uses the safetensors package. I used to serve bloom by the DeepSpeed
          framework to load the MP sharded checkpoints but I ran into some instability
          issues. I will try BLOOMZ with HuggingFace serving frameworks in the next
          few days. Hope the service will become much stable :)'
        updatedAt: '2022-11-09T17:10:41.490Z'
      numEdits: 4
      reactions: []
    id: 636baf402dea76f9b79e8fda
    type: comment
  author: pai4451
  content: '@TimeRobber Thanks for your reply. Actually I looking for a fast and stable
    serving solution for bloom model. One of the candidate I am trying to use is the
    [HuggingFace server solution](https://github.com/huggingface/text-generation-inference),
    which also uses the safetensors package. I used to serve bloom by the DeepSpeed
    framework to load the MP sharded checkpoints but I ran into some instability issues.
    I will try BLOOMZ with HuggingFace serving frameworks in the next few days. Hope
    the service will become much stable :)'
  created_at: 2022-11-09 13:46:40+00:00
  edited: true
  hidden: false
  id: 636baf402dea76f9b79e8fda
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-09T15:02:51.000Z'
    data:
      edited: true
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<p>That's super nice to hear. <span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span>\
          \ has been doing an insane job. But from our experience that solution is\
          \ very fast and more stable than the DeepSpeed version (we've had issue\
          \ with the service randomly crashing). You can run BLOOM/BLOOMZ and mt0-xxl\
          \ using that solution. </p>\n<p>BTW feel free to share your experience,\
          \ we have little signal from the outside if what we're doing is well received\
          \ or not. We might keep free services longer (typically hosting BLOOM/BLOOMZ)\
          \  if we get more signal :D</p>\n"
        raw: "That's super nice to hear. @olivierdehaene has been doing an insane\
          \ job. But from our experience that solution is very fast and more stable\
          \ than the DeepSpeed version (we've had issue with the service randomly\
          \ crashing). You can run BLOOM/BLOOMZ and mt0-xxl using that solution. \n\
          \nBTW feel free to share your experience, we have little signal from the\
          \ outside if what we're doing is well received or not. We might keep free\
          \ services longer (typically hosting BLOOM/BLOOMZ)  if we get more signal\
          \ :D"
        updatedAt: '2022-11-09T15:36:23.420Z'
      numEdits: 1
      reactions: []
    id: 636bc11b57a18fbcd60d822b
    type: comment
  author: TimeRobber
  content: "That's super nice to hear. @olivierdehaene has been doing an insane job.\
    \ But from our experience that solution is very fast and more stable than the\
    \ DeepSpeed version (we've had issue with the service randomly crashing). You\
    \ can run BLOOM/BLOOMZ and mt0-xxl using that solution. \n\nBTW feel free to share\
    \ your experience, we have little signal from the outside if what we're doing\
    \ is well received or not. We might keep free services longer (typically hosting\
    \ BLOOM/BLOOMZ)  if we get more signal :D"
  created_at: 2022-11-09 15:02:51+00:00
  edited: true
  hidden: false
  id: 636bc11b57a18fbcd60d822b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-11-09T17:00:51.000Z'
    data:
      edited: true
      editors:
      - pai4451
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
          fullname: Pai Yao Wei
          isHf: false
          isPro: false
          name: pai4451
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\">@<span class=\"\
          underline\">TimeRobber</span></a></span>\n\n\t</span></span> No problem,\
          \ I will share the experience here on using the HuggingFace service solution\
          \ once I succeed. We also had lots of crashing issues on serving BLOOM with\
          \ DeepSpeed, so the amazing work by <span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span>\
          \ is really helpful :D</p>\n"
        raw: '@TimeRobber No problem, I will share the experience here on using the
          HuggingFace service solution once I succeed. We also had lots of crashing
          issues on serving BLOOM with DeepSpeed, so the amazing work by @olivierdehaene
          is really helpful :D'
        updatedAt: '2022-11-14T13:42:55.651Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - julien-c
    id: 636bdcc357a18fbcd60e7e5b
    type: comment
  author: pai4451
  content: '@TimeRobber No problem, I will share the experience here on using the
    HuggingFace service solution once I succeed. We also had lots of crashing issues
    on serving BLOOM with DeepSpeed, so the amazing work by @olivierdehaene is really
    helpful :D'
  created_at: 2022-11-09 17:00:51+00:00
  edited: true
  hidden: false
  id: 636bdcc357a18fbcd60e7e5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2022-11-14T13:41:54.000Z'
    data:
      edited: false
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;pai4451&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/pai4451\"\
          >@<span class=\"underline\">pai4451</span></a></span>\n\n\t</span></span>!<br>Feel\
          \ free to ping me anytime or open issues on the repo if  you face any issues\
          \ running text-generation-inference! It's a new solution and we are eager\
          \ to collect community feedback on it :)</p>\n"
        raw: "Hello @pai4451! \nFeel free to ping me anytime or open issues on the\
          \ repo if  you face any issues running text-generation-inference! It's a\
          \ new solution and we are eager to collect community feedback on it :)"
        updatedAt: '2022-11-14T13:41:54.161Z'
      numEdits: 0
      reactions: []
    id: 637245a2acef705233c52942
    type: comment
  author: olivierdehaene
  content: "Hello @pai4451! \nFeel free to ping me anytime or open issues on the repo\
    \ if  you face any issues running text-generation-inference! It's a new solution\
    \ and we are eager to collect community feedback on it :)"
  created_at: 2022-11-14 13:41:54+00:00
  edited: false
  hidden: false
  id: 637245a2acef705233c52942
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-11-17T08:08:14.000Z'
    data:
      edited: true
      editors:
      - pai4451
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
          fullname: Pai Yao Wei
          isHf: false
          isPro: false
          name: pai4451
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span>,\
          \ <span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\">@<span class=\"\
          underline\">TimeRobber</span></a></span>\n\n\t</span></span>, thanks for\
          \ providing an amazing framework <a rel=\"nofollow\" href=\"https://github.com/huggingface/text-generation-inference\"\
          >text-generation-server</a> to serve BLOOMZ, and I can launch the BLOOMZ-176b\
          \ server and made inference request on my 8x A6000 (48G) server without\
          \ problems.</p>\n<p>Now since I have two servers available (both are 8x\
          \ A6000), I want to utilize both. I modified some code and was able to launch\
          \ <code> text-generation-server</code> on two nodes with 16 GPUs by NCCL\
          \ and create 16 UNIX sockets. The current issue I face is the <code>text-generation-router</code>\
          \ cannot connect to the 16 UNIX sockets used in gPRC server. I\u2019m not\
          \ sure whether I have to change to TCP/IP to support multi-nodes. Do you\
          \ guys have any suggestions on how to modify the code for multi-nodes settings?</p>\n"
        raw: "Hi @olivierdehaene, @TimeRobber, thanks for providing an amazing framework\
          \ [text-generation-server](https://github.com/huggingface/text-generation-inference)\
          \ to serve BLOOMZ, and I can launch the BLOOMZ-176b server and made inference\
          \ request on my 8x A6000 (48G) server without problems.\n\nNow since I have\
          \ two servers available (both are 8x A6000), I want to utilize both. I modified\
          \ some code and was able to launch ` text-generation-server` on two nodes\
          \ with 16 GPUs by NCCL and create 16 UNIX sockets. The current issue I face\
          \ is the ` text-generation-router ` cannot connect to the 16 UNIX sockets\
          \ used in gPRC server. I\u2019m not sure whether I have to change to TCP/IP\
          \ to support multi-nodes. Do you guys have any suggestions on how to modify\
          \ the code for multi-nodes settings?"
        updatedAt: '2022-11-17T08:36:35.164Z'
      numEdits: 4
      reactions: []
    id: 6375ebee319390860283ed70
    type: comment
  author: pai4451
  content: "Hi @olivierdehaene, @TimeRobber, thanks for providing an amazing framework\
    \ [text-generation-server](https://github.com/huggingface/text-generation-inference)\
    \ to serve BLOOMZ, and I can launch the BLOOMZ-176b server and made inference\
    \ request on my 8x A6000 (48G) server without problems.\n\nNow since I have two\
    \ servers available (both are 8x A6000), I want to utilize both. I modified some\
    \ code and was able to launch ` text-generation-server` on two nodes with 16 GPUs\
    \ by NCCL and create 16 UNIX sockets. The current issue I face is the ` text-generation-router\
    \ ` cannot connect to the 16 UNIX sockets used in gPRC server. I\u2019m not sure\
    \ whether I have to change to TCP/IP to support multi-nodes. Do you guys have\
    \ any suggestions on how to modify the code for multi-nodes settings?"
  created_at: 2022-11-17 08:08:14+00:00
  edited: true
  hidden: false
  id: 6375ebee319390860283ed70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-17T11:29:52.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>I wouldn''t recommend running inference on two nodes. You won''t
          get better latency (typically from our experience, using 8A100 is actually
          faster than 16A100). I think the best would be to have two replicas and
          have a load balancer module on top.</p>

          '
        raw: I wouldn't recommend running inference on two nodes. You won't get better
          latency (typically from our experience, using 8A100 is actually faster than
          16A100). I think the best would be to have two replicas and have a load
          balancer module on top.
        updatedAt: '2022-11-17T11:29:52.631Z'
      numEdits: 0
      reactions: []
    id: 63761b30821efd1475adfc0a
    type: comment
  author: TimeRobber
  content: I wouldn't recommend running inference on two nodes. You won't get better
    latency (typically from our experience, using 8A100 is actually faster than 16A100).
    I think the best would be to have two replicas and have a load balancer module
    on top.
  created_at: 2022-11-17 11:29:52+00:00
  edited: false
  hidden: false
  id: 63761b30821efd1475adfc0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-11-17T12:45:23.000Z'
    data:
      edited: true
      editors:
      - pai4451
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
          fullname: Pai Yao Wei
          isHf: false
          isPro: false
          name: pai4451
          type: user
        html: "<blockquote>\n<p>I wouldn't recommend running inference on two nodes.\
          \ You won't get better latency (typically, from our experience, using 8A100\
          \ is faster than 16A100). I think the best would be to have two replicas\
          \ and a load balancer module on top.</p>\n</blockquote>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;TimeRobber&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/TimeRobber\">@<span class=\"underline\"\
          >TimeRobber</span></a></span>\n\n\t</span></span> Thanks for your reply.\
          \ We made it works on multi-nodes now, but as you mentioned, the speed gets\
          \ slower. On a single 8x A6000 server, I can get time per token of about\
          \ 85ms, but on two nodes, it becomes about 150ms. </p>\n<p>When I used DeepSpeed\
          \ on two nodes, I could get a time per token of about 69ms. All tests are\
          \ measured in batch size 1, so I may get better results with batches on\
          \ the HuggingFace server framework. We found DeepSpeed had a lot of stability\
          \ issues when using batch in the past.</p>\n<p>Do I have to prepare a list\
          \ of prompts to use the dynamic batching mentioned in README? Or will current\
          \ implementations automatically gather the incoming requests as batches?\
          \ Even for batching requests with different parameters?</p>\n"
        raw: "> I wouldn't recommend running inference on two nodes. You won't get\
          \ better latency (typically, from our experience, using 8A100 is faster\
          \ than 16A100). I think the best would be to have two replicas and a load\
          \ balancer module on top.\n\n@TimeRobber Thanks for your reply. We made\
          \ it works on multi-nodes now, but as you mentioned, the speed gets slower.\
          \ On a single 8x A6000 server, I can get time per token of about 85ms, but\
          \ on two nodes, it becomes about 150ms. \n\nWhen I used DeepSpeed on two\
          \ nodes, I could get a time per token of about 69ms. All tests are measured\
          \ in batch size 1, so I may get better results with batches on the HuggingFace\
          \ server framework. We found DeepSpeed had a lot of stability issues when\
          \ using batch in the past.\n\nDo I have to prepare a list of prompts to\
          \ use the dynamic batching mentioned in README? Or will current implementations\
          \ automatically gather the incoming requests as batches? Even for batching\
          \ requests with different parameters?"
        updatedAt: '2022-11-18T09:30:55.821Z'
      numEdits: 6
      reactions: []
    id: 63762ce3f1d3d27b7ddaa8b8
    type: comment
  author: pai4451
  content: "> I wouldn't recommend running inference on two nodes. You won't get better\
    \ latency (typically, from our experience, using 8A100 is faster than 16A100).\
    \ I think the best would be to have two replicas and a load balancer module on\
    \ top.\n\n@TimeRobber Thanks for your reply. We made it works on multi-nodes now,\
    \ but as you mentioned, the speed gets slower. On a single 8x A6000 server, I\
    \ can get time per token of about 85ms, but on two nodes, it becomes about 150ms.\
    \ \n\nWhen I used DeepSpeed on two nodes, I could get a time per token of about\
    \ 69ms. All tests are measured in batch size 1, so I may get better results with\
    \ batches on the HuggingFace server framework. We found DeepSpeed had a lot of\
    \ stability issues when using batch in the past.\n\nDo I have to prepare a list\
    \ of prompts to use the dynamic batching mentioned in README? Or will current\
    \ implementations automatically gather the incoming requests as batches? Even\
    \ for batching requests with different parameters?"
  created_at: 2022-11-17 12:45:23+00:00
  edited: true
  hidden: false
  id: 63762ce3f1d3d27b7ddaa8b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2022-11-17T15:07:13.000Z'
    data:
      edited: false
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: '<blockquote>

          <p>When I used DeepSpeed on two nodes, I can get time per token about 69ms.</p>

          </blockquote>

          <p>Have you made sure that the outputs are the same? We indeed saw some
          improvements in latency when using deepspeed but the output were not the
          same which made us skeptical and revert to our own implementation.</p>

          <blockquote>

          <p>Or will the current implementations automatically gather incoming requests
          as batches? Even for requests with different parameters?</p>

          </blockquote>

          <p>Yes! That''s exactly how it works.</p>

          '
        raw: '> When I used DeepSpeed on two nodes, I can get time per token about
          69ms.


          Have you made sure that the outputs are the same? We indeed saw some improvements
          in latency when using deepspeed but the output were not the same which made
          us skeptical and revert to our own implementation.


          > Or will the current implementations automatically gather incoming requests
          as batches? Even for requests with different parameters?


          Yes! That''s exactly how it works.'
        updatedAt: '2022-11-17T15:07:13.358Z'
      numEdits: 0
      reactions: []
    id: 63764e2162ca7263a4c1b0ce
    type: comment
  author: olivierdehaene
  content: '> When I used DeepSpeed on two nodes, I can get time per token about 69ms.


    Have you made sure that the outputs are the same? We indeed saw some improvements
    in latency when using deepspeed but the output were not the same which made us
    skeptical and revert to our own implementation.


    > Or will the current implementations automatically gather incoming requests as
    batches? Even for requests with different parameters?


    Yes! That''s exactly how it works.'
  created_at: 2022-11-17 15:07:13+00:00
  edited: false
  hidden: false
  id: 63764e2162ca7263a4c1b0ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-11-18T00:53:28.000Z'
    data:
      edited: true
      editors:
      - pai4451
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
          fullname: Pai Yao Wei
          isHf: false
          isPro: false
          name: pai4451
          type: user
        html: "<blockquote>\n<p>Have you made sure that the outputs are the same?\
          \ We indeed saw some improvements in latency when using DeepSpeed, but the\
          \ output was not the same, which made us skeptical and revert to our implementation.</p>\n\
          </blockquote>\n<p>Yes, the outputs are the same. I did get some garbage\
          \ outputs from specific versions of DeepSpeed. But I think the DeepSpeed\
          \ team already solved the inconsistent output issues.</p>\n<blockquote>\n\
          <p>Yes! That's exactly how it works.</p>\n</blockquote>\n<p>Then I think\
          \ I can get better throughput using the HugginFace server because my experience\
          \ told me that DeepSpeed is unstable on batch inputs. Thanks, <span data-props=\"\
          {&quot;user&quot;:&quot;olivierdehaene&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/olivierdehaene\">@<span class=\"underline\"\
          >olivierdehaene</span></a></span>\n\n\t</span></span>!</p>\n"
        raw: "> \n> Have you made sure that the outputs are the same? We indeed saw\
          \ some improvements in latency when using DeepSpeed, but the output was\
          \ not the same, which made us skeptical and revert to our implementation.\n\
          > \n\nYes, the outputs are the same. I did get some garbage outputs from\
          \ specific versions of DeepSpeed. But I think the DeepSpeed team already\
          \ solved the inconsistent output issues.\n\n>\n> Yes! That's exactly how\
          \ it works.\n>\n\nThen I think I can get better throughput using the HugginFace\
          \ server because my experience told me that DeepSpeed is unstable on batch\
          \ inputs. Thanks, @olivierdehaene!"
        updatedAt: '2022-11-18T01:50:39.851Z'
      numEdits: 2
      reactions: []
    id: 6376d788af322520cb6b0673
    type: comment
  author: pai4451
  content: "> \n> Have you made sure that the outputs are the same? We indeed saw\
    \ some improvements in latency when using DeepSpeed, but the output was not the\
    \ same, which made us skeptical and revert to our implementation.\n> \n\nYes,\
    \ the outputs are the same. I did get some garbage outputs from specific versions\
    \ of DeepSpeed. But I think the DeepSpeed team already solved the inconsistent\
    \ output issues.\n\n>\n> Yes! That's exactly how it works.\n>\n\nThen I think\
    \ I can get better throughput using the HugginFace server because my experience\
    \ told me that DeepSpeed is unstable on batch inputs. Thanks, @olivierdehaene!"
  created_at: 2022-11-18 00:53:28+00:00
  edited: true
  hidden: false
  id: 6376d788af322520cb6b0673
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-11-18T01:39:45.000Z'
    data:
      from: Is the safetensors checkpoints same as pickle?
      to: Questions on safetensors and text-generation-inference server
    id: 6376e261cdcc1bf6308612c3
    type: title-change
  author: pai4451
  created_at: 2022-11-18 01:39:45+00:00
  id: 6376e261cdcc1bf6308612c3
  new_title: Questions on safetensors and text-generation-inference server
  old_title: Is the safetensors checkpoints same as pickle?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-18T08:42:13.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>I''m curious if you could try a single node (8A6000) using DeepSpeed.
          Typically do you get the factor 2 improvement on latency? It''d be insane
          to have 35ms per token (and would probably mean we should make improvement
          on our solution)</p>

          '
        raw: I'm curious if you could try a single node (8A6000) using DeepSpeed.
          Typically do you get the factor 2 improvement on latency? It'd be insane
          to have 35ms per token (and would probably mean we should make improvement
          on our solution)
        updatedAt: '2022-11-18T08:42:13.902Z'
      numEdits: 0
      reactions: []
    id: 6377456580c7d5df2899c116
    type: comment
  author: TimeRobber
  content: I'm curious if you could try a single node (8A6000) using DeepSpeed. Typically
    do you get the factor 2 improvement on latency? It'd be insane to have 35ms per
    token (and would probably mean we should make improvement on our solution)
  created_at: 2022-11-18 08:42:13+00:00
  edited: false
  hidden: false
  id: 6377456580c7d5df2899c116
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-18T08:52:08.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>The only things I can think are:</p>

          <ul>

          <li>guess 1: DS is not doing tensor parallel of 8 but of 4 or 2. The reason
          why I think that way is that moving from 16 A100 to 8 A100 got us factor
          2 improvement. Probably some optimizations we can do on this end.</li>

          <li>guess 2: the custom kernel written is poor on A6000. It was only tested
          on A100.</li>

          <li>guess 3: depending on how you compute latency, any chance that you''re
          actually retriggering jit compilation?</li>

          </ul>

          <p>(Mostly thinking out loud)</p>

          '
        raw: "The only things I can think are:\n - guess 1: DS is not doing tensor\
          \ parallel of 8 but of 4 or 2. The reason why I think that way is that moving\
          \ from 16 A100 to 8 A100 got us factor 2 improvement. Probably some optimizations\
          \ we can do on this end.\n - guess 2: the custom kernel written is poor\
          \ on A6000. It was only tested on A100.\n - guess 3: depending on how you\
          \ compute latency, any chance that you're actually retriggering jit compilation?\n\
          \n(Mostly thinking out loud)"
        updatedAt: '2022-11-18T08:52:08.997Z'
      numEdits: 0
      reactions: []
    id: 637747b8455f6ad89ac3fb83
    type: comment
  author: TimeRobber
  content: "The only things I can think are:\n - guess 1: DS is not doing tensor parallel\
    \ of 8 but of 4 or 2. The reason why I think that way is that moving from 16 A100\
    \ to 8 A100 got us factor 2 improvement. Probably some optimizations we can do\
    \ on this end.\n - guess 2: the custom kernel written is poor on A6000. It was\
    \ only tested on A100.\n - guess 3: depending on how you compute latency, any\
    \ chance that you're actually retriggering jit compilation?\n\n(Mostly thinking\
    \ out loud)"
  created_at: 2022-11-18 08:52:08+00:00
  edited: false
  hidden: false
  id: 637747b8455f6ad89ac3fb83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-11-18T09:20:16.000Z'
    data:
      edited: true
      editors:
      - pai4451
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
          fullname: Pai Yao Wei
          isHf: false
          isPro: false
          name: pai4451
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\">@<span class=\"\
          underline\">TimeRobber</span></a></span>\n\n\t</span></span> I cannot run\
          \ BLOOM-176b with DeepSpeed on a single 8x A6000 server because I always\
          \ got OOM. </p>\n<p>The 69ms latency on two 8x A6000 servers was measured\
          \ by this <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers-bloom-inference/tree/main/bloom-inference-scripts\"\
          >DeepSpeed inference script</a>; the link also shows the latency on A100,\
          \ which is 45ms per token on batch size 1. Although I ran on two nodes,\
          \ I guess the latency I got (69ms) on A6000 is quite reasonable, given that\
          \ A100 is superior.</p>\n<p>For the latency of HuggingFace text-generation-inference,\
          \ I recorded the numbers from the log of <code>text-generation-router</code>,\
          \ and I think I didn\u2019t retrigger the jit compilation. I installed the\
          \ server via Dockerfile provided by the HuggingFace/text-generation-inference\
          \ repo.</p>\n<p>Anyways thanks, <span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\"\
          >@<span class=\"underline\">TimeRobber</span></a></span>\n\n\t</span></span>.\
          \ I will keep looking at how to optimize my A6000 servers. (Unfortunately,\
          \ I won\u2019t have A100 servers shortly)</p>\n"
        raw: "@TimeRobber I cannot run BLOOM-176b with DeepSpeed on a single 8x A6000\
          \ server because I always got OOM. \n\nThe 69ms latency on two 8x A6000\
          \ servers was measured by this [DeepSpeed inference script](https://github.com/huggingface/transformers-bloom-inference/tree/main/bloom-inference-scripts);\
          \ the link also shows the latency on A100, which is 45ms per token on batch\
          \ size 1. Although I ran on two nodes, I guess the latency I got (69ms)\
          \ on A6000 is quite reasonable, given that A100 is superior.\n\nFor the\
          \ latency of HuggingFace text-generation-inference, I recorded the numbers\
          \ from the log of `text-generation-router`, and I think I didn\u2019t retrigger\
          \ the jit compilation. I installed the server via Dockerfile provided by\
          \ the HuggingFace/text-generation-inference repo.\n\nAnyways thanks, @TimeRobber.\
          \ I will keep looking at how to optimize my A6000 servers. (Unfortunately,\
          \ I won\u2019t have A100 servers shortly)"
        updatedAt: '2022-11-18T09:27:27.307Z'
      numEdits: 5
      reactions: []
    id: 63774e5082ce7c3b8a5243b9
    type: comment
  author: pai4451
  content: "@TimeRobber I cannot run BLOOM-176b with DeepSpeed on a single 8x A6000\
    \ server because I always got OOM. \n\nThe 69ms latency on two 8x A6000 servers\
    \ was measured by this [DeepSpeed inference script](https://github.com/huggingface/transformers-bloom-inference/tree/main/bloom-inference-scripts);\
    \ the link also shows the latency on A100, which is 45ms per token on batch size\
    \ 1. Although I ran on two nodes, I guess the latency I got (69ms) on A6000 is\
    \ quite reasonable, given that A100 is superior.\n\nFor the latency of HuggingFace\
    \ text-generation-inference, I recorded the numbers from the log of `text-generation-router`,\
    \ and I think I didn\u2019t retrigger the jit compilation. I installed the server\
    \ via Dockerfile provided by the HuggingFace/text-generation-inference repo.\n\
    \nAnyways thanks, @TimeRobber. I will keep looking at how to optimize my A6000\
    \ servers. (Unfortunately, I won\u2019t have A100 servers shortly)"
  created_at: 2022-11-18 09:20:16+00:00
  edited: true
  hidden: false
  id: 63774e5082ce7c3b8a5243b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-18T09:33:10.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<blockquote>

          <p>On a single 8x A6000 server, I can get time per token of about 85ms,</p>

          </blockquote>

          <p>How was this computed?</p>

          '
        raw: '> On a single 8x A6000 server, I can get time per token of about 85ms,


          How was this computed?'
        updatedAt: '2022-11-18T09:33:10.697Z'
      numEdits: 0
      reactions: []
    id: 6377515680c7d5df289a13df
    type: comment
  author: TimeRobber
  content: '> On a single 8x A6000 server, I can get time per token of about 85ms,


    How was this computed?'
  created_at: 2022-11-18 09:33:10+00:00
  edited: false
  hidden: false
  id: 6377515680c7d5df289a13df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-11-18T10:16:28.000Z'
    data:
      edited: true
      editors:
      - pai4451
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
          fullname: Pai Yao Wei
          isHf: false
          isPro: false
          name: pai4451
          type: user
        html: '<blockquote>

          <blockquote>

          <p>On a single 8x A6000 server, I can get time per token of about 85ms,</p>

          </blockquote>

          <p>How was this computed?</p>

          </blockquote>

          <p>I use the same input as DeepSpeed and simply recorded the numbers from
          the log of text-generation-router. The text-generation-router will show
          the statistics of the response in logs.</p>

          '
        raw: "> > On a single 8x A6000 server, I can get time per token of about 85ms,\n\
          > \n> How was this computed?\n\nI use the same input as DeepSpeed and simply\
          \ recorded the numbers from the log of text-generation-router. The text-generation-router\
          \ will show the statistics of the response in logs."
        updatedAt: '2022-11-18T10:16:39.341Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TimeRobber
    id: 63775b7c455f6ad89ac485f5
    type: comment
  author: pai4451
  content: "> > On a single 8x A6000 server, I can get time per token of about 85ms,\n\
    > \n> How was this computed?\n\nI use the same input as DeepSpeed and simply recorded\
    \ the numbers from the log of text-generation-router. The text-generation-router\
    \ will show the statistics of the response in logs."
  created_at: 2022-11-18 10:16:28+00:00
  edited: true
  hidden: false
  id: 63775b7c455f6ad89ac485f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-19T11:28:06.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Let us know how it goes!</p>

          <p>I''m quite interested into understanding what DS does that we''re not
          doing. So typically I''m surprised you could fit our solution in a single
          node, and not the DS one. And more importantly I''m interested in figuring
          out how they are that much faster. I''ll have a go at my guess 1 to see
          if it improves latency by paradoxically having less machines that run in
          parallel.</p>

          '
        raw: 'Let us know how it goes!


          I''m quite interested into understanding what DS does that we''re not doing.
          So typically I''m surprised you could fit our solution in a single node,
          and not the DS one. And more importantly I''m interested in figuring out
          how they are that much faster. I''ll have a go at my guess 1 to see if it
          improves latency by paradoxically having less machines that run in parallel.'
        updatedAt: '2022-11-19T11:28:06.091Z'
      numEdits: 0
      reactions: []
    id: 6378bdc6736e2989331f28ab
    type: comment
  author: TimeRobber
  content: 'Let us know how it goes!


    I''m quite interested into understanding what DS does that we''re not doing. So
    typically I''m surprised you could fit our solution in a single node, and not
    the DS one. And more importantly I''m interested in figuring out how they are
    that much faster. I''ll have a go at my guess 1 to see if it improves latency
    by paradoxically having less machines that run in parallel.'
  created_at: 2022-11-19 11:28:06+00:00
  edited: false
  hidden: false
  id: 6378bdc6736e2989331f28ab
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: bigscience/bloomz
repo_type: model
status: open
target_branch: null
title: Questions on safetensors and text-generation-inference server
