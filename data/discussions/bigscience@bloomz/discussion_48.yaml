!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Cyrile
conflicting_files: null
created_at: 2023-08-16 11:48:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634332912347-noauth.png?w=200&h=200&f=face
      fullname: Cyrile Delestre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cyrile
      type: user
    createdAt: '2023-08-16T12:48:14.000Z'
    data:
      edited: true
      editors:
      - Cyrile
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.911603569984436
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634332912347-noauth.png?w=200&h=200&f=face
          fullname: Cyrile Delestre
          isHf: false
          isPro: false
          name: Cyrile
          type: user
        html: '<p>Hello, I''ve observed a behavior that could be troublesome. Referring
          to the code of the BloomForSequenceClassification class,  the token in question
          works correctly when the padding strategy is set to the right. However,
          for Bloomz-560m, Bloomz-7b1, and Bloomz, the default strategy appears to
          be set to the left... which could lead to unintended behaviors. Wouldn''t
          it be desirable to set the padding strategy to the right by default for
          all models?</p>

          '
        raw: Hello, I've observed a behavior that could be troublesome. Referring
          to the code of the BloomForSequenceClassification class,  the token in question
          works correctly when the padding strategy is set to the right. However,
          for Bloomz-560m, Bloomz-7b1, and Bloomz, the default strategy appears to
          be set to the left... which could lead to unintended behaviors. Wouldn't
          it be desirable to set the padding strategy to the right by default for
          all models?
        updatedAt: '2023-08-16T12:53:30.676Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ingo-m
    id: 64dcc58e2cbce093193fb9a1
    type: comment
  author: Cyrile
  content: Hello, I've observed a behavior that could be troublesome. Referring to
    the code of the BloomForSequenceClassification class,  the token in question works
    correctly when the padding strategy is set to the right. However, for Bloomz-560m,
    Bloomz-7b1, and Bloomz, the default strategy appears to be set to the left...
    which could lead to unintended behaviors. Wouldn't it be desirable to set the
    padding strategy to the right by default for all models?
  created_at: 2023-08-16 11:48:14+00:00
  edited: true
  hidden: false
  id: 64dcc58e2cbce093193fb9a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634332912347-noauth.png?w=200&h=200&f=face
      fullname: Cyrile Delestre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cyrile
      type: user
    createdAt: '2023-09-15T04:53:28.000Z'
    data:
      status: closed
    id: 6503e348d4911441595b88fa
    type: status-change
  author: Cyrile
  created_at: 2023-09-15 03:53:28+00:00
  id: 6503e348d4911441595b88fa
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2023-09-15T20:04:07.000Z'
    data:
      edited: false
      editors:
      - julien-c
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8997592329978943
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: "<p>did you find the information in the end <span data-props=\"{&quot;user&quot;:&quot;Cyrile&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Cyrile\"\
          >@<span class=\"underline\">Cyrile</span></a></span>\n\n\t</span></span>?</p>\n"
        raw: did you find the information in the end @Cyrile?
        updatedAt: '2023-09-15T20:04:07.879Z'
      numEdits: 0
      reactions: []
    id: 6504b8b79db6e2495c5d5241
    type: comment
  author: julien-c
  content: did you find the information in the end @Cyrile?
  created_at: 2023-09-15 19:04:07+00:00
  edited: false
  hidden: false
  id: 6504b8b79db6e2495c5d5241
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634332912347-noauth.png?w=200&h=200&f=face
      fullname: Cyrile Delestre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cyrile
      type: user
    createdAt: '2023-09-17T08:22:12.000Z'
    data:
      edited: false
      editors:
      - Cyrile
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7337579131126404
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1634332912347-noauth.png?w=200&h=200&f=face
          fullname: Cyrile Delestre
          isHf: false
          isPro: false
          name: Cyrile
          type: user
        html: "<p>Hello Julien-c, thank you for your interest. I was referring to\
          \ the part: \"padding_side\":\"left\" set as the default in the tokenizer_config.json\
          \ file. I was just cautioning about this choice and the incompatibility\
          \ of this strategy with the implementation of the BloomForSequenceClassification\
          \ class, which seems to be programmed for a padding_side strategy of right.\
          \ The solution is to carefully consider placing the padding on the correct\
          \ side for classification. However, I'm concerned that this choice might\
          \ mislead less experienced individuals in this type of modeling or with\
          \ the Transformers library...</p>\n<pre><code>class BloomForSequenceClassification(BloomPreTrainedModel):\n\
          \    [...]\n    def forward(...):\n        hidden_states = transformer_outputs[0]\n\
          \        logits = self.score(hidden_states)\n\n        if input_ids is not\
          \ None:\n            batch_size = input_ids.shape[0]\n        else:\n  \
          \          batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id\
          \ is None and batch_size != 1:\n            raise ValueError(\"Cannot handle\
          \ batch sizes &gt; 1 if no padding token is defined.\")\n        if self.config.pad_token_id\
          \ is None:\n            sequence_lengths = -1\n        else:\n         \
          \   if input_ids is not None:\n                sequence_lengths = (torch.ne(input_ids,\
          \ self.config.pad_token_id).sum(-1) - 1).to(logits.device) # &lt;- this\
          \ is ok for padding_side = 'right' strategy ?\n            else:\n     \
          \           sequence_lengths = -1\n                logger.warning(\n   \
          \                 f\"{self.__class__.__name__} will not detect padding tokens\
          \ in `inputs_embeds`. Results may be \"\n                    \"unexpected\
          \ if using padding tokens in conjunction with `inputs_embeds.`\"\n     \
          \           )\n\n        pooled_logits = logits[torch.arange(batch_size,\
          \ device=logits.device), sequence_lengths]\n        [...]\n</code></pre>\n\
          <p>I have one last question: is there a reason why you chose to use the\
          \ padding ID to position the last token rather than the sum on the attention\
          \ mask? </p>\n"
        raw: "Hello Julien-c, thank you for your interest. I was referring to the\
          \ part: \"padding_side\":\"left\" set as the default in the tokenizer_config.json\
          \ file. I was just cautioning about this choice and the incompatibility\
          \ of this strategy with the implementation of the BloomForSequenceClassification\
          \ class, which seems to be programmed for a padding_side strategy of right.\
          \ The solution is to carefully consider placing the padding on the correct\
          \ side for classification. However, I'm concerned that this choice might\
          \ mislead less experienced individuals in this type of modeling or with\
          \ the Transformers library...\n```\nclass BloomForSequenceClassification(BloomPreTrainedModel):\n\
          \    [...]\n    def forward(...):\n        hidden_states = transformer_outputs[0]\n\
          \        logits = self.score(hidden_states)\n\n        if input_ids is not\
          \ None:\n            batch_size = input_ids.shape[0]\n        else:\n  \
          \          batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id\
          \ is None and batch_size != 1:\n            raise ValueError(\"Cannot handle\
          \ batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id\
          \ is None:\n            sequence_lengths = -1\n        else:\n         \
          \   if input_ids is not None:\n                sequence_lengths = (torch.ne(input_ids,\
          \ self.config.pad_token_id).sum(-1) - 1).to(logits.device) # <- this is\
          \ ok for padding_side = 'right' strategy ?\n            else:\n        \
          \        sequence_lengths = -1\n                logger.warning(\n      \
          \              f\"{self.__class__.__name__} will not detect padding tokens\
          \ in `inputs_embeds`. Results may be \"\n                    \"unexpected\
          \ if using padding tokens in conjunction with `inputs_embeds.`\"\n     \
          \           )\n\n        pooled_logits = logits[torch.arange(batch_size,\
          \ device=logits.device), sequence_lengths]\n        [...]\n```\nI have one\
          \ last question: is there a reason why you chose to use the padding ID to\
          \ position the last token rather than the sum on the attention mask? "
        updatedAt: '2023-09-17T08:22:12.831Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ingo-m
    id: 6506b734dacc94cd6c072246
    type: comment
  author: Cyrile
  content: "Hello Julien-c, thank you for your interest. I was referring to the part:\
    \ \"padding_side\":\"left\" set as the default in the tokenizer_config.json file.\
    \ I was just cautioning about this choice and the incompatibility of this strategy\
    \ with the implementation of the BloomForSequenceClassification class, which seems\
    \ to be programmed for a padding_side strategy of right. The solution is to carefully\
    \ consider placing the padding on the correct side for classification. However,\
    \ I'm concerned that this choice might mislead less experienced individuals in\
    \ this type of modeling or with the Transformers library...\n```\nclass BloomForSequenceClassification(BloomPreTrainedModel):\n\
    \    [...]\n    def forward(...):\n        hidden_states = transformer_outputs[0]\n\
    \        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n\
    \            batch_size = input_ids.shape[0]\n        else:\n            batch_size\
    \ = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and\
    \ batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes >\
    \ 1 if no padding token is defined.\")\n        if self.config.pad_token_id is\
    \ None:\n            sequence_lengths = -1\n        else:\n            if input_ids\
    \ is not None:\n                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1)\
    \ - 1).to(logits.device) # <- this is ok for padding_side = 'right' strategy ?\n\
    \            else:\n                sequence_lengths = -1\n                logger.warning(\n\
    \                    f\"{self.__class__.__name__} will not detect padding tokens\
    \ in `inputs_embeds`. Results may be \"\n                    \"unexpected if using\
    \ padding tokens in conjunction with `inputs_embeds.`\"\n                )\n\n\
    \        pooled_logits = logits[torch.arange(batch_size, device=logits.device),\
    \ sequence_lengths]\n        [...]\n```\nI have one last question: is there a\
    \ reason why you chose to use the padding ID to position the last token rather\
    \ than the sum on the attention mask? "
  created_at: 2023-09-17 07:22:12+00:00
  edited: false
  hidden: false
  id: 6506b734dacc94cd6c072246
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662672956916-noauth.jpeg?w=200&h=200&f=face
      fullname: ingo-m
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ingo-m
      type: user
    createdAt: '2023-11-04T13:17:32.000Z'
    data:
      edited: true
      editors:
      - ingo-m
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8027616739273071
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662672956916-noauth.jpeg?w=200&h=200&f=face
          fullname: ingo-m
          isHf: false
          isPro: false
          name: ingo-m
          type: user
        html: '<p>Does this mean that when finetuning <code>bigscience/bloom-560m</code>,
          it''s expected to use <code>padding_side = "right"</code>, but when finetuning
          <code>bigscience/bloomz-560m</code>, <code>padding_side = "left"</code>
          should be used?</p>

          <p>I''m seeing some inconsistency between how <code>bloom-560m</code> and
          <code>bloomz-560m</code> converge during finetuning and I suspect this might
          be the cause.</p>

          '
        raw: 'Does this mean that when finetuning `bigscience/bloom-560m`, it''s expected
          to use `padding_side = "right"`, but when finetuning `bigscience/bloomz-560m`,
          `padding_side = "left"` should be used?


          I''m seeing some inconsistency between how `bloom-560m` and `bloomz-560m`
          converge during finetuning and I suspect this might be the cause.'
        updatedAt: '2023-11-04T13:38:31.067Z'
      numEdits: 1
      reactions: []
    id: 6546446c4d1931dc93d4e37d
    type: comment
  author: ingo-m
  content: 'Does this mean that when finetuning `bigscience/bloom-560m`, it''s expected
    to use `padding_side = "right"`, but when finetuning `bigscience/bloomz-560m`,
    `padding_side = "left"` should be used?


    I''m seeing some inconsistency between how `bloom-560m` and `bloomz-560m` converge
    during finetuning and I suspect this might be the cause.'
  created_at: 2023-11-04 12:17:32+00:00
  edited: true
  hidden: false
  id: 6546446c4d1931dc93d4e37d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 48
repo_id: bigscience/bloomz
repo_type: model
status: closed
target_branch: null
title: Default padding_side
