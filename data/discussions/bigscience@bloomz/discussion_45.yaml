!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Arkea
conflicting_files: null
created_at: 2023-07-26 07:03:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
      fullname: Cyrile Delestre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Arkea
      type: user
    createdAt: '2023-07-26T08:03:55.000Z'
    data:
      edited: false
      editors:
      - Arkea
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9642503261566162
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
          fullname: Cyrile Delestre
          isHf: false
          isPro: false
          name: Arkea
          type: user
        html: '<p>Hello, I have a technical question. Why is version 176b of the model
          in bfloat16 while the other models (3b, 7b) are in float16? Does anyone
          know the reason for such a choice? Similarly, is the bfloat16 type relevant
          because it appears to maintain the same dynamic range as float32 but at
          the expense of precision? In general, the values involved are small, so
          float16, which has better precision, seems more appropriate, right?</p>

          '
        raw: Hello, I have a technical question. Why is version 176b of the model
          in bfloat16 while the other models (3b, 7b) are in float16? Does anyone
          know the reason for such a choice? Similarly, is the bfloat16 type relevant
          because it appears to maintain the same dynamic range as float32 but at
          the expense of precision? In general, the values involved are small, so
          float16, which has better precision, seems more appropriate, right?
        updatedAt: '2023-07-26T08:03:55.381Z'
      numEdits: 0
      reactions: []
    id: 64c0d36be56520a63d3ebe06
    type: comment
  author: Arkea
  content: Hello, I have a technical question. Why is version 176b of the model in
    bfloat16 while the other models (3b, 7b) are in float16? Does anyone know the
    reason for such a choice? Similarly, is the bfloat16 type relevant because it
    appears to maintain the same dynamic range as float32 but at the expense of precision?
    In general, the values involved are small, so float16, which has better precision,
    seems more appropriate, right?
  created_at: 2023-07-26 07:03:55+00:00
  edited: false
  hidden: false
  id: 64c0d36be56520a63d3ebe06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-07-26T08:56:47.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8930913209915161
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p><code>bfloat16 / bf16</code> is better than <code>float16 / fp16</code>
          in terms of performance.<br>When pretraining the BLOOM models we only add
          V100s to train the smaller models. V100s do not support bf16, so we trained
          them with fp16. We used the same types for BLOOMZ models.<br>When doing
          inference with BLOOMZ, I would recommend doing it in bf16 or fp32. You can
          also load the model in fp16 but it will be worse than bf16.<br>For the smaller
          models, you can use fp16 or fp32.</p>

          '
        raw: '`bfloat16 / bf16` is better than `float16 / fp16` in terms of performance.

          When pretraining the BLOOM models we only add V100s to train the smaller
          models. V100s do not support bf16, so we trained them with fp16. We used
          the same types for BLOOMZ models.

          When doing inference with BLOOMZ, I would recommend doing it in bf16 or
          fp32. You can also load the model in fp16 but it will be worse than bf16.

          For the smaller models, you can use fp16 or fp32.

          '
        updatedAt: '2023-07-26T08:56:47.828Z'
      numEdits: 0
      reactions: []
    id: 64c0dfcf0cfdb492ce92a571
    type: comment
  author: Muennighoff
  content: '`bfloat16 / bf16` is better than `float16 / fp16` in terms of performance.

    When pretraining the BLOOM models we only add V100s to train the smaller models.
    V100s do not support bf16, so we trained them with fp16. We used the same types
    for BLOOMZ models.

    When doing inference with BLOOMZ, I would recommend doing it in bf16 or fp32.
    You can also load the model in fp16 but it will be worse than bf16.

    For the smaller models, you can use fp16 or fp32.

    '
  created_at: 2023-07-26 07:56:47+00:00
  edited: false
  hidden: false
  id: 64c0dfcf0cfdb492ce92a571
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
      fullname: Cyrile Delestre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Arkea
      type: user
    createdAt: '2023-07-26T09:20:56.000Z'
    data:
      edited: false
      editors:
      - Arkea
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9507167339324951
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
          fullname: Cyrile Delestre
          isHf: false
          isPro: false
          name: Arkea
          type: user
        html: '<p>Thank you for your response. I now understand better why certain
          models use float16 and version 176b uses bfloat16. When you mention that
          bfloat16 is more performant, are you referring to computational performance
          or accuracy? Because for models with normalization layers, I find it counterintuitive
          that bfloat16 would be more performant in terms of accuracy, as intuitively,
          I would have thought that precision would be more sought after than dynamic
          range.</p>

          '
        raw: Thank you for your response. I now understand better why certain models
          use float16 and version 176b uses bfloat16. When you mention that bfloat16
          is more performant, are you referring to computational performance or accuracy?
          Because for models with normalization layers, I find it counterintuitive
          that bfloat16 would be more performant in terms of accuracy, as intuitively,
          I would have thought that precision would be more sought after than dynamic
          range.
        updatedAt: '2023-07-26T09:20:56.717Z'
      numEdits: 0
      reactions: []
    id: 64c0e57862983511b95f2a43
    type: comment
  author: Arkea
  content: Thank you for your response. I now understand better why certain models
    use float16 and version 176b uses bfloat16. When you mention that bfloat16 is
    more performant, are you referring to computational performance or accuracy? Because
    for models with normalization layers, I find it counterintuitive that bfloat16
    would be more performant in terms of accuracy, as intuitively, I would have thought
    that precision would be more sought after than dynamic range.
  created_at: 2023-07-26 08:20:56+00:00
  edited: false
  hidden: false
  id: 64c0e57862983511b95f2a43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
      fullname: Cyrile Delestre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Arkea
      type: user
    createdAt: '2023-07-26T09:39:56.000Z'
    data:
      edited: false
      editors:
      - Arkea
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9746657013893127
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
          fullname: Cyrile Delestre
          isHf: false
          isPro: false
          name: Arkea
          type: user
        html: '<p>sorry I was wrong it is indeed the bf16 which has better precision.
          So to fine tune the 7b model, for example, would you recommend switching
          to bf16 or preserving the original f16?</p>

          '
        raw: sorry I was wrong it is indeed the bf16 which has better precision. So
          to fine tune the 7b model, for example, would you recommend switching to
          bf16 or preserving the original f16?
        updatedAt: '2023-07-26T09:39:56.428Z'
      numEdits: 0
      reactions: []
    id: 64c0e9ecabe5f855b7a697ea
    type: comment
  author: Arkea
  content: sorry I was wrong it is indeed the bf16 which has better precision. So
    to fine tune the 7b model, for example, would you recommend switching to bf16
    or preserving the original f16?
  created_at: 2023-07-26 08:39:56+00:00
  edited: false
  hidden: false
  id: 64c0e9ecabe5f855b7a697ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-07-26T10:23:12.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9874758124351501
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>Hmm I think both work for fine-tuning. I''m not sure which one is
          better. I would guess that it depends on the number of additional training
          steps.<br>Maybe for &lt;&lt;1000 steps it''s better to stay in fp16 &amp;
          for &gt;&gt;1000 steps it''s worth it switching to bf16.</p>

          '
        raw: 'Hmm I think both work for fine-tuning. I''m not sure which one is better.
          I would guess that it depends on the number of additional training steps.

          Maybe for <<1000 steps it''s better to stay in fp16 & for >>1000 steps it''s
          worth it switching to bf16.'
        updatedAt: '2023-07-26T10:23:12.806Z'
      numEdits: 0
      reactions: []
    id: 64c0f410f070a750cdc55d6d
    type: comment
  author: Muennighoff
  content: 'Hmm I think both work for fine-tuning. I''m not sure which one is better.
    I would guess that it depends on the number of additional training steps.

    Maybe for <<1000 steps it''s better to stay in fp16 & for >>1000 steps it''s worth
    it switching to bf16.'
  created_at: 2023-07-26 09:23:12+00:00
  edited: false
  hidden: false
  id: 64c0f410f070a750cdc55d6d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
      fullname: Cyrile Delestre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Arkea
      type: user
    createdAt: '2023-07-26T10:50:45.000Z'
    data:
      edited: false
      editors:
      - Arkea
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6606848239898682
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
          fullname: Cyrile Delestre
          isHf: false
          isPro: false
          name: Arkea
          type: user
        html: '<p>Thank you for all this information!</p>

          '
        raw: Thank you for all this information!
        updatedAt: '2023-07-26T10:50:45.499Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64c0fa8592ae08de17573871
    id: 64c0fa8592ae08de1757386e
    type: comment
  author: Arkea
  content: Thank you for all this information!
  created_at: 2023-07-26 09:50:45+00:00
  edited: false
  hidden: false
  id: 64c0fa8592ae08de1757386e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
      fullname: Cyrile Delestre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Arkea
      type: user
    createdAt: '2023-07-26T10:50:45.000Z'
    data:
      status: closed
    id: 64c0fa8592ae08de17573871
    type: status-change
  author: Arkea
  created_at: 2023-07-26 09:50:45+00:00
  id: 64c0fa8592ae08de17573871
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
      fullname: Cyrile Delestre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Arkea
      type: user
    createdAt: '2023-07-26T16:09:39.000Z'
    data:
      edited: false
      editors:
      - Arkea
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9569394588470459
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
          fullname: Cyrile Delestre
          isHf: false
          isPro: false
          name: Arkea
          type: user
        html: '<p>Well, in the end, I tried to fine-tune Bloomz 3b for a task using
          bfloat16, but it seems that the degradation from the conversion makes the
          learning process very hard...</p>

          '
        raw: Well, in the end, I tried to fine-tune Bloomz 3b for a task using bfloat16,
          but it seems that the degradation from the conversion makes the learning
          process very hard...
        updatedAt: '2023-07-26T16:09:39.254Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64c1454384887aa2b43beaad
    id: 64c1454384887aa2b43beaac
    type: comment
  author: Arkea
  content: Well, in the end, I tried to fine-tune Bloomz 3b for a task using bfloat16,
    but it seems that the degradation from the conversion makes the learning process
    very hard...
  created_at: 2023-07-26 15:09:39+00:00
  edited: false
  hidden: false
  id: 64c1454384887aa2b43beaac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
      fullname: Cyrile Delestre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Arkea
      type: user
    createdAt: '2023-07-26T16:09:39.000Z'
    data:
      status: open
    id: 64c1454384887aa2b43beaad
    type: status-change
  author: Arkea
  created_at: 2023-07-26 15:09:39+00:00
  id: 64c1454384887aa2b43beaad
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-07-26T16:48:15.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.936011552810669
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>Is it better with fp16?</p>

          '
        raw: Is it better with fp16?
        updatedAt: '2023-07-26T16:48:15.738Z'
      numEdits: 0
      reactions: []
    id: 64c14e4ff111d3da2c4d173f
    type: comment
  author: Muennighoff
  content: Is it better with fp16?
  created_at: 2023-07-26 15:48:15+00:00
  edited: false
  hidden: false
  id: 64c14e4ff111d3da2c4d173f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
      fullname: Cyrile Delestre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Arkea
      type: user
    createdAt: '2023-07-26T17:55:16.000Z'
    data:
      edited: false
      editors:
      - Arkea
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9778885245323181
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
          fullname: Cyrile Delestre
          isHf: false
          isPro: false
          name: Arkea
          type: user
        html: '<p>Yes much better. Strangely, by loading the model in float 32 and
          using AMP in bf16, it goes much better (which I don''t really understand
          because the weights of the model must also go to bf16 and deteriorate the
          model just as much...). Otherwise to preserve the dynamics of the gradients
          I thought loading the model in float 32 and use AMP in float 16 (which seems
          more logical to me) and should have good performance equivalent to bf16
          (I think). I don''t really know... If the AMP bf16 learning continues as
          well why not keep it.</p>

          '
        raw: Yes much better. Strangely, by loading the model in float 32 and using
          AMP in bf16, it goes much better (which I don't really understand because
          the weights of the model must also go to bf16 and deteriorate the model
          just as much...). Otherwise to preserve the dynamics of the gradients I
          thought loading the model in float 32 and use AMP in float 16 (which seems
          more logical to me) and should have good performance equivalent to bf16
          (I think). I don't really know... If the AMP bf16 learning continues as
          well why not keep it.
        updatedAt: '2023-07-26T17:55:16.049Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Muennighoff
    id: 64c15e04cac5f9ba52be5eab
    type: comment
  author: Arkea
  content: Yes much better. Strangely, by loading the model in float 32 and using
    AMP in bf16, it goes much better (which I don't really understand because the
    weights of the model must also go to bf16 and deteriorate the model just as much...).
    Otherwise to preserve the dynamics of the gradients I thought loading the model
    in float 32 and use AMP in float 16 (which seems more logical to me) and should
    have good performance equivalent to bf16 (I think). I don't really know... If
    the AMP bf16 learning continues as well why not keep it.
  created_at: 2023-07-26 16:55:16+00:00
  edited: false
  hidden: false
  id: 64c15e04cac5f9ba52be5eab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/8698c047f9913d4ddaf185eb7920d9b3.svg
      fullname: Cyrile Delestre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Arkea
      type: user
    createdAt: '2023-07-26T20:08:08.000Z'
    data:
      status: closed
    id: 64c17d282bac49787a8b5350
    type: status-change
  author: Arkea
  created_at: 2023-07-26 19:08:08+00:00
  id: 64c17d282bac49787a8b5350
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 45
repo_id: bigscience/bloomz
repo_type: model
status: closed
target_branch: null
title: bfloat 16 vs float 16
