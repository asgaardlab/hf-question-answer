!!python/object:huggingface_hub.community.DiscussionWithDetails
author: i-am-neo
conflicting_files: null
created_at: 2022-12-09 17:34:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd3301e6209288bfdb82ee12e37fba05.svg
      fullname: neo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: i-am-neo
      type: user
    createdAt: '2022-12-09T17:34:21.000Z'
    data:
      edited: false
      editors:
      - i-am-neo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd3301e6209288bfdb82ee12e37fba05.svg
          fullname: neo
          isHf: false
          isPro: false
          name: i-am-neo
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \  I was trying BloomZ on a notebook but can't seem to have it generate\
          \ a list.  Am I missing something?</p>\n<pre><code class=\"language-prompt\"\
          >Suggest at least five related search terms to \"M\u1EA1ng neural nh\xE2\
          n t\u1EA1o\".\n\"\"\"\ninputs = tokenizer.encode(prompt, return_tensors=\"\
          pt\")\noutputs = model.generate(inputs, max_length = 1000)\n\nprint(len(outputs))\n\
          print(tokenizer.decode(outputs[0]))\n</code></pre>\n<p>Output:</p>\n<pre><code>1\n\
          \nSuggest at least five related search terms to \"M\u1EA1ng neural nh\xE2\
          n t\u1EA1o\".\nNeural network&lt;/s&gt;\n</code></pre>\n<p>I tried both<br>checkpoint\
          \ = \"bigscience/bloomz-560m\"<br>checkpoint = \"bigscience/bloomz-1b1\"\
          </p>\n"
        raw: "@Muennighoff  I was trying BloomZ on a notebook but can't seem to have\
          \ it generate a list.  Am I missing something?\r\n\r\n```prompt = \"\"\"\
          \r\nSuggest at least five related search terms to \"M\u1EA1ng neural nh\xE2\
          n t\u1EA1o\".\r\n\"\"\"\r\ninputs = tokenizer.encode(prompt, return_tensors=\"\
          pt\")\r\noutputs = model.generate(inputs, max_length = 1000)\r\n\r\nprint(len(outputs))\r\
          \nprint(tokenizer.decode(outputs[0]))\r\n```\r\n\r\nOutput:\r\n```\r\n1\r\
          \n\r\nSuggest at least five related search terms to \"M\u1EA1ng neural nh\xE2\
          n t\u1EA1o\".\r\nNeural network</s>\r\n```\r\n\r\nI tried both \r\ncheckpoint\
          \ = \"bigscience/bloomz-560m\"\r\ncheckpoint = \"bigscience/bloomz-1b1\""
        updatedAt: '2022-12-09T17:34:21.147Z'
      numEdits: 0
      reactions: []
    id: 6393719d1ac8e632caa6c2fb
    type: comment
  author: i-am-neo
  content: "@Muennighoff  I was trying BloomZ on a notebook but can't seem to have\
    \ it generate a list.  Am I missing something?\r\n\r\n```prompt = \"\"\"\r\nSuggest\
    \ at least five related search terms to \"M\u1EA1ng neural nh\xE2n t\u1EA1o\"\
    .\r\n\"\"\"\r\ninputs = tokenizer.encode(prompt, return_tensors=\"pt\")\r\noutputs\
    \ = model.generate(inputs, max_length = 1000)\r\n\r\nprint(len(outputs))\r\nprint(tokenizer.decode(outputs[0]))\r\
    \n```\r\n\r\nOutput:\r\n```\r\n1\r\n\r\nSuggest at least five related search terms\
    \ to \"M\u1EA1ng neural nh\xE2n t\u1EA1o\".\r\nNeural network</s>\r\n```\r\n\r\
    \nI tried both \r\ncheckpoint = \"bigscience/bloomz-560m\"\r\ncheckpoint = \"\
    bigscience/bloomz-1b1\""
  created_at: 2022-12-09 17:34:21+00:00
  edited: false
  hidden: false
  id: 6393719d1ac8e632caa6c2fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-12-09T18:38:18.000Z'
    data:
      edited: true
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<p>Some ideas:</p>\n<ul>\n<li><code>outputs = model.generate(inputs,\
          \ min_length=50, max_length = 1000)</code> - force a minimum length so the\
          \ model does not generate the stop token (<code>&lt;/s&gt;</code>) - This\
          \ should 100% work</li>\n<li>You can try a different prompt, e.g. <code>Suggest\
          \ at least five related search terms to \"M\u1EA1ng neural nh\xE2n t\u1EA1\
          o\" as a comma separated list.</code></li>\n<li>You can try a larger model,\
          \ which should be better</li>\n</ul>\n"
        raw: "Some ideas:\n-  `outputs = model.generate(inputs, min_length=50, max_length\
          \ = 1000)` - force a minimum length so the model does not generate the stop\
          \ token (`</s>`) - This should 100% work\n- You can try a different prompt,\
          \ e.g. `Suggest at least five related search terms to \"M\u1EA1ng neural\
          \ nh\xE2n t\u1EA1o\" as a comma separated list.`\n- You can try a larger\
          \ model, which should be better"
        updatedAt: '2022-12-09T20:40:12.265Z'
      numEdits: 1
      reactions: []
    id: 6393809adae17d49f9c2028d
    type: comment
  author: Muennighoff
  content: "Some ideas:\n-  `outputs = model.generate(inputs, min_length=50, max_length\
    \ = 1000)` - force a minimum length so the model does not generate the stop token\
    \ (`</s>`) - This should 100% work\n- You can try a different prompt, e.g. `Suggest\
    \ at least five related search terms to \"M\u1EA1ng neural nh\xE2n t\u1EA1o\"\
    \ as a comma separated list.`\n- You can try a larger model, which should be better"
  created_at: 2022-12-09 18:38:18+00:00
  edited: true
  hidden: false
  id: 6393809adae17d49f9c2028d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd3301e6209288bfdb82ee12e37fba05.svg
      fullname: neo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: i-am-neo
      type: user
    createdAt: '2022-12-09T20:39:03.000Z'
    data:
      edited: false
      editors:
      - i-am-neo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd3301e6209288bfdb82ee12e37fba05.svg
          fullname: neo
          isHf: false
          isPro: false
          name: i-am-neo
          type: user
        html: "<p>Thanks, I tried your first suggestion which produced a list.  </p>\n\
          <p>Separately, I'd like the model to produce multiple answers to multiple\
          \ questions from the same context. How best to do this without running running\
          \ the same context text through the model multiple times? </p>\n<p>Thus\
          \ far, I've had success with single Q&amp;A mimicking the P3 format:</p>\n\
          <pre><code class=\"language-question\">prompt = f\"\"\"\nGiven the following\
          \ passage \n\"{context}.\",\n answer the following question. Note that the\
          \ answer is present within the text. \n Question: {question}\n\"\"\"\n</code></pre>\n"
        raw: "Thanks, I tried your first suggestion which produced a list.  \n\nSeparately,\
          \ I'd like the model to produce multiple answers to multiple questions from\
          \ the same context. How best to do this without running running the same\
          \ context text through the model multiple times? \n\nThus far, I've had\
          \ success with single Q&A mimicking the P3 format:\n```question = \"<single_question>?\"\
          \nprompt = f\"\"\"\nGiven the following passage \n\"{context}.\",\n answer\
          \ the following question. Note that the answer is present within the text.\
          \ \n Question: {question}\n\"\"\"\n```"
        updatedAt: '2022-12-09T20:39:03.232Z'
      numEdits: 0
      reactions: []
    id: 63939ce7091ca589156c6ff4
    type: comment
  author: i-am-neo
  content: "Thanks, I tried your first suggestion which produced a list.  \n\nSeparately,\
    \ I'd like the model to produce multiple answers to multiple questions from the\
    \ same context. How best to do this without running running the same context text\
    \ through the model multiple times? \n\nThus far, I've had success with single\
    \ Q&A mimicking the P3 format:\n```question = \"<single_question>?\"\nprompt =\
    \ f\"\"\"\nGiven the following passage \n\"{context}.\",\n answer the following\
    \ question. Note that the answer is present within the text. \n Question: {question}\n\
    \"\"\"\n```"
  created_at: 2022-12-09 20:39:03+00:00
  edited: false
  hidden: false
  id: 63939ce7091ca589156c6ff4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-12-09T20:50:59.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>Technically, it should be feasible to reuse the cached hidden states
          from the context like asked <a rel="nofollow" href="https://discuss.huggingface.co/t/what-is-the-purpose-of-use-cache-in-decoder/958/4">here</a>,
          but the generate function in transformers doesn''t expose that functionality,
          so would require some changes to the code.</p>

          <p>Else I''d try sth like the prompt below either zero-shot or one-shot
          (or more):</p>

          <p>Zero-shot:</p>

          <pre><code>{context_a}


          Given the above passage, please answer the below questions (the answer is
          in the text):

          Questions:

          1. XX?

          2. YY?

          3. ZZ?

          Answers:

          </code></pre>

          <p>One-shot:</p>

          <pre><code>{context_a}


          Given the above passage, please answer the below questions (the answer is
          in the text):

          Questions:

          1. XX?

          2. YY?

          3. ZZ?

          Answers:

          1. AA.

          2. BB.

          3. CC.


          {context_b}


          Given the above passage, please answer the below questions (the answer is
          in the text):

          Questions:

          1. WW?

          2. VV?

          3. UU?

          Answers:

          1.

          </code></pre>

          '
        raw: 'Technically, it should be feasible to reuse the cached hidden states
          from the context like asked [here](https://discuss.huggingface.co/t/what-is-the-purpose-of-use-cache-in-decoder/958/4),
          but the generate function in transformers doesn''t expose that functionality,
          so would require some changes to the code.


          Else I''d try sth like the prompt below either zero-shot or one-shot (or
          more):


          Zero-shot:

          ```

          {context_a}


          Given the above passage, please answer the below questions (the answer is
          in the text):

          Questions:

          1. XX?

          2. YY?

          3. ZZ?

          Answers:

          ```


          One-shot:

          ```

          {context_a}


          Given the above passage, please answer the below questions (the answer is
          in the text):

          Questions:

          1. XX?

          2. YY?

          3. ZZ?

          Answers:

          1. AA.

          2. BB.

          3. CC.


          {context_b}


          Given the above passage, please answer the below questions (the answer is
          in the text):

          Questions:

          1. WW?

          2. VV?

          3. UU?

          Answers:

          1.

          ```'
        updatedAt: '2022-12-09T20:50:59.280Z'
      numEdits: 0
      reactions: []
    id: 63939fb3f3ab3dee36499583
    type: comment
  author: Muennighoff
  content: 'Technically, it should be feasible to reuse the cached hidden states from
    the context like asked [here](https://discuss.huggingface.co/t/what-is-the-purpose-of-use-cache-in-decoder/958/4),
    but the generate function in transformers doesn''t expose that functionality,
    so would require some changes to the code.


    Else I''d try sth like the prompt below either zero-shot or one-shot (or more):


    Zero-shot:

    ```

    {context_a}


    Given the above passage, please answer the below questions (the answer is in the
    text):

    Questions:

    1. XX?

    2. YY?

    3. ZZ?

    Answers:

    ```


    One-shot:

    ```

    {context_a}


    Given the above passage, please answer the below questions (the answer is in the
    text):

    Questions:

    1. XX?

    2. YY?

    3. ZZ?

    Answers:

    1. AA.

    2. BB.

    3. CC.


    {context_b}


    Given the above passage, please answer the below questions (the answer is in the
    text):

    Questions:

    1. WW?

    2. VV?

    3. UU?

    Answers:

    1.

    ```'
  created_at: 2022-12-09 20:50:59+00:00
  edited: false
  hidden: false
  id: 63939fb3f3ab3dee36499583
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd3301e6209288bfdb82ee12e37fba05.svg
      fullname: neo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: i-am-neo
      type: user
    createdAt: '2022-12-09T22:12:50.000Z'
    data:
      edited: false
      editors:
      - i-am-neo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd3301e6209288bfdb82ee12e37fba05.svg
          fullname: neo
          isHf: false
          isPro: false
          name: i-am-neo
          type: user
        html: "<p>Thanks for your suggestions &amp; sorry, I missed the previous q\
          \ on cached hidden states.  I tried zero-shot and 1-shot(b) - they don't\
          \ produce the desired output. Other suggestions?</p>\n<p>Output:</p>\n<pre><code>Given\
          \ the following passage \n\"\nI just caught this on Showtime...ewwwwwww,\
          \ not even fun in a bad movie kind of way. One of the lamest monster flicks\
          \ I've ever seen. Plus the TV reporter in the movie was that annoying Jerri\
          \ from a past season of Survivor. The only amusing thing was that the \"\
          secret base\" was the house from Fantasy Island (and a million other movies\
          \ and TV shows; the place is located in the L.A. area). I fully expected\
          \ Mr Roarke and Tattoo to come out and greet the visitors. If Tattoo had\
          \ gotten eaten by the snake, I might have given this movie a 2, but oh well.\
          \ Watching people stand there and scream for five minutes while the Komodo\
          \ or the cobra loomed over them instead of making a run for it was pretty\
          \ funny, especially because you could really tell that they were just screaming\
          \ at an empty spot where the computer animators would later paint in the\
          \ monster. I nearly fell out of my chair, though, when in a flashback scene\
          \ they brought in either the cobra or the komodo - then normal size - in\
          \ some indestructible solid steel container with some air holes drilled\
          \ into it. Wouldn't a wire cage have sufficed? LOL! Guess they couldn't\
          \ afford to rent a real komodo and cobra. I have to remember I rent Showtime\
          \ for their series and not their movies.\n\",\nanswer the following questions.\
          \ Note that the answers are present within the text. \nQuestions:\n1. What\
          \ are we talking about?\n2. Who are we talking about?\n3. What is the sentiment\
          \ expressed in this text?\n\nAnswers:\n1.\npositive review negative review\
          \ negative review negative review negative review negative review negative\
          \ review negative review negative review negative review negative review\
          \ negative review negative review negative review negative review negative\
          \ review negative review negative review \n&lt;repeats...&gt;\n</code></pre>\n"
        raw: "Thanks for your suggestions & sorry, I missed the previous q on cached\
          \ hidden states.  I tried zero-shot and 1-shot(b) - they don't produce the\
          \ desired output. Other suggestions?\n\nOutput:\n```\nGiven the following\
          \ passage \n\"\nI just caught this on Showtime...ewwwwwww, not even fun\
          \ in a bad movie kind of way. One of the lamest monster flicks I've ever\
          \ seen. Plus the TV reporter in the movie was that annoying Jerri from a\
          \ past season of Survivor. The only amusing thing was that the \"secret\
          \ base\" was the house from Fantasy Island (and a million other movies and\
          \ TV shows; the place is located in the L.A. area). I fully expected Mr\
          \ Roarke and Tattoo to come out and greet the visitors. If Tattoo had gotten\
          \ eaten by the snake, I might have given this movie a 2, but oh well. Watching\
          \ people stand there and scream for five minutes while the Komodo or the\
          \ cobra loomed over them instead of making a run for it was pretty funny,\
          \ especially because you could really tell that they were just screaming\
          \ at an empty spot where the computer animators would later paint in the\
          \ monster. I nearly fell out of my chair, though, when in a flashback scene\
          \ they brought in either the cobra or the komodo - then normal size - in\
          \ some indestructible solid steel container with some air holes drilled\
          \ into it. Wouldn't a wire cage have sufficed? LOL! Guess they couldn't\
          \ afford to rent a real komodo and cobra. I have to remember I rent Showtime\
          \ for their series and not their movies.\n\",\nanswer the following questions.\
          \ Note that the answers are present within the text. \nQuestions:\n1. What\
          \ are we talking about?\n2. Who are we talking about?\n3. What is the sentiment\
          \ expressed in this text?\n\nAnswers:\n1.\npositive review negative review\
          \ negative review negative review negative review negative review negative\
          \ review negative review negative review negative review negative review\
          \ negative review negative review negative review negative review negative\
          \ review negative review negative review \n<repeats...>\n```"
        updatedAt: '2022-12-09T22:12:50.181Z'
      numEdits: 0
      reactions: []
    id: 6393b2e284ea7d31ca1e31dc
    type: comment
  author: i-am-neo
  content: "Thanks for your suggestions & sorry, I missed the previous q on cached\
    \ hidden states.  I tried zero-shot and 1-shot(b) - they don't produce the desired\
    \ output. Other suggestions?\n\nOutput:\n```\nGiven the following passage \n\"\
    \nI just caught this on Showtime...ewwwwwww, not even fun in a bad movie kind\
    \ of way. One of the lamest monster flicks I've ever seen. Plus the TV reporter\
    \ in the movie was that annoying Jerri from a past season of Survivor. The only\
    \ amusing thing was that the \"secret base\" was the house from Fantasy Island\
    \ (and a million other movies and TV shows; the place is located in the L.A. area).\
    \ I fully expected Mr Roarke and Tattoo to come out and greet the visitors. If\
    \ Tattoo had gotten eaten by the snake, I might have given this movie a 2, but\
    \ oh well. Watching people stand there and scream for five minutes while the Komodo\
    \ or the cobra loomed over them instead of making a run for it was pretty funny,\
    \ especially because you could really tell that they were just screaming at an\
    \ empty spot where the computer animators would later paint in the monster. I\
    \ nearly fell out of my chair, though, when in a flashback scene they brought\
    \ in either the cobra or the komodo - then normal size - in some indestructible\
    \ solid steel container with some air holes drilled into it. Wouldn't a wire cage\
    \ have sufficed? LOL! Guess they couldn't afford to rent a real komodo and cobra.\
    \ I have to remember I rent Showtime for their series and not their movies.\n\"\
    ,\nanswer the following questions. Note that the answers are present within the\
    \ text. \nQuestions:\n1. What are we talking about?\n2. Who are we talking about?\n\
    3. What is the sentiment expressed in this text?\n\nAnswers:\n1.\npositive review\
    \ negative review negative review negative review negative review negative review\
    \ negative review negative review negative review negative review negative review\
    \ negative review negative review negative review negative review negative review\
    \ negative review negative review \n<repeats...>\n```"
  created_at: 2022-12-09 22:12:50+00:00
  edited: false
  hidden: false
  id: 6393b2e284ea7d31ca1e31dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-12-10T10:01:51.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>Hmm apart from caching which should definitely work, other ideas
          are:</p>

          <ol>

          <li>Switch from greedy to sampling or increase the temparature when generating</li>

          <li>Increase the model size - it should be more likely to work with the
          176b model</li>

          </ol>

          '
        raw: 'Hmm apart from caching which should definitely work, other ideas are:

          1) Switch from greedy to sampling or increase the temparature when generating

          2) Increase the model size - it should be more likely to work with the 176b
          model'
        updatedAt: '2022-12-10T10:01:51.572Z'
      numEdits: 0
      reactions: []
    id: 6394590fb4eb453e29664214
    type: comment
  author: Muennighoff
  content: 'Hmm apart from caching which should definitely work, other ideas are:

    1) Switch from greedy to sampling or increase the temparature when generating

    2) Increase the model size - it should be more likely to work with the 176b model'
  created_at: 2022-12-10 10:01:51+00:00
  edited: false
  hidden: false
  id: 6394590fb4eb453e29664214
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd3301e6209288bfdb82ee12e37fba05.svg
      fullname: neo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: i-am-neo
      type: user
    createdAt: '2022-12-22T03:56:43.000Z'
    data:
      edited: false
      editors:
      - i-am-neo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd3301e6209288bfdb82ee12e37fba05.svg
          fullname: neo
          isHf: false
          isPro: false
          name: i-am-neo
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \ Happy holidays.<br>I tried to get Bloomz to generate the main points in\
          \ a text with extremely limited success, hoping you have suggestions of\
          \ prompts.  Model: bigscience/bloomz-3b<br>Examples of prompts I've tried:</p>\n\
          <p><code>What are the main points in this article? Article: + text</code><br>Result:\
          \ one main point (correctly from the text), then repeats that same point.</p>\n\
          <p><code>TLDR; + text</code><br>Result: one main point (correctly from the\
          \ text), then \"Yes. Yes. Yes\"</p>\n<p><code>Can you write an outline of\
          \ the following article in a few points? Article: + text</code><br>Result:\
          \ 3 points (all invented, not in the provided text)</p>\n"
        raw: '@Muennighoff Happy holidays.

          I tried to get Bloomz to generate the main points in a text with extremely
          limited success, hoping you have suggestions of prompts.  Model: bigscience/bloomz-3b

          Examples of prompts I''ve tried:


          ```What are the main points in this article? Article: + text```

          Result: one main point (correctly from the text), then repeats that same
          point.


          ```TLDR; + text```

          Result: one main point (correctly from the text), then "Yes. Yes. Yes"


          ```Can you write an outline of the following article in a few points? Article:
          + text```

          Result: 3 points (all invented, not in the provided text)'
        updatedAt: '2022-12-22T03:56:43.741Z'
      numEdits: 0
      reactions: []
    id: 63a3d57bd039d7d01600ea3c
    type: comment
  author: i-am-neo
  content: '@Muennighoff Happy holidays.

    I tried to get Bloomz to generate the main points in a text with extremely limited
    success, hoping you have suggestions of prompts.  Model: bigscience/bloomz-3b

    Examples of prompts I''ve tried:


    ```What are the main points in this article? Article: + text```

    Result: one main point (correctly from the text), then repeats that same point.


    ```TLDR; + text```

    Result: one main point (correctly from the text), then "Yes. Yes. Yes"


    ```Can you write an outline of the following article in a few points? Article:
    + text```

    Result: 3 points (all invented, not in the provided text)'
  created_at: 2022-12-22 03:56:43+00:00
  edited: false
  hidden: false
  id: 63a3d57bd039d7d01600ea3c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-12-22T05:25:32.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \ Happy holidays.<br>I tried to get Bloomz to generate the main points in\
          \ a text with extremely limited success, hoping you have suggestions of\
          \ prompts.  Model: bigscience/bloomz-3b<br>Examples of prompts I've tried:</p>\n\
          <p><code>What are the main points in this article? Article: + text</code><br>Result:\
          \ one main point (correctly from the text), then repeats that same point.</p>\n\
          <p><code>TLDR; + text</code><br>Result: one main point (correctly from the\
          \ text), then \"Yes. Yes. Yes\"</p>\n<p><code>Can you write an outline of\
          \ the following article in a few points? Article: + text</code><br>Result:\
          \ 3 points (all invented, not in the provided text)</p>\n</blockquote>\n\
          <p>Yeah prompt engineering can be an art &amp; take some time. I would just\
          \ try a few more prompts and else maybe increase the model size to e.g.\
          \ bloomz-7b1. Some ideas could be <code>text\\n\\nSummarize the above in\
          \ a few bullet points.</code> <code>text\\n\\nWrite an outline of the prior\
          \ article in a three bullet points.</code></p>\n"
        raw: "> @Muennighoff Happy holidays.\n> I tried to get Bloomz to generate\
          \ the main points in a text with extremely limited success, hoping you have\
          \ suggestions of prompts.  Model: bigscience/bloomz-3b\n> Examples of prompts\
          \ I've tried:\n> \n> ```What are the main points in this article? Article:\
          \ + text```\n> Result: one main point (correctly from the text), then repeats\
          \ that same point.\n> \n> ```TLDR; + text```\n> Result: one main point (correctly\
          \ from the text), then \"Yes. Yes. Yes\"\n> \n> ```Can you write an outline\
          \ of the following article in a few points? Article: + text```\n> Result:\
          \ 3 points (all invented, not in the provided text)\n\nYeah prompt engineering\
          \ can be an art & take some time. I would just try a few more prompts and\
          \ else maybe increase the model size to e.g. bloomz-7b1. Some ideas could\
          \ be `text\\n\\nSummarize the above in a few bullet points.` `text\\n\\\
          nWrite an outline of the prior article in a three bullet points.`"
        updatedAt: '2022-12-22T05:25:32.398Z'
      numEdits: 0
      reactions: []
    id: 63a3ea4cd1956595f7981ae7
    type: comment
  author: Muennighoff
  content: "> @Muennighoff Happy holidays.\n> I tried to get Bloomz to generate the\
    \ main points in a text with extremely limited success, hoping you have suggestions\
    \ of prompts.  Model: bigscience/bloomz-3b\n> Examples of prompts I've tried:\n\
    > \n> ```What are the main points in this article? Article: + text```\n> Result:\
    \ one main point (correctly from the text), then repeats that same point.\n> \n\
    > ```TLDR; + text```\n> Result: one main point (correctly from the text), then\
    \ \"Yes. Yes. Yes\"\n> \n> ```Can you write an outline of the following article\
    \ in a few points? Article: + text```\n> Result: 3 points (all invented, not in\
    \ the provided text)\n\nYeah prompt engineering can be an art & take some time.\
    \ I would just try a few more prompts and else maybe increase the model size to\
    \ e.g. bloomz-7b1. Some ideas could be `text\\n\\nSummarize the above in a few\
    \ bullet points.` `text\\n\\nWrite an outline of the prior article in a three\
    \ bullet points.`"
  created_at: 2022-12-22 05:25:32+00:00
  edited: false
  hidden: false
  id: 63a3ea4cd1956595f7981ae7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: bigscience/bloomz
repo_type: model
status: open
target_branch: null
title: generating lists
