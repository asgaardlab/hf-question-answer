!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mishavee
conflicting_files: null
created_at: 2022-11-05 06:55:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-11-05T07:55:54.000Z'
    data:
      edited: false
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>how much gpu memory do I need to run Bloomz on a100 gpus? What about
          fine tune? </p>

          '
        raw: 'how much gpu memory do I need to run Bloomz on a100 gpus? What about
          fine tune? '
        updatedAt: '2022-11-05T07:55:54.452Z'
      numEdits: 0
      reactions: []
    id: 6366170a953957183c08c368
    type: comment
  author: mishavee
  content: 'how much gpu memory do I need to run Bloomz on a100 gpus? What about fine
    tune? '
  created_at: 2022-11-05 06:55:54+00:00
  edited: false
  hidden: false
  id: 6366170a953957183c08c368
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-05T08:00:46.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<blockquote>

          <p>how much gpu memory do I need to run Bloomz on a100 gpus? </p>

          </blockquote>

          <p>Ideally you want 8x A100s with 80GB each and then you can load it directly
          via accelerate + transformers. It may also work with 8x40GB or 4x80GB.</p>

          <blockquote>

          <p>What about fine tune?</p>

          </blockquote>

          <p>We trained it with <code>72x pipeline parallel, 1x tensor parallel, 4x
          data parallel</code> &amp; batch size = 1 on 288 A100 80GB GPUs. You could
          probably half the data parallel, but may have to increase tensor parallel
          to make it fit in that case. So I''d estimate 144 A100 80GB GPUs is the
          minimum.</p>

          '
        raw: ">how much gpu memory do I need to run Bloomz on a100 gpus? \n\nIdeally\
          \ you want 8x A100s with 80GB each and then you can load it directly via\
          \ accelerate + transformers. It may also work with 8x40GB or 4x80GB.\n\n\
          \n> What about fine tune?\n\nWe trained it with `72x pipeline parallel,\
          \ 1x tensor parallel, 4x data parallel` & batch size = 1 on 288 A100 80GB\
          \ GPUs. You could probably half the data parallel, but may have to increase\
          \ tensor parallel to make it fit in that case. So I'd estimate 144 A100\
          \ 80GB GPUs is the minimum."
        updatedAt: '2022-11-05T08:00:46.382Z'
      numEdits: 0
      reactions: []
    id: 6366182ed0ee6e2662b16392
    type: comment
  author: Muennighoff
  content: ">how much gpu memory do I need to run Bloomz on a100 gpus? \n\nIdeally\
    \ you want 8x A100s with 80GB each and then you can load it directly via accelerate\
    \ + transformers. It may also work with 8x40GB or 4x80GB.\n\n\n> What about fine\
    \ tune?\n\nWe trained it with `72x pipeline parallel, 1x tensor parallel, 4x data\
    \ parallel` & batch size = 1 on 288 A100 80GB GPUs. You could probably half the\
    \ data parallel, but may have to increase tensor parallel to make it fit in that\
    \ case. So I'd estimate 144 A100 80GB GPUs is the minimum."
  created_at: 2022-11-05 07:00:46+00:00
  edited: false
  hidden: false
  id: 6366182ed0ee6e2662b16392
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-11-05T08:11:10.000Z'
    data:
      edited: true
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>is there any way to fine tune it with 8 gpus? </p>

          <p>is there any way to batch the input dataset </p>

          <p>like instead of one of the below in training aka fine tuning, put many
          as a single input? </p>

          <p>paraphrase :<br>sentence1:(sentence)<br>sentence2:(sentence)</p>

          '
        raw: "is there any way to fine tune it with 8 gpus? \n\nis there any way to\
          \ batch the input dataset \n\nlike instead of one of the below in training\
          \ aka fine tuning, put many as a single input? \n\nparaphrase :\nsentence1:(sentence)\n\
          sentence2:(sentence)"
        updatedAt: '2022-11-05T08:11:49.898Z'
      numEdits: 1
      reactions: []
    id: 63661a9e6604a4fee84fb1ef
    type: comment
  author: mishavee
  content: "is there any way to fine tune it with 8 gpus? \n\nis there any way to\
    \ batch the input dataset \n\nlike instead of one of the below in training aka\
    \ fine tuning, put many as a single input? \n\nparaphrase :\nsentence1:(sentence)\n\
    sentence2:(sentence)"
  created_at: 2022-11-05 07:11:10+00:00
  edited: true
  hidden: false
  id: 63661a9e6604a4fee84fb1ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-05T10:19:52.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<blockquote>

          <p>is there any way to fine tune it with 8 gpus?</p>

          </blockquote>

          <p>You can finetune the smaller models with 8 GPUs, for example <a href="https://huggingface.co/bigscience/bloomz-7b1">bloomz-7b1</a>
          or <a href="https://huggingface.co/bigscience/mt0-xxl">mt0-xxl</a>. They
          are also very strong - mt0-xxl even outperforms bloomz on most tasks we
          measured.</p>

          <blockquote>

          <p>is there any way to batch the input dataset</p>

          </blockquote>

          <p>Yes ofc, you can batch inputs.</p>

          '
        raw: '>is there any way to fine tune it with 8 gpus?


          You can finetune the smaller models with 8 GPUs, for example [bloomz-7b1](https://huggingface.co/bigscience/bloomz-7b1)
          or [mt0-xxl](https://huggingface.co/bigscience/mt0-xxl). They are also very
          strong - mt0-xxl even outperforms bloomz on most tasks we measured.


          >is there any way to batch the input dataset


          Yes ofc, you can batch inputs.'
        updatedAt: '2022-11-05T10:19:52.693Z'
      numEdits: 0
      reactions: []
    id: 636638c8aa6a4af607381875
    type: comment
  author: Muennighoff
  content: '>is there any way to fine tune it with 8 gpus?


    You can finetune the smaller models with 8 GPUs, for example [bloomz-7b1](https://huggingface.co/bigscience/bloomz-7b1)
    or [mt0-xxl](https://huggingface.co/bigscience/mt0-xxl). They are also very strong
    - mt0-xxl even outperforms bloomz on most tasks we measured.


    >is there any way to batch the input dataset


    Yes ofc, you can batch inputs.'
  created_at: 2022-11-05 09:19:52+00:00
  edited: false
  hidden: false
  id: 636638c8aa6a4af607381875
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-11-05T10:46:47.000Z'
    data:
      edited: true
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>do you batch in the way I gave as an example? </p>

          <p>what is the difference between versions of mt and bloomz? </p>

          <p>what is the difference between bloom and bloomz?</p>

          <p>What is the vocabulary size for  bloomz-7b1 or mt0-xxl?</p>

          '
        raw: "do you batch in the way I gave as an example? \n\nwhat is the difference\
          \ between versions of mt and bloomz? \n\nwhat is the difference between\
          \ bloom and bloomz?\n\nWhat is the vocabulary size for  bloomz-7b1 or mt0-xxl?"
        updatedAt: '2022-11-05T10:58:26.637Z'
      numEdits: 1
      reactions: []
    id: 63663f17575c93ceda0034d3
    type: comment
  author: mishavee
  content: "do you batch in the way I gave as an example? \n\nwhat is the difference\
    \ between versions of mt and bloomz? \n\nwhat is the difference between bloom\
    \ and bloomz?\n\nWhat is the vocabulary size for  bloomz-7b1 or mt0-xxl?"
  created_at: 2022-11-05 09:46:47+00:00
  edited: true
  hidden: false
  id: 63663f17575c93ceda0034d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-05T15:52:20.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<blockquote>

          <p>do you batch in the way I gave as an example?</p>

          </blockquote>

          <p>We combine many examples</p>

          <blockquote>

          <p>what is the difference between versions of mt and bloomz?</p>

          </blockquote>

          <p>mt versions are fintuned on <a href="https://huggingface.co/datasets/bigscience/xP3mt">xP3mt</a>
          instead; They''re better for non-English prompts</p>

          <blockquote>

          <p>what is the difference between bloom and bloomz?</p>

          </blockquote>

          <p>BLOOMZ is better for following instructions. BLOOM is for continuing
          text.</p>

          <blockquote>

          <p>What is the vocabulary size for bloomz-7b1 or mt0-xxl?</p>

          </blockquote>

          <p>Written in the configs, e.g. <a href="https://huggingface.co/bigscience/mt0-xxl-mt/blob/main/config.json#L31">https://huggingface.co/bigscience/mt0-xxl-mt/blob/main/config.json#L31</a></p>

          '
        raw: '> do you batch in the way I gave as an example?


          We combine many examples


          > what is the difference between versions of mt and bloomz?


          mt versions are fintuned on [xP3mt](https://huggingface.co/datasets/bigscience/xP3mt)
          instead; They''re better for non-English prompts


          > what is the difference between bloom and bloomz?


          BLOOMZ is better for following instructions. BLOOM is for continuing text.



          > What is the vocabulary size for bloomz-7b1 or mt0-xxl?


          Written in the configs, e.g. https://huggingface.co/bigscience/mt0-xxl-mt/blob/main/config.json#L31'
        updatedAt: '2022-11-05T15:52:20.042Z'
      numEdits: 0
      reactions: []
    id: 636686b4a2abcdf2fd66e8f6
    type: comment
  author: Muennighoff
  content: '> do you batch in the way I gave as an example?


    We combine many examples


    > what is the difference between versions of mt and bloomz?


    mt versions are fintuned on [xP3mt](https://huggingface.co/datasets/bigscience/xP3mt)
    instead; They''re better for non-English prompts


    > what is the difference between bloom and bloomz?


    BLOOMZ is better for following instructions. BLOOM is for continuing text.



    > What is the vocabulary size for bloomz-7b1 or mt0-xxl?


    Written in the configs, e.g. https://huggingface.co/bigscience/mt0-xxl-mt/blob/main/config.json#L31'
  created_at: 2022-11-05 14:52:20+00:00
  edited: false
  hidden: false
  id: 636686b4a2abcdf2fd66e8f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-11-05T17:51:53.000Z'
    data:
      edited: true
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>what is xp3mt? How is it different? What is it normally trained
          on? </p>

          <p>please give an example of batching?</p>

          <p>when you say mt0-xxl outperforms bloomz on most tasks,  do you mean it
          outperforms the 176B model?</p>

          '
        raw: "what is xp3mt? How is it different? What is it normally trained on?\
          \ \n\nplease give an example of batching?\n\nwhen you say mt0-xxl outperforms\
          \ bloomz on most tasks,  do you mean it outperforms the 176B model?"
        updatedAt: '2022-11-05T20:22:40.990Z'
      numEdits: 1
      reactions: []
    id: 6366a2b92d6ba9a4f71b2388
    type: comment
  author: mishavee
  content: "what is xp3mt? How is it different? What is it normally trained on? \n\
    \nplease give an example of batching?\n\nwhen you say mt0-xxl outperforms bloomz\
    \ on most tasks,  do you mean it outperforms the 176B model?"
  created_at: 2022-11-05 16:51:53+00:00
  edited: true
  hidden: false
  id: 6366a2b92d6ba9a4f71b2388
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666122903077-noauth.png?w=200&h=200&f=face
      fullname: Connor Harmelink
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Charm3link
      type: user
    createdAt: '2022-12-02T22:38:10.000Z'
    data:
      edited: false
      editors:
      - Charm3link
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666122903077-noauth.png?w=200&h=200&f=face
          fullname: Connor Harmelink
          isHf: false
          isPro: false
          name: Charm3link
          type: user
        html: '<p>Hi!<br>I am trying to finetune Bloomz for data to text generation.
          The idea was to use a simple prefix like "Generate a natural english explanation
          of the following data:"<br>I was looking to use one of the smaller models,
          as my machine only has 4 gpus (48 gb total memory). If such a thing isn''t
          possible with my hardware feel free to not read on and say so! </p>

          <p>I was following this guide: <a rel="nofollow" href="https://github.com/bigscience-workshop/xmtf#bloomz">https://github.com/bigscience-workshop/xmtf#bloomz</a><br>And
          have a few questions.<br>In the slurm script linked above, it calls finetune_t0.py,
          but in the repo only finetune_t0_non_causal_decoder.py is available. Is
          this okay to use instead? </p>

          <p>I have my own data to text data set. If I wanted to finetune on this
          data, would using bloom-560-optimizer-states be the right call? If not what
          is a better option?<br>If so, should I follow this guide (linked for mt0
          models in the guide above, not sure if it applies for bloomz): <a rel="nofollow"
          href="https://github.com/google-research/t5x/blob/main/docs/usage/finetune.md">https://github.com/google-research/t5x/blob/main/docs/usage/finetune.md</a><br>Otherwise,
          how should I modify the script to work with the 560m model? It seems like
          pointing CHECKPOINT_PATH and removing no-load-optim reset-progress was not
          enough. </p>

          <p>Hopefully these questions make sense, apologies for the dense post! If
          there are more helpful resources out there in regards to finetuning bloomz
          for a new task please let me know.<br>Thanks!</p>

          '
        raw: "Hi! \nI am trying to finetune Bloomz for data to text generation. The\
          \ idea was to use a simple prefix like \"Generate a natural english explanation\
          \ of the following data:\" \nI was looking to use one of the smaller models,\
          \ as my machine only has 4 gpus (48 gb total memory). If such a thing isn't\
          \ possible with my hardware feel free to not read on and say so! \n\nI was\
          \ following this guide: https://github.com/bigscience-workshop/xmtf#bloomz\n\
          And have a few questions. \nIn the slurm script linked above, it calls finetune_t0.py,\
          \ but in the repo only finetune_t0_non_causal_decoder.py is available. Is\
          \ this okay to use instead? \n\n\nI have my own data to text data set. If\
          \ I wanted to finetune on this data, would using bloom-560-optimizer-states\
          \ be the right call? If not what is a better option? \nIf so, should I follow\
          \ this guide (linked for mt0 models in the guide above, not sure if it applies\
          \ for bloomz): https://github.com/google-research/t5x/blob/main/docs/usage/finetune.md\
          \ \nOtherwise, how should I modify the script to work with the 560m model?\
          \ It seems like pointing CHECKPOINT_PATH and removing no-load-optim reset-progress\
          \ was not enough. \n\nHopefully these questions make sense, apologies for\
          \ the dense post! If there are more helpful resources out there in regards\
          \ to finetuning bloomz for a new task please let me know. \nThanks!"
        updatedAt: '2022-12-02T22:38:10.336Z'
      numEdits: 0
      reactions: []
    id: 638a7e527a6e183d258efea9
    type: comment
  author: Charm3link
  content: "Hi! \nI am trying to finetune Bloomz for data to text generation. The\
    \ idea was to use a simple prefix like \"Generate a natural english explanation\
    \ of the following data:\" \nI was looking to use one of the smaller models, as\
    \ my machine only has 4 gpus (48 gb total memory). If such a thing isn't possible\
    \ with my hardware feel free to not read on and say so! \n\nI was following this\
    \ guide: https://github.com/bigscience-workshop/xmtf#bloomz\nAnd have a few questions.\
    \ \nIn the slurm script linked above, it calls finetune_t0.py, but in the repo\
    \ only finetune_t0_non_causal_decoder.py is available. Is this okay to use instead?\
    \ \n\n\nI have my own data to text data set. If I wanted to finetune on this data,\
    \ would using bloom-560-optimizer-states be the right call? If not what is a better\
    \ option? \nIf so, should I follow this guide (linked for mt0 models in the guide\
    \ above, not sure if it applies for bloomz): https://github.com/google-research/t5x/blob/main/docs/usage/finetune.md\
    \ \nOtherwise, how should I modify the script to work with the 560m model? It\
    \ seems like pointing CHECKPOINT_PATH and removing no-load-optim reset-progress\
    \ was not enough. \n\nHopefully these questions make sense, apologies for the\
    \ dense post! If there are more helpful resources out there in regards to finetuning\
    \ bloomz for a new task please let me know. \nThanks!"
  created_at: 2022-12-02 22:38:10+00:00
  edited: false
  hidden: false
  id: 638a7e527a6e183d258efea9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-12-02T22:46:10.000Z'
    data:
      edited: true
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<blockquote>\n<p>4 gpus (48 gb total memory)</p>\n</blockquote>\n<p>Do\
          \ you mean 48GB in total or 48GB / GPU? If it's in total, i.e. you have\
          \ 12GB on each GPU, then I think it's not feasible reasonably cc <span data-props=\"\
          {&quot;user&quot;:&quot;TimeRobber&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/TimeRobber\">@<span class=\"underline\"\
          >TimeRobber</span></a></span>\n\n\t</span></span> </p>\n<blockquote>\n<p>In\
          \ the slurm script linked above, it calls finetune_t0.py, but in the repo\
          \ only finetune_t0_non_causal_decoder.py is available. Is this okay to use\
          \ instead?</p>\n</blockquote>\n<p>That's because you need to clone the repo\
          \ on the branch <code>t0loading</code> like written in the guide \U0001F44D\
          </p>\n<blockquote>\n<p>would using bloom-560-optimizer-states be the right\
          \ call? </p>\n</blockquote>\n<p>Yes</p>\n<blockquote>\n<p>If so, should\
          \ I follow this guide</p>\n</blockquote>\n<p>You need to preprocess your\
          \ dataset first into Meg-DS format e.g. like here: <a rel=\"nofollow\" href=\"\
          https://github.com/bigscience-workshop/bigscience/blob/master/data/xp3/xp3_jsonl_to_meg.slurm\"\
          >https://github.com/bigscience-workshop/bigscience/blob/master/data/xp3/xp3_jsonl_to_meg.slurm</a></p>\n"
        raw: "> 4 gpus (48 gb total memory)\n\nDo you mean 48GB in total or 48GB /\
          \ GPU? If it's in total, i.e. you have 12GB on each GPU, then I think it's\
          \ not feasible reasonably cc @TimeRobber \n\n> In the slurm script linked\
          \ above, it calls finetune_t0.py, but in the repo only finetune_t0_non_causal_decoder.py\
          \ is available. Is this okay to use instead?\n\nThat's because you need\
          \ to clone the repo on the branch `t0loading` like written in the guide\
          \ \U0001F44D\n\n> would using bloom-560-optimizer-states be the right call?\
          \ \n\nYes\n\n> If so, should I follow this guide\n\nYou need to preprocess\
          \ your dataset first into Meg-DS format e.g. like here: https://github.com/bigscience-workshop/bigscience/blob/master/data/xp3/xp3_jsonl_to_meg.slurm"
        updatedAt: '2022-12-02T22:46:37.685Z'
      numEdits: 1
      reactions: []
    id: 638a8032f6a0bc485829e1c8
    type: comment
  author: Muennighoff
  content: "> 4 gpus (48 gb total memory)\n\nDo you mean 48GB in total or 48GB / GPU?\
    \ If it's in total, i.e. you have 12GB on each GPU, then I think it's not feasible\
    \ reasonably cc @TimeRobber \n\n> In the slurm script linked above, it calls finetune_t0.py,\
    \ but in the repo only finetune_t0_non_causal_decoder.py is available. Is this\
    \ okay to use instead?\n\nThat's because you need to clone the repo on the branch\
    \ `t0loading` like written in the guide \U0001F44D\n\n> would using bloom-560-optimizer-states\
    \ be the right call? \n\nYes\n\n> If so, should I follow this guide\n\nYou need\
    \ to preprocess your dataset first into Meg-DS format e.g. like here: https://github.com/bigscience-workshop/bigscience/blob/master/data/xp3/xp3_jsonl_to_meg.slurm"
  created_at: 2022-12-02 22:46:10+00:00
  edited: true
  hidden: false
  id: 638a8032f6a0bc485829e1c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-12-03T18:06:19.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Charm3link&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Charm3link\">@<span class=\"\
          underline\">Charm3link</span></a></span>\n\n\t</span></span> I think if\
          \ you're trying to finetuning the smallest bloom you might be able to? It\
          \ really depends on your setup, and on how fast you want to be able to finetune\
          \ it. We should probably port some of the finetuning techniques we used\
          \ in <code>transformers</code> library at some point so people can start\
          \ leveraging it.</p>\n"
        raw: '@Charm3link I think if you''re trying to finetuning the smallest bloom
          you might be able to? It really depends on your setup, and on how fast you
          want to be able to finetune it. We should probably port some of the finetuning
          techniques we used in `transformers` library at some point so people can
          start leveraging it.'
        updatedAt: '2022-12-03T18:06:19.284Z'
      numEdits: 0
      reactions: []
    id: 638b901bdaa4cd1d6b40b11d
    type: comment
  author: TimeRobber
  content: '@Charm3link I think if you''re trying to finetuning the smallest bloom
    you might be able to? It really depends on your setup, and on how fast you want
    to be able to finetune it. We should probably port some of the finetuning techniques
    we used in `transformers` library at some point so people can start leveraging
    it.'
  created_at: 2022-12-03 18:06:19+00:00
  edited: false
  hidden: false
  id: 638b901bdaa4cd1d6b40b11d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4fa926c30971ad397aa678d841e46d5d.svg
      fullname: Muhammad Ahmad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: itzNoBiTa
      type: user
    createdAt: '2023-06-14T08:10:03.000Z'
    data:
      edited: false
      editors:
      - itzNoBiTa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9061238169670105
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4fa926c30971ad397aa678d841e46d5d.svg
          fullname: Muhammad Ahmad
          isHf: false
          isPro: false
          name: itzNoBiTa
          type: user
        html: '<p>How much minimum requirements do i need to fine tune bloomz-560m
          model on custom data<br>can anyone guide?</p>

          '
        raw: "How much minimum requirements do i need to fine tune bloomz-560m model\
          \ on custom data \ncan anyone guide?"
        updatedAt: '2023-06-14T08:10:03.283Z'
      numEdits: 0
      reactions: []
    id: 648975db91e3ed1a12fcb0d0
    type: comment
  author: itzNoBiTa
  content: "How much minimum requirements do i need to fine tune bloomz-560m model\
    \ on custom data \ncan anyone guide?"
  created_at: 2023-06-14 07:10:03+00:00
  edited: false
  hidden: false
  id: 648975db91e3ed1a12fcb0d0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: bigscience/bloomz
repo_type: model
status: open
target_branch: null
title: 'running and fine tuning '
