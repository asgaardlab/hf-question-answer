!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tingxinli
conflicting_files: null
created_at: 2023-08-05 02:34:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ce92ebeefe057e7a4accd56999e85556.svg
      fullname: Tingxin Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tingxinli
      type: user
    createdAt: '2023-08-05T03:34:20.000Z'
    data:
      edited: true
      editors:
      - tingxinli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9409223198890686
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ce92ebeefe057e7a4accd56999e85556.svg
          fullname: Tingxin Li
          isHf: false
          isPro: false
          name: tingxinli
          type: user
        html: '<p>We fine-tuned BLOOMZ to do customized translation task, and it surprisingly
          works well on text masked with those entity labels like ''&lt;PERSON&gt;'',
          ''&lt;LOC&gt;'', etc.  However, when we slightly changed the label as ''&lt;PERSON_id&gt;''
          (to discriminate different entities), its performance dropped dramatically.
          Hence, we suspect that labels like ''&lt;PERSON&gt;'' are somewhat specially
          treated in the pretraining or multi-task fine-tuning process. Is our guessing
          correct? If not, what could be possible reasons? Thanks!</p>

          '
        raw: We fine-tuned BLOOMZ to do customized translation task, and it surprisingly
          works well on text masked with those entity labels like '\<PERSON\>', '\<LOC\>',
          etc.  However, when we slightly changed the label as '\<PERSON_id\>' (to
          discriminate different entities), its performance dropped dramatically.
          Hence, we suspect that labels like '\<PERSON\>' are somewhat specially treated
          in the pretraining or multi-task fine-tuning process. Is our guessing correct?
          If not, what could be possible reasons? Thanks!
        updatedAt: '2023-08-05T06:15:39.149Z'
      numEdits: 3
      reactions: []
    id: 64cdc33c7a7305c589f01e1b
    type: comment
  author: tingxinli
  content: We fine-tuned BLOOMZ to do customized translation task, and it surprisingly
    works well on text masked with those entity labels like '\<PERSON\>', '\<LOC\>',
    etc.  However, when we slightly changed the label as '\<PERSON_id\>' (to discriminate
    different entities), its performance dropped dramatically. Hence, we suspect that
    labels like '\<PERSON\>' are somewhat specially treated in the pretraining or
    multi-task fine-tuning process. Is our guessing correct? If not, what could be
    possible reasons? Thanks!
  created_at: 2023-08-05 02:34:20+00:00
  edited: true
  hidden: false
  id: 64cdc33c7a7305c589f01e1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-08-05T15:54:50.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9041165113449097
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>All special tokens of the model are here: <a href="https://huggingface.co/bigscience/bloomz/blob/main/special_tokens_map.json">https://huggingface.co/bigscience/bloomz/blob/main/special_tokens_map.json</a><br>&amp;
          they do not include such tokens. </p>

          <p>I imagine that things like <code>&lt;PERSON&gt;</code> may naturally
          appear somewhere in the datasets, but it was not added by us on purpose
          at least for the finetuning data.</p>

          '
        raw: "All special tokens of the model are here: https://huggingface.co/bigscience/bloomz/blob/main/special_tokens_map.json\n\
          & they do not include such tokens. \n\nI imagine that things like `<PERSON>`\
          \ may naturally appear somewhere in the datasets, but it was not added by\
          \ us on purpose at least for the finetuning data."
        updatedAt: '2023-08-05T15:54:50.925Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - tingxinli
    id: 64ce70cabf39f9c8be8ece01
    type: comment
  author: Muennighoff
  content: "All special tokens of the model are here: https://huggingface.co/bigscience/bloomz/blob/main/special_tokens_map.json\n\
    & they do not include such tokens. \n\nI imagine that things like `<PERSON>` may\
    \ naturally appear somewhere in the datasets, but it was not added by us on purpose\
    \ at least for the finetuning data."
  created_at: 2023-08-05 14:54:50+00:00
  edited: false
  hidden: false
  id: 64ce70cabf39f9c8be8ece01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ce92ebeefe057e7a4accd56999e85556.svg
      fullname: Tingxin Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tingxinli
      type: user
    createdAt: '2023-08-06T07:54:51.000Z'
    data:
      edited: false
      editors:
      - tingxinli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9107732176780701
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ce92ebeefe057e7a4accd56999e85556.svg
          fullname: Tingxin Li
          isHf: false
          isPro: false
          name: tingxinli
          type: user
        html: '<blockquote>

          <p>All special tokens of the model are here: <a href="https://huggingface.co/bigscience/bloomz/blob/main/special_tokens_map.json">https://huggingface.co/bigscience/bloomz/blob/main/special_tokens_map.json</a><br>&amp;
          they do not include such tokens. </p>

          <p>I imagine that things like <code>&lt;PERSON&gt;</code> may naturally
          appear somewhere in the datasets, but it was not added by us on purpose
          at least for the finetuning data.</p>

          </blockquote>

          <p>We guess so. Thanks for your reply!</p>

          '
        raw: "> All special tokens of the model are here: https://huggingface.co/bigscience/bloomz/blob/main/special_tokens_map.json\n\
          > & they do not include such tokens. \n> \n> I imagine that things like\
          \ `<PERSON>` may naturally appear somewhere in the datasets, but it was\
          \ not added by us on purpose at least for the finetuning data.\n\nWe guess\
          \ so. Thanks for your reply!"
        updatedAt: '2023-08-06T07:54:51.618Z'
      numEdits: 0
      reactions: []
    id: 64cf51cba2e7f9ff61e9c8d3
    type: comment
  author: tingxinli
  content: "> All special tokens of the model are here: https://huggingface.co/bigscience/bloomz/blob/main/special_tokens_map.json\n\
    > & they do not include such tokens. \n> \n> I imagine that things like `<PERSON>`\
    \ may naturally appear somewhere in the datasets, but it was not added by us on\
    \ purpose at least for the finetuning data.\n\nWe guess so. Thanks for your reply!"
  created_at: 2023-08-06 06:54:51+00:00
  edited: false
  hidden: false
  id: 64cf51cba2e7f9ff61e9c8d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ce92ebeefe057e7a4accd56999e85556.svg
      fullname: Tingxin Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tingxinli
      type: user
    createdAt: '2023-08-06T07:55:21.000Z'
    data:
      status: closed
    id: 64cf51e9e7b70e91a2482785
    type: status-change
  author: tingxinli
  created_at: 2023-08-06 06:55:21+00:00
  id: 64cf51e9e7b70e91a2482785
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 47
repo_id: bigscience/bloomz
repo_type: model
status: closed
target_branch: null
title: Are there any special tokens formatted as '<PERSON>', '<LOC>' in the training
  set or fine-tuning set?
