!!python/object:huggingface_hub.community.DiscussionWithDetails
author: amuhak
conflicting_files: null
created_at: 2023-01-18 00:36:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/610ab3d08e1e10f34b0859826d834f9d.svg
      fullname: Amulya Jain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: amuhak
      type: user
    createdAt: '2023-01-18T00:36:31.000Z'
    data:
      edited: false
      editors:
      - amuhak
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/610ab3d08e1e10f34b0859826d834f9d.svg
          fullname: Amulya Jain
          isHf: false
          isPro: false
          name: amuhak
          type: user
        html: '<p>When using Longer prompts I get:<br><code>input length of input_ids
          is 33, but `max_length` is set to 20. This can lead to unexpected behavior.
          You should consider increasing `max_new_tokens`.</code></p>

          <p>Setting max_length=1000 doesn''t seem to do anything.</p>

          <p>And setting max_new_tokens=100 gives me this:</p>

          <p><code>_batch_encode_plus() got an unexpected keyword argument ''max_new_tokens''
          </code></p>

          <p>What am I doing wrong?</p>

          '
        raw: "When using Longer prompts I get:\r\n```input length of input_ids is\
          \ 33, but `max_length` is set to 20. This can lead to unexpected behavior.\
          \ You should consider increasing `max_new_tokens`.```\r\n\r\nSetting max_length=1000\
          \ doesn't seem to do anything.\r\n\r\nAnd setting max_new_tokens=100 gives\
          \ me this:\r\n\r\n```_batch_encode_plus() got an unexpected keyword argument\
          \ 'max_new_tokens' ```\r\n\r\nWhat am I doing wrong?"
        updatedAt: '2023-01-18T00:36:31.631Z'
      numEdits: 0
      reactions: []
    id: 63c73f0f50cc81901dac2a38
    type: comment
  author: amuhak
  content: "When using Longer prompts I get:\r\n```input length of input_ids is 33,\
    \ but `max_length` is set to 20. This can lead to unexpected behavior. You should\
    \ consider increasing `max_new_tokens`.```\r\n\r\nSetting max_length=1000 doesn't\
    \ seem to do anything.\r\n\r\nAnd setting max_new_tokens=100 gives me this:\r\n\
    \r\n```_batch_encode_plus() got an unexpected keyword argument 'max_new_tokens'\
    \ ```\r\n\r\nWhat am I doing wrong?"
  created_at: 2023-01-18 00:36:31+00:00
  edited: false
  hidden: false
  id: 63c73f0f50cc81901dac2a38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663152870242-625e82360f1d3ed7c8cdc137.jpeg?w=200&h=200&f=face
      fullname: Hinnes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hinneslung
      type: user
    createdAt: '2023-01-18T07:39:25.000Z'
    data:
      edited: true
      editors:
      - hinneslung
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663152870242-625e82360f1d3ed7c8cdc137.jpeg?w=200&h=200&f=face
          fullname: Hinnes
          isHf: false
          isPro: false
          name: hinneslung
          type: user
        html: "<p><code>max_new_tokens</code> is a parameter in <code>model.generate()</code>,\
          \ which you can change for different prompts for the same created tokenizer\
          \ and model.</p>\n<p>See the snippet below as an example:</p>\n<pre><code\
          \ class=\"language-python\">checkpoint = <span class=\"hljs-string\">\"\
          bigscience/bloomz-7b1-mt\"</span>\n\ntokenizer = BloomTokenizerFast.from_pretrained(checkpoint)\n\
          model = BloomForCausalLM.from_pretrained(\n    checkpoint, torch_dtype=<span\
          \ class=\"hljs-string\">\"auto\"</span>, device_map=<span class=\"hljs-string\"\
          >\"auto\"</span>\n)\n\nprompt = <span class=\"hljs-string\">\"your prompt\"\
          </span>\ninputs = tokenizer.encode(prompt, return_tensors=<span class=\"\
          hljs-string\">\"pt\"</span>).to(<span class=\"hljs-string\">\"cuda\"</span>)\n\
          outputs = model.generate(inputs, max_new_tokens=<span class=\"hljs-number\"\
          >20</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(outputs[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n<p>See <code>max_new_tokens=20</code>.\
          \ This is equivalent to giving <code>max_length=len(prompt) + 20</code>,\
          \ which will allow the model to output an additional 20 tokens however long\
          \ your prompt would be.</p>\n"
        raw: "`max_new_tokens` is a parameter in `model.generate()`, which you can\
          \ change for different prompts for the same created tokenizer and model.\n\
          \nSee the snippet below as an example:\n\n```python\ncheckpoint = \"bigscience/bloomz-7b1-mt\"\
          \n\ntokenizer = BloomTokenizerFast.from_pretrained(checkpoint)\nmodel =\
          \ BloomForCausalLM.from_pretrained(\n    checkpoint, torch_dtype=\"auto\"\
          , device_map=\"auto\"\n)\n\nprompt = \"your prompt\"\ninputs = tokenizer.encode(prompt,\
          \ return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs,\
          \ max_new_tokens=20)\nprint(tokenizer.decode(outputs[0]))\n```\nSee `max_new_tokens=20`.\
          \ This is equivalent to giving `max_length=len(prompt) + 20`, which will\
          \ allow the model to output an additional 20 tokens however long your prompt\
          \ would be."
        updatedAt: '2023-01-18T09:59:01.574Z'
      numEdits: 1
      reactions:
      - count: 5
        reaction: "\u2764\uFE0F"
        users:
        - Muennighoff
        - amuhak
        - qinluo
        - joaogante
        - CR33STL
    id: 63c7a22d53af58c399105cb0
    type: comment
  author: hinneslung
  content: "`max_new_tokens` is a parameter in `model.generate()`, which you can change\
    \ for different prompts for the same created tokenizer and model.\n\nSee the snippet\
    \ below as an example:\n\n```python\ncheckpoint = \"bigscience/bloomz-7b1-mt\"\
    \n\ntokenizer = BloomTokenizerFast.from_pretrained(checkpoint)\nmodel = BloomForCausalLM.from_pretrained(\n\
    \    checkpoint, torch_dtype=\"auto\", device_map=\"auto\"\n)\n\nprompt = \"your\
    \ prompt\"\ninputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\"\
    )\noutputs = model.generate(inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0]))\n\
    ```\nSee `max_new_tokens=20`. This is equivalent to giving `max_length=len(prompt)\
    \ + 20`, which will allow the model to output an additional 20 tokens however\
    \ long your prompt would be."
  created_at: 2023-01-18 07:39:25+00:00
  edited: true
  hidden: false
  id: 63c7a22d53af58c399105cb0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666122903077-noauth.png?w=200&h=200&f=face
      fullname: Connor Harmelink
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Charm3link
      type: user
    createdAt: '2023-03-28T19:09:42.000Z'
    data:
      edited: false
      editors:
      - Charm3link
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666122903077-noauth.png?w=200&h=200&f=face
          fullname: Connor Harmelink
          isHf: false
          isPro: false
          name: Charm3link
          type: user
        html: '<p>I had the same error message, but since I was training I wasn''t
          directly calling model.generate as above.<br>To get around this, I changed
          the value of model.config.max_length before beginning training. </p>

          <p>I am not sure if this is technically correct, but it did get rid of the
          same warning you mentioned.</p>

          '
        raw: "I had the same error message, but since I was training I wasn't directly\
          \ calling model.generate as above. \nTo get around this, I changed the value\
          \ of model.config.max_length before beginning training. \n\nI am not sure\
          \ if this is technically correct, but it did get rid of the same warning\
          \ you mentioned."
        updatedAt: '2023-03-28T19:09:42.276Z'
      numEdits: 0
      reactions: []
    id: 64233b76b0e466cdd989ea48
    type: comment
  author: Charm3link
  content: "I had the same error message, but since I was training I wasn't directly\
    \ calling model.generate as above. \nTo get around this, I changed the value of\
    \ model.config.max_length before beginning training. \n\nI am not sure if this\
    \ is technically correct, but it did get rid of the same warning you mentioned."
  created_at: 2023-03-28 18:09:42+00:00
  edited: false
  hidden: false
  id: 64233b76b0e466cdd989ea48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/610ab3d08e1e10f34b0859826d834f9d.svg
      fullname: Amulya Jain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: amuhak
      type: user
    createdAt: '2023-07-19T18:04:53.000Z'
    data:
      status: closed
    id: 64b825c5f8bf823a61d3f5e8
    type: status-change
  author: amuhak
  created_at: 2023-07-19 17:04:53+00:00
  id: 64b825c5f8bf823a61d3f5e8
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 27
repo_id: bigscience/bloomz
repo_type: model
status: closed
target_branch: null
title: Unable to edit max_length
