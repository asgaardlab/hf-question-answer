!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kato-22
conflicting_files: null
created_at: 2023-04-08 23:11:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6e3298ec386e85ae189577644b05114.svg
      fullname: Carlos Guadarrama-Trejo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kato-22
      type: user
    createdAt: '2023-04-09T00:11:17.000Z'
    data:
      edited: false
      editors:
      - Kato-22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6e3298ec386e85ae189577644b05114.svg
          fullname: Carlos Guadarrama-Trejo
          isHf: false
          isPro: false
          name: Kato-22
          type: user
        html: '<p>I read that Bloomz was good for summarization tasks compared to
          the regular bloom model.<br>However, based on my experience, it refuses
          to summarize the text. I have tried several prompts to no avail, it always
          returns the input text.<br>Has anyone had success with abstraction summarization
          with Bloomz?</p>

          <pre><code class="language-py"><span class="hljs-keyword">from</span> transformers
          <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

          <span class="hljs-keyword">import</span> torch


          device = <span class="hljs-string">''cuda:0''</span> <span class="hljs-keyword">if</span>
          torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">''cpu''</span>


          tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"bigscience/bloomz-3b"</span>)

          model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">"bigscience/bloomz-3b"</span>,
          torch_dtype=<span class="hljs-string">''auto''</span>, device_map=<span
          class="hljs-string">''auto''</span>, offload_folder=<span class="hljs-string">''offload''</span>,
          offload_state_dict=<span class="hljs-literal">True</span>)


          text = <span class="hljs-string">"In this lesson, we will learn about the
          different types of clouds that exist in the atmosphere. Clouds are classified
          based on their height, shape, and composition. The three main cloud types
          are cumulus, stratus, and cirrus. Cumulus clouds are puffy and white, with
          a flat base and a rounded top. Stratus clouds are low, gray, and flat, and
          they often cover the entire sky. Cirrus clouds are high and thin, with a
          wispy appearance. They are often an indicator of an approaching storm. Understanding
          the different cloud types is important for weather forecasting and aviation
          safety."</span>


          inputs = tokenizer.encode(<span class="hljs-string">f''Write a brief summary
          for the following text that focuses on the main idea:\n\n<span class="hljs-subst">{text}</span>''</span>,
          return_tensors=<span class="hljs-string">"pt"</span>).to(device)

          outputs = model.generate(inputs, max_new_tokens=<span class="hljs-number">64</span>)


          summary = tokenizer.decode(outputs[<span class="hljs-number">0</span>],
          skip_special_tokens=<span class="hljs-literal">True</span>)


          <span class="hljs-built_in">print</span>(summary)

          </code></pre>

          <p>Output:</p>

          <blockquote>

          <p>Write a brief summary for the following text that focuses on the main
          idea:</p>

          <p>In this lesson, we will learn about the different types of clouds that
          exist in the atmosphere. Clouds are classified based on their height, shape,
          and composition. The three main cloud types are cumulus, stratus, and cirrus.
          Cumulus clouds are puffy and white, with a flat base and a rounded top.
          Stratus clouds are low, gray, and flat, and they often cover the entire
          sky. Cirrus clouds are high and thin, with a wispy appearance. They are
          often an indicator of an approaching storm. Understanding the different
          cloud types is important for weather forecasting and aviation safety.</p>

          </blockquote>

          '
        raw: "I read that Bloomz was good for summarization tasks compared to the\
          \ regular bloom model.\r\nHowever, based on my experience, it refuses to\
          \ summarize the text. I have tried several prompts to no avail, it always\
          \ returns the input text.\r\nHas anyone had success with abstraction summarization\
          \ with Bloomz?\r\n\r\n```py\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\r\nimport torch\r\n\r\ndevice = 'cuda:0' if torch.cuda.is_available()\
          \ else 'cpu'\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-3b\"\
          )\r\nmodel = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-3b\"\
          , torch_dtype='auto', device_map='auto', offload_folder='offload', offload_state_dict=True)\r\
          \n\r\ntext = \"In this lesson, we will learn about the different types of\
          \ clouds that exist in the atmosphere. Clouds are classified based on their\
          \ height, shape, and composition. The three main cloud types are cumulus,\
          \ stratus, and cirrus. Cumulus clouds are puffy and white, with a flat base\
          \ and a rounded top. Stratus clouds are low, gray, and flat, and they often\
          \ cover the entire sky. Cirrus clouds are high and thin, with a wispy appearance.\
          \ They are often an indicator of an approaching storm. Understanding the\
          \ different cloud types is important for weather forecasting and aviation\
          \ safety.\"\r\n\r\ninputs = tokenizer.encode(f'Write a brief summary for\
          \ the following text that focuses on the main idea:\\n\\n{text}', return_tensors=\"\
          pt\").to(device)\r\noutputs = model.generate(inputs, max_new_tokens=64)\r\
          \n\r\nsummary = tokenizer.decode(outputs[0], skip_special_tokens=True)\r\
          \n\r\nprint(summary)\r\n```\r\n\r\nOutput:\r\n> Write a brief summary for\
          \ the following text that focuses on the main idea:\r\n> \r\n> In this lesson,\
          \ we will learn about the different types of clouds that exist in the atmosphere.\
          \ Clouds are classified based on their height, shape, and composition. The\
          \ three main cloud types are cumulus, stratus, and cirrus. Cumulus clouds\
          \ are puffy and white, with a flat base and a rounded top. Stratus clouds\
          \ are low, gray, and flat, and they often cover the entire sky. Cirrus clouds\
          \ are high and thin, with a wispy appearance. They are often an indicator\
          \ of an approaching storm. Understanding the different cloud types is important\
          \ for weather forecasting and aviation safety."
        updatedAt: '2023-04-09T00:11:17.860Z'
      numEdits: 0
      reactions: []
    id: 643202a5288c9775673cf69d
    type: comment
  author: Kato-22
  content: "I read that Bloomz was good for summarization tasks compared to the regular\
    \ bloom model.\r\nHowever, based on my experience, it refuses to summarize the\
    \ text. I have tried several prompts to no avail, it always returns the input\
    \ text.\r\nHas anyone had success with abstraction summarization with Bloomz?\r\
    \n\r\n```py\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\
    import torch\r\n\r\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\r\
    \n\r\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-3b\")\r\n\
    model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-3b\", torch_dtype='auto',\
    \ device_map='auto', offload_folder='offload', offload_state_dict=True)\r\n\r\n\
    text = \"In this lesson, we will learn about the different types of clouds that\
    \ exist in the atmosphere. Clouds are classified based on their height, shape,\
    \ and composition. The three main cloud types are cumulus, stratus, and cirrus.\
    \ Cumulus clouds are puffy and white, with a flat base and a rounded top. Stratus\
    \ clouds are low, gray, and flat, and they often cover the entire sky. Cirrus\
    \ clouds are high and thin, with a wispy appearance. They are often an indicator\
    \ of an approaching storm. Understanding the different cloud types is important\
    \ for weather forecasting and aviation safety.\"\r\n\r\ninputs = tokenizer.encode(f'Write\
    \ a brief summary for the following text that focuses on the main idea:\\n\\n{text}',\
    \ return_tensors=\"pt\").to(device)\r\noutputs = model.generate(inputs, max_new_tokens=64)\r\
    \n\r\nsummary = tokenizer.decode(outputs[0], skip_special_tokens=True)\r\n\r\n\
    print(summary)\r\n```\r\n\r\nOutput:\r\n> Write a brief summary for the following\
    \ text that focuses on the main idea:\r\n> \r\n> In this lesson, we will learn\
    \ about the different types of clouds that exist in the atmosphere. Clouds are\
    \ classified based on their height, shape, and composition. The three main cloud\
    \ types are cumulus, stratus, and cirrus. Cumulus clouds are puffy and white,\
    \ with a flat base and a rounded top. Stratus clouds are low, gray, and flat,\
    \ and they often cover the entire sky. Cirrus clouds are high and thin, with a\
    \ wispy appearance. They are often an indicator of an approaching storm. Understanding\
    \ the different cloud types is important for weather forecasting and aviation\
    \ safety."
  created_at: 2023-04-08 23:11:17+00:00
  edited: false
  hidden: false
  id: 643202a5288c9775673cf69d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-04-09T08:51:45.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>It''s not returning the input text, but directly finishing the generation
          after your prompt. You can see that by setting <code>skip_special_tokens=False</code>.</p>

          <p>It''s best to have the instruction after the context, see the below generation
          I got with bloomz-3b:</p>

          <pre><code class="language-python">inputs = tokenizer.encode(<span class="hljs-string">f''<span
          class="hljs-subst">{text}</span>\n\nWrite a short summary for the prior
          text.''</span>, return_tensors=<span class="hljs-string">"pt"</span>).to(device)

          outputs = model.generate(inputs, max_new_tokens=<span class="hljs-number">64</span>)


          summary = tokenizer.decode(outputs[<span class="hljs-number">0</span>],
          skip_special_tokens=<span class="hljs-literal">False</span>)


          <span class="hljs-built_in">print</span>(summary)

          </code></pre>

          <blockquote>

          <blockquote>

          <p>Learn about different types of clouds.</p>

          </blockquote>

          </blockquote>

          '
        raw: 'It''s not returning the input text, but directly finishing the generation
          after your prompt. You can see that by setting `skip_special_tokens=False`.


          It''s best to have the instruction after the context, see the below generation
          I got with bloomz-3b:


          ```python

          inputs = tokenizer.encode(f''{text}\n\nWrite a short summary for the prior
          text.'', return_tensors="pt").to(device)

          outputs = model.generate(inputs, max_new_tokens=64)


          summary = tokenizer.decode(outputs[0], skip_special_tokens=False)


          print(summary)

          ```

          >> Learn about different types of clouds.'
        updatedAt: '2023-04-09T08:51:45.399Z'
      numEdits: 0
      reactions: []
    id: 64327ca1cca1de06ec0eb824
    type: comment
  author: Muennighoff
  content: 'It''s not returning the input text, but directly finishing the generation
    after your prompt. You can see that by setting `skip_special_tokens=False`.


    It''s best to have the instruction after the context, see the below generation
    I got with bloomz-3b:


    ```python

    inputs = tokenizer.encode(f''{text}\n\nWrite a short summary for the prior text.'',
    return_tensors="pt").to(device)

    outputs = model.generate(inputs, max_new_tokens=64)


    summary = tokenizer.decode(outputs[0], skip_special_tokens=False)


    print(summary)

    ```

    >> Learn about different types of clouds.'
  created_at: 2023-04-09 07:51:45+00:00
  edited: false
  hidden: false
  id: 64327ca1cca1de06ec0eb824
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6e3298ec386e85ae189577644b05114.svg
      fullname: Carlos Guadarrama-Trejo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kato-22
      type: user
    createdAt: '2023-04-10T05:02:44.000Z'
    data:
      edited: false
      editors:
      - Kato-22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6e3298ec386e85ae189577644b05114.svg
          fullname: Carlos Guadarrama-Trejo
          isHf: false
          isPro: false
          name: Kato-22
          type: user
        html: '<p>Thanks, adding the instruction after the context helped. However,
          it looks like Bloomz is doing extractive summarization rather than abstractive
          summarization.<br>I tried several prompts such as:</p>

          <ul>

          <li>What are the main points of the prior text?</li>

          <li>What was the prior text about?</li>

          <li>Summarize the prior text.</li>

          <li>Write a brief summary of the prior text that highlights its key points.</li>

          <li>Write a summary of the prior text that highlights its key points.</li>

          </ul>

          <p>But all of them returned basically the same "Learn about different types
          of clouds." output (the last two prompts changed "learn" to "identify").<br>Even
          using a larger model (bigscience/bloomz-7b1) yielded the same results.</p>

          <p>How can use Bloomz for abstractive summarization?</p>

          '
        raw: 'Thanks, adding the instruction after the context helped. However, it
          looks like Bloomz is doing extractive summarization rather than abstractive
          summarization.

          I tried several prompts such as:

          * What are the main points of the prior text?

          * What was the prior text about?

          * Summarize the prior text.

          * Write a brief summary of the prior text that highlights its key points.

          * Write a summary of the prior text that highlights its key points.


          But all of them returned basically the same "Learn about different types
          of clouds." output (the last two prompts changed "learn" to "identify").

          Even using a larger model (bigscience/bloomz-7b1) yielded the same results.


          How can use Bloomz for abstractive summarization?'
        updatedAt: '2023-04-10T05:02:44.214Z'
      numEdits: 0
      reactions: []
    id: 643398745c65189fe4f51cc0
    type: comment
  author: Kato-22
  content: 'Thanks, adding the instruction after the context helped. However, it looks
    like Bloomz is doing extractive summarization rather than abstractive summarization.

    I tried several prompts such as:

    * What are the main points of the prior text?

    * What was the prior text about?

    * Summarize the prior text.

    * Write a brief summary of the prior text that highlights its key points.

    * Write a summary of the prior text that highlights its key points.


    But all of them returned basically the same "Learn about different types of clouds."
    output (the last two prompts changed "learn" to "identify").

    Even using a larger model (bigscience/bloomz-7b1) yielded the same results.


    How can use Bloomz for abstractive summarization?'
  created_at: 2023-04-10 04:02:44+00:00
  edited: false
  hidden: false
  id: 643398745c65189fe4f51cc0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-04-10T15:44:37.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>You could give it a few-shot example. E.g. provide it one example
          with an abstractive summary in the beginning of your prompt.</p>

          '
        raw: You could give it a few-shot example. E.g. provide it one example with
          an abstractive summary in the beginning of your prompt.
        updatedAt: '2023-04-10T15:44:37.238Z'
      numEdits: 0
      reactions: []
    id: 64342ee58d68561d704f02fc
    type: comment
  author: Muennighoff
  content: You could give it a few-shot example. E.g. provide it one example with
    an abstractive summary in the beginning of your prompt.
  created_at: 2023-04-10 14:44:37+00:00
  edited: false
  hidden: false
  id: 64342ee58d68561d704f02fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94cf041d3263b5b6e8d63d95aab75032.svg
      fullname: lixn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lixiqi
      type: user
    createdAt: '2023-07-02T15:27:35.000Z'
    data:
      edited: false
      editors:
      - lixiqi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7434366345405579
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94cf041d3263b5b6e8d63d95aab75032.svg
          fullname: lixn
          isHf: false
          isPro: false
          name: lixiqi
          type: user
        html: "<blockquote>\n<p>It's not returning the input text, but directly finishing\
          \ the generation after your prompt. You can see that by setting <code>skip_special_tokens=False</code>.</p>\n\
          <p>It's best to have the instruction after the context, see the below generation\
          \ I got with bloomz-3b:</p>\n<pre><code class=\"language-python\">inputs\
          \ = tokenizer.encode(<span class=\"hljs-string\">f'<span class=\"hljs-subst\"\
          >{text}</span>\\n\\nWrite a short summary for the prior text.'</span>, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>).to(device)\noutputs = model.generate(inputs,\
          \ max_new_tokens=<span class=\"hljs-number\">64</span>)\n\nsummary = tokenizer.decode(outputs[<span\
          \ class=\"hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >False</span>)\n\n<span class=\"hljs-built_in\">print</span>(summary)\n\
          </code></pre>\n<blockquote>\n<blockquote>\n<p>Learn about different types\
          \ of clouds.</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\">@<span\
          \ class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \  Hi, I have the same problem and tried with skip_special_tokens=False\
          \ but the prompt is still printed, is there any changes to the instruction?</p>\n"
        raw: "> It's not returning the input text, but directly finishing the generation\
          \ after your prompt. You can see that by setting `skip_special_tokens=False`.\n\
          > \n> It's best to have the instruction after the context, see the below\
          \ generation I got with bloomz-3b:\n> \n> ```python\n> inputs = tokenizer.encode(f'{text}\\\
          n\\nWrite a short summary for the prior text.', return_tensors=\"pt\").to(device)\n\
          > outputs = model.generate(inputs, max_new_tokens=64)\n> \n> summary = tokenizer.decode(outputs[0],\
          \ skip_special_tokens=False)\n> \n> print(summary)\n> ```\n> >> Learn about\
          \ different types of clouds.\n\n@Muennighoff  Hi, I have the same problem\
          \ and tried with skip_special_tokens=False but the prompt is still printed,\
          \ is there any changes to the instruction?"
        updatedAt: '2023-07-02T15:27:35.283Z'
      numEdits: 0
      reactions: []
    id: 64a19767b5b55f4657c442f4
    type: comment
  author: lixiqi
  content: "> It's not returning the input text, but directly finishing the generation\
    \ after your prompt. You can see that by setting `skip_special_tokens=False`.\n\
    > \n> It's best to have the instruction after the context, see the below generation\
    \ I got with bloomz-3b:\n> \n> ```python\n> inputs = tokenizer.encode(f'{text}\\\
    n\\nWrite a short summary for the prior text.', return_tensors=\"pt\").to(device)\n\
    > outputs = model.generate(inputs, max_new_tokens=64)\n> \n> summary = tokenizer.decode(outputs[0],\
    \ skip_special_tokens=False)\n> \n> print(summary)\n> ```\n> >> Learn about different\
    \ types of clouds.\n\n@Muennighoff  Hi, I have the same problem and tried with\
    \ skip_special_tokens=False but the prompt is still printed, is there any changes\
    \ to the instruction?"
  created_at: 2023-07-02 14:27:35+00:00
  edited: false
  hidden: false
  id: 64a19767b5b55f4657c442f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-07-02T19:30:00.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7755792737007141
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<blockquote>\n<blockquote>\n<p>It's not returning the input text, but\
          \ directly finishing the generation after your prompt. You can see that\
          \ by setting <code>skip_special_tokens=False</code>.</p>\n<p>It's best to\
          \ have the instruction after the context, see the below generation I got\
          \ with bloomz-3b:</p>\n<pre><code class=\"language-python\">inputs = tokenizer.encode(<span\
          \ class=\"hljs-string\">f'<span class=\"hljs-subst\">{text}</span>\\n\\\
          nWrite a short summary for the prior text.'</span>, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>).to(device)\noutputs = model.generate(inputs,\
          \ max_new_tokens=<span class=\"hljs-number\">64</span>)\n\nsummary = tokenizer.decode(outputs[<span\
          \ class=\"hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >False</span>)\n\n<span class=\"hljs-built_in\">print</span>(summary)\n\
          </code></pre>\n<blockquote>\n<blockquote>\n<p>Learn about different types\
          \ of clouds.</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\">@<span\
          \ class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \  Hi, I have the same problem and tried with skip_special_tokens=False\
          \ but the prompt is still printed, is there any changes to the instruction?</p>\n\
          </blockquote>\n<p>If it's just generating the endoftext token i.e. it does\
          \ not generate anything and changing the prompt does not help you can enforce\
          \ a minimum number of tokens by setting <code>min_new_tokens</code> to a\
          \ value larger than 0, which will just ignore the endoftext token for x\
          \ number of tokens.</p>\n"
        raw: "> > It's not returning the input text, but directly finishing the generation\
          \ after your prompt. You can see that by setting `skip_special_tokens=False`.\n\
          > > \n> > It's best to have the instruction after the context, see the below\
          \ generation I got with bloomz-3b:\n> > \n> > ```python\n> > inputs = tokenizer.encode(f'{text}\\\
          n\\nWrite a short summary for the prior text.', return_tensors=\"pt\").to(device)\n\
          > > outputs = model.generate(inputs, max_new_tokens=64)\n> > \n> > summary\
          \ = tokenizer.decode(outputs[0], skip_special_tokens=False)\n> > \n> > print(summary)\n\
          > > ```\n> > >> Learn about different types of clouds.\n> \n> @Muennighoff\
          \  Hi, I have the same problem and tried with skip_special_tokens=False\
          \ but the prompt is still printed, is there any changes to the instruction?\n\
          \nIf it's just generating the endoftext token i.e. it does not generate\
          \ anything and changing the prompt does not help you can enforce a minimum\
          \ number of tokens by setting `min_new_tokens` to a value larger than 0,\
          \ which will just ignore the endoftext token for x number of tokens."
        updatedAt: '2023-07-02T19:30:00.089Z'
      numEdits: 0
      reactions: []
    id: 64a1d03834b46c4f731d1ac7
    type: comment
  author: Muennighoff
  content: "> > It's not returning the input text, but directly finishing the generation\
    \ after your prompt. You can see that by setting `skip_special_tokens=False`.\n\
    > > \n> > It's best to have the instruction after the context, see the below generation\
    \ I got with bloomz-3b:\n> > \n> > ```python\n> > inputs = tokenizer.encode(f'{text}\\\
    n\\nWrite a short summary for the prior text.', return_tensors=\"pt\").to(device)\n\
    > > outputs = model.generate(inputs, max_new_tokens=64)\n> > \n> > summary = tokenizer.decode(outputs[0],\
    \ skip_special_tokens=False)\n> > \n> > print(summary)\n> > ```\n> > >> Learn\
    \ about different types of clouds.\n> \n> @Muennighoff  Hi, I have the same problem\
    \ and tried with skip_special_tokens=False but the prompt is still printed, is\
    \ there any changes to the instruction?\n\nIf it's just generating the endoftext\
    \ token i.e. it does not generate anything and changing the prompt does not help\
    \ you can enforce a minimum number of tokens by setting `min_new_tokens` to a\
    \ value larger than 0, which will just ignore the endoftext token for x number\
    \ of tokens."
  created_at: 2023-07-02 18:30:00+00:00
  edited: false
  hidden: false
  id: 64a1d03834b46c4f731d1ac7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94cf041d3263b5b6e8d63d95aab75032.svg
      fullname: lixn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lixiqi
      type: user
    createdAt: '2023-07-03T12:03:40.000Z'
    data:
      edited: false
      editors:
      - lixiqi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9350663423538208
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94cf041d3263b5b6e8d63d95aab75032.svg
          fullname: lixn
          isHf: false
          isPro: false
          name: lixiqi
          type: user
        html: '<p>Thank you. Sorry I didn''t describe my problem clearly. In my case,
          the output is prompt + generate,<br>for example the output is<br>"here is
          an example.<br>Generate a summmary: This is an example.<br>Test sentence
          .....<br>Generate a summmary: This is a test sentence."</p>

          <p>What I want is only :"This is a test sentence."</p>

          '
        raw: "Thank you. Sorry I didn't describe my problem clearly. In my case, the\
          \ output is prompt + generate, \nfor example the output is\n\"here is an\
          \ example.\nGenerate a summmary: This is an example.\nTest sentence .....\n\
          Generate a summmary: This is a test sentence.\"\n\nWhat I want is only :\"\
          This is a test sentence.\""
        updatedAt: '2023-07-03T12:03:40.323Z'
      numEdits: 0
      reactions: []
    id: 64a2b91c416f35492dccf654
    type: comment
  author: lixiqi
  content: "Thank you. Sorry I didn't describe my problem clearly. In my case, the\
    \ output is prompt + generate, \nfor example the output is\n\"here is an example.\n\
    Generate a summmary: This is an example.\nTest sentence .....\nGenerate a summmary:\
    \ This is a test sentence.\"\n\nWhat I want is only :\"This is a test sentence.\""
  created_at: 2023-07-03 11:03:40+00:00
  edited: false
  hidden: false
  id: 64a2b91c416f35492dccf654
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-07-03T16:01:28.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8838800191879272
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>Oh that is just the default behavior of generate to also return
          the input prompt - the model is not actually generating that part.<br>You
          can just remove it by doing sth like <code>gen = gen[len(prompt):]</code></p>

          '
        raw: 'Oh that is just the default behavior of generate to also return the
          input prompt - the model is not actually generating that part.

          You can just remove it by doing sth like `gen = gen[len(prompt):]`'
        updatedAt: '2023-07-03T16:01:28.194Z'
      numEdits: 0
      reactions: []
    id: 64a2f0d8242d9108870ac48f
    type: comment
  author: Muennighoff
  content: 'Oh that is just the default behavior of generate to also return the input
    prompt - the model is not actually generating that part.

    You can just remove it by doing sth like `gen = gen[len(prompt):]`'
  created_at: 2023-07-03 15:01:28+00:00
  edited: false
  hidden: false
  id: 64a2f0d8242d9108870ac48f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94cf041d3263b5b6e8d63d95aab75032.svg
      fullname: lixn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lixiqi
      type: user
    createdAt: '2023-07-03T17:30:08.000Z'
    data:
      edited: false
      editors:
      - lixiqi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9612450003623962
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94cf041d3263b5b6e8d63d95aab75032.svg
          fullname: lixn
          isHf: false
          isPro: false
          name: lixiqi
          type: user
        html: '<p>Thank you for your answer! it helps. :)<br>another question:  if
          the example is in English and the test sentence in another language, is
          there any method to solve the problem that the generation is always in English?</p>

          '
        raw: 'Thank you for your answer! it helps. :)

          another question:  if the example is in English and the test sentence in
          another language, is there any method to solve the problem that the generation
          is always in English?'
        updatedAt: '2023-07-03T17:30:08.811Z'
      numEdits: 0
      reactions: []
    id: 64a305a03caf4e0bb98af3ad
    type: comment
  author: lixiqi
  content: 'Thank you for your answer! it helps. :)

    another question:  if the example is in English and the test sentence in another
    language, is there any method to solve the problem that the generation is always
    in English?'
  created_at: 2023-07-03 16:30:08+00:00
  edited: false
  hidden: false
  id: 64a305a03caf4e0bb98af3ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-07-03T21:31:57.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6859984397888184
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>Explicitly specifying the language may work, e.g. <code>Please reply
          in Japanese.</code></p>

          '
        raw: Explicitly specifying the language may work, e.g. `Please reply in Japanese.`
        updatedAt: '2023-07-03T21:31:57.204Z'
      numEdits: 0
      reactions: []
    id: 64a33e4d57b750c563ef3f64
    type: comment
  author: Muennighoff
  content: Explicitly specifying the language may work, e.g. `Please reply in Japanese.`
  created_at: 2023-07-03 20:31:57+00:00
  edited: false
  hidden: false
  id: 64a33e4d57b750c563ef3f64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94cf041d3263b5b6e8d63d95aab75032.svg
      fullname: lixn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lixiqi
      type: user
    createdAt: '2023-07-03T21:57:56.000Z'
    data:
      edited: false
      editors:
      - lixiqi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9328470826148987
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94cf041d3263b5b6e8d63d95aab75032.svg
          fullname: lixn
          isHf: false
          isPro: false
          name: lixiqi
          type: user
        html: '<p>I will try this method, thank you for your help</p>

          '
        raw: I will try this method, thank you for your help
        updatedAt: '2023-07-03T21:57:56.466Z'
      numEdits: 0
      reactions: []
    id: 64a34464030aaaf6f73acd2b
    type: comment
  author: lixiqi
  content: I will try this method, thank you for your help
  created_at: 2023-07-03 20:57:56+00:00
  edited: false
  hidden: false
  id: 64a34464030aaaf6f73acd2b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 40
repo_id: bigscience/bloomz
repo_type: model
status: open
target_branch: null
title: Bloomz-3b Refuses to Summarize Text
