!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JHenzi
conflicting_files: null
created_at: 2023-04-14 12:53:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9f8c887d68e6686cda9cc3c1b9f8945.svg
      fullname: Joseph Henzi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JHenzi
      type: user
    createdAt: '2023-04-14T13:53:18.000Z'
    data:
      edited: false
      editors:
      - JHenzi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9f8c887d68e6686cda9cc3c1b9f8945.svg
          fullname: Joseph Henzi
          isHf: false
          isPro: false
          name: JHenzi
          type: user
        html: '<p>I''m using BLOOMZ to generate text and intend or desire it to be
          "unending" in that I want to suppress the stop token. I can easily remove
          it from the result text but I''m also storing the output''s tensor data.
          I was hoping there was a way to remove the stop token from the results without
          re-encoding the text. </p>

          <p>For example; </p>

          <ol>

          <li>Encode input text into tensors</li>

          <li>Generate text (results_text)</li>

          <li>Remove input text from results_text</li>

          <li>Save results_text + input tensors (at this step)</li>

          </ol>

          <p>In step <a href="/bigscience/bloomz/discussions/4">#4</a> I want to remove
          the stop token from the input tensors I have collected in my variable. But
          my mind tells me I need to decode, remove it, re-encode it when I''m sure
          there is a way to represent the tensor or remove it from my variable another
          way?</p>

          '
        raw: "I'm using BLOOMZ to generate text and intend or desire it to be \"unending\"\
          \ in that I want to suppress the stop token. I can easily remove it from\
          \ the result text but I'm also storing the output's tensor data. I was hoping\
          \ there was a way to remove the stop token from the results without re-encoding\
          \ the text. \r\n\r\nFor example; \r\n1. Encode input text into tensors\r\
          \n2. Generate text (results_text)\r\n3. Remove input text from results_text\r\
          \n4. Save results_text + input tensors (at this step)\r\n\r\nIn step #4\
          \ I want to remove the stop token from the input tensors I have collected\
          \ in my variable. But my mind tells me I need to decode, remove it, re-encode\
          \ it when I'm sure there is a way to represent the tensor or remove it from\
          \ my variable another way?\r\n"
        updatedAt: '2023-04-14T13:53:18.283Z'
      numEdits: 0
      reactions: []
    id: 64395ace3ab54fdbab7f180f
    type: comment
  author: JHenzi
  content: "I'm using BLOOMZ to generate text and intend or desire it to be \"unending\"\
    \ in that I want to suppress the stop token. I can easily remove it from the result\
    \ text but I'm also storing the output's tensor data. I was hoping there was a\
    \ way to remove the stop token from the results without re-encoding the text.\
    \ \r\n\r\nFor example; \r\n1. Encode input text into tensors\r\n2. Generate text\
    \ (results_text)\r\n3. Remove input text from results_text\r\n4. Save results_text\
    \ + input tensors (at this step)\r\n\r\nIn step #4 I want to remove the stop token\
    \ from the input tensors I have collected in my variable. But my mind tells me\
    \ I need to decode, remove it, re-encode it when I'm sure there is a way to represent\
    \ the tensor or remove it from my variable another way?\r\n"
  created_at: 2023-04-14 12:53:18+00:00
  edited: false
  hidden: false
  id: 64395ace3ab54fdbab7f180f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9f8c887d68e6686cda9cc3c1b9f8945.svg
      fullname: Joseph Henzi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JHenzi
      type: user
    createdAt: '2023-04-14T14:14:23.000Z'
    data:
      edited: true
      editors:
      - JHenzi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9f8c887d68e6686cda9cc3c1b9f8945.svg
          fullname: Joseph Henzi
          isHf: false
          isPro: false
          name: JHenzi
          type: user
        html: '<p><del>Silly me, I used <code>skip_special_tokens=True</code> to achieve
          what I was after.</del></p>

          <p>If I skip special tokens I get a stop and repeated input text in my output.</p>

          <pre><code>Prompt + Additions + Stop Token + Prompt + NEW TEXT

          </code></pre>

          '
        raw: '~~Silly me, I used `skip_special_tokens=True` to achieve what I was
          after.~~


          If I skip special tokens I get a stop and repeated input text in my output.

          ```

          Prompt + Additions + Stop Token + Prompt + NEW TEXT

          ```'
        updatedAt: '2023-04-14T15:09:10.969Z'
      numEdits: 1
      reactions: []
    id: 64395fbf3da4490578d5c9b4
    type: comment
  author: JHenzi
  content: '~~Silly me, I used `skip_special_tokens=True` to achieve what I was after.~~


    If I skip special tokens I get a stop and repeated input text in my output.

    ```

    Prompt + Additions + Stop Token + Prompt + NEW TEXT

    ```'
  created_at: 2023-04-14 13:14:23+00:00
  edited: true
  hidden: false
  id: 64395fbf3da4490578d5c9b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-04-14T15:33:59.000Z'
    data:
      edited: true
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<p>Do you want the model to not stop?<br>In that case you can set <code>min_new_tokens=X</code>\
          \ in <code>model.generate</code> so it generates for your desired length.\
          \ \U0001F44D<br>Or do you want to remove the stop token from the generation?<br>If\
          \ so setting <code>skip_special_tokens=True</code> is the way to go</p>\n"
        raw: "Do you want the model to not stop?\nIn that case you can set `min_new_tokens=X`\
          \ in `model.generate` so it generates for your desired length. \U0001F44D\
          \nOr do you want to remove the stop token from the generation?\nIf so setting\
          \ `skip_special_tokens=True` is the way to go"
        updatedAt: '2023-04-14T16:52:01.691Z'
      numEdits: 1
      reactions: []
    id: 643972675813f0fdd852df1c
    type: comment
  author: Muennighoff
  content: "Do you want the model to not stop?\nIn that case you can set `min_new_tokens=X`\
    \ in `model.generate` so it generates for your desired length. \U0001F44D\nOr\
    \ do you want to remove the stop token from the generation?\nIf so setting `skip_special_tokens=True`\
    \ is the way to go"
  created_at: 2023-04-14 14:33:59+00:00
  edited: true
  hidden: false
  id: 643972675813f0fdd852df1c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9f8c887d68e6686cda9cc3c1b9f8945.svg
      fullname: Joseph Henzi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JHenzi
      type: user
    createdAt: '2023-04-14T15:43:21.000Z'
    data:
      edited: false
      editors:
      - JHenzi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9f8c887d68e6686cda9cc3c1b9f8945.svg
          fullname: Joseph Henzi
          isHf: false
          isPro: false
          name: JHenzi
          type: user
        html: '<p>I''m really looking for open-ended fragments, but when it decides
          to stop it stops. I''m using <code>max_length</code> now, I''ve not tried
          <code>max_new_tokens</code> on generate yet. Seems I might still accidentally
          reach the end if the new tokens/length is less than what the model considers
          ''done''.</p>

          '
        raw: I'm really looking for open-ended fragments, but when it decides to stop
          it stops. I'm using `max_length` now, I've not tried `max_new_tokens` on
          generate yet. Seems I might still accidentally reach the end if the new
          tokens/length is less than what the model considers 'done'.
        updatedAt: '2023-04-14T15:43:21.734Z'
      numEdits: 0
      reactions: []
    id: 64397499cc228b8099b2cb1a
    type: comment
  author: JHenzi
  content: I'm really looking for open-ended fragments, but when it decides to stop
    it stops. I'm using `max_length` now, I've not tried `max_new_tokens` on generate
    yet. Seems I might still accidentally reach the end if the new tokens/length is
    less than what the model considers 'done'.
  created_at: 2023-04-14 14:43:21+00:00
  edited: false
  hidden: false
  id: 64397499cc228b8099b2cb1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-04-14T16:52:24.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>Ah I meant <code>min_new_tokens</code>, sorry - I.e. you can set
          <code>min_new_tokens</code> to force a minimum number of generated tokens
          during which the eos token is ignored.</p>

          '
        raw: Ah I meant `min_new_tokens`, sorry - I.e. you can set `min_new_tokens`
          to force a minimum number of generated tokens during which the eos token
          is ignored.
        updatedAt: '2023-04-14T16:52:24.816Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - JHenzi
    id: 643984c8a6b2f278af727c85
    type: comment
  author: Muennighoff
  content: Ah I meant `min_new_tokens`, sorry - I.e. you can set `min_new_tokens`
    to force a minimum number of generated tokens during which the eos token is ignored.
  created_at: 2023-04-14 15:52:24+00:00
  edited: false
  hidden: false
  id: 643984c8a6b2f278af727c85
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 41
repo_id: bigscience/bloomz
repo_type: model
status: open
target_branch: null
title: Removing Stop Token from Decode/Generate
