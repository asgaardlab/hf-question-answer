!!python/object:huggingface_hub.community.DiscussionWithDetails
author: borzunov
conflicting_files: null
created_at: 2023-01-18 02:03:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
      fullname: Alexander Borzunov
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: borzunov
      type: user
    createdAt: '2023-01-18T02:03:24.000Z'
    data:
      edited: true
      editors:
      - borzunov
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
          fullname: Alexander Borzunov
          isHf: false
          isPro: false
          name: borzunov
          type: user
        html: '<p>Now you can run inference and fine-tune BLOOMZ (the 176B English
          version) using <a rel="nofollow" href="https://github.com/bigscience-workshop/petals">the
          Petals swarm</a>.</p>

          <p>You can use BLOOMZ via <a rel="nofollow" href="https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing">this
          Colab notebook</a> to get the inference speed of <strong>1-2 sec/token</strong>
          for a single sequence. Running the notebook on a local machine is also fine,
          you''d need only 10+ GB GPU memory or 12+ GB RAM (though it will be slower
          without a GPU).</p>

          <p>Note: Don''t forget to replace <code>bigscience/bloom-petals</code> with
          <code>bigscience/bloomz-petals</code> in the model name.</p>

          <p>As an example, there is a <a rel="nofollow" href="http://chat.petals.ml">chatbot
          app</a> running BLOOMZ this way.</p>

          <p>Sorry for some cross-posting but I really hope this may be useful, given
          that the free inference API is not available right now.</p>

          '
        raw: 'Now you can run inference and fine-tune BLOOMZ (the 176B English version)
          using [the Petals swarm](https://github.com/bigscience-workshop/petals).


          You can use BLOOMZ via [this Colab notebook](https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing)
          to get the inference speed of **1-2 sec/token** for a single sequence. Running
          the notebook on a local machine is also fine, you''d need only 10+ GB GPU
          memory or 12+ GB RAM (though it will be slower without a GPU).


          Note: Don''t forget to replace `bigscience/bloom-petals` with `bigscience/bloomz-petals`
          in the model name.


          As an example, there is a [chatbot app](http://chat.petals.ml) running BLOOMZ
          this way.


          Sorry for some cross-posting but I really hope this may be useful, given
          that the free inference API is not available right now.'
        updatedAt: '2023-01-18T02:03:56.488Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - Muennighoff
        - MariaPauley
        - ioanniskarkanias
    id: 63c7536c50cc81901dacd5c0
    type: comment
  author: borzunov
  content: 'Now you can run inference and fine-tune BLOOMZ (the 176B English version)
    using [the Petals swarm](https://github.com/bigscience-workshop/petals).


    You can use BLOOMZ via [this Colab notebook](https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing)
    to get the inference speed of **1-2 sec/token** for a single sequence. Running
    the notebook on a local machine is also fine, you''d need only 10+ GB GPU memory
    or 12+ GB RAM (though it will be slower without a GPU).


    Note: Don''t forget to replace `bigscience/bloom-petals` with `bigscience/bloomz-petals`
    in the model name.


    As an example, there is a [chatbot app](http://chat.petals.ml) running BLOOMZ
    this way.


    Sorry for some cross-posting but I really hope this may be useful, given that
    the free inference API is not available right now.'
  created_at: 2023-01-18 02:03:24+00:00
  edited: true
  hidden: false
  id: 63c7536c50cc81901dacd5c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9f8c887d68e6686cda9cc3c1b9f8945.svg
      fullname: Joseph Henzi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JHenzi
      type: user
    createdAt: '2023-04-14T13:00:11.000Z'
    data:
      edited: false
      editors:
      - JHenzi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9f8c887d68e6686cda9cc3c1b9f8945.svg
          fullname: Joseph Henzi
          isHf: false
          isPro: false
          name: JHenzi
          type: user
        html: '<p>Hey there - curious about this setup. I''m running inference on
          a smaller version of the model but could fit the notebook in memory. Is
          it truly collaborative in that I can add to latent processing when I''m
          not directly running inference? Confused on the petals goal/arch.</p>

          '
        raw: Hey there - curious about this setup. I'm running inference on a smaller
          version of the model but could fit the notebook in memory. Is it truly collaborative
          in that I can add to latent processing when I'm not directly running inference?
          Confused on the petals goal/arch.
        updatedAt: '2023-04-14T13:00:11.134Z'
      numEdits: 0
      reactions: []
    id: 64394e5beb983fe610bddb4b
    type: comment
  author: JHenzi
  content: Hey there - curious about this setup. I'm running inference on a smaller
    version of the model but could fit the notebook in memory. Is it truly collaborative
    in that I can add to latent processing when I'm not directly running inference?
    Confused on the petals goal/arch.
  created_at: 2023-04-14 12:00:11+00:00
  edited: false
  hidden: false
  id: 64394e5beb983fe610bddb4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
      fullname: Alexander Borzunov
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: borzunov
      type: user
    createdAt: '2023-04-14T15:55:06.000Z'
    data:
      edited: false
      editors:
      - borzunov
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
          fullname: Alexander Borzunov
          isHf: false
          isPro: false
          name: borzunov
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;JHenzi&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/JHenzi\">@<span class=\"\
          underline\">JHenzi</span></a></span>\n\n\t</span></span>,</p>\n<p>Yes, Petals\
          \ is truly collaborative - you can connect your GPU and increase its capacity,\
          \ as described in our GitHub readme: <a rel=\"nofollow\" href=\"https://github.com/bigscience-workshop/petals#connect-your-gpu-and-increase-petals-capacity\"\
          >https://github.com/bigscience-workshop/petals#connect-your-gpu-and-increase-petals-capacity</a></p>\n\
          <p>The Petals goal is to give a way to run 100B+ language models without\
          \ having a GPU cluster. Instead, you can load a small part of the model,\
          \ then team up with people serving the other parts to run inference or fine-tuning.\
          \ See the arch details in our paper: <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2209.01188\"\
          >https://arxiv.org/abs/2209.01188</a></p>\n"
        raw: 'Hi @JHenzi,


          Yes, Petals is truly collaborative - you can connect your GPU and increase
          its capacity, as described in our GitHub readme: https://github.com/bigscience-workshop/petals#connect-your-gpu-and-increase-petals-capacity


          The Petals goal is to give a way to run 100B+ language models without having
          a GPU cluster. Instead, you can load a small part of the model, then team
          up with people serving the other parts to run inference or fine-tuning.
          See the arch details in our paper: https://arxiv.org/abs/2209.01188'
        updatedAt: '2023-04-14T15:55:06.621Z'
      numEdits: 0
      reactions: []
    id: 6439775a0cb95b3dbc8f91d3
    type: comment
  author: borzunov
  content: 'Hi @JHenzi,


    Yes, Petals is truly collaborative - you can connect your GPU and increase its
    capacity, as described in our GitHub readme: https://github.com/bigscience-workshop/petals#connect-your-gpu-and-increase-petals-capacity


    The Petals goal is to give a way to run 100B+ language models without having a
    GPU cluster. Instead, you can load a small part of the model, then team up with
    people serving the other parts to run inference or fine-tuning. See the arch details
    in our paper: https://arxiv.org/abs/2209.01188'
  created_at: 2023-04-14 14:55:06+00:00
  edited: false
  hidden: false
  id: 6439775a0cb95b3dbc8f91d3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 28
repo_id: bigscience/bloomz
repo_type: model
status: open
target_branch: null
title: A way to inference and fine-tune BLOOMZ-176B from Google Colab or locally
