!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pohunghuang
conflicting_files: null
created_at: 2022-11-18 09:53:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
      fullname: Paul-PH Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pohunghuang
      type: user
    createdAt: '2022-11-18T09:53:48.000Z'
    data:
      edited: false
      editors:
      - pohunghuang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
          fullname: Paul-PH Huang
          isHf: false
          isPro: false
          name: pohunghuang
          type: user
        html: '<p>I don''t know if any other ever tried bloomz-7b1 (<a href="https://huggingface.co/bigscience/bloomz-7b1">https://huggingface.co/bigscience/bloomz-7b1</a>)
          and got same feeling with me.<br>Especially greedy mode, it seems output
          nothing all the time, or just output one or two tokens. </p>

          '
        raw: "I don't know if any other ever tried bloomz-7b1 (https://huggingface.co/bigscience/bloomz-7b1)\
          \ and got same feeling with me. \r\nEspecially greedy mode, it seems output\
          \ nothing all the time, or just output one or two tokens. "
        updatedAt: '2022-11-18T09:53:48.984Z'
      numEdits: 0
      reactions: []
    id: 6377562ccc034ef804cfe83d
    type: comment
  author: pohunghuang
  content: "I don't know if any other ever tried bloomz-7b1 (https://huggingface.co/bigscience/bloomz-7b1)\
    \ and got same feeling with me. \r\nEspecially greedy mode, it seems output nothing\
    \ all the time, or just output one or two tokens. "
  created_at: 2022-11-18 09:53:48+00:00
  edited: false
  hidden: false
  id: 6377562ccc034ef804cfe83d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-18T10:30:47.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<blockquote>

          <p>I don''t know if any other ever tried bloomz-7b1 (<a href="https://huggingface.co/bigscience/bloomz-7b1">https://huggingface.co/bigscience/bloomz-7b1</a>)
          and got same feeling with me.<br>Especially greedy mode, it seems output
          nothing all the time, or just output one or two tokens.</p>

          </blockquote>

          <p>That sounds surprising, can you provide some examples you tried and the
          generated tokens?</p>

          '
        raw: "> I don't know if any other ever tried bloomz-7b1 (https://huggingface.co/bigscience/bloomz-7b1)\
          \ and got same feeling with me. \n> Especially greedy mode, it seems output\
          \ nothing all the time, or just output one or two tokens.\n\nThat sounds\
          \ surprising, can you provide some examples you tried and the generated\
          \ tokens?"
        updatedAt: '2022-11-18T10:30:47.167Z'
      numEdits: 0
      reactions: []
    id: 63775ed767850c314b730a36
    type: comment
  author: Muennighoff
  content: "> I don't know if any other ever tried bloomz-7b1 (https://huggingface.co/bigscience/bloomz-7b1)\
    \ and got same feeling with me. \n> Especially greedy mode, it seems output nothing\
    \ all the time, or just output one or two tokens.\n\nThat sounds surprising, can\
    \ you provide some examples you tried and the generated tokens?"
  created_at: 2022-11-18 10:30:47+00:00
  edited: false
  hidden: false
  id: 63775ed767850c314b730a36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
      fullname: Paul-PH Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pohunghuang
      type: user
    createdAt: '2022-11-20T08:36:15.000Z'
    data:
      edited: true
      editors:
      - pohunghuang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
          fullname: Paul-PH Huang
          isHf: false
          isPro: false
          name: pohunghuang
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \  's feedback</p>\n<p>Allow me make some correction statements:<br>(1)\
          \ After clarifying again, I found It's good if I do inference through the\
          \ web page. Even the answers are not totally identical to BLOOMZ-1176b,\
          \ but I thought it's good enough to zero-shot.<br>(2) But if I load model\
          \ downloaded from here, <a href=\"https://huggingface.co/bigscience/bloomz-7b1/tree/main\"\
          >https://huggingface.co/bigscience/bloomz-7b1/tree/main</a> , and do inference\
          \ by load it in local, then output nothing else than input.<br>    (2.1)\
          \ 3 samples I tried. \"Write a code snippet with O(log(n)) computational\
          \ complexity.\", \"Why is sky blue?\", \"Translate to English: Je t\u2019\
          aime.\",<br>    (2.2) greedy mode used by set up sample=False, and max_new_tokens=100<br>\
          \    (2.3) by observing the outputs, actually 100 new tokens truly inferenced,\
          \ but all of them are index of 0, so actually the output text is identical\
          \ to input (because skip_special_tokens=True)<br>(3) The model launched\
          \ following instruction, and so as inference:<br>    \"model = AutoModelForCausalLM.from_pretrained(checkpoint,\
          \ torch_dtype=\"auto\", device_map=\"auto\")\"<br>    \"outputs = model.generate(inputs)\"\
          \  &lt;== here I add one parameter, \"sample=False\"<br>(4) The experiment\
          \ running on docker containers image from dockerhub repo: tag  = huggingface/transformers-pytorch-gpu:latest<br>(5)\
          \ I guess the checkpoint file used by web page (hosted inference api) may\
          \ not be identical to the latest version in repos (<a href=\"https://huggingface.co/bigscience/bloomz-7b1/tree/main\"\
          >https://huggingface.co/bigscience/bloomz-7b1/tree/main</a>)</p>\n"
        raw: "Thanks @Muennighoff  's feedback\n\nAllow me make some correction statements:\n\
          (1) After clarifying again, I found It's good if I do inference through\
          \ the web page. Even the answers are not totally identical to BLOOMZ-1176b,\
          \ but I thought it's good enough to zero-shot. \n(2) But if I load model\
          \ downloaded from here, https://huggingface.co/bigscience/bloomz-7b1/tree/main\
          \ , and do inference by load it in local, then output nothing else than\
          \ input. \n    (2.1) 3 samples I tried. \"Write a code snippet with O(log(n))\
          \ computational complexity.\", \"Why is sky blue?\", \"Translate to English:\
          \ Je t\u2019aime.\", \n    (2.2) greedy mode used by set up sample=False,\
          \ and max_new_tokens=100\n    (2.3) by observing the outputs, actually 100\
          \ new tokens truly inferenced, but all of them are index of 0, so actually\
          \ the output text is identical to input (because skip_special_tokens=True)\n\
          (3) The model launched following instruction, and so as inference:\n   \
          \ \"model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"\
          auto\", device_map=\"auto\")\"  \n    \"outputs = model.generate(inputs)\"\
          \  <== here I add one parameter, \"sample=False\"\n(4) The experiment running\
          \ on docker containers image from dockerhub repo: tag  = huggingface/transformers-pytorch-gpu:latest\n\
          (5) I guess the checkpoint file used by web page (hosted inference api)\
          \ may not be identical to the latest version in repos (https://huggingface.co/bigscience/bloomz-7b1/tree/main)"
        updatedAt: '2022-11-20T08:43:01.241Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pai4451
    id: 6379e6ffbdf7309aa6d3076a
    type: comment
  author: pohunghuang
  content: "Thanks @Muennighoff  's feedback\n\nAllow me make some correction statements:\n\
    (1) After clarifying again, I found It's good if I do inference through the web\
    \ page. Even the answers are not totally identical to BLOOMZ-1176b, but I thought\
    \ it's good enough to zero-shot. \n(2) But if I load model downloaded from here,\
    \ https://huggingface.co/bigscience/bloomz-7b1/tree/main , and do inference by\
    \ load it in local, then output nothing else than input. \n    (2.1) 3 samples\
    \ I tried. \"Write a code snippet with O(log(n)) computational complexity.\",\
    \ \"Why is sky blue?\", \"Translate to English: Je t\u2019aime.\", \n    (2.2)\
    \ greedy mode used by set up sample=False, and max_new_tokens=100\n    (2.3) by\
    \ observing the outputs, actually 100 new tokens truly inferenced, but all of\
    \ them are index of 0, so actually the output text is identical to input (because\
    \ skip_special_tokens=True)\n(3) The model launched following instruction, and\
    \ so as inference:\n    \"model = AutoModelForCausalLM.from_pretrained(checkpoint,\
    \ torch_dtype=\"auto\", device_map=\"auto\")\"  \n    \"outputs = model.generate(inputs)\"\
    \  <== here I add one parameter, \"sample=False\"\n(4) The experiment running\
    \ on docker containers image from dockerhub repo: tag  = huggingface/transformers-pytorch-gpu:latest\n\
    (5) I guess the checkpoint file used by web page (hosted inference api) may not\
    \ be identical to the latest version in repos (https://huggingface.co/bigscience/bloomz-7b1/tree/main)"
  created_at: 2022-11-20 08:36:15+00:00
  edited: true
  hidden: false
  id: 6379e6ffbdf7309aa6d3076a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-21T06:03:51.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<p>On my side everything works fine:</p>\n<blockquote>\n<blockquote>\n\
          <blockquote>\n<p>from transformers import AutoModelForCausalLM, AutoTokenizer<br>checkpoint\
          \ = \"bigscience/bloomz-7b1\"<br>tokenizer = AutoTokenizer.from_pretrained(checkpoint)<br>model\
          \ = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\"\
          , device_map=\"auto\")<br>inputs = tokenizer.encode(\"Translate to English:\
          \ Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")<br>outputs = model.generate(inputs,\
          \ do_sample=False)<br>print(tokenizer.decode(outputs[0]))<br>Translate to\
          \ English: Je t\u2019aime. I love you.<br>inputs = tokenizer.encode(\"Write\
          \ a code snippet with O(log(n)) computational complexity.\", return_tensors=\"\
          pt\").to(\"cuda\")<br>outputs = model.generate(inputs, do_sample=False)<br>print(tokenizer.decode(outputs[0]))<br>Write\
          \ a code snippet with O(log(n)) computational complexity. def solve(n):<br>outputs\
          \ = model.generate(inputs, do_sample=False, max_length=100)<br>print(tokenizer.decode(outputs[0]))<br>Write\
          \ a code snippet with O(log(n)) computational complexity. def solve(n):<br>\
          \    if n == 1:<br>        return 1<br>    return solve(n-1) + solve(n-2)</p>\n\
          </blockquote>\n</blockquote>\n</blockquote>\n<p>Can you double-chech you're\
          \ passing <code>do_sample</code> not <code>sample</code>?<br>Also make sure\
          \ you use the latest transformers version if you aren't</p>\n"
        raw: "On my side everything works fine:\n\n>>> from transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n>>> checkpoint = \"bigscience/bloomz-7b1\"\n>>> tokenizer\
          \ = AutoTokenizer.from_pretrained(checkpoint)\n>>> model = AutoModelForCausalLM.from_pretrained(checkpoint,\
          \ torch_dtype=\"auto\", device_map=\"auto\")\n>>> inputs = tokenizer.encode(\"\
          Translate to English: Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\"\
          )\n>>> outputs = model.generate(inputs, do_sample=False)\n>>> print(tokenizer.decode(outputs[0]))\n\
          Translate to English: Je t\u2019aime. I love you.</s>\n>>> inputs = tokenizer.encode(\"\
          Write a code snippet with O(log(n)) computational complexity.\", return_tensors=\"\
          pt\").to(\"cuda\")\n>>> outputs = model.generate(inputs, do_sample=False)\n\
          >>> print(tokenizer.decode(outputs[0]))\nWrite a code snippet with O(log(n))\
          \ computational complexity. def solve(n):\n>>> outputs = model.generate(inputs,\
          \ do_sample=False, max_length=100)\n>>> print(tokenizer.decode(outputs[0]))\n\
          Write a code snippet with O(log(n)) computational complexity. def solve(n):\n\
          \    if n == 1:\n        return 1\n    return solve(n-1) + solve(n-2)</s>\n\
          \n\nCan you double-chech you're passing `do_sample` not `sample`? \nAlso\
          \ make sure you use the latest transformers version if you aren't"
        updatedAt: '2022-11-21T06:03:51.082Z'
      numEdits: 0
      reactions: []
    id: 637b14c77ce76c3b834df385
    type: comment
  author: Muennighoff
  content: "On my side everything works fine:\n\n>>> from transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\n>>> checkpoint = \"bigscience/bloomz-7b1\"\n>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
    >>> model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\"\
    , device_map=\"auto\")\n>>> inputs = tokenizer.encode(\"Translate to English:\
    \ Je t\u2019aime.\", return_tensors=\"pt\").to(\"cuda\")\n>>> outputs = model.generate(inputs,\
    \ do_sample=False)\n>>> print(tokenizer.decode(outputs[0]))\nTranslate to English:\
    \ Je t\u2019aime. I love you.</s>\n>>> inputs = tokenizer.encode(\"Write a code\
    \ snippet with O(log(n)) computational complexity.\", return_tensors=\"pt\").to(\"\
    cuda\")\n>>> outputs = model.generate(inputs, do_sample=False)\n>>> print(tokenizer.decode(outputs[0]))\n\
    Write a code snippet with O(log(n)) computational complexity. def solve(n):\n\
    >>> outputs = model.generate(inputs, do_sample=False, max_length=100)\n>>> print(tokenizer.decode(outputs[0]))\n\
    Write a code snippet with O(log(n)) computational complexity. def solve(n):\n\
    \    if n == 1:\n        return 1\n    return solve(n-1) + solve(n-2)</s>\n\n\n\
    Can you double-chech you're passing `do_sample` not `sample`? \nAlso make sure\
    \ you use the latest transformers version if you aren't"
  created_at: 2022-11-21 06:03:51+00:00
  edited: false
  hidden: false
  id: 637b14c77ce76c3b834df385
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
      fullname: Paul-PH Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pohunghuang
      type: user
    createdAt: '2022-11-22T01:01:17.000Z'
    data:
      edited: false
      editors:
      - pohunghuang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
          fullname: Paul-PH Huang
          isHf: false
          isPro: false
          name: pohunghuang
          type: user
        html: "<p>Thanks. <span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>,\
          \ it's my typo. Actually what I passed is \"do_sample\".<br>Due to our environment\
          \ is offline, I would download the checkpoints file again to rule out effect\
          \ from incorrected \"pytorch_model.bin\".</p>\n"
        raw: "Thanks. @Muennighoff, it's my typo. Actually what I passed is \"do_sample\"\
          . \nDue to our environment is offline, I would download the checkpoints\
          \ file again to rule out effect from incorrected \"pytorch_model.bin\"."
        updatedAt: '2022-11-22T01:01:17.200Z'
      numEdits: 0
      reactions: []
    id: 637c1f5dc292c0fd3f36a59d
    type: comment
  author: pohunghuang
  content: "Thanks. @Muennighoff, it's my typo. Actually what I passed is \"do_sample\"\
    . \nDue to our environment is offline, I would download the checkpoints file again\
    \ to rule out effect from incorrected \"pytorch_model.bin\"."
  created_at: 2022-11-22 01:01:17+00:00
  edited: false
  hidden: false
  id: 637c1f5dc292c0fd3f36a59d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
      fullname: Paul-PH Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pohunghuang
      type: user
    createdAt: '2022-11-22T04:57:22.000Z'
    data:
      edited: true
      editors:
      - pohunghuang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
          fullname: Paul-PH Huang
          isHf: false
          isPro: false
          name: pohunghuang
          type: user
        html: "<p>I have verified sha256 of checkpoint file and also tokenizer.json,\
          \ but all of them identical to ones shown in <a href=\"https://huggingface.co/bigscience/bloomz-7b1/tree/main\"\
          >https://huggingface.co/bigscience/bloomz-7b1/tree/main</a><br><span data-props=\"\
          {&quot;user&quot;:&quot;Muennighoff&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/Muennighoff\">@<span class=\"underline\"\
          >Muennighoff</span></a></span>\n\n\t</span></span>  could I know the version\
          \ of pytorch and transformers you used in experiment and also the GPU (Nvidia\
          \ A100? or else?) thanks.</p>\n"
        raw: 'I have verified sha256 of checkpoint file and also tokenizer.json, but
          all of them identical to ones shown in https://huggingface.co/bigscience/bloomz-7b1/tree/main

          @Muennighoff  could I know the version of pytorch and transformers you used
          in experiment and also the GPU (Nvidia A100? or else?) thanks.'
        updatedAt: '2022-11-22T05:10:34.188Z'
      numEdits: 1
      reactions: []
    id: 637c56b23d8e2e9c40c20369
    type: comment
  author: pohunghuang
  content: 'I have verified sha256 of checkpoint file and also tokenizer.json, but
    all of them identical to ones shown in https://huggingface.co/bigscience/bloomz-7b1/tree/main

    @Muennighoff  could I know the version of pytorch and transformers you used in
    experiment and also the GPU (Nvidia A100? or else?) thanks.'
  created_at: 2022-11-22 04:57:22+00:00
  edited: true
  hidden: false
  id: 637c56b23d8e2e9c40c20369
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-22T11:55:45.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<blockquote>\n<p>I have verified sha256 of checkpoint file and also\
          \ tokenizer.json, but all of them identical to ones shown in <a href=\"\
          https://huggingface.co/bigscience/bloomz-7b1/tree/main\">https://huggingface.co/bigscience/bloomz-7b1/tree/main</a><br><span\
          \ data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\">@<span\
          \ class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \  could I know the version of pytorch and transformers you used in experiment\
          \ and also the GPU (Nvidia A100? or else?) thanks.</p>\n</blockquote>\n\
          <p>I used the latest versions of transformer (4.24.0) &amp; torch. Esp.\
          \ the transformers version is important as previous ones had bugs in bloom.<br>Nvidia\
          \ A100 w/ 80 GB.</p>\n"
        raw: '> I have verified sha256 of checkpoint file and also tokenizer.json,
          but all of them identical to ones shown in https://huggingface.co/bigscience/bloomz-7b1/tree/main

          > @Muennighoff  could I know the version of pytorch and transformers you
          used in experiment and also the GPU (Nvidia A100? or else?) thanks.


          I used the latest versions of transformer (4.24.0) & torch. Esp. the transformers
          version is important as previous ones had bugs in bloom.

          Nvidia A100 w/ 80 GB.'
        updatedAt: '2022-11-22T11:55:45.658Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pohunghuang
    id: 637cb8c1bb031d2afee27f66
    type: comment
  author: Muennighoff
  content: '> I have verified sha256 of checkpoint file and also tokenizer.json, but
    all of them identical to ones shown in https://huggingface.co/bigscience/bloomz-7b1/tree/main

    > @Muennighoff  could I know the version of pytorch and transformers you used
    in experiment and also the GPU (Nvidia A100? or else?) thanks.


    I used the latest versions of transformer (4.24.0) & torch. Esp. the transformers
    version is important as previous ones had bugs in bloom.

    Nvidia A100 w/ 80 GB.'
  created_at: 2022-11-22 11:55:45+00:00
  edited: false
  hidden: false
  id: 637cb8c1bb031d2afee27f66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
      fullname: Paul-PH Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pohunghuang
      type: user
    createdAt: '2022-11-23T02:09:52.000Z'
    data:
      edited: false
      editors:
      - pohunghuang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
          fullname: Paul-PH Huang
          isHf: false
          isPro: false
          name: pohunghuang
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \  Thanks for your quick feedback. Anyway, problem resolved. The solution\
          \ is simple, just downloaded again whole model folder. It's also said the\
          \ root cause not been found because I have checked each sha256sum of each\
          \ files in model folder, include checkpoint files, configs, and tokenizer\
          \ files, they are all identical. Now I tried bloom-7b1 and bloomz-7b1-mt,\
          \ both of them are good. Thanks again.</p>\n"
        raw: '@Muennighoff  Thanks for your quick feedback. Anyway, problem resolved.
          The solution is simple, just downloaded again whole model folder. It''s
          also said the root cause not been found because I have checked each sha256sum
          of each files in model folder, include checkpoint files, configs, and tokenizer
          files, they are all identical. Now I tried bloom-7b1 and bloomz-7b1-mt,
          both of them are good. Thanks again.'
        updatedAt: '2022-11-23T02:09:52.182Z'
      numEdits: 0
      reactions: []
    id: 637d80f0ca8c4cc8d0ec2eff
    type: comment
  author: pohunghuang
  content: '@Muennighoff  Thanks for your quick feedback. Anyway, problem resolved.
    The solution is simple, just downloaded again whole model folder. It''s also said
    the root cause not been found because I have checked each sha256sum of each files
    in model folder, include checkpoint files, configs, and tokenizer files, they
    are all identical. Now I tried bloom-7b1 and bloomz-7b1-mt, both of them are good.
    Thanks again.'
  created_at: 2022-11-23 02:09:52+00:00
  edited: false
  hidden: false
  id: 637d80f0ca8c4cc8d0ec2eff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-23T05:07:44.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \  Thanks for your quick feedback. Anyway, problem resolved. The solution\
          \ is simple, just downloaded again whole model folder. It's also said the\
          \ root cause not been found because I have checked each sha256sum of each\
          \ files in model folder, include checkpoint files, configs, and tokenizer\
          \ files, they are all identical. Now I tried bloom-7b1 and bloomz-7b1-mt,\
          \ both of them are good. Thanks again.</p>\n</blockquote>\n<p>Amazing! \U0001F917\
          \ Closing this for now, feel free to reopen if issues occur.</p>\n"
        raw: "> @Muennighoff  Thanks for your quick feedback. Anyway, problem resolved.\
          \ The solution is simple, just downloaded again whole model folder. It's\
          \ also said the root cause not been found because I have checked each sha256sum\
          \ of each files in model folder, include checkpoint files, configs, and\
          \ tokenizer files, they are all identical. Now I tried bloom-7b1 and bloomz-7b1-mt,\
          \ both of them are good. Thanks again.\n\nAmazing! \U0001F917 Closing this\
          \ for now, feel free to reopen if issues occur."
        updatedAt: '2022-11-23T05:07:44.845Z'
      numEdits: 0
      reactions: []
      relatedEventId: 637daaa06231c36c6376370f
    id: 637daaa06231c36c6376370e
    type: comment
  author: Muennighoff
  content: "> @Muennighoff  Thanks for your quick feedback. Anyway, problem resolved.\
    \ The solution is simple, just downloaded again whole model folder. It's also\
    \ said the root cause not been found because I have checked each sha256sum of\
    \ each files in model folder, include checkpoint files, configs, and tokenizer\
    \ files, they are all identical. Now I tried bloom-7b1 and bloomz-7b1-mt, both\
    \ of them are good. Thanks again.\n\nAmazing! \U0001F917 Closing this for now,\
    \ feel free to reopen if issues occur."
  created_at: 2022-11-23 05:07:44+00:00
  edited: false
  hidden: false
  id: 637daaa06231c36c6376370e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-23T05:07:44.000Z'
    data:
      status: closed
    id: 637daaa06231c36c6376370f
    type: status-change
  author: Muennighoff
  created_at: 2022-11-23 05:07:44+00:00
  id: 637daaa06231c36c6376370f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: bigscience/bloomz
repo_type: model
status: closed
target_branch: null
title: Weird behavior of BLOOMZ-7b1
