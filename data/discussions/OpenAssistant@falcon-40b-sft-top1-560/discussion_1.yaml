!!python/object:huggingface_hub.community.DiscussionWithDetails
author: eastwind
conflicting_files: null
created_at: 2023-06-02 22:34:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-02T23:34:20.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9024143218994141
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p>Just tested the model, looks good. But it seems you have inherited
          an issue from the base falcon. When inferencing over multiple gpus I get
          gibberish unless I pass <code>use_caching=False</code> in the model.generate
          function. Not sure why this happens.</p>

          '
        raw: Just tested the model, looks good. But it seems you have inherited an
          issue from the base falcon. When inferencing over multiple gpus I get gibberish
          unless I pass `use_caching=False` in the model.generate function. Not sure
          why this happens.
        updatedAt: '2023-06-02T23:34:20.870Z'
      numEdits: 0
      reactions: []
    id: 647a7c7c0f9380a25554695e
    type: comment
  author: eastwind
  content: Just tested the model, looks good. But it seems you have inherited an issue
    from the base falcon. When inferencing over multiple gpus I get gibberish unless
    I pass `use_caching=False` in the model.generate function. Not sure why this happens.
  created_at: 2023-06-02 22:34:20+00:00
  edited: false
  hidden: false
  id: 647a7c7c0f9380a25554695e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-06-03T18:52:52.000Z'
    data:
      edited: true
      editors:
      - daryl149
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9659009575843811
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
          fullname: Daryl Autar
          isHf: false
          isPro: false
          name: daryl149
          type: user
        html: '<p>Same issue with the open assistant rlhf llama model on multi gpu.
          I can''t test it right now, but if your flag fixes it, I think it''s a bitsandbytes
          issue. Because for me it only gave the errors with <code>load_in_8bit=true</code></p>

          <p><code>use_cache=False</code> does not fix it for that one :(</p>

          '
        raw: 'Same issue with the open assistant rlhf llama model on multi gpu. I
          can''t test it right now, but if your flag fixes it, I think it''s a bitsandbytes
          issue. Because for me it only gave the errors with `load_in_8bit=true`


          `use_cache=False` does not fix it for that one :('
        updatedAt: '2023-06-03T21:45:06.178Z'
      numEdits: 3
      reactions: []
    id: 647b8c046dbad6ab058151a4
    type: comment
  author: daryl149
  content: 'Same issue with the open assistant rlhf llama model on multi gpu. I can''t
    test it right now, but if your flag fixes it, I think it''s a bitsandbytes issue.
    Because for me it only gave the errors with `load_in_8bit=true`


    `use_cache=False` does not fix it for that one :('
  created_at: 2023-06-03 17:52:52+00:00
  edited: true
  hidden: false
  id: 647b8c046dbad6ab058151a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-04T16:40:00.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9735649228096008
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p>I didn''t use quantisation for the falcon. I just loaded it in 4
          v100s.</p>

          '
        raw: I didn't use quantisation for the falcon. I just loaded it in 4 v100s.
        updatedAt: '2023-06-04T16:40:00.853Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - daryl149
    id: 647cbe601c0644de8d2b9184
    type: comment
  author: eastwind
  content: I didn't use quantisation for the falcon. I just loaded it in 4 v100s.
  created_at: 2023-06-04 15:40:00+00:00
  edited: false
  hidden: false
  id: 647cbe601c0644de8d2b9184
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5f59a50a8f174164979dc72bdf604ed5.svg
      fullname: Avinoam Aharoni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WeWrite
      type: user
    createdAt: '2023-06-10T22:12:34.000Z'
    data:
      edited: false
      editors:
      - WeWrite
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9380922913551331
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5f59a50a8f174164979dc72bdf604ed5.svg
          fullname: Avinoam Aharoni
          isHf: false
          isPro: false
          name: WeWrite
          type: user
        html: '<p>Eastwind? How were you able to harness all GPUs for a single prompt?
          I tried with a 8xA100x80G machine and it only used 1 GPU crashing the model
          for lack of memory.. can you share config and code? Pretty please...? THX.</p>

          '
        raw: Eastwind? How were you able to harness all GPUs for a single prompt?
          I tried with a 8xA100x80G machine and it only used 1 GPU crashing the model
          for lack of memory.. can you share config and code? Pretty please...? THX.
        updatedAt: '2023-06-10T22:12:34.245Z'
      numEdits: 0
      reactions: []
    id: 6484f552ba97953337b75306
    type: comment
  author: WeWrite
  content: Eastwind? How were you able to harness all GPUs for a single prompt? I
    tried with a 8xA100x80G machine and it only used 1 GPU crashing the model for
    lack of memory.. can you share config and code? Pretty please...? THX.
  created_at: 2023-06-10 21:12:34+00:00
  edited: false
  hidden: false
  id: 6484f552ba97953337b75306
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-10T23:13:47.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9602546095848083
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p>I tried doing it with device_map = auto . It works with cache disabled.
          But I think for multigpu as the falcon authors replied to my original post.
          They said to use the huggingface text inference hosting solution. I haven''t
          tested it out however</p>

          '
        raw: I tried doing it with device_map = auto . It works with cache disabled.
          But I think for multigpu as the falcon authors replied to my original post.
          They said to use the huggingface text inference hosting solution. I haven't
          tested it out however
        updatedAt: '2023-06-10T23:13:47.607Z'
      numEdits: 0
      reactions: []
    id: 648503ab33e82e2c992ffc4b
    type: comment
  author: eastwind
  content: I tried doing it with device_map = auto . It works with cache disabled.
    But I think for multigpu as the falcon authors replied to my original post. They
    said to use the huggingface text inference hosting solution. I haven't tested
    it out however
  created_at: 2023-06-10 22:13:47+00:00
  edited: false
  hidden: false
  id: 648503ab33e82e2c992ffc4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5f59a50a8f174164979dc72bdf604ed5.svg
      fullname: Avinoam Aharoni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WeWrite
      type: user
    createdAt: '2023-06-11T04:31:06.000Z'
    data:
      edited: false
      editors:
      - WeWrite
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9857503175735474
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5f59a50a8f174164979dc72bdf604ed5.svg
          fullname: Avinoam Aharoni
          isHf: false
          isPro: false
          name: WeWrite
          type: user
        html: '<p>Thx. Unfortunately that hosting solution is no longer available..
          do you know what was its configuration?</p>

          '
        raw: Thx. Unfortunately that hosting solution is no longer available.. do
          you know what was its configuration?
        updatedAt: '2023-06-11T04:31:06.654Z'
      numEdits: 0
      reactions: []
    id: 64854e0a10e6b87455adc600
    type: comment
  author: WeWrite
  content: Thx. Unfortunately that hosting solution is no longer available.. do you
    know what was its configuration?
  created_at: 2023-06-11 03:31:06+00:00
  edited: false
  hidden: false
  id: 64854e0a10e6b87455adc600
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-11T08:33:43.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9749251008033752
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p>See this issue that I also made lol. I haven''t tested but given
          the fact that they have it working. I would assume it works. <a rel="nofollow"
          href="https://github.com/huggingface/text-generation-inference/issues/417">https://github.com/huggingface/text-generation-inference/issues/417</a></p>

          '
        raw: See this issue that I also made lol. I haven't tested but given the fact
          that they have it working. I would assume it works. https://github.com/huggingface/text-generation-inference/issues/417
        updatedAt: '2023-06-11T08:33:43.078Z'
      numEdits: 0
      reactions: []
    id: 648586e7cebd427dcc7f8288
    type: comment
  author: eastwind
  content: See this issue that I also made lol. I haven't tested but given the fact
    that they have it working. I would assume it works. https://github.com/huggingface/text-generation-inference/issues/417
  created_at: 2023-06-11 07:33:43+00:00
  edited: false
  hidden: false
  id: 648586e7cebd427dcc7f8288
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5f59a50a8f174164979dc72bdf604ed5.svg
      fullname: Avinoam Aharoni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WeWrite
      type: user
    createdAt: '2023-06-11T08:38:23.000Z'
    data:
      edited: false
      editors:
      - WeWrite
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7529019713401794
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5f59a50a8f174164979dc72bdf604ed5.svg
          fullname: Avinoam Aharoni
          isHf: false
          isPro: false
          name: WeWrite
          type: user
        html: '<p>Thanks :-)</p>

          '
        raw: Thanks :-)
        updatedAt: '2023-06-11T08:38:23.640Z'
      numEdits: 0
      reactions: []
    id: 648587ff0ed12e85f8c81caa
    type: comment
  author: WeWrite
  content: Thanks :-)
  created_at: 2023-06-11 07:38:23+00:00
  edited: false
  hidden: false
  id: 648587ff0ed12e85f8c81caa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb57b55883f314fab9d6be9d8a3159b0.svg
      fullname: Vinay Premchandran Nair
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vinwizard
      type: user
    createdAt: '2023-07-05T18:48:16.000Z'
    data:
      edited: false
      editors:
      - vinwizard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9145748615264893
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb57b55883f314fab9d6be9d8a3159b0.svg
          fullname: Vinay Premchandran Nair
          isHf: false
          isPro: false
          name: vinwizard
          type: user
        html: '<blockquote>

          <p>Just tested the model, looks good. But it seems you have inherited an
          issue from the base falcon. When inferencing over multiple gpus I get gibberish
          unless I pass <code>use_caching=False</code> in the model.generate function.
          Not sure why this happens.</p>

          </blockquote>

          <p>Eastwind, could you please tell how fast the inference was? Because for
          small prompts on the V100 also it is taking me a good minute to render the
          response and the longer prompts it is crashing with a CUDA OOM error. </p>

          '
        raw: '> Just tested the model, looks good. But it seems you have inherited
          an issue from the base falcon. When inferencing over multiple gpus I get
          gibberish unless I pass `use_caching=False` in the model.generate function.
          Not sure why this happens.


          Eastwind, could you please tell how fast the inference was? Because for
          small prompts on the V100 also it is taking me a good minute to render the
          response and the longer prompts it is crashing with a CUDA OOM error. '
        updatedAt: '2023-07-05T18:48:16.746Z'
      numEdits: 0
      reactions: []
    id: 64a5baf0e9bfafc70fad58e5
    type: comment
  author: vinwizard
  content: '> Just tested the model, looks good. But it seems you have inherited an
    issue from the base falcon. When inferencing over multiple gpus I get gibberish
    unless I pass `use_caching=False` in the model.generate function. Not sure why
    this happens.


    Eastwind, could you please tell how fast the inference was? Because for small
    prompts on the V100 also it is taking me a good minute to render the response
    and the longer prompts it is crashing with a CUDA OOM error. '
  created_at: 2023-07-05 17:48:16+00:00
  edited: false
  hidden: false
  id: 64a5baf0e9bfafc70fad58e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/dip_4_H7cB_YjkVoyNUhY.png?w=200&h=200&f=face
      fullname: garrett galloway
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: RecViking
      type: user
    createdAt: '2023-07-05T18:56:27.000Z'
    data:
      edited: false
      editors:
      - RecViking
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9615126252174377
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/dip_4_H7cB_YjkVoyNUhY.png?w=200&h=200&f=face
          fullname: garrett galloway
          isHf: false
          isPro: true
          name: RecViking
          type: user
        html: '<blockquote>

          <p>Eastwind, could you please tell how fast the inference was? Because for
          small prompts on the V100 also it is taking me a good minute to render the
          response and the longer prompts it is crashing with a CUDA OOM error.</p>

          </blockquote>

          <p>I''ve noticed Falcon 40B works fine in 16bit (bfloat) mode. When running
          it in 8bit, it runs like garbage and is CPU bound. The performance is HORRIBLE
          in anything but 16/32bit and it''s always CPU bound. Running in 16/32bit,
          it uses my cards sequentially and runs them up to 90% utilization and then
          pops to the next card. I''ve got 3x 48gb A6000 cards. When loaded in 16bit,
          it takes up about 30gb on each of the cards and during inference, this can
          climb as high as 45gb per card. This model is resource hungry and does no
          operate in 8bit or 4bit quantized well at all.</p>

          '
        raw: '> Eastwind, could you please tell how fast the inference was? Because
          for small prompts on the V100 also it is taking me a good minute to render
          the response and the longer prompts it is crashing with a CUDA OOM error.


          I''ve noticed Falcon 40B works fine in 16bit (bfloat) mode. When running
          it in 8bit, it runs like garbage and is CPU bound. The performance is HORRIBLE
          in anything but 16/32bit and it''s always CPU bound. Running in 16/32bit,
          it uses my cards sequentially and runs them up to 90% utilization and then
          pops to the next card. I''ve got 3x 48gb A6000 cards. When loaded in 16bit,
          it takes up about 30gb on each of the cards and during inference, this can
          climb as high as 45gb per card. This model is resource hungry and does no
          operate in 8bit or 4bit quantized well at all.'
        updatedAt: '2023-07-05T18:56:27.290Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - daryl149
    id: 64a5bcdba55e3ce4f6c3e5e5
    type: comment
  author: RecViking
  content: '> Eastwind, could you please tell how fast the inference was? Because
    for small prompts on the V100 also it is taking me a good minute to render the
    response and the longer prompts it is crashing with a CUDA OOM error.


    I''ve noticed Falcon 40B works fine in 16bit (bfloat) mode. When running it in
    8bit, it runs like garbage and is CPU bound. The performance is HORRIBLE in anything
    but 16/32bit and it''s always CPU bound. Running in 16/32bit, it uses my cards
    sequentially and runs them up to 90% utilization and then pops to the next card.
    I''ve got 3x 48gb A6000 cards. When loaded in 16bit, it takes up about 30gb on
    each of the cards and during inference, this can climb as high as 45gb per card.
    This model is resource hungry and does no operate in 8bit or 4bit quantized well
    at all.'
  created_at: 2023-07-05 17:56:27+00:00
  edited: false
  hidden: false
  id: 64a5bcdba55e3ce4f6c3e5e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-07-19T15:34:12.000Z'
    data:
      edited: true
      editors:
      - daryl149
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9591480493545532
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
          fullname: Daryl Autar
          isHf: false
          isPro: false
          name: daryl149
          type: user
        html: '<p>That definitely is in line with the performance I have seen on a
          4xV100S cluster (128GB combined VRAM). Using 8bit and 4 bit changes the
          model size, but does not speed up inference. (0.75 token per second)</p>

          '
        raw: That definitely is in line with the performance I have seen on a 4xV100S
          cluster (128GB combined VRAM). Using 8bit and 4 bit changes the model size,
          but does not speed up inference. (0.75 token per second)
        updatedAt: '2023-07-19T15:35:37.569Z'
      numEdits: 2
      reactions: []
    id: 64b8027417570fdff9c79bd5
    type: comment
  author: daryl149
  content: That definitely is in line with the performance I have seen on a 4xV100S
    cluster (128GB combined VRAM). Using 8bit and 4 bit changes the model size, but
    does not speed up inference. (0.75 token per second)
  created_at: 2023-07-19 14:34:12+00:00
  edited: true
  hidden: false
  id: 64b8027417570fdff9c79bd5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: OpenAssistant/falcon-40b-sft-top1-560
repo_type: model
status: open
target_branch: null
title: Issue with multi GPU inference.
