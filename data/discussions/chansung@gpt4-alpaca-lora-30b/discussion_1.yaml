!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KnutJaegersberg
conflicting_files: null
created_at: 2023-04-14 11:59:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-04-14T12:59:59.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Can you quantize\
          \ this lora model, too? </p>\n"
        raw: '@TheBloke Can you quantize this lora model, too? '
        updatedAt: '2023-04-14T12:59:59.971Z'
      numEdits: 0
      reactions: []
    id: 64394e4f3ab54fdbab7ead20
    type: comment
  author: KnutJaegersberg
  content: '@TheBloke Can you quantize this lora model, too? '
  created_at: 2023-04-14 11:59:59+00:00
  edited: false
  hidden: false
  id: 64394e4f3ab54fdbab7ead20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-04-14T13:00:36.000Z'
    data:
      from: '4int quantization '
      to: int4 quantization
    id: 64394e7468228e8b334235c4
    type: title-change
  author: KnutJaegersberg
  created_at: 2023-04-14 12:00:36+00:00
  id: 64394e7468228e8b334235c4
  new_title: int4 quantization
  old_title: '4int quantization '
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-04-14T13:01:45.000Z'
    data:
      edited: true
      editors:
      - KnutJaegersberg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>This might be the best general model right now on the hub, that
          people can use with consumer hardware.</p>

          '
        raw: This might be the best general model right now on the hub, that people
          can use with consumer hardware.
        updatedAt: '2023-04-14T13:01:57.224Z'
      numEdits: 1
      reactions: []
    id: 64394eb93da4490578d5485a
    type: comment
  author: KnutJaegersberg
  content: This might be the best general model right now on the hub, that people
    can use with consumer hardware.
  created_at: 2023-04-14 12:01:45+00:00
  edited: true
  hidden: false
  id: 64394eb93da4490578d5485a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-14T13:02:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Nice! I''m on it.</p>

          '
        raw: Nice! I'm on it.
        updatedAt: '2023-04-14T13:02:22.183Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - eepos
        - roxtu
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - KnutJaegersberg
    id: 64394edecc228b8099b19fd9
    type: comment
  author: TheBloke
  content: Nice! I'm on it.
  created_at: 2023-04-14 12:02:22+00:00
  edited: false
  hidden: false
  id: 64394edecc228b8099b19fd9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-14T21:01:57.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Took me a while and <span data-props=\"{&quot;user&quot;:&quot;MetaIX&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MetaIX\"\
          >@<span class=\"underline\">MetaIX</span></a></span>\n\n\t</span></span>\
          \ beat me to it! But we've used different parameters so you may want to\
          \ try both.</p>\n<ul>\n<li><a href=\"https://huggingface.co/TheBloke/gpt4-alpaca-lora-30B-GPTQ-4bit-128g\"\
          >Mine, GPTQ 4bit  with <code>--groupsize 128 --true-sequential --act-order</code></a></li>\n\
          <li><a href=\"https://huggingface.co/MetaIX/GPT4-X-Alpaca-30B-Int4\">MetalX's,\
          \ GPTQ 4bit with <code>--true-sequential --act-order</code></a></li>\n</ul>\n\
          <p>MetalX chose no groupsize so as to ensure it won't exceed 24GB VRAM.\
          \  Mine does use groupsize for hopefully maximum quality, however it will\
          \ exceed 24GB VRAM at somewhere around 1000-1200 tokens returned.  A full\
          \ 2048 token response may use as much as 35GB VRAM, although the number\
          \ fluctuates (I saw a brief peak at 35GB, before it dropped to 28GB).</p>\n"
        raw: 'Took me a while and @MetaIX beat me to it! But we''ve used different
          parameters so you may want to try both.


          * [Mine, GPTQ 4bit  with `--groupsize 128 --true-sequential --act-order`](https://huggingface.co/TheBloke/gpt4-alpaca-lora-30B-GPTQ-4bit-128g)

          * [MetalX''s, GPTQ 4bit with `--true-sequential --act-order`](https://huggingface.co/MetaIX/GPT4-X-Alpaca-30B-Int4)


          MetalX chose no groupsize so as to ensure it won''t exceed 24GB VRAM.  Mine
          does use groupsize for hopefully maximum quality, however it will exceed
          24GB VRAM at somewhere around 1000-1200 tokens returned.  A full 2048 token
          response may use as much as 35GB VRAM, although the number fluctuates (I
          saw a brief peak at 35GB, before it dropped to 28GB).'
        updatedAt: '2023-04-14T21:02:59.302Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - tensiondriven
    id: 6439bf451eddadc9404d2726
    type: comment
  author: TheBloke
  content: 'Took me a while and @MetaIX beat me to it! But we''ve used different parameters
    so you may want to try both.


    * [Mine, GPTQ 4bit  with `--groupsize 128 --true-sequential --act-order`](https://huggingface.co/TheBloke/gpt4-alpaca-lora-30B-GPTQ-4bit-128g)

    * [MetalX''s, GPTQ 4bit with `--true-sequential --act-order`](https://huggingface.co/MetaIX/GPT4-X-Alpaca-30B-Int4)


    MetalX chose no groupsize so as to ensure it won''t exceed 24GB VRAM.  Mine does
    use groupsize for hopefully maximum quality, however it will exceed 24GB VRAM
    at somewhere around 1000-1200 tokens returned.  A full 2048 token response may
    use as much as 35GB VRAM, although the number fluctuates (I saw a brief peak at
    35GB, before it dropped to 28GB).'
  created_at: 2023-04-14 20:01:57+00:00
  edited: true
  hidden: false
  id: 6439bf451eddadc9404d2726
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a3ca5c5a9b8f28b90f04852b2fb4bb23.svg
      fullname: sebi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sebibi
      type: user
    createdAt: '2023-04-14T23:05:32.000Z'
    data:
      edited: false
      editors:
      - sebibi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a3ca5c5a9b8f28b90f04852b2fb4bb23.svg
          fullname: sebi
          isHf: false
          isPro: false
          name: sebibi
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>,\
          \ that's amazing !</p>\n<p>Can we finetune a quantized model ?<br>Or should\
          \ we finetune the model and then quantize it ?</p>\n"
        raw: 'Thanks @TheBloke, that''s amazing !


          Can we finetune a quantized model ?

          Or should we finetune the model and then quantize it ?'
        updatedAt: '2023-04-14T23:05:32.052Z'
      numEdits: 0
      reactions: []
    id: 6439dc3c555050298bd55640
    type: comment
  author: sebibi
  content: 'Thanks @TheBloke, that''s amazing !


    Can we finetune a quantized model ?

    Or should we finetune the model and then quantize it ?'
  created_at: 2023-04-14 22:05:32+00:00
  edited: false
  hidden: false
  id: 6439dc3c555050298bd55640
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-14T23:09:29.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Can we finetune a quantized model ?<br>Or should we finetune the model
          and then quantize it ?</p>

          </blockquote>

          <p>The normal route is to fine tune first, then quantise afterwards. This
          is what most people do, and ensures the highest quality. But of course it
          also requires a lot of resources and VRAM to do the fine tuning.</p>

          <p>I know of at least one project that aims to enable fine tuning on 4bit
          quantised models. I''ve not tried this myself yet, but check out this repo:
          <a rel="nofollow" href="https://github.com/johnsmith0031/alpaca_lora_4bit">https://github.com/johnsmith0031/alpaca_lora_4bit</a></p>

          '
        raw: '> Can we finetune a quantized model ?

          > Or should we finetune the model and then quantize it ?


          The normal route is to fine tune first, then quantise afterwards. This is
          what most people do, and ensures the highest quality. But of course it also
          requires a lot of resources and VRAM to do the fine tuning.


          I know of at least one project that aims to enable fine tuning on 4bit quantised
          models. I''ve not tried this myself yet, but check out this repo: https://github.com/johnsmith0031/alpaca_lora_4bit'
        updatedAt: '2023-04-14T23:09:38.372Z'
      numEdits: 1
      reactions: []
    id: 6439dd29555050298bd55cd1
    type: comment
  author: TheBloke
  content: '> Can we finetune a quantized model ?

    > Or should we finetune the model and then quantize it ?


    The normal route is to fine tune first, then quantise afterwards. This is what
    most people do, and ensures the highest quality. But of course it also requires
    a lot of resources and VRAM to do the fine tuning.


    I know of at least one project that aims to enable fine tuning on 4bit quantised
    models. I''ve not tried this myself yet, but check out this repo: https://github.com/johnsmith0031/alpaca_lora_4bit'
  created_at: 2023-04-14 22:09:29+00:00
  edited: true
  hidden: false
  id: 6439dd29555050298bd55cd1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a3ca5c5a9b8f28b90f04852b2fb4bb23.svg
      fullname: sebi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sebibi
      type: user
    createdAt: '2023-04-14T23:10:17.000Z'
    data:
      edited: false
      editors:
      - sebibi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a3ca5c5a9b8f28b90f04852b2fb4bb23.svg
          fullname: sebi
          isHf: false
          isPro: false
          name: sebibi
          type: user
        html: '<p>Great, I''ll check it out :)</p>

          '
        raw: Great, I'll check it out :)
        updatedAt: '2023-04-14T23:10:17.850Z'
      numEdits: 0
      reactions: []
    id: 6439dd598ac59df4c693d6d6
    type: comment
  author: sebibi
  content: Great, I'll check it out :)
  created_at: 2023-04-14 22:10:17+00:00
  edited: false
  hidden: false
  id: 6439dd598ac59df4c693d6d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-04-14T23:54:04.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<blockquote>

          <p>Great, I''ll check it out :)<br><a rel="nofollow" href="https://github.com/stochasticai/xturing/blob/main/examples/int4_finetuning/README.md">https://github.com/stochasticai/xturing/blob/main/examples/int4_finetuning/README.md</a><br>This
          claims you can make lora''s on 4bit quantized models here</p>

          </blockquote>

          '
        raw: '> Great, I''ll check it out :)

          https://github.com/stochasticai/xturing/blob/main/examples/int4_finetuning/README.md

          This claims you can make lora''s on 4bit quantized models here'
        updatedAt: '2023-04-14T23:54:04.950Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
    id: 6439e79c5f7d67fb1919a00c
    type: comment
  author: teknium
  content: '> Great, I''ll check it out :)

    https://github.com/stochasticai/xturing/blob/main/examples/int4_finetuning/README.md

    This claims you can make lora''s on 4bit quantized models here'
  created_at: 2023-04-14 22:54:04+00:00
  edited: false
  hidden: false
  id: 6439e79c5f7d67fb1919a00c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-14T23:56:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Nice, thanks for the link!</p>

          '
        raw: Nice, thanks for the link!
        updatedAt: '2023-04-14T23:56:22.103Z'
      numEdits: 0
      reactions: []
    id: 6439e8268337a7ef7f5ee582
    type: comment
  author: TheBloke
  content: Nice, thanks for the link!
  created_at: 2023-04-14 22:56:22+00:00
  edited: false
  hidden: false
  id: 6439e8268337a7ef7f5ee582
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-04-15T06:10:52.000Z'
    data:
      status: closed
    id: 643a3fec45200ac3e70207a9
    type: status-change
  author: KnutJaegersberg
  created_at: 2023-04-15 05:10:52+00:00
  id: 643a3fec45200ac3e70207a9
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
      fullname: Jonathan Yankovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tensiondriven
      type: user
    createdAt: '2023-04-22T17:38:31.000Z'
    data:
      edited: false
      editors:
      - tensiondriven
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
          fullname: Jonathan Yankovich
          isHf: false
          isPro: false
          name: tensiondriven
          type: user
        html: '<p>FWIW (I hope this won''t re-open the comment), I''ve been able to
          use <a rel="nofollow" href="https://github.com/johnsmith0031/alpaca_lora_4bit">https://github.com/johnsmith0031/alpaca_lora_4bit</a>
          to fine-tune 4-bit quantized vanilla llama with a high degree of success.  It
          was fiddly getting it set up, and the docs aren''t great, but the loras
          it creates are usable with text-generation-webui (which also supports training,
          but not at 4 bit without a "monkey patch")</p>

          '
        raw: FWIW (I hope this won't re-open the comment), I've been able to use https://github.com/johnsmith0031/alpaca_lora_4bit
          to fine-tune 4-bit quantized vanilla llama with a high degree of success.  It
          was fiddly getting it set up, and the docs aren't great, but the loras it
          creates are usable with text-generation-webui (which also supports training,
          but not at 4 bit without a "monkey patch")
        updatedAt: '2023-04-22T17:38:31.807Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - KnutJaegersberg
        - TheBloke
    id: 64441b97c63001ae63533f4e
    type: comment
  author: tensiondriven
  content: FWIW (I hope this won't re-open the comment), I've been able to use https://github.com/johnsmith0031/alpaca_lora_4bit
    to fine-tune 4-bit quantized vanilla llama with a high degree of success.  It
    was fiddly getting it set up, and the docs aren't great, but the loras it creates
    are usable with text-generation-webui (which also supports training, but not at
    4 bit without a "monkey patch")
  created_at: 2023-04-22 16:38:31+00:00
  edited: false
  hidden: false
  id: 64441b97c63001ae63533f4e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-22T19:21:49.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>FWIW (I hope this won''t re-open the comment), I''ve been able to use
          <a rel="nofollow" href="https://github.com/johnsmith0031/alpaca_lora_4bit">https://github.com/johnsmith0031/alpaca_lora_4bit</a>
          to fine-tune 4-bit quantized vanilla llama with a high degree of success.  It
          was fiddly getting it set up, and the docs aren''t great, but the loras
          it creates are usable with text-generation-webui (which also supports training,
          but not at 4 bit without a "monkey patch")</p>

          </blockquote>

          <p>Great, thanks for the info! I''ve been meaning to test out that repo
          - and also <a rel="nofollow" href="https://github.com/stochasticai/xturing/tree/main/examples/int4_finetuning">https://github.com/stochasticai/xturing/tree/main/examples/int4_finetuning</a>
          - but haven''t had a chance yet.</p>

          <p>Can I ask how long it took, what HW you used etc?</p>

          '
        raw: '> FWIW (I hope this won''t re-open the comment), I''ve been able to
          use https://github.com/johnsmith0031/alpaca_lora_4bit to fine-tune 4-bit
          quantized vanilla llama with a high degree of success.  It was fiddly getting
          it set up, and the docs aren''t great, but the loras it creates are usable
          with text-generation-webui (which also supports training, but not at 4 bit
          without a "monkey patch")


          Great, thanks for the info! I''ve been meaning to test out that repo - and
          also https://github.com/stochasticai/xturing/tree/main/examples/int4_finetuning
          - but haven''t had a chance yet.


          Can I ask how long it took, what HW you used etc?'
        updatedAt: '2023-04-22T19:21:49.097Z'
      numEdits: 0
      reactions: []
    id: 644433cd8f795c936d0403bc
    type: comment
  author: TheBloke
  content: '> FWIW (I hope this won''t re-open the comment), I''ve been able to use
    https://github.com/johnsmith0031/alpaca_lora_4bit to fine-tune 4-bit quantized
    vanilla llama with a high degree of success.  It was fiddly getting it set up,
    and the docs aren''t great, but the loras it creates are usable with text-generation-webui
    (which also supports training, but not at 4 bit without a "monkey patch")


    Great, thanks for the info! I''ve been meaning to test out that repo - and also
    https://github.com/stochasticai/xturing/tree/main/examples/int4_finetuning - but
    haven''t had a chance yet.


    Can I ask how long it took, what HW you used etc?'
  created_at: 2023-04-22 18:21:49+00:00
  edited: false
  hidden: false
  id: 644433cd8f795c936d0403bc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: chansung/gpt4-alpaca-lora-30b
repo_type: model
status: closed
target_branch: null
title: int4 quantization
