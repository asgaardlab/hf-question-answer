!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pbjacob
conflicting_files: null
created_at: 2023-09-01 00:03:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b05c605b8961f97354d87f5a8017767a.svg
      fullname: zbo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pbjacob
      type: user
    createdAt: '2023-09-01T01:03:54.000Z'
    data:
      edited: false
      editors:
      - pbjacob
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6651236414909363
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b05c605b8961f97354d87f5a8017767a.svg
          fullname: zbo
          isHf: false
          isPro: false
          name: pbjacob
          type: user
        html: "<h1 id=\"why-are-the-output-texts-all-garbled\">Why are the output\
          \ texts all garbled?</h1>\n<h2 id=\"hardware--software\">Hardware &amp;\
          \ Software</h2>\n<p>CPU: 12700KF<br>GPU: 3090ti 24GB<br>OS: Win10 Pro 22H2<br>UI:\
          \ text-generation-webui<br>CUDA: 12.1<br>PyTorch: Nightly<br>AutoGPTQ: v0.4.2+cu121</p>\n\
          <h2 id=\"model\">Model</h2>\n<p>Loader: AutoGPTQ(no_inject_fused_attention,\
          \ disable_exllama)<br>Parameters Preset: LLaMA-Precise</p>\n<h2 id=\"issue\"\
          >Issue</h2>\n<p><strong>Input:</strong></p>\n<pre><code>Write a lambda function\
          \ in Python that takes a list of strings as input and returns the element\
          \ in the list whose first letter is 'J' or 'j'.\n</code></pre>\n<p><strong>Output:</strong></p>\n\
          <pre><code>itionalziaition, import\n\nimportiveiveiveive own?itional Cob\
          \ Uniti Censo_ daughiamsitionalisticingly\n\ndetermin ##### &lt;? Z\xFC\
          r \u043F\u043E\u043C\u0435mundation millimeter Institutitionalitionalitionalitionalitional\n\
          \nitional Kontrolaifact for packageistiveoug importitionalive Censoitionalitionalitionalclipitionalitionalitionals\n\
          \nitionalvia listade\n\nimport ?rsitional importardiitionalitionalalitionalitionalitionalitionalitionalitional\n\
          \nineitionalitional\u0441\u044F\u0441\u044Fitional dependentitional {- circumst?itional\
          \ import? {: importAAAA importdependent importces importSERTitional?\n\n\
          CHAPTER importive importitional\n\nAnal &lt;? &lt;? SchiffdditionalConstraintsitionalitionalitionalwe\
          \ import #!/itionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalivity\
          \ &lt;?itionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitional\n\
          </code></pre>\n<h2 id=\"addition\">Addition</h2>\n<p>I have also tested\
          \ the same question to TheBloke/Phind-CodeLlama-34B-Python-v1-GPTQ, recieving\
          \ the following output.</p>\n<pre><code>ziaitionalive importitional\\istic\
          \ divis variveitionalitionaliveiveitionaliveiveitionaliveiveitionaliveiveitionaliveiveitionaliveiveiveitionaliveiveitionaliveiveiveitionaliveiveiveitionaliveiveiveitionaliveiveiveiveitionaliveiveiveiveitionaliveiveiveitionaliveiveiveiveitionaliveiveiveiveitionaliveiveiveiveitionaliveiveiveitionaliveiveiveitionaliveiveitionaliveitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitional\n\
          </code></pre>\n<p>When it comes to TheBloke/CodeLlama-34B-Python-GPTQ, I\
          \ got the following.</p>\n<pre><code>__ ______________________ =_____\n\
          </code></pre>\n<p>Again, why the outputs are garbled? Any settings are wrong?\
          \ I actually followed the instruction from your model card, and it seems\
          \ that there are no specific settings.<br>By the way, the text-generation-webui\
          \ is installed correctly, with official Llama-2-7b-chat-hf using fine.</p>\n\
          <p>Thank you so much!</p>\n"
        raw: "# Why are the output texts all garbled?\r\n\r\n## Hardware & Software\r\
          \nCPU: 12700KF\r\nGPU: 3090ti 24GB\r\nOS: Win10 Pro 22H2\r\nUI: text-generation-webui\r\
          \nCUDA: 12.1\r\nPyTorch: Nightly\r\nAutoGPTQ: v0.4.2+cu121\r\n\r\n## Model\r\
          \nLoader: AutoGPTQ(no_inject_fused_attention, disable_exllama)\r\nParameters\
          \ Preset: LLaMA-Precise\r\n\r\n## Issue\r\n**Input:**\r\n``` \r\nWrite a\
          \ lambda function in Python that takes a list of strings as input and returns\
          \ the element in the list whose first letter is 'J' or 'j'.\r\n```\r\n\r\
          \n**Output:**\r\n```\r\nitionalziaition, import\r\n\r\nimportiveiveiveive\
          \ own?itional Cob Uniti Censo_ daughiamsitionalisticingly\r\n\r\ndetermin\
          \ ##### <? Z\xFCr \u043F\u043E\u043C\u0435mundation millimeter Institutitionalitionalitionalitionalitional\r\
          \n\r\nitional Kontrolaifact for packageistiveoug importitionalive Censoitionalitionalitionalclipitionalitionalitionals\r\
          \n\r\nitionalvia listade\r\n\r\nimport ?rsitional importardiitionalitionalalitionalitionalitionalitionalitionalitional\r\
          \n\r\nineitionalitional\u0441\u044F\u0441\u044Fitional dependentitional\
          \ {- circumst?itional import? {: importAAAA importdependent importces importSERTitional?\r\
          \n\r\nCHAPTER importive importitional\r\n\r\nAnal <? <? SchiffdditionalConstraintsitionalitionalitionalwe\
          \ import #!/itionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalivity\
          \ <?itionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitional\r\
          \n```\r\n\r\n## Addition\r\nI have also tested the same question to TheBloke/Phind-CodeLlama-34B-Python-v1-GPTQ,\
          \ recieving the following output.\r\n```\r\nziaitionalive importitional\\\
          istic divis variveitionalitionaliveiveitionaliveiveitionaliveiveitionaliveiveitionaliveiveitionaliveiveiveitionaliveiveitionaliveiveiveitionaliveiveiveitionaliveiveiveitionaliveiveiveiveitionaliveiveiveiveitionaliveiveiveitionaliveiveiveiveitionaliveiveiveiveitionaliveiveiveiveitionaliveiveiveitionaliveiveiveitionaliveiveitionaliveitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitional\r\
          \n```\r\nWhen it comes to TheBloke/CodeLlama-34B-Python-GPTQ, I got the\
          \ following.\r\n```\r\n__ ______________________ =_____\r\n```\r\n\r\nAgain,\
          \ why the outputs are garbled? Any settings are wrong? I actually followed\
          \ the instruction from your model card, and it seems that there are no specific\
          \ settings.\r\nBy the way, the text-generation-webui is installed correctly,\
          \ with official Llama-2-7b-chat-hf using fine.\r\n\r\nThank you so much!\r\
          \n"
        updatedAt: '2023-09-01T01:03:54.647Z'
      numEdits: 0
      reactions: []
    id: 64f1387a2efbce553bfe93e4
    type: comment
  author: pbjacob
  content: "# Why are the output texts all garbled?\r\n\r\n## Hardware & Software\r\
    \nCPU: 12700KF\r\nGPU: 3090ti 24GB\r\nOS: Win10 Pro 22H2\r\nUI: text-generation-webui\r\
    \nCUDA: 12.1\r\nPyTorch: Nightly\r\nAutoGPTQ: v0.4.2+cu121\r\n\r\n## Model\r\n\
    Loader: AutoGPTQ(no_inject_fused_attention, disable_exllama)\r\nParameters Preset:\
    \ LLaMA-Precise\r\n\r\n## Issue\r\n**Input:**\r\n``` \r\nWrite a lambda function\
    \ in Python that takes a list of strings as input and returns the element in the\
    \ list whose first letter is 'J' or 'j'.\r\n```\r\n\r\n**Output:**\r\n```\r\n\
    itionalziaition, import\r\n\r\nimportiveiveiveive own?itional Cob Uniti Censo_\
    \ daughiamsitionalisticingly\r\n\r\ndetermin ##### <? Z\xFCr \u043F\u043E\u043C\
    \u0435mundation millimeter Institutitionalitionalitionalitionalitional\r\n\r\n\
    itional Kontrolaifact for packageistiveoug importitionalive Censoitionalitionalitionalclipitionalitionalitionals\r\
    \n\r\nitionalvia listade\r\n\r\nimport ?rsitional importardiitionalitionalalitionalitionalitionalitionalitionalitional\r\
    \n\r\nineitionalitional\u0441\u044F\u0441\u044Fitional dependentitional {- circumst?itional\
    \ import? {: importAAAA importdependent importces importSERTitional?\r\n\r\nCHAPTER\
    \ importive importitional\r\n\r\nAnal <? <? SchiffdditionalConstraintsitionalitionalitionalwe\
    \ import #!/itionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalivity\
    \ <?itionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitional\r\
    \n```\r\n\r\n## Addition\r\nI have also tested the same question to TheBloke/Phind-CodeLlama-34B-Python-v1-GPTQ,\
    \ recieving the following output.\r\n```\r\nziaitionalive importitional\\istic\
    \ divis variveitionalitionaliveiveitionaliveiveitionaliveiveitionaliveiveitionaliveiveitionaliveiveiveitionaliveiveitionaliveiveiveitionaliveiveiveitionaliveiveiveitionaliveiveiveiveitionaliveiveiveiveitionaliveiveiveitionaliveiveiveiveitionaliveiveiveiveitionaliveiveiveiveitionaliveiveiveitionaliveiveiveitionaliveiveitionaliveitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitionalitional\r\
    \n```\r\nWhen it comes to TheBloke/CodeLlama-34B-Python-GPTQ, I got the following.\r\
    \n```\r\n__ ______________________ =_____\r\n```\r\n\r\nAgain, why the outputs\
    \ are garbled? Any settings are wrong? I actually followed the instruction from\
    \ your model card, and it seems that there are no specific settings.\r\nBy the\
    \ way, the text-generation-webui is installed correctly, with official Llama-2-7b-chat-hf\
    \ using fine.\r\n\r\nThank you so much!\r\n"
  created_at: 2023-09-01 00:03:54+00:00
  edited: false
  hidden: false
  id: 64f1387a2efbce553bfe93e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b05c605b8961f97354d87f5a8017767a.svg
      fullname: zbo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pbjacob
      type: user
    createdAt: '2023-09-04T03:54:05.000Z'
    data:
      edited: false
      editors:
      - pbjacob
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9394664764404297
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b05c605b8961f97354d87f5a8017767a.svg
          fullname: zbo
          isHf: false
          isPro: false
          name: pbjacob
          type: user
        html: '<p>I''ve already known the reason now.<br>This model is not for chat,
          and should be used in instruct mode, using Alpaca template.</p>

          <p>Hope the above information can be added into the instruction of the model
          card.</p>

          '
        raw: 'I''ve already known the reason now.

          This model is not for chat, and should be used in instruct mode, using Alpaca
          template.


          Hope the above information can be added into the instruction of the model
          card.'
        updatedAt: '2023-09-04T03:54:05.790Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64f554dd941b908c069a190a
    id: 64f554dd941b908c069a1906
    type: comment
  author: pbjacob
  content: 'I''ve already known the reason now.

    This model is not for chat, and should be used in instruct mode, using Alpaca
    template.


    Hope the above information can be added into the instruction of the model card.'
  created_at: 2023-09-04 02:54:05+00:00
  edited: false
  hidden: false
  id: 64f554dd941b908c069a1906
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b05c605b8961f97354d87f5a8017767a.svg
      fullname: zbo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pbjacob
      type: user
    createdAt: '2023-09-04T03:54:05.000Z'
    data:
      status: closed
    id: 64f554dd941b908c069a190a
    type: status-change
  author: pbjacob
  created_at: 2023-09-04 02:54:05+00:00
  id: 64f554dd941b908c069a190a
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/WizardCoder-Python-34B-V1.0-GPTQ
repo_type: model
status: closed
target_branch: null
title: Why are the output texts all garbled?
