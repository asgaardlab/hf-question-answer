!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DanekBigLike
conflicting_files: null
created_at: 2023-10-06 20:32:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/28f196d2613db3e381b9f242636a9e9b.svg
      fullname: Danek Maximov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DanekBigLike
      type: user
    createdAt: '2023-10-06T21:32:29.000Z'
    data:
      edited: false
      editors:
      - DanekBigLike
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4355347156524658
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/28f196d2613db3e381b9f242636a9e9b.svg
          fullname: Danek Maximov
          isHf: false
          isPro: false
          name: DanekBigLike
          type: user
        html: "<p>2023-10-07 00:24:03 INFO:Loading TheBloke_WizardCoder-Python-34B-V1.0-GPTQ_gptq-4bit-64g-actorder_True...<br>2023-10-07\
          \ 00:24:03 INFO:The AutoGPTQ params are: {'model_basename': 'model', 'device':\
          \ 'cuda:0', 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp':\
          \ True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory':\
          \ {0: '24500MiB', 'cpu': '32600MiB'}, 'quantize_config': None, 'use_cuda_fp16':\
          \ True, 'disable_exllama': False}<br>2023-10-07 00:24:35 ERROR:Failed to\
          \ load the model.<br>Traceback (most recent call last):<br>  File \"E:\\\
          ai\\ruai\\saiga\\text-generation-webui\\modules\\ui_model_menu.py\", line\
          \ 194, in load_model_wrapper<br>    shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader)<br>  File \"E:\\ai\\ruai\\saiga\\text-generation-webui\\modules\\\
          models.py\", line 75, in load_model<br>    output = load_func_map<a rel=\"\
          nofollow\" href=\"model_name\">loader</a><br>  File \"E:\\ai\\ruai\\saiga\\\
          text-generation-webui\\modules\\models.py\", line 316, in AutoGPTQ_loader<br>\
          \    return modules.AutoGPTQ_loader.load_quantized(model_name)<br>  File\
          \ \"E:\\ai\\ruai\\saiga\\text-generation-webui\\modules\\AutoGPTQ_loader.py\"\
          , line 57, in load_quantized<br>    model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params)<br>  File \"C:\\Users\\remot.conda\\envs\\textgen2\\lib\\site-packages\\\
          auto_gptq\\modeling\\auto.py\", line 108, in from_quantized<br>    return\
          \ quant_func(<br>  File \"C:\\Users\\remot.conda\\envs\\textgen2\\lib\\\
          site-packages\\auto_gptq\\modeling_base.py\", line 875, in from_quantized<br>\
          \    accelerate.utils.modeling.load_checkpoint_in_model(<br>  File \"C:\\\
          Users\\remot.conda\\envs\\textgen2\\lib\\site-packages\\accelerate\\utils\\\
          modeling.py\", line 1335, in load_checkpoint_in_model<br>    checkpoint\
          \ = load_state_dict(checkpoint_file, device_map=device_map)<br>  File \"\
          C:\\Users\\remot.conda\\envs\\textgen2\\lib\\site-packages\\accelerate\\\
          utils\\modeling.py\", line 1164, in load_state_dict<br>    return safe_load_file(checkpoint_file,\
          \ device=list(device_map.values())[0])<br>  File \"C:\\Users\\remot.conda\\\
          envs\\textgen2\\lib\\site-packages\\safetensors\\torch.py\", line 311, in\
          \ load_file<br>    result[k] = f.get_tensor(k)<br>RuntimeError: [enforce\
          \ fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator:\
          \ not enough memory: you tried to allocate 90177536 bytes.</p>\n<p>Initially,\
          \ 120 MB was not enough, I increased the swap file, 90 MB became missing,\
          \ I think this is actually not related to memory, maybe I\u2019m wrong.</p>\n\
          <p>I expanded the swap file from 32 to 64 GB</p>\n<p>My system characteristics:<br>Windows\
          \ 10 (miniconda)<br>RTX 3090 24gb<br>RAM 32gb<br>Swap file 70gb (drive C\
          \ (auto) + drive D (64gb))</p>\n"
        raw: "2023-10-07 00:24:03 INFO:Loading TheBloke_WizardCoder-Python-34B-V1.0-GPTQ_gptq-4bit-64g-actorder_True...\r\
          \n2023-10-07 00:24:03 INFO:The AutoGPTQ params are: {'model_basename': 'model',\
          \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': {0: '24500MiB', 'cpu': '32600MiB'}, 'quantize_config':\
          \ None, 'use_cuda_fp16': True, 'disable_exllama': False}\r\n2023-10-07 00:24:35\
          \ ERROR:Failed to load the model.\r\nTraceback (most recent call last):\r\
          \n  File \"E:\\ai\\ruai\\saiga\\text-generation-webui\\modules\\ui_model_menu.py\"\
          , line 194, in load_model_wrapper\r\n    shared.model, shared.tokenizer\
          \ = load_model(shared.model_name, loader)\r\n  File \"E:\\ai\\ruai\\saiga\\\
          text-generation-webui\\modules\\models.py\", line 75, in load_model\r\n\
          \    output = load_func_map[loader](model_name)\r\n  File \"E:\\ai\\ruai\\\
          saiga\\text-generation-webui\\modules\\models.py\", line 316, in AutoGPTQ_loader\r\
          \n    return modules.AutoGPTQ_loader.load_quantized(model_name)\r\n  File\
          \ \"E:\\ai\\ruai\\saiga\\text-generation-webui\\modules\\AutoGPTQ_loader.py\"\
          , line 57, in load_quantized\r\n    model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params)\r\n  File \"C:\\Users\\remot\\.conda\\envs\\textgen2\\lib\\\
          site-packages\\auto_gptq\\modeling\\auto.py\", line 108, in from_quantized\r\
          \n    return quant_func(\r\n  File \"C:\\Users\\remot\\.conda\\envs\\textgen2\\\
          lib\\site-packages\\auto_gptq\\modeling\\_base.py\", line 875, in from_quantized\r\
          \n    accelerate.utils.modeling.load_checkpoint_in_model(\r\n  File \"C:\\\
          Users\\remot\\.conda\\envs\\textgen2\\lib\\site-packages\\accelerate\\utils\\\
          modeling.py\", line 1335, in load_checkpoint_in_model\r\n    checkpoint\
          \ = load_state_dict(checkpoint_file, device_map=device_map)\r\n  File \"\
          C:\\Users\\remot\\.conda\\envs\\textgen2\\lib\\site-packages\\accelerate\\\
          utils\\modeling.py\", line 1164, in load_state_dict\r\n    return safe_load_file(checkpoint_file,\
          \ device=list(device_map.values())[0])\r\n  File \"C:\\Users\\remot\\.conda\\\
          envs\\textgen2\\lib\\site-packages\\safetensors\\torch.py\", line 311, in\
          \ load_file\r\n    result[k] = f.get_tensor(k)\r\nRuntimeError: [enforce\
          \ fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator:\
          \ not enough memory: you tried to allocate 90177536 bytes.\r\n\r\n\r\nInitially,\
          \ 120 MB was not enough, I increased the swap file, 90 MB became missing,\
          \ I think this is actually not related to memory, maybe I\u2019m wrong.\r\
          \n\r\nI expanded the swap file from 32 to 64 GB\r\n\r\n\r\nMy system characteristics:\r\
          \nWindows 10 (miniconda)\r\nRTX 3090 24gb\r\nRAM 32gb\r\nSwap file 70gb\
          \ (drive C (auto) + drive D (64gb))"
        updatedAt: '2023-10-06T21:32:29.343Z'
      numEdits: 0
      reactions: []
    id: 65207ced2d5eb021188d586a
    type: comment
  author: DanekBigLike
  content: "2023-10-07 00:24:03 INFO:Loading TheBloke_WizardCoder-Python-34B-V1.0-GPTQ_gptq-4bit-64g-actorder_True...\r\
    \n2023-10-07 00:24:03 INFO:The AutoGPTQ params are: {'model_basename': 'model',\
    \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp':\
    \ True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory': {0:\
    \ '24500MiB', 'cpu': '32600MiB'}, 'quantize_config': None, 'use_cuda_fp16': True,\
    \ 'disable_exllama': False}\r\n2023-10-07 00:24:35 ERROR:Failed to load the model.\r\
    \nTraceback (most recent call last):\r\n  File \"E:\\ai\\ruai\\saiga\\text-generation-webui\\\
    modules\\ui_model_menu.py\", line 194, in load_model_wrapper\r\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name, loader)\r\n  File \"E:\\ai\\\
    ruai\\saiga\\text-generation-webui\\modules\\models.py\", line 75, in load_model\r\
    \n    output = load_func_map[loader](model_name)\r\n  File \"E:\\ai\\ruai\\saiga\\\
    text-generation-webui\\modules\\models.py\", line 316, in AutoGPTQ_loader\r\n\
    \    return modules.AutoGPTQ_loader.load_quantized(model_name)\r\n  File \"E:\\\
    ai\\ruai\\saiga\\text-generation-webui\\modules\\AutoGPTQ_loader.py\", line 57,\
    \ in load_quantized\r\n    model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
    \ **params)\r\n  File \"C:\\Users\\remot\\.conda\\envs\\textgen2\\lib\\site-packages\\\
    auto_gptq\\modeling\\auto.py\", line 108, in from_quantized\r\n    return quant_func(\r\
    \n  File \"C:\\Users\\remot\\.conda\\envs\\textgen2\\lib\\site-packages\\auto_gptq\\\
    modeling\\_base.py\", line 875, in from_quantized\r\n    accelerate.utils.modeling.load_checkpoint_in_model(\r\
    \n  File \"C:\\Users\\remot\\.conda\\envs\\textgen2\\lib\\site-packages\\accelerate\\\
    utils\\modeling.py\", line 1335, in load_checkpoint_in_model\r\n    checkpoint\
    \ = load_state_dict(checkpoint_file, device_map=device_map)\r\n  File \"C:\\Users\\\
    remot\\.conda\\envs\\textgen2\\lib\\site-packages\\accelerate\\utils\\modeling.py\"\
    , line 1164, in load_state_dict\r\n    return safe_load_file(checkpoint_file,\
    \ device=list(device_map.values())[0])\r\n  File \"C:\\Users\\remot\\.conda\\\
    envs\\textgen2\\lib\\site-packages\\safetensors\\torch.py\", line 311, in load_file\r\
    \n    result[k] = f.get_tensor(k)\r\nRuntimeError: [enforce fail at ..\\c10\\\
    core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you\
    \ tried to allocate 90177536 bytes.\r\n\r\n\r\nInitially, 120 MB was not enough,\
    \ I increased the swap file, 90 MB became missing, I think this is actually not\
    \ related to memory, maybe I\u2019m wrong.\r\n\r\nI expanded the swap file from\
    \ 32 to 64 GB\r\n\r\n\r\nMy system characteristics:\r\nWindows 10 (miniconda)\r\
    \nRTX 3090 24gb\r\nRAM 32gb\r\nSwap file 70gb (drive C (auto) + drive D (64gb))"
  created_at: 2023-10-06 20:32:29+00:00
  edited: false
  hidden: false
  id: 65207ced2d5eb021188d586a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663604814811-noauth.png?w=200&h=200&f=face
      fullname: DONIYORBEK ADAMBAEV
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: donymorph
      type: user
    createdAt: '2024-01-05T17:20:02.000Z'
    data:
      edited: false
      editors:
      - donymorph
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6686850786209106
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663604814811-noauth.png?w=200&h=200&f=face
          fullname: DONIYORBEK ADAMBAEV
          isHf: false
          isPro: false
          name: donymorph
          type: user
        html: '<p>I am using Linux and loading with ExlamaHF. Took roughly 21 gb vram
          speed 16-17 tokens/s</p>

          '
        raw: I am using Linux and loading with ExlamaHF. Took roughly 21 gb vram speed
          16-17 tokens/s
        updatedAt: '2024-01-05T17:20:02.124Z'
      numEdits: 0
      reactions: []
    id: 65983a42f0102bce68d3a0ff
    type: comment
  author: donymorph
  content: I am using Linux and loading with ExlamaHF. Took roughly 21 gb vram speed
    16-17 tokens/s
  created_at: 2024-01-05 17:20:02+00:00
  edited: false
  hidden: false
  id: 65983a42f0102bce68d3a0ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2024-01-05T17:30:25.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9579206705093384
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;DanekBigLike&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/DanekBigLike\"\
          >@<span class=\"underline\">DanekBigLike</span></a></span>\n\n\t</span></span><br>I\
          \ would recommend using exllama(for gptq) or exllama v2(for exl2 quant format\
          \ which is slightly higher quality and faster then gptq) since both of them\
          \ take less vram and are much, much faster then auto gptq just like donymorph\
          \ said</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;donymorph&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/donymorph\"\
          >@<span class=\"underline\">donymorph</span></a></span>\n\n\t</span></span>\
          \ any reason you are using ExllamaHF? Exllama is considerably faster but\
          \ the only other difference is exllamahf has a few more samplers</p>\n"
        raw: "@DanekBigLike \nI would recommend using exllama(for gptq) or exllama\
          \ v2(for exl2 quant format which is slightly higher quality and faster then\
          \ gptq) since both of them take less vram and are much, much faster then\
          \ auto gptq just like donymorph said\n\n@donymorph any reason you are using\
          \ ExllamaHF? Exllama is considerably faster but the only other difference\
          \ is exllamahf has a few more samplers"
        updatedAt: '2024-01-05T17:30:25.072Z'
      numEdits: 0
      reactions: []
    id: 65983cb11b4a26041a54a77a
    type: comment
  author: YaTharThShaRma999
  content: "@DanekBigLike \nI would recommend using exllama(for gptq) or exllama v2(for\
    \ exl2 quant format which is slightly higher quality and faster then gptq) since\
    \ both of them take less vram and are much, much faster then auto gptq just like\
    \ donymorph said\n\n@donymorph any reason you are using ExllamaHF? Exllama is\
    \ considerably faster but the only other difference is exllamahf has a few more\
    \ samplers"
  created_at: 2024-01-05 17:30:25+00:00
  edited: false
  hidden: false
  id: 65983cb11b4a26041a54a77a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/WizardCoder-Python-34B-V1.0-GPTQ
repo_type: model
status: open
target_branch: null
title: The model does not load in "text generation webui" out of memory error
