!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rfernand
conflicting_files: null
created_at: 2023-11-22 21:53:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673810297924-62cdb1993a2cecfdabec19ee.jpeg?w=200&h=200&f=face
      fullname: Roland Fernandez
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rfernand
      type: user
    createdAt: '2023-11-22T21:53:50.000Z'
    data:
      edited: false
      editors:
      - rfernand
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.956434965133667
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673810297924-62cdb1993a2cecfdabec19ee.jpeg?w=200&h=200&f=face
          fullname: Roland Fernandez
          isHf: false
          isPro: false
          name: rfernand
          type: user
        html: '<p>Great to have this model in HF!  The inference is super slow - makes
          it hard to do real-time experiments.  Can this be sped up easily?</p>

          '
        raw: Great to have this model in HF!  The inference is super slow - makes
          it hard to do real-time experiments.  Can this be sped up easily?
        updatedAt: '2023-11-22T21:53:50.533Z'
      numEdits: 0
      reactions: []
    id: 655e786eabb296623a1a52d8
    type: comment
  author: rfernand
  content: Great to have this model in HF!  The inference is super slow - makes it
    hard to do real-time experiments.  Can this be sped up easily?
  created_at: 2023-11-22 21:53:50+00:00
  edited: false
  hidden: false
  id: 655e786eabb296623a1a52d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673810297924-62cdb1993a2cecfdabec19ee.jpeg?w=200&h=200&f=face
      fullname: Roland Fernandez
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rfernand
      type: user
    createdAt: '2023-11-22T21:56:29.000Z'
    data:
      edited: false
      editors:
      - rfernand
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8310985565185547
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673810297924-62cdb1993a2cecfdabec19ee.jpeg?w=200&h=200&f=face
          fullname: Roland Fernandez
          isHf: false
          isPro: false
          name: rfernand
          type: user
        html: '<p>As measured on Windows 11, CPU: i9-13900KF, 128 GB RAM, GPU: RTX
          3090 (24 GB).</p>

          '
        raw: 'As measured on Windows 11, CPU: i9-13900KF, 128 GB RAM, GPU: RTX 3090
          (24 GB).'
        updatedAt: '2023-11-22T21:56:29.154Z'
      numEdits: 0
      reactions: []
    id: 655e790d5808cf4c1dc785dd
    type: comment
  author: rfernand
  content: 'As measured on Windows 11, CPU: i9-13900KF, 128 GB RAM, GPU: RTX 3090
    (24 GB).'
  created_at: 2023-11-22 21:56:29+00:00
  edited: false
  hidden: false
  id: 655e790d5808cf4c1dc785dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/HqUwLKO5rKA-6YilGoBwk.png?w=200&h=200&f=face
      fullname: "\u03C8\u03C0.com"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PsiPi
      type: user
    createdAt: '2023-11-23T05:57:12.000Z'
    data:
      edited: false
      editors:
      - PsiPi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8850260376930237
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/HqUwLKO5rKA-6YilGoBwk.png?w=200&h=200&f=face
          fullname: "\u03C8\u03C0.com"
          isHf: false
          isPro: false
          name: PsiPi
          type: user
        html: '<p>use a quant. Which don''t exist yet....</p>

          '
        raw: use a quant. Which don't exist yet....
        updatedAt: '2023-11-23T05:57:12.016Z'
      numEdits: 0
      reactions: []
    id: 655ee9b832537bcc8d741b7f
    type: comment
  author: PsiPi
  content: use a quant. Which don't exist yet....
  created_at: 2023-11-23 05:57:12+00:00
  edited: false
  hidden: false
  id: 655ee9b832537bcc8d741b7f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-11-23T21:22:17.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9274199604988098
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;rfernand&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rfernand\">@<span class=\"\
          underline\">rfernand</span></a></span>\n\n\t</span></span> your best bet\
          \ is to use quantization and that should boost speed by a large amount and\
          \ also it will take up less vram. I think you should use the gptq quant\
          \ format and load it with huggingface to get best speed. Although transformers\
          \ is somewhat simple, using something like exllama v2 should get you the\
          \ fastest speed.<br><a href=\"https://huggingface.co/TheBloke/Orca-2-13B-GPTQ\"\
          >https://huggingface.co/TheBloke/Orca-2-13B-GPTQ</a></p>\n<p>Use the 8 bit\
          \ one for maximum quality</p>\n"
        raw: '@rfernand your best bet is to use quantization and that should boost
          speed by a large amount and also it will take up less vram. I think you
          should use the gptq quant format and load it with huggingface to get best
          speed. Although transformers is somewhat simple, using something like exllama
          v2 should get you the fastest speed.

          https://huggingface.co/TheBloke/Orca-2-13B-GPTQ


          Use the 8 bit one for maximum quality'
        updatedAt: '2023-11-23T21:22:17.830Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - rfernand
    id: 655fc289c4bf2627edece3fc
    type: comment
  author: YaTharThShaRma999
  content: '@rfernand your best bet is to use quantization and that should boost speed
    by a large amount and also it will take up less vram. I think you should use the
    gptq quant format and load it with huggingface to get best speed. Although transformers
    is somewhat simple, using something like exllama v2 should get you the fastest
    speed.

    https://huggingface.co/TheBloke/Orca-2-13B-GPTQ


    Use the 8 bit one for maximum quality'
  created_at: 2023-11-23 21:22:17+00:00
  edited: false
  hidden: false
  id: 655fc289c4bf2627edece3fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/HqUwLKO5rKA-6YilGoBwk.png?w=200&h=200&f=face
      fullname: "\u03C8\u03C0.com"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PsiPi
      type: user
    createdAt: '2023-11-24T01:09:40.000Z'
    data:
      edited: false
      editors:
      - PsiPi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8873037695884705
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/HqUwLKO5rKA-6YilGoBwk.png?w=200&h=200&f=face
          fullname: "\u03C8\u03C0.com"
          isHf: false
          isPro: false
          name: PsiPi
          type: user
        html: '<p>heh yeah and now they do exist ;)</p>

          '
        raw: heh yeah and now they do exist ;)
        updatedAt: '2023-11-24T01:09:40.723Z'
      numEdits: 0
      reactions: []
    id: 655ff7d4d3e983996629ca35
    type: comment
  author: PsiPi
  content: heh yeah and now they do exist ;)
  created_at: 2023-11-24 01:09:40+00:00
  edited: false
  hidden: false
  id: 655ff7d4d3e983996629ca35
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673810297924-62cdb1993a2cecfdabec19ee.jpeg?w=200&h=200&f=face
      fullname: Roland Fernandez
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rfernand
      type: user
    createdAt: '2023-11-25T19:46:19.000Z'
    data:
      edited: false
      editors:
      - rfernand
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.689791738986969
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673810297924-62cdb1993a2cecfdabec19ee.jpeg?w=200&h=200&f=face
          fullname: Roland Fernandez
          isHf: false
          isPro: false
          name: rfernand
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;YaTharThShaRma999&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/YaTharThShaRma999\"\
          >@<span class=\"underline\">YaTharThShaRma999</span></a></span>\n\n\t</span></span>\
          \  and <span data-props=\"{&quot;user&quot;:&quot;PsiPi&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/PsiPi\">@<span class=\"\
          underline\">PsiPi</span></a></span>\n\n\t</span></span>.  </p>\n<p>This\
          \ is great - I tried the 4-bit version (<a href=\"https://huggingface.co/TheBloke/Orca-2-13B-GGUF\"\
          >https://huggingface.co/TheBloke/Orca-2-13B-GGUF</a>) with following results:<br>\
          \   model loading: <strong>4x faster</strong><br>   inference <strong>12x\
          \ faster</strong></p>\n<p><em>TLDR</em> </p>\n<ol>\n<li>pip install ctransformers[cuda]</li>\n\
          <li>python script for inference:</li>\n</ol>\n<pre><code>from ctransformers\
          \ import AutoModelForCausalLM\n\n# Set gpu_layers to the number of layers\
          \ to offload to GPU. Set to 0 if no GPU acceleration is available on your\
          \ system.\nllm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Orca-2-13B-GGUF\"\
          , model_file=\"orca-2-13b.Q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\n\
          \nprint(llm(\"AI is going to\"))\n</code></pre>\n"
        raw: "Thanks @YaTharThShaRma999  and @PsiPi.  \n\nThis is great - I tried\
          \ the 4-bit version (https://huggingface.co/TheBloke/Orca-2-13B-GGUF) with\
          \ following results: \n   model loading: **4x faster**\n   inference **12x\
          \ faster**\n\n*TLDR* \n   1. pip install ctransformers[cuda]\n   2. python\
          \ script for inference:\n\n```\nfrom ctransformers import AutoModelForCausalLM\n\
          \n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if\
          \ no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"\
          TheBloke/Orca-2-13B-GGUF\", model_file=\"orca-2-13b.Q4_K_M.gguf\", model_type=\"\
          llama\", gpu_layers=50)\n\nprint(llm(\"AI is going to\"))\n```\n\n\n"
        updatedAt: '2023-11-25T19:46:19.672Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - YaTharThShaRma999
      relatedEventId: 65624f0b7ff2e1b1cf9ba4bc
    id: 65624f0b7ff2e1b1cf9ba4b7
    type: comment
  author: rfernand
  content: "Thanks @YaTharThShaRma999  and @PsiPi.  \n\nThis is great - I tried the\
    \ 4-bit version (https://huggingface.co/TheBloke/Orca-2-13B-GGUF) with following\
    \ results: \n   model loading: **4x faster**\n   inference **12x faster**\n\n\
    *TLDR* \n   1. pip install ctransformers[cuda]\n   2. python script for inference:\n\
    \n```\nfrom ctransformers import AutoModelForCausalLM\n\n# Set gpu_layers to the\
    \ number of layers to offload to GPU. Set to 0 if no GPU acceleration is available\
    \ on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Orca-2-13B-GGUF\"\
    , model_file=\"orca-2-13b.Q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\n\
    \nprint(llm(\"AI is going to\"))\n```\n\n\n"
  created_at: 2023-11-25 19:46:19+00:00
  edited: false
  hidden: false
  id: 65624f0b7ff2e1b1cf9ba4b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673810297924-62cdb1993a2cecfdabec19ee.jpeg?w=200&h=200&f=face
      fullname: Roland Fernandez
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rfernand
      type: user
    createdAt: '2023-11-25T19:46:19.000Z'
    data:
      status: closed
    id: 65624f0b7ff2e1b1cf9ba4bc
    type: status-change
  author: rfernand
  created_at: 2023-11-25 19:46:19+00:00
  id: 65624f0b7ff2e1b1cf9ba4bc
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/HqUwLKO5rKA-6YilGoBwk.png?w=200&h=200&f=face
      fullname: "\u03C8\u03C0.com"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PsiPi
      type: user
    createdAt: '2023-11-26T22:46:40.000Z'
    data:
      edited: false
      editors:
      - PsiPi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9599145650863647
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/HqUwLKO5rKA-6YilGoBwk.png?w=200&h=200&f=face
          fullname: "\u03C8\u03C0.com"
          isHf: false
          isPro: false
          name: PsiPi
          type: user
        html: '<p>Yeah LoneStriker offers an excellent version as well</p>

          '
        raw: Yeah LoneStriker offers an excellent version as well
        updatedAt: '2023-11-26T22:46:40.533Z'
      numEdits: 0
      reactions: []
    id: 6563cad084a9fbe3221035e0
    type: comment
  author: PsiPi
  content: Yeah LoneStriker offers an excellent version as well
  created_at: 2023-11-26 22:46:40+00:00
  edited: false
  hidden: false
  id: 6563cad084a9fbe3221035e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/305198a444061eed34e905ee992a5f91.svg
      fullname: Morgan Wolfgang Weiss
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vulfgang
      type: user
    createdAt: '2023-12-20T18:00:39.000Z'
    data:
      edited: false
      editors:
      - Vulfgang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9181621074676514
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/305198a444061eed34e905ee992a5f91.svg
          fullname: Morgan Wolfgang Weiss
          isHf: false
          isPro: false
          name: Vulfgang
          type: user
        html: '<p>For inference, I get the following error:</p>

          <p>`GLIBC_2.29'' not found</p>

          <p>Anyone know how to resolve this?</p>

          '
        raw: 'For inference, I get the following error:


          `GLIBC_2.29'' not found


          Anyone know how to resolve this?'
        updatedAt: '2023-12-20T18:00:39.421Z'
      numEdits: 0
      reactions: []
    id: 65832bc7bfcc3ed5a1e6f119
    type: comment
  author: Vulfgang
  content: 'For inference, I get the following error:


    `GLIBC_2.29'' not found


    Anyone know how to resolve this?'
  created_at: 2023-12-20 18:00:39+00:00
  edited: false
  hidden: false
  id: 65832bc7bfcc3ed5a1e6f119
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/305198a444061eed34e905ee992a5f91.svg
      fullname: Morgan Wolfgang Weiss
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vulfgang
      type: user
    createdAt: '2023-12-20T18:02:29.000Z'
    data:
      edited: false
      editors:
      - Vulfgang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5662875771522522
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/305198a444061eed34e905ee992a5f91.svg
          fullname: Morgan Wolfgang Weiss
          isHf: false
          isPro: false
          name: Vulfgang
          type: user
        html: '<p>Specifically</p>

          <pre><code>[`GLIBC_2.29'' not found](oserror: /lib64/libm.so.6: version
          `glibc_2.29'' not found (required by /local/home/user_name/anaconda3/envs/odi-ds/lib/python3.9/site-packages/ctransformers/lib/cuda/libctransformers.so))

          </code></pre>

          '
        raw: 'Specifically


          ```

          [`GLIBC_2.29'' not found](oserror: /lib64/libm.so.6: version `glibc_2.29''
          not found (required by /local/home/user_name/anaconda3/envs/odi-ds/lib/python3.9/site-packages/ctransformers/lib/cuda/libctransformers.so))

          ```'
        updatedAt: '2023-12-20T18:02:29.389Z'
      numEdits: 0
      reactions: []
    id: 65832c354b2f0e9f34e59a60
    type: comment
  author: Vulfgang
  content: 'Specifically


    ```

    [`GLIBC_2.29'' not found](oserror: /lib64/libm.so.6: version `glibc_2.29'' not
    found (required by /local/home/user_name/anaconda3/envs/odi-ds/lib/python3.9/site-packages/ctransformers/lib/cuda/libctransformers.so))

    ```'
  created_at: 2023-12-20 18:02:29+00:00
  edited: false
  hidden: false
  id: 65832c354b2f0e9f34e59a60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/HqUwLKO5rKA-6YilGoBwk.png?w=200&h=200&f=face
      fullname: "\u03C8\u03C0.com"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PsiPi
      type: user
    createdAt: '2023-12-20T22:31:14.000Z'
    data:
      edited: false
      editors:
      - PsiPi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8584167957305908
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/HqUwLKO5rKA-6YilGoBwk.png?w=200&h=200&f=face
          fullname: "\u03C8\u03C0.com"
          isHf: false
          isPro: false
          name: PsiPi
          type: user
        html: "<p>Says you have the wrong version of libc ? not to be glib but...\
          \ Get the right one? might need to wrap it in an env. Don't know your situation.\
          \ Good luck <span data-props=\"{&quot;user&quot;:&quot;Vulfgang&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Vulfgang\"\
          >@<span class=\"underline\">Vulfgang</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'Says you have the wrong version of libc ? not to be glib but... Get
          the right one? might need to wrap it in an env. Don''t know your situation.
          Good luck @Vulfgang '
        updatedAt: '2023-12-20T22:31:14.120Z'
      numEdits: 0
      reactions: []
    id: 65836b32cbb381e7886bb62c
    type: comment
  author: PsiPi
  content: 'Says you have the wrong version of libc ? not to be glib but... Get the
    right one? might need to wrap it in an env. Don''t know your situation. Good luck
    @Vulfgang '
  created_at: 2023-12-20 22:31:14+00:00
  edited: false
  hidden: false
  id: 65836b32cbb381e7886bb62c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/305198a444061eed34e905ee992a5f91.svg
      fullname: Morgan Wolfgang Weiss
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vulfgang
      type: user
    createdAt: '2024-01-06T05:13:02.000Z'
    data:
      edited: false
      editors:
      - Vulfgang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9693797826766968
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/305198a444061eed34e905ee992a5f91.svg
          fullname: Morgan Wolfgang Weiss
          isHf: false
          isPro: false
          name: Vulfgang
          type: user
        html: '<p>Thank you for replying, I think I have the right glib now but now
          everytime I run the code on jupyter my kernel just dies as soon as I try
          to download the model from the repo.</p>

          '
        raw: Thank you for replying, I think I have the right glib now but now everytime
          I run the code on jupyter my kernel just dies as soon as I try to download
          the model from the repo.
        updatedAt: '2024-01-06T05:13:02.531Z'
      numEdits: 0
      reactions: []
    id: 6598e15ef0102bce68011683
    type: comment
  author: Vulfgang
  content: Thank you for replying, I think I have the right glib now but now everytime
    I run the code on jupyter my kernel just dies as soon as I try to download the
    model from the repo.
  created_at: 2024-01-06 05:13:02+00:00
  edited: false
  hidden: false
  id: 6598e15ef0102bce68011683
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/305198a444061eed34e905ee992a5f91.svg
      fullname: Morgan Wolfgang Weiss
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vulfgang
      type: user
    createdAt: '2024-01-06T05:19:18.000Z'
    data:
      edited: false
      editors:
      - Vulfgang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6757495403289795
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/305198a444061eed34e905ee992a5f91.svg
          fullname: Morgan Wolfgang Weiss
          isHf: false
          isPro: false
          name: Vulfgang
          type: user
        html: '<p>wait nevermind the last comment, all good</p>

          '
        raw: wait nevermind the last comment, all good
        updatedAt: '2024-01-06T05:19:18.496Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PsiPi
    id: 6598e2d6c1144540fd2d5978
    type: comment
  author: Vulfgang
  content: wait nevermind the last comment, all good
  created_at: 2024-01-06 05:19:18+00:00
  edited: false
  hidden: false
  id: 6598e2d6c1144540fd2d5978
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: microsoft/Orca-2-13b
repo_type: model
status: closed
target_branch: null
title: Inference is very slow (about 3 secs/token)
