!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wiccanmind
conflicting_files: null
created_at: 2023-11-23 08:58:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64250ae54191f56e07339dd7/KVYP9MlmVIiH61jg9e2_r.jpeg?w=200&h=200&f=face
      fullname: "D\u0169ng Ph\xF9ng "
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wiccanmind
      type: user
    createdAt: '2023-11-23T08:58:20.000Z'
    data:
      edited: false
      editors:
      - wiccanmind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9566540718078613
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64250ae54191f56e07339dd7/KVYP9MlmVIiH61jg9e2_r.jpeg?w=200&h=200&f=face
          fullname: "D\u0169ng Ph\xF9ng "
          isHf: false
          isPro: false
          name: wiccanmind
          type: user
        html: '<p>Thank you for contributing to this excellent model.<br>I have a
          question, the model is trained using float32 data type, but due to resource
          constraints, I am performing inference with fp16. Does this significantly
          impact the performance of the model?<br>Currently, I find it not performing
          as well as Orca 1 when inferring with fp16.</p>

          '
        raw: "Thank you for contributing to this excellent model. \r\nI have a question,\
          \ the model is trained using float32 data type, but due to resource constraints,\
          \ I am performing inference with fp16. Does this significantly impact the\
          \ performance of the model? \r\nCurrently, I find it not performing as well\
          \ as Orca 1 when inferring with fp16."
        updatedAt: '2023-11-23T08:58:20.754Z'
      numEdits: 0
      reactions: []
    id: 655f142cf9829d0918fb926e
    type: comment
  author: wiccanmind
  content: "Thank you for contributing to this excellent model. \r\nI have a question,\
    \ the model is trained using float32 data type, but due to resource constraints,\
    \ I am performing inference with fp16. Does this significantly impact the performance\
    \ of the model? \r\nCurrently, I find it not performing as well as Orca 1 when\
    \ inferring with fp16."
  created_at: 2023-11-23 08:58:20+00:00
  edited: false
  hidden: false
  id: 655f142cf9829d0918fb926e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b5429d85c030a9f08e1d2bb37ce9974d.svg
      fullname: Arindam Mitra
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ari9dam
      type: user
    createdAt: '2023-11-27T18:55:11.000Z'
    data:
      edited: false
      editors:
      - ari9dam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.879964292049408
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b5429d85c030a9f08e1d2bb37ce9974d.svg
          fullname: Arindam Mitra
          isHf: false
          isPro: false
          name: ari9dam
          type: user
        html: '<p>The model is trained with bfloat16. With fp16 inference you might
          see a loss, but overall that affects both Orca 1 and Orca 2.  You can see
          the inference code here: <a href="https://huggingface.co/spaces/ari9dam/Orca-2-13B">https://huggingface.co/spaces/ari9dam/Orca-2-13B</a></p>

          <p>(imp : use slow version of the tokenizer)</p>

          '
        raw: 'The model is trained with bfloat16. With fp16 inference you might see
          a loss, but overall that affects both Orca 1 and Orca 2.  You can see the
          inference code here: https://huggingface.co/spaces/ari9dam/Orca-2-13B


          (imp : use slow version of the tokenizer)'
        updatedAt: '2023-11-27T18:55:11.667Z'
      numEdits: 0
      reactions: []
    id: 6564e60f3640137c1a9acbbe
    type: comment
  author: ari9dam
  content: 'The model is trained with bfloat16. With fp16 inference you might see
    a loss, but overall that affects both Orca 1 and Orca 2.  You can see the inference
    code here: https://huggingface.co/spaces/ari9dam/Orca-2-13B


    (imp : use slow version of the tokenizer)'
  created_at: 2023-11-27 18:55:11+00:00
  edited: false
  hidden: false
  id: 6564e60f3640137c1a9acbbe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64250ae54191f56e07339dd7/KVYP9MlmVIiH61jg9e2_r.jpeg?w=200&h=200&f=face
      fullname: "D\u0169ng Ph\xF9ng "
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wiccanmind
      type: user
    createdAt: '2023-11-28T02:06:39.000Z'
    data:
      edited: false
      editors:
      - wiccanmind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9042279124259949
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64250ae54191f56e07339dd7/KVYP9MlmVIiH61jg9e2_r.jpeg?w=200&h=200&f=face
          fullname: "D\u0169ng Ph\xF9ng "
          isHf: false
          isPro: false
          name: wiccanmind
          type: user
        html: '<blockquote>

          <p>The model is trained with bfloat16. With fp16 inference you might see
          a loss, but overall that affects both Orca 1 and Orca 2.  You can see the
          inference code here: <a href="https://huggingface.co/spaces/ari9dam/Orca-2-13B">https://huggingface.co/spaces/ari9dam/Orca-2-13B</a></p>

          <p>(imp : use slow version of the tokenizer)</p>

          </blockquote>

          <p>Thank you very much for your response.<br>As I see in the config.json
          file, Orca 2 used "torch_dtype": "float32",  in the other hand, Orca 1 used
          "torch_dtype": "bfloat16". Adding one more thing, the total weight file
          size of Orca 1 is 26GB, while that of Orca 2 is 53GB. It implies that Orca
          2 is storing weights in a data type that is twice the size of Orca 1. So
          I still do not quite understand your statement ''The model is trained with
          bfloat16.''.</p>

          '
        raw: "> The model is trained with bfloat16. With fp16 inference you might\
          \ see a loss, but overall that affects both Orca 1 and Orca 2.  You can\
          \ see the inference code here: https://huggingface.co/spaces/ari9dam/Orca-2-13B\n\
          > \n> (imp : use slow version of the tokenizer)\n\nThank you very much for\
          \ your response. \nAs I see in the config.json file, Orca 2 used \"torch_dtype\"\
          : \"float32\",  in the other hand, Orca 1 used \"torch_dtype\": \"bfloat16\"\
          . Adding one more thing, the total weight file size of Orca 1 is 26GB, while\
          \ that of Orca 2 is 53GB. It implies that Orca 2 is storing weights in a\
          \ data type that is twice the size of Orca 1. So I still do not quite understand\
          \ your statement 'The model is trained with bfloat16.'."
        updatedAt: '2023-11-28T02:06:39.428Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jasonden
    id: 65654b2f5de5b89145ad541a
    type: comment
  author: wiccanmind
  content: "> The model is trained with bfloat16. With fp16 inference you might see\
    \ a loss, but overall that affects both Orca 1 and Orca 2.  You can see the inference\
    \ code here: https://huggingface.co/spaces/ari9dam/Orca-2-13B\n> \n> (imp : use\
    \ slow version of the tokenizer)\n\nThank you very much for your response. \n\
    As I see in the config.json file, Orca 2 used \"torch_dtype\": \"float32\",  in\
    \ the other hand, Orca 1 used \"torch_dtype\": \"bfloat16\". Adding one more thing,\
    \ the total weight file size of Orca 1 is 26GB, while that of Orca 2 is 53GB.\
    \ It implies that Orca 2 is storing weights in a data type that is twice the size\
    \ of Orca 1. So I still do not quite understand your statement 'The model is trained\
    \ with bfloat16.'."
  created_at: 2023-11-28 02:06:39+00:00
  edited: false
  hidden: false
  id: 65654b2f5de5b89145ad541a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: microsoft/Orca-2-13b
repo_type: model
status: open
target_branch: null
title: Fp32 vs fp16
