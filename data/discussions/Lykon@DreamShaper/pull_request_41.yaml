!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Lykon
conflicting_files:
- text_encoder/pytorch_model.bin
- unet/diffusion_pytorch_model.bin
- vae/diffusion_pytorch_model.bin
created_at: 2023-06-15 20:55:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672788314686-noauth.png?w=200&h=200&f=face
      fullname: '-'
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Lykon
      type: user
    createdAt: '2023-06-15T21:55:29.000Z'
    data:
      edited: false
      editors:
      - Lykon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7829306125640869
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672788314686-noauth.png?w=200&h=200&f=face
          fullname: '-'
          isHf: false
          isPro: false
          name: Lykon
          type: user
        html: '<p>Add Diffusers weights converted from checkpoint</p>

          '
        raw: Add Diffusers weights converted from checkpoint
        updatedAt: '2023-06-15T21:55:29.301Z'
      numEdits: 0
      reactions: []
    id: 648b88d1fe22627c5930eb7b
    type: comment
  author: Lykon
  content: Add Diffusers weights converted from checkpoint
  created_at: 2023-06-15 20:55:29+00:00
  edited: false
  hidden: false
  id: 648b88d1fe22627c5930eb7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672788314686-noauth.png?w=200&h=200&f=face
      fullname: '-'
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Lykon
      type: user
    createdAt: '2023-06-15T21:55:29.000Z'
    data:
      oid: d773dddf08151dd3d357971270ad0e700b992006
      parents:
      - a3d26cab6bab7717315ce768f9882e0f4c7e72c4
      subject: DreamShaper upload
    id: 648b88d10000000000000000
    type: commit
  author: Lykon
  created_at: 2023-06-15 20:55:29+00:00
  id: 648b88d10000000000000000
  oid: d773dddf08151dd3d357971270ad0e700b992006
  summary: DreamShaper upload
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d710097a6808e1fdf959c8a91ecb1f4a.svg
      fullname: Leon Durivage
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RustyKettle
      type: user
    createdAt: '2023-06-16T03:17:39.000Z'
    data:
      edited: false
      editors:
      - RustyKettle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6465038657188416
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d710097a6808e1fdf959c8a91ecb1f4a.svg
          fullname: Leon Durivage
          isHf: false
          isPro: false
          name: RustyKettle
          type: user
        html: '<p>I am now getting the following error with this check-in.  </p>

          <p>Some weights of the model checkpoint at PATH were not used when initializing
          AutoencoderKL: [''encoder.mid_block.attentions.0.to_q.weight'', ''encoder.mid_block.attentions.0.to_k.bias'',
          ''decoder.mid_block.attentions.0.to_out.0.weight'', ''encoder.mid_block.attentions.0.to_out.0.bias'',
          ''encoder.mid_block.attentions.0.to_q.bias'', ''decoder.mid_block.attentions.0.to_k.bias'',
          ''decoder.mid_block.attentions.0.to_k.weight'', ''encoder.mid_block.attentions.0.to_v.weight'',
          ''decoder.mid_block.attentions.0.to_q.bias'', ''encoder.mid_block.attentions.0.to_v.bias'',
          ''decoder.mid_block.attentions.0.to_v.bias'', ''decoder.mid_block.attentions.0.to_v.weight'',
          ''decoder.mid_block.attentions.0.to_out.0.bias'', ''decoder.mid_block.attentions.0.to_q.weight'',
          ''encoder.mid_block.attentions.0.to_out.0.weight'', ''encoder.mid_block.attentions.0.to_k.weight'']</p>

          <ul>

          <li>This IS expected if you are initializing AutoencoderKL from the checkpoint
          of a model trained on another task or with another architecture (e.g. initializing
          a BertForSequenceClassification model from a BertForPreTraining model).</li>

          <li>This IS NOT expected if you are initializing AutoencoderKL from the
          checkpoint of a model that you expect to be exactly identical (initializing
          a BertForSequenceClassification model from a BertForSequenceClassification
          model).<br>Some weights of AutoencoderKL were not initialized from the model
          checkpoint at C:\Users\sarus.cache\huggingface\hub\models--Lykon--DreamShaper\snapshots\2023b93d140bf68aceff85474bfbfa5bd5afbaaf\vae
          and are newly initialized: [''encoder.mid_block.attentions.0.key.bias'',
          ''decoder.mid_block.attentions.0.key.weight'', ''decoder.mid_block.attentions.0.key.bias'',
          ''decoder.mid_block.attentions.0.value.weight'', ''encoder.mid_block.attentions.0.proj_attn.weight'',
          ''encoder.mid_block.attentions.0.query.bias'', ''decoder.mid_block.attentions.0.query.bias'',
          ''encoder.mid_block.attentions.0.value.bias'', ''encoder.mid_block.attentions.0.proj_attn.bias'',
          ''decoder.mid_block.attentions.0.proj_attn.weight'', ''decoder.mid_block.attentions.0.value.bias'',
          ''encoder.mid_block.attentions.0.key.weight'', ''decoder.mid_block.attentions.0.query.weight'',
          ''encoder.mid_block.attentions.0.query.weight'', ''encoder.mid_block.attentions.0.value.weight'',
          ''decoder.mid_block.attentions.0.proj_attn.bias'']<br>You should probably
          TRAIN this model on a down-stream task to be able to use it for predictions
          and inference.</li>

          </ul>

          '
        raw: "I am now getting the following error with this check-in.  \n\nSome weights\
          \ of the model checkpoint at PATH were not used when initializing AutoencoderKL:\
          \ ['encoder.mid_block.attentions.0.to_q.weight', 'encoder.mid_block.attentions.0.to_k.bias',\
          \ 'decoder.mid_block.attentions.0.to_out.0.weight', 'encoder.mid_block.attentions.0.to_out.0.bias',\
          \ 'encoder.mid_block.attentions.0.to_q.bias', 'decoder.mid_block.attentions.0.to_k.bias',\
          \ 'decoder.mid_block.attentions.0.to_k.weight', 'encoder.mid_block.attentions.0.to_v.weight',\
          \ 'decoder.mid_block.attentions.0.to_q.bias', 'encoder.mid_block.attentions.0.to_v.bias',\
          \ 'decoder.mid_block.attentions.0.to_v.bias', 'decoder.mid_block.attentions.0.to_v.weight',\
          \ 'decoder.mid_block.attentions.0.to_out.0.bias', 'decoder.mid_block.attentions.0.to_q.weight',\
          \ 'encoder.mid_block.attentions.0.to_out.0.weight', 'encoder.mid_block.attentions.0.to_k.weight']\n\
          - This IS expected if you are initializing AutoencoderKL from the checkpoint\
          \ of a model trained on another task or with another architecture (e.g.\
          \ initializing a BertForSequenceClassification model from a BertForPreTraining\
          \ model).\n- This IS NOT expected if you are initializing AutoencoderKL\
          \ from the checkpoint of a model that you expect to be exactly identical\
          \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).\nSome weights of AutoencoderKL were not initialized from the model\
          \ checkpoint at C:\\Users\\sarus\\.cache\\huggingface\\hub\\models--Lykon--DreamShaper\\\
          snapshots\\2023b93d140bf68aceff85474bfbfa5bd5afbaaf\\vae and are newly initialized:\
          \ ['encoder.mid_block.attentions.0.key.bias', 'decoder.mid_block.attentions.0.key.weight',\
          \ 'decoder.mid_block.attentions.0.key.bias', 'decoder.mid_block.attentions.0.value.weight',\
          \ 'encoder.mid_block.attentions.0.proj_attn.weight', 'encoder.mid_block.attentions.0.query.bias',\
          \ 'decoder.mid_block.attentions.0.query.bias', 'encoder.mid_block.attentions.0.value.bias',\
          \ 'encoder.mid_block.attentions.0.proj_attn.bias', 'decoder.mid_block.attentions.0.proj_attn.weight',\
          \ 'decoder.mid_block.attentions.0.value.bias', 'encoder.mid_block.attentions.0.key.weight',\
          \ 'decoder.mid_block.attentions.0.query.weight', 'encoder.mid_block.attentions.0.query.weight',\
          \ 'encoder.mid_block.attentions.0.value.weight', 'decoder.mid_block.attentions.0.proj_attn.bias']\n\
          You should probably TRAIN this model on a down-stream task to be able to\
          \ use it for predictions and inference."
        updatedAt: '2023-06-16T03:17:39.108Z'
      numEdits: 0
      reactions: []
    id: 648bd453b7026cc4ff4cd621
    type: comment
  author: RustyKettle
  content: "I am now getting the following error with this check-in.  \n\nSome weights\
    \ of the model checkpoint at PATH were not used when initializing AutoencoderKL:\
    \ ['encoder.mid_block.attentions.0.to_q.weight', 'encoder.mid_block.attentions.0.to_k.bias',\
    \ 'decoder.mid_block.attentions.0.to_out.0.weight', 'encoder.mid_block.attentions.0.to_out.0.bias',\
    \ 'encoder.mid_block.attentions.0.to_q.bias', 'decoder.mid_block.attentions.0.to_k.bias',\
    \ 'decoder.mid_block.attentions.0.to_k.weight', 'encoder.mid_block.attentions.0.to_v.weight',\
    \ 'decoder.mid_block.attentions.0.to_q.bias', 'encoder.mid_block.attentions.0.to_v.bias',\
    \ 'decoder.mid_block.attentions.0.to_v.bias', 'decoder.mid_block.attentions.0.to_v.weight',\
    \ 'decoder.mid_block.attentions.0.to_out.0.bias', 'decoder.mid_block.attentions.0.to_q.weight',\
    \ 'encoder.mid_block.attentions.0.to_out.0.weight', 'encoder.mid_block.attentions.0.to_k.weight']\n\
    - This IS expected if you are initializing AutoencoderKL from the checkpoint of\
    \ a model trained on another task or with another architecture (e.g. initializing\
    \ a BertForSequenceClassification model from a BertForPreTraining model).\n- This\
    \ IS NOT expected if you are initializing AutoencoderKL from the checkpoint of\
    \ a model that you expect to be exactly identical (initializing a BertForSequenceClassification\
    \ model from a BertForSequenceClassification model).\nSome weights of AutoencoderKL\
    \ were not initialized from the model checkpoint at C:\\Users\\sarus\\.cache\\\
    huggingface\\hub\\models--Lykon--DreamShaper\\snapshots\\2023b93d140bf68aceff85474bfbfa5bd5afbaaf\\\
    vae and are newly initialized: ['encoder.mid_block.attentions.0.key.bias', 'decoder.mid_block.attentions.0.key.weight',\
    \ 'decoder.mid_block.attentions.0.key.bias', 'decoder.mid_block.attentions.0.value.weight',\
    \ 'encoder.mid_block.attentions.0.proj_attn.weight', 'encoder.mid_block.attentions.0.query.bias',\
    \ 'decoder.mid_block.attentions.0.query.bias', 'encoder.mid_block.attentions.0.value.bias',\
    \ 'encoder.mid_block.attentions.0.proj_attn.bias', 'decoder.mid_block.attentions.0.proj_attn.weight',\
    \ 'decoder.mid_block.attentions.0.value.bias', 'encoder.mid_block.attentions.0.key.weight',\
    \ 'decoder.mid_block.attentions.0.query.weight', 'encoder.mid_block.attentions.0.query.weight',\
    \ 'encoder.mid_block.attentions.0.value.weight', 'decoder.mid_block.attentions.0.proj_attn.bias']\n\
    You should probably TRAIN this model on a down-stream task to be able to use it\
    \ for predictions and inference."
  created_at: 2023-06-16 02:17:39+00:00
  edited: false
  hidden: false
  id: 648bd453b7026cc4ff4cd621
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672788314686-noauth.png?w=200&h=200&f=face
      fullname: '-'
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Lykon
      type: user
    createdAt: '2023-06-16T07:31:58.000Z'
    data:
      edited: false
      editors:
      - Lykon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5638254880905151
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672788314686-noauth.png?w=200&h=200&f=face
          fullname: '-'
          isHf: false
          isPro: false
          name: Lykon
          type: user
        html: '<p>you need to update your diffusers lib to 0.17.1 or use a previous
          commit with DS6 instead of DS6.31</p>

          '
        raw: you need to update your diffusers lib to 0.17.1 or use a previous commit
          with DS6 instead of DS6.31
        updatedAt: '2023-06-16T07:31:58.531Z'
      numEdits: 0
      reactions: []
    id: 648c0fee1c113a0526283ce5
    type: comment
  author: Lykon
  content: you need to update your diffusers lib to 0.17.1 or use a previous commit
    with DS6 instead of DS6.31
  created_at: 2023-06-16 06:31:58+00:00
  edited: false
  hidden: false
  id: 648c0fee1c113a0526283ce5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1dc6d868be089fb1c4e91b7b76b1582b.svg
      fullname: Nicolai Klemke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: klemkeni
      type: user
    createdAt: '2023-06-18T11:56:10.000Z'
    data:
      edited: false
      editors:
      - klemkeni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.916111171245575
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1dc6d868be089fb1c4e91b7b76b1582b.svg
          fullname: Nicolai Klemke
          isHf: false
          isPro: false
          name: klemkeni
          type: user
        html: '<p>How do we load a previous commit with diffusers library?</p>

          '
        raw: How do we load a previous commit with diffusers library?
        updatedAt: '2023-06-18T11:56:10.489Z'
      numEdits: 0
      reactions: []
    id: 648ef0dad469965a66c27bf7
    type: comment
  author: klemkeni
  content: How do we load a previous commit with diffusers library?
  created_at: 2023-06-18 10:56:10+00:00
  edited: false
  hidden: false
  id: 648ef0dad469965a66c27bf7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672788314686-noauth.png?w=200&h=200&f=face
      fullname: '-'
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Lykon
      type: user
    createdAt: '2023-06-18T17:32:20.000Z'
    data:
      edited: false
      editors:
      - Lykon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.727245569229126
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672788314686-noauth.png?w=200&h=200&f=face
          fullname: '-'
          isHf: false
          isPro: false
          name: Lykon
          type: user
        html: '<p><a href="https://huggingface.co/Lykon/DreamShaper/commits/main">https://huggingface.co/Lykon/DreamShaper/commits/main</a></p>

          '
        raw: https://huggingface.co/Lykon/DreamShaper/commits/main
        updatedAt: '2023-06-18T17:32:20.340Z'
      numEdits: 0
      reactions: []
      relatedEventId: 648f3fa4a96dd2dac4d2f1c1
    id: 648f3fa4a96dd2dac4d2f1bf
    type: comment
  author: Lykon
  content: https://huggingface.co/Lykon/DreamShaper/commits/main
  created_at: 2023-06-18 16:32:20+00:00
  edited: false
  hidden: false
  id: 648f3fa4a96dd2dac4d2f1bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672788314686-noauth.png?w=200&h=200&f=face
      fullname: '-'
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Lykon
      type: user
    createdAt: '2023-06-18T17:32:20.000Z'
    data:
      status: closed
    id: 648f3fa4a96dd2dac4d2f1c1
    type: status-change
  author: Lykon
  created_at: 2023-06-18 16:32:20+00:00
  id: 648f3fa4a96dd2dac4d2f1c1
  new_status: closed
  type: status-change
is_pull_request: true
merge_commit_oid: null
num: 41
repo_id: Lykon/DreamShaper
repo_type: model
status: closed
target_branch: refs/heads/main
title: DreamShaper upload
