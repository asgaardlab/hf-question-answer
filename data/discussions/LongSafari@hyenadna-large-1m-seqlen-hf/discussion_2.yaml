!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tanhevg
conflicting_files: null
created_at: 2023-12-26 03:04:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9a9523c72f5d0757dc3e050ce381a692.svg
      fullname: Evgeny Tankhilevich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tanhevg
      type: user
    createdAt: '2023-12-26T03:04:48.000Z'
    data:
      edited: false
      editors:
      - tanhevg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9214383363723755
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9a9523c72f5d0757dc3e050ce381a692.svg
          fullname: Evgeny Tankhilevich
          isHf: false
          isPro: false
          name: tanhevg
          type: user
        html: '<p>Hi there! Thank you for the great model. Any reason why the attention
          mask has been removed in the latest version? It''s kind of inconsistent
          with other checkpoints (''medium'' and others). </p>

          <p>Thanks in advance,<br>Evgeny</p>

          '
        raw: "Hi there! Thank you for the great model. Any reason why the attention\
          \ mask has been removed in the latest version? It's kind of inconsistent\
          \ with other checkpoints ('medium' and others). \r\n\r\nThanks in advance,\r\
          \nEvgeny"
        updatedAt: '2023-12-26T03:04:48.111Z'
      numEdits: 0
      reactions: []
    id: 658a42d0367c76b8ee4aa14f
    type: comment
  author: tanhevg
  content: "Hi there! Thank you for the great model. Any reason why the attention\
    \ mask has been removed in the latest version? It's kind of inconsistent with\
    \ other checkpoints ('medium' and others). \r\n\r\nThanks in advance,\r\nEvgeny"
  created_at: 2023-12-26 03:04:48+00:00
  edited: false
  hidden: false
  id: 658a42d0367c76b8ee4aa14f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660312628256-60ba519750effef3a58beac3.png?w=200&h=200&f=face
      fullname: Matthew Carrigan
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Rocketknight1
      type: user
    createdAt: '2024-01-18T09:49:18.000Z'
    data:
      edited: false
      editors:
      - Rocketknight1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9669983983039856
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660312628256-60ba519750effef3a58beac3.png?w=200&h=200&f=face
          fullname: Matthew Carrigan
          isHf: true
          isPro: false
          name: Rocketknight1
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;tanhevg&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/tanhevg\">@<span class=\"\
          underline\">tanhevg</span></a></span>\n\n\t</span></span> - this is my fault,\
          \ I'm sorry! We actually plan to propagate this change to all of the HyenaDNA\
          \ models, since the attention mask doesn't really work for Hyena in the\
          \ same way that it does in transformers. I'm sorry for the period of incompatibility\
          \ between 1M and the other sizes, but the others will have the new behaviour\
          \ very soon!</p>\n"
        raw: Hi @tanhevg - this is my fault, I'm sorry! We actually plan to propagate
          this change to all of the HyenaDNA models, since the attention mask doesn't
          really work for Hyena in the same way that it does in transformers. I'm
          sorry for the period of incompatibility between 1M and the other sizes,
          but the others will have the new behaviour very soon!
        updatedAt: '2024-01-18T09:49:18.035Z'
      numEdits: 0
      reactions: []
    id: 65a8f41e91ec5d1ec6031b2a
    type: comment
  author: Rocketknight1
  content: Hi @tanhevg - this is my fault, I'm sorry! We actually plan to propagate
    this change to all of the HyenaDNA models, since the attention mask doesn't really
    work for Hyena in the same way that it does in transformers. I'm sorry for the
    period of incompatibility between 1M and the other sizes, but the others will
    have the new behaviour very soon!
  created_at: 2024-01-18 09:49:18+00:00
  edited: false
  hidden: false
  id: 65a8f41e91ec5d1ec6031b2a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: LongSafari/hyenadna-large-1m-seqlen-hf
repo_type: model
status: open
target_branch: null
title: Attention mask
