!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jonathanjordan21
conflicting_files: null
created_at: 2023-11-28 10:06:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/651c4fa8edc1d15d31028e62/Yt-92KlsinjgNMlDAFCoQ.jpeg?w=200&h=200&f=face
      fullname: Jonathan Jordan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jonathanjordan21
      type: user
    createdAt: '2023-11-28T10:06:07.000Z'
    data:
      edited: false
      editors:
      - jonathanjordan21
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7096360325813293
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/651c4fa8edc1d15d31028e62/Yt-92KlsinjgNMlDAFCoQ.jpeg?w=200&h=200&f=face
          fullname: Jonathan Jordan
          isHf: false
          isPro: false
          name: jonathanjordan21
          type: user
        html: '<p> I often get error from <code>ctransformers</code> when importing
          some GGUF models. Here is the code example</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          ctransformers <span class="hljs-keyword">import</span> AutoModelForCausalLM


          llm = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">"maddes8cht/acrastt-Marx-3B-V3-gguf"</span>,
          model_file=<span class="hljs-string">"maddes8cht/acrastt-Marx-3B-V3-gguf"</span>,
          model_type=<span class="hljs-string">''stablelm''</span>)

          <span class="hljs-built_in">print</span>(llm(<span class="hljs-string">"AI
          is going to"</span>))

          </code></pre>

          <p>It actually downloaded the <code>.gguf</code> file from repo, but returned
          this error message</p>

          <pre><code class="language-python">RuntimeError: Failed to create LLM <span
          class="hljs-string">''stablelm''</span> <span class="hljs-keyword">from</span>
          <span class="hljs-string">''/root/.cache/huggingface/hub/models--</span>

          </code></pre>

          <p>I got the same error with <code>TheBloke/Marx-3B-v3-GGUF</code> model</p>

          <p>I''ve tried using different model_type such as mistral and llama, any
          idea?</p>

          '
        raw: " I often get error from `ctransformers` when importing some GGUF models.\
          \ Here is the code example\r\n```python\r\nfrom ctransformers import AutoModelForCausalLM\r\
          \n\r\nllm = AutoModelForCausalLM.from_pretrained(\"maddes8cht/acrastt-Marx-3B-V3-gguf\"\
          , model_file=\"maddes8cht/acrastt-Marx-3B-V3-gguf\", model_type='stablelm')\r\
          \nprint(llm(\"AI is going to\"))\r\n```\r\nIt actually downloaded the `.gguf`\
          \ file from repo, but returned this error message\r\n```python\r\nRuntimeError:\
          \ Failed to create LLM 'stablelm' from '/root/.cache/huggingface/hub/models--\r\
          \n```\r\n\r\nI got the same error with `TheBloke/Marx-3B-v3-GGUF` model\r\
          \n\r\nI've tried using different model_type such as mistral and llama, any\
          \ idea?"
        updatedAt: '2023-11-28T10:06:07.683Z'
      numEdits: 0
      reactions: []
    id: 6565bb8f459394ff719ceaf3
    type: comment
  author: jonathanjordan21
  content: " I often get error from `ctransformers` when importing some GGUF models.\
    \ Here is the code example\r\n```python\r\nfrom ctransformers import AutoModelForCausalLM\r\
    \n\r\nllm = AutoModelForCausalLM.from_pretrained(\"maddes8cht/acrastt-Marx-3B-V3-gguf\"\
    , model_file=\"maddes8cht/acrastt-Marx-3B-V3-gguf\", model_type='stablelm')\r\n\
    print(llm(\"AI is going to\"))\r\n```\r\nIt actually downloaded the `.gguf` file\
    \ from repo, but returned this error message\r\n```python\r\nRuntimeError: Failed\
    \ to create LLM 'stablelm' from '/root/.cache/huggingface/hub/models--\r\n```\r\
    \n\r\nI got the same error with `TheBloke/Marx-3B-v3-GGUF` model\r\n\r\nI've tried\
    \ using different model_type such as mistral and llama, any idea?"
  created_at: 2023-11-28 10:06:07+00:00
  edited: false
  hidden: false
  id: 6565bb8f459394ff719ceaf3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: maddes8cht/acrastt-Marx-3B-V3-gguf
repo_type: model
status: open
target_branch: null
title: 'Runtime Error : Failed to create LLM'
