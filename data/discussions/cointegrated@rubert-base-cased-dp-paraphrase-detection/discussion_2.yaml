!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fikavec
conflicting_files: null
created_at: 2023-08-08 15:59:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6cba9a99b3cf3aa76d73bbe23de03750.svg
      fullname: fikavec
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fikavec
      type: user
    createdAt: '2023-08-08T16:59:00.000Z'
    data:
      edited: false
      editors:
      - fikavec
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5878438949584961
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6cba9a99b3cf3aa76d73bbe23de03750.svg
          fullname: fikavec
          isHf: false
          isPro: false
          name: fikavec
          type: user
        html: '<p>On "merionum/ru_paraphraser" (test samples) paraphrase dataset I
          got the following results:</p>

          <ul>

          <li>This model accuracy: 0.8497920997920998</li>

          <li>sentence-transformers/LaBSE accuracy: 0.7785862785862786</li>

          <li>sentence-transformers/paraphrase-multilingual-mpnet-base-v2 accuracy:
          0.7791060291060291</li>

          <li>sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 accuracy:
          0.7567567567567568</li>

          <li>sentence-transformers/distiluse-base-multilingual-cased-v1 accuracy:
          0.752079002079002</li>

          </ul>

          <p>Maybe some other models should be tested?</p>

          '
        raw: "On \"merionum/ru_paraphraser\" (test samples) paraphrase dataset I got\
          \ the following results:\r\n- This model accuracy: 0.8497920997920998\r\n\
          - sentence-transformers/LaBSE accuracy: 0.7785862785862786\r\n- sentence-transformers/paraphrase-multilingual-mpnet-base-v2\
          \ accuracy: 0.7791060291060291\r\n- sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\
          \ accuracy: 0.7567567567567568\r\n- sentence-transformers/distiluse-base-multilingual-cased-v1\
          \ accuracy: 0.752079002079002\r\n\r\nMaybe some other models should be tested?"
        updatedAt: '2023-08-08T16:59:00.148Z'
      numEdits: 0
      reactions: []
    id: 64d27454c76d716b0eb6c1c7
    type: comment
  author: fikavec
  content: "On \"merionum/ru_paraphraser\" (test samples) paraphrase dataset I got\
    \ the following results:\r\n- This model accuracy: 0.8497920997920998\r\n- sentence-transformers/LaBSE\
    \ accuracy: 0.7785862785862786\r\n- sentence-transformers/paraphrase-multilingual-mpnet-base-v2\
    \ accuracy: 0.7791060291060291\r\n- sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\
    \ accuracy: 0.7567567567567568\r\n- sentence-transformers/distiluse-base-multilingual-cased-v1\
    \ accuracy: 0.752079002079002\r\n\r\nMaybe some other models should be tested?"
  created_at: 2023-08-08 15:59:00+00:00
  edited: false
  hidden: false
  id: 64d27454c76d716b0eb6c1c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661470101034-605a1630f70acedea1f74abc.png?w=200&h=200&f=face
      fullname: David Dale
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: cointegrated
      type: user
    createdAt: '2023-08-09T14:39:25.000Z'
    data:
      edited: false
      editors:
      - cointegrated
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8992597460746765
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661470101034-605a1630f70acedea1f74abc.png?w=200&h=200&f=face
          fullname: David Dale
          isHf: false
          isPro: false
          name: cointegrated
          type: user
        html: '<blockquote>

          <p>It is possible to use this model in Vector Similarity Based Approach
          (cosine) to detect paraphrases</p>

          </blockquote>

          <p>No, this model is not intended to produce embeddings pluggable into cosine
          similarity computation. Instead, you should use it as a cross-encoder (the
          example code snipped is in the model card). </p>

          '
        raw: "> It is possible to use this model in Vector Similarity Based Approach\
          \ (cosine) to detect paraphrases\n\nNo, this model is not intended to produce\
          \ embeddings pluggable into cosine similarity computation. Instead, you\
          \ should use it as a cross-encoder (the example code snipped is in the model\
          \ card). \n"
        updatedAt: '2023-08-09T14:39:25.596Z'
      numEdits: 0
      reactions: []
    id: 64d3a51d9d841157005847b6
    type: comment
  author: cointegrated
  content: "> It is possible to use this model in Vector Similarity Based Approach\
    \ (cosine) to detect paraphrases\n\nNo, this model is not intended to produce\
    \ embeddings pluggable into cosine similarity computation. Instead, you should\
    \ use it as a cross-encoder (the example code snipped is in the model card). \n"
  created_at: 2023-08-09 13:39:25+00:00
  edited: false
  hidden: false
  id: 64d3a51d9d841157005847b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661470101034-605a1630f70acedea1f74abc.png?w=200&h=200&f=face
      fullname: David Dale
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: cointegrated
      type: user
    createdAt: '2023-08-09T14:42:00.000Z'
    data:
      edited: true
      editors:
      - cointegrated
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8874061703681946
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661470101034-605a1630f70acedea1f74abc.png?w=200&h=200&f=face
          fullname: David Dale
          isHf: false
          isPro: false
          name: cointegrated
          type: user
        html: '<blockquote>

          <p>Maybe some other models should be tested?</p>

          </blockquote>

          <p>If you want a pipeline "embeddings+cosine similarities",  I can recommend
          looking at models from this benchmark: <a rel="nofollow" href="https://github.com/avidale/encodechka">https://github.com/avidale/encodechka</a></p>

          <p>If you want a cross-encoder, maybe you can try this model (trained by
          me): <a href="https://huggingface.co/s-nlp/ruRoberta-large-paraphrase-v1">https://huggingface.co/s-nlp/ruRoberta-large-paraphrase-v1</a></p>

          '
        raw: '> Maybe some other models should be tested?


          If you want a pipeline "embeddings+cosine similarities",  I can recommend
          looking at models from this benchmark: https://github.com/avidale/encodechka


          If you want a cross-encoder, maybe you can try this model (trained by me):
          https://huggingface.co/s-nlp/ruRoberta-large-paraphrase-v1'
        updatedAt: '2023-08-09T14:42:54.152Z'
      numEdits: 1
      reactions: []
    id: 64d3a5b8dae613392cb9e714
    type: comment
  author: cointegrated
  content: '> Maybe some other models should be tested?


    If you want a pipeline "embeddings+cosine similarities",  I can recommend looking
    at models from this benchmark: https://github.com/avidale/encodechka


    If you want a cross-encoder, maybe you can try this model (trained by me): https://huggingface.co/s-nlp/ruRoberta-large-paraphrase-v1'
  created_at: 2023-08-09 13:42:00+00:00
  edited: true
  hidden: false
  id: 64d3a5b8dae613392cb9e714
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6cba9a99b3cf3aa76d73bbe23de03750.svg
      fullname: fikavec
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fikavec
      type: user
    createdAt: '2023-08-09T23:26:30.000Z'
    data:
      edited: false
      editors:
      - fikavec
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4401538074016571
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6cba9a99b3cf3aa76d73bbe23de03750.svg
          fullname: fikavec
          isHf: false
          isPro: false
          name: fikavec
          type: user
        html: "<p>Thanks for the reply, just for scientific understanding (I will\
          \ be happy to even \"yes\"/\"no\" answers ):</p>\n<ul>\n<li>Do cross-encoders\
          \ exceed the maximum achievable results of the \"embeddings+cosine similarities\"\
          \ approaches in terms of performance in the task of detecting paraphrasing?</li>\n\
          <li>Are there any methods of scaling (precomputing) cross-encoders, because\
          \ O(n^2) doesn't look like the best option for many tasks?</li>\n<li>In\
          \ the encodechka table, is the paraphrasing detection task quality equivalent\
          \ to the quality of the models in the STS task?</li>\n</ul>\n<p>It's strange,\
          \ but 's-nlp/ruRoberta-large-paraphrase-v1' showed the lowest accuracy of\
          \ 0.59 (with threshold tune 0.66) among the compared models:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">from</span> datasets\
          \ <span class=\"hljs-keyword\">import</span> load_dataset\nHF_DATASET =\
          \ <span class=\"hljs-string\">\"merionum/ru_paraphraser\"</span>\nrecords\
          \ = <span class=\"hljs-built_in\">list</span>(load_dataset(HF_DATASET, split=<span\
          \ class=\"hljs-string\">\"test\"</span>))\n<span class=\"hljs-keyword\"\
          >for</span> item <span class=\"hljs-keyword\">in</span> records:\n    <span\
          \ class=\"hljs-built_in\">print</span>(item[<span class=\"hljs-string\"\
          >'text_1'</span>],item[<span class=\"hljs-string\">'text_2'</span>],item[<span\
          \ class=\"hljs-string\">'class'</span>])\n    <span class=\"hljs-keyword\"\
          >break</span>\n    \n<span class=\"hljs-keyword\">import</span> torch\n\
          <span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoModelForSequenceClassification, AutoTokenizer\nmodel_name\
          \ = <span class=\"hljs-string\">'s-nlp/ruRoberta-large-paraphrase-v1'</span>\n\
          model = AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n\
          tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n<span class=\"\
          hljs-keyword\">def</span> <span class=\"hljs-title function_\">compare_texts</span>(<span\
          \ class=\"hljs-params\">text1, text2</span>):\n    batch = tokenizer(text1,\
          \ text2, return_tensors=<span class=\"hljs-string\">'pt'</span>).to(model.device)\n\
          \    <span class=\"hljs-keyword\">with</span> torch.inference_mode():\n\
          \        proba = torch.softmax(model(**batch).logits, -<span class=\"hljs-number\"\
          >1</span>).cpu().numpy()\n    <span class=\"hljs-keyword\">return</span>\
          \ proba[<span class=\"hljs-number\">0</span>] <span class=\"hljs-comment\"\
          ># p(non-paraphrase), p(paraphrase)</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(compare_texts(<span class=\"hljs-string\">'\u0421\u0435\u0433\
          \u043E\u0434\u043D\u044F \u043D\u0430 \u0443\u043B\u0438\u0446\u0435 \u0445\
          \u043E\u0440\u043E\u0448\u0430\u044F \u043F\u043E\u0433\u043E\u0434\u0430\
          '</span>, <span class=\"hljs-string\">'\u0421\u0435\u0433\u043E\u0434\u043D\
          \u044F \u043D\u0430 \u0443\u043B\u0438\u0446\u0435 \u043E\u0442\u0432\u0440\
          \u0430\u0442\u0438\u0442\u0435\u043B\u044C\u043D\u0430\u044F \u043F\u043E\
          \u0433\u043E\u0434\u0430'</span>))\n<span class=\"hljs-comment\"># [0.9936753\
          \ 0.0063247]</span>\n<span class=\"hljs-built_in\">print</span>(compare_texts(<span\
          \ class=\"hljs-string\">'\u0421\u0435\u0433\u043E\u0434\u043D\u044F \u043D\
          \u0430 \u0443\u043B\u0438\u0446\u0435 \u0445\u043E\u0440\u043E\u0448\u0430\
          \u044F \u043F\u043E\u0433\u043E\u0434\u0430'</span>, <span class=\"hljs-string\"\
          >'\u041E\u0442\u043B\u0438\u0447\u043D\u0430\u044F \u043F\u043E\u0433\u043E\
          \u0434\u043A\u0430 \u0441\u0435\u0433\u043E\u0434\u043D\u044F \u0432\u044B\
          \u0434\u0430\u043B\u0430\u0441\u044C'</span>))\n<span class=\"hljs-comment\"\
          ># [0.00542064 0.99457943]</span>\n\n<span class=\"hljs-keyword\">from</span>\
          \ sklearn.metrics <span class=\"hljs-keyword\">import</span> classification_report,\
          \ accuracy_score, roc_auc_score\n<span class=\"hljs-keyword\">from</span>\
          \ tqdm.auto <span class=\"hljs-keyword\">import</span> tqdm\ny_true = []\n\
          y_pred = []\ny_probas = []\n<span class=\"hljs-keyword\">for</span> item\
          \ <span class=\"hljs-keyword\">in</span> tqdm(records):\n    <span class=\"\
          hljs-comment\">#print(item['text_1'],item['text_2'],item['class'])</span>\n\
          \    y_true.append(<span class=\"hljs-number\">0</span>) <span class=\"\
          hljs-keyword\">if</span> ( <span class=\"hljs-built_in\">int</span>(item[<span\
          \ class=\"hljs-string\">'class'</span>]) &lt; <span class=\"hljs-number\"\
          >0</span>) <span class=\"hljs-keyword\">else</span> y_true.append(<span\
          \ class=\"hljs-number\">1</span>)\n    \n    paraphrase_proba = compare_texts(item[<span\
          \ class=\"hljs-string\">'text_1'</span>],item[<span class=\"hljs-string\"\
          >'text_2'</span>])[<span class=\"hljs-number\">1</span>]\n    <span class=\"\
          hljs-keyword\">if</span> ( paraphrase_proba &gt;= <span class=\"hljs-number\"\
          >0.5</span>):\n        y_pred.append(<span class=\"hljs-number\">1</span>)\n\
          \    <span class=\"hljs-keyword\">else</span>:\n        y_pred.append(<span\
          \ class=\"hljs-number\">0</span>)\n    y_probas.append(paraphrase_proba)\n\
          <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\
          Accuracy:\"</span>,accuracy_score(y_true, y_pred))\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"ROC AUC:\"</span>,roc_auc_score(y_true,\
          \ y_probas))\n<span class=\"hljs-built_in\">print</span>(classification_report(y_true,\
          \ y_pred))\n</code></pre>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63ed582c679c2cc40ab8c8e2/tDykmnyRMWVvwDXEwH6P8.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63ed582c679c2cc40ab8c8e2/tDykmnyRMWVvwDXEwH6P8.png\"\
          ></a></p>\n<p>Replacing model_name = 'cointegrated/rubert-base-cased-dp-paraphrase-detection'\
          \ in the code above shows an accuracy of 0.85:</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/63ed582c679c2cc40ab8c8e2/ZP_yv-UsGTUmpJmmbx4YH.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63ed582c679c2cc40ab8c8e2/ZP_yv-UsGTUmpJmmbx4YH.png\"\
          ></a></p>\n"
        raw: "Thanks for the reply, just for scientific understanding (I will be happy\
          \ to even \"yes\"/\"no\" answers ):\n- Do cross-encoders exceed the maximum\
          \ achievable results of the \"embeddings+cosine similarities\" approaches\
          \ in terms of performance in the task of detecting paraphrasing?\n- Are\
          \ there any methods of scaling (precomputing) cross-encoders, because O(n^2)\
          \ doesn't look like the best option for many tasks?\n- In the encodechka\
          \ table, is the paraphrasing detection task quality equivalent to the quality\
          \ of the models in the STS task?\n\nIt's strange, but 's-nlp/ruRoberta-large-paraphrase-v1'\
          \ showed the lowest accuracy of 0.59 (with threshold tune 0.66) among the\
          \ compared models:\n```python\nfrom datasets import load_dataset\nHF_DATASET\
          \ = \"merionum/ru_paraphraser\"\nrecords = list(load_dataset(HF_DATASET,\
          \ split=\"test\"))\nfor item in records:\n    print(item['text_1'],item['text_2'],item['class'])\n\
          \    break\n    \nimport torch\nfrom transformers import AutoModelForSequenceClassification,\
          \ AutoTokenizer\nmodel_name = 's-nlp/ruRoberta-large-paraphrase-v1'\nmodel\
          \ = AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n\
          tokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef compare_texts(text1,\
          \ text2):\n    batch = tokenizer(text1, text2, return_tensors='pt').to(model.device)\n\
          \    with torch.inference_mode():\n        proba = torch.softmax(model(**batch).logits,\
          \ -1).cpu().numpy()\n    return proba[0] # p(non-paraphrase), p(paraphrase)\n\
          \nprint(compare_texts('\u0421\u0435\u0433\u043E\u0434\u043D\u044F \u043D\
          \u0430 \u0443\u043B\u0438\u0446\u0435 \u0445\u043E\u0440\u043E\u0448\u0430\
          \u044F \u043F\u043E\u0433\u043E\u0434\u0430', '\u0421\u0435\u0433\u043E\u0434\
          \u043D\u044F \u043D\u0430 \u0443\u043B\u0438\u0446\u0435 \u043E\u0442\u0432\
          \u0440\u0430\u0442\u0438\u0442\u0435\u043B\u044C\u043D\u0430\u044F \u043F\
          \u043E\u0433\u043E\u0434\u0430'))\n# [0.9936753 0.0063247]\nprint(compare_texts('\u0421\
          \u0435\u0433\u043E\u0434\u043D\u044F \u043D\u0430 \u0443\u043B\u0438\u0446\
          \u0435 \u0445\u043E\u0440\u043E\u0448\u0430\u044F \u043F\u043E\u0433\u043E\
          \u0434\u0430', '\u041E\u0442\u043B\u0438\u0447\u043D\u0430\u044F \u043F\u043E\
          \u0433\u043E\u0434\u043A\u0430 \u0441\u0435\u0433\u043E\u0434\u043D\u044F\
          \ \u0432\u044B\u0434\u0430\u043B\u0430\u0441\u044C'))\n# [0.00542064 0.99457943]\n\
          \nfrom sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n\
          from tqdm.auto import tqdm\ny_true = []\ny_pred = []\ny_probas = []\nfor\
          \ item in tqdm(records):\n    #print(item['text_1'],item['text_2'],item['class'])\n\
          \    y_true.append(0) if ( int(item['class']) < 0) else y_true.append(1)\n\
          \    \n    paraphrase_proba = compare_texts(item['text_1'],item['text_2'])[1]\n\
          \    if ( paraphrase_proba >= 0.5):\n        y_pred.append(1)\n    else:\n\
          \        y_pred.append(0)\n    y_probas.append(paraphrase_proba)\nprint(\"\
          Accuracy:\",accuracy_score(y_true, y_pred))\nprint(\"ROC AUC:\",roc_auc_score(y_true,\
          \ y_probas))\nprint(classification_report(y_true, y_pred))\n```\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63ed582c679c2cc40ab8c8e2/tDykmnyRMWVvwDXEwH6P8.png)\n\
          \nReplacing model_name = 'cointegrated/rubert-base-cased-dp-paraphrase-detection'\
          \ in the code above shows an accuracy of 0.85:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63ed582c679c2cc40ab8c8e2/ZP_yv-UsGTUmpJmmbx4YH.png)\n"
        updatedAt: '2023-08-09T23:26:30.724Z'
      numEdits: 0
      reactions: []
    id: 64d420a687169466f47ba82d
    type: comment
  author: fikavec
  content: "Thanks for the reply, just for scientific understanding (I will be happy\
    \ to even \"yes\"/\"no\" answers ):\n- Do cross-encoders exceed the maximum achievable\
    \ results of the \"embeddings+cosine similarities\" approaches in terms of performance\
    \ in the task of detecting paraphrasing?\n- Are there any methods of scaling (precomputing)\
    \ cross-encoders, because O(n^2) doesn't look like the best option for many tasks?\n\
    - In the encodechka table, is the paraphrasing detection task quality equivalent\
    \ to the quality of the models in the STS task?\n\nIt's strange, but 's-nlp/ruRoberta-large-paraphrase-v1'\
    \ showed the lowest accuracy of 0.59 (with threshold tune 0.66) among the compared\
    \ models:\n```python\nfrom datasets import load_dataset\nHF_DATASET = \"merionum/ru_paraphraser\"\
    \nrecords = list(load_dataset(HF_DATASET, split=\"test\"))\nfor item in records:\n\
    \    print(item['text_1'],item['text_2'],item['class'])\n    break\n    \nimport\
    \ torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\
    model_name = 's-nlp/ruRoberta-large-paraphrase-v1'\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n\
    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef compare_texts(text1,\
    \ text2):\n    batch = tokenizer(text1, text2, return_tensors='pt').to(model.device)\n\
    \    with torch.inference_mode():\n        proba = torch.softmax(model(**batch).logits,\
    \ -1).cpu().numpy()\n    return proba[0] # p(non-paraphrase), p(paraphrase)\n\n\
    print(compare_texts('\u0421\u0435\u0433\u043E\u0434\u043D\u044F \u043D\u0430 \u0443\
    \u043B\u0438\u0446\u0435 \u0445\u043E\u0440\u043E\u0448\u0430\u044F \u043F\u043E\
    \u0433\u043E\u0434\u0430', '\u0421\u0435\u0433\u043E\u0434\u043D\u044F \u043D\u0430\
    \ \u0443\u043B\u0438\u0446\u0435 \u043E\u0442\u0432\u0440\u0430\u0442\u0438\u0442\
    \u0435\u043B\u044C\u043D\u0430\u044F \u043F\u043E\u0433\u043E\u0434\u0430'))\n\
    # [0.9936753 0.0063247]\nprint(compare_texts('\u0421\u0435\u0433\u043E\u0434\u043D\
    \u044F \u043D\u0430 \u0443\u043B\u0438\u0446\u0435 \u0445\u043E\u0440\u043E\u0448\
    \u0430\u044F \u043F\u043E\u0433\u043E\u0434\u0430', '\u041E\u0442\u043B\u0438\u0447\
    \u043D\u0430\u044F \u043F\u043E\u0433\u043E\u0434\u043A\u0430 \u0441\u0435\u0433\
    \u043E\u0434\u043D\u044F \u0432\u044B\u0434\u0430\u043B\u0430\u0441\u044C'))\n\
    # [0.00542064 0.99457943]\n\nfrom sklearn.metrics import classification_report,\
    \ accuracy_score, roc_auc_score\nfrom tqdm.auto import tqdm\ny_true = []\ny_pred\
    \ = []\ny_probas = []\nfor item in tqdm(records):\n    #print(item['text_1'],item['text_2'],item['class'])\n\
    \    y_true.append(0) if ( int(item['class']) < 0) else y_true.append(1)\n   \
    \ \n    paraphrase_proba = compare_texts(item['text_1'],item['text_2'])[1]\n \
    \   if ( paraphrase_proba >= 0.5):\n        y_pred.append(1)\n    else:\n    \
    \    y_pred.append(0)\n    y_probas.append(paraphrase_proba)\nprint(\"Accuracy:\"\
    ,accuracy_score(y_true, y_pred))\nprint(\"ROC AUC:\",roc_auc_score(y_true, y_probas))\n\
    print(classification_report(y_true, y_pred))\n```\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63ed582c679c2cc40ab8c8e2/tDykmnyRMWVvwDXEwH6P8.png)\n\
    \nReplacing model_name = 'cointegrated/rubert-base-cased-dp-paraphrase-detection'\
    \ in the code above shows an accuracy of 0.85:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63ed582c679c2cc40ab8c8e2/ZP_yv-UsGTUmpJmmbx4YH.png)\n"
  created_at: 2023-08-09 22:26:30+00:00
  edited: false
  hidden: false
  id: 64d420a687169466f47ba82d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/6cba9a99b3cf3aa76d73bbe23de03750.svg
      fullname: fikavec
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fikavec
      type: user
    createdAt: '2023-08-13T11:36:24.000Z'
    data:
      status: closed
    id: 64d8c038089bc502cedc8b96
    type: status-change
  author: fikavec
  created_at: 2023-08-13 10:36:24+00:00
  id: 64d8c038089bc502cedc8b96
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661470101034-605a1630f70acedea1f74abc.png?w=200&h=200&f=face
      fullname: David Dale
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: cointegrated
      type: user
    createdAt: '2023-08-14T08:32:43.000Z'
    data:
      edited: false
      editors:
      - cointegrated
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9128270149230957
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661470101034-605a1630f70acedea1f74abc.png?w=200&h=200&f=face
          fullname: David Dale
          isHf: false
          isPro: false
          name: cointegrated
          type: user
        html: '<blockquote>

          <p>Do cross-encoders exceed the maximum achievable results of the "embeddings+cosine
          similarities" approaches in terms of performance in the task of detecting
          paraphrasing?</p>

          </blockquote>

          <p>I don''t know; probably "the maximum achievable results" depend on the
          difficulty of the paraphrases that you want to detect. </p>

          '
        raw: '> Do cross-encoders exceed the maximum achievable results of the "embeddings+cosine
          similarities" approaches in terms of performance in the task of detecting
          paraphrasing?


          I don''t know; probably "the maximum achievable results" depend on the difficulty
          of the paraphrases that you want to detect. '
        updatedAt: '2023-08-14T08:32:43.172Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - fikavec
    id: 64d9e6ab5a0d3c3c343b0662
    type: comment
  author: cointegrated
  content: '> Do cross-encoders exceed the maximum achievable results of the "embeddings+cosine
    similarities" approaches in terms of performance in the task of detecting paraphrasing?


    I don''t know; probably "the maximum achievable results" depend on the difficulty
    of the paraphrases that you want to detect. '
  created_at: 2023-08-14 07:32:43+00:00
  edited: false
  hidden: false
  id: 64d9e6ab5a0d3c3c343b0662
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661470101034-605a1630f70acedea1f74abc.png?w=200&h=200&f=face
      fullname: David Dale
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: cointegrated
      type: user
    createdAt: '2023-08-14T08:33:42.000Z'
    data:
      edited: false
      editors:
      - cointegrated
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9467169046401978
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661470101034-605a1630f70acedea1f74abc.png?w=200&h=200&f=face
          fullname: David Dale
          isHf: false
          isPro: false
          name: cointegrated
          type: user
        html: '<blockquote>

          <p>Are there any methods of scaling (precomputing) cross-encoders, because
          O(n^2) doesn''t look like the best option for many tasks?</p>

          </blockquote>

          <p>No, AFAIC. If you need to compare many-to-many texts, you''ll have to
          use bi-encoders (or poly-encoders, which are a hybrid)</p>

          '
        raw: '> Are there any methods of scaling (precomputing) cross-encoders, because
          O(n^2) doesn''t look like the best option for many tasks?


          No, AFAIC. If you need to compare many-to-many texts, you''ll have to use
          bi-encoders (or poly-encoders, which are a hybrid)'
        updatedAt: '2023-08-14T08:33:42.423Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - fikavec
    id: 64d9e6e69d1a4aab323effc1
    type: comment
  author: cointegrated
  content: '> Are there any methods of scaling (precomputing) cross-encoders, because
    O(n^2) doesn''t look like the best option for many tasks?


    No, AFAIC. If you need to compare many-to-many texts, you''ll have to use bi-encoders
    (or poly-encoders, which are a hybrid)'
  created_at: 2023-08-14 07:33:42+00:00
  edited: false
  hidden: false
  id: 64d9e6e69d1a4aab323effc1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661470101034-605a1630f70acedea1f74abc.png?w=200&h=200&f=face
      fullname: David Dale
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: cointegrated
      type: user
    createdAt: '2023-08-14T08:34:31.000Z'
    data:
      edited: true
      editors:
      - cointegrated
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9195541739463806
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661470101034-605a1630f70acedea1f74abc.png?w=200&h=200&f=face
          fullname: David Dale
          isHf: false
          isPro: false
          name: cointegrated
          type: user
        html: '<blockquote>

          <p>In the encodechka table, is the paraphrasing detection task quality equivalent
          to the quality of the models in the STS task?</p>

          </blockquote>

          <p>No; performance in these two tasks is correlated, but not identical.
          </p>

          '
        raw: '> In the encodechka table, is the paraphrasing detection task quality
          equivalent to the quality of the models in the STS task?


          No; performance in these two tasks is correlated, but not identical. '
        updatedAt: '2023-08-14T08:34:39.959Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - fikavec
    id: 64d9e717c79ca7ce77075c91
    type: comment
  author: cointegrated
  content: '> In the encodechka table, is the paraphrasing detection task quality
    equivalent to the quality of the models in the STS task?


    No; performance in these two tasks is correlated, but not identical. '
  created_at: 2023-08-14 07:34:31+00:00
  edited: true
  hidden: false
  id: 64d9e717c79ca7ce77075c91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6cba9a99b3cf3aa76d73bbe23de03750.svg
      fullname: fikavec
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fikavec
      type: user
    createdAt: '2023-08-14T10:42:26.000Z'
    data:
      edited: true
      editors:
      - fikavec
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8085609674453735
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6cba9a99b3cf3aa76d73bbe23de03750.svg
          fullname: fikavec
          isHf: false
          isPro: false
          name: fikavec
          type: user
        html: "<p>I appreciate your reply. There are a few thoughts that I need to\
          \ report by you:</p>\n<ul>\n<li>maybe add a <strong>transcript of abbreviations</strong>\
          \ to table header or before encodechka task tables in README.md of <a rel=\"\
          nofollow\" href=\"https://github.com/avidale/encodechka\">https://github.com/avidale/encodechka</a>\
          \ from colab v2023:</li>\n</ul>\n<pre><code class=\"language-python\">tmp\
          \ = tmp.rename({<span class=\"hljs-string\">'FactRuTask'</span>: <span class=\"\
          hljs-string\">'NE1'</span>,\n <span class=\"hljs-string\">'InappropriatenessTask'</span>:\
          \ <span class=\"hljs-string\">'IA'</span>,\n <span class=\"hljs-string\"\
          >'IntentsTask'</span>: <span class=\"hljs-string\">'IC'</span>,\n <span\
          \ class=\"hljs-string\">'IntentsXTask'</span>: <span class=\"hljs-string\"\
          >'ICX'</span>,\n <span class=\"hljs-string\">'ParaphraserTask'</span>: <span\
          \ class=\"hljs-string\">'PI'</span>,\n <span class=\"hljs-string\">'RudrTask'</span>:\
          \ <span class=\"hljs-string\">'NE2'</span>,\n <span class=\"hljs-string\"\
          >'STSBTask'</span>: <span class=\"hljs-string\">'STS'</span>,\n <span class=\"\
          hljs-string\">'SentimentTask'</span>: <span class=\"hljs-string\">'SA'</span>,\n\
          \ <span class=\"hljs-string\">'ToxicityTask'</span>: <span class=\"hljs-string\"\
          >'TI'</span>,\n <span class=\"hljs-string\">'XnliTask'</span>: <span class=\"\
          hljs-string\">'NLI'</span>,\n <span class=\"hljs-string\">'cpu_speed'</span>:\
          \ <span class=\"hljs-string\">'CPU'</span>,\n <span class=\"hljs-string\"\
          >'disk_size'</span>: <span class=\"hljs-string\">'size'</span>,\n <span\
          \ class=\"hljs-string\">'gpu_speed'</span>: <span class=\"hljs-string\"\
          >'GPU'</span>,\n <span class=\"hljs-string\">'mean_s'</span>: <span class=\"\
          hljs-string\">'Mean S'</span>,\n <span class=\"hljs-string\">'mean_sw'</span>:\
          \ <span class=\"hljs-string\">'Mean S+W'</span>,}, axis=<span class=\"hljs-number\"\
          >1</span>)\n</code></pre>\n<ul>\n<li>To run encodechka colab v2023 in jupyter\
          \ under windows, I had to <strong>small fix</strong> replace in tasks.py:</li>\n\
          </ul>\n<pre><code class=\"language-python\"><span class=\"hljs-comment\"\
          ># all encountered of open function</span>\n<span class=\"hljs-keyword\"\
          >with</span> <span class=\"hljs-built_in\">open</span>(find_file(filename),\
          \ <span class=\"hljs-string\">'r'</span>) <span class=\"hljs-keyword\">as</span>\
          \ f:\n<span class=\"hljs-comment\"># replaced to open with encoding='utf-8'</span>\n\
          <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(find_file(filename),\
          \ <span class=\"hljs-string\">'r'</span>, encoding=<span class=\"hljs-string\"\
          >'utf-8'</span>) <span class=\"hljs-keyword\">as</span> f:\n<span class=\"\
          hljs-comment\"># otherwise , there was UnicodeDecodeError</span>\n</code></pre>\n\
          <ul>\n<li>Maybe add to encodechka README.md (into similar projects paragraph)\
          \ link to <strong>MTEB</strong> <a href=\"https://huggingface.co/spaces/mteb/leaderboard\"\
          >https://huggingface.co/spaces/mteb/leaderboard</a> as an additional source\
          \ of comparison of test models on english and other languages</li>\n<li>Maybe\
          \ add to encodechka README.md to leaderbord models descriptions after dim:\
          \ Multilang field: <strong>is multilingual</strong> (and maybe how many\
          \ langs supported)? Multilang alignment field: <strong>is embeddings cross-language\
          \ alignment</strong> (language-agnostic embeddings) like LASER, LaBSE?</li>\n\
          <li>Some suggestions for the 'intfloat/multilingual-e5...' model in the\
          \ encodechka:<ul>\n<li>the 'intfloat/multilingual-e5...' model card says\
          \ that you <strong>always</strong> need to add \"Each input text should\
          \ start with <strong>\"query: \" or \"passage: \"</strong>, even for non-English\
          \ texts.\" to model input, but <strong>I didn't find this</strong> place\
          \ (and words \"query: \" or \"passage: \") <strong>in encodechka source\
          \ code</strong>. If I really haven't overlooked it in source code, then\
          \ on my e5 tests, it turned out that this moment really matters, <strong>up\
          \ to +10-15% quality</strong>.</li>\n<li>Small and Base multilingual-e5\
          \ models are also interesting (maybe add it to the benchmark), the quality\
          \ does not drop much on many tasks (and still surpasses Labs), and the speed,\
          \ especially on the CPU, varies significantly (and on the CPU it can be\
          \ twice as fast as LaBSE):<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63ed582c679c2cc40ab8c8e2/g4t1dvnkcG8jz3AQ_TQsP.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63ed582c679c2cc40ab8c8e2/g4t1dvnkcG8jz3AQ_TQsP.png\"\
          ></a></li>\n</ul>\n</li>\n<li>P.S. 'cointegrated/rubert-base-cased-dp-paraphrase-detection'\
          \ <strong>still the best accuracy</strong>: 0.85 with time: 37.2 s on my\
          \ paraphrase test: large-e5 accuracy: 0.82 (with time: 6+ minutes), but\
          \ e5-small accuracy: 0.82 with time: 48.7 s.</li>\n</ul>\n"
        raw: "I appreciate your reply. There are a few thoughts that I need to report\
          \ by you:\n- maybe add a **transcript of abbreviations** to table header\
          \ or before encodechka task tables in README.md of https://github.com/avidale/encodechka\
          \ from colab v2023:\n```python\ntmp = tmp.rename({'FactRuTask': 'NE1',\n\
          \ 'InappropriatenessTask': 'IA',\n 'IntentsTask': 'IC',\n 'IntentsXTask':\
          \ 'ICX',\n 'ParaphraserTask': 'PI',\n 'RudrTask': 'NE2',\n 'STSBTask': 'STS',\n\
          \ 'SentimentTask': 'SA',\n 'ToxicityTask': 'TI',\n 'XnliTask': 'NLI',\n\
          \ 'cpu_speed': 'CPU',\n 'disk_size': 'size',\n 'gpu_speed': 'GPU',\n 'mean_s':\
          \ 'Mean S',\n 'mean_sw': 'Mean S+W',}, axis=1)\n``` \n- To run encodechka\
          \ colab v2023 in jupyter under windows, I had to **small fix** replace in\
          \ tasks.py:\n```python\n# all encountered of open function\nwith open(find_file(filename),\
          \ 'r') as f:\n# replaced to open with encoding='utf-8'\nwith open(find_file(filename),\
          \ 'r', encoding='utf-8') as f:\n# otherwise , there was UnicodeDecodeError\n\
          ```\n- Maybe add to encodechka README.md (into similar projects paragraph)\
          \ link to **MTEB** https://huggingface.co/spaces/mteb/leaderboard as an\
          \ additional source of comparison of test models on english and other languages\n\
          - Maybe add to encodechka README.md to leaderbord models descriptions after\
          \ dim: Multilang field: **is multilingual** (and maybe how many langs supported)?\
          \ Multilang alignment field: **is embeddings cross-language alignment**\
          \ (language-agnostic embeddings) like LASER, LaBSE?\n- Some suggestions\
          \ for the 'intfloat/multilingual-e5...' model in the encodechka:\n  - the\
          \ 'intfloat/multilingual-e5...' model card says that you **always** need\
          \ to add \"Each input text should start with **\"query: \" or \"passage:\
          \ \"**, even for non-English texts.\" to model input, but **I didn't find\
          \ this** place (and words \"query: \" or \"passage: \") **in encodechka\
          \ source code**. If I really haven't overlooked it in source code, then\
          \ on my e5 tests, it turned out that this moment really matters, **up to\
          \ +10-15% quality**.\n  - Small and Base multilingual-e5 models are also\
          \ interesting (maybe add it to the benchmark), the quality does not drop\
          \ much on many tasks (and still surpasses Labs), and the speed, especially\
          \ on the CPU, varies significantly (and on the CPU it can be twice as fast\
          \ as LaBSE): \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63ed582c679c2cc40ab8c8e2/g4t1dvnkcG8jz3AQ_TQsP.png)\n\
          - P.S. 'cointegrated/rubert-base-cased-dp-paraphrase-detection' **still\
          \ the best accuracy**: 0.85 with time: 37.2 s on my paraphrase test: large-e5\
          \ accuracy: 0.82 (with time: 6+ minutes), but e5-small accuracy: 0.82 with\
          \ time: 48.7 s."
        updatedAt: '2023-08-14T11:04:02.915Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - cointegrated
    id: 64da051200b80a024c37afcd
    type: comment
  author: fikavec
  content: "I appreciate your reply. There are a few thoughts that I need to report\
    \ by you:\n- maybe add a **transcript of abbreviations** to table header or before\
    \ encodechka task tables in README.md of https://github.com/avidale/encodechka\
    \ from colab v2023:\n```python\ntmp = tmp.rename({'FactRuTask': 'NE1',\n 'InappropriatenessTask':\
    \ 'IA',\n 'IntentsTask': 'IC',\n 'IntentsXTask': 'ICX',\n 'ParaphraserTask': 'PI',\n\
    \ 'RudrTask': 'NE2',\n 'STSBTask': 'STS',\n 'SentimentTask': 'SA',\n 'ToxicityTask':\
    \ 'TI',\n 'XnliTask': 'NLI',\n 'cpu_speed': 'CPU',\n 'disk_size': 'size',\n 'gpu_speed':\
    \ 'GPU',\n 'mean_s': 'Mean S',\n 'mean_sw': 'Mean S+W',}, axis=1)\n``` \n- To\
    \ run encodechka colab v2023 in jupyter under windows, I had to **small fix**\
    \ replace in tasks.py:\n```python\n# all encountered of open function\nwith open(find_file(filename),\
    \ 'r') as f:\n# replaced to open with encoding='utf-8'\nwith open(find_file(filename),\
    \ 'r', encoding='utf-8') as f:\n# otherwise , there was UnicodeDecodeError\n```\n\
    - Maybe add to encodechka README.md (into similar projects paragraph) link to\
    \ **MTEB** https://huggingface.co/spaces/mteb/leaderboard as an additional source\
    \ of comparison of test models on english and other languages\n- Maybe add to\
    \ encodechka README.md to leaderbord models descriptions after dim: Multilang\
    \ field: **is multilingual** (and maybe how many langs supported)? Multilang alignment\
    \ field: **is embeddings cross-language alignment** (language-agnostic embeddings)\
    \ like LASER, LaBSE?\n- Some suggestions for the 'intfloat/multilingual-e5...'\
    \ model in the encodechka:\n  - the 'intfloat/multilingual-e5...' model card says\
    \ that you **always** need to add \"Each input text should start with **\"query:\
    \ \" or \"passage: \"**, even for non-English texts.\" to model input, but **I\
    \ didn't find this** place (and words \"query: \" or \"passage: \") **in encodechka\
    \ source code**. If I really haven't overlooked it in source code, then on my\
    \ e5 tests, it turned out that this moment really matters, **up to +10-15% quality**.\n\
    \  - Small and Base multilingual-e5 models are also interesting (maybe add it\
    \ to the benchmark), the quality does not drop much on many tasks (and still surpasses\
    \ Labs), and the speed, especially on the CPU, varies significantly (and on the\
    \ CPU it can be twice as fast as LaBSE): \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63ed582c679c2cc40ab8c8e2/g4t1dvnkcG8jz3AQ_TQsP.png)\n\
    - P.S. 'cointegrated/rubert-base-cased-dp-paraphrase-detection' **still the best\
    \ accuracy**: 0.85 with time: 37.2 s on my paraphrase test: large-e5 accuracy:\
    \ 0.82 (with time: 6+ minutes), but e5-small accuracy: 0.82 with time: 48.7 s."
  created_at: 2023-08-14 09:42:26+00:00
  edited: true
  hidden: false
  id: 64da051200b80a024c37afcd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: cointegrated/rubert-base-cased-dp-paraphrase-detection
repo_type: model
status: closed
target_branch: null
title: It is possible to use this model in Vector Similarity Based Approach (cosine)
  to detect paraphrases?
