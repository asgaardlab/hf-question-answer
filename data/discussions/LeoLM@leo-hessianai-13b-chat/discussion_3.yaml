!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fcivardi
conflicting_files: null
created_at: 2023-10-05 06:45:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd198fa6754174efe5a06c337c678587.svg
      fullname: Francesco Civardi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fcivardi
      type: user
    createdAt: '2023-10-05T07:45:27.000Z'
    data:
      edited: true
      editors:
      - fcivardi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4795430600643158
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd198fa6754174efe5a06c337c678587.svg
          fullname: Francesco Civardi
          isHf: false
          isPro: false
          name: fcivardi
          type: user
        html: "<p>After:</p>\n<pre><code>model = \"LeoLM/leo-hessianai-13b-chat\"\n\
          tokenizer = AutoTokenizer.from_pretrained(model)\npipeline = pipeline(\n\
          \    \"text-generation\",\n    tokenizer=tokenizer,\n    model=model,\n\
          \    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    max_length\
          \ = 8192,\n    trust_remote_code=False\n)\n</code></pre>\n<p>I get the following\
          \ message.  Is it normal? I'm using it for inference and it seems working.</p>\n\
          <p>Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 3/3 [00:56&lt;00:00, 18.67s/it]<br>Some weights of LlamaForCausalLM\
          \ were not initialized from the model checkpoint at LeoLM/leo-hessianai-13b-chat\
          \ and are newly initialized: ['model.layers.28.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.11.self_attn.rotary_emb.inv_freq']<br>You should probably\
          \ TRAIN this model on a down-stream task to be able to use it for predictions\
          \ and inference.</p>\n"
        raw: "After:\n```\nmodel = \"LeoLM/leo-hessianai-13b-chat\"\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          pipeline = pipeline(\n    \"text-generation\",\n    tokenizer=tokenizer,\n\
          \    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\
          ,\n    max_length = 8192,\n    trust_remote_code=False\n)\n```\nI get the\
          \ following message.  Is it normal? I'm using it for inference and it seems\
          \ working.\n\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 3/3 [00:56<00:00, 18.67s/it]\nSome weights\
          \ of LlamaForCausalLM were not initialized from the model checkpoint at\
          \ LeoLM/leo-hessianai-13b-chat and are newly initialized: ['model.layers.28.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.11.self_attn.rotary_emb.inv_freq']\nYou should probably\
          \ TRAIN this model on a down-stream task to be able to use it for predictions\
          \ and inference."
        updatedAt: '2023-10-05T08:20:33.837Z'
      numEdits: 1
      reactions: []
    id: 651e6997b0986fb7f3ad1777
    type: comment
  author: fcivardi
  content: "After:\n```\nmodel = \"LeoLM/leo-hessianai-13b-chat\"\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
    pipeline = pipeline(\n    \"text-generation\",\n    tokenizer=tokenizer,\n   \
    \ model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n   \
    \ max_length = 8192,\n    trust_remote_code=False\n)\n```\nI get the following\
    \ message.  Is it normal? I'm using it for inference and it seems working.\n\n\
    Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588| 3/3 [00:56<00:00, 18.67s/it]\nSome weights of LlamaForCausalLM were\
    \ not initialized from the model checkpoint at LeoLM/leo-hessianai-13b-chat and\
    \ are newly initialized: ['model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq']\n\
    You should probably TRAIN this model on a down-stream task to be able to use it\
    \ for predictions and inference."
  created_at: 2023-10-05 06:45:27+00:00
  edited: true
  hidden: false
  id: 651e6997b0986fb7f3ad1777
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-10-05T08:24:32.000Z'
    data:
      edited: false
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8671898245811462
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>These are non-trainable parameters and you can ignore this warning.
          I believe in newer transformers versions it is silenced by default.</p>

          '
        raw: These are non-trainable parameters and you can ignore this warning. I
          believe in newer transformers versions it is silenced by default.
        updatedAt: '2023-10-05T08:24:32.187Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - fcivardi
      relatedEventId: 651e72c0e75682d824ed45ac
    id: 651e72c0e75682d824ed45ab
    type: comment
  author: bjoernp
  content: These are non-trainable parameters and you can ignore this warning. I believe
    in newer transformers versions it is silenced by default.
  created_at: 2023-10-05 07:24:32+00:00
  edited: false
  hidden: false
  id: 651e72c0e75682d824ed45ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-10-05T08:24:32.000Z'
    data:
      status: closed
    id: 651e72c0e75682d824ed45ac
    type: status-change
  author: bjoernp
  created_at: 2023-10-05 07:24:32+00:00
  id: 651e72c0e75682d824ed45ac
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: LeoLM/leo-hessianai-13b-chat
repo_type: model
status: closed
target_branch: null
title: 'Some weights of LlamaForCausalLM were not initialized from the model checkpoint '
