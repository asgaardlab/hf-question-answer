!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cmp-nct
conflicting_files: null
created_at: 2023-12-04 01:55:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-12-04T01:55:53.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9309009313583374
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>I''ve just ran a test with Googles (apache-2) madlad400 10B model
          and it''s flawless in german grammar.<br>Nothing I tried on any model produced
          such good translations, leo-hessianai is one of the best but regularly translates
          mistakes at a beginner level.</p>

          <p>madlad runs on an rarely used architecture (t5), is barely supported
          anywhere and it does not have the capability to translate more than one
          sentence, so it''s not "the solution" to translation. After all the translation
          of one sentence can depend on what was said in the previous one and madlad
          totally fails here.<br>Like "There is a display case." "The case is red"
          -&gt; "Da ist eine Vitrine" "Der Fall ist rot"</p>

          <p>I j ust wondered, given the flawless grammar it produces, maybe the trainind
          dataset would be a good addition to your LeoLM model.<br>Or maybe a pure
          llama based translation model could be created, fine tuned on translation
          into german (or all languages like madlad appears to handle)?</p>

          '
        raw: "I've just ran a test with Googles (apache-2) madlad400 10B model and\
          \ it's flawless in german grammar.\r\nNothing I tried on any model produced\
          \ such good translations, leo-hessianai is one of the best but regularly\
          \ translates mistakes at a beginner level.\r\n\r\nmadlad runs on an rarely\
          \ used architecture (t5), is barely supported anywhere and it does not have\
          \ the capability to translate more than one sentence, so it's not \"the\
          \ solution\" to translation. After all the translation of one sentence can\
          \ depend on what was said in the previous one and madlad totally fails here.\r\
          \nLike \"There is a display case.\" \"The case is red\" -> \"Da ist eine\
          \ Vitrine\" \"Der Fall ist rot\"\r\n\r\nI j ust wondered, given the flawless\
          \ grammar it produces, maybe the trainind dataset would be a good addition\
          \ to your LeoLM model.\r\nOr maybe a pure llama based translation model\
          \ could be created, fine tuned on translation into german (or all languages\
          \ like madlad appears to handle)?"
        updatedAt: '2023-12-04T01:55:53.708Z'
      numEdits: 0
      reactions: []
    id: 656d31a9ede71189ad5dda9a
    type: comment
  author: cmp-nct
  content: "I've just ran a test with Googles (apache-2) madlad400 10B model and it's\
    \ flawless in german grammar.\r\nNothing I tried on any model produced such good\
    \ translations, leo-hessianai is one of the best but regularly translates mistakes\
    \ at a beginner level.\r\n\r\nmadlad runs on an rarely used architecture (t5),\
    \ is barely supported anywhere and it does not have the capability to translate\
    \ more than one sentence, so it's not \"the solution\" to translation. After all\
    \ the translation of one sentence can depend on what was said in the previous\
    \ one and madlad totally fails here.\r\nLike \"There is a display case.\" \"The\
    \ case is red\" -> \"Da ist eine Vitrine\" \"Der Fall ist rot\"\r\n\r\nI j ust\
    \ wondered, given the flawless grammar it produces, maybe the trainind dataset\
    \ would be a good addition to your LeoLM model.\r\nOr maybe a pure llama based\
    \ translation model could be created, fine tuned on translation into german (or\
    \ all languages like madlad appears to handle)?"
  created_at: 2023-12-04 01:55:53+00:00
  edited: false
  hidden: false
  id: 656d31a9ede71189ad5dda9a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-12-04T06:39:14.000Z'
    data:
      edited: false
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9684633612632751
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>Hi and thanks for your interest in our models. The chat models,
          as well as the Leo base models, have not been trained for translation and
          this was not an explicit goal. I would assume that finetuning for document
          level translation would get you much closer to what you are looking for.<br>From
          my experience, our Mistral based 7b model works best for this and can be
          finetuned easily on a consumer GPU with qLora. If you want to go down this
          path, you should check out the axolotl training library. I believe a few
          hundred or maybe thousand examples should already be good to align the model.<br>Good
          luck with this and feel free to share any thoughts or results :)</p>

          '
        raw: 'Hi and thanks for your interest in our models. The chat models, as well
          as the Leo base models, have not been trained for translation and this was
          not an explicit goal. I would assume that finetuning for document level
          translation would get you much closer to what you are looking for.

          From my experience, our Mistral based 7b model works best for this and can
          be finetuned easily on a consumer GPU with qLora. If you want to go down
          this path, you should check out the axolotl training library. I believe
          a few hundred or maybe thousand examples should already be good to align
          the model.

          Good luck with this and feel free to share any thoughts or results :)'
        updatedAt: '2023-12-04T06:39:14.981Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - cmp-nct
    id: 656d74123eb5f0b6a95830e6
    type: comment
  author: bjoernp
  content: 'Hi and thanks for your interest in our models. The chat models, as well
    as the Leo base models, have not been trained for translation and this was not
    an explicit goal. I would assume that finetuning for document level translation
    would get you much closer to what you are looking for.

    From my experience, our Mistral based 7b model works best for this and can be
    finetuned easily on a consumer GPU with qLora. If you want to go down this path,
    you should check out the axolotl training library. I believe a few hundred or
    maybe thousand examples should already be good to align the model.

    Good luck with this and feel free to share any thoughts or results :)'
  created_at: 2023-12-04 06:39:14+00:00
  edited: false
  hidden: false
  id: 656d74123eb5f0b6a95830e6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: LeoLM/leo-hessianai-13b-chat
repo_type: model
status: open
target_branch: null
title: Can you incorporate madlad400 training data ?
