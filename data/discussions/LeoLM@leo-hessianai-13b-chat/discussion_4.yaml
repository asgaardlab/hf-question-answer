!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fcivardi
conflicting_files: null
created_at: 2023-10-05 07:10:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd198fa6754174efe5a06c337c678587.svg
      fullname: Francesco Civardi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fcivardi
      type: user
    createdAt: '2023-10-05T08:10:33.000Z'
    data:
      edited: true
      editors:
      - fcivardi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6295579075813293
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd198fa6754174efe5a06c337c678587.svg
          fullname: Francesco Civardi
          isHf: false
          isPro: false
          name: fcivardi
          type: user
        html: "<p>I do:</p>\n<pre><code class=\"language-llm_chain\">def look4menicusriss(report):\n\
          \    response = llm_chain.run(\n        system_prompt=system_prompt,\n \
          \       user_prompt=user_prompt,\n        report=report\n        )\n   \
          \ #print(response)\n    tear = response[0]\n    return tear\n\n\nnew_dataset\
          \ = dataset.map(\n    lambda row: {'prediction': look4menicusriss(row['Befund'])}\n\
          )\n</code></pre>\n<p>It starts applying the function to reports stored in\
          \ a dataset, but  then, at about 20%, I get an OutOfMemoryError: CUDA out\
          \ of memory. It didn't happen with the original LLama2. (GPU: V100 32 Gb)</p>\n\
          <p>OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU\
          \ 0; 31.75 GiB total capacity; 30.43 GiB already allocated; 33.50 MiB free;\
          \ 30.79 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt;\
          \ allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>\n"
        raw: "I do:\n```llm_chain = LLMChain(llm=llm, prompt=prompt)\ndef look4menicusriss(report):\n\
          \    response = llm_chain.run(\n        system_prompt=system_prompt,\n \
          \       user_prompt=user_prompt,\n        report=report\n        )\n   \
          \ #print(response)\n    tear = response[0]\n    return tear\n\n\nnew_dataset\
          \ = dataset.map(\n    lambda row: {'prediction': look4menicusriss(row['Befund'])}\n\
          )\n```\nIt starts applying the function to reports stored in a dataset,\
          \ but  then, at about 20%, I get an OutOfMemoryError: CUDA out of memory.\
          \ It didn't happen with the original LLama2. (GPU: V100 32 Gb)\n\nOutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 31.75 GiB total\
          \ capacity; 30.43 GiB already allocated; 33.50 MiB free; 30.79 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF\n"
        updatedAt: '2023-10-05T08:19:45.356Z'
      numEdits: 4
      reactions: []
    id: 651e6f79081a74588dddbd38
    type: comment
  author: fcivardi
  content: "I do:\n```llm_chain = LLMChain(llm=llm, prompt=prompt)\ndef look4menicusriss(report):\n\
    \    response = llm_chain.run(\n        system_prompt=system_prompt,\n       \
    \ user_prompt=user_prompt,\n        report=report\n        )\n    #print(response)\n\
    \    tear = response[0]\n    return tear\n\n\nnew_dataset = dataset.map(\n   \
    \ lambda row: {'prediction': look4menicusriss(row['Befund'])}\n)\n```\nIt starts\
    \ applying the function to reports stored in a dataset, but  then, at about 20%,\
    \ I get an OutOfMemoryError: CUDA out of memory. It didn't happen with the original\
    \ LLama2. (GPU: V100 32 Gb)\n\nOutOfMemoryError: CUDA out of memory. Tried to\
    \ allocate 40.00 MiB (GPU 0; 31.75 GiB total capacity; 30.43 GiB already allocated;\
    \ 33.50 MiB free; 30.79 GiB reserved in total by PyTorch) If reserved memory is\
    \ >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See\
    \ documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
  created_at: 2023-10-05 07:10:33+00:00
  edited: true
  hidden: false
  id: 651e6f79081a74588dddbd38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd198fa6754174efe5a06c337c678587.svg
      fullname: Francesco Civardi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fcivardi
      type: user
    createdAt: '2023-10-05T08:14:05.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/fd198fa6754174efe5a06c337c678587.svg
          fullname: Francesco Civardi
          isHf: false
          isPro: false
          name: fcivardi
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-10-05T08:14:31.068Z'
      numEdits: 0
      reactions: []
    id: 651e704dce0843cbf30ff7fc
    type: comment
  author: fcivardi
  content: This comment has been hidden
  created_at: 2023-10-05 07:14:05+00:00
  edited: true
  hidden: true
  id: 651e704dce0843cbf30ff7fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/fd198fa6754174efe5a06c337c678587.svg
      fullname: Francesco Civardi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fcivardi
      type: user
    createdAt: '2023-10-05T08:14:05.000Z'
    data:
      status: closed
    id: 651e704dce0843cbf30ff7fd
    type: status-change
  author: fcivardi
  created_at: 2023-10-05 07:14:05+00:00
  id: 651e704dce0843cbf30ff7fd
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/fd198fa6754174efe5a06c337c678587.svg
      fullname: Francesco Civardi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fcivardi
      type: user
    createdAt: '2023-10-05T08:16:44.000Z'
    data:
      status: open
    id: 651e70ec9dedee925f6a6852
    type: status-change
  author: fcivardi
  created_at: 2023-10-05 07:16:44+00:00
  id: 651e70ec9dedee925f6a6852
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-10-05T08:23:12.000Z'
    data:
      edited: false
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9666386246681213
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>Our models support 2x the context size of the original Llama. Perhaps
          you''re having issues with some samples being too long or much longer than
          the others? Maybe try filtering  these out or otherwise you could also try
          using a quantized model.</p>

          '
        raw: Our models support 2x the context size of the original Llama. Perhaps
          you're having issues with some samples being too long or much longer than
          the others? Maybe try filtering  these out or otherwise you could also try
          using a quantized model.
        updatedAt: '2023-10-05T08:23:12.783Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - fcivardi
    id: 651e7270b86a3c716a41623d
    type: comment
  author: bjoernp
  content: Our models support 2x the context size of the original Llama. Perhaps you're
    having issues with some samples being too long or much longer than the others?
    Maybe try filtering  these out or otherwise you could also try using a quantized
    model.
  created_at: 2023-10-05 07:23:12+00:00
  edited: false
  hidden: false
  id: 651e7270b86a3c716a41623d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd198fa6754174efe5a06c337c678587.svg
      fullname: Francesco Civardi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fcivardi
      type: user
    createdAt: '2023-10-05T09:00:10.000Z'
    data:
      edited: false
      editors:
      - fcivardi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8963965177536011
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd198fa6754174efe5a06c337c678587.svg
          fullname: Francesco Civardi
          isHf: false
          isPro: false
          name: fcivardi
          type: user
        html: '<p>Thank you. Indeed, using a retriever to pass to the LLM only the
          relevant parts of each document solved the problem.</p>

          '
        raw: Thank you. Indeed, using a retriever to pass to the LLM only the relevant
          parts of each document solved the problem.
        updatedAt: '2023-10-05T09:00:10.718Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - bjoernp
    id: 651e7b1abd6500a8d882d3ff
    type: comment
  author: fcivardi
  content: Thank you. Indeed, using a retriever to pass to the LLM only the relevant
    parts of each document solved the problem.
  created_at: 2023-10-05 08:00:10+00:00
  edited: false
  hidden: false
  id: 651e7b1abd6500a8d882d3ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/fd198fa6754174efe5a06c337c678587.svg
      fullname: Francesco Civardi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fcivardi
      type: user
    createdAt: '2023-10-05T09:02:37.000Z'
    data:
      status: closed
    id: 651e7baddc7a8b53808b391e
    type: status-change
  author: fcivardi
  created_at: 2023-10-05 08:02:37+00:00
  id: 651e7baddc7a8b53808b391e
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: LeoLM/leo-hessianai-13b-chat
repo_type: model
status: closed
target_branch: null
title: 'CUDA out of memory applying to a  dataset of texts '
