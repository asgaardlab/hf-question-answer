!!python/object:huggingface_hub.community.DiscussionWithDetails
author: darule
conflicting_files: null
created_at: 2023-10-01 20:07:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b151ad718646fb316c89fb472c311193.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darule
      type: user
    createdAt: '2023-10-01T21:07:59.000Z'
    data:
      edited: true
      editors:
      - darule
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9131280779838562
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b151ad718646fb316c89fb472c311193.svg
          fullname: None
          isHf: false
          isPro: false
          name: darule
          type: user
        html: '<p>I''m sorry if this is a newbie question, but i cannot convert this
          model because the tokenzier.model is missing here? Is that correc tor a
          mistake? (Or can i generate it by myself?)</p>

          '
        raw: I'm sorry if this is a newbie question, but i cannot convert this model
          because the tokenzier.model is missing here? Is that correc tor a mistake?
          (Or can i generate it by myself?)
        updatedAt: '2023-10-01T21:08:51.936Z'
      numEdits: 1
      reactions: []
    id: 6519dfaf4fadbeb643038c23
    type: comment
  author: darule
  content: I'm sorry if this is a newbie question, but i cannot convert this model
    because the tokenzier.model is missing here? Is that correc tor a mistake? (Or
    can i generate it by myself?)
  created_at: 2023-10-01 20:07:59+00:00
  edited: true
  hidden: false
  id: 6519dfaf4fadbeb643038c23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d00728c091dd1b8ba8658a42d1aea89.svg
      fullname: Stefan Fuchs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fuchsst
      type: user
    createdAt: '2023-10-15T16:57:02.000Z'
    data:
      edited: false
      editors:
      - fuchsst
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5733747482299805
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d00728c091dd1b8ba8658a42d1aea89.svg
          fullname: Stefan Fuchs
          isHf: false
          isPro: false
          name: fuchsst
          type: user
        html: "<p>same problem, the file is missing in this repo. wanted to convert\
          \ it to GGUF to use with llama.cpp</p>\n<p>tried <a href=\"https://huggingface.co/LeoLM/leo-hessianai-13b/blob/main/tokenizer.model\"\
          >https://huggingface.co/LeoLM/leo-hessianai-13b/blob/main/tokenizer.model</a>\
          \ but then it complains</p>\n<pre><code>Writing models/LeoML-13B/ggml-model-f16.gguf,\
          \ format 1\nTraceback (most recent call last):\n  File \"/home/whstfuch/appl/llama.cpp/convert.py\"\
          , line 1193, in &lt;module&gt;\n    main()\n  File \"/home/whstfuch/appl/llama.cpp/convert.py\"\
          , line 1188, in main\n    OutputFile.write_all(outfile, ftype, params, model,\
          \ vocab, special_vocab, concurrency = args.concurrency)\n  File \"/home/whstfuch/appl/llama.cpp/convert.py\"\
          , line 907, in write_all\n    check_vocab_size(params, vocab)\n  File \"\
          /home/whstfuch/appl/llama.cpp/convert.py\", line 802, in check_vocab_size\n\
          \    raise Exception(msg)\nException: Vocab size mismatch (model has 32128,\
          \ but models/LeoML-13B/tokenizer.model has 32000).\n</code></pre>\n"
        raw: "same problem, the file is missing in this repo. wanted to convert it\
          \ to GGUF to use with llama.cpp\n\ntried https://huggingface.co/LeoLM/leo-hessianai-13b/blob/main/tokenizer.model\
          \ but then it complains\n```\nWriting models/LeoML-13B/ggml-model-f16.gguf,\
          \ format 1\nTraceback (most recent call last):\n  File \"/home/whstfuch/appl/llama.cpp/convert.py\"\
          , line 1193, in <module>\n    main()\n  File \"/home/whstfuch/appl/llama.cpp/convert.py\"\
          , line 1188, in main\n    OutputFile.write_all(outfile, ftype, params, model,\
          \ vocab, special_vocab, concurrency = args.concurrency)\n  File \"/home/whstfuch/appl/llama.cpp/convert.py\"\
          , line 907, in write_all\n    check_vocab_size(params, vocab)\n  File \"\
          /home/whstfuch/appl/llama.cpp/convert.py\", line 802, in check_vocab_size\n\
          \    raise Exception(msg)\nException: Vocab size mismatch (model has 32128,\
          \ but models/LeoML-13B/tokenizer.model has 32000).\n```"
        updatedAt: '2023-10-15T16:57:02.933Z'
      numEdits: 0
      reactions: []
    id: 652c19ded50c9382fb58d2ad
    type: comment
  author: fuchsst
  content: "same problem, the file is missing in this repo. wanted to convert it to\
    \ GGUF to use with llama.cpp\n\ntried https://huggingface.co/LeoLM/leo-hessianai-13b/blob/main/tokenizer.model\
    \ but then it complains\n```\nWriting models/LeoML-13B/ggml-model-f16.gguf, format\
    \ 1\nTraceback (most recent call last):\n  File \"/home/whstfuch/appl/llama.cpp/convert.py\"\
    , line 1193, in <module>\n    main()\n  File \"/home/whstfuch/appl/llama.cpp/convert.py\"\
    , line 1188, in main\n    OutputFile.write_all(outfile, ftype, params, model,\
    \ vocab, special_vocab, concurrency = args.concurrency)\n  File \"/home/whstfuch/appl/llama.cpp/convert.py\"\
    , line 907, in write_all\n    check_vocab_size(params, vocab)\n  File \"/home/whstfuch/appl/llama.cpp/convert.py\"\
    , line 802, in check_vocab_size\n    raise Exception(msg)\nException: Vocab size\
    \ mismatch (model has 32128, but models/LeoML-13B/tokenizer.model has 32000).\n\
    ```"
  created_at: 2023-10-15 15:57:02+00:00
  edited: false
  hidden: false
  id: 652c19ded50c9382fb58d2ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-10-18T21:45:19.000Z'
    data:
      edited: false
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8813115954399109
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>Llama.cpp has recently implemented support for added tokens in <a
          rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/3475">#3475</a>
          ( see this <a rel="nofollow" href="https://twitter.com/ggerganov/status/1714638718084980802">tweet</a>).
          This should solve the issues you''ve been having.</p>

          '
        raw: Llama.cpp has recently implemented support for added tokens in [#3475](https://github.com/ggerganov/llama.cpp/issues/3475)
          ( see this [tweet](https://twitter.com/ggerganov/status/1714638718084980802)).
          This should solve the issues you've been having.
        updatedAt: '2023-10-18T21:45:19.665Z'
      numEdits: 0
      reactions: []
      relatedEventId: 653051ef5da070af77a6f885
    id: 653051ef5da070af77a6f884
    type: comment
  author: bjoernp
  content: Llama.cpp has recently implemented support for added tokens in [#3475](https://github.com/ggerganov/llama.cpp/issues/3475)
    ( see this [tweet](https://twitter.com/ggerganov/status/1714638718084980802)).
    This should solve the issues you've been having.
  created_at: 2023-10-18 20:45:19+00:00
  edited: false
  hidden: false
  id: 653051ef5da070af77a6f884
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-10-18T21:45:19.000Z'
    data:
      status: closed
    id: 653051ef5da070af77a6f885
    type: status-change
  author: bjoernp
  created_at: 2023-10-18 20:45:19+00:00
  id: 653051ef5da070af77a6f885
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: LeoLM/leo-hessianai-13b-chat
repo_type: model
status: closed
target_branch: null
title: tokenizer.model missing?
