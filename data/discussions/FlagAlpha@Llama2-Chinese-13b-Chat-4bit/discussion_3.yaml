!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lilsyoss
conflicting_files: null
created_at: 2023-08-24 02:54:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69868c622815825cbdb94af55d4a68a7.svg
      fullname: Lilong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lilsyoss
      type: user
    createdAt: '2023-08-24T03:54:19.000Z'
    data:
      edited: false
      editors:
      - lilsyoss
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4783499538898468
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69868c622815825cbdb94af55d4a68a7.svg
          fullname: Lilong
          isHf: false
          isPro: false
          name: lilsyoss
          type: user
        html: '<p>import torch<br>from transformers import AutoTokenizer<br>from auto_gptq
          import AutoGPTQForCausalLM<br>DEVICE = "cuda"<br>DEVICE_ID = "0"<br>CUDA_DEVICE
          = f"{DEVICE}:{DEVICE_ID}" if DEVICE_ID else DEVICE<br>device = torch.device(CUDA_DEVICE)<br>model_path
          = ''FlagAlpha/Llama2-Chinese-13b-Chat-4bit''<br>model = AutoGPTQForCausalLM.from_quantized(model_path,device=CUDA_DEVICE)<br>model
          = model.eval()<br>tokenizer = AutoTokenizer.from_pretrained(model_path1,use_fast=False)</p>

          <p>save_path="/root/autodl-tmp/hf_repo/llama2_chinese"<br>model.save_quantized(save_path)</p>

          <p>then reload the saved model error come</p>

          '
        raw: "import torch\r\nfrom transformers import AutoTokenizer\r\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM\r\nDEVICE = \"cuda\"\r\nDEVICE_ID = \"0\"\r\
          \nCUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE\r\ndevice\
          \ = torch.device(CUDA_DEVICE)\r\nmodel_path = 'FlagAlpha/Llama2-Chinese-13b-Chat-4bit'\r\
          \nmodel = AutoGPTQForCausalLM.from_quantized(model_path,device=CUDA_DEVICE)\r\
          \nmodel = model.eval()\r\ntokenizer = AutoTokenizer.from_pretrained(model_path1,use_fast=False)\r\
          \n\r\nsave_path=\"/root/autodl-tmp/hf_repo/llama2_chinese\"\r\nmodel.save_quantized(save_path)\r\
          \n\r\n\r\nthen reload the saved model error come"
        updatedAt: '2023-08-24T03:54:19.798Z'
      numEdits: 0
      reactions: []
    id: 64e6d46b86fc685a30a42add
    type: comment
  author: lilsyoss
  content: "import torch\r\nfrom transformers import AutoTokenizer\r\nfrom auto_gptq\
    \ import AutoGPTQForCausalLM\r\nDEVICE = \"cuda\"\r\nDEVICE_ID = \"0\"\r\nCUDA_DEVICE\
    \ = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE\r\ndevice = torch.device(CUDA_DEVICE)\r\
    \nmodel_path = 'FlagAlpha/Llama2-Chinese-13b-Chat-4bit'\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_path,device=CUDA_DEVICE)\r\
    \nmodel = model.eval()\r\ntokenizer = AutoTokenizer.from_pretrained(model_path1,use_fast=False)\r\
    \n\r\nsave_path=\"/root/autodl-tmp/hf_repo/llama2_chinese\"\r\nmodel.save_quantized(save_path)\r\
    \n\r\n\r\nthen reload the saved model error come"
  created_at: 2023-08-24 02:54:19+00:00
  edited: false
  hidden: false
  id: 64e6d46b86fc685a30a42add
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61cc7b5c43b3d9d55e49b7e8/iExm6DRBmwOKUOuld2wuf.jpeg?w=200&h=200&f=face
      fullname: zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: EricZHangZHeng
      type: user
    createdAt: '2023-08-24T04:00:12.000Z'
    data:
      edited: false
      editors:
      - EricZHangZHeng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8838438391685486
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61cc7b5c43b3d9d55e49b7e8/iExm6DRBmwOKUOuld2wuf.jpeg?w=200&h=200&f=face
          fullname: zhang
          isHf: false
          isPro: false
          name: EricZHangZHeng
          type: user
        html: '<p>Can you send me all the logs with errors? transformers and  auto_gptq''s
          version?</p>

          '
        raw: 'Can you send me all the logs with errors? transformers and  auto_gptq''s
          version?

          '
        updatedAt: '2023-08-24T04:00:12.340Z'
      numEdits: 0
      reactions: []
    id: 64e6d5cc29a548f66aff0753
    type: comment
  author: EricZHangZHeng
  content: 'Can you send me all the logs with errors? transformers and  auto_gptq''s
    version?

    '
  created_at: 2023-08-24 03:00:12+00:00
  edited: false
  hidden: false
  id: 64e6d5cc29a548f66aff0753
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69868c622815825cbdb94af55d4a68a7.svg
      fullname: Lilong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lilsyoss
      type: user
    createdAt: '2023-08-24T04:09:10.000Z'
    data:
      edited: false
      editors:
      - lilsyoss
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4251551330089569
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69868c622815825cbdb94af55d4a68a7.svg
          fullname: Lilong
          isHf: false
          isPro: false
          name: lilsyoss
          type: user
        html: '<blockquote>

          <p>Can you send me all the logs with errors? transformers and  auto_gptq''s
          version?</p>

          </blockquote>

          <p>transformers                   4.31.0<br>auto-gptq                      0.4.1+cu1180</p>

          <p>I install gptq by below command :<br>!pip install auto-gptq --extra-index-url
          <a rel="nofollow" href="https://huggingface.github.io/autogptq-index/whl/cu118/">https://huggingface.github.io/autogptq-index/whl/cu118/</a></p>

          <h2 id="full-log-error-">full log error :</h2>

          <p>AttributeError                            Traceback (most recent call
          last)<br>Cell In[1], line 15<br>      9 model_path = "/root/autodl-tmp/hf_repo/llama2_chinese"<br>     12
          # gptq_config = GPTQConfig(bits=4, disable_exllama=False)<br>---&gt; 15
          model = AutoGPTQForCausalLM.from_quantized(model_path,device=CUDA_DEVICE)<br>     16
          model = model.eval()<br>     17 tokenizer = AutoTokenizer.from_pretrained(model_path,use_fast=False)</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/auto_gptq/modeling/auto.py:108,
          in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, device_map,
          max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention,
          inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,
          trust_remote_code, warmup_triton, trainable, disable_exllama, **kwargs)<br>    102
          # TODO: do we need this filtering of kwargs? @PanQiWei is there a reason
          we can''t just pass all kwargs?<br>    103 keywords = {<br>    104     key:
          kwargs[key]<br>    105     for key in list(signature(quant_func).parameters.keys())
          + huggingface_kwargs<br>    106     if key in kwargs<br>    107 }<br>--&gt;
          108 return quant_func(<br>    109     model_name_or_path=model_name_or_path,<br>    110     device_map=device_map,<br>    111     max_memory=max_memory,<br>    112     device=device,<br>    113     low_cpu_mem_usage=low_cpu_mem_usage,<br>    114     use_triton=use_triton,<br>    115     inject_fused_attention=inject_fused_attention,<br>    116     inject_fused_mlp=inject_fused_mlp,<br>    117     use_cuda_fp16=use_cuda_fp16,<br>    118     quantize_config=quantize_config,<br>    119     model_basename=model_basename,<br>    120     use_safetensors=use_safetensors,<br>    121     trust_remote_code=trust_remote_code,<br>    122     warmup_triton=warmup_triton,<br>    123     trainable=trainable,<br>    124     disable_exllama=disable_exllama,<br>    125     **keywords<br>    126
          )</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/auto_gptq/modeling/_base.py:875,
          in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, device_map,
          max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention,
          inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,
          trust_remote_code, warmup_triton, trainable, disable_exllama, **kwargs)<br>    872
          if low_cpu_mem_usage:<br>    873     make_sure_no_tensor_in_meta_device(model,
          use_triton, quantize_config.desc_act, quantize_config.group_size, bits=quantize_config.bits)<br>--&gt;
          875 accelerate.utils.modeling.load_checkpoint_in_model(<br>    876     model,<br>    877     checkpoint=model_save_name,<br>    878     device_map=device_map,<br>    879     offload_state_dict=True,<br>    880     offload_buffers=True<br>    881
          )<br>    882 model = simple_dispatch_model(model, device_map)<br>    884
          # == step4: set seqlen == #</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/accelerate/utils/modeling.py:1336,
          in load_checkpoint_in_model(model, checkpoint, device_map, offload_folder,
          dtype, offload_state_dict, offload_buffers, keep_in_fp32_modules, offload_8bit_bnb)<br>   1334                 offload_weight(param,
          param_name, state_dict_folder, index=state_dict_index)<br>   1335         else:<br>-&gt;
          1336             set_module_tensor_to_device(<br>   1337                 model,<br>   1338                 param_name,<br>   1339                 param_device,<br>   1340                 value=param,<br>   1341                 dtype=new_dtype,<br>   1342                 fp16_statistics=fp16_statistics,<br>   1343             )<br>   1345
          # Force Python to clean up.<br>   1346 del checkpoint</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/accelerate/utils/modeling.py:255,
          in set_module_tensor_to_device(module, tensor_name, device, value, dtype,
          fp16_statistics)<br>    253 splits = tensor_name.split(".")<br>    254 for
          split in splits[:-1]:<br>--&gt; 255     new_module = getattr(module, split)<br>    256     if
          new_module is None:<br>    257         raise ValueError(f"{module} has no
          attribute {split}.")</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1614,
          in Module.<strong>getattr</strong>(self, name)<br>   1612     if name in
          modules:<br>   1613         return modules[name]<br>-&gt; 1614 raise AttributeError("''{}''
          object has no attribute ''{}''".format(<br>   1615     type(self).<strong>name</strong>,
          name))</p>

          <p>AttributeError: ''LlamaAttention'' object has no attribute ''qkv_proj''</p>

          '
        raw: "> Can you send me all the logs with errors? transformers and  auto_gptq's\
          \ version?\n\ntransformers                   4.31.0\nauto-gptq         \
          \             0.4.1+cu1180\n\nI install gptq by below command :\n!pip install\
          \ auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n\
          \nfull log error :\n---------------------------------------------------------------------------\n\
          AttributeError                            Traceback (most recent call last)\n\
          Cell In[1], line 15\n      9 model_path = \"/root/autodl-tmp/hf_repo/llama2_chinese\"\
          \ \n     12 # gptq_config = GPTQConfig(bits=4, disable_exllama=False)\n\
          ---> 15 model = AutoGPTQForCausalLM.from_quantized(model_path,device=CUDA_DEVICE)\n\
          \     16 model = model.eval()\n     17 tokenizer = AutoTokenizer.from_pretrained(model_path,use_fast=False)\n\
          \nFile ~/miniconda3/lib/python3.8/site-packages/auto_gptq/modeling/auto.py:108,\
          \ in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, device_map,\
          \ max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention,\
          \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
          \ trust_remote_code, warmup_triton, trainable, disable_exllama, **kwargs)\n\
          \    102 # TODO: do we need this filtering of kwargs? @PanQiWei is there\
          \ a reason we can't just pass all kwargs?\n    103 keywords = {\n    104\
          \     key: kwargs[key]\n    105     for key in list(signature(quant_func).parameters.keys())\
          \ + huggingface_kwargs\n    106     if key in kwargs\n    107 }\n--> 108\
          \ return quant_func(\n    109     model_name_or_path=model_name_or_path,\n\
          \    110     device_map=device_map,\n    111     max_memory=max_memory,\n\
          \    112     device=device,\n    113     low_cpu_mem_usage=low_cpu_mem_usage,\n\
          \    114     use_triton=use_triton,\n    115     inject_fused_attention=inject_fused_attention,\n\
          \    116     inject_fused_mlp=inject_fused_mlp,\n    117     use_cuda_fp16=use_cuda_fp16,\n\
          \    118     quantize_config=quantize_config,\n    119     model_basename=model_basename,\n\
          \    120     use_safetensors=use_safetensors,\n    121     trust_remote_code=trust_remote_code,\n\
          \    122     warmup_triton=warmup_triton,\n    123     trainable=trainable,\n\
          \    124     disable_exllama=disable_exllama,\n    125     **keywords\n\
          \    126 )\n\nFile ~/miniconda3/lib/python3.8/site-packages/auto_gptq/modeling/_base.py:875,\
          \ in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, device_map,\
          \ max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention,\
          \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
          \ trust_remote_code, warmup_triton, trainable, disable_exllama, **kwargs)\n\
          \    872 if low_cpu_mem_usage:\n    873     make_sure_no_tensor_in_meta_device(model,\
          \ use_triton, quantize_config.desc_act, quantize_config.group_size, bits=quantize_config.bits)\n\
          --> 875 accelerate.utils.modeling.load_checkpoint_in_model(\n    876   \
          \  model,\n    877     checkpoint=model_save_name,\n    878     device_map=device_map,\n\
          \    879     offload_state_dict=True,\n    880     offload_buffers=True\n\
          \    881 )\n    882 model = simple_dispatch_model(model, device_map)\n \
          \   884 # == step4: set seqlen == #\n\nFile ~/miniconda3/lib/python3.8/site-packages/accelerate/utils/modeling.py:1336,\
          \ in load_checkpoint_in_model(model, checkpoint, device_map, offload_folder,\
          \ dtype, offload_state_dict, offload_buffers, keep_in_fp32_modules, offload_8bit_bnb)\n\
          \   1334                 offload_weight(param, param_name, state_dict_folder,\
          \ index=state_dict_index)\n   1335         else:\n-> 1336             set_module_tensor_to_device(\n\
          \   1337                 model,\n   1338                 param_name,\n \
          \  1339                 param_device,\n   1340                 value=param,\n\
          \   1341                 dtype=new_dtype,\n   1342                 fp16_statistics=fp16_statistics,\n\
          \   1343             )\n   1345 # Force Python to clean up.\n   1346 del\
          \ checkpoint\n\nFile ~/miniconda3/lib/python3.8/site-packages/accelerate/utils/modeling.py:255,\
          \ in set_module_tensor_to_device(module, tensor_name, device, value, dtype,\
          \ fp16_statistics)\n    253 splits = tensor_name.split(\".\")\n    254 for\
          \ split in splits[:-1]:\n--> 255     new_module = getattr(module, split)\n\
          \    256     if new_module is None:\n    257         raise ValueError(f\"\
          {module} has no attribute {split}.\")\n\nFile ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1614,\
          \ in Module.__getattr__(self, name)\n   1612     if name in modules:\n \
          \  1613         return modules[name]\n-> 1614 raise AttributeError(\"'{}'\
          \ object has no attribute '{}'\".format(\n   1615     type(self).__name__,\
          \ name))\n\nAttributeError: 'LlamaAttention' object has no attribute 'qkv_proj'"
        updatedAt: '2023-08-24T04:09:10.112Z'
      numEdits: 0
      reactions: []
    id: 64e6d7e6c0cc3e95d1cf0b01
    type: comment
  author: lilsyoss
  content: "> Can you send me all the logs with errors? transformers and  auto_gptq's\
    \ version?\n\ntransformers                   4.31.0\nauto-gptq               \
    \       0.4.1+cu1180\n\nI install gptq by below command :\n!pip install auto-gptq\
    \ --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n\n\
    full log error :\n---------------------------------------------------------------------------\n\
    AttributeError                            Traceback (most recent call last)\n\
    Cell In[1], line 15\n      9 model_path = \"/root/autodl-tmp/hf_repo/llama2_chinese\"\
    \ \n     12 # gptq_config = GPTQConfig(bits=4, disable_exllama=False)\n---> 15\
    \ model = AutoGPTQForCausalLM.from_quantized(model_path,device=CUDA_DEVICE)\n\
    \     16 model = model.eval()\n     17 tokenizer = AutoTokenizer.from_pretrained(model_path,use_fast=False)\n\
    \nFile ~/miniconda3/lib/python3.8/site-packages/auto_gptq/modeling/auto.py:108,\
    \ in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, device_map, max_memory,\
    \ device, low_cpu_mem_usage, use_triton, inject_fused_attention, inject_fused_mlp,\
    \ use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code,\
    \ warmup_triton, trainable, disable_exllama, **kwargs)\n    102 # TODO: do we\
    \ need this filtering of kwargs? @PanQiWei is there a reason we can't just pass\
    \ all kwargs?\n    103 keywords = {\n    104     key: kwargs[key]\n    105   \
    \  for key in list(signature(quant_func).parameters.keys()) + huggingface_kwargs\n\
    \    106     if key in kwargs\n    107 }\n--> 108 return quant_func(\n    109\
    \     model_name_or_path=model_name_or_path,\n    110     device_map=device_map,\n\
    \    111     max_memory=max_memory,\n    112     device=device,\n    113     low_cpu_mem_usage=low_cpu_mem_usage,\n\
    \    114     use_triton=use_triton,\n    115     inject_fused_attention=inject_fused_attention,\n\
    \    116     inject_fused_mlp=inject_fused_mlp,\n    117     use_cuda_fp16=use_cuda_fp16,\n\
    \    118     quantize_config=quantize_config,\n    119     model_basename=model_basename,\n\
    \    120     use_safetensors=use_safetensors,\n    121     trust_remote_code=trust_remote_code,\n\
    \    122     warmup_triton=warmup_triton,\n    123     trainable=trainable,\n\
    \    124     disable_exllama=disable_exllama,\n    125     **keywords\n    126\
    \ )\n\nFile ~/miniconda3/lib/python3.8/site-packages/auto_gptq/modeling/_base.py:875,\
    \ in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, device_map, max_memory,\
    \ device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention,\
    \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
    \ trust_remote_code, warmup_triton, trainable, disable_exllama, **kwargs)\n  \
    \  872 if low_cpu_mem_usage:\n    873     make_sure_no_tensor_in_meta_device(model,\
    \ use_triton, quantize_config.desc_act, quantize_config.group_size, bits=quantize_config.bits)\n\
    --> 875 accelerate.utils.modeling.load_checkpoint_in_model(\n    876     model,\n\
    \    877     checkpoint=model_save_name,\n    878     device_map=device_map,\n\
    \    879     offload_state_dict=True,\n    880     offload_buffers=True\n    881\
    \ )\n    882 model = simple_dispatch_model(model, device_map)\n    884 # == step4:\
    \ set seqlen == #\n\nFile ~/miniconda3/lib/python3.8/site-packages/accelerate/utils/modeling.py:1336,\
    \ in load_checkpoint_in_model(model, checkpoint, device_map, offload_folder, dtype,\
    \ offload_state_dict, offload_buffers, keep_in_fp32_modules, offload_8bit_bnb)\n\
    \   1334                 offload_weight(param, param_name, state_dict_folder,\
    \ index=state_dict_index)\n   1335         else:\n-> 1336             set_module_tensor_to_device(\n\
    \   1337                 model,\n   1338                 param_name,\n   1339\
    \                 param_device,\n   1340                 value=param,\n   1341\
    \                 dtype=new_dtype,\n   1342                 fp16_statistics=fp16_statistics,\n\
    \   1343             )\n   1345 # Force Python to clean up.\n   1346 del checkpoint\n\
    \nFile ~/miniconda3/lib/python3.8/site-packages/accelerate/utils/modeling.py:255,\
    \ in set_module_tensor_to_device(module, tensor_name, device, value, dtype, fp16_statistics)\n\
    \    253 splits = tensor_name.split(\".\")\n    254 for split in splits[:-1]:\n\
    --> 255     new_module = getattr(module, split)\n    256     if new_module is\
    \ None:\n    257         raise ValueError(f\"{module} has no attribute {split}.\"\
    )\n\nFile ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1614,\
    \ in Module.__getattr__(self, name)\n   1612     if name in modules:\n   1613\
    \         return modules[name]\n-> 1614 raise AttributeError(\"'{}' object has\
    \ no attribute '{}'\".format(\n   1615     type(self).__name__, name))\n\nAttributeError:\
    \ 'LlamaAttention' object has no attribute 'qkv_proj'"
  created_at: 2023-08-24 03:09:10+00:00
  edited: false
  hidden: false
  id: 64e6d7e6c0cc3e95d1cf0b01
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: FlagAlpha/Llama2-Chinese-13b-Chat-4bit
repo_type: model
status: open
target_branch: null
title: saved model  reload error   'LlamaAttention' object has no attribute 'qkv_proj'
