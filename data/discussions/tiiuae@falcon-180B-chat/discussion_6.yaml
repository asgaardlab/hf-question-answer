!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wolfram
conflicting_files: null
created_at: 2023-09-14 22:39:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-09-14T23:39:36.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9656379818916321
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: '<p>Is extending context size via RoPE scaling supported? I''ve increased
          frequence base and frequence scale to get 4K instead of 2K context, but
          response quality degraded severely.</p>

          <p>I''d really like to use the model, but 2K context just isn''t enough
          for longer conversations. Intelligence unfortunately doesn''t mean much
          when its memory is limited to just the last few messages.</p>

          '
        raw: "Is extending context size via RoPE scaling supported? I've increased\
          \ frequence base and frequence scale to get 4K instead of 2K context, but\
          \ response quality degraded severely.\r\n\r\nI'd really like to use the\
          \ model, but 2K context just isn't enough for longer conversations. Intelligence\
          \ unfortunately doesn't mean much when its memory is limited to just the\
          \ last few messages."
        updatedAt: '2023-09-14T23:39:36.493Z'
      numEdits: 0
      reactions: []
    id: 650399b8da3344b5f10f6ba0
    type: comment
  author: wolfram
  content: "Is extending context size via RoPE scaling supported? I've increased frequence\
    \ base and frequence scale to get 4K instead of 2K context, but response quality\
    \ degraded severely.\r\n\r\nI'd really like to use the model, but 2K context just\
    \ isn't enough for longer conversations. Intelligence unfortunately doesn't mean\
    \ much when its memory is limited to just the last few messages."
  created_at: 2023-09-14 22:39:36+00:00
  edited: false
  hidden: false
  id: 650399b8da3344b5f10f6ba0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: tiiuae/falcon-180B-chat
repo_type: model
status: open
target_branch: null
title: RoPE scaling?
