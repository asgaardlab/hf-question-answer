!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Rbn3D
conflicting_files: null
created_at: 2023-05-12 10:56:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/343d806693bb9580133528b9ef684a1c.svg
      fullname: Ruben
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rbn3D
      type: user
    createdAt: '2023-05-12T11:56:29.000Z'
    data:
      edited: false
      editors:
      - Rbn3D
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/343d806693bb9580133528b9ef684a1c.svg
          fullname: Ruben
          isHf: false
          isPro: false
          name: Rbn3D
          type: user
        html: '<p>How much GPU memory does this model require to run? And in CPU mode,
          how much RAM? I''m currently trying to run it on GPU with a GTX 1080 8Gb,
          and I''m getting  a "cannot allocate memory" error, I suppose this requires
          at least 16gb or so.</p>

          '
        raw: How much GPU memory does this model require to run? And in CPU mode,
          how much RAM? I'm currently trying to run it on GPU with a GTX 1080 8Gb,
          and I'm getting  a "cannot allocate memory" error, I suppose this requires
          at least 16gb or so.
        updatedAt: '2023-05-12T11:56:29.738Z'
      numEdits: 0
      reactions: []
    id: 645e296d446af4fb6de42e75
    type: comment
  author: Rbn3D
  content: How much GPU memory does this model require to run? And in CPU mode, how
    much RAM? I'm currently trying to run it on GPU with a GTX 1080 8Gb, and I'm getting  a
    "cannot allocate memory" error, I suppose this requires at least 16gb or so.
  created_at: 2023-05-12 10:56:29+00:00
  edited: false
  hidden: false
  id: 645e296d446af4fb6de42e75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a446a1dd369ddb625d991f1d72a0b239.svg
      fullname: Oleksii
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Raspbfox
      type: user
    createdAt: '2023-05-14T23:57:50.000Z'
    data:
      edited: false
      editors:
      - Raspbfox
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a446a1dd369ddb625d991f1d72a0b239.svg
          fullname: Oleksii
          isHf: false
          isPro: false
          name: Raspbfox
          type: user
        html: '<p>I would assume it takes about ~15 GBs of VRAM without any optimizations!
          However, you can very successfully run it on a CPU with 5-bit quantization
          with just ~5.3 GBs of RAM taken!</p>

          '
        raw: I would assume it takes about ~15 GBs of VRAM without any optimizations!
          However, you can very successfully run it on a CPU with 5-bit quantization
          with just ~5.3 GBs of RAM taken!
        updatedAt: '2023-05-14T23:57:50.611Z'
      numEdits: 0
      reactions: []
    id: 6461757eb2ae2983b1093f38
    type: comment
  author: Raspbfox
  content: I would assume it takes about ~15 GBs of VRAM without any optimizations!
    However, you can very successfully run it on a CPU with 5-bit quantization with
    just ~5.3 GBs of RAM taken!
  created_at: 2023-05-14 22:57:50+00:00
  edited: false
  hidden: false
  id: 6461757eb2ae2983b1093f38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a446a1dd369ddb625d991f1d72a0b239.svg
      fullname: Oleksii
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Raspbfox
      type: user
    createdAt: '2023-05-14T23:58:31.000Z'
    data:
      edited: false
      editors:
      - Raspbfox
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a446a1dd369ddb625d991f1d72a0b239.svg
          fullname: Oleksii
          isHf: false
          isPro: false
          name: Raspbfox
          type: user
        html: '<p>In theory, you might be able to run it in bfloat16 mode, but I don''t
          know how, sry.</p>

          '
        raw: In theory, you might be able to run it in bfloat16 mode, but I don't
          know how, sry.
        updatedAt: '2023-05-14T23:58:31.376Z'
      numEdits: 0
      reactions: []
    id: 646175a7bf985b8b4eb94d05
    type: comment
  author: Raspbfox
  content: In theory, you might be able to run it in bfloat16 mode, but I don't know
    how, sry.
  created_at: 2023-05-14 22:58:31+00:00
  edited: false
  hidden: false
  id: 646175a7bf985b8b4eb94d05
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/993455271ba92052096d8a572af03818.svg
      fullname: Daniel Daugherty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danieldaugherty
      type: user
    createdAt: '2023-05-22T17:17:15.000Z'
    data:
      edited: false
      editors:
      - danieldaugherty
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/993455271ba92052096d8a572af03818.svg
          fullname: Daniel Daugherty
          isHf: false
          isPro: false
          name: danieldaugherty
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Raspbfox&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Raspbfox\">@<span class=\"\
          underline\">Raspbfox</span></a></span>\n\n\t</span></span> I searched far\
          \ and wide for a quantization example, but couldn't find one... =[</p>\n"
        raw: '@Raspbfox I searched far and wide for a quantization example, but couldn''t
          find one... =['
        updatedAt: '2023-05-22T17:17:15.625Z'
      numEdits: 0
      reactions: []
    id: 646ba39b5d68f5c15a2a1629
    type: comment
  author: danieldaugherty
  content: '@Raspbfox I searched far and wide for a quantization example, but couldn''t
    find one... =['
  created_at: 2023-05-22 16:17:15+00:00
  edited: false
  hidden: false
  id: 646ba39b5d68f5c15a2a1629
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a446a1dd369ddb625d991f1d72a0b239.svg
      fullname: Oleksii
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Raspbfox
      type: user
    createdAt: '2023-05-22T17:20:08.000Z'
    data:
      edited: false
      editors:
      - Raspbfox
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a446a1dd369ddb625d991f1d72a0b239.svg
          fullname: Oleksii
          isHf: false
          isPro: false
          name: Raspbfox
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;danieldaugherty&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/danieldaugherty\"\
          >@<span class=\"underline\">danieldaugherty</span></a></span>\n\n\t</span></span>,\
          \ just try searching for the GGML quantized models (usually q5_1) or GPTQ\
          \ \U0001F440</p>\n"
        raw: "@danieldaugherty, just try searching for the GGML quantized models (usually\
          \ q5_1) or GPTQ \U0001F440"
        updatedAt: '2023-05-22T17:20:08.581Z'
      numEdits: 0
      reactions: []
    id: 646ba448f85ebf65c5361f4d
    type: comment
  author: Raspbfox
  content: "@danieldaugherty, just try searching for the GGML quantized models (usually\
    \ q5_1) or GPTQ \U0001F440"
  created_at: 2023-05-22 16:20:08+00:00
  edited: false
  hidden: false
  id: 646ba448f85ebf65c5361f4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/993455271ba92052096d8a572af03818.svg
      fullname: Daniel Daugherty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danieldaugherty
      type: user
    createdAt: '2023-05-22T17:33:16.000Z'
    data:
      edited: false
      editors:
      - danieldaugherty
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/993455271ba92052096d8a572af03818.svg
          fullname: Daniel Daugherty
          isHf: false
          isPro: false
          name: danieldaugherty
          type: user
        html: '<p>Ah yeah, I found that. But I didn''t really understand how to use
          it...</p>

          '
        raw: Ah yeah, I found that. But I didn't really understand how to use it...
        updatedAt: '2023-05-22T17:33:16.233Z'
      numEdits: 0
      reactions: []
    id: 646ba75ced22827213358832
    type: comment
  author: danieldaugherty
  content: Ah yeah, I found that. But I didn't really understand how to use it...
  created_at: 2023-05-22 16:33:16+00:00
  edited: false
  hidden: false
  id: 646ba75ced22827213358832
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/993455271ba92052096d8a572af03818.svg
      fullname: Daniel Daugherty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danieldaugherty
      type: user
    createdAt: '2023-05-22T17:47:26.000Z'
    data:
      edited: false
      editors:
      - danieldaugherty
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/993455271ba92052096d8a572af03818.svg
          fullname: Daniel Daugherty
          isHf: false
          isPro: false
          name: danieldaugherty
          type: user
        html: '<p>GPTQ doesn''t support MPT yet =[</p>

          '
        raw: GPTQ doesn't support MPT yet =[
        updatedAt: '2023-05-22T17:47:26.344Z'
      numEdits: 0
      reactions: []
    id: 646baaaeed22827213361581
    type: comment
  author: danieldaugherty
  content: GPTQ doesn't support MPT yet =[
  created_at: 2023-05-22 16:47:26+00:00
  edited: false
  hidden: false
  id: 646baaaeed22827213361581
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/614c14d789d9fe69398765c5/U4oDFoKJ0e6Uj56uWPf8m.jpeg?w=200&h=200&f=face
      fullname: Shreyas S K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: skshreyas714
      type: user
    createdAt: '2023-05-23T08:46:33.000Z'
    data:
      edited: false
      editors:
      - skshreyas714
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/614c14d789d9fe69398765c5/U4oDFoKJ0e6Uj56uWPf8m.jpeg?w=200&h=200&f=face
          fullname: Shreyas S K
          isHf: false
          isPro: false
          name: skshreyas714
          type: user
        html: '<p>When you run this MPT-7B model in FP16 then it would consume 14
          GB of GPU memory. So you would need atleast 16 GB of GPU memory to run this
          model for Inference</p>

          '
        raw: When you run this MPT-7B model in FP16 then it would consume 14 GB of
          GPU memory. So you would need atleast 16 GB of GPU memory to run this model
          for Inference
        updatedAt: '2023-05-23T08:46:33.358Z'
      numEdits: 0
      reactions: []
    id: 646c7d6910f66cc3c75a696b
    type: comment
  author: skshreyas714
  content: When you run this MPT-7B model in FP16 then it would consume 14 GB of GPU
    memory. So you would need atleast 16 GB of GPU memory to run this model for Inference
  created_at: 2023-05-23 07:46:33+00:00
  edited: false
  hidden: false
  id: 646c7d6910f66cc3c75a696b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-03T00:54:28.000Z'
    data:
      edited: false
      editors:
      - abhi-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.921229362487793
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: '<p>Closing as stale. </p>

          <p>Also noting that we added <code>device_map</code> support as of this
          PR: <a href="https://huggingface.co/mosaicml/mpt-7b-instruct/discussions/41">https://huggingface.co/mosaicml/mpt-7b-instruct/discussions/41</a></p>

          '
        raw: "Closing as stale. \n\nAlso noting that we added `device_map` support\
          \ as of this PR: https://huggingface.co/mosaicml/mpt-7b-instruct/discussions/41"
        updatedAt: '2023-06-03T00:54:28.424Z'
      numEdits: 0
      reactions: []
    id: 647a8f44c7367455fda34ec5
    type: comment
  author: abhi-mosaic
  content: "Closing as stale. \n\nAlso noting that we added `device_map` support as\
    \ of this PR: https://huggingface.co/mosaicml/mpt-7b-instruct/discussions/41"
  created_at: 2023-06-02 23:54:28+00:00
  edited: false
  hidden: false
  id: 647a8f44c7367455fda34ec5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-03T01:01:45.000Z'
    data:
      status: closed
    id: 647a90f942abe2774762db19
    type: status-change
  author: abhi-mosaic
  created_at: 2023-06-03 00:01:45+00:00
  id: 647a90f942abe2774762db19
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: mosaicml/mpt-7b-instruct
repo_type: model
status: closed
target_branch: null
title: GPU Memory / RAM requierements
