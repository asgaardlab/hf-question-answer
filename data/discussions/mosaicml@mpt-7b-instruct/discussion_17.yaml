!!python/object:huggingface_hub.community.DiscussionWithDetails
author: debajyoti111
conflicting_files: null
created_at: 2023-05-11 10:10:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/610d7b4df9980898bf2e0d17120bd09b.svg
      fullname: Debajyoti Dhar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: debajyoti111
      type: user
    createdAt: '2023-05-11T11:10:56.000Z'
    data:
      edited: true
      editors:
      - debajyoti111
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/610d7b4df9980898bf2e0d17120bd09b.svg
          fullname: Debajyoti Dhar
          isHf: false
          isPro: false
          name: debajyoti111
          type: user
        html: "<p>Hi, I am new to NLP and am still learning. I am using a VM of GCP(e2-highmem-4\
          \ (Efficient Instance, 4 vCPUs, 32 GB RAM)) to load the model and use it.\
          \ Here is the code I have written-</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> pipeline\n\
          <span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, AutoModelForSequenceClassification\n<span\
          \ class=\"hljs-keyword\">import</span> transformers\nconfig = transformers.AutoConfig.from_pretrained(\n\
          \  <span class=\"hljs-string\">'mosaicml/mpt-7b-instruct'</span>,\n  trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>,\n)\n<span class=\"hljs-comment\">#\
          \ config.attn_config['attn_impl'] = 'flash'</span>\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \  <span class=\"hljs-string\">'mosaicml/mpt-7b-instruct'</span>,\n  config=config,\n\
          \  torch_dtype=torch.bfloat16,\n  trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>,\n  cache_dir=<span class=\"hljs-string\">\"./cache\"</span>\n\
          )\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"\
          hljs-keyword\">import</span> AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"EleutherAI/gpt-neox-20b\"</span>, cache_dir=<span\
          \ class=\"hljs-string\">\"./cache\"</span>)\ntext_gen = pipeline(<span class=\"\
          hljs-string\">\"text-generation\"</span>, model=model, tokenizer=tokenizer)\n\
          text_gen(text_inputs=<span class=\"hljs-string\">\"what is 2+2?\"</span>)\n\
          </code></pre>\n<p>Now the code is taking way too long to generate the text.\
          \ Am I doing something wrong? or is there any way to make things faster?<br>Also,\
          \ when creating the pipeline, I am getting the following warning-</p>\n\
          <pre><code>The model 'MPTForCausalLM' is not supported for text-generation\n\
          </code></pre>\n<p>I saw in another discussion that this shouldn't be a problem\
          \ as the architecture is custom?</p>\n"
        raw: "Hi, I am new to NLP and am still learning. I am using a VM of GCP(e2-highmem-4\
          \ (Efficient Instance, 4 vCPUs, 32 GB RAM)) to load the model and use it.\
          \ Here is the code I have written-\n```python\nimport torch\nfrom transformers\
          \ import pipeline\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\
          import transformers\nconfig = transformers.AutoConfig.from_pretrained(\n\
          \  'mosaicml/mpt-7b-instruct',\n  trust_remote_code=True,\n)\n# config.attn_config['attn_impl']\
          \ = 'flash'\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \  'mosaicml/mpt-7b-instruct',\n  config=config,\n  torch_dtype=torch.bfloat16,\n\
          \  trust_remote_code=True,\n  cache_dir=\"./cache\"\n)\nfrom transformers\
          \ import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\"\
          , cache_dir=\"./cache\")\ntext_gen = pipeline(\"text-generation\", model=model,\
          \ tokenizer=tokenizer)\ntext_gen(text_inputs=\"what is 2+2?\")\n```\nNow\
          \ the code is taking way too long to generate the text. Am I doing something\
          \ wrong? or is there any way to make things faster? \nAlso, when creating\
          \ the pipeline, I am getting the following warning-\n```\nThe model 'MPTForCausalLM'\
          \ is not supported for text-generation\n```\nI saw in another discussion\
          \ that this shouldn't be a problem as the architecture is custom?"
        updatedAt: '2023-05-11T12:02:39.566Z'
      numEdits: 2
      reactions: []
    id: 645ccd40d90782b1a6a59e50
    type: comment
  author: debajyoti111
  content: "Hi, I am new to NLP and am still learning. I am using a VM of GCP(e2-highmem-4\
    \ (Efficient Instance, 4 vCPUs, 32 GB RAM)) to load the model and use it. Here\
    \ is the code I have written-\n```python\nimport torch\nfrom transformers import\
    \ pipeline\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\
    import transformers\nconfig = transformers.AutoConfig.from_pretrained(\n  'mosaicml/mpt-7b-instruct',\n\
    \  trust_remote_code=True,\n)\n# config.attn_config['attn_impl'] = 'flash'\n\n\
    model = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b-instruct',\n\
    \  config=config,\n  torch_dtype=torch.bfloat16,\n  trust_remote_code=True,\n\
    \  cache_dir=\"./cache\"\n)\nfrom transformers import AutoTokenizer\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\", cache_dir=\"./cache\"\
    )\ntext_gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\
    text_gen(text_inputs=\"what is 2+2?\")\n```\nNow the code is taking way too long\
    \ to generate the text. Am I doing something wrong? or is there any way to make\
    \ things faster? \nAlso, when creating the pipeline, I am getting the following\
    \ warning-\n```\nThe model 'MPTForCausalLM' is not supported for text-generation\n\
    ```\nI saw in another discussion that this shouldn't be a problem as the architecture\
    \ is custom?"
  created_at: 2023-05-11 10:10:56+00:00
  edited: true
  hidden: false
  id: 645ccd40d90782b1a6a59e50
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-11T23:36:03.000Z'
    data:
      edited: true
      editors:
      - abhi-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: '<p>Hi @ debajyoti111, could you try removing the line <code>torch_dtype=torch.bfloat16</code>.
          I''m seeing in another post that on some CPU machines, this causes it to
          run very slowly. Removing this line will fallback to default <code>torch.float32</code>
          weights and math.</p>

          <p>The <code>not supported for text-generation</code> warning can be ignored.</p>

          <p>Also taking a step back, to separate the system from the code, can you
          confirm if MPT is faster/slower than when you run generation with other
          HF models like <code>OPT-6.7B</code>? In general, running LLMs on CPUs is
          going to be very slow without a custom framework like <a rel="nofollow"
          href="https://github.com/ggerganov/ggml">GGML</a>. Right now we are focused
          mainly on GPU inference, which should be quite fast when using the <code>attn_impl:
          triton</code> backend.</p>

          <p>Let me know if the generation speed gets better!</p>

          '
        raw: 'Hi @ debajyoti111, could you try removing the line `torch_dtype=torch.bfloat16`.
          I''m seeing in another post that on some CPU machines, this causes it to
          run very slowly. Removing this line will fallback to default `torch.float32`
          weights and math.


          The `not supported for text-generation` warning can be ignored.


          Also taking a step back, to separate the system from the code, can you confirm
          if MPT is faster/slower than when you run generation with other HF models
          like `OPT-6.7B`? In general, running LLMs on CPUs is going to be very slow
          without a custom framework like [GGML](https://github.com/ggerganov/ggml).
          Right now we are focused mainly on GPU inference, which should be quite
          fast when using the `attn_impl: triton` backend.


          Let me know if the generation speed gets better!'
        updatedAt: '2023-05-11T23:36:56.695Z'
      numEdits: 1
      reactions: []
    id: 645d7be34e2cf468917ec9f0
    type: comment
  author: abhi-mosaic
  content: 'Hi @ debajyoti111, could you try removing the line `torch_dtype=torch.bfloat16`.
    I''m seeing in another post that on some CPU machines, this causes it to run very
    slowly. Removing this line will fallback to default `torch.float32` weights and
    math.


    The `not supported for text-generation` warning can be ignored.


    Also taking a step back, to separate the system from the code, can you confirm
    if MPT is faster/slower than when you run generation with other HF models like
    `OPT-6.7B`? In general, running LLMs on CPUs is going to be very slow without
    a custom framework like [GGML](https://github.com/ggerganov/ggml). Right now we
    are focused mainly on GPU inference, which should be quite fast when using the
    `attn_impl: triton` backend.


    Let me know if the generation speed gets better!'
  created_at: 2023-05-11 22:36:03+00:00
  edited: true
  hidden: false
  id: 645d7be34e2cf468917ec9f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-03T01:01:30.000Z'
    data:
      edited: false
      editors:
      - abhi-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8983105421066284
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: '<p>Closing as stale.</p>

          <p>Also wanted to note that we added support for <code>device_map</code>
          and faster KV cacheing in this PR: <a href="https://huggingface.co/mosaicml/mpt-7b-instruct/discussions/41">https://huggingface.co/mosaicml/mpt-7b-instruct/discussions/41</a></p>

          '
        raw: 'Closing as stale.


          Also wanted to note that we added support for `device_map` and faster KV
          cacheing in this PR: https://huggingface.co/mosaicml/mpt-7b-instruct/discussions/41'
        updatedAt: '2023-06-03T01:01:30.339Z'
      numEdits: 0
      reactions: []
      relatedEventId: 647a90ea44b6a3ae9d2aa7e5
    id: 647a90ea44b6a3ae9d2aa7e3
    type: comment
  author: abhi-mosaic
  content: 'Closing as stale.


    Also wanted to note that we added support for `device_map` and faster KV cacheing
    in this PR: https://huggingface.co/mosaicml/mpt-7b-instruct/discussions/41'
  created_at: 2023-06-03 00:01:30+00:00
  edited: false
  hidden: false
  id: 647a90ea44b6a3ae9d2aa7e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-03T01:01:30.000Z'
    data:
      status: closed
    id: 647a90ea44b6a3ae9d2aa7e5
    type: status-change
  author: abhi-mosaic
  created_at: 2023-06-03 00:01:30+00:00
  id: 647a90ea44b6a3ae9d2aa7e5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: mosaicml/mpt-7b-instruct
repo_type: model
status: closed
target_branch: null
title: Help Needed!! Text Generation Taking Too Long
