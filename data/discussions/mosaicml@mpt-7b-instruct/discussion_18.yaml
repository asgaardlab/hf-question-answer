!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fathyshalab
conflicting_files: null
created_at: 2023-05-12 10:51:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63287b3c97bfeec05c87eba2/DSceq2uEcCRhP-xXfOV-V.png?w=200&h=200&f=face
      fullname: "Fath\xFD Shalaby"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fathyshalab
      type: user
    createdAt: '2023-05-12T11:51:28.000Z'
    data:
      edited: false
      editors:
      - fathyshalab
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63287b3c97bfeec05c87eba2/DSceq2uEcCRhP-xXfOV-V.png?w=200&h=200&f=face
          fullname: "Fath\xFD Shalaby"
          isHf: false
          isPro: false
          name: fathyshalab
          type: user
        html: '<p>Is there any way I can try to limit the number of hallucinations
          away from a given context?</p>

          <p>Currently, it tries to answer outside of the context if the information
          isn''t in the context rather than saying it can''t answer. The prompt I
          am using is the same as the quick_pipeline.py on your space. I''m trying
          to integrate it into the langchain Retrival QA over docs. What is the best
          way to do that. I tried reducing the topk etc. but havent gotten better
          results. </p>

          '
        raw: "Is there any way I can try to limit the number of hallucinations away\
          \ from a given context?\r\n\r\nCurrently, it tries to answer outside of\
          \ the context if the information isn't in the context rather than saying\
          \ it can't answer. The prompt I am using is the same as the quick_pipeline.py\
          \ on your space. I'm trying to integrate it into the langchain Retrival\
          \ QA over docs. What is the best way to do that. I tried reducing the topk\
          \ etc. but havent gotten better results. "
        updatedAt: '2023-05-12T11:51:28.262Z'
      numEdits: 0
      reactions: []
    id: 645e28407f5735b5435f45cd
    type: comment
  author: fathyshalab
  content: "Is there any way I can try to limit the number of hallucinations away\
    \ from a given context?\r\n\r\nCurrently, it tries to answer outside of the context\
    \ if the information isn't in the context rather than saying it can't answer.\
    \ The prompt I am using is the same as the quick_pipeline.py on your space. I'm\
    \ trying to integrate it into the langchain Retrival QA over docs. What is the\
    \ best way to do that. I tried reducing the topk etc. but havent gotten better\
    \ results. "
  created_at: 2023-05-12 10:51:28+00:00
  edited: false
  hidden: false
  id: 645e28407f5735b5435f45cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ff5fc4fe6383d50b29052e/Vk9R5rKqG-Z_ou-55J9x-.jpeg?w=200&h=200&f=face
      fullname: AayushShah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AayushShah
      type: user
    createdAt: '2023-05-15T06:00:52.000Z'
    data:
      edited: false
      editors:
      - AayushShah
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ff5fc4fe6383d50b29052e/Vk9R5rKqG-Z_ou-55J9x-.jpeg?w=200&h=200&f=face
          fullname: AayushShah
          isHf: false
          isPro: false
          name: AayushShah
          type: user
        html: '<p>I would suggest you to try with "your own prompt" first, without
          using the Langchain. I mean, just load the model with transformers and try
          generating from there - with customizing the prompt as needed.</p>

          <p>In this case, you will need to pass the documents as the context by yourself.
          But this will allow you to play with the prompt because langchain''s default
          prompt doesn''t work well for the open-source models.<br>Then, once you
          find the better prompt, which makes the model to answer from the context
          only and say ''I don''t know'' when the context is not given, then use that
          custom prompt in the langchain.</p>

          <p>Let me know if that helps.</p>

          '
        raw: 'I would suggest you to try with "your own prompt" first, without using
          the Langchain. I mean, just load the model with transformers and try generating
          from there - with customizing the prompt as needed.


          In this case, you will need to pass the documents as the context by yourself.
          But this will allow you to play with the prompt because langchain''s default
          prompt doesn''t work well for the open-source models.

          Then, once you find the better prompt, which makes the model to answer from
          the context only and say ''I don''t know'' when the context is not given,
          then use that custom prompt in the langchain.


          Let me know if that helps.'
        updatedAt: '2023-05-15T06:00:52.222Z'
      numEdits: 0
      reactions: []
    id: 6461ca94b2ae2983b10be27e
    type: comment
  author: AayushShah
  content: 'I would suggest you to try with "your own prompt" first, without using
    the Langchain. I mean, just load the model with transformers and try generating
    from there - with customizing the prompt as needed.


    In this case, you will need to pass the documents as the context by yourself.
    But this will allow you to play with the prompt because langchain''s default prompt
    doesn''t work well for the open-source models.

    Then, once you find the better prompt, which makes the model to answer from the
    context only and say ''I don''t know'' when the context is not given, then use
    that custom prompt in the langchain.


    Let me know if that helps.'
  created_at: 2023-05-15 05:00:52+00:00
  edited: false
  hidden: false
  id: 6461ca94b2ae2983b10be27e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9d5666f3447b9c2893d208c205a7a9a.svg
      fullname: WANG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YYEEEEEEEEE
      type: user
    createdAt: '2023-05-15T08:23:03.000Z'
    data:
      edited: false
      editors:
      - YYEEEEEEEEE
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9d5666f3447b9c2893d208c205a7a9a.svg
          fullname: WANG
          isHf: false
          isPro: false
          name: YYEEEEEEEEE
          type: user
        html: "<p>hi, I want to cite the dataset \"fathyshalab/clinic-travel\u201D\
          , but you donot have citation information.</p>\n"
        raw: "hi, I want to cite the dataset \"fathyshalab/clinic-travel\u201D, but\
          \ you donot have citation information."
        updatedAt: '2023-05-15T08:23:03.554Z'
      numEdits: 0
      reactions: []
    id: 6461ebe7c67e1a494d8ef4d0
    type: comment
  author: YYEEEEEEEEE
  content: "hi, I want to cite the dataset \"fathyshalab/clinic-travel\u201D, but\
    \ you donot have citation information."
  created_at: 2023-05-15 07:23:03+00:00
  edited: false
  hidden: false
  id: 6461ebe7c67e1a494d8ef4d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63287b3c97bfeec05c87eba2/DSceq2uEcCRhP-xXfOV-V.png?w=200&h=200&f=face
      fullname: "Fath\xFD Shalaby"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fathyshalab
      type: user
    createdAt: '2023-05-15T08:53:38.000Z'
    data:
      edited: false
      editors:
      - fathyshalab
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63287b3c97bfeec05c87eba2/DSceq2uEcCRhP-xXfOV-V.png?w=200&h=200&f=face
          fullname: "Fath\xFD Shalaby"
          isHf: false
          isPro: false
          name: fathyshalab
          type: user
        html: '<blockquote>

          <p>I would suggest you to try with "your own prompt" first, without using
          the Langchain. I mean, just load the model with transformers and try generating
          from there - with customizing the prompt as needed.</p>

          <p>In this case, you will need to pass the documents as the context by yourself.
          But this will allow you to play with the prompt because langchain''s default
          prompt doesn''t work well for the open-source models.<br>Then, once you
          find the better prompt, which makes the model to answer from the context
          only and say ''I don''t know'' when the context is not given, then use that
          custom prompt in the langchain.</p>

          <p>Let me know if that helps.</p>

          </blockquote>

          <p>thanks tried it out, still getting some random answers but is already
          slightly better</p>

          '
        raw: "> I would suggest you to try with \"your own prompt\" first, without\
          \ using the Langchain. I mean, just load the model with transformers and\
          \ try generating from there - with customizing the prompt as needed.\n>\
          \ \n> In this case, you will need to pass the documents as the context by\
          \ yourself. But this will allow you to play with the prompt because langchain's\
          \ default prompt doesn't work well for the open-source models.\n> Then,\
          \ once you find the better prompt, which makes the model to answer from\
          \ the context only and say 'I don't know' when the context is not given,\
          \ then use that custom prompt in the langchain.\n> \n> Let me know if that\
          \ helps.\n\nthanks tried it out, still getting some random answers but is\
          \ already slightly better"
        updatedAt: '2023-05-15T08:53:38.996Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6461f3123026e7f163c7ae03
    id: 6461f3123026e7f163c7ae02
    type: comment
  author: fathyshalab
  content: "> I would suggest you to try with \"your own prompt\" first, without using\
    \ the Langchain. I mean, just load the model with transformers and try generating\
    \ from there - with customizing the prompt as needed.\n> \n> In this case, you\
    \ will need to pass the documents as the context by yourself. But this will allow\
    \ you to play with the prompt because langchain's default prompt doesn't work\
    \ well for the open-source models.\n> Then, once you find the better prompt, which\
    \ makes the model to answer from the context only and say 'I don't know' when\
    \ the context is not given, then use that custom prompt in the langchain.\n> \n\
    > Let me know if that helps.\n\nthanks tried it out, still getting some random\
    \ answers but is already slightly better"
  created_at: 2023-05-15 07:53:38+00:00
  edited: false
  hidden: false
  id: 6461f3123026e7f163c7ae02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63287b3c97bfeec05c87eba2/DSceq2uEcCRhP-xXfOV-V.png?w=200&h=200&f=face
      fullname: "Fath\xFD Shalaby"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fathyshalab
      type: user
    createdAt: '2023-05-15T08:53:38.000Z'
    data:
      status: closed
    id: 6461f3123026e7f163c7ae03
    type: status-change
  author: fathyshalab
  created_at: 2023-05-15 07:53:38+00:00
  id: 6461f3123026e7f163c7ae03
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: mosaicml/mpt-7b-instruct
repo_type: model
status: closed
target_branch: null
title: Langchain QA hallucinations
