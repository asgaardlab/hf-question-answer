!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Akshay1996
conflicting_files: null
created_at: 2023-07-26 03:51:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/46bcb245c6f8b4ba92391948d7552caf.svg
      fullname: Akshay Sonawane
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Akshay1996
      type: user
    createdAt: '2023-07-26T04:51:22.000Z'
    data:
      edited: false
      editors:
      - Akshay1996
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8070826530456543
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/46bcb245c6f8b4ba92391948d7552caf.svg
          fullname: Akshay Sonawane
          isHf: false
          isPro: false
          name: Akshay1996
          type: user
        html: '<p>I am trying to export this model to onnx using cuda but it fails
          with CUDA OOM error.<br>torch.cuda.OutOfMemoryError: CUDA out of memory.
          Tried to allocate 256.00 MiB (GPU 0; 31.75 GiB total capacity; 31.13 GiB
          already allocated; 225.75 MiB free; 31.16 GiB reserved in total by PyTorch)
          If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb
          to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>Does anyone know how to workaround this?</p>

          '
        raw: "I am trying to export this model to onnx using cuda but it fails with\
          \ CUDA OOM error.\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried\
          \ to allocate 256.00 MiB (GPU 0; 31.75 GiB total capacity; 31.13 GiB already\
          \ allocated; 225.75 MiB free; 31.16 GiB reserved in total by PyTorch) If\
          \ reserved memory is >> allocated memory try setting max_split_size_mb to\
          \ avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
          \n\r\nDoes anyone know how to workaround this?"
        updatedAt: '2023-07-26T04:51:22.866Z'
      numEdits: 0
      reactions: []
    id: 64c0a64a52874207b2d9aca2
    type: comment
  author: Akshay1996
  content: "I am trying to export this model to onnx using cuda but it fails with\
    \ CUDA OOM error.\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to\
    \ allocate 256.00 MiB (GPU 0; 31.75 GiB total capacity; 31.13 GiB already allocated;\
    \ 225.75 MiB free; 31.16 GiB reserved in total by PyTorch) If reserved memory\
    \ is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
    \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\n\
    Does anyone know how to workaround this?"
  created_at: 2023-07-26 03:51:22+00:00
  edited: false
  hidden: false
  id: 64c0a64a52874207b2d9aca2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6450857a5fdcff143ad95ef3/NRB-s4hgDBw4oub77r4pG.png?w=200&h=200&f=face
      fullname: Chinmaya Andukuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: scandukuri
      type: user
    createdAt: '2023-07-27T17:57:47.000Z'
    data:
      edited: false
      editors:
      - scandukuri
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9733905792236328
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6450857a5fdcff143ad95ef3/NRB-s4hgDBw4oub77r4pG.png?w=200&h=200&f=face
          fullname: Chinmaya Andukuri
          isHf: false
          isPro: false
          name: scandukuri
          type: user
        html: '<p>Running into the same issue. Some discussion here on how it could
          be made to work <a rel="nofollow" href="https://github.com/huggingface/optimum/issues/1061">https://github.com/huggingface/optimum/issues/1061</a>
          I''m not quite sure where exactly but I saw someone comment somewhere that
          they just needed a large amount of memory to get it done.</p>

          '
        raw: Running into the same issue. Some discussion here on how it could be
          made to work https://github.com/huggingface/optimum/issues/1061 I'm not
          quite sure where exactly but I saw someone comment somewhere that they just
          needed a large amount of memory to get it done.
        updatedAt: '2023-07-27T17:57:47.477Z'
      numEdits: 0
      reactions: []
    id: 64c2b01b37286ca2eff9085c
    type: comment
  author: scandukuri
  content: Running into the same issue. Some discussion here on how it could be made
    to work https://github.com/huggingface/optimum/issues/1061 I'm not quite sure
    where exactly but I saw someone comment somewhere that they just needed a large
    amount of memory to get it done.
  created_at: 2023-07-27 16:57:47+00:00
  edited: false
  hidden: false
  id: 64c2b01b37286ca2eff9085c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 66
repo_id: mosaicml/mpt-7b-instruct
repo_type: model
status: open
target_branch: null
title: Onnx export fails when using cuda:0 as init device
