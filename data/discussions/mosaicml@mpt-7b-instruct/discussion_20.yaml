!!python/object:huggingface_hub.community.DiscussionWithDetails
author: uglydumpling
conflicting_files: null
created_at: 2023-05-16 18:46:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a0baebcb53d3a3ee421532eb164a9a6.svg
      fullname: Blue
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: uglydumpling
      type: user
    createdAt: '2023-05-16T19:46:56.000Z'
    data:
      edited: false
      editors:
      - uglydumpling
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a0baebcb53d3a3ee421532eb164a9a6.svg
          fullname: Blue
          isHf: false
          isPro: false
          name: uglydumpling
          type: user
        html: '<p>Can we run this model without using flash_attn on GPU?</p>

          '
        raw: Can we run this model without using flash_attn on GPU?
        updatedAt: '2023-05-16T19:46:56.310Z'
      numEdits: 0
      reactions: []
    id: 6463ddb0e0475a83333c5e79
    type: comment
  author: uglydumpling
  content: Can we run this model without using flash_attn on GPU?
  created_at: 2023-05-16 18:46:56+00:00
  edited: false
  hidden: false
  id: 6463ddb0e0475a83333c5e79
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-16T21:31:32.000Z'
    data:
      edited: false
      editors:
      - abhi-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: "<p>Yes you can! Just use <code>attn_impl: torch</code>.</p>\n<p>You\
          \ can do this by editing the <code>config.json</code> directly or by following\
          \ the instructions in the README: </p>\n<pre><code class=\"language-python\"\
          >config = transformers.AutoConfig.from_pretrained(\n  <span class=\"hljs-string\"\
          >'mosaicml/mpt-7b'</span>,\n  trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>\n)\nconfig.attn_config.attn_impl = <span class=\"hljs-string\"\
          >'torch'</span> <span class=\"hljs-comment\"># it should already be 'torch'\
          \ but just for clarity</span>\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \  <span class=\"hljs-string\">'mosaicml/mpt-7b'</span>,\n  config=config,\n\
          \  torch_dtype=torch.bfloat16,\n  trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>\n)\nmodel.to(device=<span class=\"hljs-string\">'cuda:0'</span>)\n\
          </code></pre>\n"
        raw: "Yes you can! Just use `attn_impl: torch`.\n\nYou can do this by editing\
          \ the `config.json` directly or by following the instructions in the README:\
          \ \n\n```python\nconfig = transformers.AutoConfig.from_pretrained(\n  'mosaicml/mpt-7b',\n\
          \  trust_remote_code=True\n)\nconfig.attn_config.attn_impl = 'torch' # it\
          \ should already be 'torch' but just for clarity\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \  'mosaicml/mpt-7b',\n  config=config,\n  torch_dtype=torch.bfloat16,\n\
          \  trust_remote_code=True\n)\nmodel.to(device='cuda:0')\n"
        updatedAt: '2023-05-16T21:31:32.106Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6463f6346dad99445decbd3c
    id: 6463f6346dad99445decbd3b
    type: comment
  author: abhi-mosaic
  content: "Yes you can! Just use `attn_impl: torch`.\n\nYou can do this by editing\
    \ the `config.json` directly or by following the instructions in the README: \n\
    \n```python\nconfig = transformers.AutoConfig.from_pretrained(\n  'mosaicml/mpt-7b',\n\
    \  trust_remote_code=True\n)\nconfig.attn_config.attn_impl = 'torch' # it should\
    \ already be 'torch' but just for clarity\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
    \  'mosaicml/mpt-7b',\n  config=config,\n  torch_dtype=torch.bfloat16,\n  trust_remote_code=True\n\
    )\nmodel.to(device='cuda:0')\n"
  created_at: 2023-05-16 20:31:32+00:00
  edited: false
  hidden: false
  id: 6463f6346dad99445decbd3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-16T21:31:32.000Z'
    data:
      status: closed
    id: 6463f6346dad99445decbd3c
    type: status-change
  author: abhi-mosaic
  created_at: 2023-05-16 20:31:32+00:00
  id: 6463f6346dad99445decbd3c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: mosaicml/mpt-7b-instruct
repo_type: model
status: closed
target_branch: null
title: flash_attn on gpu
