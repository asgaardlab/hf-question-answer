!!python/object:huggingface_hub.community.DiscussionWithDetails
author: narenzen
conflicting_files: null
created_at: 2023-05-10 05:59:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/11401350689b77a10d2f4d72fc216e93.svg
      fullname: Naren Babu R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: narenzen
      type: user
    createdAt: '2023-05-10T06:59:58.000Z'
    data:
      edited: true
      editors:
      - narenzen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/11401350689b77a10d2f4d72fc216e93.svg
          fullname: Naren Babu R
          isHf: false
          isPro: false
          name: narenzen
          type: user
        html: "<p>I implemented Instruct model locally.<br>Here is a sample respose\
          \ I got:</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63ec8b7b60ff4b318aceaad4/kjoCnvB8sDAA2fJe-kwN7.png\"\
          ><img alt=\"Screenshot from 2023-05-10 12-29-24.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63ec8b7b60ff4b318aceaad4/kjoCnvB8sDAA2fJe-kwN7.png\"\
          ></a></p>\n<p>I get multiple # in the response</p>\n<p>Code:</p>\n<pre><code>def\
          \ __call__(\n        self, instruction: str, **generate_kwargs: Dict[str,\
          \ Any]\n    ) -&gt; Tuple[str, str, float]:\n        s = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction)\n\
          \        input_ids = self.tokenizer(s, return_tensors=\"pt\").input_ids\n\
          \        input_ids = input_ids.to(self.model.device)\n        with torch.no_grad():\n\
          \            output_ids = self.model.generate(input_ids, **generate_kwargs)\n\
          \        # Slice the output_ids tensor to get only new tokens\n        new_tokens\
          \ = output_ids[0, len(input_ids[0]) :]\n        output_text = self.tokenizer.decode(new_tokens,\
          \ skip_special_tokens=True)\n        return output_text\n\nself.generate_text\
          \ = InstructionTextGenerationPipeline(model=self.model, tokenizer=self.tokenizer)\n\
          \nresponse =  self.generate_text(prompt, **data)\n</code></pre>\n"
        raw: "I implemented Instruct model locally.\nHere is a sample respose I got:\n\
          \n![Screenshot from 2023-05-10 12-29-24.png](https://cdn-uploads.huggingface.co/production/uploads/63ec8b7b60ff4b318aceaad4/kjoCnvB8sDAA2fJe-kwN7.png)\n\
          \nI get multiple # in the response\n\nCode:\n```\ndef __call__(\n      \
          \  self, instruction: str, **generate_kwargs: Dict[str, Any]\n    ) -> Tuple[str,\
          \ str, float]:\n        s = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction)\n\
          \        input_ids = self.tokenizer(s, return_tensors=\"pt\").input_ids\n\
          \        input_ids = input_ids.to(self.model.device)\n        with torch.no_grad():\n\
          \            output_ids = self.model.generate(input_ids, **generate_kwargs)\n\
          \        # Slice the output_ids tensor to get only new tokens\n        new_tokens\
          \ = output_ids[0, len(input_ids[0]) :]\n        output_text = self.tokenizer.decode(new_tokens,\
          \ skip_special_tokens=True)\n        return output_text\n\nself.generate_text\
          \ = InstructionTextGenerationPipeline(model=self.model, tokenizer=self.tokenizer)\n\
          \nresponse =  self.generate_text(prompt, **data)\n```"
        updatedAt: '2023-05-10T07:05:23.563Z'
      numEdits: 4
      reactions: []
    id: 645b40ee78730bcc103dc2fc
    type: comment
  author: narenzen
  content: "I implemented Instruct model locally.\nHere is a sample respose I got:\n\
    \n![Screenshot from 2023-05-10 12-29-24.png](https://cdn-uploads.huggingface.co/production/uploads/63ec8b7b60ff4b318aceaad4/kjoCnvB8sDAA2fJe-kwN7.png)\n\
    \nI get multiple # in the response\n\nCode:\n```\ndef __call__(\n        self,\
    \ instruction: str, **generate_kwargs: Dict[str, Any]\n    ) -> Tuple[str, str,\
    \ float]:\n        s = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction)\n\
    \        input_ids = self.tokenizer(s, return_tensors=\"pt\").input_ids\n    \
    \    input_ids = input_ids.to(self.model.device)\n        with torch.no_grad():\n\
    \            output_ids = self.model.generate(input_ids, **generate_kwargs)\n\
    \        # Slice the output_ids tensor to get only new tokens\n        new_tokens\
    \ = output_ids[0, len(input_ids[0]) :]\n        output_text = self.tokenizer.decode(new_tokens,\
    \ skip_special_tokens=True)\n        return output_text\n\nself.generate_text\
    \ = InstructionTextGenerationPipeline(model=self.model, tokenizer=self.tokenizer)\n\
    \nresponse =  self.generate_text(prompt, **data)\n```"
  created_at: 2023-05-10 05:59:58+00:00
  edited: true
  hidden: false
  id: 645b40ee78730bcc103dc2fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-10T08:55:10.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: '<p>What generate kwargs are you using? Is this in bfloat16?</p>

          '
        raw: What generate kwargs are you using? Is this in bfloat16?
        updatedAt: '2023-05-10T08:55:10.639Z'
      numEdits: 0
      reactions: []
    id: 645b5beeba5b7032de80bb36
    type: comment
  author: sam-mosaic
  content: What generate kwargs are you using? Is this in bfloat16?
  created_at: 2023-05-10 07:55:10+00:00
  edited: false
  hidden: false
  id: 645b5beeba5b7032de80bb36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/11401350689b77a10d2f4d72fc216e93.svg
      fullname: Naren Babu R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: narenzen
      type: user
    createdAt: '2023-05-10T09:15:30.000Z'
    data:
      edited: true
      editors:
      - narenzen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/11401350689b77a10d2f4d72fc216e93.svg
          fullname: Naren Babu R
          isHf: false
          isPro: false
          name: narenzen
          type: user
        html: '<p>data ={"max_new_tokens": 100,<br>        "temperature": 0.1,<br>        "top_p":
          1,<br>        "use_cache": True,<br>        "top_k": 0<br>        }</p>

          '
        raw: "data ={\"max_new_tokens\": 100,\n        \"temperature\": 0.1,\n   \
          \     \"top_p\": 1,\n        \"use_cache\": True,\n        \"top_k\": 0\n\
          \        }"
        updatedAt: '2023-05-10T09:15:43.618Z'
      numEdits: 1
      reactions: []
    id: 645b60b2337b2ccf07f72da3
    type: comment
  author: narenzen
  content: "data ={\"max_new_tokens\": 100,\n        \"temperature\": 0.1,\n     \
    \   \"top_p\": 1,\n        \"use_cache\": True,\n        \"top_k\": 0\n      \
    \  }"
  created_at: 2023-05-10 08:15:30+00:00
  edited: true
  hidden: false
  id: 645b60b2337b2ccf07f72da3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-10T17:47:28.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: '<p>Could this be a dtype thing? Are you using a GPU that supports bf16,
          and if not, is this fp32 or fp16?</p>

          '
        raw: Could this be a dtype thing? Are you using a GPU that supports bf16,
          and if not, is this fp32 or fp16?
        updatedAt: '2023-05-10T17:47:28.947Z'
      numEdits: 0
      reactions: []
    id: 645bd8b0c971fbab74222fa9
    type: comment
  author: sam-mosaic
  content: Could this be a dtype thing? Are you using a GPU that supports bf16, and
    if not, is this fp32 or fp16?
  created_at: 2023-05-10 16:47:28+00:00
  edited: false
  hidden: false
  id: 645bd8b0c971fbab74222fa9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Qqzq8JhnPP_mmjcarlBqf.jpeg?w=200&h=200&f=face
      fullname: Logan Markewich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cheesyFishes
      type: user
    createdAt: '2023-05-19T20:47:01.000Z'
    data:
      edited: true
      editors:
      - cheesyFishes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Qqzq8JhnPP_mmjcarlBqf.jpeg?w=200&h=200&f=face
          fullname: Logan Markewich
          isHf: false
          isPro: false
          name: cheesyFishes
          type: user
        html: "<p>I am also getting this. It generates a good first response, but\
          \ then a ton of '#' characters</p>\n<p>My only idea so far is to add this\
          \ to the stopping_ids?</p>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(\"\
          EleutherAI/gpt-neox-20b\")\n\nconfig = AutoConfig.from_pretrained(\n  'mosaicml/mpt-7b-chat',\n\
          \  trust_remote_code=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \  'mosaicml/mpt-7b-chat',\n  config=config,\n  torch_dtype=torch.bfloat16,\n\
          \  trust_remote_code=True\n)\nmodel.to(device='cuda:0')\n\nmodel_inputs\
          \ = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\noutput_ids = model.generate(\n\
          \    **model_inputs,\n    max_new_tokens=512,\n)\noutput_text = tokenizer.batch_decode(output_ids,\
          \ skip_special_tokens=True)[0]\n</code></pre>\n"
        raw: "I am also getting this. It generates a good first response, but then\
          \ a ton of '#' characters\n\nMy only idea so far is to add this to the stopping_ids?\n\
          \n```\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\"\
          )\n\nconfig = AutoConfig.from_pretrained(\n  'mosaicml/mpt-7b-chat',\n \
          \ trust_remote_code=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \  'mosaicml/mpt-7b-chat',\n  config=config,\n  torch_dtype=torch.bfloat16,\n\
          \  trust_remote_code=True\n)\nmodel.to(device='cuda:0')\n\nmodel_inputs\
          \ = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\noutput_ids = model.generate(\n\
          \    **model_inputs,\n    max_new_tokens=512,\n)\noutput_text = tokenizer.batch_decode(output_ids,\
          \ skip_special_tokens=True)[0]\n```"
        updatedAt: '2023-05-19T20:47:42.072Z'
      numEdits: 1
      reactions: []
    id: 6467e0453a7c8dda230b63c0
    type: comment
  author: cheesyFishes
  content: "I am also getting this. It generates a good first response, but then a\
    \ ton of '#' characters\n\nMy only idea so far is to add this to the stopping_ids?\n\
    \n```\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n\
    \nconfig = AutoConfig.from_pretrained(\n  'mosaicml/mpt-7b-chat',\n  trust_remote_code=True\n\
    )\n\nmodel = AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b-chat',\n\
    \  config=config,\n  torch_dtype=torch.bfloat16,\n  trust_remote_code=True\n)\n\
    model.to(device='cuda:0')\n\nmodel_inputs = tokenizer(text, return_tensors=\"\
    pt\").to(\"cuda\")\noutput_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512,\n\
    )\noutput_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n\
    ```"
  created_at: 2023-05-19 19:47:01+00:00
  edited: true
  hidden: false
  id: 6467e0453a7c8dda230b63c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/614c14d789d9fe69398765c5/U4oDFoKJ0e6Uj56uWPf8m.jpeg?w=200&h=200&f=face
      fullname: Shreyas S K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: skshreyas714
      type: user
    createdAt: '2023-05-23T08:52:47.000Z'
    data:
      edited: false
      editors:
      - skshreyas714
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/614c14d789d9fe69398765c5/U4oDFoKJ0e6Uj56uWPf8m.jpeg?w=200&h=200&f=face
          fullname: Shreyas S K
          isHf: false
          isPro: false
          name: skshreyas714
          type: user
        html: '<p>You need to use stopping criteria as mentioned below. This would
          get rid of noisy responses. </p>

          <p>stop_token_ids = generate.tokenizer.convert_tokens_to_ids(["&lt;|endoftext|&gt;"])</p>

          <h1 id="define-a-custom-stopping-criteria">Define a custom stopping criteria</h1>

          <p>class StopOnTokens(StoppingCriteria):<br>    def <strong>call</strong>(self,
          input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -&gt;
          bool:<br>        for stop_id in stop_token_ids:<br>            if input_ids[0][-1]
          == stop_id:<br>                return True<br>        return False</p>

          '
        raw: "You need to use stopping criteria as mentioned below. This would get\
          \ rid of noisy responses. \n\nstop_token_ids = generate.tokenizer.convert_tokens_to_ids([\"\
          <|endoftext|>\"])\n\n\n# Define a custom stopping criteria\nclass StopOnTokens(StoppingCriteria):\n\
          \    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor,\
          \ **kwargs) -> bool:\n        for stop_id in stop_token_ids:\n         \
          \   if input_ids[0][-1] == stop_id:\n                return True\n     \
          \   return False"
        updatedAt: '2023-05-23T08:52:47.091Z'
      numEdits: 0
      reactions: []
    id: 646c7edfe30444638e0a9fc1
    type: comment
  author: skshreyas714
  content: "You need to use stopping criteria as mentioned below. This would get rid\
    \ of noisy responses. \n\nstop_token_ids = generate.tokenizer.convert_tokens_to_ids([\"\
    <|endoftext|>\"])\n\n\n# Define a custom stopping criteria\nclass StopOnTokens(StoppingCriteria):\n\
    \    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor,\
    \ **kwargs) -> bool:\n        for stop_id in stop_token_ids:\n            if input_ids[0][-1]\
    \ == stop_id:\n                return True\n        return False"
  created_at: 2023-05-23 07:52:47+00:00
  edited: false
  hidden: false
  id: 646c7edfe30444638e0a9fc1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b40d17aab9044d851ff9d46c0dd8eab2.svg
      fullname: Praveen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prvnsmpth
      type: user
    createdAt: '2023-05-27T05:37:03.000Z'
    data:
      edited: false
      editors:
      - prvnsmpth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b40d17aab9044d851ff9d46c0dd8eab2.svg
          fullname: Praveen
          isHf: false
          isPro: false
          name: prvnsmpth
          type: user
        html: "<p>I had the same problem, but with these generate kwargs I no longer\
          \ get the spurious tokens after the end-of-sequence token is generated:</p>\n\
          <pre><code>generate_params = {\n    \"max_new_tokens\": 512, \n    \"temperature\"\
          : 1.0, \n    \"top_p\": 1.0, \n    \"top_k\": 50, \n    \"use_cache\": True,\
          \ \n    \"do_sample\": True, \n    \"eos_token_id\": 0, \n    \"pad_token_id\"\
          : 0\n}\n</code></pre>\n<p>The important arg is the <code>eos_token_id</code>,\
          \ if you don't pass this, the token generation continues past the EOS token\
          \ and we get garbage tokens.</p>\n<p>For reference, this is what the full\
          \ script looks like (using mpt-7b-chat, but it's the same for the instruct\
          \ model, except for the input format):</p>\n<pre><code>import torch\nimport\
          \ transformers\nfrom transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n\
          \nmodel_name = 'mosaicml/mpt-7b-chat'\n\nconfig = AutoConfig.from_pretrained(\n\
          \  model_name,\n  trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(\n  model_name,\n  config=config,\n\
          \  torch_dtype=torch.bfloat16,\n  trust_remote_code=True\n)\n\nmodel.to(device='cuda:0')\n\
          model.eval() # Evaluation mode is default, but calling it anyway\n\nsystem_prompt\
          \ = '''&lt;|im_start|&gt; system\nYou are am AI assistant\n&lt;|im_end|&gt;\\\
          n\n'''\n\nuser_message = '''\nwhat is the meaning of life?\n'''\n\nfmt_user_message\
          \ = f'&lt;|im_start|&gt;user {user_message}&lt;|im_end|&gt;\\n' \ninput_ids\
          \ = tokenizer(system_prompt + fmt_user_message, return_tensors=\"pt\").input_ids\n\
          input_ids = input_ids.to(model.device)\n\ngenerate_params = {\n    \"max_new_tokens\"\
          : 512, \n    \"temperature\": 1.0, \n    \"top_p\": 1.0, \n    \"top_k\"\
          : 50, \n    \"use_cache\": True, \n    \"do_sample\": True, \n    \"eos_token_id\"\
          : 0, \n    \"pad_token_id\": 0\n}\n\ngenerated_ids = model.generate(input_ids,\
          \ **generate_params)\noutput = tokenizer.decode(generated_ids.cpu().tolist()[0],\
          \ skip_special_tokens=True)\n\nfor line in output.split('\\n'):\n    print(line)\n\
          </code></pre>\n"
        raw: "I had the same problem, but with these generate kwargs I no longer get\
          \ the spurious tokens after the end-of-sequence token is generated:\n```\n\
          generate_params = {\n    \"max_new_tokens\": 512, \n    \"temperature\"\
          : 1.0, \n    \"top_p\": 1.0, \n    \"top_k\": 50, \n    \"use_cache\": True,\
          \ \n    \"do_sample\": True, \n    \"eos_token_id\": 0, \n    \"pad_token_id\"\
          : 0\n}\n```\nThe important arg is the `eos_token_id`, if you don't pass\
          \ this, the token generation continues past the EOS token and we get garbage\
          \ tokens.\n\nFor reference, this is what the full script looks like (using\
          \ mpt-7b-chat, but it's the same for the instruct model, except for the\
          \ input format):\n```\nimport torch\nimport transformers\nfrom transformers\
          \ import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n\nmodel_name =\
          \ 'mosaicml/mpt-7b-chat'\n\nconfig = AutoConfig.from_pretrained(\n  model_name,\n\
          \  trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(\n  model_name,\n  config=config,\n\
          \  torch_dtype=torch.bfloat16,\n  trust_remote_code=True\n)\n\nmodel.to(device='cuda:0')\n\
          model.eval() # Evaluation mode is default, but calling it anyway\n\nsystem_prompt\
          \ = '''<|im_start|> system\nYou are am AI assistant\n<|im_end|>\\n\n'''\n\
          \nuser_message = '''\nwhat is the meaning of life?\n'''\n\nfmt_user_message\
          \ = f'<|im_start|>user {user_message}<|im_end|>\\n' \ninput_ids = tokenizer(system_prompt\
          \ + fmt_user_message, return_tensors=\"pt\").input_ids\ninput_ids = input_ids.to(model.device)\n\
          \ngenerate_params = {\n    \"max_new_tokens\": 512, \n    \"temperature\"\
          : 1.0, \n    \"top_p\": 1.0, \n    \"top_k\": 50, \n    \"use_cache\": True,\
          \ \n    \"do_sample\": True, \n    \"eos_token_id\": 0, \n    \"pad_token_id\"\
          : 0\n}\n\ngenerated_ids = model.generate(input_ids, **generate_params)\n\
          output = tokenizer.decode(generated_ids.cpu().tolist()[0], skip_special_tokens=True)\n\
          \nfor line in output.split('\\n'):\n    print(line)\n```"
        updatedAt: '2023-05-27T05:37:03.220Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ylhe96
    id: 647196ff1c2bfd5b7b04281f
    type: comment
  author: prvnsmpth
  content: "I had the same problem, but with these generate kwargs I no longer get\
    \ the spurious tokens after the end-of-sequence token is generated:\n```\ngenerate_params\
    \ = {\n    \"max_new_tokens\": 512, \n    \"temperature\": 1.0, \n    \"top_p\"\
    : 1.0, \n    \"top_k\": 50, \n    \"use_cache\": True, \n    \"do_sample\": True,\
    \ \n    \"eos_token_id\": 0, \n    \"pad_token_id\": 0\n}\n```\nThe important\
    \ arg is the `eos_token_id`, if you don't pass this, the token generation continues\
    \ past the EOS token and we get garbage tokens.\n\nFor reference, this is what\
    \ the full script looks like (using mpt-7b-chat, but it's the same for the instruct\
    \ model, except for the input format):\n```\nimport torch\nimport transformers\n\
    from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n\nmodel_name\
    \ = 'mosaicml/mpt-7b-chat'\n\nconfig = AutoConfig.from_pretrained(\n  model_name,\n\
    \  trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
    \nmodel = AutoModelForCausalLM.from_pretrained(\n  model_name,\n  config=config,\n\
    \  torch_dtype=torch.bfloat16,\n  trust_remote_code=True\n)\n\nmodel.to(device='cuda:0')\n\
    model.eval() # Evaluation mode is default, but calling it anyway\n\nsystem_prompt\
    \ = '''<|im_start|> system\nYou are am AI assistant\n<|im_end|>\\n\n'''\n\nuser_message\
    \ = '''\nwhat is the meaning of life?\n'''\n\nfmt_user_message = f'<|im_start|>user\
    \ {user_message}<|im_end|>\\n' \ninput_ids = tokenizer(system_prompt + fmt_user_message,\
    \ return_tensors=\"pt\").input_ids\ninput_ids = input_ids.to(model.device)\n\n\
    generate_params = {\n    \"max_new_tokens\": 512, \n    \"temperature\": 1.0,\
    \ \n    \"top_p\": 1.0, \n    \"top_k\": 50, \n    \"use_cache\": True, \n   \
    \ \"do_sample\": True, \n    \"eos_token_id\": 0, \n    \"pad_token_id\": 0\n\
    }\n\ngenerated_ids = model.generate(input_ids, **generate_params)\noutput = tokenizer.decode(generated_ids.cpu().tolist()[0],\
    \ skip_special_tokens=True)\n\nfor line in output.split('\\n'):\n    print(line)\n\
    ```"
  created_at: 2023-05-27 04:37:03+00:00
  edited: false
  hidden: false
  id: 647196ff1c2bfd5b7b04281f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-06-14T06:43:30.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8612844944000244
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: '<p>Using a stopping criteria and finding good generate kwargs are good
          ideas. Closing as complete</p>

          '
        raw: Using a stopping criteria and finding good generate kwargs are good ideas.
          Closing as complete
        updatedAt: '2023-06-14T06:43:30.777Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64896192099219a6979f8db6
    id: 64896192099219a6979f8dac
    type: comment
  author: sam-mosaic
  content: Using a stopping criteria and finding good generate kwargs are good ideas.
    Closing as complete
  created_at: 2023-06-14 05:43:30+00:00
  edited: false
  hidden: false
  id: 64896192099219a6979f8dac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-06-14T06:43:30.000Z'
    data:
      status: closed
    id: 64896192099219a6979f8db6
    type: status-change
  author: sam-mosaic
  created_at: 2023-06-14 05:43:30+00:00
  id: 64896192099219a6979f8db6
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: mosaicml/mpt-7b-instruct
repo_type: model
status: closed
target_branch: null
title: 'getting messy response (response includes #)'
