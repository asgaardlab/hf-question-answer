!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bradley6597
conflicting_files: null
created_at: 2023-05-10 10:44:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/38916b4fb3abc2112112107a345d20f3.svg
      fullname: 'Bradley '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bradley6597
      type: user
    createdAt: '2023-05-10T11:44:55.000Z'
    data:
      edited: true
      editors:
      - bradley6597
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/38916b4fb3abc2112112107a345d20f3.svg
          fullname: 'Bradley '
          isHf: false
          isPro: false
          name: bradley6597
          type: user
        html: "<p>Is there any way I can try to limit the number of hallucinations\
          \ away from a given context? </p>\n<p>Currently, it tries to answer outside\
          \ of the context if the information isn't in the context rather than saying\
          \ it can't answer. The prompt I am using is the same as the quick_pipeline.py\
          \ on your space.</p>\n<pre><code class=\"language-python\">INSTRUCTION_KEY\
          \ = <span class=\"hljs-string\">\"### Instruction:\"</span>\nRESPONSE_KEY\
          \ = <span class=\"hljs-string\">\"### Response:\"</span>\nEND_KEY = <span\
          \ class=\"hljs-string\">\"### End\"</span>\nINTRO_BLURB = <span class=\"\
          hljs-string\">\"Below is an instruction that describes a task. Write a response\
          \ that appropriately completes the request.\"</span>\ninstruction = prompt\
          \ + <span class=\"hljs-string\">'\\n'</span> + context_str\nprompt = <span\
          \ class=\"hljs-string\">\"\"\"{intro}</span>\n<span class=\"hljs-string\"\
          >{instruction_key}</span>\n<span class=\"hljs-string\">{instruction}</span>\n\
          <span class=\"hljs-string\">{response_key}</span>\n<span class=\"hljs-string\"\
          >\"\"\"</span>.<span class=\"hljs-built_in\">format</span>(\n    intro=INTRO_BLURB,\n\
          \    instruction_key=INSTRUCTION_KEY,\n    instruction=instruction,\n  \
          \  response_key=RESPONSE_KEY,\n)\n</code></pre>\n<p>Also my kwargs are:</p>\n\
          <pre><code class=\"language-python\">generate_kwargs = {\n            <span\
          \ class=\"hljs-string\">\"temperature\"</span>: <span class=\"hljs-number\"\
          >0.01</span>, <span class=\"hljs-comment\">#0.01</span>\n            <span\
          \ class=\"hljs-string\">\"top_p\"</span>: <span class=\"hljs-number\">0.92</span>,\
          \ <span class=\"hljs-comment\">#0.99</span>\n            <span class=\"\
          hljs-string\">\"top_k\"</span>: <span class=\"hljs-number\">3</span>, <span\
          \ class=\"hljs-comment\"># 3</span>\n            <span class=\"hljs-string\"\
          >\"max_new_tokens\"</span>: <span class=\"hljs-number\">512</span>,\n  \
          \          <span class=\"hljs-string\">\"use_cache\"</span>: <span class=\"\
          hljs-literal\">True</span>,\n            <span class=\"hljs-string\">\"\
          do_sample\"</span>: <span class=\"hljs-literal\">True</span>,\n        \
          \    <span class=\"hljs-string\">\"eos_token_id\"</span>: tokenizer.eos_token_id,\n\
          \            <span class=\"hljs-string\">\"pad_token_id\"</span>: tokenizer.pad_token_id,\n\
          \            <span class=\"hljs-string\">\"repetition_penalty\"</span>:\
          \ <span class=\"hljs-number\">1.1</span>,  <span class=\"hljs-comment\"\
          ># 1.0 means no penalty, &gt; 1.0 means penalty, 1.2 from CTRL paper</span>\n\
          \        }\n</code></pre>\n<p>Any direction would be really helpful!</p>\n"
        raw: "Is there any way I can try to limit the number of hallucinations away\
          \ from a given context? \n\nCurrently, it tries to answer outside of the\
          \ context if the information isn't in the context rather than saying it\
          \ can't answer. The prompt I am using is the same as the quick_pipeline.py\
          \ on your space.\n\n```python\nINSTRUCTION_KEY = \"### Instruction:\"\n\
          RESPONSE_KEY = \"### Response:\"\nEND_KEY = \"### End\"\nINTRO_BLURB = \"\
          Below is an instruction that describes a task. Write a response that appropriately\
          \ completes the request.\"\ninstruction = prompt + '\\n' + context_str\n\
          prompt = \"\"\"{intro}\n{instruction_key}\n{instruction}\n{response_key}\n\
          \"\"\".format(\n    intro=INTRO_BLURB,\n    instruction_key=INSTRUCTION_KEY,\n\
          \    instruction=instruction,\n    response_key=RESPONSE_KEY,\n)\n```\n\n\
          Also my kwargs are:\n\n```python\ngenerate_kwargs = {\n            \"temperature\"\
          : 0.01, #0.01\n            \"top_p\": 0.92, #0.99\n            \"top_k\"\
          : 3, # 3\n            \"max_new_tokens\": 512,\n            \"use_cache\"\
          : True,\n            \"do_sample\": True,\n            \"eos_token_id\"\
          : tokenizer.eos_token_id,\n            \"pad_token_id\": tokenizer.pad_token_id,\n\
          \            \"repetition_penalty\": 1.1,  # 1.0 means no penalty, > 1.0\
          \ means penalty, 1.2 from CTRL paper\n        }\n```\n\nAny direction would\
          \ be really helpful!"
        updatedAt: '2023-05-10T11:45:56.761Z'
      numEdits: 1
      reactions: []
    id: 645b83b74093d5c140e0c381
    type: comment
  author: bradley6597
  content: "Is there any way I can try to limit the number of hallucinations away\
    \ from a given context? \n\nCurrently, it tries to answer outside of the context\
    \ if the information isn't in the context rather than saying it can't answer.\
    \ The prompt I am using is the same as the quick_pipeline.py on your space.\n\n\
    ```python\nINSTRUCTION_KEY = \"### Instruction:\"\nRESPONSE_KEY = \"### Response:\"\
    \nEND_KEY = \"### End\"\nINTRO_BLURB = \"Below is an instruction that describes\
    \ a task. Write a response that appropriately completes the request.\"\ninstruction\
    \ = prompt + '\\n' + context_str\nprompt = \"\"\"{intro}\n{instruction_key}\n\
    {instruction}\n{response_key}\n\"\"\".format(\n    intro=INTRO_BLURB,\n    instruction_key=INSTRUCTION_KEY,\n\
    \    instruction=instruction,\n    response_key=RESPONSE_KEY,\n)\n```\n\nAlso\
    \ my kwargs are:\n\n```python\ngenerate_kwargs = {\n            \"temperature\"\
    : 0.01, #0.01\n            \"top_p\": 0.92, #0.99\n            \"top_k\": 3, #\
    \ 3\n            \"max_new_tokens\": 512,\n            \"use_cache\": True,\n\
    \            \"do_sample\": True,\n            \"eos_token_id\": tokenizer.eos_token_id,\n\
    \            \"pad_token_id\": tokenizer.pad_token_id,\n            \"repetition_penalty\"\
    : 1.1,  # 1.0 means no penalty, > 1.0 means penalty, 1.2 from CTRL paper\n   \
    \     }\n```\n\nAny direction would be really helpful!"
  created_at: 2023-05-10 10:44:55+00:00
  edited: true
  hidden: false
  id: 645b83b74093d5c140e0c381
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-10T16:55:07.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: '<p>You can try messing with <code>top_k</code> (3 is quite low) and
          <code>repetition_penalty</code>(1.1 is somewhat high)</p>

          <p>Otherwise you may want to fine-tune on whatever data you are working
          on? Without knowing the problem type it is hard to say more.</p>

          '
        raw: 'You can try messing with `top_k` (3 is quite low) and `repetition_penalty`(1.1
          is somewhat high)


          Otherwise you may want to fine-tune on whatever data you are working on?
          Without knowing the problem type it is hard to say more.'
        updatedAt: '2023-05-10T16:55:07.410Z'
      numEdits: 0
      reactions: []
      relatedEventId: 645bcc6bc971fbab7421be8f
    id: 645bcc6bc971fbab7421be8e
    type: comment
  author: sam-mosaic
  content: 'You can try messing with `top_k` (3 is quite low) and `repetition_penalty`(1.1
    is somewhat high)


    Otherwise you may want to fine-tune on whatever data you are working on? Without
    knowing the problem type it is hard to say more.'
  created_at: 2023-05-10 15:55:07+00:00
  edited: false
  hidden: false
  id: 645bcc6bc971fbab7421be8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-10T16:55:07.000Z'
    data:
      status: closed
    id: 645bcc6bc971fbab7421be8f
    type: status-change
  author: sam-mosaic
  created_at: 2023-05-10 15:55:07+00:00
  id: 645bcc6bc971fbab7421be8f
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1637512102357-noauth.jpeg?w=200&h=200&f=face
      fullname: Fathy Shalaby
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fathyshalaby
      type: user
    createdAt: '2023-05-11T08:15:05.000Z'
    data:
      edited: false
      editors:
      - fathyshalaby
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1637512102357-noauth.jpeg?w=200&h=200&f=face
          fullname: Fathy Shalaby
          isHf: false
          isPro: false
          name: fathyshalaby
          type: user
        html: "<p>I have the same problem, I\u2019m trying to build a QA bot using\
          \ this as the LLM component in the langchain was over docs pipeline.</p>\n"
        raw: "I have the same problem, I\u2019m trying to build a QA bot using this\
          \ as the LLM component in the langchain was over docs pipeline."
        updatedAt: '2023-05-11T08:15:05.584Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - fathyshalab
    id: 645ca409f1e3b219cb06ec48
    type: comment
  author: fathyshalaby
  content: "I have the same problem, I\u2019m trying to build a QA bot using this\
    \ as the LLM component in the langchain was over docs pipeline."
  created_at: 2023-05-11 07:15:05+00:00
  edited: false
  hidden: false
  id: 645ca409f1e3b219cb06ec48
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: mosaicml/mpt-7b-instruct
repo_type: model
status: closed
target_branch: null
title: Reduce hallucinations
