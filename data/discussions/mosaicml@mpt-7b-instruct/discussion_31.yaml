!!python/object:huggingface_hub.community.DiscussionWithDetails
author: skshreyas714
conflicting_files: null
created_at: 2023-05-23 07:58:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/614c14d789d9fe69398765c5/U4oDFoKJ0e6Uj56uWPf8m.jpeg?w=200&h=200&f=face
      fullname: Shreyas S K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: skshreyas714
      type: user
    createdAt: '2023-05-23T08:58:44.000Z'
    data:
      edited: false
      editors:
      - skshreyas714
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/614c14d789d9fe69398765c5/U4oDFoKJ0e6Uj56uWPf8m.jpeg?w=200&h=200&f=face
          fullname: Shreyas S K
          isHf: false
          isPro: false
          name: skshreyas714
          type: user
        html: '<p>I benchmarked this model for Sentiment Classification but the performance
          was very poor. So I want to finetune this model for a Multilingual Sentiment
          Classification dataset. Wanted to know the GPU requirements for finetuning
          it in FP16 mode.</p>

          '
        raw: I benchmarked this model for Sentiment Classification but the performance
          was very poor. So I want to finetune this model for a Multilingual Sentiment
          Classification dataset. Wanted to know the GPU requirements for finetuning
          it in FP16 mode.
        updatedAt: '2023-05-23T08:58:44.131Z'
      numEdits: 0
      reactions: []
    id: 646c804449a37754bdef2005
    type: comment
  author: skshreyas714
  content: I benchmarked this model for Sentiment Classification but the performance
    was very poor. So I want to finetune this model for a Multilingual Sentiment Classification
    dataset. Wanted to know the GPU requirements for finetuning it in FP16 mode.
  created_at: 2023-05-23 07:58:44+00:00
  edited: false
  hidden: false
  id: 646c804449a37754bdef2005
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e2b091eeddeed2bc1203e247334ccc0e.svg
      fullname: Daniel C
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: datacow
      type: user
    createdAt: '2023-05-26T19:40:37.000Z'
    data:
      edited: false
      editors:
      - datacow
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e2b091eeddeed2bc1203e247334ccc0e.svg
          fullname: Daniel C
          isHf: false
          isPro: false
          name: datacow
          type: user
        html: '<p><em>At least</em> 84 GB: (<a rel="nofollow" href="https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#how-many-gpus-do-i-need-to-train-a-llm">https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#how-many-gpus-do-i-need-to-train-a-llm</a>)</p>

          '
        raw: '*At least* 84 GB: (https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#how-many-gpus-do-i-need-to-train-a-llm)'
        updatedAt: '2023-05-26T19:40:37.742Z'
      numEdits: 0
      reactions: []
    id: 64710b351c2bfd5b7bf898d2
    type: comment
  author: datacow
  content: '*At least* 84 GB: (https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#how-many-gpus-do-i-need-to-train-a-llm)'
  created_at: 2023-05-26 18:40:37+00:00
  edited: false
  hidden: false
  id: 64710b351c2bfd5b7bf898d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-03T01:03:10.000Z'
    data:
      edited: false
      editors:
      - abhi-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.967681884765625
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: '<p>Closing as stale. As noted above, to finetune with FP32 weights,
          and FP32 LionW optimizer state, and FP32 gradients, it would take about
          7 * 4 * 3 = 84GB total memory.</p>

          '
        raw: Closing as stale. As noted above, to finetune with FP32 weights, and
          FP32 LionW optimizer state, and FP32 gradients, it would take about 7 *
          4 * 3 = 84GB total memory.
        updatedAt: '2023-06-03T01:03:10.940Z'
      numEdits: 0
      reactions: []
      relatedEventId: 647a914e822b7e8ccbde3463
    id: 647a914e822b7e8ccbde3462
    type: comment
  author: abhi-mosaic
  content: Closing as stale. As noted above, to finetune with FP32 weights, and FP32
    LionW optimizer state, and FP32 gradients, it would take about 7 * 4 * 3 = 84GB
    total memory.
  created_at: 2023-06-03 00:03:10+00:00
  edited: false
  hidden: false
  id: 647a914e822b7e8ccbde3462
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-03T01:03:10.000Z'
    data:
      status: closed
    id: 647a914e822b7e8ccbde3463
    type: status-change
  author: abhi-mosaic
  created_at: 2023-06-03 00:03:10+00:00
  id: 647a914e822b7e8ccbde3463
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 31
repo_id: mosaicml/mpt-7b-instruct
repo_type: model
status: closed
target_branch: null
title: How much GPU memory is needed to finetune MPT-7B Instruct model?
