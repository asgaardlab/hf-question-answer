!!python/object:huggingface_hub.community.DiscussionWithDetails
author: redraptor
conflicting_files: null
created_at: 2023-07-26 15:04:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8eb23e937a51f538074c2fc65f59b41.svg
      fullname: Bob Yar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: redraptor
      type: user
    createdAt: '2023-07-26T16:04:21.000Z'
    data:
      edited: true
      editors:
      - redraptor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.959043562412262
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8eb23e937a51f538074c2fc65f59b41.svg
          fullname: Bob Yar
          isHf: false
          isPro: false
          name: redraptor
          type: user
        html: '<p>I''ve attempted several methods including <a rel="nofollow" href="https://betterprogramming.pub/speed-up-llm-inference-83653aa24c47">https://betterprogramming.pub/speed-up-llm-inference-83653aa24c47</a>
          and <a href="https://huggingface.co/docs/optimum/bettertransformer/tutorials/convert">https://huggingface.co/docs/optimum/bettertransformer/tutorials/convert</a>
          but it seems like bettertransformer doesnt work with mpt-7b yet. So was
          wondering if anyone here has had success or additional suggestions on how
          to improve inference speed. Thanks</p>

          <p>name = ''mosaicml/mpt-7b-instruct''</p>

          <p>config = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)<br>config.init_device
          = ''cuda:6''<br>model_name = ''mosaicml/mpt-7b-instruct''<br>model = AutoModelForCausalLM.from_pretrained(<br>    model_name,<br>    config=config,<br>    trust_remote_code=True,<br>    torch_dtype=bfloat16,<br>    max_seq_len=512<br>)</p>

          <p>generate_text = transformers.pipeline(<br>    model=model,<br>    tokenizer=tokenizer,<br>    return_full_text=True,<br>    task=''text-generation'',<br>    use_fast
          = True,<br>    stopping_criteria=stopping_criteria,<br>    temperature=.5,<br>    top_p=0,<br>    top_k=0,<br>    max_new_tokens=1250,<br>    repetition_penalty=1.0,</p>

          <p>)</p>

          '
        raw: "I've attempted several methods including https://betterprogramming.pub/speed-up-llm-inference-83653aa24c47\
          \ and https://huggingface.co/docs/optimum/bettertransformer/tutorials/convert\
          \ but it seems like bettertransformer doesnt work with mpt-7b yet. So was\
          \ wondering if anyone here has had success or additional suggestions on\
          \ how to improve inference speed. Thanks\n\nname = 'mosaicml/mpt-7b-instruct'\n\
          \nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\n\
          config.init_device = 'cuda:6'\nmodel_name = 'mosaicml/mpt-7b-instruct'\n\
          model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    config=config,\n\
          \    trust_remote_code=True,\n    torch_dtype=bfloat16,\n    max_seq_len=512\n\
          )\n\ngenerate_text = transformers.pipeline(\n    model=model, \n    tokenizer=tokenizer,\n\
          \    return_full_text=True,\n    task='text-generation',\n    use_fast =\
          \ True,\n    stopping_criteria=stopping_criteria,\n    temperature=.5,\n\
          \    top_p=0,\n    top_k=0,\n    max_new_tokens=1250,\n    repetition_penalty=1.0,\n\
          \    \n)\n"
        updatedAt: '2023-07-26T16:06:46.586Z'
      numEdits: 1
      reactions: []
    id: 64c14405e98a5e02c9355126
    type: comment
  author: redraptor
  content: "I've attempted several methods including https://betterprogramming.pub/speed-up-llm-inference-83653aa24c47\
    \ and https://huggingface.co/docs/optimum/bettertransformer/tutorials/convert\
    \ but it seems like bettertransformer doesnt work with mpt-7b yet. So was wondering\
    \ if anyone here has had success or additional suggestions on how to improve inference\
    \ speed. Thanks\n\nname = 'mosaicml/mpt-7b-instruct'\n\nconfig = transformers.AutoConfig.from_pretrained(name,\
    \ trust_remote_code=True)\nconfig.init_device = 'cuda:6'\nmodel_name = 'mosaicml/mpt-7b-instruct'\n\
    model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    config=config,\n\
    \    trust_remote_code=True,\n    torch_dtype=bfloat16,\n    max_seq_len=512\n\
    )\n\ngenerate_text = transformers.pipeline(\n    model=model, \n    tokenizer=tokenizer,\n\
    \    return_full_text=True,\n    task='text-generation',\n    use_fast = True,\n\
    \    stopping_criteria=stopping_criteria,\n    temperature=.5,\n    top_p=0,\n\
    \    top_k=0,\n    max_new_tokens=1250,\n    repetition_penalty=1.0,\n    \n)\n"
  created_at: 2023-07-26 15:04:21+00:00
  edited: true
  hidden: false
  id: 64c14405e98a5e02c9355126
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-07-27T15:36:23.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7730068564414978
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Yeah, I'm facing the same issue. Generation rates in Google Colab\
          \ with a 15 GB GPU are only about 1 token per second. That's really terrible.\
          \ I'm using 4 bit quantisation, which means </p>\n<p>I think it may be partly\
          \ because of not using triton</p>\n<pre><code>config.attn_config['attn_impl']\
          \ = 'triton'\n</code></pre>\n<p>However, using triton fails when I try -\
          \ see <a href=\"https://huggingface.co/mosaicml/mpt-7b/discussions/79\"\
          >here</a>.</p>\n<p>BTW, here is the config that is giving me 1 tok/s:</p>\n\
          <pre><code># Load the model in 4-bit to allow it to fit in a free Google\
          \ Colab runtime with a CPU and T4 GPU\nbnb_config = BitsAndBytesConfig(\n\
          \    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"\
          nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nconfig = transformers.AutoConfig.from_pretrained(model_id,\
          \ trust_remote_code=True)\nconfig.init_device = 'cuda:0' # Unclear whether\
          \ this really helps a lot or interacts with device_map.\nconfig.max_seq_len\
          \ = 1024\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, config=config,\
          \ quantization_config=bnb_config, device_map='auto', trust_remote_code=True,\
          \ cache_dir=cache_dir) # for inference use 'auto', for training us device_map={\"\
          \":0}\n</code></pre>\n<p>On the other hand, back of the envelope is that\
          \ T4s have got 8 TFLOPS of compute, and we need 1,000 prompt tokens x 7B\
          \ params x2 (for multiplication + addition) x ~1/2 (for quantisation benefit)\
          \ = 7T floating point operations required per token of output. So maybe\
          \ 1 tok/s is about right? I'd be interested in whether Triton helps more\
          \ (quantisation down to 4 bit should give 4x improvement, not 2x like above).</p>\n"
        raw: "Yeah, I'm facing the same issue. Generation rates in Google Colab with\
          \ a 15 GB GPU are only about 1 token per second. That's really terrible.\
          \ I'm using 4 bit quantisation, which means \n\nI think it may be partly\
          \ because of not using triton\n```\nconfig.attn_config['attn_impl'] = 'triton'\n\
          ```\n\nHowever, using triton fails when I try - see [here](https://huggingface.co/mosaicml/mpt-7b/discussions/79).\n\
          \nBTW, here is the config that is giving me 1 tok/s:\n```\n# Load the model\
          \ in 4-bit to allow it to fit in a free Google Colab runtime with a CPU\
          \ and T4 GPU\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n\
          \    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n\
          \    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nconfig = transformers.AutoConfig.from_pretrained(model_id,\
          \ trust_remote_code=True)\nconfig.init_device = 'cuda:0' # Unclear whether\
          \ this really helps a lot or interacts with device_map.\nconfig.max_seq_len\
          \ = 1024\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, config=config,\
          \ quantization_config=bnb_config, device_map='auto', trust_remote_code=True,\
          \ cache_dir=cache_dir) # for inference use 'auto', for training us device_map={\"\
          \":0}\n```\n\nOn the other hand, back of the envelope is that T4s have got\
          \ 8 TFLOPS of compute, and we need 1,000 prompt tokens x 7B params x2 (for\
          \ multiplication + addition) x ~1/2 (for quantisation benefit) = 7T floating\
          \ point operations required per token of output. So maybe 1 tok/s is about\
          \ right? I'd be interested in whether Triton helps more (quantisation down\
          \ to 4 bit should give 4x improvement, not 2x like above)."
        updatedAt: '2023-07-27T15:36:23.242Z'
      numEdits: 0
      reactions: []
    id: 64c28ef78e10666f81d4fc0d
    type: comment
  author: RonanMcGovern
  content: "Yeah, I'm facing the same issue. Generation rates in Google Colab with\
    \ a 15 GB GPU are only about 1 token per second. That's really terrible. I'm using\
    \ 4 bit quantisation, which means \n\nI think it may be partly because of not\
    \ using triton\n```\nconfig.attn_config['attn_impl'] = 'triton'\n```\n\nHowever,\
    \ using triton fails when I try - see [here](https://huggingface.co/mosaicml/mpt-7b/discussions/79).\n\
    \nBTW, here is the config that is giving me 1 tok/s:\n```\n# Load the model in\
    \ 4-bit to allow it to fit in a free Google Colab runtime with a CPU and T4 GPU\n\
    bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n\
    \    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n\
    )\n\nconfig = transformers.AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n\
    config.init_device = 'cuda:0' # Unclear whether this really helps a lot or interacts\
    \ with device_map.\nconfig.max_seq_len = 1024\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\
    \ config=config, quantization_config=bnb_config, device_map='auto', trust_remote_code=True,\
    \ cache_dir=cache_dir) # for inference use 'auto', for training us device_map={\"\
    \":0}\n```\n\nOn the other hand, back of the envelope is that T4s have got 8 TFLOPS\
    \ of compute, and we need 1,000 prompt tokens x 7B params x2 (for multiplication\
    \ + addition) x ~1/2 (for quantisation benefit) = 7T floating point operations\
    \ required per token of output. So maybe 1 tok/s is about right? I'd be interested\
    \ in whether Triton helps more (quantisation down to 4 bit should give 4x improvement,\
    \ not 2x like above)."
  created_at: 2023-07-27 14:36:23+00:00
  edited: false
  hidden: false
  id: 64c28ef78e10666f81d4fc0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8eb23e937a51f538074c2fc65f59b41.svg
      fullname: Bob Yar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: redraptor
      type: user
    createdAt: '2023-07-29T03:36:02.000Z'
    data:
      edited: false
      editors:
      - redraptor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6381348371505737
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8eb23e937a51f538074c2fc65f59b41.svg
          fullname: Bob Yar
          isHf: false
          isPro: false
          name: redraptor
          type: user
        html: "<blockquote>\n<p>Yeah, I'm facing the same issue. Generation rates\
          \ in Google Colab with a 15 GB GPU are only about 1 token per second. That's\
          \ really terrible. I'm using 4 bit quantisation, which means </p>\n<p>I\
          \ think it may be partly because of not using triton</p>\n<pre><code>config.attn_config['attn_impl']\
          \ = 'triton'\n</code></pre>\n<p>However, using triton fails when I try -\
          \ see <a href=\"https://huggingface.co/mosaicml/mpt-7b/discussions/79\"\
          >here</a>.</p>\n<p>BTW, here is the config that is giving me 1 tok/s:</p>\n\
          <pre><code># Load the model in 4-bit to allow it to fit in a free Google\
          \ Colab runtime with a CPU and T4 GPU\nbnb_config = BitsAndBytesConfig(\n\
          \    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"\
          nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nconfig = transformers.AutoConfig.from_pretrained(model_id,\
          \ trust_remote_code=True)\nconfig.init_device = 'cuda:0' # Unclear whether\
          \ this really helps a lot or interacts with device_map.\nconfig.max_seq_len\
          \ = 1024\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, config=config,\
          \ quantization_config=bnb_config, device_map='auto', trust_remote_code=True,\
          \ cache_dir=cache_dir) # for inference use 'auto', for training us device_map={\"\
          \":0}\n</code></pre>\n<p>On the other hand, back of the envelope is that\
          \ T4s have got 8 TFLOPS of compute, and we need 1,000 prompt tokens x 7B\
          \ params x2 (for multiplication + addition) x ~1/2 (for quantisation benefit)\
          \ = 7T floating point operations required per token of output. So maybe\
          \ 1 tok/s is about right? I'd be interested in whether Triton helps more\
          \ (quantisation down to 4 bit should give 4x improvement, not 2x like above).</p>\n\
          </blockquote>\n<p>Do you know why my model goes crazy after using config.attn_config['attn_impl']\
          \ = 'triton'? My output is \uFFFD\uFFFD\uFFFD exceeds\u0435\u043B\u044C\u0435\
          \u043B\u044C\u0438\u0442\u0435\u043B\u0435\u043B\u044C\u0438\u0442\u0435\
          \u043B\u3057\u3066\u3044\u308B\u3057\u3066\u3044\u308B\u6027\u3057\u3066\
          \u3044\u308B\u3057\u3066\u3044\u308B\u3057\u3066\u3044\u308B\u3057\u3066\
          \u3044\u308B\xE2\u3057\u3066\u3044\u308B\u3057\u3066\u3044\u308B\u6027</p>\n"
        raw: "> Yeah, I'm facing the same issue. Generation rates in Google Colab\
          \ with a 15 GB GPU are only about 1 token per second. That's really terrible.\
          \ I'm using 4 bit quantisation, which means \n> \n> I think it may be partly\
          \ because of not using triton\n> ```\n> config.attn_config['attn_impl']\
          \ = 'triton'\n> ```\n> \n> However, using triton fails when I try - see\
          \ [here](https://huggingface.co/mosaicml/mpt-7b/discussions/79).\n> \n>\
          \ BTW, here is the config that is giving me 1 tok/s:\n> ```\n> # Load the\
          \ model in 4-bit to allow it to fit in a free Google Colab runtime with\
          \ a CPU and T4 GPU\n> bnb_config = BitsAndBytesConfig(\n>     load_in_4bit=True,\n\
          >     bnb_4bit_use_double_quant=True,\n>     bnb_4bit_quant_type=\"nf4\"\
          ,\n>     bnb_4bit_compute_dtype=torch.bfloat16\n> )\n> \n> config = transformers.AutoConfig.from_pretrained(model_id,\
          \ trust_remote_code=True)\n> config.init_device = 'cuda:0' # Unclear whether\
          \ this really helps a lot or interacts with device_map.\n> config.max_seq_len\
          \ = 1024\n> \n> model = AutoModelForCausalLM.from_pretrained(model_id, config=config,\
          \ quantization_config=bnb_config, device_map='auto', trust_remote_code=True,\
          \ cache_dir=cache_dir) # for inference use 'auto', for training us device_map={\"\
          \":0}\n> ```\n> \n> On the other hand, back of the envelope is that T4s\
          \ have got 8 TFLOPS of compute, and we need 1,000 prompt tokens x 7B params\
          \ x2 (for multiplication + addition) x ~1/2 (for quantisation benefit) =\
          \ 7T floating point operations required per token of output. So maybe 1\
          \ tok/s is about right? I'd be interested in whether Triton helps more (quantisation\
          \ down to 4 bit should give 4x improvement, not 2x like above).\n\nDo you\
          \ know why my model goes crazy after using config.attn_config['attn_impl']\
          \ = 'triton'? My output is \uFFFD\uFFFD\uFFFD exceeds\u0435\u043B\u044C\u0435\
          \u043B\u044C\u0438\u0442\u0435\u043B\u0435\u043B\u044C\u0438\u0442\u0435\
          \u043B\u3057\u3066\u3044\u308B\u3057\u3066\u3044\u308B\u6027\u3057\u3066\
          \u3044\u308B\u3057\u3066\u3044\u308B\u3057\u3066\u3044\u308B\u3057\u3066\
          \u3044\u308B\xE2\u3057\u3066\u3044\u308B\u3057\u3066\u3044\u308B\u6027\n"
        updatedAt: '2023-07-29T03:36:02.244Z'
      numEdits: 0
      reactions: []
    id: 64c4892257e5b2cd8acde8a6
    type: comment
  author: redraptor
  content: "> Yeah, I'm facing the same issue. Generation rates in Google Colab with\
    \ a 15 GB GPU are only about 1 token per second. That's really terrible. I'm using\
    \ 4 bit quantisation, which means \n> \n> I think it may be partly because of\
    \ not using triton\n> ```\n> config.attn_config['attn_impl'] = 'triton'\n> ```\n\
    > \n> However, using triton fails when I try - see [here](https://huggingface.co/mosaicml/mpt-7b/discussions/79).\n\
    > \n> BTW, here is the config that is giving me 1 tok/s:\n> ```\n> # Load the\
    \ model in 4-bit to allow it to fit in a free Google Colab runtime with a CPU\
    \ and T4 GPU\n> bnb_config = BitsAndBytesConfig(\n>     load_in_4bit=True,\n>\
    \     bnb_4bit_use_double_quant=True,\n>     bnb_4bit_quant_type=\"nf4\",\n> \
    \    bnb_4bit_compute_dtype=torch.bfloat16\n> )\n> \n> config = transformers.AutoConfig.from_pretrained(model_id,\
    \ trust_remote_code=True)\n> config.init_device = 'cuda:0' # Unclear whether this\
    \ really helps a lot or interacts with device_map.\n> config.max_seq_len = 1024\n\
    > \n> model = AutoModelForCausalLM.from_pretrained(model_id, config=config, quantization_config=bnb_config,\
    \ device_map='auto', trust_remote_code=True, cache_dir=cache_dir) # for inference\
    \ use 'auto', for training us device_map={\"\":0}\n> ```\n> \n> On the other hand,\
    \ back of the envelope is that T4s have got 8 TFLOPS of compute, and we need 1,000\
    \ prompt tokens x 7B params x2 (for multiplication + addition) x ~1/2 (for quantisation\
    \ benefit) = 7T floating point operations required per token of output. So maybe\
    \ 1 tok/s is about right? I'd be interested in whether Triton helps more (quantisation\
    \ down to 4 bit should give 4x improvement, not 2x like above).\n\nDo you know\
    \ why my model goes crazy after using config.attn_config['attn_impl'] = 'triton'?\
    \ My output is \uFFFD\uFFFD\uFFFD exceeds\u0435\u043B\u044C\u0435\u043B\u044C\u0438\
    \u0442\u0435\u043B\u0435\u043B\u044C\u0438\u0442\u0435\u043B\u3057\u3066\u3044\
    \u308B\u3057\u3066\u3044\u308B\u6027\u3057\u3066\u3044\u308B\u3057\u3066\u3044\
    \u308B\u3057\u3066\u3044\u308B\u3057\u3066\u3044\u308B\xE2\u3057\u3066\u3044\u308B\
    \u3057\u3066\u3044\u308B\u6027\n"
  created_at: 2023-07-29 02:36:02+00:00
  edited: false
  hidden: false
  id: 64c4892257e5b2cd8acde8a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-07-30T23:15:54.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7520815134048462
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sam-mosaic&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sam-mosaic\">@<span class=\"\
          underline\">sam-mosaic</span></a></span>\n\n\t</span></span> any tips here?\
          \ Appreciate it, Ronan</p>\n"
        raw: '@sam-mosaic any tips here? Appreciate it, Ronan'
        updatedAt: '2023-07-30T23:15:54.002Z'
      numEdits: 0
      reactions: []
    id: 64c6ef2a2eb0c99c50698dfd
    type: comment
  author: RonanMcGovern
  content: '@sam-mosaic any tips here? Appreciate it, Ronan'
  created_at: 2023-07-30 22:15:54+00:00
  edited: false
  hidden: false
  id: 64c6ef2a2eb0c99c50698dfd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 67
repo_id: mosaicml/mpt-7b-instruct
repo_type: model
status: open
target_branch: null
title: How to improve inference runtime performance?
