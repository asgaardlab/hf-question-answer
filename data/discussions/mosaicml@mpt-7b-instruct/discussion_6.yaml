!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kil3r
conflicting_files: null
created_at: 2023-05-06 18:39:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669642878663-62d13bb5b473530ce564631b.png?w=200&h=200&f=face
      fullname: Mariusz Woloszyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kil3r
      type: user
    createdAt: '2023-05-06T19:39:02.000Z'
    data:
      edited: false
      editors:
      - kil3r
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669642878663-62d13bb5b473530ce564631b.png?w=200&h=200&f=face
          fullname: Mariusz Woloszyn
          isHf: false
          isPro: false
          name: kil3r
          type: user
        html: "<p>Loading the model with the following code (consecutive time):</p>\n\
          <pre><code>generate = InstructionTextGenerationPipeline(\n    \"mosaicml/mpt-7b-instruct\"\
          ,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n)\n</code></pre>\n\
          <p>takes way, way more time than loading other, even bigger models (like\
          \ <code>databricks/dolly-v2-12b</code>) on the same hardware.<br>I observe\
          \ that one thread is spinning 100% of single core for couple of minutes.</p>\n"
        raw: "Loading the model with the following code (consecutive time):\r\n```\r\
          \ngenerate = InstructionTextGenerationPipeline(\r\n    \"mosaicml/mpt-7b-instruct\"\
          ,\r\n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n\
          )\r\n```\r\ntakes way, way more time than loading other, even bigger models\
          \ (like `databricks/dolly-v2-12b`) on the same hardware.\r\nI observe that\
          \ one thread is spinning 100% of single core for couple of minutes."
        updatedAt: '2023-05-06T19:39:02.097Z'
      numEdits: 0
      reactions: []
    id: 6456acd6aaaf85a98fad787d
    type: comment
  author: kil3r
  content: "Loading the model with the following code (consecutive time):\r\n```\r\
    \ngenerate = InstructionTextGenerationPipeline(\r\n    \"mosaicml/mpt-7b-instruct\"\
    ,\r\n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n)\r\n```\r\
    \ntakes way, way more time than loading other, even bigger models (like `databricks/dolly-v2-12b`)\
    \ on the same hardware.\r\nI observe that one thread is spinning 100% of single\
    \ core for couple of minutes."
  created_at: 2023-05-06 18:39:02+00:00
  edited: false
  hidden: false
  id: 6456acd6aaaf85a98fad787d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b347df0380454ffbfdb6f2405c92f098.svg
      fullname: plycy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: plycy
      type: user
    createdAt: '2023-05-06T22:22:39.000Z'
    data:
      edited: false
      editors:
      - plycy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b347df0380454ffbfdb6f2405c92f098.svg
          fullname: plycy
          isHf: false
          isPro: false
          name: plycy
          type: user
        html: '<p>can confirm this</p>

          '
        raw: can confirm this
        updatedAt: '2023-05-06T22:22:39.703Z'
      numEdits: 0
      reactions: []
    id: 6456d32fcd6567f52fbbe4cd
    type: comment
  author: plycy
  content: can confirm this
  created_at: 2023-05-06 21:22:39+00:00
  edited: false
  hidden: false
  id: 6456d32fcd6567f52fbbe4cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/11e8bf0c352f64c3e1e93c1f02c805c2.svg
      fullname: Emma Tew
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: EmmaTew
      type: user
    createdAt: '2023-05-07T22:51:17.000Z'
    data:
      edited: false
      editors:
      - EmmaTew
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/11e8bf0c352f64c3e1e93c1f02c805c2.svg
          fullname: Emma Tew
          isHf: false
          isPro: false
          name: EmmaTew
          type: user
        html: '<p>You can try setting in config.json to <code>"use_cache": true</code>
          that''s worked for me in other models...</p>

          <p>I haven''t tried this model yet. ymmv etc.</p>

          '
        raw: 'You can try setting in config.json to `"use_cache": true` that''s worked
          for me in other models...


          I haven''t tried this model yet. ymmv etc.'
        updatedAt: '2023-05-07T22:51:17.658Z'
      numEdits: 0
      reactions: []
    id: 64582b655fc3b8a21ea7e6ab
    type: comment
  author: EmmaTew
  content: 'You can try setting in config.json to `"use_cache": true` that''s worked
    for me in other models...


    I haven''t tried this model yet. ymmv etc.'
  created_at: 2023-05-07 21:51:17+00:00
  edited: false
  hidden: false
  id: 64582b655fc3b8a21ea7e6ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76304140d99cd194d2cd0df4af474322.svg
      fullname: Movses Movsesyan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Chillarmo
      type: user
    createdAt: '2023-05-08T21:00:51.000Z'
    data:
      edited: false
      editors:
      - Chillarmo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76304140d99cd194d2cd0df4af474322.svg
          fullname: Movses Movsesyan
          isHf: false
          isPro: false
          name: Chillarmo
          type: user
        html: '<p>Yup </p>

          <blockquote>

          <p>takes way, way more time than loading other, even bigger models (like
          <code>databricks/dolly-v2-12b</code>) on the same hardware.</p>

          </blockquote>

          '
        raw: "Yup \n> takes way, way more time than loading other, even bigger models\
          \ (like `databricks/dolly-v2-12b`) on the same hardware."
        updatedAt: '2023-05-08T21:00:51.594Z'
      numEdits: 0
      reactions: []
    id: 64596303232e5f0712bab74d
    type: comment
  author: Chillarmo
  content: "Yup \n> takes way, way more time than loading other, even bigger models\
    \ (like `databricks/dolly-v2-12b`) on the same hardware."
  created_at: 2023-05-08 20:00:51+00:00
  edited: false
  hidden: false
  id: 64596303232e5f0712bab74d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669642878663-62d13bb5b473530ce564631b.png?w=200&h=200&f=face
      fullname: Mariusz Woloszyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kil3r
      type: user
    createdAt: '2023-05-08T21:17:15.000Z'
    data:
      edited: false
      editors:
      - kil3r
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669642878663-62d13bb5b473530ce564631b.png?w=200&h=200&f=face
          fullname: Mariusz Woloszyn
          isHf: false
          isPro: false
          name: kil3r
          type: user
        html: '<p>It seems to be the casting as loading in 32bit is way faster. Apparently
          the convertion happens in just one thread and very inefficiently allocates
          the memory. </p>

          <p>Both fp16 and bfp16 are equally affected.</p>

          '
        raw: "It seems to be the casting as loading in 32bit is way faster. Apparently\
          \ the convertion happens in just one thread and very inefficiently allocates\
          \ the memory. \n\nBoth fp16 and bfp16 are equally affected."
        updatedAt: '2023-05-08T21:17:15.213Z'
      numEdits: 0
      reactions: []
    id: 645966dbc5d0d57ba4271bba
    type: comment
  author: kil3r
  content: "It seems to be the casting as loading in 32bit is way faster. Apparently\
    \ the convertion happens in just one thread and very inefficiently allocates the\
    \ memory. \n\nBoth fp16 and bfp16 are equally affected."
  created_at: 2023-05-08 20:17:15+00:00
  edited: false
  hidden: false
  id: 645966dbc5d0d57ba4271bba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-11T23:30:40.000Z'
    data:
      edited: false
      editors:
      - abhi-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: '<p>Hi, could someone whose system is seeing a slowdown confirm if the
          issue affects other HF models when using <code>torch_dtype: bfloat16 or
          float16</code>? Basically I would like to know if this is correct:</p>

          <ul>

          <li>MPT-7B, torch_dtype=torch.float32 (fast)</li>

          <li>MPT-7B, torch_dtype=torch.bfloat16 (slow)</li>

          <li>OPT-6.7B, torch_dtype=torch.float32 (fast)</li>

          <li>OPT-6.7B, torch_dtype=torch.float16 (slow)</li>

          </ul>

          <p>if this is true then I will chalk it up to how torch / HF handle the
          dtype conversion, as we aren''t doing anything special with MPT other than
          saving the weights in BF16 (similar to OPT saving the weights in FP16).</p>

          '
        raw: 'Hi, could someone whose system is seeing a slowdown confirm if the issue
          affects other HF models when using `torch_dtype: bfloat16 or float16`? Basically
          I would like to know if this is correct:


          * MPT-7B, torch_dtype=torch.float32 (fast)

          * MPT-7B, torch_dtype=torch.bfloat16 (slow)

          * OPT-6.7B, torch_dtype=torch.float32 (fast)

          * OPT-6.7B, torch_dtype=torch.float16 (slow)


          if this is true then I will chalk it up to how torch / HF handle the dtype
          conversion, as we aren''t doing anything special with MPT other than saving
          the weights in BF16 (similar to OPT saving the weights in FP16).'
        updatedAt: '2023-05-11T23:30:40.271Z'
      numEdits: 0
      reactions: []
    id: 645d7aa04435a8ae3fc9deee
    type: comment
  author: abhi-mosaic
  content: 'Hi, could someone whose system is seeing a slowdown confirm if the issue
    affects other HF models when using `torch_dtype: bfloat16 or float16`? Basically
    I would like to know if this is correct:


    * MPT-7B, torch_dtype=torch.float32 (fast)

    * MPT-7B, torch_dtype=torch.bfloat16 (slow)

    * OPT-6.7B, torch_dtype=torch.float32 (fast)

    * OPT-6.7B, torch_dtype=torch.float16 (slow)


    if this is true then I will chalk it up to how torch / HF handle the dtype conversion,
    as we aren''t doing anything special with MPT other than saving the weights in
    BF16 (similar to OPT saving the weights in FP16).'
  created_at: 2023-05-11 22:30:40+00:00
  edited: false
  hidden: false
  id: 645d7aa04435a8ae3fc9deee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669642878663-62d13bb5b473530ce564631b.png?w=200&h=200&f=face
      fullname: Mariusz Woloszyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kil3r
      type: user
    createdAt: '2023-05-12T09:29:23.000Z'
    data:
      edited: true
      editors:
      - kil3r
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669642878663-62d13bb5b473530ce564631b.png?w=200&h=200&f=face
          fullname: Mariusz Woloszyn
          isHf: false
          isPro: false
          name: kil3r
          type: user
        html: "<blockquote>\n<p>if this is true then I will chalk it up to how torch\
          \ / HF handle the dtype conversion, as we aren't doing anything special\
          \ with MPT other than saving the weights in BF16 (similar to OPT saving\
          \ the weights in FP16).</p>\n</blockquote>\n<p>This is actual execution\
          \ order. I made sure the models are downloaded beforehand.</p>\n<pre><code>def\
          \ benchmark(model, drype):\n    model = AutoModelForCausalLM.from_pretrained(\n\
          \        model,\n        torch_dtype=drype,\n        trust_remote_code=True,\n\
          \        use_auth_token=False,\n    )\n\n%%time\nbenchmark(\"facebook/opt-6.7b\"\
          , torch.float32)\n\nCPU times: user 55.7 s, sys: 31.4 s, total: 1min 27s\n\
          Wall time: 1min\n\n%%time\nbenchmark(\"facebook/opt-6.7b\", torch.bfloat16)\n\
          \nCPU times: user 1min 6s, sys: 17.9 s, total: 1min 24s\nWall time: 1min\
          \ 8s\n\n###\n\n%%time\nbenchmark(\"mosaicml/mpt-7b-instruct\", torch.float32)\n\
          \nCPU times: user 1min 54s, sys: 25 s, total: 2min 19s\nWall time: 1min\
          \ 31s\n\n%%time\nbenchmark(\"mosaicml/mpt-7b-instruct\", torch.bfloat16)\n\
          \nCPU times: user 4min 13s, sys: 26.8 s, total: 4min 40s\nWall time: 3min\
          \ 50s\n</code></pre>\n"
        raw: "> if this is true then I will chalk it up to how torch / HF handle the\
          \ dtype conversion, as we aren't doing anything special with MPT other than\
          \ saving the weights in BF16 (similar to OPT saving the weights in FP16).\n\
          \nThis is actual execution order. I made sure the models are downloaded\
          \ beforehand.\n\n```\ndef benchmark(model, drype):\n    model = AutoModelForCausalLM.from_pretrained(\n\
          \        model,\n        torch_dtype=drype,\n        trust_remote_code=True,\n\
          \        use_auth_token=False,\n    )\n\n%%time\nbenchmark(\"facebook/opt-6.7b\"\
          , torch.float32)\n\nCPU times: user 55.7 s, sys: 31.4 s, total: 1min 27s\n\
          Wall time: 1min\n\n%%time\nbenchmark(\"facebook/opt-6.7b\", torch.bfloat16)\n\
          \nCPU times: user 1min 6s, sys: 17.9 s, total: 1min 24s\nWall time: 1min\
          \ 8s\n\n###\n\n%%time\nbenchmark(\"mosaicml/mpt-7b-instruct\", torch.float32)\n\
          \nCPU times: user 1min 54s, sys: 25 s, total: 2min 19s\nWall time: 1min\
          \ 31s\n\n%%time\nbenchmark(\"mosaicml/mpt-7b-instruct\", torch.bfloat16)\n\
          \nCPU times: user 4min 13s, sys: 26.8 s, total: 4min 40s\nWall time: 3min\
          \ 50s\n```"
        updatedAt: '2023-05-16T20:53:01.809Z'
      numEdits: 3
      reactions: []
    id: 645e06f3329368042025438b
    type: comment
  author: kil3r
  content: "> if this is true then I will chalk it up to how torch / HF handle the\
    \ dtype conversion, as we aren't doing anything special with MPT other than saving\
    \ the weights in BF16 (similar to OPT saving the weights in FP16).\n\nThis is\
    \ actual execution order. I made sure the models are downloaded beforehand.\n\n\
    ```\ndef benchmark(model, drype):\n    model = AutoModelForCausalLM.from_pretrained(\n\
    \        model,\n        torch_dtype=drype,\n        trust_remote_code=True,\n\
    \        use_auth_token=False,\n    )\n\n%%time\nbenchmark(\"facebook/opt-6.7b\"\
    , torch.float32)\n\nCPU times: user 55.7 s, sys: 31.4 s, total: 1min 27s\nWall\
    \ time: 1min\n\n%%time\nbenchmark(\"facebook/opt-6.7b\", torch.bfloat16)\n\nCPU\
    \ times: user 1min 6s, sys: 17.9 s, total: 1min 24s\nWall time: 1min 8s\n\n###\n\
    \n%%time\nbenchmark(\"mosaicml/mpt-7b-instruct\", torch.float32)\n\nCPU times:\
    \ user 1min 54s, sys: 25 s, total: 2min 19s\nWall time: 1min 31s\n\n%%time\nbenchmark(\"\
    mosaicml/mpt-7b-instruct\", torch.bfloat16)\n\nCPU times: user 4min 13s, sys:\
    \ 26.8 s, total: 4min 40s\nWall time: 3min 50s\n```"
  created_at: 2023-05-12 08:29:23+00:00
  edited: true
  hidden: false
  id: 645e06f3329368042025438b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669642878663-62d13bb5b473530ce564631b.png?w=200&h=200&f=face
      fullname: Mariusz Woloszyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kil3r
      type: user
    createdAt: '2023-05-26T11:51:55.000Z'
    data:
      edited: false
      editors:
      - kil3r
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669642878663-62d13bb5b473530ce564631b.png?w=200&h=200&f=face
          fullname: Mariusz Woloszyn
          isHf: false
          isPro: false
          name: kil3r
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;abhi-mosaic&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/abhi-mosaic\"\
          >@<span class=\"underline\">abhi-mosaic</span></a></span>\n\n\t</span></span>\
          \ Are you looking into that? It takes almost 3 times more time to load mpt-7b\
          \ in bfloat16 and almost 4 times more than opt-6.7b.</p>\n"
        raw: '@abhi-mosaic Are you looking into that? It takes almost 3 times more
          time to load mpt-7b in bfloat16 and almost 4 times more than opt-6.7b.'
        updatedAt: '2023-05-26T11:51:55.855Z'
      numEdits: 0
      reactions: []
    id: 64709d5b806c7d87fa15d7ca
    type: comment
  author: kil3r
  content: '@abhi-mosaic Are you looking into that? It takes almost 3 times more time
    to load mpt-7b in bfloat16 and almost 4 times more than opt-6.7b.'
  created_at: 2023-05-26 10:51:55+00:00
  edited: false
  hidden: false
  id: 64709d5b806c7d87fa15d7ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45ddcc8ce32c0fcc49a2f5c68ca922d8.svg
      fullname: KN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kxyne
      type: user
    createdAt: '2023-05-26T15:13:20.000Z'
    data:
      edited: false
      editors:
      - kxyne
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45ddcc8ce32c0fcc49a2f5c68ca922d8.svg
          fullname: KN
          isHf: false
          isPro: false
          name: kxyne
          type: user
        html: '<p>The fix for me was setting low_cpu_mem_usage=True, this would normally
          be the default is device auto mapping was turned on. It took the load time
          from 3 minutes to 6 seconds.</p>

          '
        raw: The fix for me was setting low_cpu_mem_usage=True, this would normally
          be the default is device auto mapping was turned on. It took the load time
          from 3 minutes to 6 seconds.
        updatedAt: '2023-05-26T15:13:20.914Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - kil3r
    id: 6470cc903df93fddece4a75b
    type: comment
  author: kxyne
  content: The fix for me was setting low_cpu_mem_usage=True, this would normally
    be the default is device auto mapping was turned on. It took the load time from
    3 minutes to 6 seconds.
  created_at: 2023-05-26 14:13:20+00:00
  edited: false
  hidden: false
  id: 6470cc903df93fddece4a75b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669642878663-62d13bb5b473530ce564631b.png?w=200&h=200&f=face
      fullname: Mariusz Woloszyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kil3r
      type: user
    createdAt: '2023-05-26T16:31:05.000Z'
    data:
      edited: true
      editors:
      - kil3r
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669642878663-62d13bb5b473530ce564631b.png?w=200&h=200&f=face
          fullname: Mariusz Woloszyn
          isHf: false
          isPro: false
          name: kil3r
          type: user
        html: '<blockquote>

          <p>The fix for me was setting low_cpu_mem_usage=True, this would normally
          be the default is device auto mapping was turned on. It took the load time
          from 3 minutes to 6 seconds.</p>

          </blockquote>

          <p>It does the trick for practical purposes (now model loads in mere seconds)
          but the mystery remains unsolved :)</p>

          '
        raw: '> The fix for me was setting low_cpu_mem_usage=True, this would normally
          be the default is device auto mapping was turned on. It took the load time
          from 3 minutes to 6 seconds.


          It does the trick for practical purposes (now model loads in mere seconds)
          but the mystery remains unsolved :)'
        updatedAt: '2023-05-26T16:31:23.488Z'
      numEdits: 1
      reactions: []
    id: 6470dec93df93fddece5fcde
    type: comment
  author: kil3r
  content: '> The fix for me was setting low_cpu_mem_usage=True, this would normally
    be the default is device auto mapping was turned on. It took the load time from
    3 minutes to 6 seconds.


    It does the trick for practical purposes (now model loads in mere seconds) but
    the mystery remains unsolved :)'
  created_at: 2023-05-26 15:31:05+00:00
  edited: true
  hidden: false
  id: 6470dec93df93fddece5fcde
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45ddcc8ce32c0fcc49a2f5c68ca922d8.svg
      fullname: KN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kxyne
      type: user
    createdAt: '2023-05-26T23:17:50.000Z'
    data:
      edited: false
      editors:
      - kxyne
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45ddcc8ce32c0fcc49a2f5c68ca922d8.svg
          fullname: KN
          isHf: false
          isPro: false
          name: kxyne
          type: user
        html: '<p>I don''t have the traces in front of me but I''m fairly sure it
          was doing 32-&gt;b16 in python on cpu (twice). I think that flag will make
          it just load in the final weights without blatting out the initial empty
          state.<br>Not sure why people aren''t hitting this; I suspect accelerate
          will trigger a similar effect, I was running in a container so I may not
          have had it installed.</p>

          '
        raw: 'I don''t have the traces in front of me but I''m fairly sure it was
          doing 32->b16 in python on cpu (twice). I think that flag will make it just
          load in the final weights without blatting out the initial empty state.

          Not sure why people aren''t hitting this; I suspect accelerate will trigger
          a similar effect, I was running in a container so I may not have had it
          installed.'
        updatedAt: '2023-05-26T23:17:50.696Z'
      numEdits: 0
      reactions: []
    id: 64713e1e34a8a81ebd91de4d
    type: comment
  author: kxyne
  content: 'I don''t have the traces in front of me but I''m fairly sure it was doing
    32->b16 in python on cpu (twice). I think that flag will make it just load in
    the final weights without blatting out the initial empty state.

    Not sure why people aren''t hitting this; I suspect accelerate will trigger a
    similar effect, I was running in a container so I may not have had it installed.'
  created_at: 2023-05-26 22:17:50+00:00
  edited: false
  hidden: false
  id: 64713e1e34a8a81ebd91de4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-03T00:34:25.000Z'
    data:
      edited: false
      editors:
      - abhi-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9200633764266968
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: '<p>Just wanted to note that <code>device_map</code> support and faster
          KV cacheing has been added in this PR: <a href="https://huggingface.co/mosaicml/mpt-7b-instruct/discussions/41">https://huggingface.co/mosaicml/mpt-7b-instruct/discussions/41</a></p>

          <p>I think now, finally, if you load the model with <code>device_map=auto</code>
          on a CPU machine, it should go really fast because it''s avoiding the random
          weight init.</p>

          '
        raw: 'Just wanted to note that `device_map` support and faster KV cacheing
          has been added in this PR: https://huggingface.co/mosaicml/mpt-7b-instruct/discussions/41


          I think now, finally, if you load the model with `device_map=auto` on a
          CPU machine, it should go really fast because it''s avoiding the random
          weight init.'
        updatedAt: '2023-06-03T00:34:25.377Z'
      numEdits: 0
      reactions: []
      relatedEventId: 647a8a91c7367455fda2d01c
    id: 647a8a91c7367455fda2d01a
    type: comment
  author: abhi-mosaic
  content: 'Just wanted to note that `device_map` support and faster KV cacheing has
    been added in this PR: https://huggingface.co/mosaicml/mpt-7b-instruct/discussions/41


    I think now, finally, if you load the model with `device_map=auto` on a CPU machine,
    it should go really fast because it''s avoiding the random weight init.'
  created_at: 2023-06-02 23:34:25+00:00
  edited: false
  hidden: false
  id: 647a8a91c7367455fda2d01a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-03T00:34:25.000Z'
    data:
      status: closed
    id: 647a8a91c7367455fda2d01c
    type: status-change
  author: abhi-mosaic
  created_at: 2023-06-02 23:34:25+00:00
  id: 647a8a91c7367455fda2d01c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: mosaicml/mpt-7b-instruct
repo_type: model
status: closed
target_branch: null
title: Super slow loading compared to other (even bigger) models
