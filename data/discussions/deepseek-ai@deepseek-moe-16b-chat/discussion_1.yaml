!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2024-01-11 03:49:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2024-01-11T03:49:39.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9363704323768616
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>I get that this is impressive because a 2.8b param models infrence
          speed can match 7b param models quality. But its not really practical. If
          you can run a 7b parameter model you dont really need the extra inference
          speed, what you need is better quality. </p>

          <p>This is just my opinion, but I would have much preferred a bigger MoE
          from your guys made up from your 7b parameter models.<br>like a mixtral-48b
          param model but made up from deepseeklm-7b. Or even deepseekcoder-6.7b-instruct.
          </p>

          '
        raw: "I get that this is impressive because a 2.8b param models infrence speed\
          \ can match 7b param models quality. But its not really practical. If you\
          \ can run a 7b parameter model you dont really need the extra inference\
          \ speed, what you need is better quality. \r\n\r\nThis is just my opinion,\
          \ but I would have much preferred a bigger MoE from your guys made up from\
          \ your 7b parameter models.\r\nlike a mixtral-48b param model but made up\
          \ from deepseeklm-7b. Or even deepseekcoder-6.7b-instruct. "
        updatedAt: '2024-01-11T03:49:39.094Z'
      numEdits: 0
      reactions: []
    id: 659f6553d2e705b3fb5dba60
    type: comment
  author: rombodawg
  content: "I get that this is impressive because a 2.8b param models infrence speed\
    \ can match 7b param models quality. But its not really practical. If you can\
    \ run a 7b parameter model you dont really need the extra inference speed, what\
    \ you need is better quality. \r\n\r\nThis is just my opinion, but I would have\
    \ much preferred a bigger MoE from your guys made up from your 7b parameter models.\r\
    \nlike a mixtral-48b param model but made up from deepseeklm-7b. Or even deepseekcoder-6.7b-instruct. "
  created_at: 2024-01-11 03:49:39+00:00
  edited: false
  hidden: false
  id: 659f6553d2e705b3fb5dba60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2024-01-11T03:58:17.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9709612131118774
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: "<p>I was recently informed that the 40% in your github referred to\
          \ training compute requirement needed not inference speed. Which in that\
          \ case I kindoff take back what i said. This model does alot more than i\
          \ though it did, considering you can probably fully train it on 24gb of\
          \ vram in 4-bit using qlora with no issue. </p>\n<p>But still i want to\
          \ see that deepseek-48b-MoE please \U0001F601 </p>\n"
        raw: "I was recently informed that the 40% in your github referred to training\
          \ compute requirement needed not inference speed. Which in that case I kindoff\
          \ take back what i said. This model does alot more than i though it did,\
          \ considering you can probably fully train it on 24gb of vram in 4-bit using\
          \ qlora with no issue. \n\nBut still i want to see that deepseek-48b-MoE\
          \ please \U0001F601 "
        updatedAt: '2024-01-11T03:58:17.635Z'
      numEdits: 0
      reactions: []
    id: 659f6759697a41751ba2cdba
    type: comment
  author: rombodawg
  content: "I was recently informed that the 40% in your github referred to training\
    \ compute requirement needed not inference speed. Which in that case I kindoff\
    \ take back what i said. This model does alot more than i though it did, considering\
    \ you can probably fully train it on 24gb of vram in 4-bit using qlora with no\
    \ issue. \n\nBut still i want to see that deepseek-48b-MoE please \U0001F601 "
  created_at: 2024-01-11 03:58:17+00:00
  edited: false
  hidden: false
  id: 659f6759697a41751ba2cdba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2024-01-11T03:59:06.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9166500568389893
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>If im mistaken about anything I said here please correct me, so
          me and the community can better understand your model.<br>Cheers</p>

          '
        raw: 'If im mistaken about anything I said here please correct me, so me and
          the community can better understand your model.

          Cheers'
        updatedAt: '2024-01-11T03:59:06.822Z'
      numEdits: 0
      reactions: []
    id: 659f678a126760c9114181fa
    type: comment
  author: rombodawg
  content: 'If im mistaken about anything I said here please correct me, so me and
    the community can better understand your model.

    Cheers'
  created_at: 2024-01-11 03:59:06+00:00
  edited: false
  hidden: false
  id: 659f678a126760c9114181fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/195ce0968bbaf79e5b813413e26aec37.svg
      fullname: hua
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ccYcc
      type: user
    createdAt: '2024-01-11T05:14:23.000Z'
    data:
      edited: false
      editors:
      - ccYcc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9459885358810425
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/195ce0968bbaf79e5b813413e26aec37.svg
          fullname: hua
          isHf: false
          isPro: false
          name: ccYcc
          type: user
        html: '<p>i think this title can be changed as  deepseeks-8*2.8b-chat </p>

          '
        raw: 'i think this title can be changed as  deepseeks-8*2.8b-chat '
        updatedAt: '2024-01-11T05:14:23.951Z'
      numEdits: 0
      reactions: []
    id: 659f792fd2e705b3fb621f76
    type: comment
  author: ccYcc
  content: 'i think this title can be changed as  deepseeks-8*2.8b-chat '
  created_at: 2024-01-11 05:14:23+00:00
  edited: false
  hidden: false
  id: 659f792fd2e705b3fb621f76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2024-01-19T07:13:07.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8979657292366028
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;deepseek-admin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/deepseek-admin\"\
          >@<span class=\"underline\">deepseek-admin</span></a></span>\n\n\t</span></span>\
          \ you guys took too long so i did it myself \U0001F609</p>\n<p><a href=\"\
          https://huggingface.co/rombodawg/deepseek-coder-moe_8x6.7b-base\">https://huggingface.co/rombodawg/deepseek-coder-moe_8x6.7b-base</a></p>\n"
        raw: "@deepseek-admin you guys took too long so i did it myself \U0001F609\
          \n\nhttps://huggingface.co/rombodawg/deepseek-coder-moe_8x6.7b-base"
        updatedAt: '2024-01-19T07:13:07.468Z'
      numEdits: 0
      reactions: []
    id: 65aa2103873d8623d8ede617
    type: comment
  author: rombodawg
  content: "@deepseek-admin you guys took too long so i did it myself \U0001F609\n\
    \nhttps://huggingface.co/rombodawg/deepseek-coder-moe_8x6.7b-base"
  created_at: 2024-01-19 07:13:07+00:00
  edited: false
  hidden: false
  id: 65aa2103873d8623d8ede617
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: deepseek-ai/deepseek-moe-16b-chat
repo_type: model
status: open
target_branch: null
title: Moe from your 7b param model would be more effective
