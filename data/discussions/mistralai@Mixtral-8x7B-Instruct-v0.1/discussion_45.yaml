!!python/object:huggingface_hub.community.DiscussionWithDetails
author: robotrage
conflicting_files: null
created_at: 2023-12-16 12:45:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0308afe531dabd5ce1f25faac37c32af.svg
      fullname: robo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: robotrage
      type: user
    createdAt: '2023-12-16T12:45:14.000Z'
    data:
      edited: false
      editors:
      - robotrage
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.42906686663627625
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0308afe531dabd5ce1f25faac37c32af.svg
          fullname: robo
          isHf: false
          isPro: false
          name: robotrage
          type: user
        html: '<p>i see no spike in RAM or GPU usage</p>

          <p>code:</p>

          <p>from transformers import AutoModelForCausalLM, AutoTokenizer</p>

          <p>model_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"<br>tokenizer = AutoTokenizer.from_pretrained(model_id,
          cache_dir="H:/llm/Cache", offload_folder="H:/llm/Cache", device_map="auto")<br>print("re")</p>

          <p>model = None<br>try:<br>    model = AutoModelForCausalLM.from_pretrained(model_id,cache_dir="H:/llm/Cache")<br>except
          Exception as e:<br>    print("err: " + str(e))</p>

          <p>print("result")<br>text = "what is the ld50 of alcohol"<br>inputs = tokenizer(text,
          return_tensors="pt")<br>print("result!")<br>outputs = model.generate(**inputs,
          max_new_tokens=20)<br>print(tokenizer.decode(outputs[0], skip_special_tokens=True)
          + "result!!!!!")</p>

          <p>output:</p>

          <p>H:\llm\MOE\mixtral\Mixtral-8x7B-Instruct-v0.1&gt;python bot.py<br>re</p>

          <p>H:\llm\MOE\mixtral\Mixtral-8x7B-Instruct-v0.1&gt;</p>

          '
        raw: "i see no spike in RAM or GPU usage\r\n\r\ncode:\r\n\r\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
          \r\ntokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"H:/llm/Cache\"\
          , offload_folder=\"H:/llm/Cache\", device_map=\"auto\")\r\nprint(\"re\"\
          )\r\n\r\nmodel = None\r\ntry:\r\n    model = AutoModelForCausalLM.from_pretrained(model_id,cache_dir=\"\
          H:/llm/Cache\")\r\nexcept Exception as e:\r\n    print(\"err: \" + str(e))\r\
          \n    \r\nprint(\"result\")\r\ntext = \"what is the ld50 of alcohol\"\r\n\
          inputs = tokenizer(text, return_tensors=\"pt\")\r\nprint(\"result!\")\r\n\
          outputs = model.generate(**inputs, max_new_tokens=20)\r\nprint(tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True) + \"result!!!!!\")\r\n\r\n\r\noutput:\r\n\r\n\
          H:\\llm\\MOE\\mixtral\\Mixtral-8x7B-Instruct-v0.1>python bot.py\r\nre\r\n\
          \r\nH:\\llm\\MOE\\mixtral\\Mixtral-8x7B-Instruct-v0.1>\r\n\r\n\r\n"
        updatedAt: '2023-12-16T12:45:14.459Z'
      numEdits: 0
      reactions: []
    id: 657d9bda19ca6a5e925e8018
    type: comment
  author: robotrage
  content: "i see no spike in RAM or GPU usage\r\n\r\ncode:\r\n\r\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
    \r\ntokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"H:/llm/Cache\"\
    , offload_folder=\"H:/llm/Cache\", device_map=\"auto\")\r\nprint(\"re\")\r\n\r\
    \nmodel = None\r\ntry:\r\n    model = AutoModelForCausalLM.from_pretrained(model_id,cache_dir=\"\
    H:/llm/Cache\")\r\nexcept Exception as e:\r\n    print(\"err: \" + str(e))\r\n\
    \    \r\nprint(\"result\")\r\ntext = \"what is the ld50 of alcohol\"\r\ninputs\
    \ = tokenizer(text, return_tensors=\"pt\")\r\nprint(\"result!\")\r\noutputs =\
    \ model.generate(**inputs, max_new_tokens=20)\r\nprint(tokenizer.decode(outputs[0],\
    \ skip_special_tokens=True) + \"result!!!!!\")\r\n\r\n\r\noutput:\r\n\r\nH:\\\
    llm\\MOE\\mixtral\\Mixtral-8x7B-Instruct-v0.1>python bot.py\r\nre\r\n\r\nH:\\\
    llm\\MOE\\mixtral\\Mixtral-8x7B-Instruct-v0.1>\r\n\r\n\r\n"
  created_at: 2023-12-16 12:45:14+00:00
  edited: false
  hidden: false
  id: 657d9bda19ca6a5e925e8018
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-18T19:16:56.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8552245497703552
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>Would try to load a smaller version of the model with <code>model
          = AutoModelForCausalLM.from_pretrained(model_id,cache_dir="H:/llm/Cache",
          torch_dtype = torch.float16)</code> (unless you don''t have a GPU). What
          architecture are you using? </p>

          '
        raw: 'Would try to load a smaller version of the model with `model = AutoModelForCausalLM.from_pretrained(model_id,cache_dir="H:/llm/Cache",
          torch_dtype = torch.float16)` (unless you don''t have a GPU). What architecture
          are you using? '
        updatedAt: '2023-12-18T19:16:56.598Z'
      numEdits: 0
      reactions: []
    id: 65809aa862524062d7cb4d92
    type: comment
  author: ArthurZ
  content: 'Would try to load a smaller version of the model with `model = AutoModelForCausalLM.from_pretrained(model_id,cache_dir="H:/llm/Cache",
    torch_dtype = torch.float16)` (unless you don''t have a GPU). What architecture
    are you using? '
  created_at: 2023-12-18 19:16:56+00:00
  edited: false
  hidden: false
  id: 65809aa862524062d7cb4d92
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-18T19:34:41.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5763862133026123
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;robotrage&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/robotrage\"\
          >@<span class=\"underline\">robotrage</span></a></span>\n\n\t</span></span><br>there\
          \ is no need to call <code>device_map=\"auto\"</code> on the tokenizer call,\
          \ can you call it instead in the automodel call ? Also please consider passing\
          \ <code>low_cpu_mem_usage=True</code></p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = <span class=\"\
          hljs-string\">\"mistralai/Mixtral-8x7B-Instruct-v0.1\"</span>\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_id, cache_dir=<span class=\"hljs-string\"\
          >\"H:/llm/Cache\"</span>, offload_folder=<span class=\"hljs-string\">\"\
          H:/llm/Cache\"</span>)\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">\"re\"</span>)\n\nmodel = <span class=\"hljs-literal\"\
          >None</span>\n<span class=\"hljs-keyword\">try</span>:\n    model = AutoModelForCausalLM.from_pretrained(model_id,cache_dir=<span\
          \ class=\"hljs-string\">\"H:/llm/Cache\"</span>,  device_map=<span class=\"\
          hljs-string\">\"auto\"</span>, low_cpu_mem_usage=<span class=\"hljs-literal\"\
          >True</span>)\n<span class=\"hljs-keyword\">except</span> Exception <span\
          \ class=\"hljs-keyword\">as</span> e:\n    <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"err: \"</span> + <span class=\"\
          hljs-built_in\">str</span>(e))\n</code></pre>\n"
        raw: "Hi @robotrage \nthere is no need to call `device_map=\"auto\"` on the\
          \ tokenizer call, can you call it instead in the automodel call ? Also please\
          \ consider passing `low_cpu_mem_usage=True`\n\n```python\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
          \ntokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"H:/llm/Cache\"\
          , offload_folder=\"H:/llm/Cache\")\nprint(\"re\")\n\nmodel = None\ntry:\n\
          \    model = AutoModelForCausalLM.from_pretrained(model_id,cache_dir=\"\
          H:/llm/Cache\",  device_map=\"auto\", low_cpu_mem_usage=True)\nexcept Exception\
          \ as e:\n    print(\"err: \" + str(e))\n```"
        updatedAt: '2023-12-18T19:34:41.677Z'
      numEdits: 0
      reactions: []
    id: 65809ed1a2284a018e3ac777
    type: comment
  author: ybelkada
  content: "Hi @robotrage \nthere is no need to call `device_map=\"auto\"` on the\
    \ tokenizer call, can you call it instead in the automodel call ? Also please\
    \ consider passing `low_cpu_mem_usage=True`\n\n```python\nfrom transformers import\
    \ AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
    \ntokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"H:/llm/Cache\"\
    , offload_folder=\"H:/llm/Cache\")\nprint(\"re\")\n\nmodel = None\ntry:\n    model\
    \ = AutoModelForCausalLM.from_pretrained(model_id,cache_dir=\"H:/llm/Cache\",\
    \  device_map=\"auto\", low_cpu_mem_usage=True)\nexcept Exception as e:\n    print(\"\
    err: \" + str(e))\n```"
  created_at: 2023-12-18 19:34:41+00:00
  edited: false
  hidden: false
  id: 65809ed1a2284a018e3ac777
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 45
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Model not loading and not printing any error message
