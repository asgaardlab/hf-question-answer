!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Tapendra
conflicting_files: null
created_at: 2023-12-21 07:18:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b5a0e04fde4337091d47c1636d94486.svg
      fullname: Tapendra Baduwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tapendra
      type: user
    createdAt: '2023-12-21T07:18:20.000Z'
    data:
      edited: false
      editors:
      - Tapendra
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7535695433616638
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8b5a0e04fde4337091d47c1636d94486.svg
          fullname: Tapendra Baduwal
          isHf: false
          isPro: false
          name: Tapendra
          type: user
        html: '<p>Since I am testing on 1000 data points from Task A where the response=
          xyz, Here is my Task B data point with a response= abc. If I train model1
          for Task A and fine-tune model2 on model1, my model2 is not able to respond
          with ''xyz which is response of model1 for taskA.</p>

          <p>If we merge adapters then this work for continuous learning ?<br>I expect
          the new train model to provide output for previously trained data as well.</p>

          <p>model_path = "model/path" </p>

          <h1 id="reload-model-in-fp16-and-merge-it-with-lora-weights">Reload model
          in FP16 and merge it with LoRA weights</h1>

          <p>base_model = AutoModelForCausalLM.from_pretrained(<br>    model_name,<br>    low_cpu_mem_usage=True,<br>    return_dict=True,<br>    torch_dtype=torch.float16,<br>    device_map=device_map,<br>)<br>model
          = PeftModel.from_pretrained(base_model, new_model)<br>model = model.merge_and_unload()</p>

          <h1 id="reload-tokenizer-to-save-it">Reload tokenizer to save it</h1>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)<br>tokenizer.pad_token
          = tokenizer.eos_token<br>tokenizer.padding_side = "right"</p>

          <h1 id="save-the-merged-model">Save the merged model</h1>

          <p>model.save_pretrained(model_path)<br>tokenizer.save_pretrained(model_path)</p>

          '
        raw: "\r\nSince I am testing on 1000 data points from Task A where the response=\
          \ xyz, Here is my Task B data point with a response= abc. If I train model1\
          \ for Task A and fine-tune model2 on model1, my model2 is not able to respond\
          \ with 'xyz which is response of model1 for taskA.\r\n\r\nIf we merge adapters\
          \ then this work for continuous learning ?\r\nI expect the new train model\
          \ to provide output for previously trained data as well.\r\n\r\nmodel_path\
          \ = \"model/path\" \r\n\r\n# Reload model in FP16 and merge it with LoRA\
          \ weights\r\nbase_model = AutoModelForCausalLM.from_pretrained(\r\n    model_name,\r\
          \n    low_cpu_mem_usage=True,\r\n    return_dict=True,\r\n    torch_dtype=torch.float16,\r\
          \n    device_map=device_map,\r\n)\r\nmodel = PeftModel.from_pretrained(base_model,\
          \ new_model)\r\nmodel = model.merge_and_unload()\r\n\r\n# Reload tokenizer\
          \ to save it\r\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\r\
          \ntokenizer.pad_token = tokenizer.eos_token\r\ntokenizer.padding_side =\
          \ \"right\"\r\n\r\n# Save the merged model\r\nmodel.save_pretrained(model_path)\r\
          \ntokenizer.save_pretrained(model_path)"
        updatedAt: '2023-12-21T07:18:20.094Z'
      numEdits: 0
      reactions: []
    id: 6583e6bca38dbd48cee7887f
    type: comment
  author: Tapendra
  content: "\r\nSince I am testing on 1000 data points from Task A where the response=\
    \ xyz, Here is my Task B data point with a response= abc. If I train model1 for\
    \ Task A and fine-tune model2 on model1, my model2 is not able to respond with\
    \ 'xyz which is response of model1 for taskA.\r\n\r\nIf we merge adapters then\
    \ this work for continuous learning ?\r\nI expect the new train model to provide\
    \ output for previously trained data as well.\r\n\r\nmodel_path = \"model/path\"\
    \ \r\n\r\n# Reload model in FP16 and merge it with LoRA weights\r\nbase_model\
    \ = AutoModelForCausalLM.from_pretrained(\r\n    model_name,\r\n    low_cpu_mem_usage=True,\r\
    \n    return_dict=True,\r\n    torch_dtype=torch.float16,\r\n    device_map=device_map,\r\
    \n)\r\nmodel = PeftModel.from_pretrained(base_model, new_model)\r\nmodel = model.merge_and_unload()\r\
    \n\r\n# Reload tokenizer to save it\r\ntokenizer = AutoTokenizer.from_pretrained(model_name,\
    \ trust_remote_code=True)\r\ntokenizer.pad_token = tokenizer.eos_token\r\ntokenizer.padding_side\
    \ = \"right\"\r\n\r\n# Save the merged model\r\nmodel.save_pretrained(model_path)\r\
    \ntokenizer.save_pretrained(model_path)"
  created_at: 2023-12-21 07:18:20+00:00
  edited: false
  hidden: false
  id: 6583e6bca38dbd48cee7887f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 58
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: How can we enable continuous learning with the LLM model  ?
