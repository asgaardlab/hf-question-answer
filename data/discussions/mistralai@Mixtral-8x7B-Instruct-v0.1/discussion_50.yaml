!!python/object:huggingface_hub.community.DiscussionWithDetails
author: philgrey
conflicting_files: null
created_at: 2023-12-18 03:45:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ixKl5PVwauxFVuRMwv-ON.jpeg?w=200&h=200&f=face
      fullname: John Grey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philgrey
      type: user
    createdAt: '2023-12-18T03:45:03.000Z'
    data:
      edited: true
      editors:
      - philgrey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.781268298625946
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ixKl5PVwauxFVuRMwv-ON.jpeg?w=200&h=200&f=face
          fullname: John Grey
          isHf: false
          isPro: false
          name: philgrey
          type: user
        html: '<p>Thanks for innovate model.<br>Now, I wanna save document to vectorstore
          using this model after building sagemaker endpoint.<br>does this model supports
          embedding vector generation?<br>so,<br>response[''body''].read().decode("utf-8")<br>have
          key - "vectors"?<br>(it seems that most mistral models don''t have this
          key)</p>

          '
        raw: "Thanks for innovate model.\nNow, I wanna save document to vectorstore\
          \ using this model after building sagemaker endpoint.\ndoes this model supports\
          \ embedding vector generation?\nso, \nresponse['body'].read().decode(\"\
          utf-8\") \nhave key - \"vectors\"?\n(it seems that most mistral models don't\
          \ have this key)"
        updatedAt: '2023-12-18T03:45:34.152Z'
      numEdits: 1
      reactions: []
    id: 657fc03f647c0211e7dcbb3b
    type: comment
  author: philgrey
  content: "Thanks for innovate model.\nNow, I wanna save document to vectorstore\
    \ using this model after building sagemaker endpoint.\ndoes this model supports\
    \ embedding vector generation?\nso, \nresponse['body'].read().decode(\"utf-8\"\
    ) \nhave key - \"vectors\"?\n(it seems that most mistral models don't have this\
    \ key)"
  created_at: 2023-12-18 03:45:03+00:00
  edited: true
  hidden: false
  id: 657fc03f647c0211e7dcbb3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-18T19:05:24.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8366976380348206
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>Could you provide a bit more context on how you are using the model?
          TGI? Inference endpoint? etc</p>

          '
        raw: Could you provide a bit more context on how you are using the model?
          TGI? Inference endpoint? etc
        updatedAt: '2023-12-18T19:05:24.397Z'
      numEdits: 0
      reactions: []
    id: 658097f4cc33cac1933b3acc
    type: comment
  author: ArthurZ
  content: Could you provide a bit more context on how you are using the model? TGI?
    Inference endpoint? etc
  created_at: 2023-12-18 19:05:24+00:00
  edited: false
  hidden: false
  id: 658097f4cc33cac1933b3acc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ixKl5PVwauxFVuRMwv-ON.jpeg?w=200&h=200&f=face
      fullname: John Grey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philgrey
      type: user
    createdAt: '2023-12-18T19:58:53.000Z'
    data:
      edited: true
      editors:
      - philgrey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.42014777660369873
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ixKl5PVwauxFVuRMwv-ON.jpeg?w=200&h=200&f=face
          fullname: John Grey
          isHf: false
          isPro: false
          name: philgrey
          type: user
        html: "<p>from langchain.llms.sagemaker_endpoint import LLMContentHandler</p>\n\
          <p>class ContentHandler(LLMContentHandler):<br>    content_type = \"application/json\"\
          <br>    accepts = \"application/json\"</p>\n<pre><code>def transform_input(self,\
          \ inputs: list[str], model_kwargs: Dict) -&gt; bytes:\n    \"\"\"\n    Transforms\
          \ the input into bytes that can be consumed by SageMaker endpoint.\n   \
          \ Args:\n        inputs: List of input strings.\n        model_kwargs: Additional\
          \ keyword arguments to be passed to the endpoint.\n    Returns:\n      \
          \  The transformed bytes input.\n    \"\"\"\n    # Example: inference.py\
          \ expects a JSON string with a \"inputs\" key:\n    input_str = ' '.join(inputs)\n\
          \    input_str = json.dumps({\"inputs\": input_str, **model_kwargs})\n \
          \   return input_str.encode(\"utf-8\")\n\ndef transform_output(self, output:\
          \ bytes) -&gt; List[List[float]]:\n    \"\"\"\n    Transforms the bytes\
          \ output from the endpoint into a list of embeddings.\n    Args:\n     \
          \   output: The bytes output from SageMaker endpoint.\n    Returns:\n  \
          \      The transformed output - list of embeddings\n    Note:\n        The\
          \ length of the outer list is the number of input strings.\n        The\
          \ length of the inner lists is the embedding dimension.\n    \"\"\"\n  \
          \  # Example: inference.py returns a JSON string with the list of\n    #\
          \ embeddings in a \"vectors\" key:\n    response_json = json.loads(output.read().decode(\"\
          utf-8\"))\n    return response_json\n</code></pre>\n<p>============================================================================================</p>\n\
          <p>from langchain.chains.question_answering import load_qa_chain<br>from\
          \ langchain.llms.sagemaker_endpoint import SagemakerEndpoint</p>\n<p>content_handler\
          \ = ContentHandler()</p>\n<p>llms = SagemakerEndpoint(<br>    endpoint_name=\"\
          huggingface-pytorch-tgi-inference-2023-12-18-06-04-44-513\",<br>    region_name=\"\
          eu-west-2\",<br>    model_kwargs={<br>        \"temperature\": 0,<br>  \
          \      \"maxTokens\": 1024,<br>        \"numResults\": 3<br>    },<br> \
          \   content_handler=content_handler<br>)</p>\n"
        raw: "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n\n\
          class ContentHandler(LLMContentHandler):\n    content_type = \"application/json\"\
          \n    accepts = \"application/json\"\n\n    def transform_input(self, inputs:\
          \ list[str], model_kwargs: Dict) -> bytes:\n        \"\"\"\n        Transforms\
          \ the input into bytes that can be consumed by SageMaker endpoint.\n   \
          \     Args:\n            inputs: List of input strings.\n            model_kwargs:\
          \ Additional keyword arguments to be passed to the endpoint.\n        Returns:\n\
          \            The transformed bytes input.\n        \"\"\"\n        # Example:\
          \ inference.py expects a JSON string with a \"inputs\" key:\n        input_str\
          \ = ' '.join(inputs)\n        input_str = json.dumps({\"inputs\": input_str,\
          \ **model_kwargs})\n        return input_str.encode(\"utf-8\")\n\n    def\
          \ transform_output(self, output: bytes) -> List[List[float]]:\n        \"\
          \"\"\n        Transforms the bytes output from the endpoint into a list\
          \ of embeddings.\n        Args:\n            output: The bytes output from\
          \ SageMaker endpoint.\n        Returns:\n            The transformed output\
          \ - list of embeddings\n        Note:\n            The length of the outer\
          \ list is the number of input strings.\n            The length of the inner\
          \ lists is the embedding dimension.\n        \"\"\"\n        # Example:\
          \ inference.py returns a JSON string with the list of\n        # embeddings\
          \ in a \"vectors\" key:\n        response_json = json.loads(output.read().decode(\"\
          utf-8\"))\n        return response_json\n============================================================================================\n\
          \nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain.llms.sagemaker_endpoint\
          \ import SagemakerEndpoint\n\ncontent_handler = ContentHandler()\n\nllms\
          \ = SagemakerEndpoint(\n    endpoint_name=\"huggingface-pytorch-tgi-inference-2023-12-18-06-04-44-513\"\
          ,\n    region_name=\"eu-west-2\",\n    model_kwargs={\n        \"temperature\"\
          : 0,\n        \"maxTokens\": 1024,\n        \"numResults\": 3\n    },\n\
          \    content_handler=content_handler\n)\n\n"
        updatedAt: '2023-12-18T20:00:03.561Z'
      numEdits: 1
      reactions: []
    id: 6580a47d6c6ea8c668197272
    type: comment
  author: philgrey
  content: "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n\nclass\
    \ ContentHandler(LLMContentHandler):\n    content_type = \"application/json\"\n\
    \    accepts = \"application/json\"\n\n    def transform_input(self, inputs: list[str],\
    \ model_kwargs: Dict) -> bytes:\n        \"\"\"\n        Transforms the input\
    \ into bytes that can be consumed by SageMaker endpoint.\n        Args:\n    \
    \        inputs: List of input strings.\n            model_kwargs: Additional\
    \ keyword arguments to be passed to the endpoint.\n        Returns:\n        \
    \    The transformed bytes input.\n        \"\"\"\n        # Example: inference.py\
    \ expects a JSON string with a \"inputs\" key:\n        input_str = ' '.join(inputs)\n\
    \        input_str = json.dumps({\"inputs\": input_str, **model_kwargs})\n   \
    \     return input_str.encode(\"utf-8\")\n\n    def transform_output(self, output:\
    \ bytes) -> List[List[float]]:\n        \"\"\"\n        Transforms the bytes output\
    \ from the endpoint into a list of embeddings.\n        Args:\n            output:\
    \ The bytes output from SageMaker endpoint.\n        Returns:\n            The\
    \ transformed output - list of embeddings\n        Note:\n            The length\
    \ of the outer list is the number of input strings.\n            The length of\
    \ the inner lists is the embedding dimension.\n        \"\"\"\n        # Example:\
    \ inference.py returns a JSON string with the list of\n        # embeddings in\
    \ a \"vectors\" key:\n        response_json = json.loads(output.read().decode(\"\
    utf-8\"))\n        return response_json\n============================================================================================\n\
    \nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain.llms.sagemaker_endpoint\
    \ import SagemakerEndpoint\n\ncontent_handler = ContentHandler()\n\nllms = SagemakerEndpoint(\n\
    \    endpoint_name=\"huggingface-pytorch-tgi-inference-2023-12-18-06-04-44-513\"\
    ,\n    region_name=\"eu-west-2\",\n    model_kwargs={\n        \"temperature\"\
    : 0,\n        \"maxTokens\": 1024,\n        \"numResults\": 3\n    },\n    content_handler=content_handler\n\
    )\n\n"
  created_at: 2023-12-18 19:58:53+00:00
  edited: true
  hidden: false
  id: 6580a47d6c6ea8c668197272
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ixKl5PVwauxFVuRMwv-ON.jpeg?w=200&h=200&f=face
      fullname: John Grey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philgrey
      type: user
    createdAt: '2023-12-18T20:00:31.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ixKl5PVwauxFVuRMwv-ON.jpeg?w=200&h=200&f=face
          fullname: John Grey
          isHf: false
          isPro: false
          name: philgrey
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-18T20:00:58.397Z'
      numEdits: 0
      reactions: []
    id: 6580a4df98aa9fcdd25786f0
    type: comment
  author: philgrey
  content: This comment has been hidden
  created_at: 2023-12-18 20:00:31+00:00
  edited: true
  hidden: true
  id: 6580a4df98aa9fcdd25786f0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 50
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: vectorstore
