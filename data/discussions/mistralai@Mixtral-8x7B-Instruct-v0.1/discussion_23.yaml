!!python/object:huggingface_hub.community.DiscussionWithDetails
author: krumeto
conflicting_files: null
created_at: 2023-12-13 12:35:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e55253efd9dab60819257255e2dd959d.svg
      fullname: Krum Arnaudov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krumeto
      type: user
    createdAt: '2023-12-13T12:35:32.000Z'
    data:
      edited: false
      editors:
      - krumeto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9392456412315369
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e55253efd9dab60819257255e2dd959d.svg
          fullname: Krum Arnaudov
          isHf: false
          isPro: false
          name: krumeto
          type: user
        html: '<p>My intuition has been that larger models lose relatively less quality
          after quantization vs. smaller models (e.g. Llama 2 70B in 4bits would be
          closer to the original precision model than Llama 2 7B in 4bits to its original
          precision model).</p>

          <p>Do you have any insights if that intuition holds for a MoE? </p>

          <p>If during inference only 2 of the 7B experts are active based on the
          above, I''d expect the quality loss after quantization to be relatively
          higher than, say a 45B non-MoE quantized model.</p>

          <p>Thank you in advance! </p>

          '
        raw: "My intuition has been that larger models lose relatively less quality\
          \ after quantization vs. smaller models (e.g. Llama 2 70B in 4bits would\
          \ be closer to the original precision model than Llama 2 7B in 4bits to\
          \ its original precision model).\r\n\r\nDo you have any insights if that\
          \ intuition holds for a MoE? \r\n\r\nIf during inference only 2 of the 7B\
          \ experts are active based on the above, I'd expect the quality loss after\
          \ quantization to be relatively higher than, say a 45B non-MoE quantized\
          \ model.\r\n\r\nThank you in advance! "
        updatedAt: '2023-12-13T12:35:32.611Z'
      numEdits: 0
      reactions: []
    id: 6579a51421a8b318364f82b1
    type: comment
  author: krumeto
  content: "My intuition has been that larger models lose relatively less quality\
    \ after quantization vs. smaller models (e.g. Llama 2 70B in 4bits would be closer\
    \ to the original precision model than Llama 2 7B in 4bits to its original precision\
    \ model).\r\n\r\nDo you have any insights if that intuition holds for a MoE? \r\
    \n\r\nIf during inference only 2 of the 7B experts are active based on the above,\
    \ I'd expect the quality loss after quantization to be relatively higher than,\
    \ say a 45B non-MoE quantized model.\r\n\r\nThank you in advance! "
  created_at: 2023-12-13 12:35:32+00:00
  edited: false
  hidden: false
  id: 6579a51421a8b318364f82b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-18T11:39:26.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.904967725276947
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;marcsun13&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/marcsun13\"\
          >@<span class=\"underline\">marcsun13</span></a></span>\n\n\t</span></span>\
          \ who worked on the quantization! </p>\n"
        raw: 'cc @marcsun13 who worked on the quantization! '
        updatedAt: '2023-12-18T11:39:26.467Z'
      numEdits: 0
      reactions: []
    id: 65802f6efd6100040e6dde1d
    type: comment
  author: ArthurZ
  content: 'cc @marcsun13 who worked on the quantization! '
  created_at: 2023-12-18 11:39:26+00:00
  edited: false
  hidden: false
  id: 65802f6efd6100040e6dde1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ce875d199b36f7552d4f07/NrTlnhUTAp0j0ZsMTsWIu.jpeg?w=200&h=200&f=face
      fullname: Marc Sun
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marcsun13
      type: user
    createdAt: '2023-12-18T11:46:50.000Z'
    data:
      edited: true
      editors:
      - marcsun13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.953520655632019
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ce875d199b36f7552d4f07/NrTlnhUTAp0j0ZsMTsWIu.jpeg?w=200&h=200&f=face
          fullname: Marc Sun
          isHf: true
          isPro: false
          name: marcsun13
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;krumeto&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/krumeto\">@<span class=\"\
          underline\">krumeto</span></a></span>\n\n\t</span></span>, this is right.\
          \ We've seen a decrease in quality loss comparable to a quantized LLama\
          \ 7B. </p>\n"
        raw: 'Hi @krumeto, this is right. We''ve seen a decrease in quality loss comparable
          to a quantized LLama 7B. '
        updatedAt: '2023-12-18T11:47:08.108Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - krumeto
    id: 6580312a1aaf061480982a09
    type: comment
  author: marcsun13
  content: 'Hi @krumeto, this is right. We''ve seen a decrease in quality loss comparable
    to a quantized LLama 7B. '
  created_at: 2023-12-18 11:46:50+00:00
  edited: true
  hidden: false
  id: 6580312a1aaf061480982a09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e55253efd9dab60819257255e2dd959d.svg
      fullname: Krum Arnaudov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krumeto
      type: user
    createdAt: '2023-12-18T14:09:34.000Z'
    data:
      edited: false
      editors:
      - krumeto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8637217879295349
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e55253efd9dab60819257255e2dd959d.svg
          fullname: Krum Arnaudov
          isHf: false
          isPro: false
          name: krumeto
          type: user
        html: "<p>Thank you, <span data-props=\"{&quot;user&quot;:&quot;marcsun13&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/marcsun13\"\
          >@<span class=\"underline\">marcsun13</span></a></span>\n\n\t</span></span>\
          \ ! Since I asked the question, first Open LLM Leaderboard results for the\
          \ base GPTQ version appeared. The decrease seems to be more or less similar\
          \ to what we saw with Llama 2 models:</p>\n<div class=\"max-w-full overflow-auto\"\
          >\n\t<table>\n\t\t<thead><tr>\n<th>Model</th>\n<th>Average</th>\n<th>ARC</th>\n\
          <th>HellaSwag</th>\n<th>MMLU</th>\n<th>TruthfulQA</th>\n<th>Winogrande</th>\n\
          <th>GSM8K</th>\n</tr>\n\n\t\t</thead><tbody><tr>\n<td>mistralai/Mixtral-8x7B-v0.1</td>\n\
          <td>68.42</td>\n<td>66.04</td>\n<td>86.49</td>\n<td>71.82</td>\n<td>46.78</td>\n\
          <td>81.93</td>\n<td>57.47</td>\n</tr>\n<tr>\n<td>TheBloke/Mixtral-8x7B-v0.1-GPTQ</td>\n\
          <td>65.7</td>\n<td>65.19</td>\n<td>84.72</td>\n<td>69.43</td>\n<td>45.42</td>\n\
          <td>81.14</td>\n<td>48.29</td>\n</tr>\n<tr>\n<td><strong>Score Delta</strong></td>\n\
          <td><strong>0.960</strong></td>\n<td><strong>0.987</strong></td>\n<td><strong>0.980</strong></td>\n\
          <td><strong>0.967</strong></td>\n<td><strong>0.971</strong></td>\n<td><strong>0.990</strong></td>\n\
          <td><strong>0.840</strong></td>\n</tr>\n</tbody>\n\t</table>\n</div>\n<p>This\
          \ is great news for us (waiting for the instruct model GPTQ scores, but\
          \ in general, I hope this holds). We are testing the model with TGI (in\
          \ 8bit, eetq), waiting to test GPTQ (seems like there are still some TGI\
          \ issues with GPTQ), but not quite sure which of the methods should retain\
          \ most quality (we are less interested in speed).  If you have any resources\
          \ that compare Mixtral (or even other models) any of EETQ/GPTQ/AWQ/bnb in\
          \ terms of quality, it would be very helpful. This blog was already extremely\
          \ insightful - <a href=\"https://huggingface.co/blog/overview-quantization-transformers#overview-of-natively-supported-quantization-schemes-in-%F0%9F%A4%97-transformers\"\
          >https://huggingface.co/blog/overview-quantization-transformers#overview-of-natively-supported-quantization-schemes-in-%F0%9F%A4%97-transformers</a><br>Thank\
          \ you all!</p>\n"
        raw: "Thank you, @marcsun13 ! Since I asked the question, first Open LLM Leaderboard\
          \ results for the base GPTQ version appeared. The decrease seems to be more\
          \ or less similar to what we saw with Llama 2 models:\n\n| Model       \
          \                           | Average | ARC   | HellaSwag | MMLU  | TruthfulQA\
          \ | Winogrande | GSM8K |\n| --------------------------------------- | -------\
          \ | ----- | --------- | ----- | ---------- | ---------- | ----- |\n| mistralai/Mixtral-8x7B-v0.1\
          \    | 68.42   | 66.04 | 86.49     | 71.82 | 46.78      | 81.93      | 57.47\
          \ |\n| TheBloke/Mixtral-8x7B-v0.1-GPTQ         | 65.7    | 65.19 | 84.72\
          \     | 69.43 | 45.42      | 81.14      | 48.29 |\n| **Score Delta**   \
          \                     | **0.960**| **0.987** | **0.980** | **0.967** | **0.971**\
          \ | **0.990** | **0.840** |\n\nThis is great news for us (waiting for the\
          \ instruct model GPTQ scores, but in general, I hope this holds). We are\
          \ testing the model with TGI (in 8bit, eetq), waiting to test GPTQ (seems\
          \ like there are still some TGI issues with GPTQ), but not quite sure which\
          \ of the methods should retain most quality (we are less interested in speed).\
          \  If you have any resources that compare Mixtral (or even other models)\
          \ any of EETQ/GPTQ/AWQ/bnb in terms of quality, it would be very helpful.\
          \ This blog was already extremely insightful - https://huggingface.co/blog/overview-quantization-transformers#overview-of-natively-supported-quantization-schemes-in-%F0%9F%A4%97-transformers\
          \ \nThank you all!"
        updatedAt: '2023-12-18T14:09:34.360Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
        - marcsun13
    id: 6580529ea760379049a22624
    type: comment
  author: krumeto
  content: "Thank you, @marcsun13 ! Since I asked the question, first Open LLM Leaderboard\
    \ results for the base GPTQ version appeared. The decrease seems to be more or\
    \ less similar to what we saw with Llama 2 models:\n\n| Model                \
    \                  | Average | ARC   | HellaSwag | MMLU  | TruthfulQA | Winogrande\
    \ | GSM8K |\n| --------------------------------------- | ------- | ----- | ---------\
    \ | ----- | ---------- | ---------- | ----- |\n| mistralai/Mixtral-8x7B-v0.1 \
    \   | 68.42   | 66.04 | 86.49     | 71.82 | 46.78      | 81.93      | 57.47 |\n\
    | TheBloke/Mixtral-8x7B-v0.1-GPTQ         | 65.7    | 65.19 | 84.72     | 69.43\
    \ | 45.42      | 81.14      | 48.29 |\n| **Score Delta**                     \
    \   | **0.960**| **0.987** | **0.980** | **0.967** | **0.971** | **0.990** | **0.840**\
    \ |\n\nThis is great news for us (waiting for the instruct model GPTQ scores,\
    \ but in general, I hope this holds). We are testing the model with TGI (in 8bit,\
    \ eetq), waiting to test GPTQ (seems like there are still some TGI issues with\
    \ GPTQ), but not quite sure which of the methods should retain most quality (we\
    \ are less interested in speed).  If you have any resources that compare Mixtral\
    \ (or even other models) any of EETQ/GPTQ/AWQ/bnb in terms of quality, it would\
    \ be very helpful. This blog was already extremely insightful - https://huggingface.co/blog/overview-quantization-transformers#overview-of-natively-supported-quantization-schemes-in-%F0%9F%A4%97-transformers\
    \ \nThank you all!"
  created_at: 2023-12-18 14:09:34+00:00
  edited: false
  hidden: false
  id: 6580529ea760379049a22624
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ce875d199b36f7552d4f07/NrTlnhUTAp0j0ZsMTsWIu.jpeg?w=200&h=200&f=face
      fullname: Marc Sun
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marcsun13
      type: user
    createdAt: '2023-12-18T14:37:30.000Z'
    data:
      edited: false
      editors:
      - marcsun13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9194623231887817
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ce875d199b36f7552d4f07/NrTlnhUTAp0j0ZsMTsWIu.jpeg?w=200&h=200&f=face
          fullname: Marc Sun
          isHf: true
          isPro: false
          name: marcsun13
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;krumeto&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/krumeto\">@<span class=\"\
          underline\">krumeto</span></a></span>\n\n\t</span></span> , thanks for the\
          \ awesome feedback. We are still working on the AWQ quant since the quality\
          \ is not good enough for now. For bnb, the quality should be the same. As\
          \ for GPTQ, the model that was tested is not the best gptq quant. You can\
          \ test the following branch which should give better results:  <code>gptq-4bit-128g-actorder_True</code>\
          \ or <code>gptq-4bit-32g-actorder_True</code> with 32g being the most accurate\
          \ one. However, the vram consumption will increase since these quant needs\
          \ to store more quantization statistics (128g version with an additional\
          \ 1go and 32g version with  an additional 3.5 Go)<br><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/63ce875d199b36f7552d4f07/NaL0VVgKgT2PmVGX8h6m3.png\"\
          ><img alt=\"Screenshot 2023-12-18 at 3.26.50 PM.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63ce875d199b36f7552d4f07/NaL0VVgKgT2PmVGX8h6m3.png\"\
          ></a></p>\n<p>More details: <a href=\"https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ\"\
          >https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ</a></p>\n"
        raw: 'Hi @krumeto , thanks for the awesome feedback. We are still working
          on the AWQ quant since the quality is not good enough for now. For bnb,
          the quality should be the same. As for GPTQ, the model that was tested is
          not the best gptq quant. You can test the following branch which should
          give better results:  `gptq-4bit-128g-actorder_True` or `gptq-4bit-32g-actorder_True`
          with 32g being the most accurate one. However, the vram consumption will
          increase since these quant needs to store more quantization statistics (128g
          version with an additional 1go and 32g version with  an additional 3.5 Go)

          ![Screenshot 2023-12-18 at 3.26.50 PM.png](https://cdn-uploads.huggingface.co/production/uploads/63ce875d199b36f7552d4f07/NaL0VVgKgT2PmVGX8h6m3.png)


          More details: https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ

          '
        updatedAt: '2023-12-18T14:37:30.440Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - krumeto
    id: 6580592a365f7676d24de61e
    type: comment
  author: marcsun13
  content: 'Hi @krumeto , thanks for the awesome feedback. We are still working on
    the AWQ quant since the quality is not good enough for now. For bnb, the quality
    should be the same. As for GPTQ, the model that was tested is not the best gptq
    quant. You can test the following branch which should give better results:  `gptq-4bit-128g-actorder_True`
    or `gptq-4bit-32g-actorder_True` with 32g being the most accurate one. However,
    the vram consumption will increase since these quant needs to store more quantization
    statistics (128g version with an additional 1go and 32g version with  an additional
    3.5 Go)

    ![Screenshot 2023-12-18 at 3.26.50 PM.png](https://cdn-uploads.huggingface.co/production/uploads/63ce875d199b36f7552d4f07/NaL0VVgKgT2PmVGX8h6m3.png)


    More details: https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ

    '
  created_at: 2023-12-18 14:37:30+00:00
  edited: false
  hidden: false
  id: 6580592a365f7676d24de61e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Intuition for quality decrease after quantization
