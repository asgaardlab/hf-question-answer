!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Starlento
conflicting_files: null
created_at: 2023-12-13 01:47:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/_A1RXNvyotvhd_3NsA7YB.jpeg?w=200&h=200&f=face
      fullname: Starlento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Starlento
      type: user
    createdAt: '2023-12-13T01:47:56.000Z'
    data:
      edited: true
      editors:
      - Starlento
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9569841027259827
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/_A1RXNvyotvhd_3NsA7YB.jpeg?w=200&h=200&f=face
          fullname: Starlento
          isHf: false
          isPro: false
          name: Starlento
          type: user
        html: '<p>I am not in the society of mistral, and sorry to ask certain silly
          questions.<br>According to the blog, the value of this work is training
          a 47B model but it only cost 13B during inference. Somewhat "using a 47B
          model with 13B inference speed (but using 43B VRAM?)". Is there anything
          else I am missing?<br>Another question is related to the performance. I
          take this model something between 47B and 13B. And I justed checked the
          leaderboard, there are some 34B models have higher average score. I know
          that the real use is not equal to the scores of benchmarks but I would like
          to listen to those who have more experience using these models. Could you
          kindly provide some insights?</p>

          '
        raw: 'I am not in the society of mistral, and sorry to ask certain silly questions.

          According to the blog, the value of this work is training a 47B model but
          it only cost 13B during inference. Somewhat "using a 47B model with 13B
          inference speed (but using 43B VRAM?)". Is there anything else I am missing?

          Another question is related to the performance. I take this model something
          between 47B and 13B. And I justed checked the leaderboard, there are some
          34B models have higher average score. I know that the real use is not equal
          to the scores of benchmarks but I would like to listen to those who have
          more experience using these models. Could you kindly provide some insights?'
        updatedAt: '2023-12-13T01:56:49.619Z'
      numEdits: 5
      reactions: []
    id: 65790d4cf517872a8de31e82
    type: comment
  author: Starlento
  content: 'I am not in the society of mistral, and sorry to ask certain silly questions.

    According to the blog, the value of this work is training a 47B model but it only
    cost 13B during inference. Somewhat "using a 47B model with 13B inference speed
    (but using 43B VRAM?)". Is there anything else I am missing?

    Another question is related to the performance. I take this model something between
    47B and 13B. And I justed checked the leaderboard, there are some 34B models have
    higher average score. I know that the real use is not equal to the scores of benchmarks
    but I would like to listen to those who have more experience using these models.
    Could you kindly provide some insights?'
  created_at: 2023-12-13 01:47:56+00:00
  edited: true
  hidden: false
  id: 65790d4cf517872a8de31e82
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-12-13T16:18:14.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9611413478851318
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Starlento&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Starlento\">@<span class=\"\
          underline\">Starlento</span></a></span>\n\n\t</span></span> thats because\
          \ this is a moe model.<br>you see this is made of 8 7b param models trained\
          \ on different data. a easy way to think about it is like one trained on\
          \ science stuff, another is trained on math stuff, and another is trained\
          \ on roleplay. They are probably not trained like this but its somewhat\
          \ similar.</p>\n<p>You might say 8x7 does not equal 47, and the reason mixtral\
          \ is 47b parameters is because some of the parameters are shared so its\
          \ actually 47b.</p>\n<p>The reason it uses so much vram but at 13b inference\
          \ speed is because of its architecture.<br>You MUST load all the models\
          \ so it will take a very large amount of vram(same as a normal 47b model)</p>\n\
          <p>However, when doing actual inference you just need to use 2 of the best\
          \ models suited to answer the question. so 2x7 = 14b, so roughly 13b speed.</p>\n\
          <p>The 2 models might change depending on the instruction you input so all\
          \ the 8 models have to be preloaded before.<br>Mixtral is excellent for\
          \ its size and performs really well at instruction tasks. It has decent\
          \ benchmark scores so far but it can be easily increased much more when\
          \ the community can finetune it even further. </p>\n"
        raw: '@Starlento thats because this is a moe model.

          you see this is made of 8 7b param models trained on different data. a easy
          way to think about it is like one trained on science stuff, another is trained
          on math stuff, and another is trained on roleplay. They are probably not
          trained like this but its somewhat similar.


          You might say 8x7 does not equal 47, and the reason mixtral is 47b parameters
          is because some of the parameters are shared so its actually 47b.


          The reason it uses so much vram but at 13b inference speed is because of
          its architecture.

          You MUST load all the models so it will take a very large amount of vram(same
          as a normal 47b model)


          However, when doing actual inference you just need to use 2 of the best
          models suited to answer the question. so 2x7 = 14b, so roughly 13b speed.


          The 2 models might change depending on the instruction you input so all
          the 8 models have to be preloaded before.

          Mixtral is excellent for its size and performs really well at instruction
          tasks. It has decent benchmark scores so far but it can be easily increased
          much more when the community can finetune it even further. '
        updatedAt: '2023-12-13T16:18:14.421Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Starlento
        - jlzhou
    id: 6579d9463abcb27a1c7b4d53
    type: comment
  author: YaTharThShaRma999
  content: '@Starlento thats because this is a moe model.

    you see this is made of 8 7b param models trained on different data. a easy way
    to think about it is like one trained on science stuff, another is trained on
    math stuff, and another is trained on roleplay. They are probably not trained
    like this but its somewhat similar.


    You might say 8x7 does not equal 47, and the reason mixtral is 47b parameters
    is because some of the parameters are shared so its actually 47b.


    The reason it uses so much vram but at 13b inference speed is because of its architecture.

    You MUST load all the models so it will take a very large amount of vram(same
    as a normal 47b model)


    However, when doing actual inference you just need to use 2 of the best models
    suited to answer the question. so 2x7 = 14b, so roughly 13b speed.


    The 2 models might change depending on the instruction you input so all the 8
    models have to be preloaded before.

    Mixtral is excellent for its size and performs really well at instruction tasks.
    It has decent benchmark scores so far but it can be easily increased much more
    when the community can finetune it even further. '
  created_at: 2023-12-13 16:18:14+00:00
  edited: false
  hidden: false
  id: 6579d9463abcb27a1c7b4d53
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Discuss benefits of this work
