!!python/object:huggingface_hub.community.DiscussionWithDetails
author: edw-hug-face
conflicting_files: null
created_at: 2024-01-24 22:44:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a4ec9f444bb8d864f3d50ed2f5ecf823.svg
      fullname: Edwin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edw-hug-face
      type: user
    createdAt: '2024-01-24T22:44:50.000Z'
    data:
      edited: false
      editors:
      - edw-hug-face
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9434559941291809
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a4ec9f444bb8d864f3d50ed2f5ecf823.svg
          fullname: Edwin
          isHf: false
          isPro: false
          name: edw-hug-face
          type: user
        html: '<p>I''m considering running the 4-bit quantized Mistral 7B model on
          a standard PC with 32GB RAM. This model is famed for its efficiency on mobile
          devices, so I''m wondering about its performance on a PC. My plan is to
          train it on a small local SQL database in English. Is it realistic to expect
          smooth operation on a consumer-grade PC? Any insights or advice would be
          greatly appreciated.</p>

          '
        raw: I'm considering running the 4-bit quantized Mistral 7B model on a standard
          PC with 32GB RAM. This model is famed for its efficiency on mobile devices,
          so I'm wondering about its performance on a PC. My plan is to train it on
          a small local SQL database in English. Is it realistic to expect smooth
          operation on a consumer-grade PC? Any insights or advice would be greatly
          appreciated.
        updatedAt: '2024-01-24T22:44:50.619Z'
      numEdits: 0
      reactions: []
    id: 65b192e2e1e4031f6c24a369
    type: comment
  author: edw-hug-face
  content: I'm considering running the 4-bit quantized Mistral 7B model on a standard
    PC with 32GB RAM. This model is famed for its efficiency on mobile devices, so
    I'm wondering about its performance on a PC. My plan is to train it on a small
    local SQL database in English. Is it realistic to expect smooth operation on a
    consumer-grade PC? Any insights or advice would be greatly appreciated.
  created_at: 2024-01-24 22:44:50+00:00
  edited: false
  hidden: false
  id: 65b192e2e1e4031f6c24a369
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653cd3049107029eb004f968/G1A2zca8Yi_ii2YJtHHht.png?w=200&h=200&f=face
      fullname: "Rickard Ed\xE9n"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neph1
      type: user
    createdAt: '2024-01-25T11:39:04.000Z'
    data:
      edited: false
      editors:
      - neph1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9678353667259216
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653cd3049107029eb004f968/G1A2zca8Yi_ii2YJtHHht.png?w=200&h=200&f=face
          fullname: "Rickard Ed\xE9n"
          isHf: false
          isPro: false
          name: neph1
          type: user
        html: '<p>Running is one thing, training is another. I don''t think training
          is feasible (or even possible with RAM), but please correct me if I''m wrong.
          You can use Google Colab or some other cloud run service for finetuning.
          There are loads of scripts to get up and running. It''s even possible with
          the free instance, if the dataset is small and the context is short.</p>

          '
        raw: Running is one thing, training is another. I don't think training is
          feasible (or even possible with RAM), but please correct me if I'm wrong.
          You can use Google Colab or some other cloud run service for finetuning.
          There are loads of scripts to get up and running. It's even possible with
          the free instance, if the dataset is small and the context is short.
        updatedAt: '2024-01-25T11:39:04.225Z'
      numEdits: 0
      reactions: []
    id: 65b248581566a60bda8a9ede
    type: comment
  author: neph1
  content: Running is one thing, training is another. I don't think training is feasible
    (or even possible with RAM), but please correct me if I'm wrong. You can use Google
    Colab or some other cloud run service for finetuning. There are loads of scripts
    to get up and running. It's even possible with the free instance, if the dataset
    is small and the context is short.
  created_at: 2024-01-25 11:39:04+00:00
  edited: false
  hidden: false
  id: 65b248581566a60bda8a9ede
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 109
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: 'Running a 4-bit Quantized 7B Model on a PC: Feasibility and Insights'
