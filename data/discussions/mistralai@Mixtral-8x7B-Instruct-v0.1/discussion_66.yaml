!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MukeshSharma
conflicting_files: null
created_at: 2023-12-28 10:07:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
      fullname: Mukesh Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MukeshSharma
      type: user
    createdAt: '2023-12-28T10:07:33.000Z'
    data:
      edited: false
      editors:
      - MukeshSharma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3370930850505829
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
          fullname: Mukesh Sharma
          isHf: false
          isPro: false
          name: MukeshSharma
          type: user
        html: "<p>model = AutoModelForCausalLM.from_pretrained(<br>    model_id,<br>\
          \    device_map='auto',<br>    quantization_config=nf4_config,<br>    use_cache=True,<br>\
          \    attn_implementation=\"flash_attention_2\"<br>\u200B<br>)</p>\n<hr>\n\
          <p>ImportError                               Traceback (most recent call\
          \ last)<br>/opt/conda/lib/python3.8/site-packages/transformers/utils/import_utils.py\
          \ in _get_module(self, module_name)<br>   1381         setattr(self, name,\
          \ value)<br>-&gt; 1382         return value<br>   1383 </p>\n<p>/opt/conda/lib/python3.8/importlib/<strong>init</strong>.py\
          \ in import_module(name, package)<br>    126             level += 1<br>--&gt;\
          \ 127     return _bootstrap._gcd_import(name[level:], package, level)<br>\
          \    128 </p>\n<p>/opt/conda/lib/python3.8/importlib/_bootstrap.py in _gcd_import(name,\
          \ package, level)</p>\n<p>/opt/conda/lib/python3.8/importlib/_bootstrap.py\
          \ in <em>find_and_load(name, import</em>)</p>\n<p>/opt/conda/lib/python3.8/importlib/_bootstrap.py\
          \ in <em>find_and_load_unlocked(name, import</em>)</p>\n<p>/opt/conda/lib/python3.8/importlib/_bootstrap.py\
          \ in _load_unlocked(spec)</p>\n<p>/opt/conda/lib/python3.8/importlib/_bootstrap_external.py\
          \ in exec_module(self, module)</p>\n<p>/opt/conda/lib/python3.8/importlib/_bootstrap.py\
          \ in _call_with_frames_removed(f, *args, **kwds)</p>\n<p>/opt/conda/lib/python3.8/site-packages/transformers/models/mixtral/modeling_mixtral.py\
          \ in <br>     57 if is_flash_attn_2_available():<br>---&gt; 58     from\
          \ flash_attn import flash_attn_func, flash_attn_varlen_func<br>     59 \
          \    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input\
          \  # noqa</p>\n<p>/opt/conda/lib/python3.8/site-packages/flash_attn/<strong>init</strong>.py\
          \ in <br>      2<br>----&gt; 3 from flash_attn.flash_attn_interface import\
          \ (<br>      4     flash_attn_func,</p>\n<p>/opt/conda/lib/python3.8/site-packages/flash_attn/flash_attn_interface.py\
          \ in <br>      9 # We need to import the CUDA kernels after importing torch<br>---&gt;\
          \ 10 import flash_attn_2_cuda as flash_attn_cuda<br>     11 </p>\n<p>ImportError:\
          \ libcudart.so.12: cannot open shared object file: No such file or directory</p>\n\
          <p>The above exception was the direct cause of the following exception:</p>\n\
          <p>RuntimeError                              Traceback (most recent call\
          \ last)<br> in <br>----&gt; 1 model = AutoModelForCausalLM.from_pretrained(<br>\
          \      2     model_id,<br>      3     device_map='auto',<br>      4    \
          \ quantization_config=nf4_config,<br>      5     use_cache=True,</p>\n<p>/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)<br>\
          \    563             )<br>    564         elif type(config) in cls._model_mapping.keys():<br>--&gt;\
          \ 565             model_class = _get_model_class(config, cls._model_mapping)<br>\
          \    566             return model_class.from_pretrained(<br>    567    \
          \             pretrained_model_name_or_path, *model_args, config=config,\
          \ **hub_kwargs, **kwargs</p>\n<p>/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
          \ in _get_model_class(config, model_mapping)<br>    385<br>    386 def _get_model_class(config,\
          \ model_mapping):<br>--&gt; 387     supported_models = model_mapping[type(config)]<br>\
          \    388     if not isinstance(supported_models, (list, tuple)):<br>   \
          \ 389         return supported_models</p>\n<p>/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
          \ in <strong>getitem</strong>(self, key)<br>    738         if model_type\
          \ in self._model_mapping:<br>    739             model_name = self._model_mapping[model_type]<br>--&gt;\
          \ 740             return self._load_attr_from_module(model_type, model_name)<br>\
          \    741<br>    742         # Maybe there was several model types associated\
          \ with this config.</p>\n<p>/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
          \ in _load_attr_from_module(self, model_type, attr)<br>    752         if\
          \ module_name not in self._modules:<br>    753             self._modules[module_name]\
          \ = importlib.import_module(f\".{module_name}\", \"transformers.models\"\
          )<br>--&gt; 754         return getattribute_from_module(self._modules[module_name],\
          \ attr)<br>    755<br>    756     def keys(self):</p>\n<p>/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
          \ in getattribute_from_module(module, attr)<br>    696     if isinstance(attr,\
          \ tuple):<br>    697         return tuple(getattribute_from_module(module,\
          \ a) for a in attr)<br>--&gt; 698     if hasattr(module, attr):<br>    699\
          \         return getattr(module, attr)<br>    700     # Some of the mappings\
          \ have entries model_type -&gt; object of another model type. In that case\
          \ we try to grab the</p>\n<p>/opt/conda/lib/python3.8/site-packages/transformers/utils/import_utils.py\
          \ in <strong>getattr</strong>(self, name)<br>   1370     def <strong>getattr</strong>(self,\
          \ name: str) -&gt; Any:<br>   1371         if name in self._objects:<br>-&gt;\
          \ 1372             return self._objects[name]<br>   1373         if name\
          \ in self._modules:<br>   1374             value = self._get_module(name)</p>\n\
          <p>/opt/conda/lib/python3.8/site-packages/transformers/utils/import_utils.py\
          \ in _get_module(self, module_name)<br>   1382         return value<br>\
          \   1383<br>-&gt; 1384     def _get_module(self, module_name: str):<br>\
          \   1385         try:<br>   1386             return importlib.import_module(\"\
          .\" + module_name, self.<strong>name</strong>)</p>\n<p>RuntimeError: Failed\
          \ to import transformers.models.mixtral.modeling_mixtral because of the\
          \ following error (look up to see its traceback):<br>libcudart.so.12: cannot\
          \ open shared object file: No such file or directory</p>\n"
        raw: "model = AutoModelForCausalLM.from_pretrained(\r\n    model_id,\r\n \
          \   device_map='auto',\r\n    quantization_config=nf4_config,\r\n    use_cache=True,\r\
          \n    attn_implementation=\"flash_attention_2\"\r\n\u200B\r\n)\r\n---------------------------------------------------------------------------\r\
          \nImportError                               Traceback (most recent call\
          \ last)\r\n/opt/conda/lib/python3.8/site-packages/transformers/utils/import_utils.py\
          \ in _get_module(self, module_name)\r\n   1381         setattr(self, name,\
          \ value)\r\n-> 1382         return value\r\n   1383 \r\n\r\n/opt/conda/lib/python3.8/importlib/__init__.py\
          \ in import_module(name, package)\r\n    126             level += 1\r\n\
          --> 127     return _bootstrap._gcd_import(name[level:], package, level)\r\
          \n    128 \r\n\r\n/opt/conda/lib/python3.8/importlib/_bootstrap.py in _gcd_import(name,\
          \ package, level)\r\n\r\n/opt/conda/lib/python3.8/importlib/_bootstrap.py\
          \ in _find_and_load(name, import_)\r\n\r\n/opt/conda/lib/python3.8/importlib/_bootstrap.py\
          \ in _find_and_load_unlocked(name, import_)\r\n\r\n/opt/conda/lib/python3.8/importlib/_bootstrap.py\
          \ in _load_unlocked(spec)\r\n\r\n/opt/conda/lib/python3.8/importlib/_bootstrap_external.py\
          \ in exec_module(self, module)\r\n\r\n/opt/conda/lib/python3.8/importlib/_bootstrap.py\
          \ in _call_with_frames_removed(f, *args, **kwds)\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/models/mixtral/modeling_mixtral.py\
          \ in <module>\r\n     57 if is_flash_attn_2_available():\r\n---> 58    \
          \ from flash_attn import flash_attn_func, flash_attn_varlen_func\r\n   \
          \  59     from flash_attn.bert_padding import index_first_axis, pad_input,\
          \ unpad_input  # noqa\r\n\r\n/opt/conda/lib/python3.8/site-packages/flash_attn/__init__.py\
          \ in <module>\r\n      2 \r\n----> 3 from flash_attn.flash_attn_interface\
          \ import (\r\n      4     flash_attn_func,\r\n\r\n/opt/conda/lib/python3.8/site-packages/flash_attn/flash_attn_interface.py\
          \ in <module>\r\n      9 # We need to import the CUDA kernels after importing\
          \ torch\r\n---> 10 import flash_attn_2_cuda as flash_attn_cuda\r\n     11\
          \ \r\n\r\nImportError: libcudart.so.12: cannot open shared object file:\
          \ No such file or directory\r\n\r\nThe above exception was the direct cause\
          \ of the following exception:\r\n\r\nRuntimeError                      \
          \        Traceback (most recent call last)\r\n<ipython-input-31-77465f243bdb>\
          \ in <module>\r\n----> 1 model = AutoModelForCausalLM.from_pretrained(\r\
          \n      2     model_id,\r\n      3     device_map='auto',\r\n      4   \
          \  quantization_config=nf4_config,\r\n      5     use_cache=True,\r\n\r\n\
          /opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\
          \n    563             )\r\n    564         elif type(config) in cls._model_mapping.keys():\r\
          \n--> 565             model_class = _get_model_class(config, cls._model_mapping)\r\
          \n    566             return model_class.from_pretrained(\r\n    567   \
          \              pretrained_model_name_or_path, *model_args, config=config,\
          \ **hub_kwargs, **kwargs\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
          \ in _get_model_class(config, model_mapping)\r\n    385 \r\n    386 def\
          \ _get_model_class(config, model_mapping):\r\n--> 387     supported_models\
          \ = model_mapping[type(config)]\r\n    388     if not isinstance(supported_models,\
          \ (list, tuple)):\r\n    389         return supported_models\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
          \ in __getitem__(self, key)\r\n    738         if model_type in self._model_mapping:\r\
          \n    739             model_name = self._model_mapping[model_type]\r\n-->\
          \ 740             return self._load_attr_from_module(model_type, model_name)\r\
          \n    741 \r\n    742         # Maybe there was several model types associated\
          \ with this config.\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
          \ in _load_attr_from_module(self, model_type, attr)\r\n    752         if\
          \ module_name not in self._modules:\r\n    753             self._modules[module_name]\
          \ = importlib.import_module(f\".{module_name}\", \"transformers.models\"\
          )\r\n--> 754         return getattribute_from_module(self._modules[module_name],\
          \ attr)\r\n    755 \r\n    756     def keys(self):\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
          \ in getattribute_from_module(module, attr)\r\n    696     if isinstance(attr,\
          \ tuple):\r\n    697         return tuple(getattribute_from_module(module,\
          \ a) for a in attr)\r\n--> 698     if hasattr(module, attr):\r\n    699\
          \         return getattr(module, attr)\r\n    700     # Some of the mappings\
          \ have entries model_type -> object of another model type. In that case\
          \ we try to grab the\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/utils/import_utils.py\
          \ in __getattr__(self, name)\r\n   1370     def __getattr__(self, name:\
          \ str) -> Any:\r\n   1371         if name in self._objects:\r\n-> 1372 \
          \            return self._objects[name]\r\n   1373         if name in self._modules:\r\
          \n   1374             value = self._get_module(name)\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/utils/import_utils.py\
          \ in _get_module(self, module_name)\r\n   1382         return value\r\n\
          \   1383 \r\n-> 1384     def _get_module(self, module_name: str):\r\n  \
          \ 1385         try:\r\n   1386             return importlib.import_module(\"\
          .\" + module_name, self.__name__)\r\n\r\nRuntimeError: Failed to import\
          \ transformers.models.mixtral.modeling_mixtral because of the following\
          \ error (look up to see its traceback):\r\nlibcudart.so.12: cannot open\
          \ shared object file: No such file or directory\r\n"
        updatedAt: '2023-12-28T10:07:33.415Z'
      numEdits: 0
      reactions: []
    id: 658d48e5f893598fcaf08a16
    type: comment
  author: MukeshSharma
  content: "model = AutoModelForCausalLM.from_pretrained(\r\n    model_id,\r\n   \
    \ device_map='auto',\r\n    quantization_config=nf4_config,\r\n    use_cache=True,\r\
    \n    attn_implementation=\"flash_attention_2\"\r\n\u200B\r\n)\r\n---------------------------------------------------------------------------\r\
    \nImportError                               Traceback (most recent call last)\r\
    \n/opt/conda/lib/python3.8/site-packages/transformers/utils/import_utils.py in\
    \ _get_module(self, module_name)\r\n   1381         setattr(self, name, value)\r\
    \n-> 1382         return value\r\n   1383 \r\n\r\n/opt/conda/lib/python3.8/importlib/__init__.py\
    \ in import_module(name, package)\r\n    126             level += 1\r\n--> 127\
    \     return _bootstrap._gcd_import(name[level:], package, level)\r\n    128 \r\
    \n\r\n/opt/conda/lib/python3.8/importlib/_bootstrap.py in _gcd_import(name, package,\
    \ level)\r\n\r\n/opt/conda/lib/python3.8/importlib/_bootstrap.py in _find_and_load(name,\
    \ import_)\r\n\r\n/opt/conda/lib/python3.8/importlib/_bootstrap.py in _find_and_load_unlocked(name,\
    \ import_)\r\n\r\n/opt/conda/lib/python3.8/importlib/_bootstrap.py in _load_unlocked(spec)\r\
    \n\r\n/opt/conda/lib/python3.8/importlib/_bootstrap_external.py in exec_module(self,\
    \ module)\r\n\r\n/opt/conda/lib/python3.8/importlib/_bootstrap.py in _call_with_frames_removed(f,\
    \ *args, **kwds)\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/models/mixtral/modeling_mixtral.py\
    \ in <module>\r\n     57 if is_flash_attn_2_available():\r\n---> 58     from flash_attn\
    \ import flash_attn_func, flash_attn_varlen_func\r\n     59     from flash_attn.bert_padding\
    \ import index_first_axis, pad_input, unpad_input  # noqa\r\n\r\n/opt/conda/lib/python3.8/site-packages/flash_attn/__init__.py\
    \ in <module>\r\n      2 \r\n----> 3 from flash_attn.flash_attn_interface import\
    \ (\r\n      4     flash_attn_func,\r\n\r\n/opt/conda/lib/python3.8/site-packages/flash_attn/flash_attn_interface.py\
    \ in <module>\r\n      9 # We need to import the CUDA kernels after importing\
    \ torch\r\n---> 10 import flash_attn_2_cuda as flash_attn_cuda\r\n     11 \r\n\
    \r\nImportError: libcudart.so.12: cannot open shared object file: No such file\
    \ or directory\r\n\r\nThe above exception was the direct cause of the following\
    \ exception:\r\n\r\nRuntimeError                              Traceback (most\
    \ recent call last)\r\n<ipython-input-31-77465f243bdb> in <module>\r\n----> 1\
    \ model = AutoModelForCausalLM.from_pretrained(\r\n      2     model_id,\r\n \
    \     3     device_map='auto',\r\n      4     quantization_config=nf4_config,\r\
    \n      5     use_cache=True,\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\
    \n    563             )\r\n    564         elif type(config) in cls._model_mapping.keys():\r\
    \n--> 565             model_class = _get_model_class(config, cls._model_mapping)\r\
    \n    566             return model_class.from_pretrained(\r\n    567         \
    \        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,\
    \ **kwargs\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
    \ in _get_model_class(config, model_mapping)\r\n    385 \r\n    386 def _get_model_class(config,\
    \ model_mapping):\r\n--> 387     supported_models = model_mapping[type(config)]\r\
    \n    388     if not isinstance(supported_models, (list, tuple)):\r\n    389 \
    \        return supported_models\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
    \ in __getitem__(self, key)\r\n    738         if model_type in self._model_mapping:\r\
    \n    739             model_name = self._model_mapping[model_type]\r\n--> 740\
    \             return self._load_attr_from_module(model_type, model_name)\r\n \
    \   741 \r\n    742         # Maybe there was several model types associated with\
    \ this config.\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
    \ in _load_attr_from_module(self, model_type, attr)\r\n    752         if module_name\
    \ not in self._modules:\r\n    753             self._modules[module_name] = importlib.import_module(f\"\
    .{module_name}\", \"transformers.models\")\r\n--> 754         return getattribute_from_module(self._modules[module_name],\
    \ attr)\r\n    755 \r\n    756     def keys(self):\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\
    \ in getattribute_from_module(module, attr)\r\n    696     if isinstance(attr,\
    \ tuple):\r\n    697         return tuple(getattribute_from_module(module, a)\
    \ for a in attr)\r\n--> 698     if hasattr(module, attr):\r\n    699         return\
    \ getattr(module, attr)\r\n    700     # Some of the mappings have entries model_type\
    \ -> object of another model type. In that case we try to grab the\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/utils/import_utils.py\
    \ in __getattr__(self, name)\r\n   1370     def __getattr__(self, name: str) ->\
    \ Any:\r\n   1371         if name in self._objects:\r\n-> 1372             return\
    \ self._objects[name]\r\n   1373         if name in self._modules:\r\n   1374\
    \             value = self._get_module(name)\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/utils/import_utils.py\
    \ in _get_module(self, module_name)\r\n   1382         return value\r\n   1383\
    \ \r\n-> 1384     def _get_module(self, module_name: str):\r\n   1385        \
    \ try:\r\n   1386             return importlib.import_module(\".\" + module_name,\
    \ self.__name__)\r\n\r\nRuntimeError: Failed to import transformers.models.mixtral.modeling_mixtral\
    \ because of the following error (look up to see its traceback):\r\nlibcudart.so.12:\
    \ cannot open shared object file: No such file or directory\r\n"
  created_at: 2023-12-28 10:07:33+00:00
  edited: false
  hidden: false
  id: 658d48e5f893598fcaf08a16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
      fullname: Mukesh Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MukeshSharma
      type: user
    createdAt: '2023-12-28T10:24:02.000Z'
    data:
      edited: false
      editors:
      - MukeshSharma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5912688374519348
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
          fullname: Mukesh Sharma
          isHf: false
          isPro: false
          name: MukeshSharma
          type: user
        html: '<p>I have created a venv where i am using cuda 11.8 , I installed it
          manually using<br> pip3 install numpy --pre torch torchvision torchaudio
          --force-reinstall --index-url <a rel="nofollow" href="https://download.pytorch.org/whl/nightly/cu118">https://download.pytorch.org/whl/nightly/cu118</a></p>

          <p>my torch is using cuda 11.8  but when doing nvidia-smi its still shows
          cuda version 11.4. </p>

          '
        raw: "I have created a venv where i am using cuda 11.8 , I installed it manually\
          \ using\n pip3 install numpy --pre torch torchvision torchaudio --force-reinstall\
          \ --index-url https://download.pytorch.org/whl/nightly/cu118\n\nmy torch\
          \ is using cuda 11.8  but when doing nvidia-smi its still shows cuda version\
          \ 11.4. "
        updatedAt: '2023-12-28T10:24:02.124Z'
      numEdits: 0
      reactions: []
    id: 658d4cc29b22f4d7964c93bf
    type: comment
  author: MukeshSharma
  content: "I have created a venv where i am using cuda 11.8 , I installed it manually\
    \ using\n pip3 install numpy --pre torch torchvision torchaudio --force-reinstall\
    \ --index-url https://download.pytorch.org/whl/nightly/cu118\n\nmy torch is using\
    \ cuda 11.8  but when doing nvidia-smi its still shows cuda version 11.4. "
  created_at: 2023-12-28 10:24:02+00:00
  edited: false
  hidden: false
  id: 658d4cc29b22f4d7964c93bf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 66
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: 'Failed to import transformers.models.mixtral.modeling_mixtral because of the
  following error (look up to see its traceback): libcudart.so.12: cannot open shared
  object file: No such file or directory'
