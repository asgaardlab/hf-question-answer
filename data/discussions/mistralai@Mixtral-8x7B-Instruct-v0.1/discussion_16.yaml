!!python/object:huggingface_hub.community.DiscussionWithDetails
author: harryneal
conflicting_files: null
created_at: 2023-12-12 13:11:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa773b23f21c7718fef48894097e4d51.svg
      fullname: Harry
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harryneal
      type: user
    createdAt: '2023-12-12T13:11:46.000Z'
    data:
      edited: false
      editors:
      - harryneal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6634705662727356
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa773b23f21c7718fef48894097e4d51.svg
          fullname: Harry
          isHf: false
          isPro: false
          name: harryneal
          type: user
        html: "<p>I'm attempting to deploy to a Sagemaker endpoint in eu-west-1 and\
          \ get the following error:</p>\n<pre><code>ValueError: Unsupported model\
          \ type mixtral\n</code></pre>\n<p>I'm using the following set-up:</p>\n\
          <pre><code>endpoint_name = \"mixtral-8x7B-instruct-quantized\"\n\n# Hub\
          \ Model configuration\nhub = {\n'HF_MODEL_ID':'mistralai/Mixtral-8x7B-Instruct-v0.1',\n\
          'SM_NUM_GPUS': json.dumps(1),\n'HF_MODEL_QUANTIZE': \"bitsandbytes\", \n\
          }\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n\
          image_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"\
          ),\nenv=hub,\nrole=role,\nname=endpoint_name\n)\n\n# deploy model to SageMaker\
          \ Inference\npredictor = huggingface_model.deploy(\ninitial_instance_count=1,\n\
          instance_type=\"ml.g5.2xlarge\",\ncontainer_startup_health_check_timeout=600,\n\
          endpoint_name = endpoint_name\n)\n</code></pre>\n<p>Any advice on this?</p>\n\
          <p>Also, for the record, is there any flexibility in the quantization settings\
          \ in terms of type and bit length?</p>\n"
        raw: "I'm attempting to deploy to a Sagemaker endpoint in eu-west-1 and get\
          \ the following error:\r\n\r\n    ValueError: Unsupported model type mixtral\r\
          \n\r\nI'm using the following set-up:\r\n\r\n\r\n    endpoint_name = \"\
          mixtral-8x7B-instruct-quantized\"\r\n\r\n    # Hub Model configuration\r\
          \n    hub = {\r\n\t'HF_MODEL_ID':'mistralai/Mixtral-8x7B-Instruct-v0.1',\r\
          \n\t'SM_NUM_GPUS': json.dumps(1),\r\n    'HF_MODEL_QUANTIZE': \"bitsandbytes\"\
          , \r\n    }\r\n\r\n    # create Hugging Face Model Class\r\n    huggingface_model\
          \ = HuggingFaceModel(\r\n\timage_uri=get_huggingface_llm_image_uri(\"huggingface\"\
          ,version=\"1.1.0\"),\r\n\tenv=hub,\r\n\trole=role,\r\n    name=endpoint_name\r\
          \n    )\r\n\r\n    # deploy model to SageMaker Inference\r\n    predictor\
          \ = huggingface_model.deploy(\r\n\tinitial_instance_count=1,\r\n\tinstance_type=\"\
          ml.g5.2xlarge\",\r\n\tcontainer_startup_health_check_timeout=600,\r\n  \
          \  endpoint_name = endpoint_name\r\n    )\r\n\r\nAny advice on this?\r\n\
          \r\nAlso, for the record, is there any flexibility in the quantization settings\
          \ in terms of type and bit length?"
        updatedAt: '2023-12-12T13:11:46.651Z'
      numEdits: 0
      reactions: []
    id: 65785c12dd2996f01af9914a
    type: comment
  author: harryneal
  content: "I'm attempting to deploy to a Sagemaker endpoint in eu-west-1 and get\
    \ the following error:\r\n\r\n    ValueError: Unsupported model type mixtral\r\
    \n\r\nI'm using the following set-up:\r\n\r\n\r\n    endpoint_name = \"mixtral-8x7B-instruct-quantized\"\
    \r\n\r\n    # Hub Model configuration\r\n    hub = {\r\n\t'HF_MODEL_ID':'mistralai/Mixtral-8x7B-Instruct-v0.1',\r\
    \n\t'SM_NUM_GPUS': json.dumps(1),\r\n    'HF_MODEL_QUANTIZE': \"bitsandbytes\"\
    , \r\n    }\r\n\r\n    # create Hugging Face Model Class\r\n    huggingface_model\
    \ = HuggingFaceModel(\r\n\timage_uri=get_huggingface_llm_image_uri(\"huggingface\"\
    ,version=\"1.1.0\"),\r\n\tenv=hub,\r\n\trole=role,\r\n    name=endpoint_name\r\
    \n    )\r\n\r\n    # deploy model to SageMaker Inference\r\n    predictor = huggingface_model.deploy(\r\
    \n\tinitial_instance_count=1,\r\n\tinstance_type=\"ml.g5.2xlarge\",\r\n\tcontainer_startup_health_check_timeout=600,\r\
    \n    endpoint_name = endpoint_name\r\n    )\r\n\r\nAny advice on this?\r\n\r\n\
    Also, for the record, is there any flexibility in the quantization settings in\
    \ terms of type and bit length?"
  created_at: 2023-12-12 13:11:46+00:00
  edited: false
  hidden: false
  id: 65785c12dd2996f01af9914a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-12T13:48:43.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7012900114059448
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;harryneal&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/harryneal\"\
          >@<span class=\"underline\">harryneal</span></a></span>\n\n\t</span></span><br>Please\
          \ see a similar issue here: <a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/13\"\
          >https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/13</a>\
          \ and let us know if this helps</p>\n"
        raw: "Hi @harryneal \nPlease see a similar issue here: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/13\
          \ and let us know if this helps"
        updatedAt: '2023-12-12T13:48:43.624Z'
      numEdits: 0
      reactions: []
    id: 657864bbc37954680abb9d43
    type: comment
  author: ybelkada
  content: "Hi @harryneal \nPlease see a similar issue here: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/13\
    \ and let us know if this helps"
  created_at: 2023-12-12 13:48:43+00:00
  edited: false
  hidden: false
  id: 657864bbc37954680abb9d43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa773b23f21c7718fef48894097e4d51.svg
      fullname: Harry
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harryneal
      type: user
    createdAt: '2023-12-12T13:58:52.000Z'
    data:
      edited: false
      editors:
      - harryneal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8748052716255188
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa773b23f21c7718fef48894097e4d51.svg
          fullname: Harry
          isHf: false
          isPro: false
          name: harryneal
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>,<br>I\
          \ believe these are two separate issues as I am attempting to deploy via\
          \ an AWS Sagemaker endpoint using a HuggingFace container.</p>\n"
        raw: 'Hi @ybelkada,

          I believe these are two separate issues as I am attempting to deploy via
          an AWS Sagemaker endpoint using a HuggingFace container.'
        updatedAt: '2023-12-12T13:58:52.512Z'
      numEdits: 0
      reactions: []
    id: 6578671cc37954680abc2846
    type: comment
  author: harryneal
  content: 'Hi @ybelkada,

    I believe these are two separate issues as I am attempting to deploy via an AWS
    Sagemaker endpoint using a HuggingFace container.'
  created_at: 2023-12-12 13:58:52+00:00
  edited: false
  hidden: false
  id: 6578671cc37954680abc2846
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa773b23f21c7718fef48894097e4d51.svg
      fullname: Harry
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harryneal
      type: user
    createdAt: '2023-12-12T14:39:23.000Z'
    data:
      edited: false
      editors:
      - harryneal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9200266003608704
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa773b23f21c7718fef48894097e4d51.svg
          fullname: Harry
          isHf: false
          isPro: false
          name: harryneal
          type: user
        html: '<p>I believe the problem is that on huggingface llm image URI v1.1.0,
          the transformers version is 4.33.3, but according to the HF Mixtral blog
          it needs to be the latest version (4.36.0).</p>

          <p><a href="https://huggingface.co/blog/mixtral">https://huggingface.co/blog/mixtral</a></p>

          <p>Maybe this is a case of waiting a few days for the container to be updated
          but in the meantime if anyone can help out with creating a custom one myself
          it would be much appreciated!</p>

          '
        raw: 'I believe the problem is that on huggingface llm image URI v1.1.0, the
          transformers version is 4.33.3, but according to the HF Mixtral blog it
          needs to be the latest version (4.36.0).


          https://huggingface.co/blog/mixtral


          Maybe this is a case of waiting a few days for the container to be updated
          but in the meantime if anyone can help out with creating a custom one myself
          it would be much appreciated!'
        updatedAt: '2023-12-12T14:39:23.016Z'
      numEdits: 0
      reactions: []
    id: 6578709bd6ac68e25d069a9f
    type: comment
  author: harryneal
  content: 'I believe the problem is that on huggingface llm image URI v1.1.0, the
    transformers version is 4.33.3, but according to the HF Mixtral blog it needs
    to be the latest version (4.36.0).


    https://huggingface.co/blog/mixtral


    Maybe this is a case of waiting a few days for the container to be updated but
    in the meantime if anyone can help out with creating a custom one myself it would
    be much appreciated!'
  created_at: 2023-12-12 14:39:23+00:00
  edited: false
  hidden: false
  id: 6578709bd6ac68e25d069a9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa773b23f21c7718fef48894097e4d51.svg
      fullname: Harry
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harryneal
      type: user
    createdAt: '2023-12-12T16:16:41.000Z'
    data:
      edited: false
      editors:
      - harryneal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.747593343257904
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa773b23f21c7718fef48894097e4d51.svg
          fullname: Harry
          isHf: false
          isPro: false
          name: harryneal
          type: user
        html: '<p>Resolved -  solution in link below, thanks Phil.</p>

          <p><a rel="nofollow" href="https://www.philschmid.de/sagemaker-deploy-mixtral">https://www.philschmid.de/sagemaker-deploy-mixtral</a></p>

          '
        raw: 'Resolved -  solution in link below, thanks Phil.


          https://www.philschmid.de/sagemaker-deploy-mixtral'
        updatedAt: '2023-12-12T16:16:41.106Z'
      numEdits: 0
      reactions: []
      relatedEventId: 657887691e0b436ae7860bed
    id: 657887691e0b436ae7860be8
    type: comment
  author: harryneal
  content: 'Resolved -  solution in link below, thanks Phil.


    https://www.philschmid.de/sagemaker-deploy-mixtral'
  created_at: 2023-12-12 16:16:41+00:00
  edited: false
  hidden: false
  id: 657887691e0b436ae7860be8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/aa773b23f21c7718fef48894097e4d51.svg
      fullname: Harry
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harryneal
      type: user
    createdAt: '2023-12-12T16:16:41.000Z'
    data:
      status: closed
    id: 657887691e0b436ae7860bed
    type: status-change
  author: harryneal
  created_at: 2023-12-12 16:16:41+00:00
  id: 657887691e0b436ae7860bed
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-12T16:20:07.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9794006943702698
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Thanks very much <span data-props=\"{&quot;user&quot;:&quot;harryneal&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/harryneal\"\
          >@<span class=\"underline\">harryneal</span></a></span>\n\n\t</span></span>\
          \ !</p>\n"
        raw: Thanks very much @harryneal !
        updatedAt: '2023-12-12T16:20:07.187Z'
      numEdits: 0
      reactions: []
    id: 657888379c5c26f96c0835ec
    type: comment
  author: ybelkada
  content: Thanks very much @harryneal !
  created_at: 2023-12-12 16:20:07+00:00
  edited: false
  hidden: false
  id: 657888379c5c26f96c0835ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66ff8c0a8675fb01ade951ac70db4c0b.svg
      fullname: SoraHeart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Soraheart1988
      type: user
    createdAt: '2024-01-02T03:28:06.000Z'
    data:
      edited: false
      editors:
      - Soraheart1988
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7431201338768005
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66ff8c0a8675fb01ade951ac70db4c0b.svg
          fullname: SoraHeart
          isHf: false
          isPro: false
          name: Soraheart1988
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;harryneal&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/harryneal\">@<span class=\"\
          underline\">harryneal</span></a></span>\n\n\t</span></span> , the link <a\
          \ rel=\"nofollow\" href=\"https://www.philschmid.de/sagemaker-deploy-mixtral\"\
          >https://www.philschmid.de/sagemaker-deploy-mixtral</a> requires instance\
          \ type of p4d.24xlarge. I am also trying to deploy using sagemaker.<br>Have\
          \ you deployed with quantized version using smaller instance size?</p>\n"
        raw: "@harryneal , the link https://www.philschmid.de/sagemaker-deploy-mixtral\
          \ requires instance type of p4d.24xlarge. I am also trying to deploy using\
          \ sagemaker. \nHave you deployed with quantized version using smaller instance\
          \ size?"
        updatedAt: '2024-01-02T03:28:06.818Z'
      numEdits: 0
      reactions: []
    id: 659382c6dbdeb5bf078d5345
    type: comment
  author: Soraheart1988
  content: "@harryneal , the link https://www.philschmid.de/sagemaker-deploy-mixtral\
    \ requires instance type of p4d.24xlarge. I am also trying to deploy using sagemaker.\
    \ \nHave you deployed with quantized version using smaller instance size?"
  created_at: 2024-01-02 03:28:06+00:00
  edited: false
  hidden: false
  id: 659382c6dbdeb5bf078d5345
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa773b23f21c7718fef48894097e4d51.svg
      fullname: Harry
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harryneal
      type: user
    createdAt: '2024-01-05T11:06:24.000Z'
    data:
      edited: false
      editors:
      - harryneal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9058729410171509
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa773b23f21c7718fef48894097e4d51.svg
          fullname: Harry
          isHf: false
          isPro: false
          name: harryneal
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Soraheart1988&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Soraheart1988\"\
          >@<span class=\"underline\">Soraheart1988</span></a></span>\n\n\t</span></span>\
          \ , no I haven't yet managed to deploy Mixtral to a smaller instance, as\
          \ the latest huggingface inference container doesn't support its quantization\
          \ when deploying to a Sagemaker endpoint.  I am hoping the legends at huggingface\
          \ are working on it and will release a new update soon.</p>\n<p>For reference\
          \ this is the latest inference container:<br><a rel=\"nofollow\" href=\"\
          https://github.com/aws/deep-learning-containers/releases/tag/v1.0-hf-tgi-1.3.3-pt-2.1.1-inf-gpu-py310\"\
          >https://github.com/aws/deep-learning-containers/releases/tag/v1.0-hf-tgi-1.3.3-pt-2.1.1-inf-gpu-py310</a></p>\n\
          <p>and this is a list of current and previous containers that I'm keeping\
          \ an eye in case of an update:<br><a rel=\"nofollow\" href=\"https://github.com/aws/deep-learning-containers/releases?q=tgi&amp;expanded=true\"\
          >https://github.com/aws/deep-learning-containers/releases?q=tgi&amp;expanded=true</a></p>\n"
        raw: 'Hi @Soraheart1988 , no I haven''t yet managed to deploy Mixtral to a
          smaller instance, as the latest huggingface inference container doesn''t
          support its quantization when deploying to a Sagemaker endpoint.  I am hoping
          the legends at huggingface are working on it and will release a new update
          soon.


          For reference this is the latest inference container:

          https://github.com/aws/deep-learning-containers/releases/tag/v1.0-hf-tgi-1.3.3-pt-2.1.1-inf-gpu-py310


          and this is a list of current and previous containers that I''m keeping
          an eye in case of an update:

          https://github.com/aws/deep-learning-containers/releases?q=tgi&expanded=true'
        updatedAt: '2024-01-05T11:06:24.887Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6597e2b1929cd840d807d3be
    id: 6597e2b0929cd840d807d3b9
    type: comment
  author: harryneal
  content: 'Hi @Soraheart1988 , no I haven''t yet managed to deploy Mixtral to a smaller
    instance, as the latest huggingface inference container doesn''t support its quantization
    when deploying to a Sagemaker endpoint.  I am hoping the legends at huggingface
    are working on it and will release a new update soon.


    For reference this is the latest inference container:

    https://github.com/aws/deep-learning-containers/releases/tag/v1.0-hf-tgi-1.3.3-pt-2.1.1-inf-gpu-py310


    and this is a list of current and previous containers that I''m keeping an eye
    in case of an update:

    https://github.com/aws/deep-learning-containers/releases?q=tgi&expanded=true'
  created_at: 2024-01-05 11:06:24+00:00
  edited: false
  hidden: false
  id: 6597e2b0929cd840d807d3b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/aa773b23f21c7718fef48894097e4d51.svg
      fullname: Harry
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harryneal
      type: user
    createdAt: '2024-01-05T11:06:25.000Z'
    data:
      status: open
    id: 6597e2b1929cd840d807d3be
    type: status-change
  author: harryneal
  created_at: 2024-01-05 11:06:25+00:00
  id: 6597e2b1929cd840d807d3be
  new_status: open
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: 'Error when deploying sagemaker endpoint: Unsupported model type mixtral'
