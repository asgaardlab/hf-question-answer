!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bweinstein123
conflicting_files: null
created_at: 2024-01-10 11:26:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f539452b6c4596a3390c4d05f0ca1d01.svg
      fullname: Berry Weinstein
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bweinstein123
      type: user
    createdAt: '2024-01-10T11:26:54.000Z'
    data:
      edited: true
      editors:
      - bweinstein123
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.925078809261322
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f539452b6c4596a3390c4d05f0ca1d01.svg
          fullname: Berry Weinstein
          isHf: false
          isPro: false
          name: bweinstein123
          type: user
        html: '<p>Hello,</p>

          <p>I rented an AWS p3.2xlarge machine with ubuntu 18.04 and install transformers.<br>Loading
          the model directly and using the pipeline both are getting killed by the
          process after reaching 26% of loading the checkpoint shards.</p>

          <p>Can you please share the requirements for the instance on AWS that can
          run this model? Will be very helpful<br>Pointing to specific instance and
          AMI will be even more helpful. Currently I''m using "Deep Learning AMI (Ubuntu
          18.04) Version 56.1" AMI</p>

          '
        raw: 'Hello,


          I rented an AWS p3.2xlarge machine with ubuntu 18.04 and install transformers.

          Loading the model directly and using the pipeline both are getting killed
          by the process after reaching 26% of loading the checkpoint shards.


          Can you please share the requirements for the instance on AWS that can run
          this model? Will be very helpful

          Pointing to specific instance and AMI will be even more helpful. Currently
          I''m using "Deep Learning AMI (Ubuntu 18.04) Version 56.1" AMI'
        updatedAt: '2024-01-10T11:30:40.124Z'
      numEdits: 1
      reactions: []
    id: 659e7efe830634e3d35deedc
    type: comment
  author: bweinstein123
  content: 'Hello,


    I rented an AWS p3.2xlarge machine with ubuntu 18.04 and install transformers.

    Loading the model directly and using the pipeline both are getting killed by the
    process after reaching 26% of loading the checkpoint shards.


    Can you please share the requirements for the instance on AWS that can run this
    model? Will be very helpful

    Pointing to specific instance and AMI will be even more helpful. Currently I''m
    using "Deep Learning AMI (Ubuntu 18.04) Version 56.1" AMI'
  created_at: 2024-01-10 11:26:54+00:00
  edited: true
  hidden: false
  id: 659e7efe830634e3d35deedc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f539452b6c4596a3390c4d05f0ca1d01.svg
      fullname: Berry Weinstein
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bweinstein123
      type: user
    createdAt: '2024-01-10T11:27:49.000Z'
    data:
      edited: true
      editors:
      - bweinstein123
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4779159128665924
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f539452b6c4596a3390c4d05f0ca1d01.svg
          fullname: Berry Weinstein
          isHf: false
          isPro: false
          name: bweinstein123
          type: user
        html: "<p>Adding the failure:</p>\n<p>&lt;/&gt;<br>from transformers import\
          \ AutoTokenizer, AutoModelForCausalLM</p>\n<p>In [3]: tokenizer = AutoTokenizer.from_pretrained(\"\
          mistralai/Mixtral-8x7B-Instruct-v0.1\")<br>   ...: model = AutoModelForCausalLM.from_pretrained(\"\
          mistralai/Mixtral-8x7B-Instruct-v0.1\")<br>Loading checkpoint shards:  26%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A                 \
          \                                            | 5/19 [02:21&lt;06:40, 28.60s/it]Killed<br>(pytorch_p38)\
          \ ubuntu@ip-172-23-1-218:~$ ipython<br>Python 3.8.12 | packaged by conda-forge\
          \ | (default, Oct 12 2021, 21:59:51)<br>Type 'copyright', 'credits' or 'license'\
          \ for more information<br>IPython 7.31.1 -- An enhanced Interactive Python.\
          \ Type '?' for help.</p>\n<p>In [1]: # Use a pipeline as a high-level helper<br>\
          \   ...: from transformers import pipeline<br>   ...:<br>   ...: pipe =\
          \ pipeline(\"text-generation\", model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
          )<br>Loading checkpoint shards:  26%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u258A                                               \
          \              | 5/19 [02:11&lt;06:00, 25.75s/it]Killed<br>&lt;/&gt;</p>\n"
        raw: "Adding the failure:\n\n</>\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\n\nIn [3]: tokenizer = AutoTokenizer.from_pretrained(\"\
          mistralai/Mixtral-8x7B-Instruct-v0.1\")\n   ...: model = AutoModelForCausalLM.from_pretrained(\"\
          mistralai/Mixtral-8x7B-Instruct-v0.1\")\nLoading checkpoint shards:  26%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A                 \
          \                                            | 5/19 [02:21<06:40, 28.60s/it]Killed\n\
          (pytorch_p38) ubuntu@ip-172-23-1-218:~$ ipython\nPython 3.8.12 | packaged\
          \ by conda-forge | (default, Oct 12 2021, 21:59:51) \nType 'copyright',\
          \ 'credits' or 'license' for more information\nIPython 7.31.1 -- An enhanced\
          \ Interactive Python. Type '?' for help.\n\nIn [1]: # Use a pipeline as\
          \ a high-level helper\n   ...: from transformers import pipeline\n   ...:\
          \ \n   ...: pipe = pipeline(\"text-generation\", model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
          )\nLoading checkpoint shards:  26%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u258A                                                     \
          \        | 5/19 [02:11<06:00, 25.75s/it]Killed\n</>"
        updatedAt: '2024-01-10T11:28:03.720Z'
      numEdits: 1
      reactions: []
    id: 659e7f35fc468b9071193d31
    type: comment
  author: bweinstein123
  content: "Adding the failure:\n\n</>\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
    \nIn [3]: tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
    )\n   ...: model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
    )\nLoading checkpoint shards:  26%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u258A                                                             | 5/19 [02:21<06:40,\
    \ 28.60s/it]Killed\n(pytorch_p38) ubuntu@ip-172-23-1-218:~$ ipython\nPython 3.8.12\
    \ | packaged by conda-forge | (default, Oct 12 2021, 21:59:51) \nType 'copyright',\
    \ 'credits' or 'license' for more information\nIPython 7.31.1 -- An enhanced Interactive\
    \ Python. Type '?' for help.\n\nIn [1]: # Use a pipeline as a high-level helper\n\
    \   ...: from transformers import pipeline\n   ...: \n   ...: pipe = pipeline(\"\
    text-generation\", model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\nLoading checkpoint\
    \ shards:  26%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A           \
    \                                                  | 5/19 [02:11<06:00, 25.75s/it]Killed\n\
    </>"
  created_at: 2024-01-10 11:27:49+00:00
  edited: true
  hidden: false
  id: 659e7f35fc468b9071193d31
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/dvtb8IEaiA32TYdzPYkyJ.png?w=200&h=200&f=face
      fullname: Andre Niyongabo Rubungo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreniyongabo
      type: user
    createdAt: '2024-01-17T19:08:05.000Z'
    data:
      edited: false
      editors:
      - andreniyongabo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.754997193813324
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/dvtb8IEaiA32TYdzPYkyJ.png?w=200&h=200&f=face
          fullname: Andre Niyongabo Rubungo
          isHf: false
          isPro: false
          name: andreniyongabo
          type: user
        html: '<p>Have you also tried loading it in half-precision by adding <code>torch_dtype=torch.float16</code>
          in your pipeline? Something like this <code>pipe = pipeline("text-generation",
          model="mistralai/Mixtral-8x7B-Instruct-v0.1", torch_dtype=torch.float16)</code></p>

          '
        raw: Have you also tried loading it in half-precision by adding `torch_dtype=torch.float16`
          in your pipeline? Something like this `pipe = pipeline("text-generation",
          model="mistralai/Mixtral-8x7B-Instruct-v0.1", torch_dtype=torch.float16)`
        updatedAt: '2024-01-17T19:08:05.740Z'
      numEdits: 0
      reactions: []
    id: 65a825959db09e1fc11489fa
    type: comment
  author: andreniyongabo
  content: Have you also tried loading it in half-precision by adding `torch_dtype=torch.float16`
    in your pipeline? Something like this `pipe = pipeline("text-generation", model="mistralai/Mixtral-8x7B-Instruct-v0.1",
    torch_dtype=torch.float16)`
  created_at: 2024-01-17 19:08:05+00:00
  edited: false
  hidden: false
  id: 65a825959db09e1fc11489fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454631806728ff79a3829e8/Im1TjNIXt7J5Am4aW6FZ-.jpeg?w=200&h=200&f=face
      fullname: AI Geek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aigeek0x0
      type: user
    createdAt: '2024-01-19T00:41:39.000Z'
    data:
      edited: false
      editors:
      - aigeek0x0
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8333988785743713
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454631806728ff79a3829e8/Im1TjNIXt7J5Am4aW6FZ-.jpeg?w=200&h=200&f=face
          fullname: AI Geek
          isHf: false
          isPro: false
          name: aigeek0x0
          type: user
        html: '<p>Have you considered deploying on other cloud platforms. I am using
          Runpod and it''s working great. I have put together a guide here if you
          are interested: <a rel="nofollow" href="https://github.com/aigeek0x0/radiantloom-ai/blob/main/mixtral-8x7b-instruct-v-0.1-runpod-template.md">https://github.com/aigeek0x0/radiantloom-ai/blob/main/mixtral-8x7b-instruct-v-0.1-runpod-template.md</a></p>

          '
        raw: 'Have you considered deploying on other cloud platforms. I am using Runpod
          and it''s working great. I have put together a guide here if you are interested:
          https://github.com/aigeek0x0/radiantloom-ai/blob/main/mixtral-8x7b-instruct-v-0.1-runpod-template.md'
        updatedAt: '2024-01-19T00:41:39.547Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - bweinstein123
    id: 65a9c5434d12c80c3d1ba0d4
    type: comment
  author: aigeek0x0
  content: 'Have you considered deploying on other cloud platforms. I am using Runpod
    and it''s working great. I have put together a guide here if you are interested:
    https://github.com/aigeek0x0/radiantloom-ai/blob/main/mixtral-8x7b-instruct-v-0.1-runpod-template.md'
  created_at: 2024-01-19 00:41:39+00:00
  edited: false
  hidden: false
  id: 65a9c5434d12c80c3d1ba0d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f539452b6c4596a3390c4d05f0ca1d01.svg
      fullname: Berry Weinstein
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bweinstein123
      type: user
    createdAt: '2024-01-21T07:48:34.000Z'
    data:
      edited: false
      editors:
      - bweinstein123
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8336801528930664
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f539452b6c4596a3390c4d05f0ca1d01.svg
          fullname: Berry Weinstein
          isHf: false
          isPro: false
          name: bweinstein123
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;aigeek0x0&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/aigeek0x0\">@<span class=\"\
          underline\">aigeek0x0</span></a></span>\n\n\t</span></span> have you performed\
          \ fine-tuning using one A100 80GB runpod?</p>\n"
        raw: '@aigeek0x0 have you performed fine-tuning using one A100 80GB runpod?'
        updatedAt: '2024-01-21T07:48:34.963Z'
      numEdits: 0
      reactions: []
    id: 65accc52043d53781a91b6e3
    type: comment
  author: bweinstein123
  content: '@aigeek0x0 have you performed fine-tuning using one A100 80GB runpod?'
  created_at: 2024-01-21 07:48:34+00:00
  edited: false
  hidden: false
  id: 65accc52043d53781a91b6e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454631806728ff79a3829e8/Im1TjNIXt7J5Am4aW6FZ-.jpeg?w=200&h=200&f=face
      fullname: AI Geek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aigeek0x0
      type: user
    createdAt: '2024-01-21T16:05:39.000Z'
    data:
      edited: false
      editors:
      - aigeek0x0
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8346287608146667
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454631806728ff79a3829e8/Im1TjNIXt7J5Am4aW6FZ-.jpeg?w=200&h=200&f=face
          fullname: AI Geek
          isHf: false
          isPro: false
          name: aigeek0x0
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;bweinstein123&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/bweinstein123\"\
          >@<span class=\"underline\">bweinstein123</span></a></span>\n\n\t</span></span>\
          \ yes, i have. you can finetune this model with 4-bit quantization on A100.\
          \ Even RTX A6000 would suffice if you use smaller batch size.</p>\n"
        raw: '@bweinstein123 yes, i have. you can finetune this model with 4-bit quantization
          on A100. Even RTX A6000 would suffice if you use smaller batch size.'
        updatedAt: '2024-01-21T16:05:39.335Z'
      numEdits: 0
      reactions: []
    id: 65ad40d3c8903e28aea9a839
    type: comment
  author: aigeek0x0
  content: '@bweinstein123 yes, i have. you can finetune this model with 4-bit quantization
    on A100. Even RTX A6000 would suffice if you use smaller batch size.'
  created_at: 2024-01-21 16:05:39+00:00
  edited: false
  hidden: false
  id: 65ad40d3c8903e28aea9a839
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625302367766-60891fec1e36b13a64497db5.jpeg?w=200&h=200&f=face
      fullname: mlkorra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mlkorra
      type: user
    createdAt: '2024-01-25T04:24:45.000Z'
    data:
      edited: false
      editors:
      - mlkorra
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9634301662445068
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625302367766-60891fec1e36b13a64497db5.jpeg?w=200&h=200&f=face
          fullname: mlkorra
          isHf: false
          isPro: false
          name: mlkorra
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;aigeek0x0&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/aigeek0x0\">@<span class=\"\
          underline\">aigeek0x0</span></a></span>\n\n\t</span></span> how was the\
          \ performance of mixtral instruct after fine-tuning? Any insights which\
          \ I can borrow, thanks </p>\n"
        raw: '@aigeek0x0 how was the performance of mixtral instruct after fine-tuning?
          Any insights which I can borrow, thanks '
        updatedAt: '2024-01-25T04:24:45.261Z'
      numEdits: 0
      reactions: []
    id: 65b1e28d3a41095a56445577
    type: comment
  author: mlkorra
  content: '@aigeek0x0 how was the performance of mixtral instruct after fine-tuning?
    Any insights which I can borrow, thanks '
  created_at: 2024-01-25 04:24:45+00:00
  edited: false
  hidden: false
  id: 65b1e28d3a41095a56445577
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454631806728ff79a3829e8/Im1TjNIXt7J5Am4aW6FZ-.jpeg?w=200&h=200&f=face
      fullname: AI Geek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aigeek0x0
      type: user
    createdAt: '2024-01-25T14:37:53.000Z'
    data:
      edited: false
      editors:
      - aigeek0x0
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.953676700592041
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454631806728ff79a3829e8/Im1TjNIXt7J5Am4aW6FZ-.jpeg?w=200&h=200&f=face
          fullname: AI Geek
          isHf: false
          isPro: false
          name: aigeek0x0
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mlkorra&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mlkorra\">@<span class=\"\
          underline\">mlkorra</span></a></span>\n\n\t</span></span> based on vibes-check\
          \ evaluations, it is looking promising. I haven't run benchmarks on it yet,\
          \ but I will do so after a few more modifications.</p>\n"
        raw: '@mlkorra based on vibes-check evaluations, it is looking promising.
          I haven''t run benchmarks on it yet, but I will do so after a few more modifications.'
        updatedAt: '2024-01-25T14:37:53.482Z'
      numEdits: 0
      reactions: []
    id: 65b272412b3c9da0b4ae0bad
    type: comment
  author: aigeek0x0
  content: '@mlkorra based on vibes-check evaluations, it is looking promising. I
    haven''t run benchmarks on it yet, but I will do so after a few more modifications.'
  created_at: 2024-01-25 14:37:53+00:00
  edited: false
  hidden: false
  id: 65b272412b3c9da0b4ae0bad
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 88
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Failure in loading the model on AWS
