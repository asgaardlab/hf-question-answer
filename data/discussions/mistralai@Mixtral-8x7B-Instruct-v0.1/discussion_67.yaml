!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Pradeep1995
conflicting_files: null
created_at: 2023-12-28 12:35:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
      fullname: Pradeep T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pradeep1995
      type: user
    createdAt: '2023-12-28T12:35:57.000Z'
    data:
      edited: false
      editors:
      - Pradeep1995
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5866125226020813
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
          fullname: Pradeep T
          isHf: false
          isPro: false
          name: Pradeep1995
          type: user
        html: "<p>I am finetuning the mistral model using the following configurations</p>\n\
          <pre><code>training_arguments = TrainingArguments(\n    output_dir=output_dir,\n\
          \    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n\
          \    optim=optim,\n    save_steps=save_steps,\n    logging_strategy=\"steps\"\
          ,\n    logging_steps=10,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n\
          \    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=13000,\n\
          \    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n\
          \    lr_scheduler_type=lr_scheduler_type\n)\ntrainer = SFTTrainer(\n   \
          \ model=peft_model,\n    train_dataset=data,\n    peft_config=peft_config,\n\
          \    dataset_text_field=\" column name\",\n    max_seq_length=3000,\n  \
          \  tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n\
          )\ntrainer.train()\n</code></pre>\n<p>during this training I am getting\
          \ the multiple checkpoints in the specified output directory <code>output_dir</code>.</p>\n\
          <p>Once the model training is over I can save the model using </p>\n<pre><code>trainer.save_model()\n\
          </code></pre>\n<p>Not only that i can save the final model using</p>\n<pre><code>trainer.model.save_pretrained(\"\
          path\")\n</code></pre>\n<p>So I bit confused. Which is the actual way to\
          \ store the adapter after PEFT based lora fine-tuning</p>\n<p>whether it\
          \ is<br>1 - Take the least loss checkpoint folder from the <code>output_dir</code><br>or<br>2\
          \ - save the adapter using </p>\n<pre><code>trainer.save_model()\n</code></pre>\n\
          <p>or<br>3 - this method</p>\n<pre><code>trainer.model.save_pretrained(\"\
          path\")\n</code></pre>\n"
        raw: "I am finetuning the mistral model using the following configurations\r\
          \n```\r\ntraining_arguments = TrainingArguments(\r\n    output_dir=output_dir,\r\
          \n    per_device_train_batch_size=per_device_train_batch_size,\r\n    gradient_accumulation_steps=gradient_accumulation_steps,\r\
          \n    optim=optim,\r\n    save_steps=save_steps,\r\n    logging_strategy=\"\
          steps\",\r\n    logging_steps=10,\r\n    learning_rate=learning_rate,\r\n\
          \    weight_decay=weight_decay,\r\n    fp16=fp16,\r\n    bf16=bf16,\r\n\
          \    max_grad_norm=max_grad_norm,\r\n    max_steps=13000,\r\n    warmup_ratio=warmup_ratio,\r\
          \n    group_by_length=group_by_length,\r\n    lr_scheduler_type=lr_scheduler_type\r\
          \n)\r\ntrainer = SFTTrainer(\r\n    model=peft_model,\r\n    train_dataset=data,\r\
          \n    peft_config=peft_config,\r\n    dataset_text_field=\" column name\"\
          ,\r\n    max_seq_length=3000,\r\n    tokenizer=tokenizer,\r\n    args=training_arguments,\r\
          \n    packing=packing,\r\n)\r\ntrainer.train()\r\n```\r\n\r\nduring this\
          \ training I am getting the multiple checkpoints in the specified output\
          \ directory ```output_dir```.\r\n\r\nOnce the model training is over I can\
          \ save the model using \r\n```\r\ntrainer.save_model()\r\n```\r\nNot only\
          \ that i can save the final model using\r\n```\r\ntrainer.model.save_pretrained(\"\
          path\")\r\n```\r\n\r\nSo I bit confused. Which is the actual way to store\
          \ the adapter after PEFT based lora fine-tuning\r\n\r\nwhether it is\r\n\
          1 - Take the least loss checkpoint folder from the ```output_dir```\r\n\
          or\r\n2 - save the adapter using \r\n```\r\ntrainer.save_model()\r\n```\r\
          \nor\r\n3 - this method\r\n```\r\ntrainer.model.save_pretrained(\"path\"\
          )\r\n```\r\n\r\n\r\n"
        updatedAt: '2023-12-28T12:35:57.882Z'
      numEdits: 0
      reactions: []
    id: 658d6badf893598fcaf5d7a1
    type: comment
  author: Pradeep1995
  content: "I am finetuning the mistral model using the following configurations\r\
    \n```\r\ntraining_arguments = TrainingArguments(\r\n    output_dir=output_dir,\r\
    \n    per_device_train_batch_size=per_device_train_batch_size,\r\n    gradient_accumulation_steps=gradient_accumulation_steps,\r\
    \n    optim=optim,\r\n    save_steps=save_steps,\r\n    logging_strategy=\"steps\"\
    ,\r\n    logging_steps=10,\r\n    learning_rate=learning_rate,\r\n    weight_decay=weight_decay,\r\
    \n    fp16=fp16,\r\n    bf16=bf16,\r\n    max_grad_norm=max_grad_norm,\r\n   \
    \ max_steps=13000,\r\n    warmup_ratio=warmup_ratio,\r\n    group_by_length=group_by_length,\r\
    \n    lr_scheduler_type=lr_scheduler_type\r\n)\r\ntrainer = SFTTrainer(\r\n  \
    \  model=peft_model,\r\n    train_dataset=data,\r\n    peft_config=peft_config,\r\
    \n    dataset_text_field=\" column name\",\r\n    max_seq_length=3000,\r\n   \
    \ tokenizer=tokenizer,\r\n    args=training_arguments,\r\n    packing=packing,\r\
    \n)\r\ntrainer.train()\r\n```\r\n\r\nduring this training I am getting the multiple\
    \ checkpoints in the specified output directory ```output_dir```.\r\n\r\nOnce\
    \ the model training is over I can save the model using \r\n```\r\ntrainer.save_model()\r\
    \n```\r\nNot only that i can save the final model using\r\n```\r\ntrainer.model.save_pretrained(\"\
    path\")\r\n```\r\n\r\nSo I bit confused. Which is the actual way to store the\
    \ adapter after PEFT based lora fine-tuning\r\n\r\nwhether it is\r\n1 - Take the\
    \ least loss checkpoint folder from the ```output_dir```\r\nor\r\n2 - save the\
    \ adapter using \r\n```\r\ntrainer.save_model()\r\n```\r\nor\r\n3 - this method\r\
    \n```\r\ntrainer.model.save_pretrained(\"path\")\r\n```\r\n\r\n\r\n"
  created_at: 2023-12-28 12:35:57+00:00
  edited: false
  hidden: false
  id: 658d6badf893598fcaf5d7a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-28T16:04:36.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9234058856964111
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Pradeep1995&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Pradeep1995\"\
          >@<span class=\"underline\">Pradeep1995</span></a></span>\n\n\t</span></span><br>Thanks\
          \ for the issue, if you want the model with the least loss (i.e. the \"\
          best\" model), I would advise to go for the first option. Otherwise 2- and\
          \ 3- should achieve the same goal and save the final checkpoint. Note you\
          \ can also call <code>trainer.push_to_hub()</code> and the trained adapters\
          \ will be pushed on the Hub under your name space together with training\
          \ logs</p>\n"
        raw: "Hi @Pradeep1995 \nThanks for the issue, if you want the model with the\
          \ least loss (i.e. the \"best\" model), I would advise to go for the first\
          \ option. Otherwise 2- and 3- should achieve the same goal and save the\
          \ final checkpoint. Note you can also call `trainer.push_to_hub()` and the\
          \ trained adapters will be pushed on the Hub under your name space together\
          \ with training logs"
        updatedAt: '2023-12-28T16:04:36.952Z'
      numEdits: 0
      reactions: []
    id: 658d9c94c2cf5583e9822767
    type: comment
  author: ybelkada
  content: "Hi @Pradeep1995 \nThanks for the issue, if you want the model with the\
    \ least loss (i.e. the \"best\" model), I would advise to go for the first option.\
    \ Otherwise 2- and 3- should achieve the same goal and save the final checkpoint.\
    \ Note you can also call `trainer.push_to_hub()` and the trained adapters will\
    \ be pushed on the Hub under your name space together with training logs"
  created_at: 2023-12-28 16:04:36+00:00
  edited: false
  hidden: false
  id: 658d9c94c2cf5583e9822767
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
      fullname: Pradeep T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pradeep1995
      type: user
    createdAt: '2023-12-28T17:20:34.000Z'
    data:
      edited: false
      editors:
      - Pradeep1995
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9526331424713135
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
          fullname: Pradeep T
          isHf: false
          isPro: false
          name: Pradeep1995
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> If I want the\
          \ model with the least loss (i.e. the \"best\" model), I would go for the\
          \ first option- the checkpoints.<br>So the checkpoints act as the adapters?</p>\n"
        raw: "@ybelkada If I want the model with the least loss (i.e. the \"best\"\
          \ model), I would go for the first option- the checkpoints.  \nSo the checkpoints\
          \ act as the adapters?"
        updatedAt: '2023-12-28T17:20:34.044Z'
      numEdits: 0
      reactions: []
    id: 658dae62c029759c1f125f6c
    type: comment
  author: Pradeep1995
  content: "@ybelkada If I want the model with the least loss (i.e. the \"best\" model),\
    \ I would go for the first option- the checkpoints.  \nSo the checkpoints act\
    \ as the adapters?"
  created_at: 2023-12-28 17:20:34+00:00
  edited: false
  hidden: false
  id: 658dae62c029759c1f125f6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-28T18:09:10.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8894866108894348
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>yes, if you inspect the checkpoint folders, you should be able to
          see <code>adapter_model.safetensors</code> and <code>adapter_config.json</code>
          files. Those refer to the adapter weights</p>

          '
        raw: yes, if you inspect the checkpoint folders, you should be able to see
          `adapter_model.safetensors` and `adapter_config.json` files. Those refer
          to the adapter weights
        updatedAt: '2023-12-28T18:09:10.904Z'
      numEdits: 0
      reactions: []
    id: 658db9c64d5e2630e3d124c4
    type: comment
  author: ybelkada
  content: yes, if you inspect the checkpoint folders, you should be able to see `adapter_model.safetensors`
    and `adapter_config.json` files. Those refer to the adapter weights
  created_at: 2023-12-28 18:09:10+00:00
  edited: false
  hidden: false
  id: 658db9c64d5e2630e3d124c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
      fullname: Pradeep T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pradeep1995
      type: user
    createdAt: '2023-12-29T01:53:10.000Z'
    data:
      edited: true
      editors:
      - Pradeep1995
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45173993706703186
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
          fullname: Pradeep T
          isHf: false
          isPro: false
          name: Pradeep1995
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> Thanks. so if\
          \ I use the least loss checkpoint folder as an adapter, then should I again\
          \ merge that checkpoint folder with the  base model using</p>\n<pre><code>merge_and_unload()\n\
          </code></pre>\n<p>or shall i directly use the checkpoint folder for inference\
          \ without merging?</p>\n"
        raw: '@ybelkada Thanks. so if I use the least loss checkpoint folder as an
          adapter, then should I again merge that checkpoint folder with the  base
          model using

          ```

          merge_and_unload()

          ```

          or shall i directly use the checkpoint folder for inference without merging?'
        updatedAt: '2023-12-29T03:00:03.126Z'
      numEdits: 2
      reactions: []
    id: 658e2686a260709928794083
    type: comment
  author: Pradeep1995
  content: '@ybelkada Thanks. so if I use the least loss checkpoint folder as an adapter,
    then should I again merge that checkpoint folder with the  base model using

    ```

    merge_and_unload()

    ```

    or shall i directly use the checkpoint folder for inference without merging?'
  created_at: 2023-12-29 01:53:10+00:00
  edited: true
  hidden: false
  id: 658e2686a260709928794083
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 67
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Which is the actual way to store the adapters after PEFT finetuning
