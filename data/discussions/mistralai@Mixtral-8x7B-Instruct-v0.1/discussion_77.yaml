!!python/object:huggingface_hub.community.DiscussionWithDetails
author: roboboot
conflicting_files: null
created_at: 2024-01-05 06:41:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/884b079b78c2cb13527d5b70c8aa198d.svg
      fullname: Roberto Battistoni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: roboboot
      type: user
    createdAt: '2024-01-05T06:41:48.000Z'
    data:
      edited: false
      editors:
      - roboboot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7630407214164734
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/884b079b78c2cb13527d5b70c8aa198d.svg
          fullname: Roberto Battistoni
          isHf: false
          isPro: false
          name: roboboot
          type: user
        html: "<p>I'm trying to run this model on Windows 11, with 48 GB of RAM and\
          \ without GPU. </p>\n<p>model_id = \"../Mixtral-8x7B-Instruct-v0.1\"<br>self.model\
          \ = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\", low_cpu_mem_usage=True)<br>self.tokenizer\
          \ = AutoTokenizer.from_pretrained(model_id)</p>\n<p>I see the RAM occupation\
          \ rises until the error:</p>\n<p>Loading checkpoint shards:  16%|\u2588\u258C\
          \        | 3/19 [01:13&lt;06:31, 24.49s/it]<br>Process finished with exit\
          \ code -1073741819 (0xC0000005)</p>\n<p>Do I need more memory or is it possible\
          \ to do something else?</p>\n<p>thx</p>\n<p>R.</p>\n"
        raw: "I'm trying to run this model on Windows 11, with 48 GB of RAM and without\
          \ GPU. \r\n\r\nmodel_id = \"../Mixtral-8x7B-Instruct-v0.1\"\r\nself.model\
          \ = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\", low_cpu_mem_usage=True)\r\
          \nself.tokenizer = AutoTokenizer.from_pretrained(model_id)\r\n\r\nI see\
          \ the RAM occupation rises until the error:\r\n\r\nLoading checkpoint shards:\
          \  16%|\u2588\u258C        | 3/19 [01:13<06:31, 24.49s/it]\r\nProcess finished\
          \ with exit code -1073741819 (0xC0000005)\r\n\r\nDo I need more memory or\
          \ is it possible to do something else?\r\n\r\nthx\r\n\r\nR."
        updatedAt: '2024-01-05T06:41:48.649Z'
      numEdits: 0
      reactions: []
    id: 6597a4ac33e0a98ddb6e7867
    type: comment
  author: roboboot
  content: "I'm trying to run this model on Windows 11, with 48 GB of RAM and without\
    \ GPU. \r\n\r\nmodel_id = \"../Mixtral-8x7B-Instruct-v0.1\"\r\nself.model = AutoModelForCausalLM.from_pretrained(model_id,\
    \ device_map=\"cpu\", low_cpu_mem_usage=True)\r\nself.tokenizer = AutoTokenizer.from_pretrained(model_id)\r\
    \n\r\nI see the RAM occupation rises until the error:\r\n\r\nLoading checkpoint\
    \ shards:  16%|\u2588\u258C        | 3/19 [01:13<06:31, 24.49s/it]\r\nProcess\
    \ finished with exit code -1073741819 (0xC0000005)\r\n\r\nDo I need more memory\
    \ or is it possible to do something else?\r\n\r\nthx\r\n\r\nR."
  created_at: 2024-01-05 06:41:48+00:00
  edited: false
  hidden: false
  id: 6597a4ac33e0a98ddb6e7867
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7eb63bcc4e16db8fa046118c9c008a7f.svg
      fullname: JJ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: J22
      type: user
    createdAt: '2024-01-06T08:32:22.000Z'
    data:
      edited: false
      editors:
      - J22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9498961567878723
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7eb63bcc4e16db8fa046118c9c008a7f.svg
          fullname: JJ
          isHf: false
          isPro: false
          name: J22
          type: user
        html: '<p>I have run this model on <a rel="nofollow" href="https://github.com/foldl/chatllm.cpp">ChatLLM.cpp</a>:</p>

          <ul>

          <li>For quantized int4, 32 GB of RAM is enough;</li>

          <li>For quantized int8, 64 GB of RAM is enough.</li>

          </ul>

          <p>I think it is impossible to run it with PyTorch on CPU, because PyTorch
          is not as efficient as GGML on CPU.</p>

          '
        raw: 'I have run this model on [ChatLLM.cpp](https://github.com/foldl/chatllm.cpp):


          * For quantized int4, 32 GB of RAM is enough;

          * For quantized int8, 64 GB of RAM is enough.


          I think it is impossible to run it with PyTorch on CPU, because PyTorch
          is not as efficient as GGML on CPU.'
        updatedAt: '2024-01-06T08:32:22.632Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - stinkyElias
    id: 659910160c972f4c7789542d
    type: comment
  author: J22
  content: 'I have run this model on [ChatLLM.cpp](https://github.com/foldl/chatllm.cpp):


    * For quantized int4, 32 GB of RAM is enough;

    * For quantized int8, 64 GB of RAM is enough.


    I think it is impossible to run it with PyTorch on CPU, because PyTorch is not
    as efficient as GGML on CPU.'
  created_at: 2024-01-06 08:32:22+00:00
  edited: false
  hidden: false
  id: 659910160c972f4c7789542d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/884b079b78c2cb13527d5b70c8aa198d.svg
      fullname: Roberto Battistoni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: roboboot
      type: user
    createdAt: '2024-01-06T10:19:54.000Z'
    data:
      edited: false
      editors:
      - roboboot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9037235379219055
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/884b079b78c2cb13527d5b70c8aa198d.svg
          fullname: Roberto Battistoni
          isHf: false
          isPro: false
          name: roboboot
          type: user
        html: '<p>ok how can I use quantized int4?</p>

          <p>Do I have to use "load_in_4bit=True" ?</p>

          <p>thx</p>

          <p>R</p>

          '
        raw: 'ok how can I use quantized int4?


          Do I have to use "load_in_4bit=True" ?


          thx


          R'
        updatedAt: '2024-01-06T10:19:54.798Z'
      numEdits: 0
      reactions: []
    id: 6599294a2bc3a1e0f609bab1
    type: comment
  author: roboboot
  content: 'ok how can I use quantized int4?


    Do I have to use "load_in_4bit=True" ?


    thx


    R'
  created_at: 2024-01-06 10:19:54+00:00
  edited: false
  hidden: false
  id: 6599294a2bc3a1e0f609bab1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/884b079b78c2cb13527d5b70c8aa198d.svg
      fullname: Roberto Battistoni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: roboboot
      type: user
    createdAt: '2024-01-08T10:05:38.000Z'
    data:
      edited: false
      editors:
      - roboboot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6541664004325867
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/884b079b78c2cb13527d5b70c8aa198d.svg
          fullname: Roberto Battistoni
          isHf: false
          isPro: false
          name: roboboot
          type: user
        html: "<p>Yes, I've just used:</p>\n<pre><code>    self.model = AutoModelForCausalLM.from_pretrained(model_id,\
          \ device_map=\"cpu\", low_cpu_mem_usage=True, load_in_4bit=True)\n    self.tokenizer\
          \ = AutoTokenizer.from_pretrained(model_id)\n</code></pre>\n<p>And I receive\
          \ and error that it's not possibile quantization without GPU. </p>\n<p>You\
          \ are right :(</p>\n<p>thx</p>\n"
        raw: "Yes, I've just used:\n\n        self.model = AutoModelForCausalLM.from_pretrained(model_id,\
          \ device_map=\"cpu\", low_cpu_mem_usage=True, load_in_4bit=True)\n     \
          \   self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n\nAnd I receive\
          \ and error that it's not possibile quantization without GPU. \n\nYou are\
          \ right :(\n\nthx"
        updatedAt: '2024-01-08T10:05:38.023Z'
      numEdits: 0
      reactions: []
    id: 659bc8f2b0c5357368ac71d4
    type: comment
  author: roboboot
  content: "Yes, I've just used:\n\n        self.model = AutoModelForCausalLM.from_pretrained(model_id,\
    \ device_map=\"cpu\", low_cpu_mem_usage=True, load_in_4bit=True)\n        self.tokenizer\
    \ = AutoTokenizer.from_pretrained(model_id)\n\nAnd I receive and error that it's\
    \ not possibile quantization without GPU. \n\nYou are right :(\n\nthx"
  created_at: 2024-01-08 10:05:38+00:00
  edited: false
  hidden: false
  id: 659bc8f2b0c5357368ac71d4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 77
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: How much memory do I need for this model (on Windows)?
