!!python/object:huggingface_hub.community.DiscussionWithDetails
author: willowill5
conflicting_files: null
created_at: 2023-12-17 21:09:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62acc1f1eda69b28bb64c39d/tmMMhkSyHc6HXDyhE7-jj.jpeg?w=200&h=200&f=face
      fullname: Will Reynolds
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: willowill5
      type: user
    createdAt: '2023-12-17T21:09:11.000Z'
    data:
      edited: false
      editors:
      - willowill5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6987700462341309
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62acc1f1eda69b28bb64c39d/tmMMhkSyHc6HXDyhE7-jj.jpeg?w=200&h=200&f=face
          fullname: Will Reynolds
          isHf: false
          isPro: false
          name: willowill5
          type: user
        html: '<p>OOM even on A100 80GB when deploying with </p>

          <p>python -m vllm.entrypoints.api_server --model mistralai/Mixtral-8x7B-Instruct-v0.1
          --dtype half </p>

          <p>I have also tried flags "--max-model-len 8192" and "--gpu-memory-utilization
          0.8 "</p>

          <p>Anyone else run into this? Thanks!!</p>

          '
        raw: "OOM even on A100 80GB when deploying with \r\n\r\npython -m vllm.entrypoints.api_server\
          \ --model mistralai/Mixtral-8x7B-Instruct-v0.1 --dtype half \r\n\r\nI have\
          \ also tried flags \"--max-model-len 8192\" and \"--gpu-memory-utilization\
          \ 0.8 \"\r\n\r\nAnyone else run into this? Thanks!!"
        updatedAt: '2023-12-17T21:09:11.910Z'
      numEdits: 0
      reactions: []
    id: 657f63771815b29c9a060506
    type: comment
  author: willowill5
  content: "OOM even on A100 80GB when deploying with \r\n\r\npython -m vllm.entrypoints.api_server\
    \ --model mistralai/Mixtral-8x7B-Instruct-v0.1 --dtype half \r\n\r\nI have also\
    \ tried flags \"--max-model-len 8192\" and \"--gpu-memory-utilization 0.8 \"\r\
    \n\r\nAnyone else run into this? Thanks!!"
  created_at: 2023-12-17 21:09:11+00:00
  edited: false
  hidden: false
  id: 657f63771815b29c9a060506
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62acc1f1eda69b28bb64c39d/tmMMhkSyHc6HXDyhE7-jj.jpeg?w=200&h=200&f=face
      fullname: Will Reynolds
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: willowill5
      type: user
    createdAt: '2023-12-18T00:57:34.000Z'
    data:
      status: closed
    id: 657f98fef010d76b6e65a5c3
    type: status-change
  author: willowill5
  created_at: 2023-12-18 00:57:34+00:00
  id: 657f98fef010d76b6e65a5c3
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 48
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: closed
target_branch: null
title: OOM with vllm
