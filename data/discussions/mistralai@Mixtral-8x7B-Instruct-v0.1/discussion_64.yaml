!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DaehanKim
conflicting_files: null
created_at: 2023-12-27 04:14:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1556ef2c110df415516e4d05c0b0ee3f.svg
      fullname: DaehanKim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DaehanKim
      type: user
    createdAt: '2023-12-27T04:14:31.000Z'
    data:
      edited: true
      editors:
      - DaehanKim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.902912437915802
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1556ef2c110df415516e4d05c0b0ee3f.svg
          fullname: DaehanKim
          isHf: false
          isPro: false
          name: DaehanKim
          type: user
        html: '<p>Hi Minstral Team,</p>

          <p>Thank you for sharing this excellent piece of models.</p>

          <p>I just saw a paper from <a rel="nofollow" href="https://arxiv.org/pdf/2312.15166.pdf">Upstage
          AI</a> which uses minstral 7B weights to make 10B parameter model.<br>and
          they compared it to Mixture-of-Expert architecture. I''m not sure whether
          Mixtral(minstral 7Bx8) was trained from scratch or from pretrained weights
          like mistral 7B, since it''s easily adoptable. In the <a rel="nofollow"
          href="https://mistral.ai/news/mixtral-of-experts/">blog post</a>, it says
          "pre-trained on data extracted from the open Web".</p>

          <p>Thanks in advance!</p>

          '
        raw: 'Hi Minstral Team,


          Thank you for sharing this excellent piece of models.


          I just saw a paper from [Upstage AI](https://arxiv.org/pdf/2312.15166.pdf)
          which uses minstral 7B weights to make 10B parameter model.

          and they compared it to Mixture-of-Expert architecture. I''m not sure whether
          Mixtral(minstral 7Bx8) was trained from scratch or from pretrained weights
          like mistral 7B, since it''s easily adoptable. In the [blog post](https://mistral.ai/news/mixtral-of-experts/),
          it says "pre-trained on data extracted from the open Web".


          Thanks in advance!'
        updatedAt: '2023-12-27T04:15:19.872Z'
      numEdits: 1
      reactions: []
    id: 658ba4a7f366ebe2a3c3bea8
    type: comment
  author: DaehanKim
  content: 'Hi Minstral Team,


    Thank you for sharing this excellent piece of models.


    I just saw a paper from [Upstage AI](https://arxiv.org/pdf/2312.15166.pdf) which
    uses minstral 7B weights to make 10B parameter model.

    and they compared it to Mixture-of-Expert architecture. I''m not sure whether
    Mixtral(minstral 7Bx8) was trained from scratch or from pretrained weights like
    mistral 7B, since it''s easily adoptable. In the [blog post](https://mistral.ai/news/mixtral-of-experts/),
    it says "pre-trained on data extracted from the open Web".


    Thanks in advance!'
  created_at: 2023-12-27 04:14:31+00:00
  edited: true
  hidden: false
  id: 658ba4a7f366ebe2a3c3bea8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/1556ef2c110df415516e4d05c0b0ee3f.svg
      fullname: DaehanKim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DaehanKim
      type: user
    createdAt: '2023-12-27T04:15:03.000Z'
    data:
      from: did Mixtral start from Minstral or from-scratch?
      to: did Mixtral start from Mistral or from-scratch?
    id: 658ba4c775ddc76f9dd7a7f5
    type: title-change
  author: DaehanKim
  created_at: 2023-12-27 04:15:03+00:00
  id: 658ba4c775ddc76f9dd7a7f5
  new_title: did Mixtral start from Mistral or from-scratch?
  old_title: did Mixtral start from Minstral or from-scratch?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba23aecece4c4af434210496c2d4ad52.svg
      fullname: Basu Jindal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: basujindal
      type: user
    createdAt: '2023-12-27T18:42:58.000Z'
    data:
      edited: false
      editors:
      - basujindal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8977911472320557
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba23aecece4c4af434210496c2d4ad52.svg
          fullname: Basu Jindal
          isHf: false
          isPro: false
          name: basujindal
          type: user
        html: "<p>\"Exciting times with the new Mixtral model from  @MistralAI<br>!\
          \ It\u2019s evident that they\u2019ve fine-tuned the Mistral 7B model to\
          \ an impressive 8x. The significant correlation between the weights of the\
          \ two models is a testament to the successful reuse of models. This approach\
          \ could empower the OSS community with its own robust MoE!\"</p>\n<p><a\
          \ rel=\"nofollow\" href=\"https://twitter.com/tianle_cai/status/1734188749117153684\"\
          >https://twitter.com/tianle_cai/status/1734188749117153684</a></p>\n"
        raw: "\"Exciting times with the new Mixtral model from  @MistralAI\n! It\u2019\
          s evident that they\u2019ve fine-tuned the Mistral 7B model to an impressive\
          \ 8x. The significant correlation between the weights of the two models\
          \ is a testament to the successful reuse of models. This approach could\
          \ empower the OSS community with its own robust MoE!\"\n\nhttps://twitter.com/tianle_cai/status/1734188749117153684"
        updatedAt: '2023-12-27T18:42:58.389Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - DaehanKim
    id: 658c703229ef008a12a50bd9
    type: comment
  author: basujindal
  content: "\"Exciting times with the new Mixtral model from  @MistralAI\n! It\u2019\
    s evident that they\u2019ve fine-tuned the Mistral 7B model to an impressive 8x.\
    \ The significant correlation between the weights of the two models is a testament\
    \ to the successful reuse of models. This approach could empower the OSS community\
    \ with its own robust MoE!\"\n\nhttps://twitter.com/tianle_cai/status/1734188749117153684"
  created_at: 2023-12-27 18:42:58+00:00
  edited: false
  hidden: false
  id: 658c703229ef008a12a50bd9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 64
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: did Mixtral start from Mistral or from-scratch?
