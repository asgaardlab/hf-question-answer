!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aledane
conflicting_files: null
created_at: 2023-12-20 09:17:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
      fullname: Alessandro Danesi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aledane
      type: user
    createdAt: '2023-12-20T09:17:20.000Z'
    data:
      edited: true
      editors:
      - aledane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7512196898460388
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
          fullname: Alessandro Danesi
          isHf: false
          isPro: false
          name: aledane
          type: user
        html: "<p>Hi,<br>I am using the model in the quantization version with this\
          \ setting:</p>\n<pre><code>                \"params\" : {\n            \
          \                \"trust_remote_code\" : True,\n                       \
          \     \"torch_dtype\":torch.bfloat16,\n                            \"return_full_text\"\
          \ : True,\n                            \"device_map\" : \"auto\",\n    \
          \                        \"max_new_tokens\" : 16,\n                    \
          \        \"do_sample\" : True,\n                            \"temperature\"\
          \ : 0.01,\n                            \"renormalize_logits\" : True\n \
          \                          },\n</code></pre>\n<p>However, in inference the\
          \ model is extremely slow (it is running for 1 hour for a simple question).<br>I\
          \ am using the model on a g5.4xlarge Sagemaker instance (16gb vcpu, 64gb\
          \ RAM, NVIDIA A10 GPU)<br>Any idea on how to speed up the process? Thanks</p>\n"
        raw: "Hi,\nI am using the model in the quantization version with this setting:\n\
          \n                    \"params\" : {\n                                \"\
          trust_remote_code\" : True,\n                                \"torch_dtype\"\
          :torch.bfloat16,\n                                \"return_full_text\" :\
          \ True,\n                                \"device_map\" : \"auto\",\n  \
          \                              \"max_new_tokens\" : 16,\n              \
          \                  \"do_sample\" : True,\n                             \
          \   \"temperature\" : 0.01,\n                                \"renormalize_logits\"\
          \ : True\n                               },\n\nHowever, in inference the\
          \ model is extremely slow (it is running for 1 hour for a simple question).\n\
          I am using the model on a g5.4xlarge Sagemaker instance (16gb vcpu, 64gb\
          \ RAM, NVIDIA A10 GPU)\nAny idea on how to speed up the process? Thanks"
        updatedAt: '2023-12-20T09:19:51.106Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - swapnil3597
    id: 6582b120770395b58808adb4
    type: comment
  author: aledane
  content: "Hi,\nI am using the model in the quantization version with this setting:\n\
    \n                    \"params\" : {\n                                \"trust_remote_code\"\
    \ : True,\n                                \"torch_dtype\":torch.bfloat16,\n \
    \                               \"return_full_text\" : True,\n               \
    \                 \"device_map\" : \"auto\",\n                               \
    \ \"max_new_tokens\" : 16,\n                                \"do_sample\" : True,\n\
    \                                \"temperature\" : 0.01,\n                   \
    \             \"renormalize_logits\" : True\n                               },\n\
    \nHowever, in inference the model is extremely slow (it is running for 1 hour\
    \ for a simple question).\nI am using the model on a g5.4xlarge Sagemaker instance\
    \ (16gb vcpu, 64gb RAM, NVIDIA A10 GPU)\nAny idea on how to speed up the process?\
    \ Thanks"
  created_at: 2023-12-20 09:17:20+00:00
  edited: true
  hidden: false
  id: 6582b120770395b58808adb4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-20T09:56:49.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8917336463928223
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;aledane&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/aledane\">@<span class=\"\
          underline\">aledane</span></a></span>\n\n\t</span></span> </p>\n<blockquote>\n\
          <p>I am using the model in the quantization version</p>\n</blockquote>\n\
          <p>Can you elaborate more? Which quantization method are you using?</p>\n"
        raw: "Hi @aledane \n\n> I am using the model in the quantization version\n\
          \nCan you elaborate more? Which quantization method are you using?"
        updatedAt: '2023-12-20T09:56:49.709Z'
      numEdits: 0
      reactions: []
    id: 6582ba614ee85531becc7174
    type: comment
  author: ybelkada
  content: "Hi @aledane \n\n> I am using the model in the quantization version\n\n\
    Can you elaborate more? Which quantization method are you using?"
  created_at: 2023-12-20 09:56:49+00:00
  edited: false
  hidden: false
  id: 6582ba614ee85531becc7174
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
      fullname: Alessandro Danesi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aledane
      type: user
    createdAt: '2023-12-20T10:54:21.000Z'
    data:
      edited: false
      editors:
      - aledane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9490125179290771
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
          fullname: Alessandro Danesi
          isHf: false
          isPro: false
          name: aledane
          type: user
        html: '<p>I was trying to use this: <a href="https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ">https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ</a><br>However,
          I realized I was not really using it due to a coding mistake; instead, I
          was deploying the original version mistralai/Mixtral-8x7B-Instruct-v0.1.</p>

          <p>Other than using a quantization method, is there any way to speed up
          the inference generation by using the original model?<br>Is it just a problem
          of resources (so I have to increase the Sagemaker instance), or is there
          another way?</p>

          '
        raw: 'I was trying to use this: https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ

          However, I realized I was not really using it due to a coding mistake; instead,
          I was deploying the original version mistralai/Mixtral-8x7B-Instruct-v0.1.


          Other than using a quantization method, is there any way to speed up the
          inference generation by using the original model?

          Is it just a problem of resources (so I have to increase the Sagemaker instance),
          or is there another way?'
        updatedAt: '2023-12-20T10:54:21.639Z'
      numEdits: 0
      reactions: []
    id: 6582c7dd90aa0254b0b91b47
    type: comment
  author: aledane
  content: 'I was trying to use this: https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ

    However, I realized I was not really using it due to a coding mistake; instead,
    I was deploying the original version mistralai/Mixtral-8x7B-Instruct-v0.1.


    Other than using a quantization method, is there any way to speed up the inference
    generation by using the original model?

    Is it just a problem of resources (so I have to increase the Sagemaker instance),
    or is there another way?'
  created_at: 2023-12-20 10:54:21+00:00
  edited: false
  hidden: false
  id: 6582c7dd90aa0254b0b91b47
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c3abd5cfc948a137b40ab4c73088859f.svg
      fullname: Sebastian Bustillo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: seabasshn
      type: user
    createdAt: '2023-12-20T12:49:15.000Z'
    data:
      edited: false
      editors:
      - seabasshn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9764236211776733
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c3abd5cfc948a137b40ab4c73088859f.svg
          fullname: Sebastian Bustillo
          isHf: false
          isPro: false
          name: seabasshn
          type: user
        html: '<p>Can you share your code and versions of sagemaker sdk and the TGI
          you used? I''ve been trying to deploy both the models on SM but I havent
          been able to.</p>

          '
        raw: Can you share your code and versions of sagemaker sdk and the TGI you
          used? I've been trying to deploy both the models on SM but I havent been
          able to.
        updatedAt: '2023-12-20T12:49:15.939Z'
      numEdits: 0
      reactions: []
    id: 6582e2cb0093d12709c11fb1
    type: comment
  author: seabasshn
  content: Can you share your code and versions of sagemaker sdk and the TGI you used?
    I've been trying to deploy both the models on SM but I havent been able to.
  created_at: 2023-12-20 12:49:15+00:00
  edited: false
  hidden: false
  id: 6582e2cb0093d12709c11fb1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-20T13:43:22.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9008286595344543
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;aledane&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/aledane\">@<span class=\"\
          underline\">aledane</span></a></span>\n\n\t</span></span><br>I suspect your\
          \ model is silently loaded with CPU offloading because you don't have enough\
          \ GPU RAM. You can make sure to use <code>torch.float16</code> by passing\
          \ <code>torch_dtype=torch.float16</code> in from_pretrained, or load the\
          \ model in 4-bit precision through <code>bitsandbytes</code> package so\
          \ that your model will fit into your GPU</p>\n"
        raw: "Hi @aledane \nI suspect your model is silently loaded with CPU offloading\
          \ because you don't have enough GPU RAM. You can make sure to use `torch.float16`\
          \ by passing `torch_dtype=torch.float16` in from_pretrained, or load the\
          \ model in 4-bit precision through `bitsandbytes` package so that your model\
          \ will fit into your GPU"
        updatedAt: '2023-12-20T13:43:22.630Z'
      numEdits: 0
      reactions: []
    id: 6582ef7a16dc91de6f53777a
    type: comment
  author: ybelkada
  content: "Hi @aledane \nI suspect your model is silently loaded with CPU offloading\
    \ because you don't have enough GPU RAM. You can make sure to use `torch.float16`\
    \ by passing `torch_dtype=torch.float16` in from_pretrained, or load the model\
    \ in 4-bit precision through `bitsandbytes` package so that your model will fit\
    \ into your GPU"
  created_at: 2023-12-20 13:43:22+00:00
  edited: false
  hidden: false
  id: 6582ef7a16dc91de6f53777a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
      fullname: Alessandro Danesi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aledane
      type: user
    createdAt: '2023-12-20T13:53:23.000Z'
    data:
      edited: false
      editors:
      - aledane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9169189929962158
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
          fullname: Alessandro Danesi
          isHf: false
          isPro: false
          name: aledane
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ , thank you for your reply. I am already using torch.float16 in from_pretrained\
          \ as I have already shown in the set of parameters above.<br>I can try with\
          \ 4-bit precision tho, even if I do not think will change so much honestly</p>\n"
        raw: 'Hi @ybelkada , thank you for your reply. I am already using torch.float16
          in from_pretrained as I have already shown in the set of parameters above.

          I can try with 4-bit precision tho, even if I do not think will change so
          much honestly'
        updatedAt: '2023-12-20T13:53:23.897Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - swapnil3597
    id: 6582f1d34bd14fb0805d0fec
    type: comment
  author: aledane
  content: 'Hi @ybelkada , thank you for your reply. I am already using torch.float16
    in from_pretrained as I have already shown in the set of parameters above.

    I can try with 4-bit precision tho, even if I do not think will change so much
    honestly'
  created_at: 2023-12-20 13:53:23+00:00
  edited: false
  hidden: false
  id: 6582f1d34bd14fb0805d0fec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/987c2889157355b9eac4c655e72f5155.svg
      fullname: MrDragonFox
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDragonFox
      type: user
    createdAt: '2023-12-21T01:06:39.000Z'
    data:
      edited: false
      editors:
      - MrDragonFox
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9050350785255432
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/987c2889157355b9eac4c655e72f5155.svg
          fullname: MrDragonFox
          isHf: false
          isPro: false
          name: MrDragonFox
          type: user
        html: '<p>you will need more then 1 a10 for that .. .fp16 takes about 90g
          vram in so 2 a100/h100 2 a6000 are fine either way<br>6bpw on exlv2 takes
          38g so you can cramp that into an a6000 </p>

          '
        raw: "you will need more then 1 a10 for that .. .fp16 takes about 90g vram\
          \ in so 2 a100/h100 2 a6000 are fine either way \n6bpw on exlv2 takes 38g\
          \ so you can cramp that into an a6000 "
        updatedAt: '2023-12-21T01:06:39.464Z'
      numEdits: 0
      reactions: []
    id: 65838f9faa85c512dabfa4ab
    type: comment
  author: MrDragonFox
  content: "you will need more then 1 a10 for that .. .fp16 takes about 90g vram in\
    \ so 2 a100/h100 2 a6000 are fine either way \n6bpw on exlv2 takes 38g so you\
    \ can cramp that into an a6000 "
  created_at: 2023-12-21 01:06:39+00:00
  edited: false
  hidden: false
  id: 65838f9faa85c512dabfa4ab
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 57
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Inference generation extremely slow
