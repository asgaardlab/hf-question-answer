!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ludomare
conflicting_files: null
created_at: 2024-01-07 22:27:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/baff8826f875bb1af69937b300079c2b.svg
      fullname: mare
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ludomare
      type: user
    createdAt: '2024-01-07T22:27:52.000Z'
    data:
      edited: false
      editors:
      - ludomare
      hidden: false
      identifiedLanguage:
        language: fr
        probability: 0.4512110948562622
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/baff8826f875bb1af69937b300079c2b.svg
          fullname: mare
          isHf: false
          isPro: false
          name: ludomare
          type: user
        html: "<p>Hi,<br>I try to query in french but query get a truncated response.\
          \ I try max_tokens=-1 in HuggingFaceHub(repo=...) or summary_chain.run(...)\
          \ but it didn't work, any idea?</p>\n<p>code:<br>from langchain.chains.summarize\
          \ import load_summarize_chain<br>from langchain.text_splitter import RecursiveCharacterTextSplitter<br>text\
          \ = 'travail.txt'<br>with open(text, 'r') as file:<br>    essay = file.read()<br>from\
          \ dotenv import load_dotenv<br>load_dotenv()<br>from langchain.llms import\
          \ HuggingFaceHub<br>llm = HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
          , model_kwargs={\"temperature\":0.1})<br>text_splitter = RecursiveCharacterTextSplitter(<br>\
          \                                                separators=['\\n\\n', '\\\
          n', '(?=&gt;. )', ' ', ''],<br>                                        \
          \       chunk_size=3000,<br>                                           \
          \    chunk_overlap=500<br>                                             \
          \  )<br>docs = text_splitter.create_documents([essay])<br>from langchain.prompts\
          \ import PromptTemplate<br>map_prompt = \"\"\"<br>Ecrit un r\xE9sum\xE9\
          \ pr\xE9cis en francais en 600 mots de :<br>\"{text}\"<br>R\xE9sum\xE9 pr\xE9\
          cis:<br>\"\"\"<br>map_prompt_template = PromptTemplate(template=map_prompt,\
          \ input_variables=[\"text\"])<br>combine_prompt = \"\"\"<br>Ecrit un r\xE9\
          sum\xE9 pr\xE9cis en francais de :.<br>Retourne 10 points les \xE9l\xE9\
          ments cl\xE9s du texte en francais.<br><code>{text}</code><br>R\xE9sum\xE9\
          \ par points cl\xE9s:<br>\"\"\"<br>combine_prompt_template = PromptTemplate(template=combine_prompt,\
          \ input_variables=[\"text\"])<br>summary_chain = load_summarize_chain(llm=llm,<br>\
          \                                     chain_type='map_reduce',<br>     \
          \                                map_prompt=map_prompt_template,<br>   \
          \                                  combine_prompt=combine_prompt_template,</p>\n\
          <h1 id=\"verbosetrue\">verbose=True</h1>\n<pre><code>                  \
          \              )\n</code></pre>\n<p>output = summary_chain.run(docs)</p>\n\
          <p>answer (truncated):</p>\n<ol>\n<li>Giuseppe Rensi, philosophe italien,\
          \ a \xE9crit en 1923 un essai intitul\xE9 \"Contre le travail\" dans lequel\
          \ il critique la morale de la soci\xE9t\xE9 capitaliste qui insiste sur\
          \ une conception du travail comme ph\xE9nom\xE8ne \xE9thico-religieux de\
          \ grande importance.</li>\n<li>Selon Rensi, le travail est une activit\xE9\
          \ ali\xE9nante et d\xE9sagr\xE9able qui est souvent</li>\n</ol>\n"
        raw: "Hi,\r\nI try to query in french but query get a truncated response.\
          \ I try max_tokens=-1 in HuggingFaceHub(repo=...) or summary_chain.run(...)\
          \ but it didn't work, any idea?\r\n\r\ncode:\r\nfrom langchain.chains.summarize\
          \ import load_summarize_chain\r\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\r\
          \ntext = 'travail.txt'\r\nwith open(text, 'r') as file:\r\n    essay = file.read()\r\
          \nfrom dotenv import load_dotenv\r\nload_dotenv()\r\nfrom langchain.llms\
          \ import HuggingFaceHub\r\nllm = HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
          , model_kwargs={\"temperature\":0.1})\r\ntext_splitter = RecursiveCharacterTextSplitter(\r\
          \n                                                separators=['\\n\\n',\
          \ '\\n', '(?=>\\. )', ' ', ''], \r\n                                   \
          \            chunk_size=3000, \r\n                                     \
          \          chunk_overlap=500\r\n                                       \
          \        )\r\ndocs = text_splitter.create_documents([essay])\r\nfrom langchain.prompts\
          \ import PromptTemplate\r\nmap_prompt = \"\"\"\r\nEcrit un r\xE9sum\xE9\
          \ pr\xE9cis en francais en 600 mots de :\r\n\"{text}\"\r\nR\xE9sum\xE9 pr\xE9\
          cis:\r\n\"\"\"\r\nmap_prompt_template = PromptTemplate(template=map_prompt,\
          \ input_variables=[\"text\"])\r\ncombine_prompt = \"\"\"\r\nEcrit un r\xE9\
          sum\xE9 pr\xE9cis en francais de :.\r\nRetourne 10 points les \xE9l\xE9\
          ments cl\xE9s du texte en francais.\r\n```{text}```\r\nR\xE9sum\xE9 par\
          \ points cl\xE9s:\r\n\"\"\"\r\ncombine_prompt_template = PromptTemplate(template=combine_prompt,\
          \ input_variables=[\"text\"])\r\nsummary_chain = load_summarize_chain(llm=llm,\r\
          \n                                     chain_type='map_reduce',\r\n    \
          \                                 map_prompt=map_prompt_template,\r\n  \
          \                                   combine_prompt=combine_prompt_template,\r\
          \n#                                      verbose=True\r\n              \
          \                      )\r\noutput = summary_chain.run(docs)\r\n\r\nanswer\
          \ (truncated):\r\n1. Giuseppe Rensi, philosophe italien, a \xE9crit en 1923\
          \ un essai intitul\xE9 \"Contre le travail\" dans lequel il critique la\
          \ morale de la soci\xE9t\xE9 capitaliste qui insiste sur une conception\
          \ du travail comme ph\xE9nom\xE8ne \xE9thico-religieux de grande importance.\r\
          \n2. Selon Rensi, le travail est une activit\xE9 ali\xE9nante et d\xE9sagr\xE9\
          able qui est souvent"
        updatedAt: '2024-01-07T22:27:52.099Z'
      numEdits: 0
      reactions: []
    id: 659b2568417c3c3ecde2a448
    type: comment
  author: ludomare
  content: "Hi,\r\nI try to query in french but query get a truncated response. I\
    \ try max_tokens=-1 in HuggingFaceHub(repo=...) or summary_chain.run(...) but\
    \ it didn't work, any idea?\r\n\r\ncode:\r\nfrom langchain.chains.summarize import\
    \ load_summarize_chain\r\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\r\
    \ntext = 'travail.txt'\r\nwith open(text, 'r') as file:\r\n    essay = file.read()\r\
    \nfrom dotenv import load_dotenv\r\nload_dotenv()\r\nfrom langchain.llms import\
    \ HuggingFaceHub\r\nllm = HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
    , model_kwargs={\"temperature\":0.1})\r\ntext_splitter = RecursiveCharacterTextSplitter(\r\
    \n                                                separators=['\\n\\n', '\\n',\
    \ '(?=>\\. )', ' ', ''], \r\n                                               chunk_size=3000,\
    \ \r\n                                               chunk_overlap=500\r\n   \
    \                                            )\r\ndocs = text_splitter.create_documents([essay])\r\
    \nfrom langchain.prompts import PromptTemplate\r\nmap_prompt = \"\"\"\r\nEcrit\
    \ un r\xE9sum\xE9 pr\xE9cis en francais en 600 mots de :\r\n\"{text}\"\r\nR\xE9\
    sum\xE9 pr\xE9cis:\r\n\"\"\"\r\nmap_prompt_template = PromptTemplate(template=map_prompt,\
    \ input_variables=[\"text\"])\r\ncombine_prompt = \"\"\"\r\nEcrit un r\xE9sum\xE9\
    \ pr\xE9cis en francais de :.\r\nRetourne 10 points les \xE9l\xE9ments cl\xE9\
    s du texte en francais.\r\n```{text}```\r\nR\xE9sum\xE9 par points cl\xE9s:\r\n\
    \"\"\"\r\ncombine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"\
    text\"])\r\nsummary_chain = load_summarize_chain(llm=llm,\r\n                \
    \                     chain_type='map_reduce',\r\n                           \
    \          map_prompt=map_prompt_template,\r\n                               \
    \      combine_prompt=combine_prompt_template,\r\n#                          \
    \            verbose=True\r\n                                    )\r\noutput =\
    \ summary_chain.run(docs)\r\n\r\nanswer (truncated):\r\n1. Giuseppe Rensi, philosophe\
    \ italien, a \xE9crit en 1923 un essai intitul\xE9 \"Contre le travail\" dans\
    \ lequel il critique la morale de la soci\xE9t\xE9 capitaliste qui insiste sur\
    \ une conception du travail comme ph\xE9nom\xE8ne \xE9thico-religieux de grande\
    \ importance.\r\n2. Selon Rensi, le travail est une activit\xE9 ali\xE9nante et\
    \ d\xE9sagr\xE9able qui est souvent"
  created_at: 2024-01-07 22:27:52+00:00
  edited: false
  hidden: false
  id: 659b2568417c3c3ecde2a448
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b13eeef07e7e6a0720df9727d58d661.svg
      fullname: "Fran\xE7ois Castagnos"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: castafra
      type: user
    createdAt: '2024-01-10T11:06:42.000Z'
    data:
      edited: false
      editors:
      - castafra
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9295566082000732
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b13eeef07e7e6a0720df9727d58d661.svg
          fullname: "Fran\xE7ois Castagnos"
          isHf: false
          isPro: false
          name: castafra
          type: user
        html: '<p>Hello,<br>I am encountering the same issue when trying the Inference
          API for the small model <a rel="nofollow" href="https://docs.mistral.ai/platform/endpoints/">https://docs.mistral.ai/platform/endpoints/</a>
          (which is Mixtral). Most answers seem to be of good quality but oftentimes,
          the answer is cut in the middle. The endpoint does not receive many different
          parameters and I have tested all of them, so maybe this is just a model
          problem ? </p>

          '
        raw: "Hello, \nI am encountering the same issue when trying the Inference\
          \ API for the small model https://docs.mistral.ai/platform/endpoints/ (which\
          \ is Mixtral). Most answers seem to be of good quality but oftentimes, the\
          \ answer is cut in the middle. The endpoint does not receive many different\
          \ parameters and I have tested all of them, so maybe this is just a model\
          \ problem ? "
        updatedAt: '2024-01-10T11:06:42.262Z'
      numEdits: 0
      reactions: []
    id: 659e7a42887c60124c873386
    type: comment
  author: castafra
  content: "Hello, \nI am encountering the same issue when trying the Inference API\
    \ for the small model https://docs.mistral.ai/platform/endpoints/ (which is Mixtral).\
    \ Most answers seem to be of good quality but oftentimes, the answer is cut in\
    \ the middle. The endpoint does not receive many different parameters and I have\
    \ tested all of them, so maybe this is just a model problem ? "
  created_at: 2024-01-10 11:06:42+00:00
  edited: false
  hidden: false
  id: 659e7a42887c60124c873386
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-12T07:31:41.000Z'
    data:
      edited: false
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8479377627372742
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: '<p>Just to be sure, you are using the prompt format provided? </p>

          '
        raw: 'Just to be sure, you are using the prompt format provided? '
        updatedAt: '2024-01-12T07:31:41.569Z'
      numEdits: 0
      reactions: []
    id: 65a0eadd90eb7a15241eade3
    type: comment
  author: GreyForever
  content: 'Just to be sure, you are using the prompt format provided? '
  created_at: 2024-01-12 07:31:41+00:00
  edited: false
  hidden: false
  id: 65a0eadd90eb7a15241eade3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/baff8826f875bb1af69937b300079c2b.svg
      fullname: mare
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ludomare
      type: user
    createdAt: '2024-01-12T11:03:00.000Z'
    data:
      edited: false
      editors:
      - ludomare
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7399469017982483
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/baff8826f875bb1af69937b300079c2b.svg
          fullname: mare
          isHf: false
          isPro: false
          name: ludomare
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;GreyForever&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/GreyForever\"\
          >@<span class=\"underline\">GreyForever</span></a></span>\n\n\t</span></span>\
          \ : what do you mean??</p>\n"
        raw: '@GreyForever : what do you mean??

          '
        updatedAt: '2024-01-12T11:03:00.098Z'
      numEdits: 0
      reactions: []
    id: 65a11c64151f73cb588d2f8e
    type: comment
  author: ludomare
  content: '@GreyForever : what do you mean??

    '
  created_at: 2024-01-12 11:03:00+00:00
  edited: false
  hidden: false
  id: 65a11c64151f73cb588d2f8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/640261025caf6d21d67d01ba/NHAGdq4SEjt6B0QWXncUS.jpeg?w=200&h=200&f=face
      fullname: Shaun Prince
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Suparious
      type: user
    createdAt: '2024-01-12T13:38:16.000Z'
    data:
      edited: false
      editors:
      - Suparious
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8302513360977173
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/640261025caf6d21d67d01ba/NHAGdq4SEjt6B0QWXncUS.jpeg?w=200&h=200&f=face
          fullname: Shaun Prince
          isHf: false
          isPro: false
          name: Suparious
          type: user
        html: '<p>Your var <code>map_prompt</code> is not using the correct prompt
          format, just like the others have mentioned.</p>

          <p>There is an example for you in the README.md, it looks like this:</p>

          <pre><code>&lt;s&gt; [INST] Instruction [/INST] Model answer&lt;/s&gt; [INST]
          Follow-up instruction [/INST]

          </code></pre>

          '
        raw: 'Your var `map_prompt` is not using the correct prompt format, just like
          the others have mentioned.


          There is an example for you in the README.md, it looks like this:


          ```

          <s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction
          [/INST]

          ```'
        updatedAt: '2024-01-12T13:38:16.665Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - GreyForever
    id: 65a140c80c8d993b17583b8d
    type: comment
  author: Suparious
  content: 'Your var `map_prompt` is not using the correct prompt format, just like
    the others have mentioned.


    There is an example for you in the README.md, it looks like this:


    ```

    <s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]

    ```'
  created_at: 2024-01-12 13:38:16+00:00
  edited: false
  hidden: false
  id: 65a140c80c8d993b17583b8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/640261025caf6d21d67d01ba/NHAGdq4SEjt6B0QWXncUS.jpeg?w=200&h=200&f=face
      fullname: Shaun Prince
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Suparious
      type: user
    createdAt: '2024-01-12T13:41:13.000Z'
    data:
      edited: false
      editors:
      - Suparious
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7055092453956604
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/640261025caf6d21d67d01ba/NHAGdq4SEjt6B0QWXncUS.jpeg?w=200&h=200&f=face
          fullname: Shaun Prince
          isHf: false
          isPro: false
          name: Suparious
          type: user
        html: '<p>Also, try setting <code>min_tokens</code>, <code>max_tokens</code>
          and / or <code>max_new_tokens</code> depending on what API you are using
          - do this in addition to fixing your prompt template.</p>

          '
        raw: Also, try setting `min_tokens`, `max_tokens` and / or `max_new_tokens`
          depending on what API you are using - do this in addition to fixing your
          prompt template.
        updatedAt: '2024-01-12T13:41:13.975Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - GreyForever
    id: 65a14179b1f0788359e892b3
    type: comment
  author: Suparious
  content: Also, try setting `min_tokens`, `max_tokens` and / or `max_new_tokens`
    depending on what API you are using - do this in addition to fixing your prompt
    template.
  created_at: 2024-01-12 13:41:13+00:00
  edited: false
  hidden: false
  id: 65a14179b1f0788359e892b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-12T13:44:35.000Z'
    data:
      edited: true
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.955351710319519
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;GreyForever&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/GreyForever\"\
          >@<span class=\"underline\">GreyForever</span></a></span>\n\n\t</span></span>\
          \ : what do you mean??</p>\n</blockquote>\n<p>As stated by Suparious, this\
          \ model, the Instruct version, was trained to be able to follow instructions.\
          \ But you are required to follow the prompt format like all LLMs similar\
          \ to this one. If you don't then it's almost the same as using the base\
          \ model (mistralai/Mixtral-8x7B-v0.1), so to have the Instruct version work\
          \ properly you are required to follow the prompt format they used to train\
          \ it.</p>\n"
        raw: '> @GreyForever : what do you mean??


          As stated by Suparious, this model, the Instruct version, was trained to
          be able to follow instructions. But you are required to follow the prompt
          format like all LLMs similar to this one. If you don''t then it''s almost
          the same as using the base model (mistralai/Mixtral-8x7B-v0.1), so to have
          the Instruct version work properly you are required to follow the prompt
          format they used to train it.

          '
        updatedAt: '2024-01-12T13:44:56.606Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Suparious
    id: 65a14243df9a2ca6209134b7
    type: comment
  author: GreyForever
  content: '> @GreyForever : what do you mean??


    As stated by Suparious, this model, the Instruct version, was trained to be able
    to follow instructions. But you are required to follow the prompt format like
    all LLMs similar to this one. If you don''t then it''s almost the same as using
    the base model (mistralai/Mixtral-8x7B-v0.1), so to have the Instruct version
    work properly you are required to follow the prompt format they used to train
    it.

    '
  created_at: 2024-01-12 13:44:35+00:00
  edited: true
  hidden: false
  id: 65a14243df9a2ca6209134b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/baff8826f875bb1af69937b300079c2b.svg
      fullname: mare
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ludomare
      type: user
    createdAt: '2024-01-12T15:10:34.000Z'
    data:
      edited: true
      editors:
      - ludomare
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2591281533241272
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/baff8826f875bb1af69937b300079c2b.svg
          fullname: mare
          isHf: false
          isPro: false
          name: ludomare
          type: user
        html: "<p>thank you for your help !!.....<br>but in my example, map_prompt\
          \ should be like:</p>\n<p>map_prompt =  \"\"\"[INST]<br>Ecrit un r\xE9sum\xE9\
          \ pr\xE9cis en francais en 1000 mots de :<br>\"{text}\"<br>R\xE9sum\xE9\
          \ pr\xE9cis:<br>[/INST]\"\"\"<br>map_prompt_template = PromptTemplate(template=map_prompt,\
          \ input_variables=[\"text\"])</p>\n<p>because  it doesn't work, the output\
          \ is always truncated </p>\n<p>output:</p>\n<ul>\n<li>Giuseppe Rensi, un\
          \ philosophe italien de V\xE9rone, a \xE9crit \"Contre le travail\" en 1923,\
          \ o\xF9 il argue que les humains ont une relation ambivalente avec le travail,\
          \ le consid\xE9rant \xE0 la fois n\xE9cessaire et ali\xE9nant.</li>\n<li>Simone\
          \ Weil, dans son essai \"Le travail et la culture\", explore le paradoxe\
          \ moral entourant le travail, qui est consid\xE9r\xE9 comme une</li>\n</ul>\n"
        raw: "thank you for your help !!.....\nbut in my example, map_prompt should\
          \ be like:\n\nmap_prompt =  \"\"\"[INST]\nEcrit un r\xE9sum\xE9 pr\xE9cis\
          \ en francais en 1000 mots de :\n\"{text}\"\nR\xE9sum\xE9 pr\xE9cis:\n[/INST]\"\
          \"\"\nmap_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"\
          text\"])\n\nbecause  it doesn't work, the output is always truncated \n\n\
          output:\n* Giuseppe Rensi, un philosophe italien de V\xE9rone, a \xE9crit\
          \ \"Contre le travail\" en 1923, o\xF9 il argue que les humains ont une\
          \ relation ambivalente avec le travail, le consid\xE9rant \xE0 la fois n\xE9\
          cessaire et ali\xE9nant.\n* Simone Weil, dans son essai \"Le travail et\
          \ la culture\", explore le paradoxe moral entourant le travail, qui est\
          \ consid\xE9r\xE9 comme une"
        updatedAt: '2024-01-12T15:27:58.213Z'
      numEdits: 3
      reactions: []
    id: 65a1566adb07a34736aba40b
    type: comment
  author: ludomare
  content: "thank you for your help !!.....\nbut in my example, map_prompt should\
    \ be like:\n\nmap_prompt =  \"\"\"[INST]\nEcrit un r\xE9sum\xE9 pr\xE9cis en francais\
    \ en 1000 mots de :\n\"{text}\"\nR\xE9sum\xE9 pr\xE9cis:\n[/INST]\"\"\"\nmap_prompt_template\
    \ = PromptTemplate(template=map_prompt, input_variables=[\"text\"])\n\nbecause\
    \  it doesn't work, the output is always truncated \n\noutput:\n* Giuseppe Rensi,\
    \ un philosophe italien de V\xE9rone, a \xE9crit \"Contre le travail\" en 1923,\
    \ o\xF9 il argue que les humains ont une relation ambivalente avec le travail,\
    \ le consid\xE9rant \xE0 la fois n\xE9cessaire et ali\xE9nant.\n* Simone Weil,\
    \ dans son essai \"Le travail et la culture\", explore le paradoxe moral entourant\
    \ le travail, qui est consid\xE9r\xE9 comme une"
  created_at: 2024-01-12 15:10:34+00:00
  edited: true
  hidden: false
  id: 65a1566adb07a34736aba40b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b13eeef07e7e6a0720df9727d58d661.svg
      fullname: "Fran\xE7ois Castagnos"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: castafra
      type: user
    createdAt: '2024-01-12T15:12:41.000Z'
    data:
      edited: false
      editors:
      - castafra
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.42874887585639954
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b13eeef07e7e6a0720df9727d58d661.svg
          fullname: "Fran\xE7ois Castagnos"
          isHf: false
          isPro: false
          name: castafra
          type: user
        html: "<p>I, on my side, am using the Python client (<a rel=\"nofollow\" href=\"\
          https://github.com/mistralai/client-python\">https://github.com/mistralai/client-python</a>)\
          \ to call the API endpoint provided by Mistral (not free).<br>Here is an\
          \ exemple of how I send my prompt : </p>\n<pre><code>\ndef format_prompt(prompt:\
          \ List[Dict[str, str]]) -&gt; str:\n        \"\"\"Format input prompt to\
          \ the Mistral API format\n        \"\"\"\n        prompt = [\n         \
          \   ChatMessage(role=item['role'], content=item['content']) for item in\
          \ prompt\n        ]\n        return prompt\n\ndef call_inference_endpoint(client:\
          \ MistralClient, prompt: List[ChatMessage], **kwargs):\n        \"\"\"Get\
          \ inference from the Mistral API\n        \"\"\"\n        inference = client.chat(\n\
          \            model=\"mistral-medium\",\n            messages=prompt,\n \
          \           temperature=kwargs[\"temperature\"],\n            max_tokens=kwargs[\"\
          max_tokens\"],\n            top_p=kwargs[\"top_p\"])\n\n        return inference\n\
          \nclient = MistralClient(\n            api_key=\"MY_API_KEY\")\nprompt =\
          \ [{\"role\": \"system\", \"content\": \"Tu es un assistant IA\"}, {\n \
          \       \"role\": \"user\", \"content\": \"Quelles sont les dix premi\xE8\
          res d\xE9cimales de pi ?\"}]\n\nresult = call_inference_endpoint(client,\
          \ prompt, temperature=0.2, max_tokens=100,\n                           \
          \       top_p=1, frequency_penalty=1)\n</code></pre>\n<p>Sometimes the response\
          \ is cut in the middle, especially if I ask to enumerate. Setting a greater\
          \ value of <code>max_tokens</code> does not change anything.</p>\n"
        raw: "I, on my side, am using the Python client (https://github.com/mistralai/client-python)\
          \ to call the API endpoint provided by Mistral (not free). \nHere is an\
          \ exemple of how I send my prompt : \n```\n\ndef format_prompt(prompt: List[Dict[str,\
          \ str]]) -> str:\n        \"\"\"Format input prompt to the Mistral API format\n\
          \        \"\"\"\n        prompt = [\n            ChatMessage(role=item['role'],\
          \ content=item['content']) for item in prompt\n        ]\n        return\
          \ prompt\n\ndef call_inference_endpoint(client: MistralClient, prompt: List[ChatMessage],\
          \ **kwargs):\n        \"\"\"Get inference from the Mistral API\n       \
          \ \"\"\"\n        inference = client.chat(\n            model=\"mistral-medium\"\
          ,\n            messages=prompt,\n            temperature=kwargs[\"temperature\"\
          ],\n            max_tokens=kwargs[\"max_tokens\"],\n            top_p=kwargs[\"\
          top_p\"])\n\n        return inference\n\nclient = MistralClient(\n     \
          \       api_key=\"MY_API_KEY\")\nprompt = [{\"role\": \"system\", \"content\"\
          : \"Tu es un assistant IA\"}, {\n        \"role\": \"user\", \"content\"\
          : \"Quelles sont les dix premi\xE8res d\xE9cimales de pi ?\"}]\n\nresult\
          \ = call_inference_endpoint(client, prompt, temperature=0.2, max_tokens=100,\n\
          \                                  top_p=1, frequency_penalty=1)\n```\n\n\
          Sometimes the response is cut in the middle, especially if I ask to enumerate.\
          \ Setting a greater value of `max_tokens` does not change anything."
        updatedAt: '2024-01-12T15:12:41.272Z'
      numEdits: 0
      reactions: []
    id: 65a156e92ff5b107a00a44af
    type: comment
  author: castafra
  content: "I, on my side, am using the Python client (https://github.com/mistralai/client-python)\
    \ to call the API endpoint provided by Mistral (not free). \nHere is an exemple\
    \ of how I send my prompt : \n```\n\ndef format_prompt(prompt: List[Dict[str,\
    \ str]]) -> str:\n        \"\"\"Format input prompt to the Mistral API format\n\
    \        \"\"\"\n        prompt = [\n            ChatMessage(role=item['role'],\
    \ content=item['content']) for item in prompt\n        ]\n        return prompt\n\
    \ndef call_inference_endpoint(client: MistralClient, prompt: List[ChatMessage],\
    \ **kwargs):\n        \"\"\"Get inference from the Mistral API\n        \"\"\"\
    \n        inference = client.chat(\n            model=\"mistral-medium\",\n  \
    \          messages=prompt,\n            temperature=kwargs[\"temperature\"],\n\
    \            max_tokens=kwargs[\"max_tokens\"],\n            top_p=kwargs[\"top_p\"\
    ])\n\n        return inference\n\nclient = MistralClient(\n            api_key=\"\
    MY_API_KEY\")\nprompt = [{\"role\": \"system\", \"content\": \"Tu es un assistant\
    \ IA\"}, {\n        \"role\": \"user\", \"content\": \"Quelles sont les dix premi\xE8\
    res d\xE9cimales de pi ?\"}]\n\nresult = call_inference_endpoint(client, prompt,\
    \ temperature=0.2, max_tokens=100,\n                                  top_p=1,\
    \ frequency_penalty=1)\n```\n\nSometimes the response is cut in the middle, especially\
    \ if I ask to enumerate. Setting a greater value of `max_tokens` does not change\
    \ anything."
  created_at: 2024-01-12 15:12:41+00:00
  edited: false
  hidden: false
  id: 65a156e92ff5b107a00a44af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-12T15:13:41.000Z'
    data:
      edited: false
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7800489068031311
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: '<p>Use ''max_new_tokens'' instead, should solve the problem ! </p>

          '
        raw: 'Use ''max_new_tokens'' instead, should solve the problem ! '
        updatedAt: '2024-01-12T15:13:41.946Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Suparious
    id: 65a15725faba090dc488e8ea
    type: comment
  author: GreyForever
  content: 'Use ''max_new_tokens'' instead, should solve the problem ! '
  created_at: 2024-01-12 15:13:41+00:00
  edited: false
  hidden: false
  id: 65a15725faba090dc488e8ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-12T15:16:55.000Z'
    data:
      edited: true
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8134039640426636
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: '<blockquote>

          <p>Use ''max_new_tokens'' instead, should solve the problem !</p>

          </blockquote>

          <p>Oh I''m sorry I forgot mistral does not use the same parameters hugging
          face uses, it wont work. Try it still.</p>

          '
        raw: '> Use ''max_new_tokens'' instead, should solve the problem !


          Oh I''m sorry I forgot mistral does not use the same parameters hugging
          face uses, it wont work. Try it still.


          '
        updatedAt: '2024-01-12T15:17:15.497Z'
      numEdits: 1
      reactions: []
    id: 65a157e72ff5b107a00ab744
    type: comment
  author: GreyForever
  content: '> Use ''max_new_tokens'' instead, should solve the problem !


    Oh I''m sorry I forgot mistral does not use the same parameters hugging face uses,
    it wont work. Try it still.


    '
  created_at: 2024-01-12 15:16:55+00:00
  edited: true
  hidden: false
  id: 65a157e72ff5b107a00ab744
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-12T15:21:29.000Z'
    data:
      edited: false
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.30650079250335693
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: "<blockquote>\n<p>thank you for your help !!.....<br>but in my example,\
          \ map_prompt should be like:</p>\n<p>map_prompt =  [INST]\"\"\"<br>Ecrit\
          \ un r\xE9sum\xE9 pr\xE9cis en francais en 600 mots de :<br>\"{text}\"<br>R\xE9\
          sum\xE9 pr\xE9cis:<br>\"\"\"[/INST]<br>map_prompt_template = PromptTemplate(template=map_prompt,\
          \ input_variables=[\"text\"])</p>\n<p>because  it doesn't work, I must do\
          \ stomething stupid :(</p>\n</blockquote>\n<p>why did u put [INST] before\
          \ \"\"\" ? inst it supposed to be: map_prompt = \"[INST] Ecrit un r\xE9\
          sum\xE9 pr\xE9cis en francais en 600 mots de :{text}R\xE9sum\xE9 pr\xE9\
          cis: [/INST]\"<br>I might be missing something tho...</p>\n"
        raw: "> thank you for your help !!.....\n> but in my example, map_prompt should\
          \ be like:\n> \n> map_prompt =  [INST]\"\"\"\n> Ecrit un r\xE9sum\xE9 pr\xE9\
          cis en francais en 600 mots de :\n> \"{text}\"\n> R\xE9sum\xE9 pr\xE9cis:\n\
          > \"\"\"[/INST]\n> map_prompt_template = PromptTemplate(template=map_prompt,\
          \ input_variables=[\"text\"])\n> \n> because  it doesn't work, I must do\
          \ stomething stupid :(\n\nwhy did u put [INST] before \"\"\" ? inst it supposed\
          \ to be: map_prompt = \"[INST] Ecrit un r\xE9sum\xE9 pr\xE9cis en francais\
          \ en 600 mots de :{text}R\xE9sum\xE9 pr\xE9cis: [/INST]\"\nI might be missing\
          \ something tho..."
        updatedAt: '2024-01-12T15:21:29.545Z'
      numEdits: 0
      reactions: []
    id: 65a158f90b4890c47ac9a737
    type: comment
  author: GreyForever
  content: "> thank you for your help !!.....\n> but in my example, map_prompt should\
    \ be like:\n> \n> map_prompt =  [INST]\"\"\"\n> Ecrit un r\xE9sum\xE9 pr\xE9cis\
    \ en francais en 600 mots de :\n> \"{text}\"\n> R\xE9sum\xE9 pr\xE9cis:\n> \"\"\
    \"[/INST]\n> map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"\
    text\"])\n> \n> because  it doesn't work, I must do stomething stupid :(\n\nwhy\
    \ did u put [INST] before \"\"\" ? inst it supposed to be: map_prompt = \"[INST]\
    \ Ecrit un r\xE9sum\xE9 pr\xE9cis en francais en 600 mots de :{text}R\xE9sum\xE9\
    \ pr\xE9cis: [/INST]\"\nI might be missing something tho..."
  created_at: 2024-01-12 15:21:29+00:00
  edited: false
  hidden: false
  id: 65a158f90b4890c47ac9a737
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/baff8826f875bb1af69937b300079c2b.svg
      fullname: mare
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ludomare
      type: user
    createdAt: '2024-01-12T15:40:23.000Z'
    data:
      edited: false
      editors:
      - ludomare
      hidden: false
      identifiedLanguage:
        language: fr
        probability: 0.37801897525787354
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/baff8826f875bb1af69937b300079c2b.svg
          fullname: mare
          isHf: false
          isPro: false
          name: ludomare
          type: user
        html: "<p>why did u put [INST] before \"\"\" ? inst it supposed to be: map_prompt\
          \ = \"[INST] Ecrit un r\xE9sum\xE9 pr\xE9cis en francais en 600 mots de\
          \ :{text}R\xE9sum\xE9 pr\xE9cis: [/INST]\"<br>I might be missing something\
          \ tho.</p>\n<p>----&gt;I change by         map_prompt = \"[INST] Ecrit un\
          \ r\xE9sum\xE9 pr\xE9cis en francais en 600 mots de :{text}R\xE9sum\xE9\
          \ pr\xE9cis: [/INST]\"<br>still doesn't work, output truncated :(</p>\n"
        raw: "why did u put [INST] before \"\"\" ? inst it supposed to be: map_prompt\
          \ = \"[INST] Ecrit un r\xE9sum\xE9 pr\xE9cis en francais en 600 mots de\
          \ :{text}R\xE9sum\xE9 pr\xE9cis: [/INST]\"\nI might be missing something\
          \ tho.\n\n\n---->I change by         map_prompt = \"[INST] Ecrit un r\xE9\
          sum\xE9 pr\xE9cis en francais en 600 mots de :{text}R\xE9sum\xE9 pr\xE9\
          cis: [/INST]\"\nstill doesn't work, output truncated :("
        updatedAt: '2024-01-12T15:40:23.579Z'
      numEdits: 0
      reactions: []
    id: 65a15d6733eca76f463f7cac
    type: comment
  author: ludomare
  content: "why did u put [INST] before \"\"\" ? inst it supposed to be: map_prompt\
    \ = \"[INST] Ecrit un r\xE9sum\xE9 pr\xE9cis en francais en 600 mots de :{text}R\xE9\
    sum\xE9 pr\xE9cis: [/INST]\"\nI might be missing something tho.\n\n\n---->I change\
    \ by         map_prompt = \"[INST] Ecrit un r\xE9sum\xE9 pr\xE9cis en francais\
    \ en 600 mots de :{text}R\xE9sum\xE9 pr\xE9cis: [/INST]\"\nstill doesn't work,\
    \ output truncated :("
  created_at: 2024-01-12 15:40:23+00:00
  edited: false
  hidden: false
  id: 65a15d6733eca76f463f7cac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-12T15:54:54.000Z'
    data:
      edited: false
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7838330864906311
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: "<p>Then in my opinion all comes up to the \xE9quivalent of max_new_tokens\
          \ like we stated before. By default this kind of things have a quite short\
          \ max length token output.  Try to find/search a parameter that allows u\
          \ to overwrite this. I'm not used to this library/langchain, but it should\
          \ be a parameter similar to \"max_tokens\".</p>\n"
        raw: "Then in my opinion all comes up to the \xE9quivalent of max_new_tokens\
          \ like we stated before. By default this kind of things have a quite short\
          \ max length token output.  Try to find/search a parameter that allows u\
          \ to overwrite this. I'm not used to this library/langchain, but it should\
          \ be a parameter similar to \"max_tokens\"."
        updatedAt: '2024-01-12T15:54:54.780Z'
      numEdits: 0
      reactions: []
    id: 65a160cedacb2a978c3793d0
    type: comment
  author: GreyForever
  content: "Then in my opinion all comes up to the \xE9quivalent of max_new_tokens\
    \ like we stated before. By default this kind of things have a quite short max\
    \ length token output.  Try to find/search a parameter that allows u to overwrite\
    \ this. I'm not used to this library/langchain, but it should be a parameter similar\
    \ to \"max_tokens\"."
  created_at: 2024-01-12 15:54:54+00:00
  edited: false
  hidden: false
  id: 65a160cedacb2a978c3793d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-12T16:00:18.000Z'
    data:
      edited: true
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6465097069740295
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: '<p>To be more exact, where u insert the temperature try to add a new
          parameter "max_new_tokens" with the value you want, try a big one like 2000.</p>

          <p>I cannot test it right now so I''m just trying to give advice, if nothing
          works I will check it later.</p>

          '
        raw: 'To be more exact, where u insert the temperature try to add a new parameter
          "max_new_tokens" with the value you want, try a big one like 2000.


          I cannot test it right now so I''m just trying to give advice, if nothing
          works I will check it later.'
        updatedAt: '2024-01-12T16:00:56.319Z'
      numEdits: 1
      reactions: []
    id: 65a16212dacb2a978c37da42
    type: comment
  author: GreyForever
  content: 'To be more exact, where u insert the temperature try to add a new parameter
    "max_new_tokens" with the value you want, try a big one like 2000.


    I cannot test it right now so I''m just trying to give advice, if nothing works
    I will check it later.'
  created_at: 2024-01-12 16:00:18+00:00
  edited: true
  hidden: false
  id: 65a16212dacb2a978c37da42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/baff8826f875bb1af69937b300079c2b.svg
      fullname: mare
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ludomare
      type: user
    createdAt: '2024-01-12T18:29:52.000Z'
    data:
      edited: true
      editors:
      - ludomare
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9093465209007263
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/baff8826f875bb1af69937b300079c2b.svg
          fullname: mare
          isHf: false
          isPro: false
          name: ludomare
          type: user
        html: '<p>thank you a lot for your help  !!! it works !!! "max_new_tokens"
          was the solution !!</p>

          <p>ps: I try something to test  :  increasing the temperature makes all  crash...strange...but
          with temperature at 0.1 all fine !! thank you again  </p>

          '
        raw: 'thank you a lot for your help  !!! it works !!! "max_new_tokens" was
          the solution !!


          ps: I try something to test  :  increasing the temperature makes all  crash...strange...but
          with temperature at 0.1 all fine !! thank you again  '
        updatedAt: '2024-01-12T19:33:44.871Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - GreyForever
    id: 65a185201d0037b6ceda1367
    type: comment
  author: ludomare
  content: 'thank you a lot for your help  !!! it works !!! "max_new_tokens" was the
    solution !!


    ps: I try something to test  :  increasing the temperature makes all  crash...strange...but
    with temperature at 0.1 all fine !! thank you again  '
  created_at: 2024-01-12 18:29:52+00:00
  edited: true
  hidden: false
  id: 65a185201d0037b6ceda1367
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-12T19:28:37.000Z'
    data:
      edited: false
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9063341617584229
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: '<p>I recommend you to avoid temperature values too high or too low,
          values around 1 (not 1, as it becomes deterministic) is perfect.</p>

          <p>Nevertheless I am glad to be of help ! Have fun !</p>

          '
        raw: 'I recommend you to avoid temperature values too high or too low, values
          around 1 (not 1, as it becomes deterministic) is perfect.


          Nevertheless I am glad to be of help ! Have fun !'
        updatedAt: '2024-01-12T19:28:37.203Z'
      numEdits: 0
      reactions: []
    id: 65a192e5e8fe70d60ca8056c
    type: comment
  author: GreyForever
  content: 'I recommend you to avoid temperature values too high or too low, values
    around 1 (not 1, as it becomes deterministic) is perfect.


    Nevertheless I am glad to be of help ! Have fun !'
  created_at: 2024-01-12 19:28:37+00:00
  edited: false
  hidden: false
  id: 65a192e5e8fe70d60ca8056c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 84
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Response content was truncated
