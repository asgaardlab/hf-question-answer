!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Panckackes
conflicting_files: null
created_at: 2024-01-23 13:42:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e060675d4e9e15d08a8427ff6bbdf6bc.svg
      fullname: Arthur Panckoucke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Panckackes
      type: user
    createdAt: '2024-01-23T13:42:49.000Z'
    data:
      edited: false
      editors:
      - Panckackes
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5690920352935791
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e060675d4e9e15d08a8427ff6bbdf6bc.svg
          fullname: Arthur Panckoucke
          isHf: false
          isPro: false
          name: Panckackes
          type: user
        html: '<p>hello everyone when I try to use mixtral at work to integrate it
          into a work process, however when I use the same prompt in hugging face
          chat and locally I don''t get the same results at all, locally the results
          are absolutely unusable.</p>

          <p>here is my code : </p>

          <p>#install packages<br>!pip install -q langchain<br>!pip install -q transformers<br>!pip
          install -q ctransformers[cuda]</p>

          <p>#Import the Model<br>from transformers import AutoModelForCausalLM, AutoTokenizer<br>device
          = "cuda" # the device to load the model onto<br>#model = AutoModelForCausalLM.from_pretrained("bigcode/octocoder",
          load_in_8bit=True, pad_token_id=0)<br>model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")<br>tokenizer
          = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")<br>tokenizer.pad_token
          = tokenizer.eos_token<br>tokenizer.padding_side = "right"</p>

          <p>from transformers import pipeline<br>text_generation_pipeline = transformers.pipeline(<br>    model=model,<br>    tokenizer=tokenizer,<br>    task="text-generation",<br>    temperature=0.2,<br>    repetition_penalty=1.2,<br>    max_new_tokens=500,<br>    device=0,
          # -1 CPU, 0 GPU<br>    top_k=50,<br>    top_p=0.95,<br>    do_sample=True,<br>    pad_token_id
          = 50256,<br>)<br>mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)</p>

          <p>#Define the Prompt<br>prompt = """</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64dfb95111cfae29f608182e/EO0YaZ5v7REyC91LB3Kmz.png"><img
          alt="Hugging_Face_Mixtral.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/64dfb95111cfae29f608182e/EO0YaZ5v7REyC91LB3Kmz.png"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64dfb95111cfae29f608182e/lF3QZW8kf4aDCQb2FMPSG.png"><img
          alt="Local_Mixtral.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/64dfb95111cfae29f608182e/lF3QZW8kf4aDCQb2FMPSG.png"></a></p>

          <p>I have the exact same parameters as describe in the source code of hugging
          face (temperature, etc) but I get something unusable locally and something
          super usable on hugging face chat.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64dfb95111cfae29f608182e/TjExWEpUmRI25MsacaPSX.png"><img
          alt="Local_Mixtral.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/64dfb95111cfae29f608182e/TjExWEpUmRI25MsacaPSX.png"></a></p>

          '
        raw: "hello everyone when I try to use mixtral at work to integrate it into\
          \ a work process, however when I use the same prompt in hugging face chat\
          \ and locally I don't get the same results at all, locally the results are\
          \ absolutely unusable.\r\n\r\nhere is my code : \r\n\r\n#install packages\r\
          \n!pip install -q langchain\r\n!pip install -q transformers\r\n!pip install\
          \ -q ctransformers[cuda]\r\n\r\n#Import the Model\r\nfrom transformers import\
          \ AutoModelForCausalLM, AutoTokenizer\r\ndevice = \"cuda\" # the device\
          \ to load the model onto\r\n#model = AutoModelForCausalLM.from_pretrained(\"\
          bigcode/octocoder\", load_in_8bit=True, pad_token_id=0)\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          mistralai/Mistral-7B-Instruct-v0.1\")\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          mistralai/Mistral-7B-Instruct-v0.1\")\r\ntokenizer.pad_token = tokenizer.eos_token\r\
          \ntokenizer.padding_side = \"right\"\r\n\r\nfrom transformers import pipeline\r\
          \ntext_generation_pipeline = transformers.pipeline(\r\n    model=model,\r\
          \n    tokenizer=tokenizer,\r\n    task=\"text-generation\",\r\n    temperature=0.2,\r\
          \n    repetition_penalty=1.2,\r\n    max_new_tokens=500,\r\n    device=0,\
          \ # -1 CPU, 0 GPU\r\n    top_k=50, \r\n    top_p=0.95,\r\n    do_sample=True,\r\
          \n    pad_token_id = 50256,\r\n)\r\nmistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\r\
          \n\r\n#Define the Prompt\r\nprompt = \"\"\"\r\n\r\n![Hugging_Face_Mixtral.PNG](https://cdn-uploads.huggingface.co/production/uploads/64dfb95111cfae29f608182e/EO0YaZ5v7REyC91LB3Kmz.png)\r\
          \n![Local_Mixtral.PNG](https://cdn-uploads.huggingface.co/production/uploads/64dfb95111cfae29f608182e/lF3QZW8kf4aDCQb2FMPSG.png)\r\
          \n\r\n\r\nI have the exact same parameters as describe in the source code\
          \ of hugging face (temperature, etc) but I get something unusable locally\
          \ and something super usable on hugging face chat.\r\n\r\n\r\n\r\n![Local_Mixtral.PNG](https://cdn-uploads.huggingface.co/production/uploads/64dfb95111cfae29f608182e/TjExWEpUmRI25MsacaPSX.png)\r\
          \n"
        updatedAt: '2024-01-23T13:42:49.276Z'
      numEdits: 0
      reactions: []
    id: 65afc2593e876a6389ba361c
    type: comment
  author: Panckackes
  content: "hello everyone when I try to use mixtral at work to integrate it into\
    \ a work process, however when I use the same prompt in hugging face chat and\
    \ locally I don't get the same results at all, locally the results are absolutely\
    \ unusable.\r\n\r\nhere is my code : \r\n\r\n#install packages\r\n!pip install\
    \ -q langchain\r\n!pip install -q transformers\r\n!pip install -q ctransformers[cuda]\r\
    \n\r\n#Import the Model\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
    \ndevice = \"cuda\" # the device to load the model onto\r\n#model = AutoModelForCausalLM.from_pretrained(\"\
    bigcode/octocoder\", load_in_8bit=True, pad_token_id=0)\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    mistralai/Mistral-7B-Instruct-v0.1\")\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    mistralai/Mistral-7B-Instruct-v0.1\")\r\ntokenizer.pad_token = tokenizer.eos_token\r\
    \ntokenizer.padding_side = \"right\"\r\n\r\nfrom transformers import pipeline\r\
    \ntext_generation_pipeline = transformers.pipeline(\r\n    model=model,\r\n  \
    \  tokenizer=tokenizer,\r\n    task=\"text-generation\",\r\n    temperature=0.2,\r\
    \n    repetition_penalty=1.2,\r\n    max_new_tokens=500,\r\n    device=0, # -1\
    \ CPU, 0 GPU\r\n    top_k=50, \r\n    top_p=0.95,\r\n    do_sample=True,\r\n \
    \   pad_token_id = 50256,\r\n)\r\nmistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\r\
    \n\r\n#Define the Prompt\r\nprompt = \"\"\"\r\n\r\n![Hugging_Face_Mixtral.PNG](https://cdn-uploads.huggingface.co/production/uploads/64dfb95111cfae29f608182e/EO0YaZ5v7REyC91LB3Kmz.png)\r\
    \n![Local_Mixtral.PNG](https://cdn-uploads.huggingface.co/production/uploads/64dfb95111cfae29f608182e/lF3QZW8kf4aDCQb2FMPSG.png)\r\
    \n\r\n\r\nI have the exact same parameters as describe in the source code of hugging\
    \ face (temperature, etc) but I get something unusable locally and something super\
    \ usable on hugging face chat.\r\n\r\n\r\n\r\n![Local_Mixtral.PNG](https://cdn-uploads.huggingface.co/production/uploads/64dfb95111cfae29f608182e/TjExWEpUmRI25MsacaPSX.png)\r\
    \n"
  created_at: 2024-01-23 13:42:49+00:00
  edited: false
  hidden: false
  id: 65afc2593e876a6389ba361c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2024-01-24T11:33:50.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8322546482086182
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: "<p>Have you tried to look into the prompt template? In my experience,\
          \ Mistral and Mixtral models are super sensitive with those symbols, one\
          \ extra space and you are on a very wrong path in the response! You can\
          \ use HuggingChat UI to see the actual prompt that goes into the LLM, so\
          \ when you see a good response just click on that download button in the\
          \ user's question:</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/TA0p7_5Hs2Uszt7W4iZgV.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/TA0p7_5Hs2Uszt7W4iZgV.png\"\
          ></a></p>\n<p>That will show you exactly how the prompt was constructed\
          \ (in addition to generation configs)</p>\n<pre><code>{\n  \"note\": \"\
          This is a preview of the prompt that will be sent to the model when retrying\
          \ the message. It may differ from what was sent in the past if the parameters\
          \ have been updated since\",\n  \"prompt\": \"&lt;s&gt; [INST]You are a\
          \ helpful assistant.\\n This is a test, just say test back! [/INST]\",\n\
          \  \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n  \"parameters\"\
          : {\n    \"temperature\": 0.6,\n    \"truncate\": 24576,\n    \"max_new_tokens\"\
          : 8192,\n    \"stop\": [\n      \"&lt;/s&gt;\"\n    ],\n    \"top_p\": 0.95,\n\
          \    \"top_k\": 50,\n    \"repetition_penalty\": 1.2,\n    \"stop_sequences\"\
          : [\n      \"&lt;/s&gt;\"\n    ],\n    \"return_full_text\": false\n  }\n\
          }\n</code></pre>\n"
        raw: "Have you tried to look into the prompt template? In my experience, Mistral\
          \ and Mixtral models are super sensitive with those symbols, one extra space\
          \ and you are on a very wrong path in the response! You can use HuggingChat\
          \ UI to see the actual prompt that goes into the LLM, so when you see a\
          \ good response just click on that download button in the user's question:\n\
          \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/TA0p7_5Hs2Uszt7W4iZgV.png)\n\
          \nThat will show you exactly how the prompt was constructed (in addition\
          \ to generation configs)\n```\n{\n  \"note\": \"This is a preview of the\
          \ prompt that will be sent to the model when retrying the message. It may\
          \ differ from what was sent in the past if the parameters have been updated\
          \ since\",\n  \"prompt\": \"<s> [INST]You are a helpful assistant.\\n This\
          \ is a test, just say test back! [/INST]\",\n  \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
          ,\n  \"parameters\": {\n    \"temperature\": 0.6,\n    \"truncate\": 24576,\n\
          \    \"max_new_tokens\": 8192,\n    \"stop\": [\n      \"</s>\"\n    ],\n\
          \    \"top_p\": 0.95,\n    \"top_k\": 50,\n    \"repetition_penalty\": 1.2,\n\
          \    \"stop_sequences\": [\n      \"</s>\"\n    ],\n    \"return_full_text\"\
          : false\n  }\n}\n```\n"
        updatedAt: '2024-01-24T11:33:50.771Z'
      numEdits: 0
      reactions: []
    id: 65b0f59e58b60cfd5d0ef450
    type: comment
  author: MaziyarPanahi
  content: "Have you tried to look into the prompt template? In my experience, Mistral\
    \ and Mixtral models are super sensitive with those symbols, one extra space and\
    \ you are on a very wrong path in the response! You can use HuggingChat UI to\
    \ see the actual prompt that goes into the LLM, so when you see a good response\
    \ just click on that download button in the user's question:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/TA0p7_5Hs2Uszt7W4iZgV.png)\n\
    \nThat will show you exactly how the prompt was constructed (in addition to generation\
    \ configs)\n```\n{\n  \"note\": \"This is a preview of the prompt that will be\
    \ sent to the model when retrying the message. It may differ from what was sent\
    \ in the past if the parameters have been updated since\",\n  \"prompt\": \"<s>\
    \ [INST]You are a helpful assistant.\\n This is a test, just say test back! [/INST]\"\
    ,\n  \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n  \"parameters\": {\n\
    \    \"temperature\": 0.6,\n    \"truncate\": 24576,\n    \"max_new_tokens\":\
    \ 8192,\n    \"stop\": [\n      \"</s>\"\n    ],\n    \"top_p\": 0.95,\n    \"\
    top_k\": 50,\n    \"repetition_penalty\": 1.2,\n    \"stop_sequences\": [\n  \
    \    \"</s>\"\n    ],\n    \"return_full_text\": false\n  }\n}\n```\n"
  created_at: 2024-01-24 11:33:50+00:00
  edited: false
  hidden: false
  id: 65b0f59e58b60cfd5d0ef450
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 107
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: How to get result as good as Hugging Face Chat Mixtral-8x7b-Instruct
