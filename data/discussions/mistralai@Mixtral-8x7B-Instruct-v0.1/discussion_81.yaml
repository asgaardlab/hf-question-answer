!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2024-01-06 22:32:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2024-01-06T22:32:23.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9650800228118896
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: "<p>Although leaderboard scored may say otherwise, as far as actual\
          \ use, Mixtral-8x7B-Instruct-v0.1 has the best outputs of any Mixtral fine\
          \ tune of merge. I found that MixtralOrochi was a really good merge of a\
          \ few models that had some nice niche use cases like openbuddys Multilanguage\
          \ specialties, and noromaids roleplay abilities. However even my attempts\
          \ at creating a better version (linked bellow) seem to only hinder the overall\
          \ performance of the OG Mixtral-8x7B-Instruct-v0.1 model that was released\
          \ by mistralai company.</p>\n<p>My model:</p>\n<pre><code>https://huggingface.co/rombodawg/Open_Gpt4_8x7B\n\
          </code></pre>\n<p>I want to challenge the community to find the \"secret\
          \ Sause\" to fine tuning Mixtral to be better than the intruct version made\
          \ by mistralai because as it sais in the model card this model is only a\
          \ \"quick demonstration that the base model can be easily fine-tuned to\
          \ achieve compelling performance\". So in theory the performance this model\
          \ should be able to achieve should only go up from here. </p>\n<p>Lets get\
          \ to work people. I have high hopes for this model and its ability to close\
          \ the gap between open and closed sourced ai. We can make something better\
          \ than openai, microsoft, google, apple, and all others who want to horde\
          \ ai for themselves for profit. \u270C\u270C</p>\n"
        raw: "Although leaderboard scored may say otherwise, as far as actual use,\
          \ Mixtral-8x7B-Instruct-v0.1 has the best outputs of any Mixtral fine tune\
          \ of merge. I found that MixtralOrochi was a really good merge of a few\
          \ models that had some nice niche use cases like openbuddys Multilanguage\
          \ specialties, and noromaids roleplay abilities. However even my attempts\
          \ at creating a better version (linked bellow) seem to only hinder the overall\
          \ performance of the OG Mixtral-8x7B-Instruct-v0.1 model that was released\
          \ by mistralai company.\r\n\r\nMy model:\r\n```\r\nhttps://huggingface.co/rombodawg/Open_Gpt4_8x7B\r\
          \n```\r\n\r\nI want to challenge the community to find the \"secret Sause\"\
          \ to fine tuning Mixtral to be better than the intruct version made by mistralai\
          \ because as it sais in the model card this model is only a \"quick demonstration\
          \ that the base model can be easily fine-tuned to achieve compelling performance\"\
          . So in theory the performance this model should be able to achieve should\
          \ only go up from here. \r\n\r\nLets get to work people. I have high hopes\
          \ for this model and its ability to close the gap between open and closed\
          \ sourced ai. We can make something better than openai, microsoft, google,\
          \ apple, and all others who want to horde ai for themselves for profit.\
          \ \u270C\u270C"
        updatedAt: '2024-01-06T22:32:23.855Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - KamiTzayig
    id: 6599d4f702c4796342775cc2
    type: comment
  author: rombodawg
  content: "Although leaderboard scored may say otherwise, as far as actual use, Mixtral-8x7B-Instruct-v0.1\
    \ has the best outputs of any Mixtral fine tune of merge. I found that MixtralOrochi\
    \ was a really good merge of a few models that had some nice niche use cases like\
    \ openbuddys Multilanguage specialties, and noromaids roleplay abilities. However\
    \ even my attempts at creating a better version (linked bellow) seem to only hinder\
    \ the overall performance of the OG Mixtral-8x7B-Instruct-v0.1 model that was\
    \ released by mistralai company.\r\n\r\nMy model:\r\n```\r\nhttps://huggingface.co/rombodawg/Open_Gpt4_8x7B\r\
    \n```\r\n\r\nI want to challenge the community to find the \"secret Sause\" to\
    \ fine tuning Mixtral to be better than the intruct version made by mistralai\
    \ because as it sais in the model card this model is only a \"quick demonstration\
    \ that the base model can be easily fine-tuned to achieve compelling performance\"\
    . So in theory the performance this model should be able to achieve should only\
    \ go up from here. \r\n\r\nLets get to work people. I have high hopes for this\
    \ model and its ability to close the gap between open and closed sourced ai. We\
    \ can make something better than openai, microsoft, google, apple, and all others\
    \ who want to horde ai for themselves for profit. \u270C\u270C"
  created_at: 2024-01-06 22:32:23+00:00
  edited: false
  hidden: false
  id: 6599d4f702c4796342775cc2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2024-01-06T22:35:40.000Z'
    data:
      from: Still the best Mixtral based instruct model.
      to: Still the best Mixtral based instruct model. We should change that
    id: 6599d5bc6da3461e28d51739
    type: title-change
  author: rombodawg
  created_at: 2024-01-06 22:35:40+00:00
  id: 6599d5bc6da3461e28d51739
  new_title: Still the best Mixtral based instruct model. We should change that
  old_title: Still the best Mixtral based instruct model.
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 81
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Still the best Mixtral based instruct model. We should change that
