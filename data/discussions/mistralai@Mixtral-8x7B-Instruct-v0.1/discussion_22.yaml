!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AbRds
conflicting_files: null
created_at: 2023-12-13 11:12:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
      fullname: Rojas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbRds
      type: user
    createdAt: '2023-12-13T11:12:46.000Z'
    data:
      edited: true
      editors:
      - AbRds
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8427101373672485
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
          fullname: Rojas
          isHf: false
          isPro: false
          name: AbRds
          type: user
        html: '<p>Hi everyone, I was trying to deploy the model using the text-generation-inference
          toolkit in a AWS EC2 G5.24xLarge with 96GB of GPU. (4 GPUs of 24GB each),
          but when the model is initialising I receive the following message (once
          per each GPU):</p>

          <p>--torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate
          224.00 MiB. GPU 1 has a total capacty of 21.99 GiB of which 77.00 MiB is
          free. Process 27730 has 21.90 GiB memory in use. Of the allocated memory
          21.47 GiB is allocated by PyTorch, and 42.98 MiB is reserved by PyTorch
          but unallocated. If reserved but unallocated memory is large try setting
          max_split_size_mb to avoid fragmentation.  See documentation for Memory
          Management and PYTORCH_CUDA_ALLOC_CONF--</p>

          <p>I have used this command to launch the service: </p>

          <p>sudo docker run --gpus all --shm-size 1g -p 8080:80 -v /dev/data:/data
          ghcr.io/huggingface/text-generation-inference:1.3.0 --model-id mistralai/Mixtral-8x7B-Instruct-v0.1
          --num-shard 4  --max-batch-total-tokens 1024000  --max-total-tokens 32000</p>

          <p>Seems like pythorch is reserving GPU memory causing a failure in the
          load of the model but I don''t know how to face this issue. Somebody can
          help me to understand the problem or how to figure it out?</p>

          <p>Thanks in advance.</p>

          '
        raw: "Hi everyone, I was trying to deploy the model using the text-generation-inference\
          \ toolkit in a AWS EC2 G5.24xLarge with 96GB of GPU. (4 GPUs of 24GB each),\
          \ but when the model is initialising I receive the following message (once\
          \ per each GPU):\n\n--torch.cuda.OutOfMemoryError: CUDA out of memory. Tried\
          \ to allocate 224.00 MiB. GPU 1 has a total capacty of 21.99 GiB of which\
          \ 77.00 MiB is free. Process 27730 has 21.90 GiB memory in use. Of the allocated\
          \ memory 21.47 GiB is allocated by PyTorch, and 42.98 MiB is reserved by\
          \ PyTorch but unallocated. If reserved but unallocated memory is large try\
          \ setting max_split_size_mb to avoid fragmentation.  See documentation for\
          \ Memory Management and PYTORCH_CUDA_ALLOC_CONF--\n\nI have used this command\
          \ to launch the service: \n\nsudo docker run --gpus all --shm-size 1g -p\
          \ 8080:80 -v /dev/data:/data ghcr.io/huggingface/text-generation-inference:1.3.0\
          \ --model-id mistralai/Mixtral-8x7B-Instruct-v0.1 --num-shard 4  --max-batch-total-tokens\
          \ 1024000  --max-total-tokens 32000\n\nSeems like pythorch is reserving\
          \ GPU memory causing a failure in the load of the model but I don't know\
          \ how to face this issue. Somebody can help me to understand the problem\
          \ or how to figure it out?\n\nThanks in advance."
        updatedAt: '2023-12-13T12:53:02.894Z'
      numEdits: 2
      reactions: []
    id: 657991ae0b0608ba9ccb0c4f
    type: comment
  author: AbRds
  content: "Hi everyone, I was trying to deploy the model using the text-generation-inference\
    \ toolkit in a AWS EC2 G5.24xLarge with 96GB of GPU. (4 GPUs of 24GB each), but\
    \ when the model is initialising I receive the following message (once per each\
    \ GPU):\n\n--torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate\
    \ 224.00 MiB. GPU 1 has a total capacty of 21.99 GiB of which 77.00 MiB is free.\
    \ Process 27730 has 21.90 GiB memory in use. Of the allocated memory 21.47 GiB\
    \ is allocated by PyTorch, and 42.98 MiB is reserved by PyTorch but unallocated.\
    \ If reserved but unallocated memory is large try setting max_split_size_mb to\
    \ avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF--\n\
    \nI have used this command to launch the service: \n\nsudo docker run --gpus all\
    \ --shm-size 1g -p 8080:80 -v /dev/data:/data ghcr.io/huggingface/text-generation-inference:1.3.0\
    \ --model-id mistralai/Mixtral-8x7B-Instruct-v0.1 --num-shard 4  --max-batch-total-tokens\
    \ 1024000  --max-total-tokens 32000\n\nSeems like pythorch is reserving GPU memory\
    \ causing a failure in the load of the model but I don't know how to face this\
    \ issue. Somebody can help me to understand the problem or how to figure it out?\n\
    \nThanks in advance."
  created_at: 2023-12-13 11:12:46+00:00
  edited: true
  hidden: false
  id: 657991ae0b0608ba9ccb0c4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-18T11:43:33.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8067978024482727
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>Would try to quantize the model with this: <a href="https://huggingface.co/docs/text-generation-inference/conceptual/quantization">https://huggingface.co/docs/text-generation-inference/conceptual/quantization</a>
          or run it in float16. Not super familiar with TGI but you might need more
          memory for the max batch total token you are using </p>

          '
        raw: 'Would try to quantize the model with this: https://huggingface.co/docs/text-generation-inference/conceptual/quantization
          or run it in float16. Not super familiar with TGI but you might need more
          memory for the max batch total token you are using '
        updatedAt: '2023-12-18T11:43:33.621Z'
      numEdits: 0
      reactions: []
    id: 65803065f04569760f6b43bf
    type: comment
  author: ArthurZ
  content: 'Would try to quantize the model with this: https://huggingface.co/docs/text-generation-inference/conceptual/quantization
    or run it in float16. Not super familiar with TGI but you might need more memory
    for the max batch total token you are using '
  created_at: 2023-12-18 11:43:33+00:00
  edited: false
  hidden: false
  id: 65803065f04569760f6b43bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/23b302deb879b086c49c89bd76cbf608.svg
      fullname: Benjamin Bergmann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: steilgedacht
      type: user
    createdAt: '2023-12-18T16:11:31.000Z'
    data:
      edited: false
      editors:
      - steilgedacht
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.861173152923584
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/23b302deb879b086c49c89bd76cbf608.svg
          fullname: Benjamin Bergmann
          isHf: false
          isPro: false
          name: steilgedacht
          type: user
        html: '<p>What worked for me was to enable device_map="auto".<br>So in the
          line where you load the model change it to<br><code>model = AutoModelForCausalLM.from_pretrained(model_id,
          device_map="auto")</code></p>

          <p>This makes the model use all 4 GPUs</p>

          '
        raw: 'What worked for me was to enable device_map="auto".

          So in the line where you load the model change it to

          `model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")`


          This makes the model use all 4 GPUs'
        updatedAt: '2023-12-18T16:11:31.793Z'
      numEdits: 0
      reactions: []
    id: 65806f33eca6f774556c3a10
    type: comment
  author: steilgedacht
  content: 'What worked for me was to enable device_map="auto".

    So in the line where you load the model change it to

    `model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")`


    This makes the model use all 4 GPUs'
  created_at: 2023-12-18 16:11:31+00:00
  edited: false
  hidden: false
  id: 65806f33eca6f774556c3a10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
      fullname: Rojas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbRds
      type: user
    createdAt: '2023-12-18T16:13:43.000Z'
    data:
      edited: false
      editors:
      - AbRds
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.857786238193512
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
          fullname: Rojas
          isHf: false
          isPro: false
          name: AbRds
          type: user
        html: '<blockquote>

          <p>What worked for me was to enable device_map="auto".<br>So in the line
          where you load the model change it to<br><code>model = AutoModelForCausalLM.from_pretrained(model_id,
          device_map="auto")</code></p>

          <p>This makes the model use all 4 GPUs</p>

          </blockquote>

          <p>Hi, thanks for the response, how can I apply this change using the TGI?
          </p>

          <p>thanks in advance.</p>

          '
        raw: "> What worked for me was to enable device_map=\"auto\".\n> So in the\
          \ line where you load the model change it to\n> `model = AutoModelForCausalLM.from_pretrained(model_id,\
          \ device_map=\"auto\")`\n> \n> This makes the model use all 4 GPUs\n\nHi,\
          \ thanks for the response, how can I apply this change using the TGI? \n\
          \nthanks in advance."
        updatedAt: '2023-12-18T16:13:43.163Z'
      numEdits: 0
      reactions: []
    id: 65806fb7e8c8d33e562fc985
    type: comment
  author: AbRds
  content: "> What worked for me was to enable device_map=\"auto\".\n> So in the line\
    \ where you load the model change it to\n> `model = AutoModelForCausalLM.from_pretrained(model_id,\
    \ device_map=\"auto\")`\n> \n> This makes the model use all 4 GPUs\n\nHi, thanks\
    \ for the response, how can I apply this change using the TGI? \n\nthanks in advance."
  created_at: 2023-12-18 16:13:43+00:00
  edited: false
  hidden: false
  id: 65806fb7e8c8d33e562fc985
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-18T18:56:57.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33614349365234375
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Narsil\">@<span class=\"\
          underline\">Narsil</span></a></span>\n\n\t</span></span> for TGI! </p>\n"
        raw: 'cc @Narsil for TGI! '
        updatedAt: '2023-12-18T18:56:57.921Z'
      numEdits: 0
      reactions: []
    id: 658095f96c6ea8c668170b3b
    type: comment
  author: ArthurZ
  content: 'cc @Narsil for TGI! '
  created_at: 2023-12-18 18:56:57+00:00
  edited: false
  hidden: false
  id: 658095f96c6ea8c668170b3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
      fullname: Rojas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbRds
      type: user
    createdAt: '2023-12-19T16:30:06.000Z'
    data:
      edited: true
      editors:
      - AbRds
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6455516815185547
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
          fullname: Rojas
          isHf: false
          isPro: false
          name: AbRds
          type: user
        html: '<p>Hi, finally I was able to run the model along with TGI using an
          in-place quantisation technique (I''ve supposed my current setup is not
          enough to run the model), also I used the default value for the flag  --max-total-tokens.
          </p>

          <p>Here is the command I used in case it is useful for someone else:</p>

          <pre><code class="language-docker">sudo docker <span class="hljs-keyword">run</span><span
          class="language-bash"> -d --gpus all --shm-size 1g -p <span class="hljs-variable">$port</span>:80
          -v <span class="hljs-variable">$volume</span>:/data ghcr.io/huggingface/text-generation-inference:latest
          --model-id mistralai/Mixtral-8x7B-Instruct-v0.1 --sharded <span class="hljs-literal">true</span>
          --num-shard 4 --quantize eetq</span>

          </code></pre>

          '
        raw: "Hi, finally I was able to run the model along with TGI using an in-place\
          \ quantisation technique (I've supposed my current setup is not enough to\
          \ run the model), also I used the default value for the flag  --max-total-tokens.\
          \ \n\nHere is the command I used in case it is useful for someone else:\n\
          \n```docker\nsudo docker run -d --gpus all --shm-size 1g -p $port:80 -v\
          \ $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id\
          \ mistralai/Mixtral-8x7B-Instruct-v0.1 --sharded true --num-shard 4 --quantize\
          \ eetq\n```"
        updatedAt: '2023-12-19T16:37:59.682Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - TharushaShehan
        - Benlamenace
    id: 6581c50efc304dcf7b32ef92
    type: comment
  author: AbRds
  content: "Hi, finally I was able to run the model along with TGI using an in-place\
    \ quantisation technique (I've supposed my current setup is not enough to run\
    \ the model), also I used the default value for the flag  --max-total-tokens.\
    \ \n\nHere is the command I used in case it is useful for someone else:\n\n```docker\n\
    sudo docker run -d --gpus all --shm-size 1g -p $port:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest\
    \ --model-id mistralai/Mixtral-8x7B-Instruct-v0.1 --sharded true --num-shard 4\
    \ --quantize eetq\n```"
  created_at: 2023-12-19 16:30:06+00:00
  edited: true
  hidden: false
  id: 6581c50efc304dcf7b32ef92
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
      fullname: Rojas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbRds
      type: user
    createdAt: '2023-12-19T16:30:23.000Z'
    data:
      status: closed
    id: 6581c51f5cfd2c169a749d26
    type: status-change
  author: AbRds
  created_at: 2023-12-19 16:30:23+00:00
  id: 6581c51f5cfd2c169a749d26
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2023-12-21T15:10:10.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9854797124862671
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: '<p>Glad you made it work</p>

          '
        raw: 'Glad you made it work

          '
        updatedAt: '2023-12-21T15:10:10.082Z'
      numEdits: 0
      reactions: []
    id: 658455529c0bff91b153cbb5
    type: comment
  author: Narsil
  content: 'Glad you made it work

    '
  created_at: 2023-12-21 15:10:10+00:00
  edited: false
  hidden: false
  id: 658455529c0bff91b153cbb5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: closed
target_branch: null
title: CUDA error when initialising model with text-generation-inference
