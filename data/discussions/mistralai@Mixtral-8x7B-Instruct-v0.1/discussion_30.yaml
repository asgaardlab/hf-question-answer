!!python/object:huggingface_hub.community.DiscussionWithDetails
author: swapnil3597
conflicting_files: null
created_at: 2023-12-14 09:23:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660915868952-62ff907659b9ff1ccb544035.jpeg?w=200&h=200&f=face
      fullname: Swapnil Masurekar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: swapnil3597
      type: user
    createdAt: '2023-12-14T09:23:59.000Z'
    data:
      edited: true
      editors:
      - swapnil3597
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3486287295818329
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660915868952-62ff907659b9ff1ccb544035.jpeg?w=200&h=200&f=face
          fullname: Swapnil Masurekar
          isHf: false
          isPro: false
          name: swapnil3597
          type: user
        html: "<p>Code Snippet: </p>\n<pre><code class=\"language-python3\">from transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
          \ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\
          \ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"\
          pt\")\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True))\n</code></pre>\n<p>Facing following issue:</p>\n\
          <pre><code class=\"language-bash\">---------------------------------------------------------------------------\n\
          KeyError                                  Traceback (most recent call last)\n\
          Cell In[22], line 6\n      3 model_id = <span class=\"hljs-string\">\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
          </span>\n      4 tokenizer = AutoTokenizer.from_pretrained(model_id)\n----&gt;\
          \ 6 model = AutoModelForCausalLM.from_pretrained(model_id)\n      8 text\
          \ = <span class=\"hljs-string\">\"Hello my name is\"</span>\n      9 inputs\
          \ = tokenizer(text, return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n\
          \nFile /data/environments/llm_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:526,\
          \ <span class=\"hljs-keyword\">in</span> _BaseAutoModelClass.from_pretrained(cls,\
          \ pretrained_model_name_or_path, *model_args, **kwargs)\n    523 <span class=\"\
          hljs-keyword\">if</span> kwargs.get(<span class=\"hljs-string\">\"quantization_config\"\
          </span>, None) is not None:\n    524     _ = kwargs.pop(<span class=\"hljs-string\"\
          >\"quantization_config\"</span>)\n--&gt; 526 config, kwargs = AutoConfig.from_pretrained(\n\
          \    527     pretrained_model_name_or_path,\n    528     return_unused_kwargs=True,\n\
          \    529     trust_remote_code=trust_remote_code,\n    530     code_revision=code_revision,\n\
          \    531     _commit_hash=commit_hash,\n    532     **hub_kwargs,\n    533\
          \     **kwargs,\n    534 )\n    536 <span class=\"hljs-comment\"># if torch_dtype=auto\
          \ was passed here, ensure to pass it on</span>\n    537 <span class=\"hljs-keyword\"\
          >if</span> kwargs_orig.get(<span class=\"hljs-string\">\"torch_dtype\"</span>,\
          \ None) == <span class=\"hljs-string\">\"auto\"</span>:\n\nFile /data/environments/llm_env/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1064,\
          \ <span class=\"hljs-keyword\">in</span> AutoConfig.from_pretrained(cls,\
          \ pretrained_model_name_or_path, **kwargs)\n   1062     <span class=\"hljs-built_in\"\
          >return</span> config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\n   1063 <span class=\"hljs-keyword\">elif</span> <span class=\"\
          hljs-string\">\"model_type\"</span> <span class=\"hljs-keyword\">in</span>\
          \ config_dict:\n-&gt; 1064     config_class = CONFIG_MAPPING[config_dict[<span\
          \ class=\"hljs-string\">\"model_type\"</span>]]\n   1065     <span class=\"\
          hljs-built_in\">return</span> config_class.from_dict(config_dict, **unused_kwargs)\n\
          \   1066 <span class=\"hljs-keyword\">else</span>:\n   1067     <span class=\"\
          hljs-comment\"># Fallback: use pattern matching on the string.</span>\n\
          \   1068     <span class=\"hljs-comment\"># We go from longer names to shorter\
          \ names to catch roberta before bert (for instance)</span>\n\nFile /data/environments/llm_env/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:761,\
          \ <span class=\"hljs-keyword\">in</span> _LazyConfigMapping.__getitem__(self,\
          \ key)\n    759     <span class=\"hljs-built_in\">return</span> self._extra_content[key]\n\
          \    760 <span class=\"hljs-keyword\">if</span> key not <span class=\"hljs-keyword\"\
          >in</span> self._mapping:\n--&gt; 761     raise KeyError(key)\n    762 value\
          \ = self._mapping[key]\n    763 module_name = model_type_to_module_name(key)\n\
          \nKeyError: <span class=\"hljs-string\">'mixtral'</span>\n</code></pre>\n\
          <p>Getting issue on line <code>model = AutoModelForCausalLM.from_pretrained(model_id)</code>.\
          \ Kindly help me to resolve this issue.</p>\n<p>Referred to troubleshooting\
          \ steps in: <a href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\"\
          >https://huggingface.co/mistralai/Mistral-7B-v0.1</a><br>It is suggested\
          \ to use stable version of Transformers, 4.34.0 or newer.</p>\n<p>I'm using:\
          \ <code>transformers==4.36.1</code>. Still facing this issue.</p>\n"
        raw: "Code Snippet: \n\n```python3\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n\
          tokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\
          \ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"\
          pt\")\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True))\n```\n\nFacing following issue:\n```bash\n\
          ---------------------------------------------------------------------------\n\
          KeyError                                  Traceback (most recent call last)\n\
          Cell In[22], line 6\n      3 model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
          \n      4 tokenizer = AutoTokenizer.from_pretrained(model_id)\n----> 6 model\
          \ = AutoModelForCausalLM.from_pretrained(model_id)\n      8 text = \"Hello\
          \ my name is\"\n      9 inputs = tokenizer(text, return_tensors=\"pt\")\n\
          \nFile /data/environments/llm_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:526,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    523 if kwargs.get(\"quantization_config\"\
          , None) is not None:\n    524     _ = kwargs.pop(\"quantization_config\"\
          )\n--> 526 config, kwargs = AutoConfig.from_pretrained(\n    527     pretrained_model_name_or_path,\n\
          \    528     return_unused_kwargs=True,\n    529     trust_remote_code=trust_remote_code,\n\
          \    530     code_revision=code_revision,\n    531     _commit_hash=commit_hash,\n\
          \    532     **hub_kwargs,\n    533     **kwargs,\n    534 )\n    536 #\
          \ if torch_dtype=auto was passed here, ensure to pass it on\n    537 if\
          \ kwargs_orig.get(\"torch_dtype\", None) == \"auto\":\n\nFile /data/environments/llm_env/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1064,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
          \   1062     return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\n   1063 elif \"model_type\" in config_dict:\n-> 1064     config_class\
          \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\n   1065     return config_class.from_dict(config_dict,\
          \ **unused_kwargs)\n   1066 else:\n   1067     # Fallback: use pattern matching\
          \ on the string.\n   1068     # We go from longer names to shorter names\
          \ to catch roberta before bert (for instance)\n\nFile /data/environments/llm_env/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:761,\
          \ in _LazyConfigMapping.__getitem__(self, key)\n    759     return self._extra_content[key]\n\
          \    760 if key not in self._mapping:\n--> 761     raise KeyError(key)\n\
          \    762 value = self._mapping[key]\n    763 module_name = model_type_to_module_name(key)\n\
          \nKeyError: 'mixtral'\n```\n\nGetting issue on line `model = AutoModelForCausalLM.from_pretrained(model_id)`.\
          \ Kindly help me to resolve this issue.\n\nReferred to troubleshooting steps\
          \ in: https://huggingface.co/mistralai/Mistral-7B-v0.1\nIt is suggested\
          \ to use stable version of Transformers, 4.34.0 or newer.\n\nI'm using:\
          \ `transformers==4.36.1`. Still facing this issue."
        updatedAt: '2023-12-14T09:30:35.023Z'
      numEdits: 1
      reactions: []
    id: 657ac9af67a0c464ed3dc150
    type: comment
  author: swapnil3597
  content: "Code Snippet: \n\n```python3\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\
    \ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\"\
    )\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0],\
    \ skip_special_tokens=True))\n```\n\nFacing following issue:\n```bash\n---------------------------------------------------------------------------\n\
    KeyError                                  Traceback (most recent call last)\n\
    Cell In[22], line 6\n      3 model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
    \n      4 tokenizer = AutoTokenizer.from_pretrained(model_id)\n----> 6 model =\
    \ AutoModelForCausalLM.from_pretrained(model_id)\n      8 text = \"Hello my name\
    \ is\"\n      9 inputs = tokenizer(text, return_tensors=\"pt\")\n\nFile /data/environments/llm_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:526,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n    523 if kwargs.get(\"quantization_config\", None) is not None:\n\
    \    524     _ = kwargs.pop(\"quantization_config\")\n--> 526 config, kwargs =\
    \ AutoConfig.from_pretrained(\n    527     pretrained_model_name_or_path,\n  \
    \  528     return_unused_kwargs=True,\n    529     trust_remote_code=trust_remote_code,\n\
    \    530     code_revision=code_revision,\n    531     _commit_hash=commit_hash,\n\
    \    532     **hub_kwargs,\n    533     **kwargs,\n    534 )\n    536 # if torch_dtype=auto\
    \ was passed here, ensure to pass it on\n    537 if kwargs_orig.get(\"torch_dtype\"\
    , None) == \"auto\":\n\nFile /data/environments/llm_env/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1064,\
    \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
    \   1062     return config_class.from_pretrained(pretrained_model_name_or_path,\
    \ **kwargs)\n   1063 elif \"model_type\" in config_dict:\n-> 1064     config_class\
    \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\n   1065     return config_class.from_dict(config_dict,\
    \ **unused_kwargs)\n   1066 else:\n   1067     # Fallback: use pattern matching\
    \ on the string.\n   1068     # We go from longer names to shorter names to catch\
    \ roberta before bert (for instance)\n\nFile /data/environments/llm_env/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:761,\
    \ in _LazyConfigMapping.__getitem__(self, key)\n    759     return self._extra_content[key]\n\
    \    760 if key not in self._mapping:\n--> 761     raise KeyError(key)\n    762\
    \ value = self._mapping[key]\n    763 module_name = model_type_to_module_name(key)\n\
    \nKeyError: 'mixtral'\n```\n\nGetting issue on line `model = AutoModelForCausalLM.from_pretrained(model_id)`.\
    \ Kindly help me to resolve this issue.\n\nReferred to troubleshooting steps in:\
    \ https://huggingface.co/mistralai/Mistral-7B-v0.1\nIt is suggested to use stable\
    \ version of Transformers, 4.34.0 or newer.\n\nI'm using: `transformers==4.36.1`.\
    \ Still facing this issue."
  created_at: 2023-12-14 09:23:59+00:00
  edited: true
  hidden: false
  id: 657ac9af67a0c464ed3dc150
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660915868952-62ff907659b9ff1ccb544035.jpeg?w=200&h=200&f=face
      fullname: Swapnil Masurekar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: swapnil3597
      type: user
    createdAt: '2023-12-14T09:27:25.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660915868952-62ff907659b9ff1ccb544035.jpeg?w=200&h=200&f=face
          fullname: Swapnil Masurekar
          isHf: false
          isPro: false
          name: swapnil3597
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-14T09:30:50.298Z'
      numEdits: 0
      reactions: []
    id: 657aca7dcee701e2f5dd4077
    type: comment
  author: swapnil3597
  content: This comment has been hidden
  created_at: 2023-12-14 09:27:25+00:00
  edited: true
  hidden: true
  id: 657aca7dcee701e2f5dd4077
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660915868952-62ff907659b9ff1ccb544035.jpeg?w=200&h=200&f=face
      fullname: Swapnil Masurekar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: swapnil3597
      type: user
    createdAt: '2023-12-14T09:27:25.000Z'
    data:
      status: closed
    id: 657aca7dcee701e2f5dd407a
    type: status-change
  author: swapnil3597
  created_at: 2023-12-14 09:27:25+00:00
  id: 657aca7dcee701e2f5dd407a
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660915868952-62ff907659b9ff1ccb544035.jpeg?w=200&h=200&f=face
      fullname: Swapnil Masurekar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: swapnil3597
      type: user
    createdAt: '2023-12-14T09:28:16.000Z'
    data:
      status: open
    id: 657acab064b1735fb9657c77
    type: status-change
  author: swapnil3597
  created_at: 2023-12-14 09:28:16+00:00
  id: 657acab064b1735fb9657c77
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6cff50c870c8ef3082e6d9ef8b6bce19.svg
      fullname: Bastian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bastian100
      type: user
    createdAt: '2023-12-14T09:40:52.000Z'
    data:
      edited: false
      editors:
      - bastian100
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9387434124946594
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6cff50c870c8ef3082e6d9ef8b6bce19.svg
          fullname: Bastian
          isHf: false
          isPro: false
          name: bastian100
          type: user
        html: '<p>Hello, I have the same error here.</p>

          '
        raw: Hello, I have the same error here.
        updatedAt: '2023-12-14T09:40:52.886Z'
      numEdits: 0
      reactions: []
    id: 657acda4081949e62bb47606
    type: comment
  author: bastian100
  content: Hello, I have the same error here.
  created_at: 2023-12-14 09:40:52+00:00
  edited: false
  hidden: false
  id: 657acda4081949e62bb47606
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/30864e9560a2d6f4fcd8d32f1dd3d750.svg
      fullname: Thomas Pernet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thomaspernet
      type: user
    createdAt: '2023-12-14T10:25:46.000Z'
    data:
      edited: false
      editors:
      - thomaspernet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7902897000312805
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/30864e9560a2d6f4fcd8d32f1dd3d750.svg
          fullname: Thomas Pernet
          isHf: false
          isPro: false
          name: thomaspernet
          type: user
        html: '<p>Upgrade transformers <code>pip install transformers --upgrade</code>
          (transformers 4.36.1) solved the issue for me</p>

          <p><a rel="nofollow" href="https://pypi.org/project/transformers/">https://pypi.org/project/transformers/</a></p>

          '
        raw: 'Upgrade transformers `pip install transformers --upgrade` (transformers
          4.36.1) solved the issue for me


          https://pypi.org/project/transformers/'
        updatedAt: '2023-12-14T10:25:46.886Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - bastian100
        - realVladys
        - wolfpack-g
    id: 657ad82a964fb9bc67d2dffa
    type: comment
  author: thomaspernet
  content: 'Upgrade transformers `pip install transformers --upgrade` (transformers
    4.36.1) solved the issue for me


    https://pypi.org/project/transformers/'
  created_at: 2023-12-14 10:25:46+00:00
  edited: false
  hidden: false
  id: 657ad82a964fb9bc67d2dffa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
      fullname: Alessandro Danesi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aledane
      type: user
    createdAt: '2023-12-15T16:16:58.000Z'
    data:
      edited: false
      editors:
      - aledane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9750556349754333
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
          fullname: Alessandro Danesi
          isHf: false
          isPro: false
          name: aledane
          type: user
        html: '<p>I am facing the same problem. I have installed transformers 4.36.1
          but still does not work.</p>

          '
        raw: I am facing the same problem. I have installed transformers 4.36.1 but
          still does not work.
        updatedAt: '2023-12-15T16:16:58.153Z'
      numEdits: 0
      reactions: []
    id: 657c7bfae62c244a5b5b09a4
    type: comment
  author: aledane
  content: I am facing the same problem. I have installed transformers 4.36.1 but
    still does not work.
  created_at: 2023-12-15 16:16:58+00:00
  edited: false
  hidden: false
  id: 657c7bfae62c244a5b5b09a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-18T12:07:16.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.803729772567749
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>make sure you are actually using it: <code>import transformers;
          print(transformers.__version__)</code> </p>

          '
        raw: 'make sure you are actually using it: `import transformers; print(transformers.__version__)` '
        updatedAt: '2023-12-18T12:07:16.506Z'
      numEdits: 0
      reactions: []
    id: 658035f44ae40c74682eff5f
    type: comment
  author: ArthurZ
  content: 'make sure you are actually using it: `import transformers; print(transformers.__version__)` '
  created_at: 2023-12-18 12:07:16+00:00
  edited: false
  hidden: false
  id: 658035f44ae40c74682eff5f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7d6c2f55ed2eb6a7a7585204129c5355.svg
      fullname: Niranjan Senthilvasan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Niranjan292
      type: user
    createdAt: '2023-12-23T17:11:40.000Z'
    data:
      edited: false
      editors:
      - Niranjan292
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9720715880393982
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7d6c2f55ed2eb6a7a7585204129c5355.svg
          fullname: Niranjan Senthilvasan
          isHf: false
          isPro: false
          name: Niranjan292
          type: user
        html: '<p>Yeah, Upgrading transformers actually works. Restarting the kernel
          after upgrading worked for me.</p>

          '
        raw: Yeah, Upgrading transformers actually works. Restarting the kernel after
          upgrading worked for me.
        updatedAt: '2023-12-23T17:11:40.165Z'
      numEdits: 0
      reactions: []
    id: 658714ccce38d143c40c90d9
    type: comment
  author: Niranjan292
  content: Yeah, Upgrading transformers actually works. Restarting the kernel after
    upgrading worked for me.
  created_at: 2023-12-23 17:11:40+00:00
  edited: false
  hidden: false
  id: 658714ccce38d143c40c90d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a0fb779052d1e8beb7cbe553f1fd6b8.svg
      fullname: Shivansh Mathur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShivanshMathur007
      type: user
    createdAt: '2024-01-08T18:23:25.000Z'
    data:
      edited: false
      editors:
      - ShivanshMathur007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9539569020271301
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a0fb779052d1e8beb7cbe553f1fd6b8.svg
          fullname: Shivansh Mathur
          isHf: false
          isPro: false
          name: ShivanshMathur007
          type: user
        html: '<p>What are your hardware config, that you are loading the model directly?</p>

          '
        raw: 'What are your hardware config, that you are loading the model directly?

          '
        updatedAt: '2024-01-08T18:23:25.543Z'
      numEdits: 0
      reactions: []
    id: 659c3d9d6090ab0a4931535d
    type: comment
  author: ShivanshMathur007
  content: 'What are your hardware config, that you are loading the model directly?

    '
  created_at: 2024-01-08 18:23:25+00:00
  edited: false
  hidden: false
  id: 659c3d9d6090ab0a4931535d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 30
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: 'Facing issue while loading model - KeyError: ''mixtral'''
