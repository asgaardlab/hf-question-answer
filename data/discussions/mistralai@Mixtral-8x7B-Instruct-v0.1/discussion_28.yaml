!!python/object:huggingface_hub.community.DiscussionWithDetails
author: silvacarl
conflicting_files: null
created_at: 2023-12-13 20:32:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660775548920-628d998a2b60ec0f336cc1eb.png?w=200&h=200&f=face
      fullname: Carl Silva
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: silvacarl
      type: user
    createdAt: '2023-12-13T20:32:00.000Z'
    data:
      edited: false
      editors:
      - silvacarl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9331489205360413
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660775548920-628d998a2b60ec0f336cc1eb.png?w=200&h=200&f=face
          fullname: Carl Silva
          isHf: false
          isPro: true
          name: silvacarl
          type: user
        html: '<p>has anyone been able to get this to work with Text Generation Inference?</p>

          <p><a rel="nofollow" href="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</a></p>

          '
        raw: "has anyone been able to get this to work with Text Generation Inference?\r\
          \n\r\nhttps://github.com/huggingface/text-generation-inference"
        updatedAt: '2023-12-13T20:32:00.996Z'
      numEdits: 0
      reactions: []
    id: 657a14c007b1fc747d05ee26
    type: comment
  author: silvacarl
  content: "has anyone been able to get this to work with Text Generation Inference?\r\
    \n\r\nhttps://github.com/huggingface/text-generation-inference"
  created_at: 2023-12-13 20:32:00+00:00
  edited: false
  hidden: false
  id: 657a14c007b1fc747d05ee26
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
      fullname: Rojas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbRds
      type: user
    createdAt: '2023-12-14T08:13:09.000Z'
    data:
      edited: true
      editors:
      - AbRds
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.865864098072052
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
          fullname: Rojas
          isHf: false
          isPro: false
          name: AbRds
          type: user
        html: '<p>Yes, I''ve tried to deploy the model using TGI, it is explained
          here: <a href="https://huggingface.co/blog/mixtral#using-text-generation-inference">https://huggingface.co/blog/mixtral#using-text-generation-inference</a><br>In
          my case I was using a AWS EC2 G5.24xlarge, however seems like the machine
          is not big enough to run the model and it crashes, you can see my issue
          here: <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/22#657991ae0b0608ba9ccb0c4f">https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/22#657991ae0b0608ba9ccb0c4f</a></p>

          <p> I''m awaiting for authorisation in order to use the G5.48xLarge, if
          you are able to run the model following the instructions in the first link,
          please let me know which machine you are using. </p>

          <p>Cheers. </p>

          '
        raw: "Yes, I've tried to deploy the model using TGI, it is explained here:\
          \ [https://huggingface.co/blog/mixtral#using-text-generation-inference](https://huggingface.co/blog/mixtral#using-text-generation-inference)\n\
          In my case I was using a AWS EC2 G5.24xlarge, however seems like the machine\
          \ is not big enough to run the model and it crashes, you can see my issue\
          \ here: [https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/22#657991ae0b0608ba9ccb0c4f](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/22#657991ae0b0608ba9ccb0c4f)\n\
          \n I'm awaiting for authorisation in order to use the G5.48xLarge, if you\
          \ are able to run the model following the instructions in the first link,\
          \ please let me know which machine you are using. \n\nCheers. "
        updatedAt: '2023-12-14T08:14:30.776Z'
      numEdits: 1
      reactions: []
    id: 657ab91510503b507c65ac09
    type: comment
  author: AbRds
  content: "Yes, I've tried to deploy the model using TGI, it is explained here: [https://huggingface.co/blog/mixtral#using-text-generation-inference](https://huggingface.co/blog/mixtral#using-text-generation-inference)\n\
    In my case I was using a AWS EC2 G5.24xlarge, however seems like the machine is\
    \ not big enough to run the model and it crashes, you can see my issue here: [https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/22#657991ae0b0608ba9ccb0c4f](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/22#657991ae0b0608ba9ccb0c4f)\n\
    \n I'm awaiting for authorisation in order to use the G5.48xLarge, if you are\
    \ able to run the model following the instructions in the first link, please let\
    \ me know which machine you are using. \n\nCheers. "
  created_at: 2023-12-14 08:13:09+00:00
  edited: true
  hidden: false
  id: 657ab91510503b507c65ac09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-18T11:47:46.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7924458980560303
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>see this as well <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/18">https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/18</a>
          </p>

          '
        raw: 'see this as well https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/18 '
        updatedAt: '2023-12-18T11:47:46.227Z'
      numEdits: 0
      reactions: []
    id: 65803162001c77f198c9e997
    type: comment
  author: ArthurZ
  content: 'see this as well https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/18 '
  created_at: 2023-12-18 11:47:46+00:00
  edited: false
  hidden: false
  id: 65803162001c77f198c9e997
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
      fullname: Rojas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbRds
      type: user
    createdAt: '2023-12-19T16:38:15.000Z'
    data:
      edited: false
      editors:
      - AbRds
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6665295362472534
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
          fullname: Rojas
          isHf: false
          isPro: false
          name: AbRds
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;silvacarl&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/silvacarl\"\
          >@<span class=\"underline\">silvacarl</span></a></span>\n\n\t</span></span>,\
          \ </p>\n<p>I was able to run the model along with TGI using an in-place\
          \ quantisation technique (my current setup is not able to run the model\
          \ at full), also I used the default value for the flag --max-total-tokens.<br>Here\
          \ is the command I used in case it is useful for you or someone else:</p>\n\
          <pre><code class=\"language-docker\">sudo docker <span class=\"hljs-keyword\"\
          >run</span><span class=\"language-bash\"> -d --gpus all --shm-size 1g -p\
          \ <span class=\"hljs-variable\">$port</span>:80 -v <span class=\"hljs-variable\"\
          >$volume</span>:/data ghcr.io/huggingface/text-generation-inference:latest\
          \ --model-id mistralai/Mixtral-8x7B-Instruct-v0.1 --sharded <span class=\"\
          hljs-literal\">true</span> --num-shard 4 --quantize eetq</span>\n</code></pre>\n"
        raw: "Hi @silvacarl, \n\nI was able to run the model along with TGI using\
          \ an in-place quantisation technique (my current setup is not able to run\
          \ the model at full), also I used the default value for the flag --max-total-tokens.\n\
          Here is the command I used in case it is useful for you or someone else:\n\
          \n```docker\nsudo docker run -d --gpus all --shm-size 1g -p $port:80 -v\
          \ $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id\
          \ mistralai/Mixtral-8x7B-Instruct-v0.1 --sharded true --num-shard 4 --quantize\
          \ eetq\n```"
        updatedAt: '2023-12-19T16:38:15.960Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - flavius
    id: 6581c6f7c2d7971187fc1b5b
    type: comment
  author: AbRds
  content: "Hi @silvacarl, \n\nI was able to run the model along with TGI using an\
    \ in-place quantisation technique (my current setup is not able to run the model\
    \ at full), also I used the default value for the flag --max-total-tokens.\nHere\
    \ is the command I used in case it is useful for you or someone else:\n\n```docker\n\
    sudo docker run -d --gpus all --shm-size 1g -p $port:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest\
    \ --model-id mistralai/Mixtral-8x7B-Instruct-v0.1 --sharded true --num-shard 4\
    \ --quantize eetq\n```"
  created_at: 2023-12-19 16:38:15+00:00
  edited: false
  hidden: false
  id: 6581c6f7c2d7971187fc1b5b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 28
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Text Generation Inference?
