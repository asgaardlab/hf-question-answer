!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dnovak232
conflicting_files: null
created_at: 2024-01-05 11:03:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/92dafa71e2e1be5c2296df0e3e8945fa.svg
      fullname: Dino Novak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dnovak232
      type: user
    createdAt: '2024-01-05T11:03:35.000Z'
    data:
      edited: false
      editors:
      - dnovak232
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5792868733406067
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/92dafa71e2e1be5c2296df0e3e8945fa.svg
          fullname: Dino Novak
          isHf: false
          isPro: false
          name: dnovak232
          type: user
        html: '<p>I have 2 x A6000 (so 96 GB VRAM in total)</p>

          <p>I am running vllm/vllm-openai docker image<br>and I am initialising it
          with:<br>Docker options: --runtime nvidia --gpus all -v ./workspace:/root/.cache/huggingface
          -p 8000:8000 --ipc=host<br>OnStart script: python3 -m vllm.entrypoints.openai.api_server
          --model mistralai/Mixtral-8x7B-Instruct-v0.1 --tensor-parallel-size 2 --max-model-len
          8000</p>

          <p>I tried reducing the model sequence length, but still I am unable to
          fit it in 96GB, it runs fine with 128GB.<br>Can anyone advise what needs
          to be done to make it work, do I need to use quantization, I know that vllm
          has issue with awq and mistral on multi GPU?</p>

          <p>Below is log from vLLM start<br>Initializing an LLM engine with config:
          model=''mistralai/Mixtral-8x7B-Instruct-v0.1'', tokenizer=''mistralai/Mixtral-8x7B-Instruct-v0.1'',
          tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False,
          dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=auto,
          tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)<br>INFO
          01-05 10:44:10 llm_engine.py:275] # GPU blocks: 0, # CPU blocks: 4096<br>Traceback
          (most recent call last):<br>  File "/usr/lib/python3.10/runpy.py", line
          196, in _run_module_as_main<br>    return _run_code(code, main_globals,
          None,<br>  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code<br>    exec(code,
          run_globals)<br>  File "/workspace/vllm/entrypoints/openai/api_server.py",
          line 737, in<br>    engine = AsyncLLMEngine.from_engine_args(engine_args)<br>  File
          "/workspace/vllm/engine/async_llm_engine.py", line 500, in from_engine_args<br>    engine
          = cls(parallel_config.worker_use_ray,<br>  File "/workspace/vllm/engine/async_llm_engine.py",
          line 273, in <strong>init</strong><br>    self.engine = self._init_engine(*args,
          **kwargs)<br>  File "/workspace/vllm/engine/async_llm_engine.py", line 318,
          in _init_engine<br>    return engine_class(*args, **kwargs)<br>  File "/workspace/vllm/engine/llm_engine.py",
          line 114, in <strong>init</strong><br>    self._init_cache()<br>  File "/workspace/vllm/engine/llm_engine.py",
          line 279, in _init_cache<br>    raise ValueError("No available memory for
          the cache blocks. "<br>ValueError: No available memory for the cache blocks.
          Try increasing <code>gpu_memory_utilization</code> when initializing the
          engine.</p>

          '
        raw: "I have 2 x A6000 (so 96 GB VRAM in total)\r\n\r\nI am running vllm/vllm-openai\
          \ docker image\r\nand I am initialising it with:\r\nDocker options: --runtime\
          \ nvidia --gpus all -v ./workspace:/root/.cache/huggingface -p 8000:8000\
          \ --ipc=host\r\nOnStart script: python3 -m vllm.entrypoints.openai.api_server\
          \ --model mistralai/Mixtral-8x7B-Instruct-v0.1 --tensor-parallel-size 2\
          \ --max-model-len 8000\r\n\r\nI tried reducing the model sequence length,\
          \ but still I am unable to fit it in 96GB, it runs fine with 128GB.\r\n\
          Can anyone advise what needs to be done to make it work, do I need to use\
          \ quantization, I know that vllm has issue with awq and mistral on multi\
          \ GPU?\r\n\r\nBelow is log from vLLM start\r\nInitializing an LLM engine\
          \ with config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1',\
          \ tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False,\
          \ dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=auto,\
          \ tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)\r\
          \nINFO 01-05 10:44:10 llm_engine.py:275] # GPU blocks: 0, # CPU blocks:\
          \ 4096\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\"\
          , line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals,\
          \ None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\
          \n    exec(code, run_globals)\r\n  File \"/workspace/vllm/entrypoints/openai/api_server.py\"\
          , line 737, in \r\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\r\
          \n  File \"/workspace/vllm/engine/async_llm_engine.py\", line 500, in from_engine_args\r\
          \n    engine = cls(parallel_config.worker_use_ray,\r\n  File \"/workspace/vllm/engine/async_llm_engine.py\"\
          , line 273, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\
          \n  File \"/workspace/vllm/engine/async_llm_engine.py\", line 318, in _init_engine\r\
          \n    return engine_class(*args, **kwargs)\r\n  File \"/workspace/vllm/engine/llm_engine.py\"\
          , line 114, in __init__\r\n    self._init_cache()\r\n  File \"/workspace/vllm/engine/llm_engine.py\"\
          , line 279, in _init_cache\r\n    raise ValueError(\"No available memory\
          \ for the cache blocks. \"\r\nValueError: No available memory for the cache\
          \ blocks. Try increasing `gpu_memory_utilization` when initializing the\
          \ engine."
        updatedAt: '2024-01-05T11:03:35.575Z'
      numEdits: 0
      reactions: []
    id: 6597e207ce92304a7174239e
    type: comment
  author: dnovak232
  content: "I have 2 x A6000 (so 96 GB VRAM in total)\r\n\r\nI am running vllm/vllm-openai\
    \ docker image\r\nand I am initialising it with:\r\nDocker options: --runtime\
    \ nvidia --gpus all -v ./workspace:/root/.cache/huggingface -p 8000:8000 --ipc=host\r\
    \nOnStart script: python3 -m vllm.entrypoints.openai.api_server --model mistralai/Mixtral-8x7B-Instruct-v0.1\
    \ --tensor-parallel-size 2 --max-model-len 8000\r\n\r\nI tried reducing the model\
    \ sequence length, but still I am unable to fit it in 96GB, it runs fine with\
    \ 128GB.\r\nCan anyone advise what needs to be done to make it work, do I need\
    \ to use quantization, I know that vllm has issue with awq and mistral on multi\
    \ GPU?\r\n\r\nBelow is log from vLLM start\r\nInitializing an LLM engine with\
    \ config: model='mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer='mistralai/Mixtral-8x7B-Instruct-v0.1',\
    \ tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False,\
    \ dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=auto,\
    \ tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)\r\n\
    INFO 01-05 10:44:10 llm_engine.py:275] # GPU blocks: 0, # CPU blocks: 4096\r\n\
    Traceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\"\
    , line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals,\
    \ None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n\
    \    exec(code, run_globals)\r\n  File \"/workspace/vllm/entrypoints/openai/api_server.py\"\
    , line 737, in \r\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\r\
    \n  File \"/workspace/vllm/engine/async_llm_engine.py\", line 500, in from_engine_args\r\
    \n    engine = cls(parallel_config.worker_use_ray,\r\n  File \"/workspace/vllm/engine/async_llm_engine.py\"\
    , line 273, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\
    \n  File \"/workspace/vllm/engine/async_llm_engine.py\", line 318, in _init_engine\r\
    \n    return engine_class(*args, **kwargs)\r\n  File \"/workspace/vllm/engine/llm_engine.py\"\
    , line 114, in __init__\r\n    self._init_cache()\r\n  File \"/workspace/vllm/engine/llm_engine.py\"\
    , line 279, in _init_cache\r\n    raise ValueError(\"No available memory for the\
    \ cache blocks. \"\r\nValueError: No available memory for the cache blocks. Try\
    \ increasing `gpu_memory_utilization` when initializing the engine."
  created_at: 2024-01-05 11:03:35+00:00
  edited: false
  hidden: false
  id: 6597e207ce92304a7174239e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b58bb86e919d3c82519bb970a071aec7.svg
      fullname: conia_he
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: conia
      type: user
    createdAt: '2024-01-11T07:38:59.000Z'
    data:
      edited: false
      editors:
      - conia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.476042240858078
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b58bb86e919d3c82519bb970a071aec7.svg
          fullname: conia_he
          isHf: false
          isPro: false
          name: conia
          type: user
        html: '<p>it''s ok for me to use 4*A100 40G when using vllm</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6412c51500c5dd0790bc749c/J-Rbfzgd9SJR_A19jxu82.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6412c51500c5dd0790bc749c/J-Rbfzgd9SJR_A19jxu82.png"></a></p>

          '
        raw: 'it''s ok for me to use 4*A100 40G when using vllm


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6412c51500c5dd0790bc749c/J-Rbfzgd9SJR_A19jxu82.png)

          '
        updatedAt: '2024-01-11T07:38:59.262Z'
      numEdits: 0
      reactions: []
    id: 659f9b13697a41751bb1d8a5
    type: comment
  author: conia
  content: 'it''s ok for me to use 4*A100 40G when using vllm


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6412c51500c5dd0790bc749c/J-Rbfzgd9SJR_A19jxu82.png)

    '
  created_at: 2024-01-11 07:38:59+00:00
  edited: false
  hidden: false
  id: 659f9b13697a41751bb1d8a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/92dafa71e2e1be5c2296df0e3e8945fa.svg
      fullname: Dino Novak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dnovak232
      type: user
    createdAt: '2024-01-13T10:26:34.000Z'
    data:
      edited: false
      editors:
      - dnovak232
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6303055882453918
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/92dafa71e2e1be5c2296df0e3e8945fa.svg
          fullname: Dino Novak
          isHf: false
          isPro: false
          name: dnovak232
          type: user
        html: '<blockquote>

          <p>it''s ok for me to use 4*A100 40G when using vllm</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6412c51500c5dd0790bc749c/J-Rbfzgd9SJR_A19jxu82.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6412c51500c5dd0790bc749c/J-Rbfzgd9SJR_A19jxu82.png"></a></p>

          </blockquote>

          <p>with 128GB plus all works OK,<br>looking on smallest footprint where
          I can run it without quantisation.</p>

          '
        raw: "> it's ok for me to use 4*A100 40G when using vllm\n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6412c51500c5dd0790bc749c/J-Rbfzgd9SJR_A19jxu82.png)\n\
          \nwith 128GB plus all works OK,\nlooking on smallest footprint where I can\
          \ run it without quantisation."
        updatedAt: '2024-01-13T10:26:34.816Z'
      numEdits: 0
      reactions: []
    id: 65a2655ad13926ca4b3dd915
    type: comment
  author: dnovak232
  content: "> it's ok for me to use 4*A100 40G when using vllm\n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6412c51500c5dd0790bc749c/J-Rbfzgd9SJR_A19jxu82.png)\n\
    \nwith 128GB plus all works OK,\nlooking on smallest footprint where I can run\
    \ it without quantisation."
  created_at: 2024-01-13 10:26:34+00:00
  edited: false
  hidden: false
  id: 65a2655ad13926ca4b3dd915
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 78
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Serving the model as API on vLLM and 2 x A6000
