!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Husain
conflicting_files: null
created_at: 2023-12-19 11:27:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2aa7c3b7961324fcb4bfd6bbbb88729d.svg
      fullname: Khatba
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Husain
      type: user
    createdAt: '2023-12-19T11:27:52.000Z'
    data:
      edited: true
      editors:
      - Husain
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8308947086334229
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2aa7c3b7961324fcb4bfd6bbbb88729d.svg
          fullname: Khatba
          isHf: false
          isPro: false
          name: Husain
          type: user
        html: '<p>What kind of optimizations are used to run MistralAI/Mixtral-8x7B-Instruct-v0.1
          in Hugging Face Chat <a href="https://huggingface.co/chat">https://huggingface.co/chat</a>
          ?  Is this the default model in full precision?<br>Or are there optimizations
          to reduce memory requirements for running the model? like using float16
          or (8-bit &amp; 4-bit) using bitsandbytes<br>Is Flash Attention 2 is used
          too ?</p>

          '
        raw: 'What kind of optimizations are used to run MistralAI/Mixtral-8x7B-Instruct-v0.1
          in Hugging Face Chat https://huggingface.co/chat ?  Is this the default
          model in full precision?

          Or are there optimizations to reduce memory requirements for running the
          model? like using float16 or (8-bit & 4-bit) using bitsandbytes

          Is Flash Attention 2 is used too ?'
        updatedAt: '2023-12-19T11:28:35.642Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - MaziyarPanahi
        - stmartin
    id: 65817e387c71acb646829ac6
    type: comment
  author: Husain
  content: 'What kind of optimizations are used to run MistralAI/Mixtral-8x7B-Instruct-v0.1
    in Hugging Face Chat https://huggingface.co/chat ?  Is this the default model
    in full precision?

    Or are there optimizations to reduce memory requirements for running the model?
    like using float16 or (8-bit & 4-bit) using bitsandbytes

    Is Flash Attention 2 is used too ?'
  created_at: 2023-12-19 11:27:52+00:00
  edited: true
  hidden: false
  id: 65817e387c71acb646829ac6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-20T09:02:45.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.639507532119751
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Husain&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Husain\">@<span class=\"\
          underline\">Husain</span></a></span>\n\n\t</span></span><br>I think HuggingChat\
          \ uses TGI under the hood: <a rel=\"nofollow\" href=\"https://github.com/huggingface/text-generation-inference\"\
          >https://github.com/huggingface/text-generation-inference</a><br>Specifically\
          \ here: <a rel=\"nofollow\" href=\"https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py\"\
          >https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py</a></p>\n"
        raw: "Hi @Husain \nI think HuggingChat uses TGI under the hood: https://github.com/huggingface/text-generation-inference\
          \  \nSpecifically here: https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py"
        updatedAt: '2023-12-20T09:02:45.931Z'
      numEdits: 0
      reactions: []
    id: 6582adb555a1e6cdb3c37571
    type: comment
  author: ybelkada
  content: "Hi @Husain \nI think HuggingChat uses TGI under the hood: https://github.com/huggingface/text-generation-inference\
    \  \nSpecifically here: https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py"
  created_at: 2023-12-20 09:02:45+00:00
  edited: false
  hidden: false
  id: 6582adb555a1e6cdb3c37571
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 54
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Optimizing Mixtral-8x7B-Instruct-v0.1 for Hugging Face Chat
