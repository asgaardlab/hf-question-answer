!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lemonflourorange
conflicting_files: null
created_at: 2024-01-04 03:21:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bea11e9c4bf19b2a1884eda1a8cbf00.svg
      fullname: Lemon Flour
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lemonflourorange
      type: user
    createdAt: '2024-01-04T03:21:11.000Z'
    data:
      edited: true
      editors:
      - lemonflourorange
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8470252752304077
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bea11e9c4bf19b2a1884eda1a8cbf00.svg
          fullname: Lemon Flour
          isHf: false
          isPro: false
          name: lemonflourorange
          type: user
        html: "<p>The Model card says it is important to get the prompt template correct\
          \ or else the model will produce sub-optimal outputs, but which prompt template\
          \ is correct? Two different ones have been given.</p>\n<p>The one from the\
          \ model card:</p>\n<pre><code>&lt;s&gt; [INST] Instruction [/INST] Model\
          \ answer&lt;/s&gt; [INST] Follow-up instruction [/INST]\n</code></pre>\n\
          <p>The one from tokenizer_config.json:</p>\n<pre><code>&lt;s&gt;[INST] Instruction\
          \ [/INST]Model answer&lt;/s&gt;[INST] Follow-up instruction [/INST]\n</code></pre>\n\
          <p>Also, are we supposed to leave every EOS token in for each bot response\
          \ in the conversation? That's what the pseudocode of the model card and\
          \ the prompt template in tokenizer_config.json imply but I haven't seen\
          \ that done before.</p>\n<p>Could someone from the team like <span data-props=\"\
          {&quot;user&quot;:&quot;pstock&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/pstock\">@<span class=\"underline\">pstock</span></a></span>\n\
          \n\t</span></span> or <span data-props=\"{&quot;user&quot;:&quot;timlacroix&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/timlacroix\"\
          >@<span class=\"underline\">timlacroix</span></a></span>\n\n\t</span></span>\
          \ clear this up? Appreciate it.</p>\n"
        raw: 'The Model card says it is important to get the prompt template correct
          or else the model will produce sub-optimal outputs, but which prompt template
          is correct? Two different ones have been given.


          The one from the model card:

          ```

          <s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction
          [/INST]

          ```


          The one from tokenizer_config.json:

          ```

          <s>[INST] Instruction [/INST]Model answer</s>[INST] Follow-up instruction
          [/INST]

          ```


          Also, are we supposed to leave every EOS token in for each bot response
          in the conversation? That''s what the pseudocode of the model card and the
          prompt template in tokenizer_config.json imply but I haven''t seen that
          done before.


          Could someone from the team like @pstock or @timlacroix clear this up? Appreciate
          it.'
        updatedAt: '2024-01-04T03:40:18.632Z'
      numEdits: 1
      reactions:
      - count: 10
        reaction: "\U0001F44D"
        users:
        - andysalerno
        - GillesJacobs
        - SpectreDevices
        - harsh244
        - MikeWinkelmannXL2
        - dengie
        - youssefoud
        - jusKnows
        - anmolagarwal999
        - askmyteapot
    id: 659624277cda5685b33b77e1
    type: comment
  author: lemonflourorange
  content: 'The Model card says it is important to get the prompt template correct
    or else the model will produce sub-optimal outputs, but which prompt template
    is correct? Two different ones have been given.


    The one from the model card:

    ```

    <s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]

    ```


    The one from tokenizer_config.json:

    ```

    <s>[INST] Instruction [/INST]Model answer</s>[INST] Follow-up instruction [/INST]

    ```


    Also, are we supposed to leave every EOS token in for each bot response in the
    conversation? That''s what the pseudocode of the model card and the prompt template
    in tokenizer_config.json imply but I haven''t seen that done before.


    Could someone from the team like @pstock or @timlacroix clear this up? Appreciate
    it.'
  created_at: 2024-01-04 03:21:11+00:00
  edited: true
  hidden: false
  id: 659624277cda5685b33b77e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2d1630120c665a0739f56c491c9fd0e.svg
      fullname: Juk Armstrong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jukofyork
      type: user
    createdAt: '2024-01-12T21:39:11.000Z'
    data:
      edited: false
      editors:
      - jukofyork
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.917471170425415
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2d1630120c665a0739f56c491c9fd0e.svg
          fullname: Juk Armstrong
          isHf: false
          isPro: false
          name: jukofyork
          type: user
        html: '<blockquote>

          <p>The Model card says it is important to get the prompt template correct
          or else the model will produce sub-optimal outputs, but which prompt template
          is correct? Two different ones have been given.</p>

          <p>The one from the model card:</p>

          <pre><code>&lt;s&gt; [INST] Instruction [/INST] Model answer&lt;/s&gt; [INST]
          Follow-up instruction [/INST]

          </code></pre>

          <p>The one from tokenizer_config.json:</p>

          <pre><code>&lt;s&gt;[INST] Instruction [/INST]Model answer&lt;/s&gt;[INST]
          Follow-up instruction [/INST]

          </code></pre>

          </blockquote>

          <p>I''ve been wondering this too, and after quite a bit of experimentation
          I''m fairly sure it''s the first prompt format and think it''s just a typo.
          The Mistral instruct uses the first prompt format too:</p>

          <p><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2</a></p>

          <p>I''ve also found it''s important to NOT leave a space after the closing
          [/INST] , as even though it irritatingly adds a space itself when replying,
          it can cause the output to come up with Chinese Unicode characters sometimes
          (deepseek-llm also does this is you add the space!), but for codellama it''s
          VERY important to add the space (and I think for llama2 it should be there,
          although it doesn''t seem to matter so much).</p>

          '
        raw: "> The Model card says it is important to get the prompt template correct\
          \ or else the model will produce sub-optimal outputs, but which prompt template\
          \ is correct? Two different ones have been given.\n> \n> The one from the\
          \ model card:\n> ```\n> <s> [INST] Instruction [/INST] Model answer</s>\
          \ [INST] Follow-up instruction [/INST]\n> ```\n> \n> The one from tokenizer_config.json:\n\
          > ```\n> <s>[INST] Instruction [/INST]Model answer</s>[INST] Follow-up instruction\
          \ [/INST]\n> ```\n\nI've been wondering this too, and after quite a bit\
          \ of experimentation I'm fairly sure it's the first prompt format and think\
          \ it's just a typo. The Mistral instruct uses the first prompt format too:\n\
          \nhttps://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n\nI've also\
          \ found it's important to NOT leave a space after the closing [/INST] ,\
          \ as even though it irritatingly adds a space itself when replying, it can\
          \ cause the output to come up with Chinese Unicode characters sometimes\
          \ (deepseek-llm also does this is you add the space!), but for codellama\
          \ it's VERY important to add the space (and I think for llama2 it should\
          \ be there, although it doesn't seem to matter so much)."
        updatedAt: '2024-01-12T21:39:11.005Z'
      numEdits: 0
      reactions: []
    id: 65a1b17f81a46e7dd9a516dd
    type: comment
  author: jukofyork
  content: "> The Model card says it is important to get the prompt template correct\
    \ or else the model will produce sub-optimal outputs, but which prompt template\
    \ is correct? Two different ones have been given.\n> \n> The one from the model\
    \ card:\n> ```\n> <s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up\
    \ instruction [/INST]\n> ```\n> \n> The one from tokenizer_config.json:\n> ```\n\
    > <s>[INST] Instruction [/INST]Model answer</s>[INST] Follow-up instruction [/INST]\n\
    > ```\n\nI've been wondering this too, and after quite a bit of experimentation\
    \ I'm fairly sure it's the first prompt format and think it's just a typo. The\
    \ Mistral instruct uses the first prompt format too:\n\nhttps://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n\
    \nI've also found it's important to NOT leave a space after the closing [/INST]\
    \ , as even though it irritatingly adds a space itself when replying, it can cause\
    \ the output to come up with Chinese Unicode characters sometimes (deepseek-llm\
    \ also does this is you add the space!), but for codellama it's VERY important\
    \ to add the space (and I think for llama2 it should be there, although it doesn't\
    \ seem to matter so much)."
  created_at: 2024-01-12 21:39:11+00:00
  edited: false
  hidden: false
  id: 65a1b17f81a46e7dd9a516dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bea11e9c4bf19b2a1884eda1a8cbf00.svg
      fullname: Lemon Flour
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lemonflourorange
      type: user
    createdAt: '2024-01-15T15:13:11.000Z'
    data:
      edited: true
      editors:
      - lemonflourorange
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.949970543384552
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bea11e9c4bf19b2a1884eda1a8cbf00.svg
          fullname: Lemon Flour
          isHf: false
          isPro: false
          name: lemonflourorange
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jukofyork&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jukofyork\">@<span class=\"\
          underline\">jukofyork</span></a></span>\n\n\t</span></span> I think the\
          \ tokenizer_config.json prompt is more than just a typo, as for the rest,\
          \ I've found the same.</p>\n<p>I've tested the tokenizer_config.json prompt\
          \ extensively, and I think the reason it removes spaces is because, like\
          \ you mention, the model automatically adds a space between <code>[/INST]</code>\
          \ and <code>Model answer</code> and it will cause problems if you add the\
          \ space manually. I think the same is true for the space between <code>&lt;s&gt;</code>\
          \ and <code>[INST]</code> where the model will automatically add it in,\
          \ however, the model will not add a space between <code>&lt;/s&gt;</code>\
          \ and <code>[INST]</code> so I'm not sure why that space was removed, and\
          \ I still don't know if we should leave the EOS token in or out of every\
          \ bot response.</p>\n"
        raw: '@jukofyork I think the tokenizer_config.json prompt is more than just
          a typo, as for the rest, I''ve found the same.


          I''ve tested the tokenizer_config.json prompt extensively, and I think the
          reason it removes spaces is because, like you mention, the model automatically
          adds a space between `[/INST]` and `Model answer` and it will cause problems
          if you add the space manually. I think the same is true for the space between
          `<s>` and `[INST]` where the model will automatically add it in, however,
          the model will not add a space between `</s>` and `[INST]` so I''m not sure
          why that space was removed, and I still don''t know if we should leave the
          EOS token in or out of every bot response.'
        updatedAt: '2024-01-15T15:23:59.315Z'
      numEdits: 4
      reactions: []
    id: 65a54b87215aabac48ad691b
    type: comment
  author: lemonflourorange
  content: '@jukofyork I think the tokenizer_config.json prompt is more than just
    a typo, as for the rest, I''ve found the same.


    I''ve tested the tokenizer_config.json prompt extensively, and I think the reason
    it removes spaces is because, like you mention, the model automatically adds a
    space between `[/INST]` and `Model answer` and it will cause problems if you add
    the space manually. I think the same is true for the space between `<s>` and `[INST]`
    where the model will automatically add it in, however, the model will not add
    a space between `</s>` and `[INST]` so I''m not sure why that space was removed,
    and I still don''t know if we should leave the EOS token in or out of every bot
    response.'
  created_at: 2024-01-15 15:13:11+00:00
  edited: true
  hidden: false
  id: 65a54b87215aabac48ad691b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2d1630120c665a0739f56c491c9fd0e.svg
      fullname: Juk Armstrong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jukofyork
      type: user
    createdAt: '2024-01-15T15:35:47.000Z'
    data:
      edited: true
      editors:
      - jukofyork
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9553902745246887
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2d1630120c665a0739f56c491c9fd0e.svg
          fullname: Juk Armstrong
          isHf: false
          isPro: false
          name: jukofyork
          type: user
        html: '<p>I was having a lot of trouble with the prompt templates and tried
          my best to find the correct ones for as many of the original/official models
          as possible:</p>

          <p><a rel="nofollow" href="https://github.com/jmorganca/ollama/issues/1977">https://github.com/jmorganca/ollama/issues/1977</a></p>

          <p>Just one tiny mistake can make a huge difference for some of the models
          and it''s probably not all that obvious unless you try to get them to ingest
          a very large amount of sourcecode or similar.</p>

          <p>codellama is the worst effected by this and I do wonder if a lot of the
          benchmarks people have run were with the wrong template - it''s actually
          a lot better than people realise if you use the correct template.</p>

          <p>I''m also not sure about the need to use the <code>&lt;s&gt;</code> type
          tags, but I followed the official instructions to a letter (eg: llama2 and
          codellama say to add them, Mixtral and Minstral say to only add them to
          the first message, etc). </p>

          <p>If there''s one thing we desperately need, it''s some standard prompt
          template for future models!</p>

          '
        raw: "I was having a lot of trouble with the prompt templates and tried my\
          \ best to find the correct ones for as many of the original/official models\
          \ as possible:\n\nhttps://github.com/jmorganca/ollama/issues/1977\n\nJust\
          \ one tiny mistake can make a huge difference for some of the models and\
          \ it's probably not all that obvious unless you try to get them to ingest\
          \ a very large amount of sourcecode or similar.\n\ncodellama is the worst\
          \ effected by this and I do wonder if a lot of the benchmarks people have\
          \ run were with the wrong template - it's actually a lot better than people\
          \ realise if you use the correct template.\n\nI'm also not sure about the\
          \ need to use the `<s>` type tags, but I followed the official instructions\
          \ to a letter (eg: llama2 and codellama say to add them, Mixtral and Minstral\
          \ say to only add them to the first message, etc). \n\nIf there's one thing\
          \ we desperately need, it's some standard prompt template for future models!"
        updatedAt: '2024-01-15T15:37:18.211Z'
      numEdits: 1
      reactions: []
    id: 65a550d390b5e87bcd4784f1
    type: comment
  author: jukofyork
  content: "I was having a lot of trouble with the prompt templates and tried my best\
    \ to find the correct ones for as many of the original/official models as possible:\n\
    \nhttps://github.com/jmorganca/ollama/issues/1977\n\nJust one tiny mistake can\
    \ make a huge difference for some of the models and it's probably not all that\
    \ obvious unless you try to get them to ingest a very large amount of sourcecode\
    \ or similar.\n\ncodellama is the worst effected by this and I do wonder if a\
    \ lot of the benchmarks people have run were with the wrong template - it's actually\
    \ a lot better than people realise if you use the correct template.\n\nI'm also\
    \ not sure about the need to use the `<s>` type tags, but I followed the official\
    \ instructions to a letter (eg: llama2 and codellama say to add them, Mixtral\
    \ and Minstral say to only add them to the first message, etc). \n\nIf there's\
    \ one thing we desperately need, it's some standard prompt template for future\
    \ models!"
  created_at: 2024-01-15 15:35:47+00:00
  edited: true
  hidden: false
  id: 65a550d390b5e87bcd4784f1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 75
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Inconsistent prompt format. Which is correct the Model card or the tokenizer_config.json?
