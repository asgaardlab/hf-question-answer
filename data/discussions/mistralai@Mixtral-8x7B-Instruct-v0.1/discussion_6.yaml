!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jf-11
conflicting_files: null
created_at: 2023-12-11 16:22:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ece3a86343a356327e51ea/VqHTiczuNmB42ArzviLPI.png?w=200&h=200&f=face
      fullname: jf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jf-11
      type: user
    createdAt: '2023-12-11T16:22:10.000Z'
    data:
      edited: false
      editors:
      - jf-11
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8998733758926392
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ece3a86343a356327e51ea/VqHTiczuNmB42ArzviLPI.png?w=200&h=200&f=face
          fullname: jf
          isHf: false
          isPro: false
          name: jf-11
          type: user
        html: '<p>Hi,<br>I am a bit confused regarding the instruction template.<br>It
          is stated that the template has to be followed strictly, does this also
          hold for the inference? Because in the section "Run the model" the template
          is not used with the <code>.generate</code> function.</p>

          <p>Thanks for clarifying.</p>

          '
        raw: "Hi, \r\nI am a bit confused regarding the instruction template.\r\n\
          It is stated that the template has to be followed strictly, does this also\
          \ hold for the inference? Because in the section \"Run the model\" the template\
          \ is not used with the `.generate` function.\r\n\r\nThanks for clarifying."
        updatedAt: '2023-12-11T16:22:10.473Z'
      numEdits: 0
      reactions: []
    id: 657737323ff7609fbeee924b
    type: comment
  author: jf-11
  content: "Hi, \r\nI am a bit confused regarding the instruction template.\r\nIt\
    \ is stated that the template has to be followed strictly, does this also hold\
    \ for the inference? Because in the section \"Run the model\" the template is\
    \ not used with the `.generate` function.\r\n\r\nThanks for clarifying."
  created_at: 2023-12-11 16:22:10+00:00
  edited: false
  hidden: false
  id: 657737323ff7609fbeee924b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631772607690c5b55e5b5edd/lKMkwA9LjujEkJknbSzbU.jpeg?w=200&h=200&f=face
      fullname: Salman Faroz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SalmanFaroz
      type: user
    createdAt: '2023-12-11T16:38:35.000Z'
    data:
      edited: true
      editors:
      - SalmanFaroz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8292135000228882
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631772607690c5b55e5b5edd/lKMkwA9LjujEkJknbSzbU.jpeg?w=200&h=200&f=face
          fullname: Salman Faroz
          isHf: false
          isPro: false
          name: SalmanFaroz
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jf-11&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jf-11\">@<span class=\"\
          underline\">jf-11</span></a></span>\n\n\t</span></span> It is best to follow\
          \ the recommended format in inference order to obtain better outcomes.</p>\n\
          <pre><code>&lt;s&gt; [INST] Instruction [/INST] Model answer&lt;/s&gt; [INST]\
          \ Follow-up instruction [/INST]\n</code></pre>\n<p>OR<br>you could simply\
          \ use apply_chat_template</p>\n<pre><code>from transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
          )\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your\
          \ favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"\
          Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds\
          \ just the right amount of zesty flavour to whatever I'm cooking up in the\
          \ kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise\
          \ recipes?\"}\n]\n\nmessage_formatted = tokenizer.apply_chat_template(messages,\
          \ tokenize=False)\n</code></pre>\n<pre><code>&lt;s&gt;[INST] What is your\
          \ favourite condiment? [/INST]Well, I'm quite partial to a good squeeze\
          \ of fresh lemon juice. It adds just the right amount of zesty flavour to\
          \ whatever I'm cooking up in the kitchen!&lt;/s&gt;[INST] Do you have mayonnaise\
          \ recipes? [/INST]\n</code></pre>\n<p> However, if you don't, it can still\
          \ work. They just gave an simple example.</p>\n"
        raw: "@jf-11 It is best to follow the recommended format in inference order\
          \ to obtain better outcomes.\n```\n<s> [INST] Instruction [/INST] Model\
          \ answer</s> [INST] Follow-up instruction [/INST]\n```\nOR\nyou could simply\
          \ use apply_chat_template\n```\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
          )\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your\
          \ favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"\
          Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds\
          \ just the right amount of zesty flavour to whatever I'm cooking up in the\
          \ kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise\
          \ recipes?\"}\n]\n\nmessage_formatted = tokenizer.apply_chat_template(messages,\
          \ tokenize=False)\n```\n```\n<s>[INST] What is your favourite condiment?\
          \ [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice.\
          \ It adds just the right amount of zesty flavour to whatever I'm cooking\
          \ up in the kitchen!</s>[INST] Do you have mayonnaise recipes? [/INST]\n\
          ```\n However, if you don't, it can still work. They just gave an simple\
          \ example."
        updatedAt: '2023-12-11T16:41:02.207Z'
      numEdits: 3
      reactions: []
    id: 65773b0b186988d846ae9090
    type: comment
  author: SalmanFaroz
  content: "@jf-11 It is best to follow the recommended format in inference order\
    \ to obtain better outcomes.\n```\n<s> [INST] Instruction [/INST] Model answer</s>\
    \ [INST] Follow-up instruction [/INST]\n```\nOR\nyou could simply use apply_chat_template\n\
    ```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n\n\
    messages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"\
    },\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a\
    \ good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour\
    \ to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"\
    content\": \"Do you have mayonnaise recipes?\"}\n]\n\nmessage_formatted = tokenizer.apply_chat_template(messages,\
    \ tokenize=False)\n```\n```\n<s>[INST] What is your favourite condiment? [/INST]Well,\
    \ I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right\
    \ amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>[INST]\
    \ Do you have mayonnaise recipes? [/INST]\n```\n However, if you don't, it can\
    \ still work. They just gave an simple example."
  created_at: 2023-12-11 16:38:35+00:00
  edited: true
  hidden: false
  id: 65773b0b186988d846ae9090
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ece3a86343a356327e51ea/VqHTiczuNmB42ArzviLPI.png?w=200&h=200&f=face
      fullname: jf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jf-11
      type: user
    createdAt: '2023-12-11T16:41:27.000Z'
    data:
      edited: true
      editors:
      - jf-11
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.828086793422699
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ece3a86343a356327e51ea/VqHTiczuNmB42ArzviLPI.png?w=200&h=200&f=face
          fullname: jf
          isHf: false
          isPro: false
          name: jf-11
          type: user
        html: '<p>I see. Thank you!<br>So I should also use the [INST], [/INST] literals
          when using <code>.generate()</code> and <code>tokenizer(text, return_tensors="pt")</code>
          does add special tokens by default? Otherwise it would be also better to
          use <code>&lt;s&gt;</code> in the beginning?</p>

          '
        raw: 'I see. Thank you!

          So I should also use the [INST], [/INST] literals when using `.generate()`
          and `tokenizer(text, return_tensors="pt")` does add special tokens by default?
          Otherwise it would be also better to use `<s>` in the beginning?'
        updatedAt: '2023-12-11T16:42:31.387Z'
      numEdits: 3
      reactions: []
    id: 65773bb752f02732a4624560
    type: comment
  author: jf-11
  content: 'I see. Thank you!

    So I should also use the [INST], [/INST] literals when using `.generate()` and
    `tokenizer(text, return_tensors="pt")` does add special tokens by default? Otherwise
    it would be also better to use `<s>` in the beginning?'
  created_at: 2023-12-11 16:41:27+00:00
  edited: true
  hidden: false
  id: 65773bb752f02732a4624560
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631772607690c5b55e5b5edd/lKMkwA9LjujEkJknbSzbU.jpeg?w=200&h=200&f=face
      fullname: Salman Faroz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SalmanFaroz
      type: user
    createdAt: '2023-12-11T16:49:05.000Z'
    data:
      edited: true
      editors:
      - SalmanFaroz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7649994492530823
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631772607690c5b55e5b5edd/lKMkwA9LjujEkJknbSzbU.jpeg?w=200&h=200&f=face
          fullname: Salman Faroz
          isHf: false
          isPro: false
          name: SalmanFaroz
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jf-11&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jf-11\">@<span class=\"\
          underline\">jf-11</span></a></span>\n\n\t</span></span> No , we have to\
          \ include it by ourselves. You can conveniently utilize \"apply_chat_template\"\
          \ for this purpose. As the conversation history grows, it's advisable to\
          \ manage it with \"apply_chat_template.\" Afterward, you can proceed with\
          \ the following code:</p>\n<pre><code>inputs = tokenizer(message_formatted,\
          \ return_tensors=\"pt\")\n\noutputs = model.generate(**inputs, max_new_tokens=20)\n\
          print(tokenizer.decode(outputs[0], skip_special_tokens=True)) \n</code></pre>\n"
        raw: "@jf-11 No , we have to include it by ourselves. You can conveniently\
          \ utilize \"apply_chat_template\" for this purpose. As the conversation\
          \ history grows, it's advisable to manage it with \"apply_chat_template.\"\
          \ Afterward, you can proceed with the following code:\n```\ninputs = tokenizer(message_formatted,\
          \ return_tensors=\"pt\")\n\noutputs = model.generate(**inputs, max_new_tokens=20)\n\
          print(tokenizer.decode(outputs[0], skip_special_tokens=True)) \n```"
        updatedAt: '2023-12-11T17:01:23.608Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - jf-11
        - Auxon
    id: 65773d81186988d846af4bbe
    type: comment
  author: SalmanFaroz
  content: "@jf-11 No , we have to include it by ourselves. You can conveniently utilize\
    \ \"apply_chat_template\" for this purpose. As the conversation history grows,\
    \ it's advisable to manage it with \"apply_chat_template.\" Afterward, you can\
    \ proceed with the following code:\n```\ninputs = tokenizer(message_formatted,\
    \ return_tensors=\"pt\")\n\noutputs = model.generate(**inputs, max_new_tokens=20)\n\
    print(tokenizer.decode(outputs[0], skip_special_tokens=True)) \n```"
  created_at: 2023-12-11 16:49:05+00:00
  edited: true
  hidden: false
  id: 65773d81186988d846af4bbe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-12-11T17:57:40.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9351733326911926
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: '<p>Follow-up question regarding inference with common end-user backends/frontends
          like KoboldCpp/SillyTavern:</p>

          <p>After inference generates <code>&lt;s&gt; [INST] Instruction [/INST]
          Model answer&lt;/s&gt;</code>, the stop token is usually caught to end inference,
          then special tokens are removed from the output when it is shown in the
          frontend. User replies and next context looks like this:</p>

          <p><code>&lt;s&gt; [INST] Instruction [/INST] Model answer [INST] Follow-up
          instruction [/INST]</code> - so previously generated stop tokens are gone.
          Is that OK or would that be a problem?</p>

          <p>Also, as constructive feedback, I''d have loved to see a system prompt.
          Especially if we were to implement something like <a rel="nofollow" href="https://docs.mistral.ai/platform/guardrailing/">https://docs.mistral.ai/platform/guardrailing/</a>
          - it would always be a good thing for the model to be able to differentiate
          between user and system prompts. But if you do implement one, please don''t
          do it as part of the first INST like Llama 2 Chat''s format did, which causes
          lots of problems because of its terrible design (I''d gladly elaborate if
          you''d like).</p>

          '
        raw: 'Follow-up question regarding inference with common end-user backends/frontends
          like KoboldCpp/SillyTavern:


          After inference generates `<s> [INST] Instruction [/INST] Model answer</s>`,
          the stop token is usually caught to end inference, then special tokens are
          removed from the output when it is shown in the frontend. User replies and
          next context looks like this:


          `<s> [INST] Instruction [/INST] Model answer [INST] Follow-up instruction
          [/INST]` - so previously generated stop tokens are gone. Is that OK or would
          that be a problem?


          Also, as constructive feedback, I''d have loved to see a system prompt.
          Especially if we were to implement something like https://docs.mistral.ai/platform/guardrailing/
          - it would always be a good thing for the model to be able to differentiate
          between user and system prompts. But if you do implement one, please don''t
          do it as part of the first INST like Llama 2 Chat''s format did, which causes
          lots of problems because of its terrible design (I''d gladly elaborate if
          you''d like).'
        updatedAt: '2023-12-11T17:57:40.033Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Cameron-Chen
    id: 65774d9417b474ae6b567048
    type: comment
  author: wolfram
  content: 'Follow-up question regarding inference with common end-user backends/frontends
    like KoboldCpp/SillyTavern:


    After inference generates `<s> [INST] Instruction [/INST] Model answer</s>`, the
    stop token is usually caught to end inference, then special tokens are removed
    from the output when it is shown in the frontend. User replies and next context
    looks like this:


    `<s> [INST] Instruction [/INST] Model answer [INST] Follow-up instruction [/INST]`
    - so previously generated stop tokens are gone. Is that OK or would that be a
    problem?


    Also, as constructive feedback, I''d have loved to see a system prompt. Especially
    if we were to implement something like https://docs.mistral.ai/platform/guardrailing/
    - it would always be a good thing for the model to be able to differentiate between
    user and system prompts. But if you do implement one, please don''t do it as part
    of the first INST like Llama 2 Chat''s format did, which causes lots of problems
    because of its terrible design (I''d gladly elaborate if you''d like).'
  created_at: 2023-12-11 17:57:40+00:00
  edited: false
  hidden: false
  id: 65774d9417b474ae6b567048
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a94f9d044b225b7e884c54f5a943ee58.svg
      fullname: "M\xE8o \xDA"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: duyntnet
      type: user
    createdAt: '2023-12-12T03:03:30.000Z'
    data:
      edited: true
      editors:
      - duyntnet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9563328623771667
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a94f9d044b225b7e884c54f5a943ee58.svg
          fullname: "M\xE8o \xDA"
          isHf: false
          isPro: false
          name: duyntnet
          type: user
        html: '<p>Edited: Using default prompt from llama.cpp solved the problem for
          me. The prompt from the model page didn''t work in my case, maybe I did
          something wrong ,I don''t know.</p>

          <p>I''m using this: <s> [INST] Instruction [/INST] Model answer</s> [INST]
          Follow-up instruction [/INST]<br>But the model doesn''t answer my question.
          It creates its own question then answer that question. I wonder what I did
          wrong? Here''s an example:</p>

          <p>User: what can you do?</p>

          <p>Llama: Sure, I''ll do my best to provide a model answer for your question.
          Please provide me with the question or topic you would like me to write
          about.</p>

          <p>User: can you translate</p>

          <p>Llama: Sure, I''ll do my best to provide a model answer for your question.
          Please keep in mind that the quality of the response may depend on the complexity
          and specifics of the question you have asked. Also, note that while I strive
          to provide accurate information, it is always a good idea to double-check
          any facts or figures with additional sources. With this said, please let
          me know what your follow-up instruction is.</p>

          <p>User: write a short poem</p>

          <p>Llama: Sure, I''ll do my best to provide a model answer for your question.
          Please allow me a moment to understand the context and what is being asked.</p>

          <p>If you could provide me with more information about the topic, prompt,
          or specific language point that you would like me to address in the model
          answer, it would be helpful. Otherwise, I will provide a general example
          of how to structure an effective response.</p>

          <p>Here''s a possible model answer:</p>

          <p>Prompt: How has technology impacted education?</p>

          <p>Model Answer:</p>

          <p>Technology has significantly transformed...</p>

          '
        raw: 'Edited: Using default prompt from llama.cpp solved the problem for me.
          The prompt from the model page didn''t work in my case, maybe I did something
          wrong ,I don''t know.


          I''m using this: <s> [INST] Instruction [/INST] Model answer</s> [INST]
          Follow-up instruction [/INST]

          But the model doesn''t answer my question. It creates its own question then
          answer that question. I wonder what I did wrong? Here''s an example:


          User: what can you do?


          Llama: Sure, I''ll do my best to provide a model answer for your question.
          Please provide me with the question or topic you would like me to write
          about.


          User: can you translate


          Llama: Sure, I''ll do my best to provide a model answer for your question.
          Please keep in mind that the quality of the response may depend on the complexity
          and specifics of the question you have asked. Also, note that while I strive
          to provide accurate information, it is always a good idea to double-check
          any facts or figures with additional sources. With this said, please let
          me know what your follow-up instruction is.


          User: write a short poem


          Llama: Sure, I''ll do my best to provide a model answer for your question.
          Please allow me a moment to understand the context and what is being asked.


          If you could provide me with more information about the topic, prompt, or
          specific language point that you would like me to address in the model answer,
          it would be helpful. Otherwise, I will provide a general example of how
          to structure an effective response.


          Here''s a possible model answer:


          Prompt: How has technology impacted education?


          Model Answer:


          Technology has significantly transformed...'
        updatedAt: '2023-12-12T03:27:26.012Z'
      numEdits: 3
      reactions: []
    id: 6577cd827685e1ce5c0727e0
    type: comment
  author: duyntnet
  content: 'Edited: Using default prompt from llama.cpp solved the problem for me.
    The prompt from the model page didn''t work in my case, maybe I did something
    wrong ,I don''t know.


    I''m using this: <s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up
    instruction [/INST]

    But the model doesn''t answer my question. It creates its own question then answer
    that question. I wonder what I did wrong? Here''s an example:


    User: what can you do?


    Llama: Sure, I''ll do my best to provide a model answer for your question. Please
    provide me with the question or topic you would like me to write about.


    User: can you translate


    Llama: Sure, I''ll do my best to provide a model answer for your question. Please
    keep in mind that the quality of the response may depend on the complexity and
    specifics of the question you have asked. Also, note that while I strive to provide
    accurate information, it is always a good idea to double-check any facts or figures
    with additional sources. With this said, please let me know what your follow-up
    instruction is.


    User: write a short poem


    Llama: Sure, I''ll do my best to provide a model answer for your question. Please
    allow me a moment to understand the context and what is being asked.


    If you could provide me with more information about the topic, prompt, or specific
    language point that you would like me to address in the model answer, it would
    be helpful. Otherwise, I will provide a general example of how to structure an
    effective response.


    Here''s a possible model answer:


    Prompt: How has technology impacted education?


    Model Answer:


    Technology has significantly transformed...'
  created_at: 2023-12-12 03:03:30+00:00
  edited: true
  hidden: false
  id: 6577cd827685e1ce5c0727e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/5n01J00ZaVRrebsON8iYA.jpeg?w=200&h=200&f=face
      fullname: Cameron Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cameron-Chen
      type: user
    createdAt: '2023-12-20T07:32:10.000Z'
    data:
      edited: false
      editors:
      - Cameron-Chen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.804958164691925
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/5n01J00ZaVRrebsON8iYA.jpeg?w=200&h=200&f=face
          fullname: Cameron Chen
          isHf: false
          isPro: false
          name: Cameron-Chen
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;wolfram&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/wolfram\">@<span class=\"\
          underline\">wolfram</span></a></span>\n\n\t</span></span> I have seen one\
          \ example of system prompt from <a rel=\"nofollow\" href=\"https://deepinfra.com/mistralai/Mixtral-8x7B-Instruct-v0.1/api\"\
          >deepinfra.com</a>:</p>\n<pre><code>[INST] &lt;&lt;SYS&gt;&gt;\nyour system\
          \ prompt goes here\n&lt;&lt;SYS&gt;&gt;\n\nfirst question [/INST] ...\n\
          </code></pre>\n<p>It is not from the official document though. </p>\n"
        raw: '@wolfram I have seen one example of system prompt from [deepinfra.com](https://deepinfra.com/mistralai/Mixtral-8x7B-Instruct-v0.1/api):


          ```

          [INST] <<SYS>>

          your system prompt goes here

          <<SYS>>


          first question [/INST] ...

          ```


          It is not from the official document though. '
        updatedAt: '2023-12-20T07:32:10.796Z'
      numEdits: 0
      reactions: []
    id: 6582987ab1c4ab777aecd138
    type: comment
  author: Cameron-Chen
  content: '@wolfram I have seen one example of system prompt from [deepinfra.com](https://deepinfra.com/mistralai/Mixtral-8x7B-Instruct-v0.1/api):


    ```

    [INST] <<SYS>>

    your system prompt goes here

    <<SYS>>


    first question [/INST] ...

    ```


    It is not from the official document though. '
  created_at: 2023-12-20 07:32:10+00:00
  edited: false
  hidden: false
  id: 6582987ab1c4ab777aecd138
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-12-22T00:10:53.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8761154413223267
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Cameron-Chen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Cameron-Chen\"\
          >@<span class=\"underline\">Cameron-Chen</span></a></span>\n\n\t</span></span>\
          \ Yeah, that's Llama 2 Chat. I'm using just that with Mixtral, too, and\
          \ it works good enough.</p>\n<p>Also did an extensive test of Mixtral 8x7B\
          \ Instruct with <strong>17</strong> different instruct templates:</p>\n\
          <p><a rel=\"nofollow\" href=\"https://www.reddit.com/r/LocalLLaMA/comments/18ljvxb/llm_prompt_format_comparisontest_mixtral_8x7b/\"\
          >https://www.reddit.com/r/LocalLLaMA/comments/18ljvxb/llm_prompt_format_comparisontest_mixtral_8x7b/</a></p>\n\
          <p>Probably of interest to Mistral, too, if they haven't seen it yet. Especially\
          \ my explanation of why the Llama 2 Chat format is such a terrible choice\
          \ and should be replaced with something more flexible and future-proof like\
          \ the ChatML format.</p>\n"
        raw: '@Cameron-Chen Yeah, that''s Llama 2 Chat. I''m using just that with
          Mixtral, too, and it works good enough.


          Also did an extensive test of Mixtral 8x7B Instruct with **17** different
          instruct templates:


          https://www.reddit.com/r/LocalLLaMA/comments/18ljvxb/llm_prompt_format_comparisontest_mixtral_8x7b/


          Probably of interest to Mistral, too, if they haven''t seen it yet. Especially
          my explanation of why the Llama 2 Chat format is such a terrible choice
          and should be replaced with something more flexible and future-proof like
          the ChatML format.'
        updatedAt: '2023-12-22T00:10:53.890Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - r000bin
        - MaziyarPanahi
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Cameron-Chen
    id: 6584d40dbae0736365961969
    type: comment
  author: wolfram
  content: '@Cameron-Chen Yeah, that''s Llama 2 Chat. I''m using just that with Mixtral,
    too, and it works good enough.


    Also did an extensive test of Mixtral 8x7B Instruct with **17** different instruct
    templates:


    https://www.reddit.com/r/LocalLLaMA/comments/18ljvxb/llm_prompt_format_comparisontest_mixtral_8x7b/


    Probably of interest to Mistral, too, if they haven''t seen it yet. Especially
    my explanation of why the Llama 2 Chat format is such a terrible choice and should
    be replaced with something more flexible and future-proof like the ChatML format.'
  created_at: 2023-12-22 00:10:53+00:00
  edited: false
  hidden: false
  id: 6584d40dbae0736365961969
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Instruction Template during inference?
