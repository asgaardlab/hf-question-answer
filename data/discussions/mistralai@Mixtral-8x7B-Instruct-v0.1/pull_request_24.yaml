!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alpindale
conflicting_files:
- config.json
created_at: 2023-12-13 13:20:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
      fullname: Alpin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alpindale
      type: user
    createdAt: '2023-12-13T13:20:25.000Z'
    data:
      edited: false
      editors:
      - alpindale
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9642745852470398
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
          fullname: Alpin
          isHf: false
          isPro: false
          name: alpindale
          type: user
        html: '<p>Some backends, such as vLLM, do not dynamically adjust the sliding
          window param based on the max context length provided. Modifying it here
          seems to fix that.</p>

          '
        raw: Some backends, such as vLLM, do not dynamically adjust the sliding window
          param based on the max context length provided. Modifying it here seems
          to fix that.
        updatedAt: '2023-12-13T13:20:25.589Z'
      numEdits: 0
      reactions: []
    id: 6579af99be2139f9aff04bdc
    type: comment
  author: alpindale
  content: Some backends, such as vLLM, do not dynamically adjust the sliding window
    param based on the max context length provided. Modifying it here seems to fix
    that.
  created_at: 2023-12-13 13:20:25+00:00
  edited: false
  hidden: false
  id: 6579af99be2139f9aff04bdc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
      fullname: Alpin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alpindale
      type: user
    createdAt: '2023-12-13T13:20:26.000Z'
    data:
      oid: d9a2b6594250a3e6c17ad50199ffde7ca20753e0
      parents:
      - f1ca00645f0b1565c7f9a1c863d2be6ebf896b04
      subject: Increase `sliding_window` to 32k
    id: 6579af9a0000000000000000
    type: commit
  author: alpindale
  created_at: 2023-12-13 13:20:26+00:00
  id: 6579af9a0000000000000000
  oid: d9a2b6594250a3e6c17ad50199ffde7ca20753e0
  summary: Increase `sliding_window` to 32k
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d38fcaa72d9f9cb04ba8e7f72211e34.svg
      fullname: Bohan Du
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: acrastt
      type: user
    createdAt: '2023-12-14T02:40:02.000Z'
    data:
      edited: false
      editors:
      - acrastt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9675882458686829
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d38fcaa72d9f9cb04ba8e7f72211e34.svg
          fullname: Bohan Du
          isHf: false
          isPro: false
          name: acrastt
          type: user
        html: '<blockquote>

          <p>Some backends, such as vLLM, do not dynamically adjust the sliding window
          param based on the max context length provided. Modifying it here seems
          to fix that.</p>

          </blockquote>

          <p>Though the sliding window is indeed 4k context length? Seems like a vLLM
          issue to me. I think this would break loaders that actually support SWA</p>

          '
        raw: '> Some backends, such as vLLM, do not dynamically adjust the sliding
          window param based on the max context length provided. Modifying it here
          seems to fix that.


          Though the sliding window is indeed 4k context length? Seems like a vLLM
          issue to me. I think this would break loaders that actually support SWA'
        updatedAt: '2023-12-14T02:40:02.887Z'
      numEdits: 0
      reactions: []
    id: 657a6b029ad6bcff7e5b3df9
    type: comment
  author: acrastt
  content: '> Some backends, such as vLLM, do not dynamically adjust the sliding window
    param based on the max context length provided. Modifying it here seems to fix
    that.


    Though the sliding window is indeed 4k context length? Seems like a vLLM issue
    to me. I think this would break loaders that actually support SWA'
  created_at: 2023-12-14 02:40:02+00:00
  edited: false
  hidden: false
  id: 657a6b029ad6bcff7e5b3df9
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 24
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: refs/heads/main
title: Increase `sliding_window` to 32k
