!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Boyue27
conflicting_files: null
created_at: 2023-12-12 19:49:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/48197bf36a63ec36a2b27e8eaae3729b.svg
      fullname: Boyue
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boyue27
      type: user
    createdAt: '2023-12-12T19:49:49.000Z'
    data:
      edited: false
      editors:
      - Boyue27
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7835707664489746
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/48197bf36a63ec36a2b27e8eaae3729b.svg
          fullname: Boyue
          isHf: false
          isPro: false
          name: Boyue27
          type: user
        html: '<p>I''ve been experiencing some issues with the generation speed  recently
          and was wondering if anyone else has encountered similar challenges. It
          seems like the process is slower than usual.</p>

          <p>model_path = "mixtral"<br>model = AutoModelForCausalLM.from_pretrained(<br>    model_path,device_map="auto",
          max_memory=max_memory_mapping<br>)<br>tokenizer =AutoTokenizer.from_pretrained(model_path)<br>.........................<br>output_ids
          = model.generate(input_ids=input_ids.cuda(),<br>                                do_sample=True,<br>                                temperature=0.4,<br>                                top_k=50,<br>                                max_new_tokens=300,)</p>

          '
        raw: "I've been experiencing some issues with the generation speed  recently\
          \ and was wondering if anyone else has encountered similar challenges. It\
          \ seems like the process is slower than usual.\r\n\r\nmodel_path = \"mixtral\"\
          \r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    model_path,device_map=\"\
          auto\", max_memory=max_memory_mapping\r\n)\r\ntokenizer =AutoTokenizer.from_pretrained(model_path)\r\
          \n.........................\r\noutput_ids = model.generate(input_ids=input_ids.cuda(),\r\
          \n                                do_sample=True,\r\n                  \
          \              temperature=0.4,\r\n                                top_k=50,\r\
          \n                                max_new_tokens=300,)\r\n\r\n\r\n"
        updatedAt: '2023-12-12T19:49:49.511Z'
      numEdits: 0
      reactions: []
    id: 6578b95dc1f02135b832c89c
    type: comment
  author: Boyue27
  content: "I've been experiencing some issues with the generation speed  recently\
    \ and was wondering if anyone else has encountered similar challenges. It seems\
    \ like the process is slower than usual.\r\n\r\nmodel_path = \"mixtral\"\r\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(\r\n    model_path,device_map=\"auto\"\
    , max_memory=max_memory_mapping\r\n)\r\ntokenizer =AutoTokenizer.from_pretrained(model_path)\r\
    \n.........................\r\noutput_ids = model.generate(input_ids=input_ids.cuda(),\r\
    \n                                do_sample=True,\r\n                        \
    \        temperature=0.4,\r\n                                top_k=50,\r\n   \
    \                             max_new_tokens=300,)\r\n\r\n\r\n"
  created_at: 2023-12-12 19:49:49+00:00
  edited: false
  hidden: false
  id: 6578b95dc1f02135b832c89c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5999f4c7dc1181de9a93cc958ca92588.svg
      fullname: Yuchen Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yucheny
      type: user
    createdAt: '2023-12-12T20:26:34.000Z'
    data:
      edited: false
      editors:
      - yucheny
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.974746584892273
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5999f4c7dc1181de9a93cc958ca92588.svg
          fullname: Yuchen Yang
          isHf: false
          isPro: false
          name: yucheny
          type: user
        html: '<p>Same here, the generation is very slow.</p>

          '
        raw: Same here, the generation is very slow.
        updatedAt: '2023-12-12T20:26:34.242Z'
      numEdits: 0
      reactions: []
    id: 6578c1faab3436f5adb6b9cb
    type: comment
  author: yucheny
  content: Same here, the generation is very slow.
  created_at: 2023-12-12 20:26:34+00:00
  edited: false
  hidden: false
  id: 6578c1faab3436f5adb6b9cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-13T09:04:07.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.958379864692688
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>If the model is offloaded to the CPU, then of course it''s going
          to be slow :/ The model did not change, unless you are computing the loss
          (which was not working on parallel devices). Make sure <code>output_router_logits</code>
          is set to <code>False</code> in the config</p>

          '
        raw: If the model is offloaded to the CPU, then of course it's going to be
          slow :/ The model did not change, unless you are computing the loss (which
          was not working on parallel devices). Make sure `output_router_logits` is
          set to `False` in the config
        updatedAt: '2023-12-13T09:04:07.080Z'
      numEdits: 0
      reactions: []
    id: 65797387197a6182c37a52b0
    type: comment
  author: ArthurZ
  content: If the model is offloaded to the CPU, then of course it's going to be slow
    :/ The model did not change, unless you are computing the loss (which was not
    working on parallel devices). Make sure `output_router_logits` is set to `False`
    in the config
  created_at: 2023-12-13 09:04:07+00:00
  edited: false
  hidden: false
  id: 65797387197a6182c37a52b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-13T09:19:42.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7213823199272156
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Boyue27&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Boyue27\">@<span class=\"\
          underline\">Boyue27</span></a></span>\n\n\t</span></span> your model is\
          \ most likely offloaded into CPU or disk as stated by Arthur, you need to\
          \ make sure you load your model in half-precision or 4-bit precision to\
          \ make sure your model is fit into your GPU device:</p>\n<p>For float16:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span>\
          \ torch\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"\
          hljs-keyword\">import</span> AutoModelForCausalLM\n\nmodel_path = <span\
          \ class=\"hljs-string\">\"mixtral\"</span>\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          model_path,device_map=<span class=\"hljs-string\">\"auto\"</span>, max_memory=max_memory_mapping,\
          \ torch_dtype=torch.float16\n)\ntokenizer =AutoTokenizer.from_pretrained(model_path)\n\
          </code></pre>\n<p>4-bit precision (after installing bitsandbytes (<code>pip\
          \ install bitsandbytes</code>):</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM\n\
          \nmodel_path = <span class=\"hljs-string\">\"mixtral\"</span>\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          model_path,device_map=<span class=\"hljs-string\">\"auto\"</span>, max_memory=max_memory_mapping,\
          \ load_in_4bit=<span class=\"hljs-literal\">True</span>\n)\ntokenizer =AutoTokenizer.from_pretrained(model_path)\n\
          </code></pre>\n"
        raw: '@Boyue27 your model is most likely offloaded into CPU or disk as stated
          by Arthur, you need to make sure you load your model in half-precision or
          4-bit precision to make sure your model is fit into your GPU device:


          For float16:

          ```python

          import torch

          from transformers import AutoModelForCausalLM


          model_path = "mixtral"

          model = AutoModelForCausalLM.from_pretrained(

          model_path,device_map="auto", max_memory=max_memory_mapping, torch_dtype=torch.float16

          )

          tokenizer =AutoTokenizer.from_pretrained(model_path)

          ```


          4-bit precision (after installing bitsandbytes (`pip install bitsandbytes`):


          ```python

          import torch

          from transformers import AutoModelForCausalLM


          model_path = "mixtral"

          model = AutoModelForCausalLM.from_pretrained(

          model_path,device_map="auto", max_memory=max_memory_mapping, load_in_4bit=True

          )

          tokenizer =AutoTokenizer.from_pretrained(model_path)

          ```

          '
        updatedAt: '2023-12-13T09:19:42.751Z'
      numEdits: 0
      reactions: []
    id: 6579772e7ee8616c7c4ac3df
    type: comment
  author: ybelkada
  content: '@Boyue27 your model is most likely offloaded into CPU or disk as stated
    by Arthur, you need to make sure you load your model in half-precision or 4-bit
    precision to make sure your model is fit into your GPU device:


    For float16:

    ```python

    import torch

    from transformers import AutoModelForCausalLM


    model_path = "mixtral"

    model = AutoModelForCausalLM.from_pretrained(

    model_path,device_map="auto", max_memory=max_memory_mapping, torch_dtype=torch.float16

    )

    tokenizer =AutoTokenizer.from_pretrained(model_path)

    ```


    4-bit precision (after installing bitsandbytes (`pip install bitsandbytes`):


    ```python

    import torch

    from transformers import AutoModelForCausalLM


    model_path = "mixtral"

    model = AutoModelForCausalLM.from_pretrained(

    model_path,device_map="auto", max_memory=max_memory_mapping, load_in_4bit=True

    )

    tokenizer =AutoTokenizer.from_pretrained(model_path)

    ```

    '
  created_at: 2023-12-13 09:19:42+00:00
  edited: false
  hidden: false
  id: 6579772e7ee8616c7c4ac3df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/48197bf36a63ec36a2b27e8eaae3729b.svg
      fullname: Boyue
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boyue27
      type: user
    createdAt: '2023-12-13T12:39:18.000Z'
    data:
      edited: false
      editors:
      - Boyue27
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9328567385673523
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/48197bf36a63ec36a2b27e8eaae3729b.svg
          fullname: Boyue
          isHf: false
          isPro: false
          name: Boyue27
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> Thank you for\
          \ your help. I have tested your code and it fixed the problem.</p>\n"
        raw: '@ybelkada Thank you for your help. I have tested your code and it fixed
          the problem.'
        updatedAt: '2023-12-13T12:39:18.728Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
    id: 6579a5f627dc66cbed10e153
    type: comment
  author: Boyue27
  content: '@ybelkada Thank you for your help. I have tested your code and it fixed
    the problem.'
  created_at: 2023-12-13 12:39:18+00:00
  edited: false
  hidden: false
  id: 6579a5f627dc66cbed10e153
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/48197bf36a63ec36a2b27e8eaae3729b.svg
      fullname: Boyue
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boyue27
      type: user
    createdAt: '2023-12-13T12:40:51.000Z'
    data:
      edited: false
      editors:
      - Boyue27
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9754156470298767
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/48197bf36a63ec36a2b27e8eaae3729b.svg
          fullname: Boyue
          isHf: false
          isPro: false
          name: Boyue27
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ArthurZ&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ArthurZ\">@<span class=\"\
          underline\">ArthurZ</span></a></span>\n\n\t</span></span> Thank you for\
          \ your help and the solution is working great for me.</p>\n"
        raw: '@ArthurZ Thank you for your help and the solution is working great for
          me.'
        updatedAt: '2023-12-13T12:40:51.947Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ArthurZ
    id: 6579a653a1ecf0c79c03879b
    type: comment
  author: Boyue27
  content: '@ArthurZ Thank you for your help and the solution is working great for
    me.'
  created_at: 2023-12-13 12:40:51+00:00
  edited: false
  hidden: false
  id: 6579a653a1ecf0c79c03879b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: mistralai/Mixtral-8x7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Inquiry about Generation Speed
