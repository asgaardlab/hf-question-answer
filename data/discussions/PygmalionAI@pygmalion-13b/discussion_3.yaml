!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KSlith
conflicting_files: null
created_at: 2023-06-01 03:55:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676370430241-noauth.png?w=200&h=200&f=face
      fullname: Kira Slith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KSlith
      type: user
    createdAt: '2023-06-01T04:55:03.000Z'
    data:
      edited: true
      editors:
      - KSlith
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676370430241-noauth.png?w=200&h=200&f=face
          fullname: Kira Slith
          isHf: false
          isPro: false
          name: KSlith
          type: user
        html: '<p>Is it possible the SHA256 hashes for both the vanilla LLaMA-13B
          and the PyTorch conversion could also be added to the description for troubleshooting
          purposes?</p>

          <p>After doing the xor decode, I''ve gone through and tested the hashes
          I do have for both the xor_encoded_files and the original PTH files from
          Meta, and they all match, but the SHA256 for "pytorch_model-00003-of-00003.bin"
          at the VERY end did not match those in the description. Problem is, I have
          no clue if the issue is with xor_codec.py, the output from convert_llama_weights_to_hf.py,
          or something else.</p>

          <p>Official LLaMA 13B PTH SHA256: (Add these to the description please,
          Credit Lowqualitybot on Github for the Hashes, and dylancvdean on Github
          for confirming the hashes)<br>4ab77bec4d4405ccb66a97b282574c89a94417e3c32e5f68f37e2876fc21322f
          ./13B/params.json<br>745bf4e29a4dd6f411e72976d92b452da1b49168a4f41c951cfcc8051823cf08
          ./13B/consolidated.00.pth<br>d5ccbcc465c71c0de439a5aeffebe8344c68a519bce70bc7f9f92654ee567085
          ./13B/consolidated.01.pth<br>183eb00cea5c880fd88c296af1038f4c15dc26aa2ccb7c6cf2c35b9bb00dce45
          ./13B/checklist.chk</p>

          <p>SHA256 for my results from convert_llama_weights_to_hf.py: (Edit: Reduced
          down to the confirmed miss-match):<br>2efc56cddb7877c830bc5b402ee72996aedb5694c9e8007bf1d52d72c0d97d26  LLaMA_13B_PyTorch/pytorch_model-00003-of-00003.bin<br>SHA256
          for the known bad result from xor_codec.py:<br>0c62600c4ea615684f82551c06bd4f9953195aa4b188872498728693f158286b  Output_pygmalion-13b/pytorch_model-00003-of-00003.bin  &lt;--
          BAD HASH?</p>

          <p>All other hashes for the expected result from xor_codec.py match the
          expected result given in the model card.</p>

          <p>Edit: Truncated unrelated hashes for thread readability, clarified some
          details.</p>

          '
        raw: 'Is it possible the SHA256 hashes for both the vanilla LLaMA-13B and
          the PyTorch conversion could also be added to the description for troubleshooting
          purposes?


          After doing the xor decode, I''ve gone through and tested the hashes I do
          have for both the xor_encoded_files and the original PTH files from Meta,
          and they all match, but the SHA256 for "pytorch_model-00003-of-00003.bin"
          at the VERY end did not match those in the description. Problem is, I have
          no clue if the issue is with xor_codec.py, the output from convert_llama_weights_to_hf.py,
          or something else.


          Official LLaMA 13B PTH SHA256: (Add these to the description please, Credit
          Lowqualitybot on Github for the Hashes, and dylancvdean on Github for confirming
          the hashes)

          4ab77bec4d4405ccb66a97b282574c89a94417e3c32e5f68f37e2876fc21322f ./13B/params.json

          745bf4e29a4dd6f411e72976d92b452da1b49168a4f41c951cfcc8051823cf08 ./13B/consolidated.00.pth

          d5ccbcc465c71c0de439a5aeffebe8344c68a519bce70bc7f9f92654ee567085 ./13B/consolidated.01.pth

          183eb00cea5c880fd88c296af1038f4c15dc26aa2ccb7c6cf2c35b9bb00dce45 ./13B/checklist.chk



          SHA256 for my results from convert_llama_weights_to_hf.py: (Edit: Reduced
          down to the confirmed miss-match):

          2efc56cddb7877c830bc5b402ee72996aedb5694c9e8007bf1d52d72c0d97d26  LLaMA_13B_PyTorch/pytorch_model-00003-of-00003.bin

          SHA256 for the known bad result from xor_codec.py:

          0c62600c4ea615684f82551c06bd4f9953195aa4b188872498728693f158286b  Output_pygmalion-13b/pytorch_model-00003-of-00003.bin  <--
          BAD HASH?


          All other hashes for the expected result from xor_codec.py match the expected
          result given in the model card.


          Edit: Truncated unrelated hashes for thread readability, clarified some
          details.'
        updatedAt: '2023-06-09T22:08:52.962Z'
      numEdits: 2
      reactions: []
    id: 647824a74ae93470ffc6df79
    type: comment
  author: KSlith
  content: 'Is it possible the SHA256 hashes for both the vanilla LLaMA-13B and the
    PyTorch conversion could also be added to the description for troubleshooting
    purposes?


    After doing the xor decode, I''ve gone through and tested the hashes I do have
    for both the xor_encoded_files and the original PTH files from Meta, and they
    all match, but the SHA256 for "pytorch_model-00003-of-00003.bin" at the VERY end
    did not match those in the description. Problem is, I have no clue if the issue
    is with xor_codec.py, the output from convert_llama_weights_to_hf.py, or something
    else.


    Official LLaMA 13B PTH SHA256: (Add these to the description please, Credit Lowqualitybot
    on Github for the Hashes, and dylancvdean on Github for confirming the hashes)

    4ab77bec4d4405ccb66a97b282574c89a94417e3c32e5f68f37e2876fc21322f ./13B/params.json

    745bf4e29a4dd6f411e72976d92b452da1b49168a4f41c951cfcc8051823cf08 ./13B/consolidated.00.pth

    d5ccbcc465c71c0de439a5aeffebe8344c68a519bce70bc7f9f92654ee567085 ./13B/consolidated.01.pth

    183eb00cea5c880fd88c296af1038f4c15dc26aa2ccb7c6cf2c35b9bb00dce45 ./13B/checklist.chk



    SHA256 for my results from convert_llama_weights_to_hf.py: (Edit: Reduced down
    to the confirmed miss-match):

    2efc56cddb7877c830bc5b402ee72996aedb5694c9e8007bf1d52d72c0d97d26  LLaMA_13B_PyTorch/pytorch_model-00003-of-00003.bin

    SHA256 for the known bad result from xor_codec.py:

    0c62600c4ea615684f82551c06bd4f9953195aa4b188872498728693f158286b  Output_pygmalion-13b/pytorch_model-00003-of-00003.bin  <--
    BAD HASH?


    All other hashes for the expected result from xor_codec.py match the expected
    result given in the model card.


    Edit: Truncated unrelated hashes for thread readability, clarified some details.'
  created_at: 2023-06-01 03:55:03+00:00
  edited: true
  hidden: false
  id: 647824a74ae93470ffc6df79
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
      fullname: Alpin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: alpindale
      type: user
    createdAt: '2023-06-01T09:09:39.000Z'
    data:
      edited: false
      editors:
      - alpindale
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
          fullname: Alpin
          isHf: false
          isPro: false
          name: alpindale
          type: user
        html: '<p>Looks like your <code>pth</code> -&gt; <code>hf</code> conversion
          is the issue. Your third checkpoint doesn''t match the expected hash:</p>

          <pre><code class="language-sh">$ <span class="hljs-built_in">sha256sum</span>
          llama-13b-hf/pytorch_model-00003-of-00003.bin

          eac58861ca5f3749c819676d908b906d3df38046c09efe055fe78d7678b718e7  pytorch_model-00003-of-00003.bin

          </code></pre>

          <p>You can look <a rel="nofollow" href="https://docs.alpindale.dev/pygmalion-7b/#13b-file-hashes">here</a>
          for a list of all the hashes.</p>

          '
        raw: 'Looks like your `pth` -> `hf` conversion is the issue. Your third checkpoint
          doesn''t match the expected hash:


          ```sh

          $ sha256sum llama-13b-hf/pytorch_model-00003-of-00003.bin

          eac58861ca5f3749c819676d908b906d3df38046c09efe055fe78d7678b718e7  pytorch_model-00003-of-00003.bin

          ```


          You can look [here](https://docs.alpindale.dev/pygmalion-7b/#13b-file-hashes)
          for a list of all the hashes.'
        updatedAt: '2023-06-01T09:09:39.859Z'
      numEdits: 0
      reactions: []
    id: 647860531f9756aa89ccf85e
    type: comment
  author: alpindale
  content: 'Looks like your `pth` -> `hf` conversion is the issue. Your third checkpoint
    doesn''t match the expected hash:


    ```sh

    $ sha256sum llama-13b-hf/pytorch_model-00003-of-00003.bin

    eac58861ca5f3749c819676d908b906d3df38046c09efe055fe78d7678b718e7  pytorch_model-00003-of-00003.bin

    ```


    You can look [here](https://docs.alpindale.dev/pygmalion-7b/#13b-file-hashes)
    for a list of all the hashes.'
  created_at: 2023-06-01 08:09:39+00:00
  edited: false
  hidden: false
  id: 647860531f9756aa89ccf85e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676370430241-noauth.png?w=200&h=200&f=face
      fullname: Kira Slith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KSlith
      type: user
    createdAt: '2023-06-01T19:17:01.000Z'
    data:
      edited: true
      editors:
      - KSlith
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676370430241-noauth.png?w=200&h=200&f=face
          fullname: Kira Slith
          isHf: false
          isPro: false
          name: KSlith
          type: user
        html: '<p>Edit: -SNIP- The hashes on that site are severely out of date compared
          to the model description.</p>

          '
        raw: 'Edit: -SNIP- The hashes on that site are severely out of date compared
          to the model description.'
        updatedAt: '2023-06-10T03:55:46.369Z'
      numEdits: 6
      reactions: []
    id: 6478eead25e06d2ffe8bf57c
    type: comment
  author: KSlith
  content: 'Edit: -SNIP- The hashes on that site are severely out of date compared
    to the model description.'
  created_at: 2023-06-01 18:17:01+00:00
  edited: true
  hidden: false
  id: 6478eead25e06d2ffe8bf57c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676370430241-noauth.png?w=200&h=200&f=face
      fullname: Kira Slith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KSlith
      type: user
    createdAt: '2023-06-10T05:05:16.000Z'
    data:
      edited: false
      editors:
      - KSlith
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9446672797203064
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676370430241-noauth.png?w=200&h=200&f=face
          fullname: Kira Slith
          isHf: false
          isPro: false
          name: KSlith
          type: user
        html: "<p>After messing with the versions of all the python dependencies,\
          \ including switching up transformer versions, I got the same hashes again.\
          \ I checked my PTH hashes against a few others' just in case and they've\
          \ matched with 5 others' now. I made a pass using WSL2 Ubuntu and the instructions\
          \ mentioned in the docs linked and got the exact same output there too.\
          \ The XOR's hashes match the GIT LFS hashes on the HF repository too, and\
          \ there's exactly 312mb missing off the final result vs the PTH -&gt; HF\
          \ conversion.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;alpindale&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/alpindale\"\
          >@<span class=\"underline\">alpindale</span></a></span>\n\n\t</span></span>\
          \ have you tried following the instructions from a fresh install to see\
          \ if it's on your guys' end? the version on the git is 312mb smaller than\
          \ the regular LLaMA model.</p>\n"
        raw: 'After messing with the versions of all the python dependencies, including
          switching up transformer versions, I got the same hashes again. I checked
          my PTH hashes against a few others'' just in case and they''ve matched with
          5 others'' now. I made a pass using WSL2 Ubuntu and the instructions mentioned
          in the docs linked and got the exact same output there too. The XOR''s hashes
          match the GIT LFS hashes on the HF repository too, and there''s exactly
          312mb missing off the final result vs the PTH -> HF conversion.


          @alpindale have you tried following the instructions from a fresh install
          to see if it''s on your guys'' end? the version on the git is 312mb smaller
          than the regular LLaMA model.'
        updatedAt: '2023-06-10T05:05:16.570Z'
      numEdits: 0
      reactions: []
    id: 6484048c64866a4f6cc5a109
    type: comment
  author: KSlith
  content: 'After messing with the versions of all the python dependencies, including
    switching up transformer versions, I got the same hashes again. I checked my PTH
    hashes against a few others'' just in case and they''ve matched with 5 others''
    now. I made a pass using WSL2 Ubuntu and the instructions mentioned in the docs
    linked and got the exact same output there too. The XOR''s hashes match the GIT
    LFS hashes on the HF repository too, and there''s exactly 312mb missing off the
    final result vs the PTH -> HF conversion.


    @alpindale have you tried following the instructions from a fresh install to see
    if it''s on your guys'' end? the version on the git is 312mb smaller than the
    regular LLaMA model.'
  created_at: 2023-06-10 04:05:16+00:00
  edited: false
  hidden: false
  id: 6484048c64866a4f6cc5a109
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661656026444-630ad0d0b8d7b38893236010.png?w=200&h=200&f=face
      fullname: Bru
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: P4l1ndr0m
      type: user
    createdAt: '2023-06-11T14:44:00.000Z'
    data:
      edited: false
      editors:
      - P4l1ndr0m
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8721476197242737
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661656026444-630ad0d0b8d7b38893236010.png?w=200&h=200&f=face
          fullname: Bru
          isHf: false
          isPro: false
          name: P4l1ndr0m
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;alpindale&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/alpindale\">@<span class=\"\
          underline\">alpindale</span></a></span>\n\n\t</span></span> having exactly\
          \ the same issue as KSlith, last pytorch_model-00003-of-00003.bin has a\
          \ different hash than the one displayed in the model card.</p>\n"
        raw: '@alpindale having exactly the same issue as KSlith, last pytorch_model-00003-of-00003.bin
          has a different hash than the one displayed in the model card.'
        updatedAt: '2023-06-11T14:44:00.975Z'
      numEdits: 0
      reactions: []
    id: 6485ddb0a5d53d7ed85c8685
    type: comment
  author: P4l1ndr0m
  content: '@alpindale having exactly the same issue as KSlith, last pytorch_model-00003-of-00003.bin
    has a different hash than the one displayed in the model card.'
  created_at: 2023-06-11 13:44:00+00:00
  edited: false
  hidden: false
  id: 6485ddb0a5d53d7ed85c8685
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/88c816f582c158e4a9d00a25108c5be4.svg
      fullname: M Lu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xluke
      type: user
    createdAt: '2023-07-03T06:23:16.000Z'
    data:
      edited: false
      editors:
      - xluke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8129105567932129
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/88c816f582c158e4a9d00a25108c5be4.svg
          fullname: M Lu
          isHf: false
          isPro: false
          name: xluke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;alpindale&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/alpindale\">@<span class=\"\
          underline\">alpindale</span></a></span>\n\n\t</span></span> having exactly\
          \ the same issue as KSlith, last pytorch_model-00003-of-00003.bin has a\
          \ different hash than the one displayed in the model card.<br>And I can\
          \ not load this model in oobabooga/text-generation-webui, some error occurs!</p>\n"
        raw: '@alpindale having exactly the same issue as KSlith, last pytorch_model-00003-of-00003.bin
          has a different hash than the one displayed in the model card.

          And I can not load this model in oobabooga/text-generation-webui, some error
          occurs!'
        updatedAt: '2023-07-03T06:23:16.486Z'
      numEdits: 0
      reactions: []
    id: 64a26954d3149e05bc7485dc
    type: comment
  author: xluke
  content: '@alpindale having exactly the same issue as KSlith, last pytorch_model-00003-of-00003.bin
    has a different hash than the one displayed in the model card.

    And I can not load this model in oobabooga/text-generation-webui, some error occurs!'
  created_at: 2023-07-03 05:23:16+00:00
  edited: false
  hidden: false
  id: 64a26954d3149e05bc7485dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
      fullname: Alpin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: alpindale
      type: user
    createdAt: '2023-07-03T13:20:15.000Z'
    data:
      edited: false
      editors:
      - alpindale
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9846873879432678
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
          fullname: Alpin
          isHf: false
          isPro: false
          name: alpindale
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;KSlith&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/KSlith\">@<span class=\"\
          underline\">KSlith</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;P4l1ndr0m&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/P4l1ndr0m\">@<span class=\"underline\">P4l1ndr0m</span></a></span>\n\
          \n\t</span></span> <span data-props=\"{&quot;user&quot;:&quot;xluke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/xluke\"\
          >@<span class=\"underline\">xluke</span></a></span>\n\n\t</span></span>\
          \ I've been trying to reproduce it myself but none of the Transformers commits\
          \ seem to work unfortunately. </p>\n<p>Some people have already uploaded\
          \ the merged Pygmalion 13B model, so you could look it up on huggingface.</p>\n"
        raw: "@KSlith @P4l1ndr0m @xluke I've been trying to reproduce it myself but\
          \ none of the Transformers commits seem to work unfortunately. \n\nSome\
          \ people have already uploaded the merged Pygmalion 13B model, so you could\
          \ look it up on huggingface."
        updatedAt: '2023-07-03T13:20:15.717Z'
      numEdits: 0
      reactions: []
    id: 64a2cb0fe118bb5ba4e796de
    type: comment
  author: alpindale
  content: "@KSlith @P4l1ndr0m @xluke I've been trying to reproduce it myself but\
    \ none of the Transformers commits seem to work unfortunately. \n\nSome people\
    \ have already uploaded the merged Pygmalion 13B model, so you could look it up\
    \ on huggingface."
  created_at: 2023-07-03 12:20:15+00:00
  edited: false
  hidden: false
  id: 64a2cb0fe118bb5ba4e796de
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: PygmalionAI/pygmalion-13b
repo_type: model
status: open
target_branch: null
title: Add SHA256 for LLaMA itself and the PyTorch conversion?
