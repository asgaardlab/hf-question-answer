!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RaviNaik
conflicting_files: null
created_at: 2023-10-18 17:05:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6405eb79d6843690270bae40/gGrixNBsesMs13qen7GwJ.jpeg?w=200&h=200&f=face
      fullname: Ravi Naik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RaviNaik
      type: user
    createdAt: '2023-10-18T18:05:35.000Z'
    data:
      edited: false
      editors:
      - RaviNaik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5440225005149841
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6405eb79d6843690270bae40/gGrixNBsesMs13qen7GwJ.jpeg?w=200&h=200&f=face
          fullname: Ravi Naik
          isHf: false
          isPro: false
          name: RaviNaik
          type: user
        html: "<p>Getting the below error when running the pipeline code of  <code>TheBloke/Lemur-70B-Chat-v1-AWQ</code>.\
          \ Could you please help?</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-comment\"># Inference can also be done using transformers'\
          \ pipeline</span>\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> pipeline\n\n<span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"\
          </span>)\npipe = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"\
          </span>,\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>,\n    do_sample=<span class=\"hljs-literal\"\
          >True</span>,\n    temperature=<span class=\"hljs-number\">0.7</span>,\n\
          \    top_p=<span class=\"hljs-number\">0.95</span>,\n    top_k=<span class=\"\
          hljs-number\">40</span>,\n    repetition_penalty=<span class=\"hljs-number\"\
          >1.1</span>,\n)\n\n<span class=\"hljs-built_in\">print</span>(pipe(prompt_template)[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">\"generated_text\"\
          </span>])\n<span class=\"hljs-comment\">### Error</span>\nAttributeError:\
          \ <span class=\"hljs-string\">'LlamaAWQForCausalLM'</span> <span class=\"\
          hljs-built_in\">object</span> has no attribute <span class=\"hljs-string\"\
          >'config'</span>\n</code></pre>\n<p>Thanks</p>\n"
        raw: "Getting the below error when running the pipeline code of  `TheBloke/Lemur-70B-Chat-v1-AWQ`.\
          \ Could you please help?\r\n```python\r\n# Inference can also be done using\
          \ transformers' pipeline\r\nfrom transformers import pipeline\r\n\r\nprint(\"\
          *** Pipeline:\")\r\npipe = pipeline(\r\n    \"text-generation\",\r\n   \
          \ model=model,\r\n    tokenizer=tokenizer,\r\n    max_new_tokens=512,\r\n\
          \    do_sample=True,\r\n    temperature=0.7,\r\n    top_p=0.95,\r\n    top_k=40,\r\
          \n    repetition_penalty=1.1,\r\n)\r\n\r\nprint(pipe(prompt_template)[0][\"\
          generated_text\"])\r\n### Error\r\nAttributeError: 'LlamaAWQForCausalLM'\
          \ object has no attribute 'config'\r\n```\r\nThanks"
        updatedAt: '2023-10-18T18:05:35.694Z'
      numEdits: 0
      reactions: []
    id: 65301e6f3976e5f44112b027
    type: comment
  author: RaviNaik
  content: "Getting the below error when running the pipeline code of  `TheBloke/Lemur-70B-Chat-v1-AWQ`.\
    \ Could you please help?\r\n```python\r\n# Inference can also be done using transformers'\
    \ pipeline\r\nfrom transformers import pipeline\r\n\r\nprint(\"*** Pipeline:\"\
    )\r\npipe = pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
    \n    max_new_tokens=512,\r\n    do_sample=True,\r\n    temperature=0.7,\r\n \
    \   top_p=0.95,\r\n    top_k=40,\r\n    repetition_penalty=1.1,\r\n)\r\n\r\nprint(pipe(prompt_template)[0][\"\
    generated_text\"])\r\n### Error\r\nAttributeError: 'LlamaAWQForCausalLM' object\
    \ has no attribute 'config'\r\n```\r\nThanks"
  created_at: 2023-10-18 17:05:35+00:00
  edited: false
  hidden: false
  id: 65301e6f3976e5f44112b027
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6405eb79d6843690270bae40/gGrixNBsesMs13qen7GwJ.jpeg?w=200&h=200&f=face
      fullname: Ravi Naik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RaviNaik
      type: user
    createdAt: '2023-10-28T07:46:34.000Z'
    data:
      edited: false
      editors:
      - RaviNaik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7913712859153748
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6405eb79d6843690270bae40/gGrixNBsesMs13qen7GwJ.jpeg?w=200&h=200&f=face
          fullname: Ravi Naik
          isHf: false
          isPro: false
          name: RaviNaik
          type: user
        html: '<p>Resolution is to use <code>model=model.model</code> during pipeline
          construction.<br>Refer <a rel="nofollow" href="https://github.com/casper-hansen/AutoAWQ/issues/107">https://github.com/casper-hansen/AutoAWQ/issues/107</a></p>

          '
        raw: "Resolution is to use `model=model.model` during pipeline construction.\
          \ \nRefer https://github.com/casper-hansen/AutoAWQ/issues/107"
        updatedAt: '2023-10-28T07:46:34.293Z'
      numEdits: 0
      reactions: []
    id: 653cbc5a1170823ca95c005f
    type: comment
  author: RaviNaik
  content: "Resolution is to use `model=model.model` during pipeline construction.\
    \ \nRefer https://github.com/casper-hansen/AutoAWQ/issues/107"
  created_at: 2023-10-28 06:46:34+00:00
  edited: false
  hidden: false
  id: 653cbc5a1170823ca95c005f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6405eb79d6843690270bae40/gGrixNBsesMs13qen7GwJ.jpeg?w=200&h=200&f=face
      fullname: Ravi Naik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RaviNaik
      type: user
    createdAt: '2023-10-30T06:40:49.000Z'
    data:
      status: closed
    id: 653f4ff181277ed9685001b7
    type: status-change
  author: RaviNaik
  created_at: 2023-10-30 05:40:49+00:00
  id: 653f4ff181277ed9685001b7
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Lemur-70B-Chat-v1-AWQ
repo_type: model
status: closed
target_branch: null
title: Facing issue with pipeline code
