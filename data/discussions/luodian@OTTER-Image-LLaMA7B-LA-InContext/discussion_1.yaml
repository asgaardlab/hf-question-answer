!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-05-07 08:36:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-07T09:36:09.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>HI, I am highly looking forward to trying out flamingo and otter,
          but I don''t have " at least 33G GPU memory", so would you like to roll
          out a quantized version of both models?</p>

          <p>Also, I am a user of textgen-webui which is popular, would you like to
          promote flamingo and otter to that app?</p>

          <p>Thanks!</p>

          '
        raw: "HI, I am highly looking forward to trying out flamingo and otter, but\
          \ I don't have \" at least 33G GPU memory\", so would you like to roll out\
          \ a quantized version of both models?\r\n\r\nAlso, I am a user of textgen-webui\
          \ which is popular, would you like to promote flamingo and otter to that\
          \ app?\r\n\r\nThanks!"
        updatedAt: '2023-05-07T09:36:09.980Z'
      numEdits: 0
      reactions: []
    id: 64577109cd935d48a477e706
    type: comment
  author: Yhyu13
  content: "HI, I am highly looking forward to trying out flamingo and otter, but\
    \ I don't have \" at least 33G GPU memory\", so would you like to roll out a quantized\
    \ version of both models?\r\n\r\nAlso, I am a user of textgen-webui which is popular,\
    \ would you like to promote flamingo and otter to that app?\r\n\r\nThanks!"
  created_at: 2023-05-07 08:36:09+00:00
  edited: false
  hidden: false
  id: 64577109cd935d48a477e706
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/IVZXUPBSzhS01hm55x854.png?w=200&h=200&f=face
      fullname: Bo Li
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: luodian
      type: user
    createdAt: '2023-05-07T14:13:02.000Z'
    data:
      edited: false
      editors:
      - luodian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/IVZXUPBSzhS01hm55x854.png?w=200&h=200&f=face
          fullname: Bo Li
          isHf: false
          isPro: false
          name: luodian
          type: user
        html: '<p>If you are using huggingface version openflamingo/otter, you do
          not need <code>33GB</code>, you can start with 2x 3090 or 4x 3090, each
          with 24GB GPU.</p>

          <p>We are still working on supporting 8bit quantiztion but current bitsandbytes
          package has an issue that it only supports tensor shape [batch_size, x,
          y, z], but flamingo has tensor of [batch_size, media_frames, x, y, z]. This
          needs to be fixed by bnb package.</p>

          <p>we are considering using a customized bnb package or send a PR to official
          bnb repo.</p>

          '
        raw: 'If you are using huggingface version openflamingo/otter, you do not
          need `33GB`, you can start with 2x 3090 or 4x 3090, each with 24GB GPU.


          We are still working on supporting 8bit quantiztion but current bitsandbytes
          package has an issue that it only supports tensor shape [batch_size, x,
          y, z], but flamingo has tensor of [batch_size, media_frames, x, y, z]. This
          needs to be fixed by bnb package.


          we are considering using a customized bnb package or send a PR to official
          bnb repo.'
        updatedAt: '2023-05-07T14:13:02.902Z'
      numEdits: 0
      reactions: []
    id: 6457b1ee711ee86f6eeff4d3
    type: comment
  author: luodian
  content: 'If you are using huggingface version openflamingo/otter, you do not need
    `33GB`, you can start with 2x 3090 or 4x 3090, each with 24GB GPU.


    We are still working on supporting 8bit quantiztion but current bitsandbytes package
    has an issue that it only supports tensor shape [batch_size, x, y, z], but flamingo
    has tensor of [batch_size, media_frames, x, y, z]. This needs to be fixed by bnb
    package.


    we are considering using a customized bnb package or send a PR to official bnb
    repo.'
  created_at: 2023-05-07 13:13:02+00:00
  edited: false
  hidden: false
  id: 6457b1ee711ee86f6eeff4d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8e3b018051f89a227f415d1dcf7cf99c.svg
      fullname: Enju
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Naugustogi
      type: user
    createdAt: '2023-06-10T20:57:08.000Z'
    data:
      edited: false
      editors:
      - Naugustogi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9399504661560059
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8e3b018051f89a227f415d1dcf7cf99c.svg
          fullname: Enju
          isHf: false
          isPro: false
          name: Naugustogi
          type: user
        html: '<p>i think people will int 4 quantize it anyway, or AWQ, Qk_2 technics,
          for 9b running on cpu inference, this shouldn''t take more than 8 or 9gb
          ram</p>

          '
        raw: i think people will int 4 quantize it anyway, or AWQ, Qk_2 technics,
          for 9b running on cpu inference, this shouldn't take more than 8 or 9gb
          ram
        updatedAt: '2023-06-10T20:57:08.424Z'
      numEdits: 0
      reactions: []
    id: 6484e3a45ff9a3d2ee5a0df1
    type: comment
  author: Naugustogi
  content: i think people will int 4 quantize it anyway, or AWQ, Qk_2 technics, for
    9b running on cpu inference, this shouldn't take more than 8 or 9gb ram
  created_at: 2023-06-10 19:57:08+00:00
  edited: false
  hidden: false
  id: 6484e3a45ff9a3d2ee5a0df1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: luodian/OTTER-Image-LLaMA7B-LA-InContext
repo_type: model
status: open
target_branch: null
title: Would you like to promote otter to textgen webui or release a version with
  GPTQ or ggml quntization?
