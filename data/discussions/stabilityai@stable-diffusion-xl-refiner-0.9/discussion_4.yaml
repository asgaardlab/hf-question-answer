!!python/object:huggingface_hub.community.DiscussionWithDetails
author: supwang
conflicting_files: null
created_at: 2023-07-08 15:19:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/smyJFajPaoCXyax4jU3qT.png?w=200&h=200&f=face
      fullname: WANG YUN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: supwang
      type: user
    createdAt: '2023-07-08T16:19:16.000Z'
    data:
      edited: false
      editors:
      - supwang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8236165046691895
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/smyJFajPaoCXyax4jU3qT.png?w=200&h=200&f=face
          fullname: WANG YUN
          isHf: false
          isPro: false
          name: supwang
          type: user
        html: '<p>Hi,<br>Model Version: SD-XL base, 8sec per image :)<br>Model Version:
          SD-XL Refiner, 15mins per image @_@</p>

          <p>Is this a normal situation?<br>If I switched models, why the image generation
          speed of SD-XL base will also change to 15mins per image!?<br>Thank you.</p>

          '
        raw: "Hi,\r\nModel Version: SD-XL base, 8sec per image :)\r\nModel Version:\
          \ SD-XL Refiner, 15mins per image @_@\r\n\r\nIs this a normal situation?\r\
          \nIf I switched models, why the image generation speed of SD-XL base will\
          \ also change to 15mins per image!?\r\nThank you."
        updatedAt: '2023-07-08T16:19:16.713Z'
      numEdits: 0
      reactions: []
    id: 64a98c846324705e6a36fa06
    type: comment
  author: supwang
  content: "Hi,\r\nModel Version: SD-XL base, 8sec per image :)\r\nModel Version:\
    \ SD-XL Refiner, 15mins per image @_@\r\n\r\nIs this a normal situation?\r\nIf\
    \ I switched models, why the image generation speed of SD-XL base will also change\
    \ to 15mins per image!?\r\nThank you."
  created_at: 2023-07-08 15:19:16+00:00
  edited: false
  hidden: false
  id: 64a98c846324705e6a36fa06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14f8c7e8fd146cbeb5aee874030c54a6.svg
      fullname: Samsara
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Irkson0101
      type: user
    createdAt: '2023-07-09T06:35:38.000Z'
    data:
      edited: true
      editors:
      - Irkson0101
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9497012495994568
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14f8c7e8fd146cbeb5aee874030c54a6.svg
          fullname: Samsara
          isHf: false
          isPro: false
          name: Irkson0101
          type: user
        html: '<p>You don''t have enough VRAM. when I load on batch size 2 it uses
          20GB VRAM ^^ my GPU can handle it but you may not have enough. this is a
          big 6.6b parameter model. you might be able to do it if you offload initial
          model first.</p>

          '
        raw: You don't have enough VRAM. when I load on batch size 2 it uses 20GB
          VRAM ^^ my GPU can handle it but you may not have enough. this is a big
          6.6b parameter model. you might be able to do it if you offload initial
          model first.
        updatedAt: '2023-07-09T06:36:03.692Z'
      numEdits: 1
      reactions: []
    id: 64aa553ab6512b8328250e15
    type: comment
  author: Irkson0101
  content: You don't have enough VRAM. when I load on batch size 2 it uses 20GB VRAM
    ^^ my GPU can handle it but you may not have enough. this is a big 6.6b parameter
    model. you might be able to do it if you offload initial model first.
  created_at: 2023-07-09 05:35:38+00:00
  edited: true
  hidden: false
  id: 64aa553ab6512b8328250e15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/smyJFajPaoCXyax4jU3qT.png?w=200&h=200&f=face
      fullname: WANG YUN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: supwang
      type: user
    createdAt: '2023-07-09T11:21:26.000Z'
    data:
      edited: false
      editors:
      - supwang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8895661234855652
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/smyJFajPaoCXyax4jU3qT.png?w=200&h=200&f=face
          fullname: WANG YUN
          isHf: false
          isPro: false
          name: supwang
          type: user
        html: '<blockquote>

          <p>You don''t have enough VRAM. when I load on batch size 2 it uses 20GB
          VRAM ^^ my GPU can handle it but you may not have enough. this is a big
          6.6b parameter model. you might be able to do it if you offload initial
          model first.</p>

          </blockquote>

          <p>Thank you. But I only load batch size 1 and I''m using 4090. Speed of
          refiner is too slow.</p>

          '
        raw: '> You don''t have enough VRAM. when I load on batch size 2 it uses 20GB
          VRAM ^^ my GPU can handle it but you may not have enough. this is a big
          6.6b parameter model. you might be able to do it if you offload initial
          model first.


          Thank you. But I only load batch size 1 and I''m using 4090. Speed of refiner
          is too slow.'
        updatedAt: '2023-07-09T11:21:26.675Z'
      numEdits: 0
      reactions: []
    id: 64aa98366e727d56aeb559b0
    type: comment
  author: supwang
  content: '> You don''t have enough VRAM. when I load on batch size 2 it uses 20GB
    VRAM ^^ my GPU can handle it but you may not have enough. this is a big 6.6b parameter
    model. you might be able to do it if you offload initial model first.


    Thank you. But I only load batch size 1 and I''m using 4090. Speed of refiner
    is too slow.'
  created_at: 2023-07-09 10:21:26+00:00
  edited: false
  hidden: false
  id: 64aa98366e727d56aeb559b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14f8c7e8fd146cbeb5aee874030c54a6.svg
      fullname: Samsara
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Irkson0101
      type: user
    createdAt: '2023-07-09T16:59:09.000Z'
    data:
      edited: false
      editors:
      - Irkson0101
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8899622559547424
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14f8c7e8fd146cbeb5aee874030c54a6.svg
          fullname: Samsara
          isHf: false
          isPro: false
          name: Irkson0101
          type: user
        html: '<blockquote>

          <blockquote>

          <p>You don''t have enough VRAM. when I load on batch size 2 it uses 20GB
          VRAM ^^ my GPU can handle it but you may not have enough. this is a big
          6.6b parameter model. you might be able to do it if you offload initial
          model first.</p>

          </blockquote>

          <p>Thank you. But I only load batch size 1 and I''m using 4090. Speed of
          refiner is too slow.</p>

          </blockquote>

          <p>That''s not normal, on my 3090 refiner takes no longer than the base
          model. Do you have other programs open consuming VRAM?</p>

          '
        raw: "> > You don't have enough VRAM. when I load on batch size 2 it uses\
          \ 20GB VRAM ^^ my GPU can handle it but you may not have enough. this is\
          \ a big 6.6b parameter model. you might be able to do it if you offload\
          \ initial model first.\n> \n> Thank you. But I only load batch size 1 and\
          \ I'm using 4090. Speed of refiner is too slow.\n\nThat's not normal, on\
          \ my 3090 refiner takes no longer than the base model. Do you have other\
          \ programs open consuming VRAM?"
        updatedAt: '2023-07-09T16:59:09.985Z'
      numEdits: 0
      reactions: []
    id: 64aae75dd4a402e8dce4f9db
    type: comment
  author: Irkson0101
  content: "> > You don't have enough VRAM. when I load on batch size 2 it uses 20GB\
    \ VRAM ^^ my GPU can handle it but you may not have enough. this is a big 6.6b\
    \ parameter model. you might be able to do it if you offload initial model first.\n\
    > \n> Thank you. But I only load batch size 1 and I'm using 4090. Speed of refiner\
    \ is too slow.\n\nThat's not normal, on my 3090 refiner takes no longer than the\
    \ base model. Do you have other programs open consuming VRAM?"
  created_at: 2023-07-09 15:59:09+00:00
  edited: false
  hidden: false
  id: 64aae75dd4a402e8dce4f9db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/smyJFajPaoCXyax4jU3qT.png?w=200&h=200&f=face
      fullname: WANG YUN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: supwang
      type: user
    createdAt: '2023-07-10T01:15:46.000Z'
    data:
      edited: false
      editors:
      - supwang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8792261481285095
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/smyJFajPaoCXyax4jU3qT.png?w=200&h=200&f=face
          fullname: WANG YUN
          isHf: false
          isPro: false
          name: supwang
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>You don''t have enough VRAM. when I load on batch size 2 it uses 20GB
          VRAM ^^ my GPU can handle it but you may not have enough. this is a big
          6.6b parameter model. you might be able to do it if you offload initial
          model first.</p>

          </blockquote>

          <p>Thank you. But I only load batch size 1 and I''m using 4090. Speed of
          refiner is too slow.</p>

          </blockquote>

          <p>That''s not normal, on my 3090 refiner takes no longer than the base
          model. Do you have other programs open consuming VRAM?</p>

          </blockquote>

          <p>Nothing consuming VRAM, except SDXL. So it''s strange. @_@</p>

          '
        raw: "> > > You don't have enough VRAM. when I load on batch size 2 it uses\
          \ 20GB VRAM ^^ my GPU can handle it but you may not have enough. this is\
          \ a big 6.6b parameter model. you might be able to do it if you offload\
          \ initial model first.\n> > \n> > Thank you. But I only load batch size\
          \ 1 and I'm using 4090. Speed of refiner is too slow.\n> \n> That's not\
          \ normal, on my 3090 refiner takes no longer than the base model. Do you\
          \ have other programs open consuming VRAM?\n\nNothing consuming VRAM, except\
          \ SDXL. So it's strange. @_@"
        updatedAt: '2023-07-10T01:15:46.639Z'
      numEdits: 0
      reactions: []
    id: 64ab5bc26cadc7aca57995a5
    type: comment
  author: supwang
  content: "> > > You don't have enough VRAM. when I load on batch size 2 it uses\
    \ 20GB VRAM ^^ my GPU can handle it but you may not have enough. this is a big\
    \ 6.6b parameter model. you might be able to do it if you offload initial model\
    \ first.\n> > \n> > Thank you. But I only load batch size 1 and I'm using 4090.\
    \ Speed of refiner is too slow.\n> \n> That's not normal, on my 3090 refiner takes\
    \ no longer than the base model. Do you have other programs open consuming VRAM?\n\
    \nNothing consuming VRAM, except SDXL. So it's strange. @_@"
  created_at: 2023-07-10 00:15:46+00:00
  edited: false
  hidden: false
  id: 64ab5bc26cadc7aca57995a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1fa21dd09f97e9d31bbc7fd36c84da4d.svg
      fullname: zuozuo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zuozuo
      type: user
    createdAt: '2023-07-10T12:47:28.000Z'
    data:
      edited: false
      editors:
      - zuozuo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8451034426689148
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1fa21dd09f97e9d31bbc7fd36c84da4d.svg
          fullname: zuozuo
          isHf: false
          isPro: false
          name: zuozuo
          type: user
        html: '<blockquote>

          <p>Hi,<br>Model Version: SD-XL base, 8sec per image :)<br>Model Version:
          SD-XL Refiner, 15mins per image @_@</p>

          <p>Is this a normal situation?<br>If I switched models, why the image generation
          speed of SD-XL base will also change to 15mins per image!?<br>Thank you.</p>

          </blockquote>

          <p>i have the same problems, also 4090</p>

          '
        raw: "> Hi,\n> Model Version: SD-XL base, 8sec per image :)\n> Model Version:\
          \ SD-XL Refiner, 15mins per image @_@\n> \n> Is this a normal situation?\n\
          > If I switched models, why the image generation speed of SD-XL base will\
          \ also change to 15mins per image!?\n> Thank you.\n\ni have the same problems,\
          \ also 4090"
        updatedAt: '2023-07-10T12:47:28.428Z'
      numEdits: 0
      reactions: []
    id: 64abfde0c08741fc97bb5224
    type: comment
  author: zuozuo
  content: "> Hi,\n> Model Version: SD-XL base, 8sec per image :)\n> Model Version:\
    \ SD-XL Refiner, 15mins per image @_@\n> \n> Is this a normal situation?\n> If\
    \ I switched models, why the image generation speed of SD-XL base will also change\
    \ to 15mins per image!?\n> Thank you.\n\ni have the same problems, also 4090"
  created_at: 2023-07-10 11:47:28+00:00
  edited: false
  hidden: false
  id: 64abfde0c08741fc97bb5224
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1fa21dd09f97e9d31bbc7fd36c84da4d.svg
      fullname: zuozuo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zuozuo
      type: user
    createdAt: '2023-07-10T13:00:58.000Z'
    data:
      edited: false
      editors:
      - zuozuo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5139544606208801
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1fa21dd09f97e9d31bbc7fd36c84da4d.svg
          fullname: zuozuo
          isHf: false
          isPro: false
          name: zuozuo
          type: user
        html: "<p>do not use\uFF1A<br>    pipe.unet = torch.compile(pipe.unet, mode=\"\
          reduce-overhead\", fullgraph=True)</p>\n<p>there are some warning<br>when\
          \ i use nvcr.io/nvidia/pytorch:23.06-py3 in docker container </p>\n"
        raw: "do not use\uFF1A\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\"\
          , fullgraph=True)\n\nthere are some warning\nwhen i use nvcr.io/nvidia/pytorch:23.06-py3\
          \ in docker container "
        updatedAt: '2023-07-10T13:00:58.871Z'
      numEdits: 0
      reactions: []
    id: 64ac010a6ded799c42a3fb74
    type: comment
  author: zuozuo
  content: "do not use\uFF1A\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\"\
    , fullgraph=True)\n\nthere are some warning\nwhen i use nvcr.io/nvidia/pytorch:23.06-py3\
    \ in docker container "
  created_at: 2023-07-10 12:00:58+00:00
  edited: false
  hidden: false
  id: 64ac010a6ded799c42a3fb74
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/smyJFajPaoCXyax4jU3qT.png?w=200&h=200&f=face
      fullname: WANG YUN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: supwang
      type: user
    createdAt: '2023-07-10T14:02:37.000Z'
    data:
      edited: false
      editors:
      - supwang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5436970591545105
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/smyJFajPaoCXyax4jU3qT.png?w=200&h=200&f=face
          fullname: WANG YUN
          isHf: false
          isPro: false
          name: supwang
          type: user
        html: "<blockquote>\n<p>do not use\uFF1A<br>    pipe.unet = torch.compile(pipe.unet,\
          \ mode=\"reduce-overhead\", fullgraph=True)</p>\n<p>there are some warning<br>when\
          \ i use nvcr.io/nvidia/pytorch:23.06-py3 in docker container</p>\n</blockquote>\n\
          <p>How to set the paramenter?</p>\n"
        raw: "> do not use\uFF1A\n>     pipe.unet = torch.compile(pipe.unet, mode=\"\
          reduce-overhead\", fullgraph=True)\n> \n> there are some warning\n> when\
          \ i use nvcr.io/nvidia/pytorch:23.06-py3 in docker container\n\nHow to set\
          \ the paramenter?"
        updatedAt: '2023-07-10T14:02:37.034Z'
      numEdits: 0
      reactions: []
    id: 64ac0f7d776075915cfab991
    type: comment
  author: supwang
  content: "> do not use\uFF1A\n>     pipe.unet = torch.compile(pipe.unet, mode=\"\
    reduce-overhead\", fullgraph=True)\n> \n> there are some warning\n> when i use\
    \ nvcr.io/nvidia/pytorch:23.06-py3 in docker container\n\nHow to set the paramenter?"
  created_at: 2023-07-10 13:02:37+00:00
  edited: false
  hidden: false
  id: 64ac0f7d776075915cfab991
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/smyJFajPaoCXyax4jU3qT.png?w=200&h=200&f=face
      fullname: WANG YUN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: supwang
      type: user
    createdAt: '2023-07-15T14:42:19.000Z'
    data:
      status: closed
    id: 64b2b04b25882acb62be7e15
    type: status-change
  author: supwang
  created_at: 2023-07-15 13:42:19+00:00
  id: 64b2b04b25882acb62be7e15
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: stabilityai/stable-diffusion-xl-refiner-0.9
repo_type: model
status: closed
target_branch: null
title: Why does checking "Load SDXL-Refiner" or using the refiner model, the speed
  of image generation extremely slow?
