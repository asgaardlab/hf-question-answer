!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tarruda
conflicting_files: null
created_at: 2023-05-05 12:08:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70a745746569b264a2ea4815dd04d3a7.svg
      fullname: Thiago de Arruda Padilha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tarruda
      type: user
    createdAt: '2023-05-05T13:08:25.000Z'
    data:
      edited: false
      editors:
      - tarruda
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70a745746569b264a2ea4815dd04d3a7.svg
          fullname: Thiago de Arruda Padilha
          isHf: false
          isPro: false
          name: tarruda
          type: user
        html: '<p>Hi, thanks for sharing this model</p>

          <p>I''m following the instructions <a href="https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/discussions/34">here</a>
          to load and interact with the model, but the 4bit version does not fit in
          my GPU.</p>

          <p>Using the same instructions to load your model (v2) works up to the first
          "generate" request. The second time I ask a question, it fails with an out
          of memory error.</p>

          <p>Do you know if there are any settings I can tweak to make it fit in my
          GPU, or is 8GB VRAM just too low for this model? This is the command I used
          to run: <code>python server.py --model berker_vicuna-13B-1.1-GPTQ-3bit-128g-v2
          --auto-devices --wbits 3 --groupsize 128 --chat</code></p>

          '
        raw: "Hi, thanks for sharing this model\r\n\r\nI'm following the instructions\
          \ [here](https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/discussions/34)\
          \ to load and interact with the model, but the 4bit version does not fit\
          \ in my GPU.\r\n\r\nUsing the same instructions to load your model (v2)\
          \ works up to the first \"generate\" request. The second time I ask a question,\
          \ it fails with an out of memory error.\r\n\r\nDo you know if there are\
          \ any settings I can tweak to make it fit in my GPU, or is 8GB VRAM just\
          \ too low for this model? This is the command I used to run: `python server.py\
          \ --model berker_vicuna-13B-1.1-GPTQ-3bit-128g-v2 --auto-devices --wbits\
          \ 3 --groupsize 128 --chat`"
        updatedAt: '2023-05-05T13:08:25.062Z'
      numEdits: 0
      reactions: []
    id: 6454ffc96f4ae99656269cd1
    type: comment
  author: tarruda
  content: "Hi, thanks for sharing this model\r\n\r\nI'm following the instructions\
    \ [here](https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/discussions/34)\
    \ to load and interact with the model, but the 4bit version does not fit in my\
    \ GPU.\r\n\r\nUsing the same instructions to load your model (v2) works up to\
    \ the first \"generate\" request. The second time I ask a question, it fails with\
    \ an out of memory error.\r\n\r\nDo you know if there are any settings I can tweak\
    \ to make it fit in my GPU, or is 8GB VRAM just too low for this model? This is\
    \ the command I used to run: `python server.py --model berker_vicuna-13B-1.1-GPTQ-3bit-128g-v2\
    \ --auto-devices --wbits 3 --groupsize 128 --chat`"
  created_at: 2023-05-05 12:08:25+00:00
  edited: false
  hidden: false
  id: 6454ffc96f4ae99656269cd1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/801b9d7e2b025d62ac1cb8d2b78d3c01.svg
      fullname: Abel Curmi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BookWormXtreme
      type: user
    createdAt: '2023-06-02T14:01:12.000Z'
    data:
      edited: false
      editors:
      - BookWormXtreme
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/801b9d7e2b025d62ac1cb8d2b78d3c01.svg
          fullname: Abel Curmi
          isHf: false
          isPro: false
          name: BookWormXtreme
          type: user
        html: '<p>Hello, I have just tried to load this model using KoboldAI with
          Occam''s fork for 4bit. For some reason, this model does not load into VRAM,
          which is unfortunate as the tokens per second will be very slow. Have you
          gotten this model to load on VRAM and work correctly since then?</p>

          '
        raw: Hello, I have just tried to load this model using KoboldAI with Occam's
          fork for 4bit. For some reason, this model does not load into VRAM, which
          is unfortunate as the tokens per second will be very slow. Have you gotten
          this model to load on VRAM and work correctly since then?
        updatedAt: '2023-06-02T14:01:12.956Z'
      numEdits: 0
      reactions: []
    id: 6479f628f518a860fbc9ea92
    type: comment
  author: BookWormXtreme
  content: Hello, I have just tried to load this model using KoboldAI with Occam's
    fork for 4bit. For some reason, this model does not load into VRAM, which is unfortunate
    as the tokens per second will be very slow. Have you gotten this model to load
    on VRAM and work correctly since then?
  created_at: 2023-06-02 13:01:12+00:00
  edited: false
  hidden: false
  id: 6479f628f518a860fbc9ea92
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: berker/vicuna-13B-1.1-GPTQ-3bit-128g-v2
repo_type: model
status: open
target_branch: null
title: Any way to make it work on a RTX 3070 (8GB VRAM)?
