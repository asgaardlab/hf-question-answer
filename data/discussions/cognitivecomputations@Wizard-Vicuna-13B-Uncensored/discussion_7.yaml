!!python/object:huggingface_hub.community.DiscussionWithDetails
author: itachiluan
conflicting_files: null
created_at: 2023-06-10 05:15:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bec4406ccd634cd6567e5a44b111f0c2.svg
      fullname: Yi Luan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: itachiluan
      type: user
    createdAt: '2023-06-10T06:15:53.000Z'
    data:
      edited: true
      editors:
      - itachiluan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9698222279548645
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bec4406ccd634cd6567e5a44b111f0c2.svg
          fullname: Yi Luan
          isHf: false
          isPro: false
          name: itachiluan
          type: user
        html: '<p>Hi ehartford,</p>

          <p>First of all, thanks so much for bringing this model to us! I think it
          is by far the best model suited for all my tasks.<br>One of the problem
          that I experienced, and I think a lot of people also are experiencing is
          the 2048 token length limit.<br>Before seeing this repo, I thought it was
          not possible to expand llama models'' context lengths until I found this:<br><a
          rel="nofollow" href="https://github.com/epfml/landmark-attention">https://github.com/epfml/landmark-attention</a></p>

          <p>As I have been looking through the wizard vicuna dataset, I''ve found
          that there are prompts that are way over 2048 tokens (maximum I found was
          8225), although my method of calculating the token size could be wrong (by
          using llama tokenizer to tokenize the combination of history and prompt),
          I think we can still come to a conclusion that some of the data from wizard
          vicuna were truncated due to token length limitations.</p>

          <p>Do you think it was worth a shot for trying landmark attention on the
          wizard vicuna 13b model to see if we can expand its context length?<br>Thanks!</p>

          '
        raw: 'Hi ehartford,


          First of all, thanks so much for bringing this model to us! I think it is
          by far the best model suited for all my tasks.

          One of the problem that I experienced, and I think a lot of people also
          are experiencing is the 2048 token length limit.

          Before seeing this repo, I thought it was not possible to expand llama models''
          context lengths until I found this:

          https://github.com/epfml/landmark-attention


          As I have been looking through the wizard vicuna dataset, I''ve found that
          there are prompts that are way over 2048 tokens (maximum I found was 8225),
          although my method of calculating the token size could be wrong (by using
          llama tokenizer to tokenize the combination of history and prompt), I think
          we can still come to a conclusion that some of the data from wizard vicuna
          were truncated due to token length limitations.


          Do you think it was worth a shot for trying landmark attention on the wizard
          vicuna 13b model to see if we can expand its context length?

          Thanks!'
        updatedAt: '2023-06-10T06:18:20.103Z'
      numEdits: 2
      reactions: []
    id: 64841519295256340e499b3b
    type: comment
  author: itachiluan
  content: 'Hi ehartford,


    First of all, thanks so much for bringing this model to us! I think it is by far
    the best model suited for all my tasks.

    One of the problem that I experienced, and I think a lot of people also are experiencing
    is the 2048 token length limit.

    Before seeing this repo, I thought it was not possible to expand llama models''
    context lengths until I found this:

    https://github.com/epfml/landmark-attention


    As I have been looking through the wizard vicuna dataset, I''ve found that there
    are prompts that are way over 2048 tokens (maximum I found was 8225), although
    my method of calculating the token size could be wrong (by using llama tokenizer
    to tokenize the combination of history and prompt), I think we can still come
    to a conclusion that some of the data from wizard vicuna were truncated due to
    token length limitations.


    Do you think it was worth a shot for trying landmark attention on the wizard vicuna
    13b model to see if we can expand its context length?

    Thanks!'
  created_at: 2023-06-10 05:15:53+00:00
  edited: true
  hidden: false
  id: 64841519295256340e499b3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-06-10T06:24:26.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9861989617347717
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>I''ll look into it, sounds interesting!</p>

          '
        raw: I'll look into it, sounds interesting!
        updatedAt: '2023-06-10T06:24:26.738Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - itachiluan
    id: 6484171adff6b4da44d8ced5
    type: comment
  author: ehartford
  content: I'll look into it, sounds interesting!
  created_at: 2023-06-10 05:24:26+00:00
  edited: false
  hidden: false
  id: 6484171adff6b4da44d8ced5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/upL2BEgGjiZiEXvTSyk2o.png?w=200&h=200&f=face
      fullname: Kippykip
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kippykip
      type: user
    createdAt: '2023-06-25T15:18:38.000Z'
    data:
      edited: false
      editors:
      - Kippykip
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9824117422103882
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/upL2BEgGjiZiEXvTSyk2o.png?w=200&h=200&f=face
          fullname: Kippykip
          isHf: false
          isPro: false
          name: Kippykip
          type: user
        html: '<p>I didn''t even know that was possible, but that would be amazing!<br>Whenever
          I make quirky charcters with example context / past chats, it always eats
          up quite a decent amount of the 2048 tokens. Extending that would be a dream
          come true!</p>

          '
        raw: 'I didn''t even know that was possible, but that would be amazing!

          Whenever I make quirky charcters with example context / past chats, it always
          eats up quite a decent amount of the 2048 tokens. Extending that would be
          a dream come true!'
        updatedAt: '2023-06-25T15:18:38.379Z'
      numEdits: 0
      reactions: []
    id: 64985ace4e65327a83a2b1f3
    type: comment
  author: Kippykip
  content: 'I didn''t even know that was possible, but that would be amazing!

    Whenever I make quirky charcters with example context / past chats, it always
    eats up quite a decent amount of the 2048 tokens. Extending that would be a dream
    come true!'
  created_at: 2023-06-25 14:18:38+00:00
  edited: false
  hidden: false
  id: 64985ace4e65327a83a2b1f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bec4406ccd634cd6567e5a44b111f0c2.svg
      fullname: Yi Luan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: itachiluan
      type: user
    createdAt: '2023-07-01T01:47:46.000Z'
    data:
      edited: false
      editors:
      - itachiluan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9679390788078308
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bec4406ccd634cd6567e5a44b111f0c2.svg
          fullname: Yi Luan
          isHf: false
          isPro: false
          name: itachiluan
          type: user
        html: "<blockquote>\n<p>I didn't even know that was possible, but that would\
          \ be amazing!<br>Whenever I make quirky charcters with example context /\
          \ past chats, it always eats up quite a decent amount of the 2048 tokens.\
          \ Extending that would be a dream come true!</p>\n</blockquote>\n<p>Hi,</p>\n\
          <p>If you check TheBloke\u2019s page, he has published many models that\
          \ now merged with superHOT 8k Lora that extends the context length to 8k+,\
          \ worth giving it a go!<br>The slight problem for me was that the superHOT\
          \ Lora\u2019s didn\u2019t train with the wizard vicu\xF1a dataset so the\
          \ wizard vicu\xF1a merged model has a slight less accuracy for me (for Chinese\
          \ generation), but definitely worth trying !</p>\n"
        raw: "> I didn't even know that was possible, but that would be amazing!\n\
          > Whenever I make quirky charcters with example context / past chats, it\
          \ always eats up quite a decent amount of the 2048 tokens. Extending that\
          \ would be a dream come true!\n\nHi,\n\nIf you check TheBloke\u2019s page,\
          \ he has published many models that now merged with superHOT 8k Lora that\
          \ extends the context length to 8k+, worth giving it a go!\nThe slight problem\
          \ for me was that the superHOT Lora\u2019s didn\u2019t train with the wizard\
          \ vicu\xF1a dataset so the wizard vicu\xF1a merged model has a slight less\
          \ accuracy for me (for Chinese generation), but definitely worth trying\
          \ !"
        updatedAt: '2023-07-01T01:47:46.515Z'
      numEdits: 0
      reactions: []
    id: 649f85c250251e408adfd1d6
    type: comment
  author: itachiluan
  content: "> I didn't even know that was possible, but that would be amazing!\n>\
    \ Whenever I make quirky charcters with example context / past chats, it always\
    \ eats up quite a decent amount of the 2048 tokens. Extending that would be a\
    \ dream come true!\n\nHi,\n\nIf you check TheBloke\u2019s page, he has published\
    \ many models that now merged with superHOT 8k Lora that extends the context length\
    \ to 8k+, worth giving it a go!\nThe slight problem for me was that the superHOT\
    \ Lora\u2019s didn\u2019t train with the wizard vicu\xF1a dataset so the wizard\
    \ vicu\xF1a merged model has a slight less accuracy for me (for Chinese generation),\
    \ but definitely worth trying !"
  created_at: 2023-07-01 00:47:46+00:00
  edited: false
  hidden: false
  id: 649f85c250251e408adfd1d6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: cognitivecomputations/Wizard-Vicuna-13B-Uncensored
repo_type: model
status: open
target_branch: null
title: Any plans on expanding the context length with landmark attention?
