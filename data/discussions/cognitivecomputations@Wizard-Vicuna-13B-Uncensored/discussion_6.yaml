!!python/object:huggingface_hub.community.DiscussionWithDetails
author: a749734
conflicting_files: null
created_at: 2023-06-01 11:27:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76f92b41f85d3a901911f962660830ed.svg
      fullname: a749734
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: a749734
      type: user
    createdAt: '2023-06-01T12:27:30.000Z'
    data:
      edited: false
      editors:
      - a749734
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76f92b41f85d3a901911f962660830ed.svg
          fullname: a749734
          isHf: false
          isPro: false
          name: a749734
          type: user
        html: '<p>Traceback (most recent call last):<br>  File "/home/cloud-user/testing/fine_tune.py",
          line 78, in <br>    tokens = model.generate(**inputs, generation_config=generation_config).to(device)<br>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/home/cloud-user/anaconda3/envs/fid_env/lib/python3.11/site-packages/torch/utils/_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>           ^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/home/cloud-user/anaconda3/envs/fid_env/lib/python3.11/site-packages/transformers/generation/utils.py",
          line 1267, in generate<br>    self._validate_model_kwargs(model_kwargs.copy())<br>  File
          "/home/cloud-user/anaconda3/envs/fid_env/lib/python3.11/site-packages/transformers/generation/utils.py",
          line 1140, in _validate_model_kwargs<br>    raise ValueError(<br>ValueError:
          The following <code>model_kwargs</code> are not used by the model: [''token_type_ids'']
          (note: typos in the generate arguments will also show up in this list)</p>

          '
        raw: "Traceback (most recent call last):\r\n  File \"/home/cloud-user/testing/fine_tune.py\"\
          , line 78, in <module>\r\n    tokens = model.generate(**inputs, generation_config=generation_config).to(device)\r\
          \n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/home/cloud-user/anaconda3/envs/fid_env/lib/python3.11/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cloud-user/anaconda3/envs/fid_env/lib/python3.11/site-packages/transformers/generation/utils.py\"\
          , line 1267, in generate\r\n    self._validate_model_kwargs(model_kwargs.copy())\r\
          \n  File \"/home/cloud-user/anaconda3/envs/fid_env/lib/python3.11/site-packages/transformers/generation/utils.py\"\
          , line 1140, in _validate_model_kwargs\r\n    raise ValueError(\r\nValueError:\
          \ The following `model_kwargs` are not used by the model: ['token_type_ids']\
          \ (note: typos in the generate arguments will also show up in this list)"
        updatedAt: '2023-06-01T12:27:30.670Z'
      numEdits: 0
      reactions: []
    id: 64788eb275a9a0cfabd0c834
    type: comment
  author: a749734
  content: "Traceback (most recent call last):\r\n  File \"/home/cloud-user/testing/fine_tune.py\"\
    , line 78, in <module>\r\n    tokens = model.generate(**inputs, generation_config=generation_config).to(device)\r\
    \n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/home/cloud-user/anaconda3/envs/fid_env/lib/python3.11/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n      \
    \     ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/cloud-user/anaconda3/envs/fid_env/lib/python3.11/site-packages/transformers/generation/utils.py\"\
    , line 1267, in generate\r\n    self._validate_model_kwargs(model_kwargs.copy())\r\
    \n  File \"/home/cloud-user/anaconda3/envs/fid_env/lib/python3.11/site-packages/transformers/generation/utils.py\"\
    , line 1140, in _validate_model_kwargs\r\n    raise ValueError(\r\nValueError:\
    \ The following `model_kwargs` are not used by the model: ['token_type_ids'] (note:\
    \ typos in the generate arguments will also show up in this list)"
  created_at: 2023-06-01 11:27:30+00:00
  edited: false
  hidden: false
  id: 64788eb275a9a0cfabd0c834
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76f92b41f85d3a901911f962660830ed.svg
      fullname: a749734
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: a749734
      type: user
    createdAt: '2023-06-01T12:28:26.000Z'
    data:
      edited: false
      editors:
      - a749734
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76f92b41f85d3a901911f962660830ed.svg
          fullname: a749734
          isHf: false
          isPro: false
          name: a749734
          type: user
        html: '<p>i m using this code</p>

          <p>generation_config = GenerationConfig(<br>    #max_new tokn =<br>    temperature=0.4,<br>    repetition_penalty=1.3,<br>    #
          do_sample = True,<br>    # early_stopping = False,<br>    min_length = 21,<br>    max_length
          = 21<br>)</p>

          <p>inputs = tokenizer(<br>    prompt, padding=False, add_special_tokens=False,
          return_tensors="pt"<br>).to(device)</p>

          <p>with torch.inference_mode():<br>    tokens = model.generate(**inputs,
          generation_config=generation_config).to(device)</p>

          <p>completion = tokenizer.decode(tokens[0], skip_special_tokens=True)</p>

          '
        raw: "i m using this code\n\ngeneration_config = GenerationConfig(\n    #max_new\
          \ tokn = \n    temperature=0.4,\n    repetition_penalty=1.3,\n    # do_sample\
          \ = True,\n    # early_stopping = False,\n    min_length = 21,\n    max_length\
          \ = 21\n)\n\ninputs = tokenizer(\n    prompt, padding=False, add_special_tokens=False,\
          \ return_tensors=\"pt\"\n).to(device)\n\nwith torch.inference_mode():\n\
          \    tokens = model.generate(**inputs, generation_config=generation_config).to(device)\n\
          \n\ncompletion = tokenizer.decode(tokens[0], skip_special_tokens=True)"
        updatedAt: '2023-06-01T12:28:26.777Z'
      numEdits: 0
      reactions: []
    id: 64788eeaad83f3939b4c78ef
    type: comment
  author: a749734
  content: "i m using this code\n\ngeneration_config = GenerationConfig(\n    #max_new\
    \ tokn = \n    temperature=0.4,\n    repetition_penalty=1.3,\n    # do_sample\
    \ = True,\n    # early_stopping = False,\n    min_length = 21,\n    max_length\
    \ = 21\n)\n\ninputs = tokenizer(\n    prompt, padding=False, add_special_tokens=False,\
    \ return_tensors=\"pt\"\n).to(device)\n\nwith torch.inference_mode():\n    tokens\
    \ = model.generate(**inputs, generation_config=generation_config).to(device)\n\
    \n\ncompletion = tokenizer.decode(tokens[0], skip_special_tokens=True)"
  created_at: 2023-06-01 11:28:26+00:00
  edited: false
  hidden: false
  id: 64788eeaad83f3939b4c78ef
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: cognitivecomputations/Wizard-Vicuna-13B-Uncensored
repo_type: model
status: open
target_branch: null
title: model_kwargs are not used by model
