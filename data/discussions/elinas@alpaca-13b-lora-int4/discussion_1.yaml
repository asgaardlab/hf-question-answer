!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yehiaserag
conflicting_files: null
created_at: 2023-03-18 04:22:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
      fullname: yehia serag
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yehiaserag
      type: user
    createdAt: '2023-03-18T05:22:19.000Z'
    data:
      edited: false
      editors:
      - yehiaserag
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
          fullname: yehia serag
          isHf: false
          isPro: false
          name: yehiaserag
          type: user
        html: '<p>I was downloading the model from <a href="https://huggingface.co/nealchandra/alpaca-13b-hf-int4">https://huggingface.co/nealchandra/alpaca-13b-hf-int4</a>
          and I noticed you uploaded a model but the size is different while they
          should be exactly the same.<br>Do you have any idea why that is?</p>

          '
        raw: "I was downloading the model from https://huggingface.co/nealchandra/alpaca-13b-hf-int4\
          \ and I noticed you uploaded a model but the size is different while they\
          \ should be exactly the same.\r\nDo you have any idea why that is?"
        updatedAt: '2023-03-18T05:22:19.897Z'
      numEdits: 0
      reactions: []
    id: 64154a8b95fb6f824b231883
    type: comment
  author: yehiaserag
  content: "I was downloading the model from https://huggingface.co/nealchandra/alpaca-13b-hf-int4\
    \ and I noticed you uploaded a model but the size is different while they should\
    \ be exactly the same.\r\nDo you have any idea why that is?"
  created_at: 2023-03-18 04:22:19+00:00
  edited: false
  hidden: false
  id: 64154a8b95fb6f824b231883
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-03-18T17:18:50.000Z'
    data:
      edited: true
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>This is LoRA trained and the code to convert it was found here:
          <a rel="nofollow" href="https://github.com/jooray/alpaca-lora-13b/">https://github.com/jooray/alpaca-lora-13b/</a>
          which uses the weights found here <a href="https://huggingface.co/samwit/alpaca13B-lora">https://huggingface.co/samwit/alpaca13B-lora</a></p>

          <p>I simply used the conversion script to convert to pth, then converted
          back to pytorch format in order to run GPTQ to quantize and pack the model.
          <del>There may be some size loss in that process or just because it''s LoRA
          and not finetuned.</del> Edit: My model''s size matches the original. Also,
          the model you linked is finetuned for one epoch.</p>

          <p>Though after testing, I don''t believe that LoRA replicates Stanford
          Alpaca well and would finetune it on the actual dataset for the full 3 epochs
          if I had the resources. I would recommend you use <a href="https://huggingface.co/elinas/llama-30b-int4">https://huggingface.co/elinas/llama-30b-int4</a>
          as it has much better overall coherency with the correct sampler settings,
          as long as you have 15GB vram minimum to run it, 24GB total recommended.
          If you need the sampler values for llama, let me know and I can post them
          here.</p>

          <p>Otherwise wait for a full finetuned version of alpaca-13b :)</p>

          '
        raw: 'This is LoRA trained and the code to convert it was found here: https://github.com/jooray/alpaca-lora-13b/
          which uses the weights found here https://huggingface.co/samwit/alpaca13B-lora


          I simply used the conversion script to convert to pth, then converted back
          to pytorch format in order to run GPTQ to quantize and pack the model. ~~There
          may be some size loss in that process or just because it''s LoRA and not
          finetuned.~~ Edit: My model''s size matches the original. Also, the model
          you linked is finetuned for one epoch.


          Though after testing, I don''t believe that LoRA replicates Stanford Alpaca
          well and would finetune it on the actual dataset for the full 3 epochs if
          I had the resources. I would recommend you use https://huggingface.co/elinas/llama-30b-int4
          as it has much better overall coherency with the correct sampler settings,
          as long as you have 15GB vram minimum to run it, 24GB total recommended.
          If you need the sampler values for llama, let me know and I can post them
          here.


          Otherwise wait for a full finetuned version of alpaca-13b :)'
        updatedAt: '2023-03-18T17:43:08.283Z'
      numEdits: 3
      reactions: []
      relatedEventId: 6415f27ae9e50fef5808f122
    id: 6415f27ae9e50fef5808f121
    type: comment
  author: elinas
  content: 'This is LoRA trained and the code to convert it was found here: https://github.com/jooray/alpaca-lora-13b/
    which uses the weights found here https://huggingface.co/samwit/alpaca13B-lora


    I simply used the conversion script to convert to pth, then converted back to
    pytorch format in order to run GPTQ to quantize and pack the model. ~~There may
    be some size loss in that process or just because it''s LoRA and not finetuned.~~
    Edit: My model''s size matches the original. Also, the model you linked is finetuned
    for one epoch.


    Though after testing, I don''t believe that LoRA replicates Stanford Alpaca well
    and would finetune it on the actual dataset for the full 3 epochs if I had the
    resources. I would recommend you use https://huggingface.co/elinas/llama-30b-int4
    as it has much better overall coherency with the correct sampler settings, as
    long as you have 15GB vram minimum to run it, 24GB total recommended. If you need
    the sampler values for llama, let me know and I can post them here.


    Otherwise wait for a full finetuned version of alpaca-13b :)'
  created_at: 2023-03-18 16:18:50+00:00
  edited: true
  hidden: false
  id: 6415f27ae9e50fef5808f121
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-03-18T17:18:50.000Z'
    data:
      status: closed
    id: 6415f27ae9e50fef5808f122
    type: status-change
  author: elinas
  created_at: 2023-03-18 16:18:50+00:00
  id: 6415f27ae9e50fef5808f122
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-03-18T17:28:36.000Z'
    data:
      status: open
    id: 6415f4c4590ba26395c7fb6a
    type: status-change
  author: elinas
  created_at: 2023-03-18 16:28:36+00:00
  id: 6415f4c4590ba26395c7fb6a
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
      fullname: yehia serag
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yehiaserag
      type: user
    createdAt: '2023-03-18T23:42:49.000Z'
    data:
      edited: false
      editors:
      - yehiaserag
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
          fullname: yehia serag
          isHf: false
          isPro: false
          name: yehiaserag
          type: user
        html: '<p>I think I''ll have to wait, actually the difference between the
          lora and the full mode is what drove me to get the full model<br>Also only
          12GB of vram here so it''s the wait for me for either a release of the model
          or good support for offloading to cpu</p>

          '
        raw: 'I think I''ll have to wait, actually the difference between the lora
          and the full mode is what drove me to get the full model

          Also only 12GB of vram here so it''s the wait for me for either a release
          of the model or good support for offloading to cpu'
        updatedAt: '2023-03-18T23:42:49.783Z'
      numEdits: 0
      reactions: []
    id: 64164c799b85ee6b3943c4dc
    type: comment
  author: yehiaserag
  content: 'I think I''ll have to wait, actually the difference between the lora and
    the full mode is what drove me to get the full model

    Also only 12GB of vram here so it''s the wait for me for either a release of the
    model or good support for offloading to cpu'
  created_at: 2023-03-18 22:42:49+00:00
  edited: false
  hidden: false
  id: 64164c799b85ee6b3943c4dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
      fullname: nacs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nacs
      type: user
    createdAt: '2023-03-21T00:43:20.000Z'
    data:
      edited: false
      editors:
      - nacs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
          fullname: nacs
          isHf: false
          isPro: false
          name: nacs
          type: user
        html: '<blockquote>

          <p>finetune it on the actual dataset for the full 3 epochs if I had the
          resources</p>

          </blockquote>

          <p>Someone has trained for 3 epochs on alpaca 30b here: <a href="https://huggingface.co/baseten/alpaca-30b">https://huggingface.co/baseten/alpaca-30b</a></p>

          <p>(I''m hoping you''ll release the alpaca 30b int4 though as I don''t have
          the VRAM to load it in 8-bit mode)</p>

          '
        raw: '> finetune it on the actual dataset for the full 3 epochs if I had the
          resources


          Someone has trained for 3 epochs on alpaca 30b here: https://huggingface.co/baseten/alpaca-30b


          (I''m hoping you''ll release the alpaca 30b int4 though as I don''t have
          the VRAM to load it in 8-bit mode)'
        updatedAt: '2023-03-21T00:43:20.223Z'
      numEdits: 0
      reactions: []
    id: 6418fda80841e29bbe1416f8
    type: comment
  author: nacs
  content: '> finetune it on the actual dataset for the full 3 epochs if I had the
    resources


    Someone has trained for 3 epochs on alpaca 30b here: https://huggingface.co/baseten/alpaca-30b


    (I''m hoping you''ll release the alpaca 30b int4 though as I don''t have the VRAM
    to load it in 8-bit mode)'
  created_at: 2023-03-20 23:43:20+00:00
  edited: false
  hidden: false
  id: 6418fda80841e29bbe1416f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-03-21T05:45:14.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>That''s LoRA not finetuned so it won''t replicate the original results
          as accurately. Regardless I have converted to int4 and uploaded it as it''s
          not bad.</p>

          '
        raw: That's LoRA not finetuned so it won't replicate the original results
          as accurately. Regardless I have converted to int4 and uploaded it as it's
          not bad.
        updatedAt: '2023-03-21T05:45:14.902Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - nacs
        - yehiaserag
    id: 6419446a08588020c7b15584
    type: comment
  author: elinas
  content: That's LoRA not finetuned so it won't replicate the original results as
    accurately. Regardless I have converted to int4 and uploaded it as it's not bad.
  created_at: 2023-03-21 04:45:14+00:00
  edited: false
  hidden: false
  id: 6419446a08588020c7b15584
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69a55867b771566217e9da3b2555b4ea.svg
      fullname: xpwr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xpwr
      type: user
    createdAt: '2023-03-23T14:39:36.000Z'
    data:
      edited: false
      editors:
      - xpwr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69a55867b771566217e9da3b2555b4ea.svg
          fullname: xpwr
          isHf: false
          isPro: false
          name: xpwr
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;elinas&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/elinas\">@<span class=\"\
          underline\">elinas</span></a></span>\n\n\t</span></span> could you share\
          \ the sampler values for llama?</p>\n"
        raw: '@elinas could you share the sampler values for llama?'
        updatedAt: '2023-03-23T14:39:36.434Z'
      numEdits: 0
      reactions: []
    id: 641c64a83d67778aae236b11
    type: comment
  author: xpwr
  content: '@elinas could you share the sampler values for llama?'
  created_at: 2023-03-23 13:39:36+00:00
  edited: false
  hidden: false
  id: 641c64a83d67778aae236b11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-03-28T13:10:54.000Z'
    data:
      edited: false
      editors:
      - wassname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: '<p>It looks like you might be using LoRA from the uncleaned alpaca,
          there is a big differen''t when using <a rel="nofollow" href="https://github.com/gururise/AlpacaDataCleaned">https://github.com/gururise/AlpacaDataCleaned</a></p>

          '
        raw: It looks like you might be using LoRA from the uncleaned alpaca, there
          is a big differen't when using https://github.com/gururise/AlpacaDataCleaned
        updatedAt: '2023-03-28T13:10:54.065Z'
      numEdits: 0
      reactions: []
    id: 6422e75e0b06c1a2b1f46b7f
    type: comment
  author: wassname
  content: It looks like you might be using LoRA from the uncleaned alpaca, there
    is a big differen't when using https://github.com/gururise/AlpacaDataCleaned
  created_at: 2023-03-28 12:10:54+00:00
  edited: false
  hidden: false
  id: 6422e75e0b06c1a2b1f46b7f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-03-28T15:21:55.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>There''s going to be more difference in fine tuning the model versus
          using LoRA. This is evident in the quality of alpaca 7b native vs alpaca
          7b LoRA. This is using the Stanford dataset like most other alpaca models
          on here and this "cleaned" dataset was released a week ago and only has
          claims. I see no benchmarks on it actually being better.</p>

          '
        raw: There's going to be more difference in fine tuning the model versus using
          LoRA. This is evident in the quality of alpaca 7b native vs alpaca 7b LoRA.
          This is using the Stanford dataset like most other alpaca models on here
          and this "cleaned" dataset was released a week ago and only has claims.
          I see no benchmarks on it actually being better.
        updatedAt: '2023-03-28T15:21:55.895Z'
      numEdits: 0
      reactions: []
    id: 642306131a8060b0e447699c
    type: comment
  author: elinas
  content: There's going to be more difference in fine tuning the model versus using
    LoRA. This is evident in the quality of alpaca 7b native vs alpaca 7b LoRA. This
    is using the Stanford dataset like most other alpaca models on here and this "cleaned"
    dataset was released a week ago and only has claims. I see no benchmarks on it
    actually being better.
  created_at: 2023-03-28 14:21:55+00:00
  edited: false
  hidden: false
  id: 642306131a8060b0e447699c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-03-29T21:29:43.000Z'
    data:
      edited: true
      editors:
      - wassname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: '<p>There are no benchmarks, but you will notice a big qualitative difference,
          most notably it no longer says "As a large language model..."  after cleaning
          :)</p>

          <blockquote>

          <p>There''s going to be more difference in fine tuning the model versus
          using LoRA. </p>

          </blockquote>

          <p>In the LoRA paper they benchmark it vs FT. While we can expect FT to
          be better, they found a very small difference, with LoRA even sometimes
          being better (which I''m assuming this difference is less than the uncertainty
          in the metric). </p>

          <p>Since the LoRA/FT gaps is small, personally, I would expect dataset quality
          to have larger impact than LoRA/FT, but you''re welcome to disagree. It''s
          certainly useful to have your contribution up!</p>

          '
        raw: "There are no benchmarks, but you will notice a big qualitative difference,\
          \ most notably it no longer says \"As a large language model...\"  after\
          \ cleaning :)\n\n> There's going to be more difference in fine tuning the\
          \ model versus using LoRA. \n\nIn the LoRA paper they benchmark it vs FT.\
          \ While we can expect FT to be better, they found a very small difference,\
          \ with LoRA even sometimes being better (which I'm assuming this difference\
          \ is less than the uncertainty in the metric). \n\nSince the LoRA/FT gaps\
          \ is small, personally, I would expect dataset quality to have larger impact\
          \ than LoRA/FT, but you're welcome to disagree. It's certainly useful to\
          \ have your contribution up!"
        updatedAt: '2023-03-30T07:25:18.737Z'
      numEdits: 1
      reactions: []
    id: 6424adc77841925e0b57213a
    type: comment
  author: wassname
  content: "There are no benchmarks, but you will notice a big qualitative difference,\
    \ most notably it no longer says \"As a large language model...\"  after cleaning\
    \ :)\n\n> There's going to be more difference in fine tuning the model versus\
    \ using LoRA. \n\nIn the LoRA paper they benchmark it vs FT. While we can expect\
    \ FT to be better, they found a very small difference, with LoRA even sometimes\
    \ being better (which I'm assuming this difference is less than the uncertainty\
    \ in the metric). \n\nSince the LoRA/FT gaps is small, personally, I would expect\
    \ dataset quality to have larger impact than LoRA/FT, but you're welcome to disagree.\
    \ It's certainly useful to have your contribution up!"
  created_at: 2023-03-29 20:29:43+00:00
  edited: true
  hidden: false
  id: 6424adc77841925e0b57213a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-03-29T22:40:15.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>Apologies if I came off as dismissive, but I based this off of the
          7b finetined (native) model vs LoRA and there was a noticable difference
          in quality. Though, more parameters, trained with LoRA are better than it,
          specifically Alpaca-30B.</p>

          <blockquote>

          <p>In the LoRA paper they benchmark it vs FT. While we can expect FT to
          be better, they found a very small difference, with LoRA even sometimes
          being better (which I''m assuming this difference is less than the uncertainty
          in the metric).</p>

          </blockquote>

          <p>I have not personally read through the paper, but I find that Interesting.
          Is it the case as the parameter count gets larger, the less difference there
          is between FT and LoRA?</p>

          <p>Anyway, back to the "cleaned" dataset. I do not have the compute to use
          LoRA for 13B or 30B. If someone goes ahead and releases the LoRA weights,
          I am willing to quantize them to 4bit. If you find someone who has done
          so, feel free to let me know.</p>

          '
        raw: 'Apologies if I came off as dismissive, but I based this off of the 7b
          finetined (native) model vs LoRA and there was a noticable difference in
          quality. Though, more parameters, trained with LoRA are better than it,
          specifically Alpaca-30B.


          > In the LoRA paper they benchmark it vs FT. While we can expect FT to be
          better, they found a very small difference, with LoRA even sometimes being
          better (which I''m assuming this difference is less than the uncertainty
          in the metric).


          I have not personally read through the paper, but I find that Interesting.
          Is it the case as the parameter count gets larger, the less difference there
          is between FT and LoRA?


          Anyway, back to the "cleaned" dataset. I do not have the compute to use
          LoRA for 13B or 30B. If someone goes ahead and releases the LoRA weights,
          I am willing to quantize them to 4bit. If you find someone who has done
          so, feel free to let me know.'
        updatedAt: '2023-03-29T22:40:15.494Z'
      numEdits: 0
      reactions: []
    id: 6424be4f7ca72bb4e354dc20
    type: comment
  author: elinas
  content: 'Apologies if I came off as dismissive, but I based this off of the 7b
    finetined (native) model vs LoRA and there was a noticable difference in quality.
    Though, more parameters, trained with LoRA are better than it, specifically Alpaca-30B.


    > In the LoRA paper they benchmark it vs FT. While we can expect FT to be better,
    they found a very small difference, with LoRA even sometimes being better (which
    I''m assuming this difference is less than the uncertainty in the metric).


    I have not personally read through the paper, but I find that Interesting. Is
    it the case as the parameter count gets larger, the less difference there is between
    FT and LoRA?


    Anyway, back to the "cleaned" dataset. I do not have the compute to use LoRA for
    13B or 30B. If someone goes ahead and releases the LoRA weights, I am willing
    to quantize them to 4bit. If you find someone who has done so, feel free to let
    me know.'
  created_at: 2023-03-29 21:40:15+00:00
  edited: false
  hidden: false
  id: 6424be4f7ca72bb4e354dc20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-03-30T07:32:55.000Z'
    data:
      edited: false
      editors:
      - wassname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: '<blockquote>

          <p>Apologies if I came off as dismissive, but I based this off of the 7b
          finetined (native) model vs LoRA and there was a noticable difference in
          quality. Though, more parameters, trained with LoRA are better than it,
          specifically Alpaca-30B.</p>

          </blockquote>

          <p>Hey no worries it''s the internet! And huh, that''s an interesting observation.
          LoRA is a bit annoying to support tbh, especially if you want to use text-generation-webui.</p>

          <blockquote>

          <p> I do not have the compute to use LoRA for 13B or 30B. </p>

          </blockquote>

          <p>Neither, it''s a sad state of affairs, but at least we are managing to
          get around it. Hopefully it gets easier on us in the next few years.</p>

          <blockquote>

          <p>If someone goes ahead and releases the LoRA weights</p>

          </blockquote>

          <p>Chansung has said he plans to ( <a href="https://huggingface.co/chansung/alpaca-lora-30b/discussions/2#641ef2d8dd15d15f8ed2e0b3">https://huggingface.co/chansung/alpaca-lora-30b/discussions/2#641ef2d8dd15d15f8ed2e0b3</a>
          ) once they have finished cleaning the data (they are still going, for example
          80% of maths examples were wrong :O). </p>

          <p>How did you combine the LoRA and base model btw?</p>

          '
        raw: "> Apologies if I came off as dismissive, but I based this off of the\
          \ 7b finetined (native) model vs LoRA and there was a noticable difference\
          \ in quality. Though, more parameters, trained with LoRA are better than\
          \ it, specifically Alpaca-30B.\n\nHey no worries it's the internet! And\
          \ huh, that's an interesting observation. LoRA is a bit annoying to support\
          \ tbh, especially if you want to use text-generation-webui.\n\n>  I do not\
          \ have the compute to use LoRA for 13B or 30B. \n\nNeither, it's a sad state\
          \ of affairs, but at least we are managing to get around it. Hopefully it\
          \ gets easier on us in the next few years.\n\n> If someone goes ahead and\
          \ releases the LoRA weights\n\nChansung has said he plans to ( https://huggingface.co/chansung/alpaca-lora-30b/discussions/2#641ef2d8dd15d15f8ed2e0b3\
          \ ) once they have finished cleaning the data (they are still going, for\
          \ example 80% of maths examples were wrong :O). \n\nHow did you combine\
          \ the LoRA and base model btw?"
        updatedAt: '2023-03-30T07:32:55.297Z'
      numEdits: 0
      reactions: []
    id: 64253b2708e9e0ea8ccbb713
    type: comment
  author: wassname
  content: "> Apologies if I came off as dismissive, but I based this off of the 7b\
    \ finetined (native) model vs LoRA and there was a noticable difference in quality.\
    \ Though, more parameters, trained with LoRA are better than it, specifically\
    \ Alpaca-30B.\n\nHey no worries it's the internet! And huh, that's an interesting\
    \ observation. LoRA is a bit annoying to support tbh, especially if you want to\
    \ use text-generation-webui.\n\n>  I do not have the compute to use LoRA for 13B\
    \ or 30B. \n\nNeither, it's a sad state of affairs, but at least we are managing\
    \ to get around it. Hopefully it gets easier on us in the next few years.\n\n\
    > If someone goes ahead and releases the LoRA weights\n\nChansung has said he\
    \ plans to ( https://huggingface.co/chansung/alpaca-lora-30b/discussions/2#641ef2d8dd15d15f8ed2e0b3\
    \ ) once they have finished cleaning the data (they are still going, for example\
    \ 80% of maths examples were wrong :O). \n\nHow did you combine the LoRA and base\
    \ model btw?"
  created_at: 2023-03-30 06:32:55+00:00
  edited: false
  hidden: false
  id: 64253b2708e9e0ea8ccbb713
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-03-30T14:56:01.000Z'
    data:
      edited: true
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>It''s a bit of a process, at least originally. I had to use a modified
          LoRA merging script which I modified for 30b, as it was already working
          on 13b. Then I converted the weights to PyTorch/HF (which I can use now
          to avoid the first step), and then I finally quantize the model. The code
          is on my GitHub if you''re curious.</p>

          <p>I''m not sure if there''s any interest in uploading the PyTorch / HF
          weights for Alpaca but I might.</p>

          '
        raw: 'It''s a bit of a process, at least originally. I had to use a modified
          LoRA merging script which I modified for 30b, as it was already working
          on 13b. Then I converted the weights to PyTorch/HF (which I can use now
          to avoid the first step), and then I finally quantize the model. The code
          is on my GitHub if you''re curious.


          I''m not sure if there''s any interest in uploading the PyTorch / HF weights
          for Alpaca but I might.'
        updatedAt: '2023-03-30T14:57:35.120Z'
      numEdits: 1
      reactions: []
    id: 6425a30108e9e0ea8ccefefc
    type: comment
  author: elinas
  content: 'It''s a bit of a process, at least originally. I had to use a modified
    LoRA merging script which I modified for 30b, as it was already working on 13b.
    Then I converted the weights to PyTorch/HF (which I can use now to avoid the first
    step), and then I finally quantize the model. The code is on my GitHub if you''re
    curious.


    I''m not sure if there''s any interest in uploading the PyTorch / HF weights for
    Alpaca but I might.'
  created_at: 2023-03-30 13:56:01+00:00
  edited: true
  hidden: false
  id: 6425a30108e9e0ea8ccefefc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-03-30T21:53:45.000Z'
    data:
      edited: false
      editors:
      - wassname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: '<blockquote>

          <p>. The code is on my GitHub if you''re curious.</p>

          </blockquote>

          <p>Great I''ll take a look thanks.</p>

          <blockquote>

          <p>I''m not sure if there''s any interest in uploading the PyTorch / HF
          weights for Alpaca but I might.</p>

          </blockquote>

          <p>Oh those are the best and most compatible imo, I''m sure there is.</p>

          '
        raw: '> . The code is on my GitHub if you''re curious.


          Great I''ll take a look thanks.


          > I''m not sure if there''s any interest in uploading the PyTorch / HF weights
          for Alpaca but I might.


          Oh those are the best and most compatible imo, I''m sure there is.'
        updatedAt: '2023-03-30T21:53:45.890Z'
      numEdits: 0
      reactions: []
    id: 642604e9b9dfed28cf64d123
    type: comment
  author: wassname
  content: '> . The code is on my GitHub if you''re curious.


    Great I''ll take a look thanks.


    > I''m not sure if there''s any interest in uploading the PyTorch / HF weights
    for Alpaca but I might.


    Oh those are the best and most compatible imo, I''m sure there is.'
  created_at: 2023-03-30 20:53:45+00:00
  edited: false
  hidden: false
  id: 642604e9b9dfed28cf64d123
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-03-30T21:55:36.000Z'
    data:
      edited: false
      editors:
      - wassname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: '<p>It this your github btw? <a rel="nofollow" href="https://github.com/elinas">https://github.com/elinas</a>
          there don''t seem to be any public repos?</p>

          '
        raw: It this your github btw? https://github.com/elinas there don't seem to
          be any public repos?
        updatedAt: '2023-03-30T21:55:36.245Z'
      numEdits: 0
      reactions: []
    id: 64260558a545bd6b38e249f3
    type: comment
  author: wassname
  content: It this your github btw? https://github.com/elinas there don't seem to
    be any public repos?
  created_at: 2023-03-30 20:55:36+00:00
  edited: false
  hidden: false
  id: 64260558a545bd6b38e249f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-03-30T21:57:54.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>official-elinas, someone stole my name ; )</p>

          '
        raw: official-elinas, someone stole my name ; )
        updatedAt: '2023-03-30T21:57:54.889Z'
      numEdits: 0
      reactions: []
    id: 642605e203e27cca35e4d8f0
    type: comment
  author: elinas
  content: official-elinas, someone stole my name ; )
  created_at: 2023-03-30 20:57:54+00:00
  edited: false
  hidden: false
  id: 642605e203e27cca35e4d8f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-03-30T22:46:42.000Z'
    data:
      edited: true
      editors:
      - wassname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: '<p>Damn squatters!</p>

          <p>This is the script? Hmm very nice. <a rel="nofollow" href="https://github.com/official-elinas/alpaca-lora-30b/blob/main/export_state_dict_checkpoint.py">https://github.com/official-elinas/alpaca-lora-30b/blob/main/export_state_dict_checkpoint.py</a></p>

          <p>So you had to translate state dict names and unpermute some weights too.
          Thanks for sharing, it''s much better to do this than deal with the LoRA
          &lt;&gt; GPTQ &lt;&gt; text-webui incompatibilities</p>

          '
        raw: 'Damn squatters!


          This is the script? Hmm very nice. https://github.com/official-elinas/alpaca-lora-30b/blob/main/export_state_dict_checkpoint.py


          So you had to translate state dict names and unpermute some weights too.
          Thanks for sharing, it''s much better to do this than deal with the LoRA
          <> GPTQ <> text-webui incompatibilities'
        updatedAt: '2023-03-30T22:48:41.529Z'
      numEdits: 2
      reactions: []
    id: 6426115237a416bff53ecccf
    type: comment
  author: wassname
  content: 'Damn squatters!


    This is the script? Hmm very nice. https://github.com/official-elinas/alpaca-lora-30b/blob/main/export_state_dict_checkpoint.py


    So you had to translate state dict names and unpermute some weights too. Thanks
    for sharing, it''s much better to do this than deal with the LoRA <> GPTQ <> text-webui
    incompatibilities'
  created_at: 2023-03-30 21:46:42+00:00
  edited: true
  hidden: false
  id: 6426115237a416bff53ecccf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
      fullname: nacs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nacs
      type: user
    createdAt: '2023-03-31T05:44:15.000Z'
    data:
      edited: true
      editors:
      - nacs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
          fullname: nacs
          isHf: false
          isPro: false
          name: nacs
          type: user
        html: '<p>Edit: Disregard, found my answer above . :)</p>

          '
        raw: 'Edit: Disregard, found my answer above . :)'
        updatedAt: '2023-03-31T05:45:24.814Z'
      numEdits: 1
      reactions: []
    id: 6426732f4c529bd1d2035c5e
    type: comment
  author: nacs
  content: 'Edit: Disregard, found my answer above . :)'
  created_at: 2023-03-31 04:44:15+00:00
  edited: true
  hidden: false
  id: 6426732f4c529bd1d2035c5e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-04-05T01:00:18.000Z'
    data:
      edited: true
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: '<blockquote>

          <p>It''s a bit of a process, at least originally. I had to use a modified
          LoRA merging script which I modified for 30b, as it was already working
          on 13b. Then I converted the weights to PyTorch/HF (which I can use now
          to avoid the first step), and then I finally quantize the model. The code
          is on my GitHub if you''re curious.</p>

          <p>I''m not sure if there''s any interest in uploading the PyTorch / HF
          weights for Alpaca but I might.</p>

          </blockquote>

          <p>I''m sorry, i''m trying very hard to figure out how to 4bit quantize
          weights like you have been doing (I have been using your weights! thank
          you~) but I would like to learn how to do this myself for future releases
          of llama.</p>

          <p> Could you point me in the right direction for this? I am having trouble
          finding resources..</p>

          <p>edit: nvm, Im dumb, code for 4bit quantization is in the repo:<br><a
          rel="nofollow" href="https://github.com/qwopqwop200/GPTQ-for-LLaMa/">https://github.com/qwopqwop200/GPTQ-for-LLaMa/</a></p>

          '
        raw: "> It's a bit of a process, at least originally. I had to use a modified\
          \ LoRA merging script which I modified for 30b, as it was already working\
          \ on 13b. Then I converted the weights to PyTorch/HF (which I can use now\
          \ to avoid the first step), and then I finally quantize the model. The code\
          \ is on my GitHub if you're curious.\n> \n> I'm not sure if there's any\
          \ interest in uploading the PyTorch / HF weights for Alpaca but I might.\n\
          \nI'm sorry, i'm trying very hard to figure out how to 4bit quantize weights\
          \ like you have been doing (I have been using your weights! thank you~)\
          \ but I would like to learn how to do this myself for future releases of\
          \ llama.\n\n Could you point me in the right direction for this? I am having\
          \ trouble finding resources..\n\nedit: nvm, Im dumb, code for 4bit quantization\
          \ is in the repo:\nhttps://github.com/qwopqwop200/GPTQ-for-LLaMa/"
        updatedAt: '2023-04-05T04:22:01.788Z'
      numEdits: 1
      reactions: []
    id: 642cc82277fcbd9c33077a5d
    type: comment
  author: disarmyouwitha
  content: "> It's a bit of a process, at least originally. I had to use a modified\
    \ LoRA merging script which I modified for 30b, as it was already working on 13b.\
    \ Then I converted the weights to PyTorch/HF (which I can use now to avoid the\
    \ first step), and then I finally quantize the model. The code is on my GitHub\
    \ if you're curious.\n> \n> I'm not sure if there's any interest in uploading\
    \ the PyTorch / HF weights for Alpaca but I might.\n\nI'm sorry, i'm trying very\
    \ hard to figure out how to 4bit quantize weights like you have been doing (I\
    \ have been using your weights! thank you~) but I would like to learn how to do\
    \ this myself for future releases of llama.\n\n Could you point me in the right\
    \ direction for this? I am having trouble finding resources..\n\nedit: nvm, Im\
    \ dumb, code for 4bit quantization is in the repo:\nhttps://github.com/qwopqwop200/GPTQ-for-LLaMa/"
  created_at: 2023-04-05 00:00:18+00:00
  edited: true
  hidden: false
  id: 642cc82277fcbd9c33077a5d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: elinas/alpaca-13b-lora-int4
repo_type: model
status: open
target_branch: null
title: How is it different than other 4bit quants?
