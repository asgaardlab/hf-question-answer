!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KnutJaegersberg
conflicting_files: null
created_at: 2023-03-20 18:42:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-03-20T19:42:36.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>so you did apparently use lora weights in 4int quantization</p>

          '
        raw: so you did apparently use lora weights in 4int quantization
        updatedAt: '2023-03-20T19:42:36.826Z'
      numEdits: 0
      reactions: []
    id: 6418b72c0e48a6689b2635a1
    type: comment
  author: KnutJaegersberg
  content: so you did apparently use lora weights in 4int quantization
  created_at: 2023-03-20 18:42:36+00:00
  edited: false
  hidden: false
  id: 6418b72c0e48a6689b2635a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-03-20T20:57:09.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>Yes this uses LoRA as mentioned in the README. I am currently working
          on testing LoRA on 30B to see if there is an improvement. I will upload
          it in int4 if there is.</p>

          '
        raw: Yes this uses LoRA as mentioned in the README. I am currently working
          on testing LoRA on 30B to see if there is an improvement. I will upload
          it in int4 if there is.
        updatedAt: '2023-03-20T20:57:09.481Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - KnutJaegersberg
        - wassname
    id: 6418c8a5a18a83766296ab2b
    type: comment
  author: elinas
  content: Yes this uses LoRA as mentioned in the README. I am currently working on
    testing LoRA on 30B to see if there is an improvement. I will upload it in int4
    if there is.
  created_at: 2023-03-20 19:57:09+00:00
  edited: false
  hidden: false
  id: 6418c8a5a18a83766296ab2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-03-22T02:29:58.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>Have not done quantization before, but to my understanding there
          is a script which can properly be adjusted to work with any model that works
          with huggingface transformers. I think galactica 30b would also be very
          handy to have. Do you think it is possible to quantize? Does quantization
          happen in RAM or in VRAM?</p>

          '
        raw: Have not done quantization before, but to my understanding there is a
          script which can properly be adjusted to work with any model that works
          with huggingface transformers. I think galactica 30b would also be very
          handy to have. Do you think it is possible to quantize? Does quantization
          happen in RAM or in VRAM?
        updatedAt: '2023-03-22T02:29:58.107Z'
      numEdits: 0
      reactions: []
    id: 641a6826e2ed58073cd42c9d
    type: comment
  author: KnutJaegersberg
  content: Have not done quantization before, but to my understanding there is a script
    which can properly be adjusted to work with any model that works with huggingface
    transformers. I think galactica 30b would also be very handy to have. Do you think
    it is possible to quantize? Does quantization happen in RAM or in VRAM?
  created_at: 2023-03-22 01:29:58+00:00
  edited: false
  hidden: false
  id: 641a6826e2ed58073cd42c9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-03-22T02:32:16.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>Thinking the same about nllb-moe-54b</p>

          '
        raw: Thinking the same about nllb-moe-54b
        updatedAt: '2023-03-22T02:32:16.527Z'
      numEdits: 0
      reactions: []
    id: 641a68b0e249a69a8a245880
    type: comment
  author: KnutJaegersberg
  content: Thinking the same about nllb-moe-54b
  created_at: 2023-03-22 01:32:16+00:00
  edited: false
  hidden: false
  id: 641a68b0e249a69a8a245880
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-03-23T12:47:11.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>RAM and VRAM. Approx 90GB RAM at peak and 10GB vram (30b) but you
          can also use swap. If you link me those models I can look into them but
          if there''s no GPTQ conversion script I''d have to create my own.</p>

          '
        raw: RAM and VRAM. Approx 90GB RAM at peak and 10GB vram (30b) but you can
          also use swap. If you link me those models I can look into them but if there's
          no GPTQ conversion script I'd have to create my own.
        updatedAt: '2023-03-23T12:47:11.269Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - KnutJaegersberg
    id: 641c4a4fd1e1671e3f1805e9
    type: comment
  author: elinas
  content: RAM and VRAM. Approx 90GB RAM at peak and 10GB vram (30b) but you can also
    use swap. If you link me those models I can look into them but if there's no GPTQ
    conversion script I'd have to create my own.
  created_at: 2023-03-23 11:47:11+00:00
  edited: false
  hidden: false
  id: 641c4a4fd1e1671e3f1805e9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-03-23T20:35:49.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>Good to know, thanks.  That''s thus easily possible on a beefy workstation.<br>There
          is no script yet for them. As far as I understand, the script can be adjusted
          for huggingface / transformer lib models. </p>

          <p>The links are:<br><a href="https://huggingface.co/facebook/nllb-moe-54b">https://huggingface.co/facebook/nllb-moe-54b</a><br><a
          href="https://huggingface.co/facebook/galactica-30b">https://huggingface.co/facebook/galactica-30b</a></p>

          <p>The former is huge. Galactica would be very handy to fit in VRAM. Currently
          for a lot of setups that''s not possible.<br>nllb is more like an asset.
          Currently sota for translation work, but it is unusably huge.</p>

          '
        raw: "Good to know, thanks.  That's thus easily possible on a beefy workstation.\
          \ \nThere is no script yet for them. As far as I understand, the script\
          \ can be adjusted for huggingface / transformer lib models. \n\nThe links\
          \ are: \nhttps://huggingface.co/facebook/nllb-moe-54b\nhttps://huggingface.co/facebook/galactica-30b\n\
          \nThe former is huge. Galactica would be very handy to fit in VRAM. Currently\
          \ for a lot of setups that's not possible. \nnllb is more like an asset.\
          \ Currently sota for translation work, but it is unusably huge."
        updatedAt: '2023-03-23T20:35:49.191Z'
      numEdits: 0
      reactions: []
    id: 641cb8253a58d3b736a759ca
    type: comment
  author: KnutJaegersberg
  content: "Good to know, thanks.  That's thus easily possible on a beefy workstation.\
    \ \nThere is no script yet for them. As far as I understand, the script can be\
    \ adjusted for huggingface / transformer lib models. \n\nThe links are: \nhttps://huggingface.co/facebook/nllb-moe-54b\n\
    https://huggingface.co/facebook/galactica-30b\n\nThe former is huge. Galactica\
    \ would be very handy to fit in VRAM. Currently for a lot of setups that's not\
    \ possible. \nnllb is more like an asset. Currently sota for translation work,\
    \ but it is unusably huge."
  created_at: 2023-03-23 19:35:49+00:00
  edited: false
  hidden: false
  id: 641cb8253a58d3b736a759ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-03-23T21:07:52.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>If you modify the conversion script to support them (individually)
          then I can run it on my server. Don''t really have the time to dig into
          the scripts and changing them around right now.</p>

          '
        raw: If you modify the conversion script to support them (individually) then
          I can run it on my server. Don't really have the time to dig into the scripts
          and changing them around right now.
        updatedAt: '2023-03-23T21:07:52.607Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - KnutJaegersberg
    id: 641cbfa83a58d3b736a79a28
    type: comment
  author: elinas
  content: If you modify the conversion script to support them (individually) then
    I can run it on my server. Don't really have the time to dig into the scripts
    and changing them around right now.
  created_at: 2023-03-23 20:07:52+00:00
  edited: false
  hidden: false
  id: 641cbfa83a58d3b736a79a28
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: elinas/alpaca-13b-lora-int4
repo_type: model
status: open
target_branch: null
title: oh you were the guy
