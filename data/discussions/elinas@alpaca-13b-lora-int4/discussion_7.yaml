!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rikus2904
conflicting_files: null
created_at: 2023-04-29 04:26:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ecb12d9c13f76a95060716e276b18f6.svg
      fullname: Riku Soikkeli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rikus2904
      type: user
    createdAt: '2023-04-29T05:26:46.000Z'
    data:
      edited: false
      editors:
      - rikus2904
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ecb12d9c13f76a95060716e276b18f6.svg
          fullname: Riku Soikkeli
          isHf: false
          isPro: false
          name: rikus2904
          type: user
        html: '<p>I hope this is not a stupid question, but why doesn''t this repo
          contain the binaries like, for instance, elinas/llama-65b-hf-transformers-4.29
          does? How can one procure them?</p>

          '
        raw: I hope this is not a stupid question, but why doesn't this repo contain
          the binaries like, for instance, elinas/llama-65b-hf-transformers-4.29 does?
          How can one procure them?
        updatedAt: '2023-04-29T05:26:46.057Z'
      numEdits: 0
      reactions: []
    id: 644caa96cffa7d85ee16e42a
    type: comment
  author: rikus2904
  content: I hope this is not a stupid question, but why doesn't this repo contain
    the binaries like, for instance, elinas/llama-65b-hf-transformers-4.29 does? How
    can one procure them?
  created_at: 2023-04-29 04:26:46+00:00
  edited: false
  hidden: false
  id: 644caa96cffa7d85ee16e42a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-29T06:39:56.000Z'
    data:
      edited: true
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>You get the original llama weights through legitimate means then
          you run the convert to hf weights script which can be found in the tranformers
          GitHub repo. This does not contain them because I decided to upload a 4bit
          quantized version instead.</p>

          '
        raw: You get the original llama weights through legitimate means then you
          run the convert to hf weights script which can be found in the tranformers
          GitHub repo. This does not contain them because I decided to upload a 4bit
          quantized version instead.
        updatedAt: '2023-04-29T06:40:45.307Z'
      numEdits: 1
      reactions: []
    id: 644cbbbc0dc952d2459236c6
    type: comment
  author: elinas
  content: You get the original llama weights through legitimate means then you run
    the convert to hf weights script which can be found in the tranformers GitHub
    repo. This does not contain them because I decided to upload a 4bit quantized
    version instead.
  created_at: 2023-04-29 05:39:56+00:00
  edited: true
  hidden: false
  id: 644cbbbc0dc952d2459236c6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: elinas/alpaca-13b-lora-int4
repo_type: model
status: open
target_branch: null
title: Missing binaries?
