!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jphme
conflicting_files: []
created_at: 2023-11-30 00:10:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
      fullname: Jan Philipp Harries
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jphme
      type: user
    createdAt: '2023-11-30T00:10:01.000Z'
    data:
      edited: false
      editors:
      - jphme
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33631473779678345
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
          fullname: Jan Philipp Harries
          isHf: false
          isPro: false
          name: jphme
          type: user
        html: '<pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoTokenizer

          testtokenizer=AutoTokenizer.from_pretrained(<span class="hljs-string">"LeoLM/leo-mistral-hessianai-7b-chat"</span>)

          <span class="hljs-built_in">len</span>(testtokenizer)

          <span class="hljs-comment"># 32002</span>

          </code></pre>

          <p>Leads to e.g. VLLM error:<br><code>TypeError: argument ''tokens'': ''NoneType''
          object cannot be converted to ''PyString''</code><br>(see <a rel="nofollow"
          href="https://github.com/vllm-project/vllm/issues/516#issuecomment-1657507293">here</a>)</p>

          '
        raw: "```python\nfrom transformers import AutoTokenizer\ntesttokenizer=AutoTokenizer.from_pretrained(\"\
          LeoLM/leo-mistral-hessianai-7b-chat\")\nlen(testtokenizer)\n# 32002\n```\n\
          \nLeads to e.g. VLLM error:\n`TypeError: argument 'tokens': 'NoneType' object\
          \ cannot be converted to 'PyString'` \n(see [here](https://github.com/vllm-project/vllm/issues/516#issuecomment-1657507293\
          \ ))"
        updatedAt: '2023-11-30T00:10:01.265Z'
      numEdits: 0
      reactions: []
    id: 6567d2d95424dda4f06b838c
    type: comment
  author: jphme
  content: "```python\nfrom transformers import AutoTokenizer\ntesttokenizer=AutoTokenizer.from_pretrained(\"\
    LeoLM/leo-mistral-hessianai-7b-chat\")\nlen(testtokenizer)\n# 32002\n```\n\nLeads\
    \ to e.g. VLLM error:\n`TypeError: argument 'tokens': 'NoneType' object cannot\
    \ be converted to 'PyString'` \n(see [here](https://github.com/vllm-project/vllm/issues/516#issuecomment-1657507293\
    \ ))"
  created_at: 2023-11-30 00:10:01+00:00
  edited: false
  hidden: false
  id: 6567d2d95424dda4f06b838c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
      fullname: Jan Philipp Harries
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jphme
      type: user
    createdAt: '2023-11-30T00:10:02.000Z'
    data:
      oid: 966d6a51531d6982b88f16806de5681dc7e9fa44
      parents:
      - 1feda3dca3be5783d6d2fcc42fe4be776e3afaa1
      subject: fix vocab size
    id: 6567d2da0000000000000000
    type: commit
  author: jphme
  created_at: 2023-11-30 00:10:02+00:00
  id: 6567d2da0000000000000000
  oid: 966d6a51531d6982b88f16806de5681dc7e9fa44
  summary: fix vocab size
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-11-30T00:17:00.000Z'
    data:
      edited: false
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9556376338005066
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>Have you tested this? The model''s weights have 32128 embedding
          dim so I feel like this would break no?</p>

          '
        raw: Have you tested this? The model's weights have 32128 embedding dim so
          I feel like this would break no?
        updatedAt: '2023-11-30T00:17:00.611Z'
      numEdits: 0
      reactions: []
    id: 6567d47cbed5f64bbf7761e3
    type: comment
  author: bjoernp
  content: Have you tested this? The model's weights have 32128 embedding dim so I
    feel like this would break no?
  created_at: 2023-11-30 00:17:00+00:00
  edited: false
  hidden: false
  id: 6567d47cbed5f64bbf7761e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
      fullname: Jan Philipp Harries
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jphme
      type: user
    createdAt: '2023-11-30T11:59:00.000Z'
    data:
      edited: false
      editors:
      - jphme
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8773649334907532
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
          fullname: Jan Philipp Harries
          isHf: false
          isPro: false
          name: jphme
          type: user
        html: '<blockquote>

          <p>Have you tested this? The model''s weights have 32128 embedding dim so
          I feel like this would break no?</p>

          </blockquote>

          <p>No I didn''t test this and according to the docs you could be right <a
          href="https://huggingface.co/docs/transformers/main_classes/configuration">see
          here</a> .</p>

          <p>Does it work with VLLM for you? See also <a href="https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca/blob/main/config.json">example
          config.json</a> from OpenOrca for comparison. Probably related to <code>resize_token_embeddings_to_32x</code>
          (but why its not 32032 then?) .</p>

          <p>And seems to be an issue e.g. also here: <a rel="nofollow" href="https://github.com/huggingface/transformers/issues/4875">https://github.com/huggingface/transformers/issues/4875</a></p>

          <p>I have no idea what the right solution is or whether this is more a bug
          in VLLM; probably it would work to resize token embeddings after training
          again ( <code>model.resize_token_embeddings(embeddings_len)</code>) to get
          a match for usable vocab size and embeddings?</p>

          <p>Feel free to close, just wanted to make you aware of this issue :).</p>

          '
        raw: '> Have you tested this? The model''s weights have 32128 embedding dim
          so I feel like this would break no?


          No I didn''t test this and according to the docs you could be right [see
          here](https://huggingface.co/docs/transformers/main_classes/configuration)
          .


          Does it work with VLLM for you? See also [example config.json](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca/blob/main/config.json)
          from OpenOrca for comparison. Probably related to `resize_token_embeddings_to_32x`
          (but why its not 32032 then?) .


          And seems to be an issue e.g. also here: <https://github.com/huggingface/transformers/issues/4875>


          I have no idea what the right solution is or whether this is more a bug
          in VLLM; probably it would work to resize token embeddings after training
          again ( `model.resize_token_embeddings(embeddings_len)`) to get a match
          for usable vocab size and embeddings?


          Feel free to close, just wanted to make you aware of this issue :).

          '
        updatedAt: '2023-11-30T11:59:00.012Z'
      numEdits: 0
      reactions: []
    id: 6568790411b2bbd6c2e45b2f
    type: comment
  author: jphme
  content: '> Have you tested this? The model''s weights have 32128 embedding dim
    so I feel like this would break no?


    No I didn''t test this and according to the docs you could be right [see here](https://huggingface.co/docs/transformers/main_classes/configuration)
    .


    Does it work with VLLM for you? See also [example config.json](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca/blob/main/config.json)
    from OpenOrca for comparison. Probably related to `resize_token_embeddings_to_32x`
    (but why its not 32032 then?) .


    And seems to be an issue e.g. also here: <https://github.com/huggingface/transformers/issues/4875>


    I have no idea what the right solution is or whether this is more a bug in VLLM;
    probably it would work to resize token embeddings after training again ( `model.resize_token_embeddings(embeddings_len)`)
    to get a match for usable vocab size and embeddings?


    Feel free to close, just wanted to make you aware of this issue :).

    '
  created_at: 2023-11-30 11:59:00+00:00
  edited: false
  hidden: false
  id: 6568790411b2bbd6c2e45b2f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-11-30T12:06:16.000Z'
    data:
      edited: false
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9555946588516235
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>I think the real solution is to 1. raise an issue with vLLM and
          hope they fix it or 2. add dummy tokens to the tokenizer. I resized the
          embeddings to a multiple of 128 since this is what is apparently most efficient
          on h100+ GPUs. Your idea of resizing back down might also be a good and
          easy solution. I don''t think the speed loss should be too great.</p>

          '
        raw: I think the real solution is to 1. raise an issue with vLLM and hope
          they fix it or 2. add dummy tokens to the tokenizer. I resized the embeddings
          to a multiple of 128 since this is what is apparently most efficient on
          h100+ GPUs. Your idea of resizing back down might also be a good and easy
          solution. I don't think the speed loss should be too great.
        updatedAt: '2023-11-30T12:06:16.516Z'
      numEdits: 0
      reactions: []
    id: 65687ab826d6f74919f9a1f2
    type: comment
  author: bjoernp
  content: I think the real solution is to 1. raise an issue with vLLM and hope they
    fix it or 2. add dummy tokens to the tokenizer. I resized the embeddings to a
    multiple of 128 since this is what is apparently most efficient on h100+ GPUs.
    Your idea of resizing back down might also be a good and easy solution. I don't
    think the speed loss should be too great.
  created_at: 2023-11-30 12:06:16+00:00
  edited: false
  hidden: false
  id: 65687ab826d6f74919f9a1f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ce9c9f5b478e5065909f79/uW8TrrTR9esU9O8vs3aR5.png?w=200&h=200&f=face
      fullname: Thomas B
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BirdThomas
      type: user
    createdAt: '2023-12-07T21:27:57.000Z'
    data:
      edited: false
      editors:
      - BirdThomas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8517458438873291
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ce9c9f5b478e5065909f79/uW8TrrTR9esU9O8vs3aR5.png?w=200&h=200&f=face
          fullname: Thomas B
          isHf: false
          isPro: false
          name: BirdThomas
          type: user
        html: '<p>I am trying to convert the model to gguf, llama.cpp complains about
          a <code>Vocab size mismatch (model has 32128, but tokenizer.model has 32000).</code>
          (I removed all from <code>added_tokens.json</code>. I can sure "fix" the
          vocab_size in the config which will eventually lead to an error loading
          the model: <code>''token_embd.weight'' has wrong shape; expected  4096,
          32000, got  4096, 32128,</code><br>Any ideas?</p>

          '
        raw: 'I am trying to convert the model to gguf, llama.cpp complains about
          a `Vocab size mismatch (model has 32128, but tokenizer.model has 32000).`
          (I removed all from `added_tokens.json`. I can sure "fix" the vocab_size
          in the config which will eventually lead to an error loading the model:
          `''token_embd.weight'' has wrong shape; expected  4096, 32000, got  4096,
          32128,`

          Any ideas?'
        updatedAt: '2023-12-07T21:27:57.217Z'
      numEdits: 0
      reactions: []
    id: 657238dd66ea46cd86aa2f37
    type: comment
  author: BirdThomas
  content: 'I am trying to convert the model to gguf, llama.cpp complains about a
    `Vocab size mismatch (model has 32128, but tokenizer.model has 32000).` (I removed
    all from `added_tokens.json`. I can sure "fix" the vocab_size in the config which
    will eventually lead to an error loading the model: `''token_embd.weight'' has
    wrong shape; expected  4096, 32000, got  4096, 32128,`

    Any ideas?'
  created_at: 2023-12-07 21:27:57+00:00
  edited: false
  hidden: false
  id: 657238dd66ea46cd86aa2f37
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 4
repo_id: LeoLM/leo-mistral-hessianai-7b-chat
repo_type: model
status: open
target_branch: refs/heads/main
title: fix vocab size
