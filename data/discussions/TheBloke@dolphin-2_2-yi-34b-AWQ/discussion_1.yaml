!!python/object:huggingface_hub.community.DiscussionWithDetails
author: iChrist
conflicting_files: null
created_at: 2023-11-14 10:31:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6ca84cb252615dd24f7034a52885d61.svg
      fullname: Osher Ahron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iChrist
      type: user
    createdAt: '2023-11-14T10:31:53.000Z'
    data:
      edited: false
      editors:
      - iChrist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.26139047741889954
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6ca84cb252615dd24f7034a52885d61.svg
          fullname: Osher Ahron
          isHf: false
          isPro: false
          name: iChrist
          type: user
        html: "<p>I have a functional oobabooga install, with GPTQ working great.<br>Tried\
          \ to run this model, installed from the model tab, and I am getting this\
          \ error:</p>\n<p>2023-11-14 12:27:30 INFO:Loading TheBloke_dolphin-2_2-yi-34b-AWQ...<br>Replacing\
          \ layers...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 60/60 [00:05&lt;00:00, 10.25it/s]<br>Fusing layers...:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 60/60 [00:07&lt;00:00,  7.81it/s]<br>2023-11-14\
          \ 12:27:51 ERROR:Failed to load the model.<br>Traceback (most recent call\
          \ last):<br>  File \"D:\\TextGen\\modules\\ui_model_menu.py\", line 210,\
          \ in load_model_wrapper<br>    shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader)<br>                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>\
          \  File \"D:\\TextGen\\modules\\models.py\", line 93, in load_model<br>\
          \    tokenizer = load_tokenizer(model_name, model)<br>                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>\
          \  File \"D:\\TextGen\\modules\\models.py\", line 113, in load_tokenizer<br>\
          \    tokenizer = AutoTokenizer.from_pretrained(<br>                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>\
          \  File \"D:\\TextGen\\installer_files\\env\\Lib\\site-packages\\transformers\\\
          models\\auto\\tokenization_auto.py\", line 765, in from_pretrained<br> \
          \   raise ValueError(<br>ValueError: Tokenizer class YiTokenizer does not\
          \ exist or is not currently imported.</p>\n"
        raw: "I have a functional oobabooga install, with GPTQ working great.\r\n\
          Tried to run this model, installed from the model tab, and I am getting\
          \ this error:\r\n\r\n2023-11-14 12:27:30 INFO:Loading TheBloke_dolphin-2_2-yi-34b-AWQ...\r\
          \nReplacing layers...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 60/60 [00:05<00:00, 10.25it/s]\r\nFusing\
          \ layers...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:07<00:00,  7.81it/s]\r\
          \n2023-11-14 12:27:51 ERROR:Failed to load the model.\r\nTraceback (most\
          \ recent call last):\r\n  File \"D:\\TextGen\\modules\\ui_model_menu.py\"\
          , line 210, in load_model_wrapper\r\n    shared.model, shared.tokenizer\
          \ = load_model(shared.model_name, loader)\r\n                          \
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\TextGen\\\
          modules\\models.py\", line 93, in load_model\r\n    tokenizer = load_tokenizer(model_name,\
          \ model)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File\
          \ \"D:\\TextGen\\modules\\models.py\", line 113, in load_tokenizer\r\n \
          \   tokenizer = AutoTokenizer.from_pretrained(\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"D:\\TextGen\\installer_files\\env\\Lib\\site-packages\\transformers\\\
          models\\auto\\tokenization_auto.py\", line 765, in from_pretrained\r\n \
          \   raise ValueError(\r\nValueError: Tokenizer class YiTokenizer does not\
          \ exist or is not currently imported.\r\n"
        updatedAt: '2023-11-14T10:31:53.112Z'
      numEdits: 0
      reactions: []
    id: 65534c99b7eacd21c3f10362
    type: comment
  author: iChrist
  content: "I have a functional oobabooga install, with GPTQ working great.\r\nTried\
    \ to run this model, installed from the model tab, and I am getting this error:\r\
    \n\r\n2023-11-14 12:27:30 INFO:Loading TheBloke_dolphin-2_2-yi-34b-AWQ...\r\n\
    Replacing layers...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    | 60/60 [00:05<00:00, 10.25it/s]\r\nFusing layers...: 100%|\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:07<00:00,  7.81it/s]\r\
    \n2023-11-14 12:27:51 ERROR:Failed to load the model.\r\nTraceback (most recent\
    \ call last):\r\n  File \"D:\\TextGen\\modules\\ui_model_menu.py\", line 210,\
    \ in load_model_wrapper\r\n    shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\r\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"D:\\TextGen\\modules\\models.py\", line 93, in load_model\r\n    tokenizer\
    \ = load_tokenizer(model_name, model)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"D:\\TextGen\\modules\\models.py\", line 113, in load_tokenizer\r\n\
    \    tokenizer = AutoTokenizer.from_pretrained(\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"D:\\TextGen\\installer_files\\env\\Lib\\site-packages\\transformers\\\
    models\\auto\\tokenization_auto.py\", line 765, in from_pretrained\r\n    raise\
    \ ValueError(\r\nValueError: Tokenizer class YiTokenizer does not exist or is\
    \ not currently imported.\r\n"
  created_at: 2023-11-14 10:31:53+00:00
  edited: false
  hidden: false
  id: 65534c99b7eacd21c3f10362
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6ca84cb252615dd24f7034a52885d61.svg
      fullname: Osher Ahron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iChrist
      type: user
    createdAt: '2023-11-14T11:54:27.000Z'
    data:
      edited: false
      editors:
      - iChrist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.770844042301178
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6ca84cb252615dd24f7034a52885d61.svg
          fullname: Osher Ahron
          isHf: false
          isPro: false
          name: iChrist
          type: user
        html: '<p>Also tried taking the original tokenizer from <a href="https://huggingface.co/ehartford/dolphin-2_2-yi-34b/resolve/main/tokenization_yi.py">https://huggingface.co/ehartford/dolphin-2_2-yi-34b/resolve/main/tokenization_yi.py</a><br>And
          tried using the GPTQ tokenization_yi.py file and it didnt help (GPTQ works
          fine tho)</p>

          '
        raw: "Also tried taking the original tokenizer from https://huggingface.co/ehartford/dolphin-2_2-yi-34b/resolve/main/tokenization_yi.py\
          \ \nAnd tried using the GPTQ tokenization_yi.py file and it didnt help (GPTQ\
          \ works fine tho)"
        updatedAt: '2023-11-14T11:54:27.250Z'
      numEdits: 0
      reactions: []
    id: 65535ff370fb926b75ce828d
    type: comment
  author: iChrist
  content: "Also tried taking the original tokenizer from https://huggingface.co/ehartford/dolphin-2_2-yi-34b/resolve/main/tokenization_yi.py\
    \ \nAnd tried using the GPTQ tokenization_yi.py file and it didnt help (GPTQ works\
    \ fine tho)"
  created_at: 2023-11-14 11:54:27+00:00
  edited: false
  hidden: false
  id: 65535ff370fb926b75ce828d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a3bb1cd0d8c2c2169f0b88/eT2TS0IlQbZtz-F_zHLz9.jpeg?w=200&h=200&f=face
      fullname: Joseph Pollack
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tonic
      type: user
    createdAt: '2023-11-14T12:20:23.000Z'
    data:
      edited: false
      editors:
      - Tonic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9345794320106506
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a3bb1cd0d8c2c2169f0b88/eT2TS0IlQbZtz-F_zHLz9.jpeg?w=200&h=200&f=face
          fullname: Joseph Pollack
          isHf: false
          isPro: false
          name: Tonic
          type: user
        html: '<p>made a demo with the original model, but had the same problem which
          i solved &amp; you can find  it here : <a href="https://huggingface.co/spaces/Tonic1/YiTonic/tree/main">https://huggingface.co/spaces/Tonic1/YiTonic/tree/main</a>
          just check how the tokenizer issue is handled if you like</p>

          '
        raw: 'made a demo with the original model, but had the same problem which
          i solved & you can find  it here : https://huggingface.co/spaces/Tonic1/YiTonic/tree/main
          just check how the tokenizer issue is handled if you like'
        updatedAt: '2023-11-14T12:20:23.209Z'
      numEdits: 0
      reactions: []
    id: 655366078b10d88b7ffe668b
    type: comment
  author: Tonic
  content: 'made a demo with the original model, but had the same problem which i
    solved & you can find  it here : https://huggingface.co/spaces/Tonic1/YiTonic/tree/main
    just check how the tokenizer issue is handled if you like'
  created_at: 2023-11-14 12:20:23+00:00
  edited: false
  hidden: false
  id: 655366078b10d88b7ffe668b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6ca84cb252615dd24f7034a52885d61.svg
      fullname: Osher Ahron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iChrist
      type: user
    createdAt: '2023-11-14T12:30:45.000Z'
    data:
      edited: false
      editors:
      - iChrist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9311349987983704
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6ca84cb252615dd24f7034a52885d61.svg
          fullname: Osher Ahron
          isHf: false
          isPro: false
          name: iChrist
          type: user
        html: '<blockquote>

          <p>made a demo with the original model, but had the same problem which i
          solved &amp; you can find  it here : <a href="https://huggingface.co/spaces/Tonic1/YiTonic/tree/main">https://huggingface.co/spaces/Tonic1/YiTonic/tree/main</a>
          just check how the tokenizer issue is handled if you like</p>

          </blockquote>

          <p>Thanks for the help!<br>Should I also pip install -r and your requirement
          file? It will downgrade to cu113?</p>

          '
        raw: '> made a demo with the original model, but had the same problem which
          i solved & you can find  it here : https://huggingface.co/spaces/Tonic1/YiTonic/tree/main
          just check how the tokenizer issue is handled if you like


          Thanks for the help!

          Should I also pip install -r and your requirement file? It will downgrade
          to cu113?'
        updatedAt: '2023-11-14T12:30:45.206Z'
      numEdits: 0
      reactions: []
    id: 6553687598c313bb549a63a0
    type: comment
  author: iChrist
  content: '> made a demo with the original model, but had the same problem which
    i solved & you can find  it here : https://huggingface.co/spaces/Tonic1/YiTonic/tree/main
    just check how the tokenizer issue is handled if you like


    Thanks for the help!

    Should I also pip install -r and your requirement file? It will downgrade to cu113?'
  created_at: 2023-11-14 12:30:45+00:00
  edited: false
  hidden: false
  id: 6553687598c313bb549a63a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6ca84cb252615dd24f7034a52885d61.svg
      fullname: Osher Ahron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iChrist
      type: user
    createdAt: '2023-11-14T12:35:08.000Z'
    data:
      edited: false
      editors:
      - iChrist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4428420662879944
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6ca84cb252615dd24f7034a52885d61.svg
          fullname: Osher Ahron
          isHf: false
          isPro: false
          name: iChrist
          type: user
        html: '<p>Tried to copy your files and got this error:</p>

          <p>Traceback (most recent call last):</p>

          <p>File "D:\TextGen\modules\ui_model_menu.py", line 210, in load_model_wrapper</p>

          <p>shared.model, shared.tokenizer = load_model(shared.model_name, loader)</p>

          <pre><code>                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "D:\TextGen\modules\models.py", line 85, in load_model</p>

          <p>output = load_func_map<a rel="nofollow" href="model_name">loader</a></p>

          <pre><code>     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "D:\TextGen\modules\models.py", line 299, in AutoAWQ_loader</p>

          <p>from awq import AutoAWQForCausalLM<br>File "D:\TextGen\installer_files\env\Lib\site-packages\awq_init_.py",
          line 2, in</p>

          <p>from awq.models.auto import AutoAWQForCausalLM<br>File "D:\TextGen\installer_files\env\Lib\site-packages\awq\models_init_.py",
          line 1, in</p>

          <p>from .mpt import MptAWQForCausalLM<br>File "D:\TextGen\installer_files\env\Lib\site-packages\awq\models\mpt.py",
          line 1, in</p>

          <p>from .base import BaseAWQForCausalLM<br>File "D:\TextGen\installer_files\env\Lib\site-packages\awq\models\base.py",
          line 12, in</p>

          <p>from awq.quantize.quantizer import AwqQuantizer<br>File "D:\TextGen\installer_files\env\Lib\site-packages\awq\quantize\quantizer.py",
          line 11, in</p>

          <p>from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV<br>File "D:\TextGen\installer_files\env\Lib\site-packages\awq\modules\linear.py",
          line 4, in</p>

          <p>import awq_inference_engine  # with CUDA kernels</p>

          <p>^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>ImportError: DLL load failed while importing
          awq_inference_engine: The specified module could not be found.</p>

          '
        raw: "Tried to copy your files and got this error:\n\nTraceback (most recent\
          \ call last):\n\nFile \"D:\\TextGen\\modules\\ui_model_menu.py\", line 210,\
          \ in load_model_wrapper\n\n\nshared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader)\n\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"D:\\TextGen\\modules\\models.py\", line 85, in load_model\n\n\noutput\
          \ = load_func_map[loader](model_name)\n\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"D:\\TextGen\\modules\\models.py\", line 299, in AutoAWQ_loader\n\n\
          \nfrom awq import AutoAWQForCausalLM\nFile \"D:\\TextGen\\installer_files\\\
          env\\Lib\\site-packages\\awq_init_.py\", line 2, in\n\n\nfrom awq.models.auto\
          \ import AutoAWQForCausalLM\nFile \"D:\\TextGen\\installer_files\\env\\\
          Lib\\site-packages\\awq\\models_init_.py\", line 1, in\n\n\nfrom .mpt import\
          \ MptAWQForCausalLM\nFile \"D:\\TextGen\\installer_files\\env\\Lib\\site-packages\\\
          awq\\models\\mpt.py\", line 1, in\n\n\nfrom .base import BaseAWQForCausalLM\n\
          File \"D:\\TextGen\\installer_files\\env\\Lib\\site-packages\\awq\\models\\\
          base.py\", line 12, in\n\n\nfrom awq.quantize.quantizer import AwqQuantizer\n\
          File \"D:\\TextGen\\installer_files\\env\\Lib\\site-packages\\awq\\quantize\\\
          quantizer.py\", line 11, in\n\n\nfrom awq.modules.linear import WQLinear_GEMM,\
          \ WQLinear_GEMV\nFile \"D:\\TextGen\\installer_files\\env\\Lib\\site-packages\\\
          awq\\modules\\linear.py\", line 4, in\n\n\nimport awq_inference_engine \
          \ # with CUDA kernels\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nImportError: DLL load\
          \ failed while importing awq_inference_engine: The specified module could\
          \ not be found."
        updatedAt: '2023-11-14T12:35:08.782Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6553697c0c87b30938c29e5f
    id: 6553697c0c87b30938c29e5d
    type: comment
  author: iChrist
  content: "Tried to copy your files and got this error:\n\nTraceback (most recent\
    \ call last):\n\nFile \"D:\\TextGen\\modules\\ui_model_menu.py\", line 210, in\
    \ load_model_wrapper\n\n\nshared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\n\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \"D:\\TextGen\\modules\\models.py\", line 85, in load_model\n\n\noutput =\
    \ load_func_map[loader](model_name)\n\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \"D:\\TextGen\\modules\\models.py\", line 299, in AutoAWQ_loader\n\n\nfrom\
    \ awq import AutoAWQForCausalLM\nFile \"D:\\TextGen\\installer_files\\env\\Lib\\\
    site-packages\\awq_init_.py\", line 2, in\n\n\nfrom awq.models.auto import AutoAWQForCausalLM\n\
    File \"D:\\TextGen\\installer_files\\env\\Lib\\site-packages\\awq\\models_init_.py\"\
    , line 1, in\n\n\nfrom .mpt import MptAWQForCausalLM\nFile \"D:\\TextGen\\installer_files\\\
    env\\Lib\\site-packages\\awq\\models\\mpt.py\", line 1, in\n\n\nfrom .base import\
    \ BaseAWQForCausalLM\nFile \"D:\\TextGen\\installer_files\\env\\Lib\\site-packages\\\
    awq\\models\\base.py\", line 12, in\n\n\nfrom awq.quantize.quantizer import AwqQuantizer\n\
    File \"D:\\TextGen\\installer_files\\env\\Lib\\site-packages\\awq\\quantize\\\
    quantizer.py\", line 11, in\n\n\nfrom awq.modules.linear import WQLinear_GEMM,\
    \ WQLinear_GEMV\nFile \"D:\\TextGen\\installer_files\\env\\Lib\\site-packages\\\
    awq\\modules\\linear.py\", line 4, in\n\n\nimport awq_inference_engine  # with\
    \ CUDA kernels\n\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nImportError: DLL load failed while\
    \ importing awq_inference_engine: The specified module could not be found."
  created_at: 2023-11-14 12:35:08+00:00
  edited: false
  hidden: false
  id: 6553697c0c87b30938c29e5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d6ca84cb252615dd24f7034a52885d61.svg
      fullname: Osher Ahron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iChrist
      type: user
    createdAt: '2023-11-14T12:35:08.000Z'
    data:
      status: closed
    id: 6553697c0c87b30938c29e5f
    type: status-change
  author: iChrist
  created_at: 2023-11-14 12:35:08+00:00
  id: 6553697c0c87b30938c29e5f
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d6ca84cb252615dd24f7034a52885d61.svg
      fullname: Osher Ahron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iChrist
      type: user
    createdAt: '2023-11-14T12:46:12.000Z'
    data:
      status: open
    id: 65536c140c87b30938c31bf9
    type: status-change
  author: iChrist
  created_at: 2023-11-14 12:46:12+00:00
  id: 65536c140c87b30938c31bf9
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-14T12:53:40.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8117886185646057
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Make sure you load with trust_remote_code=True</p>

          '
        raw: Make sure you load with trust_remote_code=True
        updatedAt: '2023-11-14T12:53:40.681Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F917"
        users:
        - Tonic
        - Mandelion
      - count: 2
        reaction: "\U0001F44D"
        users:
        - marinl
        - Beck777
    id: 65536dd44ff33a278c928dd3
    type: comment
  author: TheBloke
  content: Make sure you load with trust_remote_code=True
  created_at: 2023-11-14 12:53:40+00:00
  edited: false
  hidden: false
  id: 65536dd44ff33a278c928dd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/63cc30d35faa72e4092d776f9f64b180.svg
      fullname: Mike Shafia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mshafia
      type: user
    createdAt: '2023-11-14T14:25:01.000Z'
    data:
      edited: false
      editors:
      - mshafia
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2813243269920349
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/63cc30d35faa72e4092d776f9f64b180.svg
          fullname: Mike Shafia
          isHf: false
          isPro: false
          name: mshafia
          type: user
        html: '<p>I get this error when I try to load the model:</p>

          <p>File "/Downloads/text-generation-webui/modules/models.py", line 85, in
          load_model<br>output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>File
          "/Downloads/text-generation-webui/modules/models.py", line 299, in AutoAWQ_loader<br>from
          awq import AutoAWQForCausalLM<br>ModuleNotFoundError: No module named ''awq''</p>

          '
        raw: "I get this error when I try to load the model:\n\nFile \"/Downloads/text-generation-webui/modules/models.py\"\
          , line 85, in load_model\noutput = load_func_map[loader](model_name)\n \
          \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Downloads/text-generation-webui/modules/models.py\"\
          , line 299, in AutoAWQ_loader\nfrom awq import AutoAWQForCausalLM\nModuleNotFoundError:\
          \ No module named 'awq'"
        updatedAt: '2023-11-14T14:25:01.668Z'
      numEdits: 0
      reactions: []
    id: 6553833d50a52839fe7ee09c
    type: comment
  author: mshafia
  content: "I get this error when I try to load the model:\n\nFile \"/Downloads/text-generation-webui/modules/models.py\"\
    , line 85, in load_model\noutput = load_func_map[loader](model_name)\n       \
    \  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Downloads/text-generation-webui/modules/models.py\"\
    , line 299, in AutoAWQ_loader\nfrom awq import AutoAWQForCausalLM\nModuleNotFoundError:\
    \ No module named 'awq'"
  created_at: 2023-11-14 14:25:01+00:00
  edited: false
  hidden: false
  id: 6553833d50a52839fe7ee09c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6ca84cb252615dd24f7034a52885d61.svg
      fullname: Osher Ahron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iChrist
      type: user
    createdAt: '2023-11-14T16:33:54.000Z'
    data:
      edited: false
      editors:
      - iChrist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9085070490837097
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6ca84cb252615dd24f7034a52885d61.svg
          fullname: Osher Ahron
          isHf: false
          isPro: false
          name: iChrist
          type: user
        html: '<blockquote>

          <p>Make sure you load with trust_remote_code=True</p>

          </blockquote>

          <p>Thanks, it helped.<br>But for some reason the GPTQ version of this model
          gives much better results, while this AWQ version gives random words in
          repeat.<br>Same template ChatML same context.<br>And, it seems the GPTQ
          is slightly faster..</p>

          '
        raw: '> Make sure you load with trust_remote_code=True


          Thanks, it helped.

          But for some reason the GPTQ version of this model gives much better results,
          while this AWQ version gives random words in repeat.

          Same template ChatML same context.

          And, it seems the GPTQ is slightly faster..'
        updatedAt: '2023-11-14T16:33:54.871Z'
      numEdits: 0
      reactions: []
    id: 6553a1728771298696b1cc5e
    type: comment
  author: iChrist
  content: '> Make sure you load with trust_remote_code=True


    Thanks, it helped.

    But for some reason the GPTQ version of this model gives much better results,
    while this AWQ version gives random words in repeat.

    Same template ChatML same context.

    And, it seems the GPTQ is slightly faster..'
  created_at: 2023-11-14 16:33:54+00:00
  edited: false
  hidden: false
  id: 6553a1728771298696b1cc5e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da279c726101edbbf98bafc289bad25d.svg
      fullname: Marijn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marinl
      type: user
    createdAt: '2023-11-15T14:42:20.000Z'
    data:
      edited: false
      editors:
      - marinl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.954695463180542
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da279c726101edbbf98bafc289bad25d.svg
          fullname: Marijn
          isHf: false
          isPro: false
          name: marinl
          type: user
        html: '<blockquote>

          <p>But for some reason the GPTQ version of this model gives much better
          results, while this AWQ version gives random words in repeat.<br>Same template
          ChatML same context.<br>And, it seems the GPTQ is slightly faster..</p>

          </blockquote>

          <p>I have had this experience as well, with multiple models... Maybe AutoAWQ
          needs some tweaking?</p>

          '
        raw: "> But for some reason the GPTQ version of this model gives much better\
          \ results, while this AWQ version gives random words in repeat.\n> Same\
          \ template ChatML same context.\n> And, it seems the GPTQ is slightly faster..\n\
          \ \nI have had this experience as well, with multiple models... Maybe AutoAWQ\
          \ needs some tweaking?"
        updatedAt: '2023-11-15T14:42:20.062Z'
      numEdits: 0
      reactions: []
    id: 6554d8cc4e5c205097e20070
    type: comment
  author: marinl
  content: "> But for some reason the GPTQ version of this model gives much better\
    \ results, while this AWQ version gives random words in repeat.\n> Same template\
    \ ChatML same context.\n> And, it seems the GPTQ is slightly faster..\n \nI have\
    \ had this experience as well, with multiple models... Maybe AutoAWQ needs some\
    \ tweaking?"
  created_at: 2023-11-15 14:42:20+00:00
  edited: false
  hidden: false
  id: 6554d8cc4e5c205097e20070
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/182b1994ad37ed23d8a066caeaef83d5.svg
      fullname: dario
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prudant
      type: user
    createdAt: '2023-12-20T21:08:23.000Z'
    data:
      edited: false
      editors:
      - prudant
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4159557819366455
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/182b1994ad37ed23d8a066caeaef83d5.svg
          fullname: dario
          isHf: false
          isPro: false
          name: prudant
          type: user
        html: '<p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)</p>

          '
        raw: 'tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)

          '
        updatedAt: '2023-12-20T21:08:23.235Z'
      numEdits: 0
      reactions: []
    id: 658357c76064483c600fa0dd
    type: comment
  author: prudant
  content: 'tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)

    '
  created_at: 2023-12-20 21:08:23+00:00
  edited: false
  hidden: false
  id: 658357c76064483c600fa0dd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/dolphin-2_2-yi-34b-AWQ
repo_type: model
status: open
target_branch: null
title: YiTokenizer does not exist or is not currently imported.
