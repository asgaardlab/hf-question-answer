!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Vulonkul
conflicting_files: null
created_at: 2023-04-02 14:34:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a4895656abd2a09f39cc64ee47c055e0.svg
      fullname: Atanas Valkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vulonkul
      type: user
    createdAt: '2023-04-02T15:34:57.000Z'
    data:
      edited: false
      editors:
      - Vulonkul
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a4895656abd2a09f39cc64ee47c055e0.svg
          fullname: Atanas Valkov
          isHf: false
          isPro: false
          name: Vulonkul
          type: user
        html: "<p>I'm not really looking for a tutorial, but anything I try leads\
          \ to conversations like this:<br>[ME] [Real user input]<br>[Char] [Generated\
          \ Answer]<br>[ME] [Bot pretending to continue the conversation from the\
          \ generated answer]<br>[Char] [etc.]<br>Until it gets bored. Here is my\
          \ formatting, since I assume that is the issue.</p>\n<p>persona = \"AI-chan's\
          \ persona: AI-chan is a cheerful person who loves to make others smile.\
          \ She is an optimist who loves to spread happiness and positivity wherever\
          \ she goes.\"<br>history = [\"AI-chan: What's your name? You: My name is\
          \ Creator!\", \"AI-chan: Nice to meet you Creator! You: Nice to meet you\
          \ too AI-chan!\", \"AI-chan: What shall we talk about? You: I don't know,\
          \ you tell me...\", \" AI-chan: We can talk about anime. You: Ok, what's\
          \ your favourite anime?\"]<br>user_input = input(\"You: \")<br>response\
          \ = get_answer(user_input, history, persona)</p>\n<p>def get_answer(user_input,\
          \ history, persona):<br>    # Combine persona and history into one string<br>\
          \    user_input = (\"You: \" + user_input)<br>    history.append(user_input)<br>\
          \    input_text = persona + \" \".join(history) + \" AI-chan: \"</p>\n<pre><code>#\
          \ encode context the generation is conditioned on\ninput_ids = tokenizer.encode(input_text,\
          \ truncation=True, add_special_tokens=True, return_tensors='pt')\n\n   \
          \ output = model.generate(input_ids=input_ids, max_length=2048, pad_token_id=tokenizer.eos_token_id,\
          \ \n        temperature=0.7,\n        max_new_tokens=500,\n        repetition_penalty=1.2,\n\
          \        do_sample=True,\n        top_k=50,\n        top_p=0.8)\n</code></pre>\n\
          <p>I'd really appreciate if someone who has faced this issue can say how\
          \ they've delt with this while I continue to experiment. I will update this\
          \ if I get an answer!</p>\n"
        raw: "I'm not really looking for a tutorial, but anything I try leads to conversations\
          \ like this:\r\n[ME] [Real user input]\r\n[Char] [Generated Answer]\r\n\
          [ME] [Bot pretending to continue the conversation from the generated answer]\r\
          \n[Char] [etc.]\r\nUntil it gets bored. Here is my formatting, since I assume\
          \ that is the issue.\r\n\r\npersona = \"AI-chan's persona: AI-chan is a\
          \ cheerful person who loves to make others smile. She is an optimist who\
          \ loves to spread happiness and positivity wherever she goes.<START>\"\r\
          \nhistory = [\"AI-chan: What's your name? You: My name is Creator!\", \"\
          AI-chan: Nice to meet you Creator! You: Nice to meet you too AI-chan!\"\
          , \"AI-chan: What shall we talk about? You: I don't know, you tell me...\"\
          , \" AI-chan: We can talk about anime. You: Ok, what's your favourite anime?\"\
          ]\r\nuser_input = input(\"You: \")\r\nresponse = get_answer(user_input,\
          \ history, persona)\r\n\r\ndef get_answer(user_input, history, persona):\r\
          \n    # Combine persona and history into one string\r\n    user_input =\
          \ (\"You: \" + user_input)\r\n    history.append(user_input)\r\n    input_text\
          \ = persona + \" \".join(history) + \" AI-chan: \"\r\n    \r\n    # encode\
          \ context the generation is conditioned on\r\n    input_ids = tokenizer.encode(input_text,\
          \ truncation=True, add_special_tokens=True, return_tensors='pt')\r\n\r\n\
          \        output = model.generate(input_ids=input_ids, max_length=2048, pad_token_id=tokenizer.eos_token_id,\
          \ \r\n            temperature=0.7,\r\n            max_new_tokens=500,\r\n\
          \            repetition_penalty=1.2,\r\n            do_sample=True,\r\n\
          \            top_k=50,\r\n            top_p=0.8)\r\n \r\nI'd really appreciate\
          \ if someone who has faced this issue can say how they've delt with this\
          \ while I continue to experiment. I will update this if I get an answer!"
        updatedAt: '2023-04-02T15:34:57.859Z'
      numEdits: 0
      reactions: []
    id: 6429a0a18136224fee081332
    type: comment
  author: Vulonkul
  content: "I'm not really looking for a tutorial, but anything I try leads to conversations\
    \ like this:\r\n[ME] [Real user input]\r\n[Char] [Generated Answer]\r\n[ME] [Bot\
    \ pretending to continue the conversation from the generated answer]\r\n[Char]\
    \ [etc.]\r\nUntil it gets bored. Here is my formatting, since I assume that is\
    \ the issue.\r\n\r\npersona = \"AI-chan's persona: AI-chan is a cheerful person\
    \ who loves to make others smile. She is an optimist who loves to spread happiness\
    \ and positivity wherever she goes.<START>\"\r\nhistory = [\"AI-chan: What's your\
    \ name? You: My name is Creator!\", \"AI-chan: Nice to meet you Creator! You:\
    \ Nice to meet you too AI-chan!\", \"AI-chan: What shall we talk about? You: I\
    \ don't know, you tell me...\", \" AI-chan: We can talk about anime. You: Ok,\
    \ what's your favourite anime?\"]\r\nuser_input = input(\"You: \")\r\nresponse\
    \ = get_answer(user_input, history, persona)\r\n\r\ndef get_answer(user_input,\
    \ history, persona):\r\n    # Combine persona and history into one string\r\n\
    \    user_input = (\"You: \" + user_input)\r\n    history.append(user_input)\r\
    \n    input_text = persona + \" \".join(history) + \" AI-chan: \"\r\n    \r\n\
    \    # encode context the generation is conditioned on\r\n    input_ids = tokenizer.encode(input_text,\
    \ truncation=True, add_special_tokens=True, return_tensors='pt')\r\n\r\n     \
    \   output = model.generate(input_ids=input_ids, max_length=2048, pad_token_id=tokenizer.eos_token_id,\
    \ \r\n            temperature=0.7,\r\n            max_new_tokens=500,\r\n    \
    \        repetition_penalty=1.2,\r\n            do_sample=True,\r\n          \
    \  top_k=50,\r\n            top_p=0.8)\r\n \r\nI'd really appreciate if someone\
    \ who has faced this issue can say how they've delt with this while I continue\
    \ to experiment. I will update this if I get an answer!"
  created_at: 2023-04-02 14:34:57+00:00
  edited: false
  hidden: false
  id: 6429a0a18136224fee081332
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
      fullname: '0x000011b'
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: 11b
      type: user
    createdAt: '2023-04-02T17:22:19.000Z'
    data:
      edited: false
      editors:
      - 11b
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
          fullname: '0x000011b'
          isHf: false
          isPro: false
          name: 11b
          type: user
        html: '<p>This is not a problem with your prompt actually, it''s just because
          these older models (anything other than the <code>dev</code> branch of the
          6B, basically) are unsupervised fine-tunes, so they learn to spit out an
          entire conversation instead of just a response. A way to work around this
          is to stop generation as soon as you reach a <code>\nYou: </code>, then
          trim that out before returning the text to the user, see <a rel="nofollow"
          href="https://github.com/PygmalionAI/gradio-ui/blob/2e2a1a4209f8e428e8c9deae68037c4805129dbc/src/model.py#L90">this
          code</a> for an example.</p>

          '
        raw: 'This is not a problem with your prompt actually, it''s just because
          these older models (anything other than the `dev` branch of the 6B, basically)
          are unsupervised fine-tunes, so they learn to spit out an entire conversation
          instead of just a response. A way to work around this is to stop generation
          as soon as you reach a `\nYou: `, then trim that out before returning the
          text to the user, see [this code](https://github.com/PygmalionAI/gradio-ui/blob/2e2a1a4209f8e428e8c9deae68037c4805129dbc/src/model.py#L90)
          for an example.'
        updatedAt: '2023-04-02T17:22:19.513Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - Vulonkul
    id: 6429b9cb316c9207b7c0ffab
    type: comment
  author: 11b
  content: 'This is not a problem with your prompt actually, it''s just because these
    older models (anything other than the `dev` branch of the 6B, basically) are unsupervised
    fine-tunes, so they learn to spit out an entire conversation instead of just a
    response. A way to work around this is to stop generation as soon as you reach
    a `\nYou: `, then trim that out before returning the text to the user, see [this
    code](https://github.com/PygmalionAI/gradio-ui/blob/2e2a1a4209f8e428e8c9deae68037c4805129dbc/src/model.py#L90)
    for an example.'
  created_at: 2023-04-02 16:22:19+00:00
  edited: false
  hidden: false
  id: 6429b9cb316c9207b7c0ffab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a4895656abd2a09f39cc64ee47c055e0.svg
      fullname: Atanas Valkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vulonkul
      type: user
    createdAt: '2023-04-02T17:30:35.000Z'
    data:
      edited: false
      editors:
      - Vulonkul
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a4895656abd2a09f39cc64ee47c055e0.svg
          fullname: Atanas Valkov
          isHf: false
          isPro: false
          name: Vulonkul
          type: user
        html: '<p>Oh that''s tremendous help. Thanks a lot! I hope this thread helps
          others who are struggling with similar issues.</p>

          '
        raw: Oh that's tremendous help. Thanks a lot! I hope this thread helps others
          who are struggling with similar issues.
        updatedAt: '2023-04-02T17:30:35.060Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6429bbbb316c9207b7c10a66
    id: 6429bbbb316c9207b7c10a65
    type: comment
  author: Vulonkul
  content: Oh that's tremendous help. Thanks a lot! I hope this thread helps others
    who are struggling with similar issues.
  created_at: 2023-04-02 16:30:35+00:00
  edited: false
  hidden: false
  id: 6429bbbb316c9207b7c10a65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a4895656abd2a09f39cc64ee47c055e0.svg
      fullname: Atanas Valkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vulonkul
      type: user
    createdAt: '2023-04-02T17:30:35.000Z'
    data:
      status: closed
    id: 6429bbbb316c9207b7c10a66
    type: status-change
  author: Vulonkul
  created_at: 2023-04-02 16:30:35+00:00
  id: 6429bbbb316c9207b7c10a66
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: PygmalionAI/pygmalion-2.7b
repo_type: model
status: closed
target_branch: null
title: Character talks instead of me and is basically chatting with itself.
