!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AngledLuffa
conflicting_files: null
created_at: 2022-07-10 15:42:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
      fullname: John Bauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AngledLuffa
      type: user
    createdAt: '2022-07-10T16:42:10.000Z'
    data:
      edited: false
      editors:
      - AngledLuffa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
          fullname: John Bauer
          isHf: false
          isPro: false
          name: AngledLuffa
          type: user
        html: '<p>Thank you for providing these word vectors.  I tried using them
          for training an NER model using Stanza, and they are a huge improvement
          in terms of accuracy over the original fasttext vectors.</p>

          <p>One minor nitpick: the link to Wikipedia points to the "latest" Wikipedia
          dump, which means that the link no longer represents the data used to build
          these vectors.</p>

          '
        raw: "Thank you for providing these word vectors.  I tried using them for\
          \ training an NER model using Stanza, and they are a huge improvement in\
          \ terms of accuracy over the original fasttext vectors.\r\n\r\nOne minor\
          \ nitpick: the link to Wikipedia points to the \"latest\" Wikipedia dump,\
          \ which means that the link no longer represents the data used to build\
          \ these vectors.\r\n"
        updatedAt: '2022-07-10T16:42:10.274Z'
      numEdits: 0
      reactions: []
    id: 62cb016298f35782df5e95bf
    type: comment
  author: AngledLuffa
  content: "Thank you for providing these word vectors.  I tried using them for training\
    \ an NER model using Stanza, and they are a huge improvement in terms of accuracy\
    \ over the original fasttext vectors.\r\n\r\nOne minor nitpick: the link to Wikipedia\
    \ points to the \"latest\" Wikipedia dump, which means that the link no longer\
    \ represents the data used to build these vectors.\r\n"
  created_at: 2022-07-10 15:42:10+00:00
  edited: false
  hidden: false
  id: 62cb016298f35782df5e95bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1598077578196-5f40b34279c1ba4c353d0c7a.png?w=200&h=200&f=face
      fullname: Sagor Sarker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sagorsarker
      type: user
    createdAt: '2022-07-10T17:52:57.000Z'
    data:
      edited: false
      editors:
      - sagorsarker
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1598077578196-5f40b34279c1ba4c353d0c7a.png?w=200&h=200&f=face
          fullname: Sagor Sarker
          isHf: false
          isPro: false
          name: sagorsarker
          type: user
        html: '<p>Hello,<br>Thanks for the feedback.<br>You are right. I built these
          vectors three years ago. The data version, I used was the latest at that
          time. And put the latest link. But now that increase a lot. I think the
          latest version won''t impact but rather improve your result. Sorry for not
          keeping the data version track.</p>

          '
        raw: "Hello,\nThanks for the feedback. \nYou are right. I built these vectors\
          \ three years ago. The data version, I used was the latest at that time.\
          \ And put the latest link. But now that increase a lot. I think the latest\
          \ version won't impact but rather improve your result. Sorry for not keeping\
          \ the data version track."
        updatedAt: '2022-07-10T17:52:57.872Z'
      numEdits: 0
      reactions: []
    id: 62cb11f92e4fbd838f64682d
    type: comment
  author: sagorsarker
  content: "Hello,\nThanks for the feedback. \nYou are right. I built these vectors\
    \ three years ago. The data version, I used was the latest at that time. And put\
    \ the latest link. But now that increase a lot. I think the latest version won't\
    \ impact but rather improve your result. Sorry for not keeping the data version\
    \ track."
  created_at: 2022-07-10 16:52:57+00:00
  edited: false
  hidden: false
  id: 62cb11f92e4fbd838f64682d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
      fullname: John Bauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AngledLuffa
      type: user
    createdAt: '2022-07-11T02:39:57.000Z'
    data:
      edited: false
      editors:
      - AngledLuffa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
          fullname: John Bauer
          isHf: false
          isPro: false
          name: AngledLuffa
          type: user
        html: '<p>Thanks for the explanation!</p>

          <p>Can I ask a question in a slightly different direction?  In terms of
          tokenization, would it be sufficient to separate words using whitespace
          and separate punctuation, or is there a more complicated technique needed
          for tokenizing Bangla?  For example, English and German have single tokens
          which are effectively multiple words connected to each other, and Chinese
          of course requires segmentation with almost no guiding whitespace.</p>

          '
        raw: 'Thanks for the explanation!


          Can I ask a question in a slightly different direction?  In terms of tokenization,
          would it be sufficient to separate words using whitespace and separate punctuation,
          or is there a more complicated technique needed for tokenizing Bangla?  For
          example, English and German have single tokens which are effectively multiple
          words connected to each other, and Chinese of course requires segmentation
          with almost no guiding whitespace.'
        updatedAt: '2022-07-11T02:39:57.001Z'
      numEdits: 0
      reactions: []
    id: 62cb8d7d6193ba3ced81c056
    type: comment
  author: AngledLuffa
  content: 'Thanks for the explanation!


    Can I ask a question in a slightly different direction?  In terms of tokenization,
    would it be sufficient to separate words using whitespace and separate punctuation,
    or is there a more complicated technique needed for tokenizing Bangla?  For example,
    English and German have single tokens which are effectively multiple words connected
    to each other, and Chinese of course requires segmentation with almost no guiding
    whitespace.'
  created_at: 2022-07-11 01:39:57+00:00
  edited: false
  hidden: false
  id: 62cb8d7d6193ba3ced81c056
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1598077578196-5f40b34279c1ba4c353d0c7a.png?w=200&h=200&f=face
      fullname: Sagor Sarker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: sagorsarker
      type: user
    createdAt: '2022-07-11T03:10:06.000Z'
    data:
      edited: false
      editors:
      - sagorsarker
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1598077578196-5f40b34279c1ba4c353d0c7a.png?w=200&h=200&f=face
          fullname: Sagor Sarker
          isHf: false
          isPro: false
          name: sagorsarker
          type: user
        html: '<p>In terms of tokenization, I can say whitespace and punctuation based
          is enough for simple tasks. But you need a better vocabulary size because
          there are too many unique words in Bengali. You can check this library for
          different tokenization options.<br><a rel="nofollow" href="https://github.com/sagorbrur/bnlp">https://github.com/sagorbrur/bnlp</a></p>

          <p>Besides you can check the subword-based tokenizer like sentence piece.
          But you need to handle it while training and getting vector in subword level.</p>

          '
        raw: 'In terms of tokenization, I can say whitespace and punctuation based
          is enough for simple tasks. But you need a better vocabulary size because
          there are too many unique words in Bengali. You can check this library for
          different tokenization options.

          https://github.com/sagorbrur/bnlp


          Besides you can check the subword-based tokenizer like sentence piece. But
          you need to handle it while training and getting vector in subword level.'
        updatedAt: '2022-07-11T03:10:06.873Z'
      numEdits: 0
      reactions: []
    id: 62cb948e668e45aa3d8c9bed
    type: comment
  author: sagorsarker
  content: 'In terms of tokenization, I can say whitespace and punctuation based is
    enough for simple tasks. But you need a better vocabulary size because there are
    too many unique words in Bengali. You can check this library for different tokenization
    options.

    https://github.com/sagorbrur/bnlp


    Besides you can check the subword-based tokenizer like sentence piece. But you
    need to handle it while training and getting vector in subword level.'
  created_at: 2022-07-11 02:10:06+00:00
  edited: false
  hidden: false
  id: 62cb948e668e45aa3d8c9bed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
      fullname: John Bauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AngledLuffa
      type: user
    createdAt: '2022-07-11T04:05:14.000Z'
    data:
      edited: false
      editors:
      - AngledLuffa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
          fullname: John Bauer
          isHf: false
          isPro: false
          name: AngledLuffa
          type: user
        html: '<p>Thank you!  Again, very helpful.</p>

          '
        raw: Thank you!  Again, very helpful.
        updatedAt: '2022-07-11T04:05:14.547Z'
      numEdits: 0
      reactions: []
    id: 62cba17a0d72a9876be69d78
    type: comment
  author: AngledLuffa
  content: Thank you!  Again, very helpful.
  created_at: 2022-07-11 03:05:14+00:00
  edited: false
  hidden: false
  id: 62cba17a0d72a9876be69d78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
      fullname: John Bauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AngledLuffa
      type: user
    createdAt: '2022-07-11T04:05:14.000Z'
    data:
      status: closed
    id: 62cba17a0d72a9876be69d79
    type: status-change
  author: AngledLuffa
  created_at: 2022-07-11 03:05:14+00:00
  id: 62cba17a0d72a9876be69d79
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: sagorsarker/bangla-glove-vectors
repo_type: model
status: closed
target_branch: null
title: Wikipedia link points to "latest"
