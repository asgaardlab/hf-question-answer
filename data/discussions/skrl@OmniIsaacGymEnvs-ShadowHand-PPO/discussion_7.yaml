!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jow2
conflicting_files: null
created_at: 2023-05-27 05:48:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ab8bdb2127dd1c297854f4b2bd6747b.svg
      fullname: hyungrae
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jow2
      type: user
    createdAt: '2023-05-27T06:48:31.000Z'
    data:
      edited: false
      editors:
      - jow2
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ab8bdb2127dd1c297854f4b2bd6747b.svg
          fullname: hyungrae
          isHf: false
          isPro: false
          name: jow2
          type: user
        html: '<p>Experiment4</p>

          '
        raw: Experiment4
        updatedAt: '2023-05-27T06:48:31.164Z'
      numEdits: 0
      reactions: []
    id: 6471a7bfa2b0a376b8b5a474
    type: comment
  author: jow2
  content: Experiment4
  created_at: 2023-05-27 05:48:31+00:00
  edited: false
  hidden: false
  id: 6471a7bfa2b0a376b8b5a474
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ab8bdb2127dd1c297854f4b2bd6747b.svg
      fullname: hyungrae
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jow2
      type: user
    createdAt: '2023-05-27T06:51:11.000Z'
    data:
      edited: false
      editors:
      - jow2
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ab8bdb2127dd1c297854f4b2bd6747b.svg
          fullname: hyungrae
          isHf: false
          isPro: false
          name: jow2
          type: user
        html: '<h1 id="omniisaacgymenvs-shadowhand-ppo">OmniIsaacGymEnvs-ShadowHand-PPO</h1>

          <p>Trained agent model for <a rel="nofollow" href="https://github.com/NVIDIA-Omniverse/OmniIsaacGymEnvs">NVIDIA
          Omniverse Isaac Gym</a> environment</p>

          <ul>

          <li><strong>Task:</strong> ShadowHand</li>

          <li><strong>Agent:</strong> <a rel="nofollow" href="https://skrl.readthedocs.io/en/latest/modules/skrl.agents.ppo.html">PPO</a></li>

          </ul>

          <h1 id="usage-with-skrl">Usage (with skrl)</h1>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          skrl.utils.huggingface <span class="hljs-keyword">import</span> download_model_from_huggingface


          <span class="hljs-comment"># assuming that there is an agent named `agent`</span>

          path = download_model_from_huggingface(<span class="hljs-string">"skrl/OmniIsaacGymEnvs-ShadowHand-PPO"</span>)

          agent.load(path)

          </code></pre>

          <h1 id="hyperparameters">Hyperparameters</h1>

          <p>```python</p>

          <h1 id="httpsskrlreadthedocsioenlatestmodulesskrlagentsppohtmlconfiguration-and-hyperparameters"><a
          rel="nofollow" href="https://skrl.readthedocs.io/en/latest/modules/skrl.agents.ppo.html#configuration-and-hyperparameters">https://skrl.readthedocs.io/en/latest/modules/skrl.agents.ppo.html#configuration-and-hyperparameters</a></h1>

          <p>cfg_ppo = PPO_DEFAULT_CONFIG.copy()<br>cfg_ppo["rollouts"] = 16  # memory_size<br>cfg_ppo["learning_epochs"]
          = 5<br>cfg_ppo["mini_batches"] = 4  # 16 * 8192 / 32768<br>cfg_ppo["discount_factor"]
          = 0.99<br>cfg_ppo["lambda"] = 0.95<br>cfg_ppo["learning_rate"] = 5e-4<br>cfg_ppo["learning_rate_scheduler"]
          = KLAdaptiveRL<br>cfg_ppo["learning_rate_scheduler_kwargs"] = {"kl_threshold":
          0.016}<br>cfg_ppo["random_timesteps"] = 0<br>cfg_ppo["learning_starts"]
          = 0<br>cfg_ppo["grad_norm_clip"] = 1.0<br>cfg_ppo["ratio_clip"] = 0.2<br>cfg_ppo["value_clip"]
          = 0.2<br>cfg_ppo["clip_predicted_values"] = True<br>cfg_ppo["entropy_loss_scale"]
          = 0.0<br>cfg_ppo["value_loss_scale"] = 2.0<br>cfg_ppo["kl_threshold"] =
          0<br>cfg_ppo["rewards_shaper"] = lambda rewards, timestep, timesteps: rewards
          * 0.01<br>cfg_ppo["state_preprocessor"] = RunningStandardScaler</p>

          '
        raw: "---\nlibrary_name: skrl\ntags:\n- deep-reinforcement-learning\n- reinforcement-learning\n\
          - skrl\nmodel-index:\n- name: PPO\n  results:\n  - metrics:\n    - type:\
          \ mean_reward\n      value: 11175.08\n      name: Total reward (mean)\n\
          \    task:\n      type: reinforcement-learning\n      name: reinforcement-learning\n\
          \    dataset:\n      name: OmniIsaacGymEnvs-ShadowHand\n      type: OmniIsaacGymEnvs-ShadowHand\n\
          ---\n\n# OmniIsaacGymEnvs-ShadowHand-PPO\n\nTrained agent model for [NVIDIA\
          \ Omniverse Isaac Gym](https://github.com/NVIDIA-Omniverse/OmniIsaacGymEnvs)\
          \ environment\n\n- **Task:** ShadowHand\n- **Agent:** [PPO](https://skrl.readthedocs.io/en/latest/modules/skrl.agents.ppo.html)\n\
          \n# Usage (with skrl) \n\n```python\nfrom skrl.utils.huggingface import\
          \ download_model_from_huggingface\n\n# assuming that there is an agent named\
          \ `agent`\npath = download_model_from_huggingface(\"skrl/OmniIsaacGymEnvs-ShadowHand-PPO\"\
          )\nagent.load(path)\n```\n\n# Hyperparameters\n\n```python\n# https://skrl.readthedocs.io/en/latest/modules/skrl.agents.ppo.html#configuration-and-hyperparameters\n\
          cfg_ppo = PPO_DEFAULT_CONFIG.copy()\ncfg_ppo[\"rollouts\"] = 16  # memory_size\n\
          cfg_ppo[\"learning_epochs\"] = 5\ncfg_ppo[\"mini_batches\"] = 4  # 16 *\
          \ 8192 / 32768\ncfg_ppo[\"discount_factor\"] = 0.99\ncfg_ppo[\"lambda\"\
          ] = 0.95\ncfg_ppo[\"learning_rate\"] = 5e-4\ncfg_ppo[\"learning_rate_scheduler\"\
          ] = KLAdaptiveRL\ncfg_ppo[\"learning_rate_scheduler_kwargs\"] = {\"kl_threshold\"\
          : 0.016}\ncfg_ppo[\"random_timesteps\"] = 0\ncfg_ppo[\"learning_starts\"\
          ] = 0\ncfg_ppo[\"grad_norm_clip\"] = 1.0\ncfg_ppo[\"ratio_clip\"] = 0.2\n\
          cfg_ppo[\"value_clip\"] = 0.2\ncfg_ppo[\"clip_predicted_values\"] = True\n\
          cfg_ppo[\"entropy_loss_scale\"] = 0.0\ncfg_ppo[\"value_loss_scale\"] = 2.0\n\
          cfg_ppo[\"kl_threshold\"] = 0\ncfg_ppo[\"rewards_shaper\"] = lambda rewards,\
          \ timestep, timesteps: rewards * 0.01\ncfg_ppo[\"state_preprocessor\"] =\
          \ RunningStandardScaler"
        updatedAt: '2023-05-27T06:51:11.258Z'
      numEdits: 0
      reactions: []
    id: 6471a85f97a75cc77aa408de
    type: comment
  author: jow2
  content: "---\nlibrary_name: skrl\ntags:\n- deep-reinforcement-learning\n- reinforcement-learning\n\
    - skrl\nmodel-index:\n- name: PPO\n  results:\n  - metrics:\n    - type: mean_reward\n\
    \      value: 11175.08\n      name: Total reward (mean)\n    task:\n      type:\
    \ reinforcement-learning\n      name: reinforcement-learning\n    dataset:\n \
    \     name: OmniIsaacGymEnvs-ShadowHand\n      type: OmniIsaacGymEnvs-ShadowHand\n\
    ---\n\n# OmniIsaacGymEnvs-ShadowHand-PPO\n\nTrained agent model for [NVIDIA Omniverse\
    \ Isaac Gym](https://github.com/NVIDIA-Omniverse/OmniIsaacGymEnvs) environment\n\
    \n- **Task:** ShadowHand\n- **Agent:** [PPO](https://skrl.readthedocs.io/en/latest/modules/skrl.agents.ppo.html)\n\
    \n# Usage (with skrl) \n\n```python\nfrom skrl.utils.huggingface import download_model_from_huggingface\n\
    \n# assuming that there is an agent named `agent`\npath = download_model_from_huggingface(\"\
    skrl/OmniIsaacGymEnvs-ShadowHand-PPO\")\nagent.load(path)\n```\n\n# Hyperparameters\n\
    \n```python\n# https://skrl.readthedocs.io/en/latest/modules/skrl.agents.ppo.html#configuration-and-hyperparameters\n\
    cfg_ppo = PPO_DEFAULT_CONFIG.copy()\ncfg_ppo[\"rollouts\"] = 16  # memory_size\n\
    cfg_ppo[\"learning_epochs\"] = 5\ncfg_ppo[\"mini_batches\"] = 4  # 16 * 8192 /\
    \ 32768\ncfg_ppo[\"discount_factor\"] = 0.99\ncfg_ppo[\"lambda\"] = 0.95\ncfg_ppo[\"\
    learning_rate\"] = 5e-4\ncfg_ppo[\"learning_rate_scheduler\"] = KLAdaptiveRL\n\
    cfg_ppo[\"learning_rate_scheduler_kwargs\"] = {\"kl_threshold\": 0.016}\ncfg_ppo[\"\
    random_timesteps\"] = 0\ncfg_ppo[\"learning_starts\"] = 0\ncfg_ppo[\"grad_norm_clip\"\
    ] = 1.0\ncfg_ppo[\"ratio_clip\"] = 0.2\ncfg_ppo[\"value_clip\"] = 0.2\ncfg_ppo[\"\
    clip_predicted_values\"] = True\ncfg_ppo[\"entropy_loss_scale\"] = 0.0\ncfg_ppo[\"\
    value_loss_scale\"] = 2.0\ncfg_ppo[\"kl_threshold\"] = 0\ncfg_ppo[\"rewards_shaper\"\
    ] = lambda rewards, timestep, timesteps: rewards * 0.01\ncfg_ppo[\"state_preprocessor\"\
    ] = RunningStandardScaler"
  created_at: 2023-05-27 05:51:11+00:00
  edited: false
  hidden: false
  id: 6471a85f97a75cc77aa408de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/6ab8bdb2127dd1c297854f4b2bd6747b.svg
      fullname: hyungrae
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jow2
      type: user
    createdAt: '2023-05-27T06:51:14.000Z'
    data:
      status: closed
    id: 6471a8620211f85270f6f0b5
    type: status-change
  author: jow2
  created_at: 2023-05-27 05:51:14+00:00
  id: 6471a8620211f85270f6f0b5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: skrl/OmniIsaacGymEnvs-ShadowHand-PPO
repo_type: model
status: closed
target_branch: null
title: OmniIsaacGymEnvs-ShadowHand-PPO2
