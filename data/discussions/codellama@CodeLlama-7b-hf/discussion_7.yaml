!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Akanshu
conflicting_files: null
created_at: 2023-08-25 12:04:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7fad110c6ea608d97eb551e764afaff2.svg
      fullname: akanshu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Akanshu
      type: user
    createdAt: '2023-08-25T13:04:12.000Z'
    data:
      edited: true
      editors:
      - Akanshu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5041701793670654
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7fad110c6ea608d97eb551e764afaff2.svg
          fullname: akanshu
          isHf: false
          isPro: false
          name: Akanshu
          type: user
        html: '<p> RuntimeError(f"weight {tensor_name} does not exist")<br>RuntimeError:
          weight model.layers.0.self_attn.rotary_emb.inv_freq does not exist<br>Error:
          ShardCannotStart</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64d1dc64bc6c9c8bc07c7c10/T-MHf644FrlAvee484qTl.png"><img
          alt="Screenshot 2023-08-25 at 6.33.45 PM.png" src="https://cdn-uploads.huggingface.co/production/uploads/64d1dc64bc6c9c8bc07c7c10/T-MHf644FrlAvee484qTl.png"></a><br>Thankyou
          in advance and please help in this deployment.</p>

          '
        raw: ' RuntimeError(f"weight {tensor_name} does not exist")

          RuntimeError: weight model.layers.0.self_attn.rotary_emb.inv_freq does not
          exist

          Error: ShardCannotStart


          ![Screenshot 2023-08-25 at 6.33.45 PM.png](https://cdn-uploads.huggingface.co/production/uploads/64d1dc64bc6c9c8bc07c7c10/T-MHf644FrlAvee484qTl.png)

          Thankyou in advance and please help in this deployment.'
        updatedAt: '2023-08-25T13:05:06.474Z'
      numEdits: 1
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - varunkamble
        - ghvandoorn
        - akgq
        - Laxxx
      - count: 1
        reaction: "\U0001F614"
        users:
        - ghvandoorn
    id: 64e8a6ccd50f3979bea697ae
    type: comment
  author: Akanshu
  content: ' RuntimeError(f"weight {tensor_name} does not exist")

    RuntimeError: weight model.layers.0.self_attn.rotary_emb.inv_freq does not exist

    Error: ShardCannotStart


    ![Screenshot 2023-08-25 at 6.33.45 PM.png](https://cdn-uploads.huggingface.co/production/uploads/64d1dc64bc6c9c8bc07c7c10/T-MHf644FrlAvee484qTl.png)

    Thankyou in advance and please help in this deployment.'
  created_at: 2023-08-25 12:04:12+00:00
  edited: true
  hidden: false
  id: 64e8a6ccd50f3979bea697ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a68ebfe0ce4ee51a3d2fc91d243928a0.svg
      fullname: Ajay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akgq
      type: user
    createdAt: '2023-08-28T12:45:12.000Z'
    data:
      edited: false
      editors:
      - akgq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5504316091537476
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a68ebfe0ce4ee51a3d2fc91d243928a0.svg
          fullname: Ajay
          isHf: false
          isPro: false
          name: akgq
          type: user
        html: '<p>Facing the same issue <em>Error: ShardCannotStart</em> while deploying
          CodeLlama via hugging face</p>

          '
        raw: 'Facing the same issue *Error: ShardCannotStart* while deploying CodeLlama
          via hugging face'
        updatedAt: '2023-08-28T12:45:12.191Z'
      numEdits: 0
      reactions: []
    id: 64ec96d8cfa36c8ac20332a3
    type: comment
  author: akgq
  content: 'Facing the same issue *Error: ShardCannotStart* while deploying CodeLlama
    via hugging face'
  created_at: 2023-08-28 11:45:12+00:00
  edited: false
  hidden: false
  id: 64ec96d8cfa36c8ac20332a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa32e9ef969746fe54c6817b161f47a9.svg
      fullname: Choms
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Choms
      type: user
    createdAt: '2023-08-28T18:17:44.000Z'
    data:
      edited: false
      editors:
      - Choms
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9171770215034485
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa32e9ef969746fe54c6817b161f47a9.svg
          fullname: Choms
          isHf: false
          isPro: false
          name: Choms
          type: user
        html: "<p>Hey, you need the mainline version of the \U0001F917 transformers\
          \ from git to run this (<a href=\"https://huggingface.co/codellama/CodeLlama-7b-hf#model-use\"\
          >https://huggingface.co/codellama/CodeLlama-7b-hf#model-use</a>), there's\
          \ no container for it yet on sagemaker (I guess you both are using the 0.9.3\
          \ container), you'll have to run it outside of sagemaker or load it on a\
          \ notebook instance directly (that's what I'm doing for now, until this\
          \ is supported)</p>\n"
        raw: "Hey, you need the mainline version of the \U0001F917 transformers from\
          \ git to run this (https://huggingface.co/codellama/CodeLlama-7b-hf#model-use),\
          \ there's no container for it yet on sagemaker (I guess you both are using\
          \ the 0.9.3 container), you'll have to run it outside of sagemaker or load\
          \ it on a notebook instance directly (that's what I'm doing for now, until\
          \ this is supported)"
        updatedAt: '2023-08-28T18:17:44.976Z'
      numEdits: 0
      reactions: []
    id: 64ece4c84e57da899375ca0e
    type: comment
  author: Choms
  content: "Hey, you need the mainline version of the \U0001F917 transformers from\
    \ git to run this (https://huggingface.co/codellama/CodeLlama-7b-hf#model-use),\
    \ there's no container for it yet on sagemaker (I guess you both are using the\
    \ 0.9.3 container), you'll have to run it outside of sagemaker or load it on a\
    \ notebook instance directly (that's what I'm doing for now, until this is supported)"
  created_at: 2023-08-28 17:17:44+00:00
  edited: false
  hidden: false
  id: 64ece4c84e57da899375ca0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7fad110c6ea608d97eb551e764afaff2.svg
      fullname: akanshu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Akanshu
      type: user
    createdAt: '2023-08-28T18:22:32.000Z'
    data:
      edited: false
      editors:
      - Akanshu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9620575308799744
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7fad110c6ea608d97eb551e764afaff2.svg
          fullname: akanshu
          isHf: false
          isPro: false
          name: Akanshu
          type: user
        html: "<p>I just made the notebook instance and inside that i have created\
          \ a jupyter notebook and ran this code\u2026can you please elaborate how\
          \ to deploy.</p>\n"
        raw: "I just made the notebook instance and inside that i have created a jupyter\
          \ notebook and ran this code\u2026can you please elaborate how to deploy."
        updatedAt: '2023-08-28T18:22:32.248Z'
      numEdits: 0
      reactions: []
    id: 64ece5e8d51cc52a60c370fb
    type: comment
  author: Akanshu
  content: "I just made the notebook instance and inside that i have created a jupyter\
    \ notebook and ran this code\u2026can you please elaborate how to deploy."
  created_at: 2023-08-28 17:22:32+00:00
  edited: false
  hidden: false
  id: 64ece5e8d51cc52a60c370fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa32e9ef969746fe54c6817b161f47a9.svg
      fullname: Choms
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Choms
      type: user
    createdAt: '2023-08-28T18:40:52.000Z'
    data:
      edited: false
      editors:
      - Choms
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8962938785552979
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa32e9ef969746fe54c6817b161f47a9.svg
          fullname: Choms
          isHf: false
          isPro: false
          name: Choms
          type: user
        html: '<p>That''s about as far as I''ve got, I''m following the documentation
          here: <a href="https://huggingface.co/docs/transformers/main/model_doc/code_llama">https://huggingface.co/docs/transformers/main/model_doc/code_llama</a><br>plus
          the pip install from git on the readme from this model, then just use the
          notebook to play with it, as said there''s no easy way to deploy it as an
          actual interference endpoint (you could build your own container with the
          required versions though), good luck!</p>

          <p>PS: you can use thebloke''s gptq build and run it on multi GPU if you
          <code>pip install auto-gptq optimum</code></p>

          '
        raw: 'That''s about as far as I''ve got, I''m following the documentation
          here: https://huggingface.co/docs/transformers/main/model_doc/code_llama

          plus the pip install from git on the readme from this model, then just use
          the notebook to play with it, as said there''s no easy way to deploy it
          as an actual interference endpoint (you could build your own container with
          the required versions though), good luck!


          PS: you can use thebloke''s gptq build and run it on multi GPU if you `pip
          install auto-gptq optimum`'
        updatedAt: '2023-08-28T18:40:52.976Z'
      numEdits: 0
      reactions: []
    id: 64ecea34266afc1d935f0b1d
    type: comment
  author: Choms
  content: 'That''s about as far as I''ve got, I''m following the documentation here:
    https://huggingface.co/docs/transformers/main/model_doc/code_llama

    plus the pip install from git on the readme from this model, then just use the
    notebook to play with it, as said there''s no easy way to deploy it as an actual
    interference endpoint (you could build your own container with the required versions
    though), good luck!


    PS: you can use thebloke''s gptq build and run it on multi GPU if you `pip install
    auto-gptq optimum`'
  created_at: 2023-08-28 17:40:52+00:00
  edited: false
  hidden: false
  id: 64ecea34266afc1d935f0b1d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: codellama/CodeLlama-7b-hf
repo_type: model
status: open
target_branch: null
title: Error in deployment in sagemaker
