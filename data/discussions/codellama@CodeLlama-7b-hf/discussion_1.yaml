!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Leymore
conflicting_files: null
created_at: 2023-08-25 05:56:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f04fb94a788ed1dd89daf4/5XNPePED6fTMH0SAM2lhj.jpeg?w=200&h=200&f=face
      fullname: zhoufengzhe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Leymore
      type: user
    createdAt: '2023-08-25T06:56:28.000Z'
    data:
      edited: false
      editors:
      - Leymore
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4714721143245697
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f04fb94a788ed1dd89daf4/5XNPePED6fTMH0SAM2lhj.jpeg?w=200&h=200&f=face
          fullname: zhoufengzhe
          isHf: false
          isPro: false
          name: Leymore
          type: user
        html: '<pre><code class="language-python"><span class="hljs-comment"># Load
          model directly</span>

          <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span>
          AutoTokenizer, AutoModelForCausalLM


          tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"codellama/CodeLlama-7b-hf"</span>)

          model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">"codellama/CodeLlama-7b-hf"</span>)

          </code></pre>

          <p>I encountered the following warning:</p>

          <pre><code class="language-text">Some weights of LlamaForCausalLM were not
          initialized from the model checkpoint at codellama/CodeLlama-7b-hf and are
          newly initialized: [''model.layers.21.self_attn.rotary_emb.inv_freq'', ''model.layers.5.self_attn.rotary_emb.inv_freq'',
          ''model.layers.17.self_attn.rotary_emb.inv_freq'', ''model.layers.1.self_attn.rotary_emb.inv_freq'',
          ''model.layers.13.self_attn.rotary_emb.inv_freq'', ''model.layers.12.self_attn.rotary_emb.inv_freq'',
          ''model.layers.23.self_attn.rotary_emb.inv_freq'', ''model.layers.2.self_attn.rotary_emb.inv_freq'',
          ''model.layers.11.self_attn.rotary_emb.inv_freq'', ''model.layers.19.self_attn.rotary_emb.inv_freq'',
          ''model.layers.20.self_attn.rotary_emb.inv_freq'', ''model.layers.30.self_attn.rotary_emb.inv_freq'',
          ''model.layers.4.self_attn.rotary_emb.inv_freq'', ''model.layers.10.self_attn.rotary_emb.inv_freq'',
          ''model.layers.16.self_attn.rotary_emb.inv_freq'', ''model.layers.0.self_attn.rotary_emb.inv_freq'',
          ''model.layers.8.self_attn.rotary_emb.inv_freq'', ''model.layers.31.self_attn.rotary_emb.inv_freq'',
          ''model.layers.9.self_attn.rotary_emb.inv_freq'', ''model.layers.29.self_attn.rotary_emb.inv_freq'',
          ''model.layers.18.self_attn.rotary_emb.inv_freq'', ''model.layers.6.self_attn.rotary_emb.inv_freq'',
          ''model.layers.24.self_attn.rotary_emb.inv_freq'', ''model.layers.26.self_attn.rotary_emb.inv_freq'',
          ''model.layers.22.self_attn.rotary_emb.inv_freq'', ''model.layers.7.self_attn.rotary_emb.inv_freq'',
          ''model.layers.14.self_attn.rotary_emb.inv_freq'', ''model.layers.25.self_attn.rotary_emb.inv_freq'',
          ''model.layers.15.self_attn.rotary_emb.inv_freq'', ''model.layers.28.self_attn.rotary_emb.inv_freq'',
          ''model.layers.27.self_attn.rotary_emb.inv_freq'', ''model.layers.3.self_attn.rotary_emb.inv_freq'']

          You should probably TRAIN this model on a down-stream task to be able to
          use it for predictions and inference.

          </code></pre>

          <p>Is the checkpoint broken?</p>

          '
        raw: "```python\r\n# Load model directly\r\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          codellama/CodeLlama-7b-hf\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          codellama/CodeLlama-7b-hf\")\r\n```\r\n\r\nI encountered the following warning:\r\
          \n\r\n```text\r\nSome weights of LlamaForCausalLM were not initialized from\
          \ the model checkpoint at codellama/CodeLlama-7b-hf and are newly initialized:\
          \ ['model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq',\
          \ 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq']\r\
          \nYou should probably TRAIN this model on a down-stream task to be able\
          \ to use it for predictions and inference.\r\n```\r\n\r\nIs the checkpoint\
          \ broken?\r\n"
        updatedAt: '2023-08-25T06:56:28.168Z'
      numEdits: 0
      reactions: []
    id: 64e8509c6ca7f623860e9d9d
    type: comment
  author: Leymore
  content: "```python\r\n# Load model directly\r\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\"\
    )\r\nmodel = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-hf\"\
    )\r\n```\r\n\r\nI encountered the following warning:\r\n\r\n```text\r\nSome weights\
    \ of LlamaForCausalLM were not initialized from the model checkpoint at codellama/CodeLlama-7b-hf\
    \ and are newly initialized: ['model.layers.21.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq',\
    \ 'model.layers.3.self_attn.rotary_emb.inv_freq']\r\nYou should probably TRAIN\
    \ this model on a down-stream task to be able to use it for predictions and inference.\r\
    \n```\r\n\r\nIs the checkpoint broken?\r\n"
  created_at: 2023-08-25 05:56:28+00:00
  edited: false
  hidden: false
  id: 64e8509c6ca7f623860e9d9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-08-25T07:56:05.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8878579139709473
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>No, you are probably not on the latest release of transformers /  the
          correct fork that supports this  model. You need to use <code>https://github.com/huggingface/transformers/pull/25740</code></p>

          '
        raw: No, you are probably not on the latest release of transformers /  the
          correct fork that supports this  model. You need to use `https://github.com/huggingface/transformers/pull/25740`
        updatedAt: '2023-08-25T07:56:05.519Z'
      numEdits: 0
      reactions: []
    id: 64e85e950973fdd28a244070
    type: comment
  author: ArthurZ
  content: No, you are probably not on the latest release of transformers /  the correct
    fork that supports this  model. You need to use `https://github.com/huggingface/transformers/pull/25740`
  created_at: 2023-08-25 06:56:05+00:00
  edited: false
  hidden: false
  id: 64e85e950973fdd28a244070
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6032802e1f993496bc14d9e3/w6hr-DEQot4VVkoyRIBiy.png?w=200&h=200&f=face
      fullname: Omar Sanseviero
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: osanseviero
      type: user
    createdAt: '2023-08-28T16:35:07.000Z'
    data:
      status: closed
    id: 64ecccbbd287fc465ab2664b
    type: status-change
  author: osanseviero
  created_at: 2023-08-28 15:35:07+00:00
  id: 64ecccbbd287fc465ab2664b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: codellama/CodeLlama-7b-hf
repo_type: model
status: closed
target_branch: null
title: Weight missing warning
