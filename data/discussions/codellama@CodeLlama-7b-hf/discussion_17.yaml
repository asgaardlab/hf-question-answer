!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RyanAX
conflicting_files: null
created_at: 2023-09-07 01:26:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fa06e7ee9b2b69d96c6907408dbd8a80.svg
      fullname: Cool
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RyanAX
      type: user
    createdAt: '2023-09-07T02:26:32.000Z'
    data:
      edited: false
      editors:
      - RyanAX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49616700410842896
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fa06e7ee9b2b69d96c6907408dbd8a80.svg
          fullname: Cool
          isHf: false
          isPro: false
          name: RyanAX
          type: user
        html: "<p>I have set up the codellama-7b model locally and used the official\
          \ example, but the final result does not meet expectations. Here is the\
          \ code:</p>\n<pre><code class=\"language-java\">codeLlama_tokenizer = CodeLlamaTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"./CodeLlama-7b-hf\"</span>, padding_side=<span\
          \ class=\"hljs-string\">'left'</span>)\ncodeLlama_model = LlamaForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"./CodeLlama-7b-hf\"</span>)\ncodeLlama_model.to(device=<span\
          \ class=\"hljs-string\">'cuda:0'</span>, dtype=torch.bfloat16)\n\ntext =\
          \ <span class=\"hljs-string\">''</span><span class=\"hljs-string\">'def\
          \ remove_non_ascii(s: str) -&gt; str:</span>\n<span class=\"hljs-string\"\
          >        \"\"\" &lt;FILL_ME&gt;</span>\n<span class=\"hljs-string\">   \
          \     return result</span>\n<span class=\"hljs-string\">    '</span><span\
          \ class=\"hljs-string\">''</span>\n\nstart_time = time.time()\ninput_ids\
          \ = codeLlama_tokenizer(text, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>)[<span class=\"hljs-string\">\"input_ids\"</span>]\ninput_ids\
          \ = input_ids.to(<span class=\"hljs-string\">'cuda'</span>)\ngenerated_ids\
          \ = codeLlama_model.generate(input_ids, max_new_tokens=<span class=\"hljs-number\"\
          >200</span>, do_sample=True, top_p=<span class=\"hljs-number\">0.9</span>,\
          \ temperature=<span class=\"hljs-number\">0.1</span>, num_return_sequences=<span\
          \ class=\"hljs-number\">1</span>, repetition_penalty=<span class=\"hljs-number\"\
          >1.05</span>, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)\n\
          filling = codeLlama_tokenizer.batch_decode(generated_ids[:, input_ids.shape[<span\
          \ class=\"hljs-number\">1</span>]:], skip_special_tokens=True)[<span class=\"\
          hljs-number\">0</span>]\nprint(filling)\n</code></pre>\n<p>The output of\
          \ the code is:</p>\n<pre><code>Remove non-ascii characters from a string.\
          \ \"\"\"\n        result = \"\"\n        for c in s:\n            if ord(c)\
          \ &lt; 128:\n                result += c\n    }\n\n    public void setId(String\
          \ id) {\n        this.id = id;\n    }\n\n    public String getName() {\n\
          \        return name;\n    }\n\n    public void setName(String name) {\n\
          \        this.name = name;\n    }\n\n    public String getDescription()\
          \ {\n        return description;\n    }\n\n    public void setDescription(String\
          \ description) {\n        this.description = description;\n    }\n\n   \
          \ public String getType() {\n        return type;\n}\n</code></pre>\n<p>There\
          \ are two issues with the generated code that don't meet expectations:<br>1\u3001\
          It doesn't consider suffixes and seems to ignore everything after <code>&lt;FILL_ME&gt;</code>.<br>2\u3001\
          After completing the desired part of the code, it adds a lot of unnecessary\
          \ additional code.</p>\n<p>Is this behavior normal? Is there any way to\
          \ improve it?</p>\n"
        raw: "I have set up the codellama-7b model locally and used the official example,\
          \ but the final result does not meet expectations. Here is the code:\r\n\
          \r\n```java\r\ncodeLlama_tokenizer = CodeLlamaTokenizer.from_pretrained(\"\
          ./CodeLlama-7b-hf\", padding_side='left')\r\ncodeLlama_model = LlamaForCausalLM.from_pretrained(\"\
          ./CodeLlama-7b-hf\")\r\ncodeLlama_model.to(device='cuda:0', dtype=torch.bfloat16)\r\
          \n\r\ntext = '''def remove_non_ascii(s: str) -> str:\r\n        \"\"\" <FILL_ME>\r\
          \n        return result\r\n    '''\r\n\r\nstart_time = time.time()\r\ninput_ids\
          \ = codeLlama_tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\r\n\
          input_ids = input_ids.to('cuda')\r\ngenerated_ids = codeLlama_model.generate(input_ids,\
          \ max_new_tokens=200, do_sample=True, top_p=0.9, temperature=0.1, num_return_sequences=1,\
          \ repetition_penalty=1.05, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)\r\
          \nfilling = codeLlama_tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:],\
          \ skip_special_tokens=True)[0]\r\nprint(filling)\r\n```\r\n\r\nThe output\
          \ of the code is:\r\n\r\n```\r\nRemove non-ascii characters from a string.\
          \ \"\"\"\r\n        result = \"\"\r\n        for c in s:\r\n           \
          \ if ord(c) < 128:\r\n                result += c\r\n    }\r\n\r\n    public\
          \ void setId(String id) {\r\n        this.id = id;\r\n    }\r\n\r\n    public\
          \ String getName() {\r\n        return name;\r\n    }\r\n\r\n    public\
          \ void setName(String name) {\r\n        this.name = name;\r\n    }\r\n\r\
          \n    public String getDescription() {\r\n        return description;\r\n\
          \    }\r\n\r\n    public void setDescription(String description) {\r\n \
          \       this.description = description;\r\n    }\r\n\r\n    public String\
          \ getType() {\r\n        return type;\r\n}\r\n```\r\n\r\nThere are two issues\
          \ with the generated code that don't meet expectations:\r\n1\u3001It doesn't\
          \ consider suffixes and seems to ignore everything after `<FILL_ME>`.\r\n\
          2\u3001After completing the desired part of the code, it adds a lot of unnecessary\
          \ additional code.\r\n\r\nIs this behavior normal? Is there any way to improve\
          \ it?"
        updatedAt: '2023-09-07T02:26:32.631Z'
      numEdits: 0
      reactions: []
    id: 64f934d8dcd7b028c1977d67
    type: comment
  author: RyanAX
  content: "I have set up the codellama-7b model locally and used the official example,\
    \ but the final result does not meet expectations. Here is the code:\r\n\r\n```java\r\
    \ncodeLlama_tokenizer = CodeLlamaTokenizer.from_pretrained(\"./CodeLlama-7b-hf\"\
    , padding_side='left')\r\ncodeLlama_model = LlamaForCausalLM.from_pretrained(\"\
    ./CodeLlama-7b-hf\")\r\ncodeLlama_model.to(device='cuda:0', dtype=torch.bfloat16)\r\
    \n\r\ntext = '''def remove_non_ascii(s: str) -> str:\r\n        \"\"\" <FILL_ME>\r\
    \n        return result\r\n    '''\r\n\r\nstart_time = time.time()\r\ninput_ids\
    \ = codeLlama_tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\r\ninput_ids\
    \ = input_ids.to('cuda')\r\ngenerated_ids = codeLlama_model.generate(input_ids,\
    \ max_new_tokens=200, do_sample=True, top_p=0.9, temperature=0.1, num_return_sequences=1,\
    \ repetition_penalty=1.05, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)\r\
    \nfilling = codeLlama_tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:],\
    \ skip_special_tokens=True)[0]\r\nprint(filling)\r\n```\r\n\r\nThe output of the\
    \ code is:\r\n\r\n```\r\nRemove non-ascii characters from a string. \"\"\"\r\n\
    \        result = \"\"\r\n        for c in s:\r\n            if ord(c) < 128:\r\
    \n                result += c\r\n    }\r\n\r\n    public void setId(String id)\
    \ {\r\n        this.id = id;\r\n    }\r\n\r\n    public String getName() {\r\n\
    \        return name;\r\n    }\r\n\r\n    public void setName(String name) {\r\
    \n        this.name = name;\r\n    }\r\n\r\n    public String getDescription()\
    \ {\r\n        return description;\r\n    }\r\n\r\n    public void setDescription(String\
    \ description) {\r\n        this.description = description;\r\n    }\r\n\r\n \
    \   public String getType() {\r\n        return type;\r\n}\r\n```\r\n\r\nThere\
    \ are two issues with the generated code that don't meet expectations:\r\n1\u3001\
    It doesn't consider suffixes and seems to ignore everything after `<FILL_ME>`.\r\
    \n2\u3001After completing the desired part of the code, it adds a lot of unnecessary\
    \ additional code.\r\n\r\nIs this behavior normal? Is there any way to improve\
    \ it?"
  created_at: 2023-09-07 01:26:32+00:00
  edited: false
  hidden: false
  id: 64f934d8dcd7b028c1977d67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5fd9ea694bef38b62149c38f3e414e3.svg
      fullname: zxy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zxyscz
      type: user
    createdAt: '2023-09-12T09:44:00.000Z'
    data:
      edited: false
      editors:
      - zxyscz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9146879315376282
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5fd9ea694bef38b62149c38f3e414e3.svg
          fullname: zxy
          isHf: false
          isPro: false
          name: zxyscz
          type: user
        html: '<p>i have the same questions</p>

          '
        raw: i have the same questions
        updatedAt: '2023-09-12T09:44:00.406Z'
      numEdits: 0
      reactions: []
    id: 650032e00e8369f6a8e54769
    type: comment
  author: zxyscz
  content: i have the same questions
  created_at: 2023-09-12 08:44:00+00:00
  edited: false
  hidden: false
  id: 650032e00e8369f6a8e54769
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-09-20T15:39:46.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9118797779083252
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>A few things to note here. </p>

          <ol>

          <li>to check if the <code>&lt;FILL_ME&gt;</code> is taken into account,
          you need to make sure the input ids are properly formatted. </li>

          <li>the outputs we have match 1-1 with the original outputs. But when you
          generate with sampling and custom temperature etc, you should expect some
          hallucination. Especially if the eos token is not properly set, the model
          will not stop early enough :/</li>

          </ol>

          <p>Thanks for opening the issue!</p>

          '
        raw: "A few things to note here. \n1. to check if the `<FILL_ME>` is taken\
          \ into account, you need to make sure the input ids are properly formatted.\
          \ \n2. the outputs we have match 1-1 with the original outputs. But when\
          \ you generate with sampling and custom temperature etc, you should expect\
          \ some hallucination. Especially if the eos token is not properly set, the\
          \ model will not stop early enough :/ \n\nThanks for opening the issue!"
        updatedAt: '2023-09-20T15:39:46.715Z'
      numEdits: 0
      reactions: []
    id: 650b12425d9ce91d31d993a2
    type: comment
  author: ArthurZ
  content: "A few things to note here. \n1. to check if the `<FILL_ME>` is taken into\
    \ account, you need to make sure the input ids are properly formatted. \n2. the\
    \ outputs we have match 1-1 with the original outputs. But when you generate with\
    \ sampling and custom temperature etc, you should expect some hallucination. Especially\
    \ if the eos token is not properly set, the model will not stop early enough :/\
    \ \n\nThanks for opening the issue!"
  created_at: 2023-09-20 14:39:46+00:00
  edited: false
  hidden: false
  id: 650b12425d9ce91d31d993a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dc5ed1eee8b70ca13665189ee3bdebd1.svg
      fullname: Max Erler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maximotus
      type: user
    createdAt: '2023-12-06T09:48:55.000Z'
    data:
      edited: false
      editors:
      - maximotus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9668585062026978
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dc5ed1eee8b70ca13665189ee3bdebd1.svg
          fullname: Max Erler
          isHf: false
          isPro: false
          name: maximotus
          type: user
        html: '<p>Regarding the unnecessary additional code, In my case it was helpful
          to use a repetition penalty of 0.9. Maybe that helps in your case as well!
          :)</p>

          '
        raw: Regarding the unnecessary additional code, In my case it was helpful
          to use a repetition penalty of 0.9. Maybe that helps in your case as well!
          :)
        updatedAt: '2023-12-06T09:48:55.911Z'
      numEdits: 0
      reactions: []
    id: 65704387fdc0cef3785bdf28
    type: comment
  author: maximotus
  content: Regarding the unnecessary additional code, In my case it was helpful to
    use a repetition penalty of 0.9. Maybe that helps in your case as well! :)
  created_at: 2023-12-06 09:48:55+00:00
  edited: false
  hidden: false
  id: 65704387fdc0cef3785bdf28
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: codellama/CodeLlama-7b-hf
repo_type: model
status: open
target_branch: null
title: Issue with using the codellama-7b model
