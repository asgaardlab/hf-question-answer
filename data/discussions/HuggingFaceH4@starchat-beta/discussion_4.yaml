!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tjohnson
conflicting_files: null
created_at: 2023-06-11 14:20:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zDyFyY_YVuskjhMKOIJxg.jpeg?w=200&h=200&f=face
      fullname: Tailor Johnson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tjohnson
      type: user
    createdAt: '2023-06-11T15:20:29.000Z'
    data:
      edited: false
      editors:
      - tjohnson
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9468653202056885
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zDyFyY_YVuskjhMKOIJxg.jpeg?w=200&h=200&f=face
          fullname: Tailor Johnson
          isHf: false
          isPro: false
          name: tjohnson
          type: user
        html: '<p>Hello,</p>

          <p>Thank you for such a tremendous contribution! I have tried running inference
          on my RTX4090 (24GB vram) to no avail so I used TheBloke''s rendition of
          GGML and GPTQ which work great but verrrrry slow. Which is in direct contrast
          to your starchat playground which is lightning fast...</p>

          <p>I would like to try inference with this repos (native) weights on a GPU
          to get somewhere in the ballpark of the speed of your playground but how
          many GB  do I need?  Do I need to rent like an A100 80?</p>

          '
        raw: "Hello,\r\n\r\nThank you for such a tremendous contribution! I have tried\
          \ running inference on my RTX4090 (24GB vram) to no avail so I used TheBloke's\
          \ rendition of GGML and GPTQ which work great but verrrrry slow. Which is\
          \ in direct contrast to your starchat playground which is lightning fast...\r\
          \n\r\nI would like to try inference with this repos (native) weights on\
          \ a GPU to get somewhere in the ballpark of the speed of your playground\
          \ but how many GB  do I need?  Do I need to rent like an A100 80?"
        updatedAt: '2023-06-11T15:20:29.391Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - avspavan
    id: 6485e63dd20fe39906931c73
    type: comment
  author: tjohnson
  content: "Hello,\r\n\r\nThank you for such a tremendous contribution! I have tried\
    \ running inference on my RTX4090 (24GB vram) to no avail so I used TheBloke's\
    \ rendition of GGML and GPTQ which work great but verrrrry slow. Which is in direct\
    \ contrast to your starchat playground which is lightning fast...\r\n\r\nI would\
    \ like to try inference with this repos (native) weights on a GPU to get somewhere\
    \ in the ballpark of the speed of your playground but how many GB  do I need?\
    \  Do I need to rent like an A100 80?"
  created_at: 2023-06-11 14:20:29+00:00
  edited: false
  hidden: false
  id: 6485e63dd20fe39906931c73
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/321e2654794d4e860a2a526ff259dcad.svg
      fullname: Pavan Akkisetty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: avspavan
      type: user
    createdAt: '2023-06-12T21:53:28.000Z'
    data:
      edited: false
      editors:
      - avspavan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9507009387016296
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/321e2654794d4e860a2a526ff259dcad.svg
          fullname: Pavan Akkisetty
          isHf: false
          isPro: false
          name: avspavan
          type: user
        html: '<p>Ditto. I have the same question.</p>

          '
        raw: Ditto. I have the same question.
        updatedAt: '2023-06-12T21:53:28.617Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - tjohnson
    id: 648793d85cbb2b89d879431c
    type: comment
  author: avspavan
  content: Ditto. I have the same question.
  created_at: 2023-06-12 20:53:28+00:00
  edited: false
  hidden: false
  id: 648793d85cbb2b89d879431c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/b5X7LnHkwvtKUnMweIazj.png?w=200&h=200&f=face
      fullname: valdanito
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: valdanito
      type: user
    createdAt: '2023-06-14T01:18:03.000Z'
    data:
      edited: false
      editors:
      - valdanito
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9641353487968445
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/b5X7LnHkwvtKUnMweIazj.png?w=200&h=200&f=face
          fullname: valdanito
          isHf: false
          isPro: false
          name: valdanito
          type: user
        html: '<p>I''m running it on an A100 80 and most of the time it''s using 30GB
          of VRAM, peaking at 48GB.</p>

          '
        raw: I'm running it on an A100 80 and most of the time it's using 30GB of
          VRAM, peaking at 48GB.
        updatedAt: '2023-06-14T01:18:03.041Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - tjohnson
        - qftie
        - avspavan
      - count: 1
        reaction: "\U0001F44D"
        users:
        - avspavan
    id: 6489154bed19a62cae5b4884
    type: comment
  author: valdanito
  content: I'm running it on an A100 80 and most of the time it's using 30GB of VRAM,
    peaking at 48GB.
  created_at: 2023-06-14 00:18:03+00:00
  edited: false
  hidden: false
  id: 6489154bed19a62cae5b4884
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zDyFyY_YVuskjhMKOIJxg.jpeg?w=200&h=200&f=face
      fullname: Tailor Johnson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tjohnson
      type: user
    createdAt: '2023-06-14T12:06:13.000Z'
    data:
      edited: false
      editors:
      - tjohnson
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.35649654269218445
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zDyFyY_YVuskjhMKOIJxg.jpeg?w=200&h=200&f=face
          fullname: Tailor Johnson
          isHf: false
          isPro: false
          name: tjohnson
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;valdanito&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/valdanito\">@<span class=\"\
          underline\">valdanito</span></a></span>\n\n\t</span></span> thank you</p>\n"
        raw: '@valdanito thank you'
        updatedAt: '2023-06-14T12:06:13.581Z'
      numEdits: 0
      reactions: []
    id: 6489ad3519959a82a0270fd5
    type: comment
  author: tjohnson
  content: '@valdanito thank you'
  created_at: 2023-06-14 11:06:13+00:00
  edited: false
  hidden: false
  id: 6489ad3519959a82a0270fd5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c19f71cf9e34b15b996a9b922ed8614.svg
      fullname: Massimiliano
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maxrubino
      type: user
    createdAt: '2023-06-30T14:11:12.000Z'
    data:
      edited: false
      editors:
      - Maxrubino
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.624264657497406
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c19f71cf9e34b15b996a9b922ed8614.svg
          fullname: Massimiliano
          isHf: false
          isPro: false
          name: Maxrubino
          type: user
        html: '<p>If you want to safe money, you should import it in 4-bit mode you
          need only 10gb of GPU RAM</p>

          <p>More info:  <a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">Making
          LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a></p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM


          tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"HuggingFaceH4/starchat-beta"</span>)

          model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">"HuggingFaceH4/starchat-beta"</span>,load_in_4bit=<span
          class="hljs-literal">True</span>, device_map=<span class="hljs-string">"auto"</span>)

          </code></pre>

          '
        raw: 'If you want to safe money, you should import it in 4-bit mode you need
          only 10gb of GPU RAM


          More info:  [Making LLMs even more accessible with bitsandbytes, 4-bit quantization
          and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)

          ```python

          from transformers import AutoTokenizer, AutoModelForCausalLM


          tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/starchat-beta")

          model = AutoModelForCausalLM.from_pretrained("HuggingFaceH4/starchat-beta",load_in_4bit=True,
          device_map="auto")

          ```'
        updatedAt: '2023-06-30T14:11:12.839Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - tjohnson
        - abhilashpal
      - count: 1
        reaction: "\U0001F92F"
        users:
        - sidfyndx
    id: 649ee2809ca7e67fb5e56d1d
    type: comment
  author: Maxrubino
  content: 'If you want to safe money, you should import it in 4-bit mode you need
    only 10gb of GPU RAM


    More info:  [Making LLMs even more accessible with bitsandbytes, 4-bit quantization
    and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)

    ```python

    from transformers import AutoTokenizer, AutoModelForCausalLM


    tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/starchat-beta")

    model = AutoModelForCausalLM.from_pretrained("HuggingFaceH4/starchat-beta",load_in_4bit=True,
    device_map="auto")

    ```'
  created_at: 2023-06-30 13:11:12+00:00
  edited: false
  hidden: false
  id: 649ee2809ca7e67fb5e56d1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b066d8102291cbd06104581aa8650240.svg
      fullname: Simon Haxby
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: avicennax
      type: user
    createdAt: '2023-07-17T02:48:18.000Z'
    data:
      edited: false
      editors:
      - avicennax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6002556085586548
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b066d8102291cbd06104581aa8650240.svg
          fullname: Simon Haxby
          isHf: false
          isPro: false
          name: avicennax
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Maxrubino&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Maxrubino\">@<span class=\"\
          underline\">Maxrubino</span></a></span>\n\n\t</span></span> what versions\
          \ of related quantization dependencies are you running? I get this exception\
          \ on the last line:</p>\n<pre><code>TypeError: GPTBigCodeForCausalLM.__init__()\
          \ got an unexpected keyword argument 'load_in_4bit'\n</code></pre>\n"
        raw: '@Maxrubino what versions of related quantization dependencies are you
          running? I get this exception on the last line:


          ```

          TypeError: GPTBigCodeForCausalLM.__init__() got an unexpected keyword argument
          ''load_in_4bit''

          ```

          '
        updatedAt: '2023-07-17T02:48:18.888Z'
      numEdits: 0
      reactions: []
    id: 64b4abf24c3cc95a7574a786
    type: comment
  author: avicennax
  content: '@Maxrubino what versions of related quantization dependencies are you
    running? I get this exception on the last line:


    ```

    TypeError: GPTBigCodeForCausalLM.__init__() got an unexpected keyword argument
    ''load_in_4bit''

    ```

    '
  created_at: 2023-07-17 01:48:18+00:00
  edited: false
  hidden: false
  id: 64b4abf24c3cc95a7574a786
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c19f71cf9e34b15b996a9b922ed8614.svg
      fullname: Massimiliano
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maxrubino
      type: user
    createdAt: '2023-07-17T07:07:54.000Z'
    data:
      edited: false
      editors:
      - Maxrubino
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6430708169937134
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c19f71cf9e34b15b996a9b922ed8614.svg
          fullname: Massimiliano
          isHf: false
          isPro: false
          name: Maxrubino
          type: user
        html: '<p>transformers==4.30.2</p>

          '
        raw: 'transformers==4.30.2

          '
        updatedAt: '2023-07-17T07:07:54.056Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - avicennax
    id: 64b4e8cad52d67c01c042478
    type: comment
  author: Maxrubino
  content: 'transformers==4.30.2

    '
  created_at: 2023-07-17 06:07:54+00:00
  edited: false
  hidden: false
  id: 64b4e8cad52d67c01c042478
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: HuggingFaceH4/starchat-beta
repo_type: model
status: open
target_branch: null
title: Inference VRAM Size
