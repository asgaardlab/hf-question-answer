!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vermanic
conflicting_files: null
created_at: 2023-09-22 08:55:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
      fullname: Nikhil Verma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vermanic
      type: user
    createdAt: '2023-09-22T09:55:36.000Z'
    data:
      edited: false
      editors:
      - vermanic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7875591516494751
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
          fullname: Nikhil Verma
          isHf: false
          isPro: false
          name: vermanic
          type: user
        html: "<p>I am trying to do SFT for a model: <code>bigcode/starcoderbase-1b</code>\
          \ on 80Gb Gpu machine. (g5.12xlarge)</p>\n<p>SFT Dataset: <a href=\"https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k\"\
          >https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k</a> (~800bytes/row\
          \ * 1000rows) (my dataset is different and bigger than this, but I am trying\
          \ this for a benchmark)</p>\n<p>I can load the model for inferencing with\
          \ 5GB GPU memory consumed using: <code>AutoModelForCausalLM.from_pretrained(model_name,\
          \ device_map='auto')</code></p>\n<p>But when I do SFT with </p>\n<pre><code>num_train_epochs\
          \ = 1 \n\nper_device_train_batch_size = 1\n\nper_device_eval_batch_size\
          \ = 1\n</code></pre>\n<p>but it consumes 30 GB across the SFT. (total: 35)</p>\n\
          <ol>\n<li><p>Does SFT take so much memory over (loading model as a checkpoint\
          \ and inferencing) ?</p>\n</li>\n<li><p>And any way I can do this in less\
          \ memory and more time ? (as I plan to do SFT with a 15B model which takes\
          \ ~60GB just for loading model checkpoint)</p>\n</li>\n</ol>\n<p>(End goal\
          \ being using a 15B model, the 4-bit quantised version of model takes 10-12\
          \ GB for model checkpoint and additional 50GB for SFT) (can't do SFT over\
          \ non-quantised as that cause below error)</p>\n<pre><code>    return F.dropout(input,\
          \ self.p, self.training, self.inplace)\n  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\"\
          , line 1252, in dropout\n    return _VF.dropout_(input, p, training) if\
          \ inplace else _VF.dropout(input, p, training)\ntorch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 22.04 GiB total\
          \ capacity; 20.72 GiB already allocated; 43.12 MiB free; 20.87 GiB reserved\
          \ in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try\
          \ setting max_split_size_mb to avoid fragmentation.  See documentation for\
          \ Memory Management and PYTORCH_CUDA_ALLOC_CONF\n</code></pre>\n<p>tried:\
          \ <code>!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512'</code> \
          \ (still same issue)</p>\n"
        raw: "I am trying to do SFT for a model: `bigcode/starcoderbase-1b` on 80Gb\
          \ Gpu machine. (g5.12xlarge)\r\n\r\nSFT Dataset: https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k\
          \ (~800bytes/row * 1000rows) (my dataset is different and bigger than this,\
          \ but I am trying this for a benchmark)\r\n\r\nI can load the model for\
          \ inferencing with 5GB GPU memory consumed using: `AutoModelForCausalLM.from_pretrained(model_name,\
          \ device_map='auto')`\r\n\r\nBut when I do SFT with \r\n```\r\nnum_train_epochs\
          \ = 1 \r\n\r\nper_device_train_batch_size = 1\r\n\r\nper_device_eval_batch_size\
          \ = 1\r\n```\r\n\r\nbut it consumes 30 GB across the SFT. (total: 35)\r\n\
          \r\n1. Does SFT take so much memory over (loading model as a checkpoint\
          \ and inferencing) ?\r\n\r\n2. And any way I can do this in less memory\
          \ and more time ? (as I plan to do SFT with a 15B model which takes ~60GB\
          \ just for loading model checkpoint)\r\n\r\n\r\n(End goal being using a\
          \ 15B model, the 4-bit quantised version of model takes 10-12 GB for model\
          \ checkpoint and additional 50GB for SFT) (can't do SFT over non-quantised\
          \ as that cause below error)\r\n```\r\n    return F.dropout(input, self.p,\
          \ self.training, self.inplace)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\"\
          , line 1252, in dropout\r\n    return _VF.dropout_(input, p, training) if\
          \ inplace else _VF.dropout(input, p, training)\r\ntorch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 22.04 GiB total\
          \ capacity; 20.72 GiB already allocated; 43.12 MiB free; 20.87 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\n\r\ntried: `!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512'`\
          \  (still same issue)"
        updatedAt: '2023-09-22T09:55:36.924Z'
      numEdits: 0
      reactions: []
    id: 650d649836ac7eba06fd9cbb
    type: comment
  author: vermanic
  content: "I am trying to do SFT for a model: `bigcode/starcoderbase-1b` on 80Gb\
    \ Gpu machine. (g5.12xlarge)\r\n\r\nSFT Dataset: https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k\
    \ (~800bytes/row * 1000rows) (my dataset is different and bigger than this, but\
    \ I am trying this for a benchmark)\r\n\r\nI can load the model for inferencing\
    \ with 5GB GPU memory consumed using: `AutoModelForCausalLM.from_pretrained(model_name,\
    \ device_map='auto')`\r\n\r\nBut when I do SFT with \r\n```\r\nnum_train_epochs\
    \ = 1 \r\n\r\nper_device_train_batch_size = 1\r\n\r\nper_device_eval_batch_size\
    \ = 1\r\n```\r\n\r\nbut it consumes 30 GB across the SFT. (total: 35)\r\n\r\n\
    1. Does SFT take so much memory over (loading model as a checkpoint and inferencing)\
    \ ?\r\n\r\n2. And any way I can do this in less memory and more time ? (as I plan\
    \ to do SFT with a 15B model which takes ~60GB just for loading model checkpoint)\r\
    \n\r\n\r\n(End goal being using a 15B model, the 4-bit quantised version of model\
    \ takes 10-12 GB for model checkpoint and additional 50GB for SFT) (can't do SFT\
    \ over non-quantised as that cause below error)\r\n```\r\n    return F.dropout(input,\
    \ self.p, self.training, self.inplace)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\"\
    , line 1252, in dropout\r\n    return _VF.dropout_(input, p, training) if inplace\
    \ else _VF.dropout(input, p, training)\r\ntorch.cuda.OutOfMemoryError: CUDA out\
    \ of memory. Tried to allocate 192.00 MiB (GPU 0; 22.04 GiB total capacity; 20.72\
    \ GiB already allocated; 43.12 MiB free; 20.87 GiB reserved in total by PyTorch)\
    \ If reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
    \n```\r\n\r\ntried: `!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512'`\
    \  (still same issue)"
  created_at: 2023-09-22 08:55:36+00:00
  edited: false
  hidden: false
  id: 650d649836ac7eba06fd9cbb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 29
repo_id: HuggingFaceH4/starchat-beta
repo_type: model
status: open
target_branch: null
title: SFT taking high memory with Transformers (>5x the amount it takes to load model
  checkpoint )
