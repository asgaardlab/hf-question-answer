!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vermanic
conflicting_files: null
created_at: 2023-09-07 16:53:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
      fullname: Nikhil Verma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vermanic
      type: user
    createdAt: '2023-09-07T17:53:22.000Z'
    data:
      edited: false
      editors:
      - vermanic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.677327573299408
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
          fullname: Nikhil Verma
          isHf: false
          isPro: false
          name: vermanic
          type: user
        html: "<p>So the output of my model ends abruptly and I ideally want it to\
          \ complete the paragraph/sentences/code which it was it between of.<br>Although\
          \ I have provided max_new_tokens = 300 and also in prompt I give to limit\
          \ by 300 words.</p>\n<p>The response is always big and ends abruptly. Any\
          \ way I can ask for a complete output within desired number of output tokens?\
          \ </p>\n<p>Code:</p>\n<pre><code>checkpoint = \"HuggingFaceH4/starchat-beta\"\
          \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" \nclass StarCoderModel:\n\
          \  def __init__(self):\n    print(\"Running in \" + device)\n    self.tokenizer\
          \ = AutoTokenizer.from_pretrained(checkpoint)\n    self.model = AutoModelForCausalLM.from_pretrained(checkpoint,\
          \ device_map='auto')\n\n  def infer(self, input_text, token_count):\n  \
          \  inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\
          \  \n    outputs = self.model.generate(inputs,  max_new_tokens=token_count,\
          \ pad_token_id=self.tokenizer.eos_token_id)\n    return self.tokenizer.decode(outputs[0])[len(input_text):]\n\
          </code></pre>\n<p>Sample:</p>\n<pre><code>private DataType FuntionName(String\
          \ someId) {\n    // TODO: Replace with implementation that utilizes someId\
          \ to obtain information\n    return DataType.Value;\n}\n\n\nThe comment:\n\
          \n- If someId is present in the code, use the getAPI from Client with someId\
          \ as a parameter to obtain some information.\n- If the\n</code></pre>\n"
        raw: "So the output of my model ends abruptly and I ideally want it to complete\
          \ the paragraph/sentences/code which it was it between of.\r\nAlthough I\
          \ have provided max_new_tokens = 300 and also in prompt I give to limit\
          \ by 300 words.\r\n\r\nThe response is always big and ends abruptly. Any\
          \ way I can ask for a complete output within desired number of output tokens?\
          \ \r\n\r\nCode:\r\n```\r\ncheckpoint = \"HuggingFaceH4/starchat-beta\"\r\
          \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" \r\nclass\
          \ StarCoderModel:\r\n  def __init__(self):\r\n    print(\"Running in \"\
          \ + device)\r\n    self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\
          \n    self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto')\r\
          \n\r\n  def infer(self, input_text, token_count):\r\n    inputs = self.tokenizer.encode(input_text,\
          \ return_tensors=\"pt\").to(device)  \r\n    outputs = self.model.generate(inputs,\
          \  max_new_tokens=token_count, pad_token_id=self.tokenizer.eos_token_id)\r\
          \n    return self.tokenizer.decode(outputs[0])[len(input_text):]\r\n```\r\
          \n\r\n\r\nSample:\r\n\r\n```\r\nprivate DataType FuntionName(String someId)\
          \ {\r\n    // TODO: Replace with implementation that utilizes someId to\
          \ obtain information\r\n    return DataType.Value;\r\n}\r\n\r\n\r\nThe comment:\r\
          \n\r\n- If someId is present in the code, use the getAPI from Client with\
          \ someId as a parameter to obtain some information.\r\n- If the\r\n\r\n\
          ```"
        updatedAt: '2023-09-07T17:53:22.291Z'
      numEdits: 0
      reactions: []
    id: 64fa0e127eeb6dbf4064e527
    type: comment
  author: vermanic
  content: "So the output of my model ends abruptly and I ideally want it to complete\
    \ the paragraph/sentences/code which it was it between of.\r\nAlthough I have\
    \ provided max_new_tokens = 300 and also in prompt I give to limit by 300 words.\r\
    \n\r\nThe response is always big and ends abruptly. Any way I can ask for a complete\
    \ output within desired number of output tokens? \r\n\r\nCode:\r\n```\r\ncheckpoint\
    \ = \"HuggingFaceH4/starchat-beta\"\r\ndevice = \"cuda\" if torch.cuda.is_available()\
    \ else \"cpu\" \r\nclass StarCoderModel:\r\n  def __init__(self):\r\n    print(\"\
    Running in \" + device)\r\n    self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\
    \n    self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto')\r\
    \n\r\n  def infer(self, input_text, token_count):\r\n    inputs = self.tokenizer.encode(input_text,\
    \ return_tensors=\"pt\").to(device)  \r\n    outputs = self.model.generate(inputs,\
    \  max_new_tokens=token_count, pad_token_id=self.tokenizer.eos_token_id)\r\n \
    \   return self.tokenizer.decode(outputs[0])[len(input_text):]\r\n```\r\n\r\n\r\
    \nSample:\r\n\r\n```\r\nprivate DataType FuntionName(String someId) {\r\n    //\
    \ TODO: Replace with implementation that utilizes someId to obtain information\r\
    \n    return DataType.Value;\r\n}\r\n\r\n\r\nThe comment:\r\n\r\n- If someId is\
    \ present in the code, use the getAPI from Client with someId as a parameter to\
    \ obtain some information.\r\n- If the\r\n\r\n```"
  created_at: 2023-09-07 16:53:22+00:00
  edited: false
  hidden: false
  id: 64fa0e127eeb6dbf4064e527
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f892bb53b710fbecba62800787f49de9.svg
      fullname: Dou Yuxiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: douyuxiao
      type: user
    createdAt: '2023-09-12T06:01:04.000Z'
    data:
      edited: false
      editors:
      - douyuxiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9847742915153503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f892bb53b710fbecba62800787f49de9.svg
          fullname: Dou Yuxiao
          isHf: false
          isPro: false
          name: douyuxiao
          type: user
        html: '<p>I have the same question.</p>

          '
        raw: I have the same question.
        updatedAt: '2023-09-12T06:01:04.825Z'
      numEdits: 0
      reactions: []
    id: 64fffea096c544b3a6defc3f
    type: comment
  author: douyuxiao
  content: I have the same question.
  created_at: 2023-09-12 05:01:04+00:00
  edited: false
  hidden: false
  id: 64fffea096c544b3a6defc3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43734a3ef2f90f3dbd4d2e5a31ea025.svg
      fullname: vamsi K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sunny77
      type: user
    createdAt: '2023-12-15T18:22:39.000Z'
    data:
      edited: false
      editors:
      - sunny77
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9849426746368408
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43734a3ef2f90f3dbd4d2e5a31ea025.svg
          fullname: vamsi K
          isHf: false
          isPro: false
          name: sunny77
          type: user
        html: '<p>I''m having the same problem. Were you able to resolve this?</p>

          '
        raw: I'm having the same problem. Were you able to resolve this?
        updatedAt: '2023-12-15T18:22:39.215Z'
      numEdits: 0
      reactions: []
    id: 657c996f869d5bb0e532f728
    type: comment
  author: sunny77
  content: I'm having the same problem. Were you able to resolve this?
  created_at: 2023-12-15 18:22:39+00:00
  edited: false
  hidden: false
  id: 657c996f869d5bb0e532f728
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
      fullname: Pradeep T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pradeep1995
      type: user
    createdAt: '2023-12-20T06:06:58.000Z'
    data:
      edited: false
      editors:
      - Pradeep1995
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8375497460365295
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
          fullname: Pradeep T
          isHf: false
          isPro: false
          name: Pradeep1995
          type: user
        html: '<p>any update on this?</p>

          '
        raw: any update on this?
        updatedAt: '2023-12-20T06:06:58.740Z'
      numEdits: 0
      reactions: []
    id: 6582848241dad8f2f308a22a
    type: comment
  author: Pradeep1995
  content: any update on this?
  created_at: 2023-12-20 06:06:58+00:00
  edited: false
  hidden: false
  id: 6582848241dad8f2f308a22a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2cd955ba57c752fffd4943f7c070c729.svg
      fullname: 'Chintha Sai Charan '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saicharan8
      type: user
    createdAt: '2024-01-16T08:59:45.000Z'
    data:
      edited: false
      editors:
      - saicharan8
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8417139053344727
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2cd955ba57c752fffd4943f7c070c729.svg
          fullname: 'Chintha Sai Charan '
          isHf: false
          isPro: false
          name: saicharan8
          type: user
        html: '<p>same problem</p>

          '
        raw: 'same problem

          '
        updatedAt: '2024-01-16T08:59:45.493Z'
      numEdits: 0
      reactions: []
    id: 65a6458193d165b6e506ef60
    type: comment
  author: saicharan8
  content: 'same problem

    '
  created_at: 2024-01-16 08:59:45+00:00
  edited: false
  hidden: false
  id: 65a6458193d165b6e506ef60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
      fullname: Nikhil Verma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vermanic
      type: user
    createdAt: '2024-01-16T09:07:08.000Z'
    data:
      edited: false
      editors:
      - vermanic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9994736313819885
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
          fullname: Nikhil Verma
          isHf: false
          isPro: false
          name: vermanic
          type: user
        html: '<p>Hey, was not able to resolve this.</p>

          '
        raw: Hey, was not able to resolve this.
        updatedAt: '2024-01-16T09:07:08.718Z'
      numEdits: 0
      reactions: []
    id: 65a6473cd81b6fa6c8c1a956
    type: comment
  author: vermanic
  content: Hey, was not able to resolve this.
  created_at: 2024-01-16 09:07:08+00:00
  edited: false
  hidden: false
  id: 65a6473cd81b6fa6c8c1a956
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 27
repo_id: HuggingFaceH4/starchat-beta
repo_type: model
status: open
target_branch: null
title: Incomplete Output even with max_new_tokens
