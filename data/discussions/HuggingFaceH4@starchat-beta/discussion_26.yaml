!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vermanic
conflicting_files: null
created_at: 2023-09-07 09:23:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
      fullname: Nikhil Verma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vermanic
      type: user
    createdAt: '2023-09-07T10:23:18.000Z'
    data:
      edited: false
      editors:
      - vermanic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5974907279014587
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
          fullname: Nikhil Verma
          isHf: false
          isPro: false
          name: vermanic
          type: user
        html: "<p>Hey, i have this general problem of any model on HF outputting with\
          \ the input prompt always, any way to exclude that?</p>\n<p>as I just need\
          \ the output.</p>\n<p>Code:</p>\n<pre><code>from transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\nimport torch\n\ncheckpoint = \"HuggingFaceH4/starchat-beta\"\
          \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # \"cuda:X\"\
          \ for GPU usage or \"cpu\" for CPU usage\n\n\nclass Model:\n    def __init__(self):\n\
          \        print(\"Running in \" + device)\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
          \        self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto')\n\
          \n    def infer(self, input_text, token_count):\n        inputs = self.tokenizer.encode(input_text,\
          \ return_tensors=\"pt\").to(device)\n        outputs = self.model.generate(inputs,\
          \ max_new_tokens=token_count)\n        return self.tokenizer.decode(outputs[0])\n\
          </code></pre>\n<p>Also, <code>max_new_tokens</code> means the amount of\
          \ tokens with which I want the model to respond with, right?</p>\n"
        raw: "Hey, i have this general problem of any model on HF outputting with\
          \ the input prompt always, any way to exclude that?\r\n\r\nas I just need\
          \ the output.\r\n\r\nCode:\r\n```\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\r\nimport torch\r\n\r\ncheckpoint = \"HuggingFaceH4/starchat-beta\"\
          \r\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # \"cuda:X\"\
          \ for GPU usage or \"cpu\" for CPU usage\r\n\r\n\r\nclass Model:\r\n   \
          \ def __init__(self):\r\n        print(\"Running in \" + device)\r\n   \
          \     self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\n   \
          \     self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto')\r\
          \n\r\n    def infer(self, input_text, token_count):\r\n        inputs =\
          \ self.tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\r\n\
          \        outputs = self.model.generate(inputs, max_new_tokens=token_count)\r\
          \n        return self.tokenizer.decode(outputs[0])\r\n```\r\n\r\nAlso, `max_new_tokens`\
          \ means the amount of tokens with which I want the model to respond with,\
          \ right?"
        updatedAt: '2023-09-07T10:23:18.973Z'
      numEdits: 0
      reactions: []
    id: 64f9a4965cce4f4e8f339310
    type: comment
  author: vermanic
  content: "Hey, i have this general problem of any model on HF outputting with the\
    \ input prompt always, any way to exclude that?\r\n\r\nas I just need the output.\r\
    \n\r\nCode:\r\n```\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
    \nimport torch\r\n\r\ncheckpoint = \"HuggingFaceH4/starchat-beta\"\r\ndevice =\
    \ \"cuda\" if torch.cuda.is_available() else \"cpu\"  # \"cuda:X\" for GPU usage\
    \ or \"cpu\" for CPU usage\r\n\r\n\r\nclass Model:\r\n    def __init__(self):\r\
    \n        print(\"Running in \" + device)\r\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\
    \n        self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto')\r\
    \n\r\n    def infer(self, input_text, token_count):\r\n        inputs = self.tokenizer.encode(input_text,\
    \ return_tensors=\"pt\").to(device)\r\n        outputs = self.model.generate(inputs,\
    \ max_new_tokens=token_count)\r\n        return self.tokenizer.decode(outputs[0])\r\
    \n```\r\n\r\nAlso, `max_new_tokens` means the amount of tokens with which I want\
    \ the model to respond with, right?"
  created_at: 2023-09-07 09:23:18+00:00
  edited: false
  hidden: false
  id: 64f9a4965cce4f4e8f339310
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
      fullname: Nikhil Verma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vermanic
      type: user
    createdAt: '2023-09-07T10:23:35.000Z'
    data:
      from: Every Inference gives the prompt as part of output also, any way to fix
        this?
      to: Every Inference gives the prompt as part of output also, any way to remove
        that?
    id: 64f9a4a7698370119fae9d67
    type: title-change
  author: vermanic
  created_at: 2023-09-07 09:23:35+00:00
  id: 64f9a4a7698370119fae9d67
  new_title: Every Inference gives the prompt as part of output also, any way to remove
    that?
  old_title: Every Inference gives the prompt as part of output also, any way to fix
    this?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
      fullname: Nikhil Verma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vermanic
      type: user
    createdAt: '2023-09-07T17:54:19.000Z'
    data:
      edited: false
      editors:
      - vermanic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3645496368408203
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
          fullname: Nikhil Verma
          isHf: false
          isPro: false
          name: vermanic
          type: user
        html: '<p>Resolved by:</p>

          <pre><code>return self.tokenizer.decode(outputs[0])[len(input_text):]

          </code></pre>

          '
        raw: 'Resolved by:

          ```

          return self.tokenizer.decode(outputs[0])[len(input_text):]

          ```'
        updatedAt: '2023-09-07T17:54:19.817Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64fa0e4bb8d50cebd60a0645
    id: 64fa0e4bb8d50cebd60a0644
    type: comment
  author: vermanic
  content: 'Resolved by:

    ```

    return self.tokenizer.decode(outputs[0])[len(input_text):]

    ```'
  created_at: 2023-09-07 16:54:19+00:00
  edited: false
  hidden: false
  id: 64fa0e4bb8d50cebd60a0644
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
      fullname: Nikhil Verma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vermanic
      type: user
    createdAt: '2023-09-07T17:54:19.000Z'
    data:
      status: closed
    id: 64fa0e4bb8d50cebd60a0645
    type: status-change
  author: vermanic
  created_at: 2023-09-07 16:54:19+00:00
  id: 64fa0e4bb8d50cebd60a0645
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 26
repo_id: HuggingFaceH4/starchat-beta
repo_type: model
status: closed
target_branch: null
title: Every Inference gives the prompt as part of output also, any way to remove
  that?
