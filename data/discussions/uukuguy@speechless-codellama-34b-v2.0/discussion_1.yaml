!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KrisPi
conflicting_files: null
created_at: 2023-11-15 11:12:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9554e7f29422dc00d8c90e44c1ef330.svg
      fullname: Kris Podkanowicz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KrisPi
      type: user
    createdAt: '2023-11-15T11:12:22.000Z'
    data:
      edited: false
      editors:
      - KrisPi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9428519010543823
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9554e7f29422dc00d8c90e44c1ef330.svg
          fullname: Kris Podkanowicz
          isHf: false
          isPro: false
          name: KrisPi
          type: user
        html: '<p>It passes most of my internal Q&amp;As, really good model!</p>

          <p>I''m experimenting with your LORA extract, diffs and merges as well and
          I''m also doing some LIMA finetunes, which brings me to the key question:</p>

          <ul>

          <li>is this model finetuned from Phind v2 or base CodeLlama?</li>

          </ul>

          <p> I wonder if finetuning finetunes is a way forward or one of the rabbit
          holes? What''s your opinion?</p>

          <p>Thanks!</p>

          '
        raw: "It passes most of my internal Q&As, really good model!\r\n\r\nI'm experimenting\
          \ with your LORA extract, diffs and merges as well and I'm also doing some\
          \ LIMA finetunes, which brings me to the key question:\r\n\r\n- is this\
          \ model finetuned from Phind v2 or base CodeLlama? \r\n\r\n I wonder if\
          \ finetuning finetunes is a way forward or one of the rabbit holes? What's\
          \ your opinion?\r\n\r\nThanks!"
        updatedAt: '2023-11-15T11:12:22.005Z'
      numEdits: 0
      reactions: []
    id: 6554a796f87d7896a9c09dd6
    type: comment
  author: KrisPi
  content: "It passes most of my internal Q&As, really good model!\r\n\r\nI'm experimenting\
    \ with your LORA extract, diffs and merges as well and I'm also doing some LIMA\
    \ finetunes, which brings me to the key question:\r\n\r\n- is this model finetuned\
    \ from Phind v2 or base CodeLlama? \r\n\r\n I wonder if finetuning finetunes is\
    \ a way forward or one of the rabbit holes? What's your opinion?\r\n\r\nThanks!"
  created_at: 2023-11-15 11:12:22+00:00
  edited: false
  hidden: false
  id: 6554a796f87d7896a9c09dd6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64298a9b8852afdf89bd8846/it61DWZSkPbM4clfrLaPw.png?w=200&h=200&f=face
      fullname: Jiangwen Su
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: uukuguy
      type: user
    createdAt: '2023-11-15T12:49:09.000Z'
    data:
      edited: false
      editors:
      - uukuguy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9389363527297974
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64298a9b8852afdf89bd8846/it61DWZSkPbM4clfrLaPw.png?w=200&h=200&f=face
          fullname: Jiangwen Su
          isHf: false
          isPro: false
          name: uukuguy
          type: user
        html: '<p>Thank you for recognizing my current work.<br>My work goal is to
          apply the reasoning ability of large models to practical business scenarios.
          Combining with the currently known advanced work and insights in the industry,
          we have chosen a technical route based on a powerful code-capable base model.
          Therefore, the publicly available models so far are intermediate achievements
          rather than final goals. Under this guiding principle, efficient and powerful
          fine-tuning capability becomes a key foundational skill that enables us
          to respond to various hypothetical requirements of the base model at any
          time.<br>The timely appearance of CodeLlama has allowed us to smoothly carry
          out our planned work. The recent popular Mistral and Tora, which is a powerful
          mathematical reasoning model based on CodeLlama, are also our focus of attention.<br>The
          ongoing research on LoRA aims to achieve concurrent collaborative scenarios
          for multi-agent models, similar to implementing the concept of mixture-of-lora.</p>

          '
        raw: 'Thank you for recognizing my current work.

          My work goal is to apply the reasoning ability of large models to practical
          business scenarios. Combining with the currently known advanced work and
          insights in the industry, we have chosen a technical route based on a powerful
          code-capable base model. Therefore, the publicly available models so far
          are intermediate achievements rather than final goals. Under this guiding
          principle, efficient and powerful fine-tuning capability becomes a key foundational
          skill that enables us to respond to various hypothetical requirements of
          the base model at any time.

          The timely appearance of CodeLlama has allowed us to smoothly carry out
          our planned work. The recent popular Mistral and Tora, which is a powerful
          mathematical reasoning model based on CodeLlama, are also our focus of attention.

          The ongoing research on LoRA aims to achieve concurrent collaborative scenarios
          for multi-agent models, similar to implementing the concept of mixture-of-lora.'
        updatedAt: '2023-11-15T12:49:09.365Z'
      numEdits: 0
      reactions: []
    id: 6554be4540e95e9c41fa3c11
    type: comment
  author: uukuguy
  content: 'Thank you for recognizing my current work.

    My work goal is to apply the reasoning ability of large models to practical business
    scenarios. Combining with the currently known advanced work and insights in the
    industry, we have chosen a technical route based on a powerful code-capable base
    model. Therefore, the publicly available models so far are intermediate achievements
    rather than final goals. Under this guiding principle, efficient and powerful
    fine-tuning capability becomes a key foundational skill that enables us to respond
    to various hypothetical requirements of the base model at any time.

    The timely appearance of CodeLlama has allowed us to smoothly carry out our planned
    work. The recent popular Mistral and Tora, which is a powerful mathematical reasoning
    model based on CodeLlama, are also our focus of attention.

    The ongoing research on LoRA aims to achieve concurrent collaborative scenarios
    for multi-agent models, similar to implementing the concept of mixture-of-lora.'
  created_at: 2023-11-15 12:49:09+00:00
  edited: false
  hidden: false
  id: 6554be4540e95e9c41fa3c11
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: uukuguy/speechless-codellama-34b-v2.0
repo_type: model
status: open
target_branch: null
title: Amazing model
