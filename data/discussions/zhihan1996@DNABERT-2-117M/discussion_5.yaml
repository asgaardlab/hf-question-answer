!!python/object:huggingface_hub.community.DiscussionWithDetails
author: saberiato
conflicting_files: null
created_at: 2023-08-16 21:11:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e881038655065216dd9c9757fb7216b9.svg
      fullname: Ali Saberi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saberiato
      type: user
    createdAt: '2023-08-16T22:11:33.000Z'
    data:
      edited: false
      editors:
      - saberiato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.1604100614786148
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e881038655065216dd9c9757fb7216b9.svg
          fullname: Ali Saberi
          isHf: false
          isPro: false
          name: saberiato
          type: user
        html: '<p>Hi,<br>As instructed, when I run:</p>

          <pre><code>import torch

          from transformers import AutoModel

          model = AutoModel.from_pretrained("zhihan1996/DNABERT-2-117M", trust_remote_code=True)

          </code></pre>

          <p>I got this error:</p>

          <pre><code>ValueError: The model class you are passing has a `config_class`
          attribute that is not consistent with the config class you passed (model
          has &lt;class ''transformers.models.bert.configuration_bert.BertConfig''&gt;
          and you passed &lt;class ''transformers_modules.zhihan1996.DNABERT-2-117M.81ac6a98387cf94bc283553260f3fa6b88cef2fa.configuration_bert.BertConfig''&gt;.
          Fix one of those so they match!

          </code></pre>

          <p>And, when I set <code>trust_remote_code=False</code>, I got these warnings:</p>

          <pre><code>Some weights of BertModel were not initialized from the model
          checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: [''bert.encoder.layer.9.attention.self.value.bias'',
          ''bert.encoder.layer.9.attention.self.query.bias'', ''bert.encoder.layer.1.attention.self.key.weight'',
          ''bert.encoder.layer.3.intermediate.dense.weight'', ''bert.encoder.layer.4.output.dense.bias'',
          ''bert.encoder.layer.6.attention.self.value.bias'', ''bert.encoder.layer.2.attention.self.value.bias'',
          ''bert.encoder.layer.6.output.LayerNorm.weight'', ''bert.encoder.layer.10.attention.self.value.weight'',
          ''bert.encoder.layer.5.output.dense.weight'', ''bert.encoder.layer.3.attention.self.key.weight'',
          ''bert.encoder.layer.3.attention.self.query.weight'', ''bert.encoder.layer.10.output.dense.weight'',
          ''bert.encoder.layer.10.attention.self.value.bias'', ''bert.embeddings.position_embeddings.weight'',
          ''bert.encoder.layer.8.output.dense.bias'', ''bert.encoder.layer.3.attention.self.key.bias'',
          ''bert.encoder.layer.1.attention.self.query.weight'', ''bert.encoder.layer.6.attention.self.query.weight'',
          ''bert.encoder.layer.8.attention.self.value.bias'', ''bert.encoder.layer.8.attention.self.query.bias'',
          ''bert.encoder.layer.3.attention.self.query.bias'', ''bert.encoder.layer.5.attention.self.query.weight'',
          ''bert.encoder.layer.8.attention.self.query.weight'', ''bert.encoder.layer.5.attention.self.key.weight'',
          ''bert.encoder.layer.10.intermediate.dense.bias'', ''bert.encoder.layer.5.output.dense.bias'',
          ''bert.encoder.layer.10.output.LayerNorm.weight'', ''bert.encoder.layer.7.output.dense.bias'',
          ''bert.encoder.layer.0.attention.self.key.bias'', ''bert.encoder.layer.1.attention.self.value.bias'',
          ''bert.encoder.layer.9.output.LayerNorm.weight'', ''bert.encoder.layer.4.intermediate.dense.bias'',
          ''bert.encoder.layer.4.attention.self.query.weight'', ''bert.encoder.layer.6.intermediate.dense.bias'',
          ''bert.encoder.layer.8.attention.self.value.weight'', ''bert.encoder.layer.1.attention.self.query.bias'',
          ''bert.encoder.layer.10.output.dense.bias'', ''bert.encoder.layer.6.output.LayerNorm.bias'',
          ''bert.encoder.layer.11.output.LayerNorm.bias'', ''bert.encoder.layer.5.intermediate.dense.bias'',
          ''bert.pooler.dense.bias'', ''bert.encoder.layer.7.attention.self.key.weight'',
          ''bert.encoder.layer.4.attention.self.value.weight'', ''bert.encoder.layer.5.output.LayerNorm.bias'',
          ''bert.encoder.layer.4.output.LayerNorm.bias'', ''bert.encoder.layer.8.attention.self.key.weight'',
          ''bert.encoder.layer.9.attention.self.key.bias'', ''bert.encoder.layer.4.output.LayerNorm.weight'',
          ''bert.encoder.layer.0.intermediate.dense.weight'', ''bert.encoder.layer.8.output.dense.weight'',
          ''bert.encoder.layer.3.attention.self.value.bias'', ''bert.encoder.layer.6.intermediate.dense.weight'',
          ''bert.encoder.layer.2.intermediate.dense.weight'', ''bert.encoder.layer.8.output.LayerNorm.weight'',
          ''bert.encoder.layer.11.intermediate.dense.bias'', ''bert.encoder.layer.3.output.dense.bias'',
          ''bert.encoder.layer.0.attention.self.value.weight'', ''bert.encoder.layer.1.intermediate.dense.weight'',
          ''bert.encoder.layer.9.attention.self.key.weight'', ''bert.encoder.layer.7.output.dense.weight'',
          ''bert.encoder.layer.1.output.LayerNorm.bias'', ''bert.encoder.layer.2.attention.self.key.weight'',
          ''bert.encoder.layer.0.output.dense.weight'', ''bert.encoder.layer.7.output.LayerNorm.weight'',
          ''bert.encoder.layer.11.attention.self.value.bias'', ''bert.encoder.layer.7.attention.self.query.weight'',
          ''bert.encoder.layer.7.intermediate.dense.bias'', ''bert.encoder.layer.11.output.dense.weight'',
          ''bert.encoder.layer.4.attention.self.key.bias'', ''bert.encoder.layer.6.output.dense.bias'',
          ''bert.encoder.layer.7.intermediate.dense.weight'', ''bert.encoder.layer.6.attention.self.key.weight'',
          ''bert.encoder.layer.10.attention.self.query.weight'', ''bert.encoder.layer.4.attention.self.key.weight'',
          ''bert.encoder.layer.11.attention.self.key.weight'', ''bert.encoder.layer.9.output.dense.bias'',
          ''bert.encoder.layer.11.attention.self.key.bias'', ''bert.encoder.layer.11.output.LayerNorm.weight'',
          ''bert.encoder.layer.5.attention.self.value.weight'', ''bert.encoder.layer.1.attention.self.key.bias'',
          ''bert.encoder.layer.0.attention.self.key.weight'', ''bert.encoder.layer.2.output.dense.weight'',
          ''bert.encoder.layer.0.output.LayerNorm.weight'', ''bert.encoder.layer.9.output.dense.weight'',
          ''bert.encoder.layer.4.output.dense.weight'', ''bert.encoder.layer.5.attention.self.query.bias'',
          ''bert.encoder.layer.9.output.LayerNorm.bias'', ''bert.encoder.layer.1.intermediate.dense.bias'',
          ''bert.encoder.layer.6.attention.self.key.bias'', ''bert.encoder.layer.0.attention.self.query.weight'',
          ''bert.encoder.layer.11.intermediate.dense.weight'', ''bert.encoder.layer.0.output.LayerNorm.bias'',
          ''bert.encoder.layer.9.attention.self.value.weight'', ''bert.encoder.layer.3.attention.self.value.weight'',
          ''bert.encoder.layer.8.output.LayerNorm.bias'', ''bert.encoder.layer.9.intermediate.dense.bias'',
          ''bert.encoder.layer.2.attention.self.key.bias'', ''bert.encoder.layer.2.attention.self.value.weight'',
          ''bert.encoder.layer.2.attention.self.query.weight'', ''bert.encoder.layer.3.output.LayerNorm.weight'',
          ''bert.encoder.layer.5.attention.self.value.bias'', ''bert.encoder.layer.0.output.dense.bias'',
          ''bert.encoder.layer.10.attention.self.query.bias'', ''bert.encoder.layer.1.output.dense.weight'',
          ''bert.encoder.layer.7.attention.self.value.bias'', ''bert.encoder.layer.2.attention.self.query.bias'',
          ''bert.encoder.layer.0.attention.self.value.bias'', ''bert.encoder.layer.10.output.LayerNorm.bias'',
          ''bert.encoder.layer.7.output.LayerNorm.bias'', ''bert.encoder.layer.9.intermediate.dense.weight'',
          ''bert.encoder.layer.3.output.LayerNorm.bias'', ''bert.encoder.layer.11.attention.self.query.bias'',
          ''bert.encoder.layer.6.output.dense.weight'', ''bert.encoder.layer.7.attention.self.query.bias'',
          ''bert.encoder.layer.6.attention.self.value.weight'', ''bert.encoder.layer.0.attention.self.query.bias'',
          ''bert.encoder.layer.8.attention.self.key.bias'', ''bert.encoder.layer.2.output.LayerNorm.bias'',
          ''bert.encoder.layer.5.intermediate.dense.weight'', ''bert.encoder.layer.2.intermediate.dense.bias'',
          ''bert.encoder.layer.6.attention.self.query.bias'', ''bert.encoder.layer.5.output.LayerNorm.weight'',
          ''bert.encoder.layer.4.attention.self.value.bias'', ''bert.encoder.layer.1.output.LayerNorm.weight'',
          ''bert.encoder.layer.11.output.dense.bias'', ''bert.encoder.layer.11.attention.self.value.weight'',
          ''bert.encoder.layer.4.attention.self.query.bias'', ''bert.encoder.layer.4.intermediate.dense.weight'',
          ''bert.encoder.layer.10.attention.self.key.bias'', ''bert.encoder.layer.7.attention.self.value.weight'',
          ''bert.encoder.layer.10.attention.self.key.weight'', ''bert.encoder.layer.1.output.dense.bias'',
          ''bert.encoder.layer.8.intermediate.dense.bias'', ''bert.encoder.layer.11.attention.self.query.weight'',
          ''bert.encoder.layer.3.intermediate.dense.bias'', ''bert.pooler.dense.weight'',
          ''bert.encoder.layer.7.attention.self.key.bias'', ''bert.encoder.layer.2.output.dense.bias'',
          ''bert.encoder.layer.5.attention.self.key.bias'', ''bert.encoder.layer.10.intermediate.dense.weight'',
          ''bert.encoder.layer.8.intermediate.dense.weight'', ''bert.encoder.layer.2.output.LayerNorm.weight'',
          ''bert.encoder.layer.0.intermediate.dense.bias'', ''bert.encoder.layer.3.output.dense.weight'',
          ''bert.encoder.layer.9.attention.self.query.weight'', ''bert.encoder.layer.1.attention.self.value.weight'']

          You should probably TRAIN this model on a down-stream task to be able to
          use it for predictions and inference.

          </code></pre>

          <p>What should I do to have the pre-trained version of <code>DNABERT-2-117M</code>?<br>I
          want to fine-tune the model for another task.</p>

          '
        raw: "Hi,\r\nAs instructed, when I run:\r\n```\r\nimport torch\r\nfrom transformers\
          \ import AutoModel\r\nmodel = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\"\
          , trust_remote_code=True)\r\n```\r\nI got this error:\r\n```\r\nValueError:\
          \ The model class you are passing has a `config_class` attribute that is\
          \ not consistent with the config class you passed (model has <class 'transformers.models.bert.configuration_bert.BertConfig'>\
          \ and you passed <class 'transformers_modules.zhihan1996.DNABERT-2-117M.81ac6a98387cf94bc283553260f3fa6b88cef2fa.configuration_bert.BertConfig'>.\
          \ Fix one of those so they match!\r\n```\r\n\r\nAnd, when I set `trust_remote_code=False`,\
          \ I got these warnings:\r\n```\r\nSome weights of BertModel were not initialized\
          \ from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized:\
          \ ['bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.bias',\
          \ 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.3.intermediate.dense.weight',\
          \ 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias',\
          \ 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.6.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.5.output.dense.weight',\
          \ 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.weight',\
          \ 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.attention.self.value.bias',\
          \ 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.8.output.dense.bias',\
          \ 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight',\
          \ 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias',\
          \ 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.bias',\
          \ 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.weight',\
          \ 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.bias',\
          \ 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.0.attention.self.key.bias',\
          \ 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.9.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight',\
          \ 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.value.weight',\
          \ 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.10.output.dense.bias',\
          \ 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.pooler.dense.bias',\
          \ 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.value.weight',\
          \ 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias',\
          \ 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.weight',\
          \ 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias',\
          \ 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight',\
          \ 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias',\
          \ 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight',\
          \ 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight',\
          \ 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.0.output.dense.weight',\
          \ 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.bias',\
          \ 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.intermediate.dense.bias',\
          \ 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias',\
          \ 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight',\
          \ 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.weight',\
          \ 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.weight',\
          \ 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias',\
          \ 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.weight',\
          \ 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight',\
          \ 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.weight',\
          \ 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.4.output.dense.weight',\
          \ 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.key.bias',\
          \ 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.11.intermediate.dense.weight',\
          \ 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.value.weight',\
          \ 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.8.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias',\
          \ 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.query.weight',\
          \ 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.bias',\
          \ 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias',\
          \ 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias',\
          \ 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.value.bias',\
          \ 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias',\
          \ 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.6.output.dense.weight',\
          \ 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.value.weight',\
          \ 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.bias',\
          \ 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight',\
          \ 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias',\
          \ 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.value.bias',\
          \ 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias',\
          \ 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.query.bias',\
          \ 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias',\
          \ 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.key.weight',\
          \ 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.bias',\
          \ 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.3.intermediate.dense.bias',\
          \ 'bert.pooler.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias',\
          \ 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.5.attention.self.key.bias',\
          \ 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight',\
          \ 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.bias',\
          \ 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight',\
          \ 'bert.encoder.layer.1.attention.self.value.weight']\r\nYou should probably\
          \ TRAIN this model on a down-stream task to be able to use it for predictions\
          \ and inference.\r\n```\r\n\r\nWhat should I do to have the pre-trained\
          \ version of `DNABERT-2-117M`?\r\nI want to fine-tune the model for another\
          \ task."
        updatedAt: '2023-08-16T22:11:33.535Z'
      numEdits: 0
      reactions: []
    id: 64dd4995fdef63c2217ca190
    type: comment
  author: saberiato
  content: "Hi,\r\nAs instructed, when I run:\r\n```\r\nimport torch\r\nfrom transformers\
    \ import AutoModel\r\nmodel = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\"\
    , trust_remote_code=True)\r\n```\r\nI got this error:\r\n```\r\nValueError: The\
    \ model class you are passing has a `config_class` attribute that is not consistent\
    \ with the config class you passed (model has <class 'transformers.models.bert.configuration_bert.BertConfig'>\
    \ and you passed <class 'transformers_modules.zhihan1996.DNABERT-2-117M.81ac6a98387cf94bc283553260f3fa6b88cef2fa.configuration_bert.BertConfig'>.\
    \ Fix one of those so they match!\r\n```\r\n\r\nAnd, when I set `trust_remote_code=False`,\
    \ I got these warnings:\r\n```\r\nSome weights of BertModel were not initialized\
    \ from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized:\
    \ ['bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.bias',\
    \ 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.3.intermediate.dense.weight',\
    \ 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias',\
    \ 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.6.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.5.output.dense.weight',\
    \ 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.weight',\
    \ 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.attention.self.value.bias',\
    \ 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.8.output.dense.bias',\
    \ 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight',\
    \ 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias',\
    \ 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.bias',\
    \ 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.weight',\
    \ 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.bias',\
    \ 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.0.attention.self.key.bias',\
    \ 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.9.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight',\
    \ 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.value.weight',\
    \ 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.10.output.dense.bias',\
    \ 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.7.attention.self.key.weight',\
    \ 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.5.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.weight',\
    \ 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.4.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.8.output.dense.weight',\
    \ 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.weight',\
    \ 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.bias',\
    \ 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.weight',\
    \ 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.7.output.dense.weight',\
    \ 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.weight',\
    \ 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.query.weight',\
    \ 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight',\
    \ 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.6.output.dense.bias',\
    \ 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight',\
    \ 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.key.weight',\
    \ 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.9.output.dense.bias',\
    \ 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.key.bias',\
    \ 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.2.output.dense.weight',\
    \ 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight',\
    \ 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.bias',\
    \ 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.bias',\
    \ 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.query.weight',\
    \ 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.weight',\
    \ 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.bias',\
    \ 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight',\
    \ 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.3.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.0.output.dense.bias',\
    \ 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.1.output.dense.weight',\
    \ 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.query.bias',\
    \ 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight',\
    \ 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias',\
    \ 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.bias',\
    \ 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.query.bias',\
    \ 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.2.output.LayerNorm.bias',\
    \ 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias',\
    \ 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.5.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.1.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.weight',\
    \ 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.intermediate.dense.weight',\
    \ 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight',\
    \ 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.1.output.dense.bias',\
    \ 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight',\
    \ 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.pooler.dense.weight',\
    \ 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.2.output.dense.bias',\
    \ 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.10.intermediate.dense.weight',\
    \ 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.weight',\
    \ 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight',\
    \ 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.weight']\r\
    \nYou should probably TRAIN this model on a down-stream task to be able to use\
    \ it for predictions and inference.\r\n```\r\n\r\nWhat should I do to have the\
    \ pre-trained version of `DNABERT-2-117M`?\r\nI want to fine-tune the model for\
    \ another task."
  created_at: 2023-08-16 21:11:33+00:00
  edited: false
  hidden: false
  id: 64dd4995fdef63c2217ca190
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e881038655065216dd9c9757fb7216b9.svg
      fullname: Ali Saberi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saberiato
      type: user
    createdAt: '2023-08-16T22:59:42.000Z'
    data:
      edited: false
      editors:
      - saberiato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.905567467212677
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e881038655065216dd9c9757fb7216b9.svg
          fullname: Ali Saberi
          isHf: false
          isPro: false
          name: saberiato
          type: user
        html: '<p>Using <code>transformers==4.29</code> resolved the issue.</p>

          '
        raw: Using `transformers==4.29` resolved the issue.
        updatedAt: '2023-08-16T22:59:42.609Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64dd54dec797b87132f80215
    id: 64dd54dec797b87132f80213
    type: comment
  author: saberiato
  content: Using `transformers==4.29` resolved the issue.
  created_at: 2023-08-16 21:59:42+00:00
  edited: false
  hidden: false
  id: 64dd54dec797b87132f80213
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/e881038655065216dd9c9757fb7216b9.svg
      fullname: Ali Saberi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saberiato
      type: user
    createdAt: '2023-08-16T22:59:42.000Z'
    data:
      status: closed
    id: 64dd54dec797b87132f80215
    type: status-change
  author: saberiato
  created_at: 2023-08-16 21:59:42+00:00
  id: 64dd54dec797b87132f80215
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: zhihan1996/DNABERT-2-117M
repo_type: model
status: closed
target_branch: null
title: Cannot import "zhihan1996/DNABERT-2-117M" model from Huggingface
