!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hengchuangyin
conflicting_files: null
created_at: 2023-08-13 04:01:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bb155ef578240b1e525b3ae247fa034.svg
      fullname: Hengchuang Yin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hengchuangyin
      type: user
    createdAt: '2023-08-13T05:01:13.000Z'
    data:
      edited: true
      editors:
      - hengchuangyin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.22788141667842865
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bb155ef578240b1e525b3ae247fa034.svg
          fullname: Hengchuang Yin
          isHf: false
          isPro: false
          name: hengchuangyin
          type: user
        html: "<p><strong>Part 1: Tokenization and Dataset Preparation</strong></p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ BertForSequenceClassification\n<span class=\"hljs-keyword\">from</span>\
          \ torch.utils.data <span class=\"hljs-keyword\">import</span> Dataset\n\n\
          tokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">\"\
          zhihan1996/DNABERT-2-117M\"</span>, trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>)\nmodel = BertForSequenceClassification.from_pretrained(<span\
          \ class=\"hljs-string\">\"zhihan1996/DNABERT-2-117M\"</span>, num_labels=<span\
          \ class=\"hljs-number\">8</span>)\n\n<span class=\"hljs-keyword\">class</span>\
          \ <span class=\"hljs-title class_\">DNADataset</span>(<span class=\"hljs-title\
          \ class_ inherited__\">Dataset</span>):\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"\
          hljs-params\">self, data, tokenizer</span>):\n        self.data = data\n\
          \        self.tokenizer = tokenizer\n\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">__len__</span>(<span class=\"\
          hljs-params\">self</span>):\n        <span class=\"hljs-keyword\">return</span>\
          \ <span class=\"hljs-built_in\">len</span>(self.data)\n\n    <span class=\"\
          hljs-keyword\">def</span> <span class=\"hljs-title function_\">__getitem__</span>(<span\
          \ class=\"hljs-params\">self, idx</span>):\n        seq, label = self.data[idx]\n\
          \        inputs = self.tokenizer(seq, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>, padding=<span class=\"hljs-string\">'max_length'</span>, max_length=<span\
          \ class=\"hljs-number\">600</span>, truncation=<span class=\"hljs-literal\"\
          >True</span>)\n        <span class=\"hljs-keyword\">return</span> {\n  \
          \          <span class=\"hljs-string\">'input_ids'</span>: inputs[<span\
          \ class=\"hljs-string\">\"input_ids\"</span>].squeeze(),\n            <span\
          \ class=\"hljs-string\">'label'</span>: label\n        }\n</code></pre>\n\
          <p><strong>Part 2: Retrieving Model Configuration</strong></p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoConfig\n\nconfig = AutoConfig.from_pretrained(<span\
          \ class=\"hljs-string\">\"zhihan1996/DNABERT-2-117M\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n<span class=\"hljs-built_in\">print</span>(config.max_position_embeddings)\n\
          </code></pre>\n"
        raw: "\n**Part 1: Tokenization and Dataset Preparation**\n```python\nfrom\
          \ transformers import AutoTokenizer, BertForSequenceClassification\nfrom\
          \ torch.utils.data import Dataset\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\nmodel = BertForSequenceClassification.from_pretrained(\"\
          zhihan1996/DNABERT-2-117M\", num_labels=8)\n\nclass DNADataset(Dataset):\n\
          \    def __init__(self, data, tokenizer):\n        self.data = data\n  \
          \      self.tokenizer = tokenizer\n\n    def __len__(self):\n        return\
          \ len(self.data)\n\n    def __getitem__(self, idx):\n        seq, label\
          \ = self.data[idx]\n        inputs = self.tokenizer(seq, return_tensors='pt',\
          \ padding='max_length', max_length=600, truncation=True)\n        return\
          \ {\n            'input_ids': inputs[\"input_ids\"].squeeze(),\n       \
          \     'label': label\n        }\n```\n\n**Part 2: Retrieving Model Configuration**\n\
          ```python\nfrom transformers import AutoConfig\n\nconfig = AutoConfig.from_pretrained(\"\
          zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\nprint(config.max_position_embeddings)\n\
          ```\n"
        updatedAt: '2023-08-14T05:17:32.601Z'
      numEdits: 3
      reactions: []
    id: 64d863995f4814f7c336cf26
    type: comment
  author: hengchuangyin
  content: "\n**Part 1: Tokenization and Dataset Preparation**\n```python\nfrom transformers\
    \ import AutoTokenizer, BertForSequenceClassification\nfrom torch.utils.data import\
    \ Dataset\n\ntokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\"\
    , trust_remote_code=True)\nmodel = BertForSequenceClassification.from_pretrained(\"\
    zhihan1996/DNABERT-2-117M\", num_labels=8)\n\nclass DNADataset(Dataset):\n   \
    \ def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer\
    \ = tokenizer\n\n    def __len__(self):\n        return len(self.data)\n\n   \
    \ def __getitem__(self, idx):\n        seq, label = self.data[idx]\n        inputs\
    \ = self.tokenizer(seq, return_tensors='pt', padding='max_length', max_length=600,\
    \ truncation=True)\n        return {\n            'input_ids': inputs[\"input_ids\"\
    ].squeeze(),\n            'label': label\n        }\n```\n\n**Part 2: Retrieving\
    \ Model Configuration**\n```python\nfrom transformers import AutoConfig\n\nconfig\
    \ = AutoConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n\
    print(config.max_position_embeddings)\n```\n"
  created_at: 2023-08-13 04:01:13+00:00
  edited: true
  hidden: false
  id: 64d863995f4814f7c336cf26
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/2bb155ef578240b1e525b3ae247fa034.svg
      fullname: Hengchuang Yin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hengchuangyin
      type: user
    createdAt: '2023-08-14T05:17:21.000Z'
    data:
      from: from the config of the model and my multiple attempts, the model here
        still cannot support sequences longer than 512.
      to: Despite multiple trials and examining the model configuration, it seems
        that the model hosted on Hugging Face (`huggingface.co`) cannot handle sequences
        that exceed a length of 512 tokens.
    id: 64d9b8e13be3c579590903ab
    type: title-change
  author: hengchuangyin
  created_at: 2023-08-14 04:17:21+00:00
  id: 64d9b8e13be3c579590903ab
  new_title: Despite multiple trials and examining the model configuration, it seems
    that the model hosted on Hugging Face (`huggingface.co`) cannot handle sequences
    that exceed a length of 512 tokens.
  old_title: from the config of the model and my multiple attempts, the model here
    still cannot support sequences longer than 512.
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a3c9c188ef78981b1af04286a8ca0591.svg
      fullname: Wassim Jaoui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jaandoui
      type: user
    createdAt: '2023-12-30T23:07:28.000Z'
    data:
      edited: false
      editors:
      - jaandoui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9378707408905029
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a3c9c188ef78981b1af04286a8ca0591.svg
          fullname: Wassim Jaoui
          isHf: false
          isPro: false
          name: jaandoui
          type: user
        html: '<p>Any news regarding that ? I''m working on this paper and my research
          topic involves using sequences longer than 512 tokens.</p>

          '
        raw: Any news regarding that ? I'm working on this paper and my research topic
          involves using sequences longer than 512 tokens.
        updatedAt: '2023-12-30T23:07:28.439Z'
      numEdits: 0
      reactions: []
    id: 6590a2b07fe02354733d6dc6
    type: comment
  author: jaandoui
  content: Any news regarding that ? I'm working on this paper and my research topic
    involves using sequences longer than 512 tokens.
  created_at: 2023-12-30 23:07:28+00:00
  edited: false
  hidden: false
  id: 6590a2b07fe02354733d6dc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a3c9c188ef78981b1af04286a8ca0591.svg
      fullname: Wassim Jaoui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jaandoui
      type: user
    createdAt: '2024-01-02T18:10:49.000Z'
    data:
      edited: false
      editors:
      - jaandoui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8820817470550537
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a3c9c188ef78981b1af04286a8ca0591.svg
          fullname: Wassim Jaoui
          isHf: false
          isPro: false
          name: jaandoui
          type: user
        html: '<p>I tried it myself, IT DOES WORK on more than 512, make sure you
          have the right Transformers package:<br>pip/conda install transformers==4.29</p>

          '
        raw: "I tried it myself, IT DOES WORK on more than 512, make sure you have\
          \ the right Transformers package: \npip/conda install transformers==4.29"
        updatedAt: '2024-01-02T18:10:49.848Z'
      numEdits: 0
      reactions: []
    id: 659451a9f7291078f981a0c7
    type: comment
  author: jaandoui
  content: "I tried it myself, IT DOES WORK on more than 512, make sure you have the\
    \ right Transformers package: \npip/conda install transformers==4.29"
  created_at: 2024-01-02 18:10:49+00:00
  edited: false
  hidden: false
  id: 659451a9f7291078f981a0c7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: zhihan1996/DNABERT-2-117M
repo_type: model
status: open
target_branch: null
title: Despite multiple trials and examining the model configuration, it seems that
  the model hosted on Hugging Face (`huggingface.co`) cannot handle sequences that
  exceed a length of 512 tokens.
