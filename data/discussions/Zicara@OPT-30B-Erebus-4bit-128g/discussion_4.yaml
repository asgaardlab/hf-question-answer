!!python/object:huggingface_hub.community.DiscussionWithDetails
author: IngvarJackal
conflicting_files: null
created_at: 2023-04-19 07:56:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/15cacc919e8e56310a2c164954f6761c.svg
      fullname: Ingvar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IngvarJackal
      type: user
    createdAt: '2023-04-19T08:56:14.000Z'
    data:
      edited: false
      editors:
      - IngvarJackal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/15cacc919e8e56310a2c164954f6761c.svg
          fullname: Ingvar
          isHf: false
          isPro: false
          name: IngvarJackal
          type: user
        html: "<p>Tried to run <a rel=\"nofollow\" href=\"https://github.com/0cc4m/KoboldAI\"\
          >https://github.com/0cc4m/KoboldAI</a> with this model,<br>followed the\
          \ steps from the README.md:</p>\n<ul>\n<li>checked out this repo into <code>./models/OPT-30B-Erebus-4bit-128g</code></li>\n\
          <li>renamed <code>.pt</code> and <code>.safetensors</code> into <code>4bit.</code>\
          \ ones</li>\n<li>enabled experimental UI</li>\n<li>selected <code>True</code>\
          \ in 4-bit on load<br>On the loading the model, got the error:</li>\n</ul>\n\
          <pre><code>RuntimeError: Error(s) in loading state_dict for OPTForCausalLM:\n\
          \        size mismatch for model.decoder.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([56, 896]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 896]).\n        size mismatch\
          \ for model.decoder.layers.0.self_attn.k_proj.scales: copying a param with\
          \ shape torch.Size([56, 7168]) from checkpoint, the shape in current model\
          \ is torch.Size([1, 7168]).\n        ...\n</code></pre>\n<p>Question: what\
          \ am I doing wrong?</p>\n<p>cc: <span data-props=\"{&quot;user&quot;:&quot;SquidHominid&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/SquidHominid\"\
          >@<span class=\"underline\">SquidHominid</span></a></span>\n\n\t</span></span>\
          \ maybe you know, since your post indicates you managed to run the model\
          \ successfully?</p>\n"
        raw: "Tried to run https://github.com/0cc4m/KoboldAI with this model,\r\n\
          followed the steps from the README.md:\r\n * checked out this repo into\
          \ `./models/OPT-30B-Erebus-4bit-128g`\r\n * renamed `.pt` and `.safetensors`\
          \ into `4bit.` ones\r\n * enabled experimental UI\r\n * selected `True`\
          \ in 4-bit on load\r\nOn the loading the model, got the error:\r\n```\r\n\
          RuntimeError: Error(s) in loading state_dict for OPTForCausalLM:\r\n   \
          \     size mismatch for model.decoder.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([56, 896]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 896]).\r\n        size mismatch\
          \ for model.decoder.layers.0.self_attn.k_proj.scales: copying a param with\
          \ shape torch.Size([56, 7168]) from checkpoint, the shape in current model\
          \ is torch.Size([1, 7168]).\r\n        ...\r\n```\r\nQuestion: what am I\
          \ doing wrong?\r\n\r\ncc: @SquidHominid maybe you know, since your post\
          \ indicates you managed to run the model successfully?"
        updatedAt: '2023-04-19T08:56:14.150Z'
      numEdits: 0
      reactions: []
    id: 643facae18afbc4d1f3ea328
    type: comment
  author: IngvarJackal
  content: "Tried to run https://github.com/0cc4m/KoboldAI with this model,\r\nfollowed\
    \ the steps from the README.md:\r\n * checked out this repo into `./models/OPT-30B-Erebus-4bit-128g`\r\
    \n * renamed `.pt` and `.safetensors` into `4bit.` ones\r\n * enabled experimental\
    \ UI\r\n * selected `True` in 4-bit on load\r\nOn the loading the model, got the\
    \ error:\r\n```\r\nRuntimeError: Error(s) in loading state_dict for OPTForCausalLM:\r\
    \n        size mismatch for model.decoder.layers.0.self_attn.k_proj.qzeros: copying\
    \ a param with shape torch.Size([56, 896]) from checkpoint, the shape in current\
    \ model is torch.Size([1, 896]).\r\n        size mismatch for model.decoder.layers.0.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([56, 7168]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 7168]).\r\n        ...\r\n```\r\nQuestion:\
    \ what am I doing wrong?\r\n\r\ncc: @SquidHominid maybe you know, since your post\
    \ indicates you managed to run the model successfully?"
  created_at: 2023-04-19 07:56:14+00:00
  edited: false
  hidden: false
  id: 643facae18afbc4d1f3ea328
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fc6d1125e074491fd564e6d974eb9a76.svg
      fullname: Ellie Morissette
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SquidHominid
      type: user
    createdAt: '2023-04-20T09:18:24.000Z'
    data:
      edited: false
      editors:
      - SquidHominid
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fc6d1125e074491fd564e6d974eb9a76.svg
          fullname: Ellie Morissette
          isHf: false
          isPro: false
          name: SquidHominid
          type: user
        html: '<p>This isn''t the error I got, but if it helps you, I tried running
          the model in KoboldAI, and the problem I had is that mainline KoboldAI doesn''t
          support 4-bit quantized models. For that, you need Oobabooga or Occam''s
          KoboldAI fork.</p>

          '
        raw: This isn't the error I got, but if it helps you, I tried running the
          model in KoboldAI, and the problem I had is that mainline KoboldAI doesn't
          support 4-bit quantized models. For that, you need Oobabooga or Occam's
          KoboldAI fork.
        updatedAt: '2023-04-20T09:18:24.670Z'
      numEdits: 0
      reactions: []
    id: 644103607f13a7b5a259c080
    type: comment
  author: SquidHominid
  content: This isn't the error I got, but if it helps you, I tried running the model
    in KoboldAI, and the problem I had is that mainline KoboldAI doesn't support 4-bit
    quantized models. For that, you need Oobabooga or Occam's KoboldAI fork.
  created_at: 2023-04-20 08:18:24+00:00
  edited: false
  hidden: false
  id: 644103607f13a7b5a259c080
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/15cacc919e8e56310a2c164954f6761c.svg
      fullname: Ingvar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IngvarJackal
      type: user
    createdAt: '2023-04-20T18:00:48.000Z'
    data:
      edited: false
      editors:
      - IngvarJackal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/15cacc919e8e56310a2c164954f6761c.svg
          fullname: Ingvar
          isHf: false
          isPro: false
          name: IngvarJackal
          type: user
        html: '<p>I found the error which I made -- the model should be called <code>4bit-128g.pt</code>,
          not <code>4bit.pt</code></p>

          '
        raw: I found the error which I made -- the model should be called `4bit-128g.pt`,
          not `4bit.pt`
        updatedAt: '2023-04-20T18:00:48.406Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64417dd0006550f1ed7451c3
    id: 64417dd0006550f1ed7451c2
    type: comment
  author: IngvarJackal
  content: I found the error which I made -- the model should be called `4bit-128g.pt`,
    not `4bit.pt`
  created_at: 2023-04-20 17:00:48+00:00
  edited: false
  hidden: false
  id: 64417dd0006550f1ed7451c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/15cacc919e8e56310a2c164954f6761c.svg
      fullname: Ingvar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IngvarJackal
      type: user
    createdAt: '2023-04-20T18:00:48.000Z'
    data:
      status: closed
    id: 64417dd0006550f1ed7451c3
    type: status-change
  author: IngvarJackal
  created_at: 2023-04-20 17:00:48+00:00
  id: 64417dd0006550f1ed7451c3
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Zicara/OPT-30B-Erebus-4bit-128g
repo_type: model
status: closed
target_branch: null
title: Size mismatch for model.decoder.layers
