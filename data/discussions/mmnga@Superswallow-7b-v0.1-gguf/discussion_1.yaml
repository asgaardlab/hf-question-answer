!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nitky
conflicting_files: null
created_at: 2024-01-14 10:11:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/643291606f0d3d18ac136fe8c783a693.svg
      fullname: NISHINO Takuya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nitky
      type: user
    createdAt: '2024-01-14T10:11:42.000Z'
    data:
      edited: true
      editors:
      - nitky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9751831889152527
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/643291606f0d3d18ac136fe8c783a693.svg
          fullname: NISHINO Takuya
          isHf: false
          isPro: false
          name: nitky
          type: user
        html: '<p>Hi, thank you for providing the GGUF version. I am the author of
          this original model.</p>

          <p>In this GGUF model, it seems that newline code are displayed as "&lt;0x0A&gt;"
          or the stop token is lost. My friend pointed out this problem to me, but
          that did not occur with the original model.</p>

          <p>I have no idea what is causing this problem, is there anything I can
          do to help you?</p>

          '
        raw: 'Hi, thank you for providing the GGUF version. I am the author of this
          original model.


          In this GGUF model, it seems that newline code are displayed as "<0x0A>"
          or the stop token is lost. My friend pointed out this problem to me, but
          that did not occur with the original model.


          I have no idea what is causing this problem, is there anything I can do
          to help you?'
        updatedAt: '2024-01-14T10:25:58.297Z'
      numEdits: 5
      reactions: []
    id: 65a3b35e224f96d8cca6ceb8
    type: comment
  author: nitky
  content: 'Hi, thank you for providing the GGUF version. I am the author of this
    original model.


    In this GGUF model, it seems that newline code are displayed as "<0x0A>" or the
    stop token is lost. My friend pointed out this problem to me, but that did not
    occur with the original model.


    I have no idea what is causing this problem, is there anything I can do to help
    you?'
  created_at: 2024-01-14 10:11:42+00:00
  edited: true
  hidden: false
  id: 65a3b35e224f96d8cca6ceb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630faef1982455e61ccc10fd/V2bJ83SV-AfvX4fkEtFNi.jpeg?w=200&h=200&f=face
      fullname: momonga
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mmnga
      type: user
    createdAt: '2024-01-14T12:15:17.000Z'
    data:
      edited: false
      editors:
      - mmnga
      hidden: false
      identifiedLanguage:
        language: ja
        probability: 0.6350973844528198
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630faef1982455e61ccc10fd/V2bJ83SV-AfvX4fkEtFNi.jpeg?w=200&h=200&f=face
          fullname: momonga
          isHf: false
          isPro: false
          name: mmnga
          type: user
        html: "<p>Thank you for the report.<br>I have now checked and uploaded corrections\
          \ for both 7b and 13b.</p>\n<p>During the merging of the Swallow tokenizer\
          \ from the llama extension and the llama tokenizer from tulu-2,<br>the original\
          \ spm tokenizer (tokenizer.model) was converted to huggingface's FastTokenizer\
          \ (tokenizer.json), which caused the issue,<br>as there are differences\
          \ in behavior between the spm tokenizer and the one in llama.cpp.</p>\n\
          <p>Since the spm extended tokenizer used in Swallow includes the range of\
          \ the tulu-2's llama tokenizer,<br>I brought over and converted the tokenizer.model\
          \ from Swallow.</p>\n<p>It's quite a hassle to convert the tokenizer.json,\
          \ which is transformed from spm, to gguf without any issues,<br>so it would\
          \ be helpful if you could include Swallow's tokenizer.model in the model,<br>as\
          \ it would make it easier for others to perform gguf conversions in the\
          \ future.<br>(Since Swallow's tokenizer.model includes the range of tulu-2's\
          \ vocab, it should be fine to use as is.)</p>\n<p>\u5831\u544A\u3057\u3066\
          \u304F\u308C\u3066\u3042\u308A\u304C\u3068\u3046\u3054\u3056\u3044\u307E\
          \u3059\u3002<br>\u73FE\u5728\u78BA\u8A8D\u3057\u307E\u3057\u3066\u30017b\u3001\
          13b\u5171\u306B\u4FEE\u6B63\u3057\u3066\u30A2\u30C3\u30D7\u30ED\u30FC\u30C9\
          \u4E2D\u3067\u3059\u3002</p>\n<p>llama\u62E1\u5F35\u306ESwallow\u30C8\u30FC\
          \u30AF\u30CA\u30A4\u30B6\u30FC\u3068tulu-2\u306Ellama\u30C8\u30FC\u30AF\u30CA\
          \u30A4\u30B6\u30FC\u3092\u30DE\u30FC\u30B8\u3059\u308B\u969B\u306B\u3001\
          <br>\u672C\u6765\u306Espm\u30C8\u30FC\u30AF\u30CA\u30A4\u30B6\u30FC\uFF08\
          tokenizer.model\uFF09\u304Chuggingface\u306EFastTokenizer\uFF08tokenizer.json\uFF09\
          \u306B\u5909\u63DB\u3055\u308C\u3066\u304A\u308A\u3001<br>llama.cpp\u3067\
          spm\u30C8\u30FC\u30AF\u30CA\u30A4\u30B6\u30FC\u3068\u306E\u6319\u52D5\u306E\
          \u9055\u3044\u304C\u3042\u308B\u3053\u3068\u304C\u539F\u56E0\u3067\u3057\
          \u305F\u3002</p>\n<p>\u3053\u306E\u30E2\u30C7\u30EB\u306FSwallow\u3067\u4F7F\
          \u7528\u3057\u3066\u3044\u308Bspm\u62E1\u5F35\u30C8\u30FC\u30AF\u30CA\u30A4\
          \u30B6\u30FC\u304Ctulu-2\u306Ellama\u30C8\u30FC\u30AF\u30CA\u30A4\u30B6\u30FC\
          \u306E\u7BC4\u56F2\u3092\u542B\u3093\u3067\u3044\u308B\u306E\u3067\u3001\
          <br>swallow\u304B\u3089tokenizer.model\u3092\u6301\u3063\u3066\u304D\u3066\
          \u5909\u63DB\u3057\u307E\u3057\u305F\u3002</p>\n<p>spm\u304B\u3089\u5909\
          \u63DB\u3055\u308C\u305Ftokenizer.json\u3092\u554F\u984C\u306A\u3044\u3088\
          \u3046\u306Bgguf\u5909\u63DB\u3059\u308B\u306E\u306F\u7D50\u69CB\u624B\u9593\
          \u306A\u306E\u3067\u3001<br>\u3067\u304D\u308C\u3070Swallow\u306Etokenizer.model\u3092\
          \u30E2\u30C7\u30EB\u306B\u542B\u3081\u3066\u3044\u305F\u3060\u3051\u305F\
          \u3089\u3001\u4ECA\u5F8C\u4ED6\u306E\u65B9\u304Cgguf\u5909\u63DB\u3059\u308B\
          \u969B\u306B\u697D\u304B\u3068\u601D\u3044\u307E\u3059\u3002<br>\uFF08Swallow\u306E\
          tokenizer.model\u306Ftulu-2\u306Evocab\u306E\u7BC4\u56F2\u3092\u542B\u3093\
          \u3067\u3044\u308B\u306E\u3067\u305D\u306E\u307E\u307E\u7F6E\u3044\u3066\
          \u554F\u984C\u306A\u3044\u306F\u305A\u3067\u3059\uFF09</p>\n<p><a href=\"\
          https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf/tree/main\"\
          >https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf/tree/main</a></p>\n\
          <p>\u3088\u308D\u3057\u304F\u304A\u9858\u3044\u3057\u307E\u3059</p>\n"
        raw: "Thank you for the report.\nI have now checked and uploaded corrections\
          \ for both 7b and 13b.\n\nDuring the merging of the Swallow tokenizer from\
          \ the llama extension and the llama tokenizer from tulu-2,\nthe original\
          \ spm tokenizer (tokenizer.model) was converted to huggingface's FastTokenizer\
          \ (tokenizer.json), which caused the issue,\nas there are differences in\
          \ behavior between the spm tokenizer and the one in llama.cpp.\n\nSince\
          \ the spm extended tokenizer used in Swallow includes the range of the tulu-2's\
          \ llama tokenizer,\nI brought over and converted the tokenizer.model from\
          \ Swallow.\n\nIt's quite a hassle to convert the tokenizer.json, which is\
          \ transformed from spm, to gguf without any issues,\nso it would be helpful\
          \ if you could include Swallow's tokenizer.model in the model,\nas it would\
          \ make it easier for others to perform gguf conversions in the future.\n\
          (Since Swallow's tokenizer.model includes the range of tulu-2's vocab, it\
          \ should be fine to use as is.)\n\n\n\u5831\u544A\u3057\u3066\u304F\u308C\
          \u3066\u3042\u308A\u304C\u3068\u3046\u3054\u3056\u3044\u307E\u3059\u3002\
          \n\u73FE\u5728\u78BA\u8A8D\u3057\u307E\u3057\u3066\u30017b\u300113b\u5171\
          \u306B\u4FEE\u6B63\u3057\u3066\u30A2\u30C3\u30D7\u30ED\u30FC\u30C9\u4E2D\
          \u3067\u3059\u3002\n\nllama\u62E1\u5F35\u306ESwallow\u30C8\u30FC\u30AF\u30CA\
          \u30A4\u30B6\u30FC\u3068tulu-2\u306Ellama\u30C8\u30FC\u30AF\u30CA\u30A4\u30B6\
          \u30FC\u3092\u30DE\u30FC\u30B8\u3059\u308B\u969B\u306B\u3001\n\u672C\u6765\
          \u306Espm\u30C8\u30FC\u30AF\u30CA\u30A4\u30B6\u30FC\uFF08tokenizer.model\uFF09\
          \u304Chuggingface\u306EFastTokenizer\uFF08tokenizer.json\uFF09\u306B\u5909\
          \u63DB\u3055\u308C\u3066\u304A\u308A\u3001\nllama.cpp\u3067spm\u30C8\u30FC\
          \u30AF\u30CA\u30A4\u30B6\u30FC\u3068\u306E\u6319\u52D5\u306E\u9055\u3044\
          \u304C\u3042\u308B\u3053\u3068\u304C\u539F\u56E0\u3067\u3057\u305F\u3002\
          \n\n\u3053\u306E\u30E2\u30C7\u30EB\u306FSwallow\u3067\u4F7F\u7528\u3057\u3066\
          \u3044\u308Bspm\u62E1\u5F35\u30C8\u30FC\u30AF\u30CA\u30A4\u30B6\u30FC\u304C\
          tulu-2\u306Ellama\u30C8\u30FC\u30AF\u30CA\u30A4\u30B6\u30FC\u306E\u7BC4\u56F2\
          \u3092\u542B\u3093\u3067\u3044\u308B\u306E\u3067\u3001\nswallow\u304B\u3089\
          tokenizer.model\u3092\u6301\u3063\u3066\u304D\u3066\u5909\u63DB\u3057\u307E\
          \u3057\u305F\u3002\n\nspm\u304B\u3089\u5909\u63DB\u3055\u308C\u305Ftokenizer.json\u3092\
          \u554F\u984C\u306A\u3044\u3088\u3046\u306Bgguf\u5909\u63DB\u3059\u308B\u306E\
          \u306F\u7D50\u69CB\u624B\u9593\u306A\u306E\u3067\u3001\n\u3067\u304D\u308C\
          \u3070Swallow\u306Etokenizer.model\u3092\u30E2\u30C7\u30EB\u306B\u542B\u3081\
          \u3066\u3044\u305F\u3060\u3051\u305F\u3089\u3001\u4ECA\u5F8C\u4ED6\u306E\
          \u65B9\u304Cgguf\u5909\u63DB\u3059\u308B\u969B\u306B\u697D\u304B\u3068\u601D\
          \u3044\u307E\u3059\u3002\n\uFF08Swallow\u306Etokenizer.model\u306Ftulu-2\u306E\
          vocab\u306E\u7BC4\u56F2\u3092\u542B\u3093\u3067\u3044\u308B\u306E\u3067\u305D\
          \u306E\u307E\u307E\u7F6E\u3044\u3066\u554F\u984C\u306A\u3044\u306F\u305A\
          \u3067\u3059\uFF09\n\nhttps://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf/tree/main\n\
          \n\u3088\u308D\u3057\u304F\u304A\u9858\u3044\u3057\u307E\u3059"
        updatedAt: '2024-01-14T12:15:17.973Z'
      numEdits: 0
      reactions: []
    id: 65a3d055c7e6b607c2f1c716
    type: comment
  author: mmnga
  content: "Thank you for the report.\nI have now checked and uploaded corrections\
    \ for both 7b and 13b.\n\nDuring the merging of the Swallow tokenizer from the\
    \ llama extension and the llama tokenizer from tulu-2,\nthe original spm tokenizer\
    \ (tokenizer.model) was converted to huggingface's FastTokenizer (tokenizer.json),\
    \ which caused the issue,\nas there are differences in behavior between the spm\
    \ tokenizer and the one in llama.cpp.\n\nSince the spm extended tokenizer used\
    \ in Swallow includes the range of the tulu-2's llama tokenizer,\nI brought over\
    \ and converted the tokenizer.model from Swallow.\n\nIt's quite a hassle to convert\
    \ the tokenizer.json, which is transformed from spm, to gguf without any issues,\n\
    so it would be helpful if you could include Swallow's tokenizer.model in the model,\n\
    as it would make it easier for others to perform gguf conversions in the future.\n\
    (Since Swallow's tokenizer.model includes the range of tulu-2's vocab, it should\
    \ be fine to use as is.)\n\n\n\u5831\u544A\u3057\u3066\u304F\u308C\u3066\u3042\
    \u308A\u304C\u3068\u3046\u3054\u3056\u3044\u307E\u3059\u3002\n\u73FE\u5728\u78BA\
    \u8A8D\u3057\u307E\u3057\u3066\u30017b\u300113b\u5171\u306B\u4FEE\u6B63\u3057\u3066\
    \u30A2\u30C3\u30D7\u30ED\u30FC\u30C9\u4E2D\u3067\u3059\u3002\n\nllama\u62E1\u5F35\
    \u306ESwallow\u30C8\u30FC\u30AF\u30CA\u30A4\u30B6\u30FC\u3068tulu-2\u306Ellama\u30C8\
    \u30FC\u30AF\u30CA\u30A4\u30B6\u30FC\u3092\u30DE\u30FC\u30B8\u3059\u308B\u969B\
    \u306B\u3001\n\u672C\u6765\u306Espm\u30C8\u30FC\u30AF\u30CA\u30A4\u30B6\u30FC\uFF08\
    tokenizer.model\uFF09\u304Chuggingface\u306EFastTokenizer\uFF08tokenizer.json\uFF09\
    \u306B\u5909\u63DB\u3055\u308C\u3066\u304A\u308A\u3001\nllama.cpp\u3067spm\u30C8\
    \u30FC\u30AF\u30CA\u30A4\u30B6\u30FC\u3068\u306E\u6319\u52D5\u306E\u9055\u3044\
    \u304C\u3042\u308B\u3053\u3068\u304C\u539F\u56E0\u3067\u3057\u305F\u3002\n\n\u3053\
    \u306E\u30E2\u30C7\u30EB\u306FSwallow\u3067\u4F7F\u7528\u3057\u3066\u3044\u308B\
    spm\u62E1\u5F35\u30C8\u30FC\u30AF\u30CA\u30A4\u30B6\u30FC\u304Ctulu-2\u306Ellama\u30C8\
    \u30FC\u30AF\u30CA\u30A4\u30B6\u30FC\u306E\u7BC4\u56F2\u3092\u542B\u3093\u3067\
    \u3044\u308B\u306E\u3067\u3001\nswallow\u304B\u3089tokenizer.model\u3092\u6301\
    \u3063\u3066\u304D\u3066\u5909\u63DB\u3057\u307E\u3057\u305F\u3002\n\nspm\u304B\
    \u3089\u5909\u63DB\u3055\u308C\u305Ftokenizer.json\u3092\u554F\u984C\u306A\u3044\
    \u3088\u3046\u306Bgguf\u5909\u63DB\u3059\u308B\u306E\u306F\u7D50\u69CB\u624B\u9593\
    \u306A\u306E\u3067\u3001\n\u3067\u304D\u308C\u3070Swallow\u306Etokenizer.model\u3092\
    \u30E2\u30C7\u30EB\u306B\u542B\u3081\u3066\u3044\u305F\u3060\u3051\u305F\u3089\
    \u3001\u4ECA\u5F8C\u4ED6\u306E\u65B9\u304Cgguf\u5909\u63DB\u3059\u308B\u969B\u306B\
    \u697D\u304B\u3068\u601D\u3044\u307E\u3059\u3002\n\uFF08Swallow\u306Etokenizer.model\u306F\
    tulu-2\u306Evocab\u306E\u7BC4\u56F2\u3092\u542B\u3093\u3067\u3044\u308B\u306E\u3067\
    \u305D\u306E\u307E\u307E\u7F6E\u3044\u3066\u554F\u984C\u306A\u3044\u306F\u305A\
    \u3067\u3059\uFF09\n\nhttps://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf/tree/main\n\
    \n\u3088\u308D\u3057\u304F\u304A\u9858\u3044\u3057\u307E\u3059"
  created_at: 2024-01-14 12:15:17+00:00
  edited: false
  hidden: false
  id: 65a3d055c7e6b607c2f1c716
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/643291606f0d3d18ac136fe8c783a693.svg
      fullname: NISHINO Takuya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nitky
      type: user
    createdAt: '2024-01-14T13:33:50.000Z'
    data:
      edited: false
      editors:
      - nitky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8686112761497498
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/643291606f0d3d18ac136fe8c783a693.svg
          fullname: NISHINO Takuya
          isHf: false
          isPro: false
          name: nitky
          type: user
        html: '<p>I understand the details of this problem. Thank you so much!</p>

          '
        raw: I understand the details of this problem. Thank you so much!
        updatedAt: '2024-01-14T13:33:50.319Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - mmnga
      relatedEventId: 65a3e2be8448f47df25379e9
    id: 65a3e2be8448f47df25379e6
    type: comment
  author: nitky
  content: I understand the details of this problem. Thank you so much!
  created_at: 2024-01-14 13:33:50+00:00
  edited: false
  hidden: false
  id: 65a3e2be8448f47df25379e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/643291606f0d3d18ac136fe8c783a693.svg
      fullname: NISHINO Takuya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nitky
      type: user
    createdAt: '2024-01-14T13:33:50.000Z'
    data:
      status: closed
    id: 65a3e2be8448f47df25379e9
    type: status-change
  author: nitky
  created_at: 2024-01-14 13:33:50+00:00
  id: 65a3e2be8448f47df25379e9
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mmnga/Superswallow-7b-v0.1-gguf
repo_type: model
status: closed
target_branch: null
title: Differences in output from the original model
