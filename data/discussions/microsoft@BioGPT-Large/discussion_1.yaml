!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Delcos
conflicting_files: null
created_at: 2023-02-08 20:31:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662848908113-6317eee61d6018cb851af5a7.jpeg?w=200&h=200&f=face
      fullname: Devon M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Delcos
      type: user
    createdAt: '2023-02-08T20:31:11.000Z'
    data:
      edited: false
      editors:
      - Delcos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662848908113-6317eee61d6018cb851af5a7.jpeg?w=200&h=200&f=face
          fullname: Devon M
          isHf: false
          isPro: false
          name: Delcos
          type: user
        html: '<p>Does this use the same naming scheme as DialoGPT. If not what does
          the large stand for?</p>

          '
        raw: Does this use the same naming scheme as DialoGPT. If not what does the
          large stand for?
        updatedAt: '2023-02-08T20:31:11.297Z'
      numEdits: 0
      reactions: []
    id: 63e4068f54f51ea342cefde6
    type: comment
  author: Delcos
  content: Does this use the same naming scheme as DialoGPT. If not what does the
    large stand for?
  created_at: 2023-02-08 20:31:11+00:00
  edited: false
  hidden: false
  id: 63e4068f54f51ea342cefde6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-02-08T20:43:21.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Yeah actually they''re both called "large" but they differ a bit
          in terms of hyperparameters.</p>

          <p>BioGPT-large has 48 layers, and uses a hidden size of 1600. You can check
          the <a href="https://huggingface.co/microsoft/BioGPT-Large/blob/main/config.json">config</a>
          for all details.</p>

          <p>DialoGPT-large  on the other hand has 36 layers, and uses a hidden size
          of 1280. See also the <a href="https://huggingface.co/microsoft/DialoGPT-large/blob/main/config.json">config</a>
          for all details.</p>

          '
        raw: 'Yeah actually they''re both called "large" but they differ a bit in
          terms of hyperparameters.


          BioGPT-large has 48 layers, and uses a hidden size of 1600. You can check
          the [config](https://huggingface.co/microsoft/BioGPT-Large/blob/main/config.json)
          for all details.


          DialoGPT-large  on the other hand has 36 layers, and uses a hidden size
          of 1280. See also the [config](https://huggingface.co/microsoft/DialoGPT-large/blob/main/config.json)
          for all details.'
        updatedAt: '2023-02-08T20:43:21.307Z'
      numEdits: 0
      reactions: []
    id: 63e409698de575a15a68b40e
    type: comment
  author: nielsr
  content: 'Yeah actually they''re both called "large" but they differ a bit in terms
    of hyperparameters.


    BioGPT-large has 48 layers, and uses a hidden size of 1600. You can check the
    [config](https://huggingface.co/microsoft/BioGPT-Large/blob/main/config.json)
    for all details.


    DialoGPT-large  on the other hand has 36 layers, and uses a hidden size of 1280.
    See also the [config](https://huggingface.co/microsoft/DialoGPT-large/blob/main/config.json)
    for all details.'
  created_at: 2023-02-08 20:43:21+00:00
  edited: false
  hidden: false
  id: 63e409698de575a15a68b40e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662848908113-6317eee61d6018cb851af5a7.jpeg?w=200&h=200&f=face
      fullname: Devon M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Delcos
      type: user
    createdAt: '2023-02-10T06:11:22.000Z'
    data:
      edited: false
      editors:
      - Delcos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662848908113-6317eee61d6018cb851af5a7.jpeg?w=200&h=200&f=face
          fullname: Devon M
          isHf: false
          isPro: false
          name: Delcos
          type: user
        html: '<p>Awesome thanks :) .</p>

          '
        raw: Awesome thanks :) .
        updatedAt: '2023-02-10T06:11:22.022Z'
      numEdits: 0
      reactions: []
      relatedEventId: 63e5e00a8a1457bab8e6a29a
    id: 63e5e00a8a1457bab8e6a299
    type: comment
  author: Delcos
  content: Awesome thanks :) .
  created_at: 2023-02-10 06:11:22+00:00
  edited: false
  hidden: false
  id: 63e5e00a8a1457bab8e6a299
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662848908113-6317eee61d6018cb851af5a7.jpeg?w=200&h=200&f=face
      fullname: Devon M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Delcos
      type: user
    createdAt: '2023-02-10T06:11:22.000Z'
    data:
      status: closed
    id: 63e5e00a8a1457bab8e6a29a
    type: status-change
  author: Delcos
  created_at: 2023-02-10 06:11:22+00:00
  id: 63e5e00a8a1457bab8e6a29a
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: microsoft/BioGPT-Large
repo_type: model
status: closed
target_branch: null
title: Large as in what number?
