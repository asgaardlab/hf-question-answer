!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AnuIdame
conflicting_files: null
created_at: 2023-09-09 14:48:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df9e96c1bd10734d55f707320d25b6ad.svg
      fullname: Anushka Idamekorala
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AnuIdame
      type: user
    createdAt: '2023-09-09T15:48:18.000Z'
    data:
      edited: true
      editors:
      - AnuIdame
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9557147026062012
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df9e96c1bd10734d55f707320d25b6ad.svg
          fullname: Anushka Idamekorala
          isHf: false
          isPro: false
          name: AnuIdame
          type: user
        html: '<p>We aim to understand the architecture of Prithvi-100M, including<br>a)
          How to deal with heterogeneous data. In this case, different resolutions/picture
          sizes and possibly different numbers of channels<br>b) How to deal with
          metadata, such as spatial positioning of the image<br>c) What goes into
          pre-training and what into fine-tuning<br>Can you help to understand these
          details?</p>

          '
        raw: 'We aim to understand the architecture of Prithvi-100M, including

          a) How to deal with heterogeneous data. In this case, different resolutions/picture
          sizes and possibly different numbers of channels

          b) How to deal with metadata, such as spatial positioning of the image

          c) What goes into pre-training and what into fine-tuning

          Can you help to understand these details?'
        updatedAt: '2023-09-09T16:04:31.384Z'
      numEdits: 1
      reactions: []
    id: 64fc93c239d541478e1321ca
    type: comment
  author: AnuIdame
  content: 'We aim to understand the architecture of Prithvi-100M, including

    a) How to deal with heterogeneous data. In this case, different resolutions/picture
    sizes and possibly different numbers of channels

    b) How to deal with metadata, such as spatial positioning of the image

    c) What goes into pre-training and what into fine-tuning

    Can you help to understand these details?'
  created_at: 2023-09-09 14:48:18+00:00
  edited: true
  hidden: false
  id: 64fc93c239d541478e1321ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cdaddb91652ac020e32a6af3e4e01a22.svg
      fullname: Christopher Phillips
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: CEPhillips
      type: user
    createdAt: '2023-09-19T16:42:18.000Z'
    data:
      edited: false
      editors:
      - CEPhillips
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9541894793510437
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cdaddb91652ac020e32a6af3e4e01a22.svg
          fullname: Christopher Phillips
          isHf: false
          isPro: false
          name: CEPhillips
          type: user
        html: '<p>Hello, dealing with your questions in order:<br>a) The model is
          trained for specific resolution, and deviating from that will likely result
          in poor performance. Changing the number of input channels is not possible
          without re-training.<br>b) The model itself is location-agnostic in that
          spatial metadata is not an input.<br>c) Pre-training is a self-supervised
          training that sees many more images than the fine-tuning procedure. During
          pre-training, the model masks images and then reconstructs them to learn
          relationships between the inputs. During fine-tuning, the model is provided
          fewer but labelled images and learns to predict those labels.</p>

          '
        raw: 'Hello, dealing with your questions in order:

          a) The model is trained for specific resolution, and deviating from that
          will likely result in poor performance. Changing the number of input channels
          is not possible without re-training.

          b) The model itself is location-agnostic in that spatial metadata is not
          an input.

          c) Pre-training is a self-supervised training that sees many more images
          than the fine-tuning procedure. During pre-training, the model masks images
          and then reconstructs them to learn relationships between the inputs. During
          fine-tuning, the model is provided fewer but labelled images and learns
          to predict those labels.'
        updatedAt: '2023-09-19T16:42:18.186Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - AnuIdame
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hyzhak
    id: 6509cf6ac19e5b4c8a53cd23
    type: comment
  author: CEPhillips
  content: 'Hello, dealing with your questions in order:

    a) The model is trained for specific resolution, and deviating from that will
    likely result in poor performance. Changing the number of input channels is not
    possible without re-training.

    b) The model itself is location-agnostic in that spatial metadata is not an input.

    c) Pre-training is a self-supervised training that sees many more images than
    the fine-tuning procedure. During pre-training, the model masks images and then
    reconstructs them to learn relationships between the inputs. During fine-tuning,
    the model is provided fewer but labelled images and learns to predict those labels.'
  created_at: 2023-09-19 15:42:18+00:00
  edited: false
  hidden: false
  id: 6509cf6ac19e5b4c8a53cd23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df9e96c1bd10734d55f707320d25b6ad.svg
      fullname: Anushka Idamekorala
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AnuIdame
      type: user
    createdAt: '2023-09-19T17:15:33.000Z'
    data:
      edited: true
      editors:
      - AnuIdame
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9763525724411011
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df9e96c1bd10734d55f707320d25b6ad.svg
          fullname: Anushka Idamekorala
          isHf: false
          isPro: false
          name: AnuIdame
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;CEPhillips&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/CEPhillips\">@<span class=\"\
          underline\">CEPhillips</span></a></span>\n\n\t</span></span> Thank you very\
          \ much for the explanation. We have two more questions as well.</p>\n<ol>\n\
          <li>Does the pre-training dataset include data such as burn-scar and flood\
          \ detection?</li>\n<li>It is stated that pre-training images are time series\
          \ images where each input has three-time stamps. What is the time gap between\
          \ them? Do these three images have any shifts, or do they perfectly overlap\
          \ on each other perfectly?[whether they are same exact location]</li>\n\
          </ol>\n"
        raw: '@CEPhillips Thank you very much for the explanation. We have two more
          questions as well.

          1) Does the pre-training dataset include data such as burn-scar and flood
          detection?

          2) It is stated that pre-training images are time series images where each
          input has three-time stamps. What is the time gap between them? Do these
          three images have any shifts, or do they perfectly overlap on each other
          perfectly?[whether they are same exact location]'
        updatedAt: '2023-09-19T17:15:51.072Z'
      numEdits: 1
      reactions: []
    id: 6509d735d26103b6eec8390b
    type: comment
  author: AnuIdame
  content: '@CEPhillips Thank you very much for the explanation. We have two more
    questions as well.

    1) Does the pre-training dataset include data such as burn-scar and flood detection?

    2) It is stated that pre-training images are time series images where each input
    has three-time stamps. What is the time gap between them? Do these three images
    have any shifts, or do they perfectly overlap on each other perfectly?[whether
    they are same exact location]'
  created_at: 2023-09-19 16:15:33+00:00
  edited: true
  hidden: false
  id: 6509d735d26103b6eec8390b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cdaddb91652ac020e32a6af3e4e01a22.svg
      fullname: Christopher Phillips
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: CEPhillips
      type: user
    createdAt: '2023-09-19T17:29:39.000Z'
    data:
      edited: false
      editors:
      - CEPhillips
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9749059677124023
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cdaddb91652ac020e32a6af3e4e01a22.svg
          fullname: Christopher Phillips
          isHf: false
          isPro: false
          name: CEPhillips
          type: user
        html: '<p>I''m glad that helps. The pre-training does not include burn-scar
          or flood detection tasks. Those are handled during fine-tuning. Regarding
          the input data, HLS observations are tiled and there are no spatial shifts
          between times. It is my understanding that times can very somewhat due to
          irregular overpasses by the satellite.</p>

          '
        raw: I'm glad that helps. The pre-training does not include burn-scar or flood
          detection tasks. Those are handled during fine-tuning. Regarding the input
          data, HLS observations are tiled and there are no spatial shifts between
          times. It is my understanding that times can very somewhat due to irregular
          overpasses by the satellite.
        updatedAt: '2023-09-19T17:29:39.903Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - AnuIdame
    id: 6509da83f36bb51c5027269d
    type: comment
  author: CEPhillips
  content: I'm glad that helps. The pre-training does not include burn-scar or flood
    detection tasks. Those are handled during fine-tuning. Regarding the input data,
    HLS observations are tiled and there are no spatial shifts between times. It is
    my understanding that times can very somewhat due to irregular overpasses by the
    satellite.
  created_at: 2023-09-19 16:29:39+00:00
  edited: false
  hidden: false
  id: 6509da83f36bb51c5027269d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d0aac4c9d00e38470686e3/DqoGaZA_oxOtqjZ7P8mTX.jpeg?w=200&h=200&f=face
      fullname: Carlos Gomes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: carlosgomes98
      type: user
    createdAt: '2023-09-27T09:43:08.000Z'
    data:
      status: closed
    id: 6513f92cc456f503500cadad
    type: status-change
  author: carlosgomes98
  created_at: 2023-09-27 08:43:08+00:00
  id: 6513f92cc456f503500cadad
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/df9e96c1bd10734d55f707320d25b6ad.svg
      fullname: Anushka Idamekorala
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AnuIdame
      type: user
    createdAt: '2023-09-30T14:36:39.000Z'
    data:
      status: open
    id: 651832770e3a5553d484acf8
    type: status-change
  author: AnuIdame
  created_at: 2023-09-30 13:36:39+00:00
  id: 651832770e3a5553d484acf8
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df9e96c1bd10734d55f707320d25b6ad.svg
      fullname: Anushka Idamekorala
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AnuIdame
      type: user
    createdAt: '2023-09-30T14:40:15.000Z'
    data:
      edited: false
      editors:
      - AnuIdame
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9475348591804504
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df9e96c1bd10734d55f707320d25b6ad.svg
          fullname: Anushka Idamekorala
          isHf: false
          isPro: false
          name: AnuIdame
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;CEPhillips&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/CEPhillips\">@<span class=\"\
          underline\">CEPhillips</span></a></span>\n\n\t</span></span> In the description\
          \ it is stated \"The model can also handle static imagery which can be fed\
          \ into the model with T=1.\". Do you feed T=1 images for the pre-training.\
          \ If that so how the network handles both T=1 and T=3 differently?</p>\n"
        raw: '@CEPhillips In the description it is stated "The model can also handle
          static imagery which can be fed into the model with T=1.". Do you feed T=1
          images for the pre-training. If that so how the network handles both T=1
          and T=3 differently?'
        updatedAt: '2023-09-30T14:40:15.172Z'
      numEdits: 0
      reactions: []
    id: 6518334fee8c603b702eb3e2
    type: comment
  author: AnuIdame
  content: '@CEPhillips In the description it is stated "The model can also handle
    static imagery which can be fed into the model with T=1.". Do you feed T=1 images
    for the pre-training. If that so how the network handles both T=1 and T=3 differently?'
  created_at: 2023-09-30 13:40:15+00:00
  edited: false
  hidden: false
  id: 6518334fee8c603b702eb3e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7bd042f2d502604e0459c727828a5cb.svg
      fullname: Paolo Fraccaro
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Paolo-Fraccaro
      type: user
    createdAt: '2023-10-02T06:20:13.000Z'
    data:
      edited: false
      editors:
      - Paolo-Fraccaro
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9208834767341614
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7bd042f2d502604e0459c727828a5cb.svg
          fullname: Paolo Fraccaro
          isHf: false
          isPro: false
          name: Paolo-Fraccaro
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;AnuIdame&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/AnuIdame\"\
          >@<span class=\"underline\">AnuIdame</span></a></span>\n\n\t</span></span>\
          \ ! For pretraining we always fed 3 images. However, as you can see from\
          \ the downstream tasks we tackled, once you use the pretrained encoder you\
          \ can customise it to the number of T you want.</p>\n"
        raw: Hi @AnuIdame ! For pretraining we always fed 3 images. However, as you
          can see from the downstream tasks we tackled, once you use the pretrained
          encoder you can customise it to the number of T you want.
        updatedAt: '2023-10-02T06:20:13.142Z'
      numEdits: 0
      reactions: []
    id: 651a611d34c2696253a657ff
    type: comment
  author: Paolo-Fraccaro
  content: Hi @AnuIdame ! For pretraining we always fed 3 images. However, as you
    can see from the downstream tasks we tackled, once you use the pretrained encoder
    you can customise it to the number of T you want.
  created_at: 2023-10-02 05:20:13+00:00
  edited: false
  hidden: false
  id: 651a611d34c2696253a657ff
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: ibm-nasa-geospatial/Prithvi-100M
repo_type: model
status: open
target_branch: null
title: Insight about Prithvi
