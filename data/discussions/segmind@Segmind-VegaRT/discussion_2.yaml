!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AeroDEmi
conflicting_files: null
created_at: 2023-12-16 02:57:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/40e8a7ac61c166ab111f889260ad8efc.svg
      fullname: Diego Ruano
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AeroDEmi
      type: user
    createdAt: '2023-12-16T02:57:22.000Z'
    data:
      edited: true
      editors:
      - AeroDEmi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6186614036560059
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/40e8a7ac61c166ab111f889260ad8efc.svg
          fullname: Diego Ruano
          isHf: false
          isPro: false
          name: AeroDEmi
          type: user
        html: '<p>I''m going to train some loras on Vega and want to use this to speed
          up the process, I did something like this:</p>

          <p>vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix",
          torch_dtype=torch.float16)<br>pipe = DiffusionPipeline.from_pretrained("segmind/Segmind-Vega",
          vae=vae, torch_dtype=torch.float16)<br>pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)<br>pipe.load_lora_weights("segmind/Segmind-VegaRT",
          adapter_name="rt")<br>pipe.load_lora_weights(model_path, adapter_name="lora_1")<br>pipe.set_adapters(["lora_1",
          "rt"], adapter_weights=[1.0, 1.0])</p>

          <p>The results on this are not good, any other method to use this with our
          LoRAs?</p>

          <p>Thank you</p>

          '
        raw: 'I''m going to train some loras on Vega and want to use this to speed
          up the process, I did something like this:


          vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)

          pipe = DiffusionPipeline.from_pretrained("segmind/Segmind-Vega", vae=vae,
          torch_dtype=torch.float16)

          pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

          pipe.load_lora_weights("segmind/Segmind-VegaRT", adapter_name="rt")

          pipe.load_lora_weights(model_path, adapter_name="lora_1")

          pipe.set_adapters(["lora_1", "rt"], adapter_weights=[1.0, 1.0])



          The results on this are not good, any other method to use this with our
          LoRAs?


          Thank you'
        updatedAt: '2023-12-16T02:59:21.438Z'
      numEdits: 3
      reactions: []
    id: 657d1212504da7f6f3982f2d
    type: comment
  author: AeroDEmi
  content: 'I''m going to train some loras on Vega and want to use this to speed up
    the process, I did something like this:


    vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)

    pipe = DiffusionPipeline.from_pretrained("segmind/Segmind-Vega", vae=vae, torch_dtype=torch.float16)

    pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

    pipe.load_lora_weights("segmind/Segmind-VegaRT", adapter_name="rt")

    pipe.load_lora_weights(model_path, adapter_name="lora_1")

    pipe.set_adapters(["lora_1", "rt"], adapter_weights=[1.0, 1.0])



    The results on this are not good, any other method to use this with our LoRAs?


    Thank you'
  created_at: 2023-12-16 02:57:22+00:00
  edited: true
  hidden: false
  id: 657d1212504da7f6f3982f2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62f8ca074588fe31f4361dae/F2k343TPD7KVfW3P26IRs.jpeg?w=200&h=200&f=face
      fullname: Yatharth Gupta
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Warlord-K
      type: user
    createdAt: '2023-12-16T07:11:45.000Z'
    data:
      edited: false
      editors:
      - Warlord-K
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6139650940895081
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62f8ca074588fe31f4361dae/F2k343TPD7KVfW3P26IRs.jpeg?w=200&h=200&f=face
          fullname: Yatharth Gupta
          isHf: false
          isPro: false
          name: Warlord-K
          type: user
        html: '<p>You don''t need to take this roundabout way, you can easily load
          it up like this</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          torch

          <span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span>
          LCMScheduler, AutoPipelineForText2Image


          model_id = <span class="hljs-string">"segmind/Segmind-Vega"</span>

          adapter_id = <span class="hljs-string">"segmind/Segmind-VegaRT"</span>


          pipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16,
          variant=<span class="hljs-string">"fp16"</span>)

          pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

          pipe.to(<span class="hljs-string">"cuda"</span>)


          <span class="hljs-comment"># load and fuse lcm lora</span>

          pipe.load_lora_weights(adapter_id)

          pipe.fuse_lora()

          </code></pre>

          '
        raw: 'You don''t need to take this roundabout way, you can easily load it
          up like this


          ```python

          import torch

          from diffusers import LCMScheduler, AutoPipelineForText2Image


          model_id = "segmind/Segmind-Vega"

          adapter_id = "segmind/Segmind-VegaRT"


          pipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16,
          variant="fp16")

          pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

          pipe.to("cuda")


          # load and fuse lcm lora

          pipe.load_lora_weights(adapter_id)

          pipe.fuse_lora()

          ```'
        updatedAt: '2023-12-16T07:11:45.004Z'
      numEdits: 0
      reactions: []
    id: 657d4db1c7cb69069a5243cf
    type: comment
  author: Warlord-K
  content: 'You don''t need to take this roundabout way, you can easily load it up
    like this


    ```python

    import torch

    from diffusers import LCMScheduler, AutoPipelineForText2Image


    model_id = "segmind/Segmind-Vega"

    adapter_id = "segmind/Segmind-VegaRT"


    pipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16,
    variant="fp16")

    pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

    pipe.to("cuda")


    # load and fuse lcm lora

    pipe.load_lora_weights(adapter_id)

    pipe.fuse_lora()

    ```'
  created_at: 2023-12-16 07:11:45+00:00
  edited: false
  hidden: false
  id: 657d4db1c7cb69069a5243cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/40e8a7ac61c166ab111f889260ad8efc.svg
      fullname: Diego Ruano
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AeroDEmi
      type: user
    createdAt: '2023-12-16T07:55:36.000Z'
    data:
      edited: false
      editors:
      - AeroDEmi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9640519022941589
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/40e8a7ac61c166ab111f889260ad8efc.svg
          fullname: Diego Ruano
          isHf: false
          isPro: false
          name: AeroDEmi
          type: user
        html: '<p>My question is: how can we load another LoRA on top of it?<br>As
          far as my experiments go, my custom LoRA is not performing well when I load
          the VegaRT</p>

          '
        raw: 'My question is: how can we load another LoRA on top of it?

          As far as my experiments go, my custom LoRA is not performing well when
          I load the VegaRT'
        updatedAt: '2023-12-16T07:55:36.516Z'
      numEdits: 0
      reactions: []
    id: 657d57f81e2087032410b174
    type: comment
  author: AeroDEmi
  content: 'My question is: how can we load another LoRA on top of it?

    As far as my experiments go, my custom LoRA is not performing well when I load
    the VegaRT'
  created_at: 2023-12-16 07:55:36+00:00
  edited: false
  hidden: false
  id: 657d57f81e2087032410b174
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/40e8a7ac61c166ab111f889260ad8efc.svg
      fullname: Diego Ruano
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AeroDEmi
      type: user
    createdAt: '2024-01-11T17:22:15.000Z'
    data:
      edited: false
      editors:
      - AeroDEmi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8684436082839966
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/40e8a7ac61c166ab111f889260ad8efc.svg
          fullname: Diego Ruano
          isHf: false
          isPro: false
          name: AeroDEmi
          type: user
        html: '<p>Is there a way to train the VegaRT?</p>

          '
        raw: Is there a way to train the VegaRT?
        updatedAt: '2024-01-11T17:22:15.890Z'
      numEdits: 0
      reactions: []
    id: 65a023c75dce70a302e8d3f6
    type: comment
  author: AeroDEmi
  content: Is there a way to train the VegaRT?
  created_at: 2024-01-11 17:22:15+00:00
  edited: false
  hidden: false
  id: 65a023c75dce70a302e8d3f6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: segmind/Segmind-VegaRT
repo_type: model
status: open
target_branch: null
title: How can we train it or can we load it with other LoRAs?
