!!python/object:huggingface_hub.community.DiscussionWithDetails
author: crestf411
conflicting_files: null
created_at: 2023-06-22 13:05:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd49f158261551be35af60f7e58f1deb.svg
      fullname: Crestfall
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: crestf411
      type: user
    createdAt: '2023-06-22T14:05:39.000Z'
    data:
      edited: true
      editors:
      - crestf411
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8633620738983154
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd49f158261551be35af60f7e58f1deb.svg
          fullname: Crestfall
          isHf: false
          isPro: false
          name: crestf411
          type: user
        html: "<p>I am trying to generate a merged model using the unquantized chronos-hermes\
          \ model, but whenever I try to convert it to ggml format it fails with</p>\n\
          <pre><code>Exception: Vocab size mismatch (model has 32001, but MODELDIR/tokenizer.model\
          \ has 32000).  Most likely you are missing added_tokens.json (should be\
          \ in MODELDIR).\n</code></pre>\n<p>I can get around this by adding an added_tokens.json\
          \ file with e.g.</p>\n<pre><code>{\n  \"[PAD]\": 32000\n}\n</code></pre>\n\
          <p>but then quantization fails:</p>\n<pre><code>=========================\
          \ Tensor sizes 5120 x 32001 are not divisible by 256\nThis is required to\
          \ be able to use k-quants for now!\n========================================================================================\n\
          </code></pre>\n<p>I strongly suspect this model doesn't have 32001 tokens,\
          \ so added_tokens.json is probably not the way to go.</p>\n"
        raw: "I am trying to generate a merged model using the unquantized chronos-hermes\
          \ model, but whenever I try to convert it to ggml format it fails with\n\
          \n```\nException: Vocab size mismatch (model has 32001, but MODELDIR/tokenizer.model\
          \ has 32000).  Most likely you are missing added_tokens.json (should be\
          \ in MODELDIR).\n```\n\nI can get around this by adding an added_tokens.json\
          \ file with e.g.\n\n```\n{\n  \"[PAD]\": 32000\n}\n```\n\nbut then quantization\
          \ fails:\n\n```\n========================= Tensor sizes 5120 x 32001 are\
          \ not divisible by 256\nThis is required to be able to use k-quants for\
          \ now!\n========================================================================================\n\
          ```\n\nI strongly suspect this model doesn't have 32001 tokens, so added_tokens.json\
          \ is probably not the way to go."
        updatedAt: '2023-06-22T14:06:31.765Z'
      numEdits: 3
      reactions: []
    id: 64945533c76da804a4d3ccc6
    type: comment
  author: crestf411
  content: "I am trying to generate a merged model using the unquantized chronos-hermes\
    \ model, but whenever I try to convert it to ggml format it fails with\n\n```\n\
    Exception: Vocab size mismatch (model has 32001, but MODELDIR/tokenizer.model\
    \ has 32000).  Most likely you are missing added_tokens.json (should be in MODELDIR).\n\
    ```\n\nI can get around this by adding an added_tokens.json file with e.g.\n\n\
    ```\n{\n  \"[PAD]\": 32000\n}\n```\n\nbut then quantization fails:\n\n```\n=========================\
    \ Tensor sizes 5120 x 32001 are not divisible by 256\nThis is required to be able\
    \ to use k-quants for now!\n========================================================================================\n\
    ```\n\nI strongly suspect this model doesn't have 32001 tokens, so added_tokens.json\
    \ is probably not the way to go."
  created_at: 2023-06-22 13:05:39+00:00
  edited: true
  hidden: false
  id: 64945533c76da804a4d3ccc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-22T15:13:09.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9570109844207764
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>That''s exactly what I did to create it, and it worked at the time
          I did it.  But recently they''ve put in a check for not-divisible-by-256
          tensors, which you can read about here: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/1919#issuecomment-1599484900">https://github.com/ggerganov/llama.cpp/issues/1919#issuecomment-1599484900</a></p>

          <p>This does rather raise the issue of whether that check is finding false
          positives, because this model clearly worked when I made it.  Though like
          you I''ve also wondered whether models like these actually have 32,001 tokens
          or whether they''ve just inherited that config.  However on at least one
          previous model I tried just editing config.json to set it to 32000, and
          it still failed because it did actually have 32001 sized tensors.</p>

          <p>So I don''t know.  But right now, you won''t be able to make k-quant
          GGMLs with this model, unless you roll back to a version of llama.cpp from
          about 12 days ago or so. And then there''s the question of whether it will
          cause some issues or not.</p>

          <p>You can make the old q4_0, q4_1, q5_0, q5_1 and q8_0 quants - though
          if you do make those you might want to use an even older version of llama.cpp,
          so as to get the greatest possible compatibility, for anyone still using
          a library/client not updated since June 6th. I make my q[458]_[01] quants
          with tag <code>master-ffb06a3</code> (commit <code>ffb06a345e3a9e30d39aaa5b46a23201a74be6de</code>).</p>

          '
        raw: 'That''s exactly what I did to create it, and it worked at the time I
          did it.  But recently they''ve put in a check for not-divisible-by-256 tensors,
          which you can read about here: https://github.com/ggerganov/llama.cpp/issues/1919#issuecomment-1599484900


          This does rather raise the issue of whether that check is finding false
          positives, because this model clearly worked when I made it.  Though like
          you I''ve also wondered whether models like these actually have 32,001 tokens
          or whether they''ve just inherited that config.  However on at least one
          previous model I tried just editing config.json to set it to 32000, and
          it still failed because it did actually have 32001 sized tensors.


          So I don''t know.  But right now, you won''t be able to make k-quant GGMLs
          with this model, unless you roll back to a version of llama.cpp from about
          12 days ago or so. And then there''s the question of whether it will cause
          some issues or not.


          You can make the old q4_0, q4_1, q5_0, q5_1 and q8_0 quants - though if
          you do make those you might want to use an even older version of llama.cpp,
          so as to get the greatest possible compatibility, for anyone still using
          a library/client not updated since June 6th. I make my q[458]_[01] quants
          with tag `master-ffb06a3` (commit `ffb06a345e3a9e30d39aaa5b46a23201a74be6de`).'
        updatedAt: '2023-06-22T15:29:55.640Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - crestf411
    id: 64946505bf53c9e18daf46c4
    type: comment
  author: TheBloke
  content: 'That''s exactly what I did to create it, and it worked at the time I did
    it.  But recently they''ve put in a check for not-divisible-by-256 tensors, which
    you can read about here: https://github.com/ggerganov/llama.cpp/issues/1919#issuecomment-1599484900


    This does rather raise the issue of whether that check is finding false positives,
    because this model clearly worked when I made it.  Though like you I''ve also
    wondered whether models like these actually have 32,001 tokens or whether they''ve
    just inherited that config.  However on at least one previous model I tried just
    editing config.json to set it to 32000, and it still failed because it did actually
    have 32001 sized tensors.


    So I don''t know.  But right now, you won''t be able to make k-quant GGMLs with
    this model, unless you roll back to a version of llama.cpp from about 12 days
    ago or so. And then there''s the question of whether it will cause some issues
    or not.


    You can make the old q4_0, q4_1, q5_0, q5_1 and q8_0 quants - though if you do
    make those you might want to use an even older version of llama.cpp, so as to
    get the greatest possible compatibility, for anyone still using a library/client
    not updated since June 6th. I make my q[458]_[01] quants with tag `master-ffb06a3`
    (commit `ffb06a345e3a9e30d39aaa5b46a23201a74be6de`).'
  created_at: 2023-06-22 14:13:09+00:00
  edited: true
  hidden: false
  id: 64946505bf53c9e18daf46c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-22T15:25:40.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9785774350166321
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Looking back at the source model: it is a merge of Chronos and Hermes.\
          \  Chronos has 32,000 tokens, but Nous Hermes 13B has 32,001.  So that's\
          \ the source of the problem here.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;karan4d&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/karan4d\"\
          >@<span class=\"underline\">karan4d</span></a></span>\n\n\t</span></span>\
          \ - do you recall why Nous Hermes has 32,001 tokens, and is that necessary?\
          \  As you can read from the above, it's currently causing problems with\
          \ k-quant GGMLs.  Those issues will hopefully be resolved soon.  But I'm\
          \ thinking it'd probably be a good idea to avoid 32,001 token Llama models\
          \ for future models, unless there's some good reason why this is included?</p>\n\
          <p>My memory is that the first model to add that PAD token as 32,001 was\
          \ GPT4All, and it was done as a hack because they didn't set up the special\
          \ tokens correctly.  My impression is that that practice has then been inherited\
          \ by other models, maybe without an actual need to do it.</p>\n<p>It wasn't\
          \ ever a problem before, but it is causing issues now. And even if that's\
          \ resolved soon, it's probably cleaner to avoid the extra token unless it's\
          \ definitely doing something</p>\n"
        raw: 'Looking back at the source model: it is a merge of Chronos and Hermes.  Chronos
          has 32,000 tokens, but Nous Hermes 13B has 32,001.  So that''s the source
          of the problem here.


          @karan4d - do you recall why Nous Hermes has 32,001 tokens, and is that
          necessary?  As you can read from the above, it''s currently causing problems
          with k-quant GGMLs.  Those issues will hopefully be resolved soon.  But
          I''m thinking it''d probably be a good idea to avoid 32,001 token Llama
          models for future models, unless there''s some good reason why this is included?


          My memory is that the first model to add that PAD token as 32,001 was GPT4All,
          and it was done as a hack because they didn''t set up the special tokens
          correctly.  My impression is that that practice has then been inherited
          by other models, maybe without an actual need to do it.


          It wasn''t ever a problem before, but it is causing issues now. And even
          if that''s resolved soon, it''s probably cleaner to avoid the extra token
          unless it''s definitely doing something'
        updatedAt: '2023-06-22T15:25:40.005Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - crestf411
        - morka
    id: 649467f499bd5eada0801789
    type: comment
  author: TheBloke
  content: 'Looking back at the source model: it is a merge of Chronos and Hermes.  Chronos
    has 32,000 tokens, but Nous Hermes 13B has 32,001.  So that''s the source of the
    problem here.


    @karan4d - do you recall why Nous Hermes has 32,001 tokens, and is that necessary?  As
    you can read from the above, it''s currently causing problems with k-quant GGMLs.  Those
    issues will hopefully be resolved soon.  But I''m thinking it''d probably be a
    good idea to avoid 32,001 token Llama models for future models, unless there''s
    some good reason why this is included?


    My memory is that the first model to add that PAD token as 32,001 was GPT4All,
    and it was done as a hack because they didn''t set up the special tokens correctly.  My
    impression is that that practice has then been inherited by other models, maybe
    without an actual need to do it.


    It wasn''t ever a problem before, but it is causing issues now. And even if that''s
    resolved soon, it''s probably cleaner to avoid the extra token unless it''s definitely
    doing something'
  created_at: 2023-06-22 14:25:40+00:00
  edited: false
  hidden: false
  id: 649467f499bd5eada0801789
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/chronos-hermes-13B-GGML
repo_type: model
status: open
target_branch: null
title: How did you generate these? (32000 vs 32001 token issues)
