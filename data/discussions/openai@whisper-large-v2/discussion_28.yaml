!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pearlyu
conflicting_files: null
created_at: 2023-03-04 22:16:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/435f769da8105db8437ab79a6c22aef7.svg
      fullname: Pearl Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pearlyu
      type: user
    createdAt: '2023-03-04T22:16:32.000Z'
    data:
      edited: false
      editors:
      - pearlyu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/435f769da8105db8437ab79a6c22aef7.svg
          fullname: Pearl Yu
          isHf: false
          isPro: false
          name: pearlyu
          type: user
        html: "<p>When using the pipeline to get transcription with timestamps, it's\
          \ alright for some audio files, but for some of the files it returns the\
          \ error: </p>\n<pre><code>---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          &lt;ipython-input-16-8cc132230b9b&gt; in &lt;module&gt;\n----&gt; 1 prediction\
          \ = pipe(dataset[0], return_timestamps=True)[\"chunks\"]\n\n4 frames\n/usr/local/lib/python3.8/dist-packages/transformers/pipelines/automatic_speech_recognition.py\
          \ in _find_timestamp_sequence(sequences, tokenizer, feature_extractor, max_source_positions)\n\
          \    104         sequence = sequence.squeeze(0)\n    105         # get rid\
          \ of the `forced_decoder_idx` that are use to parametrize the generation\n\
          --&gt; 106         begin_idx = np.where(sequence == timestamp_begin)[0].item()\
          \ if timestamp_begin in sequence else 0\n    107         sequence = sequence[begin_idx:]\n\
          \    108 \n\nValueError: can only convert an array of size 1 to a Python\
          \ scalar\n</code></pre>\n<p>Below is the code to use the pipeline. </p>\n\
          <pre><code>device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\
          \n\npipe = pipeline(\n  \"automatic-speech-recognition\",\n  model=\"openai/whisper-tiny\"\
          ,\n  chunk_length_s=30,\n  device=device,\n)\n\nfilename = files[71][0]\n\
          mypath = '/content/drive/MyDrive/twitch_data/audios/prediction/'\naudio,\
          \ _ = librosa.load(mypath+ filename, sr = 16000)\n\nmy_dict = {\"raw\":\
          \ np.array(audio), 'sampling_rate': np.array(16000)}\nprediction = pipe(my_dict,\
          \ return_timestamps=True)[\"chunks\"]\n</code></pre>\n<p>I'm not sure if\
          \ this is a bug, or if there's something wrong with the files. Any help\
          \ is appreciated!</p>\n"
        raw: "When using the pipeline to get transcription with timestamps, it's alright\
          \ for some audio files, but for some of the files it returns the error:\
          \ \r\n\r\n```\r\n---------------------------------------------------------------------------\r\
          \nValueError                                Traceback (most recent call\
          \ last)\r\n<ipython-input-16-8cc132230b9b> in <module>\r\n----> 1 prediction\
          \ = pipe(dataset[0], return_timestamps=True)[\"chunks\"]\r\n\r\n4 frames\r\
          \n/usr/local/lib/python3.8/dist-packages/transformers/pipelines/automatic_speech_recognition.py\
          \ in _find_timestamp_sequence(sequences, tokenizer, feature_extractor, max_source_positions)\r\
          \n    104         sequence = sequence.squeeze(0)\r\n    105         # get\
          \ rid of the `forced_decoder_idx` that are use to parametrize the generation\r\
          \n--> 106         begin_idx = np.where(sequence == timestamp_begin)[0].item()\
          \ if timestamp_begin in sequence else 0\r\n    107         sequence = sequence[begin_idx:]\r\
          \n    108 \r\n\r\nValueError: can only convert an array of size 1 to a Python\
          \ scalar\r\n```\r\n\r\nBelow is the code to use the pipeline. \r\n```\r\n\
          device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n\r\npipe\
          \ = pipeline(\r\n  \"automatic-speech-recognition\",\r\n  model=\"openai/whisper-tiny\"\
          ,\r\n  chunk_length_s=30,\r\n  device=device,\r\n)\r\n\r\nfilename = files[71][0]\r\
          \nmypath = '/content/drive/MyDrive/twitch_data/audios/prediction/'\r\naudio,\
          \ _ = librosa.load(mypath+ filename, sr = 16000)\r\n\r\nmy_dict = {\"raw\"\
          : np.array(audio), 'sampling_rate': np.array(16000)}\r\nprediction = pipe(my_dict,\
          \ return_timestamps=True)[\"chunks\"]\r\n```\r\n\r\nI'm not sure if this\
          \ is a bug, or if there's something wrong with the files. Any help is appreciated!"
        updatedAt: '2023-03-04T22:16:32.117Z'
      numEdits: 0
      reactions: []
    id: 6403c340929304a3c8037f9c
    type: comment
  author: pearlyu
  content: "When using the pipeline to get transcription with timestamps, it's alright\
    \ for some audio files, but for some of the files it returns the error: \r\n\r\
    \n```\r\n---------------------------------------------------------------------------\r\
    \nValueError                                Traceback (most recent call last)\r\
    \n<ipython-input-16-8cc132230b9b> in <module>\r\n----> 1 prediction = pipe(dataset[0],\
    \ return_timestamps=True)[\"chunks\"]\r\n\r\n4 frames\r\n/usr/local/lib/python3.8/dist-packages/transformers/pipelines/automatic_speech_recognition.py\
    \ in _find_timestamp_sequence(sequences, tokenizer, feature_extractor, max_source_positions)\r\
    \n    104         sequence = sequence.squeeze(0)\r\n    105         # get rid\
    \ of the `forced_decoder_idx` that are use to parametrize the generation\r\n-->\
    \ 106         begin_idx = np.where(sequence == timestamp_begin)[0].item() if timestamp_begin\
    \ in sequence else 0\r\n    107         sequence = sequence[begin_idx:]\r\n  \
    \  108 \r\n\r\nValueError: can only convert an array of size 1 to a Python scalar\r\
    \n```\r\n\r\nBelow is the code to use the pipeline. \r\n```\r\ndevice = \"cuda:0\"\
    \ if torch.cuda.is_available() else \"cpu\"\r\n\r\npipe = pipeline(\r\n  \"automatic-speech-recognition\"\
    ,\r\n  model=\"openai/whisper-tiny\",\r\n  chunk_length_s=30,\r\n  device=device,\r\
    \n)\r\n\r\nfilename = files[71][0]\r\nmypath = '/content/drive/MyDrive/twitch_data/audios/prediction/'\r\
    \naudio, _ = librosa.load(mypath+ filename, sr = 16000)\r\n\r\nmy_dict = {\"raw\"\
    : np.array(audio), 'sampling_rate': np.array(16000)}\r\nprediction = pipe(my_dict,\
    \ return_timestamps=True)[\"chunks\"]\r\n```\r\n\r\nI'm not sure if this is a\
    \ bug, or if there's something wrong with the files. Any help is appreciated!"
  created_at: 2023-03-04 22:16:32+00:00
  edited: false
  hidden: false
  id: 6403c340929304a3c8037f9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-03-17T15:31:38.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;pearlyu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/pearlyu\"\
          >@<span class=\"underline\">pearlyu</span></a></span>\n\n\t</span></span>!\
          \ Thanks for flagging this and sorry for getting back to you so late. Are\
          \ you able to reproduce this bug using an audio file we have access to on\
          \ our end? Either you can share the audio file you get the error with, or\
          \ try using an audio sample from a HF dataset:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> datasets <span\
          \ class=\"hljs-keyword\">import</span> load_dataset\n\nlibrispeech = load_dataset(<span\
          \ class=\"hljs-string\">\"hf-internal-testing/librispeech_asr_dummy\"</span>,\
          \ <span class=\"hljs-string\">\"clean\"</span>, split=<span class=\"hljs-string\"\
          >\"validation\"</span>)\n\nsample = librispeech[<span class=\"hljs-number\"\
          >0</span>][<span class=\"hljs-string\">\"audio\"</span>]\n\nprediction =\
          \ pipe(sample, return_timestamps=<span class=\"hljs-literal\">True</span>)[<span\
          \ class=\"hljs-string\">\"chunks\"</span>]\n</code></pre>\n<p>We'd need\
          \ an audio file that breaks the pipeline in order to investigate what's\
          \ going on!</p>\n"
        raw: 'Hey @pearlyu! Thanks for flagging this and sorry for getting back to
          you so late. Are you able to reproduce this bug using an audio file we have
          access to on our end? Either you can share the audio file you get the error
          with, or try using an audio sample from a HF dataset:

          ```python

          from datasets import load_dataset


          librispeech = load_dataset("hf-internal-testing/librispeech_asr_dummy",
          "clean", split="validation")


          sample = librispeech[0]["audio"]


          prediction = pipe(sample, return_timestamps=True)["chunks"]

          ```


          We''d need an audio file that breaks the pipeline in order to investigate
          what''s going on!'
        updatedAt: '2023-03-17T15:32:06.395Z'
      numEdits: 1
      reactions: []
    id: 641487dafbb4a9ca9a28606c
    type: comment
  author: sanchit-gandhi
  content: 'Hey @pearlyu! Thanks for flagging this and sorry for getting back to you
    so late. Are you able to reproduce this bug using an audio file we have access
    to on our end? Either you can share the audio file you get the error with, or
    try using an audio sample from a HF dataset:

    ```python

    from datasets import load_dataset


    librispeech = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean",
    split="validation")


    sample = librispeech[0]["audio"]


    prediction = pipe(sample, return_timestamps=True)["chunks"]

    ```


    We''d need an audio file that breaks the pipeline in order to investigate what''s
    going on!'
  created_at: 2023-03-17 14:31:38+00:00
  edited: true
  hidden: false
  id: 641487dafbb4a9ca9a28606c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/05f3034287c1d3f67edb6e3ea70e35be.svg
      fullname: DarveenVijayan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darveen
      type: user
    createdAt: '2023-04-06T08:45:46.000Z'
    data:
      edited: false
      editors:
      - darveen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/05f3034287c1d3f67edb6e3ea70e35be.svg
          fullname: DarveenVijayan
          isHf: false
          isPro: false
          name: darveen
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>\
          \ , the piece of code that you shared throws the following error:<br>ValueError:\
          \ We cannot return_timestamps yet on non-ctc models !</p>\n"
        raw: "Hi @sanchit-gandhi , the piece of code that you shared throws the following\
          \ error: \nValueError: We cannot return_timestamps yet on non-ctc models\
          \ !"
        updatedAt: '2023-04-06T08:45:46.325Z'
      numEdits: 0
      reactions: []
    id: 642e86ba7ea623215d8f2667
    type: comment
  author: darveen
  content: "Hi @sanchit-gandhi , the piece of code that you shared throws the following\
    \ error: \nValueError: We cannot return_timestamps yet on non-ctc models !"
  created_at: 2023-04-06 07:45:46+00:00
  edited: false
  hidden: false
  id: 642e86ba7ea623215d8f2667
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-04-18T17:05:21.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>Could you update <code>transformers</code> to the latest version
          please?</p>

          <pre><code>pip install --upgrade transformers

          </code></pre>

          '
        raw: 'Could you update `transformers` to the latest version please?

          ```

          pip install --upgrade transformers

          ```'
        updatedAt: '2023-04-18T17:05:21.143Z'
      numEdits: 0
      reactions: []
    id: 643ecdd1f2ed3bc5c0611179
    type: comment
  author: sanchit-gandhi
  content: 'Could you update `transformers` to the latest version please?

    ```

    pip install --upgrade transformers

    ```'
  created_at: 2023-04-18 16:05:21+00:00
  edited: false
  hidden: false
  id: 643ecdd1f2ed3bc5c0611179
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 28
repo_id: openai/whisper-large-v2
repo_type: model
status: open
target_branch: null
title: 'return_timestamps error '
