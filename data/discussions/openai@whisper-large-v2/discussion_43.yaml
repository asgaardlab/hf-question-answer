!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jayce777
conflicting_files: null
created_at: 2023-05-10 07:03:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/81505461fc50e7cc29a4f97333760a07.svg
      fullname: jayce talis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jayce777
      type: user
    createdAt: '2023-05-10T08:03:59.000Z'
    data:
      edited: false
      editors:
      - jayce777
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/81505461fc50e7cc29a4f97333760a07.svg
          fullname: jayce talis
          isHf: false
          isPro: false
          name: jayce777
          type: user
        html: "<p>Hi everyone,</p>\n<p>I have been using the whisper-large-v2 model\
          \ to transcribe audio files that are about 10 to 20 minutes long. I have\
          \ tried many different techniques to improve the accuracy of the transcription,\
          \ but so far, nothing has worked.</p>\n<p>One of the main issues I am facing\
          \ is that there is repeated translation information between chunks, and\
          \ some translation information is missing altogether. I have tried normalizing\
          \ the audio to [-1,1], as well as tuning hyperparameters such as chunk_length_s\
          \ and stride_length_s. However, I have found that the optimal parameters\
          \ vary from file to file, and there is no generalization.</p>\n<p>I am using\
          \ the following environment:</p>\n<ul>\n<li>whisper-large-v2</li>\n<li>torch\
          \ 1.10.0+cu111</li>\n<li>transformers 4.28.1 (latest)</li>\n</ul>\n<p>Here\
          \ is the code I am using:</p>\n<pre><code>pipe = pipeline(\n    task=\"\
          automatic-speech-recognition\",\n    model=MODEL_NAME,\n    device='cuda:0',\n\
          \    generate_kwargs={\"task\": \"transcribe\"},\n)\nsound = AudioSegment.from_file(\"\
          sample.mp3\", format=\"mp3\")\nsound.export(\"sample.wav\", format=\"wav\"\
          )\ny, sr = librosa.load(\"sample.wav\", sr=None)\ny_resampled = librosa.resample(y,\
          \ orig_sr=sr, target_sr=16000)\n\noutputs = pipe(y_resampled, return_timestamps=True,\
          \ generate_kwargs={\"task\": \"transcribe\", \"language\": \"&lt;|zh|&gt;\"\
          }, chunk_length_s=30, stride_length_s=5, batch_size=16, max_new_tokens=512)\n\
          print(outputs[\"text\"])\n</code></pre>\n<p>Overall, I feel that the performance\
          \ of the model is not good enough for me to use it formally. I would appreciate\
          \ any suggestions or advice on how to improve the accuracy of the transcription.</p>\n\
          <p>Thank you.</p>\n"
        raw: "Hi everyone,\r\n\r\nI have been using the whisper-large-v2 model to\
          \ transcribe audio files that are about 10 to 20 minutes long. I have tried\
          \ many different techniques to improve the accuracy of the transcription,\
          \ but so far, nothing has worked.\r\n\r\nOne of the main issues I am facing\
          \ is that there is repeated translation information between chunks, and\
          \ some translation information is missing altogether. I have tried normalizing\
          \ the audio to [-1,1], as well as tuning hyperparameters such as chunk_length_s\
          \ and stride_length_s. However, I have found that the optimal parameters\
          \ vary from file to file, and there is no generalization.\r\n\r\nI am using\
          \ the following environment:\r\n\r\n- whisper-large-v2\r\n- torch 1.10.0+cu111\r\
          \n- transformers 4.28.1 (latest)\r\n\r\nHere is the code I am using:\r\n\
          ```\r\npipe = pipeline(\r\n    task=\"automatic-speech-recognition\",\r\n\
          \    model=MODEL_NAME,\r\n    device='cuda:0',\r\n    generate_kwargs={\"\
          task\": \"transcribe\"},\r\n)\r\nsound = AudioSegment.from_file(\"sample.mp3\"\
          , format=\"mp3\")\r\nsound.export(\"sample.wav\", format=\"wav\")\r\ny,\
          \ sr = librosa.load(\"sample.wav\", sr=None)\r\ny_resampled = librosa.resample(y,\
          \ orig_sr=sr, target_sr=16000)\r\n\r\noutputs = pipe(y_resampled, return_timestamps=True,\
          \ generate_kwargs={\"task\": \"transcribe\", \"language\": \"<|zh|>\"},\
          \ chunk_length_s=30, stride_length_s=5, batch_size=16, max_new_tokens=512)\r\
          \nprint(outputs[\"text\"])\r\n```\r\n\r\nOverall, I feel that the performance\
          \ of the model is not good enough for me to use it formally. I would appreciate\
          \ any suggestions or advice on how to improve the accuracy of the transcription.\r\
          \n\r\nThank you."
        updatedAt: '2023-05-10T08:03:59.507Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Tyler1992
    id: 645b4fef9522edfc8e4dadc9
    type: comment
  author: jayce777
  content: "Hi everyone,\r\n\r\nI have been using the whisper-large-v2 model to transcribe\
    \ audio files that are about 10 to 20 minutes long. I have tried many different\
    \ techniques to improve the accuracy of the transcription, but so far, nothing\
    \ has worked.\r\n\r\nOne of the main issues I am facing is that there is repeated\
    \ translation information between chunks, and some translation information is\
    \ missing altogether. I have tried normalizing the audio to [-1,1], as well as\
    \ tuning hyperparameters such as chunk_length_s and stride_length_s. However,\
    \ I have found that the optimal parameters vary from file to file, and there is\
    \ no generalization.\r\n\r\nI am using the following environment:\r\n\r\n- whisper-large-v2\r\
    \n- torch 1.10.0+cu111\r\n- transformers 4.28.1 (latest)\r\n\r\nHere is the code\
    \ I am using:\r\n```\r\npipe = pipeline(\r\n    task=\"automatic-speech-recognition\"\
    ,\r\n    model=MODEL_NAME,\r\n    device='cuda:0',\r\n    generate_kwargs={\"\
    task\": \"transcribe\"},\r\n)\r\nsound = AudioSegment.from_file(\"sample.mp3\"\
    , format=\"mp3\")\r\nsound.export(\"sample.wav\", format=\"wav\")\r\ny, sr = librosa.load(\"\
    sample.wav\", sr=None)\r\ny_resampled = librosa.resample(y, orig_sr=sr, target_sr=16000)\r\
    \n\r\noutputs = pipe(y_resampled, return_timestamps=True, generate_kwargs={\"\
    task\": \"transcribe\", \"language\": \"<|zh|>\"}, chunk_length_s=30, stride_length_s=5,\
    \ batch_size=16, max_new_tokens=512)\r\nprint(outputs[\"text\"])\r\n```\r\n\r\n\
    Overall, I feel that the performance of the model is not good enough for me to\
    \ use it formally. I would appreciate any suggestions or advice on how to improve\
    \ the accuracy of the transcription.\r\n\r\nThank you."
  created_at: 2023-05-10 07:03:59+00:00
  edited: false
  hidden: false
  id: 645b4fef9522edfc8e4dadc9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ce0ec1912a699f59601c9c814f7c7b1.svg
      fullname: Andreas Schmidt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ejmiddle
      type: user
    createdAt: '2023-10-01T11:36:39.000Z'
    data:
      edited: false
      editors:
      - ejmiddle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8937195539474487
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ce0ec1912a699f59601c9c814f7c7b1.svg
          fullname: Andreas Schmidt
          isHf: false
          isPro: false
          name: ejmiddle
          type: user
        html: '<p>Hi there, </p>

          <p>any ideas on this? I am having the same problem using very similar code
          and parameters. From my tests it looks like it is most likely an issue with
          the chunking algorithm. --&gt; <a href="https://huggingface.co/blog/asr-chunking">https://huggingface.co/blog/asr-chunking</a></p>

          <p>Playing around with the stride parameters sometimes gives me good results.
          Then using the same parameters for a different file, again there may be
          parts missing. </p>

          <p>I am comparing two the openai whisper API and they always get it right.
          So it must be solvable obiously :) </p>

          <p>Thx so much<br>Andi</p>

          '
        raw: "Hi there, \n\nany ideas on this? I am having the same problem using\
          \ very similar code and parameters. From my tests it looks like it is most\
          \ likely an issue with the chunking algorithm. --> https://huggingface.co/blog/asr-chunking\n\
          \nPlaying around with the stride parameters sometimes gives me good results.\
          \ Then using the same parameters for a different file, again there may be\
          \ parts missing. \n\nI am comparing two the openai whisper API and they\
          \ always get it right. So it must be solvable obiously :) \n\nThx so much\n\
          Andi"
        updatedAt: '2023-10-01T11:36:39.317Z'
      numEdits: 0
      reactions: []
    id: 651959c7404da51aa0435d64
    type: comment
  author: ejmiddle
  content: "Hi there, \n\nany ideas on this? I am having the same problem using very\
    \ similar code and parameters. From my tests it looks like it is most likely an\
    \ issue with the chunking algorithm. --> https://huggingface.co/blog/asr-chunking\n\
    \nPlaying around with the stride parameters sometimes gives me good results. Then\
    \ using the same parameters for a different file, again there may be parts missing.\
    \ \n\nI am comparing two the openai whisper API and they always get it right.\
    \ So it must be solvable obiously :) \n\nThx so much\nAndi"
  created_at: 2023-10-01 10:36:39+00:00
  edited: false
  hidden: false
  id: 651959c7404da51aa0435d64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2bb24e22400a995cefa553cf6c5be1f.svg
      fullname: Shubham Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Abaddon310
      type: user
    createdAt: '2024-01-06T20:03:58.000Z'
    data:
      edited: true
      editors:
      - Abaddon310
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8823562860488892
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2bb24e22400a995cefa553cf6c5be1f.svg
          fullname: Shubham Gupta
          isHf: false
          isPro: false
          name: Abaddon310
          type: user
        html: '<p>Hello Good People,</p>

          <p>Any update on this? I''m experiencing a similar problem where the transcription
          contains repeated text or is missing some content, particularly when the
          audio length exceeds 30 seconds.</p>

          <p>Regards<br>Abaddon - The Knight of Hell</p>

          '
        raw: 'Hello Good People,


          Any update on this? I''m experiencing a similar problem where the transcription
          contains repeated text or is missing some content, particularly when the
          audio length exceeds 30 seconds.


          Regards

          Abaddon - The Knight of Hell'
        updatedAt: '2024-01-06T20:04:34.938Z'
      numEdits: 1
      reactions: []
    id: 6599b22ef0102bce683d56f3
    type: comment
  author: Abaddon310
  content: 'Hello Good People,


    Any update on this? I''m experiencing a similar problem where the transcription
    contains repeated text or is missing some content, particularly when the audio
    length exceeds 30 seconds.


    Regards

    Abaddon - The Knight of Hell'
  created_at: 2024-01-06 20:03:58+00:00
  edited: true
  hidden: false
  id: 6599b22ef0102bce683d56f3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 43
repo_id: openai/whisper-large-v2
repo_type: model
status: open
target_branch: null
title: 'Whisper-Large-v2 Model for Audio Transcription: Repeated and Missing Translation
  Information'
