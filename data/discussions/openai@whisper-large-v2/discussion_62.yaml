!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MightyStudent
conflicting_files: null
created_at: 2023-08-31 14:08:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670502544482-noauth.jpeg?w=200&h=200&f=face
      fullname: Mohamed Ahmed
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MightyStudent
      type: user
    createdAt: '2023-08-31T15:08:33.000Z'
    data:
      edited: false
      editors:
      - MightyStudent
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.958628237247467
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670502544482-noauth.jpeg?w=200&h=200&f=face
          fullname: Mohamed Ahmed
          isHf: false
          isPro: false
          name: MightyStudent
          type: user
        html: "<p>I have trained whisper using PEFT on a small Egyptian Arabic dataset\
          \ (a dialect of Arabic). The results were fine, managed to achieve a a wer\
          \ of 40% (vanila whisper did 61%). Most of the mistakes were a misspelling\
          \ of a character or two.</p>\n<p>For my understanding there are two ways\
          \ to improve performance more:</p>\n<ul>\n<li>Simply just add more data\
          \ (can't find more online and annotation is not feasible)</li>\n<li>train\
          \ more (already reached fitting point)</li>\n<li>train a spell checker using\
          \ Egyptian Arabic text scrapped from the interet (There are alot) and post\
          \ process whisper output using it.</li>\n</ul>\n<p>I'm currently working\
          \ on the second method but I'll need more time, would like some of the experts\
          \ opinion on my approach to the matter. Thank you!</p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;sanchit-gandhi&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/sanchit-gandhi\">@<span class=\"underline\"\
          >sanchit-gandhi</span></a></span>\n\n\t</span></span> </p>\n"
        raw: "I have trained whisper using PEFT on a small Egyptian Arabic dataset\
          \ (a dialect of Arabic). The results were fine, managed to achieve a a wer\
          \ of 40% (vanila whisper did 61%). Most of the mistakes were a misspelling\
          \ of a character or two.\r\n\r\nFor my understanding there are two ways\
          \ to improve performance more:\r\n* Simply just add more data (can't find\
          \ more online and annotation is not feasible)\r\n* train more (already reached\
          \ fitting point)\r\n* train a spell checker using Egyptian Arabic text scrapped\
          \ from the interet (There are alot) and post process whisper output using\
          \ it.\r\n\r\nI'm currently working on the second method but I'll need more\
          \ time, would like some of the experts opinion on my approach to the matter.\
          \ Thank you!\r\n\r\n@sanchit-gandhi "
        updatedAt: '2023-08-31T15:08:33.885Z'
      numEdits: 0
      reactions: []
    id: 64f0acf1325f6e0f628f76ed
    type: comment
  author: MightyStudent
  content: "I have trained whisper using PEFT on a small Egyptian Arabic dataset (a\
    \ dialect of Arabic). The results were fine, managed to achieve a a wer of 40%\
    \ (vanila whisper did 61%). Most of the mistakes were a misspelling of a character\
    \ or two.\r\n\r\nFor my understanding there are two ways to improve performance\
    \ more:\r\n* Simply just add more data (can't find more online and annotation\
    \ is not feasible)\r\n* train more (already reached fitting point)\r\n* train\
    \ a spell checker using Egyptian Arabic text scrapped from the interet (There\
    \ are alot) and post process whisper output using it.\r\n\r\nI'm currently working\
    \ on the second method but I'll need more time, would like some of the experts\
    \ opinion on my approach to the matter. Thank you!\r\n\r\n@sanchit-gandhi "
  created_at: 2023-08-31 14:08:33+00:00
  edited: false
  hidden: false
  id: 64f0acf1325f6e0f628f76ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-08-31T15:59:13.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9060232043266296
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Thanks for posting <span data-props=\"{&quot;user&quot;:&quot;MightyStudent&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MightyStudent\"\
          >@<span class=\"underline\">MightyStudent</span></a></span>\n\n\t</span></span>!\
          \ For training a spell checker, you have a sufficient corpus of <code>{incorrect,\
          \ correct}</code> pairs of text? Are you just planning on training a vanilla\
          \ encoder-decoder model for this? Note that your inference time will increase\
          \ based on the size of the spell checker model that you use, whereas training\
          \ with more <code>{audio, text}</code> data and reducing the WER of the\
          \ Whisper model won't have any affect</p>\n<p>If you don't care about inference\
          \ speed, you could also run your model with beam search at inference time\
          \ (slower, but more accurate). Also make sure that you're running inference\
          \ <strong>without</strong> quantisation enabled for peak performance: <a\
          \ rel=\"nofollow\" href=\"https://github.com/huggingface/peft/discussions/477#discussion-5213394\"\
          >https://github.com/huggingface/peft/discussions/477#discussion-5213394</a></p>\n"
        raw: 'Thanks for posting @MightyStudent! For training a spell checker, you
          have a sufficient corpus of `{incorrect, correct}` pairs of text? Are you
          just planning on training a vanilla encoder-decoder model for this? Note
          that your inference time will increase based on the size of the spell checker
          model that you use, whereas training with more `{audio, text}` data and
          reducing the WER of the Whisper model won''t have any affect


          If you don''t care about inference speed, you could also run your model
          with beam search at inference time (slower, but more accurate). Also make
          sure that you''re running inference **without** quantisation enabled for
          peak performance: https://github.com/huggingface/peft/discussions/477#discussion-5213394'
        updatedAt: '2023-08-31T15:59:13.221Z'
      numEdits: 0
      reactions: []
    id: 64f0b8d1dcac1f99adb7103b
    type: comment
  author: sanchit-gandhi
  content: 'Thanks for posting @MightyStudent! For training a spell checker, you have
    a sufficient corpus of `{incorrect, correct}` pairs of text? Are you just planning
    on training a vanilla encoder-decoder model for this? Note that your inference
    time will increase based on the size of the spell checker model that you use,
    whereas training with more `{audio, text}` data and reducing the WER of the Whisper
    model won''t have any affect


    If you don''t care about inference speed, you could also run your model with beam
    search at inference time (slower, but more accurate). Also make sure that you''re
    running inference **without** quantisation enabled for peak performance: https://github.com/huggingface/peft/discussions/477#discussion-5213394'
  created_at: 2023-08-31 14:59:13+00:00
  edited: false
  hidden: false
  id: 64f0b8d1dcac1f99adb7103b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 62
repo_id: openai/whisper-large-v2
repo_type: model
status: open
target_branch: null
title: Will spell corrector on whisper output be worth it?
