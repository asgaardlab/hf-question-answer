!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rbgreenway
conflicting_files: null
created_at: 2023-10-30 21:53:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ff983b88f31127d538affca08e16916.svg
      fullname: Bryan Greenway
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rbgreenway
      type: user
    createdAt: '2023-10-30T22:53:15.000Z'
    data:
      edited: false
      editors:
      - rbgreenway
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9443103671073914
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ff983b88f31127d538affca08e16916.svg
          fullname: Bryan Greenway
          isHf: false
          isPro: false
          name: rbgreenway
          type: user
        html: '<p>I''m trying to get Torchserve to serve Whisper, but have a few issues
          I can''t seem to resolve.  None of the examples that I can find utilize
          the HF pipeline.  Instead, they use the inference API.</p>

          <p>1 - I believe that I need to use an HF pipeline in order to leverage
          the "chunking" feature.  I need to transcribe files that are much longer
          than 30 secs, so believe that the pipeline is the only way to use chunking.  I
          was able to successfully create a torchserve mar file (model archive); however,
          I can''t find a way to create a pipeline utilizing the model saved in the
          mar file.</p>

          <p>2 - If it''s not possible to create the pipeline from the resources in
          the mar file, can I just create it using:</p>

          <p>pipe = pipeline(<br>  "automatic-speech-recognition",<br>  model="openai/whisper-large-v2",<br>  chunk_length_s=30,<br>  device=device,<br>)</p>

          <p>and place the above code in the Torchserve custom handler''s initialization
          function?  If I have a local copy of the pytorch_model.bin, is there a way
          to just load that into a model object that can be passed as the model parameter
          into the pipeline constuctor?  Seems like I''d need to somehow set up the
          processor/tokenizer as well.</p>

          <p>I have no specific loyalty to Torchserve, but need some way to robustly
          serve this model.  Seems like solid approach, but if there''s something
          else that anyone would recommend, I''m open.</p>

          <p>Thanks for any suggestions.  </p>

          '
        raw: "I'm trying to get Torchserve to serve Whisper, but have a few issues\
          \ I can't seem to resolve.  None of the examples that I can find utilize\
          \ the HF pipeline.  Instead, they use the inference API.\r\n\r\n1 - I believe\
          \ that I need to use an HF pipeline in order to leverage the \"chunking\"\
          \ feature.  I need to transcribe files that are much longer than 30 secs,\
          \ so believe that the pipeline is the only way to use chunking.  I was able\
          \ to successfully create a torchserve mar file (model archive); however,\
          \ I can't find a way to create a pipeline utilizing the model saved in the\
          \ mar file.\r\n\r\n2 - If it's not possible to create the pipeline from\
          \ the resources in the mar file, can I just create it using:\r\n\r\npipe\
          \ = pipeline(\r\n  \"automatic-speech-recognition\",\r\n  model=\"openai/whisper-large-v2\"\
          ,\r\n  chunk_length_s=30,\r\n  device=device,\r\n)\r\n\r\nand place the\
          \ above code in the Torchserve custom handler's initialization function?\
          \  If I have a local copy of the pytorch_model.bin, is there a way to just\
          \ load that into a model object that can be passed as the model parameter\
          \ into the pipeline constuctor?  Seems like I'd need to somehow set up the\
          \ processor/tokenizer as well.\r\n\r\nI have no specific loyalty to Torchserve,\
          \ but need some way to robustly serve this model.  Seems like solid approach,\
          \ but if there's something else that anyone would recommend, I'm open.\r\
          \n\r\nThanks for any suggestions.  "
        updatedAt: '2023-10-30T22:53:15.556Z'
      numEdits: 0
      reactions: []
    id: 654033db99e887c0404d5a84
    type: comment
  author: rbgreenway
  content: "I'm trying to get Torchserve to serve Whisper, but have a few issues I\
    \ can't seem to resolve.  None of the examples that I can find utilize the HF\
    \ pipeline.  Instead, they use the inference API.\r\n\r\n1 - I believe that I\
    \ need to use an HF pipeline in order to leverage the \"chunking\" feature.  I\
    \ need to transcribe files that are much longer than 30 secs, so believe that\
    \ the pipeline is the only way to use chunking.  I was able to successfully create\
    \ a torchserve mar file (model archive); however, I can't find a way to create\
    \ a pipeline utilizing the model saved in the mar file.\r\n\r\n2 - If it's not\
    \ possible to create the pipeline from the resources in the mar file, can I just\
    \ create it using:\r\n\r\npipe = pipeline(\r\n  \"automatic-speech-recognition\"\
    ,\r\n  model=\"openai/whisper-large-v2\",\r\n  chunk_length_s=30,\r\n  device=device,\r\
    \n)\r\n\r\nand place the above code in the Torchserve custom handler's initialization\
    \ function?  If I have a local copy of the pytorch_model.bin, is there a way to\
    \ just load that into a model object that can be passed as the model parameter\
    \ into the pipeline constuctor?  Seems like I'd need to somehow set up the processor/tokenizer\
    \ as well.\r\n\r\nI have no specific loyalty to Torchserve, but need some way\
    \ to robustly serve this model.  Seems like solid approach, but if there's something\
    \ else that anyone would recommend, I'm open.\r\n\r\nThanks for any suggestions.\
    \  "
  created_at: 2023-10-30 21:53:15+00:00
  edited: false
  hidden: false
  id: 654033db99e887c0404d5a84
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 87
repo_id: openai/whisper-large-v2
repo_type: model
status: open
target_branch: null
title: Whisper with Torchserve
