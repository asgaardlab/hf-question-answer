!!python/object:huggingface_hub.community.DiscussionWithDetails
author: roshanjames
conflicting_files: null
created_at: 2023-01-17 18:08:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9832b403929cdb918b0da141e0c38fb9.svg
      fullname: Roshan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: roshanjames
      type: user
    createdAt: '2023-01-17T18:08:24.000Z'
    data:
      edited: false
      editors:
      - roshanjames
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9832b403929cdb918b0da141e0c38fb9.svg
          fullname: Roshan
          isHf: false
          isPro: false
          name: roshanjames
          type: user
        html: "<p>Hi, I wrote the following code that will record audio, save it to\
          \ a file and then use  whisper-large-v2 to transcribe. </p>\n<pre><code>from\
          \ transformers import pipeline\nimport sounddevice as sd\nimport scipy.io.wavfile\
          \ as wav\n\npipe = pipeline(\n    task=\"automatic-speech-recognition\"\
          ,\n    model=\"openai/whisper-large-v2\")\n\n# Set the recording parameters\n\
          fs = 44100  # Sample rate\nduration = 60  # Recording duration in seconds\n\
          channels = 2  # Number of channels\n\n# Record the audio\naudio = sd.rec(int(fs\
          \ * duration), samplerate=fs, channels=channels)\nprint(\"Recording...\"\
          , flush=True)\nsd.wait()  # Wait until recording is finished\nprint(\"Recording\
          \ stopped.\", flush=True)\nwav.write('audio.wav',fs, audio)\n\nout = pipe('audio.wav')[\"\
          text\"]\nprint(out)\n</code></pre>\n<p>I have 2 questions:</p>\n<ol>\n<li><p>Instead\
          \ of saving the audio to a file as I do above, is it possible to pass the\
          \ audio (numpy array) to the model? </p>\n</li>\n<li><p>In addition, can\
          \ I actually stream audio to the model (instead of stopping the recording\
          \ at some point and getting the transcription upto that point)?</p>\n</li>\n\
          </ol>\n"
        raw: "Hi, I wrote the following code that will record audio, save it to a\
          \ file and then use  whisper-large-v2 to transcribe. \r\n\r\n```\r\nfrom\
          \ transformers import pipeline\r\nimport sounddevice as sd\r\nimport scipy.io.wavfile\
          \ as wav\r\n\r\npipe = pipeline(\r\n    task=\"automatic-speech-recognition\"\
          ,\r\n    model=\"openai/whisper-large-v2\")\r\n\r\n# Set the recording parameters\r\
          \nfs = 44100  # Sample rate\r\nduration = 60  # Recording duration in seconds\r\
          \nchannels = 2  # Number of channels\r\n\r\n# Record the audio\r\naudio\
          \ = sd.rec(int(fs * duration), samplerate=fs, channels=channels)\r\nprint(\"\
          Recording...\", flush=True)\r\nsd.wait()  # Wait until recording is finished\r\
          \nprint(\"Recording stopped.\", flush=True)\r\nwav.write('audio.wav',fs,\
          \ audio)\r\n\r\nout = pipe('audio.wav')[\"text\"]\r\nprint(out)\r\n```\r\
          \n\r\nI have 2 questions:\r\n\r\n1) Instead of saving the audio to a file\
          \ as I do above, is it possible to pass the audio (numpy array) to the model?\
          \ \r\n\r\n2) In addition, can I actually stream audio to the model (instead\
          \ of stopping the recording at some point and getting the transcription\
          \ upto that point)?"
        updatedAt: '2023-01-17T18:08:24.011Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - sibbl
    id: 63c6e418656e7822e238c8d5
    type: comment
  author: roshanjames
  content: "Hi, I wrote the following code that will record audio, save it to a file\
    \ and then use  whisper-large-v2 to transcribe. \r\n\r\n```\r\nfrom transformers\
    \ import pipeline\r\nimport sounddevice as sd\r\nimport scipy.io.wavfile as wav\r\
    \n\r\npipe = pipeline(\r\n    task=\"automatic-speech-recognition\",\r\n    model=\"\
    openai/whisper-large-v2\")\r\n\r\n# Set the recording parameters\r\nfs = 44100\
    \  # Sample rate\r\nduration = 60  # Recording duration in seconds\r\nchannels\
    \ = 2  # Number of channels\r\n\r\n# Record the audio\r\naudio = sd.rec(int(fs\
    \ * duration), samplerate=fs, channels=channels)\r\nprint(\"Recording...\", flush=True)\r\
    \nsd.wait()  # Wait until recording is finished\r\nprint(\"Recording stopped.\"\
    , flush=True)\r\nwav.write('audio.wav',fs, audio)\r\n\r\nout = pipe('audio.wav')[\"\
    text\"]\r\nprint(out)\r\n```\r\n\r\nI have 2 questions:\r\n\r\n1) Instead of saving\
    \ the audio to a file as I do above, is it possible to pass the audio (numpy array)\
    \ to the model? \r\n\r\n2) In addition, can I actually stream audio to the model\
    \ (instead of stopping the recording at some point and getting the transcription\
    \ upto that point)?"
  created_at: 2023-01-17 18:08:24+00:00
  edited: false
  hidden: false
  id: 63c6e418656e7822e238c8d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-01-19T15:32:35.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;roshanjames&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/roshanjames\"\
          >@<span class=\"underline\">roshanjames</span></a></span>\n\n\t</span></span>!</p>\n\
          <ol>\n<li>It is indeed possible to pass a numpy array as an input to the\
          \ <code>pipeline</code> method (see <a href=\"https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline.__call__.inputs\"\
          >https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline.__call__.inputs</a>).\
          \ Just be sure to also pass the sampling rate such that the recorded audio\
          \ sample is resampled to match the sampling rate expected by Whisper (16kHz)\
          \ and that your numpy array is a 1-dimensional array (you need to double\
          \ check this if you're recording with two-channels -&gt; does <code>sounddevice</code>\
          \ return a 1-dimensional array in this case or is it 2-d?):</li>\n</ol>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> pipeline\n<span\
          \ class=\"hljs-keyword\">import</span> sounddevice <span class=\"hljs-keyword\"\
          >as</span> sd\n<span class=\"hljs-keyword\">import</span> scipy.io.wavfile\
          \ <span class=\"hljs-keyword\">as</span> wav\n\npipe = pipeline(\n    task=<span\
          \ class=\"hljs-string\">\"automatic-speech-recognition\"</span>,\n    model=<span\
          \ class=\"hljs-string\">\"openai/whisper-large-v2\"</span>)\n\n<span class=\"\
          hljs-comment\"># Set the recording parameters</span>\nfs = <span class=\"\
          hljs-number\">44100</span>  <span class=\"hljs-comment\"># Sample rate</span>\n\
          duration = <span class=\"hljs-number\">60</span>  <span class=\"hljs-comment\"\
          ># Recording duration in seconds</span>\nchannels = <span class=\"hljs-number\"\
          >2</span>  <span class=\"hljs-comment\"># Number of channels</span>\n\n\
          <span class=\"hljs-comment\"># Record the audio</span>\naudio = sd.rec(<span\
          \ class=\"hljs-built_in\">int</span>(fs * duration), samplerate=fs, channels=channels)\n\
          <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\
          Recording...\"</span>, flush=<span class=\"hljs-literal\">True</span>)\n\
          sd.wait()  <span class=\"hljs-comment\"># Wait until recording is finished</span>\n\
          <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\
          Recording stopped.\"</span>, flush=<span class=\"hljs-literal\">True</span>)\n\
          \nout = pipe({<span class=\"hljs-string\">\"raw\"</span>: audio, <span class=\"\
          hljs-string\">\"sampling_rate\"</span>: fs)[<span class=\"hljs-string\"\
          >\"text\"</span>]\n<span class=\"hljs-built_in\">print</span>(out)\n</code></pre>\n\
          <ol start=\"2\">\n<li>Unfortunately the Whisper model does not work with\
          \ streaming inference - we have to pass an entire audio sequence to the\
          \ model in order to have it transcribe. Perhaps what you could do is use\
          \ a voice activity detector model to detect when someone has started and\
          \ stopped speaking (silence). When someone starts speaking, you start recording\
          \ the audio. When they stop speaking, you stop the recording and pass whatever\
          \ audio you have to the Whisper model and have it transcribe. Meanwhile,\
          \ you continue listening for audio and start recording the audio as soon\
          \ as the voice activity detection model picks up new speech. Repeat this\
          \ for 'semi-live' inference</li>\n</ol>\n<p>You can try this model to start:\
          \ <a href=\"https://huggingface.co/pyannote/voice-activity-detection\">https://huggingface.co/pyannote/voice-activity-detection</a></p>\n"
        raw: "Hey @roshanjames!\n\n1. It is indeed possible to pass a numpy array\
          \ as an input to the `pipeline` method (see https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline.__call__.inputs).\
          \ Just be sure to also pass the sampling rate such that the recorded audio\
          \ sample is resampled to match the sampling rate expected by Whisper (16kHz)\
          \ and that your numpy array is a 1-dimensional array (you need to double\
          \ check this if you're recording with two-channels -> does `sounddevice`\
          \ return a 1-dimensional array in this case or is it 2-d?):\n\n```python\n\
          from transformers import pipeline\nimport sounddevice as sd\nimport scipy.io.wavfile\
          \ as wav\n\npipe = pipeline(\n    task=\"automatic-speech-recognition\"\
          ,\n    model=\"openai/whisper-large-v2\")\n\n# Set the recording parameters\n\
          fs = 44100  # Sample rate\nduration = 60  # Recording duration in seconds\n\
          channels = 2  # Number of channels\n\n# Record the audio\naudio = sd.rec(int(fs\
          \ * duration), samplerate=fs, channels=channels)\nprint(\"Recording...\"\
          , flush=True)\nsd.wait()  # Wait until recording is finished\nprint(\"Recording\
          \ stopped.\", flush=True)\n\nout = pipe({\"raw\": audio, \"sampling_rate\"\
          : fs)[\"text\"]\nprint(out)\n```\n\n2. Unfortunately the Whisper model does\
          \ not work with streaming inference - we have to pass an entire audio sequence\
          \ to the model in order to have it transcribe. Perhaps what you could do\
          \ is use a voice activity detector model to detect when someone has started\
          \ and stopped speaking (silence). When someone starts speaking, you start\
          \ recording the audio. When they stop speaking, you stop the recording and\
          \ pass whatever audio you have to the Whisper model and have it transcribe.\
          \ Meanwhile, you continue listening for audio and start recording the audio\
          \ as soon as the voice activity detection model picks up new speech. Repeat\
          \ this for 'semi-live' inference\n\nYou can try this model to start: https://huggingface.co/pyannote/voice-activity-detection"
        updatedAt: '2023-01-19T15:34:21.843Z'
      numEdits: 2
      reactions: []
    id: 63c962938afd58b4409d7a48
    type: comment
  author: sanchit-gandhi
  content: "Hey @roshanjames!\n\n1. It is indeed possible to pass a numpy array as\
    \ an input to the `pipeline` method (see https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline.__call__.inputs).\
    \ Just be sure to also pass the sampling rate such that the recorded audio sample\
    \ is resampled to match the sampling rate expected by Whisper (16kHz) and that\
    \ your numpy array is a 1-dimensional array (you need to double check this if\
    \ you're recording with two-channels -> does `sounddevice` return a 1-dimensional\
    \ array in this case or is it 2-d?):\n\n```python\nfrom transformers import pipeline\n\
    import sounddevice as sd\nimport scipy.io.wavfile as wav\n\npipe = pipeline(\n\
    \    task=\"automatic-speech-recognition\",\n    model=\"openai/whisper-large-v2\"\
    )\n\n# Set the recording parameters\nfs = 44100  # Sample rate\nduration = 60\
    \  # Recording duration in seconds\nchannels = 2  # Number of channels\n\n# Record\
    \ the audio\naudio = sd.rec(int(fs * duration), samplerate=fs, channels=channels)\n\
    print(\"Recording...\", flush=True)\nsd.wait()  # Wait until recording is finished\n\
    print(\"Recording stopped.\", flush=True)\n\nout = pipe({\"raw\": audio, \"sampling_rate\"\
    : fs)[\"text\"]\nprint(out)\n```\n\n2. Unfortunately the Whisper model does not\
    \ work with streaming inference - we have to pass an entire audio sequence to\
    \ the model in order to have it transcribe. Perhaps what you could do is use a\
    \ voice activity detector model to detect when someone has started and stopped\
    \ speaking (silence). When someone starts speaking, you start recording the audio.\
    \ When they stop speaking, you stop the recording and pass whatever audio you\
    \ have to the Whisper model and have it transcribe. Meanwhile, you continue listening\
    \ for audio and start recording the audio as soon as the voice activity detection\
    \ model picks up new speech. Repeat this for 'semi-live' inference\n\nYou can\
    \ try this model to start: https://huggingface.co/pyannote/voice-activity-detection"
  created_at: 2023-01-19 15:32:35+00:00
  edited: true
  hidden: false
  id: 63c962938afd58b4409d7a48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9832b403929cdb918b0da141e0c38fb9.svg
      fullname: Roshan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: roshanjames
      type: user
    createdAt: '2023-01-23T17:34:45.000Z'
    data:
      edited: false
      editors:
      - roshanjames
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9832b403929cdb918b0da141e0c38fb9.svg
          fullname: Roshan
          isHf: false
          isPro: false
          name: roshanjames
          type: user
        html: "<p>Thank you very much Sanchit! </p>\n<p>One followup question (and\
          \ I'm happy to file this under a different title if you think I should):\
          \ I can't figure out how to specify max_new_tokens to satisfy this warning.\
          \ How do I get rid of this?</p>\n<pre><code>.../hugging-face/venv/lib/python3.9/site-packages/transformers/generation/utils.py:1387:\
          \ UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length`\
          \ will default to 448 (`self.config.max_length`). Controlling `max_length`\
          \ via the config is deprecated and `max_length` will be removed from the\
          \ config in v5 of Transformers -- we recommend using `max_new_tokens` to\
          \ control the maximum length of the generation.\n  warnings.warn(\n</code></pre>\n\
          <p>I ended up using a VAD, but a different one from the one you suggested\
          \ above. My code is below just in case it is helpful to anyone else thinking\
          \ about this. The code below starts 2 threads, one which listens to audio\
          \ and puts the bytes into a queue. The other thread calls the VAD, find\
          \ a gap in the voice stream and then sends that to whisper. On my machine,\
          \ the vad+whisper calls take about 3 secs on cpu, so I chose to record in\
          \ 10s chunks. </p>\n<pre><code>from transformers import pipeline\nfrom queue\
          \ import Queue\nimport time\nimport threading\nimport sounddevice as sd\n\
          import scipy.io.wavfile as wav\nimport numpy as np\nimport torch\nimport\
          \ datetime\n\n# Create a queue with a maxsize of 5\nq = Queue(maxsize=2)\n\
          \n# Set the recording parameters\nfs = 16000     # Sample rate\nduration\
          \ = 10  # Recording duration in seconds\nchannels = 1   # Number of channels\n\
          \nmodel, utils = torch.hub.load(repo_or_dir=\"snakers4/silero-vad\",\n \
          \                             model=\"silero_vad\")\nget_speech_timestamps\
          \ = utils[0]\n\nwhisper = pipeline(task=\"automatic-speech-recognition\"\
          ,\n                   model=\"openai/whisper-medium.en\")\n\ndef transcribe(remaining,\
          \ current):\n    audio = np.concatenate((remaining, current))\n    segments\
          \ = get_speech_timestamps(audio, model, sampling_rate=fs, threshold=0.9)\n\
          \    # print(segments, flush=True)\n    end = -1\n    # we want to split\
          \ at the 'end' timestamp of teh second last snippet.\n    if len(segments)\
          \ &gt; 1:\n        end = segments[-2][\"end\"]\n    elif len(segments) ==\
          \ 1:\n        end = segments[-1][\"end\"]\n    else:\n        end = -1\n\
          \    current = audio[:end]\n    remaining = audio[end:]\n    text = whisper({\"\
          raw\": current, \"sampling_rate\": fs})[\"text\"]\n    return text, remaining\n\
          \ndef time_to_string(t):\n    dt = datetime.datetime.fromtimestamp(t)\n\
          \    return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\ndef recorder():\n    t1\
          \ = time.time()\n    t2 = t1\n    while True:\n        # Record the audio\n\
          \        audio = sd.rec(int(fs * duration), samplerate=fs, channels=channels)\n\
          \        t0 = time.time()\n        print(f\"{time_to_string(t0)}: Recording...\
          \ (last_duration:{t2-t1:.2f}s, last_gap:{(t0-t2)*1000.:.2f}ms)\", flush=True)\n\
          \        t1 = t0\n        sd.wait()  # Wait until recording is finished\n\
          \        t2 = time.time()\n        q.put(audio.flatten())\n\ndef transcriber():\n\
          \    remaining = np.array([])\n    while True:\n        current = q.get()\n\
          \        t1 = time.time()\n        text, remaining = transcribe(remaining,\
          \ current)\n        t2 = time.time()\n        print(f\"|| {t2-t1:.3f}secs:\
          \ {text}\", flush=True)\n\nt1 = threading.Thread(target=recorder)\nt2 =\
          \ threading.Thread(target=transcriber)\n\nt1.start()\nt2.start()\n\nt1.join()\n\
          t2.join()\n</code></pre>\n"
        raw: "Thank you very much Sanchit! \n\nOne followup question (and I'm happy\
          \ to file this under a different title if you think I should): I can't figure\
          \ out how to specify max_new_tokens to satisfy this warning. How do I get\
          \ rid of this?\n```\n.../hugging-face/venv/lib/python3.9/site-packages/transformers/generation/utils.py:1387:\
          \ UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length`\
          \ will default to 448 (`self.config.max_length`). Controlling `max_length`\
          \ via the config is deprecated and `max_length` will be removed from the\
          \ config in v5 of Transformers -- we recommend using `max_new_tokens` to\
          \ control the maximum length of the generation.\n  warnings.warn(\n```\n\
          \nI ended up using a VAD, but a different one from the one you suggested\
          \ above. My code is below just in case it is helpful to anyone else thinking\
          \ about this. The code below starts 2 threads, one which listens to audio\
          \ and puts the bytes into a queue. The other thread calls the VAD, find\
          \ a gap in the voice stream and then sends that to whisper. On my machine,\
          \ the vad+whisper calls take about 3 secs on cpu, so I chose to record in\
          \ 10s chunks. \n\n```\nfrom transformers import pipeline\nfrom queue import\
          \ Queue\nimport time\nimport threading\nimport sounddevice as sd\nimport\
          \ scipy.io.wavfile as wav\nimport numpy as np\nimport torch\nimport datetime\n\
          \n# Create a queue with a maxsize of 5\nq = Queue(maxsize=2)\n\n# Set the\
          \ recording parameters\nfs = 16000     # Sample rate\nduration = 10  # Recording\
          \ duration in seconds\nchannels = 1   # Number of channels\n\nmodel, utils\
          \ = torch.hub.load(repo_or_dir=\"snakers4/silero-vad\",\n              \
          \                model=\"silero_vad\")\nget_speech_timestamps = utils[0]\n\
          \nwhisper = pipeline(task=\"automatic-speech-recognition\",\n          \
          \         model=\"openai/whisper-medium.en\")\n\ndef transcribe(remaining,\
          \ current):\n    audio = np.concatenate((remaining, current))\n    segments\
          \ = get_speech_timestamps(audio, model, sampling_rate=fs, threshold=0.9)\n\
          \    # print(segments, flush=True)\n    end = -1\n    # we want to split\
          \ at the 'end' timestamp of teh second last snippet.\n    if len(segments)\
          \ > 1:\n        end = segments[-2][\"end\"]\n    elif len(segments) == 1:\n\
          \        end = segments[-1][\"end\"]\n    else:\n        end = -1\n    current\
          \ = audio[:end]\n    remaining = audio[end:]\n    text = whisper({\"raw\"\
          : current, \"sampling_rate\": fs})[\"text\"]\n    return text, remaining\n\
          \ndef time_to_string(t):\n    dt = datetime.datetime.fromtimestamp(t)\n\
          \    return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\ndef recorder():\n    t1\
          \ = time.time()\n    t2 = t1\n    while True:\n        # Record the audio\n\
          \        audio = sd.rec(int(fs * duration), samplerate=fs, channels=channels)\n\
          \        t0 = time.time()\n        print(f\"{time_to_string(t0)}: Recording...\
          \ (last_duration:{t2-t1:.2f}s, last_gap:{(t0-t2)*1000.:.2f}ms)\", flush=True)\n\
          \        t1 = t0\n        sd.wait()  # Wait until recording is finished\n\
          \        t2 = time.time()\n        q.put(audio.flatten())\n\ndef transcriber():\n\
          \    remaining = np.array([])\n    while True:\n        current = q.get()\n\
          \        t1 = time.time()\n        text, remaining = transcribe(remaining,\
          \ current)\n        t2 = time.time()\n        print(f\"|| {t2-t1:.3f}secs:\
          \ {text}\", flush=True)\n\nt1 = threading.Thread(target=recorder)\nt2 =\
          \ threading.Thread(target=transcriber)\n\nt1.start()\nt2.start()\n\nt1.join()\n\
          t2.join()\n\n```"
        updatedAt: '2023-01-23T17:34:45.284Z'
      numEdits: 0
      reactions: []
    id: 63cec53584640987fa0bf894
    type: comment
  author: roshanjames
  content: "Thank you very much Sanchit! \n\nOne followup question (and I'm happy\
    \ to file this under a different title if you think I should): I can't figure\
    \ out how to specify max_new_tokens to satisfy this warning. How do I get rid\
    \ of this?\n```\n.../hugging-face/venv/lib/python3.9/site-packages/transformers/generation/utils.py:1387:\
    \ UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length`\
    \ will default to 448 (`self.config.max_length`). Controlling `max_length` via\
    \ the config is deprecated and `max_length` will be removed from the config in\
    \ v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum\
    \ length of the generation.\n  warnings.warn(\n```\n\nI ended up using a VAD,\
    \ but a different one from the one you suggested above. My code is below just\
    \ in case it is helpful to anyone else thinking about this. The code below starts\
    \ 2 threads, one which listens to audio and puts the bytes into a queue. The other\
    \ thread calls the VAD, find a gap in the voice stream and then sends that to\
    \ whisper. On my machine, the vad+whisper calls take about 3 secs on cpu, so I\
    \ chose to record in 10s chunks. \n\n```\nfrom transformers import pipeline\n\
    from queue import Queue\nimport time\nimport threading\nimport sounddevice as\
    \ sd\nimport scipy.io.wavfile as wav\nimport numpy as np\nimport torch\nimport\
    \ datetime\n\n# Create a queue with a maxsize of 5\nq = Queue(maxsize=2)\n\n#\
    \ Set the recording parameters\nfs = 16000     # Sample rate\nduration = 10  #\
    \ Recording duration in seconds\nchannels = 1   # Number of channels\n\nmodel,\
    \ utils = torch.hub.load(repo_or_dir=\"snakers4/silero-vad\",\n              \
    \                model=\"silero_vad\")\nget_speech_timestamps = utils[0]\n\nwhisper\
    \ = pipeline(task=\"automatic-speech-recognition\",\n                   model=\"\
    openai/whisper-medium.en\")\n\ndef transcribe(remaining, current):\n    audio\
    \ = np.concatenate((remaining, current))\n    segments = get_speech_timestamps(audio,\
    \ model, sampling_rate=fs, threshold=0.9)\n    # print(segments, flush=True)\n\
    \    end = -1\n    # we want to split at the 'end' timestamp of teh second last\
    \ snippet.\n    if len(segments) > 1:\n        end = segments[-2][\"end\"]\n \
    \   elif len(segments) == 1:\n        end = segments[-1][\"end\"]\n    else:\n\
    \        end = -1\n    current = audio[:end]\n    remaining = audio[end:]\n  \
    \  text = whisper({\"raw\": current, \"sampling_rate\": fs})[\"text\"]\n    return\
    \ text, remaining\n\ndef time_to_string(t):\n    dt = datetime.datetime.fromtimestamp(t)\n\
    \    return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\ndef recorder():\n    t1 = time.time()\n\
    \    t2 = t1\n    while True:\n        # Record the audio\n        audio = sd.rec(int(fs\
    \ * duration), samplerate=fs, channels=channels)\n        t0 = time.time()\n \
    \       print(f\"{time_to_string(t0)}: Recording... (last_duration:{t2-t1:.2f}s,\
    \ last_gap:{(t0-t2)*1000.:.2f}ms)\", flush=True)\n        t1 = t0\n        sd.wait()\
    \  # Wait until recording is finished\n        t2 = time.time()\n        q.put(audio.flatten())\n\
    \ndef transcriber():\n    remaining = np.array([])\n    while True:\n        current\
    \ = q.get()\n        t1 = time.time()\n        text, remaining = transcribe(remaining,\
    \ current)\n        t2 = time.time()\n        print(f\"|| {t2-t1:.3f}secs: {text}\"\
    , flush=True)\n\nt1 = threading.Thread(target=recorder)\nt2 = threading.Thread(target=transcriber)\n\
    \nt1.start()\nt2.start()\n\nt1.join()\nt2.join()\n\n```"
  created_at: 2023-01-23 17:34:45+00:00
  edited: false
  hidden: false
  id: 63cec53584640987fa0bf894
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-01-26T17:03:56.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;roshanjames&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/roshanjames\"\
          >@<span class=\"underline\">roshanjames</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>Very cool code snippet using VAD + Whisper! </p>\n<p>In terms of setting\
          \ the max length, you can pass <code>max_new_tokens</code> when you call\
          \ the pipeline:</p>\n<pre><code class=\"language-python\">text = whisper({<span\
          \ class=\"hljs-string\">\"raw\"</span>: current, <span class=\"hljs-string\"\
          >\"sampling_rate\"</span>: fs}, max_new_tokens=<span class=\"hljs-number\"\
          >448</span>)[<span class=\"hljs-string\">\"text\"</span>]\n</code></pre>\n\
          <p>Although it's not a problem if you don't specify. The warning simply\
          \ says that the model is generating until a pre-defined length (448) that\
          \ you yourself haven't specified.</p>\n"
        raw: "Hey @roshanjames,\n\nVery cool code snippet using VAD + Whisper! \n\n\
          In terms of setting the max length, you can pass `max_new_tokens` when you\
          \ call the pipeline:\n```python\ntext = whisper({\"raw\": current, \"sampling_rate\"\
          : fs}, max_new_tokens=448)[\"text\"]\n```\n\nAlthough it's not a problem\
          \ if you don't specify. The warning simply says that the model is generating\
          \ until a pre-defined length (448) that you yourself haven't specified."
        updatedAt: '2023-01-26T17:06:03.896Z'
      numEdits: 4
      reactions: []
    id: 63d2b27cbc3d31862321390d
    type: comment
  author: sanchit-gandhi
  content: "Hey @roshanjames,\n\nVery cool code snippet using VAD + Whisper! \n\n\
    In terms of setting the max length, you can pass `max_new_tokens` when you call\
    \ the pipeline:\n```python\ntext = whisper({\"raw\": current, \"sampling_rate\"\
    : fs}, max_new_tokens=448)[\"text\"]\n```\n\nAlthough it's not a problem if you\
    \ don't specify. The warning simply says that the model is generating until a\
    \ pre-defined length (448) that you yourself haven't specified."
  created_at: 2023-01-26 17:03:56+00:00
  edited: true
  hidden: false
  id: 63d2b27cbc3d31862321390d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: openai/whisper-large-v2
repo_type: model
status: open
target_branch: null
title: Is there a way to stream audio into to this model and have it generate a stream
  of text?
