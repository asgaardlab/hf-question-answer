!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ericchen
conflicting_files: null
created_at: 2023-10-08 07:39:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12227053e027ee4d2f6627c6926a893c.svg
      fullname: chenjin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ericchen
      type: user
    createdAt: '2023-10-08T08:39:23.000Z'
    data:
      edited: true
      editors:
      - ericchen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7442866563796997
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12227053e027ee4d2f6627c6926a893c.svg
          fullname: chenjin
          isHf: false
          isPro: false
          name: ericchen
          type: user
        html: '<p>First the length of my test wave file is about 2 min, then i choose
          pipeline to do recognition.<br>Second i finetune the whisper model with
          my own speech data without punctuations, then the output of my whisper has
          no punctuation.<br>Then i use the pipeline method to run speech recognition,
          but i found between two sequential chunks, their text are concatenating
          without any blanks between them, how should i solve this problem?</p>

          <p>examples:<br>between chunks, output recognition<br>" .... iknow ..."<br>it
          should be " .... i know ..."</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/637c2870a8716d64204ed485/V9p6kCbxQnbQU8Qn3RQ8x.png"><img
          alt="1a91d7e4952bda1d5a6da8131bd2d98.png" src="https://cdn-uploads.huggingface.co/production/uploads/637c2870a8716d64204ed485/V9p6kCbxQnbQU8Qn3RQ8x.png"></a></p>

          <p>Are there any suggestions? Many thanks!!!</p>

          <p>The code i use:</p>

          <pre><code class="language-python"><span class="hljs-meta">&gt;&gt;&gt;
          </span><span class="hljs-keyword">import</span> torch

          <span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> pipeline

          <span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span>
          datasets <span class="hljs-keyword">import</span> load_dataset


          <span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">"cuda:0"</span>
          <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span>
          <span class="hljs-string">"cpu"</span>


          <span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(

          <span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">"automatic-speech-recognition"</span>,

          <span class="hljs-meta">&gt;&gt;&gt; </span>  model=<span class="hljs-string">"openai/whisper-large-v2"</span>,

          <span class="hljs-meta">&gt;&gt;&gt; </span>  chunk_length_s=<span class="hljs-number">30</span>,

          <span class="hljs-meta">&gt;&gt;&gt; </span>  device=device,

          <span class="hljs-meta">&gt;&gt;&gt; </span>)


          <span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">"hf-internal-testing/librispeech_asr_dummy"</span>,
          <span class="hljs-string">"clean"</span>, split=<span class="hljs-string">"validation"</span>)

          <span class="hljs-meta">&gt;&gt;&gt; </span>sample = ds[<span class="hljs-number">0</span>][<span
          class="hljs-string">"audio"</span>]


          <span class="hljs-meta">&gt;&gt;&gt; </span>prediction = pipe(sample.copy(),
          batch_size=<span class="hljs-number">8</span>)[<span class="hljs-string">"text"</span>]

          </code></pre>

          '
        raw: 'First the length of my test wave file is about 2 min, then i choose
          pipeline to do recognition.

          Second i finetune the whisper model with my own speech data without punctuations,
          then the output of my whisper has no punctuation.

          Then i use the pipeline method to run speech recognition, but i found between
          two sequential chunks, their text are concatenating without any blanks between
          them, how should i solve this problem?


          examples:

          between chunks, output recognition

          " .... iknow ..."

          it should be " .... i know ..."



          ![1a91d7e4952bda1d5a6da8131bd2d98.png](https://cdn-uploads.huggingface.co/production/uploads/637c2870a8716d64204ed485/V9p6kCbxQnbQU8Qn3RQ8x.png)



          Are there any suggestions? Many thanks!!!


          The code i use:


          ```python

          >>> import torch

          >>> from transformers import pipeline

          >>> from datasets import load_dataset


          >>> device = "cuda:0" if torch.cuda.is_available() else "cpu"


          >>> pipe = pipeline(

          >>>   "automatic-speech-recognition",

          >>>   model="openai/whisper-large-v2",

          >>>   chunk_length_s=30,

          >>>   device=device,

          >>> )


          >>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean",
          split="validation")

          >>> sample = ds[0]["audio"]


          >>> prediction = pipe(sample.copy(), batch_size=8)["text"]

          ```

          '
        updatedAt: '2023-10-08T08:58:34.617Z'
      numEdits: 3
      reactions: []
    id: 65226abbc709aaca9ab03103
    type: comment
  author: ericchen
  content: 'First the length of my test wave file is about 2 min, then i choose pipeline
    to do recognition.

    Second i finetune the whisper model with my own speech data without punctuations,
    then the output of my whisper has no punctuation.

    Then i use the pipeline method to run speech recognition, but i found between
    two sequential chunks, their text are concatenating without any blanks between
    them, how should i solve this problem?


    examples:

    between chunks, output recognition

    " .... iknow ..."

    it should be " .... i know ..."



    ![1a91d7e4952bda1d5a6da8131bd2d98.png](https://cdn-uploads.huggingface.co/production/uploads/637c2870a8716d64204ed485/V9p6kCbxQnbQU8Qn3RQ8x.png)



    Are there any suggestions? Many thanks!!!


    The code i use:


    ```python

    >>> import torch

    >>> from transformers import pipeline

    >>> from datasets import load_dataset


    >>> device = "cuda:0" if torch.cuda.is_available() else "cpu"


    >>> pipe = pipeline(

    >>>   "automatic-speech-recognition",

    >>>   model="openai/whisper-large-v2",

    >>>   chunk_length_s=30,

    >>>   device=device,

    >>> )


    >>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

    >>> sample = ds[0]["audio"]


    >>> prediction = pipe(sample.copy(), batch_size=8)["text"]

    ```

    '
  created_at: 2023-10-08 07:39:23+00:00
  edited: true
  hidden: false
  id: 65226abbc709aaca9ab03103
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-10-10T16:45:42.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9033649563789368
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;ericchen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ericchen\"\
          >@<span class=\"underline\">ericchen</span></a></span>\n\n\t</span></span>\
          \ - what I would recommend is sweeping over different values of <code>chunk_length_s</code>\
          \ in the interval <code>[10, 30]</code> (i.e. try 10, 15, ..., 30). You\
          \ need to set the <code>chunk_length_s</code> to match the distribution\
          \ of audio data your model was trained on. If you trained on audio segments\
          \ typically shorter than 30s, then your chunk length should be reduced.\
          \ Sweeping over different chunk lengths is the easiest way of figuring out\
          \ what chunk length is appropriate. </p>\n"
        raw: 'Hey @ericchen - what I would recommend is sweeping over different values
          of `chunk_length_s` in the interval `[10, 30]` (i.e. try 10, 15, ..., 30).
          You need to set the `chunk_length_s` to match the distribution of audio
          data your model was trained on. If you trained on audio segments typically
          shorter than 30s, then your chunk length should be reduced. Sweeping over
          different chunk lengths is the easiest way of figuring out what chunk length
          is appropriate. '
        updatedAt: '2023-10-10T16:45:42.262Z'
      numEdits: 0
      reactions: []
    id: 65257fb68a21d8f14abc1dde
    type: comment
  author: sanchit-gandhi
  content: 'Hey @ericchen - what I would recommend is sweeping over different values
    of `chunk_length_s` in the interval `[10, 30]` (i.e. try 10, 15, ..., 30). You
    need to set the `chunk_length_s` to match the distribution of audio data your
    model was trained on. If you trained on audio segments typically shorter than
    30s, then your chunk length should be reduced. Sweeping over different chunk lengths
    is the easiest way of figuring out what chunk length is appropriate. '
  created_at: 2023-10-10 15:45:42+00:00
  edited: false
  hidden: false
  id: 65257fb68a21d8f14abc1dde
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12227053e027ee4d2f6627c6926a893c.svg
      fullname: chenjin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ericchen
      type: user
    createdAt: '2023-10-12T05:11:50.000Z'
    data:
      edited: false
      editors:
      - ericchen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9338239431381226
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12227053e027ee4d2f6627c6926a893c.svg
          fullname: chenjin
          isHf: false
          isPro: false
          name: ericchen
          type: user
        html: "<blockquote>\n<p>Hey <span data-props=\"{&quot;user&quot;:&quot;ericchen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ericchen\"\
          >@<span class=\"underline\">ericchen</span></a></span>\n\n\t</span></span>\
          \ - what I would recommend is sweeping over different values of <code>chunk_length_s</code>\
          \ in the interval <code>[10, 30]</code> (i.e. try 10, 15, ..., 30). You\
          \ need to set the <code>chunk_length_s</code> to match the distribution\
          \ of audio data your model was trained on. If you trained on audio segments\
          \ typically shorter than 30s, then your chunk length should be reduced.\
          \ Sweeping over different chunk lengths is the easiest way of figuring out\
          \ what chunk length is appropriate.</p>\n</blockquote>\n<p>Thanks for your\
          \ reply. I tried '10\\15\\20\\25\\30', the concatenated words also exist.<br>I\
          \ thought it was a format error between chunks: \"for the next chunk , you\
          \ forget to put a blank at the begining of the chunk.\"<br>Then after seen\
          \ your reply, you mean it is not?</p>\n"
        raw: '> Hey @ericchen - what I would recommend is sweeping over different
          values of `chunk_length_s` in the interval `[10, 30]` (i.e. try 10, 15,
          ..., 30). You need to set the `chunk_length_s` to match the distribution
          of audio data your model was trained on. If you trained on audio segments
          typically shorter than 30s, then your chunk length should be reduced. Sweeping
          over different chunk lengths is the easiest way of figuring out what chunk
          length is appropriate.


          Thanks for your reply. I tried ''10\15\20\25\30'', the concatenated words
          also exist.

          I thought it was a format error between chunks: "for the next chunk , you
          forget to put a blank at the begining of the chunk."

          Then after seen your reply, you mean it is not?'
        updatedAt: '2023-10-12T05:11:50.910Z'
      numEdits: 0
      reactions: []
    id: 652780165fc08c60dc69a944
    type: comment
  author: ericchen
  content: '> Hey @ericchen - what I would recommend is sweeping over different values
    of `chunk_length_s` in the interval `[10, 30]` (i.e. try 10, 15, ..., 30). You
    need to set the `chunk_length_s` to match the distribution of audio data your
    model was trained on. If you trained on audio segments typically shorter than
    30s, then your chunk length should be reduced. Sweeping over different chunk lengths
    is the easiest way of figuring out what chunk length is appropriate.


    Thanks for your reply. I tried ''10\15\20\25\30'', the concatenated words also
    exist.

    I thought it was a format error between chunks: "for the next chunk , you forget
    to put a blank at the begining of the chunk."

    Then after seen your reply, you mean it is not?'
  created_at: 2023-10-12 04:11:50+00:00
  edited: false
  hidden: false
  id: 652780165fc08c60dc69a944
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 77
repo_id: openai/whisper-large-v2
repo_type: model
status: open
target_branch: null
title: How can i seperate output words between chunks?
