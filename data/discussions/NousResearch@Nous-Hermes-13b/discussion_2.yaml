!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nacs
conflicting_files: null
created_at: 2023-06-04 20:49:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
      fullname: nacs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nacs
      type: user
    createdAt: '2023-06-04T21:49:40.000Z'
    data:
      edited: true
      editors:
      - nacs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9826967120170593
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
          fullname: nacs
          isHf: false
          isPro: false
          name: nacs
          type: user
        html: '<p>I''m really impressed at the quality of the answers and how good
          it is at instruction following.</p>

          <p>I''m finding it to be as good or better than Vicuna/Wizard Vicuna/Wizard-uncensored
          models in almost every case.</p>

          <p>For those of you haven''t tried it, do -- its worth it.</p>

          <p>Thanks for training/sharing this @NousResearch </p>

          <p>[ Also: any plans for 30B? ]</p>

          '
        raw: "I'm really impressed at the quality of the answers and how good it is\
          \ at instruction following.\n\nI'm finding it to be as good or better than\
          \ Vicuna/Wizard Vicuna/Wizard-uncensored models in almost every case.\n\n\
          For those of you haven't tried it, do -- its worth it.\n\nThanks for training/sharing\
          \ this @NousResearch \n\n[ Also: any plans for 30B? ]"
        updatedAt: '2023-06-04T21:52:10.466Z'
      numEdits: 2
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - boqsc
        - digitous
        - freefallr
        - Saeran
    id: 647d06f436e109abce3f26ad
    type: comment
  author: nacs
  content: "I'm really impressed at the quality of the answers and how good it is\
    \ at instruction following.\n\nI'm finding it to be as good or better than Vicuna/Wizard\
    \ Vicuna/Wizard-uncensored models in almost every case.\n\nFor those of you haven't\
    \ tried it, do -- its worth it.\n\nThanks for training/sharing this @NousResearch\
    \ \n\n[ Also: any plans for 30B? ]"
  created_at: 2023-06-04 20:49:40+00:00
  edited: true
  hidden: false
  id: 647d06f436e109abce3f26ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/807fb7f36a4217b1e2ec348417eee923.svg
      fullname: Marius
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: se4sons
      type: user
    createdAt: '2023-06-05T01:01:49.000Z'
    data:
      edited: false
      editors:
      - se4sons
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9551799297332764
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/807fb7f36a4217b1e2ec348417eee923.svg
          fullname: Marius
          isHf: false
          isPro: false
          name: se4sons
          type: user
        html: '<p>would be great to see if 30B improves the results</p>

          '
        raw: would be great to see if 30B improves the results
        updatedAt: '2023-06-05T01:01:49.069Z'
      numEdits: 0
      reactions: []
    id: 647d33fd60dfe0f35d65e8f4
    type: comment
  author: se4sons
  content: would be great to see if 30B improves the results
  created_at: 2023-06-05 00:01:49+00:00
  edited: false
  hidden: false
  id: 647d33fd60dfe0f35d65e8f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3810fa0e968eab6be8917e5b819cb30.svg
      fullname: Stanislav Lyakhotskyy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: walking-octupus
      type: user
    createdAt: '2023-06-05T12:50:36.000Z'
    data:
      edited: true
      editors:
      - walking-octupus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9568450450897217
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3810fa0e968eab6be8917e5b819cb30.svg
          fullname: Stanislav Lyakhotskyy
          isHf: false
          isPro: false
          name: walking-octupus
          type: user
        html: '<p>I''m more interested to see someone train a 7B model on this dataset,
          since it is most accessible on consumer hardware and can be compared to
          original WizardLM.<br>Then, given we''re approaching OpenAI''s quality,
          I think running OpenAI''s Eval on these models may too be useful, since
          I think it evaluates reasoning skills of these models a little better than
          other metrics.<br>I''d be curious to test the model myself on factual QA,
          document analysis/classification, and reasoning.</p>

          '
        raw: 'I''m more interested to see someone train a 7B model on this dataset,
          since it is most accessible on consumer hardware and can be compared to
          original WizardLM.

          Then, given we''re approaching OpenAI''s quality, I think running OpenAI''s
          Eval on these models may too be useful, since I think it evaluates reasoning
          skills of these models a little better than other metrics.

          I''d be curious to test the model myself on factual QA, document analysis/classification,
          and reasoning.'
        updatedAt: '2023-06-05T12:51:15.561Z'
      numEdits: 1
      reactions: []
    id: 647dda1cf14eafc3b44f23cd
    type: comment
  author: walking-octupus
  content: 'I''m more interested to see someone train a 7B model on this dataset,
    since it is most accessible on consumer hardware and can be compared to original
    WizardLM.

    Then, given we''re approaching OpenAI''s quality, I think running OpenAI''s Eval
    on these models may too be useful, since I think it evaluates reasoning skills
    of these models a little better than other metrics.

    I''d be curious to test the model myself on factual QA, document analysis/classification,
    and reasoning.'
  created_at: 2023-06-05 11:50:36+00:00
  edited: true
  hidden: false
  id: 647dda1cf14eafc3b44f23cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fc06a221c444a56f7cc595/Ln3m7sK52hwGuMnPe4mfU.jpeg?w=200&h=200&f=face
      fullname: karan4d
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: karan4d
      type: user
    createdAt: '2023-06-05T13:05:06.000Z'
    data:
      edited: false
      editors:
      - karan4d
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9594830274581909
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fc06a221c444a56f7cc595/Ln3m7sK52hwGuMnPe4mfU.jpeg?w=200&h=200&f=face
          fullname: karan4d
          isHf: false
          isPro: false
          name: karan4d
          type: user
        html: '<blockquote>

          <p>I''m really impressed at the quality of the answers and how good it is
          at instruction following.</p>

          <p>I''m finding it to be as good or better than Vicuna/Wizard Vicuna/Wizard-uncensored
          models in almost every case.</p>

          <p>For those of you haven''t tried it, do -- its worth it.</p>

          <p>Thanks for training/sharing this @NousResearch </p>

          <p>[ Also: any plans for 30B? ]</p>

          </blockquote>

          <blockquote>

          <p>I''m more interested to see someone train a 7B model on this dataset,
          since it is most accessible on consumer hardware and can be compared to
          original WizardLM.<br>Then, given we''re approaching OpenAI''s quality,
          I think running OpenAI''s Eval on these models may too be useful, since
          I think it evaluates reasoning skills of these models a little better than
          other metrics.<br>I''d be curious to test the model myself on factual QA,
          document analysis/classification, and reasoning.</p>

          </blockquote>

          <p>7b and 40b are planned. might do 7b on MPT or falcon or something, not
          too sure yet.</p>

          '
        raw: "> I'm really impressed at the quality of the answers and how good it\
          \ is at instruction following.\n> \n> I'm finding it to be as good or better\
          \ than Vicuna/Wizard Vicuna/Wizard-uncensored models in almost every case.\n\
          > \n> For those of you haven't tried it, do -- its worth it.\n> \n> Thanks\
          \ for training/sharing this @NousResearch \n> \n> [ Also: any plans for\
          \ 30B? ]\n\n> I'm more interested to see someone train a 7B model on this\
          \ dataset, since it is most accessible on consumer hardware and can be compared\
          \ to original WizardLM.\n> Then, given we're approaching OpenAI's quality,\
          \ I think running OpenAI's Eval on these models may too be useful, since\
          \ I think it evaluates reasoning skills of these models a little better\
          \ than other metrics.\n> I'd be curious to test the model myself on factual\
          \ QA, document analysis/classification, and reasoning.\n\n7b and 40b are\
          \ planned. might do 7b on MPT or falcon or something, not too sure yet."
        updatedAt: '2023-06-05T13:05:06.074Z'
      numEdits: 0
      reactions: []
    id: 647ddd825214d172cbb796d4
    type: comment
  author: karan4d
  content: "> I'm really impressed at the quality of the answers and how good it is\
    \ at instruction following.\n> \n> I'm finding it to be as good or better than\
    \ Vicuna/Wizard Vicuna/Wizard-uncensored models in almost every case.\n> \n> For\
    \ those of you haven't tried it, do -- its worth it.\n> \n> Thanks for training/sharing\
    \ this @NousResearch \n> \n> [ Also: any plans for 30B? ]\n\n> I'm more interested\
    \ to see someone train a 7B model on this dataset, since it is most accessible\
    \ on consumer hardware and can be compared to original WizardLM.\n> Then, given\
    \ we're approaching OpenAI's quality, I think running OpenAI's Eval on these models\
    \ may too be useful, since I think it evaluates reasoning skills of these models\
    \ a little better than other metrics.\n> I'd be curious to test the model myself\
    \ on factual QA, document analysis/classification, and reasoning.\n\n7b and 40b\
    \ are planned. might do 7b on MPT or falcon or something, not too sure yet."
  created_at: 2023-06-05 12:05:06+00:00
  edited: false
  hidden: false
  id: 647ddd825214d172cbb796d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3810fa0e968eab6be8917e5b819cb30.svg
      fullname: Stanislav Lyakhotskyy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: walking-octupus
      type: user
    createdAt: '2023-06-05T15:35:46.000Z'
    data:
      edited: false
      editors:
      - walking-octupus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9582260847091675
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3810fa0e968eab6be8917e5b819cb30.svg
          fullname: Stanislav Lyakhotskyy
          isHf: false
          isPro: false
          name: walking-octupus
          type: user
        html: '<blockquote>

          <p>7b and 40b are planned. might do 7b on MPT or falcon or something, not
          too sure yet.</p>

          </blockquote>

          <p>I wonder how good MPT can get, since their default chat fine-tune was
          horribly incoherent on factual open QA, just barely associating terms with
          the question and generating some technobabble. (eg: "Did the French Revolution
          grant women suffrage?").</p>

          <p>I haven''t tried Falcon yet, still waiting for it to be implemented in
          <code>llama.cpp</code>. I hope it uses similar techniques to LLaMA, since
          even GPT-2 large doesn''t play well with my laptop, whereas 7B LLaMA GGML
          Q_4_1 works wonderfully.</p>

          <p>Also, this doesn''t really relate to this model in particular, but can
          one fine-tune a fine-tune of LLaMA? I''m currently building a closed QA
          notes app inspired by Google''s Tailwind with LLaMA, so I wonder if it''s
          possible to make WizardLM keep its overall knowledge while always answering
          in concise bullet-points. Going over each instruction in a dataset and asking
          GPT-3.5 to reformat it might get quite expensive quickly...</p>

          '
        raw: '> 7b and 40b are planned. might do 7b on MPT or falcon or something,
          not too sure yet.


          I wonder how good MPT can get, since their default chat fine-tune was horribly
          incoherent on factual open QA, just barely associating terms with the question
          and generating some technobabble. (eg: "Did the French Revolution grant
          women suffrage?").


          I haven''t tried Falcon yet, still waiting for it to be implemented in `llama.cpp`.
          I hope it uses similar techniques to LLaMA, since even GPT-2 large doesn''t
          play well with my laptop, whereas 7B LLaMA GGML Q_4_1 works wonderfully.


          Also, this doesn''t really relate to this model in particular, but can one
          fine-tune a fine-tune of LLaMA? I''m currently building a closed QA notes
          app inspired by Google''s Tailwind with LLaMA, so I wonder if it''s possible
          to make WizardLM keep its overall knowledge while always answering in concise
          bullet-points. Going over each instruction in a dataset and asking GPT-3.5
          to reformat it might get quite expensive quickly...'
        updatedAt: '2023-06-05T15:35:46.210Z'
      numEdits: 0
      reactions: []
    id: 647e00d210b7a3b15707f90e
    type: comment
  author: walking-octupus
  content: '> 7b and 40b are planned. might do 7b on MPT or falcon or something, not
    too sure yet.


    I wonder how good MPT can get, since their default chat fine-tune was horribly
    incoherent on factual open QA, just barely associating terms with the question
    and generating some technobabble. (eg: "Did the French Revolution grant women
    suffrage?").


    I haven''t tried Falcon yet, still waiting for it to be implemented in `llama.cpp`.
    I hope it uses similar techniques to LLaMA, since even GPT-2 large doesn''t play
    well with my laptop, whereas 7B LLaMA GGML Q_4_1 works wonderfully.


    Also, this doesn''t really relate to this model in particular, but can one fine-tune
    a fine-tune of LLaMA? I''m currently building a closed QA notes app inspired by
    Google''s Tailwind with LLaMA, so I wonder if it''s possible to make WizardLM
    keep its overall knowledge while always answering in concise bullet-points. Going
    over each instruction in a dataset and asking GPT-3.5 to reformat it might get
    quite expensive quickly...'
  created_at: 2023-06-05 14:35:46+00:00
  edited: false
  hidden: false
  id: 647e00d210b7a3b15707f90e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
      fullname: nacs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nacs
      type: user
    createdAt: '2023-06-05T15:38:33.000Z'
    data:
      edited: true
      editors:
      - nacs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9428388476371765
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
          fullname: nacs
          isHf: false
          isPro: false
          name: nacs
          type: user
        html: "<p>Problem with 40B models is that it won\u2019t fit in a 24GB VRAM\
          \ GPU so it excludes all consumer cards and people. </p>\n<p>30B/33B fit\
          \ in 24GB with enough context to be useful.</p>\n"
        raw: "Problem with 40B models is that it won\u2019t fit in a 24GB VRAM GPU\
          \ so it excludes all consumer cards and people. \n\n30B/33B fit in 24GB\
          \ with enough context to be useful."
        updatedAt: '2023-06-05T15:39:43.569Z'
      numEdits: 3
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - Limezero
        - digitous
        - codehappy
        - ozzeruk82
        - Takuonline
    id: 647e017932c471a7fa8e8182
    type: comment
  author: nacs
  content: "Problem with 40B models is that it won\u2019t fit in a 24GB VRAM GPU so\
    \ it excludes all consumer cards and people. \n\n30B/33B fit in 24GB with enough\
    \ context to be useful."
  created_at: 2023-06-05 14:38:33+00:00
  edited: true
  hidden: false
  id: 647e017932c471a7fa8e8182
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676448086084-62ae5fbe4ff605c0411397bb.jpeg?w=200&h=200&f=face
      fullname: Erik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: digitous
      type: user
    createdAt: '2023-06-05T17:43:51.000Z'
    data:
      edited: false
      editors:
      - digitous
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9399124979972839
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676448086084-62ae5fbe4ff605c0411397bb.jpeg?w=200&h=200&f=face
          fullname: Erik
          isHf: false
          isPro: false
          name: digitous
          type: user
        html: '<p>I agree with the above; 30bs can fit on more affordable consumer
          hardware, one RTX3090 etc after quantizing to 4bit. 40b would also be sweet
          at least when 3bit quantization reaches acceptable use.</p>

          <p>But either way, thank you for your hard work! I look forward to trying
          this out when I get the chance. Only heard good things.</p>

          '
        raw: 'I agree with the above; 30bs can fit on more affordable consumer hardware,
          one RTX3090 etc after quantizing to 4bit. 40b would also be sweet at least
          when 3bit quantization reaches acceptable use.


          But either way, thank you for your hard work! I look forward to trying this
          out when I get the chance. Only heard good things.'
        updatedAt: '2023-06-05T17:43:51.188Z'
      numEdits: 0
      reactions: []
    id: 647e1ed75214d172cbc08109
    type: comment
  author: digitous
  content: 'I agree with the above; 30bs can fit on more affordable consumer hardware,
    one RTX3090 etc after quantizing to 4bit. 40b would also be sweet at least when
    3bit quantization reaches acceptable use.


    But either way, thank you for your hard work! I look forward to trying this out
    when I get the chance. Only heard good things.'
  created_at: 2023-06-05 16:43:51+00:00
  edited: false
  hidden: false
  id: 647e1ed75214d172cbc08109
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fc06a221c444a56f7cc595/Ln3m7sK52hwGuMnPe4mfU.jpeg?w=200&h=200&f=face
      fullname: karan4d
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: karan4d
      type: user
    createdAt: '2023-06-05T18:33:31.000Z'
    data:
      edited: false
      editors:
      - karan4d
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9126173853874207
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fc06a221c444a56f7cc595/Ln3m7sK52hwGuMnPe4mfU.jpeg?w=200&h=200&f=face
          fullname: karan4d
          isHf: false
          isPro: false
          name: karan4d
          type: user
        html: '<p>good to know--will reconsider and happily take base model suggestions.</p>

          '
        raw: good to know--will reconsider and happily take base model suggestions.
        updatedAt: '2023-06-05T18:33:31.536Z'
      numEdits: 0
      reactions: []
    id: 647e2a7b5214d172cbc1cf0c
    type: comment
  author: karan4d
  content: good to know--will reconsider and happily take base model suggestions.
  created_at: 2023-06-05 17:33:31+00:00
  edited: false
  hidden: false
  id: 647e2a7b5214d172cbc1cf0c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: NousResearch/Nous-Hermes-13b
repo_type: model
status: open
target_branch: null
title: If you haven't tried it, do. This model is great
