!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rkj45
conflicting_files: null
created_at: 2023-04-01 23:23:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a7191656205bc392726f6a1c5b02d9ea.svg
      fullname: Richard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rkj45
      type: user
    createdAt: '2023-04-02T00:23:17.000Z'
    data:
      edited: false
      editors:
      - rkj45
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a7191656205bc392726f6a1c5b02d9ea.svg
          fullname: Richard
          isHf: false
          isPro: false
          name: rkj45
          type: user
        html: '<p>I am learning alpaca models, can you please point to the right direction
          on what to use to chat with the model using gpu ? Thank you.</p>

          '
        raw: I am learning alpaca models, can you please point to the right direction
          on what to use to chat with the model using gpu ? Thank you.
        updatedAt: '2023-04-02T00:23:17.343Z'
      numEdits: 0
      reactions: []
    id: 6428caf5eb2891d3746e2392
    type: comment
  author: rkj45
  content: I am learning alpaca models, can you please point to the right direction
    on what to use to chat with the model using gpu ? Thank you.
  created_at: 2023-04-01 23:23:17+00:00
  edited: false
  hidden: false
  id: 6428caf5eb2891d3746e2392
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-02T16:58:15.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>Please take a look at the README for 2 ways to run inference, including
          chat (option 2)</p>

          '
        raw: Please take a look at the README for 2 ways to run inference, including
          chat (option 2)
        updatedAt: '2023-04-02T16:58:15.559Z'
      numEdits: 0
      reactions: []
    id: 6429b42735f537352b8c7c5a
    type: comment
  author: elinas
  content: Please take a look at the README for 2 ways to run inference, including
    chat (option 2)
  created_at: 2023-04-02 15:58:15+00:00
  edited: false
  hidden: false
  id: 6429b42735f537352b8c7c5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-02T16:58:17.000Z'
    data:
      status: closed
    id: 6429b429b20cdada12fd8d35
    type: status-change
  author: elinas
  created_at: 2023-04-02 15:58:17+00:00
  id: 6429b429b20cdada12fd8d35
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a7191656205bc392726f6a1c5b02d9ea.svg
      fullname: Richard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rkj45
      type: user
    createdAt: '2023-04-03T21:11:34.000Z'
    data:
      edited: false
      editors:
      - rkj45
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a7191656205bc392726f6a1c5b02d9ea.svg
          fullname: Richard
          isHf: false
          isPro: false
          name: rkj45
          type: user
        html: '<p>Got this error</p>

          <p>CUDA SETUP: CUDA runtime path found: /opt/miniconda/envs/textgen/lib/libcudart.so<br>CUDA
          SETUP: Highest compute capability among GPUs detected: 8.6<br>CUDA SETUP:
          Detected CUDA version 117<br>CUDA SETUP: Loading binary /opt/miniconda/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...<br>Loading
          alpaca-30b-lora-int4...<br>Loading model ...<br>Traceback (most recent call
          last):<br>  File "/app/text-generation-webui/server.py", line 276, in <br>    shared.model,
          shared.tokenizer = load_model(shared.model_name)<br>  File "/app/text-generation-webui/modules/models.py",
          line 102, in load_model<br>    model = load_quantized(model_name)<br>  File
          "/app/text-generation-webui/modules/GPTQ_loader.py", line 111, in load_quantized<br>    model
          = load_quant(str(path_to_model), str(pt_path), shared.args.wbits, shared.args.groupsize,
          shared.args.pre_layer)<br>  File "/app/text-generation-webui/repositories/GPTQ-for-LLaMa/llama_inference_offload.py",
          line 228, in load_quant<br>    model.load_state_dict(torch.load(checkpoint))<br>  File
          "/opt/miniconda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 2041, in load_state_dict<br>    raise RuntimeError(''Error(s) in loading
          state_dict for {}:\n\t{}''.format(<br>RuntimeError: Error(s) in loading
          state_dict for LlamaForCausalLM:<br>        Missing key(s) in state_dict:
          "model.layers.0.self_attn.k_proj.qzeros", "model.layers.0.self_attn.o_proj.qzeros",
          "model.layers.0.self_attn.q_proj.qzeros", "model.layers.0.self_attn.v_proj.qzeros",
          "model.layers.0.mlp.down_proj.qzeros"</p>

          '
        raw: "Got this error\n\nCUDA SETUP: CUDA runtime path found: /opt/miniconda/envs/textgen/lib/libcudart.so\n\
          CUDA SETUP: Highest compute capability among GPUs detected: 8.6\nCUDA SETUP:\
          \ Detected CUDA version 117\nCUDA SETUP: Loading binary /opt/miniconda/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n\
          Loading alpaca-30b-lora-int4...\nLoading model ...\nTraceback (most recent\
          \ call last):\n  File \"/app/text-generation-webui/server.py\", line 276,\
          \ in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"/app/text-generation-webui/modules/models.py\", line 102, in load_model\n\
          \    model = load_quantized(model_name)\n  File \"/app/text-generation-webui/modules/GPTQ_loader.py\"\
          , line 111, in load_quantized\n    model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, shared.args.pre_layer)\n\
          \  File \"/app/text-generation-webui/repositories/GPTQ-for-LLaMa/llama_inference_offload.py\"\
          , line 228, in load_quant\n    model.load_state_dict(torch.load(checkpoint))\n\
          \  File \"/opt/miniconda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading\
          \ state_dict for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading\
          \ state_dict for LlamaForCausalLM:\n        Missing key(s) in state_dict:\
          \ \"model.layers.0.self_attn.k_proj.qzeros\", \"model.layers.0.self_attn.o_proj.qzeros\"\
          , \"model.layers.0.self_attn.q_proj.qzeros\", \"model.layers.0.self_attn.v_proj.qzeros\"\
          , \"model.layers.0.mlp.down_proj.qzeros\""
        updatedAt: '2023-04-03T21:11:34.816Z'
      numEdits: 0
      reactions: []
      relatedEventId: 642b4106609bb798dfb46202
    id: 642b4106609bb798dfb46201
    type: comment
  author: rkj45
  content: "Got this error\n\nCUDA SETUP: CUDA runtime path found: /opt/miniconda/envs/textgen/lib/libcudart.so\n\
    CUDA SETUP: Highest compute capability among GPUs detected: 8.6\nCUDA SETUP: Detected\
    \ CUDA version 117\nCUDA SETUP: Loading binary /opt/miniconda/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n\
    Loading alpaca-30b-lora-int4...\nLoading model ...\nTraceback (most recent call\
    \ last):\n  File \"/app/text-generation-webui/server.py\", line 276, in <module>\n\
    \    shared.model, shared.tokenizer = load_model(shared.model_name)\n  File \"\
    /app/text-generation-webui/modules/models.py\", line 102, in load_model\n    model\
    \ = load_quantized(model_name)\n  File \"/app/text-generation-webui/modules/GPTQ_loader.py\"\
    , line 111, in load_quantized\n    model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, shared.args.pre_layer)\n  File \"\
    /app/text-generation-webui/repositories/GPTQ-for-LLaMa/llama_inference_offload.py\"\
    , line 228, in load_quant\n    model.load_state_dict(torch.load(checkpoint))\n\
    \  File \"/opt/miniconda/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading state_dict\
    \ for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\n\
    \        Missing key(s) in state_dict: \"model.layers.0.self_attn.k_proj.qzeros\"\
    , \"model.layers.0.self_attn.o_proj.qzeros\", \"model.layers.0.self_attn.q_proj.qzeros\"\
    , \"model.layers.0.self_attn.v_proj.qzeros\", \"model.layers.0.mlp.down_proj.qzeros\""
  created_at: 2023-04-03 20:11:34+00:00
  edited: false
  hidden: false
  id: 642b4106609bb798dfb46201
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a7191656205bc392726f6a1c5b02d9ea.svg
      fullname: Richard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rkj45
      type: user
    createdAt: '2023-04-03T21:11:34.000Z'
    data:
      status: open
    id: 642b4106609bb798dfb46202
    type: status-change
  author: rkj45
  created_at: 2023-04-03 20:11:34+00:00
  id: 642b4106609bb798dfb46202
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-03T21:20:18.000Z'
    data:
      edited: true
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>Please see <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#step-1-install-gptq-for-llama">https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#step-1-install-gptq-for-llama</a></p>

          <p>There are breaking changes and you should use commit <code>a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773</code>
          in the <code>cuda</code> branch.</p>

          '
        raw: 'Please see https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#step-1-install-gptq-for-llama


          There are breaking changes and you should use commit `a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773`
          in the `cuda` branch.'
        updatedAt: '2023-04-03T21:20:42.748Z'
      numEdits: 1
      reactions: []
    id: 642b4312e845aafe00e0f57c
    type: comment
  author: elinas
  content: 'Please see https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#step-1-install-gptq-for-llama


    There are breaking changes and you should use commit `a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773`
    in the `cuda` branch.'
  created_at: 2023-04-03 20:20:18+00:00
  edited: true
  hidden: false
  id: 642b4312e845aafe00e0f57c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-03T21:34:57.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>added an update here <a href="https://huggingface.co/elinas/alpaca-30b-lora-int4#update-2023-04-03">https://huggingface.co/elinas/alpaca-30b-lora-int4#update-2023-04-03</a></p>

          '
        raw: added an update here https://huggingface.co/elinas/alpaca-30b-lora-int4#update-2023-04-03
        updatedAt: '2023-04-03T21:34:57.198Z'
      numEdits: 0
      reactions: []
    id: 642b4681bf3d1ac926db6df8
    type: comment
  author: elinas
  content: added an update here https://huggingface.co/elinas/alpaca-30b-lora-int4#update-2023-04-03
  created_at: 2023-04-03 20:34:57+00:00
  edited: false
  hidden: false
  id: 642b4681bf3d1ac926db6df8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ca5c1b738bc7bc22f62df602a5a4ac6.svg
      fullname: Lex Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LexSong
      type: user
    createdAt: '2023-04-04T06:59:20.000Z'
    data:
      edited: false
      editors:
      - LexSong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ca5c1b738bc7bc22f62df602a5a4ac6.svg
          fullname: Lex Song
          isHf: false
          isPro: false
          name: LexSong
          type: user
        html: '<p>Hi, I think <code>a6f36e3</code> is not a reference of <a rel="nofollow"
          href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">qwopqwop200/GPTQ-for-LLaMa</a>,
          right?</p>

          <p>Currently, I can use <code>468c47c</code> of qwopqwop200/GPTQ-for-LLaMa
          with old <code>alpaca-30b-4bit.pt</code>. But what version can I use with
          the safetensor checkpoints? I tried the latest version of GPTQ and it didn''t
          work. Which one should I use to load safetensors?</p>

          '
        raw: 'Hi, I think `a6f36e3` is not a reference of [qwopqwop200/GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa),
          right?


          Currently, I can use `468c47c` of qwopqwop200/GPTQ-for-LLaMa with old `alpaca-30b-4bit.pt`.
          But what version can I use with the safetensor checkpoints? I tried the
          latest version of GPTQ and it didn''t work. Which one should I use to load
          safetensors?'
        updatedAt: '2023-04-04T06:59:20.804Z'
      numEdits: 0
      reactions: []
    id: 642bcac886dbbc0d6fcedf15
    type: comment
  author: LexSong
  content: 'Hi, I think `a6f36e3` is not a reference of [qwopqwop200/GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa),
    right?


    Currently, I can use `468c47c` of qwopqwop200/GPTQ-for-LLaMa with old `alpaca-30b-4bit.pt`.
    But what version can I use with the safetensor checkpoints? I tried the latest
    version of GPTQ and it didn''t work. Which one should I use to load safetensors?'
  created_at: 2023-04-04 05:59:20+00:00
  edited: false
  hidden: false
  id: 642bcac886dbbc0d6fcedf15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a7191656205bc392726f6a1c5b02d9ea.svg
      fullname: Richard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rkj45
      type: user
    createdAt: '2023-04-04T15:10:12.000Z'
    data:
      edited: false
      editors:
      - rkj45
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a7191656205bc392726f6a1c5b02d9ea.svg
          fullname: Richard
          isHf: false
          isPro: false
          name: rkj45
          type: user
        html: '<p>btw a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773 didnt work, got the
          same error</p>

          '
        raw: btw a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773 didnt work, got the same
          error
        updatedAt: '2023-04-04T15:10:12.573Z'
      numEdits: 0
      reactions: []
    id: 642c3dd4225f748ce305f03f
    type: comment
  author: rkj45
  content: btw a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773 didnt work, got the same error
  created_at: 2023-04-04 14:10:12+00:00
  edited: false
  hidden: false
  id: 642c3dd4225f748ce305f03f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-04T15:16:13.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: "<blockquote>\n<p>Hi, I think a6f36e3 is not a reference of qwopqwop200/GPTQ-for-LLaMa,\
          \ right?</p>\n</blockquote>\n<p>Yes it is, for the <code>cuda</code> branch.\
          \ There is also a <code>triton</code> branch but I haven't messed with it.</p>\n\
          <blockquote>\n<p>btw a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773 didnt work,\
          \ got the same error</p>\n</blockquote>\n<p>Do <code>git log</code> and\
          \ ensure you're on the correct commit. It works fine for me. If you are\
          \ and it still does not work, try to re-install all of the requirements\
          \ and run <code>python setup_cuda.py install</code>. </p>\n<pre><code>commit\
          \ a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773 (HEAD -&gt; cuda-stable)\nAuthor:\
          \ oobabooga &lt;112222186+oobabooga@users.noreply.github.com&gt;\nDate:\
          \   Fri Mar 31 00:31:06 2023 -0300\n\n    Move model saving back to the\
          \ end\n</code></pre>\n"
        raw: "> Hi, I think a6f36e3 is not a reference of qwopqwop200/GPTQ-for-LLaMa,\
          \ right?\n\nYes it is, for the `cuda` branch. There is also a `triton` branch\
          \ but I haven't messed with it.\n\n> btw a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773\
          \ didnt work, got the same error\n\nDo `git log` and ensure you're on the\
          \ correct commit. It works fine for me. If you are and it still does not\
          \ work, try to re-install all of the requirements and run `python setup_cuda.py\
          \ install`. \n \n```\ncommit a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773 (HEAD\
          \ -> cuda-stable)\nAuthor: oobabooga <112222186+oobabooga@users.noreply.github.com>\n\
          Date:   Fri Mar 31 00:31:06 2023 -0300\n\n    Move model saving back to\
          \ the end\n```"
        updatedAt: '2023-04-04T15:16:13.284Z'
      numEdits: 0
      reactions: []
    id: 642c3f3dcd5f4f52d4a5ef1d
    type: comment
  author: elinas
  content: "> Hi, I think a6f36e3 is not a reference of qwopqwop200/GPTQ-for-LLaMa,\
    \ right?\n\nYes it is, for the `cuda` branch. There is also a `triton` branch\
    \ but I haven't messed with it.\n\n> btw a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773\
    \ didnt work, got the same error\n\nDo `git log` and ensure you're on the correct\
    \ commit. It works fine for me. If you are and it still does not work, try to\
    \ re-install all of the requirements and run `python setup_cuda.py install`. \n\
    \ \n```\ncommit a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773 (HEAD -> cuda-stable)\n\
    Author: oobabooga <112222186+oobabooga@users.noreply.github.com>\nDate:   Fri\
    \ Mar 31 00:31:06 2023 -0300\n\n    Move model saving back to the end\n```"
  created_at: 2023-04-04 14:16:13+00:00
  edited: false
  hidden: false
  id: 642c3f3dcd5f4f52d4a5ef1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a7191656205bc392726f6a1c5b02d9ea.svg
      fullname: Richard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rkj45
      type: user
    createdAt: '2023-04-04T16:09:31.000Z'
    data:
      edited: false
      editors:
      - rkj45
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a7191656205bc392726f6a1c5b02d9ea.svg
          fullname: Richard
          isHf: false
          isPro: false
          name: rkj45
          type: user
        html: '<p>Sadly no luck, tried with py 3.9, 3.10 , started from scratch with
          the right requirements</p>

          <p>(textgen) root@9b843d1d1b8e:/app/text-generation-webui# python server.py
          --model alpaca-30b-lora-int4 --wbits 4<br>CUDA SETUP: CUDA runtime path
          found: /usr/local/cuda/lib64/libcudart.so<br>CUDA SETUP: Highest compute
          capability among GPUs detected: 8.6<br>CUDA SETUP: Detected CUDA version
          117<br>CUDA SETUP: Loading binary /opt/miniconda/envs/textgen/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...<br>Loading
          alpaca-30b-lora-int4...<br>Loading model ...<br>Traceback (most recent call
          last):<br>  File "/app/text-generation-webui/server.py", line 276, in <br>    shared.model,
          shared.tokenizer = load_model(shared.model_name)<br>  File "/app/text-generation-webui/modules/models.py",
          line 102, in load_model<br>    model = load_quantized(model_name)<br>  File
          "/app/text-generation-webui/modules/GPTQ_loader.py", line 114, in load_quantized<br>    model
          = load_quant(str(path_to_model), str(pt_path), shared.args.wbits, shared.args.groupsize,
          kernel_switch_threshold=threshold)<br>  File "/app/text-generation-webui/modules/GPTQ_loader.py",
          line 45, in _load_quant<br>    model.load_state_dict(torch.load(checkpoint))<br>  File
          "/opt/miniconda/envs/textgen/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 2041, in load_state_dict<br>    raise RuntimeError(''Error(s) in loading
          state_dict for {}:\n\t{}''.format(<br>RuntimeError: Error(s) in loading
          state_dict for LlamaForCausalLM:<br>        Missing key(s) in state_dict:
          "model.layers.0.self_attn.k_proj.qzeros", "model.layers.0.self_attn.o_proj.qzeros",
          "model.layers.0.self_attn.q_proj.qzeros", "model.layers.0.self_attn.v_proj.qzeros"</p>

          <p>(textgen) root@9b843d1d1b8e:/app/text-generation-webui/repositories/GPTQ-for-LLaMa#
          git log<br>commit a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773 (HEAD -&gt; cuda-stable)<br>Author:
          oobabooga <a rel="nofollow" href="mailto:112222186+oobabooga@users.noreply.github.com">112222186+oobabooga@users.noreply.github.com</a><br>Date:   Fri
          Mar 31 00:31:06 2023 -0300</p>

          '
        raw: "Sadly no luck, tried with py 3.9, 3.10 , started from scratch with the\
          \ right requirements\n\n(textgen) root@9b843d1d1b8e:/app/text-generation-webui#\
          \ python server.py --model alpaca-30b-lora-int4 --wbits 4\nCUDA SETUP: CUDA\
          \ runtime path found: /usr/local/cuda/lib64/libcudart.so\nCUDA SETUP: Highest\
          \ compute capability among GPUs detected: 8.6\nCUDA SETUP: Detected CUDA\
          \ version 117\nCUDA SETUP: Loading binary /opt/miniconda/envs/textgen/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n\
          Loading alpaca-30b-lora-int4...\nLoading model ...\nTraceback (most recent\
          \ call last):\n  File \"/app/text-generation-webui/server.py\", line 276,\
          \ in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"/app/text-generation-webui/modules/models.py\", line 102, in load_model\n\
          \    model = load_quantized(model_name)\n  File \"/app/text-generation-webui/modules/GPTQ_loader.py\"\
          , line 114, in load_quantized\n    model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          \  File \"/app/text-generation-webui/modules/GPTQ_loader.py\", line 45,\
          \ in _load_quant\n    model.load_state_dict(torch.load(checkpoint))\n  File\
          \ \"/opt/miniconda/envs/textgen/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading\
          \ state_dict for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading\
          \ state_dict for LlamaForCausalLM:\n        Missing key(s) in state_dict:\
          \ \"model.layers.0.self_attn.k_proj.qzeros\", \"model.layers.0.self_attn.o_proj.qzeros\"\
          , \"model.layers.0.self_attn.q_proj.qzeros\", \"model.layers.0.self_attn.v_proj.qzeros\"\
          \n\n(textgen) root@9b843d1d1b8e:/app/text-generation-webui/repositories/GPTQ-for-LLaMa#\
          \ git log\ncommit a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773 (HEAD -> cuda-stable)\n\
          Author: oobabooga <112222186+oobabooga@users.noreply.github.com>\nDate:\
          \   Fri Mar 31 00:31:06 2023 -0300"
        updatedAt: '2023-04-04T16:09:31.269Z'
      numEdits: 0
      reactions: []
    id: 642c4bbbd3340aee04414f9b
    type: comment
  author: rkj45
  content: "Sadly no luck, tried with py 3.9, 3.10 , started from scratch with the\
    \ right requirements\n\n(textgen) root@9b843d1d1b8e:/app/text-generation-webui#\
    \ python server.py --model alpaca-30b-lora-int4 --wbits 4\nCUDA SETUP: CUDA runtime\
    \ path found: /usr/local/cuda/lib64/libcudart.so\nCUDA SETUP: Highest compute\
    \ capability among GPUs detected: 8.6\nCUDA SETUP: Detected CUDA version 117\n\
    CUDA SETUP: Loading binary /opt/miniconda/envs/textgen/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n\
    Loading alpaca-30b-lora-int4...\nLoading model ...\nTraceback (most recent call\
    \ last):\n  File \"/app/text-generation-webui/server.py\", line 276, in <module>\n\
    \    shared.model, shared.tokenizer = load_model(shared.model_name)\n  File \"\
    /app/text-generation-webui/modules/models.py\", line 102, in load_model\n    model\
    \ = load_quantized(model_name)\n  File \"/app/text-generation-webui/modules/GPTQ_loader.py\"\
    , line 114, in load_quantized\n    model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
    \  File \"/app/text-generation-webui/modules/GPTQ_loader.py\", line 45, in _load_quant\n\
    \    model.load_state_dict(torch.load(checkpoint))\n  File \"/opt/miniconda/envs/textgen/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading state_dict\
    \ for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\n\
    \        Missing key(s) in state_dict: \"model.layers.0.self_attn.k_proj.qzeros\"\
    , \"model.layers.0.self_attn.o_proj.qzeros\", \"model.layers.0.self_attn.q_proj.qzeros\"\
    , \"model.layers.0.self_attn.v_proj.qzeros\"\n\n(textgen) root@9b843d1d1b8e:/app/text-generation-webui/repositories/GPTQ-for-LLaMa#\
    \ git log\ncommit a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773 (HEAD -> cuda-stable)\n\
    Author: oobabooga <112222186+oobabooga@users.noreply.github.com>\nDate:   Fri\
    \ Mar 31 00:31:06 2023 -0300"
  created_at: 2023-04-04 15:09:31+00:00
  edited: false
  hidden: false
  id: 642c4bbbd3340aee04414f9b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-04T16:12:10.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>Are you using the old <code>.pt</code> model or one of the new <code>safetensors</code>
          models? The former will not work unless you''re on a pretty old commit.</p>

          '
        raw: Are you using the old `.pt` model or one of the new `safetensors` models?
          The former will not work unless you're on a pretty old commit.
        updatedAt: '2023-04-04T16:12:10.970Z'
      numEdits: 0
      reactions: []
    id: 642c4c5a344576fa5eb626d9
    type: comment
  author: elinas
  content: Are you using the old `.pt` model or one of the new `safetensors` models?
    The former will not work unless you're on a pretty old commit.
  created_at: 2023-04-04 15:12:10+00:00
  edited: false
  hidden: false
  id: 642c4c5a344576fa5eb626d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a7191656205bc392726f6a1c5b02d9ea.svg
      fullname: Richard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rkj45
      type: user
    createdAt: '2023-04-04T16:14:56.000Z'
    data:
      edited: false
      editors:
      - rkj45
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a7191656205bc392726f6a1c5b02d9ea.svg
          fullname: Richard
          isHf: false
          isPro: false
          name: rkj45
          type: user
        html: '<p>(textgen) root@9b843d1d1b8e:/app/text-generation-webui/models/alpaca-30b-lora-int4#
          ls -alh<br>total 49G<br>drwxr-xr-x 1 root root 4.0K Apr  3 18:10 .<br>drwxr-xr-x
          1 root root 4.0K Apr  3 16:42 ..<br>drwxr-xr-x 1 root root 4.0K Apr  3 18:10
          .git<br>-rw-r--r-- 1 root root 1.5K Apr  3 16:42 .gitattributes<br>-rw-r--r--
          1 root root  11K Apr  3 16:42 README.md<br>-rw-r--r-- 1 root root  17G Apr  3
          18:10 alpaca-30b-4bit-128g.safetensors<br>-rw-r--r-- 1 root root  16G Apr  3
          18:07 alpaca-30b-4bit.pt<br>-rw-r--r-- 1 root root  16G Apr  3 18:04 alpaca-30b-4bit.safetensors<br>-rw-r--r--
          1 root root  426 Apr  3 16:42 config.json<br>-rw-r--r-- 1 root root  124
          Apr  3 16:42 generation_config.json<br>-rw-r--r-- 1 root root  47K Apr  3
          16:42 pytorch_model.bin.index.json<br>-rw-r--r-- 1 root root    2 Apr  3
          16:42 special_tokens_map.json<br>-rw-r--r-- 1 root root 489K Apr  3 16:42
          tokenizer.model<br>-rw-r--r-- 1 root root  141 Apr  3 16:42 tokenizer_config.json</p>

          '
        raw: '(textgen) root@9b843d1d1b8e:/app/text-generation-webui/models/alpaca-30b-lora-int4#
          ls -alh

          total 49G

          drwxr-xr-x 1 root root 4.0K Apr  3 18:10 .

          drwxr-xr-x 1 root root 4.0K Apr  3 16:42 ..

          drwxr-xr-x 1 root root 4.0K Apr  3 18:10 .git

          -rw-r--r-- 1 root root 1.5K Apr  3 16:42 .gitattributes

          -rw-r--r-- 1 root root  11K Apr  3 16:42 README.md

          -rw-r--r-- 1 root root  17G Apr  3 18:10 alpaca-30b-4bit-128g.safetensors

          -rw-r--r-- 1 root root  16G Apr  3 18:07 alpaca-30b-4bit.pt

          -rw-r--r-- 1 root root  16G Apr  3 18:04 alpaca-30b-4bit.safetensors

          -rw-r--r-- 1 root root  426 Apr  3 16:42 config.json

          -rw-r--r-- 1 root root  124 Apr  3 16:42 generation_config.json

          -rw-r--r-- 1 root root  47K Apr  3 16:42 pytorch_model.bin.index.json

          -rw-r--r-- 1 root root    2 Apr  3 16:42 special_tokens_map.json

          -rw-r--r-- 1 root root 489K Apr  3 16:42 tokenizer.model

          -rw-r--r-- 1 root root  141 Apr  3 16:42 tokenizer_config.json'
        updatedAt: '2023-04-04T16:14:56.994Z'
      numEdits: 0
      reactions: []
    id: 642c4d00c811cd1de5b9c62f
    type: comment
  author: rkj45
  content: '(textgen) root@9b843d1d1b8e:/app/text-generation-webui/models/alpaca-30b-lora-int4#
    ls -alh

    total 49G

    drwxr-xr-x 1 root root 4.0K Apr  3 18:10 .

    drwxr-xr-x 1 root root 4.0K Apr  3 16:42 ..

    drwxr-xr-x 1 root root 4.0K Apr  3 18:10 .git

    -rw-r--r-- 1 root root 1.5K Apr  3 16:42 .gitattributes

    -rw-r--r-- 1 root root  11K Apr  3 16:42 README.md

    -rw-r--r-- 1 root root  17G Apr  3 18:10 alpaca-30b-4bit-128g.safetensors

    -rw-r--r-- 1 root root  16G Apr  3 18:07 alpaca-30b-4bit.pt

    -rw-r--r-- 1 root root  16G Apr  3 18:04 alpaca-30b-4bit.safetensors

    -rw-r--r-- 1 root root  426 Apr  3 16:42 config.json

    -rw-r--r-- 1 root root  124 Apr  3 16:42 generation_config.json

    -rw-r--r-- 1 root root  47K Apr  3 16:42 pytorch_model.bin.index.json

    -rw-r--r-- 1 root root    2 Apr  3 16:42 special_tokens_map.json

    -rw-r--r-- 1 root root 489K Apr  3 16:42 tokenizer.model

    -rw-r--r-- 1 root root  141 Apr  3 16:42 tokenizer_config.json'
  created_at: 2023-04-04 15:14:56+00:00
  edited: false
  hidden: false
  id: 642c4d00c811cd1de5b9c62f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-04T16:16:58.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>Only have one checkpoint in your directory that you plan to use.</p>

          '
        raw: Only have one checkpoint in your directory that you plan to use.
        updatedAt: '2023-04-04T16:16:58.283Z'
      numEdits: 0
      reactions: []
    id: 642c4d7a7bbd304049324d3a
    type: comment
  author: elinas
  content: Only have one checkpoint in your directory that you plan to use.
  created_at: 2023-04-04 15:16:58+00:00
  edited: false
  hidden: false
  id: 642c4d7a7bbd304049324d3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ca5c1b738bc7bc22f62df602a5a4ac6.svg
      fullname: Lex Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LexSong
      type: user
    createdAt: '2023-04-04T23:46:36.000Z'
    data:
      edited: true
      editors:
      - LexSong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ca5c1b738bc7bc22f62df602a5a4ac6.svg
          fullname: Lex Song
          isHf: false
          isPro: false
          name: LexSong
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Hi, I think a6f36e3 is not a reference of qwopqwop200/GPTQ-for-LLaMa,
          right?</p>

          </blockquote>

          <p>Yes it is, for the <code>cuda</code> branch. There is also a <code>triton</code>
          branch but I haven''t messed with it.</p>

          </blockquote>

          <p>I found the issue. <code>a6f36e3</code> is on <a rel="nofollow" href="https://github.com/oobabooga/GPTQ-for-LLaMa">oobabooga/GPTQ-for-LLaMa</a>
          and not on the qwopqwop200''s original repo.</p>

          '
        raw: "> > Hi, I think a6f36e3 is not a reference of qwopqwop200/GPTQ-for-LLaMa,\
          \ right?\n> \n> Yes it is, for the `cuda` branch. There is also a `triton`\
          \ branch but I haven't messed with it.\n> \n\nI found the issue. `a6f36e3`\
          \ is on [oobabooga/GPTQ-for-LLaMa](https://github.com/oobabooga/GPTQ-for-LLaMa)\
          \ and not on the qwopqwop200's original repo."
        updatedAt: '2023-04-04T23:46:50.638Z'
      numEdits: 1
      reactions: []
    id: 642cb6dc3bf67e4014215a9c
    type: comment
  author: LexSong
  content: "> > Hi, I think a6f36e3 is not a reference of qwopqwop200/GPTQ-for-LLaMa,\
    \ right?\n> \n> Yes it is, for the `cuda` branch. There is also a `triton` branch\
    \ but I haven't messed with it.\n> \n\nI found the issue. `a6f36e3` is on [oobabooga/GPTQ-for-LLaMa](https://github.com/oobabooga/GPTQ-for-LLaMa)\
    \ and not on the qwopqwop200's original repo."
  created_at: 2023-04-04 22:46:36+00:00
  edited: true
  hidden: false
  id: 642cb6dc3bf67e4014215a9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a7191656205bc392726f6a1c5b02d9ea.svg
      fullname: Richard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rkj45
      type: user
    createdAt: '2023-04-05T02:32:39.000Z'
    data:
      edited: false
      editors:
      - rkj45
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a7191656205bc392726f6a1c5b02d9ea.svg
          fullname: Richard
          isHf: false
          isPro: false
          name: rkj45
          type: user
        html: '<p>That worked!, thank you, now I am facing another issue, there seems
          to be an extra text on every response, do you know what could it be ?<br><a
          rel="nofollow" href="https://prnt.sc/a-NmywcPdKAa">https://prnt.sc/a-NmywcPdKAa</a></p>

          '
        raw: 'That worked!, thank you, now I am facing another issue, there seems
          to be an extra text on every response, do you know what could it be ?

          https://prnt.sc/a-NmywcPdKAa'
        updatedAt: '2023-04-05T02:32:39.888Z'
      numEdits: 0
      reactions: []
    id: 642cddc73bf67e4014224742
    type: comment
  author: rkj45
  content: 'That worked!, thank you, now I am facing another issue, there seems to
    be an extra text on every response, do you know what could it be ?

    https://prnt.sc/a-NmywcPdKAa'
  created_at: 2023-04-05 01:32:39+00:00
  edited: false
  hidden: false
  id: 642cddc73bf67e4014224742
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-04-05T16:23:51.000Z'
    data:
      edited: true
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: '<blockquote>

          <p>That worked!, thank you, now I am facing another issue, there seems to
          be an extra text on every response, do you know what could it be ?<br><a
          rel="nofollow" href="https://prnt.sc/a-NmywcPdKAa">https://prnt.sc/a-NmywcPdKAa</a></p>

          </blockquote>

          <p>Haha, I get this too.. It seems to go away if I try the "example" character
          card, so I think may be the default parameters.</p>

          <p>The Model card seems to have some preferred params^^</p>

          '
        raw: '> That worked!, thank you, now I am facing another issue, there seems
          to be an extra text on every response, do you know what could it be ?

          > https://prnt.sc/a-NmywcPdKAa


          Haha, I get this too.. It seems to go away if I try the "example" character
          card, so I think may be the default parameters.


          The Model card seems to have some preferred params^^'
        updatedAt: '2023-04-05T16:27:18.616Z'
      numEdits: 2
      reactions: []
    id: 642da09724bf366459b189c3
    type: comment
  author: disarmyouwitha
  content: '> That worked!, thank you, now I am facing another issue, there seems
    to be an extra text on every response, do you know what could it be ?

    > https://prnt.sc/a-NmywcPdKAa


    Haha, I get this too.. It seems to go away if I try the "example" character card,
    so I think may be the default parameters.


    The Model card seems to have some preferred params^^'
  created_at: 2023-04-05 15:23:51+00:00
  edited: true
  hidden: false
  id: 642da09724bf366459b189c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-12T23:43:08.000Z'
    data:
      status: closed
    id: 6437420c2c5801f1163bb761
    type: status-change
  author: elinas
  created_at: 2023-04-12 22:43:08+00:00
  id: 6437420c2c5801f1163bb761
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: elinas/alpaca-30b-lora-int4
repo_type: model
status: closed
target_branch: null
title: Chat interface
