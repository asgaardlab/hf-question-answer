!!python/object:huggingface_hub.community.DiscussionWithDetails
author: haydenhong
conflicting_files: null
created_at: 2023-03-29 12:51:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a05379ab140073de0281000e1b21f119.svg
      fullname: hhong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: haydenhong
      type: user
    createdAt: '2023-03-29T13:51:37.000Z'
    data:
      edited: true
      editors:
      - haydenhong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a05379ab140073de0281000e1b21f119.svg
          fullname: hhong
          isHf: false
          isPro: false
          name: haydenhong
          type: user
        html: '<p>Thanks for the great works!<br>I have just pulled your latest, done
          pip-install the requirements, and used the following GPTQ command to run:<br>python
          setup_cuda.py install<br>CUDA_VISIBLE_DEVICES=0 python llama_inference.py  alpaca-30b-lora-int4
          --wbits 4 --load alpaca-30b-lora-int4/alpaca-30b-4bit-128g.safetensors  --text
          "count up to 10" --max_length 1500 --min_length 100<br>Where alpaca-30b-lora-int4
          --wbits 4 is the subfolder, git clone of your code<br>it fails with the
          following mismatch; any suggestion how to fix this? thanks!</p>

          <p>Loading model ...<br>Traceback (most recent call last):<br>  File "llama_inference.py",
          line 112, in <br>    model = load_quant(args.model, args.load, args.wbits,
          args.groupsize)<br>  File "llama_inference.py", line 50, in load_quant<br>    model.load_state_dict(safe_load(checkpoint))<br>  File
          "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line
          1672, in load_state_dict<br>    self.<strong>class</strong>.<strong>name</strong>,
          "\n\t".join(error_msgs)))<br>RuntimeError: Error(s) in loading state_dict
          for LlamaForCausalLM:<br>        size mismatch for model.layers.0.self_attn.k_proj.qzeros:
          copying a param with shape torch.Size([52, 832]) from checkpoint, the shape
          in current model is torch.Size([1, 832]).<br>        size mismatch for model.layers.0.self_attn.k_proj.scales:
          copying a param with shape torch.Size([52, 6656]) from checkpoint, the shape
          in current model is torch.Size([1, 6656]).<br>        size mismatch for
          model.layers.0.self_attn.o_proj.qzeros: copying a param with shape torch.Size([52,
          832]) from checkpoint, the shape in current model is torch.Size([1, 832]).<br>        size
          mismatch for model.layers.0.self_attn.o_proj.scales: copying a param with
          shape torch.Size([52, 6656]) from checkpoint, the shape in current model
          is torch.Size([1, 6656]).<br>        size mismatch for model.layers.0.self_attn.q_proj.qzeros:
          copying a param with shape torch.Size([52, 832]) from checkpoint, the shape
          in current model is torch.Size([1, 832]).<br>        size mismatch for model.layers.0.self_attn.q_proj.scales:
          copying a param with shape torch.Size([52, 6656]) from checkpoint, the shape
          in current model is torch.Size([1, 6656]).<br>        size mismatch for
          model.layers.0.self_attn.v_proj.qzeros: copying a param with shape torch.Size([52,
          832]) from checkpoint, the shape in current model is torch.Size([1, 832]).<br>        size
          mismatch for model.layers.0.self_attn.v_proj.scales: copying a param with
          shape torch.Size([52, 6656]) from checkpoint, the shape in current model
          is torch.Size([1, 6656]).<br>....</p>

          '
        raw: "Thanks for the great works!\nI have just pulled your latest, done pip-install\
          \ the requirements, and used the following GPTQ command to run:\npython\
          \ setup_cuda.py install\nCUDA_VISIBLE_DEVICES=0 python llama_inference.py\
          \  alpaca-30b-lora-int4 --wbits 4 --load alpaca-30b-lora-int4/alpaca-30b-4bit-128g.safetensors\
          \  --text \"count up to 10\" --max_length 1500 --min_length 100\nWhere alpaca-30b-lora-int4\
          \ --wbits 4 is the subfolder, git clone of your code\nit fails with the\
          \ following mismatch; any suggestion how to fix this? thanks!\n\nLoading\
          \ model ...\nTraceback (most recent call last):\n  File \"llama_inference.py\"\
          , line 112, in <module>\n    model = load_quant(args.model, args.load, args.wbits,\
          \ args.groupsize)\n  File \"llama_inference.py\", line 50, in load_quant\n\
          \    model.load_state_dict(safe_load(checkpoint))\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\"\
          , line 1672, in load_state_dict\n    self.__class__.__name__, \"\\n\\t\"\
          .join(error_msgs)))\nRuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\n\
          \        size mismatch for model.layers.0.self_attn.k_proj.qzeros: copying\
          \ a param with shape torch.Size([52, 832]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 832]).\n        size mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([52, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 6656]).\n        size mismatch\
          \ for model.layers.0.self_attn.o_proj.qzeros: copying a param with shape\
          \ torch.Size([52, 832]) from checkpoint, the shape in current model is torch.Size([1,\
          \ 832]).\n        size mismatch for model.layers.0.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([52, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 6656]).\n        size mismatch\
          \ for model.layers.0.self_attn.q_proj.qzeros: copying a param with shape\
          \ torch.Size([52, 832]) from checkpoint, the shape in current model is torch.Size([1,\
          \ 832]).\n        size mismatch for model.layers.0.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([52, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 6656]).\n        size mismatch\
          \ for model.layers.0.self_attn.v_proj.qzeros: copying a param with shape\
          \ torch.Size([52, 832]) from checkpoint, the shape in current model is torch.Size([1,\
          \ 832]).\n        size mismatch for model.layers.0.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([52, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 6656]).\n...."
        updatedAt: '2023-03-30T00:43:42.350Z'
      numEdits: 1
      reactions: []
    id: 6424426932599aa4bb3be5fe
    type: comment
  author: haydenhong
  content: "Thanks for the great works!\nI have just pulled your latest, done pip-install\
    \ the requirements, and used the following GPTQ command to run:\npython setup_cuda.py\
    \ install\nCUDA_VISIBLE_DEVICES=0 python llama_inference.py  alpaca-30b-lora-int4\
    \ --wbits 4 --load alpaca-30b-lora-int4/alpaca-30b-4bit-128g.safetensors  --text\
    \ \"count up to 10\" --max_length 1500 --min_length 100\nWhere alpaca-30b-lora-int4\
    \ --wbits 4 is the subfolder, git clone of your code\nit fails with the following\
    \ mismatch; any suggestion how to fix this? thanks!\n\nLoading model ...\nTraceback\
    \ (most recent call last):\n  File \"llama_inference.py\", line 112, in <module>\n\
    \    model = load_quant(args.model, args.load, args.wbits, args.groupsize)\n \
    \ File \"llama_inference.py\", line 50, in load_quant\n    model.load_state_dict(safe_load(checkpoint))\n\
    \  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\"\
    , line 1672, in load_state_dict\n    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\
    RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\n        size\
    \ mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param with shape\
    \ torch.Size([52, 832]) from checkpoint, the shape in current model is torch.Size([1,\
    \ 832]).\n        size mismatch for model.layers.0.self_attn.k_proj.scales: copying\
    \ a param with shape torch.Size([52, 6656]) from checkpoint, the shape in current\
    \ model is torch.Size([1, 6656]).\n        size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([52, 832]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 832]).\n        size mismatch for model.layers.0.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([52, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 6656]).\n        size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([52, 832]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 832]).\n        size mismatch for model.layers.0.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([52, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 6656]).\n        size mismatch for model.layers.0.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([52, 832]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 832]).\n        size mismatch for model.layers.0.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([52, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 6656]).\n...."
  created_at: 2023-03-29 12:51:37+00:00
  edited: true
  hidden: false
  id: 6424426932599aa4bb3be5fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a05379ab140073de0281000e1b21f119.svg
      fullname: hhong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: haydenhong
      type: user
    createdAt: '2023-03-30T08:32:37.000Z'
    data:
      edited: false
      editors:
      - haydenhong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a05379ab140073de0281000e1b21f119.svg
          fullname: hhong
          isHf: false
          isPro: false
          name: haydenhong
          type: user
        html: '<p>Use the latest weight alpaca-30b-4bit.safetensors fixes the problem.
          Thanks!</p>

          '
        raw: Use the latest weight alpaca-30b-4bit.safetensors fixes the problem.
          Thanks!
        updatedAt: '2023-03-30T08:32:37.677Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64254925d9896a8a0b864592
    id: 64254925d9896a8a0b864591
    type: comment
  author: haydenhong
  content: Use the latest weight alpaca-30b-4bit.safetensors fixes the problem. Thanks!
  created_at: 2023-03-30 07:32:37+00:00
  edited: false
  hidden: false
  id: 64254925d9896a8a0b864591
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a05379ab140073de0281000e1b21f119.svg
      fullname: hhong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: haydenhong
      type: user
    createdAt: '2023-03-30T08:32:37.000Z'
    data:
      status: closed
    id: 64254925d9896a8a0b864592
    type: status-change
  author: haydenhong
  created_at: 2023-03-30 07:32:37+00:00
  id: 64254925d9896a8a0b864592
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: elinas/alpaca-30b-lora-int4
repo_type: model
status: closed
target_branch: null
title: load_state_dict fails
