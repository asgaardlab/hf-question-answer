!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vdruts
conflicting_files: null
created_at: 2023-04-10 19:38:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
      fullname: Vladimir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vdruts
      type: user
    createdAt: '2023-04-10T20:38:46.000Z'
    data:
      edited: false
      editors:
      - vdruts
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
          fullname: Vladimir
          isHf: false
          isPro: false
          name: vdruts
          type: user
        html: '<h2 id="likely-converted-in-old-gptq-no-longer-compaitble">Likely converted
          in old GPTQ, no longer compaitble</h2>

          <p>storage = cls(wrap_storage=untyped_storage)<br>Traceback (most recent
          call last):<br>  File "C:\Users\xxxx\Deep\text-diffusion-webui\text-generation-webui\server.py",
          line 347, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "C:\Users\xxxx\Deep\text-diffusion-webui\text-generation-webui\modules\models.py",
          line 103, in load_model<br>    model = load_quantized(model_name)<br>  File
          "C:\Users\xxxx\Deep\text-diffusion-webui\text-generation-webui\modules\GPTQ_loader.py",
          line 136, in load_quantized<br>    model = load_quant(str(path_to_model),
          str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)<br>  File
          "C:\Users\xxxx\Deep\text-diffusion-webui\text-generation-webui\modules\GPTQ_loader.py",
          line 61, in _load_quant<br>    model.load_state_dict(safe_load(checkpoint),
          strict=False)<br>  File "C:\Users\xxxx\Deep\text-diffusion-webui\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 2041, in load_state_dict<br>    raise RuntimeError(''Error(s) in loading
          state_dict for {}:\n\t{}''.format(<br>RuntimeError: Error(s) in loading
          state_dict for LlamaForCausalLM:<br>        size mismatch for model.layers.0.self_attn.k_proj.qzeros:
          copying a param with shape torch.Size([52, 832]) from checkpoint, the shape
          in current model is torch.Size([1, 832]).<br>        size mismatch for model.layers.0.self_attn.k_proj.scales:
          copying a param with shape torch.Size([52, 6656]) from checkpoint, the shape
          in current model is torch.Size([1, 6656]).<br>        size mismatch for
          model.layers.0.self_attn.o_proj.qzeros: copying a param with shape torch.Size([52,
          832]) from checkpoint, the shape in current model is torch.Size([1, 832]).<br>        size
          mismatch for model.layers.0.self_attn.o_proj.scales: copying a param with
          shape torch.Size([52, 6656]) from checkpoint, the shape in current model
          is torch.Size([1, 6656]).<br>        size mismatch for model.layers.0.self_attn.q_proj.qzeros:
          copying a param with shape torch.Size([52, 832]) from checkpoint, the shape
          in current model is torch.Size([1, 832]).<br>        size mismatch for model.layers.0.self_attn.q_proj.scales:
          copying a param with shape torch.Size([52, 6656]) from checkpoint, the shape
          in current model is torch.Size([1, 6656]).<br>        size mismatch for
          model.layers.0.self_attn.v_proj.qzeros: copying a param with shape torch.Size([52,
          832]) from checkpoint, the shape in current model is torch.Size([1, 832]).<br>        size
          mismatch for model.layers.0.self_attn.v_proj.scales: copying a param with
          shape torch.Size([52, 6656]) from checkpoint, the shape in current model
          is torch.Size([1, 6656]).<br>        size mismatch for model.layers.0.mlp.down_proj.qzeros:
          copying a param with shape torch.Size([140, 832]) from checkpoint, the shape
          in current model is torch.Size([1, 832]).<br>        size mismatch for model.layers.0.mlp.down_proj.scales:
          copying a param with shape torch.Size([140, 6656]) from checkpoint, the
          shape in current model is torch.Size([1, 6656]).<br>        size mismatch
          for model.layers.0.mlp.gate_proj.qzeros: copying a param with shape torch.Size([52,
          2240]) from checkpoint, the shape in current model is torch.Size([1, 2240]).<br>        size
          mismatch for model.layers.0.mlp.gate_proj.scales: copying a param with shape
          torch.Size([52, 17920]) from checkpoint, the shape in current model is torch.Size([1,
          17920]).. </p>

          '
        raw: "Likely converted in old GPTQ, no longer compaitble\r\n-\r\n\r\nstorage\
          \ = cls(wrap_storage=untyped_storage)\r\nTraceback (most recent call last):\r\
          \n  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\text-generation-webui\\\
          server.py\", line 347, in <module>\r\n    shared.model, shared.tokenizer\
          \ = load_model(shared.model_name)\r\n  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\\
          text-generation-webui\\modules\\models.py\", line 103, in load_model\r\n\
          \    model = load_quantized(model_name)\r\n  File \"C:\\Users\\xxxx\\Deep\\\
          text-diffusion-webui\\text-generation-webui\\modules\\GPTQ_loader.py\",\
          \ line 136, in load_quantized\r\n    model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\r\
          \n  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\text-generation-webui\\\
          modules\\GPTQ_loader.py\", line 61, in _load_quant\r\n    model.load_state_dict(safe_load(checkpoint),\
          \ strict=False)\r\n  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 2041, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading\
          \ state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading\
          \ state_dict for LlamaForCausalLM:\r\n        size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([52, 832]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 832]).\r\n        size mismatch\
          \ for model.layers.0.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([52, 6656]) from checkpoint, the shape in current model is\
          \ torch.Size([1, 6656]).\r\n        size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
          \ copying a param with shape torch.Size([52, 832]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 832]).\r\n        size mismatch\
          \ for model.layers.0.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([52, 6656]) from checkpoint, the shape in current model is\
          \ torch.Size([1, 6656]).\r\n        size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([52, 832]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 832]).\r\n        size mismatch\
          \ for model.layers.0.self_attn.q_proj.scales: copying a param with shape\
          \ torch.Size([52, 6656]) from checkpoint, the shape in current model is\
          \ torch.Size([1, 6656]).\r\n        size mismatch for model.layers.0.self_attn.v_proj.qzeros:\
          \ copying a param with shape torch.Size([52, 832]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 832]).\r\n        size mismatch\
          \ for model.layers.0.self_attn.v_proj.scales: copying a param with shape\
          \ torch.Size([52, 6656]) from checkpoint, the shape in current model is\
          \ torch.Size([1, 6656]).\r\n        size mismatch for model.layers.0.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([140, 832]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 832]).\r\n        size mismatch\
          \ for model.layers.0.mlp.down_proj.scales: copying a param with shape torch.Size([140,\
          \ 6656]) from checkpoint, the shape in current model is torch.Size([1, 6656]).\r\
          \n        size mismatch for model.layers.0.mlp.gate_proj.qzeros: copying\
          \ a param with shape torch.Size([52, 2240]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 2240]).\r\n        size mismatch for model.layers.0.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([52, 17920]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 17920]).. "
        updatedAt: '2023-04-10T20:38:46.979Z'
      numEdits: 0
      reactions: []
    id: 643473d695b8ab049389f5dc
    type: comment
  author: vdruts
  content: "Likely converted in old GPTQ, no longer compaitble\r\n-\r\n\r\nstorage\
    \ = cls(wrap_storage=untyped_storage)\r\nTraceback (most recent call last):\r\n\
    \  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\text-generation-webui\\\
    server.py\", line 347, in <module>\r\n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\
    \n  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\text-generation-webui\\\
    modules\\models.py\", line 103, in load_model\r\n    model = load_quantized(model_name)\r\
    \n  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\text-generation-webui\\\
    modules\\GPTQ_loader.py\", line 136, in load_quantized\r\n    model = load_quant(str(path_to_model),\
    \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\r\
    \n  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\text-generation-webui\\\
    modules\\GPTQ_loader.py\", line 61, in _load_quant\r\n    model.load_state_dict(safe_load(checkpoint),\
    \ strict=False)\r\n  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 2041, in load_state_dict\r\
    \n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\
    \nRuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\r\n     \
    \   size mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param\
    \ with shape torch.Size([52, 832]) from checkpoint, the shape in current model\
    \ is torch.Size([1, 832]).\r\n        size mismatch for model.layers.0.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([52, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 6656]).\r\n        size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([52, 832]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 832]).\r\n        size mismatch for model.layers.0.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([52, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 6656]).\r\n        size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([52, 832]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 832]).\r\n        size mismatch for model.layers.0.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([52, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 6656]).\r\n        size mismatch for model.layers.0.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([52, 832]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 832]).\r\n        size mismatch for model.layers.0.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([52, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 6656]).\r\n        size mismatch for model.layers.0.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([140, 832]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 832]).\r\n        size mismatch for model.layers.0.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([140, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 6656]).\r\n        size mismatch for model.layers.0.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([52, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 2240]).\r\n        size mismatch for model.layers.0.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([52, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 17920]).. "
  created_at: 2023-04-10 19:38:46+00:00
  edited: false
  hidden: false
  id: 643473d695b8ab049389f5dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-10T22:43:11.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>It works fine for the stable version, you''re just not using the
          correct GPTQ version as stated in his wiki. See <a href="https://huggingface.co/elinas/alpaca-30b-lora-int4#important---update-2023-04-05">https://huggingface.co/elinas/alpaca-30b-lora-int4#important---update-2023-04-05</a></p>

          '
        raw: It works fine for the stable version, you're just not using the correct
          GPTQ version as stated in his wiki. See https://huggingface.co/elinas/alpaca-30b-lora-int4#important---update-2023-04-05
        updatedAt: '2023-04-10T22:43:11.144Z'
      numEdits: 0
      reactions: []
    id: 643490ffd12a239d72e70f27
    type: comment
  author: elinas
  content: It works fine for the stable version, you're just not using the correct
    GPTQ version as stated in his wiki. See https://huggingface.co/elinas/alpaca-30b-lora-int4#important---update-2023-04-05
  created_at: 2023-04-10 21:43:11+00:00
  edited: false
  hidden: false
  id: 643490ffd12a239d72e70f27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
      fullname: Vladimir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vdruts
      type: user
    createdAt: '2023-04-10T22:51:03.000Z'
    data:
      edited: false
      editors:
      - vdruts
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
          fullname: Vladimir
          isHf: false
          isPro: false
          name: vdruts
          type: user
        html: '<p>He made some recent changes which required the GPTQ version to be
          updated otherwise it didnt work with any other models. All other models
          are working with the latest GPTQ</p>

          '
        raw: He made some recent changes which required the GPTQ version to be updated
          otherwise it didnt work with any other models. All other models are working
          with the latest GPTQ
        updatedAt: '2023-04-10T22:51:03.362Z'
      numEdits: 0
      reactions: []
    id: 643492d72a81c2a48480e21b
    type: comment
  author: vdruts
  content: He made some recent changes which required the GPTQ version to be updated
    otherwise it didnt work with any other models. All other models are working with
    the latest GPTQ
  created_at: 2023-04-10 21:51:03+00:00
  edited: false
  hidden: false
  id: 643492d72a81c2a48480e21b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-10T23:04:52.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>I''m not seeing that, point me to the commit. The only breaking
          changes I''m seeing is a transformers update which requires updating the
          tokenizer - see: <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/issues/931#issuecomment-1501259027">https://github.com/oobabooga/text-generation-webui/issues/931#issuecomment-1501259027</a></p>

          '
        raw: 'I''m not seeing that, point me to the commit. The only breaking changes
          I''m seeing is a transformers update which requires updating the tokenizer
          - see: https://github.com/oobabooga/text-generation-webui/issues/931#issuecomment-1501259027'
        updatedAt: '2023-04-10T23:04:52.521Z'
      numEdits: 0
      reactions: []
    id: 643496141a1ba6b55b1af651
    type: comment
  author: elinas
  content: 'I''m not seeing that, point me to the commit. The only breaking changes
    I''m seeing is a transformers update which requires updating the tokenizer - see:
    https://github.com/oobabooga/text-generation-webui/issues/931#issuecomment-1501259027'
  created_at: 2023-04-10 22:04:52+00:00
  edited: false
  hidden: false
  id: 643496141a1ba6b55b1af651
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
      fullname: Vladimir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vdruts
      type: user
    createdAt: '2023-04-11T00:45:48.000Z'
    data:
      edited: false
      editors:
      - vdruts
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
          fullname: Vladimir
          isHf: false
          isPro: false
          name: vdruts
          type: user
        html: '<p>You''re right. Actually I was on the correct branch of GPTQ, havent
          touched it.</p>

          '
        raw: You're right. Actually I was on the correct branch of GPTQ, havent touched
          it.
        updatedAt: '2023-04-11T00:45:48.930Z'
      numEdits: 0
      reactions: []
    id: 6434adbc1d83dc03c8eb96ff
    type: comment
  author: vdruts
  content: You're right. Actually I was on the correct branch of GPTQ, havent touched
    it.
  created_at: 2023-04-10 23:45:48+00:00
  edited: false
  hidden: false
  id: 6434adbc1d83dc03c8eb96ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
      fullname: Vladimir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vdruts
      type: user
    createdAt: '2023-04-11T13:11:34.000Z'
    data:
      edited: false
      editors:
      - vdruts
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
          fullname: Vladimir
          isHf: false
          isPro: false
          name: vdruts
          type: user
        html: '<p>Confirmed. I''m on Oobas fork of GPTQ. Even pulled it fresh.</p>

          <p>Loading elinas_alpaca-30b-lora-int4...<br>Found the following quantized
          model: models\elinas_alpaca-30b-lora-int4\alpaca-30b-4bit-128g.safetensors<br>Loading
          model ...<br>C:\Users\xxxx\Deep\text-diffusion-webui\installer_files\env\lib\site-packages\safetensors\torch.py:99:
          UserWarning: TypedStorage is deprecated. It will be removed in the future
          and UntypedStorage will be the only storage class. This should only matter
          to you if you are using storages directly.  To access UntypedStorage directly,
          use tensor.untyped_storage() instead of tensor.storage()<br>  with safe_open(filename,
          framework="pt", device=device) as f:<br>C:\Users\xxxx\Deep\text-diffusion-webui\installer_files\env\lib\site-packages\torch_utils.py:776:
          UserWarning: TypedStorage is deprecated. It will be removed in the future
          and UntypedStorage will be the only storage class. This should only matter
          to you if you are using storages directly.  To access UntypedStorage directly,
          use tensor.untyped_storage() instead of tensor.storage()<br>  return self.fget.<strong>get</strong>(instance,
          owner)()<br>C:\Users\xxxx\Deep\text-diffusion-webui\installer_files\env\lib\site-packages\torch\storage.py:899:
          UserWarning: TypedStorage is deprecated. It will be removed in the future
          and UntypedStorage will be the only storage class. This should only matter
          to you if you are using storages directly.  To access UntypedStorage directly,
          use tensor.untyped_storage() instead of tensor.storage()<br>  storage =
          cls(wrap_storage=untyped_storage)</p>

          <p>Traceback (most recent call last):<br>  File "C:\Users\xxxx\Deep\text-diffusion-webui\text-generation-webui\server.py",
          line 347, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "C:\Users\xxxx\Deep\text-diffusion-webui\text-generation-webui\modules\models.py",
          line 103, in load_model<br>    model = load_quantized(model_name)<br>  File
          "C:\Users\xxxx\Deep\text-diffusion-webui\text-generation-webui\modules\GPTQ_loader.py",
          line 136, in load_quantized<br>    model = load_quant(str(path_to_model),
          str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)<br>  File
          "C:\Users\xxxx\Deep\text-diffusion-webui\text-generation-webui\modules\GPTQ_loader.py",
          line 61, in _load_quant<br>    model.load_state_dict(safe_load(checkpoint),
          strict=False)<br>  File "C:\Users\xxxx\Deep\text-diffusion-webui\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 2041, in load_state_dict<br>    raise RuntimeError(''Error(s) in loading
          state_dict for {}:\n\t{}''.format(<br>RuntimeError: Error(s) in loading
          state_dict for LlamaForCausalLM:<br>        size mismatch for model.layers.0.self_attn.k_proj.qzeros:
          copying a param with shape torch.Size([52, 832]) from checkpoint, the shape
          in current model is torch.Size([1, 832]).<br>        size mismatch for model.layers.0.self_attn.k_proj.scales:
          copying a param with shape torch.Size([52, 6656]) from checkpoint, the shape
          in current model is torch.Size([1, 6656]).<br>        size mismatch for
          model.layers.0.self_attn.o_proj.qzeros: copying a param with shape torch.Size([52,
          832]) from checkpoint, the shape in current model is torch.Size([1, 832]).<br>        size
          mismatch for model.layers.0.self_attn.o_proj.scales: copying a param with
          shape torch.Size([52, 6656]) from checkpoint, the shape in current model
          is torch.Size([1, 6656]).<br>        size mismatch for model.layers.0.self_attn.q_proj.qzeros:
          copying a param with shape torch.Size([52, 832]) from checkpoint, the shape
          in current model is torch.Size([1, 832]).</p>

          '
        raw: "Confirmed. I'm on Oobas fork of GPTQ. Even pulled it fresh.\n\n\nLoading\
          \ elinas_alpaca-30b-lora-int4...\nFound the following quantized model: models\\\
          elinas_alpaca-30b-lora-int4\\alpaca-30b-4bit-128g.safetensors\nLoading model\
          \ ...\nC:\\Users\\xxxx\\Deep\\text-diffusion-webui\\installer_files\\env\\\
          lib\\site-packages\\safetensors\\torch.py:99: UserWarning: TypedStorage\
          \ is deprecated. It will be removed in the future and UntypedStorage will\
          \ be the only storage class. This should only matter to you if you are using\
          \ storages directly.  To access UntypedStorage directly, use tensor.untyped_storage()\
          \ instead of tensor.storage()\n  with safe_open(filename, framework=\"pt\"\
          , device=device) as f:\nC:\\Users\\xxxx\\Deep\\text-diffusion-webui\\installer_files\\\
          env\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage\
          \ is deprecated. It will be removed in the future and UntypedStorage will\
          \ be the only storage class. This should only matter to you if you are using\
          \ storages directly.  To access UntypedStorage directly, use tensor.untyped_storage()\
          \ instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n\
          C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\installer_files\\env\\lib\\\
          site-packages\\torch\\storage.py:899: UserWarning: TypedStorage is deprecated.\
          \ It will be removed in the future and UntypedStorage will be the only storage\
          \ class. This should only matter to you if you are using storages directly.\
          \  To access UntypedStorage directly, use tensor.untyped_storage() instead\
          \ of tensor.storage()\n  storage = cls(wrap_storage=untyped_storage)\n\n\
          Traceback (most recent call last):\n  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\\
          text-generation-webui\\server.py\", line 347, in <module>\n    shared.model,\
          \ shared.tokenizer = load_model(shared.model_name)\n  File \"C:\\Users\\\
          xxxx\\Deep\\text-diffusion-webui\\text-generation-webui\\modules\\models.py\"\
          , line 103, in load_model\n    model = load_quantized(model_name)\n  File\
          \ \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\text-generation-webui\\\
          modules\\GPTQ_loader.py\", line 136, in load_quantized\n    model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          \  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\text-generation-webui\\\
          modules\\GPTQ_loader.py\", line 61, in _load_quant\n    model.load_state_dict(safe_load(checkpoint),\
          \ strict=False)\n  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading\
          \ state_dict for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading\
          \ state_dict for LlamaForCausalLM:\n        size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([52, 832]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 832]).\n        size mismatch\
          \ for model.layers.0.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([52, 6656]) from checkpoint, the shape in current model is\
          \ torch.Size([1, 6656]).\n        size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
          \ copying a param with shape torch.Size([52, 832]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 832]).\n        size mismatch\
          \ for model.layers.0.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([52, 6656]) from checkpoint, the shape in current model is\
          \ torch.Size([1, 6656]).\n        size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([52, 832]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 832])."
        updatedAt: '2023-04-11T13:11:34.541Z'
      numEdits: 0
      reactions: []
    id: 64355c86adb7b87154e6f623
    type: comment
  author: vdruts
  content: "Confirmed. I'm on Oobas fork of GPTQ. Even pulled it fresh.\n\n\nLoading\
    \ elinas_alpaca-30b-lora-int4...\nFound the following quantized model: models\\\
    elinas_alpaca-30b-lora-int4\\alpaca-30b-4bit-128g.safetensors\nLoading model ...\n\
    C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\installer_files\\env\\lib\\site-packages\\\
    safetensors\\torch.py:99: UserWarning: TypedStorage is deprecated. It will be\
    \ removed in the future and UntypedStorage will be the only storage class. This\
    \ should only matter to you if you are using storages directly.  To access UntypedStorage\
    \ directly, use tensor.untyped_storage() instead of tensor.storage()\n  with safe_open(filename,\
    \ framework=\"pt\", device=device) as f:\nC:\\Users\\xxxx\\Deep\\text-diffusion-webui\\\
    installer_files\\env\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage\
    \ is deprecated. It will be removed in the future and UntypedStorage will be the\
    \ only storage class. This should only matter to you if you are using storages\
    \ directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead\
    \ of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nC:\\Users\\\
    xxxx\\Deep\\text-diffusion-webui\\installer_files\\env\\lib\\site-packages\\torch\\\
    storage.py:899: UserWarning: TypedStorage is deprecated. It will be removed in\
    \ the future and UntypedStorage will be the only storage class. This should only\
    \ matter to you if you are using storages directly.  To access UntypedStorage\
    \ directly, use tensor.untyped_storage() instead of tensor.storage()\n  storage\
    \ = cls(wrap_storage=untyped_storage)\n\nTraceback (most recent call last):\n\
    \  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\text-generation-webui\\\
    server.py\", line 347, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    \  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\text-generation-webui\\\
    modules\\models.py\", line 103, in load_model\n    model = load_quantized(model_name)\n\
    \  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\text-generation-webui\\\
    modules\\GPTQ_loader.py\", line 136, in load_quantized\n    model = load_quant(str(path_to_model),\
    \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
    \  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\text-generation-webui\\\
    modules\\GPTQ_loader.py\", line 61, in _load_quant\n    model.load_state_dict(safe_load(checkpoint),\
    \ strict=False)\n  File \"C:\\Users\\xxxx\\Deep\\text-diffusion-webui\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 2041, in load_state_dict\n\
    \    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\
    RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\n        size\
    \ mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param with shape\
    \ torch.Size([52, 832]) from checkpoint, the shape in current model is torch.Size([1,\
    \ 832]).\n        size mismatch for model.layers.0.self_attn.k_proj.scales: copying\
    \ a param with shape torch.Size([52, 6656]) from checkpoint, the shape in current\
    \ model is torch.Size([1, 6656]).\n        size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([52, 832]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 832]).\n        size mismatch for model.layers.0.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([52, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 6656]).\n        size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([52, 832]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 832])."
  created_at: 2023-04-11 12:11:34+00:00
  edited: false
  hidden: false
  id: 64355c86adb7b87154e6f623
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
      fullname: Vladimir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vdruts
      type: user
    createdAt: '2023-04-11T13:20:15.000Z'
    data:
      edited: false
      editors:
      - vdruts
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
          fullname: Vladimir
          isHf: false
          isPro: false
          name: vdruts
          type: user
        html: '<p>For the record, I just downloaded and ran your vicuna model with
          the same settings and it runs fine, but this one does not. Thanks for your
          help!</p>

          '
        raw: For the record, I just downloaded and ran your vicuna model with the
          same settings and it runs fine, but this one does not. Thanks for your help!
        updatedAt: '2023-04-11T13:20:15.899Z'
      numEdits: 0
      reactions: []
    id: 64355e8fa9265d9aaf8d7843
    type: comment
  author: vdruts
  content: For the record, I just downloaded and ran your vicuna model with the same
    settings and it runs fine, but this one does not. Thanks for your help!
  created_at: 2023-04-11 12:20:15+00:00
  edited: false
  hidden: false
  id: 64355e8fa9265d9aaf8d7843
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
      fullname: Vladimir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vdruts
      type: user
    createdAt: '2023-04-11T14:25:43.000Z'
    data:
      edited: false
      editors:
      - vdruts
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
          fullname: Vladimir
          isHf: false
          isPro: false
          name: vdruts
          type: user
        html: '<p>Just tried loading this model and it worked even though it threw
          the storage warnings &gt; <a href="https://huggingface.co/MetaIX/Alpaca-30B-Int4-128G-Safetensors/discussions/new">https://huggingface.co/MetaIX/Alpaca-30B-Int4-128G-Safetensors/discussions/new</a></p>

          '
        raw: Just tried loading this model and it worked even though it threw the
          storage warnings > https://huggingface.co/MetaIX/Alpaca-30B-Int4-128G-Safetensors/discussions/new
        updatedAt: '2023-04-11T14:25:43.167Z'
      numEdits: 0
      reactions: []
    id: 64356de73cec2a2f659943a3
    type: comment
  author: vdruts
  content: Just tried loading this model and it worked even though it threw the storage
    warnings > https://huggingface.co/MetaIX/Alpaca-30B-Int4-128G-Safetensors/discussions/new
  created_at: 2023-04-11 13:25:43+00:00
  edited: false
  hidden: false
  id: 64356de73cec2a2f659943a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-11T23:18:06.000Z'
    data:
      status: closed
    id: 6435eaae57c3f4b161f7d016
    type: status-change
  author: elinas
  created_at: 2023-04-11 22:18:06+00:00
  id: 6435eaae57c3f4b161f7d016
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
      fullname: Vladimir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vdruts
      type: user
    createdAt: '2023-04-11T23:22:32.000Z'
    data:
      edited: false
      editors:
      - vdruts
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
          fullname: Vladimir
          isHf: false
          isPro: false
          name: vdruts
          type: user
        html: '<p>I realize you closed this, but this error is still present. I loaded
          another 30B model (which worked).</p>

          '
        raw: I realize you closed this, but this error is still present. I loaded
          another 30B model (which worked).
        updatedAt: '2023-04-11T23:22:32.175Z'
      numEdits: 0
      reactions: []
    id: 6435ebb87e07c5aee238f6a6
    type: comment
  author: vdruts
  content: I realize you closed this, but this error is still present. I loaded another
    30B model (which worked).
  created_at: 2023-04-11 22:22:32+00:00
  edited: false
  hidden: false
  id: 6435ebb87e07c5aee238f6a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-12T18:01:55.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>Vicuna is quantized on the same commit. I can''t think of anything
          else unless you''re using the old <code>.pt</code> file.</p>

          '
        raw: Vicuna is quantized on the same commit. I can't think of anything else
          unless you're using the old `.pt` file.
        updatedAt: '2023-04-12T18:01:55.824Z'
      numEdits: 0
      reactions: []
    id: 6436f213f8962b4332ba3600
    type: comment
  author: elinas
  content: Vicuna is quantized on the same commit. I can't think of anything else
    unless you're using the old `.pt` file.
  created_at: 2023-04-12 17:01:55+00:00
  edited: false
  hidden: false
  id: 6436f213f8962b4332ba3600
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-12T18:01:59.000Z'
    data:
      status: open
    id: 6436f217f0a935edd1cc1852
    type: status-change
  author: elinas
  created_at: 2023-04-12 17:01:59+00:00
  id: 6436f217f0a935edd1cc1852
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
      fullname: Vladimir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vdruts
      type: user
    createdAt: '2023-04-12T18:12:33.000Z'
    data:
      edited: false
      editors:
      - vdruts
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
          fullname: Vladimir
          isHf: false
          isPro: false
          name: vdruts
          type: user
        html: '<p>No I was using the safe tensors.</p>

          <blockquote>

          <p>Alpaca 30B 4-bit working with GPTQ versions used in Oobabooga''s Text
          Generation Webui and KoboldAI. was this one?<br>This one is not vicuna,
          but yeah my vicuna works. No idea what''s going on lol.</p>

          </blockquote>

          '
        raw: 'No I was using the safe tensors.


          > Alpaca 30B 4-bit working with GPTQ versions used in Oobabooga''s Text
          Generation Webui and KoboldAI. was this one?

          This one is not vicuna, but yeah my vicuna works. No idea what''s going
          on lol.'
        updatedAt: '2023-04-12T18:12:33.365Z'
      numEdits: 0
      reactions: []
    id: 6436f491197d3e05c1cbd95a
    type: comment
  author: vdruts
  content: 'No I was using the safe tensors.


    > Alpaca 30B 4-bit working with GPTQ versions used in Oobabooga''s Text Generation
    Webui and KoboldAI. was this one?

    This one is not vicuna, but yeah my vicuna works. No idea what''s going on lol.'
  created_at: 2023-04-12 17:12:33+00:00
  edited: false
  hidden: false
  id: 6436f491197d3e05c1cbd95a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-12T23:42:23.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>These are the last releases of this model. If you cannot get them
          to work. Then wait until I release a better model. </p>

          '
        raw: 'These are the last releases of this model. If you cannot get them to
          work. Then wait until I release a better model. '
        updatedAt: '2023-04-12T23:42:23.041Z'
      numEdits: 0
      reactions: []
      relatedEventId: 643741df1c434dbfbeb21bd9
    id: 643741df1c434dbfbeb21bd8
    type: comment
  author: elinas
  content: 'These are the last releases of this model. If you cannot get them to work.
    Then wait until I release a better model. '
  created_at: 2023-04-12 22:42:23+00:00
  edited: false
  hidden: false
  id: 643741df1c434dbfbeb21bd8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-12T23:42:23.000Z'
    data:
      status: closed
    id: 643741df1c434dbfbeb21bd9
    type: status-change
  author: elinas
  created_at: 2023-04-12 22:42:23+00:00
  id: 643741df1c434dbfbeb21bd9
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
      fullname: Vladimir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vdruts
      type: user
    createdAt: '2023-04-12T23:44:24.000Z'
    data:
      edited: false
      editors:
      - vdruts
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
          fullname: Vladimir
          isHf: false
          isPro: false
          name: vdruts
          type: user
        html: '<p>Thanks for your effort :)</p>

          '
        raw: Thanks for your effort :)
        updatedAt: '2023-04-12T23:44:24.966Z'
      numEdits: 0
      reactions: []
    id: 643742583cb6c4b5707a0327
    type: comment
  author: vdruts
  content: Thanks for your effort :)
  created_at: 2023-04-12 22:44:24+00:00
  edited: false
  hidden: false
  id: 643742583cb6c4b5707a0327
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: elinas/alpaca-30b-lora-int4
repo_type: model
status: closed
target_branch: null
title: No longer works in OOba
