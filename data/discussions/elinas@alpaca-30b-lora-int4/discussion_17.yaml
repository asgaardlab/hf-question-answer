!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Goldenblood56
conflicting_files: null
created_at: 2023-04-21 05:03:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-04-21T06:03:55.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: "<p>Thanks I hope someone can give me guidance.<br>I want to run \"\
          alpaca-30b-4bit-128g.safetensors\" which I think is the best model Alpaca-30b-lora-int4\
          \ right? </p>\n<p>I have RTX 4080 and 64GB of RAM<br>I want to split this\
          \ between GPU and CPU/System Memory if it supports it in Oobabooga.<br>OR\
          \ run it on CPU only if I can't split it. </p>\n<p>I copied the repository\
          \ into ---&gt; text-generation-webui<br>I am using the most up to date version\
          \ of Oobabooga updated via git. </p>\n<p>When I load the  \"alpaca-30b-4bit-128g.safetensors\"\
          \ I can make it start to fill VRAM or System RAM but it always crashes with\
          \ various errors neat end. </p>\n<p>I don't know what to set Model Type\
          \ too? Or Wbits or Groupsize? I assume groupsize 128. I have tried checking\
          \ auto devices or CPU etc. Generally I can't get this to work at all. Various\
          \ errors mostly trace back errors. </p>\n<p>This is not the only kind of\
          \ error I get. </p>\n<p>Traceback (most recent call last):<br>File \u201C\
          F:\\AI2\\oobabooga-windowsBest\\text-generation-webui\\server.py\u201D,\
          \ line 100, in load_model_wrapper<br>shared.model, shared.tokenizer = load_model(shared.model_name)<br>File\
          \ \u201CF:\\AI2\\oobabooga-windowsBest\\text-generation-webui\\modules\\\
          models.py\u201D, line 208, in load_model<br>tokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}/\u201C), clean_up_tokenization_spaces=True)<br>File\
          \ \u201CF:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
          transformers\\tokenization_utils_base.py\u201D, line 1811, in from_pretrained<br>return\
          \ cls.from_pretrained(<br>File \u201CF:\\AI2\\oobabooga-windowsBest\\installer_files\\\
          env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u201D\
          , line 1965, in from_pretrained<br>tokenizer = cls(*init_inputs, **init_kwargs)<br>File\
          \ \u201CF:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\tokenization_llama.py\u201D, line 96, in init<br>self.sp_model.Load(vocab_file)<br>File\
          \ \"F:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
          sentencepiece_init.py\", line 905, in Load<br>return self.LoadFromFile(model_file)<br>File\
          \ \"F:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
          sentencepiece_init.py\u201D, line 310, in LoadFromFile<br>return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)<br>TypeError: not a string</p>\n"
        raw: "Thanks I hope someone can give me guidance. \nI want to run \"alpaca-30b-4bit-128g.safetensors\"\
          \ which I think is the best model Alpaca-30b-lora-int4 right? \n\nI have\
          \ RTX 4080 and 64GB of RAM\nI want to split this between GPU and CPU/System\
          \ Memory if it supports it in Oobabooga. \nOR run it on CPU only if I can't\
          \ split it. \n\nI copied the repository into ---> text-generation-webui\n\
          I am using the most up to date version of Oobabooga updated via git. \n\n\
          When I load the  \"alpaca-30b-4bit-128g.safetensors\" I can make it start\
          \ to fill VRAM or System RAM but it always crashes with various errors neat\
          \ end. \n\nI don't know what to set Model Type too? Or Wbits or Groupsize?\
          \ I assume groupsize 128. I have tried checking auto devices or CPU etc.\
          \ Generally I can't get this to work at all. Various errors mostly trace\
          \ back errors. \n\nThis is not the only kind of error I get. \n\nTraceback\
          \ (most recent call last):\nFile \u201CF:\\AI2\\oobabooga-windowsBest\\\
          text-generation-webui\\server.py\u201D, line 100, in load_model_wrapper\n\
          shared.model, shared.tokenizer = load_model(shared.model_name)\nFile \u201C\
          F:\\AI2\\oobabooga-windowsBest\\text-generation-webui\\modules\\models.py\u201D\
          , line 208, in load_model\ntokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}/\u201C), clean_up_tokenization_spaces=True)\n\
          File \u201CF:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
          transformers\\tokenization_utils_base.py\u201D, line 1811, in from_pretrained\n\
          return cls.from_pretrained(\nFile \u201CF:\\AI2\\oobabooga-windowsBest\\\
          installer_files\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u201D\
          , line 1965, in from_pretrained\ntokenizer = cls(*init_inputs, **init_kwargs)\n\
          File \u201CF:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\tokenization_llama.py\u201D, line 96, in init\n\
          self.sp_model.Load(vocab_file)\nFile \"F:\\AI2\\oobabooga-windowsBest\\\
          installer_files\\env\\lib\\site-packages\\sentencepiece_init.py\", line\
          \ 905, in Load\nreturn self.LoadFromFile(model_file)\nFile \"F:\\AI2\\oobabooga-windowsBest\\\
          installer_files\\env\\lib\\site-packages\\sentencepiece_init.py\u201D, line\
          \ 310, in LoadFromFile\nreturn _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\nTypeError: not a string"
        updatedAt: '2023-04-21T06:04:37.272Z'
      numEdits: 1
      reactions: []
    id: 6442274bfe2499e2396f5c67
    type: comment
  author: Goldenblood56
  content: "Thanks I hope someone can give me guidance. \nI want to run \"alpaca-30b-4bit-128g.safetensors\"\
    \ which I think is the best model Alpaca-30b-lora-int4 right? \n\nI have RTX 4080\
    \ and 64GB of RAM\nI want to split this between GPU and CPU/System Memory if it\
    \ supports it in Oobabooga. \nOR run it on CPU only if I can't split it. \n\n\
    I copied the repository into ---> text-generation-webui\nI am using the most up\
    \ to date version of Oobabooga updated via git. \n\nWhen I load the  \"alpaca-30b-4bit-128g.safetensors\"\
    \ I can make it start to fill VRAM or System RAM but it always crashes with various\
    \ errors neat end. \n\nI don't know what to set Model Type too? Or Wbits or Groupsize?\
    \ I assume groupsize 128. I have tried checking auto devices or CPU etc. Generally\
    \ I can't get this to work at all. Various errors mostly trace back errors. \n\
    \nThis is not the only kind of error I get. \n\nTraceback (most recent call last):\n\
    File \u201CF:\\AI2\\oobabooga-windowsBest\\text-generation-webui\\server.py\u201D\
    , line 100, in load_model_wrapper\nshared.model, shared.tokenizer = load_model(shared.model_name)\n\
    File \u201CF:\\AI2\\oobabooga-windowsBest\\text-generation-webui\\modules\\models.py\u201D\
    , line 208, in load_model\ntokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
    {shared.args.model_dir}/{model_name}/\u201C), clean_up_tokenization_spaces=True)\n\
    File \u201CF:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
    transformers\\tokenization_utils_base.py\u201D, line 1811, in from_pretrained\n\
    return cls.from_pretrained(\nFile \u201CF:\\AI2\\oobabooga-windowsBest\\installer_files\\\
    env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u201D, line\
    \ 1965, in from_pretrained\ntokenizer = cls(*init_inputs, **init_kwargs)\nFile\
    \ \u201CF:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\tokenization_llama.py\u201D, line 96, in init\nself.sp_model.Load(vocab_file)\n\
    File \"F:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
    sentencepiece_init.py\", line 905, in Load\nreturn self.LoadFromFile(model_file)\n\
    File \"F:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
    sentencepiece_init.py\u201D, line 310, in LoadFromFile\nreturn _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\nTypeError: not a string"
  created_at: 2023-04-21 05:03:55+00:00
  edited: true
  hidden: false
  id: 6442274bfe2499e2396f5c67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-21T22:25:12.000Z'
    data:
      edited: true
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p><code>python server.py --model model-folder-name --wbits 4 --groupsize
          128 --model_type llama</code></p>

          <p>Remove the <code>--groupsize</code> parameter if you''re using the non-grouped
          version.</p>

          '
        raw: '`python server.py --model model-folder-name --wbits 4 --groupsize 128
          --model_type llama`


          Remove the `--groupsize` parameter if you''re using the non-grouped version.'
        updatedAt: '2023-04-21T22:25:46.336Z'
      numEdits: 1
      reactions: []
    id: 64430d488569978432ff8420
    type: comment
  author: elinas
  content: '`python server.py --model model-folder-name --wbits 4 --groupsize 128
    --model_type llama`


    Remove the `--groupsize` parameter if you''re using the non-grouped version.'
  created_at: 2023-04-21 21:25:12+00:00
  edited: true
  hidden: false
  id: 64430d488569978432ff8420
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-04-21T22:33:50.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Thank you Elinas.<br>I just tried what you suggested.</p>

          <p>In edit for Start-webui my arguments command line is.</p>

          <p>python server.py --model alpaca-30b-lora-int4 --wbits 4 --groupsize 128
          --model_type llama</p>

          <p>When I boot I get this error in the CMD/command window</p>

          <p>Starting the web UI...<br>Gradio HTTP request redirected to localhost
          :)<br>Loading alpaca-30b-lora-int4...<br>Found the following quantized model:
          models\alpaca-30b-lora-int4\alpaca-30b-4bit-128g.safetensors<br>Loading
          model ...<br>Done.<br>Traceback (most recent call last):<br>  File "F:\AI2\oobabooga-windowsBest\text-generation-webui\server.py",
          line 917, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "F:\AI2\oobabooga-windowsBest\text-generation-webui\modules\models.py",
          line 208, in load_model<br>    tokenizer = LlamaTokenizer.from_pretrained(Path(f"{shared.args.model_dir}/{model_name}/"),
          clean_up_tokenization_spaces=True)<br>  File "F:\AI2\oobabooga-windowsBest\installer_files\env\lib\site-packages\transformers\tokenization_utils_base.py",
          line 1811, in from_pretrained<br>    return cls.<em>from_pretrained(<br>  File
          "F:\AI2\oobabooga-windowsBest\installer_files\env\lib\site-packages\transformers\tokenization_utils_base.py",
          line 1965, in <em>from_pretrained<br>    tokenizer = cls(*init_inputs, **init_kwargs)<br>  File
          "F:\AI2\oobabooga-windowsBest\installer_files\env\lib\site-packages\transformers\models\llama\tokenization_llama.py",
          line 96, in <strong>init</strong><br>    self.sp_model.Load(vocab_file)<br>  File
          "F:\AI2\oobabooga-windowsBest\installer_files\env\lib\site-packages\sentencepiece_<em>init</em></em>.py",
          line 905, in Load<br>    return self.LoadFromFile(model_file)<br>  File
          "F:\AI2\oobabooga-windowsBest\installer_files\env\lib\site-packages\sentencepiece_<em>init</em></em>.py",
          line 310, in LoadFromFile<br>    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,
          arg)<br>TypeError: not a string<br>Press any key to continue . . .</p>

          <p>And of course if I click to continue it closes? </p>

          <p>Any ideas?</p>

          <p>Here is a small update. I was even given advice to try adding layers
          into my command line.<br>My command line is<br>python server.py --auto-devices
          --chat --pre_layer 31 --model alpaca-30b-lora-int4 --wbits 4 --groupsize
          128 --model_type llama<br>But I have tried with and without --auto-devices
          and I have changed --pre_layer from like 10,20,30,40,50 etc.<br>But I still
          get the same error as above.</p>

          '
        raw: "Thank you Elinas. \nI just tried what you suggested.\n\nIn edit for\
          \ Start-webui my arguments command line is.\n\npython server.py --model\
          \ alpaca-30b-lora-int4 --wbits 4 --groupsize 128 --model_type llama\n\n\
          When I boot I get this error in the CMD/command window\n\nStarting the web\
          \ UI...\nGradio HTTP request redirected to localhost :)\nLoading alpaca-30b-lora-int4...\n\
          Found the following quantized model: models\\alpaca-30b-lora-int4\\alpaca-30b-4bit-128g.safetensors\n\
          Loading model ...\nDone.\nTraceback (most recent call last):\n  File \"\
          F:\\AI2\\oobabooga-windowsBest\\text-generation-webui\\server.py\", line\
          \ 917, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"F:\\AI2\\oobabooga-windowsBest\\text-generation-webui\\modules\\\
          models.py\", line 208, in load_model\n    tokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}/\"), clean_up_tokenization_spaces=True)\n\
          \  File \"F:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
          transformers\\tokenization_utils_base.py\", line 1811, in from_pretrained\n\
          \    return cls._from_pretrained(\n  File \"F:\\AI2\\oobabooga-windowsBest\\\
          installer_files\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\"\
          , line 1965, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
          \  File \"F:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\tokenization_llama.py\", line 96, in __init__\n\
          \    self.sp_model.Load(vocab_file)\n  File \"F:\\AI2\\oobabooga-windowsBest\\\
          installer_files\\env\\lib\\site-packages\\sentencepiece\\__init__.py\",\
          \ line 905, in Load\n    return self.LoadFromFile(model_file)\n  File \"\
          F:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
          sentencepiece\\__init__.py\", line 310, in LoadFromFile\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\nTypeError: not a string\nPress any key to continue . . .\n\nAnd\
          \ of course if I click to continue it closes? \n\nAny ideas?\n\nHere is\
          \ a small update. I was even given advice to try adding layers into my command\
          \ line. \nMy command line is\npython server.py --auto-devices --chat --pre_layer\
          \ 31 --model alpaca-30b-lora-int4 --wbits 4 --groupsize 128 --model_type\
          \ llama\nBut I have tried with and without --auto-devices and I have changed\
          \ --pre_layer from like 10,20,30,40,50 etc.\nBut I still get the same error\
          \ as above."
        updatedAt: '2023-04-22T19:45:55.141Z'
      numEdits: 2
      reactions: []
    id: 64430f4ef8b647fa4f580e6f
    type: comment
  author: Goldenblood56
  content: "Thank you Elinas. \nI just tried what you suggested.\n\nIn edit for Start-webui\
    \ my arguments command line is.\n\npython server.py --model alpaca-30b-lora-int4\
    \ --wbits 4 --groupsize 128 --model_type llama\n\nWhen I boot I get this error\
    \ in the CMD/command window\n\nStarting the web UI...\nGradio HTTP request redirected\
    \ to localhost :)\nLoading alpaca-30b-lora-int4...\nFound the following quantized\
    \ model: models\\alpaca-30b-lora-int4\\alpaca-30b-4bit-128g.safetensors\nLoading\
    \ model ...\nDone.\nTraceback (most recent call last):\n  File \"F:\\AI2\\oobabooga-windowsBest\\\
    text-generation-webui\\server.py\", line 917, in <module>\n    shared.model, shared.tokenizer\
    \ = load_model(shared.model_name)\n  File \"F:\\AI2\\oobabooga-windowsBest\\text-generation-webui\\\
    modules\\models.py\", line 208, in load_model\n    tokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
    {shared.args.model_dir}/{model_name}/\"), clean_up_tokenization_spaces=True)\n\
    \  File \"F:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
    transformers\\tokenization_utils_base.py\", line 1811, in from_pretrained\n  \
    \  return cls._from_pretrained(\n  File \"F:\\AI2\\oobabooga-windowsBest\\installer_files\\\
    env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 1965,\
    \ in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n  File\
    \ \"F:\\AI2\\oobabooga-windowsBest\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\tokenization_llama.py\", line 96, in __init__\n \
    \   self.sp_model.Load(vocab_file)\n  File \"F:\\AI2\\oobabooga-windowsBest\\\
    installer_files\\env\\lib\\site-packages\\sentencepiece\\__init__.py\", line 905,\
    \ in Load\n    return self.LoadFromFile(model_file)\n  File \"F:\\AI2\\oobabooga-windowsBest\\\
    installer_files\\env\\lib\\site-packages\\sentencepiece\\__init__.py\", line 310,\
    \ in LoadFromFile\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\nTypeError: not a string\nPress any key to continue . . .\n\nAnd of course\
    \ if I click to continue it closes? \n\nAny ideas?\n\nHere is a small update.\
    \ I was even given advice to try adding layers into my command line. \nMy command\
    \ line is\npython server.py --auto-devices --chat --pre_layer 31 --model alpaca-30b-lora-int4\
    \ --wbits 4 --groupsize 128 --model_type llama\nBut I have tried with and without\
    \ --auto-devices and I have changed --pre_layer from like 10,20,30,40,50 etc.\n\
    But I still get the same error as above."
  created_at: 2023-04-21 21:33:50+00:00
  edited: true
  hidden: false
  id: 64430f4ef8b647fa4f580e6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-22T20:18:21.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>What transformers version are you running? These quantizations are
          only compatible with <code>4.27.0.dev0</code></p>

          '
        raw: What transformers version are you running? These quantizations are only
          compatible with `4.27.0.dev0`
        updatedAt: '2023-04-22T20:18:21.848Z'
      numEdits: 0
      reactions: []
    id: 6444410d5298d19c9c036cd7
    type: comment
  author: elinas
  content: What transformers version are you running? These quantizations are only
    compatible with `4.27.0.dev0`
  created_at: 2023-04-22 19:18:21+00:00
  edited: false
  hidden: false
  id: 6444410d5298d19c9c036cd7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-04-22T20:52:09.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>I installed oobabooga and then updated it I got several other models
          working. I don''t know what transformer are. And I don''t know anything
          about "These quantizations are only compatible with 4.27.0.dev0" I have
          no idea what that means.<br>If it''s to difficult to help me that is okay.
          If there is better software than oobabooga let me know if you recommend
          that. Thanks Elinas.</p>

          '
        raw: "I installed oobabooga and then updated it I got several other models\
          \ working. I don't know what transformer are. And I don't know anything\
          \ about \"These quantizations are only compatible with 4.27.0.dev0\" I have\
          \ no idea what that means. \nIf it's to difficult to help me that is okay.\
          \ If there is better software than oobabooga let me know if you recommend\
          \ that. Thanks Elinas."
        updatedAt: '2023-04-22T20:52:59.050Z'
      numEdits: 1
      reactions: []
    id: 644448f98f795c936d055a93
    type: comment
  author: Goldenblood56
  content: "I installed oobabooga and then updated it I got several other models working.\
    \ I don't know what transformer are. And I don't know anything about \"These quantizations\
    \ are only compatible with 4.27.0.dev0\" I have no idea what that means. \nIf\
    \ it's to difficult to help me that is okay. If there is better software than\
    \ oobabooga let me know if you recommend that. Thanks Elinas."
  created_at: 2023-04-22 19:52:09+00:00
  edited: true
  hidden: false
  id: 644448f98f795c936d055a93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-22T21:22:51.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>I prefer Kobold AI for storywriting + connecting it with TavernAI/SillyTavern
          for chatbots (as a frontend). Though it''s a fork for 4-bit support right
          now, not in the official branch <a rel="nofollow" href="https://github.com/0cc4m/KoboldAI">https://github.com/0cc4m/KoboldAI</a></p>

          <p>Ooba updated to 4.28.0 I believe which broke older models. You can check
          your version by doing <code>pip list</code> then looking for a <code>transformers</code>
          package. Since it looks like you''re using Windows, make sure to run that
          in the minniconda bat file that launches the environment, or activate the
          environment if you installed it some other way.</p>

          '
        raw: 'I prefer Kobold AI for storywriting + connecting it with TavernAI/SillyTavern
          for chatbots (as a frontend). Though it''s a fork for 4-bit support right
          now, not in the official branch https://github.com/0cc4m/KoboldAI


          Ooba updated to 4.28.0 I believe which broke older models. You can check
          your version by doing `pip list` then looking for a `transformers` package.
          Since it looks like you''re using Windows, make sure to run that in the
          minniconda bat file that launches the environment, or activate the environment
          if you installed it some other way.'
        updatedAt: '2023-04-22T21:22:51.422Z'
      numEdits: 0
      reactions: []
    id: 6444502bc63001ae635682e3
    type: comment
  author: elinas
  content: 'I prefer Kobold AI for storywriting + connecting it with TavernAI/SillyTavern
    for chatbots (as a frontend). Though it''s a fork for 4-bit support right now,
    not in the official branch https://github.com/0cc4m/KoboldAI


    Ooba updated to 4.28.0 I believe which broke older models. You can check your
    version by doing `pip list` then looking for a `transformers` package. Since it
    looks like you''re using Windows, make sure to run that in the minniconda bat
    file that launches the environment, or activate the environment if you installed
    it some other way.'
  created_at: 2023-04-22 20:22:51+00:00
  edited: false
  hidden: false
  id: 6444502bc63001ae635682e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-04-22T21:37:23.000Z'
    data:
      edited: false
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Thanks. I might give up on this. You are right I am using windows.
          I use git to update. I assume I am running the most recent version. </p>

          <p>I tried cmd and pip list.<br>Package       Version</p>

          <hr>

          <p>numpy         1.24.2<br>opencv-python 4.7.0.68<br>pip           23.0.1<br>setuptools    63.2.0</p>

          <p>I can''t figure out were else to run pip list. But regardless I assume
          I have 4.28. So I am likely out of luck. They need to fix it so old models
          work. Or I need to find a different solution like Kobald AI? I have that
          installed. I might try that later. Anyhow thanks your welcome to close this
          if you want.</p>

          '
        raw: "Thanks. I might give up on this. You are right I am using windows. I\
          \ use git to update. I assume I am running the most recent version. \n\n\
          I tried cmd and pip list. \nPackage       Version\n------------- --------\n\
          numpy         1.24.2\nopencv-python 4.7.0.68\npip           23.0.1\nsetuptools\
          \    63.2.0\n\nI can't figure out were else to run pip list. But regardless\
          \ I assume I have 4.28. So I am likely out of luck. They need to fix it\
          \ so old models work. Or I need to find a different solution like Kobald\
          \ AI? I have that installed. I might try that later. Anyhow thanks your\
          \ welcome to close this if you want."
        updatedAt: '2023-04-22T21:37:23.483Z'
      numEdits: 0
      reactions: []
    id: 644453933dc28377633296bf
    type: comment
  author: Goldenblood56
  content: "Thanks. I might give up on this. You are right I am using windows. I use\
    \ git to update. I assume I am running the most recent version. \n\nI tried cmd\
    \ and pip list. \nPackage       Version\n------------- --------\nnumpy       \
    \  1.24.2\nopencv-python 4.7.0.68\npip           23.0.1\nsetuptools    63.2.0\n\
    \nI can't figure out were else to run pip list. But regardless I assume I have\
    \ 4.28. So I am likely out of luck. They need to fix it so old models work. Or\
    \ I need to find a different solution like Kobald AI? I have that installed. I\
    \ might try that later. Anyhow thanks your welcome to close this if you want."
  created_at: 2023-04-22 20:37:23+00:00
  edited: false
  hidden: false
  id: 644453933dc28377633296bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-22T21:57:26.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>You''re better off using KAI if you want backwards compatibility
          for now. You''ll need to create a new installation using the git repo I
          linked above.</p>

          '
        raw: You're better off using KAI if you want backwards compatibility for now.
          You'll need to create a new installation using the git repo I linked above.
        updatedAt: '2023-04-22T21:57:26.474Z'
      numEdits: 0
      reactions: []
    id: 64445846c63001ae6356f670
    type: comment
  author: elinas
  content: You're better off using KAI if you want backwards compatibility for now.
    You'll need to create a new installation using the git repo I linked above.
  created_at: 2023-04-22 20:57:26+00:00
  edited: false
  hidden: false
  id: 64445846c63001ae6356f670
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-04-22T22:02:41.000Z'
    data:
      edited: false
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Thanks I appreciate it.</p>

          '
        raw: Thanks I appreciate it.
        updatedAt: '2023-04-22T22:02:41.654Z'
      numEdits: 0
      reactions: []
    id: 644459818f795c936d0654f3
    type: comment
  author: Goldenblood56
  content: Thanks I appreciate it.
  created_at: 2023-04-22 21:02:41+00:00
  edited: false
  hidden: false
  id: 644459818f795c936d0654f3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: elinas/alpaca-30b-lora-int4
repo_type: model
status: open
target_branch: null
title: How to get this running on Oobabooga with RTX 4080 16GB?
