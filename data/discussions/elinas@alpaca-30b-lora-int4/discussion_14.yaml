!!python/object:huggingface_hub.community.DiscussionWithDetails
author: donflopez
conflicting_files: null
created_at: 2023-04-06 04:59:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94720fb2d14c527318b1b452fc0933ac.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: donflopez
      type: user
    createdAt: '2023-04-06T05:59:27.000Z'
    data:
      edited: false
      editors:
      - donflopez
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94720fb2d14c527318b1b452fc0933ac.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: donflopez
          type: user
        html: '<p>When I try to run oobabooga with this model I get the error on the
          following error:</p>

          <pre><code>Could not find the quantized model in .pt or .safetensors format,
          exiting...

          </code></pre>

          <p>Command used:</p>

          <pre><code>python server.py --model elinas_alpaca-30b-lora-int4 --wbits
          4 --groupsize 128 --chat --model_type llama --listen

          </code></pre>

          '
        raw: "When I try to run oobabooga with this model I get the error on the following\
          \ error:\r\n\r\n```\r\nCould not find the quantized model in .pt or .safetensors\
          \ format, exiting...\r\n```\r\n\r\nCommand used:\r\n\r\n```\r\npython server.py\
          \ --model elinas_alpaca-30b-lora-int4 --wbits 4 --groupsize 128 --chat --model_type\
          \ llama --listen\r\n```"
        updatedAt: '2023-04-06T05:59:27.116Z'
      numEdits: 0
      reactions: []
    id: 642e5fbfc7d4cd5644ec7f1d
    type: comment
  author: donflopez
  content: "When I try to run oobabooga with this model I get the error on the following\
    \ error:\r\n\r\n```\r\nCould not find the quantized model in .pt or .safetensors\
    \ format, exiting...\r\n```\r\n\r\nCommand used:\r\n\r\n```\r\npython server.py\
    \ --model elinas_alpaca-30b-lora-int4 --wbits 4 --groupsize 128 --chat --model_type\
    \ llama --listen\r\n```"
  created_at: 2023-04-06 04:59:27+00:00
  edited: false
  hidden: false
  id: 642e5fbfc7d4cd5644ec7f1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-06T17:03:36.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: '<p>Please read this, you need to switch to another fork due to breaking
          changes, and this only works on cuda <a href="https://huggingface.co/elinas/alpaca-30b-lora-int4#important---update-2023-04-05">https://huggingface.co/elinas/alpaca-30b-lora-int4#important---update-2023-04-05</a></p>

          '
        raw: Please read this, you need to switch to another fork due to breaking
          changes, and this only works on cuda https://huggingface.co/elinas/alpaca-30b-lora-int4#important---update-2023-04-05
        updatedAt: '2023-04-06T17:03:36.183Z'
      numEdits: 0
      reactions: []
    id: 642efb68654a3f7660feaed8
    type: comment
  author: elinas
  content: Please read this, you need to switch to another fork due to breaking changes,
    and this only works on cuda https://huggingface.co/elinas/alpaca-30b-lora-int4#important---update-2023-04-05
  created_at: 2023-04-06 16:03:36+00:00
  edited: false
  hidden: false
  id: 642efb68654a3f7660feaed8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-12T23:42:48.000Z'
    data:
      status: closed
    id: 643741f85824aa4f5cea63e9
    type: status-change
  author: elinas
  created_at: 2023-04-12 22:42:48+00:00
  id: 643741f85824aa4f5cea63e9
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: elinas/alpaca-30b-lora-int4
repo_type: model
status: closed
target_branch: null
title: Could not find the quantized model in .pt or .safetensors format, exiting...
