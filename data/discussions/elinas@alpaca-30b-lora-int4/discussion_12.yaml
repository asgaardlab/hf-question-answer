!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ColinS97
conflicting_files: null
created_at: 2023-04-02 14:32:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84e26c02fb8cc850585976b29209e439.svg
      fullname: Colin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ColinS97
      type: user
    createdAt: '2023-04-02T15:32:21.000Z'
    data:
      edited: false
      editors:
      - ColinS97
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84e26c02fb8cc850585976b29209e439.svg
          fullname: Colin
          isHf: false
          isPro: false
          name: ColinS97
          type: user
        html: '<p>Hi I have tried to run this repo but I am receiving a state dict
          error: "RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:
          Missing key(s) in state_dict:"</p>

          <p>Here is what I did:</p>

          <ol>

          <li>Clone this repo and clone <a rel="nofollow" href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">https://github.com/qwopqwop200/GPTQ-for-LLaMa</a></li>

          <li>Create a new env and install all dependencies</li>

          <li>Confirmed that GPTQ works (converted a 7B model downloaded from facebook
          to hf and then quantized it with gptq and ran the sample prompt.</li>

          <li>I tried the following two commands to execute the model from this repo
          but both failed:<br>without groupsize:<br>CUDA_VISIBLE_DEVICES=0 python
          llama_inference.py ../alpaca-30b-lora-int4 --wbits 4 --load ../alpaca-30b-lora-int4/alpaca-30b-4bit.safetensors
          --text "this is llama" --device=0</li>

          </ol>

          <p>with groupsize:<br>CUDA_VISIBLE_DEVICES=0 python llama_inference.py ../alpaca-30b-lora-int4
          --wbits 4 --groupsize 128 --load ../alpaca-30b-lora-int4/alpaca-30b-4bit.safetensors
          --text "this is llama" --device=0</p>

          <p>I feel like I would need to convert my 30B Llama Model from Huggingface
          format into another format and put it into ../alpaca-30b-lora-int4 folder.
          So I can use it from GPTQ, but I don''t know how that works with this repo
          and there is no info provided.</p>

          '
        raw: "Hi I have tried to run this repo but I am receiving a state dict error:\
          \ \"RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM: Missing\
          \ key(s) in state_dict:\"\r\n\r\nHere is what I did:\r\n1. Clone this repo\
          \ and clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\r\n2. Create a\
          \ new env and install all dependencies\r\n3. Confirmed that GPTQ works (converted\
          \ a 7B model downloaded from facebook to hf and then quantized it with gptq\
          \ and ran the sample prompt.\r\n4. I tried the following two commands to\
          \ execute the model from this repo but both failed:\r\nwithout groupsize:\r\
          \nCUDA_VISIBLE_DEVICES=0 python llama_inference.py ../alpaca-30b-lora-int4\
          \ --wbits 4 --load ../alpaca-30b-lora-int4/alpaca-30b-4bit.safetensors --text\
          \ \"this is llama\" --device=0\r\n\r\nwith groupsize:\r\nCUDA_VISIBLE_DEVICES=0\
          \ python llama_inference.py ../alpaca-30b-lora-int4 --wbits 4 --groupsize\
          \ 128 --load ../alpaca-30b-lora-int4/alpaca-30b-4bit.safetensors --text\
          \ \"this is llama\" --device=0\r\n\r\n\r\nI feel like I would need to convert\
          \ my 30B Llama Model from Huggingface format into another format and put\
          \ it into ../alpaca-30b-lora-int4 folder. So I can use it from GPTQ, but\
          \ I don't know how that works with this repo and there is no info provided.\r\
          \n\r\n"
        updatedAt: '2023-04-02T15:32:21.659Z'
      numEdits: 0
      reactions: []
    id: 6429a0055673845d98517b0f
    type: comment
  author: ColinS97
  content: "Hi I have tried to run this repo but I am receiving a state dict error:\
    \ \"RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM: Missing\
    \ key(s) in state_dict:\"\r\n\r\nHere is what I did:\r\n1. Clone this repo and\
    \ clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\r\n2. Create a new env and\
    \ install all dependencies\r\n3. Confirmed that GPTQ works (converted a 7B model\
    \ downloaded from facebook to hf and then quantized it with gptq and ran the sample\
    \ prompt.\r\n4. I tried the following two commands to execute the model from this\
    \ repo but both failed:\r\nwithout groupsize:\r\nCUDA_VISIBLE_DEVICES=0 python\
    \ llama_inference.py ../alpaca-30b-lora-int4 --wbits 4 --load ../alpaca-30b-lora-int4/alpaca-30b-4bit.safetensors\
    \ --text \"this is llama\" --device=0\r\n\r\nwith groupsize:\r\nCUDA_VISIBLE_DEVICES=0\
    \ python llama_inference.py ../alpaca-30b-lora-int4 --wbits 4 --groupsize 128\
    \ --load ../alpaca-30b-lora-int4/alpaca-30b-4bit.safetensors --text \"this is\
    \ llama\" --device=0\r\n\r\n\r\nI feel like I would need to convert my 30B Llama\
    \ Model from Huggingface format into another format and put it into ../alpaca-30b-lora-int4\
    \ folder. So I can use it from GPTQ, but I don't know how that works with this\
    \ repo and there is no info provided.\r\n\r\n"
  created_at: 2023-04-02 14:32:21+00:00
  edited: false
  hidden: false
  id: 6429a0055673845d98517b0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-02T16:57:07.000Z'
    data:
      edited: false
      editors:
      - elinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
          fullname: elinas
          isHf: false
          isPro: false
          name: elinas
          type: user
        html: "<p>Since this is related to GPTQ and since you seem to be quantizing\
          \ your own model, it\u2019s best to ask in their repository.</p>\n"
        raw: "Since this is related to GPTQ and since you seem to be quantizing your\
          \ own model, it\u2019s best to ask in their repository."
        updatedAt: '2023-04-02T16:57:07.190Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6429b3e34059ac9e68387486
    id: 6429b3e34059ac9e68387485
    type: comment
  author: elinas
  content: "Since this is related to GPTQ and since you seem to be quantizing your\
    \ own model, it\u2019s best to ask in their repository."
  created_at: 2023-04-02 15:57:07+00:00
  edited: false
  hidden: false
  id: 6429b3e34059ac9e68387485
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630417380907b9a115c6aa9f/hsmz_dU2AyXe1DWHW7Pvd.png?w=200&h=200&f=face
      fullname: elinas
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: elinas
      type: user
    createdAt: '2023-04-02T16:57:07.000Z'
    data:
      status: closed
    id: 6429b3e34059ac9e68387486
    type: status-change
  author: elinas
  created_at: 2023-04-02 15:57:07+00:00
  id: 6429b3e34059ac9e68387486
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: elinas/alpaca-30b-lora-int4
repo_type: model
status: closed
target_branch: null
title: How to run?
