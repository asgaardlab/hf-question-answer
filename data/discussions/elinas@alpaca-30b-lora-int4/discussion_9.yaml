!!python/object:huggingface_hub.community.DiscussionWithDetails
author: haydenhong
conflicting_files: null
created_at: 2023-03-30 10:51:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a05379ab140073de0281000e1b21f119.svg
      fullname: hhong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: haydenhong
      type: user
    createdAt: '2023-03-30T11:51:35.000Z'
    data:
      edited: false
      editors:
      - haydenhong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a05379ab140073de0281000e1b21f119.svg
          fullname: hhong
          isHf: false
          isPro: false
          name: haydenhong
          type: user
        html: '<p>Thanks again for the great works. Do you have plan to share your
          works with the public? As in how you are able to combine lora layers into
          the llama model and yet able to load the quantized weight into a Llama model.
          The standard "lora" added model would have extra lora-A and lora-B layers
          etc., which would not be able to fit into a Llama model from Transformer
          library, I would think. How is this accomplished if you do not mind sharing?
          Share or not, thanks!</p>

          '
        raw: Thanks again for the great works. Do you have plan to share your works
          with the public? As in how you are able to combine lora layers into the
          llama model and yet able to load the quantized weight into a Llama model.
          The standard "lora" added model would have extra lora-A and lora-B layers
          etc., which would not be able to fit into a Llama model from Transformer
          library, I would think. How is this accomplished if you do not mind sharing?
          Share or not, thanks!
        updatedAt: '2023-03-30T11:51:35.103Z'
      numEdits: 0
      reactions: []
    id: 642577c7d3e6fdf87e465538
    type: comment
  author: haydenhong
  content: Thanks again for the great works. Do you have plan to share your works
    with the public? As in how you are able to combine lora layers into the llama
    model and yet able to load the quantized weight into a Llama model. The standard
    "lora" added model would have extra lora-A and lora-B layers etc., which would
    not be able to fit into a Llama model from Transformer library, I would think.
    How is this accomplished if you do not mind sharing? Share or not, thanks!
  created_at: 2023-03-30 10:51:35+00:00
  edited: false
  hidden: false
  id: 642577c7d3e6fdf87e465538
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a05379ab140073de0281000e1b21f119.svg
      fullname: hhong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: haydenhong
      type: user
    createdAt: '2023-03-30T14:16:24.000Z'
    data:
      status: closed
    id: 642599b8348d0ea085e78014
    type: status-change
  author: haydenhong
  created_at: 2023-03-30 13:16:24+00:00
  id: 642599b8348d0ea085e78014
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: elinas/alpaca-30b-lora-int4
repo_type: model
status: closed
target_branch: null
title: any chance to share the conversion code?
