!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jahhs0n
conflicting_files: null
created_at: 2023-06-29 12:50:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90d7a07083f2b6237d3e2f78ae9a0936.svg
      fullname: jason
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jahhs0n
      type: user
    createdAt: '2023-06-29T13:50:16.000Z'
    data:
      edited: false
      editors:
      - jahhs0n
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7965379953384399
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90d7a07083f2b6237d3e2f78ae9a0936.svg
          fullname: jason
          isHf: false
          isPro: false
          name: jahhs0n
          type: user
        html: '<p>I notice the <code>max_position_embeddings</code> in <code>config.json</code>
          has already been set to 8192, but there are no <code>scale</code> variable
          in <code>config.json</code>, thus it will be default set to 1 as per the
          <code>modelling_llama.py</code> in this repo. Do we need to add a <code>scale</code>
          variable in the <code>config.json</code> and set it to 0.25 for the model
          to work in 8K context?</p>

          '
        raw: I notice the `max_position_embeddings` in `config.json` has already been
          set to 8192, but there are no `scale` variable in `config.json`, thus it
          will be default set to 1 as per the `modelling_llama.py` in this repo. Do
          we need to add a `scale` variable in the `config.json` and set it to 0.25
          for the model to work in 8K context?
        updatedAt: '2023-06-29T13:50:16.876Z'
      numEdits: 0
      reactions: []
    id: 649d8c1865079a8cc70c484d
    type: comment
  author: jahhs0n
  content: I notice the `max_position_embeddings` in `config.json` has already been
    set to 8192, but there are no `scale` variable in `config.json`, thus it will
    be default set to 1 as per the `modelling_llama.py` in this repo. Do we need to
    add a `scale` variable in the `config.json` and set it to 0.25 for the model to
    work in 8K context?
  created_at: 2023-06-29 12:50:16+00:00
  edited: false
  hidden: false
  id: 649d8c1865079a8cc70c484d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-29T14:17:59.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9304271936416626
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah I defaulted the max_position_embeddings to 8192 then  the code
          that''s run by trust_remote_code=True will then set <code>scale</code> to
          4 automatically.  If you edited max positions to 4K, it would use 2 instead.</p>

          <p>So no you don''t need to set a scale param, just edit max_position_embeddings
          according to what you want, and the customised Llama modelling code will
          figure it out.  Just make sure to set <code>trust_remote_code=True</code></p>

          <p>Sorry I should have mentioned that in the fp16 READMEs - it is described
          in my GPTQ readmes but I didn''t put it in the fp16s.</p>

          '
        raw: 'Yeah I defaulted the max_position_embeddings to 8192 then  the code
          that''s run by trust_remote_code=True will then set `scale` to 4 automatically.  If
          you edited max positions to 4K, it would use 2 instead.


          So no you don''t need to set a scale param, just edit max_position_embeddings
          according to what you want, and the customised Llama modelling code will
          figure it out.  Just make sure to set `trust_remote_code=True`


          Sorry I should have mentioned that in the fp16 READMEs - it is described
          in my GPTQ readmes but I didn''t put it in the fp16s.'
        updatedAt: '2023-06-29T14:17:59.763Z'
      numEdits: 0
      reactions: []
    id: 649d92979191bdc238f47d1f
    type: comment
  author: TheBloke
  content: 'Yeah I defaulted the max_position_embeddings to 8192 then  the code that''s
    run by trust_remote_code=True will then set `scale` to 4 automatically.  If you
    edited max positions to 4K, it would use 2 instead.


    So no you don''t need to set a scale param, just edit max_position_embeddings
    according to what you want, and the customised Llama modelling code will figure
    it out.  Just make sure to set `trust_remote_code=True`


    Sorry I should have mentioned that in the fp16 READMEs - it is described in my
    GPTQ readmes but I didn''t put it in the fp16s.'
  created_at: 2023-06-29 13:17:59+00:00
  edited: false
  hidden: false
  id: 649d92979191bdc238f47d1f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90d7a07083f2b6237d3e2f78ae9a0936.svg
      fullname: jason
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jahhs0n
      type: user
    createdAt: '2023-06-29T14:25:26.000Z'
    data:
      edited: false
      editors:
      - jahhs0n
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7428117990493774
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90d7a07083f2b6237d3e2f78ae9a0936.svg
          fullname: jason
          isHf: false
          isPro: false
          name: jahhs0n
          type: user
        html: '<p>No worries, thanks for the clarifications! I just saw the <a href="https://huggingface.co/TheBloke/wizard-vicuna-13B-SuperHOT-8K-fp16/blob/main/modelling_llama.py#L172">code
          implementation</a> where the scale is calculated from the max_position_embeddings.
          </p>

          '
        raw: 'No worries, thanks for the clarifications! I just saw the [code implementation](https://huggingface.co/TheBloke/wizard-vicuna-13B-SuperHOT-8K-fp16/blob/main/modelling_llama.py#L172)
          where the scale is calculated from the max_position_embeddings. '
        updatedAt: '2023-06-29T14:25:26.719Z'
      numEdits: 0
      reactions: []
      relatedEventId: 649d9456b1b4ddc55e95b7ae
    id: 649d9456b1b4ddc55e95b7ad
    type: comment
  author: jahhs0n
  content: 'No worries, thanks for the clarifications! I just saw the [code implementation](https://huggingface.co/TheBloke/wizard-vicuna-13B-SuperHOT-8K-fp16/blob/main/modelling_llama.py#L172)
    where the scale is calculated from the max_position_embeddings. '
  created_at: 2023-06-29 13:25:26+00:00
  edited: false
  hidden: false
  id: 649d9456b1b4ddc55e95b7ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/90d7a07083f2b6237d3e2f78ae9a0936.svg
      fullname: jason
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jahhs0n
      type: user
    createdAt: '2023-06-29T14:25:26.000Z'
    data:
      status: closed
    id: 649d9456b1b4ddc55e95b7ae
    type: status-change
  author: jahhs0n
  created_at: 2023-06-29 13:25:26+00:00
  id: 649d9456b1b4ddc55e95b7ae
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/wizard-vicuna-13B-SuperHOT-8K-fp16
repo_type: model
status: closed
target_branch: null
title: Need to set scale to 0.25 in `config.json`?
