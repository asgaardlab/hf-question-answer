!!python/object:huggingface_hub.community.DiscussionWithDetails
author: iyunya
conflicting_files: null
created_at: 2024-01-23 21:42:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644b6e57b2e7823a76abb7e1/9LyLSebSvptllUZM35G8Y.png?w=200&h=200&f=face
      fullname: iyunya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iyunya
      type: user
    createdAt: '2024-01-23T21:42:38.000Z'
    data:
      edited: false
      editors:
      - iyunya
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7399883270263672
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644b6e57b2e7823a76abb7e1/9LyLSebSvptllUZM35G8Y.png?w=200&h=200&f=face
          fullname: iyunya
          isHf: false
          isPro: false
          name: iyunya
          type: user
        html: '<p>I have a question regarding the memory requirements for loading
          this model. I''m currently working with a system that has 24GB of GPU memory,
          but I''m encountering an ''OutOfMemory'' issue. Could you please advise
          me on the amount of GPU memory needed to successfully load and run this
          model? Any insights or suggestions would be greatly appreciated.</p>

          <p>code:<br>import torch<br>from transformers import AutoModel, AutoTokenizer</p>

          <p>torch.set_default_device("cuda")<br>model_name_of_path=''Yhyu13/LMCocktail-Mistral-7B-v1''</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name_of_path, trust_remote_code=True)<br>model
          = AutoModel.from_pretrained(model_name_of_path, trust_remote_code=True)</p>

          <p>inputs = tokenizer("who are you?", return_tensors="pt", return_attention_mask=False)<br>print(''inputs='',
          inputs)</p>

          <p>outputs = model.generate(**inputs, max_length=200)</p>

          <p>print(''outputs='', outputs)</p>

          <p>text = tokenizer.batch_decode(outputs)[0]<br>print(''text='', text)</p>

          '
        raw: "I have a question regarding the memory requirements for loading this\
          \ model. I'm currently working with a system that has 24GB of GPU memory,\
          \ but I'm encountering an 'OutOfMemory' issue. Could you please advise me\
          \ on the amount of GPU memory needed to successfully load and run this model?\
          \ Any insights or suggestions would be greatly appreciated.\r\n\r\ncode:\r\
          \nimport torch\r\nfrom transformers import AutoModel, AutoTokenizer\r\n\r\
          \ntorch.set_default_device(\"cuda\")\r\nmodel_name_of_path='Yhyu13/LMCocktail-Mistral-7B-v1'\r\
          \n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_of_path, trust_remote_code=True)\r\
          \nmodel = AutoModel.from_pretrained(model_name_of_path, trust_remote_code=True)\r\
          \n\r\ninputs = tokenizer(\"who are you?\", return_tensors=\"pt\", return_attention_mask=False)\r\
          \nprint('inputs=', inputs)\r\n\r\noutputs = model.generate(**inputs, max_length=200)\r\
          \n\r\nprint('outputs=', outputs)\r\n\r\ntext = tokenizer.batch_decode(outputs)[0]\r\
          \nprint('text=', text)\r\n"
        updatedAt: '2024-01-23T21:42:38.454Z'
      numEdits: 0
      reactions: []
    id: 65b032cebe8efb3dd33051a6
    type: comment
  author: iyunya
  content: "I have a question regarding the memory requirements for loading this model.\
    \ I'm currently working with a system that has 24GB of GPU memory, but I'm encountering\
    \ an 'OutOfMemory' issue. Could you please advise me on the amount of GPU memory\
    \ needed to successfully load and run this model? Any insights or suggestions\
    \ would be greatly appreciated.\r\n\r\ncode:\r\nimport torch\r\nfrom transformers\
    \ import AutoModel, AutoTokenizer\r\n\r\ntorch.set_default_device(\"cuda\")\r\n\
    model_name_of_path='Yhyu13/LMCocktail-Mistral-7B-v1'\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_of_path,\
    \ trust_remote_code=True)\r\nmodel = AutoModel.from_pretrained(model_name_of_path,\
    \ trust_remote_code=True)\r\n\r\ninputs = tokenizer(\"who are you?\", return_tensors=\"\
    pt\", return_attention_mask=False)\r\nprint('inputs=', inputs)\r\n\r\noutputs\
    \ = model.generate(**inputs, max_length=200)\r\n\r\nprint('outputs=', outputs)\r\
    \n\r\ntext = tokenizer.batch_decode(outputs)[0]\r\nprint('text=', text)\r\n"
  created_at: 2024-01-23 21:42:38+00:00
  edited: false
  hidden: false
  id: 65b032cebe8efb3dd33051a6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Yhyu13/LMCocktail-Mistral-7B-v1
repo_type: model
status: open
target_branch: null
title: amount of GPU memory needed?
