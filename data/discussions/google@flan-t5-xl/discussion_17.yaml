!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aan64
conflicting_files: null
created_at: 2023-06-01 10:08:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a0c1abfeae5371648951bb5d2272d9a.svg
      fullname: Andreea Anghel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aan64
      type: user
    createdAt: '2023-06-01T11:08:33.000Z'
    data:
      edited: false
      editors:
      - aan64
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a0c1abfeae5371648951bb5d2272d9a.svg
          fullname: Andreea Anghel
          isHf: false
          isPro: false
          name: aan64
          type: user
        html: "<p>Hello,</p>\n<p>I've prompt-tuned the flan-t5-xl model and used it\
          \ to run inference on a tweet dataset:</p>\n<pre><code>model_name_or_path\
          \     = \"google/flan-t5-xl\"  \n\npeft_config = PromptTuningConfig(\n \
          \   task_type=TaskType.CAUSAL_LM, \n    prompt_tuning_init=PromptTuningInit.TEXT,\n\
          \    num_virtual_tokens=16,\n    prompt_tuning_init_text=\"Classify if the\
          \ tweet is a complaint or not:\",\n    tokenizer_name_or_path=model_name_or_path,\n\
          )\n\nmodel = T5ForConditionalGeneration.from_pretrained(model_name_or_path,\
          \ torch_dtype=torch.bfloat16) \nmodel = get_peft_model(model, peft_config)\n\
          \n# fine-tune ...\n\ninputs = tokenizer(\n    f'{text_column} : {\"@nationalgridus\
          \ I have no water and the bill is current and paid. Can you do something\
          \ about this?\"} Label : ',\n    return_tensors=\"pt\",\n)\nprint(inputs)\n\
          \nmodel.to(device)\n\nwith torch.no_grad():\n    inputs = {k: v.to(device)\
          \ for k, v in inputs.items()}\n    print(inputs)\n    outputs = model.generate(\n\
          \        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"\
          ]\n    )\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(),\
          \ skip_special_tokens=True))\n</code></pre>\n<p>The code fails at inference\
          \ time in <code>model.generate</code> with the following error:</p>\n<pre><code>RuntimeError:\
          \ The size of tensor a (32) must match the size of tensor b (48) at non-singleton\
          \ dimension 3\n</code></pre>\n<p>The difference between the b tensor size\
          \ and a tensor size is exactly the number of virtual tokens in the soft\
          \ prompt.<br>To me, this error indicates that the soft prompt is not appended\
          \ to the input tensor?</p>\n<p>Does anybody know what could the issue be\
          \ here? Any feedback is highly appreciated, thanks!</p>\n"
        raw: "Hello,\r\n\r\nI've prompt-tuned the flan-t5-xl model and used it to\
          \ run inference on a tweet dataset:\r\n\r\n```\r\nmodel_name_or_path   \
          \  = \"google/flan-t5-xl\"  \r\n\r\npeft_config = PromptTuningConfig(\r\n\
          \    task_type=TaskType.CAUSAL_LM, \r\n    prompt_tuning_init=PromptTuningInit.TEXT,\r\
          \n    num_virtual_tokens=16,\r\n    prompt_tuning_init_text=\"Classify if\
          \ the tweet is a complaint or not:\",\r\n    tokenizer_name_or_path=model_name_or_path,\r\
          \n)\r\n\r\nmodel = T5ForConditionalGeneration.from_pretrained(model_name_or_path,\
          \ torch_dtype=torch.bfloat16) \r\nmodel = get_peft_model(model, peft_config)\r\
          \n\r\n# fine-tune ...\r\n\r\ninputs = tokenizer(\r\n    f'{text_column}\
          \ : {\"@nationalgridus I have no water and the bill is current and paid.\
          \ Can you do something about this?\"} Label : ',\r\n    return_tensors=\"\
          pt\",\r\n)\r\nprint(inputs)\r\n\r\nmodel.to(device)\r\n\r\nwith torch.no_grad():\r\
          \n    inputs = {k: v.to(device) for k, v in inputs.items()}\r\n    print(inputs)\r\
          \n    outputs = model.generate(\r\n        input_ids=inputs[\"input_ids\"\
          ], attention_mask=inputs[\"attention_mask\"]\r\n    )\r\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(),\
          \ skip_special_tokens=True))\r\n```\r\n\r\nThe code fails at inference time\
          \ in `model.generate` with the following error:\r\n```\r\nRuntimeError:\
          \ The size of tensor a (32) must match the size of tensor b (48) at non-singleton\
          \ dimension 3\r\n```\r\n\r\nThe difference between the b tensor size and\
          \ a tensor size is exactly the number of virtual tokens in the soft prompt.\
          \ \r\nTo me, this error indicates that the soft prompt is not appended to\
          \ the input tensor?\r\n\r\nDoes anybody know what could the issue be here?\
          \ Any feedback is highly appreciated, thanks!"
        updatedAt: '2023-06-01T11:08:33.839Z'
      numEdits: 0
      reactions: []
    id: 64787c318315f8751452beed
    type: comment
  author: aan64
  content: "Hello,\r\n\r\nI've prompt-tuned the flan-t5-xl model and used it to run\
    \ inference on a tweet dataset:\r\n\r\n```\r\nmodel_name_or_path     = \"google/flan-t5-xl\"\
    \  \r\n\r\npeft_config = PromptTuningConfig(\r\n    task_type=TaskType.CAUSAL_LM,\
    \ \r\n    prompt_tuning_init=PromptTuningInit.TEXT,\r\n    num_virtual_tokens=16,\r\
    \n    prompt_tuning_init_text=\"Classify if the tweet is a complaint or not:\"\
    ,\r\n    tokenizer_name_or_path=model_name_or_path,\r\n)\r\n\r\nmodel = T5ForConditionalGeneration.from_pretrained(model_name_or_path,\
    \ torch_dtype=torch.bfloat16) \r\nmodel = get_peft_model(model, peft_config)\r\
    \n\r\n# fine-tune ...\r\n\r\ninputs = tokenizer(\r\n    f'{text_column} : {\"\
    @nationalgridus I have no water and the bill is current and paid. Can you do something\
    \ about this?\"} Label : ',\r\n    return_tensors=\"pt\",\r\n)\r\nprint(inputs)\r\
    \n\r\nmodel.to(device)\r\n\r\nwith torch.no_grad():\r\n    inputs = {k: v.to(device)\
    \ for k, v in inputs.items()}\r\n    print(inputs)\r\n    outputs = model.generate(\r\
    \n        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"\
    ]\r\n    )\r\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(),\
    \ skip_special_tokens=True))\r\n```\r\n\r\nThe code fails at inference time in\
    \ `model.generate` with the following error:\r\n```\r\nRuntimeError: The size\
    \ of tensor a (32) must match the size of tensor b (48) at non-singleton dimension\
    \ 3\r\n```\r\n\r\nThe difference between the b tensor size and a tensor size is\
    \ exactly the number of virtual tokens in the soft prompt. \r\nTo me, this error\
    \ indicates that the soft prompt is not appended to the input tensor?\r\n\r\n\
    Does anybody know what could the issue be here? Any feedback is highly appreciated,\
    \ thanks!"
  created_at: 2023-06-01 10:08:33+00:00
  edited: false
  hidden: false
  id: 64787c318315f8751452beed
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: google/flan-t5-xl
repo_type: model
status: open
target_branch: null
title: Inference of prompt tuned model fails with tensor size mismatch
