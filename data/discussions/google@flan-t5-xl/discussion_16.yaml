!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ashitasaxena
conflicting_files: null
created_at: 2023-05-29 11:49:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4e80a3f753dfaff318e713a50f64e2a1.svg
      fullname: Ashita Saxena
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ashitasaxena
      type: user
    createdAt: '2023-05-29T12:49:55.000Z'
    data:
      edited: false
      editors:
      - Ashitasaxena
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4e80a3f753dfaff318e713a50f64e2a1.svg
          fullname: Ashita Saxena
          isHf: false
          isPro: false
          name: Ashitasaxena
          type: user
        html: '<p>I am new to huggingface. I am using Pytorch for development. I have
          a query. </p>

          <p>The model card for inference looks like this - </p>

          <p>from transformers import T5Tokenizer, T5ForConditionalGeneration</p>

          <p>tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-xl")<br>model
          = T5ForConditionalGeneration.from_pretrained("google/flan-t5-xl", device_map="auto")</p>

          <p>input_text = "translate English to German: How old are you?"<br>input_ids
          = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")</p>

          <p>outputs = model.generate(input_ids)<br>print(tokenizer.decode(outputs[0]))</p>

          <p>If I have a large list consisting of input_texts, how can I give them
          to the model.generate() function? Is there a way to perform this inference
          in batches?<br>Can someone provide code/references for this?</p>

          '
        raw: "I am new to huggingface. I am using Pytorch for development. I have\
          \ a query. \r\n\r\nThe model card for inference looks like this - \r\n\r\
          \nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\r\n\r\
          \ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\r\nmodel\
          \ = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"\
          auto\")\r\n\r\ninput_text = \"translate English to German: How old are you?\"\
          \r\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"\
          cuda\")\r\n\r\noutputs = model.generate(input_ids)\r\nprint(tokenizer.decode(outputs[0]))\r\
          \n\r\nIf I have a large list consisting of input_texts, how can I give them\
          \ to the model.generate() function? Is there a way to perform this inference\
          \ in batches? \r\nCan someone provide code/references for this?"
        updatedAt: '2023-05-29T12:49:55.892Z'
      numEdits: 0
      reactions: []
    id: 64749f73d815855e4ef5269c
    type: comment
  author: Ashitasaxena
  content: "I am new to huggingface. I am using Pytorch for development. I have a\
    \ query. \r\n\r\nThe model card for inference looks like this - \r\n\r\nfrom transformers\
    \ import T5Tokenizer, T5ForConditionalGeneration\r\n\r\ntokenizer = T5Tokenizer.from_pretrained(\"\
    google/flan-t5-xl\")\r\nmodel = T5ForConditionalGeneration.from_pretrained(\"\
    google/flan-t5-xl\", device_map=\"auto\")\r\n\r\ninput_text = \"translate English\
    \ to German: How old are you?\"\r\ninput_ids = tokenizer(input_text, return_tensors=\"\
    pt\").input_ids.to(\"cuda\")\r\n\r\noutputs = model.generate(input_ids)\r\nprint(tokenizer.decode(outputs[0]))\r\
    \n\r\nIf I have a large list consisting of input_texts, how can I give them to\
    \ the model.generate() function? Is there a way to perform this inference in batches?\
    \ \r\nCan someone provide code/references for this?"
  created_at: 2023-05-29 11:49:55+00:00
  edited: false
  hidden: false
  id: 64749f73d815855e4ef5269c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: google/flan-t5-xl
repo_type: model
status: open
target_branch: null
title: 'Giving multiple inputs to model.generate() '
