!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xanthexu
conflicting_files: null
created_at: 2023-08-20 08:12:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/282a9e8b1b8ebc0ab1b9a07a9be9d8ae.svg
      fullname: xanthexu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xanthexu
      type: user
    createdAt: '2023-08-20T09:12:41.000Z'
    data:
      edited: true
      editors:
      - xanthexu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8254650235176086
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/282a9e8b1b8ebc0ab1b9a07a9be9d8ae.svg
          fullname: xanthexu
          isHf: false
          isPro: false
          name: xanthexu
          type: user
        html: '<p>I am trying to repeat some results from the paper . In the supplementary
          Figure 1 , the author provide some useful prompt for tuning Med-PalM . For
          example, here I my code : </p>

          <pre><code>input_text="You are a helpful medical knowledge assistant. Provide
          useful , complete and scientifically-grounded answers to common questions
          about health. Question: How do you treat skin redness ?"

          input_ids = tokenizer(input_text, return_tensors="pt").input_ids


          outputs = model.generate(input_ids)

          print(tokenizer.decode(outputs[0]))

          </code></pre>

          <p>The output only gives a very simple answer below : </p>

          <pre><code>You can treat skin redness with a topical ointment,

          </code></pre>

          <p>Seems like the answer is not complete and too simple. Does anyone know
          how to fix the problem by providing more comprehensive answer ?<br>PS: I
          am setting it up in CPU environment. Does CPU and GPU make a difference
          in terms of the answer ? </p>

          <p>Thanks,</p>

          '
        raw: "I am trying to repeat some results from the paper . In the supplementary\
          \ Figure 1 , the author provide some useful prompt for tuning Med-PalM .\
          \ For example, here I my code : \n```\ninput_text=\"You are a helpful medical\
          \ knowledge assistant. Provide useful , complete and scientifically-grounded\
          \ answers to common questions about health. Question: How do you treat skin\
          \ redness ?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\
          \noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n\
          ```\n\nThe output only gives a very simple answer below : \n```\nYou can\
          \ treat skin redness with a topical ointment,\n``` \n\nSeems like the answer\
          \ is not complete and too simple. Does anyone know how to fix the problem\
          \ by providing more comprehensive answer ? \nPS: I am setting it up in CPU\
          \ environment. Does CPU and GPU make a difference in terms of the answer\
          \ ? \n\n\nThanks,\n"
        updatedAt: '2023-08-20T09:13:57.549Z'
      numEdits: 2
      reactions: []
    id: 64e1d909de27e92beaa11302
    type: comment
  author: xanthexu
  content: "I am trying to repeat some results from the paper . In the supplementary\
    \ Figure 1 , the author provide some useful prompt for tuning Med-PalM . For example,\
    \ here I my code : \n```\ninput_text=\"You are a helpful medical knowledge assistant.\
    \ Provide useful , complete and scientifically-grounded answers to common questions\
    \ about health. Question: How do you treat skin redness ?\"\ninput_ids = tokenizer(input_text,\
    \ return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n\
    ```\n\nThe output only gives a very simple answer below : \n```\nYou can treat\
    \ skin redness with a topical ointment,\n``` \n\nSeems like the answer is not\
    \ complete and too simple. Does anyone know how to fix the problem by providing\
    \ more comprehensive answer ? \nPS: I am setting it up in CPU environment. Does\
    \ CPU and GPU make a difference in terms of the answer ? \n\n\nThanks,\n"
  created_at: 2023-08-20 08:12:41+00:00
  edited: true
  hidden: false
  id: 64e1d909de27e92beaa11302
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-05T09:37:09.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8898568153381348
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;xanthexu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/xanthexu\"\
          >@<span class=\"underline\">xanthexu</span></a></span>\n\n\t</span></span><br>Thanks\
          \ for the issue<br>The prompt you are providing is quite specific to chat\
          \ based models or instruction-fine tuned models. I don't think Flan-T5 is\
          \ adapted for those type of tasks unfortunately. In my experience flan-t5\
          \ models works well for small answers and if you want to generate longer\
          \ answers you might want to try sampling methods. <a href=\"https://huggingface.co/blog/how-to-generate\"\
          >https://huggingface.co/blog/how-to-generate</a></p>\n<p>For prompting general\
          \ questions to a Language Model, I advise you to look into popular causal\
          \ language models such as LLama-2, or different variants of Mistral-7b such\
          \ as:   <a rel=\"nofollow\" href=\"%5Bhttps://huggingface.co/Open-Orca/Mistral-7B-OpenOrca%5D(https://huggingface.co/mistralai/Mistral-7B-v0.1)\"\
          >Mistral-7-v1.0</a> /  <a href=\"https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca\"\
          >Mistral-7B Orca</a><br>I highly recommend the last models as you should\
          \ be able to run them on a free-tier google colab instance, a 7B model contains\
          \ ~16GB of weights in float16 precision but you can easily reduce that requirement\
          \ to 5GB thanks to 4bit quantization. Read more about it here: <a href=\"\
          https://huggingface.co/blog/4bit-transformers-bitsandbytes\">https://huggingface.co/blog/4bit-transformers-bitsandbytes</a></p>\n"
        raw: "Hi @xanthexu \nThanks for the issue\nThe prompt you are providing is\
          \ quite specific to chat based models or instruction-fine tuned models.\
          \ I don't think Flan-T5 is adapted for those type of tasks unfortunately.\
          \ In my experience flan-t5 models works well for small answers and if you\
          \ want to generate longer answers you might want to try sampling methods.\
          \ https://huggingface.co/blog/how-to-generate\n\nFor prompting general questions\
          \ to a Language Model, I advise you to look into popular causal language\
          \ models such as LLama-2, or different variants of Mistral-7b such as: \
          \  [Mistral-7-v1.0]([https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca](https://huggingface.co/mistralai/Mistral-7B-v0.1))\
          \ /  [Mistral-7B Orca](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca)\n\
          I highly recommend the last models as you should be able to run them on\
          \ a free-tier google colab instance, a 7B model contains ~16GB of weights\
          \ in float16 precision but you can easily reduce that requirement to 5GB\
          \ thanks to 4bit quantization. Read more about it here: https://huggingface.co/blog/4bit-transformers-bitsandbytes"
        updatedAt: '2023-10-05T09:37:09.511Z'
      numEdits: 0
      reactions: []
    id: 651e83c5a3f2da3e8df4c558
    type: comment
  author: ybelkada
  content: "Hi @xanthexu \nThanks for the issue\nThe prompt you are providing is quite\
    \ specific to chat based models or instruction-fine tuned models. I don't think\
    \ Flan-T5 is adapted for those type of tasks unfortunately. In my experience flan-t5\
    \ models works well for small answers and if you want to generate longer answers\
    \ you might want to try sampling methods. https://huggingface.co/blog/how-to-generate\n\
    \nFor prompting general questions to a Language Model, I advise you to look into\
    \ popular causal language models such as LLama-2, or different variants of Mistral-7b\
    \ such as:   [Mistral-7-v1.0]([https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca](https://huggingface.co/mistralai/Mistral-7B-v0.1))\
    \ /  [Mistral-7B Orca](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca)\n\
    I highly recommend the last models as you should be able to run them on a free-tier\
    \ google colab instance, a 7B model contains ~16GB of weights in float16 precision\
    \ but you can easily reduce that requirement to 5GB thanks to 4bit quantization.\
    \ Read more about it here: https://huggingface.co/blog/4bit-transformers-bitsandbytes"
  created_at: 2023-10-05 08:37:09+00:00
  edited: false
  hidden: false
  id: 651e83c5a3f2da3e8df4c558
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: google/flan-t5-xl
repo_type: model
status: open
target_branch: null
title: 'Can flan-t5-xl provide a more complete answer ? '
