!!python/object:huggingface_hub.community.DiscussionWithDetails
author: anhnh2002
conflicting_files: []
created_at: 2023-11-21 04:53:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6530a1c9f145530101e84b45/PHdHXW-XuX9_kQTPum21R.jpeg?w=200&h=200&f=face
      fullname: "Nguy\u1EC5n Ho\xE0ng Anh"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: anhnh2002
      type: user
    createdAt: '2023-11-21T04:53:41.000Z'
    data:
      edited: false
      editors:
      - anhnh2002
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.11044929921627045
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6530a1c9f145530101e84b45/PHdHXW-XuX9_kQTPum21R.jpeg?w=200&h=200&f=face
          fullname: "Nguy\u1EC5n Ho\xE0ng Anh"
          isHf: false
          isPro: false
          name: anhnh2002
          type: user
        html: ''
        raw: ''
        updatedAt: '2023-11-21T04:53:41.620Z'
      numEdits: 0
      reactions: []
    id: 655c37d5935d0f9a75ea0f1c
    type: comment
  author: anhnh2002
  content: ''
  created_at: 2023-11-21 04:53:41+00:00
  edited: false
  hidden: false
  id: 655c37d5935d0f9a75ea0f1c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6530a1c9f145530101e84b45/PHdHXW-XuX9_kQTPum21R.jpeg?w=200&h=200&f=face
      fullname: "Nguy\u1EC5n Ho\xE0ng Anh"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: anhnh2002
      type: user
    createdAt: '2023-11-21T04:53:42.000Z'
    data:
      oid: d8a38f8a40e9701737a467962146a174745d959b
      parents:
      - 240bb0e0b0eee16be1a868c12f37723ef3454c59
      subject: Upload model
    id: 655c37d60000000000000000
    type: commit
  author: anhnh2002
  created_at: 2023-11-21 04:53:42+00:00
  id: 655c37d60000000000000000
  oid: d8a38f8a40e9701737a467962146a174745d959b
  summary: Upload model
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6530a1c9f145530101e84b45/PHdHXW-XuX9_kQTPum21R.jpeg?w=200&h=200&f=face
      fullname: "Nguy\u1EC5n Ho\xE0ng Anh"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: anhnh2002
      type: user
    createdAt: '2024-01-23T03:19:13.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6530a1c9f145530101e84b45/PHdHXW-XuX9_kQTPum21R.jpeg?w=200&h=200&f=face
          fullname: "Nguy\u1EC5n Ho\xE0ng Anh"
          isHf: false
          isPro: false
          name: anhnh2002
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2024-01-23T03:20:02.753Z'
      numEdits: 0
      reactions: []
    id: 65af3031d0a5cc99d508a9e4
    type: comment
  author: anhnh2002
  content: This comment has been hidden
  created_at: 2024-01-23 03:19:13+00:00
  edited: true
  hidden: true
  id: 65af3031d0a5cc99d508a9e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6530a1c9f145530101e84b45/PHdHXW-XuX9_kQTPum21R.jpeg?w=200&h=200&f=face
      fullname: "Nguy\u1EC5n Ho\xE0ng Anh"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: anhnh2002
      type: user
    createdAt: '2024-01-23T03:22:34.000Z'
    data:
      edited: true
      editors:
      - anhnh2002
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3138388991355896
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6530a1c9f145530101e84b45/PHdHXW-XuX9_kQTPum21R.jpeg?w=200&h=200&f=face
          fullname: "Nguy\u1EC5n Ho\xE0ng Anh"
          isHf: false
          isPro: false
          name: anhnh2002
          type: user
        html: "<p>import os<br>import wandb<br>import torch<br>from datasets import\
          \ load_dataset<br>from transformers import (<br>    AutoModelForCausalLM,<br>\
          \    AutoTokenizer,<br>    BitsAndBytesConfig,<br>    TrainingArguments,<br>\
          \    pipeline,<br>    logging,<br>)<br>from peft import get_peft_config,\
          \ get_peft_model, LoraConfig, TaskType, PeftModel<br>from trl import SFTTrainer,\
          \ DataCollatorForCompletionOnlyLM</p>\n<p>os.environ[\"TOKENIZERS_PARALLELISM\"\
          ] = \"false\"</p>\n<p>base_model_name = \"Viet-Mistral/Vistral-7B-Chat\"\
          <br>adapter_model_name = \"/home/vu.ho_ttsds/vistral-adapter\"<br>auth_token\
          \ = \"\"</p>\n<p>device_map=\"auto\"</p>\n<p>output_dir = \"/home/vu.ho_ttsds/llama_2/vistral-checkpoint\"\
          <br>num_train_epochs = 2<br>max_seq_length = 1536</p>\n<p>if <strong>name</strong>\
          \ == \"<strong>main</strong>\":<br>    # quantization config<br>    compute_dtype\
          \ = getattr(torch, \"float16\")<br>    bnb_config = BitsAndBytesConfig(<br>\
          \        # load_in_8bit=True,<br>        load_in_4bit=True,<br>        bnb_4bit_quant_type=\"\
          nf4\",<br>        bnb_4bit_compute_dtype=compute_dtype,<br>        bnb_4bit_use_double_quant=False,<br>\
          \    )</p>\n<pre><code># lora config\npeft_config = LoraConfig(\n    lora_alpha=32,\n\
          \    lora_dropout=0.05,\n    r=8,\n    bias=\"none\",\n    task_type=\"\
          CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"\
          o_proj\"],\n    # target_modules=[\"Wqkv\", \"down_proj\", \"out_proj\"\
          , \"up_proj\"]\n    # target_modules=[\"q_proj\", \"o_proj\", \"v_proj\"\
          , \"gate_proj\", \"down_proj\", \"k_proj\", \"up_proj\"]\n)\n\n# Training\
          \ arguments\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n\
          \    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=1,\n\
          \    gradient_accumulation_steps=16,\n    optim=\"paged_adamw_32bit\",\n\
          \    save_steps=500,\n    save_total_limit=1,\n    logging_steps=10,\n \
          \   learning_rate=0.0005,\n    weight_decay=0.001,\n    fp16=False,\n  \
          \  bf16=True,\n    max_grad_norm=0.5,\n    max_steps=-1,\n    warmup_ratio=0.03,\n\
          \    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"\
          wandb\"\n)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    base_model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n\
          )\nmodel.config.use_cache = False #return last key value\nmodel.config.pretraining_tp\
          \ = 1 #parallelism \n\n# load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model_name,\
          \ trust_remote_code=True)\n# tokenizer.pad_token = tokenizer.eos_token\n\
          tokenizer.padding_side = \"right\"\ntokenizer.add_bos_token = False\n# tokenizer.pad_token\
          \ = \"&lt;/p&gt;\"\ntokenizer.add_eos_token = False\n\n# dataset\ntrain_dataset\
          \ = load_dataset(\"json\", data_files=\"/home/vu.ho_ttsds/merge_train.jsonl\"\
          , split=\"train\")\nprint(len(train_dataset))\n#eval_dataset = load_dataset(\"\
          json\", data_files=\"\", split=\"train\")\ncollator = DataCollatorForCompletionOnlyLM(response_template=\"\
          [/INST]\", tokenizer=tokenizer, mlm=False)\n\n# Supervised fine-tuning parameters\n\
          trainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n\
          \    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n\
          \    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n\
          \    # formatting_func=formatting_prompts_func,\n    data_collator=collator,\n\
          \    neftune_noise_alpha=5,\n)\n\n# wandb init\nwandb.init(\nproject=\"\
          LLM\",\nname=f\"vistral\",\nconfig={\n    \"learning_rate\": 0.0001,\n \
          \   \"architecture\": \"mistral\",\n    \"dataset\": \"p3andopenai\",\n\
          \    \"epochs\": num_train_epochs,\n}\n)\n\n# Train model\n# trainer.train()\n\
          trainer.train()#resume_from_checkpoint = True)\n\n# Save trained adapter\n\
          trainer.model.save_pretrained(adapter_model_name)\n\n# wandb finish\nwandb.finish()\n\
          </code></pre>\n"
        raw: "import os\nimport wandb\nimport torch\nfrom datasets import load_dataset\n\
          from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n\
          \    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n    logging,\n\
          )\nfrom peft import get_peft_config, get_peft_model, LoraConfig, TaskType,\
          \ PeftModel\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n\
          \nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nbase_model_name\
          \ = \"Viet-Mistral/Vistral-7B-Chat\"\nadapter_model_name = \"/home/vu.ho_ttsds/vistral-adapter\"\
          \nauth_token = \"\"\n\ndevice_map=\"auto\"\n\noutput_dir = \"/home/vu.ho_ttsds/llama_2/vistral-checkpoint\"\
          \nnum_train_epochs = 2\nmax_seq_length = 1536\n\n\nif __name__ == \"__main__\"\
          :\n    # quantization config\n    compute_dtype = getattr(torch, \"float16\"\
          )\n    bnb_config = BitsAndBytesConfig(\n        # load_in_8bit=True,\n\
          \        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n    \
          \    bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n\
          \    )\n\n    # lora config\n    peft_config = LoraConfig(\n        lora_alpha=32,\n\
          \        lora_dropout=0.05,\n        r=8,\n        bias=\"none\",\n    \
          \    task_type=\"CAUSAL_LM\",\n        target_modules=[\"q_proj\", \"v_proj\"\
          , \"k_proj\", \"o_proj\"],\n        # target_modules=[\"Wqkv\", \"down_proj\"\
          , \"out_proj\", \"up_proj\"]\n        # target_modules=[\"q_proj\", \"o_proj\"\
          , \"v_proj\", \"gate_proj\", \"down_proj\", \"k_proj\", \"up_proj\"]\n \
          \   )\n\n    # Training arguments\n    training_arguments = TrainingArguments(\n\
          \        output_dir=output_dir,\n        num_train_epochs=num_train_epochs,\n\
          \        per_device_train_batch_size=1,\n        gradient_accumulation_steps=16,\n\
          \        optim=\"paged_adamw_32bit\",\n        save_steps=500,\n       \
          \ save_total_limit=1,\n        logging_steps=10,\n        learning_rate=0.0005,\n\
          \        weight_decay=0.001,\n        fp16=False,\n        bf16=True,\n\
          \        max_grad_norm=0.5,\n        max_steps=-1,\n        warmup_ratio=0.03,\n\
          \        group_by_length=True,\n        lr_scheduler_type=\"cosine\",\n\
          \        report_to=\"wandb\"\n    )\n\n    # Load base model\n    model\
          \ = AutoModelForCausalLM.from_pretrained(\n        base_model_name,\n  \
          \      quantization_config=bnb_config,\n        device_map=device_map\n\
          \    )\n    model.config.use_cache = False #return last key value\n    model.config.pretraining_tp\
          \ = 1 #parallelism \n\n    # load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(base_model_name,\
          \ trust_remote_code=True)\n    # tokenizer.pad_token = tokenizer.eos_token\n\
          \    tokenizer.padding_side = \"right\"\n    tokenizer.add_bos_token = False\n\
          \    # tokenizer.pad_token = \"</p>\"\n    tokenizer.add_eos_token = False\n\
          \n    # dataset\n    train_dataset = load_dataset(\"json\", data_files=\"\
          /home/vu.ho_ttsds/merge_train.jsonl\", split=\"train\")\n    print(len(train_dataset))\n\
          \    #eval_dataset = load_dataset(\"json\", data_files=\"\", split=\"train\"\
          )\n    collator = DataCollatorForCompletionOnlyLM(response_template=\"[/INST]\"\
          , tokenizer=tokenizer, mlm=False)\n\n    # Supervised fine-tuning parameters\n\
          \    trainer = SFTTrainer(\n        model=model,\n        train_dataset=train_dataset,\n\
          \        peft_config=peft_config,\n        dataset_text_field=\"text\",\n\
          \        max_seq_length=max_seq_length,\n        tokenizer=tokenizer,\n\
          \        args=training_arguments,\n        packing=False,\n        # formatting_func=formatting_prompts_func,\n\
          \        data_collator=collator,\n        neftune_noise_alpha=5,\n    )\n\
          \n    # wandb init\n    wandb.init(\n    project=\"LLM\",\n    name=f\"\
          vistral\",\n    config={\n        \"learning_rate\": 0.0001,\n        \"\
          architecture\": \"mistral\",\n        \"dataset\": \"p3andopenai\",\n  \
          \      \"epochs\": num_train_epochs,\n    }\n    )\n\n    # Train model\n\
          \    # trainer.train()\n    trainer.train()#resume_from_checkpoint = True)\n\
          \n    # Save trained adapter\n    trainer.model.save_pretrained(adapter_model_name)\n\
          \n    # wandb finish\n    wandb.finish()"
        updatedAt: '2024-01-23T03:23:58.477Z'
      numEdits: 1
      reactions: []
    id: 65af30fa9d5b422218c3ce6e
    type: comment
  author: anhnh2002
  content: "import os\nimport wandb\nimport torch\nfrom datasets import load_dataset\n\
    from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n  \
    \  BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n    logging,\n\
    )\nfrom peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftModel\n\
    from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n\nos.environ[\"TOKENIZERS_PARALLELISM\"\
    ] = \"false\"\n\nbase_model_name = \"Viet-Mistral/Vistral-7B-Chat\"\nadapter_model_name\
    \ = \"/home/vu.ho_ttsds/vistral-adapter\"\nauth_token = \"\"\n\ndevice_map=\"\
    auto\"\n\noutput_dir = \"/home/vu.ho_ttsds/llama_2/vistral-checkpoint\"\nnum_train_epochs\
    \ = 2\nmax_seq_length = 1536\n\n\nif __name__ == \"__main__\":\n    # quantization\
    \ config\n    compute_dtype = getattr(torch, \"float16\")\n    bnb_config = BitsAndBytesConfig(\n\
    \        # load_in_8bit=True,\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"\
    nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n\
    \    )\n\n    # lora config\n    peft_config = LoraConfig(\n        lora_alpha=32,\n\
    \        lora_dropout=0.05,\n        r=8,\n        bias=\"none\",\n        task_type=\"\
    CAUSAL_LM\",\n        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"\
    ],\n        # target_modules=[\"Wqkv\", \"down_proj\", \"out_proj\", \"up_proj\"\
    ]\n        # target_modules=[\"q_proj\", \"o_proj\", \"v_proj\", \"gate_proj\"\
    , \"down_proj\", \"k_proj\", \"up_proj\"]\n    )\n\n    # Training arguments\n\
    \    training_arguments = TrainingArguments(\n        output_dir=output_dir,\n\
    \        num_train_epochs=num_train_epochs,\n        per_device_train_batch_size=1,\n\
    \        gradient_accumulation_steps=16,\n        optim=\"paged_adamw_32bit\"\
    ,\n        save_steps=500,\n        save_total_limit=1,\n        logging_steps=10,\n\
    \        learning_rate=0.0005,\n        weight_decay=0.001,\n        fp16=False,\n\
    \        bf16=True,\n        max_grad_norm=0.5,\n        max_steps=-1,\n     \
    \   warmup_ratio=0.03,\n        group_by_length=True,\n        lr_scheduler_type=\"\
    cosine\",\n        report_to=\"wandb\"\n    )\n\n    # Load base model\n    model\
    \ = AutoModelForCausalLM.from_pretrained(\n        base_model_name,\n        quantization_config=bnb_config,\n\
    \        device_map=device_map\n    )\n    model.config.use_cache = False #return\
    \ last key value\n    model.config.pretraining_tp = 1 #parallelism \n\n    # load\
    \ tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n\
    \    # tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side =\
    \ \"right\"\n    tokenizer.add_bos_token = False\n    # tokenizer.pad_token =\
    \ \"</p>\"\n    tokenizer.add_eos_token = False\n\n    # dataset\n    train_dataset\
    \ = load_dataset(\"json\", data_files=\"/home/vu.ho_ttsds/merge_train.jsonl\"\
    , split=\"train\")\n    print(len(train_dataset))\n    #eval_dataset = load_dataset(\"\
    json\", data_files=\"\", split=\"train\")\n    collator = DataCollatorForCompletionOnlyLM(response_template=\"\
    [/INST]\", tokenizer=tokenizer, mlm=False)\n\n    # Supervised fine-tuning parameters\n\
    \    trainer = SFTTrainer(\n        model=model,\n        train_dataset=train_dataset,\n\
    \        peft_config=peft_config,\n        dataset_text_field=\"text\",\n    \
    \    max_seq_length=max_seq_length,\n        tokenizer=tokenizer,\n        args=training_arguments,\n\
    \        packing=False,\n        # formatting_func=formatting_prompts_func,\n\
    \        data_collator=collator,\n        neftune_noise_alpha=5,\n    )\n\n  \
    \  # wandb init\n    wandb.init(\n    project=\"LLM\",\n    name=f\"vistral\"\
    ,\n    config={\n        \"learning_rate\": 0.0001,\n        \"architecture\"\
    : \"mistral\",\n        \"dataset\": \"p3andopenai\",\n        \"epochs\": num_train_epochs,\n\
    \    }\n    )\n\n    # Train model\n    # trainer.train()\n    trainer.train()#resume_from_checkpoint\
    \ = True)\n\n    # Save trained adapter\n    trainer.model.save_pretrained(adapter_model_name)\n\
    \n    # wandb finish\n    wandb.finish()"
  created_at: 2024-01-23 03:22:34+00:00
  edited: true
  hidden: false
  id: 65af30fa9d5b422218c3ce6e
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 1
repo_id: anhnh2002/finance-news-classification
repo_type: model
status: open
target_branch: refs/heads/main
title: Upload model
