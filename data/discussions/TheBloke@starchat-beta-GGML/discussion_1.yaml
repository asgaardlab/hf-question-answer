!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mancub
conflicting_files: null
created_at: 2023-06-08 23:24:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-09T00:24:10.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.973223090171814
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Might wait for q6_K if so, as that does nicely compared to 4, 5
          or 8 and usually has a good perplexity score.</p>

          '
        raw: Might wait for q6_K if so, as that does nicely compared to 4, 5 or 8
          and usually has a good perplexity score.
        updatedAt: '2023-06-09T00:24:10.932Z'
      numEdits: 0
      reactions: []
    id: 6482712a93362a0d120c918d
    type: comment
  author: mancub
  content: Might wait for q6_K if so, as that does nicely compared to 4, 5 or 8 and
    usually has a good perplexity score.
  created_at: 2023-06-08 23:24:10+00:00
  edited: false
  hidden: false
  id: 6482712a93362a0d120c918d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-09T00:26:38.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6069163680076599
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>I will when I can, but those new k-quants are exclusive to llama.cpp\
          \ at the moment</p>\n<pre><code>[pytorch2] ubuntu@h100:/workspace/process\
          \ $ /workspace/git/ggml/build/bin/starcoder-quantize -h\nusage: /workspace/git/ggml/build/bin/starcoder-quantize\
          \ model-f32.bin model-quant.bin type\n  type = \"q4_0\" or 2\n  type = \"\
          q4_1\" or 3\n  type = \"q5_0\" or 8\n  type = \"q5_1\" or 9\n  type = \"\
          q8_0\" or 7\n[pytorch2] ubuntu@h100:/workspace/process $\n</code></pre>\n"
        raw: "I will when I can, but those new k-quants are exclusive to llama.cpp\
          \ at the moment\n\n```\n[pytorch2] ubuntu@h100:/workspace/process $ /workspace/git/ggml/build/bin/starcoder-quantize\
          \ -h\nusage: /workspace/git/ggml/build/bin/starcoder-quantize model-f32.bin\
          \ model-quant.bin type\n  type = \"q4_0\" or 2\n  type = \"q4_1\" or 3\n\
          \  type = \"q5_0\" or 8\n  type = \"q5_1\" or 9\n  type = \"q8_0\" or 7\n\
          [pytorch2] ubuntu@h100:/workspace/process $\n```"
        updatedAt: '2023-06-09T00:26:38.852Z'
      numEdits: 0
      reactions: []
    id: 648271be85ce4d2973c84fb8
    type: comment
  author: TheBloke
  content: "I will when I can, but those new k-quants are exclusive to llama.cpp at\
    \ the moment\n\n```\n[pytorch2] ubuntu@h100:/workspace/process $ /workspace/git/ggml/build/bin/starcoder-quantize\
    \ -h\nusage: /workspace/git/ggml/build/bin/starcoder-quantize model-f32.bin model-quant.bin\
    \ type\n  type = \"q4_0\" or 2\n  type = \"q4_1\" or 3\n  type = \"q5_0\" or 8\n\
    \  type = \"q5_1\" or 9\n  type = \"q8_0\" or 7\n[pytorch2] ubuntu@h100:/workspace/process\
    \ $\n```"
  created_at: 2023-06-08 23:26:38+00:00
  edited: false
  hidden: false
  id: 648271be85ce4d2973c84fb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-06-09T01:14:42.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9767497181892395
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I guess my bad, I was reading the model card and it mentioned 2,
          3, 4, 5, 6 and 8 versions, so somehow I equated seeing 6 with K, duh.</p>

          <p>I''ll give GPTQ model a try instead since that''ll probably provide best
          speed.</p>

          <p>No rush otherwise, I have no idea how do you even accomplish everything
          you do, in just 24 hrs a day. :)</p>

          '
        raw: 'I guess my bad, I was reading the model card and it mentioned 2, 3,
          4, 5, 6 and 8 versions, so somehow I equated seeing 6 with K, duh.


          I''ll give GPTQ model a try instead since that''ll probably provide best
          speed.


          No rush otherwise, I have no idea how do you even accomplish everything
          you do, in just 24 hrs a day. :)'
        updatedAt: '2023-06-09T01:14:42.994Z'
      numEdits: 0
      reactions: []
    id: 64827d02d367932bfab36096
    type: comment
  author: mancub
  content: 'I guess my bad, I was reading the model card and it mentioned 2, 3, 4,
    5, 6 and 8 versions, so somehow I equated seeing 6 with K, duh.


    I''ll give GPTQ model a try instead since that''ll probably provide best speed.


    No rush otherwise, I have no idea how do you even accomplish everything you do,
    in just 24 hrs a day. :)'
  created_at: 2023-06-09 00:14:42+00:00
  edited: false
  hidden: false
  id: 64827d02d367932bfab36096
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-09T01:19:17.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9448877573013306
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh yeah sorry it did say that - I''ve edited it now. I have a standard
          GGML template that assumes the Llama k-quants. I''ve not yet got to the
          point of implementing different README templates for non-Llama models</p>

          <p>I''ve fixed that now</p>

          '
        raw: 'Oh yeah sorry it did say that - I''ve edited it now. I have a standard
          GGML template that assumes the Llama k-quants. I''ve not yet got to the
          point of implementing different README templates for non-Llama models


          I''ve fixed that now'
        updatedAt: '2023-06-09T01:19:17.520Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - mancub
        - PrimeD
    id: 64827e15954578a9d1d99227
    type: comment
  author: TheBloke
  content: 'Oh yeah sorry it did say that - I''ve edited it now. I have a standard
    GGML template that assumes the Llama k-quants. I''ve not yet got to the point
    of implementing different README templates for non-Llama models


    I''ve fixed that now'
  created_at: 2023-06-09 00:19:17+00:00
  edited: false
  hidden: false
  id: 64827e15954578a9d1d99227
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/starchat-beta-GGML
repo_type: model
status: open
target_branch: null
title: Are you making k-quant series of this model?
