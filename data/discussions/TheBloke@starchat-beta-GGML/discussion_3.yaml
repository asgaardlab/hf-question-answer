!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mljxy
conflicting_files: null
created_at: 2023-06-12 19:12:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4e3b34f0605e6e2c9b5c5beb1a9c192f.svg
      fullname: Xiao Jin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mljxy
      type: user
    createdAt: '2023-06-12T20:12:18.000Z'
    data:
      edited: false
      editors:
      - mljxy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6443032026290894
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4e3b34f0605e6e2c9b5c5beb1a9c192f.svg
          fullname: Xiao Jin
          isHf: false
          isPro: false
          name: mljxy
          type: user
        html: "<p>Using the starcoder example in ggml, the special tokens in prompt\
          \ does not got tokenized correctly.  For example,</p>\n<pre><code>main:\
          \ token[0] =     46, &lt;                                              \
          \                                                                      \
          \                                                \nmain: token[1] =    110,\
          \ |                                                                    \
          \                                                                      \
          \                          \nmain: token[2] =   2946, system           \
          \                                                                      \
          \                                                                      \
          \        \nmain: token[3] =  28318, |&gt;                              \
          \                                                                      \
          \                                                               \n</code></pre>\n\
          <p>The correct tokenization should map <code>&lt;|system|&gt;</code> to\
          \  49152 instead. The same incorrect tokenizations happen to <code>&lt;|user|&gt;</code>,\
          \ <code>&lt;|assistant|&gt;</code>, and <code>&lt;|end|&gt;</code>.</p>\n"
        raw: "Using the starcoder example in ggml, the special tokens in prompt does\
          \ not got tokenized correctly.  For example,\r\n\r\n```\r\nmain: token[0]\
          \ =     46, <                                                          \
          \                                                                      \
          \                                    \r\nmain: token[1] =    110, |    \
          \                                                                      \
          \                                                                      \
          \                    \r\nmain: token[2] =   2946, system               \
          \                                                                      \
          \                                                                      \
          \    \r\nmain: token[3] =  28318, |>                                   \
          \                                                                      \
          \                                                          \r\n```\r\n\r\
          \nThe correct tokenization should map `<|system|>` to  49152 instead. The\
          \ same incorrect tokenizations happen to `<|user|>`, `<|assistant|>`, and\
          \ `<|end|>`."
        updatedAt: '2023-06-12T20:12:18.518Z'
      numEdits: 0
      reactions: []
    id: 64877c22eaf65f1261359e24
    type: comment
  author: mljxy
  content: "Using the starcoder example in ggml, the special tokens in prompt does\
    \ not got tokenized correctly.  For example,\r\n\r\n```\r\nmain: token[0] =  \
    \   46, <                                                                    \
    \                                                                            \
    \                    \r\nmain: token[1] =    110, |                          \
    \                                                                            \
    \                                                              \r\nmain: token[2]\
    \ =   2946, system                                                           \
    \                                                                            \
    \                        \r\nmain: token[3] =  28318, |>                     \
    \                                                                            \
    \                                                                  \r\n```\r\n\
    \r\nThe correct tokenization should map `<|system|>` to  49152 instead. The same\
    \ incorrect tokenizations happen to `<|user|>`, `<|assistant|>`, and `<|end|>`."
  created_at: 2023-06-12 19:12:18+00:00
  edited: false
  hidden: false
  id: 64877c22eaf65f1261359e24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
      fullname: Mike Ravkine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mike-ravkine
      type: user
    createdAt: '2023-06-26T20:04:42.000Z'
    data:
      edited: false
      editors:
      - mike-ravkine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8638243675231934
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7865ac39276762682d8e20a33ff9f257.svg
          fullname: Mike Ravkine
          isHf: false
          isPro: false
          name: mike-ravkine
          type: user
        html: '<p>This was fixed last week: <a rel="nofollow" href="https://github.com/ggerganov/ggml/commit/e456108433017d5586b35fd36ce781b4c3aed631">https://github.com/ggerganov/ggml/commit/e456108433017d5586b35fd36ce781b4c3aed631</a></p>

          <p>But only kinda-sorta fixed I think, there''s still somethign up here
          I can''t get SantaCoder to spit out token 49152 (&lt;|end|&gt;) the GGML
          inference diverges from what the HF model does.</p>

          '
        raw: 'This was fixed last week: https://github.com/ggerganov/ggml/commit/e456108433017d5586b35fd36ce781b4c3aed631


          But only kinda-sorta fixed I think, there''s still somethign up here I can''t
          get SantaCoder to spit out token 49152 (<|end|>) the GGML inference diverges
          from what the HF model does.'
        updatedAt: '2023-06-26T20:04:42.832Z'
      numEdits: 0
      reactions: []
    id: 6499ef5a48bf56b95e5118e7
    type: comment
  author: mike-ravkine
  content: 'This was fixed last week: https://github.com/ggerganov/ggml/commit/e456108433017d5586b35fd36ce781b4c3aed631


    But only kinda-sorta fixed I think, there''s still somethign up here I can''t
    get SantaCoder to spit out token 49152 (<|end|>) the GGML inference diverges from
    what the HF model does.'
  created_at: 2023-06-26 19:04:42+00:00
  edited: false
  hidden: false
  id: 6499ef5a48bf56b95e5118e7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/starchat-beta-GGML
repo_type: model
status: open
target_branch: null
title: special tokens in prompt with ggml/examples/starcoder
