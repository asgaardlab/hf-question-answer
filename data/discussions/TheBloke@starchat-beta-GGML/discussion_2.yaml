!!python/object:huggingface_hub.community.DiscussionWithDetails
author: PunchPunch22
conflicting_files: null
created_at: 2023-06-10 09:52:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cfd497c87dd6a3366101d6ec623efb5e.svg
      fullname: can akan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PunchPunch22
      type: user
    createdAt: '2023-06-10T10:52:13.000Z'
    data:
      edited: false
      editors:
      - PunchPunch22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.929100751876831
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cfd497c87dd6a3366101d6ec623efb5e.svg
          fullname: can akan
          isHf: false
          isPro: false
          name: PunchPunch22
          type: user
        html: '<p>model cant be find it says in logs. l made  re named it ggml-starcha.....<br>but
          ui cant load it</p>

          '
        raw: "model cant be find it says in logs. l made  re named it ggml-starcha.....\
          \ \r\nbut ui cant load it"
        updatedAt: '2023-06-10T10:52:13.823Z'
      numEdits: 0
      reactions: []
    id: 648455ddd61548b0fb089ebd
    type: comment
  author: PunchPunch22
  content: "model cant be find it says in logs. l made  re named it ggml-starcha.....\
    \ \r\nbut ui cant load it"
  created_at: 2023-06-10 09:52:13+00:00
  edited: false
  hidden: false
  id: 648455ddd61548b0fb089ebd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d563e0483fde1b42b500c09437954e5b.svg
      fullname: boqsc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: boqsc
      type: user
    createdAt: '2023-06-10T16:10:44.000Z'
    data:
      edited: false
      editors:
      - boqsc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.596676230430603
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d563e0483fde1b42b500c09437954e5b.svg
          fullname: boqsc
          isHf: false
          isPro: false
          name: boqsc
          type: user
        html: '<p>Confirming that it does not work on gpt4all. (ggml-starchat-beta.ggmlv3.q4_0.bin)</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62f7f0c147d782a6e286662a/fPC9koeLlF4E8NLqlREIN.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/62f7f0c147d782a6e286662a/fPC9koeLlF4E8NLqlREIN.png"></a></p>

          '
        raw: 'Confirming that it does not work on gpt4all. (ggml-starchat-beta.ggmlv3.q4_0.bin)



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/62f7f0c147d782a6e286662a/fPC9koeLlF4E8NLqlREIN.png)'
        updatedAt: '2023-06-10T16:10:44.345Z'
      numEdits: 0
      reactions: []
    id: 6484a084546a5cc21008ea7f
    type: comment
  author: boqsc
  content: 'Confirming that it does not work on gpt4all. (ggml-starchat-beta.ggmlv3.q4_0.bin)



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/62f7f0c147d782a6e286662a/fPC9koeLlF4E8NLqlREIN.png)'
  created_at: 2023-06-10 15:10:44+00:00
  edited: false
  hidden: false
  id: 6484a084546a5cc21008ea7f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-10T16:14:52.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9732385277748108
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>I didn't claim it would work with gpt4all, I don't test in that.\
          \ They have their own private ecosystem and I don't really look into it.</p>\n\
          <p>I though it would work with <a rel=\"nofollow\" href=\"https://github.com/ParisNeo/gpt4all-ui\"\
          >ParisNeo/GPT4All-Ui</a> using the <code>ctransformers</code> backend, because\
          \ I thought <code>ctransformers</code> should support this model type.</p>\n\
          <p>Perhaps <span data-props=\"{&quot;user&quot;:&quot;ParisNeo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ParisNeo\"\
          >@<span class=\"underline\">ParisNeo</span></a></span>\n\n\t</span></span>\
          \ or <span data-props=\"{&quot;user&quot;:&quot;marella&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/marella\">@<span class=\"\
          underline\">marella</span></a></span>\n\n\t</span></span> could comment\
          \ on compatibility of BigCoder models in their respective  software?</p>\n"
        raw: 'I didn''t claim it would work with gpt4all, I don''t test in that. They
          have their own private ecosystem and I don''t really look into it.


          I though it would work with [ParisNeo/GPT4All-Ui](https://github.com/ParisNeo/gpt4all-ui)
          using the `ctransformers` backend, because I thought `ctransformers` should
          support this model type.


          Perhaps @ParisNeo or @marella could comment on compatibility of BigCoder
          models in their respective  software?'
        updatedAt: '2023-06-10T16:15:07.288Z'
      numEdits: 1
      reactions: []
    id: 6484a17c437def2ff8eab3e8
    type: comment
  author: TheBloke
  content: 'I didn''t claim it would work with gpt4all, I don''t test in that. They
    have their own private ecosystem and I don''t really look into it.


    I though it would work with [ParisNeo/GPT4All-Ui](https://github.com/ParisNeo/gpt4all-ui)
    using the `ctransformers` backend, because I thought `ctransformers` should support
    this model type.


    Perhaps @ParisNeo or @marella could comment on compatibility of BigCoder models
    in their respective  software?'
  created_at: 2023-06-10 15:14:52+00:00
  edited: true
  hidden: false
  id: 6484a17c437def2ff8eab3e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/93b6433d2741979d473811732948b04d.svg
      fullname: Ravindra Marella
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marella
      type: user
    createdAt: '2023-06-10T17:24:08.000Z'
    data:
      edited: false
      editors:
      - marella
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5042850375175476
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/93b6433d2741979d473811732948b04d.svg
          fullname: Ravindra Marella
          isHf: false
          isPro: false
          name: marella
          type: user
        html: '<p>The following works with <a rel="nofollow" href="https://github.com/marella/ctransformers"><code>ctransformers</code></a>:</p>

          <pre><code class="language-py"><span class="hljs-keyword">from</span> ctransformers
          <span class="hljs-keyword">import</span> AutoModelForCausalLM


          llm = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">"TheBloke/starchat-beta-GGML"</span>)


          <span class="hljs-built_in">print</span>(llm(prompt, stop=<span class="hljs-string">"&lt;|end|&gt;"</span>))

          </code></pre>

          <p>Here is an example using LangChain:</p>

          <pre><code class="language-py"><span class="hljs-keyword">from</span> langchain.llms
          <span class="hljs-keyword">import</span> CTransformers

          <span class="hljs-keyword">from</span> langchain <span class="hljs-keyword">import</span>
          PromptTemplate, LLMChain


          config = {<span class="hljs-string">"stop"</span>: <span class="hljs-string">"&lt;|end|&gt;"</span>}

          llm = CTransformers(model=<span class="hljs-string">"TheBloke/starchat-beta-GGML"</span>,
          config=config)


          template = <span class="hljs-string">"""&lt;|system|&gt; Below is a conversation
          between a human user and a helpful AI coding assistant. &lt;|end|&gt;</span>

          <span class="hljs-string">&lt;|user|&gt; {question} &lt;|end|&gt;</span>

          <span class="hljs-string">&lt;|assistant|&gt; """</span>

          prompt = PromptTemplate(template=template, input_variables=[<span class="hljs-string">"question"</span>])

          llm_chain = LLMChain(prompt=prompt, llm=llm)

          response = llm_chain.run(<span class="hljs-string">"How do I sort a list
          in Python?"</span>)

          <span class="hljs-built_in">print</span>(response)

          </code></pre>

          <p>Make sure you have installed the latest version of <code>ctransformers</code>
          and <code>langchain</code> packages.</p>

          '
        raw: 'The following works with [`ctransformers`](https://github.com/marella/ctransformers):


          ```py

          from ctransformers import AutoModelForCausalLM


          llm = AutoModelForCausalLM.from_pretrained("TheBloke/starchat-beta-GGML")


          print(llm(prompt, stop="<|end|>"))

          ```


          Here is an example using LangChain:


          ```py

          from langchain.llms import CTransformers

          from langchain import PromptTemplate, LLMChain


          config = {"stop": "<|end|>"}

          llm = CTransformers(model="TheBloke/starchat-beta-GGML", config=config)


          template = """<|system|> Below is a conversation between a human user and
          a helpful AI coding assistant. <|end|>

          <|user|> {question} <|end|>

          <|assistant|> """

          prompt = PromptTemplate(template=template, input_variables=["question"])

          llm_chain = LLMChain(prompt=prompt, llm=llm)

          response = llm_chain.run("How do I sort a list in Python?")

          print(response)

          ```


          Make sure you have installed the latest version of `ctransformers` and `langchain`
          packages.'
        updatedAt: '2023-06-10T17:24:08.584Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - PrimeD
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 6484b1b853b7a6bf04fb1ba7
    type: comment
  author: marella
  content: 'The following works with [`ctransformers`](https://github.com/marella/ctransformers):


    ```py

    from ctransformers import AutoModelForCausalLM


    llm = AutoModelForCausalLM.from_pretrained("TheBloke/starchat-beta-GGML")


    print(llm(prompt, stop="<|end|>"))

    ```


    Here is an example using LangChain:


    ```py

    from langchain.llms import CTransformers

    from langchain import PromptTemplate, LLMChain


    config = {"stop": "<|end|>"}

    llm = CTransformers(model="TheBloke/starchat-beta-GGML", config=config)


    template = """<|system|> Below is a conversation between a human user and a helpful
    AI coding assistant. <|end|>

    <|user|> {question} <|end|>

    <|assistant|> """

    prompt = PromptTemplate(template=template, input_variables=["question"])

    llm_chain = LLMChain(prompt=prompt, llm=llm)

    response = llm_chain.run("How do I sort a list in Python?")

    print(response)

    ```


    Make sure you have installed the latest version of `ctransformers` and `langchain`
    packages.'
  created_at: 2023-06-10 16:24:08+00:00
  edited: false
  hidden: false
  id: 6484b1b853b7a6bf04fb1ba7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d70ae605c07e368f8a50543ca42e0eba.svg
      fullname: Pablito
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: boricuapab
      type: user
    createdAt: '2023-06-11T08:28:42.000Z'
    data:
      edited: false
      editors:
      - boricuapab
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5950828194618225
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d70ae605c07e368f8a50543ca42e0eba.svg
          fullname: Pablito
          isHf: false
          isPro: false
          name: boricuapab
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;PunchPunch22&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/PunchPunch22\"\
          >@<span class=\"underline\">PunchPunch22</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;boqsc&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/boqsc\">@<span class=\"\
          underline\">boqsc</span></a></span>\n\n\t</span></span> I got it working\
          \ in gpt4all webui with a slight modification to the c_transformers binding\
          \ init file as well as the gpt4all personality config file</p>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/nzQ4krOqS5kFfy7Eq8Oa9.png\"\
          ><img alt=\"addStarChat-Beta.PNG\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/nzQ4krOqS5kFfy7Eq8Oa9.png\"\
          ></a></p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/SXP2toXyfLxB8t_tiXSIO.png\"\
          ><img alt=\"starChatAntiPrompt.PNG\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/SXP2toXyfLxB8t_tiXSIO.png\"\
          ></a></p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/6MZGFguEkcpQPsDeC6MhE.png\"\
          ><img alt=\"starChatBetaWorksWithoutContinue.PNG\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/6MZGFguEkcpQPsDeC6MhE.png\"\
          ></a></p>\n"
        raw: '@PunchPunch22 @boqsc I got it working in gpt4all webui with a slight
          modification to the c_transformers binding init file as well as the gpt4all
          personality config file


          ![addStarChat-Beta.PNG](https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/nzQ4krOqS5kFfy7Eq8Oa9.png)


          ![starChatAntiPrompt.PNG](https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/SXP2toXyfLxB8t_tiXSIO.png)


          ![starChatBetaWorksWithoutContinue.PNG](https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/6MZGFguEkcpQPsDeC6MhE.png)'
        updatedAt: '2023-06-11T08:28:42.403Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PrimeD
    id: 648585bac75db6cb0030386b
    type: comment
  author: boricuapab
  content: '@PunchPunch22 @boqsc I got it working in gpt4all webui with a slight modification
    to the c_transformers binding init file as well as the gpt4all personality config
    file


    ![addStarChat-Beta.PNG](https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/nzQ4krOqS5kFfy7Eq8Oa9.png)


    ![starChatAntiPrompt.PNG](https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/SXP2toXyfLxB8t_tiXSIO.png)


    ![starChatBetaWorksWithoutContinue.PNG](https://cdn-uploads.huggingface.co/production/uploads/6344929432ccc5ca993b0d2a/6MZGFguEkcpQPsDeC6MhE.png)'
  created_at: 2023-06-11 07:28:42+00:00
  edited: false
  hidden: false
  id: 648585bac75db6cb0030386b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-11T08:45:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8026405572891235
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Thanks, both</p>\n<p>Good point re <code>&lt;|end|&gt;</code>. \
          \  I created the GGML before they fixed the <code>special_tokens.json</code>\
          \ to add the correct EOS token of <code>&lt;|end|&gt;</code>.  </p>\n<p>Should\
          \ I re-do the GGMLs?  If I do, would it then bake in the correct correct\
          \ end token so users wouldn't need to manually specify it?</p>\n<p>Any thoughts,\
          \ <span data-props=\"{&quot;user&quot;:&quot;marella&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/marella\">@<span class=\"\
          underline\">marella</span></a></span>\n\n\t</span></span> ?</p>\n"
        raw: "Thanks, both\n\nGood point re `<|end|>`.   I created the GGML before\
          \ they fixed the `special_tokens.json` to add the correct EOS token of `<|end|>`.\
          \  \n\nShould I re-do the GGMLs?  If I do, would it then bake in the correct\
          \ correct end token so users wouldn't need to manually specify it?\n\nAny\
          \ thoughts, @marella ?"
        updatedAt: '2023-06-11T08:45:27.570Z'
      numEdits: 0
      reactions: []
    id: 648589a70ed12e85f8c8a3be
    type: comment
  author: TheBloke
  content: "Thanks, both\n\nGood point re `<|end|>`.   I created the GGML before they\
    \ fixed the `special_tokens.json` to add the correct EOS token of `<|end|>`. \
    \ \n\nShould I re-do the GGMLs?  If I do, would it then bake in the correct correct\
    \ end token so users wouldn't need to manually specify it?\n\nAny thoughts, @marella\
    \ ?"
  created_at: 2023-06-11 07:45:27+00:00
  edited: false
  hidden: false
  id: 648589a70ed12e85f8c8a3be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/93b6433d2741979d473811732948b04d.svg
      fullname: Ravindra Marella
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marella
      type: user
    createdAt: '2023-06-11T09:33:02.000Z'
    data:
      edited: false
      editors:
      - marella
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8222145438194275
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/93b6433d2741979d473811732948b04d.svg
          fullname: Ravindra Marella
          isHf: false
          isPro: false
          name: marella
          type: user
        html: '<p>I don''t think GGML model file supports special_tokens so re-doing
          might not help. Currently it is handled in code for <a rel="nofollow" href="https://github.com/ggerganov/ggml/blob/f52d2a05cf8327baf6c0d49e7b231953179e03d3/examples/dolly-v2/main.cpp#L152-L154">dolly-v2
          example</a>.<br>Similarly I can update the <a rel="nofollow" href="https://github.com/ggerganov/ggml/blob/master/examples/starcoder/main.cpp">starcoder
          example</a> to add these special tokens for starchat. I can check if it
          is a starchat model by looking up if special tokens like <code>&lt;|end|&gt;</code>
          are part of vocab. Then in <code>ctransformers</code> I can handle it similar
          to dolly-v2.<br>I will try to make these changes today and release it.</p>

          '
        raw: 'I don''t think GGML model file supports special_tokens so re-doing might
          not help. Currently it is handled in code for [dolly-v2 example](https://github.com/ggerganov/ggml/blob/f52d2a05cf8327baf6c0d49e7b231953179e03d3/examples/dolly-v2/main.cpp#L152-L154).

          Similarly I can update the [starcoder example](https://github.com/ggerganov/ggml/blob/master/examples/starcoder/main.cpp)
          to add these special tokens for starchat. I can check if it is a starchat
          model by looking up if special tokens like `<|end|>` are part of vocab.
          Then in `ctransformers` I can handle it similar to dolly-v2.

          I will try to make these changes today and release it.'
        updatedAt: '2023-06-11T09:33:02.964Z'
      numEdits: 0
      reactions: []
    id: 648594ce0863eadaba03aa13
    type: comment
  author: marella
  content: 'I don''t think GGML model file supports special_tokens so re-doing might
    not help. Currently it is handled in code for [dolly-v2 example](https://github.com/ggerganov/ggml/blob/f52d2a05cf8327baf6c0d49e7b231953179e03d3/examples/dolly-v2/main.cpp#L152-L154).

    Similarly I can update the [starcoder example](https://github.com/ggerganov/ggml/blob/master/examples/starcoder/main.cpp)
    to add these special tokens for starchat. I can check if it is a starchat model
    by looking up if special tokens like `<|end|>` are part of vocab. Then in `ctransformers`
    I can handle it similar to dolly-v2.

    I will try to make these changes today and release it.'
  created_at: 2023-06-11 08:33:02+00:00
  edited: false
  hidden: false
  id: 648594ce0863eadaba03aa13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-11T09:46:38.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.885351300239563
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ah OK. I know it embeds the vocab so I thought it would embed the
          stopping tokens also.  But you''re right, I can''t see any mention of <code>special_token_map.json</code>
          in llama.cpp''s <code>convert.py</code>.  It reads the tokenizer and added_tokens.json,
          but nothing else.</p>

          <p>I guess this means that GGML models are always hardcoded to look for
          <code>&lt;/s&gt;</code> and nothing else</p>

          '
        raw: 'Ah OK. I know it embeds the vocab so I thought it would embed the stopping
          tokens also.  But you''re right, I can''t see any mention of `special_token_map.json`
          in llama.cpp''s `convert.py`.  It reads the tokenizer and added_tokens.json,
          but nothing else.


          I guess this means that GGML models are always hardcoded to look for `</s>`
          and nothing else'
        updatedAt: '2023-06-11T09:46:38.011Z'
      numEdits: 0
      reactions: []
    id: 648597fe30366eb3d847a766
    type: comment
  author: TheBloke
  content: 'Ah OK. I know it embeds the vocab so I thought it would embed the stopping
    tokens also.  But you''re right, I can''t see any mention of `special_token_map.json`
    in llama.cpp''s `convert.py`.  It reads the tokenizer and added_tokens.json, but
    nothing else.


    I guess this means that GGML models are always hardcoded to look for `</s>` and
    nothing else'
  created_at: 2023-06-11 08:46:38+00:00
  edited: false
  hidden: false
  id: 648597fe30366eb3d847a766
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/93b6433d2741979d473811732948b04d.svg
      fullname: Ravindra Marella
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marella
      type: user
    createdAt: '2023-06-11T15:38:02.000Z'
    data:
      edited: false
      editors:
      - marella
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7561707496643066
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/93b6433d2741979d473811732948b04d.svg
          fullname: Ravindra Marella
          isHf: false
          isPro: false
          name: marella
          type: user
        html: '<p>Support for the <code>&lt;|end|&gt;</code> special token is added
          in the latest version (0.2.7) of <code>ctransformers</code>. So now you
          don''t have to specify <code>stop="&lt;|end|&gt;"</code></p>

          <p>I will send a PR to update the example in GGML repo.</p>

          '
        raw: 'Support for the `<|end|>` special token is added in the latest version
          (0.2.7) of `ctransformers`. So now you don''t have to specify `stop="<|end|>"`


          I will send a PR to update the example in GGML repo.'
        updatedAt: '2023-06-11T15:38:02.958Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - PrimeD
        - Azamorn
    id: 6485ea5a77076d551d50d1f7
    type: comment
  author: marella
  content: 'Support for the `<|end|>` special token is added in the latest version
    (0.2.7) of `ctransformers`. So now you don''t have to specify `stop="<|end|>"`


    I will send a PR to update the example in GGML repo.'
  created_at: 2023-06-11 14:38:02+00:00
  edited: false
  hidden: false
  id: 6485ea5a77076d551d50d1f7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/starchat-beta-GGML
repo_type: model
status: open
target_branch: null
title: doesnt work with gpt4all ui
