!!python/object:huggingface_hub.community.DiscussionWithDetails
author: maveriq
conflicting_files: null
created_at: 2023-09-28 21:31:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
      fullname: Haris Jabbar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maveriq
      type: user
    createdAt: '2023-09-28T22:31:22.000Z'
    data:
      edited: false
      editors:
      - maveriq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9376799464225769
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
          fullname: Haris Jabbar
          isHf: false
          isPro: false
          name: maveriq
          type: user
        html: '<p>Hi. Thank you for such an interesting work. </p>

          <p>In the footnote of page 2 of the paper, it is mentioned that the context
          length of the model is 512. However, when I look at the model config on
          HF, it says max_position_embeddings=2048. Am I missing something?</p>

          '
        raw: "Hi. Thank you for such an interesting work. \r\n\r\nIn the footnote\
          \ of page 2 of the paper, it is mentioned that the context length of the\
          \ model is 512. However, when I look at the model config on HF, it says\
          \ max_position_embeddings=2048. Am I missing something?"
        updatedAt: '2023-09-28T22:31:22.644Z'
      numEdits: 0
      reactions: []
    id: 6515feba60757b8c8f7709b1
    type: comment
  author: maveriq
  content: "Hi. Thank you for such an interesting work. \r\n\r\nIn the footnote of\
    \ page 2 of the paper, it is mentioned that the context length of the model is\
    \ 512. However, when I look at the model config on HF, it says max_position_embeddings=2048.\
    \ Am I missing something?"
  created_at: 2023-09-28 21:31:22+00:00
  edited: false
  hidden: false
  id: 6515feba60757b8c8f7709b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22bb971597e9f3abfa343280a9d0f65f.svg
      fullname: Ronen Eldan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: roneneldan
      type: user
    createdAt: '2023-09-28T23:11:38.000Z'
    data:
      edited: false
      editors:
      - roneneldan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9676262140274048
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22bb971597e9f3abfa343280a9d0f65f.svg
          fullname: Ronen Eldan
          isHf: false
          isPro: false
          name: roneneldan
          type: user
        html: '<p>The max position embeddings is indeed 2048, but during training
          it only saw examples with context length up to 512, so its positional encodings
          beyond that length basically have random values, which is why it''s likely
          that the quality of completion will very significantly decrease beyond a
          length of 512 (that wouldn''t be the case with rotary embeddings, but we
          just have a trained positional embedding).</p>

          '
        raw: The max position embeddings is indeed 2048, but during training it only
          saw examples with context length up to 512, so its positional encodings
          beyond that length basically have random values, which is why it's likely
          that the quality of completion will very significantly decrease beyond a
          length of 512 (that wouldn't be the case with rotary embeddings, but we
          just have a trained positional embedding).
        updatedAt: '2023-09-28T23:11:38.746Z'
      numEdits: 0
      reactions: []
    id: 6516082ab84fb7bc6cb51915
    type: comment
  author: roneneldan
  content: The max position embeddings is indeed 2048, but during training it only
    saw examples with context length up to 512, so its positional encodings beyond
    that length basically have random values, which is why it's likely that the quality
    of completion will very significantly decrease beyond a length of 512 (that wouldn't
    be the case with rotary embeddings, but we just have a trained positional embedding).
  created_at: 2023-09-28 22:11:38+00:00
  edited: false
  hidden: false
  id: 6516082ab84fb7bc6cb51915
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
      fullname: Haris Jabbar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maveriq
      type: user
    createdAt: '2023-09-29T06:21:24.000Z'
    data:
      edited: false
      editors:
      - maveriq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8436321020126343
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
          fullname: Haris Jabbar
          isHf: false
          isPro: false
          name: maveriq
          type: user
        html: '<p>Thank you for clarification</p>

          '
        raw: Thank you for clarification
        updatedAt: '2023-09-29T06:21:24.562Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65166ce4c33a8b19196f853d
    id: 65166ce4c33a8b19196f853c
    type: comment
  author: maveriq
  content: Thank you for clarification
  created_at: 2023-09-29 05:21:24+00:00
  edited: false
  hidden: false
  id: 65166ce4c33a8b19196f853c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
      fullname: Haris Jabbar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maveriq
      type: user
    createdAt: '2023-09-29T06:21:24.000Z'
    data:
      status: closed
    id: 65166ce4c33a8b19196f853d
    type: status-change
  author: maveriq
  created_at: 2023-09-29 05:21:24+00:00
  id: 65166ce4c33a8b19196f853d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: roneneldan/TinyStories-33M
repo_type: model
status: closed
target_branch: null
title: Context Length of the model 512 or 2048?
