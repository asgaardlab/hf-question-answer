!!python/object:huggingface_hub.community.DiscussionWithDetails
author: brucethemoose
conflicting_files: null
created_at: 2024-01-01 22:14:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-01T22:14:44.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9848538637161255
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>I''m very excited to try this model, especially once the DPO version
          comes out!</p>

          <p>Just out of curiosity, what context length was it trained it?</p>

          '
        raw: "I'm very excited to try this model, especially once the DPO version\
          \ comes out!\r\n\r\nJust out of curiosity, what context length was it trained\
          \ it?"
        updatedAt: '2024-01-01T22:14:44.742Z'
      numEdits: 0
      reactions: []
    id: 659339543ce574ff3ce694c9
    type: comment
  author: brucethemoose
  content: "I'm very excited to try this model, especially once the DPO version comes\
    \ out!\r\n\r\nJust out of curiosity, what context length was it trained it?"
  created_at: 2024-01-01 22:14:44+00:00
  edited: false
  hidden: false
  id: 659339543ce574ff3ce694c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2024-01-01T22:27:02.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9774211049079895
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>I used 4096, mainly because nearly all of the instructions fell
          within that (and actually the vast majority was under that).  I may do one
          more pass on the full scripts of cinematika to add some coherence out to
          the tens of thousands, but it''s costly.</p>

          <p>DPO version is ready BTW.</p>

          '
        raw: 'I used 4096, mainly because nearly all of the instructions fell within
          that (and actually the vast majority was under that).  I may do one more
          pass on the full scripts of cinematika to add some coherence out to the
          tens of thousands, but it''s costly.


          DPO version is ready BTW.'
        updatedAt: '2024-01-01T22:27:02.460Z'
      numEdits: 0
      reactions: []
    id: 65933c36a78a2778032b31a8
    type: comment
  author: jondurbin
  content: 'I used 4096, mainly because nearly all of the instructions fell within
    that (and actually the vast majority was under that).  I may do one more pass
    on the full scripts of cinematika to add some coherence out to the tens of thousands,
    but it''s costly.


    DPO version is ready BTW.'
  created_at: 2024-01-01 22:27:02+00:00
  edited: false
  hidden: false
  id: 65933c36a78a2778032b31a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-01T22:39:24.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9480707049369812
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Yeah I just saw that! Hard to find stuff on HF. For what it''s worth,
          other 200K finetunes seemed to preserve some long context performance even
          trained at 4K, but I would be extremely interested in a bagel finetuned
          out to just 40K-75K.</p>

          <p>I''m not sure what you use to train, but you might find this context
          length VRAM usage/perplexity graph from a paper interesting: <a rel="nofollow"
          href="https://github.com/huggingface/peft/issues/958">https://github.com/huggingface/peft/issues/958</a></p>

          <p>As well as unsloth, which does reduce VRAM usage significantly: <a rel="nofollow"
          href="https://github.com/unslothai/unsloth">https://github.com/unslothai/unsloth</a></p>

          <p>Technically unsloth and axoltl don''t integrate longlora into lora training,
          but its probably fine?</p>

          '
        raw: 'Yeah I just saw that! Hard to find stuff on HF. For what it''s worth,
          other 200K finetunes seemed to preserve some long context performance even
          trained at 4K, but I would be extremely interested in a bagel finetuned
          out to just 40K-75K.


          I''m not sure what you use to train, but you might find this context length
          VRAM usage/perplexity graph from a paper interesting: https://github.com/huggingface/peft/issues/958


          As well as unsloth, which does reduce VRAM usage significantly: https://github.com/unslothai/unsloth


          Technically unsloth and axoltl don''t integrate longlora into lora training,
          but its probably fine?'
        updatedAt: '2024-01-01T22:40:39.965Z'
      numEdits: 2
      reactions: []
    id: 65933f1c075245eadd1daedf
    type: comment
  author: brucethemoose
  content: 'Yeah I just saw that! Hard to find stuff on HF. For what it''s worth,
    other 200K finetunes seemed to preserve some long context performance even trained
    at 4K, but I would be extremely interested in a bagel finetuned out to just 40K-75K.


    I''m not sure what you use to train, but you might find this context length VRAM
    usage/perplexity graph from a paper interesting: https://github.com/huggingface/peft/issues/958


    As well as unsloth, which does reduce VRAM usage significantly: https://github.com/unslothai/unsloth


    Technically unsloth and axoltl don''t integrate longlora into lora training, but
    its probably fine?'
  created_at: 2024-01-01 22:39:24+00:00
  edited: true
  hidden: false
  id: 65933f1c075245eadd1daedf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2024-01-01T23:37:56.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9727106094360352
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>I used a mix of qlora and some full weight tuning for this.  Thanks
          for the link and info, very interesting!</p>

          <p>I''d probably do full weight if/when I try extending trained ctx length,
          but I was hoping it would just inherit longer ctx capabilities from base.</p>

          '
        raw: 'I used a mix of qlora and some full weight tuning for this.  Thanks
          for the link and info, very interesting!


          I''d probably do full weight if/when I try extending trained ctx length,
          but I was hoping it would just inherit longer ctx capabilities from base.'
        updatedAt: '2024-01-01T23:37:56.510Z'
      numEdits: 0
      reactions: []
    id: 65934cd4a02954c98298f62d
    type: comment
  author: jondurbin
  content: 'I used a mix of qlora and some full weight tuning for this.  Thanks for
    the link and info, very interesting!


    I''d probably do full weight if/when I try extending trained ctx length, but I
    was hoping it would just inherit longer ctx capabilities from base.'
  created_at: 2024-01-01 23:37:56+00:00
  edited: false
  hidden: false
  id: 65934cd4a02954c98298f62d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-02T00:50:55.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9476483464241028
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>I hope you do! But of course I appreciate the DPO finetune as is!</p>

          <blockquote>

          <p>but I was hoping it would just inherit longer ctx capabilities from base.</p>

          </blockquote>

          <p>Other models do, still quantizing bagel to test it myself. But I bet
          long context data would really help. This would make Bagel 34B particularly
          unique, as no one else (AFAIK) is really finetuning Yi 200K on a long context.</p>

          <p> And yeah, I would recommend unsloth in particular, its just a huge drop-in
          VRAM savings + speed boost with no downside, at least in my own testing.</p>

          '
        raw: "I hope you do! But of course I appreciate the DPO finetune as is!\n\n\
          > but I was hoping it would just inherit longer ctx capabilities from base.\n\
          \nOther models do, still quantizing bagel to test it myself. But I bet long\
          \ context data would really help. This would make Bagel 34B particularly\
          \ unique, as no one else (AFAIK) is really finetuning Yi 200K on a long\
          \ context.\n\n And yeah, I would recommend unsloth in particular, its just\
          \ a huge drop-in VRAM savings + speed boost with no downside, at least in\
          \ my own testing."
        updatedAt: '2024-01-02T00:54:28.790Z'
      numEdits: 4
      reactions: []
      relatedEventId: 65935def77105e6e40711a77
    id: 65935def77105e6e40711a6e
    type: comment
  author: brucethemoose
  content: "I hope you do! But of course I appreciate the DPO finetune as is!\n\n\
    > but I was hoping it would just inherit longer ctx capabilities from base.\n\n\
    Other models do, still quantizing bagel to test it myself. But I bet long context\
    \ data would really help. This would make Bagel 34B particularly unique, as no\
    \ one else (AFAIK) is really finetuning Yi 200K on a long context.\n\n And yeah,\
    \ I would recommend unsloth in particular, its just a huge drop-in VRAM savings\
    \ + speed boost with no downside, at least in my own testing."
  created_at: 2024-01-02 00:50:55+00:00
  edited: true
  hidden: false
  id: 65935def77105e6e40711a6e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-02T00:50:55.000Z'
    data:
      status: closed
    id: 65935def77105e6e40711a77
    type: status-change
  author: brucethemoose
  created_at: 2024-01-02 00:50:55+00:00
  id: 65935def77105e6e40711a77
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: jondurbin/bagel-34b-v0.2
repo_type: model
status: closed
target_branch: null
title: Context Length?
