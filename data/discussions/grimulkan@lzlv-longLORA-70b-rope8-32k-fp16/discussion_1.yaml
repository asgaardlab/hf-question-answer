!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Amajiro
conflicting_files: null
created_at: 2024-01-23 08:39:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6936ba1ae9cb2e3a82a0996b74208d9a.svg
      fullname: Chris Gilmore
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Amajiro
      type: user
    createdAt: '2024-01-23T08:39:25.000Z'
    data:
      edited: false
      editors:
      - Amajiro
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.915189802646637
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6936ba1ae9cb2e3a82a0996b74208d9a.svg
          fullname: Chris Gilmore
          isHf: false
          isPro: false
          name: Amajiro
          type: user
        html: '<p>Lzlv long context is the holy grail for me so if you have solved
          this, I tip my hat to you.</p>

          <p>I have a question. To quantize this to exl2, can I just use one of the
          usual calibration datasets (e.g. PIPPA)? Or do I need something with longer
          token lengths?</p>

          '
        raw: "Lzlv long context is the holy grail for me so if you have solved this,\
          \ I tip my hat to you.\r\n\r\nI have a question. To quantize this to exl2,\
          \ can I just use one of the usual calibration datasets (e.g. PIPPA)? Or\
          \ do I need something with longer token lengths?"
        updatedAt: '2024-01-23T08:39:25.807Z'
      numEdits: 0
      reactions: []
    id: 65af7b3d5033724f45d5500a
    type: comment
  author: Amajiro
  content: "Lzlv long context is the holy grail for me so if you have solved this,\
    \ I tip my hat to you.\r\n\r\nI have a question. To quantize this to exl2, can\
    \ I just use one of the usual calibration datasets (e.g. PIPPA)? Or do I need\
    \ something with longer token lengths?"
  created_at: 2024-01-23 08:39:25+00:00
  edited: false
  hidden: false
  id: 65af7b3d5033724f45d5500a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-23T18:24:11.000Z'
    data:
      edited: true
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9700033664703369
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>I wouldn''t say it is "solved", but it seems to not be broken :)
          It is probably "solvable" with either further 32K fine-tuning or more merging,
          if it isn''t solved already.</p>

          <p>I also experimented with fine-tuning a Goliath-style interleaved merge
          of lzlv and Euryale, each with the longLORA merge, and using the same training
          regimen as Aurelian. It seems to be working also... so there is hope for
          this either way.</p>

          <p>For EXL2, I experimented with 32K datasets, and at least for 4 bits and
          above, there was no benefit from measuring over the full 32K context length
          (2048 or 4096 seemed to saturate the benefit). Also, the 32K gave me headaches
          with numerical issues and .safetensor saving limits that were a pain to
          deal with (in addition to being super slow). It might matter more for lower
          bit depths, I''m not sure. The important thing is to use the correct rope
          scaling while measuring for all cases (in this case, linear scaling of 8).</p>

          <p>I didn''t try PIPPA specifically (I ran into some robustness issues when
          trying PIPPA), but I tried the default EXL2 calibration dataset, and a subset
          of the Aurelian training set (which goes to 32K), for example.</p>

          '
        raw: 'I wouldn''t say it is "solved", but it seems to not be broken :) It
          is probably "solvable" with either further 32K fine-tuning or more merging,
          if it isn''t solved already.


          I also experimented with fine-tuning a Goliath-style interleaved merge of
          lzlv and Euryale, each with the longLORA merge, and using the same training
          regimen as Aurelian. It seems to be working also... so there is hope for
          this either way.


          For EXL2, I experimented with 32K datasets, and at least for 4 bits and
          above, there was no benefit from measuring over the full 32K context length
          (2048 or 4096 seemed to saturate the benefit). Also, the 32K gave me headaches
          with numerical issues and .safetensor saving limits that were a pain to
          deal with (in addition to being super slow). It might matter more for lower
          bit depths, I''m not sure. The important thing is to use the correct rope
          scaling while measuring for all cases (in this case, linear scaling of 8).


          I didn''t try PIPPA specifically (I ran into some robustness issues when
          trying PIPPA), but I tried the default EXL2 calibration dataset, and a subset
          of the Aurelian training set (which goes to 32K), for example.'
        updatedAt: '2024-01-23T18:57:01.920Z'
      numEdits: 3
      reactions: []
    id: 65b0044ba3085d018b1ee392
    type: comment
  author: grimulkan
  content: 'I wouldn''t say it is "solved", but it seems to not be broken :) It is
    probably "solvable" with either further 32K fine-tuning or more merging, if it
    isn''t solved already.


    I also experimented with fine-tuning a Goliath-style interleaved merge of lzlv
    and Euryale, each with the longLORA merge, and using the same training regimen
    as Aurelian. It seems to be working also... so there is hope for this either way.


    For EXL2, I experimented with 32K datasets, and at least for 4 bits and above,
    there was no benefit from measuring over the full 32K context length (2048 or
    4096 seemed to saturate the benefit). Also, the 32K gave me headaches with numerical
    issues and .safetensor saving limits that were a pain to deal with (in addition
    to being super slow). It might matter more for lower bit depths, I''m not sure.
    The important thing is to use the correct rope scaling while measuring for all
    cases (in this case, linear scaling of 8).


    I didn''t try PIPPA specifically (I ran into some robustness issues when trying
    PIPPA), but I tried the default EXL2 calibration dataset, and a subset of the
    Aurelian training set (which goes to 32K), for example.'
  created_at: 2024-01-23 18:24:11+00:00
  edited: true
  hidden: false
  id: 65b0044ba3085d018b1ee392
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6936ba1ae9cb2e3a82a0996b74208d9a.svg
      fullname: Chris Gilmore
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Amajiro
      type: user
    createdAt: '2024-01-24T02:36:43.000Z'
    data:
      edited: false
      editors:
      - Amajiro
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9608949422836304
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6936ba1ae9cb2e3a82a0996b74208d9a.svg
          fullname: Chris Gilmore
          isHf: false
          isPro: false
          name: Amajiro
          type: user
        html: '<p>Thanks, I''ve successfully quantized to 3.3bpw using the PIPPA dataset
          and it all seems to be working well, picking up the long context beautifully.</p>

          '
        raw: Thanks, I've successfully quantized to 3.3bpw using the PIPPA dataset
          and it all seems to be working well, picking up the long context beautifully.
        updatedAt: '2024-01-24T02:36:43.631Z'
      numEdits: 0
      reactions: []
    id: 65b077bba0b4bf3b0ebfdc03
    type: comment
  author: Amajiro
  content: Thanks, I've successfully quantized to 3.3bpw using the PIPPA dataset and
    it all seems to be working well, picking up the long context beautifully.
  created_at: 2024-01-24 02:36:43+00:00
  edited: false
  hidden: false
  id: 65b077bba0b4bf3b0ebfdc03
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: grimulkan/lzlv-longLORA-70b-rope8-32k-fp16
repo_type: model
status: open
target_branch: null
title: Quantization calibration dataset
