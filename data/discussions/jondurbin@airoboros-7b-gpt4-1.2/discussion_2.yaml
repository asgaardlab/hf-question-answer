!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kriss
conflicting_files: null
created_at: 2023-06-17 18:05:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/99a9e22c0c58b8fa6eb9d1905c750f36.svg
      fullname: Ofir kr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kriss
      type: user
    createdAt: '2023-06-17T19:05:19.000Z'
    data:
      edited: false
      editors:
      - kriss
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7130101323127747
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/99a9e22c0c58b8fa6eb9d1905c750f36.svg
          fullname: Ofir kr
          isHf: false
          isPro: false
          name: kriss
          type: user
        html: "<p>Great work!</p>\n<p>Getting the following error when trying to load\
          \ it with the new ExLlama loader via WebUi - text-generation-webui/repositories/exllama/model.py\u201D\
          , line 554, in init with safe_open(self.config.model_path, framework=\u201C\
          pt\u201D, device=\u201Ccpu\u201D) as f: safetensors_rust.SafetensorError:\
          \ Error while deserializing header: HeaderTooLarge<br>Is it just a vram\
          \ issue on my end?</p>\n<p>Works well with transformers though</p>\n"
        raw: "Great work!\r\n\r\nGetting the following error when trying to load it\
          \ with the new ExLlama loader via WebUi - text-generation-webui/repositories/exllama/model.py\u201D\
          , line 554, in init with safe_open(self.config.model_path, framework=\u201C\
          pt\u201D, device=\u201Ccpu\u201D) as f: safetensors_rust.SafetensorError:\
          \ Error while deserializing header: HeaderTooLarge\r\nIs it just a vram\
          \ issue on my end?\r\n\r\nWorks well with transformers though"
        updatedAt: '2023-06-17T19:05:19.259Z'
      numEdits: 0
      reactions: []
    id: 648e03ef4d08891912181d36
    type: comment
  author: kriss
  content: "Great work!\r\n\r\nGetting the following error when trying to load it\
    \ with the new ExLlama loader via WebUi - text-generation-webui/repositories/exllama/model.py\u201D\
    , line 554, in init with safe_open(self.config.model_path, framework=\u201Cpt\u201D\
    , device=\u201Ccpu\u201D) as f: safetensors_rust.SafetensorError: Error while\
    \ deserializing header: HeaderTooLarge\r\nIs it just a vram issue on my end?\r\
    \n\r\nWorks well with transformers though"
  created_at: 2023-06-17 18:05:19+00:00
  edited: false
  hidden: false
  id: 648e03ef4d08891912181d36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-06-17T19:58:50.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9401493668556213
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>I haven''t used exllama, but taking a quick peek at the repo, it
          looks like it''s intended for the 4-but GPTQ versions of the models.  TheBloke
          has kindly quantized all of the GPTQ (and GGML) versions of all of these
          (7b through 65b).  The 7b version is here:<br><a href="https://huggingface.co/TheBloke/airoboros-7B-gpt4-1.2-GPTQ">https://huggingface.co/TheBloke/airoboros-7B-gpt4-1.2-GPTQ</a></p>

          <p>I used qlora for all versions this time, rather than a full fine-tune,
          so the smaller 7b/13b models may be a bit worse than 1.1 versions for some
          prompts but I don''t have any direct evidence for that.</p>

          <p>The 33b and 65b versions perform quite well with qlora tuning however.</p>

          '
        raw: 'I haven''t used exllama, but taking a quick peek at the repo, it looks
          like it''s intended for the 4-but GPTQ versions of the models.  TheBloke
          has kindly quantized all of the GPTQ (and GGML) versions of all of these
          (7b through 65b).  The 7b version is here:

          https://huggingface.co/TheBloke/airoboros-7B-gpt4-1.2-GPTQ


          I used qlora for all versions this time, rather than a full fine-tune, so
          the smaller 7b/13b models may be a bit worse than 1.1 versions for some
          prompts but I don''t have any direct evidence for that.


          The 33b and 65b versions perform quite well with qlora tuning however.'
        updatedAt: '2023-06-17T19:58:50.714Z'
      numEdits: 0
      reactions: []
    id: 648e107adf53671f33e3ff4b
    type: comment
  author: jondurbin
  content: 'I haven''t used exllama, but taking a quick peek at the repo, it looks
    like it''s intended for the 4-but GPTQ versions of the models.  TheBloke has kindly
    quantized all of the GPTQ (and GGML) versions of all of these (7b through 65b).  The
    7b version is here:

    https://huggingface.co/TheBloke/airoboros-7B-gpt4-1.2-GPTQ


    I used qlora for all versions this time, rather than a full fine-tune, so the
    smaller 7b/13b models may be a bit worse than 1.1 versions for some prompts but
    I don''t have any direct evidence for that.


    The 33b and 65b versions perform quite well with qlora tuning however.'
  created_at: 2023-06-17 18:58:50+00:00
  edited: false
  hidden: false
  id: 648e107adf53671f33e3ff4b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: jondurbin/airoboros-7b-gpt4-1.2
repo_type: model
status: open
target_branch: null
title: Doesn't work for me with ExLlama loader only with transformers
