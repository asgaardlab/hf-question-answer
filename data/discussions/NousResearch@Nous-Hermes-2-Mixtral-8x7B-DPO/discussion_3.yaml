!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dividebythree
conflicting_files: null
created_at: 2024-01-18 08:06:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f299d02c57d21f58280c0c8d9db2046.svg
      fullname: John Jacobs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dividebythree
      type: user
    createdAt: '2024-01-18T08:06:31.000Z'
    data:
      edited: false
      editors:
      - dividebythree
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.977434515953064
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f299d02c57d21f58280c0c8d9db2046.svg
          fullname: John Jacobs
          isHf: false
          isPro: false
          name: dividebythree
          type: user
        html: '<p>First, congrats on the excellent model! I''ve used it only a bit
          for RP, and it seems noticeably better than even Mixtral-Instruct.</p>

          <p>I know there has been lots of discussion on how to finetune Mixtral.
          E.g. if something is broken with the load balancing loss in Transformers,
          if DPO is the secret sauce... I see that you uploaded a qlora adapter for
          the DPO phase. But what about SFT? Was that also a qlora, or was it a full
          finetune?</p>

          <p>If this model ends up being as good as it seems at first glance, it would
          be very helpful for the community to know what makes it so good. Perhaps
          a full finetune SFT phase explains everything (other Mixtral finetunes all
          seem to be qlora). Any other training details that aren''t obvious that
          you could share would also be much appreciated.</p>

          '
        raw: "First, congrats on the excellent model! I've used it only a bit for\
          \ RP, and it seems noticeably better than even Mixtral-Instruct.\r\n\r\n\
          I know there has been lots of discussion on how to finetune Mixtral. E.g.\
          \ if something is broken with the load balancing loss in Transformers, if\
          \ DPO is the secret sauce... I see that you uploaded a qlora adapter for\
          \ the DPO phase. But what about SFT? Was that also a qlora, or was it a\
          \ full finetune?\r\n\r\nIf this model ends up being as good as it seems\
          \ at first glance, it would be very helpful for the community to know what\
          \ makes it so good. Perhaps a full finetune SFT phase explains everything\
          \ (other Mixtral finetunes all seem to be qlora). Any other training details\
          \ that aren't obvious that you could share would also be much appreciated."
        updatedAt: '2024-01-18T08:06:31.872Z'
      numEdits: 0
      reactions: []
    id: 65a8dc073212568def1d4c3b
    type: comment
  author: dividebythree
  content: "First, congrats on the excellent model! I've used it only a bit for RP,\
    \ and it seems noticeably better than even Mixtral-Instruct.\r\n\r\nI know there\
    \ has been lots of discussion on how to finetune Mixtral. E.g. if something is\
    \ broken with the load balancing loss in Transformers, if DPO is the secret sauce...\
    \ I see that you uploaded a qlora adapter for the DPO phase. But what about SFT?\
    \ Was that also a qlora, or was it a full finetune?\r\n\r\nIf this model ends\
    \ up being as good as it seems at first glance, it would be very helpful for the\
    \ community to know what makes it so good. Perhaps a full finetune SFT phase explains\
    \ everything (other Mixtral finetunes all seem to be qlora). Any other training\
    \ details that aren't obvious that you could share would also be much appreciated."
  created_at: 2024-01-18 08:06:31+00:00
  edited: false
  hidden: false
  id: 65a8dc073212568def1d4c3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2024-01-18T09:07:25.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9512196779251099
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>The sft phase was full finetune</p>

          '
        raw: The sft phase was full finetune
        updatedAt: '2024-01-18T09:07:25.252Z'
      numEdits: 0
      reactions: []
    id: 65a8ea4dd0803e7abc7a3428
    type: comment
  author: teknium
  content: The sft phase was full finetune
  created_at: 2024-01-18 09:07:25+00:00
  edited: false
  hidden: false
  id: 65a8ea4dd0803e7abc7a3428
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2024-01-18T09:07:42.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: it
        probability: 0.48337429761886597
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>Nothing non standard </p>

          '
        raw: 'Nothing non standard '
        updatedAt: '2024-01-18T09:07:42.736Z'
      numEdits: 0
      reactions: []
    id: 65a8ea5e26598b9955537370
    type: comment
  author: teknium
  content: 'Nothing non standard '
  created_at: 2024-01-18 09:07:42+00:00
  edited: false
  hidden: false
  id: 65a8ea5e26598b9955537370
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO
repo_type: model
status: open
target_branch: null
title: Is the SFT phase a full finetune or a lora?
