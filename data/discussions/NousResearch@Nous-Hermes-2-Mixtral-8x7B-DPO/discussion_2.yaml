!!python/object:huggingface_hub.community.DiscussionWithDetails
author: andreP
conflicting_files: null
created_at: 2024-01-17 11:19:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
      fullname: "Andr\xE9 Pankraz"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreP
      type: user
    createdAt: '2024-01-17T11:19:58.000Z'
    data:
      edited: true
      editors:
      - andreP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5773451328277588
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
          fullname: "Andr\xE9 Pankraz"
          isHf: false
          isPro: false
          name: andreP
          type: user
        html: "<p>Model falls back into completion mode on \"1+1=\" without an explicit\
          \ system prompt.</p>\n<pre><code class=\"language-bash\">$ curl http://ai1.dev.init:8000/v1/chat/completions\
          \ \\\n-H <span class=\"hljs-string\">\"Content-Type: application/json\"\
          </span> \\\n-d <span class=\"hljs-string\">'{</span>\n<span class=\"hljs-string\"\
          >\"model\": \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",</span>\n<span\
          \ class=\"hljs-string\">\"messages\": [{\"role\": \"user\", \"content\"\
          : \"1+1=\"}],</span>\n<span class=\"hljs-string\">\"temperature\": 0.2,</span>\n\
          <span class=\"hljs-string\">\"top_p\": 0.1,</span>\n<span class=\"hljs-string\"\
          >\"top_k\": 20,</span>\n<span class=\"hljs-string\">\"frequency_penalty\"\
          : 0.2</span>\n<span class=\"hljs-string\">}'</span>\n{<span class=\"hljs-string\"\
          >\"id\"</span>:<span class=\"hljs-string\">\"cmpl-8a1978632c814956a87b19832220daf9\"\
          </span>,<span class=\"hljs-string\">\"object\"</span>:<span class=\"hljs-string\"\
          >\"chat.completion\"</span>,<span class=\"hljs-string\">\"created\"</span>:60034,<span\
          \ class=\"hljs-string\">\"model\"</span>:<span class=\"hljs-string\">\"\
          NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"</span>,<span class=\"hljs-string\"\
          >\"choices\"</span>:[{<span class=\"hljs-string\">\"index\"</span>:0,<span\
          \ class=\"hljs-string\">\"message\"</span>:{<span class=\"hljs-string\"\
          >\"role\"</span>:<span class=\"hljs-string\">\"assistant\"</span>,<span\
          \ class=\"hljs-string\">\"content\"</span>:<span class=\"hljs-string\">\"\
          2\\n\\n1+1\u7B49\u4E8E2\u3002\\n\\n\u8BF7\u6CE8\u610F\uFF0C\u5728\u8FD9\u4E2A\
          \ \u7B54\u6848\u4E2D\uFF0C\u6211\u4F7F\u7528\u4E86\u81EA\u7136\u8BED\u8A00\
          \u5904\u7406\u6280\u672F\u6765\u7406\u89E3\u60A8\u7684\u95EE\u9898\uFF0C\
          \u5E76\u63D0\u4F9B\u4E86\u4E00\u4E2A\u7B80\u5355\u7684\u6570\u5B66\u7B54\
          \u6848\u3002\u5982\u679C\u60A8\u6709\u5176\u4ED6\u95EE\u9898\u6216\u9700\
          \u8981\u66F4\u8BE6\u7EC6\u7684\u89E3\u91CA\uFF0C\u8BF7\u968F\u65F6\u544A\
          \u8BC9\u6211\u3002\"</span>},<span class=\"hljs-string\">\"finish_reason\"\
          </span>:<span class=\"hljs-string\">\"stop\"</span>}],<span class=\"hljs-string\"\
          >\"usage\"</span>:{<span class=\"hljs-string\">\"prompt_tokens\"</span>:14,<span\
          \ class=\"hljs-string\">\"total_tokens\"</span>:96,<span class=\"hljs-string\"\
          >\"completion_tokens\"</span>:82}}\n</code></pre>\n<p>With other sampling\
          \ params the model starts rambling forever.</p>\n<p>It works OK with \"\
          1+1\" without trailing =<br>This seems to trigger completion mode without\
          \ reaching stop token.</p>\n<pre><code class=\"language-bash\">curl http://ai1.dev.init:8000/v1/chat/completions\
          \ -H <span class=\"hljs-string\">\"Content-Type: application/json\"</span>\
          \ -d <span class=\"hljs-string\">'{</span>\n<span class=\"hljs-string\"\
          >\"model\": \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",</span>\n<span\
          \ class=\"hljs-string\">\"messages\": [{\"role\":\"system\", \"content\"\
          : \"Just ultrashort responses!\"}, {\"role\": \"user\", \"content\": \"\
          1+1=\"}],</span>\n<span class=\"hljs-string\">\"temperature\": 0.2,</span>\n\
          <span class=\"hljs-string\">\"top_p\": 0.1,</span>\n<span class=\"hljs-string\"\
          >\"top_k\": 20,</span>\n<span class=\"hljs-string\">\"frequency_penalty\"\
          : 0.2</span>\n<span class=\"hljs-string\">}'</span>\n{<span class=\"hljs-string\"\
          >\"id\"</span>:<span class=\"hljs-string\">\"cmpl-89c1d1402873495abe254658ba342621\"\
          </span>,<span class=\"hljs-string\">\"object\"</span>:<span class=\"hljs-string\"\
          >\"chat.completion\"</span>,<span class=\"hljs-string\">\"created\"</span>:60621,<span\
          \ class=\"hljs-string\">\"model\"</span>:<span class=\"hljs-string\">\"\
          NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"</span>,<span class=\"hljs-string\"\
          >\"choices\"</span>:[{<span class=\"hljs-string\">\"index\"</span>:0,<span\
          \ class=\"hljs-string\">\"message\"</span>:{<span class=\"hljs-string\"\
          >\"role\"</span>:<span class=\"hljs-string\">\"assistant\"</span>,<span\
          \ class=\"hljs-string\">\"content\"</span>:<span class=\"hljs-string\">\"\
          2\"</span>},<span class=\"hljs-string\">\"finish_reason\"</span>:<span class=\"\
          hljs-string\">\"stop\"</span>}],<span class=\"hljs-string\">\"usage\"</span>:{<span\
          \ class=\"hljs-string\">\"prompt_tokens\"</span>:27,<span class=\"hljs-string\"\
          >\"total_tokens\"</span>:29,<span class=\"hljs-string\">\"completion_tokens\"\
          </span>:2}}\n</code></pre>\n<p>With system prompt (whatever it is), it works:</p>\n\
          <pre><code class=\"language-bash\">$ curl http://ai1.dev.init:8000/v1/chat/completions\
          \ -H <span class=\"hljs-string\">\"Content-Type: application/json\"</span>\
          \ -d <span class=\"hljs-string\">'{</span>\n<span class=\"hljs-string\"\
          >\"model\": \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",</span>\n<span\
          \ class=\"hljs-string\">\"messages\": [{\"role\":\"system\", \"content\"\
          : \"Just ultrashort responses!\"}, {\"role\": \"user\", \"content\": \"\
          1+1=\"}],</span>\n<span class=\"hljs-string\">\"temperature\": 0.2,</span>\n\
          <span class=\"hljs-string\">\"top_p\": 0.1,</span>\n<span class=\"hljs-string\"\
          >\"top_k\": 20,</span>\n<span class=\"hljs-string\">\"frequency_penalty\"\
          : 0.2</span>\n<span class=\"hljs-string\">}'</span>\n{<span class=\"hljs-string\"\
          >\"id\"</span>:<span class=\"hljs-string\">\"cmpl-3c4e1c863acc4b2ea7770467f8297c9d\"\
          </span>,<span class=\"hljs-string\">\"object\"</span>:<span class=\"hljs-string\"\
          >\"chat.completion\"</span>,<span class=\"hljs-string\">\"created\"</span>:60914,<span\
          \ class=\"hljs-string\">\"model\"</span>:<span class=\"hljs-string\">\"\
          NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"</span>,<span class=\"hljs-string\"\
          >\"choices\"</span>:[{<span class=\"hljs-string\">\"index\"</span>:0,<span\
          \ class=\"hljs-string\">\"message\"</span>:{<span class=\"hljs-string\"\
          >\"role\"</span>:<span class=\"hljs-string\">\"assistant\"</span>,<span\
          \ class=\"hljs-string\">\"content\"</span>:<span class=\"hljs-string\">\"\
          2\"</span>},<span class=\"hljs-string\">\"finish_reason\"</span>:<span class=\"\
          hljs-string\">\"stop\"</span>}],<span class=\"hljs-string\">\"usage\"</span>:{<span\
          \ class=\"hljs-string\">\"prompt_tokens\"</span>:27,<span class=\"hljs-string\"\
          >\"total_tokens\"</span>:29,<span class=\"hljs-string\">\"completion_tokens\"\
          </span>:2}}\n</code></pre>\n<p>We use vllm-openai:</p>\n<pre><code class=\"\
          language-bash\">version: <span class=\"hljs-string\">'3.8'</span>\nservices:\n\
          \  vllm-nous-hermes-mixtral-instruct:\n    image: vllm/vllm-openai\n   \
          \ container_name: vllm-nous-hermes-mixtral-instruct\n    ipc: host\n   \
          \ deploy:\n      resources:\n        reservations:\n          devices:\n\
          \            - driver: nvidia\n              capabilities: [ gpu ]\n   \
          \ environment:\n      - NVIDIA_VISIBLE_DEVICES=0,1,2,3\n    volumes:\n \
          \     - /mnt/sda/huggingface:/root/.cache/huggingface\n    ports:\n    \
          \  - <span class=\"hljs-string\">\"8000:8000\"</span>\n    <span class=\"\
          hljs-built_in\">command</span>:\n      - --model=NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\n\
          \      - --gpu-memory-utilization=0.6\n      - --tensor-parallel-size=4\n\
          \    restart: unless-stopped\n</code></pre>\n<p>This is just a minimalistic\
          \ example, we don't want to misuse the model as calculator for 1+1 ;)<br>We\
          \ had other stuff, where the model also kept outputting nonsense like the\
          \ first completion GPT-models.</p>\n<p>May be you should add some more behaviour\
          \ train data for DPO?</p>\n"
        raw: "Model falls back into completion mode on \"1+1=\" without an explicit\
          \ system prompt.\n\n```bash\n$ curl http://ai1.dev.init:8000/v1/chat/completions\
          \ \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\
          ,\n\"messages\": [{\"role\": \"user\", \"content\": \"1+1=\"}],\n\"temperature\"\
          : 0.2,\n\"top_p\": 0.1,\n\"top_k\": 20,\n\"frequency_penalty\": 0.2\n}'\n\
          {\"id\":\"cmpl-8a1978632c814956a87b19832220daf9\",\"object\":\"chat.completion\"\
          ,\"created\":60034,\"model\":\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\
          ,\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\"\
          :\"2\\n\\n1+1\u7B49\u4E8E2\u3002\\n\\n\u8BF7\u6CE8\u610F\uFF0C\u5728\u8FD9\
          \u4E2A \u7B54\u6848\u4E2D\uFF0C\u6211\u4F7F\u7528\u4E86\u81EA\u7136\u8BED\
          \u8A00\u5904\u7406\u6280\u672F\u6765\u7406\u89E3\u60A8\u7684\u95EE\u9898\
          \uFF0C\u5E76\u63D0\u4F9B\u4E86\u4E00\u4E2A\u7B80\u5355\u7684\u6570\u5B66\
          \u7B54\u6848\u3002\u5982\u679C\u60A8\u6709\u5176\u4ED6\u95EE\u9898\u6216\
          \u9700\u8981\u66F4\u8BE6\u7EC6\u7684\u89E3\u91CA\uFF0C\u8BF7\u968F\u65F6\
          \u544A\u8BC9\u6211\u3002\"},\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\"\
          :14,\"total_tokens\":96,\"completion_tokens\":82}}\n```\nWith other sampling\
          \ params the model starts rambling forever.\n\nIt works OK with \"1+1\"\
          \ without trailing =\nThis seems to trigger completion mode without reaching\
          \ stop token.\n\n```bash\ncurl http://ai1.dev.init:8000/v1/chat/completions\
          \ -H \"Content-Type: application/json\" -d '{\n\"model\": \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\
          ,\n\"messages\": [{\"role\":\"system\", \"content\": \"Just ultrashort responses!\"\
          }, {\"role\": \"user\", \"content\": \"1+1=\"}],\n\"temperature\": 0.2,\n\
          \"top_p\": 0.1,\n\"top_k\": 20,\n\"frequency_penalty\": 0.2\n}'\n{\"id\"\
          :\"cmpl-89c1d1402873495abe254658ba342621\",\"object\":\"chat.completion\"\
          ,\"created\":60621,\"model\":\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\
          ,\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\"\
          :\"2\"},\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":27,\"\
          total_tokens\":29,\"completion_tokens\":2}}\n```\nWith system prompt (whatever\
          \ it is), it works:\n```bash\n$ curl http://ai1.dev.init:8000/v1/chat/completions\
          \ -H \"Content-Type: application/json\" -d '{\n\"model\": \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\
          ,\n\"messages\": [{\"role\":\"system\", \"content\": \"Just ultrashort responses!\"\
          }, {\"role\": \"user\", \"content\": \"1+1=\"}],\n\"temperature\": 0.2,\n\
          \"top_p\": 0.1,\n\"top_k\": 20,\n\"frequency_penalty\": 0.2\n}'\n{\"id\"\
          :\"cmpl-3c4e1c863acc4b2ea7770467f8297c9d\",\"object\":\"chat.completion\"\
          ,\"created\":60914,\"model\":\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\
          ,\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\"\
          :\"2\"},\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":27,\"\
          total_tokens\":29,\"completion_tokens\":2}}\n```\n\nWe use vllm-openai:\n\
          ```bash\nversion: '3.8'\nservices:\n  vllm-nous-hermes-mixtral-instruct:\n\
          \    image: vllm/vllm-openai\n    container_name: vllm-nous-hermes-mixtral-instruct\n\
          \    ipc: host\n    deploy:\n      resources:\n        reservations:\n \
          \         devices:\n            - driver: nvidia\n              capabilities:\
          \ [ gpu ]\n    environment:\n      - NVIDIA_VISIBLE_DEVICES=0,1,2,3\n  \
          \  volumes:\n      - /mnt/sda/huggingface:/root/.cache/huggingface\n   \
          \ ports:\n      - \"8000:8000\"\n    command:\n      - --model=NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\n\
          \      - --gpu-memory-utilization=0.6\n      - --tensor-parallel-size=4\n\
          \    restart: unless-stopped\n```\n\nThis is just a minimalistic example,\
          \ we don't want to misuse the model as calculator for 1+1 ;)\nWe had other\
          \ stuff, where the model also kept outputting nonsense like the first completion\
          \ GPT-models.\n\n\nMay be you should add some more behaviour train data\
          \ for DPO?\n"
        updatedAt: '2024-01-17T11:21:27.419Z'
      numEdits: 1
      reactions: []
    id: 65a7b7de4623e107b90d0954
    type: comment
  author: andreP
  content: "Model falls back into completion mode on \"1+1=\" without an explicit\
    \ system prompt.\n\n```bash\n$ curl http://ai1.dev.init:8000/v1/chat/completions\
    \ \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\
    ,\n\"messages\": [{\"role\": \"user\", \"content\": \"1+1=\"}],\n\"temperature\"\
    : 0.2,\n\"top_p\": 0.1,\n\"top_k\": 20,\n\"frequency_penalty\": 0.2\n}'\n{\"id\"\
    :\"cmpl-8a1978632c814956a87b19832220daf9\",\"object\":\"chat.completion\",\"created\"\
    :60034,\"model\":\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",\"choices\":[{\"\
    index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"2\\n\\n1+1\u7B49\u4E8E\
    2\u3002\\n\\n\u8BF7\u6CE8\u610F\uFF0C\u5728\u8FD9\u4E2A \u7B54\u6848\u4E2D\uFF0C\
    \u6211\u4F7F\u7528\u4E86\u81EA\u7136\u8BED\u8A00\u5904\u7406\u6280\u672F\u6765\
    \u7406\u89E3\u60A8\u7684\u95EE\u9898\uFF0C\u5E76\u63D0\u4F9B\u4E86\u4E00\u4E2A\
    \u7B80\u5355\u7684\u6570\u5B66\u7B54\u6848\u3002\u5982\u679C\u60A8\u6709\u5176\
    \u4ED6\u95EE\u9898\u6216\u9700\u8981\u66F4\u8BE6\u7EC6\u7684\u89E3\u91CA\uFF0C\
    \u8BF7\u968F\u65F6\u544A\u8BC9\u6211\u3002\"},\"finish_reason\":\"stop\"}],\"\
    usage\":{\"prompt_tokens\":14,\"total_tokens\":96,\"completion_tokens\":82}}\n\
    ```\nWith other sampling params the model starts rambling forever.\n\nIt works\
    \ OK with \"1+1\" without trailing =\nThis seems to trigger completion mode without\
    \ reaching stop token.\n\n```bash\ncurl http://ai1.dev.init:8000/v1/chat/completions\
    \ -H \"Content-Type: application/json\" -d '{\n\"model\": \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\
    ,\n\"messages\": [{\"role\":\"system\", \"content\": \"Just ultrashort responses!\"\
    }, {\"role\": \"user\", \"content\": \"1+1=\"}],\n\"temperature\": 0.2,\n\"top_p\"\
    : 0.1,\n\"top_k\": 20,\n\"frequency_penalty\": 0.2\n}'\n{\"id\":\"cmpl-89c1d1402873495abe254658ba342621\"\
    ,\"object\":\"chat.completion\",\"created\":60621,\"model\":\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\
    ,\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"\
    2\"},\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":27,\"total_tokens\"\
    :29,\"completion_tokens\":2}}\n```\nWith system prompt (whatever it is), it works:\n\
    ```bash\n$ curl http://ai1.dev.init:8000/v1/chat/completions -H \"Content-Type:\
    \ application/json\" -d '{\n\"model\": \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\
    ,\n\"messages\": [{\"role\":\"system\", \"content\": \"Just ultrashort responses!\"\
    }, {\"role\": \"user\", \"content\": \"1+1=\"}],\n\"temperature\": 0.2,\n\"top_p\"\
    : 0.1,\n\"top_k\": 20,\n\"frequency_penalty\": 0.2\n}'\n{\"id\":\"cmpl-3c4e1c863acc4b2ea7770467f8297c9d\"\
    ,\"object\":\"chat.completion\",\"created\":60914,\"model\":\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\
    ,\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"\
    2\"},\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":27,\"total_tokens\"\
    :29,\"completion_tokens\":2}}\n```\n\nWe use vllm-openai:\n```bash\nversion: '3.8'\n\
    services:\n  vllm-nous-hermes-mixtral-instruct:\n    image: vllm/vllm-openai\n\
    \    container_name: vllm-nous-hermes-mixtral-instruct\n    ipc: host\n    deploy:\n\
    \      resources:\n        reservations:\n          devices:\n            - driver:\
    \ nvidia\n              capabilities: [ gpu ]\n    environment:\n      - NVIDIA_VISIBLE_DEVICES=0,1,2,3\n\
    \    volumes:\n      - /mnt/sda/huggingface:/root/.cache/huggingface\n    ports:\n\
    \      - \"8000:8000\"\n    command:\n      - --model=NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\n\
    \      - --gpu-memory-utilization=0.6\n      - --tensor-parallel-size=4\n    restart:\
    \ unless-stopped\n```\n\nThis is just a minimalistic example, we don't want to\
    \ misuse the model as calculator for 1+1 ;)\nWe had other stuff, where the model\
    \ also kept outputting nonsense like the first completion GPT-models.\n\n\nMay\
    \ be you should add some more behaviour train data for DPO?\n"
  created_at: 2024-01-17 11:19:58+00:00
  edited: true
  hidden: false
  id: 65a7b7de4623e107b90d0954
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2024-01-17T15:49:27.000Z'
    data:
      edited: true
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9817060828208923
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>I think something is wrong in general with VLLM inference, but I
          am not sure what</p>

          '
        raw: I think something is wrong in general with VLLM inference, but I am not
          sure what
        updatedAt: '2024-01-17T15:49:59.633Z'
      numEdits: 1
      reactions: []
    id: 65a7f70726598b9955fa4f7d
    type: comment
  author: teknium
  content: I think something is wrong in general with VLLM inference, but I am not
    sure what
  created_at: 2024-01-17 15:49:27+00:00
  edited: true
  hidden: false
  id: 65a7f70726598b9955fa4f7d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2024-01-17T15:50:14.000Z'
    data:
      status: closed
    id: 65a7f736d0803e7abc11e2ec
    type: status-change
  author: teknium
  created_at: 2024-01-17 15:50:14+00:00
  id: 65a7f736d0803e7abc11e2ec
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
      fullname: "Andr\xE9 Pankraz"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreP
      type: user
    createdAt: '2024-01-17T16:06:16.000Z'
    data:
      edited: false
      editors:
      - andreP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8896878361701965
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
          fullname: "Andr\xE9 Pankraz"
          isHf: false
          isPro: false
          name: andreP
          type: user
        html: '<p>Hmm, OK...also a method.<br>Other Mixtral models like the original
          one or Sauerkraut-Mixtral don''t have this problem at all, with the same
          inference server setup.<br>Just adding a system message also changes it''s
          behaviour. Just try it on your side with your setup, whatever that is.<br>vLLM
          is one of the most used server-side inference servers besider tensorrt.
          local playground servers like ollama/llama.cpp are not very relevant for
          production without continuous batching.</p>

          '
        raw: 'Hmm, OK...also a method.

          Other Mixtral models like the original one or Sauerkraut-Mixtral don''t
          have this problem at all, with the same inference server setup.

          Just adding a system message also changes it''s behaviour. Just try it on
          your side with your setup, whatever that is.

          vLLM is one of the most used server-side inference servers besider tensorrt.
          local playground servers like ollama/llama.cpp are not very relevant for
          production without continuous batching.

          '
        updatedAt: '2024-01-17T16:06:16.909Z'
      numEdits: 0
      reactions: []
    id: 65a7faf85e3029d4d560b220
    type: comment
  author: andreP
  content: 'Hmm, OK...also a method.

    Other Mixtral models like the original one or Sauerkraut-Mixtral don''t have this
    problem at all, with the same inference server setup.

    Just adding a system message also changes it''s behaviour. Just try it on your
    side with your setup, whatever that is.

    vLLM is one of the most used server-side inference servers besider tensorrt. local
    playground servers like ollama/llama.cpp are not very relevant for production
    without continuous batching.

    '
  created_at: 2024-01-17 16:06:16+00:00
  edited: false
  hidden: false
  id: 65a7faf85e3029d4d560b220
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2024-01-18T09:11:34.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9019182324409485
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: "<blockquote>\n<p>Hmm, OK...also a method.<br>Other Mixtral models like\
          \ the original one or Sauerkraut-Mixtral don't have this problem at all,\
          \ with the same inference server setup.<br>Just adding a system message\
          \ also changes it's behaviour. Just try it on your side with your setup,\
          \ whatever that is.<br>vLLM is one of the most used server-side inference\
          \ servers besider tensorrt. local playground servers like ollama/llama.cpp\
          \ are not very relevant for production without continuous batching.</p>\n\
          </blockquote>\n<p>You can compare your outputs with that of huggingchat\
          \ or lmstudio, they are fine without system prompts, it is vllm but I don\u2019\
          t know why</p>\n"
        raw: "> Hmm, OK...also a method.\n> Other Mixtral models like the original\
          \ one or Sauerkraut-Mixtral don't have this problem at all, with the same\
          \ inference server setup.\n> Just adding a system message also changes it's\
          \ behaviour. Just try it on your side with your setup, whatever that is.\n\
          > vLLM is one of the most used server-side inference servers besider tensorrt.\
          \ local playground servers like ollama/llama.cpp are not very relevant for\
          \ production without continuous batching.\n\nYou can compare your outputs\
          \ with that of huggingchat or lmstudio, they are fine without system prompts,\
          \ it is vllm but I don\u2019t know why"
        updatedAt: '2024-01-18T09:11:34.029Z'
      numEdits: 0
      reactions: []
    id: 65a8eb461972c812ea9e5ffb
    type: comment
  author: teknium
  content: "> Hmm, OK...also a method.\n> Other Mixtral models like the original one\
    \ or Sauerkraut-Mixtral don't have this problem at all, with the same inference\
    \ server setup.\n> Just adding a system message also changes it's behaviour. Just\
    \ try it on your side with your setup, whatever that is.\n> vLLM is one of the\
    \ most used server-side inference servers besider tensorrt. local playground servers\
    \ like ollama/llama.cpp are not very relevant for production without continuous\
    \ batching.\n\nYou can compare your outputs with that of huggingchat or lmstudio,\
    \ they are fine without system prompts, it is vllm but I don\u2019t know why"
  created_at: 2024-01-18 09:11:34+00:00
  edited: false
  hidden: false
  id: 65a8eb461972c812ea9e5ffb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
      fullname: "Andr\xE9 Pankraz"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreP
      type: user
    createdAt: '2024-01-18T10:16:18.000Z'
    data:
      edited: false
      editors:
      - andreP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9511911273002625
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c06bf31639cc2bba7d9fb0e5e055047.svg
          fullname: "Andr\xE9 Pankraz"
          isHf: false
          isPro: false
          name: andreP
          type: user
        html: "<p>First try on hugging chat (with no previous system message or chat):</p>\n\
          <p>1+1=</p>\n<p>Output:</p>\n<hr>\n<p>2</p>\n<p>1+1\u7B49\u4E8E2\u3002</p>\n\
          <hr>\n<p>We see - same problem - even though it's shorter, but I don't know\
          \ what they use for sampling params.<br>It's not vLLM. Test UIs or local\
          \ play servers arn't really relevant - without proper support for real practice\
          \ inference servers the model is not very useful.</p>\n<p>I think the model\
          \ isn't trained sufficiently with a default system prompt or missing system\
          \ prompt. He falls into strange patterns.</p>\n<p>It is as it is, I didn't\
          \ expect to change the model. It's just feedback, so that you hopefully\
          \ can improve future model. I'm a huge fan of your work.</p>\n"
        raw: "First try on hugging chat (with no previous system message or chat):\n\
          \n1+1=\n\nOutput:\n\n---\n2\n\n1+1\u7B49\u4E8E2\u3002\n\n---\n\nWe see -\
          \ same problem - even though it's shorter, but I don't know what they use\
          \ for sampling params.\nIt's not vLLM. Test UIs or local play servers arn't\
          \ really relevant - without proper support for real practice inference servers\
          \ the model is not very useful.\n\nI think the model isn't trained sufficiently\
          \ with a default system prompt or missing system prompt. He falls into strange\
          \ patterns.\n\nIt is as it is, I didn't expect to change the model. It's\
          \ just feedback, so that you hopefully can improve future model. I'm a huge\
          \ fan of your work.\n"
        updatedAt: '2024-01-18T10:16:18.029Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MaziyarPanahi
    id: 65a8fa725e3029d4d5c8df0a
    type: comment
  author: andreP
  content: "First try on hugging chat (with no previous system message or chat):\n\
    \n1+1=\n\nOutput:\n\n---\n2\n\n1+1\u7B49\u4E8E2\u3002\n\n---\n\nWe see - same\
    \ problem - even though it's shorter, but I don't know what they use for sampling\
    \ params.\nIt's not vLLM. Test UIs or local play servers arn't really relevant\
    \ - without proper support for real practice inference servers the model is not\
    \ very useful.\n\nI think the model isn't trained sufficiently with a default\
    \ system prompt or missing system prompt. He falls into strange patterns.\n\n\
    It is as it is, I didn't expect to change the model. It's just feedback, so that\
    \ you hopefully can improve future model. I'm a huge fan of your work.\n"
  created_at: 2024-01-18 10:16:18+00:00
  edited: false
  hidden: false
  id: 65a8fa725e3029d4d5c8df0a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO
repo_type: model
status: closed
target_branch: null
title: ChatML without SystemPrompt doesn't work well
