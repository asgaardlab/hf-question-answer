!!python/object:huggingface_hub.community.DiscussionWithDetails
author: prajdabre
conflicting_files: null
created_at: 2022-08-08 13:06:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7b47bfaba15dfaf37ce38cb0216f3102.svg
      fullname: Raj Dabre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prajdabre
      type: user
    createdAt: '2022-08-08T14:06:56.000Z'
    data:
      edited: true
      editors:
      - prajdabre
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7b47bfaba15dfaf37ce38cb0216f3102.svg
          fullname: Raj Dabre
          isHf: false
          isPro: false
          name: prajdabre
          type: user
        html: '<p>I modified the model loading line as follows:</p>

          <p>''''''<br>model = BloomForCausalLM.from_pretrained(''joaoalvarenga/bloom-8bit'',
          low_cpu_mem_usage=True, device_map="auto")<br>''''''</p>

          <p>Using device_map="auto" automatically moves everything to the GPU but
          when I try generating, I get the following:</p>

          <p>''''''<br>RuntimeError: CUDA out of memory. Tried to allocate 13.40 GiB
          (GPU 0; 31.75 GiB total capacity; 19.45 GiB already allocated; 11.20 GiB
          free; 19.47 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt;
          allocated memory try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CON<br>''''''</p>

          <p>I have 8 32GB gpus so everything fits. Am I doing this right, or is there
          a specific way to do decoding using a GPU?</p>

          <p>Thanks in advance.</p>

          '
        raw: 'I modified the model loading line as follows:


          ''''''

          model = BloomForCausalLM.from_pretrained(''joaoalvarenga/bloom-8bit'', low_cpu_mem_usage=True,
          device_map="auto")

          ''''''


          Using device_map="auto" automatically moves everything to the GPU but when
          I try generating, I get the following:


          ''''''

          RuntimeError: CUDA out of memory. Tried to allocate 13.40 GiB (GPU 0; 31.75
          GiB total capacity; 19.45 GiB already allocated; 11.20 GiB free; 19.47 GiB
          reserved in total by PyTorch) If reserved memory is >> allocated memory
          try setting max_split_size_mb to avoid fragmentation.  See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CON

          ''''''


          I have 8 32GB gpus so everything fits. Am I doing this right, or is there
          a specific way to do decoding using a GPU?


          Thanks in advance.'
        updatedAt: '2022-08-08T14:07:32.327Z'
      numEdits: 1
      reactions: []
    id: 62f11880335ce25164bc6df3
    type: comment
  author: prajdabre
  content: 'I modified the model loading line as follows:


    ''''''

    model = BloomForCausalLM.from_pretrained(''joaoalvarenga/bloom-8bit'', low_cpu_mem_usage=True,
    device_map="auto")

    ''''''


    Using device_map="auto" automatically moves everything to the GPU but when I try
    generating, I get the following:


    ''''''

    RuntimeError: CUDA out of memory. Tried to allocate 13.40 GiB (GPU 0; 31.75 GiB
    total capacity; 19.45 GiB already allocated; 11.20 GiB free; 19.47 GiB reserved
    in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb
    to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CON

    ''''''


    I have 8 32GB gpus so everything fits. Am I doing this right, or is there a specific
    way to do decoding using a GPU?


    Thanks in advance.'
  created_at: 2022-08-08 13:06:56+00:00
  edited: true
  hidden: false
  id: 62f11880335ce25164bc6df3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/7b47bfaba15dfaf37ce38cb0216f3102.svg
      fullname: Raj Dabre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prajdabre
      type: user
    createdAt: '2022-08-08T14:07:06.000Z'
    data:
      from: How to use this on GPUS?
      to: How to use this on GPUs?
    id: 62f1188a2fb21ccff0d01ec8
    type: title-change
  author: prajdabre
  created_at: 2022-08-08 13:07:06+00:00
  id: 62f1188a2fb21ccff0d01ec8
  new_title: How to use this on GPUs?
  old_title: How to use this on GPUS?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7b47bfaba15dfaf37ce38cb0216f3102.svg
      fullname: Raj Dabre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prajdabre
      type: user
    createdAt: '2022-08-26T13:21:00.000Z'
    data:
      status: closed
    id: 6308c8bce3246e2be8897869
    type: status-change
  author: prajdabre
  created_at: 2022-08-26 12:21:00+00:00
  id: 6308c8bce3246e2be8897869
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: joaoalvarenga/bloom-8bit
repo_type: model
status: closed
target_branch: null
title: How to use this on GPUs?
