!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BigDeeper
conflicting_files: null
created_at: 2022-12-12 16:04:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ace4395d175d9c1d3d11f191c3709b0.svg
      fullname: Big Deeper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BigDeeper
      type: user
    createdAt: '2022-12-12T16:04:41.000Z'
    data:
      edited: false
      editors:
      - BigDeeper
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ace4395d175d9c1d3d11f191c3709b0.svg
          fullname: Big Deeper
          isHf: false
          isPro: false
          name: BigDeeper
          type: user
        html: '<p>Although on disk the model takes up 180GB, as python builds it up
          in RAM, it runs out of memory. I am not sure if data gets loaded for inference
          at full 32bit precision or it is just Python''s needs for extra RAM wrt.
          to how it represents data.</p>

          <p>Does numpy have 8bit float support?</p>

          '
        raw: "Although on disk the model takes up 180GB, as python builds it up in\
          \ RAM, it runs out of memory. I am not sure if data gets loaded for inference\
          \ at full 32bit precision or it is just Python's needs for extra RAM wrt.\
          \ to how it represents data.\r\n\r\nDoes numpy have 8bit float support?"
        updatedAt: '2022-12-12T16:04:41.999Z'
      numEdits: 0
      reactions: []
    id: 639751193cd6591aebc09cce
    type: comment
  author: BigDeeper
  content: "Although on disk the model takes up 180GB, as python builds it up in RAM,\
    \ it runs out of memory. I am not sure if data gets loaded for inference at full\
    \ 32bit precision or it is just Python's needs for extra RAM wrt. to how it represents\
    \ data.\r\n\r\nDoes numpy have 8bit float support?"
  created_at: 2022-12-12 16:04:41+00:00
  edited: false
  hidden: false
  id: 639751193cd6591aebc09cce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b98a020e1457053a38bb72c0e618cf4b.svg
      fullname: Eli E
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: okoyl3
      type: user
    createdAt: '2022-12-15T16:21:02.000Z'
    data:
      edited: false
      editors:
      - okoyl3
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b98a020e1457053a38bb72c0e618cf4b.svg
          fullname: Eli E
          isHf: false
          isPro: false
          name: okoyl3
          type: user
        html: '<p>In my machine I needed about 370GB of memory just to load the model,
          after it loads, it drops to 170GB.<br>But there is virtually no cpu parallelization
          (in my case), so getting prompt response is slow.</p>

          '
        raw: 'In my machine I needed about 370GB of memory just to load the model,
          after it loads, it drops to 170GB.

          But there is virtually no cpu parallelization (in my case), so getting prompt
          response is slow.'
        updatedAt: '2022-12-15T16:21:02.843Z'
      numEdits: 0
      reactions: []
    id: 639b496e621e87aa0d4283a8
    type: comment
  author: okoyl3
  content: 'In my machine I needed about 370GB of memory just to load the model, after
    it loads, it drops to 170GB.

    But there is virtually no cpu parallelization (in my case), so getting prompt
    response is slow.'
  created_at: 2022-12-15 16:21:02+00:00
  edited: false
  hidden: false
  id: 639b496e621e87aa0d4283a8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: joaoalvarenga/bloom-8bit
repo_type: model
status: open
target_branch: null
title: Even 330GB of host RAM is not enough
