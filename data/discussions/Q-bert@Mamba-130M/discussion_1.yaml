!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kabumbus
conflicting_files: null
created_at: 2023-12-26 21:26:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/557d7fa74d7ec23c6726f1c16986b032.svg
      fullname: Ja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kabumbus
      type: user
    createdAt: '2023-12-26T21:26:14.000Z'
    data:
      edited: false
      editors:
      - Kabumbus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4398089051246643
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/557d7fa74d7ec23c6726f1c16986b032.svg
          fullname: Ja
          isHf: false
          isPro: false
          name: Kabumbus
          type: user
        html: "<p>Getting</p>\n<pre><code>-&gt; 2770 outputs = model(**inputs)\n \
          \  2771 # Save past state if it exists\n   2772 # TODO: this needs to be\
          \ fixed and made cleaner later.\n   2773 if self.args.past_index &gt;= 0:\n\
          \nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518,\
          \ in Module._wrapped_call_impl(self, *args, **kwargs)\n   1516     return\
          \ self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1517\
          \ else:\n-&gt; 1518     return self._call_impl(*args, **kwargs)\n\nFile\
          \ /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1522 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1523 # this function,\
          \ and just call forward.\n   1524 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1525         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1526        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1527  \
          \   return forward_call(*args, **kwargs)\n   1529 try:\n   1530     result\
          \ = None\n\nFile /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:680,\
          \ in convert_outputs_to_fp32.&lt;locals&gt;.forward(*args, **kwargs)\n \
          \   679 def forward(*args, **kwargs):\n--&gt; 680     return model_forward(*args,\
          \ **kwargs)\n\nFile /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:668,\
          \ in ConvertOutputsToFp32.__call__(self, *args, **kwargs)\n    667 def __call__(self,\
          \ *args, **kwargs):\n--&gt; 668     return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))\n\nFile /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:16,\
          \ in autocast_decorator.&lt;locals&gt;.decorate_autocast(*args, **kwargs)\n\
          \     13 @functools.wraps(func)\n     14 def decorate_autocast(*args, **kwargs):\n\
          \     15     with autocast_instance:\n---&gt; 16         return func(*args,\
          \ **kwargs)\n\nTypeError: MambaForCausalLM.forward() got an unexpected keyword\
          \ argument 'attention_mask'\n</code></pre>\n<p>On code like:</p>\n<pre><code>model_name\
          \ = \"Q-bert/Mamba-130M\"#\"euclaise/falcon_1b_stage2\"#\"mosaicml/mpt-7b-storywriter\"\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\
          tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n#...\n\ntraining_args\
          \ = TrainingArguments(\n    output_dir=\"./results\",\n    max_steps=100000,\n\
          \    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"./logs\"\
          ,\n    logging_steps=500,\n    fp16=True,\n    learning_rate=1e-5,\n   \
          \ evaluation_strategy=\"steps\",  # Add this line\n    eval_steps=10000,\
          \  # Add this line to run validation every 1000 steps\n    save_steps=10000,\n\
          \    neftune_noise_alpha=5,\n)\n\ntrainer = Trainer( \n    model=model,\n\
          \    tokenizer=tokenizer,\n    train_dataset=tokenized_train_data[\"train\"\
          ],\n    eval_dataset=tokenized_val_data[\"validation\"],\n    args=training_args\n\
          )\n\ntrainer.train()\n</code></pre>\n<p>Also fails on SFTTrainer.</p>\n"
        raw: "Getting\r\n```\r\n-> 2770 outputs = model(**inputs)\r\n   2771 # Save\
          \ past state if it exists\r\n   2772 # TODO: this needs to be fixed and\
          \ made cleaner later.\r\n   2773 if self.args.past_index >= 0:\r\n\r\nFile\
          \ /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518,\
          \ in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return\
          \ self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n  \
          \ 1517 else:\r\n-> 1518     return self._call_impl(*args, **kwargs)\r\n\r\
          \nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527,\
          \ in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\r\n   1523 # this\
          \ function, and just call forward.\r\n   1524 if not (self._backward_hooks\
          \ or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\
          \n   1525         or _global_backward_pre_hooks or _global_backward_hooks\r\
          \n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\
          \n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n \
          \  1530     result = None\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:680,\
          \ in convert_outputs_to_fp32.<locals>.forward(*args, **kwargs)\r\n    679\
          \ def forward(*args, **kwargs):\r\n--> 680     return model_forward(*args,\
          \ **kwargs)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:668,\
          \ in ConvertOutputsToFp32.__call__(self, *args, **kwargs)\r\n    667 def\
          \ __call__(self, *args, **kwargs):\r\n--> 668     return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:16,\
          \ in autocast_decorator.<locals>.decorate_autocast(*args, **kwargs)\r\n\
          \     13 @functools.wraps(func)\r\n     14 def decorate_autocast(*args,\
          \ **kwargs):\r\n     15     with autocast_instance:\r\n---> 16         return\
          \ func(*args, **kwargs)\r\n\r\nTypeError: MambaForCausalLM.forward() got\
          \ an unexpected keyword argument 'attention_mask'\r\n```\r\n\r\nOn code\
          \ like:\r\n```\r\nmodel_name = \"Q-bert/Mamba-130M\"#\"euclaise/falcon_1b_stage2\"\
          #\"mosaicml/mpt-7b-storywriter\"\r\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\
          \ trust_remote_code=True)\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\
          \n\r\n#...\r\n\r\ntraining_args = TrainingArguments(\r\n    output_dir=\"\
          ./results\",\r\n    max_steps=100000,\r\n    warmup_steps=500,\r\n    weight_decay=0.01,\r\
          \n    logging_dir=\"./logs\",\r\n    logging_steps=500,\r\n    fp16=True,\r\
          \n    learning_rate=1e-5,\r\n    evaluation_strategy=\"steps\",  # Add this\
          \ line\r\n    eval_steps=10000,  # Add this line to run validation every\
          \ 1000 steps\r\n    save_steps=10000,\r\n    neftune_noise_alpha=5,\r\n\
          )\r\n\r\ntrainer = Trainer( \r\n    model=model,\r\n    tokenizer=tokenizer,\r\
          \n    train_dataset=tokenized_train_data[\"train\"],\r\n    eval_dataset=tokenized_val_data[\"\
          validation\"],\r\n    args=training_args\r\n)\r\n\r\ntrainer.train()\r\n\
          ```\r\n\r\nAlso fails on SFTTrainer."
        updatedAt: '2023-12-26T21:26:14.935Z'
      numEdits: 0
      reactions: []
    id: 658b44f67459b6e471aab985
    type: comment
  author: Kabumbus
  content: "Getting\r\n```\r\n-> 2770 outputs = model(**inputs)\r\n   2771 # Save\
    \ past state if it exists\r\n   2772 # TODO: this needs to be fixed and made cleaner\
    \ later.\r\n   2773 if self.args.past_index >= 0:\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518,\
    \ in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1516     return self._compiled_call_impl(*args,\
    \ **kwargs)  # type: ignore[misc]\r\n   1517 else:\r\n-> 1518     return self._call_impl(*args,\
    \ **kwargs)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527,\
    \ in Module._call_impl(self, *args, **kwargs)\r\n   1522 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\r\n   1523 # this function,\
    \ and just call forward.\r\n   1524 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\r\n   1525         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\r\n   1526         or _global_forward_hooks or _global_forward_pre_hooks):\r\
    \n-> 1527     return forward_call(*args, **kwargs)\r\n   1529 try:\r\n   1530\
    \     result = None\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:680,\
    \ in convert_outputs_to_fp32.<locals>.forward(*args, **kwargs)\r\n    679 def\
    \ forward(*args, **kwargs):\r\n--> 680     return model_forward(*args, **kwargs)\r\
    \n\r\nFile /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:668,\
    \ in ConvertOutputsToFp32.__call__(self, *args, **kwargs)\r\n    667 def __call__(self,\
    \ *args, **kwargs):\r\n--> 668     return convert_to_fp32(self.model_forward(*args,\
    \ **kwargs))\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:16,\
    \ in autocast_decorator.<locals>.decorate_autocast(*args, **kwargs)\r\n     13\
    \ @functools.wraps(func)\r\n     14 def decorate_autocast(*args, **kwargs):\r\n\
    \     15     with autocast_instance:\r\n---> 16         return func(*args, **kwargs)\r\
    \n\r\nTypeError: MambaForCausalLM.forward() got an unexpected keyword argument\
    \ 'attention_mask'\r\n```\r\n\r\nOn code like:\r\n```\r\nmodel_name = \"Q-bert/Mamba-130M\"\
    #\"euclaise/falcon_1b_stage2\"#\"mosaicml/mpt-7b-storywriter\"\r\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\
    \ trust_remote_code=True)\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\
    \n\r\n#...\r\n\r\ntraining_args = TrainingArguments(\r\n    output_dir=\"./results\"\
    ,\r\n    max_steps=100000,\r\n    warmup_steps=500,\r\n    weight_decay=0.01,\r\
    \n    logging_dir=\"./logs\",\r\n    logging_steps=500,\r\n    fp16=True,\r\n\
    \    learning_rate=1e-5,\r\n    evaluation_strategy=\"steps\",  # Add this line\r\
    \n    eval_steps=10000,  # Add this line to run validation every 1000 steps\r\n\
    \    save_steps=10000,\r\n    neftune_noise_alpha=5,\r\n)\r\n\r\ntrainer = Trainer(\
    \ \r\n    model=model,\r\n    tokenizer=tokenizer,\r\n    train_dataset=tokenized_train_data[\"\
    train\"],\r\n    eval_dataset=tokenized_val_data[\"validation\"],\r\n    args=training_args\r\
    \n)\r\n\r\ntrainer.train()\r\n```\r\n\r\nAlso fails on SFTTrainer."
  created_at: 2023-12-26 21:26:14+00:00
  edited: false
  hidden: false
  id: 658b44f67459b6e471aab985
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
      fullname: "Talha R\xFCzgar Akku\u015F"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Q-bert
      type: user
    createdAt: '2023-12-27T05:56:52.000Z'
    data:
      edited: false
      editors:
      - Q-bert
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9553408026695251
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
          fullname: "Talha R\xFCzgar Akku\u015F"
          isHf: false
          isPro: false
          name: Q-bert
          type: user
        html: '<p>Yes so i''m working on this. Trainer does not support. I will solve
          but i am creating new trainer class . And this isn''t gonna be happen<br>But
          there is an error in the  my class, I am trying to solve it right now, I
          am having some problems with loss.</p>

          '
        raw: 'Yes so i''m working on this. Trainer does not support. I will solve
          but i am creating new trainer class . And this isn''t gonna be happen

          But there is an error in the  my class, I am trying to solve it right now,
          I am having some problems with loss.'
        updatedAt: '2023-12-27T05:56:52.485Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Kabumbus
    id: 658bbca463bf36e4583df5f0
    type: comment
  author: Q-bert
  content: 'Yes so i''m working on this. Trainer does not support. I will solve but
    i am creating new trainer class . And this isn''t gonna be happen

    But there is an error in the  my class, I am trying to solve it right now, I am
    having some problems with loss.'
  created_at: 2023-12-27 05:56:52+00:00
  edited: false
  hidden: false
  id: 658bbca463bf36e4583df5f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
      fullname: "Talha R\xFCzgar Akku\u015F"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Q-bert
      type: user
    createdAt: '2023-12-27T06:30:54.000Z'
    data:
      edited: false
      editors:
      - Q-bert
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2730248272418976
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
          fullname: "Talha R\xFCzgar Akku\u015F"
          isHf: false
          isPro: false
          name: Q-bert
          type: user
        html: "<pre><code>from transformers import Trainer ,TrainingArguments\nimport\
          \ torch\nimport os\n\n\nclass MambaTrainer(Trainer):\n    def compute_loss(self,\
          \ model, inputs, return_outputs=False):\n        input_ids = inputs.pop(\"\
          input_ids\")\n        lm_logits = model(input_ids)[0]\n\n        labels\
          \ = input_ids.to(lm_logits.device)\n        shift_logits = lm_logits[:,\
          \ :-1, :].contiguous()\n        labels = labels[:, 1:].contiguous()\n\n\
          \        loss_fct = torch.nn.CrossEntropyLoss()\n        lm_loss = loss_fct(shift_logits.view(-1,\
          \ shift_logits.size(-1)), labels.view(-1))\n\n        return lm_loss\n</code></pre>\n\
          <p>You can use this trainer but fp16 must be <strong>False</strong>  else\
          \ loss will be NaN or 0.</p>\n"
        raw: "```\nfrom transformers import Trainer ,TrainingArguments\nimport torch\n\
          import os\n\n\nclass MambaTrainer(Trainer):\n    def compute_loss(self,\
          \ model, inputs, return_outputs=False):\n        input_ids = inputs.pop(\"\
          input_ids\")\n        lm_logits = model(input_ids)[0]\n\n        labels\
          \ = input_ids.to(lm_logits.device)\n        shift_logits = lm_logits[:,\
          \ :-1, :].contiguous()\n        labels = labels[:, 1:].contiguous()\n\n\
          \        loss_fct = torch.nn.CrossEntropyLoss()\n        lm_loss = loss_fct(shift_logits.view(-1,\
          \ shift_logits.size(-1)), labels.view(-1))\n\n        return lm_loss\n```\n\
          You can use this trainer but fp16 must be **False**  else loss will be NaN\
          \ or 0."
        updatedAt: '2023-12-27T06:30:54.831Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - Kabumbus
    id: 658bc49e255b4737234cf520
    type: comment
  author: Q-bert
  content: "```\nfrom transformers import Trainer ,TrainingArguments\nimport torch\n\
    import os\n\n\nclass MambaTrainer(Trainer):\n    def compute_loss(self, model,\
    \ inputs, return_outputs=False):\n        input_ids = inputs.pop(\"input_ids\"\
    )\n        lm_logits = model(input_ids)[0]\n\n        labels = input_ids.to(lm_logits.device)\n\
    \        shift_logits = lm_logits[:, :-1, :].contiguous()\n        labels = labels[:,\
    \ 1:].contiguous()\n\n        loss_fct = torch.nn.CrossEntropyLoss()\n       \
    \ lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n\
    \n        return lm_loss\n```\nYou can use this trainer but fp16 must be **False**\
    \  else loss will be NaN or 0."
  created_at: 2023-12-27 06:30:54+00:00
  edited: false
  hidden: false
  id: 658bc49e255b4737234cf520
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
      fullname: "Talha R\xFCzgar Akku\u015F"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Q-bert
      type: user
    createdAt: '2023-12-27T10:45:27.000Z'
    data:
      edited: false
      editors:
      - Q-bert
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9771395325660706
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
          fullname: "Talha R\xFCzgar Akku\u015F"
          isHf: false
          isPro: false
          name: Q-bert
          type: user
        html: '<p>I will solve fp16 problem as soon as possible. I''m searching why
          i guess norm is problem. Like old t5 models.</p>

          '
        raw: I will solve fp16 problem as soon as possible. I'm searching why i guess
          norm is problem. Like old t5 models.
        updatedAt: '2023-12-27T10:45:27.304Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Kabumbus
    id: 658c0047796d6fdf73a7316f
    type: comment
  author: Q-bert
  content: I will solve fp16 problem as soon as possible. I'm searching why i guess
    norm is problem. Like old t5 models.
  created_at: 2023-12-27 10:45:27+00:00
  edited: false
  hidden: false
  id: 658c0047796d6fdf73a7316f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/557d7fa74d7ec23c6726f1c16986b032.svg
      fullname: Ja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kabumbus
      type: user
    createdAt: '2023-12-27T10:52:07.000Z'
    data:
      edited: false
      editors:
      - Kabumbus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7513327598571777
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/557d7fa74d7ec23c6726f1c16986b032.svg
          fullname: Ja
          isHf: false
          isPro: false
          name: Kabumbus
          type: user
        html: '<p>Thank you! That is great news!=)</p>

          '
        raw: Thank you! That is great news!=)
        updatedAt: '2023-12-27T10:52:07.764Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - Q-bert
    id: 658c01d760969e6d66fa15f7
    type: comment
  author: Kabumbus
  content: Thank you! That is great news!=)
  created_at: 2023-12-27 10:52:07+00:00
  edited: false
  hidden: false
  id: 658c01d760969e6d66fa15f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/557d7fa74d7ec23c6726f1c16986b032.svg
      fullname: Ja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kabumbus
      type: user
    createdAt: '2023-12-29T15:52:38.000Z'
    data:
      edited: false
      editors:
      - Kabumbus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7907757759094238
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/557d7fa74d7ec23c6726f1c16986b032.svg
          fullname: Ja
          isHf: false
          isPro: false
          name: Kabumbus
          type: user
        html: '<p>Tried it out... And oh boy it is Slooooooow to finetune on 4090...
          200h...<br>vs 2h for similar sized gpt2, and even 8h for falcon-1b...</p>

          '
        raw: 'Tried it out... And oh boy it is Slooooooow to finetune on 4090... 200h...

          vs 2h for similar sized gpt2, and even 8h for falcon-1b...'
        updatedAt: '2023-12-29T15:52:38.998Z'
      numEdits: 0
      reactions: []
    id: 658eeb46ac02633c0d5014a3
    type: comment
  author: Kabumbus
  content: 'Tried it out... And oh boy it is Slooooooow to finetune on 4090... 200h...

    vs 2h for similar sized gpt2, and even 8h for falcon-1b...'
  created_at: 2023-12-29 15:52:38+00:00
  edited: false
  hidden: false
  id: 658eeb46ac02633c0d5014a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
      fullname: "Talha R\xFCzgar Akku\u015F"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Q-bert
      type: user
    createdAt: '2024-01-02T17:43:58.000Z'
    data:
      edited: false
      editors:
      - Q-bert
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8708271980285645
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
          fullname: "Talha R\xFCzgar Akku\u015F"
          isHf: false
          isPro: false
          name: Q-bert
          type: user
        html: '<blockquote>

          <p>Tried it out... And oh boy it is Slooooooow to finetune on 4090... 200h...<br>vs
          2h for similar sized gpt2, and even 8h for falcon-1b...</p>

          </blockquote>

          <p><a href="https://huggingface.co/Q-bert/Mamba-370M/discussions/1">https://huggingface.co/Q-bert/Mamba-370M/discussions/1</a></p>

          <p>I answered your question at here. </p>

          '
        raw: '> Tried it out... And oh boy it is Slooooooow to finetune on 4090...
          200h...

          > vs 2h for similar sized gpt2, and even 8h for falcon-1b...


          https://huggingface.co/Q-bert/Mamba-370M/discussions/1


          I answered your question at here. '
        updatedAt: '2024-01-02T17:43:58.147Z'
      numEdits: 0
      reactions: []
    id: 65944b5e0f4519bfc2221572
    type: comment
  author: Q-bert
  content: '> Tried it out... And oh boy it is Slooooooow to finetune on 4090... 200h...

    > vs 2h for similar sized gpt2, and even 8h for falcon-1b...


    https://huggingface.co/Q-bert/Mamba-370M/discussions/1


    I answered your question at here. '
  created_at: 2024-01-02 17:43:58+00:00
  edited: false
  hidden: false
  id: 65944b5e0f4519bfc2221572
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Q-bert/Mamba-130M
repo_type: model
status: open
target_branch: null
title: How can this be trained using trainer?
