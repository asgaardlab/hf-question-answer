!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mmoya
conflicting_files: null
created_at: 2023-07-11 18:29:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/05eb054fd48120f9a05c869495ff28e3.svg
      fullname: Maria Moya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mmoya
      type: user
    createdAt: '2023-07-11T19:29:53.000Z'
    data:
      edited: true
      editors:
      - mmoya
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5750507712364197
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/05eb054fd48120f9a05c869495ff28e3.svg
          fullname: Maria Moya
          isHf: false
          isPro: false
          name: mmoya
          type: user
        html: "<p>Hello, I'm trying to load the following model using 8 bit precision</p>\n\
          <pre><code>from transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"\
          philschmid/flan-t5-xxl-sharded-fp16\", load_in_8bit=True,device_map='auto')\n\
          </code></pre>\n<p>but whenever I try loading using <code>load_in_8bit</code>\
          \ I run into the traceback below. I'm currently using a <code>ml.p3.2xlarge</code>\
          \ instance via Sagemaker. Would greatly appreciate any help on this <span\
          \ data-props=\"{&quot;user&quot;:&quot;philschmid&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/philschmid\">@<span class=\"\
          underline\">philschmid</span></a></span>\n\n\t</span></span> </p>\n<pre><code>ValueError\
          \                                Traceback (most recent call last)\nCell\
          \ In[134], line 3\n      1 from transformers import AutoModelForSeq2SeqLM\n\
          ----&gt; 3 model = AutoModelForSeq2SeqLM.from_pretrained(\"philschmid/flan-t5-xxl-sharded-fp16\"\
          , load_in_8bit=True,device_map='auto')\n\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    469 elif type(config) in cls._model_mapping.keys():\n\
          \    470     model_class = _get_model_class(config, cls._model_mapping)\n\
          --&gt; 471     return model_class.from_pretrained(\n    472         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\n    473     )\n  \
          \  474 raise ValueError(\n    475     f\"Unrecognized configuration class\
          \ {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n\
          \    476     f\"Model type should be one of {', '.join(c.__name__ for c\
          \ in cls._model_mapping.keys())}.\"\n    477 )\n\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/modeling_utils.py:2591,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n   2587         device_map_without_lm_head = {\n\
          \   2588             key: device_map[key] for key in device_map.keys() if\
          \ key not in modules_to_not_convert\n   2589         }\n   2590        \
          \ if \"cpu\" in device_map_without_lm_head.values() or \"disk\" in device_map_without_lm_head.values():\n\
          -&gt; 2591             raise ValueError(\n   2592                 \"\"\"\
          \n   2593                 Some modules are dispatched on the CPU or the\
          \ disk. Make sure you have enough GPU RAM to fit\n   2594              \
          \   the quantized model. If you want to dispatch the model on the CPU or\
          \ the disk while keeping\n   2595                 these modules in 32-bit,\
          \ you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n\
          \   2596                 `device_map` to `from_pretrained`. Check\n   2597\
          \                 https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n\
          \   2598                 for more details.\n   2599                 \"\"\
          \"\n   2600             )\n   2601         del device_map_without_lm_head\n\
          \   2603 if from_tf:\n\nValueError: \n                        Some modules\
          \ are dispatched on the CPU or the disk. Make sure you have enough GPU RAM\
          \ to fit\n                        the quantized model. If you want to dispatch\
          \ the model on the CPU or the disk while keeping\n                     \
          \   these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True`\
          \ and pass a custom\n                        `device_map` to `from_pretrained`.\
          \ Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n\
          \                        for more details.\n</code></pre>\n"
        raw: "Hello, I'm trying to load the following model using 8 bit precision\n\
          \n```\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"\
          philschmid/flan-t5-xxl-sharded-fp16\", load_in_8bit=True,device_map='auto')\n\
          ```\nbut whenever I try loading using `load_in_8bit` I run into the traceback\
          \ below. I'm currently using a `ml.p3.2xlarge` instance via Sagemaker. Would\
          \ greatly appreciate any help on this @philschmid \n\n```\nValueError  \
          \                              Traceback (most recent call last)\nCell In[134],\
          \ line 3\n      1 from transformers import AutoModelForSeq2SeqLM\n---->\
          \ 3 model = AutoModelForSeq2SeqLM.from_pretrained(\"philschmid/flan-t5-xxl-sharded-fp16\"\
          , load_in_8bit=True,device_map='auto')\n\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    469 elif type(config) in cls._model_mapping.keys():\n\
          \    470     model_class = _get_model_class(config, cls._model_mapping)\n\
          --> 471     return model_class.from_pretrained(\n    472         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\n    473     )\n  \
          \  474 raise ValueError(\n    475     f\"Unrecognized configuration class\
          \ {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n\
          \    476     f\"Model type should be one of {', '.join(c.__name__ for c\
          \ in cls._model_mapping.keys())}.\"\n    477 )\n\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/modeling_utils.py:2591,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n   2587         device_map_without_lm_head = {\n\
          \   2588             key: device_map[key] for key in device_map.keys() if\
          \ key not in modules_to_not_convert\n   2589         }\n   2590        \
          \ if \"cpu\" in device_map_without_lm_head.values() or \"disk\" in device_map_without_lm_head.values():\n\
          -> 2591             raise ValueError(\n   2592                 \"\"\"\n\
          \   2593                 Some modules are dispatched on the CPU or the disk.\
          \ Make sure you have enough GPU RAM to fit\n   2594                 the\
          \ quantized model. If you want to dispatch the model on the CPU or the disk\
          \ while keeping\n   2595                 these modules in 32-bit, you need\
          \ to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n   2596\
          \                 `device_map` to `from_pretrained`. Check\n   2597    \
          \             https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n\
          \   2598                 for more details.\n   2599                 \"\"\
          \"\n   2600             )\n   2601         del device_map_without_lm_head\n\
          \   2603 if from_tf:\n\nValueError: \n                        Some modules\
          \ are dispatched on the CPU or the disk. Make sure you have enough GPU RAM\
          \ to fit\n                        the quantized model. If you want to dispatch\
          \ the model on the CPU or the disk while keeping\n                     \
          \   these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True`\
          \ and pass a custom\n                        `device_map` to `from_pretrained`.\
          \ Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n\
          \                        for more details.\n```\n"
        updatedAt: '2023-07-11T19:32:56.326Z'
      numEdits: 1
      reactions: []
    id: 64adadb1ca3a496191d2bacf
    type: comment
  author: mmoya
  content: "Hello, I'm trying to load the following model using 8 bit precision\n\n\
    ```\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"\
    philschmid/flan-t5-xxl-sharded-fp16\", load_in_8bit=True,device_map='auto')\n\
    ```\nbut whenever I try loading using `load_in_8bit` I run into the traceback\
    \ below. I'm currently using a `ml.p3.2xlarge` instance via Sagemaker. Would greatly\
    \ appreciate any help on this @philschmid \n\n```\nValueError                \
    \                Traceback (most recent call last)\nCell In[134], line 3\n   \
    \   1 from transformers import AutoModelForSeq2SeqLM\n----> 3 model = AutoModelForSeq2SeqLM.from_pretrained(\"\
    philschmid/flan-t5-xxl-sharded-fp16\", load_in_8bit=True,device_map='auto')\n\n\
    File ~/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n    469 elif type(config) in cls._model_mapping.keys():\n    470\
    \     model_class = _get_model_class(config, cls._model_mapping)\n--> 471    \
    \ return model_class.from_pretrained(\n    472         pretrained_model_name_or_path,\
    \ *model_args, config=config, **hub_kwargs, **kwargs\n    473     )\n    474 raise\
    \ ValueError(\n    475     f\"Unrecognized configuration class {config.__class__}\
    \ for this kind of AutoModel: {cls.__name__}.\\n\"\n    476     f\"Model type\
    \ should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\
    \n    477 )\n\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/modeling_utils.py:2591,\
    \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n   2587         device_map_without_lm_head = {\n   2588         \
    \    key: device_map[key] for key in device_map.keys() if key not in modules_to_not_convert\n\
    \   2589         }\n   2590         if \"cpu\" in device_map_without_lm_head.values()\
    \ or \"disk\" in device_map_without_lm_head.values():\n-> 2591             raise\
    \ ValueError(\n   2592                 \"\"\"\n   2593                 Some modules\
    \ are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to\
    \ fit\n   2594                 the quantized model. If you want to dispatch the\
    \ model on the CPU or the disk while keeping\n   2595                 these modules\
    \ in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n\
    \   2596                 `device_map` to `from_pretrained`. Check\n   2597   \
    \              https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n\
    \   2598                 for more details.\n   2599                 \"\"\"\n \
    \  2600             )\n   2601         del device_map_without_lm_head\n   2603\
    \ if from_tf:\n\nValueError: \n                        Some modules are dispatched\
    \ on the CPU or the disk. Make sure you have enough GPU RAM to fit\n         \
    \               the quantized model. If you want to dispatch the model on the\
    \ CPU or the disk while keeping\n                        these modules in 32-bit,\
    \ you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n   \
    \                     `device_map` to `from_pretrained`. Check\n             \
    \           https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n\
    \                        for more details.\n```\n"
  created_at: 2023-07-11 18:29:53+00:00
  edited: true
  hidden: false
  id: 64adadb1ca3a496191d2bacf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: philschmid/flan-t5-xxl-sharded-fp16
repo_type: model
status: open
target_branch: null
title: 8 bit quantized loading issues
