!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mmoya
conflicting_files: null
created_at: 2023-07-10 13:16:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/05eb054fd48120f9a05c869495ff28e3.svg
      fullname: Maria Moya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mmoya
      type: user
    createdAt: '2023-07-10T14:16:35.000Z'
    data:
      edited: true
      editors:
      - mmoya
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7334980368614197
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/05eb054fd48120f9a05c869495ff28e3.svg
          fullname: Maria Moya
          isHf: false
          isPro: false
          name: mmoya
          type: user
        html: '<p>Hello, I''m trying to fine tune this model using the alpaca dataset,
          <a rel="nofollow" href="https://github.com/declare-lab/flan-alpaca">this</a>
          repo and the command below</p>

          <pre><code>python training.py --output_dir outputs/model/xxl \

          --use_compile \

          --data_path data/train.json \

          --model_name_or_path "philschmid/flan-t5-xxl-sharded-fp16" \

          --train_batch_size 1 \

          --gradient_accumulation_steps 64 \

          --use_gradient_checkpointing \

          --use_lora

          </code></pre>

          <p>on a <code>ml.p3.2xlarge</code> instance. I also considered a <code>ml.p3.8xlarge</code>
          instance and swapped <code>--use_compile</code> to <code>--use_fsdp</code>.
          Lastly, I tried changing <a rel="nofollow" href="https://github.com/declare-lab/flan-alpaca/blob/main/training.py#L225">bf16-mixed</a>
          to <code>16-mixed</code> (I believe the p3 instance uses v100s that do not
          support bfloat16).  However,  I keep running into OOM issues<br><code>torch.cuda.OutOfMemoryError:
          CUDA out of memory. Tried to allocate 160.00 MiB (GPU 1; 15.77 GiB total
          capacity; 14.71 GiB already allocated; 133.38 MiB free; 14.71 GiB reserved
          in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try
          setting max_split_size_mb to avoid fragmentation.  See documentation for
          Memory Management and PYTORCH_CUDA_ALLOC_CONF</code></p>

          <p>going to try to add <code>load_in_8bit=True</code> <a rel="nofollow"
          href="https://github.com/declare-lab/flan-alpaca/blob/main/training.py#L101-L103">here</a>
          and <code>self.model = prepare_model_for_int8_training(self.model)</code>
          above <a rel="nofollow" href="https://github.com/declare-lab/flan-alpaca/blob/main/training.py#L114">this</a>
          line</p>

          <p>I''d greatly appreciate any help on anything else I should be considering/might
          be overlooking</p>

          '
        raw: 'Hello, I''m trying to fine tune this model using the alpaca dataset,
          [this](https://github.com/declare-lab/flan-alpaca) repo and the command
          below

          ```

          python training.py --output_dir outputs/model/xxl \

          --use_compile \

          --data_path data/train.json \

          --model_name_or_path "philschmid/flan-t5-xxl-sharded-fp16" \

          --train_batch_size 1 \

          --gradient_accumulation_steps 64 \

          --use_gradient_checkpointing \

          --use_lora

          ```

          on a `ml.p3.2xlarge` instance. I also considered a `ml.p3.8xlarge` instance
          and swapped `--use_compile` to `--use_fsdp`. Lastly, I tried changing [bf16-mixed](https://github.com/declare-lab/flan-alpaca/blob/main/training.py#L225)
          to `16-mixed` (I believe the p3 instance uses v100s that do not support
          bfloat16).  However,  I keep running into OOM issues

          ```torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00
          MiB (GPU 1; 15.77 GiB total capacity; 14.71 GiB already allocated; 133.38
          MiB free; 14.71 GiB reserved in total by PyTorch) If reserved memory is
          >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF```


          going to try to add `load_in_8bit=True` [here](https://github.com/declare-lab/flan-alpaca/blob/main/training.py#L101-L103)
          and `self.model = prepare_model_for_int8_training(self.model)` above [this](https://github.com/declare-lab/flan-alpaca/blob/main/training.py#L114)
          line


          I''d greatly appreciate any help on anything else I should be considering/might
          be overlooking'
        updatedAt: '2023-07-10T18:19:59.873Z'
      numEdits: 8
      reactions: []
    id: 64ac12c3160d690c7183f8fe
    type: comment
  author: mmoya
  content: 'Hello, I''m trying to fine tune this model using the alpaca dataset, [this](https://github.com/declare-lab/flan-alpaca)
    repo and the command below

    ```

    python training.py --output_dir outputs/model/xxl \

    --use_compile \

    --data_path data/train.json \

    --model_name_or_path "philschmid/flan-t5-xxl-sharded-fp16" \

    --train_batch_size 1 \

    --gradient_accumulation_steps 64 \

    --use_gradient_checkpointing \

    --use_lora

    ```

    on a `ml.p3.2xlarge` instance. I also considered a `ml.p3.8xlarge` instance and
    swapped `--use_compile` to `--use_fsdp`. Lastly, I tried changing [bf16-mixed](https://github.com/declare-lab/flan-alpaca/blob/main/training.py#L225)
    to `16-mixed` (I believe the p3 instance uses v100s that do not support bfloat16).  However,  I
    keep running into OOM issues

    ```torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 160.00 MiB
    (GPU 1; 15.77 GiB total capacity; 14.71 GiB already allocated; 133.38 MiB free;
    14.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory
    try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory
    Management and PYTORCH_CUDA_ALLOC_CONF```


    going to try to add `load_in_8bit=True` [here](https://github.com/declare-lab/flan-alpaca/blob/main/training.py#L101-L103)
    and `self.model = prepare_model_for_int8_training(self.model)` above [this](https://github.com/declare-lab/flan-alpaca/blob/main/training.py#L114)
    line


    I''d greatly appreciate any help on anything else I should be considering/might
    be overlooking'
  created_at: 2023-07-10 13:16:35+00:00
  edited: true
  hidden: false
  id: 64ac12c3160d690c7183f8fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/05eb054fd48120f9a05c869495ff28e3.svg
      fullname: Maria Moya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mmoya
      type: user
    createdAt: '2023-07-10T14:19:13.000Z'
    data:
      edited: true
      editors:
      - mmoya
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9646643996238708
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/05eb054fd48120f9a05c869495ff28e3.svg
          fullname: Maria Moya
          isHf: false
          isPro: false
          name: mmoya
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;philschmid&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/philschmid\">@<span class=\"\
          underline\">philschmid</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;lewtun&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/lewtun\">@<span class=\"underline\">lewtun</span></a></span>\n\
          \n\t</span></span>  I'd greatly appreciate any advice around what I might\
          \ be missing as far as fine tuning. My understanding is that I should be\
          \ able to fine tune this model on a <code>ml.p3.2xlarge</code> instance\
          \ without OOM?</p>\n"
        raw: '@philschmid @lewtun  I''d greatly appreciate any advice around what
          I might be missing as far as fine tuning. My understanding is that I should
          be able to fine tune this model on a `ml.p3.2xlarge` instance without OOM?'
        updatedAt: '2023-07-10T14:29:10.996Z'
      numEdits: 1
      reactions: []
    id: 64ac1361a8ec0fddb73bfa20
    type: comment
  author: mmoya
  content: '@philschmid @lewtun  I''d greatly appreciate any advice around what I
    might be missing as far as fine tuning. My understanding is that I should be able
    to fine tune this model on a `ml.p3.2xlarge` instance without OOM?'
  created_at: 2023-07-10 13:19:13+00:00
  edited: true
  hidden: false
  id: 64ac1361a8ec0fddb73bfa20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/05eb054fd48120f9a05c869495ff28e3.svg
      fullname: Maria Moya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mmoya
      type: user
    createdAt: '2023-07-10T14:27:21.000Z'
    data:
      from: How to efficiently fine
      to: How to efficiently fine tune
    id: 64ac154913329fb828b8d2ea
    type: title-change
  author: mmoya
  created_at: 2023-07-10 13:27:21+00:00
  id: 64ac154913329fb828b8d2ea
  new_title: How to efficiently fine tune
  old_title: How to efficiently fine
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: philschmid/flan-t5-xxl-sharded-fp16
repo_type: model
status: open
target_branch: null
title: How to efficiently fine tune
