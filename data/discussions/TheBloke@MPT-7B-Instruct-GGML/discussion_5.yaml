!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HAvietisov
conflicting_files: null
created_at: 2023-07-10 15:32:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
      fullname: Hlib Avietisov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HAvietisov
      type: user
    createdAt: '2023-07-10T16:32:40.000Z'
    data:
      edited: true
      editors:
      - HAvietisov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9288078546524048
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
          fullname: Hlib Avietisov
          isHf: false
          isPro: false
          name: HAvietisov
          type: user
        html: '<p>All quantized variations, as well as fp16, perform extremely poorly
          on extractive question answering, when inference ran via ctransformers.<br>Responses
          are extremely different on avx and avx2 engines, given the same promt, and
          in general, really bad and often don''t contain answer to the question at
          all, contrary to un-quantized MPT-7B-instruct or quantized MPT-30B-instruct</p>

          '
        raw: 'All quantized variations, as well as fp16, perform extremely poorly
          on extractive question answering, when inference ran via ctransformers.

          Responses are extremely different on avx and avx2 engines, given the same
          promt, and in general, really bad and often don''t contain answer to the
          question at all, contrary to un-quantized MPT-7B-instruct or quantized MPT-30B-instruct'
        updatedAt: '2023-07-10T17:17:14.287Z'
      numEdits: 4
      reactions: []
    id: 64ac32a8f7bbcd2f8add8d83
    type: comment
  author: HAvietisov
  content: 'All quantized variations, as well as fp16, perform extremely poorly on
    extractive question answering, when inference ran via ctransformers.

    Responses are extremely different on avx and avx2 engines, given the same promt,
    and in general, really bad and often don''t contain answer to the question at
    all, contrary to un-quantized MPT-7B-instruct or quantized MPT-30B-instruct'
  created_at: 2023-07-10 15:32:40+00:00
  edited: true
  hidden: false
  id: 64ac32a8f7bbcd2f8add8d83
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/MPT-7B-Instruct-GGML
repo_type: model
status: open
target_branch: null
title: Poor performance
