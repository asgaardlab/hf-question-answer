!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BlueAquilae
conflicting_files: null
created_at: 2023-05-18 19:53:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662165280702-62ec5f117913a6141995dbef.png?w=200&h=200&f=face
      fullname: Robert Felker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BlueAquilae
      type: user
    createdAt: '2023-05-18T20:53:01.000Z'
    data:
      edited: true
      editors:
      - BlueAquilae
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662165280702-62ec5f117913a6141995dbef.png?w=200&h=200&f=face
          fullname: Robert Felker
          isHf: false
          isPro: false
          name: BlueAquilae
          type: user
        html: '<p>llama.cpp: loading model from models/mpt-7b-instruct.ggmlv2.q5_0.bin<br>libc++abi:
          terminating due to uncaught exception of type std::runtime_error: unexpectedly
          reached end of file</p>

          <p>Also THANK YOU for all the quantization jobs you''re doing !!!</p>

          '
        raw: 'llama.cpp: loading model from models/mpt-7b-instruct.ggmlv2.q5_0.bin

          libc++abi: terminating due to uncaught exception of type std::runtime_error:
          unexpectedly reached end of file


          Also THANK YOU for all the quantization jobs you''re doing !!!'
        updatedAt: '2023-05-18T20:53:37.164Z'
      numEdits: 2
      reactions: []
    id: 6466902d875b1a86a7895ced
    type: comment
  author: BlueAquilae
  content: 'llama.cpp: loading model from models/mpt-7b-instruct.ggmlv2.q5_0.bin

    libc++abi: terminating due to uncaught exception of type std::runtime_error: unexpectedly
    reached end of file


    Also THANK YOU for all the quantization jobs you''re doing !!!'
  created_at: 2023-05-18 19:53:01+00:00
  edited: true
  hidden: false
  id: 6466902d875b1a86a7895ced
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/248fd83199b95d35b5cce33e65ab3d5c.svg
      fullname: Ivan Stepanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ivanstepanovftw
      type: user
    createdAt: '2023-05-18T20:59:25.000Z'
    data:
      edited: true
      editors:
      - ivanstepanovftw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/248fd83199b95d35b5cce33e65ab3d5c.svg
          fullname: Ivan Stepanov
          isHf: false
          isPro: false
          name: ivanstepanovftw
          type: user
        html: '<p>These files are <strong>not</strong> compatible with llama.cpp.</p>

          <p>Currently they can be used with:</p>

          <ul>

          <li>The example <code>mpt</code> binary provided with <a rel="nofollow"
          href="https://github.com/ggerganov/ggml">ggml</a></li>

          <li><a rel="nofollow" href="https://github.com/rustformers/llm">rustformers''
          llm</a></li>

          </ul>

          '
        raw: 'These files are **not** compatible with llama.cpp.


          Currently they can be used with:


          - The example `mpt` binary provided with [ggml](https://github.com/ggerganov/ggml)

          - [rustformers'' llm](https://github.com/rustformers/llm)'
        updatedAt: '2023-05-18T21:00:33.066Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - BlueAquilae
    id: 646691ad119ad94383d5d076
    type: comment
  author: ivanstepanovftw
  content: 'These files are **not** compatible with llama.cpp.


    Currently they can be used with:


    - The example `mpt` binary provided with [ggml](https://github.com/ggerganov/ggml)

    - [rustformers'' llm](https://github.com/rustformers/llm)'
  created_at: 2023-05-18 19:59:25+00:00
  edited: true
  hidden: false
  id: 646691ad119ad94383d5d076
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-18T20:59:55.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You''re welcome.</p>

          <p>But I''m afraid these models can''t be loaded in llama.cpp. Please see
          the README - I''ve added a section indicating where they can be loaded,
          which right now is just the basic <code>mpt</code> example CLI tool that
          comes with the ggml repo, and the rustformers llm tool.  I am sure this
          list will expand soon!</p>

          '
        raw: 'You''re welcome.


          But I''m afraid these models can''t be loaded in llama.cpp. Please see the
          README - I''ve added a section indicating where they can be loaded, which
          right now is just the basic `mpt` example CLI tool that comes with the ggml
          repo, and the rustformers llm tool.  I am sure this list will expand soon!'
        updatedAt: '2023-05-18T20:59:55.893Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - BlueAquilae
    id: 646691cb9c627c78f870110f
    type: comment
  author: TheBloke
  content: 'You''re welcome.


    But I''m afraid these models can''t be loaded in llama.cpp. Please see the README
    - I''ve added a section indicating where they can be loaded, which right now is
    just the basic `mpt` example CLI tool that comes with the ggml repo, and the rustformers
    llm tool.  I am sure this list will expand soon!'
  created_at: 2023-05-18 19:59:55+00:00
  edited: false
  hidden: false
  id: 646691cb9c627c78f870110f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662165280702-62ec5f117913a6141995dbef.png?w=200&h=200&f=face
      fullname: Robert Felker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BlueAquilae
      type: user
    createdAt: '2023-05-18T21:58:43.000Z'
    data:
      edited: false
      editors:
      - BlueAquilae
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662165280702-62ec5f117913a6141995dbef.png?w=200&h=200&f=face
          fullname: Robert Felker
          isHf: false
          isPro: false
          name: BlueAquilae
          type: user
        html: '<p>Ok I think I understand this model is build on a different protocol.<br>All
          this is moving fast and I''m still learning.</p>

          <p>Thank you guys for the inputs, I''m gonna educate myself on those points.<br>I
          totally messed up LLama with ggml!!!</p>

          '
        raw: 'Ok I think I understand this model is build on a different protocol.

          All this is moving fast and I''m still learning.


          Thank you guys for the inputs, I''m gonna educate myself on those points.

          I totally messed up LLama with ggml!!!'
        updatedAt: '2023-05-18T21:58:43.597Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64669f93119ad94383d6a3e5
    id: 64669f93119ad94383d6a3e4
    type: comment
  author: BlueAquilae
  content: 'Ok I think I understand this model is build on a different protocol.

    All this is moving fast and I''m still learning.


    Thank you guys for the inputs, I''m gonna educate myself on those points.

    I totally messed up LLama with ggml!!!'
  created_at: 2023-05-18 20:58:43+00:00
  edited: false
  hidden: false
  id: 64669f93119ad94383d6a3e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662165280702-62ec5f117913a6141995dbef.png?w=200&h=200&f=face
      fullname: Robert Felker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BlueAquilae
      type: user
    createdAt: '2023-05-18T21:58:43.000Z'
    data:
      status: closed
    id: 64669f93119ad94383d6a3e5
    type: status-change
  author: BlueAquilae
  created_at: 2023-05-18 20:58:43+00:00
  id: 64669f93119ad94383d6a3e5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/MPT-7B-Instruct-GGML
repo_type: model
status: closed
target_branch: null
title: Corrupted?
