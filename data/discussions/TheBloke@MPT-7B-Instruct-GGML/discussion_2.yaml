!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kiran2405
conflicting_files: null
created_at: 2023-05-22 09:49:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5e1ea4e4747052d5d3673151069e9905.svg
      fullname: KIRAN BASIL PAUL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kiran2405
      type: user
    createdAt: '2023-05-22T10:49:04.000Z'
    data:
      edited: false
      editors:
      - kiran2405
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5e1ea4e4747052d5d3673151069e9905.svg
          fullname: KIRAN BASIL PAUL
          isHf: false
          isPro: false
          name: kiran2405
          type: user
        html: '<p>Is it possible to load this quantised model for integration to a
          Langchain via langchain''s HuggingFace Local Pipeline Integration? The original
          MPT-7B-Instruct could be loaded in a similar fashion.</p>

          '
        raw: Is it possible to load this quantised model for integration to a Langchain
          via langchain's HuggingFace Local Pipeline Integration? The original MPT-7B-Instruct
          could be loaded in a similar fashion.
        updatedAt: '2023-05-22T10:49:04.541Z'
      numEdits: 0
      reactions: []
    id: 646b48a0df2609a541c070ca
    type: comment
  author: kiran2405
  content: Is it possible to load this quantised model for integration to a Langchain
    via langchain's HuggingFace Local Pipeline Integration? The original MPT-7B-Instruct
    could be loaded in a similar fashion.
  created_at: 2023-05-22 09:49:04+00:00
  edited: false
  hidden: false
  id: 646b48a0df2609a541c070ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-22T11:42:57.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Check out <a rel="nofollow" href="https://github.com/marella/ctransformers">ctransformers</a>.
          This has LangChain integration and supports CPU inference on these GGML
          MPT models.</p>

          '
        raw: Check out [ctransformers](https://github.com/marella/ctransformers).
          This has LangChain integration and supports CPU inference on these GGML
          MPT models.
        updatedAt: '2023-05-22T11:42:57.795Z'
      numEdits: 0
      reactions: []
    id: 646b5541db697c798a3769c1
    type: comment
  author: TheBloke
  content: Check out [ctransformers](https://github.com/marella/ctransformers). This
    has LangChain integration and supports CPU inference on these GGML MPT models.
  created_at: 2023-05-22 10:42:57+00:00
  edited: false
  hidden: false
  id: 646b5541db697c798a3769c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b95a531a8f5b359f14cef4b04f773ea.svg
      fullname: Vasanth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vsns
      type: user
    createdAt: '2023-05-22T11:51:57.000Z'
    data:
      edited: true
      editors:
      - vsns
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4b95a531a8f5b359f14cef4b04f773ea.svg
          fullname: Vasanth
          isHf: false
          isPro: false
          name: vsns
          type: user
        html: "<p>My partial code with this model, rest can be referred from langchain\
          \ and ctranformers docs. It works well.</p>\n<pre><code>from langchain.vectorstores\
          \ import FAISS\nfrom ctransformers.langchain import CTransformers\nfrom\
          \ langchain.chains import RetrievalQA\nfrom langchain.embeddings import\
          \ HuggingFaceInstructEmbeddings\n\nllm = CTransformers(model='D:\\\\Ai\\\
          \\models\\\\MPT-7B-Instruct-GGML\\\\mpt-7b-instruct.ggmlv3.q5_0.bin', \n\
          \                    model_type='mpt')\n\ninstructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"\
          sentence-transformers/all-MiniLM-L6-v2\", \n                           \
          \                           model_kwargs={\"device\": \"cpu\"})\n\ndb =\
          \ FAISS.load_local(\"faiss_index\", instructor_embeddings)\nretriever =\
          \ db.as_retriever(search_kwargs={\"k\": 3})\n\nqa_chain = RetrievalQA.from_chain_type(llm=llm,\
          \ \n                                  chain_type=\"stuff\", \n         \
          \                         retriever=retriever)\n</code></pre>\n"
        raw: "My partial code with this model, rest can be referred from langchain\
          \ and ctranformers docs. It works well.\n\n```\nfrom langchain.vectorstores\
          \ import FAISS\nfrom ctransformers.langchain import CTransformers\nfrom\
          \ langchain.chains import RetrievalQA\nfrom langchain.embeddings import\
          \ HuggingFaceInstructEmbeddings\n\nllm = CTransformers(model='D:\\\\Ai\\\
          \\models\\\\MPT-7B-Instruct-GGML\\\\mpt-7b-instruct.ggmlv3.q5_0.bin', \n\
          \                    model_type='mpt')\n\ninstructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"\
          sentence-transformers/all-MiniLM-L6-v2\", \n                           \
          \                           model_kwargs={\"device\": \"cpu\"})\n\ndb =\
          \ FAISS.load_local(\"faiss_index\", instructor_embeddings)\nretriever =\
          \ db.as_retriever(search_kwargs={\"k\": 3})\n\nqa_chain = RetrievalQA.from_chain_type(llm=llm,\
          \ \n                                  chain_type=\"stuff\", \n         \
          \                         retriever=retriever)\n```"
        updatedAt: '2023-05-22T15:13:13.062Z'
      numEdits: 1
      reactions:
      - count: 11
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - nandaki
        - hellopedro
        - soyva3
        - MaziyarPanahi
        - skuma307
        - kiran2405
        - Randull
        - Dreamless
        - wsfung2008
        - bjhchong
      - count: 1
        reaction: "\U0001F917"
        users:
        - nicoleds
    id: 646b575d5d68f5c15a1e2a99
    type: comment
  author: vsns
  content: "My partial code with this model, rest can be referred from langchain and\
    \ ctranformers docs. It works well.\n\n```\nfrom langchain.vectorstores import\
    \ FAISS\nfrom ctransformers.langchain import CTransformers\nfrom langchain.chains\
    \ import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\n\
    \nllm = CTransformers(model='D:\\\\Ai\\\\models\\\\MPT-7B-Instruct-GGML\\\\mpt-7b-instruct.ggmlv3.q5_0.bin',\
    \ \n                    model_type='mpt')\n\ninstructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"\
    sentence-transformers/all-MiniLM-L6-v2\", \n                                 \
    \                     model_kwargs={\"device\": \"cpu\"})\n\ndb = FAISS.load_local(\"\
    faiss_index\", instructor_embeddings)\nretriever = db.as_retriever(search_kwargs={\"\
    k\": 3})\n\nqa_chain = RetrievalQA.from_chain_type(llm=llm, \n               \
    \                   chain_type=\"stuff\", \n                                 \
    \ retriever=retriever)\n```"
  created_at: 2023-05-22 10:51:57+00:00
  edited: true
  hidden: false
  id: 646b575d5d68f5c15a1e2a99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/83c53f9c10036158d388980b78df97d1.svg
      fullname: "Yunus Emre DEM\u0130RDA\u011E"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yunus-emre
      type: user
    createdAt: '2023-06-13T08:13:11.000Z'
    data:
      edited: true
      editors:
      - yunus-emre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7965354323387146
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/83c53f9c10036158d388980b78df97d1.svg
          fullname: "Yunus Emre DEM\u0130RDA\u011E"
          isHf: false
          isPro: false
          name: yunus-emre
          type: user
        html: '<p>If this code is used with the llama-65B-GGML model, qa_chain.run
          method is takes a  very long time. How to solve this problem?</p>

          '
        raw: If this code is used with the llama-65B-GGML model, qa_chain.run method
          is takes a  very long time. How to solve this problem?
        updatedAt: '2023-06-13T08:14:35.384Z'
      numEdits: 1
      reactions: []
    id: 648825178e004bb92b10bfac
    type: comment
  author: yunus-emre
  content: If this code is used with the llama-65B-GGML model, qa_chain.run method
    is takes a  very long time. How to solve this problem?
  created_at: 2023-06-13 07:13:11+00:00
  edited: true
  hidden: false
  id: 648825178e004bb92b10bfac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7622c1e940db9d5cfb4ae599c608d5c8.svg
      fullname: Nicole
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicoleds
      type: user
    createdAt: '2023-06-20T04:53:29.000Z'
    data:
      edited: false
      editors:
      - nicoleds
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4603234827518463
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7622c1e940db9d5cfb4ae599c608d5c8.svg
          fullname: Nicole
          isHf: false
          isPro: false
          name: nicoleds
          type: user
        html: "<blockquote>\n<p>My partial code with this model, rest can be referred\
          \ from langchain and ctranformers docs. It works well.</p>\n<pre><code>from\
          \ langchain.vectorstores import FAISS\nfrom ctransformers.langchain import\
          \ CTransformers\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings\
          \ import HuggingFaceInstructEmbeddings\n\nllm = CTransformers(model='D:\\\
          \\Ai\\\\models\\\\MPT-7B-Instruct-GGML\\\\mpt-7b-instruct.ggmlv3.q5_0.bin',\
          \ \n                    model_type='mpt')\n\ninstructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"\
          sentence-transformers/all-MiniLM-L6-v2\", \n                           \
          \                           model_kwargs={\"device\": \"cpu\"})\n\ndb =\
          \ FAISS.load_local(\"faiss_index\", instructor_embeddings)\nretriever =\
          \ db.as_retriever(search_kwargs={\"k\": 3})\n\nqa_chain = RetrievalQA.from_chain_type(llm=llm,\
          \ \n                                  chain_type=\"stuff\", \n         \
          \                         retriever=retriever)\n</code></pre>\n</blockquote>\n\
          <p>When trying the code above, it returns OSError: /lib64/libm.so.6: version\
          \ `GLIBC_2.29' not found for the Ctransformers library.. any way to use\
          \ Ctransformers without upgrading the GLIBC version?</p>\n"
        raw: "> My partial code with this model, rest can be referred from langchain\
          \ and ctranformers docs. It works well.\n> \n> ```\n> from langchain.vectorstores\
          \ import FAISS\n> from ctransformers.langchain import CTransformers\n> from\
          \ langchain.chains import RetrievalQA\n> from langchain.embeddings import\
          \ HuggingFaceInstructEmbeddings\n> \n> llm = CTransformers(model='D:\\\\\
          Ai\\\\models\\\\MPT-7B-Instruct-GGML\\\\mpt-7b-instruct.ggmlv3.q5_0.bin',\
          \ \n>                     model_type='mpt')\n> \n> instructor_embeddings\
          \ = HuggingFaceInstructEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\
          , \n>                                                       model_kwargs={\"\
          device\": \"cpu\"})\n> \n> db = FAISS.load_local(\"faiss_index\", instructor_embeddings)\n\
          > retriever = db.as_retriever(search_kwargs={\"k\": 3})\n> \n> qa_chain\
          \ = RetrievalQA.from_chain_type(llm=llm, \n>                           \
          \        chain_type=\"stuff\", \n>                                   retriever=retriever)\n\
          > ```\n\nWhen trying the code above, it returns OSError: /lib64/libm.so.6:\
          \ version `GLIBC_2.29' not found for the Ctransformers library.. any way\
          \ to use Ctransformers without upgrading the GLIBC version?"
        updatedAt: '2023-06-20T04:53:29.343Z'
      numEdits: 0
      reactions: []
    id: 649130c92c826e334ea669f8
    type: comment
  author: nicoleds
  content: "> My partial code with this model, rest can be referred from langchain\
    \ and ctranformers docs. It works well.\n> \n> ```\n> from langchain.vectorstores\
    \ import FAISS\n> from ctransformers.langchain import CTransformers\n> from langchain.chains\
    \ import RetrievalQA\n> from langchain.embeddings import HuggingFaceInstructEmbeddings\n\
    > \n> llm = CTransformers(model='D:\\\\Ai\\\\models\\\\MPT-7B-Instruct-GGML\\\\\
    mpt-7b-instruct.ggmlv3.q5_0.bin', \n>                     model_type='mpt')\n\
    > \n> instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\
    , \n>                                                       model_kwargs={\"device\"\
    : \"cpu\"})\n> \n> db = FAISS.load_local(\"faiss_index\", instructor_embeddings)\n\
    > retriever = db.as_retriever(search_kwargs={\"k\": 3})\n> \n> qa_chain = RetrievalQA.from_chain_type(llm=llm,\
    \ \n>                                   chain_type=\"stuff\", \n>            \
    \                       retriever=retriever)\n> ```\n\nWhen trying the code above,\
    \ it returns OSError: /lib64/libm.so.6: version `GLIBC_2.29' not found for the\
    \ Ctransformers library.. any way to use Ctransformers without upgrading the GLIBC\
    \ version?"
  created_at: 2023-06-20 03:53:29+00:00
  edited: false
  hidden: false
  id: 649130c92c826e334ea669f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-20T08:49:46.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8459957242012024
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;nicoleds&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nicoleds\">@<span class=\"\
          underline\">nicoleds</span></a></span>\n\n\t</span></span> Try building\
          \ from source, which will also enable you to get GPU acceleration if you\
          \ have CUDA toolkit installed:</p>\n<pre><code>CT_CUBLAS=1 pip install ctransformers\
          \ --no-binary ctransformers\n</code></pre>\n"
        raw: '@nicoleds Try building from source, which will also enable you to get
          GPU acceleration if you have CUDA toolkit installed:


          ```

          CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers

          ```

          '
        updatedAt: '2023-06-20T08:49:46.759Z'
      numEdits: 0
      reactions: []
    id: 6491682af6e6408af26ac62c
    type: comment
  author: TheBloke
  content: '@nicoleds Try building from source, which will also enable you to get
    GPU acceleration if you have CUDA toolkit installed:


    ```

    CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers

    ```

    '
  created_at: 2023-06-20 07:49:46+00:00
  edited: false
  hidden: false
  id: 6491682af6e6408af26ac62c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7622c1e940db9d5cfb4ae599c608d5c8.svg
      fullname: Nicole
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicoleds
      type: user
    createdAt: '2023-06-20T09:19:28.000Z'
    data:
      edited: false
      editors:
      - nicoleds
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8462417125701904
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7622c1e940db9d5cfb4ae599c608d5c8.svg
          fullname: Nicole
          isHf: false
          isPro: false
          name: nicoleds
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;nicoleds&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nicoleds\"\
          >@<span class=\"underline\">nicoleds</span></a></span>\n\n\t</span></span>\
          \ Try building from source, which will also enable you to get GPU acceleration\
          \ if you have CUDA toolkit installed:</p>\n<pre><code>CT_CUBLAS=1 pip install\
          \ ctransformers --no-binary ctransformers\n</code></pre>\n<p>Thanks for\
          \ the reply, what if I only have CPU? No available GPU</p>\n</blockquote>\n"
        raw: "> @nicoleds Try building from source, which will also enable you to\
          \ get GPU acceleration if you have CUDA toolkit installed:\n> \n> ```\n\
          > CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers\n> ```\n\
          Thanks for the reply, what if I only have CPU? No available GPU\n"
        updatedAt: '2023-06-20T09:19:28.459Z'
      numEdits: 0
      reactions: []
    id: 64916f20c174166623871c5d
    type: comment
  author: nicoleds
  content: "> @nicoleds Try building from source, which will also enable you to get\
    \ GPU acceleration if you have CUDA toolkit installed:\n> \n> ```\n> CT_CUBLAS=1\
    \ pip install ctransformers --no-binary ctransformers\n> ```\nThanks for the reply,\
    \ what if I only have CPU? No available GPU\n"
  created_at: 2023-06-20 08:19:28+00:00
  edited: false
  hidden: false
  id: 64916f20c174166623871c5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-20T09:59:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6268947124481201
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Then just leave out the <code>CT_CUBLAS=1</code> part:</p>

          <pre><code>pip install ctransformers --no-binary ctransformers

          </code></pre>

          '
        raw: 'Then just leave out the `CT_CUBLAS=1` part:

          ```

          pip install ctransformers --no-binary ctransformers

          ```'
        updatedAt: '2023-06-20T09:59:27.134Z'
      numEdits: 0
      reactions: []
    id: 6491787f3620f1a33ed4b250
    type: comment
  author: TheBloke
  content: 'Then just leave out the `CT_CUBLAS=1` part:

    ```

    pip install ctransformers --no-binary ctransformers

    ```'
  created_at: 2023-06-20 08:59:27+00:00
  edited: false
  hidden: false
  id: 6491787f3620f1a33ed4b250
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7a563a2b4878296d8bcfff54aad63335.svg
      fullname: Baskar Jayaraman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: beejay
      type: user
    createdAt: '2023-06-24T03:18:37.000Z'
    data:
      edited: false
      editors:
      - beejay
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6985148191452026
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7a563a2b4878296d8bcfff54aad63335.svg
          fullname: Baskar Jayaraman
          isHf: false
          isPro: false
          name: beejay
          type: user
        html: "<blockquote>\n<p>My partial code with this model, rest can be referred\
          \ from langchain and ctranformers docs. It works well.</p>\n<pre><code>from\
          \ langchain.vectorstores import FAISS\nfrom ctransformers.langchain import\
          \ CTransformers\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings\
          \ import HuggingFaceInstructEmbeddings\n\nllm = CTransformers(model='D:\\\
          \\Ai\\\\models\\\\MPT-7B-Instruct-GGML\\\\mpt-7b-instruct.ggmlv3.q5_0.bin',\
          \ \n                    model_type='mpt')\n\ninstructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"\
          sentence-transformers/all-MiniLM-L6-v2\", \n                           \
          \                           model_kwargs={\"device\": \"cpu\"})\n\ndb =\
          \ FAISS.load_local(\"faiss_index\", instructor_embeddings)\nretriever =\
          \ db.as_retriever(search_kwargs={\"k\": 3})\n\nqa_chain = RetrievalQA.from_chain_type(llm=llm,\
          \ \n                                  chain_type=\"stuff\", \n         \
          \                         retriever=retriever)\n</code></pre>\n</blockquote>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;vsns&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/vsns\">@<span class=\"\
          underline\">vsns</span></a></span>\n\n\t</span></span> It would be great\
          \ if you can share a more complete example code  where this works for you.\
          \ I have been trying your example and others from langchain on many of these\
          \ models but the responses are non-sensical and/or completely outside the\
          \ context. Very similar code just works with OpenAI models (ada for embedding\
          \ and 3.5 turbo as the model) making me wonder if I am doing something wrong\
          \ or these models are just not capable.</p>\n"
        raw: "> My partial code with this model, rest can be referred from langchain\
          \ and ctranformers docs. It works well.\n> \n> ```\n> from langchain.vectorstores\
          \ import FAISS\n> from ctransformers.langchain import CTransformers\n> from\
          \ langchain.chains import RetrievalQA\n> from langchain.embeddings import\
          \ HuggingFaceInstructEmbeddings\n> \n> llm = CTransformers(model='D:\\\\\
          Ai\\\\models\\\\MPT-7B-Instruct-GGML\\\\mpt-7b-instruct.ggmlv3.q5_0.bin',\
          \ \n>                     model_type='mpt')\n> \n> instructor_embeddings\
          \ = HuggingFaceInstructEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\
          , \n>                                                       model_kwargs={\"\
          device\": \"cpu\"})\n> \n> db = FAISS.load_local(\"faiss_index\", instructor_embeddings)\n\
          > retriever = db.as_retriever(search_kwargs={\"k\": 3})\n> \n> qa_chain\
          \ = RetrievalQA.from_chain_type(llm=llm, \n>                           \
          \        chain_type=\"stuff\", \n>                                   retriever=retriever)\n\
          > ```\n\n@vsns It would be great if you can share a more complete example\
          \ code  where this works for you. I have been trying your example and others\
          \ from langchain on many of these models but the responses are non-sensical\
          \ and/or completely outside the context. Very similar code just works with\
          \ OpenAI models (ada for embedding and 3.5 turbo as the model) making me\
          \ wonder if I am doing something wrong or these models are just not capable."
        updatedAt: '2023-06-24T03:18:37.336Z'
      numEdits: 0
      reactions: []
    id: 6496608d5e85f281060e2ff7
    type: comment
  author: beejay
  content: "> My partial code with this model, rest can be referred from langchain\
    \ and ctranformers docs. It works well.\n> \n> ```\n> from langchain.vectorstores\
    \ import FAISS\n> from ctransformers.langchain import CTransformers\n> from langchain.chains\
    \ import RetrievalQA\n> from langchain.embeddings import HuggingFaceInstructEmbeddings\n\
    > \n> llm = CTransformers(model='D:\\\\Ai\\\\models\\\\MPT-7B-Instruct-GGML\\\\\
    mpt-7b-instruct.ggmlv3.q5_0.bin', \n>                     model_type='mpt')\n\
    > \n> instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\
    , \n>                                                       model_kwargs={\"device\"\
    : \"cpu\"})\n> \n> db = FAISS.load_local(\"faiss_index\", instructor_embeddings)\n\
    > retriever = db.as_retriever(search_kwargs={\"k\": 3})\n> \n> qa_chain = RetrievalQA.from_chain_type(llm=llm,\
    \ \n>                                   chain_type=\"stuff\", \n>            \
    \                       retriever=retriever)\n> ```\n\n@vsns It would be great\
    \ if you can share a more complete example code  where this works for you. I have\
    \ been trying your example and others from langchain on many of these models but\
    \ the responses are non-sensical and/or completely outside the context. Very similar\
    \ code just works with OpenAI models (ada for embedding and 3.5 turbo as the model)\
    \ making me wonder if I am doing something wrong or these models are just not\
    \ capable."
  created_at: 2023-06-24 02:18:37+00:00
  edited: false
  hidden: false
  id: 6496608d5e85f281060e2ff7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b95a531a8f5b359f14cef4b04f773ea.svg
      fullname: Vasanth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vsns
      type: user
    createdAt: '2023-06-24T08:17:16.000Z'
    data:
      edited: false
      editors:
      - vsns
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5967510342597961
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4b95a531a8f5b359f14cef4b04f773ea.svg
          fullname: Vasanth
          isHf: false
          isPro: false
          name: vsns
          type: user
        html: "<blockquote>\n<blockquote>\n<p>My partial code with this model, rest\
          \ can be referred from langchain and ctranformers docs. It works well.</p>\n\
          <pre><code>from langchain.vectorstores import FAISS\nfrom ctransformers.langchain\
          \ import CTransformers\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings\
          \ import HuggingFaceInstructEmbeddings\n\nllm = CTransformers(model='D:\\\
          \\Ai\\\\models\\\\MPT-7B-Instruct-GGML\\\\mpt-7b-instruct.ggmlv3.q5_0.bin',\
          \ \n                    model_type='mpt')\n\ninstructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"\
          sentence-transformers/all-MiniLM-L6-v2\", \n                           \
          \                           model_kwargs={\"device\": \"cpu\"})\n\ndb =\
          \ FAISS.load_local(\"faiss_index\", instructor_embeddings)\nretriever =\
          \ db.as_retriever(search_kwargs={\"k\": 3})\n\nqa_chain = RetrievalQA.from_chain_type(llm=llm,\
          \ \n                                  chain_type=\"stuff\", \n         \
          \                         retriever=retriever)\n</code></pre>\n</blockquote>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;vsns&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/vsns\">@<span class=\"\
          underline\">vsns</span></a></span>\n\n\t</span></span> It would be great\
          \ if you can share a more complete example code  where this works for you.\
          \ I have been trying your example and others from langchain on many of these\
          \ models but the responses are non-sensical and/or completely outside the\
          \ context. Very similar code just works with OpenAI models (ada for embedding\
          \ and 3.5 turbo as the model) making me wonder if I am doing something wrong\
          \ or these models are just not capable.</p>\n</blockquote>\n<p>Here you\
          \  go:</p>\n<pre><code>import typer\n\n# 0xVs\n\nfrom ctransformers.langchain\
          \ import CTransformers\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings\
          \ import HuggingFaceInstructEmbeddings\nfrom langchain.document_loaders\
          \ import DirectoryLoader\nfrom langchain.text_splitter import SentenceTransformersTokenTextSplitter\n\
          from langchain.document_loaders import PDFPlumberLoader\nfrom langchain.text_splitter\
          \ import RecursiveCharacterTextSplitter\nfrom langchain.chains import ConversationalRetrievalChain\n\
          from langchain.memory import ConversationBufferMemory\nfrom rich import\
          \ print\nfrom rich.prompt import Prompt\n\napp = typer.Typer()\ndevice =\
          \ \"cpu\"\n\n<span data-props=\"{&quot;user&quot;:&quot;app&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/app\">@<span class=\"\
          underline\">app</span></a></span>\n\n\t</span></span>.command()\ndef import_pdfs(dir:\
          \ str, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n  \
          \  loader = DirectoryLoader(dir, glob=\"./*.pdf\", loader_cls=PDFPlumberLoader,\
          \ show_progress=True)\n    documents = loader.load()\n    text_splitter\
          \ = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n  \
          \  docs = text_splitter.split_documents(documents)\n\n    embeddings = HuggingFaceInstructEmbeddings(model_name=embedding_model,\
          \ \n                                               model_kwargs={\"device\"\
          : device})\n    db = FAISS.from_documents(docs, embeddings)\n    db.save_local(\"\
          faiss_index\")\n\n<span data-props=\"{&quot;user&quot;:&quot;app&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/app\"\
          >@<span class=\"underline\">app</span></a></span>\n\n\t</span></span>.command()\n\
          def question(model_path: str = \"./models/mpt-7b-instruct.ggmlv3.q5_0.bin\"\
          ,\n             model_type='mpt',\n             embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\
          ,\n             search_breadth : int = 5, threads : int = 6, temperature\
          \ : float = 0.4):\n    embeddings = HuggingFaceInstructEmbeddings(model_name=embedding_model,\
          \ \n                                               model_kwargs={\"device\"\
          : device})\n    config = {'temperature': temperature, 'threads' : threads}\n\
          \    llm = CTransformers(model=model_path, model_type=model_type, config=config)\n\
          \    db = FAISS.load_local(\"faiss_index\", embeddings)\n    retriever =\
          \ db.as_retriever(search_kwargs={\"k\": search_breadth})\n    memory = ConversationBufferMemory(memory_key=\"\
          chat_history\", output_key=\"answer\", return_messages=True)\n    qa = ConversationalRetrievalChain.from_llm(llm=llm,\
          \ retriever=retriever,\n                                               memory=memory,\
          \ return_source_documents=True)\n    while True:\n        query = Prompt.ask('[bright_yellow]\\\
          nQuestion[/bright_yellow] ')\n        res = qa({\"question\": query})\n\
          \        print(\"[spring_green4]\"+res['answer']+\"[/spring_green4]\")\n\
          \        if \"source_documents\" in res:\n            print(\"\\n[italic\
          \ grey46]References[/italic grey46]:\")\n            for ref in res[\"source_documents\"\
          ]:\n                print(\"&gt; [grey19]\" + ref.metadata['source'] + \"\
          [/grey19]\")\n\nif __name__ == \"__main__\":\n    app()\n</code></pre>\n\
          <p>Some notes:</p>\n<ol>\n<li>From my experience (take it with pinch of\
          \ salt) for QA, creating a  good vector data is more important than model\
          \ (i avoid proprietary systems or models)</li>\n<li>I haven't tested code\
          \ much, and so multiple optimizations are possible. To name a few (different\
          \ embedding model,  use of custom prompt template, configuration tweaks\
          \ etc)</li>\n<li>Currently considering VMware/open-llama-7b-open-instruct\
          \ with llama-cpp-python, as when I use docs on narrow domains with less\
          \ text, not getting good results</li>\n<li>Ultimately will be planning to\
          \ have a single static binary (with naive assumption that qdrant can be\
          \ packed inside it) using Rustformers and falcon-40b-instruct, when the\
          \ support is available in it</li>\n</ol>\n<p><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/6427d2ce88215cee63b1728b/PXtm4bDVRIkEnpvvnopWq.png\"\
          ><img alt=\"docQA.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6427d2ce88215cee63b1728b/PXtm4bDVRIkEnpvvnopWq.png\"\
          ></a></p>\n"
        raw: "> > My partial code with this model, rest can be referred from langchain\
          \ and ctranformers docs. It works well.\n> > \n> > ```\n> > from langchain.vectorstores\
          \ import FAISS\n> > from ctransformers.langchain import CTransformers\n\
          > > from langchain.chains import RetrievalQA\n> > from langchain.embeddings\
          \ import HuggingFaceInstructEmbeddings\n> > \n> > llm = CTransformers(model='D:\\\
          \\Ai\\\\models\\\\MPT-7B-Instruct-GGML\\\\mpt-7b-instruct.ggmlv3.q5_0.bin',\
          \ \n> >                     model_type='mpt')\n> > \n> > instructor_embeddings\
          \ = HuggingFaceInstructEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\
          , \n> >                                                       model_kwargs={\"\
          device\": \"cpu\"})\n> > \n> > db = FAISS.load_local(\"faiss_index\", instructor_embeddings)\n\
          > > retriever = db.as_retriever(search_kwargs={\"k\": 3})\n> > \n> > qa_chain\
          \ = RetrievalQA.from_chain_type(llm=llm, \n> >                         \
          \          chain_type=\"stuff\", \n> >                                 \
          \  retriever=retriever)\n> > ```\n> \n> @vsns It would be great if you can\
          \ share a more complete example code  where this works for you. I have been\
          \ trying your example and others from langchain on many of these models\
          \ but the responses are non-sensical and/or completely outside the context.\
          \ Very similar code just works with OpenAI models (ada for embedding and\
          \ 3.5 turbo as the model) making me wonder if I am doing something wrong\
          \ or these models are just not capable.\n\nHere you  go:\n```\nimport typer\n\
          \n# 0xVs\n\nfrom ctransformers.langchain import CTransformers\nfrom langchain.vectorstores\
          \ import FAISS\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\n\
          from langchain.document_loaders import DirectoryLoader\nfrom langchain.text_splitter\
          \ import SentenceTransformersTokenTextSplitter\nfrom langchain.document_loaders\
          \ import PDFPlumberLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\
          from langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory\
          \ import ConversationBufferMemory\nfrom rich import print\nfrom rich.prompt\
          \ import Prompt\n\napp = typer.Typer()\ndevice = \"cpu\"\n\n@app.command()\n\
          def import_pdfs(dir: str, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\
          ):\n    loader = DirectoryLoader(dir, glob=\"./*.pdf\", loader_cls=PDFPlumberLoader,\
          \ show_progress=True)\n    documents = loader.load()\n    text_splitter\
          \ = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n  \
          \  docs = text_splitter.split_documents(documents)\n\n    embeddings = HuggingFaceInstructEmbeddings(model_name=embedding_model,\
          \ \n                                               model_kwargs={\"device\"\
          : device})\n    db = FAISS.from_documents(docs, embeddings)\n    db.save_local(\"\
          faiss_index\")\n\n@app.command()\ndef question(model_path: str = \"./models/mpt-7b-instruct.ggmlv3.q5_0.bin\"\
          ,\n             model_type='mpt',\n             embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\
          ,\n             search_breadth : int = 5, threads : int = 6, temperature\
          \ : float = 0.4):\n    embeddings = HuggingFaceInstructEmbeddings(model_name=embedding_model,\
          \ \n                                               model_kwargs={\"device\"\
          : device})\n    config = {'temperature': temperature, 'threads' : threads}\n\
          \    llm = CTransformers(model=model_path, model_type=model_type, config=config)\n\
          \    db = FAISS.load_local(\"faiss_index\", embeddings)\n    retriever =\
          \ db.as_retriever(search_kwargs={\"k\": search_breadth})\n    memory = ConversationBufferMemory(memory_key=\"\
          chat_history\", output_key=\"answer\", return_messages=True)\n    qa = ConversationalRetrievalChain.from_llm(llm=llm,\
          \ retriever=retriever,\n                                               memory=memory,\
          \ return_source_documents=True)\n    while True:\n        query = Prompt.ask('[bright_yellow]\\\
          nQuestion[/bright_yellow] ')\n        res = qa({\"question\": query})\n\
          \        print(\"[spring_green4]\"+res['answer']+\"[/spring_green4]\")\n\
          \        if \"source_documents\" in res:\n            print(\"\\n[italic\
          \ grey46]References[/italic grey46]:\")\n            for ref in res[\"source_documents\"\
          ]:\n                print(\"> [grey19]\" + ref.metadata['source'] + \"[/grey19]\"\
          )\n\nif __name__ == \"__main__\":\n    app()\n```\nSome notes:\n1. From\
          \ my experience (take it with pinch of salt) for QA, creating a  good vector\
          \ data is more important than model (i avoid proprietary systems or models)\n\
          2. I haven't tested code much, and so multiple optimizations are possible.\
          \ To name a few (different embedding model,  use of custom prompt template,\
          \ configuration tweaks etc)\n3. Currently considering VMware/open-llama-7b-open-instruct\
          \ with llama-cpp-python, as when I use docs on narrow domains with less\
          \ text, not getting good results\n4. Ultimately will be planning to have\
          \ a single static binary (with naive assumption that qdrant can be packed\
          \ inside it) using Rustformers and falcon-40b-instruct, when the support\
          \ is available in it\n\n\n![docQA.png](https://cdn-uploads.huggingface.co/production/uploads/6427d2ce88215cee63b1728b/PXtm4bDVRIkEnpvvnopWq.png)\n"
        updatedAt: '2023-06-24T08:17:16.338Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - beejay
        - WajihUllahBaig
      - count: 1
        reaction: "\U0001F44D"
        users:
        - cvesel
    id: 6496a68c90060ff18c36fb0f
    type: comment
  author: vsns
  content: "> > My partial code with this model, rest can be referred from langchain\
    \ and ctranformers docs. It works well.\n> > \n> > ```\n> > from langchain.vectorstores\
    \ import FAISS\n> > from ctransformers.langchain import CTransformers\n> > from\
    \ langchain.chains import RetrievalQA\n> > from langchain.embeddings import HuggingFaceInstructEmbeddings\n\
    > > \n> > llm = CTransformers(model='D:\\\\Ai\\\\models\\\\MPT-7B-Instruct-GGML\\\
    \\mpt-7b-instruct.ggmlv3.q5_0.bin', \n> >                     model_type='mpt')\n\
    > > \n> > instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"\
    sentence-transformers/all-MiniLM-L6-v2\", \n> >                              \
    \                         model_kwargs={\"device\": \"cpu\"})\n> > \n> > db =\
    \ FAISS.load_local(\"faiss_index\", instructor_embeddings)\n> > retriever = db.as_retriever(search_kwargs={\"\
    k\": 3})\n> > \n> > qa_chain = RetrievalQA.from_chain_type(llm=llm, \n> >    \
    \                               chain_type=\"stuff\", \n> >                  \
    \                 retriever=retriever)\n> > ```\n> \n> @vsns It would be great\
    \ if you can share a more complete example code  where this works for you. I have\
    \ been trying your example and others from langchain on many of these models but\
    \ the responses are non-sensical and/or completely outside the context. Very similar\
    \ code just works with OpenAI models (ada for embedding and 3.5 turbo as the model)\
    \ making me wonder if I am doing something wrong or these models are just not\
    \ capable.\n\nHere you  go:\n```\nimport typer\n\n# 0xVs\n\nfrom ctransformers.langchain\
    \ import CTransformers\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings\
    \ import HuggingFaceInstructEmbeddings\nfrom langchain.document_loaders import\
    \ DirectoryLoader\nfrom langchain.text_splitter import SentenceTransformersTokenTextSplitter\n\
    from langchain.document_loaders import PDFPlumberLoader\nfrom langchain.text_splitter\
    \ import RecursiveCharacterTextSplitter\nfrom langchain.chains import ConversationalRetrievalChain\n\
    from langchain.memory import ConversationBufferMemory\nfrom rich import print\n\
    from rich.prompt import Prompt\n\napp = typer.Typer()\ndevice = \"cpu\"\n\n@app.command()\n\
    def import_pdfs(dir: str, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\
    ):\n    loader = DirectoryLoader(dir, glob=\"./*.pdf\", loader_cls=PDFPlumberLoader,\
    \ show_progress=True)\n    documents = loader.load()\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512,\
    \ chunk_overlap=0)\n    docs = text_splitter.split_documents(documents)\n\n  \
    \  embeddings = HuggingFaceInstructEmbeddings(model_name=embedding_model, \n \
    \                                              model_kwargs={\"device\": device})\n\
    \    db = FAISS.from_documents(docs, embeddings)\n    db.save_local(\"faiss_index\"\
    )\n\n@app.command()\ndef question(model_path: str = \"./models/mpt-7b-instruct.ggmlv3.q5_0.bin\"\
    ,\n             model_type='mpt',\n             embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\
    ,\n             search_breadth : int = 5, threads : int = 6, temperature : float\
    \ = 0.4):\n    embeddings = HuggingFaceInstructEmbeddings(model_name=embedding_model,\
    \ \n                                               model_kwargs={\"device\": device})\n\
    \    config = {'temperature': temperature, 'threads' : threads}\n    llm = CTransformers(model=model_path,\
    \ model_type=model_type, config=config)\n    db = FAISS.load_local(\"faiss_index\"\
    , embeddings)\n    retriever = db.as_retriever(search_kwargs={\"k\": search_breadth})\n\
    \    memory = ConversationBufferMemory(memory_key=\"chat_history\", output_key=\"\
    answer\", return_messages=True)\n    qa = ConversationalRetrievalChain.from_llm(llm=llm,\
    \ retriever=retriever,\n                                               memory=memory,\
    \ return_source_documents=True)\n    while True:\n        query = Prompt.ask('[bright_yellow]\\\
    nQuestion[/bright_yellow] ')\n        res = qa({\"question\": query})\n      \
    \  print(\"[spring_green4]\"+res['answer']+\"[/spring_green4]\")\n        if \"\
    source_documents\" in res:\n            print(\"\\n[italic grey46]References[/italic\
    \ grey46]:\")\n            for ref in res[\"source_documents\"]:\n           \
    \     print(\"> [grey19]\" + ref.metadata['source'] + \"[/grey19]\")\n\nif __name__\
    \ == \"__main__\":\n    app()\n```\nSome notes:\n1. From my experience (take it\
    \ with pinch of salt) for QA, creating a  good vector data is more important than\
    \ model (i avoid proprietary systems or models)\n2. I haven't tested code much,\
    \ and so multiple optimizations are possible. To name a few (different embedding\
    \ model,  use of custom prompt template, configuration tweaks etc)\n3. Currently\
    \ considering VMware/open-llama-7b-open-instruct with llama-cpp-python, as when\
    \ I use docs on narrow domains with less text, not getting good results\n4. Ultimately\
    \ will be planning to have a single static binary (with naive assumption that\
    \ qdrant can be packed inside it) using Rustformers and falcon-40b-instruct, when\
    \ the support is available in it\n\n\n![docQA.png](https://cdn-uploads.huggingface.co/production/uploads/6427d2ce88215cee63b1728b/PXtm4bDVRIkEnpvvnopWq.png)\n"
  created_at: 2023-06-24 07:17:16+00:00
  edited: false
  hidden: false
  id: 6496a68c90060ff18c36fb0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03e3d02ea6d1c4fbfb1ff92b5d80a960.svg
      fullname: rodrigo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rodrigofarias
      type: user
    createdAt: '2023-07-09T18:53:05.000Z'
    data:
      edited: false
      editors:
      - rodrigofarias
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5826694965362549
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03e3d02ea6d1c4fbfb1ff92b5d80a960.svg
          fullname: rodrigo
          isHf: false
          isPro: false
          name: rodrigofarias
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<p>My partial code with this\
          \ model, rest can be referred from langchain and ctranformers docs. It works\
          \ well.</p>\n<pre><code>from langchain.vectorstores import FAISS\nfrom ctransformers.langchain\
          \ import CTransformers\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings\
          \ import HuggingFaceInstructEmbeddings\n\nllm = CTransformers(model='D:\\\
          \\Ai\\\\models\\\\MPT-7B-Instruct-GGML\\\\mpt-7b-instruct.ggmlv3.q5_0.bin',\
          \ \n                    model_type='mpt')\n\ninstructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"\
          sentence-transformers/all-MiniLM-L6-v2\", \n                           \
          \                           model_kwargs={\"device\": \"cpu\"})\n\ndb =\
          \ FAISS.load_local(\"faiss_index\", instructor_embeddings)\nretriever =\
          \ db.as_retriever(search_kwargs={\"k\": 3})\n\nqa_chain = RetrievalQA.from_chain_type(llm=llm,\
          \ \n                                  chain_type=\"stuff\", \n         \
          \                         retriever=retriever)\n</code></pre>\n</blockquote>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;vsns&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/vsns\">@<span class=\"\
          underline\">vsns</span></a></span>\n\n\t</span></span> It would be great\
          \ if you can share a more complete example code  where this works for you.\
          \ I have been trying your example and others from langchain on many of these\
          \ models but the responses are non-sensical and/or completely outside the\
          \ context. Very similar code just works with OpenAI models (ada for embedding\
          \ and 3.5 turbo as the model) making me wonder if I am doing something wrong\
          \ or these models are just not capable.</p>\n</blockquote>\n<p>Here you\
          \  go:</p>\n<pre><code>import typer\n\n# 0xVs\n\nfrom ctransformers.langchain\
          \ import CTransformers\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings\
          \ import HuggingFaceInstructEmbeddings\nfrom langchain.document_loaders\
          \ import DirectoryLoader\nfrom langchain.text_splitter import SentenceTransformersTokenTextSplitter\n\
          from langchain.document_loaders import PDFPlumberLoader\nfrom langchain.text_splitter\
          \ import RecursiveCharacterTextSplitter\nfrom langchain.chains import ConversationalRetrievalChain\n\
          from langchain.memory import ConversationBufferMemory\nfrom rich import\
          \ print\nfrom rich.prompt import Prompt\n\napp = typer.Typer()\ndevice =\
          \ \"cpu\"\n\n<span data-props=\"{&quot;user&quot;:&quot;app&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/app\">@<span class=\"\
          underline\">app</span></a></span>\n\n\t</span></span>.command()\ndef import_pdfs(dir:\
          \ str, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n  \
          \  loader = DirectoryLoader(dir, glob=\"./*.pdf\", loader_cls=PDFPlumberLoader,\
          \ show_progress=True)\n    documents = loader.load()\n    text_splitter\
          \ = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n  \
          \  docs = text_splitter.split_documents(documents)\n\n    embeddings = HuggingFaceInstructEmbeddings(model_name=embedding_model,\
          \ \n                                               model_kwargs={\"device\"\
          : device})\n    db = FAISS.from_documents(docs, embeddings)\n    db.save_local(\"\
          faiss_index\")\n\n<span data-props=\"{&quot;user&quot;:&quot;app&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/app\"\
          >@<span class=\"underline\">app</span></a></span>\n\n\t</span></span>.command()\n\
          def question(model_path: str = \"./models/mpt-7b-instruct.ggmlv3.q5_0.bin\"\
          ,\n             model_type='mpt',\n             embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\
          ,\n             search_breadth : int = 5, threads : int = 6, temperature\
          \ : float = 0.4):\n    embeddings = HuggingFaceInstructEmbeddings(model_name=embedding_model,\
          \ \n                                               model_kwargs={\"device\"\
          : device})\n    config = {'temperature': temperature, 'threads' : threads}\n\
          \    llm = CTransformers(model=model_path, model_type=model_type, config=config)\n\
          \    db = FAISS.load_local(\"faiss_index\", embeddings)\n    retriever =\
          \ db.as_retriever(search_kwargs={\"k\": search_breadth})\n    memory = ConversationBufferMemory(memory_key=\"\
          chat_history\", output_key=\"answer\", return_messages=True)\n    qa = ConversationalRetrievalChain.from_llm(llm=llm,\
          \ retriever=retriever,\n                                               memory=memory,\
          \ return_source_documents=True)\n    while True:\n        query = Prompt.ask('[bright_yellow]\\\
          nQuestion[/bright_yellow] ')\n        res = qa({\"question\": query})\n\
          \        print(\"[spring_green4]\"+res['answer']+\"[/spring_green4]\")\n\
          \        if \"source_documents\" in res:\n            print(\"\\n[italic\
          \ grey46]References[/italic grey46]:\")\n            for ref in res[\"source_documents\"\
          ]:\n                print(\"&gt; [grey19]\" + ref.metadata['source'] + \"\
          [/grey19]\")\n\nif __name__ == \"__main__\":\n    app()\n</code></pre>\n\
          <p>Some notes:</p>\n<ol>\n<li>From my experience (take it with pinch of\
          \ salt) for QA, creating a  good vector data is more important than model\
          \ (i avoid proprietary systems or models)</li>\n<li>I haven't tested code\
          \ much, and so multiple optimizations are possible. To name a few (different\
          \ embedding model,  use of custom prompt template, configuration tweaks\
          \ etc)</li>\n<li>Currently considering VMware/open-llama-7b-open-instruct\
          \ with llama-cpp-python, as when I use docs on narrow domains with less\
          \ text, not getting good results</li>\n<li>Ultimately will be planning to\
          \ have a single static binary (with naive assumption that qdrant can be\
          \ packed inside it) using Rustformers and falcon-40b-instruct, when the\
          \ support is available in it</li>\n</ol>\n<p><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/6427d2ce88215cee63b1728b/PXtm4bDVRIkEnpvvnopWq.png\"\
          ><img alt=\"docQA.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6427d2ce88215cee63b1728b/PXtm4bDVRIkEnpvvnopWq.png\"\
          ></a></p>\n</blockquote>\n<p>What should be \"question\" in 'python test.py\
          \ question'? A string? Another python file?</p>\n"
        raw: "> > > My partial code with this model, rest can be referred from langchain\
          \ and ctranformers docs. It works well.\n> > > \n> > > ```\n> > > from langchain.vectorstores\
          \ import FAISS\n> > > from ctransformers.langchain import CTransformers\n\
          > > > from langchain.chains import RetrievalQA\n> > > from langchain.embeddings\
          \ import HuggingFaceInstructEmbeddings\n> > > \n> > > llm = CTransformers(model='D:\\\
          \\Ai\\\\models\\\\MPT-7B-Instruct-GGML\\\\mpt-7b-instruct.ggmlv3.q5_0.bin',\
          \ \n> > >                     model_type='mpt')\n> > > \n> > > instructor_embeddings\
          \ = HuggingFaceInstructEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\
          , \n> > >                                                       model_kwargs={\"\
          device\": \"cpu\"})\n> > > \n> > > db = FAISS.load_local(\"faiss_index\"\
          , instructor_embeddings)\n> > > retriever = db.as_retriever(search_kwargs={\"\
          k\": 3})\n> > > \n> > > qa_chain = RetrievalQA.from_chain_type(llm=llm,\
          \ \n> > >                                   chain_type=\"stuff\", \n> >\
          \ >                                   retriever=retriever)\n> > > ```\n\
          > > \n> > @vsns It would be great if you can share a more complete example\
          \ code  where this works for you. I have been trying your example and others\
          \ from langchain on many of these models but the responses are non-sensical\
          \ and/or completely outside the context. Very similar code just works with\
          \ OpenAI models (ada for embedding and 3.5 turbo as the model) making me\
          \ wonder if I am doing something wrong or these models are just not capable.\n\
          > \n> Here you  go:\n> ```\n> import typer\n> \n> # 0xVs\n> \n> from ctransformers.langchain\
          \ import CTransformers\n> from langchain.vectorstores import FAISS\n> from\
          \ langchain.embeddings import HuggingFaceInstructEmbeddings\n> from langchain.document_loaders\
          \ import DirectoryLoader\n> from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n\
          > from langchain.document_loaders import PDFPlumberLoader\n> from langchain.text_splitter\
          \ import RecursiveCharacterTextSplitter\n> from langchain.chains import\
          \ ConversationalRetrievalChain\n> from langchain.memory import ConversationBufferMemory\n\
          > from rich import print\n> from rich.prompt import Prompt\n> \n> app =\
          \ typer.Typer()\n> device = \"cpu\"\n> \n> @app.command()\n> def import_pdfs(dir:\
          \ str, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n> \
          \    loader = DirectoryLoader(dir, glob=\"./*.pdf\", loader_cls=PDFPlumberLoader,\
          \ show_progress=True)\n>     documents = loader.load()\n>     text_splitter\
          \ = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n> \
          \    docs = text_splitter.split_documents(documents)\n> \n>     embeddings\
          \ = HuggingFaceInstructEmbeddings(model_name=embedding_model, \n>      \
          \                                          model_kwargs={\"device\": device})\n\
          >     db = FAISS.from_documents(docs, embeddings)\n>     db.save_local(\"\
          faiss_index\")\n> \n> @app.command()\n> def question(model_path: str = \"\
          ./models/mpt-7b-instruct.ggmlv3.q5_0.bin\",\n>              model_type='mpt',\n\
          >              embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\
          ,\n>              search_breadth : int = 5, threads : int = 6, temperature\
          \ : float = 0.4):\n>     embeddings = HuggingFaceInstructEmbeddings(model_name=embedding_model,\
          \ \n>                                                model_kwargs={\"device\"\
          : device})\n>     config = {'temperature': temperature, 'threads' : threads}\n\
          >     llm = CTransformers(model=model_path, model_type=model_type, config=config)\n\
          >     db = FAISS.load_local(\"faiss_index\", embeddings)\n>     retriever\
          \ = db.as_retriever(search_kwargs={\"k\": search_breadth})\n>     memory\
          \ = ConversationBufferMemory(memory_key=\"chat_history\", output_key=\"\
          answer\", return_messages=True)\n>     qa = ConversationalRetrievalChain.from_llm(llm=llm,\
          \ retriever=retriever,\n>                                              \
          \  memory=memory, return_source_documents=True)\n>     while True:\n>  \
          \       query = Prompt.ask('[bright_yellow]\\nQuestion[/bright_yellow] ')\n\
          >         res = qa({\"question\": query})\n>         print(\"[spring_green4]\"\
          +res['answer']+\"[/spring_green4]\")\n>         if \"source_documents\"\
          \ in res:\n>             print(\"\\n[italic grey46]References[/italic grey46]:\"\
          )\n>             for ref in res[\"source_documents\"]:\n>              \
          \   print(\"> [grey19]\" + ref.metadata['source'] + \"[/grey19]\")\n> \n\
          > if __name__ == \"__main__\":\n>     app()\n> ```\n> Some notes:\n> 1.\
          \ From my experience (take it with pinch of salt) for QA, creating a  good\
          \ vector data is more important than model (i avoid proprietary systems\
          \ or models)\n> 2. I haven't tested code much, and so multiple optimizations\
          \ are possible. To name a few (different embedding model,  use of custom\
          \ prompt template, configuration tweaks etc)\n> 3. Currently considering\
          \ VMware/open-llama-7b-open-instruct with llama-cpp-python, as when I use\
          \ docs on narrow domains with less text, not getting good results\n> 4.\
          \ Ultimately will be planning to have a single static binary (with naive\
          \ assumption that qdrant can be packed inside it) using Rustformers and\
          \ falcon-40b-instruct, when the support is available in it\n> \n> \n> ![docQA.png](https://cdn-uploads.huggingface.co/production/uploads/6427d2ce88215cee63b1728b/PXtm4bDVRIkEnpvvnopWq.png)\n\
          \n\nWhat should be \"question\" in 'python test.py question'? A string?\
          \ Another python file?"
        updatedAt: '2023-07-09T18:53:05.181Z'
      numEdits: 0
      reactions: []
    id: 64ab021132c9473fedd6ecad
    type: comment
  author: rodrigofarias
  content: "> > > My partial code with this model, rest can be referred from langchain\
    \ and ctranformers docs. It works well.\n> > > \n> > > ```\n> > > from langchain.vectorstores\
    \ import FAISS\n> > > from ctransformers.langchain import CTransformers\n> > >\
    \ from langchain.chains import RetrievalQA\n> > > from langchain.embeddings import\
    \ HuggingFaceInstructEmbeddings\n> > > \n> > > llm = CTransformers(model='D:\\\
    \\Ai\\\\models\\\\MPT-7B-Instruct-GGML\\\\mpt-7b-instruct.ggmlv3.q5_0.bin', \n\
    > > >                     model_type='mpt')\n> > > \n> > > instructor_embeddings\
    \ = HuggingFaceInstructEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\
    , \n> > >                                                       model_kwargs={\"\
    device\": \"cpu\"})\n> > > \n> > > db = FAISS.load_local(\"faiss_index\", instructor_embeddings)\n\
    > > > retriever = db.as_retriever(search_kwargs={\"k\": 3})\n> > > \n> > > qa_chain\
    \ = RetrievalQA.from_chain_type(llm=llm, \n> > >                             \
    \      chain_type=\"stuff\", \n> > >                                   retriever=retriever)\n\
    > > > ```\n> > \n> > @vsns It would be great if you can share a more complete\
    \ example code  where this works for you. I have been trying your example and\
    \ others from langchain on many of these models but the responses are non-sensical\
    \ and/or completely outside the context. Very similar code just works with OpenAI\
    \ models (ada for embedding and 3.5 turbo as the model) making me wonder if I\
    \ am doing something wrong or these models are just not capable.\n> \n> Here you\
    \  go:\n> ```\n> import typer\n> \n> # 0xVs\n> \n> from ctransformers.langchain\
    \ import CTransformers\n> from langchain.vectorstores import FAISS\n> from langchain.embeddings\
    \ import HuggingFaceInstructEmbeddings\n> from langchain.document_loaders import\
    \ DirectoryLoader\n> from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n\
    > from langchain.document_loaders import PDFPlumberLoader\n> from langchain.text_splitter\
    \ import RecursiveCharacterTextSplitter\n> from langchain.chains import ConversationalRetrievalChain\n\
    > from langchain.memory import ConversationBufferMemory\n> from rich import print\n\
    > from rich.prompt import Prompt\n> \n> app = typer.Typer()\n> device = \"cpu\"\
    \n> \n> @app.command()\n> def import_pdfs(dir: str, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\
    ):\n>     loader = DirectoryLoader(dir, glob=\"./*.pdf\", loader_cls=PDFPlumberLoader,\
    \ show_progress=True)\n>     documents = loader.load()\n>     text_splitter =\
    \ RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n>     docs\
    \ = text_splitter.split_documents(documents)\n> \n>     embeddings = HuggingFaceInstructEmbeddings(model_name=embedding_model,\
    \ \n>                                                model_kwargs={\"device\"\
    : device})\n>     db = FAISS.from_documents(docs, embeddings)\n>     db.save_local(\"\
    faiss_index\")\n> \n> @app.command()\n> def question(model_path: str = \"./models/mpt-7b-instruct.ggmlv3.q5_0.bin\"\
    ,\n>              model_type='mpt',\n>              embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\
    ,\n>              search_breadth : int = 5, threads : int = 6, temperature : float\
    \ = 0.4):\n>     embeddings = HuggingFaceInstructEmbeddings(model_name=embedding_model,\
    \ \n>                                                model_kwargs={\"device\"\
    : device})\n>     config = {'temperature': temperature, 'threads' : threads}\n\
    >     llm = CTransformers(model=model_path, model_type=model_type, config=config)\n\
    >     db = FAISS.load_local(\"faiss_index\", embeddings)\n>     retriever = db.as_retriever(search_kwargs={\"\
    k\": search_breadth})\n>     memory = ConversationBufferMemory(memory_key=\"chat_history\"\
    , output_key=\"answer\", return_messages=True)\n>     qa = ConversationalRetrievalChain.from_llm(llm=llm,\
    \ retriever=retriever,\n>                                                memory=memory,\
    \ return_source_documents=True)\n>     while True:\n>         query = Prompt.ask('[bright_yellow]\\\
    nQuestion[/bright_yellow] ')\n>         res = qa({\"question\": query})\n>   \
    \      print(\"[spring_green4]\"+res['answer']+\"[/spring_green4]\")\n>      \
    \   if \"source_documents\" in res:\n>             print(\"\\n[italic grey46]References[/italic\
    \ grey46]:\")\n>             for ref in res[\"source_documents\"]:\n>        \
    \         print(\"> [grey19]\" + ref.metadata['source'] + \"[/grey19]\")\n> \n\
    > if __name__ == \"__main__\":\n>     app()\n> ```\n> Some notes:\n> 1. From my\
    \ experience (take it with pinch of salt) for QA, creating a  good vector data\
    \ is more important than model (i avoid proprietary systems or models)\n> 2. I\
    \ haven't tested code much, and so multiple optimizations are possible. To name\
    \ a few (different embedding model,  use of custom prompt template, configuration\
    \ tweaks etc)\n> 3. Currently considering VMware/open-llama-7b-open-instruct with\
    \ llama-cpp-python, as when I use docs on narrow domains with less text, not getting\
    \ good results\n> 4. Ultimately will be planning to have a single static binary\
    \ (with naive assumption that qdrant can be packed inside it) using Rustformers\
    \ and falcon-40b-instruct, when the support is available in it\n> \n> \n> ![docQA.png](https://cdn-uploads.huggingface.co/production/uploads/6427d2ce88215cee63b1728b/PXtm4bDVRIkEnpvvnopWq.png)\n\
    \n\nWhat should be \"question\" in 'python test.py question'? A string? Another\
    \ python file?"
  created_at: 2023-07-09 17:53:05+00:00
  edited: false
  hidden: false
  id: 64ab021132c9473fedd6ecad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60d2bf8cfba448b4227154c74fa8b12a.svg
      fullname: Brian Law
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Data-drone
      type: user
    createdAt: '2023-07-18T04:58:02.000Z'
    data:
      edited: false
      editors:
      - Data-drone
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6536747217178345
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60d2bf8cfba448b4227154c74fa8b12a.svg
          fullname: Brian Law
          isHf: false
          isPro: false
          name: Data-drone
          type: user
        html: "<p>I am getting:</p>\n<p>AttributeError: 'CTransformers' object has\
          \ no attribute 'task'</p>\n<p>That is appearing due to:</p>\n<p>this block\
          \ of code:</p>\n<pre><code>huggingface_pipeline.py:169, in HuggingFacePipeline._call(self,\
          \ prompt, stop, run_manager)\n    162 def _call(\n    163     self,\n  \
          \  164     prompt: str,\n    165     stop: Optional[List[str]] = None,\n\
          \    166     run_manager: Optional[CallbackManagerForLLMRun] = None,\n \
          \   167 ) -&gt; str:\n    168     response = self.pipeline(prompt)\n--&gt;\
          \ 169     if self.pipeline.task == \"text-generation\":\n    170       \
          \  # Text generation return includes the starter text.\n    171        \
          \ text = response[0][\"generated_text\"][len(prompt) :]\n    172     elif\
          \ self.pipeline.task == \"text2text-generation\":\n</code></pre>\n<p>It\
          \ looks like we need to add some sort of pipeline abstraction to ctransformers\
          \ now?</p>\n"
        raw: "I am getting:\n\nAttributeError: 'CTransformers' object has no attribute\
          \ 'task'\n\nThat is appearing due to:\n\nthis block of code:\n\n```\nhuggingface_pipeline.py:169,\
          \ in HuggingFacePipeline._call(self, prompt, stop, run_manager)\n    162\
          \ def _call(\n    163     self,\n    164     prompt: str,\n    165     stop:\
          \ Optional[List[str]] = None,\n    166     run_manager: Optional[CallbackManagerForLLMRun]\
          \ = None,\n    167 ) -> str:\n    168     response = self.pipeline(prompt)\n\
          --> 169     if self.pipeline.task == \"text-generation\":\n    170     \
          \    # Text generation return includes the starter text.\n    171      \
          \   text = response[0][\"generated_text\"][len(prompt) :]\n    172     elif\
          \ self.pipeline.task == \"text2text-generation\":\n```\n\nIt looks like\
          \ we need to add some sort of pipeline abstraction to ctransformers now?"
        updatedAt: '2023-07-18T04:58:02.625Z'
      numEdits: 0
      reactions: []
    id: 64b61bda107dfba5fd1eba89
    type: comment
  author: Data-drone
  content: "I am getting:\n\nAttributeError: 'CTransformers' object has no attribute\
    \ 'task'\n\nThat is appearing due to:\n\nthis block of code:\n\n```\nhuggingface_pipeline.py:169,\
    \ in HuggingFacePipeline._call(self, prompt, stop, run_manager)\n    162 def _call(\n\
    \    163     self,\n    164     prompt: str,\n    165     stop: Optional[List[str]]\
    \ = None,\n    166     run_manager: Optional[CallbackManagerForLLMRun] = None,\n\
    \    167 ) -> str:\n    168     response = self.pipeline(prompt)\n--> 169    \
    \ if self.pipeline.task == \"text-generation\":\n    170         # Text generation\
    \ return includes the starter text.\n    171         text = response[0][\"generated_text\"\
    ][len(prompt) :]\n    172     elif self.pipeline.task == \"text2text-generation\"\
    :\n```\n\nIt looks like we need to add some sort of pipeline abstraction to ctransformers\
    \ now?"
  created_at: 2023-07-18 03:58:02+00:00
  edited: false
  hidden: false
  id: 64b61bda107dfba5fd1eba89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a054a9390447bfd5251ac63993a92efd.svg
      fullname: Vaidik Nakrani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VaidikML0508
      type: user
    createdAt: '2023-10-08T08:59:01.000Z'
    data:
      edited: false
      editors:
      - VaidikML0508
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.732709527015686
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a054a9390447bfd5251ac63993a92efd.svg
          fullname: Vaidik Nakrani
          isHf: false
          isPro: false
          name: VaidikML0508
          type: user
        html: '<p>how can i increase context_length and max_input_seq_token of this
          MPT quantized model?</p>

          '
        raw: how can i increase context_length and max_input_seq_token of this MPT
          quantized model?
        updatedAt: '2023-10-08T08:59:01.798Z'
      numEdits: 0
      reactions: []
    id: 65226f5595df08170c758f1f
    type: comment
  author: VaidikML0508
  content: how can i increase context_length and max_input_seq_token of this MPT quantized
    model?
  created_at: 2023-10-08 07:59:01+00:00
  edited: false
  hidden: false
  id: 65226f5595df08170c758f1f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/MPT-7B-Instruct-GGML
repo_type: model
status: open
target_branch: null
title: Support for Langchain Intergration
