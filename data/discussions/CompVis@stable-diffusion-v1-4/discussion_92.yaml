!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MrSusanovo
conflicting_files: null
created_at: 2022-10-07 19:56:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/265de6c892871c9da0fca3cd853b55b1.svg
      fullname: Bingzheng Feng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrSusanovo
      type: user
    createdAt: '2022-10-07T20:56:35.000Z'
    data:
      edited: false
      editors:
      - MrSusanovo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/265de6c892871c9da0fca3cd853b55b1.svg
          fullname: Bingzheng Feng
          isHf: false
          isPro: false
          name: MrSusanovo
          type: user
        html: '<p>I know I''m dumb enough to be completely lost in papers. But I didn''t
          expect myself to be too dumb to understand the brief intro of state-of-art
          computer vision concepts. There''s something I find quite confusing after
          reading and playing around with the example Colab notebook. </p>

          <p>To my understanding, the general structure of the current stable diffusion
          pipeline consists of mainly 4 parts, a VAE, a UNet that does the reverse
          diffusion process in latent space, a text encoder trained on caption image
          pairs, and a scheduler. </p>

          <p>My questions are:</p>

          <ol>

          <li>What does the scheduler do? Why not directly subtract the noise predicted
          after each inference step from the latent?</li>

          <li>Among these 4 parts, which are the ones that you trained? The notebook
          says the text encoder is off-the-shelf. I guess the latent UNet is trained.
          I''m not quite sure about VAE and schedulers (do they need to be trained
          or are they some fixed-steps algorithms?)</li>

          <li>Probably not too relevant here if the VAE is something need to be trained
          but off-the-shelf. If they need to be trained, how should we train them?
          Input and output should be equal dimension and the best result would be
          VAE just does nothing but produces original input. How do we make it "compress"
          the input but keep the important info effectively.</li>

          <li>About text encoders, can we switch it to some other model other than
          "openai/clip-vit-large-patch14" (say I want to support multi-lingual or
          a very specific category of text-image pair)? What would be the restrictions
          here? Or what are the constrains for making the text encoder or even the
          whole diffusion pipeline more modular. </li>

          <li>How does the cross-attention layer work if the UNnet takes 3<em>64</em>64
          latent but text encoder produces 2<em>77</em>768 (I think it''s always encoding
          an empty string for unconditional prompt, thus I guess it''s 2<em>77</em>768
          here) output.</li>

          <li>If we simply want to denoise a picture, among the 4 parts (text encoder,
          VAE, UNet, scheduelr) which are still necessary and which don''t?</li>

          </ol>

          <p>Apologize for dumping a ton of dumb questions and for any duplicate questions,
          hugging face doesn''t seem to provide a search function here, would appreciate
          any answer or links. </p>

          <p>Last but not least, thank you for such amazing project and almost the
          best intro notebook I''ve ever seen.</p>

          '
        raw: "I know I'm dumb enough to be completely lost in papers. But I didn't\
          \ expect myself to be too dumb to understand the brief intro of state-of-art\
          \ computer vision concepts. There's something I find quite confusing after\
          \ reading and playing around with the example Colab notebook. \r\n\r\nTo\
          \ my understanding, the general structure of the current stable diffusion\
          \ pipeline consists of mainly 4 parts, a VAE, a UNet that does the reverse\
          \ diffusion process in latent space, a text encoder trained on caption image\
          \ pairs, and a scheduler. \r\n\r\nMy questions are:\r\n1.  What does the\
          \ scheduler do? Why not directly subtract the noise predicted after each\
          \ inference step from the latent?\r\n2. Among these 4 parts, which are the\
          \ ones that you trained? The notebook says the text encoder is off-the-shelf.\
          \ I guess the latent UNet is trained. I'm not quite sure about VAE and schedulers\
          \ (do they need to be trained or are they some fixed-steps algorithms?)\r\
          \n3. Probably not too relevant here if the VAE is something need to be trained\
          \ but off-the-shelf. If they need to be trained, how should we train them?\
          \ Input and output should be equal dimension and the best result would be\
          \ VAE just does nothing but produces original input. How do we make it \"\
          compress\" the input but keep the important info effectively.\r\n4. About\
          \ text encoders, can we switch it to some other model other than \"openai/clip-vit-large-patch14\"\
          \ (say I want to support multi-lingual or a very specific category of text-image\
          \ pair)? What would be the restrictions here? Or what are the constrains\
          \ for making the text encoder or even the whole diffusion pipeline more\
          \ modular. \r\n5. How does the cross-attention layer work if the UNnet takes\
          \ 3*64*64 latent but text encoder produces 2*77*768 (I think it's always\
          \ encoding an empty string for unconditional prompt, thus I guess it's 2*77*768\
          \ here) output.\r\n6. If we simply want to denoise a picture, among the\
          \ 4 parts (text encoder, VAE, UNet, scheduelr) which are still necessary\
          \ and which don't?\r\n\r\nApologize for dumping a ton of dumb questions\
          \ and for any duplicate questions, hugging face doesn't seem to provide\
          \ a search function here, would appreciate any answer or links. \r\n\r\n\
          Last but not least, thank you for such amazing project and almost the best\
          \ intro notebook I've ever seen."
        updatedAt: '2022-10-07T20:56:35.257Z'
      numEdits: 0
      reactions: []
    id: 634092839dbfe0d48b2e5a4d
    type: comment
  author: MrSusanovo
  content: "I know I'm dumb enough to be completely lost in papers. But I didn't expect\
    \ myself to be too dumb to understand the brief intro of state-of-art computer\
    \ vision concepts. There's something I find quite confusing after reading and\
    \ playing around with the example Colab notebook. \r\n\r\nTo my understanding,\
    \ the general structure of the current stable diffusion pipeline consists of mainly\
    \ 4 parts, a VAE, a UNet that does the reverse diffusion process in latent space,\
    \ a text encoder trained on caption image pairs, and a scheduler. \r\n\r\nMy questions\
    \ are:\r\n1.  What does the scheduler do? Why not directly subtract the noise\
    \ predicted after each inference step from the latent?\r\n2. Among these 4 parts,\
    \ which are the ones that you trained? The notebook says the text encoder is off-the-shelf.\
    \ I guess the latent UNet is trained. I'm not quite sure about VAE and schedulers\
    \ (do they need to be trained or are they some fixed-steps algorithms?)\r\n3.\
    \ Probably not too relevant here if the VAE is something need to be trained but\
    \ off-the-shelf. If they need to be trained, how should we train them? Input and\
    \ output should be equal dimension and the best result would be VAE just does\
    \ nothing but produces original input. How do we make it \"compress\" the input\
    \ but keep the important info effectively.\r\n4. About text encoders, can we switch\
    \ it to some other model other than \"openai/clip-vit-large-patch14\" (say I want\
    \ to support multi-lingual or a very specific category of text-image pair)? What\
    \ would be the restrictions here? Or what are the constrains for making the text\
    \ encoder or even the whole diffusion pipeline more modular. \r\n5. How does the\
    \ cross-attention layer work if the UNnet takes 3*64*64 latent but text encoder\
    \ produces 2*77*768 (I think it's always encoding an empty string for unconditional\
    \ prompt, thus I guess it's 2*77*768 here) output.\r\n6. If we simply want to\
    \ denoise a picture, among the 4 parts (text encoder, VAE, UNet, scheduelr) which\
    \ are still necessary and which don't?\r\n\r\nApologize for dumping a ton of dumb\
    \ questions and for any duplicate questions, hugging face doesn't seem to provide\
    \ a search function here, would appreciate any answer or links. \r\n\r\nLast but\
    \ not least, thank you for such amazing project and almost the best intro notebook\
    \ I've ever seen."
  created_at: 2022-10-07 19:56:35+00:00
  edited: false
  hidden: false
  id: 634092839dbfe0d48b2e5a4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31f227e021b82ea748150e09beca3b03.svg
      fullname: sdfsf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ersdf
      type: user
    createdAt: '2023-03-23T02:31:39.000Z'
    data:
      edited: false
      editors:
      - ersdf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31f227e021b82ea748150e09beca3b03.svg
          fullname: sdfsf
          isHf: false
          isPro: false
          name: ersdf
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6414a20b479c98a0b36ff3aa/2vZxLOSQSvmC7Qbaw1enw.png"><img
          alt="st.png" src="https://cdn-uploads.huggingface.co/production/uploads/6414a20b479c98a0b36ff3aa/2vZxLOSQSvmC7Qbaw1enw.png"></a><br>
          Hello! can anybody help? i cant train.</p>

          '
        raw: "![st.png](https://cdn-uploads.huggingface.co/production/uploads/6414a20b479c98a0b36ff3aa/2vZxLOSQSvmC7Qbaw1enw.png)\n\
          \ Hello! can anybody help? i cant train."
        updatedAt: '2023-03-23T02:31:39.120Z'
      numEdits: 0
      reactions: []
    id: 641bba0b1b6ca4151da503c2
    type: comment
  author: ersdf
  content: "![st.png](https://cdn-uploads.huggingface.co/production/uploads/6414a20b479c98a0b36ff3aa/2vZxLOSQSvmC7Qbaw1enw.png)\n\
    \ Hello! can anybody help? i cant train."
  created_at: 2023-03-23 01:31:39+00:00
  edited: false
  hidden: false
  id: 641bba0b1b6ca4151da503c2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 92
repo_id: CompVis/stable-diffusion-v1-4
repo_type: model
status: open
target_branch: null
title: Questions about How Stable Diffusion works and was trained.
