!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xalex
conflicting_files: null
created_at: 2022-08-28 13:15:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xalex
      type: user
    createdAt: '2022-08-28T14:15:47.000Z'
    data:
      edited: false
      editors:
      - xalex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: xalex
          type: user
        html: '<p>There are <a rel="nofollow" href="https://gist.github.com/karpathy/00103b0037c5aaea32fe1da1af553355">scripts</a>
          for interpolating between different seeds to explore the latent space. Using
          a slerp function between two seeds is interesting, but I wonder if one can
          re-seed with the old latent space.</p>

          <p>I tried to return <code>cond_latents</code> (without the <code>1/0.18215</code>
          factor)  from the <code>diffuse</code> method and then slerp between a normal
          distribution (like <code>init2</code>) and the returned <code>cont_latents</code>
          to only add a bit of noise to the current latent state in the hope to find
          solutions near the previous state. But when I try to slerp or linearly interpolate
          between a new random seed and the latent space, I don''t get usable results.</p>

          <p>I especially have problems how to normalize the tensor as using it naively
          results in diverging values and images getting brighter and brighter. Substracting
          the mean results in too dim images.</p>

          '
        raw: "There are [scripts](https://gist.github.com/karpathy/00103b0037c5aaea32fe1da1af553355)\
          \ for interpolating between different seeds to explore the latent space.\
          \ Using a slerp function between two seeds is interesting, but I wonder\
          \ if one can re-seed with the old latent space.\r\n\r\nI tried to return\
          \ `cond_latents` (without the `1/0.18215` factor)  from the `diffuse` method\
          \ and then slerp between a normal distribution (like `init2`) and the returned\
          \ `cont_latents` to only add a bit of noise to the current latent state\
          \ in the hope to find solutions near the previous state. But when I try\
          \ to slerp or linearly interpolate between a new random seed and the latent\
          \ space, I don't get usable results.\r\n\r\nI especially have problems how\
          \ to normalize the tensor as using it naively results in diverging values\
          \ and images getting brighter and brighter. Substracting the mean results\
          \ in too dim images.\r\n\r\n"
        updatedAt: '2022-08-28T14:15:47.905Z'
      numEdits: 0
      reactions: []
    id: 630b789386b8b9904c30e729
    type: comment
  author: xalex
  content: "There are [scripts](https://gist.github.com/karpathy/00103b0037c5aaea32fe1da1af553355)\
    \ for interpolating between different seeds to explore the latent space. Using\
    \ a slerp function between two seeds is interesting, but I wonder if one can re-seed\
    \ with the old latent space.\r\n\r\nI tried to return `cond_latents` (without\
    \ the `1/0.18215` factor)  from the `diffuse` method and then slerp between a\
    \ normal distribution (like `init2`) and the returned `cont_latents` to only add\
    \ a bit of noise to the current latent state in the hope to find solutions near\
    \ the previous state. But when I try to slerp or linearly interpolate between\
    \ a new random seed and the latent space, I don't get usable results.\r\n\r\n\
    I especially have problems how to normalize the tensor as using it naively results\
    \ in diverging values and images getting brighter and brighter. Substracting the\
    \ mean results in too dim images.\r\n\r\n"
  created_at: 2022-08-28 13:15:47+00:00
  edited: false
  hidden: false
  id: 630b789386b8b9904c30e729
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a872e4d2f01216debf40daa446e90ab9.svg
      fullname: Alexander Tarashansky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: atarashansky
      type: user
    createdAt: '2022-08-28T19:06:07.000Z'
    data:
      edited: false
      editors:
      - atarashansky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a872e4d2f01216debf40daa446e90ab9.svg
          fullname: Alexander Tarashansky
          isHf: false
          isPro: false
          name: atarashansky
          type: user
        html: '<p>Let''s say you want to interpolate between two prompts, with each
          resulting image "rooted" in the output of the first prompt.</p>

          <p>This is what I''ve found to work for me:</p>

          <ol>

          <li>Your noise at each interpolation step will be the output of <code>slerp</code>.</li>

          <li>That noise is added at timestep 0 to the encoded "root" image (using
          scheduler <code>add_noise</code> function)</li>

          <li>Start diffusing</li>

          </ol>

          '
        raw: 'Let''s say you want to interpolate between two prompts, with each resulting
          image "rooted" in the output of the first prompt.


          This is what I''ve found to work for me:

          1. Your noise at each interpolation step will be the output of `slerp`.

          2. That noise is added at timestep 0 to the encoded "root" image (using
          scheduler `add_noise` function)

          3. Start diffusing'
        updatedAt: '2022-08-28T19:06:07.354Z'
      numEdits: 0
      reactions: []
    id: 630bbc9fa20a5367811dd112
    type: comment
  author: atarashansky
  content: 'Let''s say you want to interpolate between two prompts, with each resulting
    image "rooted" in the output of the first prompt.


    This is what I''ve found to work for me:

    1. Your noise at each interpolation step will be the output of `slerp`.

    2. That noise is added at timestep 0 to the encoded "root" image (using scheduler
    `add_noise` function)

    3. Start diffusing'
  created_at: 2022-08-28 18:06:07+00:00
  edited: false
  hidden: false
  id: 630bbc9fa20a5367811dd112
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xalex
      type: user
    createdAt: '2022-08-28T19:18:13.000Z'
    data:
      edited: true
      editors:
      - xalex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: xalex
          type: user
        html: '<p>That''s kind of what the script does. It just interpolates two seeds
          and then diffuses step 0 to N for the interpolated noise vector.</p>

          <p>Suppose I interpolate with a small <code>alpha</code> the noise <code>init2
          = init1 * (1.0-alpha) + new_noise * alpha</code> for a second image.<br>Then
          it would probably be a waste of time to diffuse <code>init2</code> for N
          steps to get an almost identical image as you''ve got before with <code>init1</code>,
          when I could diffuse <code>result_latent_vector + alpha * new_noise</code>
          for only a few steps to find the second image.</p>

          <p>Of course it is a good question how much noise you have to add to avoid
          getting the same result, but to explore how to work with the latent vector
          I would first need to understand how the latent vector can be reused. As
          the code uses <code>cond_latents</code> as name for the seed noise, I suppose
          the seed is the initialization of the latent vector and it should in principle
          be possible to use the vector as seed for a new diffusion. But in practice
          it didn''t work for me.</p>

          '
        raw: 'That''s kind of what the script does. It just interpolates two seeds
          and then diffuses step 0 to N for the interpolated noise vector.


          Suppose I interpolate with a small `alpha` the noise `init2 = init1 * (1.0-alpha)
          + new_noise * alpha` for a second image.

          Then it would probably be a waste of time to diffuse `init2` for N steps
          to get an almost identical image as you''ve got before with `init1`, when
          I could diffuse `result_latent_vector + alpha * new_noise` for only a few
          steps to find the second image.


          Of course it is a good question how much noise you have to add to avoid
          getting the same result, but to explore how to work with the latent vector
          I would first need to understand how the latent vector can be reused. As
          the code uses `cond_latents` as name for the seed noise, I suppose the seed
          is the initialization of the latent vector and it should in principle be
          possible to use the vector as seed for a new diffusion. But in practice
          it didn''t work for me.'
        updatedAt: '2022-08-28T19:19:36.059Z'
      numEdits: 2
      reactions: []
    id: 630bbf758b327c7b8b9b8ed9
    type: comment
  author: xalex
  content: 'That''s kind of what the script does. It just interpolates two seeds and
    then diffuses step 0 to N for the interpolated noise vector.


    Suppose I interpolate with a small `alpha` the noise `init2 = init1 * (1.0-alpha)
    + new_noise * alpha` for a second image.

    Then it would probably be a waste of time to diffuse `init2` for N steps to get
    an almost identical image as you''ve got before with `init1`, when I could diffuse
    `result_latent_vector + alpha * new_noise` for only a few steps to find the second
    image.


    Of course it is a good question how much noise you have to add to avoid getting
    the same result, but to explore how to work with the latent vector I would first
    need to understand how the latent vector can be reused. As the code uses `cond_latents`
    as name for the seed noise, I suppose the seed is the initialization of the latent
    vector and it should in principle be possible to use the vector as seed for a
    new diffusion. But in practice it didn''t work for me.'
  created_at: 2022-08-28 18:18:13+00:00
  edited: true
  hidden: false
  id: 630bbf758b327c7b8b9b8ed9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a872e4d2f01216debf40daa446e90ab9.svg
      fullname: Alexander Tarashansky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: atarashansky
      type: user
    createdAt: '2022-08-28T19:34:29.000Z'
    data:
      edited: false
      editors:
      - atarashansky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a872e4d2f01216debf40daa446e90ab9.svg
          fullname: Alexander Tarashansky
          isHf: false
          isPro: false
          name: atarashansky
          type: user
        html: '<p>The image-to-image script <code>img2img.py</code> essentially does
          what you''re describing, you could adapt it to work with the interpolation
          script. It takes an image, corrupts it with a lot-or-a-little amount of
          noise, depending on the strength parameter, and then "resumes" the diffusion
          process for the corresponding number of steps.</p>

          <p>For example, let num_inference_steps=50, strength=0.5:<br>img2img will:</p>

          <ol>

          <li>encode your input image (could be the output of previous interpolation
          step)</li>

          <li>corrupt it with the corresponding amount of noise you''d expect from
          step strength*num_inference_steps=25</li>

          <li>resume diffusion for the remaining 25 steps<br>strength=0 means that
          you''re starting from pure noise as usual, strength will give you back the
          input image.</li>

          </ol>

          '
        raw: 'The image-to-image script `img2img.py` essentially does what you''re
          describing, you could adapt it to work with the interpolation script. It
          takes an image, corrupts it with a lot-or-a-little amount of noise, depending
          on the strength parameter, and then "resumes" the diffusion process for
          the corresponding number of steps.


          For example, let num_inference_steps=50, strength=0.5:

          img2img will:

          1) encode your input image (could be the output of previous interpolation
          step)

          2) corrupt it with the corresponding amount of noise you''d expect from
          step strength*num_inference_steps=25

          3) resume diffusion for the remaining 25 steps

          strength=0 means that you''re starting from pure noise as usual, strength
          will give you back the input image.'
        updatedAt: '2022-08-28T19:34:29.432Z'
      numEdits: 0
      reactions: []
    id: 630bc345e67c604e9b7f27f9
    type: comment
  author: atarashansky
  content: 'The image-to-image script `img2img.py` essentially does what you''re describing,
    you could adapt it to work with the interpolation script. It takes an image, corrupts
    it with a lot-or-a-little amount of noise, depending on the strength parameter,
    and then "resumes" the diffusion process for the corresponding number of steps.


    For example, let num_inference_steps=50, strength=0.5:

    img2img will:

    1) encode your input image (could be the output of previous interpolation step)

    2) corrupt it with the corresponding amount of noise you''d expect from step strength*num_inference_steps=25

    3) resume diffusion for the remaining 25 steps

    strength=0 means that you''re starting from pure noise as usual, strength will
    give you back the input image.'
  created_at: 2022-08-28 18:34:29+00:00
  edited: false
  hidden: false
  id: 630bc345e67c604e9b7f27f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xalex
      type: user
    createdAt: '2022-08-28T20:17:06.000Z'
    data:
      edited: false
      editors:
      - xalex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: xalex
          type: user
        html: '<p>I had a look into the diffusers <code>StableDiffusionImg2ImgPipeline</code>
          yesterday but didn''t get it to work yet.<br>For the img2img script in the
          <code>stable-diffusion</code> repo I first need to get the original code
          running as I am currently using the <code>diffusers</code> implementation
          for everything. But maybe I can reuse some parts of the script in the diffusers-based
          script.</p>

          '
        raw: 'I had a look into the diffusers `StableDiffusionImg2ImgPipeline` yesterday
          but didn''t get it to work yet.

          For the img2img script in the `stable-diffusion` repo I first need to get
          the original code running as I am currently using the `diffusers` implementation
          for everything. But maybe I can reuse some parts of the script in the diffusers-based
          script.'
        updatedAt: '2022-08-28T20:17:06.993Z'
      numEdits: 0
      reactions: []
    id: 630bcd428b327c7b8b9c118a
    type: comment
  author: xalex
  content: 'I had a look into the diffusers `StableDiffusionImg2ImgPipeline` yesterday
    but didn''t get it to work yet.

    For the img2img script in the `stable-diffusion` repo I first need to get the
    original code running as I am currently using the `diffusers` implementation for
    everything. But maybe I can reuse some parts of the script in the diffusers-based
    script.'
  created_at: 2022-08-28 19:17:06+00:00
  edited: false
  hidden: false
  id: 630bcd428b327c7b8b9c118a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a872e4d2f01216debf40daa446e90ab9.svg
      fullname: Alexander Tarashansky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: atarashansky
      type: user
    createdAt: '2022-08-28T20:57:30.000Z'
    data:
      edited: true
      editors:
      - atarashansky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a872e4d2f01216debf40daa446e90ab9.svg
          fullname: Alexander Tarashansky
          isHf: false
          isPro: false
          name: atarashansky
          type: user
        html: "<p>Check out my fork of diffusers - I got image-to-image to work: <a\
          \ rel=\"nofollow\" href=\"https://github.com/atarashansky/diffusers\">https://github.com/atarashansky/diffusers</a></p>\n\
          <p>Example usage (be warned, I am not using the safety checker as I found\
          \ it a little too restrictive):</p>\n<pre><code class=\"language-python\"\
          >model_id = <span class=\"hljs-string\">\"CompVis/stable-diffusion-v1-4\"\
          </span>\npipe = StableDiffusionPipeline.from_pretrained(model_id,...)\n\
          pipe = pipe.to(<span class=\"hljs-string\">\"cuda\"</span>)\nprompt = <span\
          \ class=\"hljs-string\">\"a fantasy landscape, trending on artstation\"\
          </span>\ninit_image = .... <span class=\"hljs-comment\"># a PIL.Image object.</span>\n\
          grid,images,seeds = pipe.make_grid(\n    prompt,\n    seed=<span class=\"\
          hljs-number\">1234</span>,\n    height=<span class=\"hljs-number\">512</span>,\n\
          \    width=<span class=\"hljs-number\">512</span>,\n    num_rows=<span class=\"\
          hljs-number\">3</span>,\n    num_columns=<span class=\"hljs-number\">3</span>,\n\
          \    num_inference_steps=<span class=\"hljs-number\">59</span>,\n    guidance_scale=<span\
          \ class=\"hljs-number\">7.5</span>,\n    init_image=init_image, <span class=\"\
          hljs-comment\"># set this to None to do normal text2image</span>\n    strength=<span\
          \ class=\"hljs-number\">0.4</span> <span class=\"hljs-comment\"># this is\
          \ the strength parameter.</span>\n)\n</code></pre>\n<p>One more disclaimer,\
          \ this currently only works with the default scheduler.</p>\n"
        raw: "Check out my fork of diffusers - I got image-to-image to work: https://github.com/atarashansky/diffusers\n\
          \nExample usage (be warned, I am not using the safety checker as I found\
          \ it a little too restrictive):\n\n```python\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\
          \npipe = StableDiffusionPipeline.from_pretrained(model_id,...)\npipe = pipe.to(\"\
          cuda\")\nprompt = \"a fantasy landscape, trending on artstation\"\ninit_image\
          \ = .... # a PIL.Image object.\ngrid,images,seeds = pipe.make_grid(\n  \
          \  prompt,\n    seed=1234,\n    height=512,\n    width=512,\n    num_rows=3,\n\
          \    num_columns=3,\n    num_inference_steps=59,\n    guidance_scale=7.5,\n\
          \    init_image=init_image, # set this to None to do normal text2image\n\
          \    strength=0.4 # this is the strength parameter.\n)\n```\n\nOne more\
          \ disclaimer, this currently only works with the default scheduler."
        updatedAt: '2022-08-28T21:06:50.537Z'
      numEdits: 4
      reactions: []
    id: 630bd6ba4c0945d20b88dd5a
    type: comment
  author: atarashansky
  content: "Check out my fork of diffusers - I got image-to-image to work: https://github.com/atarashansky/diffusers\n\
    \nExample usage (be warned, I am not using the safety checker as I found it a\
    \ little too restrictive):\n\n```python\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\
    \npipe = StableDiffusionPipeline.from_pretrained(model_id,...)\npipe = pipe.to(\"\
    cuda\")\nprompt = \"a fantasy landscape, trending on artstation\"\ninit_image\
    \ = .... # a PIL.Image object.\ngrid,images,seeds = pipe.make_grid(\n    prompt,\n\
    \    seed=1234,\n    height=512,\n    width=512,\n    num_rows=3,\n    num_columns=3,\n\
    \    num_inference_steps=59,\n    guidance_scale=7.5,\n    init_image=init_image,\
    \ # set this to None to do normal text2image\n    strength=0.4 # this is the strength\
    \ parameter.\n)\n```\n\nOne more disclaimer, this currently only works with the\
    \ default scheduler."
  created_at: 2022-08-28 19:57:30+00:00
  edited: true
  hidden: false
  id: 630bd6ba4c0945d20b88dd5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xalex
      type: user
    createdAt: '2022-08-29T18:51:24.000Z'
    data:
      edited: true
      editors:
      - xalex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: xalex
          type: user
        html: '<blockquote>

          <p>One more disclaimer, this currently only works with the default scheduler.</p>

          </blockquote>

          <p>Thank you, that was my problem why I didn''t get the <code>image_to_image</code>
          example class to work in an own script.</p>

          '
        raw: '> One more disclaimer, this currently only works with the default scheduler.


          Thank you, that was my problem why I didn''t get the `image_to_image` example
          class to work in an own script.'
        updatedAt: '2022-08-29T19:08:31.391Z'
      numEdits: 1
      reactions: []
    id: 630d0aac4ca0a22768bd04a2
    type: comment
  author: xalex
  content: '> One more disclaimer, this currently only works with the default scheduler.


    Thank you, that was my problem why I didn''t get the `image_to_image` example
    class to work in an own script.'
  created_at: 2022-08-29 17:51:24+00:00
  edited: true
  hidden: false
  id: 630d0aac4ca0a22768bd04a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2022-09-01T21:17:25.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: '<p>This colab might also be useful! It shows how to reuse images of
          the same seed:<br><a rel="nofollow" href="https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb">https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb</a></p>

          '
        raw: 'This colab might also be useful! It shows how to reuse images of the
          same seed:

          https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb'
        updatedAt: '2022-09-01T21:17:25.237Z'
      numEdits: 0
      reactions: []
    id: 631121657680dc699b1c2885
    type: comment
  author: patrickvonplaten
  content: 'This colab might also be useful! It shows how to reuse images of the same
    seed:

    https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb'
  created_at: 2022-09-01 20:17:25+00:00
  edited: false
  hidden: false
  id: 631121657680dc699b1c2885
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: CompVis/stable-diffusion-v1-4
repo_type: model
status: open
target_branch: null
title: How to re-use latent state as a new seed?
