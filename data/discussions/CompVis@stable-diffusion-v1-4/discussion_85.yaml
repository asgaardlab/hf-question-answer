!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xalex
conflicting_files: null
created_at: 2022-10-05 16:10:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xalex
      type: user
    createdAt: '2022-10-05T17:10:40.000Z'
    data:
      edited: false
      editors:
      - xalex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: xalex
          type: user
        html: '<p>I wonder if textual inversion can overfit or may only work better
          with more training. If I understand it correctly, it optimizes the position
          of a single token in the text embedding. I wonder if an overfitted solution
          is still a good solution or even the best and if local minima are the bigger
          problem.<br>In one test, 6000 steps (with default parameters, i.e., a constant
          learning rate) seemed to perform worse than 3000, though.</p>

          '
        raw: "I wonder if textual inversion can overfit or may only work better with\
          \ more training. If I understand it correctly, it optimizes the position\
          \ of a single token in the text embedding. I wonder if an overfitted solution\
          \ is still a good solution or even the best and if local minima are the\
          \ bigger problem.\r\nIn one test, 6000 steps (with default parameters, i.e.,\
          \ a constant learning rate) seemed to perform worse than 3000, though."
        updatedAt: '2022-10-05T17:10:40.787Z'
      numEdits: 0
      reactions: []
    id: 633dba90d3c648c81d4cc97f
    type: comment
  author: xalex
  content: "I wonder if textual inversion can overfit or may only work better with\
    \ more training. If I understand it correctly, it optimizes the position of a\
    \ single token in the text embedding. I wonder if an overfitted solution is still\
    \ a good solution or even the best and if local minima are the bigger problem.\r\
    \nIn one test, 6000 steps (with default parameters, i.e., a constant learning\
    \ rate) seemed to perform worse than 3000, though."
  created_at: 2022-10-05 16:10:40+00:00
  edited: false
  hidden: false
  id: 633dba90d3c648c81d4cc97f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 85
repo_id: CompVis/stable-diffusion-v1-4
repo_type: model
status: open
target_branch: null
title: Can textual inversion overfit?
