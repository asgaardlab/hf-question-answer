!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wangzehua99
conflicting_files: null
created_at: 2022-12-02 02:29:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6058afd007bd30a7dae722016006d1c8.svg
      fullname: zehua
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wangzehua99
      type: user
    createdAt: '2022-12-02T02:29:16.000Z'
    data:
      edited: false
      editors:
      - wangzehua99
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6058afd007bd30a7dae722016006d1c8.svg
          fullname: zehua
          isHf: false
          isPro: false
          name: wangzehua99
          type: user
        html: '<p>import torch<br>from diffusers import StableDiffusionPipeline</p>

          <p>model_id = "CompVis/stable-diffusion-v1-4"<br>device = "cuda"</p>

          <p>pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16,
          revision="fp16"). ---- show the bug , i do know why?<br>pipe = pipe.to(device)</p>

          '
        raw: "import torch\r\nfrom diffusers import StableDiffusionPipeline\r\n\r\n\
          model_id = \"CompVis/stable-diffusion-v1-4\"\r\ndevice = \"cuda\"\r\n\r\n\
          \r\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16,\
          \ revision=\"fp16\"). ---- show the bug , i do know why?\r\npipe = pipe.to(device)\r\
          \n"
        updatedAt: '2022-12-02T02:29:16.015Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F91D"
        users:
        - YutaNago
        - HikaruBear
        - fiojja
      - count: 3
        reaction: "\U0001F44D"
        users:
        - HikaruBear
        - ricgu8086
        - Guytron
    id: 638962fc80134ba508d3f39c
    type: comment
  author: wangzehua99
  content: "import torch\r\nfrom diffusers import StableDiffusionPipeline\r\n\r\n\
    model_id = \"CompVis/stable-diffusion-v1-4\"\r\ndevice = \"cuda\"\r\n\r\n\r\n\
    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16,\
    \ revision=\"fp16\"). ---- show the bug , i do know why?\r\npipe = pipe.to(device)\r\
    \n"
  created_at: 2022-12-02 02:29:16+00:00
  edited: false
  hidden: false
  id: 638962fc80134ba508d3f39c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5ebf58aca34e7eb7cb56d1188118e7ed.svg
      fullname: Jiahao Xie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jiahao000
      type: user
    createdAt: '2022-12-02T13:39:42.000Z'
    data:
      edited: true
      editors:
      - Jiahao000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5ebf58aca34e7eb7cb56d1188118e7ed.svg
          fullname: Jiahao Xie
          isHf: false
          isPro: false
          name: Jiahao000
          type: user
        html: '<p>same issue to you</p>

          '
        raw: same issue to you
        updatedAt: '2022-12-02T13:40:27.192Z'
      numEdits: 1
      reactions: []
    id: 638a001e78e0e04244922f7c
    type: comment
  author: Jiahao000
  content: same issue to you
  created_at: 2022-12-02 13:39:42+00:00
  edited: true
  hidden: false
  id: 638a001e78e0e04244922f7c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/abd31d5eda3171cd05c2a0891d637a4e.svg
      fullname: wt.liao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cvddl
      type: user
    createdAt: '2022-12-02T13:45:00.000Z'
    data:
      edited: false
      editors:
      - cvddl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/abd31d5eda3171cd05c2a0891d637a4e.svg
          fullname: wt.liao
          isHf: false
          isPro: false
          name: cvddl
          type: user
        html: '<p>same issue to you</p>

          '
        raw: same issue to you
        updatedAt: '2022-12-02T13:45:00.989Z'
      numEdits: 0
      reactions: []
    id: 638a015c3f241d84efb6abb2
    type: comment
  author: cvddl
  content: same issue to you
  created_at: 2022-12-02 13:45:00+00:00
  edited: false
  hidden: false
  id: 638a015c3f241d84efb6abb2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d71b08b4bb4983951517464b5dca9fe6.svg
      fullname: Martinez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jeyner
      type: user
    createdAt: '2022-12-02T14:05:02.000Z'
    data:
      edited: false
      editors:
      - Jeyner
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d71b08b4bb4983951517464b5dca9fe6.svg
          fullname: Martinez
          isHf: false
          isPro: false
          name: Jeyner
          type: user
        html: '<p>same issue</p>

          '
        raw: same issue
        updatedAt: '2022-12-02T14:05:02.425Z'
      numEdits: 0
      reactions: []
    id: 638a060e2e5eabcc7228cd81
    type: comment
  author: Jeyner
  content: same issue
  created_at: 2022-12-02 14:05:02+00:00
  edited: false
  hidden: false
  id: 638a060e2e5eabcc7228cd81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0a8a2408a7edb381335de3482f196b7.svg
      fullname: grinbla6t
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shirgr
      type: user
    createdAt: '2022-12-02T15:03:37.000Z'
    data:
      edited: false
      editors:
      - shirgr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0a8a2408a7edb381335de3482f196b7.svg
          fullname: grinbla6t
          isHf: false
          isPro: false
          name: shirgr
          type: user
        html: '<p>same issue!</p>

          '
        raw: same issue!
        updatedAt: '2022-12-02T15:03:37.710Z'
      numEdits: 0
      reactions: []
    id: 638a13c958afc73b9c6449d1
    type: comment
  author: shirgr
  content: same issue!
  created_at: 2022-12-02 15:03:37+00:00
  edited: false
  hidden: false
  id: 638a13c958afc73b9c6449d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f96aad1a6fb9d0c5d5b6f32273ad85c5.svg
      fullname: Lionel Happy Head
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lionelhappyhead
      type: user
    createdAt: '2022-12-02T16:05:25.000Z'
    data:
      edited: false
      editors:
      - lionelhappyhead
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f96aad1a6fb9d0c5d5b6f32273ad85c5.svg
          fullname: Lionel Happy Head
          isHf: false
          isPro: false
          name: lionelhappyhead
          type: user
        html: '<p>Same issue</p>

          '
        raw: Same issue
        updatedAt: '2022-12-02T16:05:25.390Z'
      numEdits: 0
      reactions: []
    id: 638a2245dfb52fb6e971d3a8
    type: comment
  author: lionelhappyhead
  content: Same issue
  created_at: 2022-12-02 16:05:25+00:00
  edited: false
  hidden: false
  id: 638a2245dfb52fb6e971d3a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0b06e76d81f0a0593a81f261ebd753c2.svg
      fullname: Sebastien Samson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sebsamson
      type: user
    createdAt: '2022-12-02T18:32:17.000Z'
    data:
      edited: false
      editors:
      - sebsamson
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0b06e76d81f0a0593a81f261ebd753c2.svg
          fullname: Sebastien Samson
          isHf: false
          isPro: false
          name: sebsamson
          type: user
        html: '<p>same here too. </p>

          <p> in <br>      3<br>      4 # make sure you''re logged in with <code>huggingface-cli
          login</code><br>----&gt; 5 pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4",
          revision="fp16", torch_dtype=torch.float16)</p>

          <p>/usr/local/lib/python3.8/dist-packages/diffusers/pipeline_utils.py in
          from_pretrained(cls, pretrained_model_name_or_path, **kwargs)<br>    453                         load_method_name
          = importable_classes[class_name][1]<br>    454<br>--&gt; 455                 load_method
          = getattr(class_obj, load_method_name)<br>    456<br>    457                 loading_kwargs
          = {}</p>

          <p>TypeError: getattr(): attribute name must be string</p>

          '
        raw: "same here too. \n\n<ipython-input-5-9f35f56a3ca3> in <module>\n    \
          \  3 \n      4 # make sure you're logged in with `huggingface-cli login`\n\
          ----> 5 pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\"\
          , revision=\"fp16\", torch_dtype=torch.float16)\n\n/usr/local/lib/python3.8/dist-packages/diffusers/pipeline_utils.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n   \
          \ 453                         load_method_name = importable_classes[class_name][1]\n\
          \    454 \n--> 455                 load_method = getattr(class_obj, load_method_name)\n\
          \    456 \n    457                 loading_kwargs = {}\n\nTypeError: getattr():\
          \ attribute name must be string"
        updatedAt: '2022-12-02T18:32:17.522Z'
      numEdits: 0
      reactions: []
    id: 638a44b1002d8574992428ad
    type: comment
  author: sebsamson
  content: "same here too. \n\n<ipython-input-5-9f35f56a3ca3> in <module>\n      3\
    \ \n      4 # make sure you're logged in with `huggingface-cli login`\n----> 5\
    \ pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\"\
    , revision=\"fp16\", torch_dtype=torch.float16)\n\n/usr/local/lib/python3.8/dist-packages/diffusers/pipeline_utils.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n    453  \
    \                       load_method_name = importable_classes[class_name][1]\n\
    \    454 \n--> 455                 load_method = getattr(class_obj, load_method_name)\n\
    \    456 \n    457                 loading_kwargs = {}\n\nTypeError: getattr():\
    \ attribute name must be string"
  created_at: 2022-12-02 18:32:17+00:00
  edited: false
  hidden: false
  id: 638a44b1002d8574992428ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5587e5fade1edb0df0f81e89ece09377.svg
      fullname: Simon B
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phoenire
      type: user
    createdAt: '2022-12-02T19:15:02.000Z'
    data:
      edited: false
      editors:
      - phoenire
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5587e5fade1edb0df0f81e89ece09377.svg
          fullname: Simon B
          isHf: false
          isPro: false
          name: phoenire
          type: user
        html: '<p>I had the issue. I updated the version of the diffusers to 0.8.0
          instead of 0.3.0 and it works now.</p>

          '
        raw: I had the issue. I updated the version of the diffusers to 0.8.0 instead
          of 0.3.0 and it works now.
        updatedAt: '2022-12-02T19:15:02.310Z'
      numEdits: 0
      reactions:
      - count: 26
        reaction: "\U0001F44D"
        users:
        - lionelhappyhead
        - shirgr
        - andrewsagio
        - HikaruBear
        - ponzu
        - SunMoon
        - ricgu8086
        - muggle
        - ayas
        - hargup
        - Chrisantha
        - peiran-yu
        - flora0420
        - rayHam
        - NichoDasca
        - vivekadithya
        - Vermillion-Qi
        - ThinkingLTV
        - Mit1208
        - asyraf
        - HoarfrostRaven
        - lordkroog
        - meteorming
        - JiahPreston
        - keivanipchihagh
        - SaiAnoop
      - count: 13
        reaction: "\u2764\uFE0F"
        users:
        - hargup
        - KamiSM
        - Chrisantha
        - matipina
        - patrickvonplaten
        - flora0420
        - vivekadithya
        - Vermillion-Qi
        - DarkEye45
        - lordkroog
        - JiahPreston
        - SaiAnoop
        - BlastoiseEs
    id: 638a4eb6cebef0d13aa0b1f4
    type: comment
  author: phoenire
  content: I had the issue. I updated the version of the diffusers to 0.8.0 instead
    of 0.3.0 and it works now.
  created_at: 2022-12-02 19:15:02+00:00
  edited: false
  hidden: false
  id: 638a4eb6cebef0d13aa0b1f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0b06e76d81f0a0593a81f261ebd753c2.svg
      fullname: Sebastien Samson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sebsamson
      type: user
    createdAt: '2022-12-02T19:59:52.000Z'
    data:
      edited: false
      editors:
      - sebsamson
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0b06e76d81f0a0593a81f261ebd753c2.svg
          fullname: Sebastien Samson
          isHf: false
          isPro: false
          name: sebsamson
          type: user
        html: '<p>thanks phoenire<br>I was running 0.4.0. I changed it to 0.8.0<br>it
          seems to have changed the error a bit but the issue remains:</p>

          <p>TypeError                                 Traceback (most recent call
          last)<br> in <br>      3<br>      4 # make sure you''re logged in with <code>huggingface-cli
          login</code><br>----&gt; 5 pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4",
          revision="fp16", torch_dtype=torch.float16)</p>

          <p>/usr/local/lib/python3.8/dist-packages/diffusers/pipeline_utils.py in
          from_pretrained(cls, pretrained_model_name_or_path, **kwargs)<br>    453<br>    454             if
          custom_pipeline is not None:<br>--&gt; 455                 allow_patterns
          += [CUSTOM_PIPELINE_FILE_NAME]<br>    456<br>    457             if cls
          != DiffusionPipeline:</p>

          <p>TypeError: getattr(): attribute name must be string</p>

          '
        raw: "thanks phoenire\nI was running 0.4.0. I changed it to 0.8.0\nit seems\
          \ to have changed the error a bit but the issue remains:\n\n\nTypeError\
          \                                 Traceback (most recent call last)\n<ipython-input-10-9f35f56a3ca3>\
          \ in <module>\n      3 \n      4 # make sure you're logged in with `huggingface-cli\
          \ login`\n----> 5 pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\"\
          , revision=\"fp16\", torch_dtype=torch.float16)\n\n/usr/local/lib/python3.8/dist-packages/diffusers/pipeline_utils.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n   \
          \ 453 \n    454             if custom_pipeline is not None:\n--> 455   \
          \              allow_patterns += [CUSTOM_PIPELINE_FILE_NAME]\n    456 \n\
          \    457             if cls != DiffusionPipeline:\n\nTypeError: getattr():\
          \ attribute name must be string"
        updatedAt: '2022-12-02T19:59:52.925Z'
      numEdits: 0
      reactions: []
    id: 638a5938f6a0bc485828c767
    type: comment
  author: sebsamson
  content: "thanks phoenire\nI was running 0.4.0. I changed it to 0.8.0\nit seems\
    \ to have changed the error a bit but the issue remains:\n\n\nTypeError      \
    \                           Traceback (most recent call last)\n<ipython-input-10-9f35f56a3ca3>\
    \ in <module>\n      3 \n      4 # make sure you're logged in with `huggingface-cli\
    \ login`\n----> 5 pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\"\
    , revision=\"fp16\", torch_dtype=torch.float16)\n\n/usr/local/lib/python3.8/dist-packages/diffusers/pipeline_utils.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n    453 \n\
    \    454             if custom_pipeline is not None:\n--> 455                \
    \ allow_patterns += [CUSTOM_PIPELINE_FILE_NAME]\n    456 \n    457           \
    \  if cls != DiffusionPipeline:\n\nTypeError: getattr(): attribute name must be\
    \ string"
  created_at: 2022-12-02 19:59:52+00:00
  edited: false
  hidden: false
  id: 638a5938f6a0bc485828c767
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/16f4d03d87c660fece6a00fc69f4cbed.svg
      fullname: Edmund Elite
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: suit1337
      type: user
    createdAt: '2022-12-02T20:35:19.000Z'
    data:
      edited: false
      editors:
      - suit1337
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/16f4d03d87c660fece6a00fc69f4cbed.svg
          fullname: Edmund Elite
          isHf: false
          isPro: false
          name: suit1337
          type: user
        html: '<p>same here, after updateing diffusers i''m getting the following
          now:</p>

          <p>Traceback (most recent call last):<br>  File "E:\stable_diffusion_amd\diffusers-dml\examples\inference\dml_onnx.py",
          line 206, in <br>    pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4",
          scheduler=lms, use_auth_token=True)<br>  File "E:\stable_diffusion_amd\amd_venv\lib\site-packages\diffusers\pipeline_utils.py",
          line 679, in from_pretrained<br>    model = pipeline_class(**init_kwargs)<br>  File
          "E:\stable_diffusion_amd\diffusers-dml\examples\inference\dml_onnx.py",
          line 28, in <strong>init</strong><br>    scheduler = scheduler.set_format(format)<br>AttributeError:
          ''LMSDiscreteScheduler'' object has no attribute ''set_format''</p>

          <p>running with onyx on Windows/AMD</p>

          '
        raw: "same here, after updateing diffusers i'm getting the following now:\n\
          \nTraceback (most recent call last):\n  File \"E:\\stable_diffusion_amd\\\
          diffusers-dml\\examples\\inference\\dml_onnx.py\", line 206, in <module>\n\
          \    pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\"\
          , scheduler=lms, use_auth_token=True)\n  File \"E:\\stable_diffusion_amd\\\
          amd_venv\\lib\\site-packages\\diffusers\\pipeline_utils.py\", line 679,\
          \ in from_pretrained\n    model = pipeline_class(**init_kwargs)\n  File\
          \ \"E:\\stable_diffusion_amd\\diffusers-dml\\examples\\inference\\dml_onnx.py\"\
          , line 28, in __init__\n    scheduler = scheduler.set_format(format)\nAttributeError:\
          \ 'LMSDiscreteScheduler' object has no attribute 'set_format'\n\nrunning\
          \ with onyx on Windows/AMD"
        updatedAt: '2022-12-02T20:35:19.717Z'
      numEdits: 0
      reactions: []
    id: 638a6187cebef0d13aa141d4
    type: comment
  author: suit1337
  content: "same here, after updateing diffusers i'm getting the following now:\n\n\
    Traceback (most recent call last):\n  File \"E:\\stable_diffusion_amd\\diffusers-dml\\\
    examples\\inference\\dml_onnx.py\", line 206, in <module>\n    pipe = StableDiffusionPipeline.from_pretrained(\"\
    CompVis/stable-diffusion-v1-4\", scheduler=lms, use_auth_token=True)\n  File \"\
    E:\\stable_diffusion_amd\\amd_venv\\lib\\site-packages\\diffusers\\pipeline_utils.py\"\
    , line 679, in from_pretrained\n    model = pipeline_class(**init_kwargs)\n  File\
    \ \"E:\\stable_diffusion_amd\\diffusers-dml\\examples\\inference\\dml_onnx.py\"\
    , line 28, in __init__\n    scheduler = scheduler.set_format(format)\nAttributeError:\
    \ 'LMSDiscreteScheduler' object has no attribute 'set_format'\n\nrunning with\
    \ onyx on Windows/AMD"
  created_at: 2022-12-02 20:35:19+00:00
  edited: false
  hidden: false
  id: 638a6187cebef0d13aa141d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0b06e76d81f0a0593a81f261ebd753c2.svg
      fullname: Sebastien Samson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sebsamson
      type: user
    createdAt: '2022-12-02T20:44:22.000Z'
    data:
      edited: false
      editors:
      - sebsamson
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0b06e76d81f0a0593a81f261ebd753c2.svg
          fullname: Sebastien Samson
          isHf: false
          isPro: false
          name: sebsamson
          type: user
        html: '<p>UPDATE: I reseted the runtime environment and ran it at 0.8.0 and
          it worked</p>

          '
        raw: 'UPDATE: I reseted the runtime environment and ran it at 0.8.0 and it
          worked'
        updatedAt: '2022-12-02T20:44:22.088Z'
      numEdits: 0
      reactions:
      - count: 12
        reaction: "\U0001F44D"
        users:
        - shirgr
        - Malegiraldo22
        - HikaruBear
        - Marinara
        - ricgu8086
        - teresaxyx
        - the4th
        - patrickvonplaten
        - yunshi
        - Ahwar
        - lordkroog
        - Rohan98
    id: 638a63a6c432da48c71241b0
    type: comment
  author: sebsamson
  content: 'UPDATE: I reseted the runtime environment and ran it at 0.8.0 and it worked'
  created_at: 2022-12-02 20:44:22+00:00
  edited: false
  hidden: false
  id: 638a63a6c432da48c71241b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f96aad1a6fb9d0c5d5b6f32273ad85c5.svg
      fullname: Lionel Happy Head
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lionelhappyhead
      type: user
    createdAt: '2022-12-02T20:49:49.000Z'
    data:
      edited: false
      editors:
      - lionelhappyhead
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f96aad1a6fb9d0c5d5b6f32273ad85c5.svg
          fullname: Lionel Happy Head
          isHf: false
          isPro: false
          name: lionelhappyhead
          type: user
        html: '<blockquote>

          <p>I had the issue. I updated the version of the diffusers to 0.8.0 instead
          of 0.3.0 and it works now.</p>

          </blockquote>

          <p>This works for me too!</p>

          '
        raw: '> I had the issue. I updated the version of the diffusers to 0.8.0 instead
          of 0.3.0 and it works now.


          This works for me too!'
        updatedAt: '2022-12-02T20:49:49.568Z'
      numEdits: 0
      reactions: []
    id: 638a64eded88cf97afd3fcb9
    type: comment
  author: lionelhappyhead
  content: '> I had the issue. I updated the version of the diffusers to 0.8.0 instead
    of 0.3.0 and it works now.


    This works for me too!'
  created_at: 2022-12-02 20:49:49+00:00
  edited: false
  hidden: false
  id: 638a64eded88cf97afd3fcb9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d81225563fa7a1de907ac552c9ff1e49.svg
      fullname: nogueira
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mju
      type: user
    createdAt: '2022-12-03T20:06:17.000Z'
    data:
      edited: false
      editors:
      - mju
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d81225563fa7a1de907ac552c9ff1e49.svg
          fullname: nogueira
          isHf: false
          isPro: false
          name: mju
          type: user
        html: '<p>Could you please share a notebook withe the 0.8.0 updated and running
          here<br>I tried to replace it and the code keep having the error<br>ypeError:
          getattr(): attribute name must be string</p>

          '
        raw: 'Could you please share a notebook withe the 0.8.0 updated and running
          here

          I tried to replace it and the code keep having the error

          ypeError: getattr(): attribute name must be string'
        updatedAt: '2022-12-03T20:06:17.455Z'
      numEdits: 0
      reactions: []
    id: 638bac397f62078b2a768c4b
    type: comment
  author: mju
  content: 'Could you please share a notebook withe the 0.8.0 updated and running
    here

    I tried to replace it and the code keep having the error

    ypeError: getattr(): attribute name must be string'
  created_at: 2022-12-03 20:06:17+00:00
  edited: false
  hidden: false
  id: 638bac397f62078b2a768c4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663993186531-noauth.png?w=200&h=200&f=face
      fullname: dean kunter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Electricity
      type: user
    createdAt: '2022-12-03T20:07:38.000Z'
    data:
      edited: false
      editors:
      - Electricity
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663993186531-noauth.png?w=200&h=200&f=face
          fullname: dean kunter
          isHf: false
          isPro: false
          name: Electricity
          type: user
        html: '<p>Same issue; where is 0.8.0 located?</p>

          '
        raw: Same issue; where is 0.8.0 located?
        updatedAt: '2022-12-03T20:07:38.337Z'
      numEdits: 0
      reactions: []
    id: 638bac8ad274cbbad2827510
    type: comment
  author: Electricity
  content: Same issue; where is 0.8.0 located?
  created_at: 2022-12-03 20:07:38+00:00
  edited: false
  hidden: false
  id: 638bac8ad274cbbad2827510
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/033b40bf3b343277c240570ac3c72e34.svg
      fullname: Jo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Marinara
      type: user
    createdAt: '2022-12-03T20:16:32.000Z'
    data:
      edited: false
      editors:
      - Marinara
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/033b40bf3b343277c240570ac3c72e34.svg
          fullname: Jo
          isHf: false
          isPro: false
          name: Marinara
          type: user
        html: '<p>Restart the Runtime and update the version of diffusion to 0.8.0,
          it works!</p>

          '
        raw: Restart the Runtime and update the version of diffusion to 0.8.0, it
          works!
        updatedAt: '2022-12-03T20:16:32.693Z'
      numEdits: 0
      reactions: []
    id: 638baea0b0cc3ae137c5a830
    type: comment
  author: Marinara
  content: Restart the Runtime and update the version of diffusion to 0.8.0, it works!
  created_at: 2022-12-03 20:16:32+00:00
  edited: false
  hidden: false
  id: 638baea0b0cc3ae137c5a830
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670016870632-noauth.jpeg?w=200&h=200&f=face
      fullname: Siddharth Singi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SiddharthSingi
      type: user
    createdAt: '2022-12-03T20:20:55.000Z'
    data:
      edited: false
      editors:
      - SiddharthSingi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670016870632-noauth.jpeg?w=200&h=200&f=face
          fullname: Siddharth Singi
          isHf: false
          isPro: false
          name: SiddharthSingi
          type: user
        html: '<p>I am not getting that error after running:<br>!pip install --upgrade
          diffusers transformers scipy</p>

          '
        raw: 'I am not getting that error after running:

          !pip install --upgrade diffusers transformers scipy'
        updatedAt: '2022-12-03T20:20:55.884Z'
      numEdits: 0
      reactions:
      - count: 9
        reaction: "\u2764\uFE0F"
        users:
        - joddy
        - LittleEiffel
        - alex-1
        - WhoAmIBro
        - CharanMC
        - yky31
        - elFesu
        - Blue142
        - PalionTech
    id: 638bafa7bbe083dfbbad2556
    type: comment
  author: SiddharthSingi
  content: 'I am not getting that error after running:

    !pip install --upgrade diffusers transformers scipy'
  created_at: 2022-12-03 20:20:55+00:00
  edited: false
  hidden: false
  id: 638bafa7bbe083dfbbad2556
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663993186531-noauth.png?w=200&h=200&f=face
      fullname: dean kunter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Electricity
      type: user
    createdAt: '2022-12-03T20:25:33.000Z'
    data:
      edited: true
      editors:
      - Electricity
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663993186531-noauth.png?w=200&h=200&f=face
          fullname: dean kunter
          isHf: false
          isPro: false
          name: Electricity
          type: user
        html: '<p><strong><strong>READ</strong></strong>    SETUP of StableDiffusionPipeline<br>in
          the "SETUP <a href="/CompVis/stable-diffusion-v1-4/discussions/2">#2</a>"
          just under the name of the google GPU you connect with you will see:<br>
          !pip install diffusers==0.4.0 highlight the "4" and change it to -  !pip
          install diffusers==0.8.0</p>

          <p>edit: you probably will need to disconnect and rerun</p>

          '
        raw: "****READ****    SETUP of StableDiffusionPipeline\nin the \"SETUP #2\"\
          \ just under the name of the google GPU you connect with you will see:\n\
          \ !pip install diffusers==0.4.0 highlight the \"4\" and change it to - \
          \ !pip install diffusers==0.8.0\n\n\nedit: you probably will need to disconnect\
          \ and rerun"
        updatedAt: '2022-12-03T20:26:24.954Z'
      numEdits: 1
      reactions: []
    id: 638bb0bddaa4cd1d6b41a8ef
    type: comment
  author: Electricity
  content: "****READ****    SETUP of StableDiffusionPipeline\nin the \"SETUP #2\"\
    \ just under the name of the google GPU you connect with you will see:\n !pip\
    \ install diffusers==0.4.0 highlight the \"4\" and change it to -  !pip install\
    \ diffusers==0.8.0\n\n\nedit: you probably will need to disconnect and rerun"
  created_at: 2022-12-03 20:25:33+00:00
  edited: true
  hidden: false
  id: 638bb0bddaa4cd1d6b41a8ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2657f32da14ee52ccbe7793af3511f2a.svg
      fullname: Fuwei Zhao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fyviezhao
      type: user
    createdAt: '2022-12-06T03:10:08.000Z'
    data:
      edited: false
      editors:
      - fyviezhao
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2657f32da14ee52ccbe7793af3511f2a.svg
          fullname: Fuwei Zhao
          isHf: false
          isPro: false
          name: fyviezhao
          type: user
        html: '<p>This  <code>getattr()</code> error is caused by the latest update
          on the transformers package (4.25.1/4.25.0: Dec.2, 2022). To fix it, simply
          downgrade the transformers package to an earlier version, e.g., <code>pip
          install transformers==4.24.0</code>, then everything goes fine.</p>

          '
        raw: 'This  `getattr()` error is caused by the latest update on the transformers
          package (4.25.1/4.25.0: Dec.2, 2022). To fix it, simply downgrade the transformers
          package to an earlier version, e.g., `pip install transformers==4.24.0`,
          then everything goes fine.'
        updatedAt: '2022-12-06T03:10:08.569Z'
      numEdits: 0
      reactions:
      - count: 10
        reaction: "\u2764\uFE0F"
        users:
        - joddy
        - dit-to
        - Spiderfffun
        - hadleighbird
        - tuniao
        - DanR
        - AlianaGe
        - mohamed1ai
        - asyraf
        - tiiktak
      - count: 7
        reaction: "\U0001F44D"
        users:
        - Spiderfffun
        - tuniao
        - bibona
        - DanR
        - mohamed1ai
        - jaechul1
        - ABC321123
      - count: 4
        reaction: "\U0001F917"
        users:
        - Spiderfffun
        - tuniao
        - DanR
        - mohamed1ai
    id: 638eb290e160c226bac9d428
    type: comment
  author: fyviezhao
  content: 'This  `getattr()` error is caused by the latest update on the transformers
    package (4.25.1/4.25.0: Dec.2, 2022). To fix it, simply downgrade the transformers
    package to an earlier version, e.g., `pip install transformers==4.24.0`, then
    everything goes fine.'
  created_at: 2022-12-06 03:10:08+00:00
  edited: false
  hidden: false
  id: 638eb290e160c226bac9d428
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69bb35c63f538a0125c6c5536b4a3200.svg
      fullname: Harsh Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hargup
      type: user
    createdAt: '2022-12-06T22:55:51.000Z'
    data:
      edited: true
      editors:
      - hargup
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69bb35c63f538a0125c6c5536b4a3200.svg
          fullname: Harsh Gupta
          isHf: false
          isPro: false
          name: hargup
          type: user
        html: '<blockquote>

          <p>I had the issue. I updated the version of the diffusers to 0.8.0 instead
          of 0.3.0 and it works now.</p>

          </blockquote>

          <p>This worked for me as well</p>

          '
        raw: '> I had the issue. I updated the version of the diffusers to 0.8.0 instead
          of 0.3.0 and it works now.


          This worked for me as well'
        updatedAt: '2022-12-06T22:56:02.792Z'
      numEdits: 1
      reactions: []
    id: 638fc8773abfb507a38e1fcb
    type: comment
  author: hargup
  content: '> I had the issue. I updated the version of the diffusers to 0.8.0 instead
    of 0.3.0 and it works now.


    This worked for me as well'
  created_at: 2022-12-06 22:55:51+00:00
  edited: true
  hidden: false
  id: 638fc8773abfb507a38e1fcb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668115790990-noauth.jpeg?w=200&h=200&f=face
      fullname: yassine ben arbia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KamiSM
      type: user
    createdAt: '2022-12-06T23:03:33.000Z'
    data:
      edited: false
      editors:
      - KamiSM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668115790990-noauth.jpeg?w=200&h=200&f=face
          fullname: yassine ben arbia
          isHf: false
          isPro: false
          name: KamiSM
          type: user
        html: "<blockquote>\n<p>I had the issue. I updated the version of the diffusers\
          \ to 0.8.0 instead of 0.3.0 and it works now.</p>\n</blockquote>\n<p>ur\
          \ a day saver <span data-props=\"{&quot;user&quot;:&quot;hargup&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/hargup\"\
          >@<span class=\"underline\">hargup</span></a></span>\n\n\t</span></span>,\
          \ appreciated : )</p>\n"
        raw: '> I had the issue. I updated the version of the diffusers to 0.8.0 instead
          of 0.3.0 and it works now.


          ur a day saver @hargup, appreciated : )'
        updatedAt: '2022-12-06T23:03:33.059Z'
      numEdits: 0
      reactions: []
    id: 638fca452380ffd99cb140f2
    type: comment
  author: KamiSM
  content: '> I had the issue. I updated the version of the diffusers to 0.8.0 instead
    of 0.3.0 and it works now.


    ur a day saver @hargup, appreciated : )'
  created_at: 2022-12-06 23:03:33+00:00
  edited: false
  hidden: false
  id: 638fca452380ffd99cb140f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f49732501ad014ac2e5266bc4837e073.svg
      fullname: Deniel K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dennoquellovero
      type: user
    createdAt: '2022-12-07T02:04:29.000Z'
    data:
      edited: false
      editors:
      - Dennoquellovero
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f49732501ad014ac2e5266bc4837e073.svg
          fullname: Deniel K
          isHf: false
          isPro: false
          name: Dennoquellovero
          type: user
        html: '<blockquote>

          <p>I am not getting that error after running:<br>!pip install --upgrade
          diffusers transformers scipy</p>

          </blockquote>

          <p>this worked</p>

          '
        raw: '> I am not getting that error after running:

          > !pip install --upgrade diffusers transformers scipy


          this worked'
        updatedAt: '2022-12-07T02:04:29.262Z'
      numEdits: 0
      reactions: []
    id: 638ff4ad888447611c225f4f
    type: comment
  author: Dennoquellovero
  content: '> I am not getting that error after running:

    > !pip install --upgrade diffusers transformers scipy


    this worked'
  created_at: 2022-12-07 02:04:29+00:00
  edited: false
  hidden: false
  id: 638ff4ad888447611c225f4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663993186531-noauth.png?w=200&h=200&f=face
      fullname: dean kunter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Electricity
      type: user
    createdAt: '2022-12-07T02:09:42.000Z'
    data:
      edited: false
      editors:
      - Electricity
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663993186531-noauth.png?w=200&h=200&f=face
          fullname: dean kunter
          isHf: false
          isPro: false
          name: Electricity
          type: user
        html: '<blockquote>

          <p>This  <code>getattr()</code> error is caused by the latest update on
          the transformers package (4.25.1/4.25.0: Dec.2, 2022). To fix it, simply
          downgrade the transformers package to an earlier version, e.g., <code>pip
          install transformers==4.24.0</code>, then everything goes fine.</p>

          </blockquote>

          <p>when a guide provides a statement like: "with a simple _________" you
          know life as you know is forever changed.</p>

          '
        raw: '> This  `getattr()` error is caused by the latest update on the transformers
          package (4.25.1/4.25.0: Dec.2, 2022). To fix it, simply downgrade the transformers
          package to an earlier version, e.g., `pip install transformers==4.24.0`,
          then everything goes fine.


          when a guide provides a statement like: "with a simple _________" you know
          life as you know is forever changed.'
        updatedAt: '2022-12-07T02:09:42.580Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Jotarx
        - tuniao
        - dxli
    id: 638ff5e6888447611c227c64
    type: comment
  author: Electricity
  content: '> This  `getattr()` error is caused by the latest update on the transformers
    package (4.25.1/4.25.0: Dec.2, 2022). To fix it, simply downgrade the transformers
    package to an earlier version, e.g., `pip install transformers==4.24.0`, then
    everything goes fine.


    when a guide provides a statement like: "with a simple _________" you know life
    as you know is forever changed.'
  created_at: 2022-12-07 02:09:42+00:00
  edited: false
  hidden: false
  id: 638ff5e6888447611c227c64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d4a16b630fde8637b9af1cdbd57eaa61.svg
      fullname: extermos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: extermos
      type: user
    createdAt: '2022-12-08T00:03:35.000Z'
    data:
      edited: false
      editors:
      - extermos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d4a16b630fde8637b9af1cdbd57eaa61.svg
          fullname: extermos
          isHf: false
          isPro: false
          name: extermos
          type: user
        html: '<p>Now 0.8.0 is having the same type error</p>

          '
        raw: Now 0.8.0 is having the same type error
        updatedAt: '2022-12-08T00:03:35.410Z'
      numEdits: 0
      reactions: []
    id: 639129d70cf6b11c487b2487
    type: comment
  author: extermos
  content: Now 0.8.0 is having the same type error
  created_at: 2022-12-08 00:03:35+00:00
  edited: false
  hidden: false
  id: 639129d70cf6b11c487b2487
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d21dcf407c642eaf481f19820ef3c71.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: G19934
      type: user
    createdAt: '2022-12-09T05:23:19.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/3d21dcf407c642eaf481f19820ef3c71.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: G19934
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-12-09T05:26:27.213Z'
      numEdits: 0
      reactions: []
    id: 6392c64738d326fa71945ba0
    type: comment
  author: G19934
  content: This comment has been hidden
  created_at: 2022-12-09 05:23:19+00:00
  edited: true
  hidden: true
  id: 6392c64738d326fa71945ba0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d4a16b630fde8637b9af1cdbd57eaa61.svg
      fullname: extermos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: extermos
      type: user
    createdAt: '2022-12-09T06:00:28.000Z'
    data:
      edited: false
      editors:
      - extermos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d4a16b630fde8637b9af1cdbd57eaa61.svg
          fullname: extermos
          isHf: false
          isPro: false
          name: extermos
          type: user
        html: '<p>0.9.0 works</p>

          '
        raw: 0.9.0 works
        updatedAt: '2022-12-09T06:00:28.185Z'
      numEdits: 0
      reactions: []
    id: 6392cefca2b4a8e081bbd4b9
    type: comment
  author: extermos
  content: 0.9.0 works
  created_at: 2022-12-09 06:00:28+00:00
  edited: false
  hidden: false
  id: 6392cefca2b4a8e081bbd4b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/53fe5d5068418b55c0f4ad20bce7ec35.svg
      fullname: evan pan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wdpm
      type: user
    createdAt: '2022-12-09T12:33:48.000Z'
    data:
      edited: false
      editors:
      - wdpm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/53fe5d5068418b55c0f4ad20bce7ec35.svg
          fullname: evan pan
          isHf: false
          isPro: false
          name: wdpm
          type: user
        html: '<p>0.9.0 works!</p>

          '
        raw: 0.9.0 works!
        updatedAt: '2022-12-09T12:33:48.684Z'
      numEdits: 0
      reactions: []
    id: 63932b2ccceea148b5a1b044
    type: comment
  author: wdpm
  content: 0.9.0 works!
  created_at: 2022-12-09 12:33:48+00:00
  edited: false
  hidden: false
  id: 63932b2ccceea148b5a1b044
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24e7db3c966bb450ab3b94ac1a1ad39c.svg
      fullname: day
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lianprofile
      type: user
    createdAt: '2022-12-09T13:13:11.000Z'
    data:
      edited: false
      editors:
      - Lianprofile
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24e7db3c966bb450ab3b94ac1a1ad39c.svg
          fullname: day
          isHf: false
          isPro: false
          name: Lianprofile
          type: user
        html: '<p>I reseted the runtime environment  and 0.9.0 works ~</p>

          '
        raw: I reseted the runtime environment  and 0.9.0 works ~
        updatedAt: '2022-12-09T13:13:11.154Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pcuenq
    id: 63933467f8b4767ae646cf79
    type: comment
  author: Lianprofile
  content: I reseted the runtime environment  and 0.9.0 works ~
  created_at: 2022-12-09 13:13:11+00:00
  edited: false
  hidden: false
  id: 63933467f8b4767ae646cf79
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d4a16b630fde8637b9af1cdbd57eaa61.svg
      fullname: extermos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: extermos
      type: user
    createdAt: '2022-12-12T02:47:18.000Z'
    data:
      edited: false
      editors:
      - extermos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d4a16b630fde8637b9af1cdbd57eaa61.svg
          fullname: extermos
          isHf: false
          isPro: false
          name: extermos
          type: user
        html: '<p>Now it doesn''t require me to log in..... which is odd.  How do
          I make sure I am logged in with my token so that there is a record that
          the creations I make are a result of my prompt engineering?</p>

          '
        raw: Now it doesn't require me to log in..... which is odd.  How do I make
          sure I am logged in with my token so that there is a record that the creations
          I make are a result of my prompt engineering?
        updatedAt: '2022-12-12T02:47:18.868Z'
      numEdits: 0
      reactions: []
    id: 639696366313f729ae7e2b0e
    type: comment
  author: extermos
  content: Now it doesn't require me to log in..... which is odd.  How do I make sure
    I am logged in with my token so that there is a record that the creations I make
    are a result of my prompt engineering?
  created_at: 2022-12-12 02:47:18+00:00
  edited: false
  hidden: false
  id: 639696366313f729ae7e2b0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/40807191acb8ae05fba8fb3b98350e16.svg
      fullname: Fiojja R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fiojja
      type: user
    createdAt: '2022-12-15T22:27:17.000Z'
    data:
      edited: false
      editors:
      - fiojja
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/40807191acb8ae05fba8fb3b98350e16.svg
          fullname: Fiojja R
          isHf: false
          isPro: false
          name: fiojja
          type: user
        html: '<p>It''s not working for me either--setting the diffuser to 0.7.0,
          0.8.0, or 0.9.0 is unsuccessful; setting the transformers to an earlier
          version is unsuccessful. Running <code>!pip install --upgrade diffusers
          transformers scipy</code> results in both of those being replaced with later
          versions, but no positive effect.</p>

          <p>Traceback (most recent call last):<br>  File "C:\Users\Fiojja\stable-diffusion-amd\diffusers\examples\inference\dml_onnx.py",
          line 206, in <br>    pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4",
          scheduler=lms, use_auth_token=True)<br>  File "C:\Python310\lib\site-packages\diffusers\pipeline_utils.py",
          line 730, in from_pretrained<br>    model = pipeline_class(**init_kwargs)<br>  File
          "C:\Users\Fiojja\stable-diffusion-amd\diffusers\examples\inference\dml_onnx.py",
          line 28, in <strong>init</strong><br>    scheduler = scheduler.set_format(format)<br>AttributeError:
          ''LMSDiscreteScheduler'' object has no attribute ''set_format''</p>

          '
        raw: "It's not working for me either--setting the diffuser to 0.7.0, 0.8.0,\
          \ or 0.9.0 is unsuccessful; setting the transformers to an earlier version\
          \ is unsuccessful. Running `!pip install --upgrade diffusers transformers\
          \ scipy` results in both of those being replaced with later versions, but\
          \ no positive effect.\n\nTraceback (most recent call last):\n  File \"C:\\\
          Users\\Fiojja\\stable-diffusion-amd\\diffusers\\examples\\inference\\dml_onnx.py\"\
          , line 206, in <module>\n    pipe = StableDiffusionPipeline.from_pretrained(\"\
          CompVis/stable-diffusion-v1-4\", scheduler=lms, use_auth_token=True)\n \
          \ File \"C:\\Python310\\lib\\site-packages\\diffusers\\pipeline_utils.py\"\
          , line 730, in from_pretrained\n    model = pipeline_class(**init_kwargs)\n\
          \  File \"C:\\Users\\Fiojja\\stable-diffusion-amd\\diffusers\\examples\\\
          inference\\dml_onnx.py\", line 28, in __init__\n    scheduler = scheduler.set_format(format)\n\
          AttributeError: 'LMSDiscreteScheduler' object has no attribute 'set_format'"
        updatedAt: '2022-12-15T22:27:17.133Z'
      numEdits: 0
      reactions: []
    id: 639b9f4559473c6ae026f70e
    type: comment
  author: fiojja
  content: "It's not working for me either--setting the diffuser to 0.7.0, 0.8.0,\
    \ or 0.9.0 is unsuccessful; setting the transformers to an earlier version is\
    \ unsuccessful. Running `!pip install --upgrade diffusers transformers scipy`\
    \ results in both of those being replaced with later versions, but no positive\
    \ effect.\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Fiojja\\\
    stable-diffusion-amd\\diffusers\\examples\\inference\\dml_onnx.py\", line 206,\
    \ in <module>\n    pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\"\
    , scheduler=lms, use_auth_token=True)\n  File \"C:\\Python310\\lib\\site-packages\\\
    diffusers\\pipeline_utils.py\", line 730, in from_pretrained\n    model = pipeline_class(**init_kwargs)\n\
    \  File \"C:\\Users\\Fiojja\\stable-diffusion-amd\\diffusers\\examples\\inference\\\
    dml_onnx.py\", line 28, in __init__\n    scheduler = scheduler.set_format(format)\n\
    AttributeError: 'LMSDiscreteScheduler' object has no attribute 'set_format'"
  created_at: 2022-12-15 22:27:17+00:00
  edited: false
  hidden: false
  id: 639b9f4559473c6ae026f70e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d4a16b630fde8637b9af1cdbd57eaa61.svg
      fullname: extermos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: extermos
      type: user
    createdAt: '2022-12-16T02:50:15.000Z'
    data:
      edited: false
      editors:
      - extermos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d4a16b630fde8637b9af1cdbd57eaa61.svg
          fullname: extermos
          isHf: false
          isPro: false
          name: extermos
          type: user
        html: '<p>The newest is 0.10.0</p>

          <p>0.9.0 stopped working for me as well, but 0.10.0 works</p>

          '
        raw: 'The newest is 0.10.0


          0.9.0 stopped working for me as well, but 0.10.0 works'
        updatedAt: '2022-12-16T02:50:15.108Z'
      numEdits: 0
      reactions: []
    id: 639bdce7f44c46d8af5e7841
    type: comment
  author: extermos
  content: 'The newest is 0.10.0


    0.9.0 stopped working for me as well, but 0.10.0 works'
  created_at: 2022-12-16 02:50:15+00:00
  edited: false
  hidden: false
  id: 639bdce7f44c46d8af5e7841
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c614bc978f55e3058d400fa038b0b48.svg
      fullname: Spiderfffun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Spiderfffun
      type: user
    createdAt: '2022-12-17T12:13:52.000Z'
    data:
      edited: false
      editors:
      - Spiderfffun
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c614bc978f55e3058d400fa038b0b48.svg
          fullname: Spiderfffun
          isHf: false
          isPro: false
          name: Spiderfffun
          type: user
        html: '<p>The newest transformers has a bug as fyviezhao said, so download
          transformers==4.24.0</p>

          '
        raw: The newest transformers has a bug as fyviezhao said, so download transformers==4.24.0
        updatedAt: '2022-12-17T12:13:52.836Z'
      numEdits: 0
      reactions: []
    id: 639db2807145123e0d47a072
    type: comment
  author: Spiderfffun
  content: The newest transformers has a bug as fyviezhao said, so download transformers==4.24.0
  created_at: 2022-12-17 12:13:52+00:00
  edited: false
  hidden: false
  id: 639db2807145123e0d47a072
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41f389818b3eab73c61e10fe7d554f98.svg
      fullname: Julian Ramirez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jotarx
      type: user
    createdAt: '2022-12-22T16:22:33.000Z'
    data:
      edited: false
      editors:
      - Jotarx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41f389818b3eab73c61e10fe7d554f98.svg
          fullname: Julian Ramirez
          isHf: false
          isPro: false
          name: Jotarx
          type: user
        html: '<blockquote>

          <blockquote>

          <p>This  <code>getattr()</code> error is caused by the latest update on
          the transformers package (4.25.1/4.25.0: Dec.2, 2022). To fix it, simply
          downgrade the transformers package to an earlier version, e.g., <code>pip
          install transformers==4.24.0</code>, then everything goes fine.</p>

          </blockquote>

          <p>when a guide provides a statement like: "with a simple _________" you
          know life as you know is forever changed.</p>

          </blockquote>

          <p>Perfect</p>

          '
        raw: "> > This  `getattr()` error is caused by the latest update on the transformers\
          \ package (4.25.1/4.25.0: Dec.2, 2022). To fix it, simply downgrade the\
          \ transformers package to an earlier version, e.g., `pip install transformers==4.24.0`,\
          \ then everything goes fine.\n> \n> when a guide provides a statement like:\
          \ \"with a simple _________\" you know life as you know is forever changed.\n\
          \nPerfect"
        updatedAt: '2022-12-22T16:22:33.120Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - vortexwing
    id: 63a4844927f1f64ed725a83f
    type: comment
  author: Jotarx
  content: "> > This  `getattr()` error is caused by the latest update on the transformers\
    \ package (4.25.1/4.25.0: Dec.2, 2022). To fix it, simply downgrade the transformers\
    \ package to an earlier version, e.g., `pip install transformers==4.24.0`, then\
    \ everything goes fine.\n> \n> when a guide provides a statement like: \"with\
    \ a simple _________\" you know life as you know is forever changed.\n\nPerfect"
  created_at: 2022-12-22 16:22:33+00:00
  edited: false
  hidden: false
  id: 63a4844927f1f64ed725a83f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/083efdec5108365ece2456c41e1a1038.svg
      fullname: Boos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PeterB123A
      type: user
    createdAt: '2023-02-01T12:53:21.000Z'
    data:
      edited: false
      editors:
      - PeterB123A
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/083efdec5108365ece2456c41e1a1038.svg
          fullname: Boos
          isHf: false
          isPro: false
          name: PeterB123A
          type: user
        html: '<p>This worked for me i think its the order of installation<br>put
          transformers last, in other orders i had the error too</p>

          <p>!pip install diffusers==0.3.0<br>!pip install scipy ftfy<br>!pip install
          transformers==4.24.0<br>!pip install "ipywidgets&gt;=7,&lt;8"</p>

          '
        raw: "This worked for me i think its the order of installation \nput transformers\
          \ last, in other orders i had the error too\n\n!pip install diffusers==0.3.0\n\
          !pip install scipy ftfy\n!pip install transformers==4.24.0\n!pip install\
          \ \"ipywidgets>=7,<8\""
        updatedAt: '2023-02-01T12:53:21.577Z'
      numEdits: 0
      reactions: []
    id: 63da60c1c1d9ac51bb591c56
    type: comment
  author: PeterB123A
  content: "This worked for me i think its the order of installation \nput transformers\
    \ last, in other orders i had the error too\n\n!pip install diffusers==0.3.0\n\
    !pip install scipy ftfy\n!pip install transformers==4.24.0\n!pip install \"ipywidgets>=7,<8\""
  created_at: 2023-02-01 12:53:21+00:00
  edited: false
  hidden: false
  id: 63da60c1c1d9ac51bb591c56
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ccda413e297be2f4ca219a17afdd376a.svg
      fullname: zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: efan95
      type: user
    createdAt: '2023-03-19T07:27:54.000Z'
    data:
      edited: false
      editors:
      - efan95
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ccda413e297be2f4ca219a17afdd376a.svg
          fullname: zhang
          isHf: false
          isPro: false
          name: efan95
          type: user
        html: '<blockquote>

          <p>This worked for me i think its the order of installation<br>put transformers
          last, in other orders i had the error too</p>

          <p>!pip install diffusers==0.3.0<br>!pip install scipy ftfy<br>!pip install
          transformers==4.24.0<br>!pip install "ipywidgets&gt;=7,&lt;8"</p>

          </blockquote>

          <p>Thanks , It works</p>

          '
        raw: "> This worked for me i think its the order of installation \n> put transformers\
          \ last, in other orders i had the error too\n> \n> !pip install diffusers==0.3.0\n\
          > !pip install scipy ftfy\n> !pip install transformers==4.24.0\n> !pip install\
          \ \"ipywidgets>=7,<8\"\n\nThanks , It works"
        updatedAt: '2023-03-19T07:27:54.923Z'
      numEdits: 0
      reactions: []
    id: 6416b97addf61f50be502c54
    type: comment
  author: efan95
  content: "> This worked for me i think its the order of installation \n> put transformers\
    \ last, in other orders i had the error too\n> \n> !pip install diffusers==0.3.0\n\
    > !pip install scipy ftfy\n> !pip install transformers==4.24.0\n> !pip install\
    \ \"ipywidgets>=7,<8\"\n\nThanks , It works"
  created_at: 2023-03-19 06:27:54+00:00
  edited: false
  hidden: false
  id: 6416b97addf61f50be502c54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04b6001b0cdc51dba8469c14f486b9b7.svg
      fullname: nope
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Blue142
      type: user
    createdAt: '2023-03-21T07:47:22.000Z'
    data:
      edited: true
      editors:
      - Blue142
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04b6001b0cdc51dba8469c14f486b9b7.svg
          fullname: nope
          isHf: false
          isPro: false
          name: Blue142
          type: user
        html: "<blockquote>\n<p>I am not getting that error after running:<br>!pip\
          \ install --upgrade diffusers transformers scipy</p>\n</blockquote>\n<p>this\
          \ almost worked then this popped up:<br>what popped up before that was it\
          \ crying that i had to install \"accelerate\" so I did then this popped\
          \ up I don't want to reinstall again ;\\</p>\n<p>(amd_venv) e:\\ai\\diffusers-dml\\\
          examples\\inference&gt;python dml_onnx.py<br>Fetching 16 files: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00&lt;?,\
          \ ?it/s]<br><code>text_config_dict</code> is provided which will be used\
          \ to initialize <code>CLIPTextConfig</code>. The value <code>text_config[\"\
          id2label\"]</code> will be overriden.<br>e:\\ai\\amd_venv\\lib\\site-packages\\\
          transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning:\
          \ The class CLIPFeatureExtractor is deprecated and will be removed in version\
          \ 5 of Transformers. Please use CLIPImageProcessor instead.<br>  warnings.warn(<br>Traceback\
          \ (most recent call last):<br>  File \"e:\\ai\\diffusers-dml\\examples\\\
          inference\\dml_onnx.py\", line 206, in <br>    pipe = StableDiffusionPipeline.from_pretrained(\"\
          CompVis/stable-diffusion-v1-4\", scheduler=lms, use_auth_token=True)<br>\
          \  File \"e:\\ai\\amd_venv\\lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py\"\
          , line 965, in from_pretrained<br>    model = pipeline_class(**init_kwargs)<br>\
          \  File \"e:\\ai\\diffusers-dml\\examples\\inference\\dml_onnx.py\", line\
          \ 28, in <strong>init</strong><br>    scheduler = scheduler.set_format(format)<br>AttributeError:\
          \ 'LMSDiscreteScheduler' object has no attribute 'set_format'</p>\n<p>small\
          \ edit: I did a full reinstall and tried \"pip install --upgrade diffusers\
          \ transformers scipy ftfy\" (i added ftfy after trying it once) but now\
          \ im getting this:</p>\n<p>(amd_venv) E:\\ai\\diffusers-dml\\diffusers-dml\\\
          examples\\inference&gt;python save_onnx.py<br>Some weights of the model\
          \ checkpoint at openai/clip-vit-large-patch14 were not used when initializing\
          \ CLIPTextModel: ['vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.post_layernorm.weight',\
          \ 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias',\
          \ 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias',\
          \ 'logit_scale', 'vision_model.encoder.layers.6.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight',\
          \ 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.embeddings.position_ids',\
          \ 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight',\
          \ 'text_projection.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.12.mlp.fc2.weight', 'visual_projection.weight',\
          \ 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight',\
          \ 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.post_layernorm.bias',\
          \ 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight',\
          \ 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.16.layer_norm2.weight']</p>\n<ul>\n<li>This\
          \ IS expected if you are initializing CLIPTextModel from the checkpoint\
          \ of a model trained on another task or with another architecture (e.g.\
          \ initializing a BertForSequenceClassification model from a BertForPreTraining\
          \ model).</li>\n<li>This IS NOT expected if you are initializing CLIPTextModel\
          \ from the checkpoint of a model that you expect to be exactly identical\
          \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).<br>Fetching 16 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 16/16 [00:00&lt;?, ?it/s]<br>Cannot initialize\
          \ model with low cpu memory usage because <code>accelerate</code> was not\
          \ found in the environment. Defaulting to <code>low_cpu_mem_usage=False</code>.\
          \ It is strongly recommended to install <code>accelerate</code> for faster\
          \ and less memory-intense model loading. You can do so with:</li>\n</ul>\n\
          <pre><code>pip install accelerate\n</code></pre>\n<p>.<br><code>text_config_dict</code>\
          \ is provided which will be used to initialize <code>CLIPTextConfig</code>.\
          \ The value <code>text_config[\"id2label\"]</code> will be overriden.<br>E:\\\
          ai\\amd_venv\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28:\
          \ FutureWarning: The class CLIPFeatureExtractor is deprecated and will be\
          \ removed in version 5 of Transformers. Please use CLIPImageProcessor instead.<br>\
          \  warnings.warn(<br>E:\\ai\\amd_venv\\lib\\site-packages\\diffusers\\models\\\
          unet_2d_condition.py:526: TracerWarning: Converting a tensor to a Python\
          \ boolean might cause the trace to be incorrect. We can't record the data\
          \ flow of Python values, so this value will be treated as a constant in\
          \ the future. This means that the trace might not generalize to other inputs!<br>\
          \  if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):<br>E:\\\
          ai\\amd_venv\\lib\\site-packages\\diffusers\\models\\resnet.py:185: TracerWarning:\
          \ Converting a tensor to a Python boolean might cause the trace to be incorrect.\
          \ We can't record the data flow of Python values, so this value will be\
          \ treated as a constant in the future. This means that the trace might not\
          \ generalize to other inputs!<br>  assert hidden_states.shape[1] == self.channels<br>E:\\\
          ai\\amd_venv\\lib\\site-packages\\diffusers\\models\\resnet.py:190: TracerWarning:\
          \ Converting a tensor to a Python boolean might cause the trace to be incorrect.\
          \ We can't record the data flow of Python values, so this value will be\
          \ treated as a constant in the future. This means that the trace might not\
          \ generalize to other inputs!<br>  assert hidden_states.shape[1] == self.channels<br>E:\\\
          ai\\amd_venv\\lib\\site-packages\\diffusers\\models\\resnet.py:112: TracerWarning:\
          \ Converting a tensor to a Python boolean might cause the trace to be incorrect.\
          \ We can't record the data flow of Python values, so this value will be\
          \ treated as a constant in the future. This means that the trace might not\
          \ generalize to other inputs!<br>  assert hidden_states.shape[1] == self.channels<br>E:\\\
          ai\\amd_venv\\lib\\site-packages\\diffusers\\models\\resnet.py:125: TracerWarning:\
          \ Converting a tensor to a Python boolean might cause the trace to be incorrect.\
          \ We can't record the data flow of Python values, so this value will be\
          \ treated as a constant in the future. This means that the trace might not\
          \ generalize to other inputs!<br>  if hidden_states.shape[0] &gt;= 64:<br>Traceback\
          \ (most recent call last):<br>  File \"E:\\ai\\diffusers-dml\\diffusers-dml\\\
          examples\\inference\\save_onnx.py\", line 66, in <br>    convert_to_onnx(pipe.unet,\
          \ pipe.vae.post_quant_conv, pipe.vae.decoder, text_encoder, height=512,\
          \ width=512)<br>  File \"E:\\ai\\diffusers-dml\\diffusers-dml\\examples\\\
          inference\\save_onnx.py\", line 41, in convert_to_onnx<br>    traced_model\
          \ = torch.jit.trace(unet, check_inputs[0], check_inputs=[check_inputs[1]],\
          \ strict=True)<br>  File \"E:\\ai\\amd_venv\\lib\\site-packages\\torch\\\
          jit_trace.py\", line 794, in trace<br>    return trace_module(<br>  File\
          \ \"E:\\ai\\amd_venv\\lib\\site-packages\\torch\\jit_trace.py\", line 1056,\
          \ in trace_module<br>    module._c._create_method_from_trace(<br>RuntimeError:\
          \ Encountering a dict at the output of the tracer might cause the trace\
          \ to be incorrect, this is only valid if the container structure does not\
          \ change based on the module's inputs. Consider using a constant container\
          \ instead (e.g. for <code>list</code>, use a <code>tuple</code> instead.\
          \ for <code>dict</code>, use a <code>NamedTuple</code> instead). If you\
          \ absolutely need this and know the side effects, pass strict=False to trace()\
          \ to allow this behavior.</p>\n<p>I'm sorry if this is long but most of\
          \ it can be ignored as far as  I know when it talks about \"some weights\
          \ were not used\"</p>\n"
        raw: "> I am not getting that error after running:\n> !pip install --upgrade\
          \ diffusers transformers scipy\n\nthis almost worked then this popped up:\n\
          what popped up before that was it crying that i had to install \"accelerate\"\
          \ so I did then this popped up I don't want to reinstall again ;\\\n\n(amd_venv)\
          \ e:\\ai\\diffusers-dml\\examples\\inference>python dml_onnx.py\nFetching\
          \ 16 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 16/16 [00:00<?, ?it/s]\n`text_config_dict` is provided which will\
          \ be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"\
          ]` will be overriden.\ne:\\ai\\amd_venv\\lib\\site-packages\\transformers\\\
          models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor\
          \ is deprecated and will be removed in version 5 of Transformers. Please\
          \ use CLIPImageProcessor instead.\n  warnings.warn(\nTraceback (most recent\
          \ call last):\n  File \"e:\\ai\\diffusers-dml\\examples\\inference\\dml_onnx.py\"\
          , line 206, in <module>\n    pipe = StableDiffusionPipeline.from_pretrained(\"\
          CompVis/stable-diffusion-v1-4\", scheduler=lms, use_auth_token=True)\n \
          \ File \"e:\\ai\\amd_venv\\lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py\"\
          , line 965, in from_pretrained\n    model = pipeline_class(**init_kwargs)\n\
          \  File \"e:\\ai\\diffusers-dml\\examples\\inference\\dml_onnx.py\", line\
          \ 28, in __init__\n    scheduler = scheduler.set_format(format)\nAttributeError:\
          \ 'LMSDiscreteScheduler' object has no attribute 'set_format'\n\nsmall edit:\
          \ I did a full reinstall and tried \"pip install --upgrade diffusers transformers\
          \ scipy ftfy\" (i added ftfy after trying it once) but now im getting this:\n\
          \n(amd_venv) E:\\ai\\diffusers-dml\\diffusers-dml\\examples\\inference>python\
          \ save_onnx.py\nSome weights of the model checkpoint at openai/clip-vit-large-patch14\
          \ were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.7.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight',\
          \ 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias',\
          \ 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.embeddings.class_embedding',\
          \ 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.12.mlp.fc1.bias', 'logit_scale', 'vision_model.encoder.layers.6.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight',\
          \ 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.embeddings.position_ids',\
          \ 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight',\
          \ 'text_projection.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.12.mlp.fc2.weight', 'visual_projection.weight',\
          \ 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight',\
          \ 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.post_layernorm.bias',\
          \ 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight',\
          \ 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.16.layer_norm2.weight']\n- This IS expected\
          \ if you are initializing CLIPTextModel from the checkpoint of a model trained\
          \ on another task or with another architecture (e.g. initializing a BertForSequenceClassification\
          \ model from a BertForPreTraining model).\n- This IS NOT expected if you\
          \ are initializing CLIPTextModel from the checkpoint of a model that you\
          \ expect to be exactly identical (initializing a BertForSequenceClassification\
          \ model from a BertForSequenceClassification model).\nFetching 16 files:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 16/16 [00:00<?, ?it/s]\nCannot initialize model with low cpu memory usage\
          \ because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`.\
          \ It is strongly recommended to install `accelerate` for faster and less\
          \ memory-intense model loading. You can do so with:\n```\npip install accelerate\n\
          ```\n.\n`text_config_dict` is provided which will be used to initialize\
          \ `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n\
          E:\\ai\\amd_venv\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28:\
          \ FutureWarning: The class CLIPFeatureExtractor is deprecated and will be\
          \ removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n\
          \  warnings.warn(\nE:\\ai\\amd_venv\\lib\\site-packages\\diffusers\\models\\\
          unet_2d_condition.py:526: TracerWarning: Converting a tensor to a Python\
          \ boolean might cause the trace to be incorrect. We can't record the data\
          \ flow of Python values, so this value will be treated as a constant in\
          \ the future. This means that the trace might not generalize to other inputs!\n\
          \  if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):\n\
          E:\\ai\\amd_venv\\lib\\site-packages\\diffusers\\models\\resnet.py:185:\
          \ TracerWarning: Converting a tensor to a Python boolean might cause the\
          \ trace to be incorrect. We can't record the data flow of Python values,\
          \ so this value will be treated as a constant in the future. This means\
          \ that the trace might not generalize to other inputs!\n  assert hidden_states.shape[1]\
          \ == self.channels\nE:\\ai\\amd_venv\\lib\\site-packages\\diffusers\\models\\\
          resnet.py:190: TracerWarning: Converting a tensor to a Python boolean might\
          \ cause the trace to be incorrect. We can't record the data flow of Python\
          \ values, so this value will be treated as a constant in the future. This\
          \ means that the trace might not generalize to other inputs!\n  assert hidden_states.shape[1]\
          \ == self.channels\nE:\\ai\\amd_venv\\lib\\site-packages\\diffusers\\models\\\
          resnet.py:112: TracerWarning: Converting a tensor to a Python boolean might\
          \ cause the trace to be incorrect. We can't record the data flow of Python\
          \ values, so this value will be treated as a constant in the future. This\
          \ means that the trace might not generalize to other inputs!\n  assert hidden_states.shape[1]\
          \ == self.channels\nE:\\ai\\amd_venv\\lib\\site-packages\\diffusers\\models\\\
          resnet.py:125: TracerWarning: Converting a tensor to a Python boolean might\
          \ cause the trace to be incorrect. We can't record the data flow of Python\
          \ values, so this value will be treated as a constant in the future. This\
          \ means that the trace might not generalize to other inputs!\n  if hidden_states.shape[0]\
          \ >= 64:\nTraceback (most recent call last):\n  File \"E:\\ai\\diffusers-dml\\\
          diffusers-dml\\examples\\inference\\save_onnx.py\", line 66, in <module>\n\
          \    convert_to_onnx(pipe.unet, pipe.vae.post_quant_conv, pipe.vae.decoder,\
          \ text_encoder, height=512, width=512)\n  File \"E:\\ai\\diffusers-dml\\\
          diffusers-dml\\examples\\inference\\save_onnx.py\", line 41, in convert_to_onnx\n\
          \    traced_model = torch.jit.trace(unet, check_inputs[0], check_inputs=[check_inputs[1]],\
          \ strict=True)\n  File \"E:\\ai\\amd_venv\\lib\\site-packages\\torch\\jit\\\
          _trace.py\", line 794, in trace\n    return trace_module(\n  File \"E:\\\
          ai\\amd_venv\\lib\\site-packages\\torch\\jit\\_trace.py\", line 1056, in\
          \ trace_module\n    module._c._create_method_from_trace(\nRuntimeError:\
          \ Encountering a dict at the output of the tracer might cause the trace\
          \ to be incorrect, this is only valid if the container structure does not\
          \ change based on the module's inputs. Consider using a constant container\
          \ instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple`\
          \ instead). If you absolutely need this and know the side effects, pass\
          \ strict=False to trace() to allow this behavior.\n\nI'm sorry if this is\
          \ long but most of it can be ignored as far as  I know when it talks about\
          \ \"some weights were not used\""
        updatedAt: '2023-03-22T01:56:22.034Z'
      numEdits: 2
      reactions: []
    id: 6419610a3c73fa42caec56df
    type: comment
  author: Blue142
  content: "> I am not getting that error after running:\n> !pip install --upgrade\
    \ diffusers transformers scipy\n\nthis almost worked then this popped up:\nwhat\
    \ popped up before that was it crying that i had to install \"accelerate\" so\
    \ I did then this popped up I don't want to reinstall again ;\\\n\n(amd_venv)\
    \ e:\\ai\\diffusers-dml\\examples\\inference>python dml_onnx.py\nFetching 16 files:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00<?, ?it/s]\n`text_config_dict`\
    \ is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"\
    id2label\"]` will be overriden.\ne:\\ai\\amd_venv\\lib\\site-packages\\transformers\\\
    models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor\
    \ is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor\
    \ instead.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"e:\\\
    ai\\diffusers-dml\\examples\\inference\\dml_onnx.py\", line 206, in <module>\n\
    \    pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\"\
    , scheduler=lms, use_auth_token=True)\n  File \"e:\\ai\\amd_venv\\lib\\site-packages\\\
    diffusers\\pipelines\\pipeline_utils.py\", line 965, in from_pretrained\n    model\
    \ = pipeline_class(**init_kwargs)\n  File \"e:\\ai\\diffusers-dml\\examples\\\
    inference\\dml_onnx.py\", line 28, in __init__\n    scheduler = scheduler.set_format(format)\n\
    AttributeError: 'LMSDiscreteScheduler' object has no attribute 'set_format'\n\n\
    small edit: I did a full reinstall and tried \"pip install --upgrade diffusers\
    \ transformers scipy ftfy\" (i added ftfy after trying it once) but now im getting\
    \ this:\n\n(amd_venv) E:\\ai\\diffusers-dml\\diffusers-dml\\examples\\inference>python\
    \ save_onnx.py\nSome weights of the model checkpoint at openai/clip-vit-large-patch14\
    \ were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.7.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight',\
    \ 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias',\
    \ 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.embeddings.class_embedding',\
    \ 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.12.mlp.fc1.bias', 'logit_scale', 'vision_model.encoder.layers.6.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight',\
    \ 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.embeddings.position_ids',\
    \ 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight',\
    \ 'text_projection.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.12.mlp.fc2.weight', 'visual_projection.weight',\
    \ 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight',\
    \ 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.post_layernorm.bias',\
    \ 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight',\
    \ 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.16.layer_norm2.weight']\n- This IS expected if\
    \ you are initializing CLIPTextModel from the checkpoint of a model trained on\
    \ another task or with another architecture (e.g. initializing a BertForSequenceClassification\
    \ model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing\
    \ CLIPTextModel from the checkpoint of a model that you expect to be exactly identical\
    \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
    \ model).\nFetching 16 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00<?,\
    \ ?it/s]\nCannot initialize model with low cpu memory usage because `accelerate`\
    \ was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It\
    \ is strongly recommended to install `accelerate` for faster and less memory-intense\
    \ model loading. You can do so with:\n```\npip install accelerate\n```\n.\n`text_config_dict`\
    \ is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"\
    id2label\"]` will be overriden.\nE:\\ai\\amd_venv\\lib\\site-packages\\transformers\\\
    models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor\
    \ is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor\
    \ instead.\n  warnings.warn(\nE:\\ai\\amd_venv\\lib\\site-packages\\diffusers\\\
    models\\unet_2d_condition.py:526: TracerWarning: Converting a tensor to a Python\
    \ boolean might cause the trace to be incorrect. We can't record the data flow\
    \ of Python values, so this value will be treated as a constant in the future.\
    \ This means that the trace might not generalize to other inputs!\n  if any(s\
    \ % default_overall_up_factor != 0 for s in sample.shape[-2:]):\nE:\\ai\\amd_venv\\\
    lib\\site-packages\\diffusers\\models\\resnet.py:185: TracerWarning: Converting\
    \ a tensor to a Python boolean might cause the trace to be incorrect. We can't\
    \ record the data flow of Python values, so this value will be treated as a constant\
    \ in the future. This means that the trace might not generalize to other inputs!\n\
    \  assert hidden_states.shape[1] == self.channels\nE:\\ai\\amd_venv\\lib\\site-packages\\\
    diffusers\\models\\resnet.py:190: TracerWarning: Converting a tensor to a Python\
    \ boolean might cause the trace to be incorrect. We can't record the data flow\
    \ of Python values, so this value will be treated as a constant in the future.\
    \ This means that the trace might not generalize to other inputs!\n  assert hidden_states.shape[1]\
    \ == self.channels\nE:\\ai\\amd_venv\\lib\\site-packages\\diffusers\\models\\\
    resnet.py:112: TracerWarning: Converting a tensor to a Python boolean might cause\
    \ the trace to be incorrect. We can't record the data flow of Python values, so\
    \ this value will be treated as a constant in the future. This means that the\
    \ trace might not generalize to other inputs!\n  assert hidden_states.shape[1]\
    \ == self.channels\nE:\\ai\\amd_venv\\lib\\site-packages\\diffusers\\models\\\
    resnet.py:125: TracerWarning: Converting a tensor to a Python boolean might cause\
    \ the trace to be incorrect. We can't record the data flow of Python values, so\
    \ this value will be treated as a constant in the future. This means that the\
    \ trace might not generalize to other inputs!\n  if hidden_states.shape[0] >=\
    \ 64:\nTraceback (most recent call last):\n  File \"E:\\ai\\diffusers-dml\\diffusers-dml\\\
    examples\\inference\\save_onnx.py\", line 66, in <module>\n    convert_to_onnx(pipe.unet,\
    \ pipe.vae.post_quant_conv, pipe.vae.decoder, text_encoder, height=512, width=512)\n\
    \  File \"E:\\ai\\diffusers-dml\\diffusers-dml\\examples\\inference\\save_onnx.py\"\
    , line 41, in convert_to_onnx\n    traced_model = torch.jit.trace(unet, check_inputs[0],\
    \ check_inputs=[check_inputs[1]], strict=True)\n  File \"E:\\ai\\amd_venv\\lib\\\
    site-packages\\torch\\jit\\_trace.py\", line 794, in trace\n    return trace_module(\n\
    \  File \"E:\\ai\\amd_venv\\lib\\site-packages\\torch\\jit\\_trace.py\", line\
    \ 1056, in trace_module\n    module._c._create_method_from_trace(\nRuntimeError:\
    \ Encountering a dict at the output of the tracer might cause the trace to be\
    \ incorrect, this is only valid if the container structure does not change based\
    \ on the module's inputs. Consider using a constant container instead (e.g. for\
    \ `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you\
    \ absolutely need this and know the side effects, pass strict=False to trace()\
    \ to allow this behavior.\n\nI'm sorry if this is long but most of it can be ignored\
    \ as far as  I know when it talks about \"some weights were not used\""
  created_at: 2023-03-21 06:47:22+00:00
  edited: true
  hidden: false
  id: 6419610a3c73fa42caec56df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52dacbf83688a0d0060bbce0c074d241.svg
      fullname: San San
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sansanliu
      type: user
    createdAt: '2023-04-02T17:17:05.000Z'
    data:
      edited: false
      editors:
      - sansanliu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52dacbf83688a0d0060bbce0c074d241.svg
          fullname: San San
          isHf: false
          isPro: false
          name: sansanliu
          type: user
        html: '<p>I got the error with Diffuser == 0.4.0 and these command help to
          solve problem:<br>pip install --upgrade diffusers transformers scipy<br>=&gt;
          restart Runtime<br>Work!</p>

          '
        raw: 'I got the error with Diffuser == 0.4.0 and these command help to solve
          problem:

          pip install --upgrade diffusers transformers scipy

          => restart Runtime

          Work!'
        updatedAt: '2023-04-02T17:17:05.365Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - danielajisafe
    id: 6429b891316c9207b7c0f8a6
    type: comment
  author: sansanliu
  content: 'I got the error with Diffuser == 0.4.0 and these command help to solve
    problem:

    pip install --upgrade diffusers transformers scipy

    => restart Runtime

    Work!'
  created_at: 2023-04-02 16:17:05+00:00
  edited: false
  hidden: false
  id: 6429b891316c9207b7c0f8a6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 158
repo_id: CompVis/stable-diffusion-v1-4
repo_type: model
status: open
target_branch: null
title: 'why bug :  getattr(): attribute name must be string?'
