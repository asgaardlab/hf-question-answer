!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tomwjhtom
conflicting_files: null
created_at: 2022-08-24 00:20:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/083e21832a595b0e92e2a9133912ed2d.svg
      fullname: Junhao Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomwjhtom
      type: user
    createdAt: '2022-08-24T01:20:44.000Z'
    data:
      edited: false
      editors:
      - tomwjhtom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/083e21832a595b0e92e2a9133912ed2d.svg
          fullname: Junhao Wang
          isHf: false
          isPro: false
          name: tomwjhtom
          type: user
        html: "<p>I don't have Nvidia GPU, so tried to use M1 on my Macbook air. However,\
          \ executing the code below leads to kernel death.</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">import</span> torch\n<span\
          \ class=\"hljs-keyword\">from</span> torch <span class=\"hljs-keyword\"\
          >import</span> autocast\n<span class=\"hljs-keyword\">from</span> diffusers\
          \ <span class=\"hljs-keyword\">import</span> StableDiffusionPipeline\n\n\
          model_id = <span class=\"hljs-string\">\"CompVis/stable-diffusion-v1-4\"\
          </span>\ndevice = <span class=\"hljs-string\">\"mps\"</span>\n\n\npipe =\
          \ StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16,\
          \ revision=<span class=\"hljs-string\">\"fp16\"</span>, use_auth_token=<span\
          \ class=\"hljs-literal\">True</span>)\npipe = pipe.to(device)\n\nprompt\
          \ = <span class=\"hljs-string\">\"a photo of an astronaut riding a horse\
          \ on mars\"</span>\nimage = pipe(prompt, guidance_scale=<span class=\"hljs-number\"\
          >7.5</span>)[<span class=\"hljs-string\">\"sample\"</span>][<span class=\"\
          hljs-number\">0</span>]  \n</code></pre>\n<p>Note that <code>pipe.to(device)</code>\
          \ executes successfully.<br>Have anyone made M1 work yet? My pytorch version\
          \ is '1.13.0.dev20220823'</p>\n"
        raw: "I don't have Nvidia GPU, so tried to use M1 on my Macbook air. However,\
          \ executing the code below leads to kernel death.\r\n```python\r\nimport\
          \ torch\r\nfrom torch import autocast\r\nfrom diffusers import StableDiffusionPipeline\r\
          \n\r\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\r\ndevice = \"mps\"\r\
          \n\r\n\r\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16,\
          \ revision=\"fp16\", use_auth_token=True)\r\npipe = pipe.to(device)\r\n\r\
          \nprompt = \"a photo of an astronaut riding a horse on mars\"\r\nimage =\
          \ pipe(prompt, guidance_scale=7.5)[\"sample\"][0]  \r\n```\r\nNote that\
          \ `pipe.to(device)` executes successfully.\r\nHave anyone made M1 work yet?\
          \ My pytorch version is '1.13.0.dev20220823'"
        updatedAt: '2022-08-24T01:20:44.532Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - davidpomerenke
        - robertwt7
        - massisenergy
        - knetwork
        - alexfromapex
    id: 63057cec8eb426386042db3c
    type: comment
  author: tomwjhtom
  content: "I don't have Nvidia GPU, so tried to use M1 on my Macbook air. However,\
    \ executing the code below leads to kernel death.\r\n```python\r\nimport torch\r\
    \nfrom torch import autocast\r\nfrom diffusers import StableDiffusionPipeline\r\
    \n\r\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\r\ndevice = \"mps\"\r\n\r\n\
    \r\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16,\
    \ revision=\"fp16\", use_auth_token=True)\r\npipe = pipe.to(device)\r\n\r\nprompt\
    \ = \"a photo of an astronaut riding a horse on mars\"\r\nimage = pipe(prompt,\
    \ guidance_scale=7.5)[\"sample\"][0]  \r\n```\r\nNote that `pipe.to(device)` executes\
    \ successfully.\r\nHave anyone made M1 work yet? My pytorch version is '1.13.0.dev20220823'"
  created_at: 2022-08-24 00:20:44+00:00
  edited: false
  hidden: false
  id: 63057cec8eb426386042db3c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2022-08-25T19:52:10.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>We're working on exactly this! Pinging <span data-props=\"{&quot;user&quot;:&quot;pcuenq&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/pcuenq\"\
          >@<span class=\"underline\">pcuenq</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;apolinario&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/apolinario\">@<span class=\"\
          underline\">apolinario</span></a></span>\n\n\t</span></span> here as well</p>\n"
        raw: We're working on exactly this! Pinging @pcuenq and @apolinario here as
          well
        updatedAt: '2022-08-25T19:52:10.324Z'
      numEdits: 0
      reactions:
      - count: 7
        reaction: "\u2764\uFE0F"
        users:
        - tomwjhtom
        - loretoparisi
        - osanseviero
        - multimodalart
        - scottinallcaps
        - victor
        - massisenergy
    id: 6307d2eaa670ed10f9d16580
    type: comment
  author: patrickvonplaten
  content: We're working on exactly this! Pinging @pcuenq and @apolinario here as
    well
  created_at: 2022-08-25 18:52:10+00:00
  edited: false
  hidden: false
  id: 6307d2eaa670ed10f9d16580
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2022-08-25T19:53:07.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: '<p>Please also check announcements on Twitter - we''ll publish something
          about that soon!</p>

          '
        raw: Please also check announcements on Twitter - we'll publish something
          about that soon!
        updatedAt: '2022-08-25T19:53:07.221Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\u2764\uFE0F"
        users:
        - tomwjhtom
        - loretoparisi
        - multimodalart
        - scottinallcaps
        - massisenergy
    id: 6307d323f0dc38fb47beaf3b
    type: comment
  author: patrickvonplaten
  content: Please also check announcements on Twitter - we'll publish something about
    that soon!
  created_at: 2022-08-25 18:53:07+00:00
  edited: false
  hidden: false
  id: 6307d323f0dc38fb47beaf3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/083e21832a595b0e92e2a9133912ed2d.svg
      fullname: Junhao Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomwjhtom
      type: user
    createdAt: '2022-08-25T20:38:25.000Z'
    data:
      edited: false
      editors:
      - tomwjhtom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/083e21832a595b0e92e2a9133912ed2d.svg
          fullname: Junhao Wang
          isHf: false
          isPro: false
          name: tomwjhtom
          type: user
        html: '<p>Thanks!</p>

          '
        raw: Thanks!
        updatedAt: '2022-08-25T20:38:25.818Z'
      numEdits: 0
      reactions: []
    id: 6307ddc1161cfe1383a45de6
    type: comment
  author: tomwjhtom
  content: Thanks!
  created_at: 2022-08-25 19:38:25+00:00
  edited: false
  hidden: false
  id: 6307ddc1161cfe1383a45de6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
      fullname: Loreto Parisi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: loretoparisi
      type: user
    createdAt: '2022-08-25T22:27:31.000Z'
    data:
      edited: true
      editors:
      - loretoparisi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
          fullname: Loreto Parisi
          isHf: false
          isPro: false
          name: loretoparisi
          type: user
        html: "<p>In my case on Apple M1  with the code</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-comment\"># make sure you're logged in with `huggingface-cli\
          \ login`</span>\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"\
          hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">from</span>\
          \ torch <span class=\"hljs-keyword\">import</span> autocast\n<span class=\"\
          hljs-keyword\">from</span> diffusers <span class=\"hljs-keyword\">import</span>\
          \ StableDiffusionPipeline, LMSDiscreteScheduler\n\n<span class=\"hljs-comment\"\
          ># To swap out the noise scheduler, pass it to from_pretrained:</span>\n\
          lms = LMSDiscreteScheduler(\n    beta_start=<span class=\"hljs-number\"\
          >0.00085</span>, \n    beta_end=<span class=\"hljs-number\">0.012</span>,\
          \ \n    beta_schedule=<span class=\"hljs-string\">\"scaled_linear\"</span>\n\
          )\n\ndevice = <span class=\"hljs-string\">'cuda'</span> <span class=\"hljs-keyword\"\
          >if</span> torch.cuda.is_available() <span class=\"hljs-keyword\">else</span>\
          \ <span class=\"hljs-string\">'cpu'</span>\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">f'running on <span class=\"hljs-subst\"\
          >{device}</span>'</span>)\npipe = StableDiffusionPipeline.from_pretrained(\n\
          \    <span class=\"hljs-string\">\"CompVis/stable-diffusion-v1-3\"</span>,\
          \ \n    scheduler=lms,\n    torch_dtype=torch.float16,\n    revision=<span\
          \ class=\"hljs-string\">\"fp16\"</span>,\n    use_auth_token=<span class=\"\
          hljs-literal\">True</span>,\n    cache_dir=os.getenv(<span class=\"hljs-string\"\
          >\"cache_dir\"</span>, <span class=\"hljs-string\">\"./models\"</span>)\n\
          ).to(device)\n\nprompt = <span class=\"hljs-string\">\"a photo of an astronaut\
          \ riding a horse on mars\"</span>\n<span class=\"hljs-keyword\">with</span>\
          \ autocast(device):\n    image = pipe(prompt)[<span class=\"hljs-string\"\
          >\"sample\"</span>][<span class=\"hljs-number\">0</span>]  \n    \nimage.save(<span\
          \ class=\"hljs-string\">\"astronaut_rides_horse.png\"</span>)\n</code></pre>\n\
          <p>I get the following error</p>\n<pre><code>Traceback (most recent call\
          \ last):\n  File \"diffuser.py\", line 27, in &lt;module&gt;\n    image\
          \ = pipe(prompt)[\"sample\"][0]  \n  ...\n  File \"/Documents/Projects/bloom/.venv/lib/python3.7/site-packages/torch/nn/functional.py\"\
          , line 2503, in layer_norm\n    return torch.layer_norm(input, normalized_shape,\
          \ weight, bias, eps, torch.backends.cudnn.enabled)\nRuntimeError: \"LayerNormKernelImpl\"\
          \ not implemented for 'Half'\n</code></pre>\n"
        raw: "In my case on Apple M1  with the code\n\n```python\n# make sure you're\
          \ logged in with `huggingface-cli login`\nimport os\nimport torch\nfrom\
          \ torch import autocast\nfrom diffusers import StableDiffusionPipeline,\
          \ LMSDiscreteScheduler\n\n# To swap out the noise scheduler, pass it to\
          \ from_pretrained:\nlms = LMSDiscreteScheduler(\n    beta_start=0.00085,\
          \ \n    beta_end=0.012, \n    beta_schedule=\"scaled_linear\"\n)\n\ndevice\
          \ = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'running on {device}')\n\
          pipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-3\"\
          , \n    scheduler=lms,\n    torch_dtype=torch.float16,\n    revision=\"\
          fp16\",\n    use_auth_token=True,\n    cache_dir=os.getenv(\"cache_dir\"\
          , \"./models\")\n).to(device)\n\nprompt = \"a photo of an astronaut riding\
          \ a horse on mars\"\nwith autocast(device):\n    image = pipe(prompt)[\"\
          sample\"][0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\n\n\
          \nI get the following error\n\n```\nTraceback (most recent call last):\n\
          \  File \"diffuser.py\", line 27, in <module>\n    image = pipe(prompt)[\"\
          sample\"][0]  \n  ...\n  File \"/Documents/Projects/bloom/.venv/lib/python3.7/site-packages/torch/nn/functional.py\"\
          , line 2503, in layer_norm\n    return torch.layer_norm(input, normalized_shape,\
          \ weight, bias, eps, torch.backends.cudnn.enabled)\nRuntimeError: \"LayerNormKernelImpl\"\
          \ not implemented for 'Half'\n```"
        updatedAt: '2022-08-25T22:28:26.931Z'
      numEdits: 1
      reactions: []
    id: 6307f753a670ed10f9d2d117
    type: comment
  author: loretoparisi
  content: "In my case on Apple M1  with the code\n\n```python\n# make sure you're\
    \ logged in with `huggingface-cli login`\nimport os\nimport torch\nfrom torch\
    \ import autocast\nfrom diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n\
    \n# To swap out the noise scheduler, pass it to from_pretrained:\nlms = LMSDiscreteScheduler(\n\
    \    beta_start=0.00085, \n    beta_end=0.012, \n    beta_schedule=\"scaled_linear\"\
    \n)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'running\
    \ on {device}')\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-3\"\
    , \n    scheduler=lms,\n    torch_dtype=torch.float16,\n    revision=\"fp16\"\
    ,\n    use_auth_token=True,\n    cache_dir=os.getenv(\"cache_dir\", \"./models\"\
    )\n).to(device)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\
    \nwith autocast(device):\n    image = pipe(prompt)[\"sample\"][0]  \n    \nimage.save(\"\
    astronaut_rides_horse.png\")\n```\n\n\nI get the following error\n\n```\nTraceback\
    \ (most recent call last):\n  File \"diffuser.py\", line 27, in <module>\n   \
    \ image = pipe(prompt)[\"sample\"][0]  \n  ...\n  File \"/Documents/Projects/bloom/.venv/lib/python3.7/site-packages/torch/nn/functional.py\"\
    , line 2503, in layer_norm\n    return torch.layer_norm(input, normalized_shape,\
    \ weight, bias, eps, torch.backends.cudnn.enabled)\nRuntimeError: \"LayerNormKernelImpl\"\
    \ not implemented for 'Half'\n```"
  created_at: 2022-08-25 21:27:31+00:00
  edited: true
  hidden: false
  id: 6307f753a670ed10f9d2d117
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1649143001781-624bebf604abc7ebb01789af.jpeg?w=200&h=200&f=face
      fullname: "Apolin\xE1rio from multimodal AI art"
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multimodalart
      type: user
    createdAt: '2022-08-26T09:41:58.000Z'
    data:
      edited: true
      editors:
      - multimodalart
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1649143001781-624bebf604abc7ebb01789af.jpeg?w=200&h=200&f=face
          fullname: "Apolin\xE1rio from multimodal AI art"
          isHf: true
          isPro: false
          name: multimodalart
          type: user
        html: "<p>For device <code>mps</code> it doesn't work out of the box yet indeed,\
          \ however if device = <code>cpu</code> it should work <span data-props=\"\
          {&quot;user&quot;:&quot;loretoparisi&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/loretoparisi\">@<span class=\"underline\"\
          >loretoparisi</span></a></span>\n\n\t</span></span>. Can you try removing\
          \ the <code>with autocast(device)</code>? Autocast doesn't work for CPU\
          \ as of now</p>\n"
        raw: For device `mps` it doesn't work out of the box yet indeed, however if
          device = `cpu` it should work @loretoparisi. Can you try removing the `with
          autocast(device)`? Autocast doesn't work for CPU as of now
        updatedAt: '2022-08-26T09:42:37.158Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - loretoparisi
    id: 630895667dc1b1a54cc879be
    type: comment
  author: multimodalart
  content: For device `mps` it doesn't work out of the box yet indeed, however if
    device = `cpu` it should work @loretoparisi. Can you try removing the `with autocast(device)`?
    Autocast doesn't work for CPU as of now
  created_at: 2022-08-26 08:41:58+00:00
  edited: true
  hidden: false
  id: 630895667dc1b1a54cc879be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
      fullname: Loreto Parisi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: loretoparisi
      type: user
    createdAt: '2022-08-26T10:15:58.000Z'
    data:
      edited: false
      editors:
      - loretoparisi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
          fullname: Loreto Parisi
          isHf: false
          isPro: false
          name: loretoparisi
          type: user
        html: "<blockquote>\n<p>For device <code>mps</code> it doesn't work out of\
          \ the box yet indeed, however if device = <code>cpu</code> it should work\
          \ <span data-props=\"{&quot;user&quot;:&quot;loretoparisi&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/loretoparisi\">@<span\
          \ class=\"underline\">loretoparisi</span></a></span>\n\n\t</span></span>.\
          \ Can you try removing the <code>with autocast(device)</code>? Autocast\
          \ doesn't work for CPU as of now</p>\n</blockquote>\n<p>Thanks, I slightly\
          \ modified the code like</p>\n<pre><code class=\"language-python\">prompt\
          \ = <span class=\"hljs-string\">\"a photo of an astronaut riding a horse\
          \ on mars\"</span>\nsamples = <span class=\"hljs-number\">2</span>\nsteps\
          \ = <span class=\"hljs-number\">45</span>\nscale = <span class=\"hljs-number\"\
          >7.5</span>\n<span class=\"hljs-keyword\">if</span> device==<span class=\"\
          hljs-string\">'cuda'</span>:\n    <span class=\"hljs-keyword\">with</span>\
          \ autocast(device):\n        image = pipe(\n            [prompt]*samples,\n\
          \            num_inference_steps=steps,\n            guidance_scale=scale,\n\
          \            )[<span class=\"hljs-string\">\"sample\"</span>][<span class=\"\
          hljs-number\">0</span>]\n<span class=\"hljs-keyword\">else</span>:\n   \
          \ image = pipe(prompt)[<span class=\"hljs-string\">\"sample\"</span>][<span\
          \ class=\"hljs-number\">0</span>]\n</code></pre>\n<p>but I'm still getting\
          \ the same error:</p>\n<pre><code>Traceback (most recent call last):\n \
          \ File \"diffuser.py\", line 39, in &lt;module&gt;\n    image = pipe(prompt)[\"\
          sample\"][0]\n  File \"/Projects/bloom/.venv/lib/python3.7/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\n...\n  File \"/Projects/bloom/.venv/lib/python3.7/site-packages/torch/nn/functional.py\"\
          , line 2503, in layer_norm\n    return torch.layer_norm(input, normalized_shape,\
          \ weight, bias, eps, torch.backends.cudnn.enabled)\nRuntimeError: \"LayerNormKernelImpl\"\
          \ not implemented for 'Half'\n</code></pre>\n"
        raw: "> For device `mps` it doesn't work out of the box yet indeed, however\
          \ if device = `cpu` it should work @loretoparisi. Can you try removing the\
          \ `with autocast(device)`? Autocast doesn't work for CPU as of now\n\nThanks,\
          \ I slightly modified the code like\n\n```python\nprompt = \"a photo of\
          \ an astronaut riding a horse on mars\"\nsamples = 2\nsteps = 45\nscale\
          \ = 7.5\nif device=='cuda':\n    with autocast(device):\n        image =\
          \ pipe(\n            [prompt]*samples,\n            num_inference_steps=steps,\n\
          \            guidance_scale=scale,\n            )[\"sample\"][0]\nelse:\n\
          \    image = pipe(prompt)[\"sample\"][0]\n```\n\nbut I'm still getting the\
          \ same error:\n\n```\nTraceback (most recent call last):\n  File \"diffuser.py\"\
          , line 39, in <module>\n    image = pipe(prompt)[\"sample\"][0]\n  File\
          \ \"/Projects/bloom/.venv/lib/python3.7/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\n...\n  File \"/Projects/bloom/.venv/lib/python3.7/site-packages/torch/nn/functional.py\"\
          , line 2503, in layer_norm\n    return torch.layer_norm(input, normalized_shape,\
          \ weight, bias, eps, torch.backends.cudnn.enabled)\nRuntimeError: \"LayerNormKernelImpl\"\
          \ not implemented for 'Half'\n```"
        updatedAt: '2022-08-26T10:15:58.352Z'
      numEdits: 0
      reactions: []
    id: 63089d5e454dc257521a54f2
    type: comment
  author: loretoparisi
  content: "> For device `mps` it doesn't work out of the box yet indeed, however\
    \ if device = `cpu` it should work @loretoparisi. Can you try removing the `with\
    \ autocast(device)`? Autocast doesn't work for CPU as of now\n\nThanks, I slightly\
    \ modified the code like\n\n```python\nprompt = \"a photo of an astronaut riding\
    \ a horse on mars\"\nsamples = 2\nsteps = 45\nscale = 7.5\nif device=='cuda':\n\
    \    with autocast(device):\n        image = pipe(\n            [prompt]*samples,\n\
    \            num_inference_steps=steps,\n            guidance_scale=scale,\n \
    \           )[\"sample\"][0]\nelse:\n    image = pipe(prompt)[\"sample\"][0]\n\
    ```\n\nbut I'm still getting the same error:\n\n```\nTraceback (most recent call\
    \ last):\n  File \"diffuser.py\", line 39, in <module>\n    image = pipe(prompt)[\"\
    sample\"][0]\n  File \"/Projects/bloom/.venv/lib/python3.7/site-packages/torch/autograd/grad_mode.py\"\
    , line 27, in decorate_context\n...\n  File \"/Projects/bloom/.venv/lib/python3.7/site-packages/torch/nn/functional.py\"\
    , line 2503, in layer_norm\n    return torch.layer_norm(input, normalized_shape,\
    \ weight, bias, eps, torch.backends.cudnn.enabled)\nRuntimeError: \"LayerNormKernelImpl\"\
    \ not implemented for 'Half'\n```"
  created_at: 2022-08-26 09:15:58+00:00
  edited: false
  hidden: false
  id: 63089d5e454dc257521a54f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1649143001781-624bebf604abc7ebb01789af.jpeg?w=200&h=200&f=face
      fullname: "Apolin\xE1rio from multimodal AI art"
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multimodalart
      type: user
    createdAt: '2022-08-26T11:21:09.000Z'
    data:
      edited: true
      editors:
      - multimodalart
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1649143001781-624bebf604abc7ebb01789af.jpeg?w=200&h=200&f=face
          fullname: "Apolin\xE1rio from multimodal AI art"
          isHf: true
          isPro: false
          name: multimodalart
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;loretoparisi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/loretoparisi\"\
          >@<span class=\"underline\">loretoparisi</span></a></span>\n\n\t</span></span>,\
          \ oh this is probably because you are trying to load the fp16 version of\
          \ the model - which also doesn't work on CPU \U0001F605<br>Try this for\
          \ <code>pipe</code></p>\n<pre><code class=\"language-py\">pipe = StableDiffusionPipeline.from_pretrained(\n\
          \    <span class=\"hljs-string\">\"CompVis/stable-diffusion-v1-4\"</span>,\
          \ <span class=\"hljs-comment\">#better model btw </span>\n    scheduler=lms,\n\
          \    use_auth_token=<span class=\"hljs-literal\">True</span>,\n    cache_dir=os.getenv(<span\
          \ class=\"hljs-string\">\"cache_dir\"</span>, <span class=\"hljs-string\"\
          >\"./models\"</span>)\n).to(device)\n</code></pre>\n"
        raw: "@loretoparisi, oh this is probably because you are trying to load the\
          \ fp16 version of the model - which also doesn't work on CPU \U0001F605\n\
          Try this for `pipe`\n```py\npipe = StableDiffusionPipeline.from_pretrained(\n\
          \    \"CompVis/stable-diffusion-v1-4\", #better model btw \n    scheduler=lms,\n\
          \    use_auth_token=True,\n    cache_dir=os.getenv(\"cache_dir\", \"./models\"\
          )\n).to(device)\n```"
        updatedAt: '2022-08-26T11:43:20.798Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - patrickvonplaten
        - face-bidet
    id: 6308aca5f48eff2e8eb83f32
    type: comment
  author: multimodalart
  content: "@loretoparisi, oh this is probably because you are trying to load the\
    \ fp16 version of the model - which also doesn't work on CPU \U0001F605\nTry this\
    \ for `pipe`\n```py\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\"\
    , #better model btw \n    scheduler=lms,\n    use_auth_token=True,\n    cache_dir=os.getenv(\"\
    cache_dir\", \"./models\")\n).to(device)\n```"
  created_at: 2022-08-26 10:21:09+00:00
  edited: true
  hidden: false
  id: 6308aca5f48eff2e8eb83f32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
      fullname: Loreto Parisi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: loretoparisi
      type: user
    createdAt: '2022-08-27T10:29:09.000Z'
    data:
      edited: false
      editors:
      - loretoparisi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
          fullname: Loreto Parisi
          isHf: false
          isPro: false
          name: loretoparisi
          type: user
        html: '<p>Thank you it works without on Apple M1, removing autocast and fp16!</p>

          '
        raw: Thank you it works without on Apple M1, removing autocast and fp16!
        updatedAt: '2022-08-27T10:29:09.454Z'
      numEdits: 0
      reactions: []
    id: 6309f1f5450a9475bd124453
    type: comment
  author: loretoparisi
  content: Thank you it works without on Apple M1, removing autocast and fp16!
  created_at: 2022-08-27 09:29:09+00:00
  edited: false
  hidden: false
  id: 6309f1f5450a9475bd124453
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e267426537c8c6649155116e248ee496.svg
      fullname: simon thompson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgt101
      type: user
    createdAt: '2022-08-27T21:10:52.000Z'
    data:
      edited: false
      editors:
      - sgt101
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e267426537c8c6649155116e248ee496.svg
          fullname: simon thompson
          isHf: false
          isPro: false
          name: sgt101
          type: user
        html: "<p>Here is the code for other people's convenience </p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-comment\"># make sure you're\
          \ logged in with `huggingface-cli login`</span>\n<span class=\"hljs-keyword\"\
          >import</span> os\n<span class=\"hljs-keyword\">import</span> torch\n<span\
          \ class=\"hljs-keyword\">from</span> torch <span class=\"hljs-keyword\"\
          >import</span> autocast\n<span class=\"hljs-keyword\">from</span> diffusers\
          \ <span class=\"hljs-keyword\">import</span> StableDiffusionPipeline, LMSDiscreteScheduler\n\
          \n<span class=\"hljs-comment\"># To swap out the noise scheduler, pass it\
          \ to from_pretrained:</span>\nlms = LMSDiscreteScheduler(\n    beta_start=<span\
          \ class=\"hljs-number\">0.00085</span>, \n    beta_end=<span class=\"hljs-number\"\
          >0.012</span>, \n    beta_schedule=<span class=\"hljs-string\">\"scaled_linear\"\
          </span>\n)\n\ndevice = <span class=\"hljs-string\">'cuda'</span> <span class=\"\
          hljs-keyword\">if</span> torch.cuda.is_available() <span class=\"hljs-keyword\"\
          >else</span> <span class=\"hljs-string\">'cpu'</span>\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">f'running on <span class=\"hljs-subst\"\
          >{device}</span>'</span>)\n\npipe = StableDiffusionPipeline.from_pretrained(\n\
          \    <span class=\"hljs-string\">\"CompVis/stable-diffusion-v1-4\"</span>,\
          \ <span class=\"hljs-comment\">#better model btw </span>\n    scheduler=lms,\n\
          \    use_auth_token=<span class=\"hljs-literal\">True</span>,\n    cache_dir=os.getenv(<span\
          \ class=\"hljs-string\">\"cache_dir\"</span>, <span class=\"hljs-string\"\
          >\"./models\"</span>)\n).to(device)\n\n\nprompt = <span class=\"hljs-string\"\
          >\"a photo of an astronaut riding a horse on mars\"</span>\nsamples = <span\
          \ class=\"hljs-number\">2</span>\nsteps = <span class=\"hljs-number\">45</span>\n\
          scale = <span class=\"hljs-number\">7.5</span>\nimage = pipe(prompt)[<span\
          \ class=\"hljs-string\">\"sample\"</span>][<span class=\"hljs-number\">0</span>]\n\
          \    \n</code></pre>\n<p>it definitely works, very slowly though.</p>\n"
        raw: "Here is the code for other people's convenience \n\n```python\n# make\
          \ sure you're logged in with `huggingface-cli login`\nimport os\nimport\
          \ torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline,\
          \ LMSDiscreteScheduler\n\n# To swap out the noise scheduler, pass it to\
          \ from_pretrained:\nlms = LMSDiscreteScheduler(\n    beta_start=0.00085,\
          \ \n    beta_end=0.012, \n    beta_schedule=\"scaled_linear\"\n)\n\ndevice\
          \ = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'running on {device}')\n\
          \npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\"\
          , #better model btw \n    scheduler=lms,\n    use_auth_token=True,\n   \
          \ cache_dir=os.getenv(\"cache_dir\", \"./models\")\n).to(device)\n\n\nprompt\
          \ = \"a photo of an astronaut riding a horse on mars\"\nsamples = 2\nsteps\
          \ = 45\nscale = 7.5\nimage = pipe(prompt)[\"sample\"][0]\n    \n```\nit\
          \ definitely works, very slowly though."
        updatedAt: '2022-08-27T21:10:52.331Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\u2764\uFE0F"
        users:
        - tomwjhtom
        - multimodalart
        - osanseviero
        - yv-dku
        - KwankiAhn
        - shrey14
    id: 630a885ce81e1dea2cef53f0
    type: comment
  author: sgt101
  content: "Here is the code for other people's convenience \n\n```python\n# make\
    \ sure you're logged in with `huggingface-cli login`\nimport os\nimport torch\n\
    from torch import autocast\nfrom diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n\
    \n# To swap out the noise scheduler, pass it to from_pretrained:\nlms = LMSDiscreteScheduler(\n\
    \    beta_start=0.00085, \n    beta_end=0.012, \n    beta_schedule=\"scaled_linear\"\
    \n)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'running\
    \ on {device}')\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\"\
    , #better model btw \n    scheduler=lms,\n    use_auth_token=True,\n    cache_dir=os.getenv(\"\
    cache_dir\", \"./models\")\n).to(device)\n\n\nprompt = \"a photo of an astronaut\
    \ riding a horse on mars\"\nsamples = 2\nsteps = 45\nscale = 7.5\nimage = pipe(prompt)[\"\
    sample\"][0]\n    \n```\nit definitely works, very slowly though."
  created_at: 2022-08-27 20:10:52+00:00
  edited: false
  hidden: false
  id: 630a885ce81e1dea2cef53f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/511a73f80299e84df711beae2ac3922a.svg
      fullname: samson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fragmede
      type: user
    createdAt: '2022-08-30T12:24:35.000Z'
    data:
      edited: true
      editors:
      - fragmede
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/511a73f80299e84df711beae2ac3922a.svg
          fullname: samson
          isHf: false
          isPro: false
          name: fragmede
          type: user
        html: '<p>On an M1 (not M1 Max) I get <code>TypeError: Cannot convert a MPS
          Tensor to float64 dtype as the MPS framework doesn''t support float64. Please
          use float32 instead.</code> if I don''t specify revision and torch_dtype.
          The script <code>python scripts/txt2img.py</code> works to create images
          though, so it''s an issue with <code>diffusers</code> and not stable-diffusion
          I think.</p>

          '
        raw: 'On an M1 (not M1 Max) I get `TypeError: Cannot convert a MPS Tensor
          to float64 dtype as the MPS framework doesn''t support float64. Please use
          float32 instead.` if I don''t specify revision and torch_dtype. The script
          `python scripts/txt2img.py` works to create images though, so it''s an issue
          with `diffusers` and not stable-diffusion I think.'
        updatedAt: '2022-08-30T12:25:36.659Z'
      numEdits: 1
      reactions: []
    id: 630e01838df86f1e5becf2f3
    type: comment
  author: fragmede
  content: 'On an M1 (not M1 Max) I get `TypeError: Cannot convert a MPS Tensor to
    float64 dtype as the MPS framework doesn''t support float64. Please use float32
    instead.` if I don''t specify revision and torch_dtype. The script `python scripts/txt2img.py`
    works to create images though, so it''s an issue with `diffusers` and not stable-diffusion
    I think.'
  created_at: 2022-08-30 11:24:35+00:00
  edited: true
  hidden: false
  id: 630e01838df86f1e5becf2f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e267426537c8c6649155116e248ee496.svg
      fullname: simon thompson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgt101
      type: user
    createdAt: '2022-08-30T12:54:56.000Z'
    data:
      edited: false
      editors:
      - sgt101
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e267426537c8c6649155116e248ee496.svg
          fullname: simon thompson
          isHf: false
          isPro: false
          name: sgt101
          type: user
        html: '<blockquote>

          <p>On an M1 (not M1 Max) I get <code>TypeError: Cannot convert a MPS Tensor
          to float64 dtype as the MPS framework doesn''t support float64. Please use
          float32 instead.</code> if I don''t specify revision and torch_dtype. The
          script <code>python scripts/txt2img.py</code> works to create images though,
          so it''s an issue with <code>diffusers</code> and not stable-diffusion I
          think.</p>

          </blockquote>

          <p>Can you say how you specify revision and torch_dtype? </p>

          <p>I think txt2image.py is using CPU if Cuda is not available</p>

          '
        raw: "> On an M1 (not M1 Max) I get `TypeError: Cannot convert a MPS Tensor\
          \ to float64 dtype as the MPS framework doesn't support float64. Please\
          \ use float32 instead.` if I don't specify revision and torch_dtype. The\
          \ script `python scripts/txt2img.py` works to create images though, so it's\
          \ an issue with `diffusers` and not stable-diffusion I think.\n\nCan you\
          \ say how you specify revision and torch_dtype? \n\nI think txt2image.py\
          \ is using CPU if Cuda is not available"
        updatedAt: '2022-08-30T12:54:56.599Z'
      numEdits: 0
      reactions: []
    id: 630e08a08df86f1e5bed44e4
    type: comment
  author: sgt101
  content: "> On an M1 (not M1 Max) I get `TypeError: Cannot convert a MPS Tensor\
    \ to float64 dtype as the MPS framework doesn't support float64. Please use float32\
    \ instead.` if I don't specify revision and torch_dtype. The script `python scripts/txt2img.py`\
    \ works to create images though, so it's an issue with `diffusers` and not stable-diffusion\
    \ I think.\n\nCan you say how you specify revision and torch_dtype? \n\nI think\
    \ txt2image.py is using CPU if Cuda is not available"
  created_at: 2022-08-30 11:54:56+00:00
  edited: false
  hidden: false
  id: 630e08a08df86f1e5bed44e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/511a73f80299e84df711beae2ac3922a.svg
      fullname: samson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fragmede
      type: user
    createdAt: '2022-08-30T15:08:52.000Z'
    data:
      edited: false
      editors:
      - fragmede
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/511a73f80299e84df711beae2ac3922a.svg
          fullname: samson
          isHf: false
          isPro: false
          name: fragmede
          type: user
        html: '<p>To sgt101, I think you''re running in CPU mode, because of the line
          that says <code>device = ''cuda'' if torch.cuda.is_available() else ''cpu''
          </code></p>

          '
        raw: 'To sgt101, I think you''re running in CPU mode, because of the line
          that says `device = ''cuda'' if torch.cuda.is_available() else ''cpu''

          `'
        updatedAt: '2022-08-30T15:08:52.022Z'
      numEdits: 0
      reactions: []
    id: 630e2804c0eca3037a008692
    type: comment
  author: fragmede
  content: 'To sgt101, I think you''re running in CPU mode, because of the line that
    says `device = ''cuda'' if torch.cuda.is_available() else ''cpu''

    `'
  created_at: 2022-08-30 14:08:52+00:00
  edited: false
  hidden: false
  id: 630e2804c0eca3037a008692
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/511a73f80299e84df711beae2ac3922a.svg
      fullname: samson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fragmede
      type: user
    createdAt: '2022-08-30T15:11:02.000Z'
    data:
      edited: true
      editors:
      - fragmede
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/511a73f80299e84df711beae2ac3922a.svg
          fullname: samson
          isHf: false
          isPro: false
          name: fragmede
          type: user
        html: "<p>I'm pretty sure my <code>txt2image</code> is using MPS (<a rel=\"\
          nofollow\" href=\"https://github.com/magnusviri/stable-diffusion/tree/apple-silicon-mps-support\"\
          >magnusviri's fork</a>) because it takes about a minute to run instead of\
          \ upwards of 30 mins, among other things. I have</p>\n<pre><code>pipe =\
          \ StableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\"\
          , \n    torch_dtype=torch.float16, revision=\"fp16\",\n    use_auth_token=True,\n\
          ).to(\"mps\")\n</code></pre>\n<p>But that errors out with </p>\n<pre><code>0it\
          \ [00:00, ?it/s]loc(\"mps_add\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/20d6c351-ee94-11ec-bcaf-7247572f23b4/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\"\
          :219:0)): error: input types 'tensor&lt;2x1280xf32&gt;' and 'tensor&lt;*xf16&gt;'\
          \ are not broadcast compatible\nLLVM ERROR: Failed to infer result type(s).\n\
          Abort trap: 6\n/Users/fragmede/miniforge3/envs/ldm/lib/python3.10/multiprocessing/resource_tracker.py:224:\
          \ UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects\
          \ to clean up at shutdown\n  warnings.warn('resource_tracker: There appear\
          \ to be %d '\n</code></pre>\n"
        raw: "I'm pretty sure my `txt2image` is using MPS ([magnusviri's fork](https://github.com/magnusviri/stable-diffusion/tree/apple-silicon-mps-support))\
          \ because it takes about a minute to run instead of upwards of 30 mins,\
          \ among other things. I have\n\n```\npipe = StableDiffusionPipeline.from_pretrained(\n\
          \t\"CompVis/stable-diffusion-v1-4\", \n    torch_dtype=torch.float16, revision=\"\
          fp16\",\n\tuse_auth_token=True,\n).to(\"mps\")\n```\n\nBut that errors out\
          \ with \n\n```\n0it [00:00, ?it/s]loc(\"mps_add\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/20d6c351-ee94-11ec-bcaf-7247572f23b4/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\"\
          :219:0)): error: input types 'tensor<2x1280xf32>' and 'tensor<*xf16>' are\
          \ not broadcast compatible\nLLVM ERROR: Failed to infer result type(s).\n\
          Abort trap: 6\n/Users/fragmede/miniforge3/envs/ldm/lib/python3.10/multiprocessing/resource_tracker.py:224:\
          \ UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects\
          \ to clean up at shutdown\n  warnings.warn('resource_tracker: There appear\
          \ to be %d '\n```"
        updatedAt: '2022-08-30T15:18:16.920Z'
      numEdits: 7
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - davidpomerenke
    id: 630e288678e898134040561c
    type: comment
  author: fragmede
  content: "I'm pretty sure my `txt2image` is using MPS ([magnusviri's fork](https://github.com/magnusviri/stable-diffusion/tree/apple-silicon-mps-support))\
    \ because it takes about a minute to run instead of upwards of 30 mins, among\
    \ other things. I have\n\n```\npipe = StableDiffusionPipeline.from_pretrained(\n\
    \t\"CompVis/stable-diffusion-v1-4\", \n    torch_dtype=torch.float16, revision=\"\
    fp16\",\n\tuse_auth_token=True,\n).to(\"mps\")\n```\n\nBut that errors out with\
    \ \n\n```\n0it [00:00, ?it/s]loc(\"mps_add\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/20d6c351-ee94-11ec-bcaf-7247572f23b4/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\"\
    :219:0)): error: input types 'tensor<2x1280xf32>' and 'tensor<*xf16>' are not\
    \ broadcast compatible\nLLVM ERROR: Failed to infer result type(s).\nAbort trap:\
    \ 6\n/Users/fragmede/miniforge3/envs/ldm/lib/python3.10/multiprocessing/resource_tracker.py:224:\
    \ UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects\
    \ to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to\
    \ be %d '\n```"
  created_at: 2022-08-30 14:11:02+00:00
  edited: true
  hidden: false
  id: 630e288678e898134040561c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/511a73f80299e84df711beae2ac3922a.svg
      fullname: samson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fragmede
      type: user
    createdAt: '2022-08-30T17:47:18.000Z'
    data:
      edited: false
      editors:
      - fragmede
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/511a73f80299e84df711beae2ac3922a.svg
          fullname: samson
          isHf: false
          isPro: false
          name: fragmede
          type: user
        html: '<p>Figured it out! I opened <a rel="nofollow" href="https://github.com/huggingface/diffusers/pull/278/">a
          PR</a> so hugging face can get my fix to diffusers to get MPS to work.</p>

          '
        raw: Figured it out! I opened [a PR](https://github.com/huggingface/diffusers/pull/278/)
          so hugging face can get my fix to diffusers to get MPS to work.
        updatedAt: '2022-08-30T17:47:18.677Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - coolcloud
        - crazysnail
    id: 630e4d261ef92d4e37a2d1f0
    type: comment
  author: fragmede
  content: Figured it out! I opened [a PR](https://github.com/huggingface/diffusers/pull/278/)
    so hugging face can get my fix to diffusers to get MPS to work.
  created_at: 2022-08-30 16:47:18+00:00
  edited: false
  hidden: false
  id: 630e4d261ef92d4e37a2d1f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fdb9e0d28c40816da7efa7c5b6eefd2c.svg
      fullname: wangxiaoyun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: coolcloud
      type: user
    createdAt: '2022-09-15T07:03:48.000Z'
    data:
      edited: true
      editors:
      - coolcloud
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fdb9e0d28c40816da7efa7c5b6eefd2c.svg
          fullname: wangxiaoyun
          isHf: false
          isPro: false
          name: coolcloud
          type: user
        html: '<blockquote>

          <p>Figured it out! I opened <a rel="nofollow" href="https://github.com/huggingface/diffusers/pull/278/">a
          PR</a> so hugging face can get my fix to diffusers to get MPS to work.</p>

          </blockquote>

          <p>It works after upgrading diffusers.</p>

          <pre><code> pip install -U diffusers

          </code></pre>

          '
        raw: "> Figured it out! I opened [a PR](https://github.com/huggingface/diffusers/pull/278/)\
          \ so hugging face can get my fix to diffusers to get MPS to work.\n\nIt\
          \ works after upgrading diffusers.\n\n```\n pip install -U diffusers\n```"
        updatedAt: '2022-09-15T07:05:09.803Z'
      numEdits: 1
      reactions: []
    id: 6322ce547596f15341e33724
    type: comment
  author: coolcloud
  content: "> Figured it out! I opened [a PR](https://github.com/huggingface/diffusers/pull/278/)\
    \ so hugging face can get my fix to diffusers to get MPS to work.\n\nIt works\
    \ after upgrading diffusers.\n\n```\n pip install -U diffusers\n```"
  created_at: 2022-09-15 06:03:48+00:00
  edited: true
  hidden: false
  id: 6322ce547596f15341e33724
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
      fullname: Pedro Cuenca
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pcuenq
      type: user
    createdAt: '2022-09-15T13:53:39.000Z'
    data:
      status: closed
    id: 63232e6375bf010a73d889b1
    type: status-change
  author: pcuenq
  created_at: 2022-09-15 12:53:39+00:00
  id: 63232e6375bf010a73d889b1
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5512f10021235e6a8f1b638019e176f0.svg
      fullname: happy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: liyang-happy
      type: user
    createdAt: '2023-03-03T17:08:19.000Z'
    data:
      edited: false
      editors:
      - liyang-happy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5512f10021235e6a8f1b638019e176f0.svg
          fullname: happy
          isHf: false
          isPro: false
          name: liyang-happy
          type: user
        html: '<p>torch_dtype=torch.float16<br>remove this, and it works for me</p>

          '
        raw: 'torch_dtype=torch.float16

          remove this, and it works for me'
        updatedAt: '2023-03-03T17:08:19.971Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - ParityError
        - knetwork
        - Jackieeeee
        - crazysnail
    id: 64022983fc948f5b16a45d66
    type: comment
  author: liyang-happy
  content: 'torch_dtype=torch.float16

    remove this, and it works for me'
  created_at: 2023-03-03 17:08:19+00:00
  edited: false
  hidden: false
  id: 64022983fc948f5b16a45d66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/247168e5507f9d4e514b189b77c9fdb9.svg
      fullname: Montessuit
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moltes74
      type: user
    createdAt: '2023-05-11T07:28:01.000Z'
    data:
      edited: false
      editors:
      - Moltes74
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/247168e5507f9d4e514b189b77c9fdb9.svg
          fullname: Montessuit
          isHf: false
          isPro: false
          name: Moltes74
          type: user
        html: '<p>Hey !</p>

          <p>I have the same problem : </p>

          <p>loc("varianceEps"("(mpsFileLoc): /AppleInternal/Library/BuildRoots/a0876c02-1788-11ed-b9c4-96898e02b808/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm":219:0)):
          error: input types ''tensor&lt;1x77x1xf16&gt;'' and ''tensor&lt;1xf32&gt;''
          are not broadcast compatible<br>LLVM ERROR: Failed to infer result type(s).</p>

          <p>But I don''t understand how to fix it ... I''m an architect and I don''t
          have skills with coding ... Can you please develop a bit the method (if
          there is any) ?</p>

          <p>Thanks a lot in advance !</p>

          '
        raw: "Hey !\n\nI have the same problem : \n\nloc(\"varianceEps\"(\"(mpsFileLoc):\
          \ /AppleInternal/Library/BuildRoots/a0876c02-1788-11ed-b9c4-96898e02b808/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\"\
          :219:0)): error: input types 'tensor<1x77x1xf16>' and 'tensor<1xf32>' are\
          \ not broadcast compatible\nLLVM ERROR: Failed to infer result type(s).\n\
          \nBut I don't understand how to fix it ... I'm an architect and I don't\
          \ have skills with coding ... Can you please develop a bit the method (if\
          \ there is any) ?\n\nThanks a lot in advance !"
        updatedAt: '2023-05-11T07:28:01.074Z'
      numEdits: 0
      reactions:
      - count: 7
        reaction: "\U0001F44D"
        users:
        - bartekupartek
        - mxms
        - DocPixel
        - timmal
        - knetwork
        - Jonathanroiz
        - UlyssesHeart
    id: 645c99018ce4443cae6c04ef
    type: comment
  author: Moltes74
  content: "Hey !\n\nI have the same problem : \n\nloc(\"varianceEps\"(\"(mpsFileLoc):\
    \ /AppleInternal/Library/BuildRoots/a0876c02-1788-11ed-b9c4-96898e02b808/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\"\
    :219:0)): error: input types 'tensor<1x77x1xf16>' and 'tensor<1xf32>' are not\
    \ broadcast compatible\nLLVM ERROR: Failed to infer result type(s).\n\nBut I don't\
    \ understand how to fix it ... I'm an architect and I don't have skills with coding\
    \ ... Can you please develop a bit the method (if there is any) ?\n\nThanks a\
    \ lot in advance !"
  created_at: 2023-05-11 06:28:01+00:00
  edited: false
  hidden: false
  id: 645c99018ce4443cae6c04ef
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: CompVis/stable-diffusion-v1-4
repo_type: model
status: closed
target_branch: null
title: Utilize Apple M1 chip causes error (kernel death)
