!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lgleznah
conflicting_files: null
created_at: 2022-09-21 14:06:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a57baab17ad8cb7aa7ef0a12758235b.svg
      fullname: Luis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lgleznah
      type: user
    createdAt: '2022-09-21T15:06:09.000Z'
    data:
      edited: false
      editors:
      - lgleznah
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a57baab17ad8cb7aa7ef0a12758235b.svg
          fullname: Luis
          isHf: false
          isPro: false
          name: lgleznah
          type: user
        html: "<p>Hello everybody!</p>\n<p>I've had the chance of fiddling a little\
          \ bit with Stable Diffusion these days, and let me state the obvious: <strong>it\
          \ is amazing!</strong></p>\n<p>However, I've read the <em>Limitations</em>\
          \ section, and I couldn't help but take a look at the point that says, \"\
          The model does not perform well on more difficult tasks which involve compositionality,\
          \ such as rendering an image corresponding to \u201CA red cube on top of\
          \ a blue sphere\u201D\". Indeed, on all the composition scenes I tried to\
          \ generate, the results weren't as good as the ones obtained with simpler\
          \ scenes.</p>\n<p>However, I think there <strong>might</strong> be a way\
          \ of improving this. The thing is, this model is, as stated in the <a rel=\"\
          nofollow\" href=\"https://github.com/CompVis/stable-diffusion#reference-sampling-script\"\
          >official repo</a>, <em>conditioned on the (non-pooled) text embeddings\
          \ of a CLIP ViT-L/14 text encoder</em>.</p>\n<p>The main issue for me is,\
          \ I don't think this CLIP model is the most optimal way of training Stable\
          \ Diffusion. You see, CLIP was originally trained for zero-shot image classification\
          \ by learning to pair images with their descriptions, an approach that maybe\
          \ is not the best one for text-to-image generation. </p>\n<p>And I heavily\
          \ suspect that we could get better compositionality results, by training/fine-tuning\
          \ this CLIP model on top of a dataset more focused on complex images whose\
          \ descriptions detail all the object relationships. Either this, or we would\
          \ need to maybe modify this CLIP model for it to more accurately reflect\
          \ scene composition.</p>\n<p>TL;DR: I believe vanilla CLIP is not optimal\
          \ for text-to-image generation since it was not trained for reflecting scene\
          \ composition specifically. Either training/fine-tuning or modifying CLIP\
          \ could be good approaches to tackle this problem.</p>\n<p>If anybody thinks\
          \ this is a good approach and wants to give it a go, I will be more than\
          \ willing to give a helping hand. Unfortunately, we don't have that many\
          \ resources in my laboratory (just around 5 GPUs...).</p>\n<p>Cheers everybody!\
          \ I hope my comments were helpful \U0001F607</p>\n"
        raw: "Hello everybody!\r\n\r\nI've had the chance of fiddling a little bit\
          \ with Stable Diffusion these days, and let me state the obvious: **it is\
          \ amazing!**\r\n\r\nHowever, I've read the *Limitations* section, and I\
          \ couldn't help but take a look at the point that says, \"The model does\
          \ not perform well on more difficult tasks which involve compositionality,\
          \ such as rendering an image corresponding to \u201CA red cube on top of\
          \ a blue sphere\u201D\". Indeed, on all the composition scenes I tried to\
          \ generate, the results weren't as good as the ones obtained with simpler\
          \ scenes.\r\n\r\nHowever, I think there **might** be a way of improving\
          \ this. The thing is, this model is, as stated in the [official repo](https://github.com/CompVis/stable-diffusion#reference-sampling-script),\
          \ *conditioned on the (non-pooled) text embeddings of a CLIP ViT-L/14 text\
          \ encoder*.\r\n\r\nThe main issue for me is, I don't think this CLIP model\
          \ is the most optimal way of training Stable Diffusion. You see, CLIP was\
          \ originally trained for zero-shot image classification by learning to pair\
          \ images with their descriptions, an approach that maybe is not the best\
          \ one for text-to-image generation. \r\n\r\nAnd I heavily suspect that we\
          \ could get better compositionality results, by training/fine-tuning this\
          \ CLIP model on top of a dataset more focused on complex images whose descriptions\
          \ detail all the object relationships. Either this, or we would need to\
          \ maybe modify this CLIP model for it to more accurately reflect scene composition.\r\
          \n\r\nTL;DR: I believe vanilla CLIP is not optimal for text-to-image generation\
          \ since it was not trained for reflecting scene composition specifically.\
          \ Either training/fine-tuning or modifying CLIP could be good approaches\
          \ to tackle this problem.\r\n\r\nIf anybody thinks this is a good approach\
          \ and wants to give it a go, I will be more than willing to give a helping\
          \ hand. Unfortunately, we don't have that many resources in my laboratory\
          \ (just around 5 GPUs...).\r\n\r\nCheers everybody! I hope my comments were\
          \ helpful \U0001F607"
        updatedAt: '2022-09-21T15:06:09.136Z'
      numEdits: 0
      reactions: []
    id: 632b2861eafe8eca5e971553
    type: comment
  author: lgleznah
  content: "Hello everybody!\r\n\r\nI've had the chance of fiddling a little bit with\
    \ Stable Diffusion these days, and let me state the obvious: **it is amazing!**\r\
    \n\r\nHowever, I've read the *Limitations* section, and I couldn't help but take\
    \ a look at the point that says, \"The model does not perform well on more difficult\
    \ tasks which involve compositionality, such as rendering an image corresponding\
    \ to \u201CA red cube on top of a blue sphere\u201D\". Indeed, on all the composition\
    \ scenes I tried to generate, the results weren't as good as the ones obtained\
    \ with simpler scenes.\r\n\r\nHowever, I think there **might** be a way of improving\
    \ this. The thing is, this model is, as stated in the [official repo](https://github.com/CompVis/stable-diffusion#reference-sampling-script),\
    \ *conditioned on the (non-pooled) text embeddings of a CLIP ViT-L/14 text encoder*.\r\
    \n\r\nThe main issue for me is, I don't think this CLIP model is the most optimal\
    \ way of training Stable Diffusion. You see, CLIP was originally trained for zero-shot\
    \ image classification by learning to pair images with their descriptions, an\
    \ approach that maybe is not the best one for text-to-image generation. \r\n\r\
    \nAnd I heavily suspect that we could get better compositionality results, by\
    \ training/fine-tuning this CLIP model on top of a dataset more focused on complex\
    \ images whose descriptions detail all the object relationships. Either this,\
    \ or we would need to maybe modify this CLIP model for it to more accurately reflect\
    \ scene composition.\r\n\r\nTL;DR: I believe vanilla CLIP is not optimal for text-to-image\
    \ generation since it was not trained for reflecting scene composition specifically.\
    \ Either training/fine-tuning or modifying CLIP could be good approaches to tackle\
    \ this problem.\r\n\r\nIf anybody thinks this is a good approach and wants to\
    \ give it a go, I will be more than willing to give a helping hand. Unfortunately,\
    \ we don't have that many resources in my laboratory (just around 5 GPUs...).\r\
    \n\r\nCheers everybody! I hope my comments were helpful \U0001F607"
  created_at: 2022-09-21 14:06:09+00:00
  edited: false
  hidden: false
  id: 632b2861eafe8eca5e971553
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/01fce1618b56808c97d98baf2232a584.svg
      fullname: J. Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JWNoctis
      type: user
    createdAt: '2022-09-22T02:58:39.000Z'
    data:
      edited: true
      editors:
      - JWNoctis
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/01fce1618b56808c97d98baf2232a584.svg
          fullname: J. Wu
          isHf: false
          isPro: false
          name: JWNoctis
          type: user
        html: '<p>I''m not well-informed in these matters, but I think that''s basically
          a large part of what they did with Imagen, with positive results.</p>

          <p>Reduced compositionality, and other related things like not reproducing
          legible text from prompt (which has been a known if unreliable capability
          in not-necessarily-larger models of this kind) in a publically available
          model also reduced opportunity for abuse, for good or ill.</p>

          '
        raw: 'I''m not well-informed in these matters, but I think that''s basically
          a large part of what they did with Imagen, with positive results.


          Reduced compositionality, and other related things like not reproducing
          legible text from prompt (which has been a known if unreliable capability
          in not-necessarily-larger models of this kind) in a publically available
          model also reduced opportunity for abuse, for good or ill.'
        updatedAt: '2022-09-22T04:31:26.260Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - lgleznah
    id: 632bcf5feafe8eca5e9c3850
    type: comment
  author: JWNoctis
  content: 'I''m not well-informed in these matters, but I think that''s basically
    a large part of what they did with Imagen, with positive results.


    Reduced compositionality, and other related things like not reproducing legible
    text from prompt (which has been a known if unreliable capability in not-necessarily-larger
    models of this kind) in a publically available model also reduced opportunity
    for abuse, for good or ill.'
  created_at: 2022-09-22 01:58:39+00:00
  edited: true
  hidden: false
  id: 632bcf5feafe8eca5e9c3850
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 62
repo_id: CompVis/stable-diffusion-v1-4
repo_type: model
status: open
target_branch: null
title: A way of improving compositionality?
