!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wass-grass
conflicting_files: null
created_at: 2022-08-27 15:01:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ebd38f38c6ab96dc9ac50fae8f3abad.svg
      fullname: wass
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wass-grass
      type: user
    createdAt: '2022-08-27T16:01:20.000Z'
    data:
      edited: false
      editors:
      - wass-grass
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ebd38f38c6ab96dc9ac50fae8f3abad.svg
          fullname: wass
          isHf: false
          isPro: false
          name: wass-grass
          type: user
        html: '<p>So what''s the minimal requirement to run this model ?</p>

          '
        raw: So what's the minimal requirement to run this model ?
        updatedAt: '2022-08-27T16:01:20.924Z'
      numEdits: 0
      reactions: []
    id: 630a3fd0b8d7b388931dfb6f
    type: comment
  author: wass-grass
  content: So what's the minimal requirement to run this model ?
  created_at: 2022-08-27 15:01:20+00:00
  edited: false
  hidden: false
  id: 630a3fd0b8d7b388931dfb6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ebd38f38c6ab96dc9ac50fae8f3abad.svg
      fullname: wass
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wass-grass
      type: user
    createdAt: '2022-08-27T16:01:56.000Z'
    data:
      edited: false
      editors:
      - wass-grass
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ebd38f38c6ab96dc9ac50fae8f3abad.svg
          fullname: wass
          isHf: false
          isPro: false
          name: wass-grass
          type: user
        html: '<p>I tried with a 4GB GPU and got a :<br>RuntimeError: CUDA out of
          memory. Tried to allocate 50.00 MiB (GPU 0; 4.00 GiB total capacity; 3.43
          GiB already allocated; 0 bytes free; 3.50 GiB reserved in total by PyTorch)
          If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb
          to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>:/</p>

          '
        raw: 'I tried with a 4GB GPU and got a :

          RuntimeError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 4.00
          GiB total capacity; 3.43 GiB already allocated; 0 bytes free; 3.50 GiB reserved
          in total by PyTorch) If reserved memory is >> allocated memory try setting
          max_split_size_mb to avoid fragmentation.  See documentation for Memory
          Management and PYTORCH_CUDA_ALLOC_CONF


          :/'
        updatedAt: '2022-08-27T16:01:56.078Z'
      numEdits: 0
      reactions: []
    id: 630a3ff4a58e1be42ebaa639
    type: comment
  author: wass-grass
  content: 'I tried with a 4GB GPU and got a :

    RuntimeError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 4.00 GiB
    total capacity; 3.43 GiB already allocated; 0 bytes free; 3.50 GiB reserved in
    total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb
    to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


    :/'
  created_at: 2022-08-27 15:01:56+00:00
  edited: false
  hidden: false
  id: 630a3ff4a58e1be42ebaa639
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
      fullname: Loreto Parisi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: loretoparisi
      type: user
    createdAt: '2022-08-27T16:21:42.000Z'
    data:
      edited: true
      editors:
      - loretoparisi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
          fullname: Loreto Parisi
          isHf: false
          isPro: false
          name: loretoparisi
          type: user
        html: "<p>I had the same error some times in Google Colab (free not plus/Pro)\
          \ with Tesla T4, and I was not able to setup the <code>PYTORCH_CUDA_ALLOC_CONF</code>\
          \ in any way. The only solution I have found was to use some memory check\
          \ and clean up script:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> torch <span class=\"hljs-keyword\">import</span>\
          \ cuda\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\
          \ function_\">get_less_used_gpu</span>(<span class=\"hljs-params\">gpus=<span\
          \ class=\"hljs-literal\">None</span>, debug=<span class=\"hljs-literal\"\
          >False</span></span>):\n    <span class=\"hljs-string\">\"\"\"Inspect cached/reserved\
          \ and allocated memory on specified gpus and return the id of the less used\
          \ device\"\"\"</span>\n    <span class=\"hljs-keyword\">if</span> gpus <span\
          \ class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n\
          \        warn = <span class=\"hljs-string\">'Falling back to default: all\
          \ gpus'</span>\n        gpus = <span class=\"hljs-built_in\">range</span>(cuda.device_count())\n\
          \    <span class=\"hljs-keyword\">elif</span> <span class=\"hljs-built_in\"\
          >isinstance</span>(gpus, <span class=\"hljs-built_in\">str</span>):\n  \
          \      gpus = [<span class=\"hljs-built_in\">int</span>(el) <span class=\"\
          hljs-keyword\">for</span> el <span class=\"hljs-keyword\">in</span> gpus.split(<span\
          \ class=\"hljs-string\">','</span>)]\n\n    <span class=\"hljs-comment\"\
          ># check gpus arg VS available gpus</span>\n    sys_gpus = <span class=\"\
          hljs-built_in\">list</span>(<span class=\"hljs-built_in\">range</span>(cuda.device_count()))\n\
          \    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\"\
          >len</span>(gpus) &gt; <span class=\"hljs-built_in\">len</span>(sys_gpus):\n\
          \        gpus = sys_gpus\n        warn = <span class=\"hljs-string\">f'WARNING:\
          \ Specified <span class=\"hljs-subst\">{<span class=\"hljs-built_in\">len</span>(gpus)}</span>\
          \ gpus, but only <span class=\"hljs-subst\">{cuda.device_count()}</span>\
          \ available. Falling back to default: all gpus.\\nIDs:\\t<span class=\"\
          hljs-subst\">{<span class=\"hljs-built_in\">list</span>(gpus)}</span>'</span>\n\
          \    <span class=\"hljs-keyword\">elif</span> <span class=\"hljs-built_in\"\
          >set</span>(gpus).difference(sys_gpus):\n        <span class=\"hljs-comment\"\
          ># take correctly specified and add as much bad specifications as unused\
          \ system gpus</span>\n        available_gpus = <span class=\"hljs-built_in\"\
          >set</span>(gpus).intersection(sys_gpus)\n        unavailable_gpus = <span\
          \ class=\"hljs-built_in\">set</span>(gpus).difference(sys_gpus)\n      \
          \  unused_gpus = <span class=\"hljs-built_in\">set</span>(sys_gpus).difference(gpus)\n\
          \        gpus = <span class=\"hljs-built_in\">list</span>(available_gpus)\
          \ + <span class=\"hljs-built_in\">list</span>(unused_gpus)[:<span class=\"\
          hljs-built_in\">len</span>(unavailable_gpus)]\n        warn = <span class=\"\
          hljs-string\">f'GPU ids <span class=\"hljs-subst\">{unavailable_gpus}</span>\
          \ not available. Falling back to <span class=\"hljs-subst\">{<span class=\"\
          hljs-built_in\">len</span>(gpus)}</span> device(s).\\nIDs:\\t<span class=\"\
          hljs-subst\">{<span class=\"hljs-built_in\">list</span>(gpus)}</span>'</span>\n\
          \n    cur_allocated_mem = {}\n    cur_cached_mem = {}\n    max_allocated_mem\
          \ = {}\n    max_cached_mem = {}\n    <span class=\"hljs-keyword\">for</span>\
          \ i <span class=\"hljs-keyword\">in</span> gpus:\n        cur_allocated_mem[i]\
          \ = cuda.memory_allocated(i)\n        cur_cached_mem[i] = cuda.memory_reserved(i)\n\
          \        max_allocated_mem[i] = cuda.max_memory_allocated(i)\n        max_cached_mem[i]\
          \ = cuda.max_memory_reserved(i)\n    min_allocated = <span class=\"hljs-built_in\"\
          >min</span>(cur_allocated_mem, key=cur_allocated_mem.get)\n    <span class=\"\
          hljs-keyword\">if</span> debug:\n        <span class=\"hljs-built_in\">print</span>(warn)\n\
          \        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >'Current allocated memory:'</span>, {<span class=\"hljs-string\">f'cuda:<span\
          \ class=\"hljs-subst\">{k}</span>'</span>: v <span class=\"hljs-keyword\"\
          >for</span> k, v <span class=\"hljs-keyword\">in</span> cur_allocated_mem.items()})\n\
          \        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >'Current reserved memory:'</span>, {<span class=\"hljs-string\">f'cuda:<span\
          \ class=\"hljs-subst\">{k}</span>'</span>: v <span class=\"hljs-keyword\"\
          >for</span> k, v <span class=\"hljs-keyword\">in</span> cur_cached_mem.items()})\n\
          \        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >'Maximum allocated memory:'</span>, {<span class=\"hljs-string\">f'cuda:<span\
          \ class=\"hljs-subst\">{k}</span>'</span>: v <span class=\"hljs-keyword\"\
          >for</span> k, v <span class=\"hljs-keyword\">in</span> max_allocated_mem.items()})\n\
          \        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >'Maximum reserved memory:'</span>, {<span class=\"hljs-string\">f'cuda:<span\
          \ class=\"hljs-subst\">{k}</span>'</span>: v <span class=\"hljs-keyword\"\
          >for</span> k, v <span class=\"hljs-keyword\">in</span> max_cached_mem.items()})\n\
          \        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >'Suggested GPU:'</span>, min_allocated)\n    <span class=\"hljs-keyword\"\
          >return</span> min_allocated\n\n\n<span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">free_memory</span>(<span class=\"\
          hljs-params\">to_delete: <span class=\"hljs-built_in\">list</span>, debug=<span\
          \ class=\"hljs-literal\">False</span></span>):\n    <span class=\"hljs-keyword\"\
          >import</span> gc\n    <span class=\"hljs-keyword\">import</span> inspect\n\
          \    calling_namespace = inspect.currentframe().f_back\n    <span class=\"\
          hljs-keyword\">if</span> debug:\n        <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">'Before:'</span>)\n        get_less_used_gpu(debug=<span\
          \ class=\"hljs-literal\">True</span>)\n\n    <span class=\"hljs-keyword\"\
          >if</span> to_delete:\n      <span class=\"hljs-keyword\">for</span> _var\
          \ <span class=\"hljs-keyword\">in</span> to_delete:\n          calling_namespace.f_locals.pop(_var,\
          \ <span class=\"hljs-literal\">None</span>)\n          gc.collect()\n  \
          \        cuda.empty_cache()\n      <span class=\"hljs-keyword\">else</span>:\n\
          \        gc.collect()\n        cuda.empty_cache()\n    <span class=\"hljs-keyword\"\
          >if</span> debug:\n        <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">'After:'</span>)\n        get_less_used_gpu(debug=<span\
          \ class=\"hljs-literal\">True</span>)\n</code></pre>\n<p>What I did, after\
          \ running the pipelines e.g.</p>\n<pre><code class=\"language-python\">lms\
          \ = LMSDiscreteScheduler(\n    beta_start=<span class=\"hljs-number\">0.00085</span>,\
          \ \n    beta_end=<span class=\"hljs-number\">0.012</span>, \n    beta_schedule=<span\
          \ class=\"hljs-string\">\"scaled_linear\"</span>\n)\npipe = MyStableDiffusionPipeline.from_pretrained(\n\
          \    <span class=\"hljs-string\">\"CompVis/stable-diffusion-v1-3\"</span>,\
          \ \n    scheduler=lms,\n    <span class=\"hljs-comment\">#float16 is cuda\
          \ only</span>\n    use_auth_token=<span class=\"hljs-literal\">True</span>\n\
          ).to(device)\n</code></pre>\n<p>was to immediately cleanup resources just\
          \ after the inference (e.g. <code>images = pipe([prompt]*samples)</code>,\
          \  so in another cell I did like</p>\n<pre><code class=\"language-python\"\
          >to_delete = (lms, pipe)\nfree_memory(to_delete, debug=<span class=\"hljs-literal\"\
          >True</span>)\n</code></pre>\n<p>this saved me to restart the kernel and\
          \ losing the work done at least, hope it help someone else!</p>\n"
        raw: "I had the same error some times in Google Colab (free not plus/Pro)\
          \ with Tesla T4, and I was not able to setup the `PYTORCH_CUDA_ALLOC_CONF`\
          \ in any way. The only solution I have found was to use some memory check\
          \ and clean up script:\n\n```python\nfrom torch import cuda\ndef get_less_used_gpu(gpus=None,\
          \ debug=False):\n    \"\"\"Inspect cached/reserved and allocated memory\
          \ on specified gpus and return the id of the less used device\"\"\"\n  \
          \  if gpus is None:\n        warn = 'Falling back to default: all gpus'\n\
          \        gpus = range(cuda.device_count())\n    elif isinstance(gpus, str):\n\
          \        gpus = [int(el) for el in gpus.split(',')]\n\n    # check gpus\
          \ arg VS available gpus\n    sys_gpus = list(range(cuda.device_count()))\n\
          \    if len(gpus) > len(sys_gpus):\n        gpus = sys_gpus\n        warn\
          \ = f'WARNING: Specified {len(gpus)} gpus, but only {cuda.device_count()}\
          \ available. Falling back to default: all gpus.\\nIDs:\\t{list(gpus)}'\n\
          \    elif set(gpus).difference(sys_gpus):\n        # take correctly specified\
          \ and add as much bad specifications as unused system gpus\n        available_gpus\
          \ = set(gpus).intersection(sys_gpus)\n        unavailable_gpus = set(gpus).difference(sys_gpus)\n\
          \        unused_gpus = set(sys_gpus).difference(gpus)\n        gpus = list(available_gpus)\
          \ + list(unused_gpus)[:len(unavailable_gpus)]\n        warn = f'GPU ids\
          \ {unavailable_gpus} not available. Falling back to {len(gpus)} device(s).\\\
          nIDs:\\t{list(gpus)}'\n\n    cur_allocated_mem = {}\n    cur_cached_mem\
          \ = {}\n    max_allocated_mem = {}\n    max_cached_mem = {}\n    for i in\
          \ gpus:\n        cur_allocated_mem[i] = cuda.memory_allocated(i)\n     \
          \   cur_cached_mem[i] = cuda.memory_reserved(i)\n        max_allocated_mem[i]\
          \ = cuda.max_memory_allocated(i)\n        max_cached_mem[i] = cuda.max_memory_reserved(i)\n\
          \    min_allocated = min(cur_allocated_mem, key=cur_allocated_mem.get)\n\
          \    if debug:\n        print(warn)\n        print('Current allocated memory:',\
          \ {f'cuda:{k}': v for k, v in cur_allocated_mem.items()})\n        print('Current\
          \ reserved memory:', {f'cuda:{k}': v for k, v in cur_cached_mem.items()})\n\
          \        print('Maximum allocated memory:', {f'cuda:{k}': v for k, v in\
          \ max_allocated_mem.items()})\n        print('Maximum reserved memory:',\
          \ {f'cuda:{k}': v for k, v in max_cached_mem.items()})\n        print('Suggested\
          \ GPU:', min_allocated)\n    return min_allocated\n\n\ndef free_memory(to_delete:\
          \ list, debug=False):\n    import gc\n    import inspect\n    calling_namespace\
          \ = inspect.currentframe().f_back\n    if debug:\n        print('Before:')\n\
          \        get_less_used_gpu(debug=True)\n\n    if to_delete:\n      for _var\
          \ in to_delete:\n          calling_namespace.f_locals.pop(_var, None)\n\
          \          gc.collect()\n          cuda.empty_cache()\n      else:\n   \
          \     gc.collect()\n        cuda.empty_cache()\n    if debug:\n        print('After:')\n\
          \        get_less_used_gpu(debug=True)\n```\n\nWhat I did, after running\
          \ the pipelines e.g.\n\n```python\nlms = LMSDiscreteScheduler(\n    beta_start=0.00085,\
          \ \n    beta_end=0.012, \n    beta_schedule=\"scaled_linear\"\n)\npipe =\
          \ MyStableDiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-3\"\
          , \n    scheduler=lms,\n    #float16 is cuda only\n    use_auth_token=True\n\
          ).to(device)\n```\n\nwas to immediately cleanup resources just after the\
          \ inference (e.g. `images = pipe([prompt]*samples)`,  so in another cell\
          \ I did like\n\n```python\nto_delete = (lms, pipe)\nfree_memory(to_delete,\
          \ debug=True)\n```\n\nthis saved me to restart the kernel and losing the\
          \ work done at least, hope it help someone else!"
        updatedAt: '2022-08-27T16:22:46.638Z'
      numEdits: 1
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - wass-grass
        - majortal
        - patrickvonplaten
        - zuleo
    id: 630a44961d1650ec2f8e44ed
    type: comment
  author: loretoparisi
  content: "I had the same error some times in Google Colab (free not plus/Pro) with\
    \ Tesla T4, and I was not able to setup the `PYTORCH_CUDA_ALLOC_CONF` in any way.\
    \ The only solution I have found was to use some memory check and clean up script:\n\
    \n```python\nfrom torch import cuda\ndef get_less_used_gpu(gpus=None, debug=False):\n\
    \    \"\"\"Inspect cached/reserved and allocated memory on specified gpus and\
    \ return the id of the less used device\"\"\"\n    if gpus is None:\n        warn\
    \ = 'Falling back to default: all gpus'\n        gpus = range(cuda.device_count())\n\
    \    elif isinstance(gpus, str):\n        gpus = [int(el) for el in gpus.split(',')]\n\
    \n    # check gpus arg VS available gpus\n    sys_gpus = list(range(cuda.device_count()))\n\
    \    if len(gpus) > len(sys_gpus):\n        gpus = sys_gpus\n        warn = f'WARNING:\
    \ Specified {len(gpus)} gpus, but only {cuda.device_count()} available. Falling\
    \ back to default: all gpus.\\nIDs:\\t{list(gpus)}'\n    elif set(gpus).difference(sys_gpus):\n\
    \        # take correctly specified and add as much bad specifications as unused\
    \ system gpus\n        available_gpus = set(gpus).intersection(sys_gpus)\n   \
    \     unavailable_gpus = set(gpus).difference(sys_gpus)\n        unused_gpus =\
    \ set(sys_gpus).difference(gpus)\n        gpus = list(available_gpus) + list(unused_gpus)[:len(unavailable_gpus)]\n\
    \        warn = f'GPU ids {unavailable_gpus} not available. Falling back to {len(gpus)}\
    \ device(s).\\nIDs:\\t{list(gpus)}'\n\n    cur_allocated_mem = {}\n    cur_cached_mem\
    \ = {}\n    max_allocated_mem = {}\n    max_cached_mem = {}\n    for i in gpus:\n\
    \        cur_allocated_mem[i] = cuda.memory_allocated(i)\n        cur_cached_mem[i]\
    \ = cuda.memory_reserved(i)\n        max_allocated_mem[i] = cuda.max_memory_allocated(i)\n\
    \        max_cached_mem[i] = cuda.max_memory_reserved(i)\n    min_allocated =\
    \ min(cur_allocated_mem, key=cur_allocated_mem.get)\n    if debug:\n        print(warn)\n\
    \        print('Current allocated memory:', {f'cuda:{k}': v for k, v in cur_allocated_mem.items()})\n\
    \        print('Current reserved memory:', {f'cuda:{k}': v for k, v in cur_cached_mem.items()})\n\
    \        print('Maximum allocated memory:', {f'cuda:{k}': v for k, v in max_allocated_mem.items()})\n\
    \        print('Maximum reserved memory:', {f'cuda:{k}': v for k, v in max_cached_mem.items()})\n\
    \        print('Suggested GPU:', min_allocated)\n    return min_allocated\n\n\n\
    def free_memory(to_delete: list, debug=False):\n    import gc\n    import inspect\n\
    \    calling_namespace = inspect.currentframe().f_back\n    if debug:\n      \
    \  print('Before:')\n        get_less_used_gpu(debug=True)\n\n    if to_delete:\n\
    \      for _var in to_delete:\n          calling_namespace.f_locals.pop(_var,\
    \ None)\n          gc.collect()\n          cuda.empty_cache()\n      else:\n \
    \       gc.collect()\n        cuda.empty_cache()\n    if debug:\n        print('After:')\n\
    \        get_less_used_gpu(debug=True)\n```\n\nWhat I did, after running the pipelines\
    \ e.g.\n\n```python\nlms = LMSDiscreteScheduler(\n    beta_start=0.00085, \n \
    \   beta_end=0.012, \n    beta_schedule=\"scaled_linear\"\n)\npipe = MyStableDiffusionPipeline.from_pretrained(\n\
    \    \"CompVis/stable-diffusion-v1-3\", \n    scheduler=lms,\n    #float16 is\
    \ cuda only\n    use_auth_token=True\n).to(device)\n```\n\nwas to immediately\
    \ cleanup resources just after the inference (e.g. `images = pipe([prompt]*samples)`,\
    \  so in another cell I did like\n\n```python\nto_delete = (lms, pipe)\nfree_memory(to_delete,\
    \ debug=True)\n```\n\nthis saved me to restart the kernel and losing the work\
    \ done at least, hope it help someone else!"
  created_at: 2022-08-27 15:21:42+00:00
  edited: true
  hidden: false
  id: 630a44961d1650ec2f8e44ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ff60201c2af69b94f94287/flc8mFRrST35P3DFEOufY.jpeg?w=200&h=200&f=face
      fullname: Boi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: boi-doingthings
      type: user
    createdAt: '2022-09-08T20:39:51.000Z'
    data:
      edited: false
      editors:
      - boi-doingthings
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ff60201c2af69b94f94287/flc8mFRrST35P3DFEOufY.jpeg?w=200&h=200&f=face
          fullname: Boi
          isHf: false
          isPro: false
          name: boi-doingthings
          type: user
        html: '<p>You tried on 4, I am not able to get 1.4 running on a 8GB card,</p>

          <p><code>RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB
          (GPU 0; 7.77 GiB total capacity; 5.36 GiB already allocated; 351.25 MiB
          free; 5.38 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt;
          allocated memory try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF </code>
          </p>

          <p>Were you able to run atleast any single inference before deleting and
          freeing up the memory?</p>

          '
        raw: "You tried on 4, I am not able to get 1.4 running on a 8GB card,\n\n\
          `RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0;\
          \ 7.77 GiB total capacity; 5.36 GiB already allocated; 351.25 MiB free;\
          \ 5.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated\
          \ memory try setting max_split_size_mb to avoid fragmentation.  See documentation\
          \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n` \n\nWere you able\
          \ to run atleast any single inference before deleting and freeing up the\
          \ memory?"
        updatedAt: '2022-09-08T20:39:51.639Z'
      numEdits: 0
      reactions: []
    id: 631a5317fac58c9c81626315
    type: comment
  author: boi-doingthings
  content: "You tried on 4, I am not able to get 1.4 running on a 8GB card,\n\n`RuntimeError:\
    \ CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 7.77 GiB total capacity;\
    \ 5.36 GiB already allocated; 351.25 MiB free; 5.38 GiB reserved in total by PyTorch)\
    \ If reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
    ` \n\nWere you able to run atleast any single inference before deleting and freeing\
    \ up the memory?"
  created_at: 2022-09-08 19:39:51+00:00
  edited: false
  hidden: false
  id: 631a5317fac58c9c81626315
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ebd38f38c6ab96dc9ac50fae8f3abad.svg
      fullname: wass
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wass-grass
      type: user
    createdAt: '2022-09-08T21:04:42.000Z'
    data:
      edited: false
      editors:
      - wass-grass
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ebd38f38c6ab96dc9ac50fae8f3abad.svg
          fullname: wass
          isHf: false
          isPro: false
          name: wass-grass
          type: user
        html: '<p>Apparently it should be possible.</p>

          <p>From the readme:<br>Note: If you are limited by GPU memory and have less
          than 10GB of GPU RAM available, please make sure to load the StableDiffusionPipeline
          in float16 precision instead of the default float32 precision as done above.
          You can do so by telling diffusers to expect the weights to be in float16
          precision:</p>

          <p>I haven''t tried it since I could play it on Google collab with 16gb
          of vram wich were almost all used.</p>

          '
        raw: 'Apparently it should be possible.


          From the readme:

          Note: If you are limited by GPU memory and have less than 10GB of GPU RAM
          available, please make sure to load the StableDiffusionPipeline in float16
          precision instead of the default float32 precision as done above. You can
          do so by telling diffusers to expect the weights to be in float16 precision:


          I haven''t tried it since I could play it on Google collab with 16gb of
          vram wich were almost all used.'
        updatedAt: '2022-09-08T21:04:42.826Z'
      numEdits: 0
      reactions: []
    id: 631a58eaee2a8b515d483d89
    type: comment
  author: wass-grass
  content: 'Apparently it should be possible.


    From the readme:

    Note: If you are limited by GPU memory and have less than 10GB of GPU RAM available,
    please make sure to load the StableDiffusionPipeline in float16 precision instead
    of the default float32 precision as done above. You can do so by telling diffusers
    to expect the weights to be in float16 precision:


    I haven''t tried it since I could play it on Google collab with 16gb of vram wich
    were almost all used.'
  created_at: 2022-09-08 20:04:42+00:00
  edited: false
  hidden: false
  id: 631a58eaee2a8b515d483d89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ff60201c2af69b94f94287/flc8mFRrST35P3DFEOufY.jpeg?w=200&h=200&f=face
      fullname: Boi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: boi-doingthings
      type: user
    createdAt: '2022-09-09T10:13:37.000Z'
    data:
      edited: false
      editors:
      - boi-doingthings
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ff60201c2af69b94f94287/flc8mFRrST35P3DFEOufY.jpeg?w=200&h=200&f=face
          fullname: Boi
          isHf: false
          isPro: false
          name: boi-doingthings
          type: user
        html: "<p>Exactly what I did later on posting this. Downloaded the weights\
          \ in 16 bit precision, smaller model and runs easy. Ofcourse, a bit compro\
          \ on quality but yeah good enough for learning.<br>Thanks <span data-props=\"\
          {&quot;user&quot;:&quot;wass-grass&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/wass-grass\">@<span class=\"underline\"\
          >wass-grass</span></a></span>\n\n\t</span></span></p>\n"
        raw: 'Exactly what I did later on posting this. Downloaded the weights in
          16 bit precision, smaller model and runs easy. Ofcourse, a bit compro on
          quality but yeah good enough for learning.

          Thanks @wass-grass'
        updatedAt: '2022-09-09T10:13:37.760Z'
      numEdits: 0
      reactions: []
    id: 631b11d14aa47d6afbda302a
    type: comment
  author: boi-doingthings
  content: 'Exactly what I did later on posting this. Downloaded the weights in 16
    bit precision, smaller model and runs easy. Ofcourse, a bit compro on quality
    but yeah good enough for learning.

    Thanks @wass-grass'
  created_at: 2022-09-09 09:13:37+00:00
  edited: false
  hidden: false
  id: 631b11d14aa47d6afbda302a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6032802e1f993496bc14d9e3/w6hr-DEQot4VVkoyRIBiy.png?w=200&h=200&f=face
      fullname: Omar Sanseviero
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: osanseviero
      type: user
    createdAt: '2022-10-24T09:00:22.000Z'
    data:
      status: closed
    id: 635654262d14fcd7d837ec1c
    type: status-change
  author: osanseviero
  created_at: 2022-10-24 08:00:22+00:00
  id: 635654262d14fcd7d837ec1c
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/89edf0aed5e2faf9b3d0f6e71678f8c6.svg
      fullname: TOMAS MONOPOLI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tm95mon
      type: user
    createdAt: '2023-04-25T08:02:55.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/89edf0aed5e2faf9b3d0f6e71678f8c6.svg
          fullname: TOMAS MONOPOLI
          isHf: false
          isPro: false
          name: tm95mon
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-04-25T08:05:11.347Z'
      numEdits: 0
      reactions: []
    id: 6447892f3e498d669187cb4d
    type: comment
  author: tm95mon
  content: This comment has been hidden
  created_at: 2023-04-25 07:02:55+00:00
  edited: true
  hidden: true
  id: 6447892f3e498d669187cb4d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: CompVis/stable-diffusion-v1-4
repo_type: model
status: closed
target_branch: null
title: Minimal CUDA GPU requirements
