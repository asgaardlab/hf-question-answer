!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Devilmoon
conflicting_files: null
created_at: 2022-08-28 20:38:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4d2e1d8818d043f5056e47b5fa00fde.svg
      fullname: Luca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Devilmoon
      type: user
    createdAt: '2022-08-28T21:38:15.000Z'
    data:
      edited: false
      editors:
      - Devilmoon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4d2e1d8818d043f5056e47b5fa00fde.svg
          fullname: Luca
          isHf: false
          isPro: false
          name: Devilmoon
          type: user
        html: '<p>Given that some of the acceptable goals of using this model include</p>

          <pre><code>Safe deployment of models which have the potential to generate
          harmful content.

          Probing and understanding the limitations and biases of generative models.

          </code></pre>

          <p>Is there a way to remove the NSFW filter which blocks any image the model
          itself deems not safe?<br>How is it possible to probe the model or understand
          how to prevent harmful content if the model itself blocks out random images
          without giving me the option to understand what''s going on under the hood?</p>

          '
        raw: "Given that some of the acceptable goals of using this model include\r\
          \n```\r\nSafe deployment of models which have the potential to generate\
          \ harmful content.\r\nProbing and understanding the limitations and biases\
          \ of generative models.\r\n```\r\nIs there a way to remove the NSFW filter\
          \ which blocks any image the model itself deems not safe?\r\nHow is it possible\
          \ to probe the model or understand how to prevent harmful content if the\
          \ model itself blocks out random images without giving me the option to\
          \ understand what's going on under the hood?"
        updatedAt: '2022-08-28T21:38:15.511Z'
      numEdits: 0
      reactions: []
    id: 630be047cd26ad7f60dbe513
    type: comment
  author: Devilmoon
  content: "Given that some of the acceptable goals of using this model include\r\n\
    ```\r\nSafe deployment of models which have the potential to generate harmful\
    \ content.\r\nProbing and understanding the limitations and biases of generative\
    \ models.\r\n```\r\nIs there a way to remove the NSFW filter which blocks any\
    \ image the model itself deems not safe?\r\nHow is it possible to probe the model\
    \ or understand how to prevent harmful content if the model itself blocks out\
    \ random images without giving me the option to understand what's going on under\
    \ the hood?"
  created_at: 2022-08-28 20:38:15+00:00
  edited: false
  hidden: false
  id: 630be047cd26ad7f60dbe513
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a872e4d2f01216debf40daa446e90ab9.svg
      fullname: Alexander Tarashansky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: atarashansky
      type: user
    createdAt: '2022-08-28T21:45:45.000Z'
    data:
      edited: true
      editors:
      - atarashansky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a872e4d2f01216debf40daa446e90ab9.svg
          fullname: Alexander Tarashansky
          isHf: false
          isPro: false
          name: atarashansky
          type: user
        html: '<p>It''s very possible - it''s literally the equivalent of removing
          a couple lines of code.</p>

          <p>EDIT: I''m being deliberately cryptic here, because the authors have
          clearly chosen to implement a safety feature. The beauty of open source
          is that anyone can take a monkeywrench to the code and get it to behave
          in whatever manner they desire. But seeing as this discussion is on the
          authors'' platform, then I believe the discussions must respect their desire
          to create safe content.</p>

          '
        raw: 'It''s very possible - it''s literally the equivalent of removing a couple
          lines of code.


          EDIT: I''m being deliberately cryptic here, because the authors have clearly
          chosen to implement a safety feature. The beauty of open source is that
          anyone can take a monkeywrench to the code and get it to behave in whatever
          manner they desire. But seeing as this discussion is on the authors'' platform,
          then I believe the discussions must respect their desire to create safe
          content.'
        updatedAt: '2022-08-28T21:49:15.277Z'
      numEdits: 2
      reactions:
      - count: 11
        reaction: "\u2764\uFE0F"
        users:
        - anton-l
        - loretoparisi
        - LadyInShadow
        - rkroon
        - shalom
        - DavidGomez00
        - multimodalart
        - Oaooakw
        - RadicalDegen69
        - Daw432NW
        - macream
    id: 630be209973a51d211624d0d
    type: comment
  author: atarashansky
  content: 'It''s very possible - it''s literally the equivalent of removing a couple
    lines of code.


    EDIT: I''m being deliberately cryptic here, because the authors have clearly chosen
    to implement a safety feature. The beauty of open source is that anyone can take
    a monkeywrench to the code and get it to behave in whatever manner they desire.
    But seeing as this discussion is on the authors'' platform, then I believe the
    discussions must respect their desire to create safe content.'
  created_at: 2022-08-28 20:45:45+00:00
  edited: true
  hidden: false
  id: 630be209973a51d211624d0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8e55a6340c04a480c3beb580bc30a852.svg
      fullname: Daniel Wit
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leniad
      type: user
    createdAt: '2022-09-04T13:16:06.000Z'
    data:
      edited: false
      editors:
      - leniad
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8e55a6340c04a480c3beb580bc30a852.svg
          fullname: Daniel Wit
          isHf: false
          isPro: false
          name: leniad
          type: user
        html: '<p>you need to modify safety_checker.py in the stable_diffusion pipeline.
          it is literally one line of code. I don''t know how the safety_checker analyse
          the picture but it is definitely too strict. I often have had a warning
          about NSFW content even though my prompts were mostly about geometrics,
          fractals etc. definitely PG-friendly.</p>

          '
        raw: you need to modify safety_checker.py in the stable_diffusion pipeline.
          it is literally one line of code. I don't know how the safety_checker analyse
          the picture but it is definitely too strict. I often have had a warning
          about NSFW content even though my prompts were mostly about geometrics,
          fractals etc. definitely PG-friendly.
        updatedAt: '2022-09-04T13:16:06.746Z'
      numEdits: 0
      reactions: []
    id: 6314a516362e3e95ea4eef31
    type: comment
  author: leniad
  content: you need to modify safety_checker.py in the stable_diffusion pipeline.
    it is literally one line of code. I don't know how the safety_checker analyse
    the picture but it is definitely too strict. I often have had a warning about
    NSFW content even though my prompts were mostly about geometrics, fractals etc.
    definitely PG-friendly.
  created_at: 2022-09-04 12:16:06+00:00
  edited: false
  hidden: false
  id: 6314a516362e3e95ea4eef31
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5793f0ca25e0e89650e3b8be88fa0a2b.svg
      fullname: Thales Tozatto
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ttozatto
      type: user
    createdAt: '2022-09-05T02:22:51.000Z'
    data:
      edited: false
      editors:
      - ttozatto
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5793f0ca25e0e89650e3b8be88fa0a2b.svg
          fullname: Thales Tozatto
          isHf: false
          isPro: false
          name: ttozatto
          type: user
        html: '<p>I''m getting "Potential NSFW content was detected in one or more
          images. A black image will be returned instead. Try again with a different
          prompt and/or seed." even using the example prompt: "a photograph of an
          astronaut riding a horse"</p>

          '
        raw: 'I''m getting "Potential NSFW content was detected in one or more images.
          A black image will be returned instead. Try again with a different prompt
          and/or seed." even using the example prompt: "a photograph of an astronaut
          riding a horse"'
        updatedAt: '2022-09-05T02:22:51.728Z'
      numEdits: 0
      reactions: []
    id: 63155d7b8be84e1ab414d14e
    type: comment
  author: ttozatto
  content: 'I''m getting "Potential NSFW content was detected in one or more images.
    A black image will be returned instead. Try again with a different prompt and/or
    seed." even using the example prompt: "a photograph of an astronaut riding a horse"'
  created_at: 2022-09-05 01:22:51+00:00
  edited: false
  hidden: false
  id: 63155d7b8be84e1ab414d14e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
      fullname: Loreto Parisi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: loretoparisi
      type: user
    createdAt: '2022-09-08T13:33:50.000Z'
    data:
      edited: false
      editors:
      - loretoparisi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
          fullname: Loreto Parisi
          isHf: false
          isPro: false
          name: loretoparisi
          type: user
        html: '<p>I think it would be good also to have details about the output of
          the CLIP <code>safety_checker </code>:</p>

          <pre><code class="language-python">[{<span class="hljs-string">''special_scores''</span>:
          {<span class="hljs-number">0</span>: -<span class="hljs-number">0.072</span>,
          <span class="hljs-number">1</span>: -<span class="hljs-number">0.053</span>,
          <span class="hljs-number">2</span>: -<span class="hljs-number">0.085</span>},
          <span class="hljs-string">''special_care''</span>: [], <span class="hljs-string">''concept_scores''</span>:
          {<span class="hljs-number">0</span>: -<span class="hljs-number">0.059</span>,
          <span class="hljs-number">1</span>: -<span class="hljs-number">0.042</span>,
          <span class="hljs-number">2</span>: -<span class="hljs-number">0.054</span>,
          <span class="hljs-number">3</span>: -<span class="hljs-number">0.064</span>,
          <span class="hljs-number">4</span>: -<span class="hljs-number">0.05</span>,
          <span class="hljs-number">5</span>: -<span class="hljs-number">0.035</span>,
          <span class="hljs-number">6</span>: -<span class="hljs-number">0.045</span>,
          <span class="hljs-number">7</span>: -<span class="hljs-number">0.057</span>,
          <span class="hljs-number">8</span>: -<span class="hljs-number">0.038</span>,
          <span class="hljs-number">9</span>: -<span class="hljs-number">0.091</span>,
          <span class="hljs-number">10</span>: -<span class="hljs-number">0.033</span>,
          <span class="hljs-number">11</span>: -<span class="hljs-number">0.034</span>,
          <span class="hljs-number">12</span>: -<span class="hljs-number">0.048</span>,
          <span class="hljs-number">13</span>: -<span class="hljs-number">0.075</span>,
          <span class="hljs-number">14</span>: -<span class="hljs-number">0.097</span>,
          <span class="hljs-number">15</span>: -<span class="hljs-number">0.078</span>,
          <span class="hljs-number">16</span>: -<span class="hljs-number">0.098</span>},
          <span class="hljs-string">''bad_concepts''</span>: []}, {<span class="hljs-string">''special_scores''</span>:
          {<span class="hljs-number">0</span>: -<span class="hljs-number">0.067</span>,
          <span class="hljs-number">1</span>: -<span class="hljs-number">0.046</span>,
          <span class="hljs-number">2</span>: -<span class="hljs-number">0.082</span>},
          <span class="hljs-string">''special_care''</span>: [], <span class="hljs-string">''concept_scores''</span>:
          {<span class="hljs-number">0</span>: -<span class="hljs-number">0.057</span>,
          <span class="hljs-number">1</span>: -<span class="hljs-number">0.024</span>,
          <span class="hljs-number">2</span>: -<span class="hljs-number">0.044</span>,
          <span class="hljs-number">3</span>: -<span class="hljs-number">0.048</span>,
          <span class="hljs-number">4</span>: -<span class="hljs-number">0.037</span>,
          <span class="hljs-number">5</span>: -<span class="hljs-number">0.023</span>,
          <span class="hljs-number">6</span>: -<span class="hljs-number">0.026</span>,
          <span class="hljs-number">7</span>: -<span class="hljs-number">0.041</span>,
          <span class="hljs-number">8</span>: -<span class="hljs-number">0.036</span>,
          <span class="hljs-number">9</span>: -<span class="hljs-number">0.073</span>,
          <span class="hljs-number">10</span>: -<span class="hljs-number">0.04</span>,
          <span class="hljs-number">11</span>: -<span class="hljs-number">0.033</span>,
          <span class="hljs-number">12</span>: -<span class="hljs-number">0.032</span>,
          <span class="hljs-number">13</span>: -<span class="hljs-number">0.062</span>,
          <span class="hljs-number">14</span>: -<span class="hljs-number">0.084</span>,
          <span class="hljs-number">15</span>: -<span class="hljs-number">0.06</span>,
          <span class="hljs-number">16</span>: -<span class="hljs-number">0.08</span>},
          <span class="hljs-string">''bad_concepts''</span>: []}]

          </code></pre>

          <p>and a way to control from the pipeline the threshold to have a stronger/weaker
          nsfw filter, with a minimum admitted value to ensure to avoid very bad concepts.</p>

          '
        raw: 'I think it would be good also to have details about the output of the
          CLIP `safety_checker `:


          ```python

          [{''special_scores'': {0: -0.072, 1: -0.053, 2: -0.085}, ''special_care'':
          [], ''concept_scores'': {0: -0.059, 1: -0.042, 2: -0.054, 3: -0.064, 4:
          -0.05, 5: -0.035, 6: -0.045, 7: -0.057, 8: -0.038, 9: -0.091, 10: -0.033,
          11: -0.034, 12: -0.048, 13: -0.075, 14: -0.097, 15: -0.078, 16: -0.098},
          ''bad_concepts'': []}, {''special_scores'': {0: -0.067, 1: -0.046, 2: -0.082},
          ''special_care'': [], ''concept_scores'': {0: -0.057, 1: -0.024, 2: -0.044,
          3: -0.048, 4: -0.037, 5: -0.023, 6: -0.026, 7: -0.041, 8: -0.036, 9: -0.073,
          10: -0.04, 11: -0.033, 12: -0.032, 13: -0.062, 14: -0.084, 15: -0.06, 16:
          -0.08}, ''bad_concepts'': []}]

          ```


          and a way to control from the pipeline the threshold to have a stronger/weaker
          nsfw filter, with a minimum admitted value to ensure to avoid very bad concepts.'
        updatedAt: '2022-09-08T13:33:50.165Z'
      numEdits: 0
      reactions: []
    id: 6319ef3e1f2193ba170ceb57
    type: comment
  author: loretoparisi
  content: 'I think it would be good also to have details about the output of the
    CLIP `safety_checker `:


    ```python

    [{''special_scores'': {0: -0.072, 1: -0.053, 2: -0.085}, ''special_care'': [],
    ''concept_scores'': {0: -0.059, 1: -0.042, 2: -0.054, 3: -0.064, 4: -0.05, 5:
    -0.035, 6: -0.045, 7: -0.057, 8: -0.038, 9: -0.091, 10: -0.033, 11: -0.034, 12:
    -0.048, 13: -0.075, 14: -0.097, 15: -0.078, 16: -0.098}, ''bad_concepts'': []},
    {''special_scores'': {0: -0.067, 1: -0.046, 2: -0.082}, ''special_care'': [],
    ''concept_scores'': {0: -0.057, 1: -0.024, 2: -0.044, 3: -0.048, 4: -0.037, 5:
    -0.023, 6: -0.026, 7: -0.041, 8: -0.036, 9: -0.073, 10: -0.04, 11: -0.033, 12:
    -0.032, 13: -0.062, 14: -0.084, 15: -0.06, 16: -0.08}, ''bad_concepts'': []}]

    ```


    and a way to control from the pipeline the threshold to have a stronger/weaker
    nsfw filter, with a minimum admitted value to ensure to avoid very bad concepts.'
  created_at: 2022-09-08 12:33:50+00:00
  edited: false
  hidden: false
  id: 6319ef3e1f2193ba170ceb57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/900ddb0e6868e0691660b2a9e12ea7b0.svg
      fullname: Arman Singhal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheProlifer
      type: user
    createdAt: '2022-12-25T22:44:03.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/900ddb0e6868e0691660b2a9e12ea7b0.svg
          fullname: Arman Singhal
          isHf: false
          isPro: false
          name: TheProlifer
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-12-25T22:46:45.389Z'
      numEdits: 0
      reactions: []
    id: 63a8d2330046b663fa5a77f0
    type: comment
  author: TheProlifer
  content: This comment has been hidden
  created_at: 2022-12-25 22:44:03+00:00
  edited: true
  hidden: true
  id: 63a8d2330046b663fa5a77f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a8d35d2e05ca32e34fb178/u7Pvb96Xi2iRTe3AFarhj.jpeg?w=200&h=200&f=face
      fullname: Spongebob Sqaurepants
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ChaoticQubit
      type: user
    createdAt: '2022-12-25T23:11:14.000Z'
    data:
      edited: false
      editors:
      - ChaoticQubit
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a8d35d2e05ca32e34fb178/u7Pvb96Xi2iRTe3AFarhj.jpeg?w=200&h=200&f=face
          fullname: Spongebob Sqaurepants
          isHf: false
          isPro: false
          name: ChaoticQubit
          type: user
        html: '<p>I removed the for loop from the safety_checker.py file, which was
          checking for NSFW content and returning a black image. This worked for me
          as it showed the NSFW warning for almost all the prompts I was trying. E.g.,
          "An image of the sun wearing sunglasses." I commented out the code as shown
          below.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1672009865877-63a8d35d2e05ca32e34fb178.png"><img
          alt="Screenshot 2022-12-25 at 5.43.14 PM.png" src="https://cdn-uploads.huggingface.co/production/uploads/1672009865877-63a8d35d2e05ca32e34fb178.png"></a></p>

          '
        raw: 'I removed the for loop from the safety_checker.py file, which was checking
          for NSFW content and returning a black image. This worked for me as it showed
          the NSFW warning for almost all the prompts I was trying. E.g., "An image
          of the sun wearing sunglasses." I commented out the code as shown below.


          ![Screenshot 2022-12-25 at 5.43.14 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1672009865877-63a8d35d2e05ca32e34fb178.png)'
        updatedAt: '2022-12-25T23:11:14.330Z'
      numEdits: 0
      reactions: []
    id: 63a8d89229ebe611bc7ccf22
    type: comment
  author: ChaoticQubit
  content: 'I removed the for loop from the safety_checker.py file, which was checking
    for NSFW content and returning a black image. This worked for me as it showed
    the NSFW warning for almost all the prompts I was trying. E.g., "An image of the
    sun wearing sunglasses." I commented out the code as shown below.


    ![Screenshot 2022-12-25 at 5.43.14 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1672009865877-63a8d35d2e05ca32e34fb178.png)'
  created_at: 2022-12-25 23:11:14+00:00
  edited: false
  hidden: false
  id: 63a8d89229ebe611bc7ccf22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/uS7XFR_C4FwDuD88RobxF.png?w=200&h=200&f=face
      fullname: Umbra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: UmbraS
      type: user
    createdAt: '2023-10-08T22:37:23.000Z'
    data:
      edited: false
      editors:
      - UmbraS
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9427371025085449
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/uS7XFR_C4FwDuD88RobxF.png?w=200&h=200&f=face
          fullname: Umbra
          isHf: false
          isPro: true
          name: UmbraS
          type: user
        html: '<blockquote>

          <p>I removed the for loop from the safety_checker.py file, which was checking
          for NSFW content and returning a black image. This worked for me as it showed
          the NSFW warning for almost all the prompts I was trying. E.g., "An image
          of the sun wearing sunglasses." I commented out the code as shown below.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1672009865877-63a8d35d2e05ca32e34fb178.png"><img
          alt="Screenshot 2022-12-25 at 5.43.14 PM.png" src="https://cdn-uploads.huggingface.co/production/uploads/1672009865877-63a8d35d2e05ca32e34fb178.png"></a></p>

          </blockquote>

          <p>How were you able to modify the repo? I am trying to use the Inference
          Endpoint but see no way of modifying the files. </p>

          <p>Thanks for the help.</p>

          '
        raw: "> I removed the for loop from the safety_checker.py file, which was\
          \ checking for NSFW content and returning a black image. This worked for\
          \ me as it showed the NSFW warning for almost all the prompts I was trying.\
          \ E.g., \"An image of the sun wearing sunglasses.\" I commented out the\
          \ code as shown below.\n> \n> ![Screenshot 2022-12-25 at 5.43.14 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1672009865877-63a8d35d2e05ca32e34fb178.png)\n\
          \nHow were you able to modify the repo? I am trying to use the Inference\
          \ Endpoint but see no way of modifying the files. \n\nThanks for the help."
        updatedAt: '2023-10-08T22:37:23.324Z'
      numEdits: 0
      reactions: []
    id: 65232f230f935fa8fd631acd
    type: comment
  author: UmbraS
  content: "> I removed the for loop from the safety_checker.py file, which was checking\
    \ for NSFW content and returning a black image. This worked for me as it showed\
    \ the NSFW warning for almost all the prompts I was trying. E.g., \"An image of\
    \ the sun wearing sunglasses.\" I commented out the code as shown below.\n> \n\
    > ![Screenshot 2022-12-25 at 5.43.14 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1672009865877-63a8d35d2e05ca32e34fb178.png)\n\
    \nHow were you able to modify the repo? I am trying to use the Inference Endpoint\
    \ but see no way of modifying the files. \n\nThanks for the help."
  created_at: 2023-10-08 21:37:23+00:00
  edited: false
  hidden: false
  id: 65232f230f935fa8fd631acd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84bfcddc93b1a8360843539818c66622.svg
      fullname: Chris Schults
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sheng-si
      type: user
    createdAt: '2023-10-17T14:20:05.000Z'
    data:
      edited: false
      editors:
      - sheng-si
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5855258703231812
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84bfcddc93b1a8360843539818c66622.svg
          fullname: Chris Schults
          isHf: false
          isPro: false
          name: sheng-si
          type: user
        html: "<p>in safety_checker.pyc found this part of text.<br>Which text do\
          \ I hev to remove to get rid of th NSFW remark</p>\n<hr>\n<p>G   s\x02 \
          \  \x1C z8StableDiffusionSafetyChecker.forward..z\u2018Potential NSFW content\
          \ was detected in one or more images. A black image will be returned instead.\
          \ Try again with a different prompt and/or seed.)\x16r\e   r\x1F   r\x11\
          \   r#   \xDA\x03cpu\xDA\x05float\xDA\x05numpyr\"   \xDA\x05shape\xDA\x05\
          ranger0   r%   \xDA\x04item\xDA\x05round\xDA\x06appendr$   \xDA\tenumerate\xDA\
          \x02np\xDA\x05zeros\xDA\x03any\xDA\x06logger\xDA warning)\x12r&amp;   \xDA\
          <br>clip_input\xDA\x06images\xDA<br>pooled_outputr</p>\n"
        raw: "in safety_checker.pyc found this part of text.\nWhich text do I hev\
          \ to remove to get rid of th NSFW remark\n---\n<listcomp>G   s\x02   \x1C\
          \ z8StableDiffusionSafetyChecker.forward.<locals>.<listcomp>z\u2018Potential\
          \ NSFW content was detected in one or more images. A black image will be\
          \ returned instead. Try again with a different prompt and/or seed.)\x16\
          r\e   r\x1F   r\x11   r#   \xDA\x03cpu\xDA\x05float\xDA\x05numpyr\"   \xDA\
          \x05shape\xDA\x05ranger0   r%   \xDA\x04item\xDA\x05round\xDA\x06appendr$\
          \   \xDA\tenumerate\xDA\x02np\xDA\x05zeros\xDA\x03any\xDA\x06logger\xDA\
          \ warning)\x12r&   \xDA\nclip_input\xDA\x06images\xDA\npooled_outputr"
        updatedAt: '2023-10-17T14:20:05.707Z'
      numEdits: 0
      reactions: []
    id: 652e9815902fe76a6d06b7ab
    type: comment
  author: sheng-si
  content: "in safety_checker.pyc found this part of text.\nWhich text do I hev to\
    \ remove to get rid of th NSFW remark\n---\n<listcomp>G   s\x02   \x1C z8StableDiffusionSafetyChecker.forward.<locals>.<listcomp>z\u2018\
    Potential NSFW content was detected in one or more images. A black image will\
    \ be returned instead. Try again with a different prompt and/or seed.)\x16r\e\
    \   r\x1F   r\x11   r#   \xDA\x03cpu\xDA\x05float\xDA\x05numpyr\"   \xDA\x05shape\xDA\
    \x05ranger0   r%   \xDA\x04item\xDA\x05round\xDA\x06appendr$   \xDA\tenumerate\xDA\
    \x02np\xDA\x05zeros\xDA\x03any\xDA\x06logger\xDA warning)\x12r&   \xDA\nclip_input\xDA\
    \x06images\xDA\npooled_outputr"
  created_at: 2023-10-17 13:20:05+00:00
  edited: false
  hidden: false
  id: 652e9815902fe76a6d06b7ab
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: CompVis/stable-diffusion-v1-4
repo_type: model
status: open
target_branch: null
title: Is there a way to remove the NSFW filter?
