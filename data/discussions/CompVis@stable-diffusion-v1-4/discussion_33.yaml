!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xalex
conflicting_files: null
created_at: 2022-09-01 18:43:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xalex
      type: user
    createdAt: '2022-09-01T19:43:17.000Z'
    data:
      edited: false
      editors:
      - xalex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: xalex
          type: user
        html: '<p>Did someone experiment with how to stay close to one image while
          changing other things in the image?</p>

          <p>I think of, for example, generating an image of a person and then trying
          to change what''s in the background or even what the person is holding in
          the hand without changing the face too much. Exploring the latent space
          by interpolating to seeds near the original seed works good to find random
          new features, but changing the prompt, e.g., by adding "... reading a book"
          to the description of the person can change the whole image even when starting
          with the same seed.<br>Using img2img doesn''t work in many cases as the
          person in the image may need to move for example and img2img seems to optimize
          for detecting the same features at the same places as in the seed image,
          which would be the person who did not move (e.g. sitting down to read a
          book).</p>

          <p>I wonder if one could somehow extract features about the generated person,
          which then can be incorporated for new images, like "a person [i](having
          features with seed X)[/i] sitting down to read a book" where the features
          part is extracted from an image by detecting for example the facial features
          and generating a seed that is understood by the network and leads it to
          generate similar faces.</p>

          '
        raw: "Did someone experiment with how to stay close to one image while changing\
          \ other things in the image?\r\n\r\nI think of, for example, generating\
          \ an image of a person and then trying to change what's in the background\
          \ or even what the person is holding in the hand without changing the face\
          \ too much. Exploring the latent space by interpolating to seeds near the\
          \ original seed works good to find random new features, but changing the\
          \ prompt, e.g., by adding \"... reading a book\" to the description of the\
          \ person can change the whole image even when starting with the same seed.\
          \ \r\nUsing img2img doesn't work in many cases as the person in the image\
          \ may need to move for example and img2img seems to optimize for detecting\
          \ the same features at the same places as in the seed image, which would\
          \ be the person who did not move (e.g. sitting down to read a book).\r\n\
          \r\nI wonder if one could somehow extract features about the generated person,\
          \ which then can be incorporated for new images, like \"a person [i](having\
          \ features with seed X)[/i] sitting down to read a book\" where the features\
          \ part is extracted from an image by detecting for example the facial features\
          \ and generating a seed that is understood by the network and leads it to\
          \ generate similar faces."
        updatedAt: '2022-09-01T19:43:17.858Z'
      numEdits: 0
      reactions: []
    id: 63110b5564939fabc00c24f3
    type: comment
  author: xalex
  content: "Did someone experiment with how to stay close to one image while changing\
    \ other things in the image?\r\n\r\nI think of, for example, generating an image\
    \ of a person and then trying to change what's in the background or even what\
    \ the person is holding in the hand without changing the face too much. Exploring\
    \ the latent space by interpolating to seeds near the original seed works good\
    \ to find random new features, but changing the prompt, e.g., by adding \"...\
    \ reading a book\" to the description of the person can change the whole image\
    \ even when starting with the same seed. \r\nUsing img2img doesn't work in many\
    \ cases as the person in the image may need to move for example and img2img seems\
    \ to optimize for detecting the same features at the same places as in the seed\
    \ image, which would be the person who did not move (e.g. sitting down to read\
    \ a book).\r\n\r\nI wonder if one could somehow extract features about the generated\
    \ person, which then can be incorporated for new images, like \"a person [i](having\
    \ features with seed X)[/i] sitting down to read a book\" where the features part\
    \ is extracted from an image by detecting for example the facial features and\
    \ generating a seed that is understood by the network and leads it to generate\
    \ similar faces."
  created_at: 2022-09-01 18:43:17+00:00
  edited: false
  hidden: false
  id: 63110b5564939fabc00c24f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xalex
      type: user
    createdAt: '2022-09-01T19:43:43.000Z'
    data:
      from: Best way to stay close to a main image while changing surroundings?
      to: Best way to stay close to a main image while changing details, poses, surroundings,
        etc.?
    id: 63110b6fbc152fa3e8113ba5
    type: title-change
  author: xalex
  created_at: 2022-09-01 18:43:43+00:00
  id: 63110b6fbc152fa3e8113ba5
  new_title: Best way to stay close to a main image while changing details, poses,
    surroundings, etc.?
  old_title: Best way to stay close to a main image while changing surroundings?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2022-09-01T21:29:08.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>That's a cool use case!<br>Could maybe a combination of image segmentation:\
          \ <a href=\"https://huggingface.co/tasks/image-segmentation\">https://huggingface.co/tasks/image-segmentation</a><br>and\
          \ the inpaint pipeline:<br><a rel=\"nofollow\" href=\"https://github.com/huggingface/diffusers#in-painting-using-stable-diffusion\"\
          >https://github.com/huggingface/diffusers#in-painting-using-stable-diffusion</a><br>\
          \ work here?</p>\n<p>Also cc <span data-props=\"{&quot;user&quot;:&quot;multimodalart&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/multimodalart\"\
          >@<span class=\"underline\">multimodalart</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;valhalla&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/valhalla\">@<span class=\"\
          underline\">valhalla</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;pcuenq&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/pcuenq\">@<span class=\"underline\">pcuenq</span></a></span>\n\
          \n\t</span></span></p>\n"
        raw: "That's a cool use case!\nCould maybe a combination of image segmentation:\
          \ https://huggingface.co/tasks/image-segmentation\nand the inpaint pipeline:\n\
          https://github.com/huggingface/diffusers#in-painting-using-stable-diffusion\n\
          \ work here?\n\nAlso cc @multimodalart @valhalla @pcuenq"
        updatedAt: '2022-09-01T21:29:08.648Z'
      numEdits: 0
      reactions: []
    id: 63112424631a69165c09a020
    type: comment
  author: patrickvonplaten
  content: "That's a cool use case!\nCould maybe a combination of image segmentation:\
    \ https://huggingface.co/tasks/image-segmentation\nand the inpaint pipeline:\n\
    https://github.com/huggingface/diffusers#in-painting-using-stable-diffusion\n\
    \ work here?\n\nAlso cc @multimodalart @valhalla @pcuenq"
  created_at: 2022-09-01 20:29:08+00:00
  edited: false
  hidden: false
  id: 63112424631a69165c09a020
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xalex
      type: user
    createdAt: '2022-09-01T21:46:35.000Z'
    data:
      edited: true
      editors:
      - xalex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: xalex
          type: user
        html: '<p>I need to test the Inpainting Pipeline. The problem is that img2img
          and probably the inpainting pipeline start with an image, which they change,
          but I would like to preserve features like "red hair" which should be independent
          from the pose of the person. Segmenting and moving around can surely work
          for some images (maybe one could like to generate a few persons and then
          place them in a group image and "diffuse them together").</p>

          <p>The question is, can one extract some of the actual information from
          the latent space? Let''s say large eyes and red hair and certain facial
          features and then generate random images with these features, but, for example,
          totally different poses of the person with large eyes and red hair?</p>

          <p><a href="/CompVis/stable-diffusion-v1-4/discussions/27">#27</a> looks
          like it could helpful for this. Let''s say I generated a random image for
          "female person" and got these features, then could produce a prompt that
          may reproduce this images by extracting more information than the initial
          prompt contained. This could not describe the facial features in detail,
          but something like "red hair" can probably be extracted. For the other features,
          one I guess the initial noise would need to be modified.</p>

          '
        raw: 'I need to test the Inpainting Pipeline. The problem is that img2img
          and probably the inpainting pipeline start with an image, which they change,
          but I would like to preserve features like "red hair" which should be independent
          from the pose of the person. Segmenting and moving around can surely work
          for some images (maybe one could like to generate a few persons and then
          place them in a group image and "diffuse them together").


          The question is, can one extract some of the actual information from the
          latent space? Let''s say large eyes and red hair and certain facial features
          and then generate random images with these features, but, for example, totally
          different poses of the person with large eyes and red hair?


          #27 looks like it could helpful for this. Let''s say I generated a random
          image for "female person" and got these features, then could produce a prompt
          that may reproduce this images by extracting more information than the initial
          prompt contained. This could not describe the facial features in detail,
          but something like "red hair" can probably be extracted. For the other features,
          one I guess the initial noise would need to be modified.'
        updatedAt: '2022-09-01T21:47:04.445Z'
      numEdits: 1
      reactions: []
    id: 6311283b07a7682790269a33
    type: comment
  author: xalex
  content: 'I need to test the Inpainting Pipeline. The problem is that img2img and
    probably the inpainting pipeline start with an image, which they change, but I
    would like to preserve features like "red hair" which should be independent from
    the pose of the person. Segmenting and moving around can surely work for some
    images (maybe one could like to generate a few persons and then place them in
    a group image and "diffuse them together").


    The question is, can one extract some of the actual information from the latent
    space? Let''s say large eyes and red hair and certain facial features and then
    generate random images with these features, but, for example, totally different
    poses of the person with large eyes and red hair?


    #27 looks like it could helpful for this. Let''s say I generated a random image
    for "female person" and got these features, then could produce a prompt that may
    reproduce this images by extracting more information than the initial prompt contained.
    This could not describe the facial features in detail, but something like "red
    hair" can probably be extracted. For the other features, one I guess the initial
    noise would need to be modified.'
  created_at: 2022-09-01 20:46:35+00:00
  edited: true
  hidden: false
  id: 6311283b07a7682790269a33
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xalex
      type: user
    createdAt: '2022-09-01T22:18:51.000Z'
    data:
      edited: true
      editors:
      - xalex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: xalex
          type: user
        html: '<p>Currently I use a really simple approach for generating similar
          images:</p>

          <ul>

          <li>Generate a random initialization init1 and keep it fixed</li>

          <li>In each Iteration generate a random initialization init2</li>

          <li>Create n steps interpolating between init1 and init2</li>

          </ul>

          <p>For good images one can for example use <code>slerp(t, init1, init2)</code>
          with <code>t</code> from <code>linspace(0, 0.3, 6)</code>.</p>

          <p>Init1:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1662070255096-63040e844ec2dfa82a585ad0.jpeg"><img
          alt="1.jpg" src="https://cdn-uploads.huggingface.co/production/uploads/1662070255096-63040e844ec2dfa82a585ad0.jpeg"></a></p>

          <p>Some example generated using seeds interpolated between <code>init1</code>
          and different random initizations <code>init2</code> with <code>t</code>
          in [0.05:0.3]:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1662070269263-63040e844ec2dfa82a585ad0.jpeg"><img
          alt="2.jpg" src="https://cdn-uploads.huggingface.co/production/uploads/1662070269263-63040e844ec2dfa82a585ad0.jpeg"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1662070272448-63040e844ec2dfa82a585ad0.jpeg"><img
          alt="3.jpg" src="https://cdn-uploads.huggingface.co/production/uploads/1662070272448-63040e844ec2dfa82a585ad0.jpeg"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1662070274581-63040e844ec2dfa82a585ad0.jpeg"><img
          alt="4.jpg" src="https://cdn-uploads.huggingface.co/production/uploads/1662070274581-63040e844ec2dfa82a585ad0.jpeg"></a></p>

          <p>The diagonal riverside is always preserved quite good, but I would like
          to have something that helps to keep the bird looking the same.<br>In these
          images one could segment the first bird and insert it into the other images,
          but maybe I would like to have the bird flying in one image and then one
          needs to get the information "the bird is green" to the new position of
          the bird.</p>

          '
        raw: 'Currently I use a really simple approach for generating similar images:

          - Generate a random initialization init1 and keep it fixed

          - In each Iteration generate a random initialization init2

          - Create n steps interpolating between init1 and init2


          For good images one can for example use `slerp(t, init1, init2)` with `t`
          from `linspace(0, 0.3, 6)`.


          Init1:

          ![1.jpg](https://cdn-uploads.huggingface.co/production/uploads/1662070255096-63040e844ec2dfa82a585ad0.jpeg)


          Some example generated using seeds interpolated between `init1` and different
          random initizations `init2` with `t` in [0.05:0.3]:

          ![2.jpg](https://cdn-uploads.huggingface.co/production/uploads/1662070269263-63040e844ec2dfa82a585ad0.jpeg)

          ![3.jpg](https://cdn-uploads.huggingface.co/production/uploads/1662070272448-63040e844ec2dfa82a585ad0.jpeg)

          ![4.jpg](https://cdn-uploads.huggingface.co/production/uploads/1662070274581-63040e844ec2dfa82a585ad0.jpeg)


          The diagonal riverside is always preserved quite good, but I would like
          to have something that helps to keep the bird looking the same.

          In these images one could segment the first bird and insert it into the
          other images, but maybe I would like to have the bird flying in one image
          and then one needs to get the information "the bird is green" to the new
          position of the bird.'
        updatedAt: '2022-09-01T22:21:10.202Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Heisenberg08
    id: 63112fcb31257261d20d6ef2
    type: comment
  author: xalex
  content: 'Currently I use a really simple approach for generating similar images:

    - Generate a random initialization init1 and keep it fixed

    - In each Iteration generate a random initialization init2

    - Create n steps interpolating between init1 and init2


    For good images one can for example use `slerp(t, init1, init2)` with `t` from
    `linspace(0, 0.3, 6)`.


    Init1:

    ![1.jpg](https://cdn-uploads.huggingface.co/production/uploads/1662070255096-63040e844ec2dfa82a585ad0.jpeg)


    Some example generated using seeds interpolated between `init1` and different
    random initizations `init2` with `t` in [0.05:0.3]:

    ![2.jpg](https://cdn-uploads.huggingface.co/production/uploads/1662070269263-63040e844ec2dfa82a585ad0.jpeg)

    ![3.jpg](https://cdn-uploads.huggingface.co/production/uploads/1662070272448-63040e844ec2dfa82a585ad0.jpeg)

    ![4.jpg](https://cdn-uploads.huggingface.co/production/uploads/1662070274581-63040e844ec2dfa82a585ad0.jpeg)


    The diagonal riverside is always preserved quite good, but I would like to have
    something that helps to keep the bird looking the same.

    In these images one could segment the first bird and insert it into the other
    images, but maybe I would like to have the bird flying in one image and then one
    needs to get the information "the bird is green" to the new position of the bird.'
  created_at: 2022-09-01 21:18:51+00:00
  edited: true
  hidden: false
  id: 63112fcb31257261d20d6ef2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xalex
      type: user
    createdAt: '2022-09-02T14:33:30.000Z'
    data:
      edited: false
      editors:
      - xalex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: xalex
          type: user
        html: '<p>It may also be useful to be able to weight the attention. When I
          describe a character and then add "in front of the Eiffel tower" I get a
          whole new image in which the character is much smaller (and looking different)
          to make room for the Eiffel tower.<br>It would be useful when the network
          would first look for the characters (and detect the same shape) and then
          give attention to the Eiffel tower in the pixels which rendered as a single
          color background before.</p>

          <p>This could also be an use-case for the inpainting pipeline but this would
          require a good segmentation and possibly the character had before a more
          complex background and not a single color and then segmentation is a hard
          task as well.</p>

          '
        raw: 'It may also be useful to be able to weight the attention. When I describe
          a character and then add "in front of the Eiffel tower" I get a whole new
          image in which the character is much smaller (and looking different) to
          make room for the Eiffel tower.

          It would be useful when the network would first look for the characters
          (and detect the same shape) and then give attention to the Eiffel tower
          in the pixels which rendered as a single color background before.


          This could also be an use-case for the inpainting pipeline but this would
          require a good segmentation and possibly the character had before a more
          complex background and not a single color and then segmentation is a hard
          task as well.'
        updatedAt: '2022-09-02T14:33:30.596Z'
      numEdits: 0
      reactions: []
    id: 6312143a8231e4718b96ba6f
    type: comment
  author: xalex
  content: 'It may also be useful to be able to weight the attention. When I describe
    a character and then add "in front of the Eiffel tower" I get a whole new image
    in which the character is much smaller (and looking different) to make room for
    the Eiffel tower.

    It would be useful when the network would first look for the characters (and detect
    the same shape) and then give attention to the Eiffel tower in the pixels which
    rendered as a single color background before.


    This could also be an use-case for the inpainting pipeline but this would require
    a good segmentation and possibly the character had before a more complex background
    and not a single color and then segmentation is a hard task as well.'
  created_at: 2022-09-02 13:33:30+00:00
  edited: false
  hidden: false
  id: 6312143a8231e4718b96ba6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xalex
      type: user
    createdAt: '2022-09-10T13:00:35.000Z'
    data:
      edited: false
      editors:
      - xalex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: xalex
          type: user
        html: '<p>Experimenting with it, it looks like it would be interesting to
          build some UI to interpolate between an arbitrary number of seeds to combine
          features found in different seeds to steer the image into a direction.</p>

          '
        raw: Experimenting with it, it looks like it would be interesting to build
          some UI to interpolate between an arbitrary number of seeds to combine features
          found in different seeds to steer the image into a direction.
        updatedAt: '2022-09-10T13:00:35.954Z'
      numEdits: 0
      reactions: []
    id: 631c8a73fe95faea335304fa
    type: comment
  author: xalex
  content: Experimenting with it, it looks like it would be interesting to build some
    UI to interpolate between an arbitrary number of seeds to combine features found
    in different seeds to steer the image into a direction.
  created_at: 2022-09-10 12:00:35+00:00
  edited: false
  hidden: false
  id: 631c8a73fe95faea335304fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e607f13358711d00c329060792a06cb1.svg
      fullname: Victor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Victorious
      type: user
    createdAt: '2022-09-14T14:32:06.000Z'
    data:
      edited: true
      editors:
      - Victorious
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e607f13358711d00c329060792a06cb1.svg
          fullname: Victor
          isHf: false
          isPro: false
          name: Victorious
          type: user
        html: "<p>Very interesting research, <span data-props=\"{&quot;user&quot;:&quot;xalex&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/xalex\"\
          >@<span class=\"underline\">xalex</span></a></span>\n\n\t</span></span>.\
          \ Eventually, some UI would make stuff like drawing masks for inpainting\
          \ on the fly, or do advanced prompt engineering, make Stable Diffusion more\
          \ awesome.<br>For now <span data-props=\"{&quot;user&quot;:&quot;bloc97&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/bloc97\"\
          >@<span class=\"underline\">bloc97</span></a></span>\n\n\t</span></span>\
          \ has made the Cross Attention Control repo, lets you fine tune output,\
          \ effectively giving much more artistic control.<br><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/1663165696650-6317f0cdaea37e1333e36943.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1663165696650-6317f0cdaea37e1333e36943.png\"\
          ></a><br>^ CrossAttentionControl repo</p>\n"
        raw: "Very interesting research, @xalex. Eventually, some UI would make stuff\
          \ like drawing masks for inpainting on the fly, or do advanced prompt engineering,\
          \ make Stable Diffusion more awesome. \nFor now @bloc97 has made the Cross\
          \ Attention Control repo, lets you fine tune output, effectively giving\
          \ much more artistic control. \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1663165696650-6317f0cdaea37e1333e36943.png)\n\
          ^ CrossAttentionControl repo"
        updatedAt: '2022-09-14T14:32:46.258Z'
      numEdits: 1
      reactions: []
    id: 6321e5e615b7beab57c41b88
    type: comment
  author: Victorious
  content: "Very interesting research, @xalex. Eventually, some UI would make stuff\
    \ like drawing masks for inpainting on the fly, or do advanced prompt engineering,\
    \ make Stable Diffusion more awesome. \nFor now @bloc97 has made the Cross Attention\
    \ Control repo, lets you fine tune output, effectively giving much more artistic\
    \ control. \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1663165696650-6317f0cdaea37e1333e36943.png)\n\
    ^ CrossAttentionControl repo"
  created_at: 2022-09-14 13:32:06+00:00
  edited: true
  hidden: false
  id: 6321e5e615b7beab57c41b88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xalex
      type: user
    createdAt: '2022-09-14T14:44:41.000Z'
    data:
      edited: false
      editors:
      - xalex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: xalex
          type: user
        html: '<p>Adding the link here: <a rel="nofollow" href="https://github.com/bloc97/CrossAttentionControl">https://github.com/bloc97/CrossAttentionControl</a></p>

          <p>This looks interesting, because it does not change the initialization
          but the attention. Maybe one also could try to extend inpainting approaches
          to inpaint the same noise.<br>Let''s say I had rendered a knight, I wonder
          if I could segment the knight and then paste the noise in that area into
          the noise for that fantasy background and steer the prompt/attention a bit
          and get the knight integrated into the background better than pasting the
          image and then using img2img.</p>

          <p>Still I am not sure if any of these approaches allow to get the same
          knight looking into another direction. When I have a knight (as image, prompt,
          seed) who is riding from left to right and now want a front facing image,
          I would need to steer the features that make up his face and, e.g., the
          armor of his horse and not the image/image initialization.</p>

          <p>Steering it via prompts works only roughly for many things. So one could
          probably use a technique that allows to find the weights that contributed
          to a part of an image (maybe based on approaches like diffusers-interpret)
          and then change them a bit to find out which weight steer what. If one then
          has weights for the face, one could try to keep them fixed when generating
          an image with another perspective.</p>

          '
        raw: 'Adding the link here: https://github.com/bloc97/CrossAttentionControl


          This looks interesting, because it does not change the initialization but
          the attention. Maybe one also could try to extend inpainting approaches
          to inpaint the same noise.

          Let''s say I had rendered a knight, I wonder if I could segment the knight
          and then paste the noise in that area into the noise for that fantasy background
          and steer the prompt/attention a bit and get the knight integrated into
          the background better than pasting the image and then using img2img.


          Still I am not sure if any of these approaches allow to get the same knight
          looking into another direction. When I have a knight (as image, prompt,
          seed) who is riding from left to right and now want a front facing image,
          I would need to steer the features that make up his face and, e.g., the
          armor of his horse and not the image/image initialization.


          Steering it via prompts works only roughly for many things. So one could
          probably use a technique that allows to find the weights that contributed
          to a part of an image (maybe based on approaches like diffusers-interpret)
          and then change them a bit to find out which weight steer what. If one then
          has weights for the face, one could try to keep them fixed when generating
          an image with another perspective.'
        updatedAt: '2022-09-14T14:44:41.449Z'
      numEdits: 0
      reactions: []
    id: 6321e8d9a418a789a238ad76
    type: comment
  author: xalex
  content: 'Adding the link here: https://github.com/bloc97/CrossAttentionControl


    This looks interesting, because it does not change the initialization but the
    attention. Maybe one also could try to extend inpainting approaches to inpaint
    the same noise.

    Let''s say I had rendered a knight, I wonder if I could segment the knight and
    then paste the noise in that area into the noise for that fantasy background and
    steer the prompt/attention a bit and get the knight integrated into the background
    better than pasting the image and then using img2img.


    Still I am not sure if any of these approaches allow to get the same knight looking
    into another direction. When I have a knight (as image, prompt, seed) who is riding
    from left to right and now want a front facing image, I would need to steer the
    features that make up his face and, e.g., the armor of his horse and not the image/image
    initialization.


    Steering it via prompts works only roughly for many things. So one could probably
    use a technique that allows to find the weights that contributed to a part of
    an image (maybe based on approaches like diffusers-interpret) and then change
    them a bit to find out which weight steer what. If one then has weights for the
    face, one could try to keep them fixed when generating an image with another perspective.'
  created_at: 2022-09-14 13:44:41+00:00
  edited: false
  hidden: false
  id: 6321e8d9a418a789a238ad76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xalex
      type: user
    createdAt: '2022-09-16T11:23:09.000Z'
    data:
      edited: false
      editors:
      - xalex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: xalex
          type: user
        html: '<p>I wonder if the CrossAttentionControl code should be added to diffusers.
          And it seems that it still needs some convenience functions, e.g. to map
          words to their tokens. And what happens when a word consists of several
          tokens and they have different weights?<br>diffusers could add some abstract
          to, for example, allow weighting with text operators like "beetle car -animal
          -insect"</p>

          '
        raw: 'I wonder if the CrossAttentionControl code should be added to diffusers.
          And it seems that it still needs some convenience functions, e.g. to map
          words to their tokens. And what happens when a word consists of several
          tokens and they have different weights?

          diffusers could add some abstract to, for example, allow weighting with
          text operators like "beetle car -animal -insect"'
        updatedAt: '2022-09-16T11:23:09.674Z'
      numEdits: 0
      reactions: []
    id: 63245c9d1ed511c0c52353d7
    type: comment
  author: xalex
  content: 'I wonder if the CrossAttentionControl code should be added to diffusers.
    And it seems that it still needs some convenience functions, e.g. to map words
    to their tokens. And what happens when a word consists of several tokens and they
    have different weights?

    diffusers could add some abstract to, for example, allow weighting with text operators
    like "beetle car -animal -insect"'
  created_at: 2022-09-16 10:23:09+00:00
  edited: false
  hidden: false
  id: 63245c9d1ed511c0c52353d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd6d17606f1e21550bcb23386e78c2ac.svg
      fullname: Pranav Kushare
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Heisenberg08
      type: user
    createdAt: '2023-06-20T16:29:40.000Z'
    data:
      edited: false
      editors:
      - Heisenberg08
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9585447311401367
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd6d17606f1e21550bcb23386e78c2ac.svg
          fullname: Pranav Kushare
          isHf: false
          isPro: false
          name: Heisenberg08
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;xalex&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/xalex\">@<span class=\"\
          underline\">xalex</span></a></span>\n\n\t</span></span> any update on this?\
          \ Were you able to achieve it?</p>\n<blockquote>\n<p>Did someone experiment\
          \ with how to stay close to one image while changing other things in the\
          \ image?</p>\n<p>I think of, for example, generating an image of a person\
          \ and then trying to change what's in the background or even what the person\
          \ is holding in the hand without changing the face too much. Exploring the\
          \ latent space by interpolating to seeds near the original seed works good\
          \ to find random new features, but changing the prompt, e.g., by adding\
          \ \"... reading a book\" to the description of the person can change the\
          \ whole image even when starting with the same seed.<br>Using img2img doesn't\
          \ work in many cases as the person in the image may need to move for example\
          \ and img2img seems to optimize for detecting the same features at the same\
          \ places as in the seed image, which would be the person who did not move\
          \ (e.g. sitting down to read a book).</p>\n<p>I wonder if one could somehow\
          \ extract features about the generated person, which then can be incorporated\
          \ for new images, like \"a person [i](having features with seed X)[/i] sitting\
          \ down to read a book\" where the features part is extracted from an image\
          \ by detecting for example the facial features and generating a seed that\
          \ is understood by the network and leads it to generate similar faces.</p>\n\
          </blockquote>\n"
        raw: "@xalex any update on this? Were you able to achieve it?\n\n> Did someone\
          \ experiment with how to stay close to one image while changing other things\
          \ in the image?\n> \n> I think of, for example, generating an image of a\
          \ person and then trying to change what's in the background or even what\
          \ the person is holding in the hand without changing the face too much.\
          \ Exploring the latent space by interpolating to seeds near the original\
          \ seed works good to find random new features, but changing the prompt,\
          \ e.g., by adding \"... reading a book\" to the description of the person\
          \ can change the whole image even when starting with the same seed. \n>\
          \ Using img2img doesn't work in many cases as the person in the image may\
          \ need to move for example and img2img seems to optimize for detecting the\
          \ same features at the same places as in the seed image, which would be\
          \ the person who did not move (e.g. sitting down to read a book).\n> \n\
          > I wonder if one could somehow extract features about the generated person,\
          \ which then can be incorporated for new images, like \"a person [i](having\
          \ features with seed X)[/i] sitting down to read a book\" where the features\
          \ part is extracted from an image by detecting for example the facial features\
          \ and generating a seed that is understood by the network and leads it to\
          \ generate similar faces.\n\n"
        updatedAt: '2023-06-20T16:29:40.625Z'
      numEdits: 0
      reactions: []
    id: 6491d3f457b9a47e9af1b666
    type: comment
  author: Heisenberg08
  content: "@xalex any update on this? Were you able to achieve it?\n\n> Did someone\
    \ experiment with how to stay close to one image while changing other things in\
    \ the image?\n> \n> I think of, for example, generating an image of a person and\
    \ then trying to change what's in the background or even what the person is holding\
    \ in the hand without changing the face too much. Exploring the latent space by\
    \ interpolating to seeds near the original seed works good to find random new\
    \ features, but changing the prompt, e.g., by adding \"... reading a book\" to\
    \ the description of the person can change the whole image even when starting\
    \ with the same seed. \n> Using img2img doesn't work in many cases as the person\
    \ in the image may need to move for example and img2img seems to optimize for\
    \ detecting the same features at the same places as in the seed image, which would\
    \ be the person who did not move (e.g. sitting down to read a book).\n> \n> I\
    \ wonder if one could somehow extract features about the generated person, which\
    \ then can be incorporated for new images, like \"a person [i](having features\
    \ with seed X)[/i] sitting down to read a book\" where the features part is extracted\
    \ from an image by detecting for example the facial features and generating a\
    \ seed that is understood by the network and leads it to generate similar faces.\n\
    \n"
  created_at: 2023-06-20 15:29:40+00:00
  edited: false
  hidden: false
  id: 6491d3f457b9a47e9af1b666
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 33
repo_id: CompVis/stable-diffusion-v1-4
repo_type: model
status: open
target_branch: null
title: Best way to stay close to a main image while changing details, poses, surroundings,
  etc.?
