!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Tarun1986
conflicting_files: null
created_at: 2023-05-08 10:34:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a8fcbc35a348f47cd192f394aeaa72f.svg
      fullname: Tarun Mishra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tarun1986
      type: user
    createdAt: '2023-05-08T11:34:36.000Z'
    data:
      edited: false
      editors:
      - Tarun1986
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a8fcbc35a348f47cd192f394aeaa72f.svg
          fullname: Tarun Mishra
          isHf: false
          isPro: false
          name: Tarun1986
          type: user
        html: '<p>I tried running the model using release of lamma.cpp git. I get
          the floowing error:-</p>

          <blockquote>

          <p>main.exe -m ggml-vic13b-uncensored-q4_3.bin -n -1 --color -r "User:"
          --in-prefix " " -e --prompt "User: Hi\nAI: Hello. I am an AI chatbot. Would
          you like to talk?\nUser: Sure!\nAI: What would you like to talk about?\nUser:"</p>

          </blockquote>

          <p>main: build = 518 (1f48b0a)<br>main: seed  = 1683545508<br>llama.cpp:
          loading model from ggml-vic13b-uncensored-q4_3.bin<br>error loading model:
          unrecognized tensor type 5</p>

          <p>llama_init_from_file: failed to load model<br>llama_init_from_gpt_params:
          error: failed to load m</p>

          '
        raw: "I tried running the model using release of lamma.cpp git. I get the\
          \ floowing error:-\r\n>main.exe -m ggml-vic13b-uncensored-q4_3.bin -n -1\
          \ --color -r \"User:\" --in-prefix \" \" -e --prompt \"User: Hi\\nAI: Hello.\
          \ I am an AI chatbot. Would you like to talk?\\nUser: Sure!\\nAI: What would\
          \ you like to talk about?\\nUser:\"\r\n\r\nmain: build = 518 (1f48b0a)\r\
          \nmain: seed  = 1683545508\r\nllama.cpp: loading model from ggml-vic13b-uncensored-q4_3.bin\r\
          \nerror loading model: unrecognized tensor type 5\r\n\r\nllama_init_from_file:\
          \ failed to load model\r\nllama_init_from_gpt_params: error: failed to load\
          \ m"
        updatedAt: '2023-05-08T11:34:36.289Z'
      numEdits: 0
      reactions: []
    id: 6458de4c4b7baff9a84a5c8b
    type: comment
  author: Tarun1986
  content: "I tried running the model using release of lamma.cpp git. I get the floowing\
    \ error:-\r\n>main.exe -m ggml-vic13b-uncensored-q4_3.bin -n -1 --color -r \"\
    User:\" --in-prefix \" \" -e --prompt \"User: Hi\\nAI: Hello. I am an AI chatbot.\
    \ Would you like to talk?\\nUser: Sure!\\nAI: What would you like to talk about?\\\
    nUser:\"\r\n\r\nmain: build = 518 (1f48b0a)\r\nmain: seed  = 1683545508\r\nllama.cpp:\
    \ loading model from ggml-vic13b-uncensored-q4_3.bin\r\nerror loading model: unrecognized\
    \ tensor type 5\r\n\r\nllama_init_from_file: failed to load model\r\nllama_init_from_gpt_params:\
    \ error: failed to load m"
  created_at: 2023-05-08 10:34:36+00:00
  edited: false
  hidden: false
  id: 6458de4c4b7baff9a84a5c8b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642b2aa85df44ff24543d8be/mEnbGB_0Flleoa6SWp5b6.jpeg?w=200&h=200&f=face
      fullname: amkdg
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: eachadea
      type: user
    createdAt: '2023-05-09T13:36:19.000Z'
    data:
      edited: false
      editors:
      - eachadea
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642b2aa85df44ff24543d8be/mEnbGB_0Flleoa6SWp5b6.jpeg?w=200&h=200&f=face
          fullname: amkdg
          isHf: false
          isPro: false
          name: eachadea
          type: user
        html: '<p>q4_3 has been removed - use q5_1</p>

          '
        raw: q4_3 has been removed - use q5_1
        updatedAt: '2023-05-09T13:36:19.514Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Free-Radical
    id: 645a4c53ffcd6a698fb0bf4f
    type: comment
  author: eachadea
  content: q4_3 has been removed - use q5_1
  created_at: 2023-05-09 12:36:19+00:00
  edited: false
  hidden: false
  id: 645a4c53ffcd6a698fb0bf4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9274bb571e8eee45fec4a1d91fc106fd.svg
      fullname: karuppusamy Prabhu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kmylsamy
      type: user
    createdAt: '2023-05-10T04:41:19.000Z'
    data:
      edited: false
      editors:
      - kmylsamy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9274bb571e8eee45fec4a1d91fc106fd.svg
          fullname: karuppusamy Prabhu
          isHf: false
          isPro: false
          name: kmylsamy
          type: user
        html: '<p>Hey eachadea,</p>

          <p>I too face same issue even after changing the model to q5_1,  any idea
          ? before facing this issue i used to run <code>ggml-vic13b-q4_1.bin</code>
          model</p>

          <pre><code>main: seed = 1683693461

          llama.cpp: loading model from ./models/ggml-vic13b-q5_1.bin

          error loading model: unrecognized tensor type 7


          llama_init_from_file: failed to load model

          main: error: failed to load model ''./models/ggml-vic13b-q5_1.bin''

          </code></pre>

          '
        raw: 'Hey eachadea,


          I too face same issue even after changing the model to q5_1,  any idea ?
          before facing this issue i used to run `ggml-vic13b-q4_1.bin` model


          ```

          main: seed = 1683693461

          llama.cpp: loading model from ./models/ggml-vic13b-q5_1.bin

          error loading model: unrecognized tensor type 7


          llama_init_from_file: failed to load model

          main: error: failed to load model ''./models/ggml-vic13b-q5_1.bin''

          ```'
        updatedAt: '2023-05-10T04:41:19.026Z'
      numEdits: 0
      reactions: []
    id: 645b206fbca1aab2a281d2a8
    type: comment
  author: kmylsamy
  content: 'Hey eachadea,


    I too face same issue even after changing the model to q5_1,  any idea ? before
    facing this issue i used to run `ggml-vic13b-q4_1.bin` model


    ```

    main: seed = 1683693461

    llama.cpp: loading model from ./models/ggml-vic13b-q5_1.bin

    error loading model: unrecognized tensor type 7


    llama_init_from_file: failed to load model

    main: error: failed to load model ''./models/ggml-vic13b-q5_1.bin''

    ```'
  created_at: 2023-05-10 03:41:19+00:00
  edited: false
  hidden: false
  id: 645b206fbca1aab2a281d2a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642b2aa85df44ff24543d8be/mEnbGB_0Flleoa6SWp5b6.jpeg?w=200&h=200&f=face
      fullname: amkdg
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: eachadea
      type: user
    createdAt: '2023-05-10T05:14:00.000Z'
    data:
      edited: false
      editors:
      - eachadea
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642b2aa85df44ff24543d8be/mEnbGB_0Flleoa6SWp5b6.jpeg?w=200&h=200&f=face
          fullname: amkdg
          isHf: false
          isPro: false
          name: eachadea
          type: user
        html: '<p>That means you''re running an old version of llama.cpp. Do a <code>git
          pull</code> then <code>make</code></p>

          '
        raw: That means you're running an old version of llama.cpp. Do a `git pull`
          then `make`
        updatedAt: '2023-05-10T05:14:00.334Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - kmylsamy
    id: 645b2818bca1aab2a2821d94
    type: comment
  author: eachadea
  content: That means you're running an old version of llama.cpp. Do a `git pull`
    then `make`
  created_at: 2023-05-10 04:14:00+00:00
  edited: false
  hidden: false
  id: 645b2818bca1aab2a2821d94
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d83060ae1625fe0b47b97d54ee4fbdd3.svg
      fullname: Alberto
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: agonzalez
      type: user
    createdAt: '2023-05-10T18:21:06.000Z'
    data:
      edited: false
      editors:
      - agonzalez
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d83060ae1625fe0b47b97d54ee4fbdd3.svg
          fullname: Alberto
          isHf: false
          isPro: false
          name: agonzalez
          type: user
        html: '<p>I am trying to run in  text-generation-webui but fails with this
          error: any idea?</p>

          <p>$ python server.py --auto-devices --chat --model eachadea_ggml-vicuna-13b-4bit<br>INFO:generated
          new fontManager<br>INFO:Gradio HTTP request redirected to localhost :)<br>bin
          /data/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so<br>/data/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/cextension.py:33:
          UserWarning: The installed version of bitsandbytes was compiled without
          GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization
          are unavailable.<br>  warn("The installed version of bitsandbytes was compiled
          without GPU support. "<br>INFO:Loading eachadea_ggml-vicuna-13b-4bit...<br>Traceback
          (most recent call last):<br>  File "/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py",
          line 259, in hf_raise_for_status<br>    response.raise_for_status()<br>  File
          "/data/miniconda3/envs/textgen/lib/python3.10/site-packages/requests/models.py",
          line 1021, in raise_for_status<br>    raise HTTPError(http_error_msg, response=self)<br>requests.exceptions.HTTPError:
          401 Client Error: Unauthorized for url: <a href="https://huggingface.co/models/eachadea_ggml-vicuna-13b-4bit/resolve/main/config.json">https://huggingface.co/models/eachadea_ggml-vicuna-13b-4bit/resolve/main/config.json</a></p>

          <p>The above exception was the direct cause of the following exception:</p>

          <p>Traceback (most recent call last):<br>  File "/data//envs/textgen/lib/python3.10/site-packages/transformers/utils/hub.py",
          line 417, in cached_file<br>    resolved_file = hf_hub_download(<br>  File
          "/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py",
          line 120, in _inner_fn<br>    return fn(*args, **kwargs)<br>  File "/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/file_download.py",
          line 1195, in hf_hub_download<br>    metadata = get_hf_file_metadata(<br>  File
          "/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py",
          line 120, in _inner_fn<br>    return fn(*args, **kwargs)<br>  File "/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/file_download.py",
          line 1541, in get_hf_file_metadata<br>    hf_raise_for_status(r)<br>  File
          "/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py",
          line 291, in hf_raise_for_status<br>    raise RepositoryNotFoundError(message,
          response) from e<br>huggingface_hub.utils._errors.RepositoryNotFoundError:
          401 Client Error. (Request ID: Root=1-645bdf2a-21dd20f9390cd6aa10267b52)</p>

          <p>Repository Not Found for url: <a href="https://huggingface.co/models/eachadea_ggml-vicuna-13b-4bit/resolve/main/config.json">https://huggingface.co/models/eachadea_ggml-vicuna-13b-4bit/resolve/main/config.json</a>.<br>Please
          make sure you specified the correct <code>repo_id</code> and <code>repo_type</code>.<br>If
          you are trying to access a private or gated repo, make sure you are authenticated.<br>Invalid
          username or password.</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "/data//text-generation-webui/server.py",
          line 919, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "/data/text-generation-webui/modules/models.py", line 74, in load_model<br>    shared.model_type
          = find_model_type(model_name)<br>  File "/data//text-generation-webui/modules/models.py",
          line 62, in find_model_type<br>    config = AutoConfig.from_pretrained(Path(f''{shared.args.model_dir}/{model_name}''),
          trust_remote_code=shared.args.trust_remote_code)<br>  File "/data//miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py",
          line 928, in from_pretrained<br>    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,
          **kwargs)<br>  File "/data//miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/configuration_utils.py",
          line 574, in get_config_dict<br>    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path,
          **kwargs)<br>  File "/data//miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/configuration_utils.py",
          line 629, in _get_config_dict<br>    resolved_config_file = cached_file(<br>  File
          "/data//miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/utils/hub.py",
          line 433, in cached_file<br>    raise EnvironmentError(<br>OSError: models/eachadea_ggml-vicuna-13b-4bit
          is not a local folder and is not a valid model identifier listed on ''<a
          href="https://huggingface.co/models''">https://huggingface.co/models''</a><br>If
          this is a private repository, make sure to pass a token having permission
          to this repo with <code>use_auth_token</code> or log in with <code>huggingface-cli
          login</code> and pass <code>use_auth_token=True</code>.</p>

          '
        raw: "I am trying to run in  text-generation-webui but fails with this error:\
          \ any idea?\n\n$ python server.py --auto-devices --chat --model eachadea_ggml-vicuna-13b-4bit\n\
          INFO:generated new fontManager\nINFO:Gradio HTTP request redirected to localhost\
          \ :)\nbin /data/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n\
          /data/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/cextension.py:33:\
          \ UserWarning: The installed version of bitsandbytes was compiled without\
          \ GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization\
          \ are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled\
          \ without GPU support. \"\nINFO:Loading eachadea_ggml-vicuna-13b-4bit...\n\
          Traceback (most recent call last):\n  File \"/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\"\
          , line 259, in hf_raise_for_status\n    response.raise_for_status()\n  File\
          \ \"/data/miniconda3/envs/textgen/lib/python3.10/site-packages/requests/models.py\"\
          , line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\n\
          requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/models/eachadea_ggml-vicuna-13b-4bit/resolve/main/config.json\n\
          \nThe above exception was the direct cause of the following exception:\n\
          \nTraceback (most recent call last):\n  File \"/data//envs/textgen/lib/python3.10/site-packages/transformers/utils/hub.py\"\
          , line 417, in cached_file\n    resolved_file = hf_hub_download(\n  File\
          \ \"/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\"\
          , line 120, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/file_download.py\"\
          , line 1195, in hf_hub_download\n    metadata = get_hf_file_metadata(\n\
          \  File \"/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\"\
          , line 120, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/file_download.py\"\
          , line 1541, in get_hf_file_metadata\n    hf_raise_for_status(r)\n  File\
          \ \"/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\"\
          , line 291, in hf_raise_for_status\n    raise RepositoryNotFoundError(message,\
          \ response) from e\nhuggingface_hub.utils._errors.RepositoryNotFoundError:\
          \ 401 Client Error. (Request ID: Root=1-645bdf2a-21dd20f9390cd6aa10267b52)\n\
          \nRepository Not Found for url: https://huggingface.co/models/eachadea_ggml-vicuna-13b-4bit/resolve/main/config.json.\n\
          Please make sure you specified the correct `repo_id` and `repo_type`.\n\
          If you are trying to access a private or gated repo, make sure you are authenticated.\n\
          Invalid username or password.\n\nDuring handling of the above exception,\
          \ another exception occurred:\n\nTraceback (most recent call last):\n  File\
          \ \"/data//text-generation-webui/server.py\", line 919, in <module>\n  \
          \  shared.model, shared.tokenizer = load_model(shared.model_name)\n  File\
          \ \"/data/text-generation-webui/modules/models.py\", line 74, in load_model\n\
          \    shared.model_type = find_model_type(model_name)\n  File \"/data//text-generation-webui/modules/models.py\"\
          , line 62, in find_model_type\n    config = AutoConfig.from_pretrained(Path(f'{shared.args.model_dir}/{model_name}'),\
          \ trust_remote_code=shared.args.trust_remote_code)\n  File \"/data//miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 928, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)\n  File \"/data//miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/configuration_utils.py\"\
          , line 574, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)\n  File \"/data//miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/configuration_utils.py\"\
          , line 629, in _get_config_dict\n    resolved_config_file = cached_file(\n\
          \  File \"/data//miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/utils/hub.py\"\
          , line 433, in cached_file\n    raise EnvironmentError(\nOSError: models/eachadea_ggml-vicuna-13b-4bit\
          \ is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n\
          If this is a private repository, make sure to pass a token having permission\
          \ to this repo with `use_auth_token` or log in with `huggingface-cli login`\
          \ and pass `use_auth_token=True`."
        updatedAt: '2023-05-10T18:21:06.788Z'
      numEdits: 0
      reactions: []
    id: 645be0920120c98d16aa548c
    type: comment
  author: agonzalez
  content: "I am trying to run in  text-generation-webui but fails with this error:\
    \ any idea?\n\n$ python server.py --auto-devices --chat --model eachadea_ggml-vicuna-13b-4bit\n\
    INFO:generated new fontManager\nINFO:Gradio HTTP request redirected to localhost\
    \ :)\nbin /data/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n\
    /data/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/cextension.py:33:\
    \ UserWarning: The installed version of bitsandbytes was compiled without GPU\
    \ support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n\
    \  warn(\"The installed version of bitsandbytes was compiled without GPU support.\
    \ \"\nINFO:Loading eachadea_ggml-vicuna-13b-4bit...\nTraceback (most recent call\
    \ last):\n  File \"/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\"\
    , line 259, in hf_raise_for_status\n    response.raise_for_status()\n  File \"\
    /data/miniconda3/envs/textgen/lib/python3.10/site-packages/requests/models.py\"\
    , line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\n\
    requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/models/eachadea_ggml-vicuna-13b-4bit/resolve/main/config.json\n\
    \nThe above exception was the direct cause of the following exception:\n\nTraceback\
    \ (most recent call last):\n  File \"/data//envs/textgen/lib/python3.10/site-packages/transformers/utils/hub.py\"\
    , line 417, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\"\
    , line 120, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/file_download.py\"\
    , line 1195, in hf_hub_download\n    metadata = get_hf_file_metadata(\n  File\
    \ \"/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\"\
    , line 120, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/file_download.py\"\
    , line 1541, in get_hf_file_metadata\n    hf_raise_for_status(r)\n  File \"/data/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\"\
    , line 291, in hf_raise_for_status\n    raise RepositoryNotFoundError(message,\
    \ response) from e\nhuggingface_hub.utils._errors.RepositoryNotFoundError: 401\
    \ Client Error. (Request ID: Root=1-645bdf2a-21dd20f9390cd6aa10267b52)\n\nRepository\
    \ Not Found for url: https://huggingface.co/models/eachadea_ggml-vicuna-13b-4bit/resolve/main/config.json.\n\
    Please make sure you specified the correct `repo_id` and `repo_type`.\nIf you\
    \ are trying to access a private or gated repo, make sure you are authenticated.\n\
    Invalid username or password.\n\nDuring handling of the above exception, another\
    \ exception occurred:\n\nTraceback (most recent call last):\n  File \"/data//text-generation-webui/server.py\"\
    , line 919, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    \  File \"/data/text-generation-webui/modules/models.py\", line 74, in load_model\n\
    \    shared.model_type = find_model_type(model_name)\n  File \"/data//text-generation-webui/modules/models.py\"\
    , line 62, in find_model_type\n    config = AutoConfig.from_pretrained(Path(f'{shared.args.model_dir}/{model_name}'),\
    \ trust_remote_code=shared.args.trust_remote_code)\n  File \"/data//miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 928, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
    \ **kwargs)\n  File \"/data//miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/configuration_utils.py\"\
    , line 574, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path,\
    \ **kwargs)\n  File \"/data//miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/configuration_utils.py\"\
    , line 629, in _get_config_dict\n    resolved_config_file = cached_file(\n  File\
    \ \"/data//miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/utils/hub.py\"\
    , line 433, in cached_file\n    raise EnvironmentError(\nOSError: models/eachadea_ggml-vicuna-13b-4bit\
    \ is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n\
    If this is a private repository, make sure to pass a token having permission to\
    \ this repo with `use_auth_token` or log in with `huggingface-cli login` and pass\
    \ `use_auth_token=True`."
  created_at: 2023-05-10 17:21:06+00:00
  edited: false
  hidden: false
  id: 645be0920120c98d16aa548c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: eachadea/ggml-vicuna-13b-1.1
repo_type: model
status: open
target_branch: null
title: How to run these quantised model.
