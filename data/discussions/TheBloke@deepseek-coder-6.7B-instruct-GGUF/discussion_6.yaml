!!python/object:huggingface_hub.community.DiscussionWithDetails
author: whatever1983
conflicting_files: null
created_at: 2023-11-12 07:24:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b628b91320688dcf2e954c4b22d4a630.svg
      fullname: TS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: whatever1983
      type: user
    createdAt: '2023-11-12T07:24:22.000Z'
    data:
      edited: false
      editors:
      - whatever1983
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9575921893119812
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b628b91320688dcf2e954c4b22d4a630.svg
          fullname: TS
          isHf: false
          isPro: false
          name: whatever1983
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span>: how did you\
          \ use llama.cpp ./quantize to get those GGUFs when the tokenizer.model file\
          \ isn't even uploaded by the model creator?  If you managed to do a tokenizer.model\
          \ file yourself, care to share the file on this repo?  Also for the 33B\
          \ one.</p>\n<p>Thanks</p>\n"
        raw: "@TheBloke: how did you use llama.cpp ./quantize to get those GGUFs when\
          \ the tokenizer.model file isn't even uploaded by the model creator?  If\
          \ you managed to do a tokenizer.model file yourself, care to share the file\
          \ on this repo?  Also for the 33B one.\r\n\r\nThanks"
        updatedAt: '2023-11-12T07:24:23.000Z'
      numEdits: 0
      reactions: []
    id: 65507da6c67f60a3689f9e22
    type: comment
  author: whatever1983
  content: "@TheBloke: how did you use llama.cpp ./quantize to get those GGUFs when\
    \ the tokenizer.model file isn't even uploaded by the model creator?  If you managed\
    \ to do a tokenizer.model file yourself, care to share the file on this repo?\
    \  Also for the 33B one.\r\n\r\nThanks"
  created_at: 2023-11-12 07:24:22+00:00
  edited: false
  hidden: false
  id: 65507da6c67f60a3689f9e22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-12T15:52:42.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9786133170127869
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah i couldn''t make it using the standard Llama.cpp convert.py.
          Fortunately there was a PR to add support for the HF tokenizer format. That
          had a few problems and bugs at first but after a few fixes I was able to
          make working GGUFs.</p>

          <p>That PR hasn''t been merged as its still being reviewed. But if you want
          to make your own GGUFs of this model or any other without tokenizer.model
          you can use the covert.py from <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/3633">https://github.com/ggerganov/llama.cpp/pull/3633</a></p>

          '
        raw: 'Yeah i couldn''t make it using the standard Llama.cpp convert.py. Fortunately
          there was a PR to add support for the HF tokenizer format. That had a few
          problems and bugs at first but after a few fixes I was able to make working
          GGUFs.


          That PR hasn''t been merged as its still being reviewed. But if you want
          to make your own GGUFs of this model or any other without tokenizer.model
          you can use the covert.py from https://github.com/ggerganov/llama.cpp/pull/3633'
        updatedAt: '2023-11-12T15:52:42.001Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - afrideva
        - dlface
        - jiuyue
    id: 6550f4ca32f278f503981924
    type: comment
  author: TheBloke
  content: 'Yeah i couldn''t make it using the standard Llama.cpp convert.py. Fortunately
    there was a PR to add support for the HF tokenizer format. That had a few problems
    and bugs at first but after a few fixes I was able to make working GGUFs.


    That PR hasn''t been merged as its still being reviewed. But if you want to make
    your own GGUFs of this model or any other without tokenizer.model you can use
    the covert.py from https://github.com/ggerganov/llama.cpp/pull/3633'
  created_at: 2023-11-12 15:52:42+00:00
  edited: false
  hidden: false
  id: 6550f4ca32f278f503981924
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-05T20:59:45.000Z'
    data:
      edited: true
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8244709968566895
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>,\
          \ Please provide a quantize for this model: <a href=\"https://huggingface.co/ise-uiuc/Magicoder-S-DS-6.7B\"\
          >https://huggingface.co/ise-uiuc/Magicoder-S-DS-6.7B</a>  I followed the\
          \ same strategy (PR), but I wasn't successful.</p>\n"
        raw: 'Hey @TheBloke, Please provide a quantize for this model: https://huggingface.co/ise-uiuc/Magicoder-S-DS-6.7B  I
          followed the same strategy (PR), but I wasn''t successful.'
        updatedAt: '2023-12-05T21:00:32.790Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jiuyue
    id: 656f8f41990fa7ae20280d1b
    type: comment
  author: eramax
  content: 'Hey @TheBloke, Please provide a quantize for this model: https://huggingface.co/ise-uiuc/Magicoder-S-DS-6.7B  I
    followed the same strategy (PR), but I wasn''t successful.'
  created_at: 2023-12-05 20:59:45+00:00
  edited: true
  hidden: false
  id: 656f8f41990fa7ae20280d1b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/deepseek-coder-6.7B-instruct-GGUF
repo_type: model
status: open
target_branch: null
title: Missing tokenizer.model file
