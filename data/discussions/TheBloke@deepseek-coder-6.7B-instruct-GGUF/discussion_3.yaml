!!python/object:huggingface_hub.community.DiscussionWithDetails
author: imhsouna
conflicting_files: null
created_at: 2023-11-06 13:47:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/162f40cfc2b8828f56adc89b0fbef341.svg
      fullname: Hsouna ZINOUBI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: imhsouna
      type: user
    createdAt: '2023-11-06T13:47:24.000Z'
    data:
      edited: false
      editors:
      - imhsouna
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8185141086578369
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/162f40cfc2b8828f56adc89b0fbef341.svg
          fullname: Hsouna ZINOUBI
          isHf: false
          isPro: false
          name: imhsouna
          type: user
        html: '<p>all deepseek models you upload are not working, im using lm studio
          on macos m2</p>

          '
        raw: all deepseek models you upload are not working, im using lm studio on
          macos m2
        updatedAt: '2023-11-06T13:47:24.037Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - deanrie
    id: 6548ee6cc0bc1b5104f202db
    type: comment
  author: imhsouna
  content: all deepseek models you upload are not working, im using lm studio on macos
    m2
  created_at: 2023-11-06 13:47:24+00:00
  edited: false
  hidden: false
  id: 6548ee6cc0bc1b5104f202db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
      fullname: Ziggy Stardust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nurb432
      type: user
    createdAt: '2023-11-06T14:03:43.000Z'
    data:
      edited: false
      editors:
      - Nurb432
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9714410305023193
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
          fullname: Ziggy Stardust
          isHf: false
          isPro: false
          name: Nurb432
          type: user
        html: '<p>Not everything supports them yet. Couple of threads about this now
          going on. </p>

          '
        raw: 'Not everything supports them yet. Couple of threads about this now going
          on. '
        updatedAt: '2023-11-06T14:03:43.061Z'
      numEdits: 0
      reactions: []
    id: 6548f23f6f7be0d330796eb8
    type: comment
  author: Nurb432
  content: 'Not everything supports them yet. Couple of threads about this now going
    on. '
  created_at: 2023-11-06 14:03:43+00:00
  edited: false
  hidden: false
  id: 6548f23f6f7be0d330796eb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
      fullname: Andreas Rozek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rozek
      type: user
    createdAt: '2023-11-24T15:56:56.000Z'
    data:
      edited: false
      editors:
      - rozek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8329328894615173
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
          fullname: Andreas Rozek
          isHf: false
          isPro: false
          name: rozek
          type: user
        html: '<p>I just downloaded "deepseek-coder-6.7b-instruct.Q5_K_M.gguf" and
          tested it with the current version of llama.cpp - it works like a charm</p>

          '
        raw: I just downloaded "deepseek-coder-6.7b-instruct.Q5_K_M.gguf" and tested
          it with the current version of llama.cpp - it works like a charm
        updatedAt: '2023-11-24T15:56:56.199Z'
      numEdits: 0
      reactions: []
    id: 6560c7c87a465cdcb3316a2d
    type: comment
  author: rozek
  content: I just downloaded "deepseek-coder-6.7b-instruct.Q5_K_M.gguf" and tested
    it with the current version of llama.cpp - it works like a charm
  created_at: 2023-11-24 15:56:56+00:00
  edited: false
  hidden: false
  id: 6560c7c87a465cdcb3316a2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ba5b02438945d10ebe055b/aX9b6BSBN2ecQ3n_4Lbo3.png?w=200&h=200&f=face
      fullname: Dean Rie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deanrie
      type: user
    createdAt: '2023-11-26T07:50:33.000Z'
    data:
      edited: false
      editors:
      - deanrie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9692145586013794
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ba5b02438945d10ebe055b/aX9b6BSBN2ecQ3n_4Lbo3.png?w=200&h=200&f=face
          fullname: Dean Rie
          isHf: false
          isPro: false
          name: deanrie
          type: user
        html: '<blockquote>

          <p>Not everything supports them yet. Couple of threads about this now going
          on.</p>

          </blockquote>

          <p>Hi, where can I read?</p>

          '
        raw: '> Not everything supports them yet. Couple of threads about this now
          going on.


          Hi, where can I read?'
        updatedAt: '2023-11-26T07:50:33.742Z'
      numEdits: 0
      reactions: []
    id: 6562f8c92c671784e2f118b7
    type: comment
  author: deanrie
  content: '> Not everything supports them yet. Couple of threads about this now going
    on.


    Hi, where can I read?'
  created_at: 2023-11-26 07:50:33+00:00
  edited: false
  hidden: false
  id: 6562f8c92c671784e2f118b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-26T13:12:33.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8541393280029297
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<blockquote>\n<p>I just downloaded \"deepseek-coder-6.7b-instruct.Q5_K_M.gguf\"\
          \ and tested it with the current version of llama.cpp - it works like a\
          \ charm</p>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;rozek&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rozek\"\
          >@<span class=\"underline\">rozek</span></a></span>\n\n\t</span></span>\
          \ It would great if you can share the cmd line for running deepseek with\
          \ llama.cpp</p>\n"
        raw: '> I just downloaded "deepseek-coder-6.7b-instruct.Q5_K_M.gguf" and tested
          it with the current version of llama.cpp - it works like a charm


          @rozek It would great if you can share the cmd line for running deepseek
          with llama.cpp'
        updatedAt: '2023-11-26T13:12:33.365Z'
      numEdits: 0
      reactions: []
    id: 65634441462e5ebcbf2e06f4
    type: comment
  author: Yhyu13
  content: '> I just downloaded "deepseek-coder-6.7b-instruct.Q5_K_M.gguf" and tested
    it with the current version of llama.cpp - it works like a charm


    @rozek It would great if you can share the cmd line for running deepseek with
    llama.cpp'
  created_at: 2023-11-26 13:12:33+00:00
  edited: false
  hidden: false
  id: 65634441462e5ebcbf2e06f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
      fullname: Andreas Rozek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rozek
      type: user
    createdAt: '2023-11-27T03:50:13.000Z'
    data:
      edited: false
      editors:
      - rozek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.20507530868053436
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
          fullname: Andreas Rozek
          isHf: false
          isPro: false
          name: rozek
          type: user
        html: "<p>sure, no problem.</p>\n<p>Here is what I just ran (using a LLaMA.cpp\
          \ executable compiled approx. a week ago on a Mac mini M1 with 16GB running\
          \ macOS 13.6.1 Ventura:</p>\n<pre><code>./llama --model ./deepseek-coder-6.7b-instruct.Q5_K_M.gguf\
          \ --prompt \"write a quicksort algorithm in JavaScript\"\n</code></pre>\n\
          <p>This command produced the following output:</p>\n<pre><code>Log start\n\
          main: warning: changing RoPE frequency base to 0 (default 10000.0)\nmain:\
          \ warning: scaling RoPE frequency by 0 (default 1.0)\nmain: build = 1597\
          \ (6707f74)\nmain: built with Apple clang version 15.0.0 (clang-1500.0.40.1)\
          \ for arm64-apple-darwin22.6.0\nmain: seed  = 1701056675\nllama_model_loader:\
          \ loaded meta data with 22 key-value pairs and 291 tensors from ./deepseek-coder-6.7b-instruct.Q5_K_M.gguf\
          \ (version GGUF V3 (latest))\nllama_model_loader: - tensor    0:       \
          \         token_embd.weight q5_K     [  4096, 32256,     1,     1 ]\nllama_model_loader:\
          \ - tensor    1:              blk.0.attn_q.weight q5_K     [  4096,  4096,\
          \     1,     1 ]\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \    3:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor    4:         blk.0.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \    5:            blk.0.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \    7:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor    8:           blk.0.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   10:              blk.1.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   11:              blk.1.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   12:              blk.1.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   13:         blk.1.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   14:            blk.1.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   15:              blk.1.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   16:            blk.1.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   18:            blk.1.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   19:              blk.2.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   20:              blk.2.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   21:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   22:         blk.2.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   23:            blk.2.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   24:              blk.2.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   25:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   26:           blk.2.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   28:              blk.3.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   29:              blk.3.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   30:              blk.3.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   31:         blk.3.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   32:            blk.3.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   33:              blk.3.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   34:            blk.3.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   36:            blk.3.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   37:              blk.4.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   38:              blk.4.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   39:              blk.4.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   40:         blk.4.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   41:            blk.4.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   42:              blk.4.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   43:            blk.4.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   44:           blk.4.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   46:              blk.5.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   47:              blk.5.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   48:              blk.5.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   49:         blk.5.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   50:            blk.5.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   51:              blk.5.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   52:            blk.5.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   54:            blk.5.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   55:              blk.6.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   56:              blk.6.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   57:              blk.6.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   58:         blk.6.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   59:            blk.6.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   60:              blk.6.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   61:            blk.6.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   62:           blk.6.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   64:              blk.7.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   65:              blk.7.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   66:              blk.7.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   67:         blk.7.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   68:            blk.7.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   69:              blk.7.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   70:            blk.7.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   72:            blk.7.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   73:              blk.8.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   74:              blk.8.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   75:              blk.8.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   76:         blk.8.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   77:            blk.8.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   78:              blk.8.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   79:            blk.8.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   80:           blk.8.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   82:              blk.9.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   83:              blk.9.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   84:              blk.9.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   85:         blk.9.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   86:            blk.9.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   87:              blk.9.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   88:            blk.9.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   90:            blk.9.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   91:             blk.10.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   92:             blk.10.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   93:             blk.10.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   94:        blk.10.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   95:           blk.10.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   96:             blk.10.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   97:           blk.10.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   98:          blk.10.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  100:             blk.11.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  101:             blk.11.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  102:             blk.11.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  103:        blk.11.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  104:           blk.11.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  105:             blk.11.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  106:           blk.11.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  108:           blk.11.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  109:             blk.12.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  110:             blk.12.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  111:             blk.12.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  112:        blk.12.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  113:           blk.12.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  114:             blk.12.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  115:           blk.12.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  116:          blk.12.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  118:             blk.13.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  119:             blk.13.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  120:             blk.13.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  121:        blk.13.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  122:           blk.13.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  123:             blk.13.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  124:           blk.13.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  126:           blk.13.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  127:             blk.14.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  128:             blk.14.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  129:             blk.14.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  130:        blk.14.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  131:           blk.14.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  132:             blk.14.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  133:           blk.14.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  134:          blk.14.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  136:             blk.15.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  137:             blk.15.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  138:             blk.15.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  139:        blk.15.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  141:             blk.15.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  142:           blk.15.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  144:           blk.15.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  145:             blk.16.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  146:             blk.16.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  147:             blk.16.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  148:        blk.16.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  149:           blk.16.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  150:             blk.16.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  151:           blk.16.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  152:          blk.16.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  154:             blk.17.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  155:             blk.17.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  156:             blk.17.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  157:        blk.17.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  158:           blk.17.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  159:             blk.17.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  160:           blk.17.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  162:           blk.17.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  163:             blk.18.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  164:             blk.18.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  165:             blk.18.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  166:        blk.18.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  167:           blk.18.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  168:             blk.18.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  169:           blk.18.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  170:          blk.18.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  172:             blk.19.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  173:             blk.19.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  174:             blk.19.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  175:        blk.19.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  176:           blk.19.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  177:             blk.19.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  178:           blk.19.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  180:           blk.19.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  181:             blk.20.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  182:             blk.20.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  183:             blk.20.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  184:        blk.20.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  185:           blk.20.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  186:             blk.20.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  187:           blk.20.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  188:          blk.20.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  190:             blk.21.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  191:             blk.21.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  192:             blk.21.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  193:        blk.21.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  194:           blk.21.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  195:             blk.21.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  196:           blk.21.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  198:           blk.21.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  199:             blk.22.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  200:             blk.22.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  201:             blk.22.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  202:        blk.22.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  203:           blk.22.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  204:             blk.22.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  205:           blk.22.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  206:          blk.22.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  208:             blk.23.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  209:             blk.23.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  210:             blk.23.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  211:        blk.23.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  212:           blk.23.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  213:             blk.23.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  214:           blk.23.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  216:           blk.23.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  217:             blk.24.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  218:             blk.24.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  219:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  220:        blk.24.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  221:           blk.24.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  222:             blk.24.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  223:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  224:          blk.24.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  226:             blk.25.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  227:             blk.25.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  228:             blk.25.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  229:        blk.25.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  230:           blk.25.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  231:             blk.25.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  232:           blk.25.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  234:           blk.25.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  235:             blk.26.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  236:             blk.26.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  237:             blk.26.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  238:        blk.26.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  239:           blk.26.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  240:             blk.26.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  241:           blk.26.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  242:          blk.26.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  244:             blk.27.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  245:             blk.27.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  246:             blk.27.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  247:        blk.27.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  248:           blk.27.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  249:             blk.27.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  250:           blk.27.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  252:           blk.27.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  253:             blk.28.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  254:             blk.28.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  255:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  256:        blk.28.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  257:           blk.28.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  258:             blk.28.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  259:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  260:          blk.28.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  262:             blk.29.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  263:             blk.29.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  264:             blk.29.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  265:        blk.29.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  266:           blk.29.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  267:             blk.29.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  268:           blk.29.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  270:           blk.29.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  271:             blk.30.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  272:             blk.30.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  273:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  274:        blk.30.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  275:           blk.30.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  276:             blk.30.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  277:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  278:          blk.30.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  280:             blk.31.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  281:             blk.31.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  282:             blk.31.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  283:        blk.31.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  284:           blk.31.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  285:             blk.31.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  286:           blk.31.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  288:           blk.31.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  289:               output_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  290:                    output.weight\
          \ q6_K     [  4096, 32256,     1,     1 ]\nllama_model_loader: - kv   0:\
          \                       general.architecture str              = llama\n\
          llama_model_loader: - kv   1:                               general.name\
          \ str              = deepseek-ai_deepseek-coder-6.7b-instruct\nllama_model_loader:\
          \ - kv   2:                       llama.context_length u32             \
          \ = 16384\nllama_model_loader: - kv   3:                     llama.embedding_length\
          \ u32              = 4096\nllama_model_loader: - kv   4:               \
          \           llama.block_count u32              = 32\nllama_model_loader:\
          \ - kv   5:                  llama.feed_forward_length u32             \
          \ = 11008\nllama_model_loader: - kv   6:                 llama.rope.dimension_count\
          \ u32              = 128\nllama_model_loader: - kv   7:                \
          \ llama.attention.head_count u32              = 32\nllama_model_loader:\
          \ - kv   8:              llama.attention.head_count_kv u32             \
          \ = 32\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
          \ f32              = 0,000001\nllama_model_loader: - kv  10:           \
          \            llama.rope.freq_base f32              = 100000,000000\nllama_model_loader:\
          \ - kv  11:                    llama.rope.scale_linear f32             \
          \ = 4,000000\nllama_model_loader: - kv  12:                          general.file_type\
          \ u32              = 17\nllama_model_loader: - kv  13:                 \
          \      tokenizer.ggml.model str              = gpt2\nllama_model_loader:\
          \ - kv  14:                      tokenizer.ggml.tokens arr[str,32256]  \
          \ = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&amp;\", \"'\", ...\nllama_model_loader:\
          \ - kv  15:                      tokenizer.ggml.scores arr[f32,32256]  \
          \ = [0,000000, 0,000000, 0,000000, 0,0000...\nllama_model_loader: - kv \
          \ 16:                  tokenizer.ggml.token_type arr[i32,32256]   = [1,\
          \ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  17:  \
          \                    tokenizer.ggml.merges arr[str,31757]   = [\"\u0120\
          \ \u0120\", \"\u0120 t\", \"\u0120 a\", \"i n\", \"h e...\nllama_model_loader:\
          \ - kv  18:                tokenizer.ggml.bos_token_id u32             \
          \ = 32013\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id\
          \ u32              = 32021\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id\
          \ u32              = 32014\nllama_model_loader: - kv  21:              \
          \ general.quantization_version u32              = 2\nllama_model_loader:\
          \ - type  f32:   65 tensors\nllama_model_loader: - type q5_K:  193 tensors\n\
          llama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: mismatch\
          \ in special tokens definition ( 243/32256 vs 237/32256 ).\nllm_load_print_meta:\
          \ format           = GGUF V3 (latest)\nllm_load_print_meta: arch       \
          \      = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta:\
          \ n_vocab          = 32256\nllm_load_print_meta: n_merges         = 31757\n\
          llm_load_print_meta: n_ctx_train      = 16384\nllm_load_print_meta: n_embd\
          \           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta:\
          \ n_head_kv        = 32\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta:\
          \ n_rot            = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta:\
          \ f_norm_eps       = 0,0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1,0e-06\n\
          llm_load_print_meta: f_clamp_kqv      = 0,0e+00\nllm_load_print_meta: f_max_alibi_bias\
          \ = 0,0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta:\
          \ rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 100000,0\n\
          llm_load_print_meta: freq_scale_train = 0,25\nllm_load_print_meta: n_yarn_orig_ctx\
          \  = 16384\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta:\
          \ model type       = 7B\nllm_load_print_meta: model ftype      = mostly\
          \ Q5_K - Medium\nllm_load_print_meta: model params     = 6,74 B\nllm_load_print_meta:\
          \ model size       = 4,46 GiB (5,68 BPW) \nllm_load_print_meta: general.name\
          \   = deepseek-ai_deepseek-coder-6.7b-instruct\nllm_load_print_meta: BOS\
          \ token = 32013 '&lt;\uFF5Cbegin\u2581of\u2581sentence\uFF5C&gt;'\nllm_load_print_meta:\
          \ EOS token = 32021 '&lt;|EOT|&gt;'\nllm_load_print_meta: PAD token = 32014\
          \ '&lt;\uFF5Cend\u2581of\u2581sentence\uFF5C&gt;'\nllm_load_print_meta:\
          \ LF token  = 126 '\xC4'\nllm_load_tensors: ggml ctx size =    0,11 MiB\n\
          llm_load_tensors: mem required  = 4562,48 MiB\n..................................................................................................\n\
          llama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model:\
          \ freq_base  = 100000,0\nllama_new_context_with_model: freq_scale = 0,25\n\
          llama_new_context_with_model: kv self size  =  256,00 MiB\nllama_build_graph:\
          \ non-view tensors processed: 740/740\nggml_metal_init: allocating\nggml_metal_init:\
          \ found device: Apple M1\nggml_metal_init: picking default device: Apple\
          \ M1\nggml_metal_init: default.metallib not found, loading from source\n\
          ggml_metal_init: loading '/Users/andreas/rozek/Node-RED/ggml-metal.metal'\n\
          ggml_metal_init: GPU name:   Apple M1\nggml_metal_init: GPU family: MTLGPUFamilyApple7\
          \ (1007)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init:\
          \ recommendedMaxWorkingSetSize  = 10922,67 MiB\nggml_metal_init: maxTransferRate\
          \               = built-in GPU\nllama_new_context_with_model: compute buffer\
          \ total size = 74,06 MiB\nllama_new_context_with_model: max tensor size\
          \ =   103,36 MiB\nggml_metal_add_buffer: allocated 'data            ' buffer,\
          \ size =  4563,62 MiB, ( 4564,12 / 10922,67)\nggml_metal_add_buffer: allocated\
          \ 'kv              ' buffer, size =   256,02 MiB, ( 4820,14 / 10922,67)\n\
          ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    71,02\
          \ MiB, ( 4891,16 / 10922,67)\n\nsystem_info: n_threads = 4 / 8 | AVX = 0\
          \ | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0\
          \ | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS\
          \ = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \nsampling: \n    repeat_last_n\
          \ = 64, repeat_penalty = 1,100, frequency_penalty = 0,000, presence_penalty\
          \ = 0,000\n    top_k = 40, tfs_z = 1,000, top_p = 0,950, min_p = 0,050,\
          \ typical_p = 1,000, temp = 0,800\n    mirostat = 0, mirostat_lr = 0,100,\
          \ mirostat_ent = 5,000\ngenerate: n_ctx = 512, n_batch = 512, n_predict\
          \ = -1, n_keep = 0\n\n\nwrite a quicksort algorithm in JavaScript that sorts\
          \ an array of numbers in place, using the Hoare partition scheme.\n\n\n\
          function quickSort(arr) {\n   if (arr.length &lt; 2) return arr;\n    let\
          \ pivotIndex = Math.floor(arr.length / 2);\n    let pivotValue = arr[pivotIndex];\n\
          \    let lesserArray = [];\n    let greaterArray = [];\n    for (let i =\
          \ 0; i &lt; arr.length; i++) {\n        if (i !== pivotIndex){\n       \
          \     arr[i] &lt; pivotValue ? lesserArray.push(arr[i]) : greaterArray.push(arr[i]);\n\
          \        }\n        \n    }\n     return [...quickSort(lesserArray), pivotValue,\
          \ ...quickSort(greaterArray)]; \n}\n\nconsole.log(quickSort([34,56,21,89,76]));\
          \ //[ 21, 34, 56, 76, 89 ]// write a JavaScript function that takes an array\
          \ of numbers as input and returns the sum of all even numbers in that array...\n\
          </code></pre>\n<p>here I stopped the program because it began generating\
          \ code I did not ask for.</p>\n<p>Nevertheless, it worked in principle</p>\n"
        raw: "sure, no problem.\n\nHere is what I just ran (using a LLaMA.cpp executable\
          \ compiled approx. a week ago on a Mac mini M1 with 16GB running macOS 13.6.1\
          \ Ventura:\n\n```\n./llama --model ./deepseek-coder-6.7b-instruct.Q5_K_M.gguf\
          \ --prompt \"write a quicksort algorithm in JavaScript\"\n```\n\nThis command\
          \ produced the following output:\n\n```\nLog start\nmain: warning: changing\
          \ RoPE frequency base to 0 (default 10000.0)\nmain: warning: scaling RoPE\
          \ frequency by 0 (default 1.0)\nmain: build = 1597 (6707f74)\nmain: built\
          \ with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin22.6.0\n\
          main: seed  = 1701056675\nllama_model_loader: loaded meta data with 22 key-value\
          \ pairs and 291 tensors from ./deepseek-coder-6.7b-instruct.Q5_K_M.gguf\
          \ (version GGUF V3 (latest))\nllama_model_loader: - tensor    0:       \
          \         token_embd.weight q5_K     [  4096, 32256,     1,     1 ]\nllama_model_loader:\
          \ - tensor    1:              blk.0.attn_q.weight q5_K     [  4096,  4096,\
          \     1,     1 ]\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \    3:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor    4:         blk.0.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \    5:            blk.0.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \    7:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor    8:           blk.0.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   10:              blk.1.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   11:              blk.1.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   12:              blk.1.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   13:         blk.1.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   14:            blk.1.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   15:              blk.1.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   16:            blk.1.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   18:            blk.1.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   19:              blk.2.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   20:              blk.2.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   21:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   22:         blk.2.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   23:            blk.2.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   24:              blk.2.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   25:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   26:           blk.2.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   28:              blk.3.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   29:              blk.3.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   30:              blk.3.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   31:         blk.3.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   32:            blk.3.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   33:              blk.3.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   34:            blk.3.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   36:            blk.3.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   37:              blk.4.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   38:              blk.4.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   39:              blk.4.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   40:         blk.4.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   41:            blk.4.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   42:              blk.4.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   43:            blk.4.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   44:           blk.4.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   46:              blk.5.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   47:              blk.5.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   48:              blk.5.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   49:         blk.5.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   50:            blk.5.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   51:              blk.5.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   52:            blk.5.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   54:            blk.5.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   55:              blk.6.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   56:              blk.6.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   57:              blk.6.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   58:         blk.6.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   59:            blk.6.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   60:              blk.6.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   61:            blk.6.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   62:           blk.6.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   64:              blk.7.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   65:              blk.7.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   66:              blk.7.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   67:         blk.7.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   68:            blk.7.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   69:              blk.7.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   70:            blk.7.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   72:            blk.7.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   73:              blk.8.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   74:              blk.8.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   75:              blk.8.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   76:         blk.8.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   77:            blk.8.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   78:              blk.8.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   79:            blk.8.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   80:           blk.8.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   82:              blk.9.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   83:              blk.9.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   84:              blk.9.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   85:         blk.9.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   86:            blk.9.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   87:              blk.9.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   88:            blk.9.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   90:            blk.9.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   91:             blk.10.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   92:             blk.10.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   93:             blk.10.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   94:        blk.10.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   95:           blk.10.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   96:             blk.10.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   97:           blk.10.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   98:          blk.10.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  100:             blk.11.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  101:             blk.11.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  102:             blk.11.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  103:        blk.11.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  104:           blk.11.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  105:             blk.11.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  106:           blk.11.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  108:           blk.11.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  109:             blk.12.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  110:             blk.12.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  111:             blk.12.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  112:        blk.12.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  113:           blk.12.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  114:             blk.12.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  115:           blk.12.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  116:          blk.12.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  118:             blk.13.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  119:             blk.13.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  120:             blk.13.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  121:        blk.13.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  122:           blk.13.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  123:             blk.13.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  124:           blk.13.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  126:           blk.13.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  127:             blk.14.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  128:             blk.14.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  129:             blk.14.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  130:        blk.14.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  131:           blk.14.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  132:             blk.14.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  133:           blk.14.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  134:          blk.14.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  136:             blk.15.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  137:             blk.15.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  138:             blk.15.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  139:        blk.15.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  141:             blk.15.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  142:           blk.15.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  144:           blk.15.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  145:             blk.16.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  146:             blk.16.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  147:             blk.16.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  148:        blk.16.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  149:           blk.16.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  150:             blk.16.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  151:           blk.16.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  152:          blk.16.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  154:             blk.17.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  155:             blk.17.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  156:             blk.17.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  157:        blk.17.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  158:           blk.17.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  159:             blk.17.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  160:           blk.17.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  162:           blk.17.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  163:             blk.18.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  164:             blk.18.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  165:             blk.18.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  166:        blk.18.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  167:           blk.18.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  168:             blk.18.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  169:           blk.18.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  170:          blk.18.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  172:             blk.19.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  173:             blk.19.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  174:             blk.19.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  175:        blk.19.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  176:           blk.19.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  177:             blk.19.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  178:           blk.19.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  180:           blk.19.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  181:             blk.20.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  182:             blk.20.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  183:             blk.20.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  184:        blk.20.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  185:           blk.20.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  186:             blk.20.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  187:           blk.20.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  188:          blk.20.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  190:             blk.21.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  191:             blk.21.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  192:             blk.21.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  193:        blk.21.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  194:           blk.21.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  195:             blk.21.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  196:           blk.21.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  198:           blk.21.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  199:             blk.22.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  200:             blk.22.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  201:             blk.22.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  202:        blk.22.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  203:           blk.22.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  204:             blk.22.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  205:           blk.22.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  206:          blk.22.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  208:             blk.23.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  209:             blk.23.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  210:             blk.23.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  211:        blk.23.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  212:           blk.23.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  213:             blk.23.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  214:           blk.23.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  216:           blk.23.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  217:             blk.24.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  218:             blk.24.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  219:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  220:        blk.24.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  221:           blk.24.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  222:             blk.24.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  223:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  224:          blk.24.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  226:             blk.25.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  227:             blk.25.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  228:             blk.25.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  229:        blk.25.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  230:           blk.25.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  231:             blk.25.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  232:           blk.25.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  234:           blk.25.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  235:             blk.26.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  236:             blk.26.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  237:             blk.26.attn_v.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  238:        blk.26.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  239:           blk.26.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  240:             blk.26.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  241:           blk.26.ffn_down.weight q5_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  242:          blk.26.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  244:             blk.27.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  245:             blk.27.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  246:             blk.27.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  247:        blk.27.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  248:           blk.27.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  249:             blk.27.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  250:           blk.27.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  252:           blk.27.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  253:             blk.28.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  254:             blk.28.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  255:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  256:        blk.28.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  257:           blk.28.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  258:             blk.28.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  259:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  260:          blk.28.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  262:             blk.29.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  263:             blk.29.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  264:             blk.29.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  265:        blk.29.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  266:           blk.29.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  267:             blk.29.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  268:           blk.29.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  270:           blk.29.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  271:             blk.30.attn_q.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  272:             blk.30.attn_k.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  273:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  274:        blk.30.attn_output.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  275:           blk.30.ffn_gate.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  276:             blk.30.ffn_up.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  277:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  278:          blk.30.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  280:             blk.31.attn_q.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  281:             blk.31.attn_k.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  282:             blk.31.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  283:        blk.31.attn_output.weight q5_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  284:           blk.31.ffn_gate.weight\
          \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  285:             blk.31.ffn_up.weight q5_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  286:           blk.31.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  288:           blk.31.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  289:               output_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  290:                    output.weight\
          \ q6_K     [  4096, 32256,     1,     1 ]\nllama_model_loader: - kv   0:\
          \                       general.architecture str              = llama\n\
          llama_model_loader: - kv   1:                               general.name\
          \ str              = deepseek-ai_deepseek-coder-6.7b-instruct\nllama_model_loader:\
          \ - kv   2:                       llama.context_length u32             \
          \ = 16384\nllama_model_loader: - kv   3:                     llama.embedding_length\
          \ u32              = 4096\nllama_model_loader: - kv   4:               \
          \           llama.block_count u32              = 32\nllama_model_loader:\
          \ - kv   5:                  llama.feed_forward_length u32             \
          \ = 11008\nllama_model_loader: - kv   6:                 llama.rope.dimension_count\
          \ u32              = 128\nllama_model_loader: - kv   7:                \
          \ llama.attention.head_count u32              = 32\nllama_model_loader:\
          \ - kv   8:              llama.attention.head_count_kv u32             \
          \ = 32\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
          \ f32              = 0,000001\nllama_model_loader: - kv  10:           \
          \            llama.rope.freq_base f32              = 100000,000000\nllama_model_loader:\
          \ - kv  11:                    llama.rope.scale_linear f32             \
          \ = 4,000000\nllama_model_loader: - kv  12:                          general.file_type\
          \ u32              = 17\nllama_model_loader: - kv  13:                 \
          \      tokenizer.ggml.model str              = gpt2\nllama_model_loader:\
          \ - kv  14:                      tokenizer.ggml.tokens arr[str,32256]  \
          \ = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader:\
          \ - kv  15:                      tokenizer.ggml.scores arr[f32,32256]  \
          \ = [0,000000, 0,000000, 0,000000, 0,0000...\nllama_model_loader: - kv \
          \ 16:                  tokenizer.ggml.token_type arr[i32,32256]   = [1,\
          \ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  17:  \
          \                    tokenizer.ggml.merges arr[str,31757]   = [\"\u0120\
          \ \u0120\", \"\u0120 t\", \"\u0120 a\", \"i n\", \"h e...\nllama_model_loader:\
          \ - kv  18:                tokenizer.ggml.bos_token_id u32             \
          \ = 32013\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id\
          \ u32              = 32021\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id\
          \ u32              = 32014\nllama_model_loader: - kv  21:              \
          \ general.quantization_version u32              = 2\nllama_model_loader:\
          \ - type  f32:   65 tensors\nllama_model_loader: - type q5_K:  193 tensors\n\
          llama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: mismatch\
          \ in special tokens definition ( 243/32256 vs 237/32256 ).\nllm_load_print_meta:\
          \ format           = GGUF V3 (latest)\nllm_load_print_meta: arch       \
          \      = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta:\
          \ n_vocab          = 32256\nllm_load_print_meta: n_merges         = 31757\n\
          llm_load_print_meta: n_ctx_train      = 16384\nllm_load_print_meta: n_embd\
          \           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta:\
          \ n_head_kv        = 32\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta:\
          \ n_rot            = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta:\
          \ f_norm_eps       = 0,0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1,0e-06\n\
          llm_load_print_meta: f_clamp_kqv      = 0,0e+00\nllm_load_print_meta: f_max_alibi_bias\
          \ = 0,0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta:\
          \ rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 100000,0\n\
          llm_load_print_meta: freq_scale_train = 0,25\nllm_load_print_meta: n_yarn_orig_ctx\
          \  = 16384\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta:\
          \ model type       = 7B\nllm_load_print_meta: model ftype      = mostly\
          \ Q5_K - Medium\nllm_load_print_meta: model params     = 6,74 B\nllm_load_print_meta:\
          \ model size       = 4,46 GiB (5,68 BPW) \nllm_load_print_meta: general.name\
          \   = deepseek-ai_deepseek-coder-6.7b-instruct\nllm_load_print_meta: BOS\
          \ token = 32013 '<\uFF5Cbegin\u2581of\u2581sentence\uFF5C>'\nllm_load_print_meta:\
          \ EOS token = 32021 '<|EOT|>'\nllm_load_print_meta: PAD token = 32014 '<\uFF5C\
          end\u2581of\u2581sentence\uFF5C>'\nllm_load_print_meta: LF token  = 126\
          \ '\xC4'\nllm_load_tensors: ggml ctx size =    0,11 MiB\nllm_load_tensors:\
          \ mem required  = 4562,48 MiB\n..................................................................................................\n\
          llama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model:\
          \ freq_base  = 100000,0\nllama_new_context_with_model: freq_scale = 0,25\n\
          llama_new_context_with_model: kv self size  =  256,00 MiB\nllama_build_graph:\
          \ non-view tensors processed: 740/740\nggml_metal_init: allocating\nggml_metal_init:\
          \ found device: Apple M1\nggml_metal_init: picking default device: Apple\
          \ M1\nggml_metal_init: default.metallib not found, loading from source\n\
          ggml_metal_init: loading '/Users/andreas/rozek/Node-RED/ggml-metal.metal'\n\
          ggml_metal_init: GPU name:   Apple M1\nggml_metal_init: GPU family: MTLGPUFamilyApple7\
          \ (1007)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init:\
          \ recommendedMaxWorkingSetSize  = 10922,67 MiB\nggml_metal_init: maxTransferRate\
          \               = built-in GPU\nllama_new_context_with_model: compute buffer\
          \ total size = 74,06 MiB\nllama_new_context_with_model: max tensor size\
          \ =   103,36 MiB\nggml_metal_add_buffer: allocated 'data            ' buffer,\
          \ size =  4563,62 MiB, ( 4564,12 / 10922,67)\nggml_metal_add_buffer: allocated\
          \ 'kv              ' buffer, size =   256,02 MiB, ( 4820,14 / 10922,67)\n\
          ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    71,02\
          \ MiB, ( 4891,16 / 10922,67)\n\nsystem_info: n_threads = 4 / 8 | AVX = 0\
          \ | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0\
          \ | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS\
          \ = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \nsampling: \n\trepeat_last_n =\
          \ 64, repeat_penalty = 1,100, frequency_penalty = 0,000, presence_penalty\
          \ = 0,000\n\ttop_k = 40, tfs_z = 1,000, top_p = 0,950, min_p = 0,050, typical_p\
          \ = 1,000, temp = 0,800\n\tmirostat = 0, mirostat_lr = 0,100, mirostat_ent\
          \ = 5,000\ngenerate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep\
          \ = 0\n\n\nwrite a quicksort algorithm in JavaScript that sorts an array\
          \ of numbers in place, using the Hoare partition scheme.\n\n\nfunction quickSort(arr)\
          \ {\n   if (arr.length < 2) return arr;\n    let pivotIndex = Math.floor(arr.length\
          \ / 2);\n    let pivotValue = arr[pivotIndex];\n    let lesserArray = [];\n\
          \    let greaterArray = [];\n    for (let i = 0; i < arr.length; i++) {\n\
          \        if (i !== pivotIndex){\n            arr[i] < pivotValue ? lesserArray.push(arr[i])\
          \ : greaterArray.push(arr[i]);\n        }\n        \n    }\n     return\
          \ [...quickSort(lesserArray), pivotValue, ...quickSort(greaterArray)]; \n\
          }\n\nconsole.log(quickSort([34,56,21,89,76])); //[ 21, 34, 56, 76, 89 ]//\
          \ write a JavaScript function that takes an array of numbers as input and\
          \ returns the sum of all even numbers in that array...\n```\n\nhere I stopped\
          \ the program because it began generating code I did not ask for.\n\nNevertheless,\
          \ it worked in principle"
        updatedAt: '2023-11-27T03:50:13.623Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - deanrie
    id: 656411f52cd8dac568b18f8b
    type: comment
  author: rozek
  content: "sure, no problem.\n\nHere is what I just ran (using a LLaMA.cpp executable\
    \ compiled approx. a week ago on a Mac mini M1 with 16GB running macOS 13.6.1\
    \ Ventura:\n\n```\n./llama --model ./deepseek-coder-6.7b-instruct.Q5_K_M.gguf\
    \ --prompt \"write a quicksort algorithm in JavaScript\"\n```\n\nThis command\
    \ produced the following output:\n\n```\nLog start\nmain: warning: changing RoPE\
    \ frequency base to 0 (default 10000.0)\nmain: warning: scaling RoPE frequency\
    \ by 0 (default 1.0)\nmain: build = 1597 (6707f74)\nmain: built with Apple clang\
    \ version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin22.6.0\nmain: seed\
    \  = 1701056675\nllama_model_loader: loaded meta data with 22 key-value pairs\
    \ and 291 tensors from ./deepseek-coder-6.7b-instruct.Q5_K_M.gguf (version GGUF\
    \ V3 (latest))\nllama_model_loader: - tensor    0:                token_embd.weight\
    \ q5_K     [  4096, 32256,     1,     1 ]\nllama_model_loader: - tensor    1:\
    \              blk.0.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor    2:              blk.0.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor    3:              blk.0.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    4:\
    \         blk.0.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor    5:            blk.0.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor    7:\
    \            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor    9:            blk.0.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   10:\
    \              blk.1.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   11:              blk.1.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   12:              blk.1.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   13:\
    \         blk.1.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   14:            blk.1.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor   15:              blk.1.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   16:\
    \            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   18:            blk.1.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   19:\
    \              blk.2.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   20:              blk.2.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   21:              blk.2.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   22:\
    \         blk.2.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   23:            blk.2.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor   24:              blk.2.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   25:\
    \            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   27:            blk.2.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   28:\
    \              blk.3.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   29:              blk.3.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   30:              blk.3.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   31:\
    \         blk.3.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   32:            blk.3.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor   33:              blk.3.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   34:\
    \            blk.3.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   36:            blk.3.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   37:\
    \              blk.4.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   38:              blk.4.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   39:              blk.4.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   40:\
    \         blk.4.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   41:            blk.4.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor   42:              blk.4.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   43:\
    \            blk.4.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   45:            blk.4.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   46:\
    \              blk.5.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   47:              blk.5.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   48:              blk.5.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   49:\
    \         blk.5.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   50:            blk.5.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor   51:              blk.5.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   52:\
    \            blk.5.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   54:            blk.5.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   55:\
    \              blk.6.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   56:              blk.6.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   57:              blk.6.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   58:\
    \         blk.6.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   59:            blk.6.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor   60:              blk.6.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   61:\
    \            blk.6.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   63:            blk.6.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   64:\
    \              blk.7.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   65:              blk.7.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   66:              blk.7.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   67:\
    \         blk.7.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   68:            blk.7.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor   69:              blk.7.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   70:\
    \            blk.7.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   72:            blk.7.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   73:\
    \              blk.8.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   74:              blk.8.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   75:              blk.8.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   76:\
    \         blk.8.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   77:            blk.8.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor   78:              blk.8.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   79:\
    \            blk.8.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   81:            blk.8.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   82:\
    \              blk.9.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   83:              blk.9.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   84:              blk.9.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   85:\
    \         blk.9.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   86:            blk.9.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor   87:              blk.9.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   88:\
    \            blk.9.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   90:            blk.9.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   91:\
    \             blk.10.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   92:             blk.10.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   93:             blk.10.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   94:\
    \        blk.10.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   95:           blk.10.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor   96:             blk.10.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   97:\
    \           blk.10.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   99:           blk.10.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  100:\
    \             blk.11.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  101:             blk.11.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  102:             blk.11.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  103:\
    \        blk.11.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  104:           blk.11.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  105:             blk.11.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  106:\
    \           blk.11.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  108:           blk.11.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  109:\
    \             blk.12.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  110:             blk.12.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  111:             blk.12.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  112:\
    \        blk.12.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  113:           blk.12.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  114:             blk.12.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  115:\
    \           blk.12.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  117:           blk.12.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  118:\
    \             blk.13.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  119:             blk.13.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  120:             blk.13.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  121:\
    \        blk.13.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  122:           blk.13.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  123:             blk.13.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  124:\
    \           blk.13.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  126:           blk.13.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  127:\
    \             blk.14.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  128:             blk.14.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  129:             blk.14.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  130:\
    \        blk.14.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  131:           blk.14.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  132:             blk.14.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  133:\
    \           blk.14.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  135:           blk.14.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  136:\
    \             blk.15.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  137:             blk.15.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  138:             blk.15.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  139:\
    \        blk.15.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  140:           blk.15.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  141:             blk.15.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  142:\
    \           blk.15.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  144:           blk.15.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  145:\
    \             blk.16.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  146:             blk.16.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  147:             blk.16.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  148:\
    \        blk.16.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  149:           blk.16.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  150:             blk.16.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  151:\
    \           blk.16.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  153:           blk.16.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  154:\
    \             blk.17.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  155:             blk.17.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  156:             blk.17.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  157:\
    \        blk.17.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  158:           blk.17.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  159:             blk.17.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  160:\
    \           blk.17.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  162:           blk.17.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  163:\
    \             blk.18.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  164:             blk.18.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  165:             blk.18.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  166:\
    \        blk.18.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  167:           blk.18.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  168:             blk.18.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  169:\
    \           blk.18.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  171:           blk.18.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  172:\
    \             blk.19.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  173:             blk.19.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  174:             blk.19.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  175:\
    \        blk.19.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  176:           blk.19.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  177:             blk.19.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  178:\
    \           blk.19.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  180:           blk.19.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  181:\
    \             blk.20.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  182:             blk.20.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  183:             blk.20.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  184:\
    \        blk.20.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  185:           blk.20.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  186:             blk.20.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  187:\
    \           blk.20.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  189:           blk.20.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  190:\
    \             blk.21.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  191:             blk.21.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  192:             blk.21.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  193:\
    \        blk.21.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  194:           blk.21.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  195:             blk.21.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  196:\
    \           blk.21.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  198:           blk.21.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  199:\
    \             blk.22.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  200:             blk.22.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  201:             blk.22.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  202:\
    \        blk.22.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  203:           blk.22.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  204:             blk.22.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  205:\
    \           blk.22.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  207:           blk.22.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  208:\
    \             blk.23.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  209:             blk.23.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  210:             blk.23.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  211:\
    \        blk.23.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  212:           blk.23.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  213:             blk.23.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  214:\
    \           blk.23.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  216:           blk.23.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  217:\
    \             blk.24.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  218:             blk.24.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  219:             blk.24.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  220:\
    \        blk.24.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  221:           blk.24.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  222:             blk.24.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  223:\
    \           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  225:           blk.24.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  226:\
    \             blk.25.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  227:             blk.25.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  228:             blk.25.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  229:\
    \        blk.25.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  230:           blk.25.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  231:             blk.25.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  232:\
    \           blk.25.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  234:           blk.25.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  235:\
    \             blk.26.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  236:             blk.26.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  237:             blk.26.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  238:\
    \        blk.26.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  239:           blk.26.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  240:             blk.26.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  241:\
    \           blk.26.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  243:           blk.26.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  244:\
    \             blk.27.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  245:             blk.27.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  246:             blk.27.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  247:\
    \        blk.27.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  248:           blk.27.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  249:             blk.27.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  250:\
    \           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  252:           blk.27.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  253:\
    \             blk.28.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  254:             blk.28.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  255:             blk.28.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  256:\
    \        blk.28.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  257:           blk.28.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  258:             blk.28.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  259:\
    \           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  261:           blk.28.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  262:\
    \             blk.29.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  263:             blk.29.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  264:             blk.29.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  265:\
    \        blk.29.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  266:           blk.29.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  267:             blk.29.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  268:\
    \           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  270:           blk.29.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  271:\
    \             blk.30.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  272:             blk.30.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  273:             blk.30.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  274:\
    \        blk.30.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  275:           blk.30.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  276:             blk.30.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  277:\
    \           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  279:           blk.30.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  280:\
    \             blk.31.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  281:             blk.31.attn_k.weight q5_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  282:             blk.31.attn_v.weight\
    \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  283:\
    \        blk.31.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  284:           blk.31.ffn_gate.weight q5_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor  285:             blk.31.ffn_up.weight\
    \ q5_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  286:\
    \           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  288:           blk.31.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  289:\
    \               output_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  290:                    output.weight q6_K     [  4096, 32256,   \
    \  1,     1 ]\nllama_model_loader: - kv   0:                       general.architecture\
    \ str              = llama\nllama_model_loader: - kv   1:                    \
    \           general.name str              = deepseek-ai_deepseek-coder-6.7b-instruct\n\
    llama_model_loader: - kv   2:                       llama.context_length u32 \
    \             = 16384\nllama_model_loader: - kv   3:                     llama.embedding_length\
    \ u32              = 4096\nllama_model_loader: - kv   4:                     \
    \     llama.block_count u32              = 32\nllama_model_loader: - kv   5: \
    \                 llama.feed_forward_length u32              = 11008\nllama_model_loader:\
    \ - kv   6:                 llama.rope.dimension_count u32              = 128\n\
    llama_model_loader: - kv   7:                 llama.attention.head_count u32 \
    \             = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv\
    \ u32              = 32\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
    \ f32              = 0,000001\nllama_model_loader: - kv  10:                 \
    \      llama.rope.freq_base f32              = 100000,000000\nllama_model_loader:\
    \ - kv  11:                    llama.rope.scale_linear f32              = 4,000000\n\
    llama_model_loader: - kv  12:                          general.file_type u32 \
    \             = 17\nllama_model_loader: - kv  13:                       tokenizer.ggml.model\
    \ str              = gpt2\nllama_model_loader: - kv  14:                     \
    \ tokenizer.ggml.tokens arr[str,32256]   = [\"!\", \"\\\"\", \"#\", \"$\", \"\
    %\", \"&\", \"'\", ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.scores\
    \ arr[f32,32256]   = [0,000000, 0,000000, 0,000000, 0,0000...\nllama_model_loader:\
    \ - kv  16:                  tokenizer.ggml.token_type arr[i32,32256]   = [1,\
    \ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  17:        \
    \              tokenizer.ggml.merges arr[str,31757]   = [\"\u0120 \u0120\", \"\
    \u0120 t\", \"\u0120 a\", \"i n\", \"h e...\nllama_model_loader: - kv  18:   \
    \             tokenizer.ggml.bos_token_id u32              = 32013\nllama_model_loader:\
    \ - kv  19:                tokenizer.ggml.eos_token_id u32              = 32021\n\
    llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32 \
    \             = 32014\nllama_model_loader: - kv  21:               general.quantization_version\
    \ u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader:\
    \ - type q5_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab:\
    \ mismatch in special tokens definition ( 243/32256 vs 237/32256 ).\nllm_load_print_meta:\
    \ format           = GGUF V3 (latest)\nllm_load_print_meta: arch             =\
    \ llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab\
    \          = 32256\nllm_load_print_meta: n_merges         = 31757\nllm_load_print_meta:\
    \ n_ctx_train      = 16384\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta:\
    \ n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta:\
    \ n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta:\
    \ n_gqa            = 1\nllm_load_print_meta: f_norm_eps       = 0,0e+00\nllm_load_print_meta:\
    \ f_norm_rms_eps   = 1,0e-06\nllm_load_print_meta: f_clamp_kqv      = 0,0e+00\n\
    llm_load_print_meta: f_max_alibi_bias = 0,0e+00\nllm_load_print_meta: n_ff   \
    \          = 11008\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta:\
    \ freq_base_train  = 100000,0\nllm_load_print_meta: freq_scale_train = 0,25\n\
    llm_load_print_meta: n_yarn_orig_ctx  = 16384\nllm_load_print_meta: rope_finetuned\
    \   = unknown\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta:\
    \ model ftype      = mostly Q5_K - Medium\nllm_load_print_meta: model params \
    \    = 6,74 B\nllm_load_print_meta: model size       = 4,46 GiB (5,68 BPW) \n\
    llm_load_print_meta: general.name   = deepseek-ai_deepseek-coder-6.7b-instruct\n\
    llm_load_print_meta: BOS token = 32013 '<\uFF5Cbegin\u2581of\u2581sentence\uFF5C\
    >'\nllm_load_print_meta: EOS token = 32021 '<|EOT|>'\nllm_load_print_meta: PAD\
    \ token = 32014 '<\uFF5Cend\u2581of\u2581sentence\uFF5C>'\nllm_load_print_meta:\
    \ LF token  = 126 '\xC4'\nllm_load_tensors: ggml ctx size =    0,11 MiB\nllm_load_tensors:\
    \ mem required  = 4562,48 MiB\n..................................................................................................\n\
    llama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model:\
    \ freq_base  = 100000,0\nllama_new_context_with_model: freq_scale = 0,25\nllama_new_context_with_model:\
    \ kv self size  =  256,00 MiB\nllama_build_graph: non-view tensors processed:\
    \ 740/740\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M1\n\
    ggml_metal_init: picking default device: Apple M1\nggml_metal_init: default.metallib\
    \ not found, loading from source\nggml_metal_init: loading '/Users/andreas/rozek/Node-RED/ggml-metal.metal'\n\
    ggml_metal_init: GPU name:   Apple M1\nggml_metal_init: GPU family: MTLGPUFamilyApple7\
    \ (1007)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init:\
    \ recommendedMaxWorkingSetSize  = 10922,67 MiB\nggml_metal_init: maxTransferRate\
    \               = built-in GPU\nllama_new_context_with_model: compute buffer total\
    \ size = 74,06 MiB\nllama_new_context_with_model: max tensor size =   103,36 MiB\n\
    ggml_metal_add_buffer: allocated 'data            ' buffer, size =  4563,62 MiB,\
    \ ( 4564,12 / 10922,67)\nggml_metal_add_buffer: allocated 'kv              ' buffer,\
    \ size =   256,02 MiB, ( 4820,14 / 10922,67)\nggml_metal_add_buffer: allocated\
    \ 'alloc           ' buffer, size =    71,02 MiB, ( 4891,16 / 10922,67)\n\nsystem_info:\
    \ n_threads = 4 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
    \ = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD\
    \ = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \nsampling: \n\trepeat_last_n\
    \ = 64, repeat_penalty = 1,100, frequency_penalty = 0,000, presence_penalty =\
    \ 0,000\n\ttop_k = 40, tfs_z = 1,000, top_p = 0,950, min_p = 0,050, typical_p\
    \ = 1,000, temp = 0,800\n\tmirostat = 0, mirostat_lr = 0,100, mirostat_ent = 5,000\n\
    generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n\n\nwrite a\
    \ quicksort algorithm in JavaScript that sorts an array of numbers in place, using\
    \ the Hoare partition scheme.\n\n\nfunction quickSort(arr) {\n   if (arr.length\
    \ < 2) return arr;\n    let pivotIndex = Math.floor(arr.length / 2);\n    let\
    \ pivotValue = arr[pivotIndex];\n    let lesserArray = [];\n    let greaterArray\
    \ = [];\n    for (let i = 0; i < arr.length; i++) {\n        if (i !== pivotIndex){\n\
    \            arr[i] < pivotValue ? lesserArray.push(arr[i]) : greaterArray.push(arr[i]);\n\
    \        }\n        \n    }\n     return [...quickSort(lesserArray), pivotValue,\
    \ ...quickSort(greaterArray)]; \n}\n\nconsole.log(quickSort([34,56,21,89,76]));\
    \ //[ 21, 34, 56, 76, 89 ]// write a JavaScript function that takes an array of\
    \ numbers as input and returns the sum of all even numbers in that array...\n\
    ```\n\nhere I stopped the program because it began generating code I did not ask\
    \ for.\n\nNevertheless, it worked in principle"
  created_at: 2023-11-27 03:50:13+00:00
  edited: false
  hidden: false
  id: 656411f52cd8dac568b18f8b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/deepseek-coder-6.7B-instruct-GGUF
repo_type: model
status: open
target_branch: null
title: not working
