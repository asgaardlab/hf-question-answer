!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sgugger
conflicting_files:
- pytorch_model.bin
created_at: 2023-06-20 14:29:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
      fullname: Sylvain Gugger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgugger
      type: user
    createdAt: '2023-06-20T15:29:46.000Z'
    data:
      edited: false
      editors:
      - sgugger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9755750894546509
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
          fullname: Sylvain Gugger
          isHf: false
          isPro: false
          name: sgugger
          type: user
        html: '<p>There was probably a bug in the initial conversion script that created
          those models, as the weights they have have a<br>different value for <code>lm_head.weight</code>
          and <code>model.decoder.embed_tokens.weight</code>. Those models are tied
          though.</p>

          <p>This was not a problem until now as the model was tied after the load
          and the (wrong) value of <code>lm_head.weight</code> was<br>replaced by
          the value of <code>model.decoder.embed_tokens.weight</code>. This does not
          work any more if we tie the weights before<br>the load however, as the value
          picked might be the one from <code>lm_head.weight</code> depending on how
          the models are tied.<br>As far as I can see, the model stop generating properly
          on Transformers main.</p>

          <p>This should fix the bug without any side effect.</p>

          '
        raw: 'There was probably a bug in the initial conversion script that created
          those models, as the weights they have have a

          different value for `lm_head.weight` and `model.decoder.embed_tokens.weight`.
          Those models are tied though.


          This was not a problem until now as the model was tied after the load and
          the (wrong) value of `lm_head.weight` was

          replaced by the value of `model.decoder.embed_tokens.weight`. This does
          not work any more if we tie the weights before

          the load however, as the value picked might be the one from `lm_head.weight`
          depending on how the models are tied.

          As far as I can see, the model stop generating properly on Transformers
          main.


          This should fix the bug without any side effect.'
        updatedAt: '2023-06-20T15:29:46.481Z'
      numEdits: 0
      reactions: []
    id: 6491c5eaf847621c795c5c55
    type: comment
  author: sgugger
  content: 'There was probably a bug in the initial conversion script that created
    those models, as the weights they have have a

    different value for `lm_head.weight` and `model.decoder.embed_tokens.weight`.
    Those models are tied though.


    This was not a problem until now as the model was tied after the load and the
    (wrong) value of `lm_head.weight` was

    replaced by the value of `model.decoder.embed_tokens.weight`. This does not work
    any more if we tie the weights before

    the load however, as the value picked might be the one from `lm_head.weight` depending
    on how the models are tied.

    As far as I can see, the model stop generating properly on Transformers main.


    This should fix the bug without any side effect.'
  created_at: 2023-06-20 14:29:46+00:00
  edited: false
  hidden: false
  id: 6491c5eaf847621c795c5c55
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
      fullname: Sylvain Gugger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgugger
      type: user
    createdAt: '2023-06-20T15:29:47.000Z'
    data:
      oid: de0b0a6f9b981a7cea6e43a2508653aac066b1ca
      parents:
      - 4aa0c45d7c42c3097c5681ef8af73826b06a6cd2
      subject: Fix weights by putting the right value in `lm_head.weight`
    id: 6491c5eb0000000000000000
    type: commit
  author: sgugger
  created_at: 2023-06-20 14:29:47+00:00
  id: 6491c5eb0000000000000000
  oid: de0b0a6f9b981a7cea6e43a2508653aac066b1ca
  summary: Fix weights by putting the right value in `lm_head.weight`
  type: commit
is_pull_request: true
merge_commit_oid: null
num: 2
repo_id: Helsinki-NLP/opus-tatoeba-af-ru
repo_type: model
status: open
target_branch: refs/heads/main
title: Fix weights by putting the right value in `lm_head.weight`
