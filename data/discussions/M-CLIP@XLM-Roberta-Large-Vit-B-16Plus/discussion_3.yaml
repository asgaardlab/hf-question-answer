!!python/object:huggingface_hub.community.DiscussionWithDetails
author: floschne
conflicting_files: null
created_at: 2023-01-16 18:59:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62dfd54798815401141c47fe/6vk0hfoSqt1cb-Xu55JZu.jpeg?w=200&h=200&f=face
      fullname: Flo Schneider
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: floschne
      type: user
    createdAt: '2023-01-16T18:59:19.000Z'
    data:
      edited: false
      editors:
      - floschne
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62dfd54798815401141c47fe/6vk0hfoSqt1cb-Xu55JZu.jpeg?w=200&h=200&f=face
          fullname: Flo Schneider
          isHf: false
          isPro: false
          name: floschne
          type: user
        html: '<p>Hi and thanks for you M-CLIP publications ! :)</p>

          <p>I''m just trying this model and compared to the multilingual models from
          sentence-transformers or the large laion xlm-roberta models, the inference
          performance is very slow. I.e., about 61x slower than ''sentence-transformers-clip-ViT-B-32-multilingual-v1''
          and about 4x slower than the ''laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90''.</p>

          <p>Do you have an idea, how to boost performance? I''m running the models
          on a RTX A 6000 (50GB)</p>

          '
        raw: "Hi and thanks for you M-CLIP publications ! :)\r\n\r\nI'm just trying\
          \ this model and compared to the multilingual models from sentence-transformers\
          \ or the large laion xlm-roberta models, the inference performance is very\
          \ slow. I.e., about 61x slower than 'sentence-transformers-clip-ViT-B-32-multilingual-v1'\
          \ and about 4x slower than the 'laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90'.\r\
          \n\r\nDo you have an idea, how to boost performance? I'm running the models\
          \ on a RTX A 6000 (50GB)\r\n"
        updatedAt: '2023-01-16T18:59:19.568Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - peixiang
    id: 63c59e87e2804cb2407b93fc
    type: comment
  author: floschne
  content: "Hi and thanks for you M-CLIP publications ! :)\r\n\r\nI'm just trying\
    \ this model and compared to the multilingual models from sentence-transformers\
    \ or the large laion xlm-roberta models, the inference performance is very slow.\
    \ I.e., about 61x slower than 'sentence-transformers-clip-ViT-B-32-multilingual-v1'\
    \ and about 4x slower than the 'laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90'.\r\
    \n\r\nDo you have an idea, how to boost performance? I'm running the models on\
    \ a RTX A 6000 (50GB)\r\n"
  created_at: 2023-01-16 18:59:19+00:00
  edited: false
  hidden: false
  id: 63c59e87e2804cb2407b93fc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: M-CLIP/XLM-Roberta-Large-Vit-B-16Plus
repo_type: model
status: open
target_branch: null
title: Slow inference
