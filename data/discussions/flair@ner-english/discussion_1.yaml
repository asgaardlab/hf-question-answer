!!python/object:huggingface_hub.community.DiscussionWithDetails
author: initesh
conflicting_files: null
created_at: 2022-09-17 14:44:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d82828eeea96274540c72c31003c951.svg
      fullname: Nitesh Methani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: initesh
      type: user
    createdAt: '2022-09-17T15:44:20.000Z'
    data:
      edited: false
      editors:
      - initesh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d82828eeea96274540c72c31003c951.svg
          fullname: Nitesh Methani
          isHf: false
          isPro: false
          name: initesh
          type: user
        html: "<p>Hi,</p>\n<p>I have pre-trained FastText as well as bert-base-uncased\
          \ on custom corpus.<br>How can I use it for NER training?</p>\n<p>Specifically,\
          \ is something like this possible in flair:</p>\n<pre><code>embedding_types\
          \ = [\n    WordEmbeddings('path to custom fasttext vectors'), # .txt file\
          \ generated from fasttext\n    FlairEmbeddings('path to custom flair-forward\
          \ vectors'),\n    FlairEmbeddings('path to custom flair-backward vectors'),\n\
          ]\n</code></pre>\n<p>Furthermore,<br>When we use this as embedder, what\
          \ happens behind the scenes?</p>\n<pre><code>embeddings = TransformerWordEmbeddings(model='xlm-roberta-large',\n\
          \                                       layers=\"-1\",\n               \
          \                        subtoken_pooling=\"first\",\n                 \
          \                      fine_tune=True,\n                               \
          \        use_context=True,\n                                       )\n</code></pre>\n\
          <ul>\n<li>Is it simply taking the token embeddings from 'xlm-roberta-large'\
          \ and puts a linear layer for NER on top of it? </li>\n<li>Where exactly\
          \ FLERT's functionalities are used? Are they automatically handled behind\
          \ the scenes?<ul>\n<li>If yes, how can I turn off FLERT's features so that\
          \ I can compare the gains we are getting before and after using FLERT</li>\n\
          </ul>\n</li>\n<li>I have pre-trained \"bert-base-uncased\" on custom dataset.\
          \ How should I use this instead of \"xlm-roberta-large\"?</li>\n<li>Any\
          \ suggestions on whether or not we should use CRF layer on top of bert embeddings\
          \ for NER tasks</li>\n<li>The flag \"use_rnn\" what exactly it does? If\
          \ I switch it off, what does it do? Will it switch off char-rnn layer or\
          \ word-rnn layer?</li>\n</ul>\n<p>I know these are lot of questions but\
          \ simplicity of Flair enabled me to quickly run experiments and hence the\
          \ curiosity :)</p>\n<p>-Nitesh</p>\n"
        raw: "Hi,\r\n\r\nI have pre-trained FastText as well as bert-base-uncased\
          \ on custom corpus.\r\nHow can I use it for NER training?\r\n\r\nSpecifically,\
          \ is something like this possible in flair:\r\n```\r\nembedding_types =\
          \ [\r\n    WordEmbeddings('path to custom fasttext vectors'), # .txt file\
          \ generated from fasttext\r\n    FlairEmbeddings('path to custom flair-forward\
          \ vectors'),\r\n    FlairEmbeddings('path to custom flair-backward vectors'),\r\
          \n]\r\n```\r\n\r\nFurthermore, \r\nWhen we use this as embedder, what happens\
          \ behind the scenes?\r\n```\r\nembeddings = TransformerWordEmbeddings(model='xlm-roberta-large',\r\
          \n                                       layers=\"-1\",\r\n            \
          \                           subtoken_pooling=\"first\",\r\n            \
          \                           fine_tune=True,\r\n                        \
          \               use_context=True,\r\n                                  \
          \     )\r\n```\r\n- Is it simply taking the token embeddings from 'xlm-roberta-large'\
          \ and puts a linear layer for NER on top of it? \r\n- Where exactly FLERT's\
          \ functionalities are used? Are they automatically handled behind the scenes?\r\
          \n    - If yes, how can I turn off FLERT's features so that I can compare\
          \ the gains we are getting before and after using FLERT\r\n- I have pre-trained\
          \ \"bert-base-uncased\" on custom dataset. How should I use this instead\
          \ of \"xlm-roberta-large\"?\r\n- Any suggestions on whether or not we should\
          \ use CRF layer on top of bert embeddings for NER tasks\r\n- The flag \"\
          use_rnn\" what exactly it does? If I switch it off, what does it do? Will\
          \ it switch off char-rnn layer or word-rnn layer?\r\n\r\nI know these are\
          \ lot of questions but simplicity of Flair enabled me to quickly run experiments\
          \ and hence the curiosity :)\r\n\r\n\r\n-Nitesh"
        updatedAt: '2022-09-17T15:44:20.653Z'
      numEdits: 0
      reactions: []
    id: 6325eb544f4f4ea5a860c556
    type: comment
  author: initesh
  content: "Hi,\r\n\r\nI have pre-trained FastText as well as bert-base-uncased on\
    \ custom corpus.\r\nHow can I use it for NER training?\r\n\r\nSpecifically, is\
    \ something like this possible in flair:\r\n```\r\nembedding_types = [\r\n   \
    \ WordEmbeddings('path to custom fasttext vectors'), # .txt file generated from\
    \ fasttext\r\n    FlairEmbeddings('path to custom flair-forward vectors'),\r\n\
    \    FlairEmbeddings('path to custom flair-backward vectors'),\r\n]\r\n```\r\n\
    \r\nFurthermore, \r\nWhen we use this as embedder, what happens behind the scenes?\r\
    \n```\r\nembeddings = TransformerWordEmbeddings(model='xlm-roberta-large',\r\n\
    \                                       layers=\"-1\",\r\n                   \
    \                    subtoken_pooling=\"first\",\r\n                         \
    \              fine_tune=True,\r\n                                       use_context=True,\r\
    \n                                       )\r\n```\r\n- Is it simply taking the\
    \ token embeddings from 'xlm-roberta-large' and puts a linear layer for NER on\
    \ top of it? \r\n- Where exactly FLERT's functionalities are used? Are they automatically\
    \ handled behind the scenes?\r\n    - If yes, how can I turn off FLERT's features\
    \ so that I can compare the gains we are getting before and after using FLERT\r\
    \n- I have pre-trained \"bert-base-uncased\" on custom dataset. How should I use\
    \ this instead of \"xlm-roberta-large\"?\r\n- Any suggestions on whether or not\
    \ we should use CRF layer on top of bert embeddings for NER tasks\r\n- The flag\
    \ \"use_rnn\" what exactly it does? If I switch it off, what does it do? Will\
    \ it switch off char-rnn layer or word-rnn layer?\r\n\r\nI know these are lot\
    \ of questions but simplicity of Flair enabled me to quickly run experiments and\
    \ hence the curiosity :)\r\n\r\n\r\n-Nitesh"
  created_at: 2022-09-17 14:44:20+00:00
  edited: false
  hidden: false
  id: 6325eb544f4f4ea5a860c556
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: flair/ner-english
repo_type: model
status: open
target_branch: null
title: Doubts around NER
