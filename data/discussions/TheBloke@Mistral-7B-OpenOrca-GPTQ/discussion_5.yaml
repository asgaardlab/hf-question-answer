!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ayenem
conflicting_files: null
created_at: 2023-10-06 18:45:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f17bee7bfc587b5899586c4193f65986.svg
      fullname: Ahmed Moubtahij
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ayenem
      type: user
    createdAt: '2023-10-06T19:45:30.000Z'
    data:
      edited: true
      editors:
      - Ayenem
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.30462417006492615
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f17bee7bfc587b5899586c4193f65986.svg
          fullname: Ahmed Moubtahij
          isHf: false
          isPro: false
          name: Ayenem
          type: user
        html: "<pre><code>Traceback (most recent call last):\n  File \".../demo.py\"\
          , line 168, in docker_pipeline\n    llm_output = rag_chain(instruction)\n\
          \                 ^^^^^^^^^^^^^^^^^^^^^^\n  File \".../langchain/chains/base.py\"\
          , line 306, in __call__\n    raise e\n  File \".../langchain/chains/base.py\"\
          , line 300, in __call__\n    self._call(inputs, run_manager=run_manager)\n\
          \  File \".../langchain/chains/retrieval_qa/base.py\", line 139, in _call\n\
          \    answer = self.combine_documents_chain.run(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../langchain/chains/base.py\", line 506, in run\n    return self(kwargs,\
          \ callbacks=callbacks, tags=tags, metadata=metadata)[\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../langchain/chains/base.py\", line 306, in __call__\n    raise\
          \ e\n  File \".../langchain/chains/base.py\", line 300, in __call__\n  \
          \  self._call(inputs, run_manager=run_manager)\n  File \".../langchain/chains/combine_documents/base.py\"\
          , line 119, in _call\n    output, extra_return_dict = self.combine_docs(\n\
          \                                ^^^^^^^^^^^^^^^^^^\n  File \".../langchain/chains/combine_documents/stuff.py\"\
          , line 171, in combine_docs\n    return self.llm_chain.predict(callbacks=callbacks,\
          \ **inputs), {}\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../langchain/chains/llm.py\", line 257, in predict\n    return\
          \ self(kwargs, callbacks=callbacks)[self.output_key]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../langchain/chains/base.py\", line 306, in __call__\n    raise\
          \ e\n  File \".../langchain/chains/base.py\", line 300, in __call__\n  \
          \  self._call(inputs, run_manager=run_manager)\n  File \".../langchain/chains/llm.py\"\
          , line 93, in _call\n    response = self.generate([inputs], run_manager=run_manager)\n\
          \               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \".../langchain/chains/llm.py\", line 103, in generate\n    return self.llm.generate_prompt(\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".../langchain/llms/base.py\"\
          , line 498, in generate_prompt\n    return self.generate(prompt_strings,\
          \ stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../langchain/llms/base.py\", line 647, in generate\n    output\
          \ = self._generate_helper(\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \".../langchain/llms/base.py\", line 535, in _generate_helper\n    raise\
          \ e\n  File \".../langchain/llms/base.py\", line 522, in _generate_helper\n\
          \    self._generate(\n  File \".../langchain/llms/huggingface_pipeline.py\"\
          , line 183, in _generate\n    responses = self.pipeline(batch_prompts)\n\
          \                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".../transformers/pipelines/text_generation.py\"\
          , line 208, in __call__\n    return super().__call__(text_inputs, **kwargs)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".../transformers/pipelines/base.py\"\
          , line 1121, in __call__\n    outputs = list(final_iterator)\n         \
          \     ^^^^^^^^^^^^^^^^^^^^\n  File \".../transformers/pipelines/pt_utils.py\"\
          , line 124, in __next__\n    item = next(self.iterator)\n           ^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../transformers/pipelines/pt_utils.py\", line 125, in __next__\n\
          \    processed = self.infer(item, **self.params)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../transformers/pipelines/base.py\", line 1046, in forward\n\
          \    model_outputs = self._forward(model_inputs, **forward_params)\n   \
          \                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \".../transformers/pipelines/text_generation.py\", line 271, in _forward\n\
          \    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask,\
          \ **generate_kwargs)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../torch/utils/_contextlib.py\", line 115, in decorate_context\n\
          \    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \".../transformers/generation/utils.py\", line 1652, in generate\n   \
          \ return self.sample(\n           ^^^^^^^^^^^^\n  File \".../transformers/generation/utils.py\"\
          , line 2775, in sample\n    raise ValueError(\"If `eos_token_id` is defined,\
          \ make sure that `pad_token_id` is defined.\")\nValueError: If `eos_token_id`\
          \ is defined, make sure that `pad_token_id` is defined.\n</code></pre>\n\
          <p>I looked up <a href=\"https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/blob/main/tokenizer_config.json#L52\"\
          >https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/blob/main/tokenizer_config.json#L52</a>\
          \ and it is indeed <code>null</code>. Is that what it's supposed to be?\
          \ And if so, how can I deal with this error?</p>\n"
        raw: "```\nTraceback (most recent call last):\n  File \".../demo.py\", line\
          \ 168, in docker_pipeline\n    llm_output = rag_chain(instruction)\n   \
          \              ^^^^^^^^^^^^^^^^^^^^^^\n  File \".../langchain/chains/base.py\"\
          , line 306, in __call__\n    raise e\n  File \".../langchain/chains/base.py\"\
          , line 300, in __call__\n    self._call(inputs, run_manager=run_manager)\n\
          \  File \".../langchain/chains/retrieval_qa/base.py\", line 139, in _call\n\
          \    answer = self.combine_documents_chain.run(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../langchain/chains/base.py\", line 506, in run\n    return self(kwargs,\
          \ callbacks=callbacks, tags=tags, metadata=metadata)[\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../langchain/chains/base.py\", line 306, in __call__\n    raise\
          \ e\n  File \".../langchain/chains/base.py\", line 300, in __call__\n  \
          \  self._call(inputs, run_manager=run_manager)\n  File \".../langchain/chains/combine_documents/base.py\"\
          , line 119, in _call\n    output, extra_return_dict = self.combine_docs(\n\
          \                                ^^^^^^^^^^^^^^^^^^\n  File \".../langchain/chains/combine_documents/stuff.py\"\
          , line 171, in combine_docs\n    return self.llm_chain.predict(callbacks=callbacks,\
          \ **inputs), {}\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../langchain/chains/llm.py\", line 257, in predict\n    return\
          \ self(kwargs, callbacks=callbacks)[self.output_key]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../langchain/chains/base.py\", line 306, in __call__\n    raise\
          \ e\n  File \".../langchain/chains/base.py\", line 300, in __call__\n  \
          \  self._call(inputs, run_manager=run_manager)\n  File \".../langchain/chains/llm.py\"\
          , line 93, in _call\n    response = self.generate([inputs], run_manager=run_manager)\n\
          \               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \".../langchain/chains/llm.py\", line 103, in generate\n    return self.llm.generate_prompt(\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".../langchain/llms/base.py\"\
          , line 498, in generate_prompt\n    return self.generate(prompt_strings,\
          \ stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../langchain/llms/base.py\", line 647, in generate\n    output\
          \ = self._generate_helper(\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \".../langchain/llms/base.py\", line 535, in _generate_helper\n    raise\
          \ e\n  File \".../langchain/llms/base.py\", line 522, in _generate_helper\n\
          \    self._generate(\n  File \".../langchain/llms/huggingface_pipeline.py\"\
          , line 183, in _generate\n    responses = self.pipeline(batch_prompts)\n\
          \                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".../transformers/pipelines/text_generation.py\"\
          , line 208, in __call__\n    return super().__call__(text_inputs, **kwargs)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".../transformers/pipelines/base.py\"\
          , line 1121, in __call__\n    outputs = list(final_iterator)\n         \
          \     ^^^^^^^^^^^^^^^^^^^^\n  File \".../transformers/pipelines/pt_utils.py\"\
          , line 124, in __next__\n    item = next(self.iterator)\n           ^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../transformers/pipelines/pt_utils.py\", line 125, in __next__\n\
          \    processed = self.infer(item, **self.params)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../transformers/pipelines/base.py\", line 1046, in forward\n\
          \    model_outputs = self._forward(model_inputs, **forward_params)\n   \
          \                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \".../transformers/pipelines/text_generation.py\", line 271, in _forward\n\
          \    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask,\
          \ **generate_kwargs)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \".../torch/utils/_contextlib.py\", line 115, in decorate_context\n\
          \    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \".../transformers/generation/utils.py\", line 1652, in generate\n   \
          \ return self.sample(\n           ^^^^^^^^^^^^\n  File \".../transformers/generation/utils.py\"\
          , line 2775, in sample\n    raise ValueError(\"If `eos_token_id` is defined,\
          \ make sure that `pad_token_id` is defined.\")\nValueError: If `eos_token_id`\
          \ is defined, make sure that `pad_token_id` is defined.\n```\nI looked up\
          \ https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/blob/main/tokenizer_config.json#L52\
          \ and it is indeed `null`. Is that what it's supposed to be? And if so,\
          \ how can I deal with this error?"
        updatedAt: '2023-10-07T09:10:42.172Z'
      numEdits: 1
      reactions: []
    id: 652063da9004117947e3f300
    type: comment
  author: Ayenem
  content: "```\nTraceback (most recent call last):\n  File \".../demo.py\", line\
    \ 168, in docker_pipeline\n    llm_output = rag_chain(instruction)\n         \
    \        ^^^^^^^^^^^^^^^^^^^^^^\n  File \".../langchain/chains/base.py\", line\
    \ 306, in __call__\n    raise e\n  File \".../langchain/chains/base.py\", line\
    \ 300, in __call__\n    self._call(inputs, run_manager=run_manager)\n  File \"\
    .../langchain/chains/retrieval_qa/base.py\", line 139, in _call\n    answer =\
    \ self.combine_documents_chain.run(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \".../langchain/chains/base.py\", line 506, in run\n    return self(kwargs,\
    \ callbacks=callbacks, tags=tags, metadata=metadata)[\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \".../langchain/chains/base.py\", line 306, in __call__\n    raise e\n\
    \  File \".../langchain/chains/base.py\", line 300, in __call__\n    self._call(inputs,\
    \ run_manager=run_manager)\n  File \".../langchain/chains/combine_documents/base.py\"\
    , line 119, in _call\n    output, extra_return_dict = self.combine_docs(\n   \
    \                             ^^^^^^^^^^^^^^^^^^\n  File \".../langchain/chains/combine_documents/stuff.py\"\
    , line 171, in combine_docs\n    return self.llm_chain.predict(callbacks=callbacks,\
    \ **inputs), {}\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \".../langchain/chains/llm.py\", line 257, in predict\n    return self(kwargs,\
    \ callbacks=callbacks)[self.output_key]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \".../langchain/chains/base.py\", line 306, in __call__\n    raise e\n\
    \  File \".../langchain/chains/base.py\", line 300, in __call__\n    self._call(inputs,\
    \ run_manager=run_manager)\n  File \".../langchain/chains/llm.py\", line 93, in\
    \ _call\n    response = self.generate([inputs], run_manager=run_manager)\n   \
    \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".../langchain/chains/llm.py\"\
    , line 103, in generate\n    return self.llm.generate_prompt(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \".../langchain/llms/base.py\", line 498, in generate_prompt\n    return\
    \ self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n  \
    \         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \".../langchain/llms/base.py\", line 647, in generate\n    output = self._generate_helper(\n\
    \             ^^^^^^^^^^^^^^^^^^^^^^\n  File \".../langchain/llms/base.py\", line\
    \ 535, in _generate_helper\n    raise e\n  File \".../langchain/llms/base.py\"\
    , line 522, in _generate_helper\n    self._generate(\n  File \".../langchain/llms/huggingface_pipeline.py\"\
    , line 183, in _generate\n    responses = self.pipeline(batch_prompts)\n     \
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".../transformers/pipelines/text_generation.py\"\
    , line 208, in __call__\n    return super().__call__(text_inputs, **kwargs)\n\
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".../transformers/pipelines/base.py\"\
    , line 1121, in __call__\n    outputs = list(final_iterator)\n              ^^^^^^^^^^^^^^^^^^^^\n\
    \  File \".../transformers/pipelines/pt_utils.py\", line 124, in __next__\n  \
    \  item = next(self.iterator)\n           ^^^^^^^^^^^^^^^^^^^\n  File \".../transformers/pipelines/pt_utils.py\"\
    , line 125, in __next__\n    processed = self.infer(item, **self.params)\n   \
    \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".../transformers/pipelines/base.py\"\
    , line 1046, in forward\n    model_outputs = self._forward(model_inputs, **forward_params)\n\
    \                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
    .../transformers/pipelines/text_generation.py\", line 271, in _forward\n    generated_sequence\
    \ = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n\
    \                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \".../torch/utils/_contextlib.py\", line 115, in decorate_context\n  \
    \  return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \".../transformers/generation/utils.py\"\
    , line 1652, in generate\n    return self.sample(\n           ^^^^^^^^^^^^\n \
    \ File \".../transformers/generation/utils.py\", line 2775, in sample\n    raise\
    \ ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is\
    \ defined.\")\nValueError: If `eos_token_id` is defined, make sure that `pad_token_id`\
    \ is defined.\n```\nI looked up https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/blob/main/tokenizer_config.json#L52\
    \ and it is indeed `null`. Is that what it's supposed to be? And if so, how can\
    \ I deal with this error?"
  created_at: 2023-10-06 18:45:30+00:00
  edited: true
  hidden: false
  id: 652063da9004117947e3f300
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/63bfb4c9341b2429b0e332e700a9b1c2.svg
      fullname: Kiran M Prajapati
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KiranMP
      type: user
    createdAt: '2023-12-11T07:21:44.000Z'
    data:
      edited: false
      editors:
      - KiranMP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6330314874649048
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/63bfb4c9341b2429b0e332e700a9b1c2.svg
          fullname: Kiran M Prajapati
          isHf: false
          isPro: false
          name: KiranMP
          type: user
        html: '<p><code>model.generate(**input_ids, pad_token_id=tokenizer.eos_token_id,
          ...)</code> Did you tried replacing pad token with eos token? This should
          work.</p>

          '
        raw: '```model.generate(**input_ids, pad_token_id=tokenizer.eos_token_id,
          ...)``` Did you tried replacing pad token with eos token? This should work.'
        updatedAt: '2023-12-11T07:21:44.182Z'
      numEdits: 0
      reactions: []
    id: 6576b88868ea3e91dcc5840f
    type: comment
  author: KiranMP
  content: '```model.generate(**input_ids, pad_token_id=tokenizer.eos_token_id, ...)```
    Did you tried replacing pad token with eos token? This should work.'
  created_at: 2023-12-11 07:21:44+00:00
  edited: false
  hidden: false
  id: 6576b88868ea3e91dcc5840f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/Mistral-7B-OpenOrca-GPTQ
repo_type: model
status: open
target_branch: null
title: 'ValueError: If `eos_token_id` is defined, make sure that `pad_token_id` is
  defined.'
