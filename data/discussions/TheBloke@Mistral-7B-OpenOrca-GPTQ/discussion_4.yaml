!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ayenem
conflicting_files: null
created_at: 2023-10-06 14:42:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f17bee7bfc587b5899586c4193f65986.svg
      fullname: Ahmed Moubtahij
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ayenem
      type: user
    createdAt: '2023-10-06T15:42:07.000Z'
    data:
      edited: true
      editors:
      - Ayenem
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.39538732171058655
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f17bee7bfc587b5899586c4193f65986.svg
          fullname: Ahmed Moubtahij
          isHf: false
          isPro: false
          name: Ayenem
          type: user
        html: "<pre><code>    model = AutoGPTQForCausalLM.from_quantized(\n      \
          \      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \".../auto_gptq/modeling/auto.py\"\
          , line 87, in from_quantized\n    model_type = check_and_get_model_type(model_name_or_path,\
          \ trust_remote_code)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \".../auto_gptq/modeling/_utils.py\", line 149, in check_and_get_model_type\n\
          \    raise TypeError(f\"{config.model_type} isn't supported yet.\")\nTypeError:\
          \ mistral isn't supported yet.\n</code></pre>\n<p>I followed instructions\
          \ from <a href=\"https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ#how-to-use-this-gptq-model-from-python-code\"\
          >https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ#how-to-use-this-gptq-model-from-python-code</a>\
          \ (I tried installations from wheel and from source).<br>Is mistral not\
          \ runnable with autoGPTQ yet?</p>\n<p>(same problem with <a href=\"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\
          >https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ</a>)</p>\n"
        raw: "``` \n    model = AutoGPTQForCausalLM.from_quantized(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \".../auto_gptq/modeling/auto.py\", line 87, in from_quantized\n  \
          \  model_type = check_and_get_model_type(model_name_or_path, trust_remote_code)\n\
          \                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \".../auto_gptq/modeling/_utils.py\", line 149, in check_and_get_model_type\n\
          \    raise TypeError(f\"{config.model_type} isn't supported yet.\")\nTypeError:\
          \ mistral isn't supported yet.\n```\n\nI followed instructions from <https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ#how-to-use-this-gptq-model-from-python-code>\
          \ (I tried installations from wheel and from source).\nIs mistral not runnable\
          \ with autoGPTQ yet?\n\n(same problem with https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ)"
        updatedAt: '2023-10-06T16:00:49.860Z'
      numEdits: 1
      reactions: []
    id: 65202acfb0e0d57453c5daef
    type: comment
  author: Ayenem
  content: "``` \n    model = AutoGPTQForCausalLM.from_quantized(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \".../auto_gptq/modeling/auto.py\", line 87, in from_quantized\n    model_type\
    \ = check_and_get_model_type(model_name_or_path, trust_remote_code)\n        \
    \         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile\
    \ \".../auto_gptq/modeling/_utils.py\", line 149, in check_and_get_model_type\n\
    \    raise TypeError(f\"{config.model_type} isn't supported yet.\")\nTypeError:\
    \ mistral isn't supported yet.\n```\n\nI followed instructions from <https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ#how-to-use-this-gptq-model-from-python-code>\
    \ (I tried installations from wheel and from source).\nIs mistral not runnable\
    \ with autoGPTQ yet?\n\n(same problem with https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ)"
  created_at: 2023-10-06 14:42:07+00:00
  edited: true
  hidden: false
  id: 65202acfb0e0d57453c5daef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-10-06T15:58:52.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7943206429481506
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p>Yeah, it\u2019s not supported with autogptq. You gotta do it with\
          \ exllama or transformers itself.</p>\n"
        raw: "Yeah, it\u2019s not supported with autogptq. You gotta do it with exllama\
          \ or transformers itself."
        updatedAt: '2023-10-06T15:58:52.160Z'
      numEdits: 0
      reactions: []
    id: 65202ebc971e5c1370d2f511
    type: comment
  author: YaTharThShaRma999
  content: "Yeah, it\u2019s not supported with autogptq. You gotta do it with exllama\
    \ or transformers itself."
  created_at: 2023-10-06 14:58:52+00:00
  edited: false
  hidden: false
  id: 65202ebc971e5c1370d2f511
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f17bee7bfc587b5899586c4193f65986.svg
      fullname: Ahmed Moubtahij
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ayenem
      type: user
    createdAt: '2023-10-06T16:01:46.000Z'
    data:
      edited: true
      editors:
      - Ayenem
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9788874387741089
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f17bee7bfc587b5899586c4193f65986.svg
          fullname: Ahmed Moubtahij
          isHf: false
          isPro: false
          name: Ayenem
          type: user
        html: '<p>Thank you for the reply.<br>Would you happen to know where I can
          find instructions for that?</p>

          '
        raw: 'Thank you for the reply.

          Would you happen to know where I can find instructions for that?'
        updatedAt: '2023-10-06T16:10:55.635Z'
      numEdits: 1
      reactions: []
    id: 65202f6a2470ec6d1a4d2b30
    type: comment
  author: Ayenem
  content: 'Thank you for the reply.

    Would you happen to know where I can find instructions for that?'
  created_at: 2023-10-06 15:01:46+00:00
  edited: true
  hidden: false
  id: 65202f6a2470ec6d1a4d2b30
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-06T16:03:22.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9082807302474976
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Instructions are in the README - there''s a Transformers Python
          example, and instructions for using text-generation-webui (which supports
          ExLlama), and Text Generation Inference</p>

          '
        raw: Instructions are in the README - there's a Transformers Python example,
          and instructions for using text-generation-webui (which supports ExLlama),
          and Text Generation Inference
        updatedAt: '2023-10-06T16:04:37.632Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Ayenem
        - jlzhou
    id: 65202fcaa7716548c58aad22
    type: comment
  author: TheBloke
  content: Instructions are in the README - there's a Transformers Python example,
    and instructions for using text-generation-webui (which supports ExLlama), and
    Text Generation Inference
  created_at: 2023-10-06 15:03:22+00:00
  edited: true
  hidden: false
  id: 65202fcaa7716548c58aad22
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Mistral-7B-OpenOrca-GPTQ
repo_type: model
status: open
target_branch: null
title: 'TypeError: mistral isn''t supported yet.'
