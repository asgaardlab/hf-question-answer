!!python/object:huggingface_hub.community.DiscussionWithDetails
author: latent-variable
conflicting_files: null
created_at: 2023-10-04 03:19:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6aaebf08394185fdb5963873e569bc8.svg
      fullname: Lino Valdovinos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: latent-variable
      type: user
    createdAt: '2023-10-04T04:19:05.000Z'
    data:
      edited: false
      editors:
      - latent-variable
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3927200734615326
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6aaebf08394185fdb5963873e569bc8.svg
          fullname: Lino Valdovinos
          isHf: false
          isPro: false
          name: latent-variable
          type: user
        html: '<p>Using Exllama &amp; Exllamav2 I get the following error with this
          model in the Text-generation-webui</p>

          <p>Traceback (most recent call last):<br>  File "C:\AI\text-generation-webui\modules\text_generation.py",
          line 384, in generate_reply_custom<br>    for reply in shared.model.generate_with_streaming(question,
          state):<br>  File "C:\AI\text-generation-webui\modules\exllamav2.py", line
          119, in generate_with_streaming<br>    decoded_text = self.tokenizer.decode(ids[:,
          initial_len:])[0]<br>  File "C:\AI\text-generation-webui\installer_files\env\lib\site-packages\exllamav2\tokenizer.py",
          line 92, in decode<br>    texts.append(self.tokenizer.Decode(seq))<br>  File
          "C:\AI\text-generation-webui\installer_files\env\lib\site-packages\sentencepiece_<em>init</em>_.py",
          line 780, in Decode<br>    return self.<em>DecodeIds(input)<br>  File "C:\AI\text-generation-webui\installer_files\env\lib\site-packages\sentencepiece_<em>init</em></em>.py",
          line 337, in _DecodeIds<br>    return _sentencepiece.SentencePieceProcessor__DecodeIds(self,
          ids)<br>IndexError: Out of range: piece id is out of range.<br>Output generated
          in 4.94 seconds (53.20 tokens/s, 263 tokens, context 1071, seed 587620518)</p>

          '
        raw: "\r\nUsing Exllama & Exllamav2 I get the following error with this model\
          \ in the Text-generation-webui\r\n\r\nTraceback (most recent call last):\r\
          \n  File \"C:\\AI\\text-generation-webui\\modules\\text_generation.py\"\
          , line 384, in generate_reply_custom\r\n    for reply in shared.model.generate_with_streaming(question,\
          \ state):\r\n  File \"C:\\AI\\text-generation-webui\\modules\\exllamav2.py\"\
          , line 119, in generate_with_streaming\r\n    decoded_text = self.tokenizer.decode(ids[:,\
          \ initial_len:])[0]\r\n  File \"C:\\AI\\text-generation-webui\\installer_files\\\
          env\\lib\\site-packages\\exllamav2\\tokenizer.py\", line 92, in decode\r\
          \n    texts.append(self.tokenizer.Decode(seq))\r\n  File \"C:\\AI\\text-generation-webui\\\
          installer_files\\env\\lib\\site-packages\\sentencepiece\\__init__.py\",\
          \ line 780, in Decode\r\n    return self._DecodeIds(input)\r\n  File \"\
          C:\\AI\\text-generation-webui\\installer_files\\env\\lib\\site-packages\\\
          sentencepiece\\__init__.py\", line 337, in _DecodeIds\r\n    return _sentencepiece.SentencePieceProcessor__DecodeIds(self,\
          \ ids)\r\nIndexError: Out of range: piece id is out of range.\r\nOutput\
          \ generated in 4.94 seconds (53.20 tokens/s, 263 tokens, context 1071, seed\
          \ 587620518)"
        updatedAt: '2023-10-04T04:19:05.499Z'
      numEdits: 0
      reactions: []
    id: 651ce7b90aab43acf989af20
    type: comment
  author: latent-variable
  content: "\r\nUsing Exllama & Exllamav2 I get the following error with this model\
    \ in the Text-generation-webui\r\n\r\nTraceback (most recent call last):\r\n \
    \ File \"C:\\AI\\text-generation-webui\\modules\\text_generation.py\", line 384,\
    \ in generate_reply_custom\r\n    for reply in shared.model.generate_with_streaming(question,\
    \ state):\r\n  File \"C:\\AI\\text-generation-webui\\modules\\exllamav2.py\",\
    \ line 119, in generate_with_streaming\r\n    decoded_text = self.tokenizer.decode(ids[:,\
    \ initial_len:])[0]\r\n  File \"C:\\AI\\text-generation-webui\\installer_files\\\
    env\\lib\\site-packages\\exllamav2\\tokenizer.py\", line 92, in decode\r\n   \
    \ texts.append(self.tokenizer.Decode(seq))\r\n  File \"C:\\AI\\text-generation-webui\\\
    installer_files\\env\\lib\\site-packages\\sentencepiece\\__init__.py\", line 780,\
    \ in Decode\r\n    return self._DecodeIds(input)\r\n  File \"C:\\AI\\text-generation-webui\\\
    installer_files\\env\\lib\\site-packages\\sentencepiece\\__init__.py\", line 337,\
    \ in _DecodeIds\r\n    return _sentencepiece.SentencePieceProcessor__DecodeIds(self,\
    \ ids)\r\nIndexError: Out of range: piece id is out of range.\r\nOutput generated\
    \ in 4.94 seconds (53.20 tokens/s, 263 tokens, context 1071, seed 587620518)"
  created_at: 2023-10-04 03:19:05+00:00
  edited: false
  hidden: false
  id: 651ce7b90aab43acf989af20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f6aaebf08394185fdb5963873e569bc8.svg
      fullname: Lino Valdovinos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: latent-variable
      type: user
    createdAt: '2023-10-04T04:27:59.000Z'
    data:
      status: closed
    id: 651ce9cfc8f29ac916e72dc3
    type: status-change
  author: latent-variable
  created_at: 2023-10-04 03:27:59+00:00
  id: 651ce9cfc8f29ac916e72dc3
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f6aaebf08394185fdb5963873e569bc8.svg
      fullname: Lino Valdovinos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: latent-variable
      type: user
    createdAt: '2023-10-04T05:42:08.000Z'
    data:
      status: open
    id: 651cfb309a47f703e6b66870
    type: status-change
  author: latent-variable
  created_at: 2023-10-04 04:42:08+00:00
  id: 651cfb309a47f703e6b66870
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663165866347-noauth.jpeg?w=200&h=200&f=face
      fullname: Ryan Stout
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ryanstout
      type: user
    createdAt: '2023-10-04T14:39:38.000Z'
    data:
      edited: false
      editors:
      - ryanstout
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9725375771522522
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663165866347-noauth.jpeg?w=200&h=200&f=face
          fullname: Ryan Stout
          isHf: false
          isPro: false
          name: ryanstout
          type: user
        html: '<p>Same issue here, the error is coming from the sentencepiece library,
          perhaps something with the vocabulary size isn''t set correctly.</p>

          '
        raw: Same issue here, the error is coming from the sentencepiece library,
          perhaps something with the vocabulary size isn't set correctly.
        updatedAt: '2023-10-04T14:39:38.622Z'
      numEdits: 0
      reactions: []
    id: 651d792a99b198681d48508b
    type: comment
  author: ryanstout
  content: Same issue here, the error is coming from the sentencepiece library, perhaps
    something with the vocabulary size isn't set correctly.
  created_at: 2023-10-04 13:39:38+00:00
  edited: false
  hidden: false
  id: 651d792a99b198681d48508b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a18a93471c139f4446c4dae7b33a2c39.svg
      fullname: Scott Sumpter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Imathenerd
      type: user
    createdAt: '2023-10-05T18:04:31.000Z'
    data:
      edited: false
      editors:
      - Imathenerd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.22800616919994354
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a18a93471c139f4446c4dae7b33a2c39.svg
          fullname: Scott Sumpter
          isHf: false
          isPro: false
          name: Imathenerd
          type: user
        html: '<p>Is that related to this at all? Traceback (most recent call last):<br>  File
          "c:\Users\Scott\VSC_Source\LLM_DB\EXLLAMAv2y production.py", line 181, in
          <br>    output = generator.generate_simple(prompt,settings,max_new_tokens)<br>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\Scott\anaconda3\envs\ELLAMA\Lib\site-packages\exllamav2\generator\base.py",
          line 60, in generate_simple<br>    token, _ = ExLlamaV2Sampler.sample(logits,
          gen_settings, self.sequence_ids, random.random())<br>               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\Scott\anaconda3\envs\ELLAMA\Lib\site-packages\exllamav2\generator\sampler.py",
          line 63, in sample<br>    if settings.token_bias is not None: logits +=
          settings.token_bias<br>                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>RuntimeError:
          The size of tensor a (32032) must match the size of tensor b (32002) at
          non-singleton dimension 1</p>

          '
        raw: "Is that related to this at all? Traceback (most recent call last):\n\
          \  File \"c:\\Users\\Scott\\VSC_Source\\LLM_DB\\EXLLAMAv2y production.py\"\
          , line 181, in <module>\n    output = generator.generate_simple(prompt,settings,max_new_tokens)\n\
          \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\Scott\\anaconda3\\envs\\ELLAMA\\Lib\\site-packages\\\
          exllamav2\\generator\\base.py\", line 60, in generate_simple\n    token,\
          \ _ = ExLlamaV2Sampler.sample(logits, gen_settings, self.sequence_ids, random.random())\n\
          \               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\Scott\\anaconda3\\envs\\ELLAMA\\Lib\\site-packages\\\
          exllamav2\\generator\\sampler.py\", line 63, in sample\n    if settings.token_bias\
          \ is not None: logits += settings.token_bias\n                         \
          \               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of\
          \ tensor a (32032) must match the size of tensor b (32002) at non-singleton\
          \ dimension 1"
        updatedAt: '2023-10-05T18:04:31.087Z'
      numEdits: 0
      reactions: []
    id: 651efaaf320661974b178c1d
    type: comment
  author: Imathenerd
  content: "Is that related to this at all? Traceback (most recent call last):\n \
    \ File \"c:\\Users\\Scott\\VSC_Source\\LLM_DB\\EXLLAMAv2y production.py\", line\
    \ 181, in <module>\n    output = generator.generate_simple(prompt,settings,max_new_tokens)\n\
    \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
    \ \"C:\\Users\\Scott\\anaconda3\\envs\\ELLAMA\\Lib\\site-packages\\exllamav2\\\
    generator\\base.py\", line 60, in generate_simple\n    token, _ = ExLlamaV2Sampler.sample(logits,\
    \ gen_settings, self.sequence_ids, random.random())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\Scott\\anaconda3\\envs\\ELLAMA\\Lib\\site-packages\\exllamav2\\\
    generator\\sampler.py\", line 63, in sample\n    if settings.token_bias is not\
    \ None: logits += settings.token_bias\n                                      \
    \  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (32032) must\
    \ match the size of tensor b (32002) at non-singleton dimension 1"
  created_at: 2023-10-05 17:04:31+00:00
  edited: false
  hidden: false
  id: 651efaaf320661974b178c1d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Mistral-7B-OpenOrca-GPTQ
repo_type: model
status: open
target_branch: null
title: 'IndexError: Out of range: piece id is out of range'
