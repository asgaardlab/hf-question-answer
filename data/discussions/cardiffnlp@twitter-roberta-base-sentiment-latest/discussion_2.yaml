!!python/object:huggingface_hub.community.DiscussionWithDetails
author: victor-roris
conflicting_files: null
created_at: 2022-08-02 17:53:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/89bf19eee0b05f62832446fabff94cde.svg
      fullname: Victor Manuel Alonso Roris
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: victor-roris
      type: user
    createdAt: '2022-08-02T18:53:37.000Z'
    data:
      edited: false
      editors:
      - victor-roris
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/89bf19eee0b05f62832446fabff94cde.svg
          fullname: Victor Manuel Alonso Roris
          isHf: false
          isPro: false
          name: victor-roris
          type: user
        html: "<p>I tried your code with long sentences and the automatic truncation\
          \ to the max length of the model fails:</p>\n<pre><code>encoded_input =\
          \ tokenizer(really_long_sentence, truncation=True, max_length=None, return_tensors='pt')\
          \ \n</code></pre>\n<p>It raises an error about the dimension:</p>\n<pre><code>RuntimeError:\
          \ The expanded size of the tensor (5227) must match the existing size (514)\
          \ at non-singleton dimension 1.  Target sizes: [1, 5227].  Tensor sizes:\
          \ [1, 514]\n</code></pre>\n<p>So, I tried to fix the max length to 514:</p>\n\
          <pre><code>encoded_input = tokenizer(really_long_sentence, truncation=True,\
          \ max_length=514, return_tensors='pt') \n</code></pre>\n<p>But it continues\
          \ failing:</p>\n<pre><code>   2041         # remove once script supports\
          \ set_grad_enabled\n   2042         _no_grad_embedding_renorm_(weight, input,\
          \ max_norm, norm_type)\n-&gt; 2043     return torch.embedding(weight, input,\
          \ padding_idx, scale_grad_by_freq, sparse)\n   2044 \n   2045 \n\nIndexError:\
          \ index out of range in self\n</code></pre>\n<p>Can you tell me if there\
          \ is some way to obtain the appropriate model max length from the model/tokenizer\
          \ configuration?</p>\n"
        raw: "I tried your code with long sentences and the automatic truncation to\
          \ the max length of the model fails:\r\n\r\n```\r\nencoded_input = tokenizer(really_long_sentence,\
          \ truncation=True, max_length=None, return_tensors='pt') \r\n```\r\n\r\n\
          It raises an error about the dimension:\r\n```\r\nRuntimeError: The expanded\
          \ size of the tensor (5227) must match the existing size (514) at non-singleton\
          \ dimension 1.  Target sizes: [1, 5227].  Tensor sizes: [1, 514]\r\n```\r\
          \n\r\nSo, I tried to fix the max length to 514:\r\n```\r\nencoded_input\
          \ = tokenizer(really_long_sentence, truncation=True, max_length=514, return_tensors='pt')\
          \ \r\n```\r\n\r\nBut it continues failing:\r\n\r\n```\r\n   2041       \
          \  # remove once script supports set_grad_enabled\r\n   2042         _no_grad_embedding_renorm_(weight,\
          \ input, max_norm, norm_type)\r\n-> 2043     return torch.embedding(weight,\
          \ input, padding_idx, scale_grad_by_freq, sparse)\r\n   2044 \r\n   2045\
          \ \r\n\r\nIndexError: index out of range in self\r\n```\r\n\r\nCan you tell\
          \ me if there is some way to obtain the appropriate model max length from\
          \ the model/tokenizer configuration?"
        updatedAt: '2022-08-02T18:53:37.531Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - alaksana96
        - iamgroot42
    id: 62e972b161ea75825d436a15
    type: comment
  author: victor-roris
  content: "I tried your code with long sentences and the automatic truncation to\
    \ the max length of the model fails:\r\n\r\n```\r\nencoded_input = tokenizer(really_long_sentence,\
    \ truncation=True, max_length=None, return_tensors='pt') \r\n```\r\n\r\nIt raises\
    \ an error about the dimension:\r\n```\r\nRuntimeError: The expanded size of the\
    \ tensor (5227) must match the existing size (514) at non-singleton dimension\
    \ 1.  Target sizes: [1, 5227].  Tensor sizes: [1, 514]\r\n```\r\n\r\nSo, I tried\
    \ to fix the max length to 514:\r\n```\r\nencoded_input = tokenizer(really_long_sentence,\
    \ truncation=True, max_length=514, return_tensors='pt') \r\n```\r\n\r\nBut it\
    \ continues failing:\r\n\r\n```\r\n   2041         # remove once script supports\
    \ set_grad_enabled\r\n   2042         _no_grad_embedding_renorm_(weight, input,\
    \ max_norm, norm_type)\r\n-> 2043     return torch.embedding(weight, input, padding_idx,\
    \ scale_grad_by_freq, sparse)\r\n   2044 \r\n   2045 \r\n\r\nIndexError: index\
    \ out of range in self\r\n```\r\n\r\nCan you tell me if there is some way to obtain\
    \ the appropriate model max length from the model/tokenizer configuration?"
  created_at: 2022-08-02 17:53:37+00:00
  edited: false
  hidden: false
  id: 62e972b161ea75825d436a15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/28f8b373d85d120db20e501bbaa2a5e3.svg
      fullname: Aufar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alaksana96
      type: user
    createdAt: '2022-08-08T14:15:24.000Z'
    data:
      edited: false
      editors:
      - alaksana96
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/28f8b373d85d120db20e501bbaa2a5e3.svg
          fullname: Aufar
          isHf: false
          isPro: false
          name: alaksana96
          type: user
        html: '<p>I had a similar issue. I found (through trial and error) that if
          you set the max_length to <code>511</code>, it seems to work.</p>

          <p>I''d like to understand why thats the case though</p>

          '
        raw: 'I had a similar issue. I found (through trial and error) that if you
          set the max_length to `511`, it seems to work.


          I''d like to understand why thats the case though'
        updatedAt: '2022-08-08T14:15:24.975Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - lokesh005
    id: 62f11a7c27cc18b5fc355572
    type: comment
  author: alaksana96
  content: 'I had a similar issue. I found (through trial and error) that if you set
    the max_length to `511`, it seems to work.


    I''d like to understand why thats the case though'
  created_at: 2022-08-08 13:15:24+00:00
  edited: false
  hidden: false
  id: 62f11a7c27cc18b5fc355572
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661885600300-noauth.jpeg?w=200&h=200&f=face
      fullname: Mehmet Mustafa Icer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mmustafaicer
      type: user
    createdAt: '2022-09-08T15:55:00.000Z'
    data:
      edited: false
      editors:
      - mmustafaicer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661885600300-noauth.jpeg?w=200&h=200&f=face
          fullname: Mehmet Mustafa Icer
          isHf: false
          isPro: false
          name: mmustafaicer
          type: user
        html: '<p>I had the same issue, but if you choose <code>max_length</code>
          option way less than existing size <strong>514</strong>, it would solve
          your problem. Try <strong><code>max_length=500</code></strong> or <strong><code>max_length=400</code></strong>
          and see if it works.</p>

          '
        raw: I had the same issue, but if you choose `max_length` option way less
          than existing size **514**, it would solve your problem. Try **`max_length=500`**
          or **`max_length=400`** and see if it works.
        updatedAt: '2022-09-08T15:55:00.067Z'
      numEdits: 0
      reactions: []
    id: 631a1054223d36c5706ce762
    type: comment
  author: mmustafaicer
  content: I had the same issue, but if you choose `max_length` option way less than
    existing size **514**, it would solve your problem. Try **`max_length=500`** or
    **`max_length=400`** and see if it works.
  created_at: 2022-09-08 14:55:00+00:00
  edited: false
  hidden: false
  id: 631a1054223d36c5706ce762
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666954820942-noauth.jpeg?w=200&h=200&f=face
      fullname: yassir acharki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yassiracharki
      type: user
    createdAt: '2022-11-14T08:58:16.000Z'
    data:
      edited: false
      editors:
      - yassiracharki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666954820942-noauth.jpeg?w=200&h=200&f=face
          fullname: yassir acharki
          isHf: false
          isPro: false
          name: yassiracharki
          type: user
        html: '<p>max_length = 511 woks fine for me.</p>

          '
        raw: max_length = 511 woks fine for me.
        updatedAt: '2022-11-14T08:58:16.722Z'
      numEdits: 0
      reactions: []
    id: 6372032870c2b860a0775b55
    type: comment
  author: yassiracharki
  content: max_length = 511 woks fine for me.
  created_at: 2022-11-14 08:58:16+00:00
  edited: false
  hidden: false
  id: 6372032870c2b860a0775b55
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da4a731836b3d4a665ca1f0e262bd999.svg
      fullname: Stewart Dille
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NosLeeP
      type: user
    createdAt: '2023-01-31T02:30:46.000Z'
    data:
      edited: true
      editors:
      - NosLeeP
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da4a731836b3d4a665ca1f0e262bd999.svg
          fullname: Stewart Dille
          isHf: false
          isPro: false
          name: NosLeeP
          type: user
        html: '<p>Remember that with RoBERTA the encoding process will add a token
          to the signify beg and end of a document, this is why when you truncate
          to 514 it adds 2 in to make tensor make 516.  I would say that is this is
          the case it SHOULD be fixed in the config.json file.  If you look at other
          BERT models you will see that the config.json file will have <code>max_position_embeddings</code>
          be set to 2 less than the tensor is long to account for beg and end (in
          this model they are 0 and 2 respective when you look at tokenized document
          you will see that).  So to those who used <code>max_length &lt;= 512 </code>that
          is why is worked.</p>

          '
        raw: Remember that with RoBERTA the encoding process will add a token to the
          signify beg and end of a document, this is why when you truncate to 514
          it adds 2 in to make tensor make 516.  I would say that is this is the case
          it SHOULD be fixed in the config.json file.  If you look at other BERT models
          you will see that the config.json file will have ```max_position_embeddings```
          be set to 2 less than the tensor is long to account for beg and end (in
          this model they are 0 and 2 respective when you look at tokenized document
          you will see that).  So to those who used ```max_length <= 512 ```that is
          why is worked.
        updatedAt: '2023-02-01T14:44:03.321Z'
      numEdits: 1
      reactions: []
    id: 63d87d56f8fb9f0e9e1aa39a
    type: comment
  author: NosLeeP
  content: Remember that with RoBERTA the encoding process will add a token to the
    signify beg and end of a document, this is why when you truncate to 514 it adds
    2 in to make tensor make 516.  I would say that is this is the case it SHOULD
    be fixed in the config.json file.  If you look at other BERT models you will see
    that the config.json file will have ```max_position_embeddings``` be set to 2
    less than the tensor is long to account for beg and end (in this model they are
    0 and 2 respective when you look at tokenized document you will see that).  So
    to those who used ```max_length <= 512 ```that is why is worked.
  created_at: 2023-01-31 02:30:46+00:00
  edited: true
  hidden: false
  id: 63d87d56f8fb9f0e9e1aa39a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: cardiffnlp/twitter-roberta-base-sentiment-latest
repo_type: model
status: open
target_branch: null
title: 'Model max length '
