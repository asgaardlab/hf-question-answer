!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ibrim
conflicting_files: null
created_at: 2023-08-02 07:34:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ce3bd48dea8f39daa70ad116d092b04.svg
      fullname: ibru
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ibrim
      type: user
    createdAt: '2023-08-02T08:34:48.000Z'
    data:
      edited: false
      editors:
      - ibrim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5917383432388306
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ce3bd48dea8f39daa70ad116d092b04.svg
          fullname: ibru
          isHf: false
          isPro: false
          name: ibrim
          type: user
        html: '<p>import torch<br>from transformers import AutoModelForCausalLM, AutoTokenizer,
          pipeline</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("/raid/users/Beluga", use_fast=False)<br>model
          = AutoModelForCausalLM.from_pretrained("/raid/users/Beluga", torch_dtype=torch.float16,
          low_cpu_mem_usage=True, trust_remote_code = True,device_map="auto")<br>system_prompt
          = "### System:\nYou are Stable Beluga 13B, an AI that follows instructions
          extremely well. Help as much as you can. Remember, be safe, and don''t do
          anything illegal.\n\n"</p>

          <p>message = "Write me a poem please"<br>prompt = f"{system_prompt}### User:
          {message}\n\n### Assistant:\n"<br>inputs = tokenizer(prompt, return_tensors="pt").to("cuda")<br>output
          = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)</p>

          <p>print(tokenizer.decode(output[0], skip_special_tokens=True))</p>

          '
        raw: "import torch\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,\
          \ pipeline\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"/raid/users/Beluga\"\
          , use_fast=False)\r\nmodel = AutoModelForCausalLM.from_pretrained(\"/raid/users/Beluga\"\
          , torch_dtype=torch.float16, low_cpu_mem_usage=True, trust_remote_code =\
          \ True,device_map=\"auto\")\r\nsystem_prompt = \"### System:\\nYou are Stable\
          \ Beluga 13B, an AI that follows instructions extremely well. Help as much\
          \ as you can. Remember, be safe, and don't do anything illegal.\\n\\n\"\r\
          \n\r\nmessage = \"Write me a poem please\"\r\nprompt = f\"{system_prompt}###\
          \ User: {message}\\n\\n### Assistant:\\n\"\r\ninputs = tokenizer(prompt,\
          \ return_tensors=\"pt\").to(\"cuda\")\r\noutput = model.generate(**inputs,\
          \ do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\r\n\r\nprint(tokenizer.decode(output[0],\
          \ skip_special_tokens=True))"
        updatedAt: '2023-08-02T08:34:48.135Z'
      numEdits: 0
      reactions: []
    id: 64ca15280862ee243d55a126
    type: comment
  author: ibrim
  content: "import torch\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,\
    \ pipeline\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"/raid/users/Beluga\"\
    , use_fast=False)\r\nmodel = AutoModelForCausalLM.from_pretrained(\"/raid/users/Beluga\"\
    , torch_dtype=torch.float16, low_cpu_mem_usage=True, trust_remote_code = True,device_map=\"\
    auto\")\r\nsystem_prompt = \"### System:\\nYou are Stable Beluga 13B, an AI that\
    \ follows instructions extremely well. Help as much as you can. Remember, be safe,\
    \ and don't do anything illegal.\\n\\n\"\r\n\r\nmessage = \"Write me a poem please\"\
    \r\nprompt = f\"{system_prompt}### User: {message}\\n\\n### Assistant:\\n\"\r\n\
    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\r\noutput = model.generate(**inputs,\
    \ do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\r\n\r\nprint(tokenizer.decode(output[0],\
    \ skip_special_tokens=True))"
  created_at: 2023-08-02 07:34:48+00:00
  edited: false
  hidden: false
  id: 64ca15280862ee243d55a126
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: stabilityai/StableBeluga-13B
repo_type: model
status: open
target_branch: null
title: 'OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index
  or flax_model.msgpack found in directory /path/to/local/model.'
