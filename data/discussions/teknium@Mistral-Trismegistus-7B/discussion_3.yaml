!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nps798
conflicting_files: null
created_at: 2023-10-09 10:56:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6550b60a4863272fef6c45c3602d9269.svg
      fullname: YuJuLin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nps798
      type: user
    createdAt: '2023-10-09T11:56:04.000Z'
    data:
      edited: false
      editors:
      - nps798
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5872299671173096
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6550b60a4863272fef6c45c3602d9269.svg
          fullname: YuJuLin
          isHf: false
          isPro: false
          name: nps798
          type: user
        html: "<p>As title.<br>What are the code you use to train this model ?<br>I\
          \ noticed it's qlora. and have viewed the wandb records.<br>I am doing myself\
          \ qlora fine tuning but facing train loss unstable issue</p>\n<p>What are\
          \ the target_modules you use ?<br>What are the tokenizer parameter you use\
          \ ?</p>\n<p>My setting. which fail<br>Tokenizer initialization</p>\n<pre><code>tokenizer\
          \ = AutoTokenizer.from_pretrained(\n    f\"{path_to_save}/tokenizer\",\n\
          \    model_max_length=512,\n    padding_side=\"left\",\n    trust_remote_code=True,\n\
          \    add_eos_token=True, \n    )\n</code></pre>\n<p>TOKENIZATION</p>\n<pre><code>\
          \    tokenized_full_prompt = tokenizer(full_prompt,\n        truncation=True,\n\
          \        max_length=512 , \n        padding=True, \n        return_tensors=\"\
          pt\")\n</code></pre>\n<p>---&gt; in the trainer i use<br>data_collator=DataCollatorForLanguageModeling(tokenizer,\
          \ mlm=False)<br>which will dynamically pad for my sequence</p>\n<p>LORA</p>\n\
          <pre><code>config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\n\
          \        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n       \
          \ \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"\
          down_proj\",\n        \"lm_head\",\n    ],\n...\n</code></pre>\n<p>Thannnnks\
          \ a lot !</p>\n"
        raw: "As title. \r\nWhat are the code you use to train this model ?\r\nI noticed\
          \ it's qlora. and have viewed the wandb records. \r\nI am doing myself qlora\
          \ fine tuning but facing train loss unstable issue\r\n\r\nWhat are the target_modules\
          \ you use ?\r\nWhat are the tokenizer parameter you use ?\r\n\r\nMy setting.\
          \ which fail\r\nTokenizer initialization\r\n```\r\ntokenizer = AutoTokenizer.from_pretrained(\r\
          \n    f\"{path_to_save}/tokenizer\",\r\n    model_max_length=512,\r\n  \
          \  padding_side=\"left\",\r\n    trust_remote_code=True,\r\n    add_eos_token=True,\
          \ \r\n    )\r\n```\r\n\r\nTOKENIZATION\r\n```\r\n    tokenized_full_prompt\
          \ = tokenizer(full_prompt,\r\n        truncation=True,\r\n        max_length=512\
          \ , \r\n        padding=True, \r\n        return_tensors=\"pt\")\r\n\r\n\
          ```\r\n\r\n---> in the trainer i use \r\ndata_collator=DataCollatorForLanguageModeling(tokenizer,\
          \ mlm=False)\r\nwhich will dynamically pad for my sequence\r\n\r\n\r\nLORA\r\
          \n```\r\nconfig = LoraConfig(\r\n    r=16,\r\n    lora_alpha=16,\r\n   \
          \ target_modules=[\r\n        \"q_proj\",\r\n        \"k_proj\",\r\n   \
          \     \"v_proj\",\r\n        \"o_proj\",\r\n        \"gate_proj\",\r\n \
          \       \"up_proj\",\r\n        \"down_proj\",\r\n        \"lm_head\",\r\
          \n    ],\r\n...\r\n```\r\n\r\n\r\nThannnnks a lot !"
        updatedAt: '2023-10-09T11:56:04.751Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - abdurnawaz
    id: 6523ea544f3337c6b10d5497
    type: comment
  author: nps798
  content: "As title. \r\nWhat are the code you use to train this model ?\r\nI noticed\
    \ it's qlora. and have viewed the wandb records. \r\nI am doing myself qlora fine\
    \ tuning but facing train loss unstable issue\r\n\r\nWhat are the target_modules\
    \ you use ?\r\nWhat are the tokenizer parameter you use ?\r\n\r\nMy setting. which\
    \ fail\r\nTokenizer initialization\r\n```\r\ntokenizer = AutoTokenizer.from_pretrained(\r\
    \n    f\"{path_to_save}/tokenizer\",\r\n    model_max_length=512,\r\n    padding_side=\"\
    left\",\r\n    trust_remote_code=True,\r\n    add_eos_token=True, \r\n    )\r\n\
    ```\r\n\r\nTOKENIZATION\r\n```\r\n    tokenized_full_prompt = tokenizer(full_prompt,\r\
    \n        truncation=True,\r\n        max_length=512 , \r\n        padding=True,\
    \ \r\n        return_tensors=\"pt\")\r\n\r\n```\r\n\r\n---> in the trainer i use\
    \ \r\ndata_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\r\n\
    which will dynamically pad for my sequence\r\n\r\n\r\nLORA\r\n```\r\nconfig =\
    \ LoraConfig(\r\n    r=16,\r\n    lora_alpha=16,\r\n    target_modules=[\r\n \
    \       \"q_proj\",\r\n        \"k_proj\",\r\n        \"v_proj\",\r\n        \"\
    o_proj\",\r\n        \"gate_proj\",\r\n        \"up_proj\",\r\n        \"down_proj\"\
    ,\r\n        \"lm_head\",\r\n    ],\r\n...\r\n```\r\n\r\n\r\nThannnnks a lot !"
  created_at: 2023-10-09 10:56:04+00:00
  edited: false
  hidden: false
  id: 6523ea544f3337c6b10d5497
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: teknium/Mistral-Trismegistus-7B
repo_type: model
status: open
target_branch: null
title: Excellent model ! Asking about training details
