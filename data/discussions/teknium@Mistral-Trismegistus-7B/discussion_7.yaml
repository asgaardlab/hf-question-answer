!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Krisolada
conflicting_files: null
created_at: 2023-11-17 06:20:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/629a7f5a17b178fedca0c52e79f27eca.svg
      fullname: Liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Krisolada
      type: user
    createdAt: '2023-11-17T06:20:45.000Z'
    data:
      edited: true
      editors:
      - Krisolada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8253740072250366
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/629a7f5a17b178fedca0c52e79f27eca.svg
          fullname: Liu
          isHf: false
          isPro: false
          name: Krisolada
          type: user
        html: "<p>Please see the attached image:</p>\n<p>We hope that each response\
          \ can be longer and complete. We have to keep clicking on \"Compute\" button\
          \ when testing to get more generated text back.</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/65550ac438985840b34fac49/_m3CavP5upQgYLzIGEli3.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/65550ac438985840b34fac49/_m3CavP5upQgYLzIGEli3.png\"\
          ></a></p>\n<p>We wonder if this has to do with our host instance size:</p>\n\
          <p>Nvidia A10G<br>1x GPU \xB7 24 GB<br>6 vCPU \xB7 28 GB<br>1.3/hr</p>\n\
          <p>Thanks! Please help!</p>\n"
        raw: "Please see the attached image:\n\nWe hope that each response can be\
          \ longer and complete. We have to keep clicking on \"Compute\" button when\
          \ testing to get more generated text back.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/65550ac438985840b34fac49/_m3CavP5upQgYLzIGEli3.png)\n\
          \nWe wonder if this has to do with our host instance size:\n\nNvidia A10G\n\
          1x GPU \xB7 24 GB\n6 vCPU \xB7 28 GB\n1.3/hr\n\nThanks! Please help!"
        updatedAt: '2023-11-17T06:23:25.740Z'
      numEdits: 2
      reactions: []
    id: 6557063d976a1381669996ad
    type: comment
  author: Krisolada
  content: "Please see the attached image:\n\nWe hope that each response can be longer\
    \ and complete. We have to keep clicking on \"Compute\" button when testing to\
    \ get more generated text back.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/65550ac438985840b34fac49/_m3CavP5upQgYLzIGEli3.png)\n\
    \nWe wonder if this has to do with our host instance size:\n\nNvidia A10G\n1x\
    \ GPU \xB7 24 GB\n6 vCPU \xB7 28 GB\n1.3/hr\n\nThanks! Please help!"
  created_at: 2023-11-17 06:20:45+00:00
  edited: true
  hidden: false
  id: 6557063d976a1381669996ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-11-17T15:40:12.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8461762070655823
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: "<blockquote>\n<p>Please see the attached image:</p>\n<p>We hope that\
          \ each response can be longer and complete. We have to keep clicking on\
          \ \"Compute\" button when testing to get more generated text back.</p>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/65550ac438985840b34fac49/_m3CavP5upQgYLzIGEli3.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/65550ac438985840b34fac49/_m3CavP5upQgYLzIGEli3.png\"\
          ></a></p>\n<p>We wonder if this has to do with our host instance size:</p>\n\
          <p>Nvidia A10G<br>1x GPU \xB7 24 GB<br>6 vCPU \xB7 28 GB<br>1.3/hr</p>\n\
          <p>Thanks! Please help!</p>\n</blockquote>\n<p>What is the code you are\
          \ using or where are you using it? </p>\n"
        raw: "> Please see the attached image:\n> \n> We hope that each response can\
          \ be longer and complete. We have to keep clicking on \"Compute\" button\
          \ when testing to get more generated text back.\n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65550ac438985840b34fac49/_m3CavP5upQgYLzIGEli3.png)\n\
          > \n> We wonder if this has to do with our host instance size:\n> \n> Nvidia\
          \ A10G\n> 1x GPU \xB7 24 GB\n> 6 vCPU \xB7 28 GB\n> 1.3/hr\n> \n> Thanks!\
          \ Please help!\n\nWhat is the code you are using or where are you using\
          \ it? "
        updatedAt: '2023-11-17T15:40:12.962Z'
      numEdits: 0
      reactions: []
    id: 6557895c6539c9a5a5577fe1
    type: comment
  author: teknium
  content: "> Please see the attached image:\n> \n> We hope that each response can\
    \ be longer and complete. We have to keep clicking on \"Compute\" button when\
    \ testing to get more generated text back.\n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65550ac438985840b34fac49/_m3CavP5upQgYLzIGEli3.png)\n\
    > \n> We wonder if this has to do with our host instance size:\n> \n> Nvidia A10G\n\
    > 1x GPU \xB7 24 GB\n> 6 vCPU \xB7 28 GB\n> 1.3/hr\n> \n> Thanks! Please help!\n\
    \nWhat is the code you are using or where are you using it? "
  created_at: 2023-11-17 15:40:12+00:00
  edited: false
  hidden: false
  id: 6557895c6539c9a5a5577fe1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/629a7f5a17b178fedca0c52e79f27eca.svg
      fullname: Liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Krisolada
      type: user
    createdAt: '2023-11-17T18:46:33.000Z'
    data:
      edited: false
      editors:
      - Krisolada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8502828478813171
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/629a7f5a17b178fedca0c52e79f27eca.svg
          fullname: Liu
          isHf: false
          isPro: false
          name: Krisolada
          type: user
        html: "<blockquote>\n<blockquote>\n<p>Please see the attached image:</p>\n\
          <p>We hope that each response can be longer and complete. We have to keep\
          \ clicking on \"Compute\" button when testing to get more generated text\
          \ back.</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/65550ac438985840b34fac49/_m3CavP5upQgYLzIGEli3.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/65550ac438985840b34fac49/_m3CavP5upQgYLzIGEli3.png\"\
          ></a></p>\n<p>We wonder if this has to do with our host instance size:</p>\n\
          <p>Nvidia A10G<br>1x GPU \xB7 24 GB<br>6 vCPU \xB7 28 GB<br>1.3/hr</p>\n\
          <p>Thanks! Please help!</p>\n</blockquote>\n<p>What is the code you are\
          \ using or where are you using it?</p>\n</blockquote>\n<p>Hi, thanks for\
          \ getting back to us!</p>\n<p>We are simply testing at this stage using\
          \ POSTMAN:</p>\n<p>We pass in the token, set Content-Type: application/json,\
          \ and send the example question '{\"inputs\": \"What is the philosopher'''s\
          \ stone, really?\"}' via the request body.</p>\n<p>The text generated in\
          \ the screenshot above is all we get. </p>\n<p>Hope the information helps!</p>\n"
        raw: "> > Please see the attached image:\n> > \n> > We hope that each response\
          \ can be longer and complete. We have to keep clicking on \"Compute\" button\
          \ when testing to get more generated text back.\n> > \n> > ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65550ac438985840b34fac49/_m3CavP5upQgYLzIGEli3.png)\n\
          > > \n> > We wonder if this has to do with our host instance size:\n> >\
          \ \n> > Nvidia A10G\n> > 1x GPU \xB7 24 GB\n> > 6 vCPU \xB7 28 GB\n> > 1.3/hr\n\
          > > \n> > Thanks! Please help!\n> \n> What is the code you are using or\
          \ where are you using it?\n\nHi, thanks for getting back to us!\n\nWe are\
          \ simply testing at this stage using POSTMAN:\n\nWe pass in the token, set\
          \ Content-Type: application/json, and send the example question '{\"inputs\"\
          : \"What is the philosopher'\\''s stone, really?\"}' via the request body.\n\
          \nThe text generated in the screenshot above is all we get. \n\nHope the\
          \ information helps!"
        updatedAt: '2023-11-17T18:46:33.884Z'
      numEdits: 0
      reactions: []
    id: 6557b50911d3fc50277a0d12
    type: comment
  author: Krisolada
  content: "> > Please see the attached image:\n> > \n> > We hope that each response\
    \ can be longer and complete. We have to keep clicking on \"Compute\" button when\
    \ testing to get more generated text back.\n> > \n> > ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65550ac438985840b34fac49/_m3CavP5upQgYLzIGEli3.png)\n\
    > > \n> > We wonder if this has to do with our host instance size:\n> > \n> >\
    \ Nvidia A10G\n> > 1x GPU \xB7 24 GB\n> > 6 vCPU \xB7 28 GB\n> > 1.3/hr\n> > \n\
    > > Thanks! Please help!\n> \n> What is the code you are using or where are you\
    \ using it?\n\nHi, thanks for getting back to us!\n\nWe are simply testing at\
    \ this stage using POSTMAN:\n\nWe pass in the token, set Content-Type: application/json,\
    \ and send the example question '{\"inputs\": \"What is the philosopher'\\''s\
    \ stone, really?\"}' via the request body.\n\nThe text generated in the screenshot\
    \ above is all we get. \n\nHope the information helps!"
  created_at: 2023-11-17 18:46:33+00:00
  edited: false
  hidden: false
  id: 6557b50911d3fc50277a0d12
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-11-18T20:53:53.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9054629802703857
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<blockquote>

          <p>Hi, thanks for getting back to us!</p>

          <p>We are simply testing at this stage using POSTMAN:</p>

          <p>We pass in the token, set Content-Type: application/json, and send the
          example question ''{"inputs": "What is the philosopher''''''s stone, really?"}''
          via the request body.</p>

          <p>The text generated in the screenshot above is all we get. </p>

          <p>Hope the information helps!</p>

          </blockquote>

          <p>No I mean, are you using HF Transformers, TGI, VLLM, Llama.cpp, or what
          inference engine. It sounds like you are using some API based serving of
          the model, but not what that backend is</p>

          '
        raw: "> Hi, thanks for getting back to us!\n> \n> We are simply testing at\
          \ this stage using POSTMAN:\n> \n> We pass in the token, set Content-Type:\
          \ application/json, and send the example question '{\"inputs\": \"What is\
          \ the philosopher'\\''s stone, really?\"}' via the request body.\n> \n>\
          \ The text generated in the screenshot above is all we get. \n> \n> Hope\
          \ the information helps!\n\nNo I mean, are you using HF Transformers, TGI,\
          \ VLLM, Llama.cpp, or what inference engine. It sounds like you are using\
          \ some API based serving of the model, but not what that backend is"
        updatedAt: '2023-11-18T20:53:53.150Z'
      numEdits: 0
      reactions: []
    id: 65592461ed8df8312814c28e
    type: comment
  author: teknium
  content: "> Hi, thanks for getting back to us!\n> \n> We are simply testing at this\
    \ stage using POSTMAN:\n> \n> We pass in the token, set Content-Type: application/json,\
    \ and send the example question '{\"inputs\": \"What is the philosopher'\\''s\
    \ stone, really?\"}' via the request body.\n> \n> The text generated in the screenshot\
    \ above is all we get. \n> \n> Hope the information helps!\n\nNo I mean, are you\
    \ using HF Transformers, TGI, VLLM, Llama.cpp, or what inference engine. It sounds\
    \ like you are using some API based serving of the model, but not what that backend\
    \ is"
  created_at: 2023-11-18 20:53:53+00:00
  edited: false
  hidden: false
  id: 65592461ed8df8312814c28e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: teknium/Mistral-Trismegistus-7B
repo_type: model
status: open
target_branch: null
title: Why is each generated response so short / cut off / not complete?
