!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fblgit
conflicting_files: null
created_at: 2024-01-10 19:24:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6401c8c9f98fbc64bcd7dca1/MOSgc_mPbfUZ-354osy1v.png?w=200&h=200&f=face
      fullname: FBL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: fblgit
      type: user
    createdAt: '2024-01-10T19:24:46.000Z'
    data:
      edited: false
      editors:
      - fblgit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9774478673934937
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6401c8c9f98fbc64bcd7dca1/MOSgc_mPbfUZ-354osy1v.png?w=200&h=200&f=face
          fullname: FBL
          isHf: false
          isPro: true
          name: fblgit
          type: user
        html: '<p>why ?<br>because it was also noticed increased TQA when TQA was
          decontaminated on the used dataset.<br>so this shows actually opposite result,
          but in other metrics. its unlikely for those samples to hold any "magic".
          how big was the portion?  the score is clearly out of mark.</p>

          <p>will be looking forward for your paper, interim I would suggest you to
          raise a discussion to get it flagged before the community outrages....</p>

          '
        raw: "why ?\r\nbecause it was also noticed increased TQA when TQA was decontaminated\
          \ on the used dataset.\r\nso this shows actually opposite result, but in\
          \ other metrics. its unlikely for those samples to hold any \"magic\". how\
          \ big was the portion?  the score is clearly out of mark.\r\n\r\nwill be\
          \ looking forward for your paper, interim I would suggest you to raise a\
          \ discussion to get it flagged before the community outrages....\r\n"
        updatedAt: '2024-01-10T19:24:46.854Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Moonphase
        - Fristender
        - vasilee
    id: 659eeefec0c53b7cb5c21e8a
    type: comment
  author: fblgit
  content: "why ?\r\nbecause it was also noticed increased TQA when TQA was decontaminated\
    \ on the used dataset.\r\nso this shows actually opposite result, but in other\
    \ metrics. its unlikely for those samples to hold any \"magic\". how big was the\
    \ portion?  the score is clearly out of mark.\r\n\r\nwill be looking forward for\
    \ your paper, interim I would suggest you to raise a discussion to get it flagged\
    \ before the community outrages....\r\n"
  created_at: 2024-01-10 19:24:46+00:00
  edited: false
  hidden: false
  id: 659eeefec0c53b7cb5c21e8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b016d70049c7aa346e796b8626645636.svg
      fullname: Daniel
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: hromi
      type: user
    createdAt: '2024-01-10T21:54:19.000Z'
    data:
      edited: false
      editors:
      - hromi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9587260484695435
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b016d70049c7aa346e796b8626645636.svg
          fullname: Daniel
          isHf: false
          isPro: false
          name: hromi
          type: user
        html: '<p>I am new here and have now clue what "flagged" means or how to raise
          a discussion about it. </p>

          <p>I simply took 1200 winograd schemata, modified them slightly  * and with
          them, DPOed the model which dominated the leaderboard. </p>

          <p>Surprisingly - or maybe not so, if one realizes what attention layers
          are about and what Winograd schemata are about - additionally to Winogrande,
          three other metrics apparently went up.</p>

          <p>Gonna dive deeper into the thing when I''ll have time, putting the small
          speculative paper on Researchgate, Researchub or even here in git repo.</p>

          <ul>

          <li>maybe the magic consists in the way how that "slight" modification happened</li>

          </ul>

          '
        raw: "I am new here and have now clue what \"flagged\" means or how to raise\
          \ a discussion about it. \n\nI simply took 1200 winograd schemata, modified\
          \ them slightly  * and with them, DPOed the model which dominated the leaderboard.\
          \ \n\nSurprisingly - or maybe not so, if one realizes what attention layers\
          \ are about and what Winograd schemata are about - additionally to Winogrande,\
          \ three other metrics apparently went up.\n\nGonna dive deeper into the\
          \ thing when I'll have time, putting the small speculative paper on Researchgate,\
          \ Researchub or even here in git repo.\n\n* maybe the magic consists in\
          \ the way how that \"slight\" modification happened"
        updatedAt: '2024-01-10T21:54:19.945Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - Moonphase
        - Fristender
        - cele-stin
        - vasilee
    id: 659f120b2bb9a1311e6227f0
    type: comment
  author: hromi
  content: "I am new here and have now clue what \"flagged\" means or how to raise\
    \ a discussion about it. \n\nI simply took 1200 winograd schemata, modified them\
    \ slightly  * and with them, DPOed the model which dominated the leaderboard.\
    \ \n\nSurprisingly - or maybe not so, if one realizes what attention layers are\
    \ about and what Winograd schemata are about - additionally to Winogrande, three\
    \ other metrics apparently went up.\n\nGonna dive deeper into the thing when I'll\
    \ have time, putting the small speculative paper on Researchgate, Researchub or\
    \ even here in git repo.\n\n* maybe the magic consists in the way how that \"\
    slight\" modification happened"
  created_at: 2024-01-10 21:54:19+00:00
  edited: false
  hidden: false
  id: 659f120b2bb9a1311e6227f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6401c8c9f98fbc64bcd7dca1/MOSgc_mPbfUZ-354osy1v.png?w=200&h=200&f=face
      fullname: FBL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: fblgit
      type: user
    createdAt: '2024-01-10T22:55:19.000Z'
    data:
      edited: false
      editors:
      - fblgit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9822797179222107
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6401c8c9f98fbc64bcd7dca1/MOSgc_mPbfUZ-354osy1v.png?w=200&h=200&f=face
          fullname: FBL
          isHf: false
          isPro: true
          name: fblgit
          type: user
        html: '<p>the contaminated models are not part of the leaderboard, as they
          are contaminated. I believe yours is not the first model that is contaminated
          for academic purposes. but i believe they are not ranked within the non
          contaminated models. At the end of the day the model you uploaded has in
          its corpus the answers that he is being asked.<br>IMO due the age of those
          datasets, it is possible that there are some overlap, or maybe just reinforcing
          the format (arc, tqa, etc) follows a very dry format. To be bonest the most
          surprising is the ARC itself, does this means that reinforcing some type
          of conversation can affect the model performace? &lt;= very probable.</p>

          '
        raw: 'the contaminated models are not part of the leaderboard, as they are
          contaminated. I believe yours is not the first model that is contaminated
          for academic purposes. but i believe they are not ranked within the non
          contaminated models. At the end of the day the model you uploaded has in
          its corpus the answers that he is being asked.

          IMO due the age of those datasets, it is possible that there are some overlap,
          or maybe just reinforcing the format (arc, tqa, etc) follows a very dry
          format. To be bonest the most surprising is the ARC itself, does this means
          that reinforcing some type of conversation can affect the model performace?
          <= very probable.'
        updatedAt: '2024-01-10T22:55:19.307Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Moonphase
        - Fristender
    id: 659f2057b7afaf005ca6aeab
    type: comment
  author: fblgit
  content: 'the contaminated models are not part of the leaderboard, as they are contaminated.
    I believe yours is not the first model that is contaminated for academic purposes.
    but i believe they are not ranked within the non contaminated models. At the end
    of the day the model you uploaded has in its corpus the answers that he is being
    asked.

    IMO due the age of those datasets, it is possible that there are some overlap,
    or maybe just reinforcing the format (arc, tqa, etc) follows a very dry format.
    To be bonest the most surprising is the ARC itself, does this means that reinforcing
    some type of conversation can affect the model performace? <= very probable.'
  created_at: 2024-01-10 22:55:19+00:00
  edited: false
  hidden: false
  id: 659f2057b7afaf005ca6aeab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b016d70049c7aa346e796b8626645636.svg
      fullname: Daniel
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: hromi
      type: user
    createdAt: '2024-01-10T23:36:37.000Z'
    data:
      edited: false
      editors:
      - hromi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9502874612808228
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b016d70049c7aa346e796b8626645636.svg
          fullname: Daniel
          isHf: false
          isPro: false
          name: hromi
          type: user
        html: '<p>Just uploaded the dataset with which I executed the DPO : <a href="https://huggingface.co/hromi/winograd_dpo">hromi/winograd_dpo</a></p>

          <p>As You may notice, the dataset has slightly special properties not to
          be found in the wild... Can one, in such case, still speak of contamination
          and if yes, to what degree ? Any tool to quantify that ?</p>

          <p>It''s a fairly short dataset and I didn''t use anything else.  I doubt
          there would be explicit overlaps with tqa, arc or so, but this is definitely
          to be explored.</p>

          <p>My intuition would be that  either the combination of winograd, DPO,
          and repetition  either</p>

          <ul>

          <li>does something interesting to attention layers, transformers etc. in
          the model </li>

          <li>exploits the way how diverse tasks are evaluated by lm-eval-harness</li>

          </ul>

          <p>The first option sounds cool, the second one less so. Still, I look forward
          into diving deeper into this riddle and would be grateful for any insights
          / feedback / mutual colaboration in writing the paper about this oddity.</p>

          '
        raw: "Just uploaded the dataset with which I executed the DPO : [hromi/winograd_dpo](https://huggingface.co/hromi/winograd_dpo)\n\
          \nAs You may notice, the dataset has slightly special properties not to\
          \ be found in the wild... Can one, in such case, still speak of contamination\
          \ and if yes, to what degree ? Any tool to quantify that ?\n\nIt's a fairly\
          \ short dataset and I didn't use anything else.  I doubt there would be\
          \ explicit overlaps with tqa, arc or so, but this is definitely to be explored.\n\
          \nMy intuition would be that  either the combination of winograd, DPO, and\
          \ repetition  either\n \n* does something interesting to attention layers,\
          \ transformers etc. in the model \n* exploits the way how diverse tasks\
          \ are evaluated by lm-eval-harness\n\nThe first option sounds cool, the\
          \ second one less so. Still, I look forward into diving deeper into this\
          \ riddle and would be grateful for any insights / feedback / mutual colaboration\
          \ in writing the paper about this oddity."
        updatedAt: '2024-01-10T23:36:37.035Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - Moonphase
        - Meowdeep
        - Fristender
        - cele-stin
        - vasilee
    id: 659f2a05f0f3ed62ab1bc5df
    type: comment
  author: hromi
  content: "Just uploaded the dataset with which I executed the DPO : [hromi/winograd_dpo](https://huggingface.co/hromi/winograd_dpo)\n\
    \nAs You may notice, the dataset has slightly special properties not to be found\
    \ in the wild... Can one, in such case, still speak of contamination and if yes,\
    \ to what degree ? Any tool to quantify that ?\n\nIt's a fairly short dataset\
    \ and I didn't use anything else.  I doubt there would be explicit overlaps with\
    \ tqa, arc or so, but this is definitely to be explored.\n\nMy intuition would\
    \ be that  either the combination of winograd, DPO, and repetition  either\n \n\
    * does something interesting to attention layers, transformers etc. in the model\
    \ \n* exploits the way how diverse tasks are evaluated by lm-eval-harness\n\n\
    The first option sounds cool, the second one less so. Still, I look forward into\
    \ diving deeper into this riddle and would be grateful for any insights / feedback\
    \ / mutual colaboration in writing the paper about this oddity."
  created_at: 2024-01-10 23:36:37+00:00
  edited: false
  hidden: false
  id: 659f2a05f0f3ed62ab1bc5df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2024-01-13T04:00:42.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9589043855667114
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: '<p>I find this very interesting. The Winogrande questions were carefully
          chosen to challenge the language skills of LLMs. So it makes perfect sense
          that they would also be the ideal questions to DPO train them with. </p>

          <p>And once said training is generalized it should help on Arc in particular
          because many LLMs get easy questions wrong only because they fail to comprehend
          what is being asked. So a boost in comprehension afforded by improved language
          skills thanks to deliberate contamination with Winogrande questions designed
          to exploit common blind spots in LLMs should help.</p>

          <p>What would be interesting is if someone trained the exact same way with
          an equal number of equivalent but different Winogrande questions (e.g. the
          same type and complexity of each question, just with different subject matter
          so it''s not contamination), and then see if other tests like Arc get the
          same boost.</p>

          '
        raw: "I find this very interesting. The Winogrande questions were carefully\
          \ chosen to challenge the language skills of LLMs. So it makes perfect sense\
          \ that they would also be the ideal questions to DPO train them with. \n\
          \nAnd once said training is generalized it should help on Arc in particular\
          \ because many LLMs get easy questions wrong only because they fail to comprehend\
          \ what is being asked. So a boost in comprehension afforded by improved\
          \ language skills thanks to deliberate contamination with Winogrande questions\
          \ designed to exploit common blind spots in LLMs should help.\n\nWhat would\
          \ be interesting is if someone trained the exact same way with an equal\
          \ number of equivalent but different Winogrande questions (e.g. the same\
          \ type and complexity of each question, just with different subject matter\
          \ so it's not contamination), and then see if other tests like Arc get the\
          \ same boost."
        updatedAt: '2024-01-13T04:00:42.830Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Elisha-Bentzi
        - cele-stin
    id: 65a20aea8ac06028205b6058
    type: comment
  author: Phil337
  content: "I find this very interesting. The Winogrande questions were carefully\
    \ chosen to challenge the language skills of LLMs. So it makes perfect sense that\
    \ they would also be the ideal questions to DPO train them with. \n\nAnd once\
    \ said training is generalized it should help on Arc in particular because many\
    \ LLMs get easy questions wrong only because they fail to comprehend what is being\
    \ asked. So a boost in comprehension afforded by improved language skills thanks\
    \ to deliberate contamination with Winogrande questions designed to exploit common\
    \ blind spots in LLMs should help.\n\nWhat would be interesting is if someone\
    \ trained the exact same way with an equal number of equivalent but different\
    \ Winogrande questions (e.g. the same type and complexity of each question, just\
    \ with different subject matter so it's not contamination), and then see if other\
    \ tests like Arc get the same boost."
  created_at: 2024-01-13 04:00:42+00:00
  edited: false
  hidden: false
  id: 65a20aea8ac06028205b6058
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
      fullname: Vasile Ermicioi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vasilee
      type: user
    createdAt: '2024-01-17T23:08:30.000Z'
    data:
      edited: true
      editors:
      - vasilee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9863755702972412
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
          fullname: Vasile Ermicioi
          isHf: false
          isPro: false
          name: vasilee
          type: user
        html: '<p>there was a case like that before, a coding model was contaminated
          and the authors deleted it and also their account,<br>but people were saying
          it has very good performance in practice even if it was contaminated, so
          TheBloke left the model in place,<br>see here <a href="https://huggingface.co/TheBloke/NewHope-GGML/discussions/4">https://huggingface.co/TheBloke/NewHope-GGML/discussions/4</a></p>

          <p>my intuition is that if you take olympiad tasks and train a student,
          he will become proficient in the school curricula too</p>

          '
        raw: "there was a case like that before, a coding model was contaminated and\
          \ the authors deleted it and also their account, \nbut people were saying\
          \ it has very good performance in practice even if it was contaminated,\
          \ so TheBloke left the model in place,\nsee here https://huggingface.co/TheBloke/NewHope-GGML/discussions/4\n\
          \nmy intuition is that if you take olympiad tasks and train a student, he\
          \ will become proficient in the school curricula too"
        updatedAt: '2024-01-17T23:09:36.774Z'
      numEdits: 1
      reactions: []
    id: 65a85deef45ee5e5b554980c
    type: comment
  author: vasilee
  content: "there was a case like that before, a coding model was contaminated and\
    \ the authors deleted it and also their account, \nbut people were saying it has\
    \ very good performance in practice even if it was contaminated, so TheBloke left\
    \ the model in place,\nsee here https://huggingface.co/TheBloke/NewHope-GGML/discussions/4\n\
    \nmy intuition is that if you take olympiad tasks and train a student, he will\
    \ become proficient in the school curricula too"
  created_at: 2024-01-17 23:08:30+00:00
  edited: true
  hidden: false
  id: 65a85deef45ee5e5b554980c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6401c8c9f98fbc64bcd7dca1/MOSgc_mPbfUZ-354osy1v.png?w=200&h=200&f=face
      fullname: FBL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: fblgit
      type: user
    createdAt: '2024-01-18T15:44:57.000Z'
    data:
      edited: false
      editors:
      - fblgit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9278573989868164
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6401c8c9f98fbc64bcd7dca1/MOSgc_mPbfUZ-354osy1v.png?w=200&h=200&f=face
          fullname: FBL
          isHf: false
          isPro: true
          name: fblgit
          type: user
        html: '<p>IMO thats the important point. But why? Is not that those small
          test sets holds any super magical thing.. and how can we holistically evaluate
          them further..?</p>

          '
        raw: IMO thats the important point. But why? Is not that those small test
          sets holds any super magical thing.. and how can we holistically evaluate
          them further..?
        updatedAt: '2024-01-18T15:44:57.483Z'
      numEdits: 0
      reactions: []
    id: 65a947798abada9cc191f1c7
    type: comment
  author: fblgit
  content: IMO thats the important point. But why? Is not that those small test sets
    holds any super magical thing.. and how can we holistically evaluate them further..?
  created_at: 2024-01-18 15:44:57+00:00
  edited: false
  hidden: false
  id: 65a947798abada9cc191f1c7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: udkai/Garrulus
repo_type: model
status: open
target_branch: null
title: so whats the hypothesis?
