!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KnutJaegersberg
conflicting_files: null
created_at: 2023-04-15 07:12:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-04-15T08:12:28.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>I don''t know when HF releases support for int4 fine tuning. Others
          are alread onto building it.<br>Since llama 30b is properly the best model
          that fits on an rtx 3090, I guess, this model here could be used as well.
          However, the original weights quantized to int4 for fine tuning will be
          useful, too.<br>I think lora fine tuning does not depend a lot on parameter
          count. It is possible to lora fine tune gptneox 20b in 8 bit.<br>I''d guess
          it should be possible to lora fine tune llama 30b int4 on an rtx 3090.<br>Will
          you watch this space, too?<br>Such a base model would be very valuable to
          the community, I''d guess. </p>

          '
        raw: "I don't know when HF releases support for int4 fine tuning. Others are\
          \ alread onto building it. \r\nSince llama 30b is properly the best model\
          \ that fits on an rtx 3090, I guess, this model here could be used as well.\
          \ However, the original weights quantized to int4 for fine tuning will be\
          \ useful, too. \r\nI think lora fine tuning does not depend a lot on parameter\
          \ count. It is possible to lora fine tune gptneox 20b in 8 bit. \r\nI'd\
          \ guess it should be possible to lora fine tune llama 30b int4 on an rtx\
          \ 3090. \r\nWill you watch this space, too? \r\nSuch a base model would\
          \ be very valuable to the community, I'd guess. "
        updatedAt: '2023-04-15T08:12:28.362Z'
      numEdits: 0
      reactions: []
    id: 643a5c6cc5f633a7fa7ca259
    type: comment
  author: KnutJaegersberg
  content: "I don't know when HF releases support for int4 fine tuning. Others are\
    \ alread onto building it. \r\nSince llama 30b is properly the best model that\
    \ fits on an rtx 3090, I guess, this model here could be used as well. However,\
    \ the original weights quantized to int4 for fine tuning will be useful, too.\
    \ \r\nI think lora fine tuning does not depend a lot on parameter count. It is\
    \ possible to lora fine tune gptneox 20b in 8 bit. \r\nI'd guess it should be\
    \ possible to lora fine tune llama 30b int4 on an rtx 3090. \r\nWill you watch\
    \ this space, too? \r\nSuch a base model would be very valuable to the community,\
    \ I'd guess. "
  created_at: 2023-04-15 07:12:28+00:00
  edited: false
  hidden: false
  id: 643a5c6cc5f633a7fa7ca259
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd6d702643cd5875d6a43e3369945ac5.svg
      fullname: Metal
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: MetaIX
      type: user
    createdAt: '2023-04-23T03:35:49.000Z'
    data:
      edited: false
      editors:
      - MetaIX
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd6d702643cd5875d6a43e3369945ac5.svg
          fullname: Metal
          isHf: false
          isPro: false
          name: MetaIX
          type: user
        html: '<p>There are some people fine-tuning on 4-bit already. See: <a rel="nofollow"
          href="https://github.com/johnsmith0031/alpaca_lora_4bit">https://github.com/johnsmith0031/alpaca_lora_4bit</a></p>

          '
        raw: 'There are some people fine-tuning on 4-bit already. See: https://github.com/johnsmith0031/alpaca_lora_4bit'
        updatedAt: '2023-04-23T03:35:49.661Z'
      numEdits: 0
      reactions: []
    id: 6444a7953dc283776337fae7
    type: comment
  author: MetaIX
  content: 'There are some people fine-tuning on 4-bit already. See: https://github.com/johnsmith0031/alpaca_lora_4bit'
  created_at: 2023-04-23 02:35:49+00:00
  edited: false
  hidden: false
  id: 6444a7953dc283776337fae7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-04-23T04:01:03.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>learned about that only yesterday. I heard hf library got an update
          and it might be necessary to reconvert the weights. how does that all the
          happy fine tuning? will the models still work in the future?</p>

          '
        raw: learned about that only yesterday. I heard hf library got an update and
          it might be necessary to reconvert the weights. how does that all the happy
          fine tuning? will the models still work in the future?
        updatedAt: '2023-04-23T04:01:03.244Z'
      numEdits: 0
      reactions: []
    id: 6444ad7f5298d19c9c0a6cab
    type: comment
  author: KnutJaegersberg
  content: learned about that only yesterday. I heard hf library got an update and
    it might be necessary to reconvert the weights. how does that all the happy fine
    tuning? will the models still work in the future?
  created_at: 2023-04-23 03:01:03+00:00
  edited: false
  hidden: false
  id: 6444ad7f5298d19c9c0a6cab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/B3s_gOB7oXTckYDkSSl0a.png?w=200&h=200&f=face
      fullname: Nathan Bollman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Acrious
      type: user
    createdAt: '2023-05-04T16:06:35.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/B3s_gOB7oXTckYDkSSl0a.png?w=200&h=200&f=face
          fullname: Nathan Bollman
          isHf: false
          isPro: false
          name: Acrious
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-05-06T05:57:36.384Z'
      numEdits: 0
      reactions: []
    id: 6453d80b68cbb276cb515c02
    type: comment
  author: Acrious
  content: This comment has been hidden
  created_at: 2023-05-04 15:06:35+00:00
  edited: true
  hidden: true
  id: 6453d80b68cbb276cb515c02
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: MetaIX/GPT4-X-Alpaca-30B-4bit
repo_type: model
status: open
target_branch: null
title: Model size for int4 fine tuning on rtx 3090
