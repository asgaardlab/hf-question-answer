!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Co0ode
conflicting_files: null
created_at: 2023-04-22 20:37:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61ddb82cc59d602a3a85afb1/AmEj4WwRmxz1v7YWDm3f-.png?w=200&h=200&f=face
      fullname: Co0ode
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Co0ode
      type: user
    createdAt: '2023-04-22T21:37:11.000Z'
    data:
      edited: false
      editors:
      - Co0ode
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61ddb82cc59d602a3a85afb1/AmEj4WwRmxz1v7YWDm3f-.png?w=200&h=200&f=face
          fullname: Co0ode
          isHf: false
          isPro: false
          name: Co0ode
          type: user
        html: "<p>Hi so in both KoboldAI Occams fork and 4 bit capable ooba its both\
          \ coming back with the same error.  Its unclear what is causing this as\
          \ the version and machine that I have not updated is still working fine\
          \ but all of the machines I have tried to install and load have not worked\
          \ at all.  Can you please confirm what might be causing this issue?  its\
          \ not totally apparent to me what changed<br>Internal: src/sentencepiece_processor.cc</p>\n\
          <p>RuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto-&gt;ParseFromArray(serialized.data(),\
          \ serialized.size())] </p>\n<p>From Ooba</p>\n<p>Traceback (most recent\
          \ call last):<br>  File \"/workspace/text-generation-webui/server.py\",\
          \ line 921, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>\
          \  File \"/workspace/text-generation-webui/modules/models.py\", line 229,\
          \ in load_model<br>    tokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}/\"), clean_up_tokenization_spaces=True)<br>\
          \  File \"/workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1811, in from_pretrained<br>    return cls._from_pretrained(<br>\
          \  File \"/workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1965, in _from_pretrained<br>    tokenizer = cls(*init_inputs, **init_kwargs)<br>\
          \  File \"/workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in <strong>init</strong><br>    self.sp_model.Load(vocab_file)<br>\
          \  File \"/workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/sentencepiece/<strong>init</strong>.py\"\
          , line 905, in Load<br>    return self.LoadFromFile(model_file)<br>  File\
          \ \"/workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/sentencepiece/<strong>init</strong>.py\"\
          , line 310, in LoadFromFile<br>    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)<br>RuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto-&gt;ParseFromArray(serialized.data(),\
          \ serialized.size())] </p>\n<p>From KAI</p>\n<p>sentencepiece_processor.cc(923)\
          \ LOG(ERROR) src/sentencepiece_processor.cc(290) [model_] Model is not initialized.<br>Returns\
          \ default value 0<br>ERROR      | <strong>main</strong>:g:609 - An error\
          \ has been caught in function 'g', process 'MainProcess' (359), thread 'MainThread'\
          \ (140485838542656):<br>Traceback (most recent call last):</p>\n<p>  File\
          \ \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/eventlet/green/thread.py\"\
          , line 43, in __thread_body<br>    func(*args, **kwargs)<br>    \u2502 \
          \    \u2502       \u2514 {}<br>    \u2502     \u2514 ()<br>    \u2514 &lt;bound\
          \ method Thread._bootstrap of &lt;Thread(Thread-62, started daemon 140480538463168)&gt;&gt;<br>\
          \  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/threading.py\"\
          , line 890, in _bootstrap<br>    self._bootstrap_inner()<br>    \u2502 \
          \   \u2514 &lt;function start_new_thread..wrap_bootstrap_inner at 0x7fc42c476c10&gt;<br>\
          \    \u2514 &lt;Thread(Thread-62, started daemon 140480538463168)&gt;<br>\
          \  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/eventlet/green/thread.py\"\
          , line 64, in wrap_bootstrap_inner<br>    bootstrap_inner()<br>    \u2514\
          \ &lt;bound method Thread._bootstrap_inner of &lt;Thread(Thread-62, started\
          \ daemon 140480538463168)&gt;&gt;<br>  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/threading.py\"\
          , line 932, in _bootstrap_inner<br>    self.run()<br>    \u2502    \u2514\
          \ &lt;function Thread.run at 0x7fc5669b9940&gt;<br>    \u2514 &lt;Thread(Thread-62,\
          \ started daemon 140480538463168)&gt;<br>  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/threading.py\"\
          , line 870, in run<br>    self._target(*self._args, **self._kwargs)<br>\
          \    \u2502    \u2502        \u2502    \u2502        \u2502    \u2514 {}<br>\
          \    \u2502    \u2502        \u2502    \u2502        \u2514 &lt;Thread(Thread-62,\
          \ started daemon 140480538463168)&gt;<br>    \u2502    \u2502        \u2502\
          \    \u2514 (&lt;socketio.server.Server object at 0x7fc43000d250&gt;, '4nVKA6iQiTr-FeLUAAAB',\
          \ 'FHLMbFRf-y0wxsPJAAAA', ['load_model', {'model': ...<br>    \u2502   \
          \ \u2502        \u2514 &lt;Thread(Thread-62, started daemon 140480538463168)&gt;<br>\
          \    \u2502    \u2514 &lt;bound method Server._handle_event_internal of\
          \ &lt;socketio.server.Server object at 0x7fc43000d250&gt;&gt;<br>    \u2514\
          \ &lt;Thread(Thread-62, started daemon 140480538463168)&gt;<br>  File \"\
          /workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/socketio/server.py\"\
          , line 731, in _handle_event_internal<br>    r = server._trigger_event(data[0],\
          \ namespace, sid, *data[1:])<br>        \u2502      \u2502             \
          \ \u2502        \u2502          \u2502     \u2514 ['load_model', {'model':\
          \ 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True, 'key': ''...<br>        \u2502      \u2502          \
          \    \u2502        \u2502          \u2514 '4nVKA6iQiTr-FeLUAAAB'<br>   \
          \     \u2502      \u2502              \u2502        \u2514 '/'<br>     \
          \   \u2502      \u2502              \u2514 ['load_model', {'model': 'NeoCustom',\
          \ 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4', 'use_gpu':\
          \ True, 'key': ''...<br>        \u2502      \u2514 &lt;function Server._trigger_event\
          \ at 0x7fc430e2f160&gt;<br>        \u2514 &lt;socketio.server.Server object\
          \ at 0x7fc43000d250&gt;<br>  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/socketio/server.py\"\
          , line 756, in _trigger_event<br>    return self.handlers[namespace]<a rel=\"\
          nofollow\" href=\"*args\">event</a><br>           \u2502    \u2502     \
          \   \u2502          \u2502       \u2514 ('4nVKA6iQiTr-FeLUAAAB', {'model':\
          \ 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True,...<br>           \u2502    \u2502        \u2502     \
          \     \u2514 'load_model'<br>           \u2502    \u2502        \u2514 '/'<br>\
          \           \u2502    \u2514 {'/': {'get_model_info': &lt;function get_model_info\
          \ at 0x7fc42cd30430&gt;, 'OAI_Key_Update': &lt;function get_oai_models at\
          \ 0x7fc42cd...<br>           \u2514 &lt;socketio.server.Server object at\
          \ 0x7fc43000d250&gt;<br>  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/flask_socketio/<strong>init</strong>.py\"\
          , line 282, in _handler<br>    return self._handle_event(handler, message,\
          \ namespace, sid,<br>           \u2502    \u2502             \u2502    \
          \    \u2502        \u2502          \u2514 '4nVKA6iQiTr-FeLUAAAB'<br>   \
          \        \u2502    \u2502             \u2502        \u2502        \u2514\
          \ '/'<br>           \u2502    \u2502             \u2502        \u2514 'load_model'<br>\
          \           \u2502    \u2502             \u2514 &lt;function UI_2_load_model\
          \ at 0x7fc42cceedc0&gt;<br>           \u2502    \u2514 &lt;function SocketIO._handle_event\
          \ at 0x7fc4309d1ee0&gt;<br>           \u2514 &lt;flask_socketio.SocketIO\
          \ object at 0x7fc43000d2b0&gt;<br>  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/flask_socketio/<strong>init</strong>.py\"\
          , line 828, in _handle_event<br>    ret = handler(*args)<br>          \u2502\
          \        \u2514 ({'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True, 'key': '', 'gpu_layers'...<br>          \u2514 &lt;function\
          \ UI_2_load_model at 0x7fc42cceedc0&gt;</p>\n<blockquote>\n<p>File \"aiserver.py\"\
          , line 609, in g<br>    return f(*a, **k)<br>           \u2502  \u2502 \
          \   \u2514 {}<br>           \u2502  \u2514 ({'model': 'NeoCustom', 'path':\
          \ '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4', 'use_gpu': True,\
          \ 'key': '', 'gpu_layers'...<br>           \u2514 &lt;function UI_2_load_model\
          \ at 0x7fc42cceeaf0&gt;</p>\n</blockquote>\n<p>  File \"aiserver.py\", line\
          \ 8960, in UI_2_load_model<br>    load_model(use_gpu=data['use_gpu'], gpu_layers=data['gpu_layers'],\
          \ disk_layers=data['disk_layers'], online_model=data['online_model'], url=koboldai_vars.colaburl,\
          \ use_8_bit=data['use_8_bit'], use_4_bit=data['use_4_bit'])<br>    \u2502\
          \                  \u2502                           \u2502             \
          \                  \u2502                                 \u2502       \
          \                  \u2502                                 \u2502       \
          \                     \u2514 {'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True, 'key': '', 'gpu_layers':...<br>    \u2502           \
          \       \u2502                           \u2502                        \
          \       \u2502                                 \u2502                  \
          \       \u2502                                 \u2514 {'model': 'NeoCustom',\
          \ 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4', 'use_gpu':\
          \ True, 'key': '', 'gpu_layers':...<br>    \u2502                  \u2502\
          \                           \u2502                               \u2502\
          \                                 \u2502                         \u2514\
          \ &lt;koboldai_settings.koboldai_vars object at 0x7fc42ff75610&gt;<br> \
          \   \u2502                  \u2502                           \u2502    \
          \                           \u2502                                 \u2514\
          \ {'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True, 'key': '', 'gpu_layers':...<br>    \u2502           \
          \       \u2502                           \u2502                        \
          \       \u2514 {'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True, 'key': '', 'gpu_layers':...<br>    \u2502           \
          \       \u2502                           \u2514 {'model': 'NeoCustom', 'path':\
          \ '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4', 'use_gpu': True,\
          \ 'key': '', 'gpu_layers':...<br>    \u2502                  \u2514 {'model':\
          \ 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True, 'key': '', 'gpu_layers':...<br>    \u2514 &lt;function\
          \ load_model at 0x7fc42cd30af0&gt;</p>\n<p>  File \"aiserver.py\", line\
          \ 3231, in load_model<br>    tokenizer = LlamaTokenizer.from_pretrained(koboldai_vars.custmodpth)<br>\
          \                \u2502              \u2502               \u2514 &lt;koboldai_settings.koboldai_vars\
          \ object at 0x7fc42ff75610&gt;<br>                \u2502              \u2514\
          \ &lt;classmethod object at 0x7fc4309d6430&gt;<br>                \u2514\
          \ &lt;class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'&gt;</p>\n\
          <p>  File \"aiserver.py\", line 128, in new_pretrainedtokenizerbase_from_pretrained<br>\
          \    tokenizer = old_pretrainedtokenizerbase_from_pretrained(cls, *args,\
          \ **kwargs)<br>                \u2502                                  \
          \         \u2502     \u2502       \u2514 {}<br>                \u2502  \
          \                                         \u2502     \u2514 ('/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',)<br>\
          \                \u2502                                           \u2514\
          \ &lt;class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'&gt;<br>\
          \                \u2514 &lt;function PreTrainedTokenizerBase.from_pretrained\
          \ at 0x7fc431cb7040&gt;</p>\n<p>  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1811, in from_pretrained<br>    return cls._from_pretrained(<br>\
          \           \u2502   \u2514 &lt;classmethod object at 0x7fc431cb8040&gt;<br>\
          \           \u2514 &lt;class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'&gt;<br>\
          \  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1965, in _from_pretrained<br>    tokenizer = cls(*init_inputs, **init_kwargs)<br>\
          \                \u2502    \u2502              \u2514 {'add_bos_token':\
          \ True, 'add_eos_token': False, 'bos_token': AddedToken(\"<s>\", rstrip=False,\
          \ lstrip=False, single_word=False,...<br>                \u2502    \u2514\
          \ ()<br>                \u2514 &lt;class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'&gt;<br>\
          \  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in <strong>init</strong><br>    self.sp_model.Load(vocab_file)<br>\
          \    \u2502    \u2502        \u2502    \u2514 '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4/tokenizer.model'<br>\
          \    \u2502    \u2502        \u2514 &lt;function SentencePieceProcessor.Load\
          \ at 0x7fc4345cca60&gt;<br>    \u2502    \u2514 &lt;sentencepiece.SentencePieceProcessor;\
          \ proxy of &lt;Swig Object of type 'sentencepiece::SentencePieceProcessor\
          \ *' at 0x7fc42c243...<br>    \u2514 LlamaTokenizer(name_or_path='/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ vocab_size=0, model_max_length=1000000000000...<br>  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/sentencepiece/__init__.py\"\
          , line 905, in Load<br>    return self.LoadFromFile(model_file)<br>    \
          \       \u2502    \u2502            \u2514 '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4/tokenizer.model'<br>\
          \           \u2502    \u2514 &lt;function SentencePieceProcessor.LoadFromFile\
          \ at 0x7fc4345ca5e0&gt;<br>           \u2514 &lt;sentencepiece.SentencePieceProcessor;\
          \ proxy of &lt;Swig Object of type 'sentencepiece::SentencePieceProcessor\
          \ *' at 0x7fc42c243...<br>  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/sentencepiece/__init__.py\"\
          , line 310, in LoadFromFile<br>    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)<br>           \u2502              \u2502                        \
          \           \u2502     \u2514 '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4/tokenizer.model'<br>\
          \           \u2502              \u2502                                 \
          \  \u2514 &lt;sentencepiece.SentencePieceProcessor; proxy of &lt;Swig Object\
          \ of type 'sentencepiece::SentencePieceProcessor *' at 0x7fc42c243...<br>\
          \           \u2502              \u2514 <br>           \u2514 &lt;module\
          \ 'sentencepiece._sentencepiece' from '/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/sentencepi...</s></p><s>\n\
          <p>RuntimeError: Internal: src/sentencepiece_processor.cc(1102) [model_proto-&gt;ParseFromArray(serialized.data(),\
          \ serialized.size())]<br>sentencepiece_processor.cc(923) LOG(ERROR) src/sentencepiece_processor.cc(290)\
          \ [model_] Model is not initialized.</p>\n</s>"
        raw: "Hi so in both KoboldAI Occams fork and 4 bit capable ooba its both coming\
          \ back with the same error.  Its unclear what is causing this as the version\
          \ and machine that I have not updated is still working fine but all of the\
          \ machines I have tried to install and load have not worked at all.  Can\
          \ you please confirm what might be causing this issue?  its not totally\
          \ apparent to me what changed\r\nInternal: src/sentencepiece_processor.cc\r\
          \n\r\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(),\
          \ serialized.size())] \r\n\r\nFrom Ooba\r\n\r\nTraceback (most recent call\
          \ last):\r\n  File \"/workspace/text-generation-webui/server.py\", line\
          \ 921, in <module>\r\n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\
          \n  File \"/workspace/text-generation-webui/modules/models.py\", line 229,\
          \ in load_model\r\n    tokenizer = LlamaTokenizer.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}/\"), clean_up_tokenization_spaces=True)\r\
          \n  File \"/workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1811, in from_pretrained\r\n    return cls._from_pretrained(\r\n\
          \  File \"/workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1965, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
          \n  File \"/workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\n  File \"\
          /workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n  File\
          \ \"/workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\r\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(),\
          \ serialized.size())] \r\n\r\nFrom KAI\r\n\r\nsentencepiece_processor.cc(923)\
          \ LOG(ERROR) src/sentencepiece_processor.cc(290) [model_] Model is not initialized.\r\
          \nReturns default value 0\r\nERROR      | __main__:g:609 - An error has\
          \ been caught in function 'g', process 'MainProcess' (359), thread 'MainThread'\
          \ (140485838542656):\r\nTraceback (most recent call last):\r\n\r\n  File\
          \ \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/eventlet/green/thread.py\"\
          , line 43, in __thread_body\r\n    func(*args, **kwargs)\r\n    \u2502 \
          \    \u2502       \u2514 {}\r\n    \u2502     \u2514 ()\r\n    \u2514 <bound\
          \ method Thread._bootstrap of <Thread(Thread-62, started daemon 140480538463168)>>\r\
          \n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/threading.py\"\
          , line 890, in _bootstrap\r\n    self._bootstrap_inner()\r\n    \u2502 \
          \   \u2514 <function start_new_thread.<locals>.wrap_bootstrap_inner at 0x7fc42c476c10>\r\
          \n    \u2514 <Thread(Thread-62, started daemon 140480538463168)>\r\n  File\
          \ \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/eventlet/green/thread.py\"\
          , line 64, in wrap_bootstrap_inner\r\n    bootstrap_inner()\r\n    \u2514\
          \ <bound method Thread._bootstrap_inner of <Thread(Thread-62, started daemon\
          \ 140480538463168)>>\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/threading.py\"\
          , line 932, in _bootstrap_inner\r\n    self.run()\r\n    \u2502    \u2514\
          \ <function Thread.run at 0x7fc5669b9940>\r\n    \u2514 <Thread(Thread-62,\
          \ started daemon 140480538463168)>\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/threading.py\"\
          , line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n\
          \    \u2502    \u2502        \u2502    \u2502        \u2502    \u2514 {}\r\
          \n    \u2502    \u2502        \u2502    \u2502        \u2514 <Thread(Thread-62,\
          \ started daemon 140480538463168)>\r\n    \u2502    \u2502        \u2502\
          \    \u2514 (<socketio.server.Server object at 0x7fc43000d250>, '4nVKA6iQiTr-FeLUAAAB',\
          \ 'FHLMbFRf-y0wxsPJAAAA', ['load_model', {'model': ...\r\n    \u2502   \
          \ \u2502        \u2514 <Thread(Thread-62, started daemon 140480538463168)>\r\
          \n    \u2502    \u2514 <bound method Server._handle_event_internal of <socketio.server.Server\
          \ object at 0x7fc43000d250>>\r\n    \u2514 <Thread(Thread-62, started daemon\
          \ 140480538463168)>\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/socketio/server.py\"\
          , line 731, in _handle_event_internal\r\n    r = server._trigger_event(data[0],\
          \ namespace, sid, *data[1:])\r\n        \u2502      \u2502             \
          \ \u2502        \u2502          \u2502     \u2514 ['load_model', {'model':\
          \ 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True, 'key': ''...\r\n        \u2502      \u2502          \
          \    \u2502        \u2502          \u2514 '4nVKA6iQiTr-FeLUAAAB'\r\n   \
          \     \u2502      \u2502              \u2502        \u2514 '/'\r\n     \
          \   \u2502      \u2502              \u2514 ['load_model', {'model': 'NeoCustom',\
          \ 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4', 'use_gpu':\
          \ True, 'key': ''...\r\n        \u2502      \u2514 <function Server._trigger_event\
          \ at 0x7fc430e2f160>\r\n        \u2514 <socketio.server.Server object at\
          \ 0x7fc43000d250>\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/socketio/server.py\"\
          , line 756, in _trigger_event\r\n    return self.handlers[namespace][event](*args)\r\
          \n           \u2502    \u2502        \u2502          \u2502       \u2514\
          \ ('4nVKA6iQiTr-FeLUAAAB', {'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True,...\r\n           \u2502    \u2502        \u2502     \
          \     \u2514 'load_model'\r\n           \u2502    \u2502        \u2514 '/'\r\
          \n           \u2502    \u2514 {'/': {'get_model_info': <function get_model_info\
          \ at 0x7fc42cd30430>, 'OAI_Key_Update': <function get_oai_models at 0x7fc42cd...\r\
          \n           \u2514 <socketio.server.Server object at 0x7fc43000d250>\r\n\
          \  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/flask_socketio/__init__.py\"\
          , line 282, in _handler\r\n    return self._handle_event(handler, message,\
          \ namespace, sid,\r\n           \u2502    \u2502             \u2502    \
          \    \u2502        \u2502          \u2514 '4nVKA6iQiTr-FeLUAAAB'\r\n   \
          \        \u2502    \u2502             \u2502        \u2502        \u2514\
          \ '/'\r\n           \u2502    \u2502             \u2502        \u2514 'load_model'\r\
          \n           \u2502    \u2502             \u2514 <function UI_2_load_model\
          \ at 0x7fc42cceedc0>\r\n           \u2502    \u2514 <function SocketIO._handle_event\
          \ at 0x7fc4309d1ee0>\r\n           \u2514 <flask_socketio.SocketIO object\
          \ at 0x7fc43000d2b0>\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/flask_socketio/__init__.py\"\
          , line 828, in _handle_event\r\n    ret = handler(*args)\r\n          \u2502\
          \        \u2514 ({'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True, 'key': '', 'gpu_layers'...\r\n          \u2514 <function\
          \ UI_2_load_model at 0x7fc42cceedc0>\r\n\r\n> File \"aiserver.py\", line\
          \ 609, in g\r\n    return f(*a, **k)\r\n           \u2502  \u2502    \u2514\
          \ {}\r\n           \u2502  \u2514 ({'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True, 'key': '', 'gpu_layers'...\r\n           \u2514 <function\
          \ UI_2_load_model at 0x7fc42cceeaf0>\r\n\r\n  File \"aiserver.py\", line\
          \ 8960, in UI_2_load_model\r\n    load_model(use_gpu=data['use_gpu'], gpu_layers=data['gpu_layers'],\
          \ disk_layers=data['disk_layers'], online_model=data['online_model'], url=koboldai_vars.colaburl,\
          \ use_8_bit=data['use_8_bit'], use_4_bit=data['use_4_bit'])\r\n    \u2502\
          \                  \u2502                           \u2502             \
          \                  \u2502                                 \u2502       \
          \                  \u2502                                 \u2502       \
          \                     \u2514 {'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True, 'key': '', 'gpu_layers':...\r\n    \u2502           \
          \       \u2502                           \u2502                        \
          \       \u2502                                 \u2502                  \
          \       \u2502                                 \u2514 {'model': 'NeoCustom',\
          \ 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4', 'use_gpu':\
          \ True, 'key': '', 'gpu_layers':...\r\n    \u2502                  \u2502\
          \                           \u2502                               \u2502\
          \                                 \u2502                         \u2514\
          \ <koboldai_settings.koboldai_vars object at 0x7fc42ff75610>\r\n    \u2502\
          \                  \u2502                           \u2502             \
          \                  \u2502                                 \u2514 {'model':\
          \ 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True, 'key': '', 'gpu_layers':...\r\n    \u2502           \
          \       \u2502                           \u2502                        \
          \       \u2514 {'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True, 'key': '', 'gpu_layers':...\r\n    \u2502           \
          \       \u2502                           \u2514 {'model': 'NeoCustom', 'path':\
          \ '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4', 'use_gpu': True,\
          \ 'key': '', 'gpu_layers':...\r\n    \u2502                  \u2514 {'model':\
          \ 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ 'use_gpu': True, 'key': '', 'gpu_layers':...\r\n    \u2514 <function load_model\
          \ at 0x7fc42cd30af0>\r\n\r\n  File \"aiserver.py\", line 3231, in load_model\r\
          \n    tokenizer = LlamaTokenizer.from_pretrained(koboldai_vars.custmodpth)\r\
          \n                \u2502              \u2502               \u2514 <koboldai_settings.koboldai_vars\
          \ object at 0x7fc42ff75610>\r\n                \u2502              \u2514\
          \ <classmethod object at 0x7fc4309d6430>\r\n                \u2514 <class\
          \ 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\r\n\r\n\
          \  File \"aiserver.py\", line 128, in new_pretrainedtokenizerbase_from_pretrained\r\
          \n    tokenizer = old_pretrainedtokenizerbase_from_pretrained(cls, *args,\
          \ **kwargs)\r\n                \u2502                                  \
          \         \u2502     \u2502       \u2514 {}\r\n                \u2502  \
          \                                         \u2502     \u2514 ('/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',)\r\
          \n                \u2502                                           \u2514\
          \ <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\r\
          \n                \u2514 <function PreTrainedTokenizerBase.from_pretrained\
          \ at 0x7fc431cb7040>\r\n\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1811, in from_pretrained\r\n    return cls._from_pretrained(\r\n\
          \           \u2502   \u2514 <classmethod object at 0x7fc431cb8040>\r\n \
          \          \u2514 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\r\
          \n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1965, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
          \n                \u2502    \u2502              \u2514 {'add_bos_token':\
          \ True, 'add_eos_token': False, 'bos_token': AddedToken(\"<s>\", rstrip=False,\
          \ lstrip=False, single_word=False,...\r\n                \u2502    \u2514\
          \ ()\r\n                \u2514 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\r\
          \n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\n    \u2502\
          \    \u2502        \u2502    \u2514 '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4/tokenizer.model'\r\
          \n    \u2502    \u2502        \u2514 <function SentencePieceProcessor.Load\
          \ at 0x7fc4345cca60>\r\n    \u2502    \u2514 <sentencepiece.SentencePieceProcessor;\
          \ proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *'\
          \ at 0x7fc42c243...\r\n    \u2514 LlamaTokenizer(name_or_path='/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
          \ vocab_size=0, model_max_length=1000000000000...\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/sentencepiece/__init__.py\"\
          , line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n    \
          \       \u2502    \u2502            \u2514 '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4/tokenizer.model'\r\
          \n           \u2502    \u2514 <function SentencePieceProcessor.LoadFromFile\
          \ at 0x7fc4345ca5e0>\r\n           \u2514 <sentencepiece.SentencePieceProcessor;\
          \ proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *'\
          \ at 0x7fc42c243...\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/sentencepiece/__init__.py\"\
          , line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\r\n           \u2502              \u2502                        \
          \           \u2502     \u2514 '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4/tokenizer.model'\r\
          \n           \u2502              \u2502                                \
          \   \u2514 <sentencepiece.SentencePieceProcessor; proxy of <Swig Object\
          \ of type 'sentencepiece::SentencePieceProcessor *' at 0x7fc42c243...\r\n\
          \           \u2502              \u2514 <built-in function SentencePieceProcessor_LoadFromFile>\r\
          \n           \u2514 <module 'sentencepiece._sentencepiece' from '/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/sentencepi...\r\
          \n\r\nRuntimeError: Internal: src/sentencepiece_processor.cc(1102) [model_proto->ParseFromArray(serialized.data(),\
          \ serialized.size())] \r\nsentencepiece_processor.cc(923) LOG(ERROR) src/sentencepiece_processor.cc(290)\
          \ [model_] Model is not initialized."
        updatedAt: '2023-04-22T21:37:11.919Z'
      numEdits: 0
      reactions: []
    id: 6444538792c17f01a171e7a5
    type: comment
  author: Co0ode
  content: "Hi so in both KoboldAI Occams fork and 4 bit capable ooba its both coming\
    \ back with the same error.  Its unclear what is causing this as the version and\
    \ machine that I have not updated is still working fine but all of the machines\
    \ I have tried to install and load have not worked at all.  Can you please confirm\
    \ what might be causing this issue?  its not totally apparent to me what changed\r\
    \nInternal: src/sentencepiece_processor.cc\r\n\r\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101)\
    \ [model_proto->ParseFromArray(serialized.data(), serialized.size())] \r\n\r\n\
    From Ooba\r\n\r\nTraceback (most recent call last):\r\n  File \"/workspace/text-generation-webui/server.py\"\
    , line 921, in <module>\r\n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\
    \n  File \"/workspace/text-generation-webui/modules/models.py\", line 229, in\
    \ load_model\r\n    tokenizer = LlamaTokenizer.from_pretrained(Path(f\"{shared.args.model_dir}/{model_name}/\"\
    ), clean_up_tokenization_spaces=True)\r\n  File \"/workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1811, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File\
    \ \"/workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1965, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n  File \"/workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py\"\
    , line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\n  File \"/workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
    , line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n  File \"/workspace/miniconda3/envs/textgen/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
    , line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\r\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(),\
    \ serialized.size())] \r\n\r\nFrom KAI\r\n\r\nsentencepiece_processor.cc(923)\
    \ LOG(ERROR) src/sentencepiece_processor.cc(290) [model_] Model is not initialized.\r\
    \nReturns default value 0\r\nERROR      | __main__:g:609 - An error has been caught\
    \ in function 'g', process 'MainProcess' (359), thread 'MainThread' (140485838542656):\r\
    \nTraceback (most recent call last):\r\n\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/eventlet/green/thread.py\"\
    , line 43, in __thread_body\r\n    func(*args, **kwargs)\r\n    \u2502     \u2502\
    \       \u2514 {}\r\n    \u2502     \u2514 ()\r\n    \u2514 <bound method Thread._bootstrap\
    \ of <Thread(Thread-62, started daemon 140480538463168)>>\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/threading.py\"\
    , line 890, in _bootstrap\r\n    self._bootstrap_inner()\r\n    \u2502    \u2514\
    \ <function start_new_thread.<locals>.wrap_bootstrap_inner at 0x7fc42c476c10>\r\
    \n    \u2514 <Thread(Thread-62, started daemon 140480538463168)>\r\n  File \"\
    /workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/eventlet/green/thread.py\"\
    , line 64, in wrap_bootstrap_inner\r\n    bootstrap_inner()\r\n    \u2514 <bound\
    \ method Thread._bootstrap_inner of <Thread(Thread-62, started daemon 140480538463168)>>\r\
    \n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/threading.py\"\
    , line 932, in _bootstrap_inner\r\n    self.run()\r\n    \u2502    \u2514 <function\
    \ Thread.run at 0x7fc5669b9940>\r\n    \u2514 <Thread(Thread-62, started daemon\
    \ 140480538463168)>\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/threading.py\"\
    , line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n    \u2502\
    \    \u2502        \u2502    \u2502        \u2502    \u2514 {}\r\n    \u2502 \
    \   \u2502        \u2502    \u2502        \u2514 <Thread(Thread-62, started daemon\
    \ 140480538463168)>\r\n    \u2502    \u2502        \u2502    \u2514 (<socketio.server.Server\
    \ object at 0x7fc43000d250>, '4nVKA6iQiTr-FeLUAAAB', 'FHLMbFRf-y0wxsPJAAAA', ['load_model',\
    \ {'model': ...\r\n    \u2502    \u2502        \u2514 <Thread(Thread-62, started\
    \ daemon 140480538463168)>\r\n    \u2502    \u2514 <bound method Server._handle_event_internal\
    \ of <socketio.server.Server object at 0x7fc43000d250>>\r\n    \u2514 <Thread(Thread-62,\
    \ started daemon 140480538463168)>\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/socketio/server.py\"\
    , line 731, in _handle_event_internal\r\n    r = server._trigger_event(data[0],\
    \ namespace, sid, *data[1:])\r\n        \u2502      \u2502              \u2502\
    \        \u2502          \u2502     \u2514 ['load_model', {'model': 'NeoCustom',\
    \ 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4', 'use_gpu': True,\
    \ 'key': ''...\r\n        \u2502      \u2502              \u2502        \u2502\
    \          \u2514 '4nVKA6iQiTr-FeLUAAAB'\r\n        \u2502      \u2502       \
    \       \u2502        \u2514 '/'\r\n        \u2502      \u2502              \u2514\
    \ ['load_model', {'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
    \ 'use_gpu': True, 'key': ''...\r\n        \u2502      \u2514 <function Server._trigger_event\
    \ at 0x7fc430e2f160>\r\n        \u2514 <socketio.server.Server object at 0x7fc43000d250>\r\
    \n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/socketio/server.py\"\
    , line 756, in _trigger_event\r\n    return self.handlers[namespace][event](*args)\r\
    \n           \u2502    \u2502        \u2502          \u2502       \u2514 ('4nVKA6iQiTr-FeLUAAAB',\
    \ {'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
    \ 'use_gpu': True,...\r\n           \u2502    \u2502        \u2502          \u2514\
    \ 'load_model'\r\n           \u2502    \u2502        \u2514 '/'\r\n          \
    \ \u2502    \u2514 {'/': {'get_model_info': <function get_model_info at 0x7fc42cd30430>,\
    \ 'OAI_Key_Update': <function get_oai_models at 0x7fc42cd...\r\n           \u2514\
    \ <socketio.server.Server object at 0x7fc43000d250>\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/flask_socketio/__init__.py\"\
    , line 282, in _handler\r\n    return self._handle_event(handler, message, namespace,\
    \ sid,\r\n           \u2502    \u2502             \u2502        \u2502       \
    \ \u2502          \u2514 '4nVKA6iQiTr-FeLUAAAB'\r\n           \u2502    \u2502\
    \             \u2502        \u2502        \u2514 '/'\r\n           \u2502    \u2502\
    \             \u2502        \u2514 'load_model'\r\n           \u2502    \u2502\
    \             \u2514 <function UI_2_load_model at 0x7fc42cceedc0>\r\n        \
    \   \u2502    \u2514 <function SocketIO._handle_event at 0x7fc4309d1ee0>\r\n \
    \          \u2514 <flask_socketio.SocketIO object at 0x7fc43000d2b0>\r\n  File\
    \ \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/flask_socketio/__init__.py\"\
    , line 828, in _handle_event\r\n    ret = handler(*args)\r\n          \u2502 \
    \       \u2514 ({'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
    \ 'use_gpu': True, 'key': '', 'gpu_layers'...\r\n          \u2514 <function UI_2_load_model\
    \ at 0x7fc42cceedc0>\r\n\r\n> File \"aiserver.py\", line 609, in g\r\n    return\
    \ f(*a, **k)\r\n           \u2502  \u2502    \u2514 {}\r\n           \u2502  \u2514\
    \ ({'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
    \ 'use_gpu': True, 'key': '', 'gpu_layers'...\r\n           \u2514 <function UI_2_load_model\
    \ at 0x7fc42cceeaf0>\r\n\r\n  File \"aiserver.py\", line 8960, in UI_2_load_model\r\
    \n    load_model(use_gpu=data['use_gpu'], gpu_layers=data['gpu_layers'], disk_layers=data['disk_layers'],\
    \ online_model=data['online_model'], url=koboldai_vars.colaburl, use_8_bit=data['use_8_bit'],\
    \ use_4_bit=data['use_4_bit'])\r\n    \u2502                  \u2502         \
    \                  \u2502                               \u2502               \
    \                  \u2502                         \u2502                     \
    \            \u2502                            \u2514 {'model': 'NeoCustom', 'path':\
    \ '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4', 'use_gpu': True, 'key':\
    \ '', 'gpu_layers':...\r\n    \u2502                  \u2502                 \
    \          \u2502                               \u2502                       \
    \          \u2502                         \u2502                             \
    \    \u2514 {'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
    \ 'use_gpu': True, 'key': '', 'gpu_layers':...\r\n    \u2502                 \
    \ \u2502                           \u2502                               \u2502\
    \                                 \u2502                         \u2514 <koboldai_settings.koboldai_vars\
    \ object at 0x7fc42ff75610>\r\n    \u2502                  \u2502            \
    \               \u2502                               \u2502                  \
    \               \u2514 {'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
    \ 'use_gpu': True, 'key': '', 'gpu_layers':...\r\n    \u2502                 \
    \ \u2502                           \u2502                               \u2514\
    \ {'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
    \ 'use_gpu': True, 'key': '', 'gpu_layers':...\r\n    \u2502                 \
    \ \u2502                           \u2514 {'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
    \ 'use_gpu': True, 'key': '', 'gpu_layers':...\r\n    \u2502                 \
    \ \u2514 {'model': 'NeoCustom', 'path': '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
    \ 'use_gpu': True, 'key': '', 'gpu_layers':...\r\n    \u2514 <function load_model\
    \ at 0x7fc42cd30af0>\r\n\r\n  File \"aiserver.py\", line 3231, in load_model\r\
    \n    tokenizer = LlamaTokenizer.from_pretrained(koboldai_vars.custmodpth)\r\n\
    \                \u2502              \u2502               \u2514 <koboldai_settings.koboldai_vars\
    \ object at 0x7fc42ff75610>\r\n                \u2502              \u2514 <classmethod\
    \ object at 0x7fc4309d6430>\r\n                \u2514 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\r\
    \n\r\n  File \"aiserver.py\", line 128, in new_pretrainedtokenizerbase_from_pretrained\r\
    \n    tokenizer = old_pretrainedtokenizerbase_from_pretrained(cls, *args, **kwargs)\r\
    \n                \u2502                                           \u2502    \
    \ \u2502       \u2514 {}\r\n                \u2502                           \
    \                \u2502     \u2514 ('/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',)\r\
    \n                \u2502                                           \u2514 <class\
    \ 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\r\n         \
    \       \u2514 <function PreTrainedTokenizerBase.from_pretrained at 0x7fc431cb7040>\r\
    \n\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1811, in from_pretrained\r\n    return cls._from_pretrained(\r\n      \
    \     \u2502   \u2514 <classmethod object at 0x7fc431cb8040>\r\n           \u2514\
    \ <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\r\n  File\
    \ \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1965, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n                \u2502    \u2502              \u2514 {'add_bos_token': True,\
    \ 'add_eos_token': False, 'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False,\
    \ single_word=False,...\r\n                \u2502    \u2514 ()\r\n           \
    \     \u2514 <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\r\
    \n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama.py\"\
    , line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\n    \u2502   \
    \ \u2502        \u2502    \u2514 '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4/tokenizer.model'\r\
    \n    \u2502    \u2502        \u2514 <function SentencePieceProcessor.Load at\
    \ 0x7fc4345cca60>\r\n    \u2502    \u2514 <sentencepiece.SentencePieceProcessor;\
    \ proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x7fc42c243...\r\
    \n    \u2514 LlamaTokenizer(name_or_path='/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4',\
    \ vocab_size=0, model_max_length=1000000000000...\r\n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/sentencepiece/__init__.py\"\
    , line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n          \
    \ \u2502    \u2502            \u2514 '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4/tokenizer.model'\r\
    \n           \u2502    \u2514 <function SentencePieceProcessor.LoadFromFile at\
    \ 0x7fc4345ca5e0>\r\n           \u2514 <sentencepiece.SentencePieceProcessor;\
    \ proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x7fc42c243...\r\
    \n  File \"/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/sentencepiece/__init__.py\"\
    , line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\r\n           \u2502              \u2502                              \
    \     \u2502     \u2514 '/workspace/KoboldAI/models/GPT4-X-Alpaca-30B-Int4/tokenizer.model'\r\
    \n           \u2502              \u2502                                   \u2514\
    \ <sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor\
    \ *' at 0x7fc42c243...\r\n           \u2502              \u2514 <built-in function\
    \ SentencePieceProcessor_LoadFromFile>\r\n           \u2514 <module 'sentencepiece._sentencepiece'\
    \ from '/workspace/KoboldAI/runtime/envs/koboldai/lib/python3.8/site-packages/sentencepi...\r\
    \n\r\nRuntimeError: Internal: src/sentencepiece_processor.cc(1102) [model_proto->ParseFromArray(serialized.data(),\
    \ serialized.size())] \r\nsentencepiece_processor.cc(923) LOG(ERROR) src/sentencepiece_processor.cc(290)\
    \ [model_] Model is not initialized."
  created_at: 2023-04-22 20:37:11+00:00
  edited: false
  hidden: false
  id: 6444538792c17f01a171e7a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61ddb82cc59d602a3a85afb1/AmEj4WwRmxz1v7YWDm3f-.png?w=200&h=200&f=face
      fullname: Co0ode
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Co0ode
      type: user
    createdAt: '2023-04-22T21:45:31.000Z'
    data:
      edited: false
      editors:
      - Co0ode
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61ddb82cc59d602a3a85afb1/AmEj4WwRmxz1v7YWDm3f-.png?w=200&h=200&f=face
          fullname: Co0ode
          isHf: false
          isPro: false
          name: Co0ode
          type: user
        html: '<p>Rolled back to version from 3/4 days ago and works again.  Thanks</p>

          '
        raw: Rolled back to version from 3/4 days ago and works again.  Thanks
        updatedAt: '2023-04-22T21:45:31.450Z'
      numEdits: 0
      reactions: []
    id: 6444557b8f795c936d062091
    type: comment
  author: Co0ode
  content: Rolled back to version from 3/4 days ago and works again.  Thanks
  created_at: 2023-04-22 20:45:31+00:00
  edited: false
  hidden: false
  id: 6444557b8f795c936d062091
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
      fullname: Jonathan Yankovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tensiondriven
      type: user
    createdAt: '2023-04-23T02:20:13.000Z'
    data:
      edited: false
      editors:
      - tensiondriven
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
          fullname: Jonathan Yankovich
          isHf: false
          isPro: false
          name: tensiondriven
          type: user
        html: '<p>Thanks for posting that, very helpful!</p>

          '
        raw: Thanks for posting that, very helpful!
        updatedAt: '2023-04-23T02:20:13.035Z'
      numEdits: 0
      reactions: []
    id: 644495ddc63001ae635ace37
    type: comment
  author: tensiondriven
  content: Thanks for posting that, very helpful!
  created_at: 2023-04-23 01:20:13+00:00
  edited: false
  hidden: false
  id: 644495ddc63001ae635ace37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd6d702643cd5875d6a43e3369945ac5.svg
      fullname: Metal
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: MetaIX
      type: user
    createdAt: '2023-04-23T03:23:20.000Z'
    data:
      edited: true
      editors:
      - MetaIX
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd6d702643cd5875d6a43e3369945ac5.svg
          fullname: Metal
          isHf: false
          isPro: false
          name: MetaIX
          type: user
        html: '<p>I just tested with the latest version of ooba and it works. Make
          sure you''re using the old cuda branch. More info here: <a rel="nofollow"
          href="https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md">https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md</a>
          Also, If you follow the instructions in his repo, it should work as is.</p>

          '
        raw: 'I just tested with the latest version of ooba and it works. Make sure
          you''re using the old cuda branch. More info here: https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md
          Also, If you follow the instructions in his repo, it should work as is.'
        updatedAt: '2023-04-23T03:25:57.740Z'
      numEdits: 2
      reactions: []
    id: 6444a4a83dc283776337c66e
    type: comment
  author: MetaIX
  content: 'I just tested with the latest version of ooba and it works. Make sure
    you''re using the old cuda branch. More info here: https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md
    Also, If you follow the instructions in his repo, it should work as is.'
  created_at: 2023-04-23 02:23:20+00:00
  edited: true
  hidden: false
  id: 6444a4a83dc283776337c66e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-04-26T09:04:56.000Z'
    data:
      edited: false
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>MetaIX How can it run on the most recent version and old kuda branch?
          Is that like an add on? Or do I have to install a second version the older
          version of ooba? I don''t really understand how to use the old branch. If
          you were using it I don''t see how you could also be on the latest version?
          Sorry I don''t know to much about this. How do I get this model running
          when I have ooba fully updated at the moment. What is my next step. Thanks.</p>

          '
        raw: MetaIX How can it run on the most recent version and old kuda branch?
          Is that like an add on? Or do I have to install a second version the older
          version of ooba? I don't really understand how to use the old branch. If
          you were using it I don't see how you could also be on the latest version?
          Sorry I don't know to much about this. How do I get this model running when
          I have ooba fully updated at the moment. What is my next step. Thanks.
        updatedAt: '2023-04-26T09:04:56.825Z'
      numEdits: 0
      reactions: []
    id: 6448e938f88f1495f092c10f
    type: comment
  author: Goldenblood56
  content: MetaIX How can it run on the most recent version and old kuda branch? Is
    that like an add on? Or do I have to install a second version the older version
    of ooba? I don't really understand how to use the old branch. If you were using
    it I don't see how you could also be on the latest version? Sorry I don't know
    to much about this. How do I get this model running when I have ooba fully updated
    at the moment. What is my next step. Thanks.
  created_at: 2023-04-26 08:04:56+00:00
  edited: false
  hidden: false
  id: 6448e938f88f1495f092c10f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: MetaIX/GPT4-X-Alpaca-30B-4bit
repo_type: model
status: open
target_branch: null
title: 'Error: Internal: src/sentencepiece_processor.cc in Ooba and KAI 4bit'
