!!python/object:huggingface_hub.community.DiscussionWithDetails
author: l-i
conflicting_files: []
created_at: 2023-11-13 23:17:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f78489d36951307fd0e379/LHaUSVsG2pqUovulFMXd8.png?w=200&h=200&f=face
      fullname: Wenchen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: l-i
      type: user
    createdAt: '2023-11-13T23:17:57.000Z'
    data:
      edited: false
      editors:
      - l-i
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.11044929921627045
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f78489d36951307fd0e379/LHaUSVsG2pqUovulFMXd8.png?w=200&h=200&f=face
          fullname: Wenchen
          isHf: false
          isPro: false
          name: l-i
          type: user
        html: ''
        raw: ''
        updatedAt: '2023-11-13T23:17:57.106Z'
      numEdits: 0
      reactions: []
    id: 6552aea50cb3b8bcd1eac181
    type: comment
  author: l-i
  content: ''
  created_at: 2023-11-13 23:17:57+00:00
  edited: false
  hidden: false
  id: 6552aea50cb3b8bcd1eac181
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61a50668cd3eb3043f38e445/E_M7m93Ou6d-XJij59VGF.jpeg?w=200&h=200&f=face
      fullname: Jingya Huang
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Jingya
      type: user
    createdAt: '2023-11-14T09:16:24.000Z'
    data:
      edited: false
      editors:
      - Jingya
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7384852170944214
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61a50668cd3eb3043f38e445/E_M7m93Ou6d-XJij59VGF.jpeg?w=200&h=200&f=face
          fullname: Jingya Huang
          isHf: true
          isPro: false
          name: Jingya
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;l-i&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/l-i\">@<span class=\"\
          underline\">l-i</span></a></span>\n\n\t</span></span>, </p>\n<p>Thanks for\
          \ opening the issue. You would always be able to enable <a href=\"https://huggingface.co/docs/optimum-neuron/package_reference/modeling#optimum.neuron.modeling_diffusion.NeuronStableDiffusionPipelineBase.load_model.dynamic_batch_size\"\
          ><code>dynamic_batch_size</code></a> while compiling the checkpoint.  </p>\n\
          <p>According to previous experience, we used to reach better latency with\
          \ static batch size. But maybe we can add a checkpoint with dynamic batch\
          \ size <span data-props=\"{&quot;user&quot;:&quot;philschmid&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/philschmid\">@<span class=\"\
          underline\">philschmid</span></a></span>\n\n\t</span></span>, WDYT?</p>\n"
        raw: "Hi @l-i, \n\nThanks for opening the issue. You would always be able\
          \ to enable [`dynamic_batch_size`](https://huggingface.co/docs/optimum-neuron/package_reference/modeling#optimum.neuron.modeling_diffusion.NeuronStableDiffusionPipelineBase.load_model.dynamic_batch_size)\
          \ while compiling the checkpoint.  \n\nAccording to previous experience,\
          \ we used to reach better latency with static batch size. But maybe we can\
          \ add a checkpoint with dynamic batch size @philschmid, WDYT?"
        updatedAt: '2023-11-14T09:16:24.428Z'
      numEdits: 0
      reactions: []
    id: 65533ae80177de30b31e8251
    type: comment
  author: Jingya
  content: "Hi @l-i, \n\nThanks for opening the issue. You would always be able to\
    \ enable [`dynamic_batch_size`](https://huggingface.co/docs/optimum-neuron/package_reference/modeling#optimum.neuron.modeling_diffusion.NeuronStableDiffusionPipelineBase.load_model.dynamic_batch_size)\
    \ while compiling the checkpoint.  \n\nAccording to previous experience, we used\
    \ to reach better latency with static batch size. But maybe we can add a checkpoint\
    \ with dynamic batch size @philschmid, WDYT?"
  created_at: 2023-11-14 09:16:24+00:00
  edited: false
  hidden: false
  id: 65533ae80177de30b31e8251
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
      fullname: Philipp Schmid
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: philschmid
      type: user
    createdAt: '2023-11-14T09:19:26.000Z'
    data:
      edited: false
      editors:
      - philschmid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8675296306610107
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
          fullname: Philipp Schmid
          isHf: true
          isPro: false
          name: philschmid
          type: user
        html: "<p>Yes, the latest information we have is that its optimized for BS=1,\
          \ but <span data-props=\"{&quot;user&quot;:&quot;l-i&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/l-i\">@<span class=\"\
          underline\">l-i</span></a></span>\n\n\t</span></span> you should be able\
          \ to compile it using: <a href=\"https://huggingface.co/docs/optimum-neuron/guides/export_model#exporting-stable-diffusion-xl-to-neuron\"\
          >https://huggingface.co/docs/optimum-neuron/guides/export_model#exporting-stable-diffusion-xl-to-neuron</a>\
          \ and setting the batch size as you want. </p>\n"
        raw: "Yes, the latest information we have is that its optimized for BS=1,\
          \ but @l-i you should be able to compile it using: https://huggingface.co/docs/optimum-neuron/guides/export_model#exporting-stable-diffusion-xl-to-neuron\
          \ and setting the batch size as you want. \n"
        updatedAt: '2023-11-14T09:19:26.464Z'
      numEdits: 0
      reactions: []
    id: 65533b9e86270cc7f8dbb58b
    type: comment
  author: philschmid
  content: "Yes, the latest information we have is that its optimized for BS=1, but\
    \ @l-i you should be able to compile it using: https://huggingface.co/docs/optimum-neuron/guides/export_model#exporting-stable-diffusion-xl-to-neuron\
    \ and setting the batch size as you want. \n"
  created_at: 2023-11-14 09:19:26+00:00
  edited: false
  hidden: false
  id: 65533b9e86270cc7f8dbb58b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f78489d36951307fd0e379/LHaUSVsG2pqUovulFMXd8.png?w=200&h=200&f=face
      fullname: Wenchen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: l-i
      type: user
    createdAt: '2023-11-14T16:21:53.000Z'
    data:
      edited: false
      editors:
      - l-i
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9531289935112
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f78489d36951307fd0e379/LHaUSVsG2pqUovulFMXd8.png?w=200&h=200&f=face
          fullname: Wenchen
          isHf: false
          isPro: false
          name: l-i
          type: user
        html: '<p>thank you for sharing the details!</p>

          <p>I have two follow up questions:</p>

          <ul>

          <li>if I enable dynamic batch size during compilation but still inference
          at the same original static batch size, would it still affect the latency?</li>

          <li>if I compile with a larger static batch size, how much larger my machine
          needs to be? (I tried with 6 on 24xl, and it seemed to fail)</li>

          </ul>

          '
        raw: 'thank you for sharing the details!


          I have two follow up questions:

          - if I enable dynamic batch size during compilation but still inference
          at the same original static batch size, would it still affect the latency?

          - if I compile with a larger static batch size, how much larger my machine
          needs to be? (I tried with 6 on 24xl, and it seemed to fail)'
        updatedAt: '2023-11-14T16:21:53.981Z'
      numEdits: 0
      reactions: []
    id: 65539ea11f0870d221d298e2
    type: comment
  author: l-i
  content: 'thank you for sharing the details!


    I have two follow up questions:

    - if I enable dynamic batch size during compilation but still inference at the
    same original static batch size, would it still affect the latency?

    - if I compile with a larger static batch size, how much larger my machine needs
    to be? (I tried with 6 on 24xl, and it seemed to fail)'
  created_at: 2023-11-14 16:21:53+00:00
  edited: false
  hidden: false
  id: 65539ea11f0870d221d298e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61a50668cd3eb3043f38e445/E_M7m93Ou6d-XJij59VGF.jpeg?w=200&h=200&f=face
      fullname: Jingya Huang
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Jingya
      type: user
    createdAt: '2023-11-17T17:24:06.000Z'
    data:
      edited: false
      editors:
      - Jingya
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9372769594192505
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61a50668cd3eb3043f38e445/E_M7m93Ou6d-XJij59VGF.jpeg?w=200&h=200&f=face
          fullname: Jingya Huang
          isHf: true
          isPro: false
          name: Jingya
          type: user
        html: '<p>Just to point out if you are compiling for large batch size and
          even 24xlarge run oom, you could try with CPU-only instance just for the
          compilation. </p>

          '
        raw: 'Just to point out if you are compiling for large batch size and even
          24xlarge run oom, you could try with CPU-only instance just for the compilation. '
        updatedAt: '2023-11-17T17:24:06.395Z'
      numEdits: 0
      reactions: []
    id: 6557a1b6f89cf9c8df87fb2a
    type: comment
  author: Jingya
  content: 'Just to point out if you are compiling for large batch size and even 24xlarge
    run oom, you could try with CPU-only instance just for the compilation. '
  created_at: 2023-11-17 17:24:06+00:00
  edited: false
  hidden: false
  id: 6557a1b6f89cf9c8df87fb2a
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 3
repo_id: aws-neuron/stable-diffusion-xl-base-1-0-1024x1024
repo_type: model
status: draft
target_branch: refs/heads/main
title: allow dynamic batch size
