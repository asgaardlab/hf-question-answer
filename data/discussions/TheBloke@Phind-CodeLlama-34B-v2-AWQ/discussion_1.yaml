!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SebastianBodza
conflicting_files: null
created_at: 2023-10-10 08:04:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400c075f4ff62c2617023f7/DNrK0LTDuj9ZEpXl0_k_A.jpeg?w=200&h=200&f=face
      fullname: SebastianBoo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SebastianBodza
      type: user
    createdAt: '2023-10-10T09:04:13.000Z'
    data:
      edited: false
      editors:
      - SebastianBodza
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6704549789428711
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400c075f4ff62c2617023f7/DNrK0LTDuj9ZEpXl0_k_A.jpeg?w=200&h=200&f=face
          fullname: SebastianBoo
          isHf: false
          isPro: false
          name: SebastianBodza
          type: user
        html: "<p>Hey,<br>just tried out the model with vllm and I am getting OOM\
          \ errors.<br>Shouldn't the 34B model run on a single RTX 3090? I also tried\
          \ using two 3090s but I am still getting OOM with vllm 0.2.0</p>\n<p>My\
          \ inference code:</p>\n<pre><code>from vllm import LLM, SamplingParams\n\
          \nllm = LLM(\n    model=\"TheBloke/Phind-CodeLlama-34B-v2-AWQ\",\n    tensor_parallel_size=1,\n\
          \    dtype=\"half\",\n    quantization=\"awq\",\n    gpu_memory_utilization=0.01,\n\
          \    swap_space=30\n)\n</code></pre>\n<pre><code>python -m vllm.entrypoints.api_server\
          \ --model TheBloke/Phind-CodeLlama-34B-v2-AWQ  --quantization awq --dtype=float16\
          \ --tensor-parallel-size 2 --gpu-memory-utilization 0.1\n</code></pre>\n\
          <p>I also tried various other gpu-memory-utilization like 0.4, 0.5, 0.8,\
          \ ... </p>\n"
        raw: "Hey, \r\njust tried out the model with vllm and I am getting OOM errors.\
          \ \r\nShouldn't the 34B model run on a single RTX 3090? I also tried using\
          \ two 3090s but I am still getting OOM with vllm 0.2.0\r\n\r\nMy inference\
          \ code:\r\n```\r\nfrom vllm import LLM, SamplingParams\r\n\r\nllm = LLM(\r\
          \n    model=\"TheBloke/Phind-CodeLlama-34B-v2-AWQ\",\r\n    tensor_parallel_size=1,\r\
          \n    dtype=\"half\",\r\n    quantization=\"awq\",\r\n    gpu_memory_utilization=0.01,\r\
          \n    swap_space=30\r\n)\r\n```\r\n```\r\npython -m vllm.entrypoints.api_server\
          \ --model TheBloke/Phind-CodeLlama-34B-v2-AWQ  --quantization awq --dtype=float16\
          \ --tensor-parallel-size 2 --gpu-memory-utilization 0.1\r\n```\r\n\r\nI\
          \ also tried various other gpu-memory-utilization like 0.4, 0.5, 0.8, ... "
        updatedAt: '2023-10-10T09:04:13.882Z'
      numEdits: 0
      reactions: []
    id: 6525138d9aee934755777133
    type: comment
  author: SebastianBodza
  content: "Hey, \r\njust tried out the model with vllm and I am getting OOM errors.\
    \ \r\nShouldn't the 34B model run on a single RTX 3090? I also tried using two\
    \ 3090s but I am still getting OOM with vllm 0.2.0\r\n\r\nMy inference code:\r\
    \n```\r\nfrom vllm import LLM, SamplingParams\r\n\r\nllm = LLM(\r\n    model=\"\
    TheBloke/Phind-CodeLlama-34B-v2-AWQ\",\r\n    tensor_parallel_size=1,\r\n    dtype=\"\
    half\",\r\n    quantization=\"awq\",\r\n    gpu_memory_utilization=0.01,\r\n \
    \   swap_space=30\r\n)\r\n```\r\n```\r\npython -m vllm.entrypoints.api_server\
    \ --model TheBloke/Phind-CodeLlama-34B-v2-AWQ  --quantization awq --dtype=float16\
    \ --tensor-parallel-size 2 --gpu-memory-utilization 0.1\r\n```\r\n\r\nI also tried\
    \ various other gpu-memory-utilization like 0.4, 0.5, 0.8, ... "
  created_at: 2023-10-10 08:04:13+00:00
  edited: false
  hidden: false
  id: 6525138d9aee934755777133
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-10-10T09:07:58.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9115538597106934
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>You need at least 18GB of VRAM, single card with gpu-memory-utilization
          1 should be enought.</p>

          <p>vLLM''s AWQ does not support mult-ti gpu for the time being. try GPTQ
          version for dual-gpu inferencing instead</p>

          '
        raw: 'You need at least 18GB of VRAM, single card with gpu-memory-utilization
          1 should be enought.


          vLLM''s AWQ does not support mult-ti gpu for the time being. try GPTQ version
          for dual-gpu inferencing instead'
        updatedAt: '2023-10-10T09:07:58.336Z'
      numEdits: 0
      reactions: []
    id: 6525146ea6c9cbd7e92b9fc2
    type: comment
  author: Yhyu13
  content: 'You need at least 18GB of VRAM, single card with gpu-memory-utilization
    1 should be enought.


    vLLM''s AWQ does not support mult-ti gpu for the time being. try GPTQ version
    for dual-gpu inferencing instead'
  created_at: 2023-10-10 08:07:58+00:00
  edited: false
  hidden: false
  id: 6525146ea6c9cbd7e92b9fc2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400c075f4ff62c2617023f7/DNrK0LTDuj9ZEpXl0_k_A.jpeg?w=200&h=200&f=face
      fullname: SebastianBoo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SebastianBodza
      type: user
    createdAt: '2023-10-10T09:33:37.000Z'
    data:
      edited: false
      editors:
      - SebastianBodza
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8629128336906433
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400c075f4ff62c2617023f7/DNrK0LTDuj9ZEpXl0_k_A.jpeg?w=200&h=200&f=face
          fullname: SebastianBoo
          isHf: false
          isPro: false
          name: SebastianBodza
          type: user
        html: '<p>The 3090 has 24 GB of VRAM, so it should easily handle the model.
          Also vllm supports tensor-parallel execution with multi-GPUs. </p>

          <p>I found the problem. The default max-model-len (param)/ model_max_len
          (Python-API) seems to be too long. Setting it manually to 4000 works for
          a single 3090. For two RTX 3090 &gt;6000 is possible.</p>

          '
        raw: "The 3090 has 24 GB of VRAM, so it should easily handle the model. Also\
          \ vllm supports tensor-parallel execution with multi-GPUs. \n\nI found the\
          \ problem. The default max-model-len (param)/ model_max_len (Python-API)\
          \ seems to be too long. Setting it manually to 4000 works for a single 3090.\
          \ For two RTX 3090 >6000 is possible.\n"
        updatedAt: '2023-10-10T09:33:37.201Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65251a71b9f25427c937f770
    id: 65251a71b9f25427c937f76f
    type: comment
  author: SebastianBodza
  content: "The 3090 has 24 GB of VRAM, so it should easily handle the model. Also\
    \ vllm supports tensor-parallel execution with multi-GPUs. \n\nI found the problem.\
    \ The default max-model-len (param)/ model_max_len (Python-API) seems to be too\
    \ long. Setting it manually to 4000 works for a single 3090. For two RTX 3090\
    \ >6000 is possible.\n"
  created_at: 2023-10-10 08:33:37+00:00
  edited: false
  hidden: false
  id: 65251a71b9f25427c937f76f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400c075f4ff62c2617023f7/DNrK0LTDuj9ZEpXl0_k_A.jpeg?w=200&h=200&f=face
      fullname: SebastianBoo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SebastianBodza
      type: user
    createdAt: '2023-10-10T09:33:37.000Z'
    data:
      status: closed
    id: 65251a71b9f25427c937f770
    type: status-change
  author: SebastianBodza
  created_at: 2023-10-10 08:33:37+00:00
  id: 65251a71b9f25427c937f770
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Phind-CodeLlama-34B-v2-AWQ
repo_type: model
status: closed
target_branch: null
title: OOM on RTX 3090
