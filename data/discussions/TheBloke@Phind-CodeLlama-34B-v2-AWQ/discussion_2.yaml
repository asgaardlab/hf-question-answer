!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Pizzarino
conflicting_files: null
created_at: 2023-11-02 13:57:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6be072e81baff59b44b5d9e519079a33.svg
      fullname: Connor C
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pizzarino
      type: user
    createdAt: '2023-11-02T14:57:09.000Z'
    data:
      edited: false
      editors:
      - Pizzarino
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8304726481437683
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6be072e81baff59b44b5d9e519079a33.svg
          fullname: Connor C
          isHf: false
          isPro: false
          name: Pizzarino
          type: user
        html: '<p>Hey, I tried the vLLM example in the model card (just copied and
          pasted it) and I''m running into this error:</p>

          <p>ValueError: torch.bfloat16 is not supported for quantization method awq.
          Supported dtypes: [torch.float16]</p>

          <p>Is there a fix to be able to use the AWQ model with vLLM instead of AutoAWQ?</p>

          '
        raw: "Hey, I tried the vLLM example in the model card (just copied and pasted\
          \ it) and I'm running into this error:\r\n\r\nValueError: torch.bfloat16\
          \ is not supported for quantization method awq. Supported dtypes: [torch.float16]\r\
          \n\r\nIs there a fix to be able to use the AWQ model with vLLM instead of\
          \ AutoAWQ?"
        updatedAt: '2023-11-02T14:57:09.772Z'
      numEdits: 0
      reactions: []
    id: 6543b8c5c868220f1c93ac2e
    type: comment
  author: Pizzarino
  content: "Hey, I tried the vLLM example in the model card (just copied and pasted\
    \ it) and I'm running into this error:\r\n\r\nValueError: torch.bfloat16 is not\
    \ supported for quantization method awq. Supported dtypes: [torch.float16]\r\n\
    \r\nIs there a fix to be able to use the AWQ model with vLLM instead of AutoAWQ?"
  created_at: 2023-11-02 13:57:09+00:00
  edited: false
  hidden: false
  id: 6543b8c5c868220f1c93ac2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-02T15:04:31.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8758171796798706
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>What version of vLLM are you using? I had thought that the latest
          supported bfloat16 with AWQ.  2.0,  the first with AWQ support, definitely
          did not. But I thought it came later.</p>

          <p>Either way, you should specify <code>dtype="auto"</code> in either Python
          code or as a command line parameter.  That will load it in bfloat16 if it
          can, otherwise float16.</p>

          <p>This README hasn''t been updated in a while - my newer README template
          include the <code>dtype="auto"</code> parameter in the examples. </p>

          <p>All my AWQ READMEs are going to be updated later today anyway when I
          update for Transformers AWQ support, so that will get changed then.</p>

          '
        raw: "What version of vLLM are you using? I had thought that the latest supported\
          \ bfloat16 with AWQ.  2.0,  the first with AWQ support, definitely did not.\
          \ But I thought it came later.\n\nEither way, you should specify `dtype=\"\
          auto\"` in either Python code or as a command line parameter.  That will\
          \ load it in bfloat16 if it can, otherwise float16.\n\nThis README hasn't\
          \ been updated in a while - my newer README template include the `dtype=\"\
          auto\"` parameter in the examples. \n\nAll my AWQ READMEs are going to be\
          \ updated later today anyway when I update for Transformers AWQ support,\
          \ so that will get changed then."
        updatedAt: '2023-11-02T15:04:31.968Z'
      numEdits: 0
      reactions: []
    id: 6543ba7ff0d77ba425c3eda7
    type: comment
  author: TheBloke
  content: "What version of vLLM are you using? I had thought that the latest supported\
    \ bfloat16 with AWQ.  2.0,  the first with AWQ support, definitely did not. But\
    \ I thought it came later.\n\nEither way, you should specify `dtype=\"auto\"`\
    \ in either Python code or as a command line parameter.  That will load it in\
    \ bfloat16 if it can, otherwise float16.\n\nThis README hasn't been updated in\
    \ a while - my newer README template include the `dtype=\"auto\"` parameter in\
    \ the examples. \n\nAll my AWQ READMEs are going to be updated later today anyway\
    \ when I update for Transformers AWQ support, so that will get changed then."
  created_at: 2023-11-02 14:04:31+00:00
  edited: false
  hidden: false
  id: 6543ba7ff0d77ba425c3eda7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6be072e81baff59b44b5d9e519079a33.svg
      fullname: Connor C
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pizzarino
      type: user
    createdAt: '2023-11-03T11:50:24.000Z'
    data:
      edited: false
      editors:
      - Pizzarino
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9317770600318909
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6be072e81baff59b44b5d9e519079a33.svg
          fullname: Connor C
          isHf: false
          isPro: false
          name: Pizzarino
          type: user
        html: '<p>I''m using version 0.2.1.post1; I did a reinstall of it too just
          in case something got messed up during installation and the issue with bfloat16
          still persisted.</p>

          <p>I''ll definitely specify the dtype in my Python code! :)</p>

          <p>Thank you so much for your help, you''re a legend. &lt;3</p>

          '
        raw: 'I''m using version 0.2.1.post1; I did a reinstall of it too just in
          case something got messed up during installation and the issue with bfloat16
          still persisted.


          I''ll definitely specify the dtype in my Python code! :)


          Thank you so much for your help, you''re a legend. <3'
        updatedAt: '2023-11-03T11:50:24.094Z'
      numEdits: 0
      reactions: []
    id: 6544de8012da508864c29da6
    type: comment
  author: Pizzarino
  content: 'I''m using version 0.2.1.post1; I did a reinstall of it too just in case
    something got messed up during installation and the issue with bfloat16 still
    persisted.


    I''ll definitely specify the dtype in my Python code! :)


    Thank you so much for your help, you''re a legend. <3'
  created_at: 2023-11-03 10:50:24+00:00
  edited: false
  hidden: false
  id: 6544de8012da508864c29da6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c6952ef1e56b08567e385026812e7bf.svg
      fullname: Nelson Baker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ikaro79
      type: user
    createdAt: '2023-11-10T12:18:23.000Z'
    data:
      edited: false
      editors:
      - ikaro79
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6756068468093872
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c6952ef1e56b08567e385026812e7bf.svg
          fullname: Nelson Baker
          isHf: false
          isPro: false
          name: ikaro79
          type: user
        html: '<p>Hi, you can apply the following workaround, edit config.json and
          change<br>"torch_dtype": "bfloat16"  --&gt; "torch_dtype": "float16", </p>

          '
        raw: "Hi, you can apply the following workaround, edit config.json and change\
          \ \n\"torch_dtype\": \"bfloat16\"  --> \"torch_dtype\": \"float16\", "
        updatedAt: '2023-11-10T12:18:23.467Z'
      numEdits: 0
      reactions: []
    id: 654e1f8f8c40ad420ba862e7
    type: comment
  author: ikaro79
  content: "Hi, you can apply the following workaround, edit config.json and change\
    \ \n\"torch_dtype\": \"bfloat16\"  --> \"torch_dtype\": \"float16\", "
  created_at: 2023-11-10 12:18:23+00:00
  edited: false
  hidden: false
  id: 654e1f8f8c40ad420ba862e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-10T12:26:18.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7090078592300415
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah but it''s easier just to pass <code>--dtype auto</code> or
          <code>dtype="auto"</code></p>

          '
        raw: Yeah but it's easier just to pass `--dtype auto` or `dtype="auto"`
        updatedAt: '2023-11-10T12:26:18.689Z'
      numEdits: 0
      reactions: []
    id: 654e216a714135794e9e3154
    type: comment
  author: TheBloke
  content: Yeah but it's easier just to pass `--dtype auto` or `dtype="auto"`
  created_at: 2023-11-10 12:26:18+00:00
  edited: false
  hidden: false
  id: 654e216a714135794e9e3154
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ffb04fb77c943623c4f0fa6100fe0b1c.svg
      fullname: Roman Treutlein
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: romant319
      type: user
    createdAt: '2023-11-28T07:24:05.000Z'
    data:
      edited: false
      editors:
      - romant319
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8905940651893616
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ffb04fb77c943623c4f0fa6100fe0b1c.svg
          fullname: Roman Treutlein
          isHf: false
          isPro: false
          name: romant319
          type: user
        html: '<p>For me specifying auto didn''t work i still got the same error.
          But specifiying dtype="float16" did work.</p>

          '
        raw: For me specifying auto didn't work i still got the same error. But specifiying
          dtype="float16" did work.
        updatedAt: '2023-11-28T07:24:05.366Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - DuskNone
        - gaviego
    id: 6565959539002bd4273c4d5d
    type: comment
  author: romant319
  content: For me specifying auto didn't work i still got the same error. But specifiying
    dtype="float16" did work.
  created_at: 2023-11-28 07:24:05+00:00
  edited: false
  hidden: false
  id: 6565959539002bd4273c4d5d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Phind-CodeLlama-34B-v2-AWQ
repo_type: model
status: open
target_branch: null
title: torch.bfloat16 is not supported for quantization method awq
