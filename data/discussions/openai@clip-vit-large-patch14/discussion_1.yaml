!!python/object:huggingface_hub.community.DiscussionWithDetails
author: eugeneware
conflicting_files: null
created_at: 2022-06-21 11:26:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0bf404dab1d507b945334856610f8847.svg
      fullname: Eugene Ware
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eugeneware
      type: user
    createdAt: '2022-06-21T12:26:11.000Z'
    data:
      edited: false
      editors:
      - eugeneware
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0bf404dab1d507b945334856610f8847.svg
          fullname: Eugene Ware
          isHf: false
          isPro: false
          name: eugeneware
          type: user
        html: "<p>The normalized image embeddings generated by this huggingface version\
          \ of the CLIP model and the official openai implementation produce different\
          \ embeddings.</p>\n<p>I downloaded the following image: <a rel=\"nofollow\"\
          \ href=\"https://thumbs.dreamstime.com/b/lovely-cat-as-domestic-animal-view-pictures-182393057.jpg\"\
          >https://thumbs.dreamstime.com/b/lovely-cat-as-domestic-animal-view-pictures-182393057.jpg</a></p>\n\
          <p>I generated image embeddings using this model with the following code:</p>\n\
          <pre><code class=\"language-py\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> CLIPModel, CLIPProcessor\n\
          _model = CLIPModel.from_pretrained(<span class=\"hljs-string\">'openai/clip-vit-large-patch14'</span>)\n\
          _processor = CLIPProcessor.from_pretrained(<span class=\"hljs-string\">'openai/clip-vit-large-patch14'</span>)\n\
          img = Image.<span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\"\
          >'lovely-cat-as-domestic-animal-view-pictures-182393057.jpg'</span>).convert(<span\
          \ class=\"hljs-string\">'RGB'</span>)\ninputs = processor(images=img, return_tensors=<span\
          \ class=\"hljs-string\">'pt'</span>, padding=<span class=\"hljs-literal\"\
          >True</span>)\n<span class=\"hljs-keyword\">with</span> torch.no_grad():\n\
          \    vision_outputs = _model.vision_model(**inputs)\n    image_embeds =\
          \ vision_outputs[<span class=\"hljs-number\">1</span>]\n    image_embeds\
          \ = _model.visual_projection(image_embeds)\n    image_embeds = image_embeds\
          \ / image_embeds.norm(dim=-<span class=\"hljs-number\">1</span>, keepdim=<span\
          \ class=\"hljs-literal\">True</span>) \n<span class=\"hljs-built_in\">print</span>(image_embeds[<span\
          \ class=\"hljs-number\">0</span>, :<span class=\"hljs-number\">10</span>])\n\
          </code></pre>\n<p>I get:</p>\n<pre><code>tensor([-0.0262,  0.0541,  0.0122,\
          \  0.0053,  0.0453,  0.0138,  0.0141,  0.0035,\n         0.0202, -0.0173])\n\
          </code></pre>\n<p>When I use the official implementation with this code:</p>\n\
          <pre><code class=\"language-py\"><span class=\"hljs-keyword\">import</span>\
          \ clip\n__model, __preprocess = clip.load(<span class=\"hljs-string\">\"\
          ViT-L/14\"</span>, device=<span class=\"hljs-string\">'cpu'</span>)\n<span\
          \ class=\"hljs-keyword\">with</span> torch.no_grad():\n    __image_features\
          \ = __model.encode_image(__image)\n    __image_features /= __image_features.norm(dim=-<span\
          \ class=\"hljs-number\">1</span>, keepdim=<span class=\"hljs-literal\">True</span>)\n\
          <span class=\"hljs-built_in\">print</span>(__image_features[<span class=\"\
          hljs-number\">0</span>, :<span class=\"hljs-number\">10</span>])\n</code></pre>\n\
          <p>I get:</p>\n<pre><code>tensor([-0.0192,  0.0559,  0.0147,  0.0041,  0.0461,\
          \  0.0098,  0.0115,  0.0014,\n         0.0174, -0.0151])\n</code></pre>\n\
          <p>You can see the that values are similar, but are out by a bit.</p>\n\
          <p>If I calculate the cosine similarity / dot product I get:</p>\n<pre><code\
          \ class=\"language-py\">image_embeds @ image_features.t()\n<span class=\"\
          hljs-comment\"># tensor([[0.9971]])</span>\n</code></pre>\n<p>I get the\
          \ same result when I load up the official openai weights with the open_clip\
          \ implementation also.</p>\n<p>So, there's some subtle difference here.</p>\n\
          <p>I'm running transformers <code>4.20.0</code></p>\n"
        raw: "The normalized image embeddings generated by this huggingface version\
          \ of the CLIP model and the official openai implementation produce different\
          \ embeddings.\r\n\r\nI downloaded the following image: https://thumbs.dreamstime.com/b/lovely-cat-as-domestic-animal-view-pictures-182393057.jpg\r\
          \n\r\nI generated image embeddings using this model with the following code:\r\
          \n\r\n``` py\r\nfrom transformers import CLIPModel, CLIPProcessor\r\n_model\
          \ = CLIPModel.from_pretrained('openai/clip-vit-large-patch14')\r\n_processor\
          \ = CLIPProcessor.from_pretrained('openai/clip-vit-large-patch14')\r\nimg\
          \ = Image.open('lovely-cat-as-domestic-animal-view-pictures-182393057.jpg').convert('RGB')\r\
          \ninputs = processor(images=img, return_tensors='pt', padding=True)\r\n\
          with torch.no_grad():\r\n    vision_outputs = _model.vision_model(**inputs)\r\
          \n    image_embeds = vision_outputs[1]\r\n    image_embeds = _model.visual_projection(image_embeds)\r\
          \n    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\
          \ \r\nprint(image_embeds[0, :10])\r\n```\r\n\r\nI get:\r\n\r\n```\r\ntensor([-0.0262,\
          \  0.0541,  0.0122,  0.0053,  0.0453,  0.0138,  0.0141,  0.0035,\r\n   \
          \      0.0202, -0.0173])\r\n```\r\n\r\nWhen I use the official implementation\
          \ with this code:\r\n\r\n``` py\r\nimport clip\r\n__model, __preprocess\
          \ = clip.load(\"ViT-L/14\", device='cpu')\r\nwith torch.no_grad():\r\n \
          \   __image_features = __model.encode_image(__image)\r\n    __image_features\
          \ /= __image_features.norm(dim=-1, keepdim=True)\r\nprint(__image_features[0,\
          \ :10])\r\n```\r\n\r\nI get:\r\n\r\n```\r\ntensor([-0.0192,  0.0559,  0.0147,\
          \  0.0041,  0.0461,  0.0098,  0.0115,  0.0014,\r\n         0.0174, -0.0151])\r\
          \n```\r\n\r\nYou can see the that values are similar, but are out by a bit.\r\
          \n\r\nIf I calculate the cosine similarity / dot product I get:\r\n``` py\r\
          \nimage_embeds @ image_features.t()\r\n# tensor([[0.9971]])\r\n```\r\n\r\
          \nI get the same result when I load up the official openai weights with\
          \ the open_clip implementation also.\r\n\r\nSo, there's some subtle difference\
          \ here.\r\n\r\nI'm running transformers `4.20.0`"
        updatedAt: '2022-06-21T12:26:11.620Z'
      numEdits: 0
      reactions: []
    id: 62b1b8e3b9bc778fe4dcbad7
    type: comment
  author: eugeneware
  content: "The normalized image embeddings generated by this huggingface version\
    \ of the CLIP model and the official openai implementation produce different embeddings.\r\
    \n\r\nI downloaded the following image: https://thumbs.dreamstime.com/b/lovely-cat-as-domestic-animal-view-pictures-182393057.jpg\r\
    \n\r\nI generated image embeddings using this model with the following code:\r\
    \n\r\n``` py\r\nfrom transformers import CLIPModel, CLIPProcessor\r\n_model =\
    \ CLIPModel.from_pretrained('openai/clip-vit-large-patch14')\r\n_processor = CLIPProcessor.from_pretrained('openai/clip-vit-large-patch14')\r\
    \nimg = Image.open('lovely-cat-as-domestic-animal-view-pictures-182393057.jpg').convert('RGB')\r\
    \ninputs = processor(images=img, return_tensors='pt', padding=True)\r\nwith torch.no_grad():\r\
    \n    vision_outputs = _model.vision_model(**inputs)\r\n    image_embeds = vision_outputs[1]\r\
    \n    image_embeds = _model.visual_projection(image_embeds)\r\n    image_embeds\
    \ = image_embeds / image_embeds.norm(dim=-1, keepdim=True) \r\nprint(image_embeds[0,\
    \ :10])\r\n```\r\n\r\nI get:\r\n\r\n```\r\ntensor([-0.0262,  0.0541,  0.0122,\
    \  0.0053,  0.0453,  0.0138,  0.0141,  0.0035,\r\n         0.0202, -0.0173])\r\
    \n```\r\n\r\nWhen I use the official implementation with this code:\r\n\r\n```\
    \ py\r\nimport clip\r\n__model, __preprocess = clip.load(\"ViT-L/14\", device='cpu')\r\
    \nwith torch.no_grad():\r\n    __image_features = __model.encode_image(__image)\r\
    \n    __image_features /= __image_features.norm(dim=-1, keepdim=True)\r\nprint(__image_features[0,\
    \ :10])\r\n```\r\n\r\nI get:\r\n\r\n```\r\ntensor([-0.0192,  0.0559,  0.0147,\
    \  0.0041,  0.0461,  0.0098,  0.0115,  0.0014,\r\n         0.0174, -0.0151])\r\
    \n```\r\n\r\nYou can see the that values are similar, but are out by a bit.\r\n\
    \r\nIf I calculate the cosine similarity / dot product I get:\r\n``` py\r\nimage_embeds\
    \ @ image_features.t()\r\n# tensor([[0.9971]])\r\n```\r\n\r\nI get the same result\
    \ when I load up the official openai weights with the open_clip implementation\
    \ also.\r\n\r\nSo, there's some subtle difference here.\r\n\r\nI'm running transformers\
    \ `4.20.0`"
  created_at: 2022-06-21 11:26:11+00:00
  edited: false
  hidden: false
  id: 62b1b8e3b9bc778fe4dcbad7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0bf404dab1d507b945334856610f8847.svg
      fullname: Eugene Ware
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eugeneware
      type: user
    createdAt: '2022-06-21T12:34:04.000Z'
    data:
      edited: true
      editors:
      - eugeneware
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0bf404dab1d507b945334856610f8847.svg
          fullname: Eugene Ware
          isHf: false
          isPro: false
          name: eugeneware
          type: user
        html: "<p>Actually, I worked it out. The preprocessing is different from the\
          \ huggingface CLIPProcessor, and the default clip implementations. So the\
          \ model was getting a slightly different version of the image. </p>\n<p>From\
          \ what I can tell so far, due to different implementations for the center\
          \ cropping, it's changing pixels.</p>\n<p>TL;DR if you need exactly the\
          \ same input for a given image, then use the openai input processing pipeline\
          \ like this:</p>\n<pre><code class=\"language-py\"><span class=\"hljs-keyword\"\
          >from</span> torchvision.transforms <span class=\"hljs-keyword\">import</span>\
          \ Compose, Resize, CenterCrop, ToTensor, Normalize\n<span class=\"hljs-keyword\"\
          >from</span> PIL <span class=\"hljs-keyword\">import</span> Image\nimage_processor\
          \ = Compose([\n    Resize(size=<span class=\"hljs-number\">224</span>, interpolation=Image.BICUBIC),\n\
          \    CenterCrop(size=(<span class=\"hljs-number\">224</span>, <span class=\"\
          hljs-number\">224</span>)),\n    <span class=\"hljs-keyword\">lambda</span>\
          \ img: img.convert(<span class=\"hljs-string\">'RGB'</span>),\n    ToTensor(),\n\
          \    Normalize(mean=(<span class=\"hljs-number\">0.48145466</span>, <span\
          \ class=\"hljs-number\">0.4578275</span>, <span class=\"hljs-number\">0.40821073</span>),\
          \ std=(<span class=\"hljs-number\">0.26862954</span>, <span class=\"hljs-number\"\
          >0.26130258</span>, <span class=\"hljs-number\">0.27577711</span>))\n])\n\
          inputs=<span class=\"hljs-built_in\">dict</span>(pixel_values=image_processor(img).unsqueeze(<span\
          \ class=\"hljs-number\">0</span>))\n<span class=\"hljs-keyword\">with</span>\
          \ torch.no_grad():\n    vision_outputs = _model.vision_model(**inputs)\n\
          \    image_embeds = vision_outputs[<span class=\"hljs-number\">1</span>]\n\
          \    image_embeds = _model.visual_projection(image_embeds)\n    image_embeds\
          \ = image_embeds / image_embeds.norm(dim=-<span class=\"hljs-number\">1</span>,\
          \ keepdim=<span class=\"hljs-literal\">True</span>)\n<span class=\"hljs-built_in\"\
          >print</span>(image_embeds[<span class=\"hljs-number\">0</span>, :<span\
          \ class=\"hljs-number\">10</span>])\n</code></pre>\n<pre><code>tensor([-0.0192,\
          \  0.0559,  0.0147,  0.0041,  0.0461,  0.0098,  0.0115,  0.0014,\n     \
          \    0.0174, -0.0151])\n</code></pre>\n"
        raw: "Actually, I worked it out. The preprocessing is different from the huggingface\
          \ CLIPProcessor, and the default clip implementations. So the model was\
          \ getting a slightly different version of the image. \n\nFrom what I can\
          \ tell so far, due to different implementations for the center cropping,\
          \ it's changing pixels.\n\nTL;DR if you need exactly the same input for\
          \ a given image, then use the openai input processing pipeline like this:\n\
          \n``` py\nfrom torchvision.transforms import Compose, Resize, CenterCrop,\
          \ ToTensor, Normalize\nfrom PIL import Image\nimage_processor = Compose([\n\
          \    Resize(size=224, interpolation=Image.BICUBIC),\n    CenterCrop(size=(224,\
          \ 224)),\n    lambda img: img.convert('RGB'),\n    ToTensor(),\n    Normalize(mean=(0.48145466,\
          \ 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n])\n\
          inputs=dict(pixel_values=image_processor(img).unsqueeze(0))\nwith torch.no_grad():\n\
          \    vision_outputs = _model.vision_model(**inputs)\n    image_embeds =\
          \ vision_outputs[1]\n    image_embeds = _model.visual_projection(image_embeds)\n\
          \    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n\
          print(image_embeds[0, :10])\n```\n\n```\ntensor([-0.0192,  0.0559,  0.0147,\
          \  0.0041,  0.0461,  0.0098,  0.0115,  0.0014,\n         0.0174, -0.0151])\n\
          ```"
        updatedAt: '2022-06-21T13:17:57.782Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Champagne
        - abdalrahmanshahrour
    id: 62b1babc63c396bbf025ca8c
    type: comment
  author: eugeneware
  content: "Actually, I worked it out. The preprocessing is different from the huggingface\
    \ CLIPProcessor, and the default clip implementations. So the model was getting\
    \ a slightly different version of the image. \n\nFrom what I can tell so far,\
    \ due to different implementations for the center cropping, it's changing pixels.\n\
    \nTL;DR if you need exactly the same input for a given image, then use the openai\
    \ input processing pipeline like this:\n\n``` py\nfrom torchvision.transforms\
    \ import Compose, Resize, CenterCrop, ToTensor, Normalize\nfrom PIL import Image\n\
    image_processor = Compose([\n    Resize(size=224, interpolation=Image.BICUBIC),\n\
    \    CenterCrop(size=(224, 224)),\n    lambda img: img.convert('RGB'),\n    ToTensor(),\n\
    \    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258,\
    \ 0.27577711))\n])\ninputs=dict(pixel_values=image_processor(img).unsqueeze(0))\n\
    with torch.no_grad():\n    vision_outputs = _model.vision_model(**inputs)\n  \
    \  image_embeds = vision_outputs[1]\n    image_embeds = _model.visual_projection(image_embeds)\n\
    \    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\nprint(image_embeds[0,\
    \ :10])\n```\n\n```\ntensor([-0.0192,  0.0559,  0.0147,  0.0041,  0.0461,  0.0098,\
    \  0.0115,  0.0014,\n         0.0174, -0.0151])\n```"
  created_at: 2022-06-21 11:34:04+00:00
  edited: true
  hidden: false
  id: 62b1babc63c396bbf025ca8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2022-06-22T12:27:27.000Z'
    data:
      edited: false
      editors:
      - julien-c
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;valhalla&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/valhalla\"\
          >@<span class=\"underline\">valhalla</span></a></span>\n\n\t</span></span>\
          \ in case you hadn't seen this!</p>\n"
        raw: cc @valhalla in case you hadn't seen this!
        updatedAt: '2022-06-22T12:27:27.462Z'
      numEdits: 0
      reactions: []
    id: 62b30aaf29a410b7f6aca2a0
    type: comment
  author: julien-c
  content: cc @valhalla in case you hadn't seen this!
  created_at: 2022-06-22 11:27:27+00:00
  edited: false
  hidden: false
  id: 62b30aaf29a410b7f6aca2a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c064a9e39c31a4b996e9cf1b70e7e21.svg
      fullname: Steve
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FusionLi
      type: user
    createdAt: '2022-09-16T12:35:55.000Z'
    data:
      edited: false
      editors:
      - FusionLi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c064a9e39c31a4b996e9cf1b70e7e21.svg
          fullname: Steve
          isHf: false
          isPro: false
          name: FusionLi
          type: user
        html: '<p>I found the text embedding differs quite a lot. Does this make sense?</p>

          '
        raw: I found the text embedding differs quite a lot. Does this make sense?
        updatedAt: '2022-09-16T12:35:55.936Z'
      numEdits: 0
      reactions: []
    id: 63246dab3c2e0515b3620df9
    type: comment
  author: FusionLi
  content: I found the text embedding differs quite a lot. Does this make sense?
  created_at: 2022-09-16 11:35:55+00:00
  edited: false
  hidden: false
  id: 63246dab3c2e0515b3620df9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: openai/clip-vit-large-patch14
repo_type: model
status: open
target_branch: null
title: Image embeddings are different from the official OpenAI clip model
