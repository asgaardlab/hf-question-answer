!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HR1777
conflicting_files: null
created_at: 2024-01-19 13:21:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
      fullname: H R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HR1777
      type: user
    createdAt: '2024-01-19T13:21:42.000Z'
    data:
      edited: false
      editors:
      - HR1777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9182139039039612
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
          fullname: H R
          isHf: false
          isPro: false
          name: HR1777
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mlabonne&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mlabonne\">@<span class=\"\
          underline\">mlabonne</span></a></span>\n\n\t</span></span>, the issue i\
          \ am mentioning now is global with every single model i have used so far\
          \ (even 128K models) including NeuralBeagle14-7B. As you know this context\
          \ length of NeuralBeagle14-7B is supposed to be 8K. But when i want to do\
          \ some operations on my desired texts, for example with 4 pages of text\
          \ which is less than 4096 tokens, i have to split to text into several parts\
          \ to get the desired output.<br>For instance if i want to ask to model to\
          \ extend the input text, if i give the 4 pages on text at once to the model,\
          \ it gives me 2 pages as output, but if i split that 4 pages to 8 parts,\
          \ the output will become 6 pages when i combine them together.<br>I am using\
          \ Oobabooga as interface but i have the same issue with Koboldcpp too even\
          \ though i change the settings like alpha_value, n_batch and other parameters\
          \ to increase the capability of model to process longer inputs.<br>Do you\
          \ know happen to know any solution for this issue that enables me to get\
          \ almost the same results without having to split the input into multiple\
          \ parts manually?</p>\n"
        raw: "@mlabonne, the issue i am mentioning now is global with every single\
          \ model i have used so far (even 128K models) including NeuralBeagle14-7B.\
          \ As you know this context length of NeuralBeagle14-7B is supposed to be\
          \ 8K. But when i want to do some operations on my desired texts, for example\
          \ with 4 pages of text which is less than 4096 tokens, i have to split to\
          \ text into several parts to get the desired output. \r\nFor instance if\
          \ i want to ask to model to extend the input text, if i give the 4 pages\
          \ on text at once to the model, it gives me 2 pages as output, but if i\
          \ split that 4 pages to 8 parts, the output will become 6 pages when i combine\
          \ them together.\r\nI am using Oobabooga as interface but i have the same\
          \ issue with Koboldcpp too even though i change the settings like alpha_value,\
          \ n_batch and other parameters to increase the capability of model to process\
          \ longer inputs. \r\nDo you know happen to know any solution for this issue\
          \ that enables me to get almost the same results without having to split\
          \ the input into multiple parts manually?"
        updatedAt: '2024-01-19T13:21:42.074Z'
      numEdits: 0
      reactions: []
    id: 65aa7766819fbfaf4950dff9
    type: comment
  author: HR1777
  content: "@mlabonne, the issue i am mentioning now is global with every single model\
    \ i have used so far (even 128K models) including NeuralBeagle14-7B. As you know\
    \ this context length of NeuralBeagle14-7B is supposed to be 8K. But when i want\
    \ to do some operations on my desired texts, for example with 4 pages of text\
    \ which is less than 4096 tokens, i have to split to text into several parts to\
    \ get the desired output. \r\nFor instance if i want to ask to model to extend\
    \ the input text, if i give the 4 pages on text at once to the model, it gives\
    \ me 2 pages as output, but if i split that 4 pages to 8 parts, the output will\
    \ become 6 pages when i combine them together.\r\nI am using Oobabooga as interface\
    \ but i have the same issue with Koboldcpp too even though i change the settings\
    \ like alpha_value, n_batch and other parameters to increase the capability of\
    \ model to process longer inputs. \r\nDo you know happen to know any solution\
    \ for this issue that enables me to get almost the same results without having\
    \ to split the input into multiple parts manually?"
  created_at: 2024-01-19 13:21:42+00:00
  edited: false
  hidden: false
  id: 65aa7766819fbfaf4950dff9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2024-01-19T15:40:16.000Z'
    data:
      edited: false
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9571461081504822
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: "<p>Sorry <span data-props=\"{&quot;user&quot;:&quot;HR1777&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/HR1777\"\
          >@<span class=\"underline\">HR1777</span></a></span>\n\n\t</span></span>\
          \ I haven't experimented with this long context retrieval task. I'm surprised\
          \ that even 128k models can't do it. Have you tried Mixtral or 200k models?</p>\n"
        raw: Sorry @HR1777 I haven't experimented with this long context retrieval
          task. I'm surprised that even 128k models can't do it. Have you tried Mixtral
          or 200k models?
        updatedAt: '2024-01-19T15:40:16.820Z'
      numEdits: 0
      reactions: []
    id: 65aa97e068139e3c42826973
    type: comment
  author: mlabonne
  content: Sorry @HR1777 I haven't experimented with this long context retrieval task.
    I'm surprised that even 128k models can't do it. Have you tried Mixtral or 200k
    models?
  created_at: 2024-01-19 15:40:16+00:00
  edited: false
  hidden: false
  id: 65aa97e068139e3c42826973
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
      fullname: H R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HR1777
      type: user
    createdAt: '2024-01-19T15:48:36.000Z'
    data:
      edited: false
      editors:
      - HR1777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9943934082984924
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
          fullname: H R
          isHf: false
          isPro: false
          name: HR1777
          type: user
        html: '<p>I didn''t try Mixtral but i have tested some 200k models, as well
          as 64K or 128K models in addition to more than 40 32k models that i tried
          and all of them failed. I am sure Mixtral will fail too. I don''t know what
          the solution is. I hoped you could help me.</p>

          '
        raw: I didn't try Mixtral but i have tested some 200k models, as well as 64K
          or 128K models in addition to more than 40 32k models that i tried and all
          of them failed. I am sure Mixtral will fail too. I don't know what the solution
          is. I hoped you could help me.
        updatedAt: '2024-01-19T15:48:36.078Z'
      numEdits: 0
      reactions: []
    id: 65aa99d468139e3c428332a1
    type: comment
  author: HR1777
  content: I didn't try Mixtral but i have tested some 200k models, as well as 64K
    or 128K models in addition to more than 40 32k models that i tried and all of
    them failed. I am sure Mixtral will fail too. I don't know what the solution is.
    I hoped you could help me.
  created_at: 2024-01-19 15:48:36+00:00
  edited: false
  hidden: false
  id: 65aa99d468139e3c428332a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8728906f66ef319875cb48b6cd5c771.svg
      fullname: Jim Lloyd
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jimlloyd
      type: user
    createdAt: '2024-01-23T04:16:02.000Z'
    data:
      edited: false
      editors:
      - jimlloyd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9619777202606201
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8728906f66ef319875cb48b6cd5c771.svg
          fullname: Jim Lloyd
          isHf: false
          isPro: false
          name: jimlloyd
          type: user
        html: "<p>I have been using this model with context length of 25'600 by setting\
          \ the rope_base to 32'000. I obtained these settings through trial and error\
          \ after talking with others about how they configure RoPE, most of whom\
          \ only attempt to double the context size. I found that to get 16K I could\
          \ set the rope base to 50'000 and get excellent results. The original 8K\
          \ is with rope base at 100'000. Making only an educated guess I reasoned\
          \ that the configurations that might work will all have ctx-size*rope-base\
          \ = 819'200'000. I tried for ctx-size of 32K using rope-base at 25600 but\
          \ the perplexity was awful. The settings for ctx-size of 25'600 gave perpexity\
          \ values of about 14.0, which is quite a bit higher than the ~6.5 with the\
          \ default 8K context, but with actual testing the model seems to perform\
          \ well.</p>\n<p>I am doing this testing using MemGPT and long conversations\
          \ in which the context is allowed to grow to near the limit and then old\
          \ conversation is purged. I have had conversations that were long enough\
          \ to fill &amp; then purge the full 25'600 context window two times without\
          \ any evidence of poor performance when the context is nearly full.</p>\n\
          <p>This is the best 7B model I have found for use with MemGPT. Thank you\
          \ <span data-props=\"{&quot;user&quot;:&quot;mlabonne&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mlabonne\">@<span class=\"\
          underline\">mlabonne</span></a></span>\n\n\t</span></span> for your excellent\
          \ work!</p>\n"
        raw: 'I have been using this model with context length of 25''600 by setting
          the rope_base to 32''000. I obtained these settings through trial and error
          after talking with others about how they configure RoPE, most of whom only
          attempt to double the context size. I found that to get 16K I could set
          the rope base to 50''000 and get excellent results. The original 8K is with
          rope base at 100''000. Making only an educated guess I reasoned that the
          configurations that might work will all have ctx-size*rope-base = 819''200''000.
          I tried for ctx-size of 32K using rope-base at 25600 but the perplexity
          was awful. The settings for ctx-size of 25''600 gave perpexity values of
          about 14.0, which is quite a bit higher than the ~6.5 with the default 8K
          context, but with actual testing the model seems to perform well.


          I am doing this testing using MemGPT and long conversations in which the
          context is allowed to grow to near the limit and then old conversation is
          purged. I have had conversations that were long enough to fill & then purge
          the full 25''600 context window two times without any evidence of poor performance
          when the context is nearly full.


          This is the best 7B model I have found for use with MemGPT. Thank you @mlabonne
          for your excellent work!

          '
        updatedAt: '2024-01-23T04:16:02.314Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mlabonne
    id: 65af3d8266ad44e2d5ba9d99
    type: comment
  author: jimlloyd
  content: 'I have been using this model with context length of 25''600 by setting
    the rope_base to 32''000. I obtained these settings through trial and error after
    talking with others about how they configure RoPE, most of whom only attempt to
    double the context size. I found that to get 16K I could set the rope base to
    50''000 and get excellent results. The original 8K is with rope base at 100''000.
    Making only an educated guess I reasoned that the configurations that might work
    will all have ctx-size*rope-base = 819''200''000. I tried for ctx-size of 32K
    using rope-base at 25600 but the perplexity was awful. The settings for ctx-size
    of 25''600 gave perpexity values of about 14.0, which is quite a bit higher than
    the ~6.5 with the default 8K context, but with actual testing the model seems
    to perform well.


    I am doing this testing using MemGPT and long conversations in which the context
    is allowed to grow to near the limit and then old conversation is purged. I have
    had conversations that were long enough to fill & then purge the full 25''600
    context window two times without any evidence of poor performance when the context
    is nearly full.


    This is the best 7B model I have found for use with MemGPT. Thank you @mlabonne
    for your excellent work!

    '
  created_at: 2024-01-23 04:16:02+00:00
  edited: false
  hidden: false
  id: 65af3d8266ad44e2d5ba9d99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
      fullname: H R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HR1777
      type: user
    createdAt: '2024-01-23T08:03:34.000Z'
    data:
      edited: true
      editors:
      - HR1777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9737486243247986
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
          fullname: H R
          isHf: false
          isPro: false
          name: HR1777
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jimlloyd&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jimlloyd\">@<span class=\"\
          underline\">jimlloyd</span></a></span>\n\n\t</span></span>, thank you for\
          \ sharing your experience, i just would like to know do you happen to know\
          \ the equivalent of  rope-base in webui Oobabooga? is it the same as rope_freq_base?</p>\n"
        raw: '@jimlloyd, thank you for sharing your experience, i just would like
          to know do you happen to know the equivalent of  rope-base in webui Oobabooga?
          is it the same as rope_freq_base?'
        updatedAt: '2024-01-23T08:03:44.694Z'
      numEdits: 1
      reactions: []
    id: 65af72d63e876a6389957047
    type: comment
  author: HR1777
  content: '@jimlloyd, thank you for sharing your experience, i just would like to
    know do you happen to know the equivalent of  rope-base in webui Oobabooga? is
    it the same as rope_freq_base?'
  created_at: 2024-01-23 08:03:34+00:00
  edited: true
  hidden: false
  id: 65af72d63e876a6389957047
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8728906f66ef319875cb48b6cd5c771.svg
      fullname: Jim Lloyd
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jimlloyd
      type: user
    createdAt: '2024-01-23T22:42:09.000Z'
    data:
      edited: false
      editors:
      - jimlloyd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8061636686325073
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8728906f66ef319875cb48b6cd5c771.svg
          fullname: Jim Lloyd
          isHf: false
          isPro: false
          name: jimlloyd
          type: user
        html: "<p>Sorry <span data-props=\"{&quot;user&quot;:&quot;HR1777&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/HR1777\"\
          >@<span class=\"underline\">HR1777</span></a></span>\n\n\t</span></span>,\
          \ I was sloppy to not use the exact parameter names. In <code>llama.cpp</code>\
          \ the exact parameter name is <code>--rope-freq-base</code>. I don't use\
          \ oobabooga webui but I would expect that <code>rope_freq_base</code> is\
          \ the same thing.</p>\n"
        raw: 'Sorry @HR1777, I was sloppy to not use the exact parameter names. In
          `llama.cpp` the exact parameter name is `--rope-freq-base`. I don''t use
          oobabooga webui but I would expect that `rope_freq_base` is the same thing.

          '
        updatedAt: '2024-01-23T22:42:09.316Z'
      numEdits: 0
      reactions: []
    id: 65b040c16eaa79e358e6f9a2
    type: comment
  author: jimlloyd
  content: 'Sorry @HR1777, I was sloppy to not use the exact parameter names. In `llama.cpp`
    the exact parameter name is `--rope-freq-base`. I don''t use oobabooga webui but
    I would expect that `rope_freq_base` is the same thing.

    '
  created_at: 2024-01-23 22:42:09+00:00
  edited: false
  hidden: false
  id: 65b040c16eaa79e358e6f9a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
      fullname: H R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HR1777
      type: user
    createdAt: '2024-01-24T07:31:00.000Z'
    data:
      edited: true
      editors:
      - HR1777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9722893834114075
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2707764ac65c625b420c698440ca226c.svg
          fullname: H R
          isHf: false
          isPro: false
          name: HR1777
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jimlloyd&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jimlloyd\">@<span class=\"\
          underline\">jimlloyd</span></a></span>\n\n\t</span></span>, thank you so\
          \ much for your information. I tested your method and it seems much better.\
          \ If its possible i would like to ask you a favor.<br> Is it possible for\
          \ you to put a full article with more than 1500 words as input and ask the\
          \ model to expand and extend it. then adjust the settings to get output\
          \ more than 1500 words without forgetting the details on the input.<br>\
          \ I have tried it so many times with different models, but every time the\
          \ output is much less than 1000 words. I would like to get at least output\
          \ length equal ( or greater) than the input, but i failed every time. Thank\
          \ you so much for your help.</p>\n"
        raw: "@jimlloyd, thank you so much for your information. I tested your method\
          \ and it seems much better. If its possible i would like to ask you a favor.\n\
          \ Is it possible for you to put a full article with more than 1500 words\
          \ as input and ask the model to expand and extend it. then adjust the settings\
          \ to get output more than 1500 words without forgetting the details on the\
          \ input.\n I have tried it so many times with different models, but every\
          \ time the output is much less than 1000 words. I would like to get at least\
          \ output length equal ( or greater) than the input, but i failed every time.\
          \ Thank you so much for your help."
        updatedAt: '2024-01-24T07:31:26.014Z'
      numEdits: 1
      reactions: []
    id: 65b0bcb43d8a48439cd43d73
    type: comment
  author: HR1777
  content: "@jimlloyd, thank you so much for your information. I tested your method\
    \ and it seems much better. If its possible i would like to ask you a favor.\n\
    \ Is it possible for you to put a full article with more than 1500 words as input\
    \ and ask the model to expand and extend it. then adjust the settings to get output\
    \ more than 1500 words without forgetting the details on the input.\n I have tried\
    \ it so many times with different models, but every time the output is much less\
    \ than 1000 words. I would like to get at least output length equal ( or greater)\
    \ than the input, but i failed every time. Thank you so much for your help."
  created_at: 2024-01-24 07:31:00+00:00
  edited: true
  hidden: false
  id: 65b0bcb43d8a48439cd43d73
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: mlabonne/NeuralBeagle14-7B-GGUF
repo_type: model
status: open
target_branch: null
title: Input context length issue
