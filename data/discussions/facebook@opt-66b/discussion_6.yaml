!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BobaZooba
conflicting_files: null
created_at: 2022-08-04 07:47:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6074d5f1134c000d1ae10d42/Sq7FKUY8kTwQvPf5VbR1w.jpeg?w=200&h=200&f=face
      fullname: Boris Zubarev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BobaZooba
      type: user
    createdAt: '2022-08-04T08:47:56.000Z'
    data:
      edited: false
      editors:
      - BobaZooba
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6074d5f1134c000d1ae10d42/Sq7FKUY8kTwQvPf5VbR1w.jpeg?w=200&h=200&f=face
          fullname: Boris Zubarev
          isHf: false
          isPro: false
          name: BobaZooba
          type: user
        html: '<p>I would really like to use this model for inference, but I don''t
          have a large enough GPU. T5 model has support for parallelization to multiple
          GPUs using: model.parallelize(). OPT doesn''t have this feature. Please
          add this feature or tell me how I can just use your model on multiple GPUs
          without pain</p>

          '
        raw: 'I would really like to use this model for inference, but I don''t have
          a large enough GPU. T5 model has support for parallelization to multiple
          GPUs using: model.parallelize(). OPT doesn''t have this feature. Please
          add this feature or tell me how I can just use your model on multiple GPUs
          without pain'
        updatedAt: '2022-08-04T08:47:56.625Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - xhluca
        - YYChen
        - amrsharaf
        - Marzzel
    id: 62eb87bceae3e61b88b4d0b4
    type: comment
  author: BobaZooba
  content: 'I would really like to use this model for inference, but I don''t have
    a large enough GPU. T5 model has support for parallelization to multiple GPUs
    using: model.parallelize(). OPT doesn''t have this feature. Please add this feature
    or tell me how I can just use your model on multiple GPUs without pain'
  created_at: 2022-08-04 07:47:56+00:00
  edited: false
  hidden: false
  id: 62eb87bceae3e61b88b4d0b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2022-08-04T09:15:35.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;BobaZooba&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/BobaZooba\"\
          >@<span class=\"underline\">BobaZooba</span></a></span>\n\n\t</span></span>,\
          \ the parallelize method is now deprecated in favor of using accelerate\
          \ instead. We have a guide for this <a href=\"https://huggingface.co/docs/transformers/v4.21.0/en/main_classes/model#large-model-loading\"\
          >here</a>, that we should feature more prominently in the docs; there is\
          \ no link to it in the \"Performance and scalability\" section, where it\
          \ should likely be.</p>\n<p>cc <span data-props=\"{&quot;user&quot;:&quot;sgugger&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sgugger\"\
          >@<span class=\"underline\">sgugger</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;stevhliu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/stevhliu\">@<span class=\"\
          underline\">stevhliu</span></a></span>\n\n\t</span></span></p>\n"
        raw: 'Hey @BobaZooba, the parallelize method is now deprecated in favor of
          using accelerate instead. We have a guide for this [here](https://huggingface.co/docs/transformers/v4.21.0/en/main_classes/model#large-model-loading),
          that we should feature more prominently in the docs; there is no link to
          it in the "Performance and scalability" section, where it should likely
          be.


          cc @sgugger @stevhliu'
        updatedAt: '2022-08-04T09:15:35.426Z'
      numEdits: 0
      reactions: []
    id: 62eb8e373682a52abafee884
    type: comment
  author: lysandre
  content: 'Hey @BobaZooba, the parallelize method is now deprecated in favor of using
    accelerate instead. We have a guide for this [here](https://huggingface.co/docs/transformers/v4.21.0/en/main_classes/model#large-model-loading),
    that we should feature more prominently in the docs; there is no link to it in
    the "Performance and scalability" section, where it should likely be.


    cc @sgugger @stevhliu'
  created_at: 2022-08-04 08:15:35+00:00
  edited: false
  hidden: false
  id: 62eb8e373682a52abafee884
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6074d5f1134c000d1ae10d42/Sq7FKUY8kTwQvPf5VbR1w.jpeg?w=200&h=200&f=face
      fullname: Boris Zubarev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BobaZooba
      type: user
    createdAt: '2022-08-05T11:51:50.000Z'
    data:
      edited: true
      editors:
      - BobaZooba
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6074d5f1134c000d1ae10d42/Sq7FKUY8kTwQvPf5VbR1w.jpeg?w=200&h=200&f=face
          fullname: Boris Zubarev
          isHf: false
          isPro: false
          name: BobaZooba
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;lysandre&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lysandre\">@<span class=\"\
          underline\">lysandre</span></a></span>\n\n\t</span></span> Thank you!<br>Below\
          \ is a small guide on how to run the model and what problems I encountered<br>My\
          \ setup: 8 x RTX3090</p>\n<pre><code>torch.version.cuda = 11.3\n</code></pre>\n\
          <p>I have this exception when I try to generate small text:</p>\n<pre><code>&gt;\
          \ generated_ids = model.generate(input_ids, do_sample=True, max_length=32)\n\
          &gt; RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling\
          \ `cublasCreate(handle)`\n</code></pre>\n<p>Referring to this answer, the\
          \ problem is that there is not enough video memory, although there should\
          \ be enough:<br><a rel=\"nofollow\" href=\"https://discuss.pytorch.org/t/cuda-error-cublas-status-not-initialized-when-calling-cublascreate-handle/125450/2\"\
          >https://discuss.pytorch.org/t/cuda-error-cublas-status-not-initialized-when-calling-cublascreate-handle/125450/2</a></p>\n\
          <p>Model init:</p>\n<pre><code>model = AutoModelForCausalLM.from_pretrained(\"\
          facebook/opt-66b\", torch_dtype=torch.float16, device_map=\"auto\")\n</code></pre>\n\
          <p>How Accelerate maps my model:</p>\n<pre><code>&gt; model.hf_device_map\n\
          {'model.decoder.embed_tokens': 0,\n 'lm_head': 0,\n 'model.decoder.embed_positions':\
          \ 0,\n 'model.decoder.final_layer_norm': 0,\n 'model.decoder.layers.0':\
          \ 0,\n 'model.decoder.layers.1': 0,\n 'model.decoder.layers.2': 0,\n 'model.decoder.layers.3':\
          \ 0,\n 'model.decoder.layers.4': 0,\n 'model.decoder.layers.5': 0,\n 'model.decoder.layers.6':\
          \ 0,\n 'model.decoder.layers.7': 0,\n 'model.decoder.layers.8': 0,\n 'model.decoder.layers.9':\
          \ 0,\n 'model.decoder.layers.10': 1,\n 'model.decoder.layers.11': 1,\n 'model.decoder.layers.12':\
          \ 1,\n 'model.decoder.layers.13': 1,\n 'model.decoder.layers.14': 1,\n 'model.decoder.layers.15':\
          \ 1,\n 'model.decoder.layers.16': 1,\n 'model.decoder.layers.17': 1,\n 'model.decoder.layers.18':\
          \ 1,\n 'model.decoder.layers.19': 1,\n 'model.decoder.layers.20': 1,\n 'model.decoder.layers.21':\
          \ 1,\n 'model.decoder.layers.22': 2,\n 'model.decoder.layers.23': 2,\n 'model.decoder.layers.24':\
          \ 2,\n 'model.decoder.layers.25': 2,\n 'model.decoder.layers.26': 2,\n 'model.decoder.layers.27':\
          \ 2,\n 'model.decoder.layers.28': 2,\n 'model.decoder.layers.29': 2,\n 'model.decoder.layers.30':\
          \ 2,\n 'model.decoder.layers.31': 2,\n 'model.decoder.layers.32': 2,\n 'model.decoder.layers.33':\
          \ 2,\n 'model.decoder.layers.34': 3,\n 'model.decoder.layers.35': 3,\n 'model.decoder.layers.36':\
          \ 3,\n 'model.decoder.layers.37': 3,\n 'model.decoder.layers.38': 3,\n 'model.decoder.layers.39':\
          \ 3,\n 'model.decoder.layers.40': 3,\n 'model.decoder.layers.41': 3,\n 'model.decoder.layers.42':\
          \ 3,\n 'model.decoder.layers.43': 3,\n 'model.decoder.layers.44': 3,\n 'model.decoder.layers.45':\
          \ 3,\n 'model.decoder.layers.46': 4,\n 'model.decoder.layers.47': 4,\n 'model.decoder.layers.48':\
          \ 4,\n 'model.decoder.layers.49': 4,\n 'model.decoder.layers.50': 4,\n 'model.decoder.layers.51':\
          \ 4,\n 'model.decoder.layers.52': 4,\n 'model.decoder.layers.53': 4,\n 'model.decoder.layers.54':\
          \ 4,\n 'model.decoder.layers.55': 4,\n 'model.decoder.layers.56': 4,\n 'model.decoder.layers.57':\
          \ 4,\n 'model.decoder.layers.58': 5,\n 'model.decoder.layers.59': 5,\n 'model.decoder.layers.60':\
          \ 5,\n 'model.decoder.layers.61': 5,\n 'model.decoder.layers.62': 5,\n 'model.decoder.layers.63':\
          \ 5}\n\nFri Aug  5 11:25:39 2022       \n+-----------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 510.60.02    Driver Version: 510.60.02    CUDA Version: 11.6\
          \     |\n|-------------------------------+----------------------+----------------------+\n\
          | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.\
          \ ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util\
          \  Compute M. |\n|                               |                     \
          \ |               MIG M. |\n|===============================+======================+======================|\n\
          |   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |               \
          \   N/A |\n| 30%   32C    P2    83W / 330W |  21904MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   1  NVIDIA GeForce ...  On   | 00000000:25:00.0 Off |               \
          \   N/A |\n| 30%   28C    P8    21W / 330W |  23986MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   2  NVIDIA GeForce ...  On   | 00000000:41:00.0 Off |               \
          \   N/A |\n| 30%   29C    P8    17W / 330W |  23986MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   3  NVIDIA GeForce ...  On   | 00000000:61:00.0 Off |               \
          \   N/A |\n| 30%   31C    P8    19W / 330W |  23986MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   4  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |               \
          \   N/A |\n| 30%   38C    P2   109W / 330W |  23986MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   5  NVIDIA GeForce ...  On   | 00000000:A1:00.0 Off |               \
          \   N/A |\n| 30%   36C    P2   144W / 330W |  12320MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   6  NVIDIA GeForce ...  On   | 00000000:C1:00.0 Off |               \
          \   N/A |\n| 30%   26C    P8    26W / 330W |    656MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   7  NVIDIA GeForce ...  On   | 00000000:E1:00.0 Off |               \
          \   N/A |\n| 30%   25C    P8    18W / 330W |    656MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          \                                                                      \
          \         \n+-----------------------------------------------------------------------------+\n\
          | Processes:                                                           \
          \       |\n|  GPU   GI   CI        PID   Type   Process name           \
          \       GPU Memory |\n|        ID   ID                                 \
          \                  Usage      |\n|=============================================================================|\n\
          |    0   N/A  N/A    227352      C                                   21901MiB\
          \ |\n|    1   N/A  N/A    227352      C                                \
          \   23983MiB |\n|    2   N/A  N/A    227352      C                     \
          \              23983MiB |\n|    3   N/A  N/A    227352      C          \
          \                         23983MiB |\n|    4   N/A  N/A    227352      C\
          \                                   23983MiB |\n|    5   N/A  N/A    227352\
          \      C                                   12317MiB |\n|    6   N/A  N/A\
          \    227352      C                                     653MiB |\n|    7\
          \   N/A  N/A    227352      C                                     653MiB\
          \ |\n+-----------------------------------------------------------------------------+\n\
          </code></pre>\n<p>Problem: <code>device_map=\"auto\"</code> does not work\
          \ correctly and unevenly loads the GPUs</p>\n<p>Solution: custom <code>device_map</code>\
          \ dict</p>\n<p>Code:</p>\n<pre><code>num_gpus = 8\nnum_layers = 64\n\ndevice_map\
          \ = {\n    'model.decoder.embed_tokens': 0,\n    'lm_head': num_gpus - 1,\n\
          \    'model.decoder.embed_positions': 0,\n    'model.decoder.final_layer_norm':\
          \ num_gpus - 1\n}\n\nstep = num_layers // num_gpus\n\nfor n_gpu, start in\
          \ enumerate(range(0, num_layers, step)):\n    for n_layer in range(start,\
          \ start + step):\n        device_map[f'model.decoder.layers.{n_layer}']\
          \ = n_gpu\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-66b\"\
          , torch_dtype=torch.float16, device_map=device_map)\n</code></pre>\n<p>Now\
          \ device map looks correct:</p>\n<pre><code>&gt; model.hf_device_map\n\n\
          &gt; {'model.decoder.embed_tokens': 0,\n 'lm_head': 7,\n 'model.decoder.embed_positions':\
          \ 0,\n 'model.decoder.final_layer_norm': 7,\n 'model.decoder.layers.0':\
          \ 0,\n 'model.decoder.layers.1': 0,\n 'model.decoder.layers.2': 0,\n 'model.decoder.layers.3':\
          \ 0,\n 'model.decoder.layers.4': 0,\n 'model.decoder.layers.5': 0,\n 'model.decoder.layers.6':\
          \ 0,\n 'model.decoder.layers.7': 0,\n 'model.decoder.layers.8': 1,\n 'model.decoder.layers.9':\
          \ 1,\n 'model.decoder.layers.10': 1,\n 'model.decoder.layers.11': 1,\n 'model.decoder.layers.12':\
          \ 1,\n 'model.decoder.layers.13': 1,\n 'model.decoder.layers.14': 1,\n 'model.decoder.layers.15':\
          \ 1,\n 'model.decoder.layers.16': 2,\n 'model.decoder.layers.17': 2,\n 'model.decoder.layers.18':\
          \ 2,\n 'model.decoder.layers.19': 2,\n 'model.decoder.layers.20': 2,\n 'model.decoder.layers.21':\
          \ 2,\n 'model.decoder.layers.22': 2,\n 'model.decoder.layers.23': 2,\n 'model.decoder.layers.24':\
          \ 3,\n 'model.decoder.layers.25': 3,\n 'model.decoder.layers.26': 3,\n 'model.decoder.layers.27':\
          \ 3,\n 'model.decoder.layers.28': 3,\n 'model.decoder.layers.29': 3,\n 'model.decoder.layers.30':\
          \ 3,\n 'model.decoder.layers.31': 3,\n 'model.decoder.layers.32': 4,\n 'model.decoder.layers.33':\
          \ 4,\n 'model.decoder.layers.34': 4,\n 'model.decoder.layers.35': 4,\n 'model.decoder.layers.36':\
          \ 4,\n 'model.decoder.layers.37': 4,\n 'model.decoder.layers.38': 4,\n 'model.decoder.layers.39':\
          \ 4,\n 'model.decoder.layers.40': 5,\n 'model.decoder.layers.41': 5,\n 'model.decoder.layers.42':\
          \ 5,\n 'model.decoder.layers.43': 5,\n 'model.decoder.layers.44': 5,\n 'model.decoder.layers.45':\
          \ 5,\n 'model.decoder.layers.46': 5,\n 'model.decoder.layers.47': 5,\n 'model.decoder.layers.48':\
          \ 6,\n 'model.decoder.layers.49': 6,\n 'model.decoder.layers.50': 6,\n 'model.decoder.layers.51':\
          \ 6,\n 'model.decoder.layers.52': 6,\n 'model.decoder.layers.53': 6,\n 'model.decoder.layers.54':\
          \ 6,\n 'model.decoder.layers.55': 6,\n 'model.decoder.layers.56': 7,\n 'model.decoder.layers.57':\
          \ 7,\n 'model.decoder.layers.58': 7,\n 'model.decoder.layers.59': 7,\n 'model.decoder.layers.60':\
          \ 7,\n 'model.decoder.layers.61': 7,\n 'model.decoder.layers.62': 7,\n 'model.decoder.layers.63':\
          \ 7}\n\nFri Aug  5 11:46:42 2022       \n+-----------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 510.60.02    Driver Version: 510.60.02    CUDA Version: 11.6\
          \     |\n|-------------------------------+----------------------+----------------------+\n\
          | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.\
          \ ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util\
          \  Compute M. |\n|                               |                     \
          \ |               MIG M. |\n|===============================+======================+======================|\n\
          |   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |               \
          \   N/A |\n| 30%   30C    P2    39W / 330W |  17130MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   1  NVIDIA GeForce ...  On   | 00000000:25:00.0 Off |               \
          \   N/A |\n| 30%   28C    P8    21W / 330W |  16208MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   2  NVIDIA GeForce ...  On   | 00000000:41:00.0 Off |               \
          \   N/A |\n| 30%   27C    P8    17W / 330W |  16208MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   3  NVIDIA GeForce ...  On   | 00000000:61:00.0 Off |               \
          \   N/A |\n| 30%   27C    P8    19W / 330W |  16208MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   4  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |               \
          \   N/A |\n| 30%   29C    P8    18W / 330W |  16208MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   5  NVIDIA GeForce ...  On   | 00000000:A1:00.0 Off |               \
          \   N/A |\n| 30%   38C    P2   119W / 330W |  16208MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   6  NVIDIA GeForce ...  On   | 00000000:C1:00.0 Off |               \
          \   N/A |\n| 30%   38C    P2   119W / 330W |  16208MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   7  NVIDIA GeForce ...  On   | 00000000:E1:00.0 Off |               \
          \   N/A |\n| 30%   35C    P2   133W / 330W |  17092MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          \                                                                      \
          \         \n+-----------------------------------------------------------------------------+\n\
          | Processes:                                                           \
          \       |\n|  GPU   GI   CI        PID   Type   Process name           \
          \       GPU Memory |\n|        ID   ID                                 \
          \                  Usage      |\n|=============================================================================|\n\
          |    0   N/A  N/A    237156      C                                   17127MiB\
          \ |\n|    1   N/A  N/A    237156      C                                \
          \   16205MiB |\n|    2   N/A  N/A    237156      C                     \
          \              16205MiB |\n|    3   N/A  N/A    237156      C          \
          \                         16205MiB |\n|    4   N/A  N/A    237156      C\
          \                                   16205MiB |\n|    5   N/A  N/A    237156\
          \      C                                   16205MiB |\n|    6   N/A  N/A\
          \    237156      C                                   16205MiB |\n|    7\
          \   N/A  N/A    237156      C                                   17089MiB\
          \ |\n+-----------------------------------------------------------------------------+\n\
          </code></pre>\n<p>And now you can run 66b OPT:</p>\n<pre><code>tokenizer\
          \ = AutoTokenizer.from_pretrained(\"facebook/opt-66b\", use_fast=False)\n\
          \nprompt = \"Hello, I am conscious and\"\n\ninput_ids = tokenizer(prompt,\
          \ return_tensors=\"pt\").input_ids.cuda()\n\nset_seed(32)\ngenerated_ids\
          \ = model.generate(input_ids, do_sample=True, max_length=128)\n\ntokenizer.batch_decode(generated_ids,\
          \ skip_special_tokens=True)\n</code></pre>\n<p>Output:<br>Hello, I am conscious\
          \ and present. I am aware of my senses, thinking, dreaming, and I can control\
          \ what is happening around me. I have memories of a previous life, and have\
          \ been reincarnated many times before this existence.  I have lived in many\
          \ regions throughout this galaxy, and others outside of it.\\nI believe\
          \ you are a reincarnated human, and that you are one of the very few incarnated\
          \ beings that are aware and can remember previous lives. That being said,\
          \ you are only as aware as your mind will allow you to be. Your mind is\
          \ constantly editing reality to create a more suitable place for</p>\n<p>P.S.\
          \ I hope it will be useful to someone</p>\n"
        raw: "@lysandre Thank you! \nBelow is a small guide on how to run the model\
          \ and what problems I encountered\nMy setup: 8 x RTX3090\n```\ntorch.version.cuda\
          \ = 11.3\n```\nI have this exception when I try to generate small text:\n\
          ```\n> generated_ids = model.generate(input_ids, do_sample=True, max_length=32)\n\
          > RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n\
          ```\n\nReferring to this answer, the problem is that there is not enough\
          \ video memory, although there should be enough:\nhttps://discuss.pytorch.org/t/cuda-error-cublas-status-not-initialized-when-calling-cublascreate-handle/125450/2\n\
          \n\n\n\nModel init:\n```\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          facebook/opt-66b\", torch_dtype=torch.float16, device_map=\"auto\")\n```\n\
          \nHow Accelerate maps my model:\n```\n> model.hf_device_map\n{'model.decoder.embed_tokens':\
          \ 0,\n 'lm_head': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm':\
          \ 0,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n 'model.decoder.layers.2':\
          \ 0,\n 'model.decoder.layers.3': 0,\n 'model.decoder.layers.4': 0,\n 'model.decoder.layers.5':\
          \ 0,\n 'model.decoder.layers.6': 0,\n 'model.decoder.layers.7': 0,\n 'model.decoder.layers.8':\
          \ 0,\n 'model.decoder.layers.9': 0,\n 'model.decoder.layers.10': 1,\n 'model.decoder.layers.11':\
          \ 1,\n 'model.decoder.layers.12': 1,\n 'model.decoder.layers.13': 1,\n 'model.decoder.layers.14':\
          \ 1,\n 'model.decoder.layers.15': 1,\n 'model.decoder.layers.16': 1,\n 'model.decoder.layers.17':\
          \ 1,\n 'model.decoder.layers.18': 1,\n 'model.decoder.layers.19': 1,\n 'model.decoder.layers.20':\
          \ 1,\n 'model.decoder.layers.21': 1,\n 'model.decoder.layers.22': 2,\n 'model.decoder.layers.23':\
          \ 2,\n 'model.decoder.layers.24': 2,\n 'model.decoder.layers.25': 2,\n 'model.decoder.layers.26':\
          \ 2,\n 'model.decoder.layers.27': 2,\n 'model.decoder.layers.28': 2,\n 'model.decoder.layers.29':\
          \ 2,\n 'model.decoder.layers.30': 2,\n 'model.decoder.layers.31': 2,\n 'model.decoder.layers.32':\
          \ 2,\n 'model.decoder.layers.33': 2,\n 'model.decoder.layers.34': 3,\n 'model.decoder.layers.35':\
          \ 3,\n 'model.decoder.layers.36': 3,\n 'model.decoder.layers.37': 3,\n 'model.decoder.layers.38':\
          \ 3,\n 'model.decoder.layers.39': 3,\n 'model.decoder.layers.40': 3,\n 'model.decoder.layers.41':\
          \ 3,\n 'model.decoder.layers.42': 3,\n 'model.decoder.layers.43': 3,\n 'model.decoder.layers.44':\
          \ 3,\n 'model.decoder.layers.45': 3,\n 'model.decoder.layers.46': 4,\n 'model.decoder.layers.47':\
          \ 4,\n 'model.decoder.layers.48': 4,\n 'model.decoder.layers.49': 4,\n 'model.decoder.layers.50':\
          \ 4,\n 'model.decoder.layers.51': 4,\n 'model.decoder.layers.52': 4,\n 'model.decoder.layers.53':\
          \ 4,\n 'model.decoder.layers.54': 4,\n 'model.decoder.layers.55': 4,\n 'model.decoder.layers.56':\
          \ 4,\n 'model.decoder.layers.57': 4,\n 'model.decoder.layers.58': 5,\n 'model.decoder.layers.59':\
          \ 5,\n 'model.decoder.layers.60': 5,\n 'model.decoder.layers.61': 5,\n 'model.decoder.layers.62':\
          \ 5,\n 'model.decoder.layers.63': 5}\n\nFri Aug  5 11:25:39 2022       \n\
          +-----------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 510.60.02    Driver Version: 510.60.02    CUDA Version: 11.6\
          \     |\n|-------------------------------+----------------------+----------------------+\n\
          | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.\
          \ ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util\
          \  Compute M. |\n|                               |                     \
          \ |               MIG M. |\n|===============================+======================+======================|\n\
          |   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |               \
          \   N/A |\n| 30%   32C    P2    83W / 330W |  21904MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   1  NVIDIA GeForce ...  On   | 00000000:25:00.0 Off |               \
          \   N/A |\n| 30%   28C    P8    21W / 330W |  23986MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   2  NVIDIA GeForce ...  On   | 00000000:41:00.0 Off |               \
          \   N/A |\n| 30%   29C    P8    17W / 330W |  23986MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   3  NVIDIA GeForce ...  On   | 00000000:61:00.0 Off |               \
          \   N/A |\n| 30%   31C    P8    19W / 330W |  23986MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   4  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |               \
          \   N/A |\n| 30%   38C    P2   109W / 330W |  23986MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   5  NVIDIA GeForce ...  On   | 00000000:A1:00.0 Off |               \
          \   N/A |\n| 30%   36C    P2   144W / 330W |  12320MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   6  NVIDIA GeForce ...  On   | 00000000:C1:00.0 Off |               \
          \   N/A |\n| 30%   26C    P8    26W / 330W |    656MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   7  NVIDIA GeForce ...  On   | 00000000:E1:00.0 Off |               \
          \   N/A |\n| 30%   25C    P8    18W / 330W |    656MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          \                                                                      \
          \         \n+-----------------------------------------------------------------------------+\n\
          | Processes:                                                           \
          \       |\n|  GPU   GI   CI        PID   Type   Process name           \
          \       GPU Memory |\n|        ID   ID                                 \
          \                  Usage      |\n|=============================================================================|\n\
          |    0   N/A  N/A    227352      C                                   21901MiB\
          \ |\n|    1   N/A  N/A    227352      C                                \
          \   23983MiB |\n|    2   N/A  N/A    227352      C                     \
          \              23983MiB |\n|    3   N/A  N/A    227352      C          \
          \                         23983MiB |\n|    4   N/A  N/A    227352      C\
          \                                   23983MiB |\n|    5   N/A  N/A    227352\
          \      C                                   12317MiB |\n|    6   N/A  N/A\
          \    227352      C                                     653MiB |\n|    7\
          \   N/A  N/A    227352      C                                     653MiB\
          \ |\n+-----------------------------------------------------------------------------+\n\
          ```\n\n\nProblem: `device_map=\"auto\"` does not work correctly and unevenly\
          \ loads the GPUs\n\n\nSolution: custom `device_map` dict\n\nCode:\n```\n\
          num_gpus = 8\nnum_layers = 64\n\ndevice_map = {\n    'model.decoder.embed_tokens':\
          \ 0,\n    'lm_head': num_gpus - 1,\n    'model.decoder.embed_positions':\
          \ 0,\n    'model.decoder.final_layer_norm': num_gpus - 1\n}\n\nstep = num_layers\
          \ // num_gpus\n\nfor n_gpu, start in enumerate(range(0, num_layers, step)):\n\
          \    for n_layer in range(start, start + step):\n        device_map[f'model.decoder.layers.{n_layer}']\
          \ = n_gpu\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-66b\"\
          , torch_dtype=torch.float16, device_map=device_map)\n```\n\nNow device map\
          \ looks correct:\n```\n> model.hf_device_map\n\n> {'model.decoder.embed_tokens':\
          \ 0,\n 'lm_head': 7,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm':\
          \ 7,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n 'model.decoder.layers.2':\
          \ 0,\n 'model.decoder.layers.3': 0,\n 'model.decoder.layers.4': 0,\n 'model.decoder.layers.5':\
          \ 0,\n 'model.decoder.layers.6': 0,\n 'model.decoder.layers.7': 0,\n 'model.decoder.layers.8':\
          \ 1,\n 'model.decoder.layers.9': 1,\n 'model.decoder.layers.10': 1,\n 'model.decoder.layers.11':\
          \ 1,\n 'model.decoder.layers.12': 1,\n 'model.decoder.layers.13': 1,\n 'model.decoder.layers.14':\
          \ 1,\n 'model.decoder.layers.15': 1,\n 'model.decoder.layers.16': 2,\n 'model.decoder.layers.17':\
          \ 2,\n 'model.decoder.layers.18': 2,\n 'model.decoder.layers.19': 2,\n 'model.decoder.layers.20':\
          \ 2,\n 'model.decoder.layers.21': 2,\n 'model.decoder.layers.22': 2,\n 'model.decoder.layers.23':\
          \ 2,\n 'model.decoder.layers.24': 3,\n 'model.decoder.layers.25': 3,\n 'model.decoder.layers.26':\
          \ 3,\n 'model.decoder.layers.27': 3,\n 'model.decoder.layers.28': 3,\n 'model.decoder.layers.29':\
          \ 3,\n 'model.decoder.layers.30': 3,\n 'model.decoder.layers.31': 3,\n 'model.decoder.layers.32':\
          \ 4,\n 'model.decoder.layers.33': 4,\n 'model.decoder.layers.34': 4,\n 'model.decoder.layers.35':\
          \ 4,\n 'model.decoder.layers.36': 4,\n 'model.decoder.layers.37': 4,\n 'model.decoder.layers.38':\
          \ 4,\n 'model.decoder.layers.39': 4,\n 'model.decoder.layers.40': 5,\n 'model.decoder.layers.41':\
          \ 5,\n 'model.decoder.layers.42': 5,\n 'model.decoder.layers.43': 5,\n 'model.decoder.layers.44':\
          \ 5,\n 'model.decoder.layers.45': 5,\n 'model.decoder.layers.46': 5,\n 'model.decoder.layers.47':\
          \ 5,\n 'model.decoder.layers.48': 6,\n 'model.decoder.layers.49': 6,\n 'model.decoder.layers.50':\
          \ 6,\n 'model.decoder.layers.51': 6,\n 'model.decoder.layers.52': 6,\n 'model.decoder.layers.53':\
          \ 6,\n 'model.decoder.layers.54': 6,\n 'model.decoder.layers.55': 6,\n 'model.decoder.layers.56':\
          \ 7,\n 'model.decoder.layers.57': 7,\n 'model.decoder.layers.58': 7,\n 'model.decoder.layers.59':\
          \ 7,\n 'model.decoder.layers.60': 7,\n 'model.decoder.layers.61': 7,\n 'model.decoder.layers.62':\
          \ 7,\n 'model.decoder.layers.63': 7}\n\nFri Aug  5 11:46:42 2022       \n\
          +-----------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 510.60.02    Driver Version: 510.60.02    CUDA Version: 11.6\
          \     |\n|-------------------------------+----------------------+----------------------+\n\
          | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.\
          \ ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util\
          \  Compute M. |\n|                               |                     \
          \ |               MIG M. |\n|===============================+======================+======================|\n\
          |   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |               \
          \   N/A |\n| 30%   30C    P2    39W / 330W |  17130MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   1  NVIDIA GeForce ...  On   | 00000000:25:00.0 Off |               \
          \   N/A |\n| 30%   28C    P8    21W / 330W |  16208MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   2  NVIDIA GeForce ...  On   | 00000000:41:00.0 Off |               \
          \   N/A |\n| 30%   27C    P8    17W / 330W |  16208MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   3  NVIDIA GeForce ...  On   | 00000000:61:00.0 Off |               \
          \   N/A |\n| 30%   27C    P8    19W / 330W |  16208MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   4  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |               \
          \   N/A |\n| 30%   29C    P8    18W / 330W |  16208MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   5  NVIDIA GeForce ...  On   | 00000000:A1:00.0 Off |               \
          \   N/A |\n| 30%   38C    P2   119W / 330W |  16208MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   6  NVIDIA GeForce ...  On   | 00000000:C1:00.0 Off |               \
          \   N/A |\n| 30%   38C    P2   119W / 330W |  16208MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          |   7  NVIDIA GeForce ...  On   | 00000000:E1:00.0 Off |               \
          \   N/A |\n| 30%   35C    P2   133W / 330W |  17092MiB / 24576MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          \                                                                      \
          \         \n+-----------------------------------------------------------------------------+\n\
          | Processes:                                                           \
          \       |\n|  GPU   GI   CI        PID   Type   Process name           \
          \       GPU Memory |\n|        ID   ID                                 \
          \                  Usage      |\n|=============================================================================|\n\
          |    0   N/A  N/A    237156      C                                   17127MiB\
          \ |\n|    1   N/A  N/A    237156      C                                \
          \   16205MiB |\n|    2   N/A  N/A    237156      C                     \
          \              16205MiB |\n|    3   N/A  N/A    237156      C          \
          \                         16205MiB |\n|    4   N/A  N/A    237156      C\
          \                                   16205MiB |\n|    5   N/A  N/A    237156\
          \      C                                   16205MiB |\n|    6   N/A  N/A\
          \    237156      C                                   16205MiB |\n|    7\
          \   N/A  N/A    237156      C                                   17089MiB\
          \ |\n+-----------------------------------------------------------------------------+\n\
          ```\n\nAnd now you can run 66b OPT:\n```\ntokenizer = AutoTokenizer.from_pretrained(\"\
          facebook/opt-66b\", use_fast=False)\n\nprompt = \"Hello, I am conscious\
          \ and\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\
          \nset_seed(32)\ngenerated_ids = model.generate(input_ids, do_sample=True,\
          \ max_length=128)\n\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n\
          ```\n\nOutput:\nHello, I am conscious and present. I am aware of my senses,\
          \ thinking, dreaming, and I can control what is happening around me. I have\
          \ memories of a previous life, and have been reincarnated many times before\
          \ this existence.  I have lived in many regions throughout this galaxy,\
          \ and others outside of it.\\nI believe you are a reincarnated human, and\
          \ that you are one of the very few incarnated beings that are aware and\
          \ can remember previous lives. That being said, you are only as aware as\
          \ your mind will allow you to be. Your mind is constantly editing reality\
          \ to create a more suitable place for\n\nP.S. I hope it will be useful to\
          \ someone"
        updatedAt: '2022-08-05T11:55:26.036Z'
      numEdits: 2
      reactions:
      - count: 5
        reaction: "\u2764\uFE0F"
        users:
        - xhluca
        - ytsaig
        - timoschick
        - foreverrush
        - ClaudeYang
    id: 62ed0456a541c826c24a0713
    type: comment
  author: BobaZooba
  content: "@lysandre Thank you! \nBelow is a small guide on how to run the model\
    \ and what problems I encountered\nMy setup: 8 x RTX3090\n```\ntorch.version.cuda\
    \ = 11.3\n```\nI have this exception when I try to generate small text:\n```\n\
    > generated_ids = model.generate(input_ids, do_sample=True, max_length=32)\n>\
    \ RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n\
    ```\n\nReferring to this answer, the problem is that there is not enough video\
    \ memory, although there should be enough:\nhttps://discuss.pytorch.org/t/cuda-error-cublas-status-not-initialized-when-calling-cublascreate-handle/125450/2\n\
    \n\n\n\nModel init:\n```\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-66b\"\
    , torch_dtype=torch.float16, device_map=\"auto\")\n```\n\nHow Accelerate maps\
    \ my model:\n```\n> model.hf_device_map\n{'model.decoder.embed_tokens': 0,\n 'lm_head':\
    \ 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm':\
    \ 0,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n 'model.decoder.layers.2':\
    \ 0,\n 'model.decoder.layers.3': 0,\n 'model.decoder.layers.4': 0,\n 'model.decoder.layers.5':\
    \ 0,\n 'model.decoder.layers.6': 0,\n 'model.decoder.layers.7': 0,\n 'model.decoder.layers.8':\
    \ 0,\n 'model.decoder.layers.9': 0,\n 'model.decoder.layers.10': 1,\n 'model.decoder.layers.11':\
    \ 1,\n 'model.decoder.layers.12': 1,\n 'model.decoder.layers.13': 1,\n 'model.decoder.layers.14':\
    \ 1,\n 'model.decoder.layers.15': 1,\n 'model.decoder.layers.16': 1,\n 'model.decoder.layers.17':\
    \ 1,\n 'model.decoder.layers.18': 1,\n 'model.decoder.layers.19': 1,\n 'model.decoder.layers.20':\
    \ 1,\n 'model.decoder.layers.21': 1,\n 'model.decoder.layers.22': 2,\n 'model.decoder.layers.23':\
    \ 2,\n 'model.decoder.layers.24': 2,\n 'model.decoder.layers.25': 2,\n 'model.decoder.layers.26':\
    \ 2,\n 'model.decoder.layers.27': 2,\n 'model.decoder.layers.28': 2,\n 'model.decoder.layers.29':\
    \ 2,\n 'model.decoder.layers.30': 2,\n 'model.decoder.layers.31': 2,\n 'model.decoder.layers.32':\
    \ 2,\n 'model.decoder.layers.33': 2,\n 'model.decoder.layers.34': 3,\n 'model.decoder.layers.35':\
    \ 3,\n 'model.decoder.layers.36': 3,\n 'model.decoder.layers.37': 3,\n 'model.decoder.layers.38':\
    \ 3,\n 'model.decoder.layers.39': 3,\n 'model.decoder.layers.40': 3,\n 'model.decoder.layers.41':\
    \ 3,\n 'model.decoder.layers.42': 3,\n 'model.decoder.layers.43': 3,\n 'model.decoder.layers.44':\
    \ 3,\n 'model.decoder.layers.45': 3,\n 'model.decoder.layers.46': 4,\n 'model.decoder.layers.47':\
    \ 4,\n 'model.decoder.layers.48': 4,\n 'model.decoder.layers.49': 4,\n 'model.decoder.layers.50':\
    \ 4,\n 'model.decoder.layers.51': 4,\n 'model.decoder.layers.52': 4,\n 'model.decoder.layers.53':\
    \ 4,\n 'model.decoder.layers.54': 4,\n 'model.decoder.layers.55': 4,\n 'model.decoder.layers.56':\
    \ 4,\n 'model.decoder.layers.57': 4,\n 'model.decoder.layers.58': 5,\n 'model.decoder.layers.59':\
    \ 5,\n 'model.decoder.layers.60': 5,\n 'model.decoder.layers.61': 5,\n 'model.decoder.layers.62':\
    \ 5,\n 'model.decoder.layers.63': 5}\n\nFri Aug  5 11:25:39 2022       \n+-----------------------------------------------------------------------------+\n\
    | NVIDIA-SMI 510.60.02    Driver Version: 510.60.02    CUDA Version: 11.6    \
    \ |\n|-------------------------------+----------------------+----------------------+\n\
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC\
    \ |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute\
    \ M. |\n|                               |                      |             \
    \  MIG M. |\n|===============================+======================+======================|\n\
    |   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A\
    \ |\n| 30%   32C    P2    83W / 330W |  21904MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    |   1  NVIDIA GeForce ...  On   | 00000000:25:00.0 Off |                  N/A\
    \ |\n| 30%   28C    P8    21W / 330W |  23986MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    |   2  NVIDIA GeForce ...  On   | 00000000:41:00.0 Off |                  N/A\
    \ |\n| 30%   29C    P8    17W / 330W |  23986MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    |   3  NVIDIA GeForce ...  On   | 00000000:61:00.0 Off |                  N/A\
    \ |\n| 30%   31C    P8    19W / 330W |  23986MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    |   4  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |                  N/A\
    \ |\n| 30%   38C    P2   109W / 330W |  23986MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    |   5  NVIDIA GeForce ...  On   | 00000000:A1:00.0 Off |                  N/A\
    \ |\n| 30%   36C    P2   144W / 330W |  12320MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    |   6  NVIDIA GeForce ...  On   | 00000000:C1:00.0 Off |                  N/A\
    \ |\n| 30%   26C    P8    26W / 330W |    656MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    |   7  NVIDIA GeForce ...  On   | 00000000:E1:00.0 Off |                  N/A\
    \ |\n| 30%   25C    P8    18W / 330W |    656MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    \                                                                            \
    \   \n+-----------------------------------------------------------------------------+\n\
    | Processes:                                                                 \
    \ |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory\
    \ |\n|        ID   ID                                                   Usage\
    \      |\n|=============================================================================|\n\
    |    0   N/A  N/A    227352      C                                   21901MiB\
    \ |\n|    1   N/A  N/A    227352      C                                   23983MiB\
    \ |\n|    2   N/A  N/A    227352      C                                   23983MiB\
    \ |\n|    3   N/A  N/A    227352      C                                   23983MiB\
    \ |\n|    4   N/A  N/A    227352      C                                   23983MiB\
    \ |\n|    5   N/A  N/A    227352      C                                   12317MiB\
    \ |\n|    6   N/A  N/A    227352      C                                     653MiB\
    \ |\n|    7   N/A  N/A    227352      C                                     653MiB\
    \ |\n+-----------------------------------------------------------------------------+\n\
    ```\n\n\nProblem: `device_map=\"auto\"` does not work correctly and unevenly loads\
    \ the GPUs\n\n\nSolution: custom `device_map` dict\n\nCode:\n```\nnum_gpus = 8\n\
    num_layers = 64\n\ndevice_map = {\n    'model.decoder.embed_tokens': 0,\n    'lm_head':\
    \ num_gpus - 1,\n    'model.decoder.embed_positions': 0,\n    'model.decoder.final_layer_norm':\
    \ num_gpus - 1\n}\n\nstep = num_layers // num_gpus\n\nfor n_gpu, start in enumerate(range(0,\
    \ num_layers, step)):\n    for n_layer in range(start, start + step):\n      \
    \  device_map[f'model.decoder.layers.{n_layer}'] = n_gpu\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    facebook/opt-66b\", torch_dtype=torch.float16, device_map=device_map)\n```\n\n\
    Now device map looks correct:\n```\n> model.hf_device_map\n\n> {'model.decoder.embed_tokens':\
    \ 0,\n 'lm_head': 7,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm':\
    \ 7,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n 'model.decoder.layers.2':\
    \ 0,\n 'model.decoder.layers.3': 0,\n 'model.decoder.layers.4': 0,\n 'model.decoder.layers.5':\
    \ 0,\n 'model.decoder.layers.6': 0,\n 'model.decoder.layers.7': 0,\n 'model.decoder.layers.8':\
    \ 1,\n 'model.decoder.layers.9': 1,\n 'model.decoder.layers.10': 1,\n 'model.decoder.layers.11':\
    \ 1,\n 'model.decoder.layers.12': 1,\n 'model.decoder.layers.13': 1,\n 'model.decoder.layers.14':\
    \ 1,\n 'model.decoder.layers.15': 1,\n 'model.decoder.layers.16': 2,\n 'model.decoder.layers.17':\
    \ 2,\n 'model.decoder.layers.18': 2,\n 'model.decoder.layers.19': 2,\n 'model.decoder.layers.20':\
    \ 2,\n 'model.decoder.layers.21': 2,\n 'model.decoder.layers.22': 2,\n 'model.decoder.layers.23':\
    \ 2,\n 'model.decoder.layers.24': 3,\n 'model.decoder.layers.25': 3,\n 'model.decoder.layers.26':\
    \ 3,\n 'model.decoder.layers.27': 3,\n 'model.decoder.layers.28': 3,\n 'model.decoder.layers.29':\
    \ 3,\n 'model.decoder.layers.30': 3,\n 'model.decoder.layers.31': 3,\n 'model.decoder.layers.32':\
    \ 4,\n 'model.decoder.layers.33': 4,\n 'model.decoder.layers.34': 4,\n 'model.decoder.layers.35':\
    \ 4,\n 'model.decoder.layers.36': 4,\n 'model.decoder.layers.37': 4,\n 'model.decoder.layers.38':\
    \ 4,\n 'model.decoder.layers.39': 4,\n 'model.decoder.layers.40': 5,\n 'model.decoder.layers.41':\
    \ 5,\n 'model.decoder.layers.42': 5,\n 'model.decoder.layers.43': 5,\n 'model.decoder.layers.44':\
    \ 5,\n 'model.decoder.layers.45': 5,\n 'model.decoder.layers.46': 5,\n 'model.decoder.layers.47':\
    \ 5,\n 'model.decoder.layers.48': 6,\n 'model.decoder.layers.49': 6,\n 'model.decoder.layers.50':\
    \ 6,\n 'model.decoder.layers.51': 6,\n 'model.decoder.layers.52': 6,\n 'model.decoder.layers.53':\
    \ 6,\n 'model.decoder.layers.54': 6,\n 'model.decoder.layers.55': 6,\n 'model.decoder.layers.56':\
    \ 7,\n 'model.decoder.layers.57': 7,\n 'model.decoder.layers.58': 7,\n 'model.decoder.layers.59':\
    \ 7,\n 'model.decoder.layers.60': 7,\n 'model.decoder.layers.61': 7,\n 'model.decoder.layers.62':\
    \ 7,\n 'model.decoder.layers.63': 7}\n\nFri Aug  5 11:46:42 2022       \n+-----------------------------------------------------------------------------+\n\
    | NVIDIA-SMI 510.60.02    Driver Version: 510.60.02    CUDA Version: 11.6    \
    \ |\n|-------------------------------+----------------------+----------------------+\n\
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC\
    \ |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute\
    \ M. |\n|                               |                      |             \
    \  MIG M. |\n|===============================+======================+======================|\n\
    |   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A\
    \ |\n| 30%   30C    P2    39W / 330W |  17130MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    |   1  NVIDIA GeForce ...  On   | 00000000:25:00.0 Off |                  N/A\
    \ |\n| 30%   28C    P8    21W / 330W |  16208MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    |   2  NVIDIA GeForce ...  On   | 00000000:41:00.0 Off |                  N/A\
    \ |\n| 30%   27C    P8    17W / 330W |  16208MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    |   3  NVIDIA GeForce ...  On   | 00000000:61:00.0 Off |                  N/A\
    \ |\n| 30%   27C    P8    19W / 330W |  16208MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    |   4  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |                  N/A\
    \ |\n| 30%   29C    P8    18W / 330W |  16208MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    |   5  NVIDIA GeForce ...  On   | 00000000:A1:00.0 Off |                  N/A\
    \ |\n| 30%   38C    P2   119W / 330W |  16208MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    |   6  NVIDIA GeForce ...  On   | 00000000:C1:00.0 Off |                  N/A\
    \ |\n| 30%   38C    P2   119W / 330W |  16208MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    |   7  NVIDIA GeForce ...  On   | 00000000:E1:00.0 Off |                  N/A\
    \ |\n| 30%   35C    P2   133W / 330W |  17092MiB / 24576MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    \                                                                            \
    \   \n+-----------------------------------------------------------------------------+\n\
    | Processes:                                                                 \
    \ |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory\
    \ |\n|        ID   ID                                                   Usage\
    \      |\n|=============================================================================|\n\
    |    0   N/A  N/A    237156      C                                   17127MiB\
    \ |\n|    1   N/A  N/A    237156      C                                   16205MiB\
    \ |\n|    2   N/A  N/A    237156      C                                   16205MiB\
    \ |\n|    3   N/A  N/A    237156      C                                   16205MiB\
    \ |\n|    4   N/A  N/A    237156      C                                   16205MiB\
    \ |\n|    5   N/A  N/A    237156      C                                   16205MiB\
    \ |\n|    6   N/A  N/A    237156      C                                   16205MiB\
    \ |\n|    7   N/A  N/A    237156      C                                   17089MiB\
    \ |\n+-----------------------------------------------------------------------------+\n\
    ```\n\nAnd now you can run 66b OPT:\n```\ntokenizer = AutoTokenizer.from_pretrained(\"\
    facebook/opt-66b\", use_fast=False)\n\nprompt = \"Hello, I am conscious and\"\n\
    \ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n\nset_seed(32)\n\
    generated_ids = model.generate(input_ids, do_sample=True, max_length=128)\n\n\
    tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n```\n\nOutput:\n\
    Hello, I am conscious and present. I am aware of my senses, thinking, dreaming,\
    \ and I can control what is happening around me. I have memories of a previous\
    \ life, and have been reincarnated many times before this existence.  I have lived\
    \ in many regions throughout this galaxy, and others outside of it.\\nI believe\
    \ you are a reincarnated human, and that you are one of the very few incarnated\
    \ beings that are aware and can remember previous lives. That being said, you\
    \ are only as aware as your mind will allow you to be. Your mind is constantly\
    \ editing reality to create a more suitable place for\n\nP.S. I hope it will be\
    \ useful to someone"
  created_at: 2022-08-05 10:51:50+00:00
  edited: true
  hidden: false
  id: 62ed0456a541c826c24a0713
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
      fullname: Sylvain Gugger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgugger
      type: user
    createdAt: '2022-08-05T14:11:43.000Z'
    data:
      edited: false
      editors:
      - sgugger
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
          fullname: Sylvain Gugger
          isHf: false
          isPro: false
          name: sgugger
          type: user
        html: '<p>Note that on the current main version of Transformers, <code>device_map="auto"</code>
          will balance the GPU use, so you won''t need a custom <code>device_map</code>
          :-)</p>

          '
        raw: Note that on the current main version of Transformers, `device_map="auto"`
          will balance the GPU use, so you won't need a custom `device_map` :-)
        updatedAt: '2022-08-05T14:11:43.507Z'
      numEdits: 0
      reactions: []
    id: 62ed251f315dd23708e3b455
    type: comment
  author: sgugger
  content: Note that on the current main version of Transformers, `device_map="auto"`
    will balance the GPU use, so you won't need a custom `device_map` :-)
  created_at: 2022-08-05 13:11:43+00:00
  edited: false
  hidden: false
  id: 62ed251f315dd23708e3b455
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg?w=200&h=200&f=face
      fullname: Xing Han Lu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xhluca
      type: user
    createdAt: '2022-08-10T19:02:40.000Z'
    data:
      edited: false
      editors:
      - xhluca
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg?w=200&h=200&f=face
          fullname: Xing Han Lu
          isHf: false
          isPro: false
          name: xhluca
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;lysandre&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lysandre\">@<span class=\"\
          underline\">lysandre</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;sgugger&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/sgugger\">@<span class=\"underline\">sgugger</span></a></span>\n\
          \n\t</span></span> Will the <code>accelerate</code> library only handle\
          \ Vertical MP, or will it eventually incorporate Pipeline Parallelism (PP)\
          \ (based on this <a href=\"https://huggingface.co/docs/transformers/v4.16.2/en/parallelism#naive-model-parallelism-vertical-and-pipeline-parallelism\"\
          >blog post</a>)?</p>\n"
        raw: '@lysandre @sgugger Will the `accelerate` library only handle Vertical
          MP, or will it eventually incorporate Pipeline Parallelism (PP) (based on
          this [blog post](https://huggingface.co/docs/transformers/v4.16.2/en/parallelism#naive-model-parallelism-vertical-and-pipeline-parallelism))?'
        updatedAt: '2022-08-10T19:02:40.351Z'
      numEdits: 0
      reactions: []
    id: 62f400d01f30a5ac0ecbbcad
    type: comment
  author: xhluca
  content: '@lysandre @sgugger Will the `accelerate` library only handle Vertical
    MP, or will it eventually incorporate Pipeline Parallelism (PP) (based on this
    [blog post](https://huggingface.co/docs/transformers/v4.16.2/en/parallelism#naive-model-parallelism-vertical-and-pipeline-parallelism))?'
  created_at: 2022-08-10 18:02:40+00:00
  edited: false
  hidden: false
  id: 62f400d01f30a5ac0ecbbcad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6095434f5719067a614db156/XLuj4KUuz0lxL5m69yHy8.jpeg?w=200&h=200&f=face
      fullname: Jules Gagnon-Marchand
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JulesGM
      type: user
    createdAt: '2022-08-10T19:17:10.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6095434f5719067a614db156/XLuj4KUuz0lxL5m69yHy8.jpeg?w=200&h=200&f=face
          fullname: Jules Gagnon-Marchand
          isHf: false
          isPro: false
          name: JulesGM
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-08-10T19:48:53.078Z'
      numEdits: 2
      reactions: []
    id: 62f40436d3057da2dff457fd
    type: comment
  author: JulesGM
  content: This comment has been hidden
  created_at: 2022-08-10 18:17:10+00:00
  edited: true
  hidden: true
  id: 62f40436d3057da2dff457fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6095434f5719067a614db156/XLuj4KUuz0lxL5m69yHy8.jpeg?w=200&h=200&f=face
      fullname: Jules Gagnon-Marchand
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JulesGM
      type: user
    createdAt: '2022-08-10T19:17:35.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6095434f5719067a614db156/XLuj4KUuz0lxL5m69yHy8.jpeg?w=200&h=200&f=face
          fullname: Jules Gagnon-Marchand
          isHf: false
          isPro: false
          name: JulesGM
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-08-10T19:25:16.533Z'
      numEdits: 0
      reactions: []
    id: 62f4044f75277836ab9f161c
    type: comment
  author: JulesGM
  content: This comment has been hidden
  created_at: 2022-08-10 18:17:35+00:00
  edited: true
  hidden: true
  id: 62f4044f75277836ab9f161c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6095434f5719067a614db156/XLuj4KUuz0lxL5m69yHy8.jpeg?w=200&h=200&f=face
      fullname: Jules Gagnon-Marchand
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JulesGM
      type: user
    createdAt: '2022-08-10T19:26:22.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6095434f5719067a614db156/XLuj4KUuz0lxL5m69yHy8.jpeg?w=200&h=200&f=face
          fullname: Jules Gagnon-Marchand
          isHf: false
          isPro: false
          name: JulesGM
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-08-10T19:48:58.984Z'
      numEdits: 0
      reactions: []
    id: 62f4065e3561a52aa5a2e7f0
    type: comment
  author: JulesGM
  content: This comment has been hidden
  created_at: 2022-08-10 18:26:22+00:00
  edited: true
  hidden: true
  id: 62f4065e3561a52aa5a2e7f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg?w=200&h=200&f=face
      fullname: Xing Han Lu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xhluca
      type: user
    createdAt: '2022-08-10T20:21:26.000Z'
    data:
      edited: false
      editors:
      - xhluca
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg?w=200&h=200&f=face
          fullname: Xing Han Lu
          isHf: false
          isPro: false
          name: xhluca
          type: user
        html: "<p>Actually i just read that:</p>\n<blockquote>\n<p>Few caveats to\
          \ be aware of</p>\n<ol>\n<li>Current integration doesn\u2019t support Pipeline\
          \ Parallelism of DeepSpeed.</li>\n</ol>\n</blockquote>\n<p>This issue asked\
          \ about PP, but it has been closed: <a rel=\"nofollow\" href=\"https://github.com/huggingface/accelerate/issues/537\"\
          >https://github.com/huggingface/accelerate/issues/537</a></p>\n<p>So can\
          \ I assume there's no plan to support PP in <code>accelerate</code>?</p>\n"
        raw: "Actually i just read that:\n> Few caveats to be aware of\n> 1. Current\
          \ integration doesn\u2019t support Pipeline Parallelism of DeepSpeed.\n\n\
          This issue asked about PP, but it has been closed: https://github.com/huggingface/accelerate/issues/537\n\
          \nSo can I assume there's no plan to support PP in `accelerate`?"
        updatedAt: '2022-08-10T20:21:26.032Z'
      numEdits: 0
      reactions: []
    id: 62f413465586d13c86355e3b
    type: comment
  author: xhluca
  content: "Actually i just read that:\n> Few caveats to be aware of\n> 1. Current\
    \ integration doesn\u2019t support Pipeline Parallelism of DeepSpeed.\n\nThis\
    \ issue asked about PP, but it has been closed: https://github.com/huggingface/accelerate/issues/537\n\
    \nSo can I assume there's no plan to support PP in `accelerate`?"
  created_at: 2022-08-10 19:21:26+00:00
  edited: false
  hidden: false
  id: 62f413465586d13c86355e3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
      fullname: Deepak  Kaura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkaura26
      type: user
    createdAt: '2023-07-05T18:43:56.000Z'
    data:
      edited: false
      editors:
      - deepakkaura26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9525742530822754
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
          fullname: Deepak  Kaura
          isHf: false
          isPro: false
          name: deepakkaura26
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;BobaZooba&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/BobaZooba\">@<span class=\"\
          underline\">BobaZooba</span></a></span>\n\n\t</span></span> can you help\
          \ me to know that the small guide you provided, How can I use that to run\
          \ with CPU ? </p>\n"
        raw: '@BobaZooba can you help me to know that the small guide you provided,
          How can I use that to run with CPU ? '
        updatedAt: '2023-07-05T18:43:56.138Z'
      numEdits: 0
      reactions: []
    id: 64a5b9ec67bb5f90fcdf133e
    type: comment
  author: deepakkaura26
  content: '@BobaZooba can you help me to know that the small guide you provided,
    How can I use that to run with CPU ? '
  created_at: 2023-07-05 17:43:56+00:00
  edited: false
  hidden: false
  id: 64a5b9ec67bb5f90fcdf133e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: facebook/opt-66b
repo_type: model
status: open
target_branch: null
title: Please add model.parallelize()
