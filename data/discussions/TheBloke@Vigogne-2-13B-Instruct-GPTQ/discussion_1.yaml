!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Achtung
conflicting_files: null
created_at: 2023-09-14 12:45:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df0e6eb3130577a71909a2a52bd24570.svg
      fullname: Baguette
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Achtung
      type: user
    createdAt: '2023-09-14T13:45:14.000Z'
    data:
      edited: false
      editors:
      - Achtung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8757246136665344
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df0e6eb3130577a71909a2a52bd24570.svg
          fullname: Baguette
          isHf: false
          isPro: false
          name: Achtung
          type: user
        html: "<p>Hello !</p>\n<p>I'm working on a Nvidia V100S with 32GB of VRAM\
          \ but the model takes around 36GB of memory  (it used this much memory on\
          \ Google Collab A100), which is too much to be able to load (almost as much\
          \ as bofenghuang's model). Do you have any ideas why ?</p>\n<p>I'm working\
          \ with vLLM with GPTQ support through autoGPTQ and accelerate.<br><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64915b79fa736a2ad5f189be/5i-DFfI9bAtluDmU5NN3G.png\"\
          ><img alt=\"Capture d'\xE9cran 2023-09-13 101222.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64915b79fa736a2ad5f189be/5i-DFfI9bAtluDmU5NN3G.png\"\
          ></a></p>\n"
        raw: "Hello !\r\n\r\nI'm working on a Nvidia V100S with 32GB of VRAM but the\
          \ model takes around 36GB of memory  (it used this much memory on Google\
          \ Collab A100), which is too much to be able to load (almost as much as\
          \ bofenghuang's model). Do you have any ideas why ?\r\n\r\nI'm working with\
          \ vLLM with GPTQ support through autoGPTQ and accelerate.\r\n![Capture d'\xE9\
          cran 2023-09-13 101222.png](https://cdn-uploads.huggingface.co/production/uploads/64915b79fa736a2ad5f189be/5i-DFfI9bAtluDmU5NN3G.png)\r\
          \n"
        updatedAt: '2023-09-14T13:45:14.557Z'
      numEdits: 0
      reactions: []
    id: 65030e6a51990ba05a8c2594
    type: comment
  author: Achtung
  content: "Hello !\r\n\r\nI'm working on a Nvidia V100S with 32GB of VRAM but the\
    \ model takes around 36GB of memory  (it used this much memory on Google Collab\
    \ A100), which is too much to be able to load (almost as much as bofenghuang's\
    \ model). Do you have any ideas why ?\r\n\r\nI'm working with vLLM with GPTQ support\
    \ through autoGPTQ and accelerate.\r\n![Capture d'\xE9cran 2023-09-13 101222.png](https://cdn-uploads.huggingface.co/production/uploads/64915b79fa736a2ad5f189be/5i-DFfI9bAtluDmU5NN3G.png)\r\
    \n"
  created_at: 2023-09-14 12:45:14+00:00
  edited: false
  hidden: false
  id: 65030e6a51990ba05a8c2594
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Vigogne-2-13B-Instruct-GPTQ
repo_type: model
status: open
target_branch: null
title: VRAM Usage
