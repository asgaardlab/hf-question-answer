!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sanchimittal
conflicting_files: null
created_at: 2023-09-22 03:59:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9b31d31abd784ff5fa416c2cbbdf7a42.svg
      fullname: Sanchi Mittal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchimittal
      type: user
    createdAt: '2023-09-22T04:59:10.000Z'
    data:
      edited: true
      editors:
      - sanchimittal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6975406408309937
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9b31d31abd784ff5fa416c2cbbdf7a42.svg
          fullname: Sanchi Mittal
          isHf: false
          isPro: false
          name: sanchimittal
          type: user
        html: "<p>Hello, I am using the Instruct-GPT-J model with &lt;|endoftext|&gt;\
          \ as eos_token as well as bos_token.<br>With many input texts I'm getting\
          \ only &lt;|endoftext|&gt; as output or you could say empty response, as\
          \ model generates only the input + eos_token. My generate function looks\
          \ like this:</p>\n<pre><code>gen_tokens = model.generate(\n        input_ids,\n\
          \        do_sample=True,\n        bos_token_id = 50256,\n        eos_token_id\
          \ = 50256,\n        temperature=0.7,\n        min_new_tokens = 5,\n    \
          \    max_new_tokens = 2048,\n    )\ngen_text = tokenizer.batch_decode(gen_tokens)[0]\n\
          </code></pre>\n<p>&lt;|endoftext|&gt; has token id 50256 for the tokenizer.<br>Tokenizer\
          \ and model initialization:</p>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(\"\
          ../weights/instruct-gpt-j-fp16/\", bos_token='&lt;|endoftext|&gt;', eos_token='&lt;|endoftext|&gt;',\
          \ pad_token='&lt;|pad|&gt;')\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          ../weights/instruct-gpt-j-fp16/\").cuda()\nmodel.resize_token_embeddings(len(tokenizer))\n\
          </code></pre>\n<p>Sometimes it generates good enough output text as well,\
          \ but like 2/10 times. Can someone please help with why is this issue happening\
          \ and what can I try to resolve?</p>\n"
        raw: "Hello, I am using the Instruct-GPT-J model with <|endoftext|> as eos_token\
          \ as well as bos_token. \nWith many input texts I'm getting only <|endoftext|>\
          \ as output or you could say empty response, as model generates only the\
          \ input + eos_token. My generate function looks like this:\n\n```\ngen_tokens\
          \ = model.generate(\n        input_ids,\n        do_sample=True,\n     \
          \   bos_token_id = 50256,\n        eos_token_id = 50256,\n        temperature=0.7,\n\
          \        min_new_tokens = 5,\n        max_new_tokens = 2048,\n    )\ngen_text\
          \ = tokenizer.batch_decode(gen_tokens)[0]\n```\n<|endoftext|> has token\
          \ id 50256 for the tokenizer.\nTokenizer and model initialization:\n```\n\
          tokenizer = AutoTokenizer.from_pretrained(\"../weights/instruct-gpt-j-fp16/\"\
          , bos_token='<|endoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n\
          model = AutoModelForCausalLM.from_pretrained(\"../weights/instruct-gpt-j-fp16/\"\
          ).cuda()\nmodel.resize_token_embeddings(len(tokenizer))\n```\n\nSometimes\
          \ it generates good enough output text as well, but like 2/10 times. Can\
          \ someone please help with why is this issue happening and what can I try\
          \ to resolve?"
        updatedAt: '2023-09-22T05:00:36.137Z'
      numEdits: 1
      reactions: []
    id: 650d1f1ed61f5cca20df214a
    type: comment
  author: sanchimittal
  content: "Hello, I am using the Instruct-GPT-J model with <|endoftext|> as eos_token\
    \ as well as bos_token. \nWith many input texts I'm getting only <|endoftext|>\
    \ as output or you could say empty response, as model generates only the input\
    \ + eos_token. My generate function looks like this:\n\n```\ngen_tokens = model.generate(\n\
    \        input_ids,\n        do_sample=True,\n        bos_token_id = 50256,\n\
    \        eos_token_id = 50256,\n        temperature=0.7,\n        min_new_tokens\
    \ = 5,\n        max_new_tokens = 2048,\n    )\ngen_text = tokenizer.batch_decode(gen_tokens)[0]\n\
    ```\n<|endoftext|> has token id 50256 for the tokenizer.\nTokenizer and model\
    \ initialization:\n```\ntokenizer = AutoTokenizer.from_pretrained(\"../weights/instruct-gpt-j-fp16/\"\
    , bos_token='<|endoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n\
    model = AutoModelForCausalLM.from_pretrained(\"../weights/instruct-gpt-j-fp16/\"\
    ).cuda()\nmodel.resize_token_embeddings(len(tokenizer))\n```\n\nSometimes it generates\
    \ good enough output text as well, but like 2/10 times. Can someone please help\
    \ with why is this issue happening and what can I try to resolve?"
  created_at: 2023-09-22 03:59:10+00:00
  edited: true
  hidden: false
  id: 650d1f1ed61f5cca20df214a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: nlpcloud/instruct-gpt-j-fp16
repo_type: model
status: open
target_branch: null
title: Empty responses / Zero new tokens generated as output
