!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gsaivinay
conflicting_files: null
created_at: 2023-03-21 13:18:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-03-21T14:18:40.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<p>This is awesome. Thanks for providing this model. I was looking
          for such a model that is trained on instructions and this works very good.
          Is it possible to provide the original training script? or at least the
          data preprocessing scripts (involving split to chunks, converting to tokens
          etc..). So that I can fine tune this model in the same way for my use cases.</p>

          '
        raw: This is awesome. Thanks for providing this model. I was looking for such
          a model that is trained on instructions and this works very good. Is it
          possible to provide the original training script? or at least the data preprocessing
          scripts (involving split to chunks, converting to tokens etc..). So that
          I can fine tune this model in the same way for my use cases.
        updatedAt: '2023-03-21T14:18:40.256Z'
      numEdits: 0
      reactions: []
    id: 6419bcc0ed725fef6443e49d
    type: comment
  author: gsaivinay
  content: This is awesome. Thanks for providing this model. I was looking for such
    a model that is trained on instructions and this works very good. Is it possible
    to provide the original training script? or at least the data preprocessing scripts
    (involving split to chunks, converting to tokens etc..). So that I can fine tune
    this model in the same way for my use cases.
  created_at: 2023-03-21 13:18:40+00:00
  edited: false
  hidden: false
  id: 6419bcc0ed725fef6443e49d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678959361486-6412e2653e3f06f67efaf21d.jpeg?w=200&h=200&f=face
      fullname: Julien Salinas
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juliensalinas
      type: user
    createdAt: '2023-03-22T07:56:11.000Z'
    data:
      edited: false
      editors:
      - juliensalinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678959361486-6412e2653e3f06f67efaf21d.jpeg?w=200&h=200&f=face
          fullname: Julien Salinas
          isHf: false
          isPro: false
          name: juliensalinas
          type: user
        html: '<p>Thanks, great to see it''s useful!<br>Here is the dataset I used:
          <a href="https://huggingface.co/datasets/nlpcloud/instructions-dataset-adapted-from-stanford-alpaca-for-gpt-j">https://huggingface.co/datasets/nlpcloud/instructions-dataset-adapted-from-stanford-alpaca-for-gpt-j</a><br>Here''s
          the repo I used for the fine-tuning: <a rel="nofollow" href="https://github.com/kingoflolz/mesh-transformer-jax">https://github.com/kingoflolz/mesh-transformer-jax</a>.
          I used 5 epochs, and apart from that I kept the default parameters in the
          repo.</p>

          '
        raw: 'Thanks, great to see it''s useful!

          Here is the dataset I used: https://huggingface.co/datasets/nlpcloud/instructions-dataset-adapted-from-stanford-alpaca-for-gpt-j

          Here''s the repo I used for the fine-tuning: https://github.com/kingoflolz/mesh-transformer-jax.
          I used 5 epochs, and apart from that I kept the default parameters in the
          repo.'
        updatedAt: '2023-03-22T07:56:11.244Z'
      numEdits: 0
      reactions: []
    id: 641ab49bb84e86f4c7450e86
    type: comment
  author: juliensalinas
  content: 'Thanks, great to see it''s useful!

    Here is the dataset I used: https://huggingface.co/datasets/nlpcloud/instructions-dataset-adapted-from-stanford-alpaca-for-gpt-j

    Here''s the repo I used for the fine-tuning: https://github.com/kingoflolz/mesh-transformer-jax.
    I used 5 epochs, and apart from that I kept the default parameters in the repo.'
  created_at: 2023-03-22 06:56:11+00:00
  edited: false
  hidden: false
  id: 641ab49bb84e86f4c7450e86
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-03-22T10:57:13.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<p>Thanks for your reply. Much appreciated. Could you please answer
          few more questions for me? While training, did you freeze any layers? and
          what kind of hardware/software config is used (like GPU VRAM, CPU Memory
          etc..)</p>

          <p>I''ve initially trained base GPT-J model in fp-16 by freezing all layers
          and unfreezing top <code>n</code> layers, which provided decent results
          for my use case and data, but unable to train entire model on a single GPU
          with 24GB VRAM. Looking into AWS sagemaker SMP.</p>

          '
        raw: 'Thanks for your reply. Much appreciated. Could you please answer few
          more questions for me? While training, did you freeze any layers? and what
          kind of hardware/software config is used (like GPU VRAM, CPU Memory etc..)


          I''ve initially trained base GPT-J model in fp-16 by freezing all layers
          and unfreezing top `n` layers, which provided decent results for my use
          case and data, but unable to train entire model on a single GPU with 24GB
          VRAM. Looking into AWS sagemaker SMP.'
        updatedAt: '2023-03-22T10:57:13.672Z'
      numEdits: 0
      reactions: []
    id: 641adf09758193999712d74e
    type: comment
  author: gsaivinay
  content: 'Thanks for your reply. Much appreciated. Could you please answer few more
    questions for me? While training, did you freeze any layers? and what kind of
    hardware/software config is used (like GPU VRAM, CPU Memory etc..)


    I''ve initially trained base GPT-J model in fp-16 by freezing all layers and unfreezing
    top `n` layers, which provided decent results for my use case and data, but unable
    to train entire model on a single GPU with 24GB VRAM. Looking into AWS sagemaker
    SMP.'
  created_at: 2023-03-22 09:57:13+00:00
  edited: false
  hidden: false
  id: 641adf09758193999712d74e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678959361486-6412e2653e3f06f67efaf21d.jpeg?w=200&h=200&f=face
      fullname: Julien Salinas
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juliensalinas
      type: user
    createdAt: '2023-03-22T14:48:44.000Z'
    data:
      edited: false
      editors:
      - juliensalinas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678959361486-6412e2653e3f06f67efaf21d.jpeg?w=200&h=200&f=face
          fullname: Julien Salinas
          isHf: false
          isPro: false
          name: juliensalinas
          type: user
        html: '<p>You''re welcome.<br>No I did not freeze any layer. I fine-tuned
          the model on a TPU v3-8 , and followed this guide: <a rel="nofollow" href="https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md">https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md</a><br>The
          fine-tuning took around one hour.</p>

          '
        raw: 'You''re welcome.

          No I did not freeze any layer. I fine-tuned the model on a TPU v3-8 , and
          followed this guide: https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md

          The fine-tuning took around one hour.'
        updatedAt: '2023-03-22T14:48:44.330Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - gsaivinay
    id: 641b154cfc01c26fcaedb5d6
    type: comment
  author: juliensalinas
  content: 'You''re welcome.

    No I did not freeze any layer. I fine-tuned the model on a TPU v3-8 , and followed
    this guide: https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md

    The fine-tuning took around one hour.'
  created_at: 2023-03-22 13:48:44+00:00
  edited: false
  hidden: false
  id: 641b154cfc01c26fcaedb5d6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: nlpcloud/instruct-gpt-j-fp16
repo_type: model
status: open
target_branch: null
title: Awesome. Request for training script.
