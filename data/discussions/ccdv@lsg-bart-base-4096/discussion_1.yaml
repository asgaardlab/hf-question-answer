!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rhussain21
conflicting_files: null
created_at: 2023-04-02 17:59:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/acb3e66ec28fe1e133050b5d997a53dd.svg
      fullname: Redwan Hussain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rhussain21
      type: user
    createdAt: '2023-04-02T18:59:32.000Z'
    data:
      edited: false
      editors:
      - rhussain21
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/acb3e66ec28fe1e133050b5d997a53dd.svg
          fullname: Redwan Hussain
          isHf: false
          isPro: false
          name: rhussain21
          type: user
        html: '<p>Hi! I need help implementing this model. I followed the guidelines
          on how to use it to encode input texts... however, I''m stuck on decoding.
          model.generate() isn''t compatible with this. Any sample code on how to
          use this for summary generation? I''d like to convert tensors into actual
          text as with the regular bart</p>

          '
        raw: Hi! I need help implementing this model. I followed the guidelines on
          how to use it to encode input texts... however, I'm stuck on decoding. model.generate()
          isn't compatible with this. Any sample code on how to use this for summary
          generation? I'd like to convert tensors into actual text as with the regular
          bart
        updatedAt: '2023-04-02T18:59:32.742Z'
      numEdits: 0
      reactions: []
    id: 6429d0948852afdf89bf8efe
    type: comment
  author: rhussain21
  content: Hi! I need help implementing this model. I followed the guidelines on how
    to use it to encode input texts... however, I'm stuck on decoding. model.generate()
    isn't compatible with this. Any sample code on how to use this for summary generation?
    I'd like to convert tensors into actual text as with the regular bart
  created_at: 2023-04-02 17:59:32+00:00
  edited: false
  hidden: false
  id: 6429d0948852afdf89bf8efe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659877476012-616f3a4cf92cbd8e03c1d541.jpeg?w=200&h=200&f=face
      fullname: ccdv
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ccdv
      type: user
    createdAt: '2023-04-03T18:05:15.000Z'
    data:
      edited: false
      editors:
      - ccdv
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659877476012-616f3a4cf92cbd8e03c1d541.jpeg?w=200&h=200&f=face
          fullname: ccdv
          isHf: false
          isPro: false
          name: ccdv
          type: user
        html: "<p>hi <span data-props=\"{&quot;user&quot;:&quot;rhussain21&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rhussain21\"\
          >@<span class=\"underline\">rhussain21</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>This model isn't fine tuned, so you can only predict a mask.</p>\n\
          <pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,\
          \ pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"ccdv/lsg-bart-base-4096\"\
          , trust_remote_code=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"\
          ccdv/lsg-bart-base-4096\", trust_remote_code=True)\n\ntext = \"Paris is\
          \ the &lt;mask&gt; of France.\"\n\n# With fill-mask pipeline\npipe = pipeline(\n\
          \  \"fill-mask\", \n  model=model, \n  tokenizer=tokenizer) # for gpu\n\n\
          predictions = pipe(text)\n\npredictions\n</code></pre>\n<p>You can also\
          \ use \"text2text-generation\" pipeline for generation/summarization (see:\
          \ <a href=\"https://huggingface.co/ccdv/lsg-bart-base-4096-multinews\">https://huggingface.co/ccdv/lsg-bart-base-4096-multinews</a>)</p>\n"
        raw: "hi @rhussain21 \n\nThis model isn't fine tuned, so you can only predict\
          \ a mask.\n\n```\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM,\
          \ pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"ccdv/lsg-bart-base-4096\"\
          , trust_remote_code=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"\
          ccdv/lsg-bart-base-4096\", trust_remote_code=True)\n\ntext = \"Paris is\
          \ the <mask> of France.\"\n\n# With fill-mask pipeline\npipe = pipeline(\n\
          \  \"fill-mask\", \n  model=model, \n  tokenizer=tokenizer) # for gpu\n\n\
          predictions = pipe(text)\n\npredictions\n```\n\nYou can also use \"text2text-generation\"\
          \ pipeline for generation/summarization (see: https://huggingface.co/ccdv/lsg-bart-base-4096-multinews)"
        updatedAt: '2023-04-03T18:05:15.310Z'
      numEdits: 0
      reactions: []
    id: 642b155b0a586b371e08ec37
    type: comment
  author: ccdv
  content: "hi @rhussain21 \n\nThis model isn't fine tuned, so you can only predict\
    \ a mask.\n\n```\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM,\
    \ pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"ccdv/lsg-bart-base-4096\"\
    , trust_remote_code=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ccdv/lsg-bart-base-4096\"\
    , trust_remote_code=True)\n\ntext = \"Paris is the <mask> of France.\"\n\n# With\
    \ fill-mask pipeline\npipe = pipeline(\n  \"fill-mask\", \n  model=model, \n \
    \ tokenizer=tokenizer) # for gpu\n\npredictions = pipe(text)\n\npredictions\n\
    ```\n\nYou can also use \"text2text-generation\" pipeline for generation/summarization\
    \ (see: https://huggingface.co/ccdv/lsg-bart-base-4096-multinews)"
  created_at: 2023-04-03 17:05:15+00:00
  edited: false
  hidden: false
  id: 642b155b0a586b371e08ec37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff2dd194f7faa850a0410f93735280b8.svg
      fullname: X5XJU0gG#nHlj0TqgJlp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lefnire
      type: user
    createdAt: '2023-06-10T23:43:53.000Z'
    data:
      edited: false
      editors:
      - lefnire
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.814889132976532
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff2dd194f7faa850a0410f93735280b8.svg
          fullname: X5XJU0gG#nHlj0TqgJlp
          isHf: false
          isPro: false
          name: lefnire
          type: user
        html: "<p>Wanna expand on the question some to make sure I'm understanding\
          \ the answer correctly. I tried fine-tuning my own LSG model, using the\
          \ code from <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing#scrollTo=tLM3niQqhEzP\"\
          >LED training</a>. After the model was trained, I tried:</p>\n<pre><code\
          \ class=\"language-python\">input_tokens = tokenizer.encode(\n    input_text,\n\
          \    return_tensors=<span class=\"hljs-string\">'pt'</span>,\n    max_length=encoder_max_length,\n\
          \    padding=<span class=\"hljs-string\">\"max_length\"</span>,\n    truncation=<span\
          \ class=\"hljs-literal\">True</span>\n)\noutput_tokens = model.generate(input_tokens)\n\
          output_text = tokenizer.decode(output_tokens[<span class=\"hljs-number\"\
          >0</span>])\n</code></pre>\n<p>But it hangs, I'm assuming because as mentioned\
          \ above <code>.generate()</code> isn't compatible with this model. However\
          \ when I do the following: </p>\n<pre><code class=\"language-python\">inputs\
          \ = tokenizer(...) <span class=\"hljs-comment\"># not tokenizer.encode</span>\n\
          outputs = model(**inputs)\noutput_tokens = torch.argmax(outputs.logits,\
          \ dim=-<span class=\"hljs-number\">1</span>)\nresult = tokenizer.decode(output_tokens[<span\
          \ class=\"hljs-number\">0</span>])\n</code></pre>\n<p>Then it works, but\
          \ the quality is poor. I'm a bit new to fine-tuning; when I tried the <code>model(**inputs)</code>\
          \ in the LED example, it similarly suffered from poor quality; so there's\
          \ a sense in which I think <code>model.generate()</code> would do better\
          \ here with LSG, if I could get it to work.</p>\n<p>I'm also wondering:\
          \ is this model not meant to be fine-tuned, and instead we're intended to\
          \ <a rel=\"nofollow\" href=\"https://github.com/ccdv-ai/convert_checkpoint_to_lsg\"\
          >convert_checkpoint</a> a fine-tuned other-model (LED)?</p>\n"
        raw: "Wanna expand on the question some to make sure I'm understanding the\
          \ answer correctly. I tried fine-tuning my own LSG model, using the code\
          \ from [LED training](https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing#scrollTo=tLM3niQqhEzP).\
          \ After the model was trained, I tried:\n```python\ninput_tokens = tokenizer.encode(\n\
          \    input_text,\n    return_tensors='pt',\n    max_length=encoder_max_length,\n\
          \    padding=\"max_length\",\n    truncation=True\n)\noutput_tokens = model.generate(input_tokens)\n\
          output_text = tokenizer.decode(output_tokens[0])\n```\nBut it hangs, I'm\
          \ assuming because as mentioned above `.generate()` isn't compatible with\
          \ this model. However when I do the following: \n```python\ninputs = tokenizer(...)\
          \ # not tokenizer.encode\noutputs = model(**inputs)\noutput_tokens = torch.argmax(outputs.logits,\
          \ dim=-1)\nresult = tokenizer.decode(output_tokens[0])\n```\nThen it works,\
          \ but the quality is poor. I'm a bit new to fine-tuning; when I tried the\
          \ `model(**inputs)` in the LED example, it similarly suffered from poor\
          \ quality; so there's a sense in which I think `model.generate()` would\
          \ do better here with LSG, if I could get it to work.\n\nI'm also wondering:\
          \ is this model not meant to be fine-tuned, and instead we're intended to\
          \ [convert_checkpoint](https://github.com/ccdv-ai/convert_checkpoint_to_lsg)\
          \ a fine-tuned other-model (LED)?"
        updatedAt: '2023-06-10T23:43:53.444Z'
      numEdits: 0
      reactions: []
    id: 64850ab9de06719db2e74618
    type: comment
  author: lefnire
  content: "Wanna expand on the question some to make sure I'm understanding the answer\
    \ correctly. I tried fine-tuning my own LSG model, using the code from [LED training](https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing#scrollTo=tLM3niQqhEzP).\
    \ After the model was trained, I tried:\n```python\ninput_tokens = tokenizer.encode(\n\
    \    input_text,\n    return_tensors='pt',\n    max_length=encoder_max_length,\n\
    \    padding=\"max_length\",\n    truncation=True\n)\noutput_tokens = model.generate(input_tokens)\n\
    output_text = tokenizer.decode(output_tokens[0])\n```\nBut it hangs, I'm assuming\
    \ because as mentioned above `.generate()` isn't compatible with this model. However\
    \ when I do the following: \n```python\ninputs = tokenizer(...) # not tokenizer.encode\n\
    outputs = model(**inputs)\noutput_tokens = torch.argmax(outputs.logits, dim=-1)\n\
    result = tokenizer.decode(output_tokens[0])\n```\nThen it works, but the quality\
    \ is poor. I'm a bit new to fine-tuning; when I tried the `model(**inputs)` in\
    \ the LED example, it similarly suffered from poor quality; so there's a sense\
    \ in which I think `model.generate()` would do better here with LSG, if I could\
    \ get it to work.\n\nI'm also wondering: is this model not meant to be fine-tuned,\
    \ and instead we're intended to [convert_checkpoint](https://github.com/ccdv-ai/convert_checkpoint_to_lsg)\
    \ a fine-tuned other-model (LED)?"
  created_at: 2023-06-10 22:43:53+00:00
  edited: false
  hidden: false
  id: 64850ab9de06719db2e74618
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff2dd194f7faa850a0410f93735280b8.svg
      fullname: X5XJU0gG#nHlj0TqgJlp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lefnire
      type: user
    createdAt: '2023-06-10T23:52:57.000Z'
    data:
      edited: false
      editors:
      - lefnire
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9637124538421631
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff2dd194f7faa850a0410f93735280b8.svg
          fullname: X5XJU0gG#nHlj0TqgJlp
          isHf: false
          isPro: false
          name: lefnire
          type: user
        html: '<p>Do you happen to have a sample training script handy? It strikes
          me that using the LED training code here isn''t wise, given the difference
          in handling global attention (they zero it all out, then attend the first
          special token). My trained LSG model "works" (with the above caveats) when
          I use that code; as well as when I remove all custom handling of global
          attention, but I fear I''m working against it.</p>

          '
        raw: Do you happen to have a sample training script handy? It strikes me that
          using the LED training code here isn't wise, given the difference in handling
          global attention (they zero it all out, then attend the first special token).
          My trained LSG model "works" (with the above caveats) when I use that code;
          as well as when I remove all custom handling of global attention, but I
          fear I'm working against it.
        updatedAt: '2023-06-10T23:52:57.667Z'
      numEdits: 0
      reactions: []
    id: 64850cd96bc1cfb5dc263189
    type: comment
  author: lefnire
  content: Do you happen to have a sample training script handy? It strikes me that
    using the LED training code here isn't wise, given the difference in handling
    global attention (they zero it all out, then attend the first special token).
    My trained LSG model "works" (with the above caveats) when I use that code; as
    well as when I remove all custom handling of global attention, but I fear I'm
    working against it.
  created_at: 2023-06-10 22:52:57+00:00
  edited: false
  hidden: false
  id: 64850cd96bc1cfb5dc263189
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659877476012-616f3a4cf92cbd8e03c1d541.jpeg?w=200&h=200&f=face
      fullname: ccdv
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ccdv
      type: user
    createdAt: '2023-08-05T21:05:17.000Z'
    data:
      edited: false
      editors:
      - ccdv
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.706805944442749
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659877476012-616f3a4cf92cbd8e03c1d541.jpeg?w=200&h=200&f=face
          fullname: ccdv
          isHf: false
          isPro: false
          name: ccdv
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;lefnire&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lefnire\">@<span class=\"\
          underline\">lefnire</span></a></span>\n\n\t</span></span><br>I didn't see\
          \ your comment.</p>\n<p>The best way to fine tune this kind of model is\
          \ to use an <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization\"\
          >example script</a> from the transformers library.<br>You have to use the\
          \ <code>AutoModelForConditionalGeneration</code> to fine tune on a generative\
          \ task.</p>\n<p>To make predictions, it is better to use a pipeline object\
          \ like that:</p>\n<pre><code class=\"language-python\">tokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"ccdv/lsg-bart-base-4096-mediasum\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"ccdv/lsg-bart-base-4096-mediasum\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n\ntext = <span class=\"hljs-string\"\
          >\"Replace by what you want.\"</span>\npipe = pipeline(<span class=\"hljs-string\"\
          >\"text2text-generation\"</span>, model=model, tokenizer=tokenizer, device=<span\
          \ class=\"hljs-number\">0</span>)\ngenerated_text = pipe(\n  text, \n  truncation=<span\
          \ class=\"hljs-literal\">True</span>, \n  max_length=<span class=\"hljs-number\"\
          >64</span>, \n  no_repeat_ngram_size=<span class=\"hljs-number\">7</span>,\n\
          \  num_beams=<span class=\"hljs-number\">2</span>,\n  early_stopping=<span\
          \ class=\"hljs-literal\">True</span>\n  )\n</code></pre>\n<p>LED requires\
          \ to set global tokens manually which is not the case with this model.</p>\n"
        raw: "Hi @lefnire \nI didn't see your comment.\n\nThe best way to fine tune\
          \ this kind of model is to use an [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)\
          \ from the transformers library.\nYou have to use the `AutoModelForConditionalGeneration`\
          \ to fine tune on a generative task.\n\nTo make predictions, it is better\
          \ to use a pipeline object like that:\n\n```python\ntokenizer = AutoTokenizer.from_pretrained(\"\
          ccdv/lsg-bart-base-4096-mediasum\", trust_remote_code=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"\
          ccdv/lsg-bart-base-4096-mediasum\", trust_remote_code=True)\n\ntext = \"\
          Replace by what you want.\"\npipe = pipeline(\"text2text-generation\", model=model,\
          \ tokenizer=tokenizer, device=0)\ngenerated_text = pipe(\n  text, \n  truncation=True,\
          \ \n  max_length=64, \n  no_repeat_ngram_size=7,\n  num_beams=2,\n  early_stopping=True\n\
          \  )\n```\nLED requires to set global tokens manually which is not the case\
          \ with this model."
        updatedAt: '2023-08-05T21:05:17.847Z'
      numEdits: 0
      reactions: []
    id: 64ceb98d5c86caf951c8c6e7
    type: comment
  author: ccdv
  content: "Hi @lefnire \nI didn't see your comment.\n\nThe best way to fine tune\
    \ this kind of model is to use an [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)\
    \ from the transformers library.\nYou have to use the `AutoModelForConditionalGeneration`\
    \ to fine tune on a generative task.\n\nTo make predictions, it is better to use\
    \ a pipeline object like that:\n\n```python\ntokenizer = AutoTokenizer.from_pretrained(\"\
    ccdv/lsg-bart-base-4096-mediasum\", trust_remote_code=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"\
    ccdv/lsg-bart-base-4096-mediasum\", trust_remote_code=True)\n\ntext = \"Replace\
    \ by what you want.\"\npipe = pipeline(\"text2text-generation\", model=model,\
    \ tokenizer=tokenizer, device=0)\ngenerated_text = pipe(\n  text, \n  truncation=True,\
    \ \n  max_length=64, \n  no_repeat_ngram_size=7,\n  num_beams=2,\n  early_stopping=True\n\
    \  )\n```\nLED requires to set global tokens manually which is not the case with\
    \ this model."
  created_at: 2023-08-05 20:05:17+00:00
  edited: false
  hidden: false
  id: 64ceb98d5c86caf951c8c6e7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ccdv/lsg-bart-base-4096
repo_type: model
status: open
target_branch: null
title: Need help
