!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rkow
conflicting_files: null
created_at: 2023-11-07 14:43:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0fdec490a61a9034159f1182037d7e8c.svg
      fullname: Radoslaw Kowalczyk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rkow
      type: user
    createdAt: '2023-11-07T14:43:51.000Z'
    data:
      edited: false
      editors:
      - rkow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8450514674186707
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0fdec490a61a9034159f1182037d7e8c.svg
          fullname: Radoslaw Kowalczyk
          isHf: false
          isPro: false
          name: rkow
          type: user
        html: '<p>Are you able to share any other information about the dataset used
          to fine-tune the BLING models?</p>

          <p>Many thanks</p>

          '
        raw: "Are you able to share any other information about the dataset used to\
          \ fine-tune the BLING models?\r\n\r\nMany thanks"
        updatedAt: '2023-11-07T14:43:51.851Z'
      numEdits: 0
      reactions: []
    id: 654a4d276167ff03f7ffa4cc
    type: comment
  author: rkow
  content: "Are you able to share any other information about the dataset used to\
    \ fine-tune the BLING models?\r\n\r\nMany thanks"
  created_at: 2023-11-07 14:43:51+00:00
  edited: false
  hidden: false
  id: 654a4d276167ff03f7ffa4cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e813b3bdc5ab53bcffc5f9d6b3eeb084.svg
      fullname: Darren Oberst
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: doberst
      type: user
    createdAt: '2023-11-07T17:53:33.000Z'
    data:
      edited: false
      editors:
      - doberst
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9419937133789062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e813b3bdc5ab53bcffc5f9d6b3eeb084.svg
          fullname: Darren Oberst
          isHf: false
          isPro: false
          name: doberst
          type: user
        html: '<p>Thanks for your question - Yes, the BLING models are fine-tuned
          on a custom instruct dataset that from a domain point of view consists primarily
          of legal, finance, regulatory, and general business content.</p>

          '
        raw: Thanks for your question - Yes, the BLING models are fine-tuned on a
          custom instruct dataset that from a domain point of view consists primarily
          of legal, finance, regulatory, and general business content.
        updatedAt: '2023-11-07T17:53:33.744Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - rkow
    id: 654a799dfa25dfb7809eee4b
    type: comment
  author: doberst
  content: Thanks for your question - Yes, the BLING models are fine-tuned on a custom
    instruct dataset that from a domain point of view consists primarily of legal,
    finance, regulatory, and general business content.
  created_at: 2023-11-07 17:53:33+00:00
  edited: false
  hidden: false
  id: 654a799dfa25dfb7809eee4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a2536ed806274bf1a4d6bf56e740a08.svg
      fullname: RS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Practical-AI
      type: user
    createdAt: '2023-11-13T10:35:46.000Z'
    data:
      edited: false
      editors:
      - Practical-AI
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6471643447875977
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a2536ed806274bf1a4d6bf56e740a08.svg
          fullname: RS
          isHf: false
          isPro: false
          name: Practical-AI
          type: user
        html: '<p>Can I specify multiple documents in the context and prompt the model
          to choose the best context and rephrase it</p>

          '
        raw: Can I specify multiple documents in the context and prompt the model
          to choose the best context and rephrase it
        updatedAt: '2023-11-13T10:35:46.803Z'
      numEdits: 0
      reactions: []
    id: 6551fc02ea9be8f1e6faa67f
    type: comment
  author: Practical-AI
  content: Can I specify multiple documents in the context and prompt the model to
    choose the best context and rephrase it
  created_at: 2023-11-13 10:35:46+00:00
  edited: false
  hidden: false
  id: 6551fc02ea9be8f1e6faa67f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e813b3bdc5ab53bcffc5f9d6b3eeb084.svg
      fullname: Darren Oberst
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: doberst
      type: user
    createdAt: '2023-11-17T11:23:03.000Z'
    data:
      edited: false
      editors:
      - doberst
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9361173510551453
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e813b3bdc5ab53bcffc5f9d6b3eeb084.svg
          fullname: Darren Oberst
          isHf: false
          isPro: false
          name: doberst
          type: user
        html: '<p>Thanks for your question on multiple documents in the context.   The
          short answer is - "results may vary" and may take a little experimentation
          to set up the prompt in the most effective way for the use case.     </p>

          <p>What will likely work the best:   (1) If you connect together a list
          of unrelated sentences, and ask a fact retrieval question contained in one
          of the sentences, then Yes, the model will generally be OK to use the applicable
          information, or (B)  If it is a relatively simple selection task, such as
          a listing of 3 sections of an agreement, such as Section 6.3, Section 6.4
          and Section 6.5, and the question is which is the right section that addresses
          a particular topic, then results should generally be OK.</p>

          <p>Where the model will struggle:   more complex open-ended selection and
          multiple choice tasks.  Models in this size range can also fail to recognize
          when a question can not be answered from aa particular context.    We have
          found it difficult to train models in the 1B-3B range to recognize these
          more complex instructions.   Please feel free to try the "llmware/dragon"
          models which are 7B parameter models that are best suited for more complex
          tasks like this.</p>

          '
        raw: "Thanks for your question on multiple documents in the context.   The\
          \ short answer is - \"results may vary\" and may take a little experimentation\
          \ to set up the prompt in the most effective way for the use case.     \n\
          \nWhat will likely work the best:   (1) If you connect together a list of\
          \ unrelated sentences, and ask a fact retrieval question contained in one\
          \ of the sentences, then Yes, the model will generally be OK to use the\
          \ applicable information, or (B)  If it is a relatively simple selection\
          \ task, such as a listing of 3 sections of an agreement, such as Section\
          \ 6.3, Section 6.4 and Section 6.5, and the question is which is the right\
          \ section that addresses a particular topic, then results should generally\
          \ be OK.\n\nWhere the model will struggle:   more complex open-ended selection\
          \ and multiple choice tasks.  Models in this size range can also fail to\
          \ recognize when a question can not be answered from aa particular context.\
          \    We have found it difficult to train models in the 1B-3B range to recognize\
          \ these more complex instructions.   Please feel free to try the \"llmware/dragon\"\
          \ models which are 7B parameter models that are best suited for more complex\
          \ tasks like this.\n"
        updatedAt: '2023-11-17T11:23:03.065Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Aavenir
    id: 65574d173541e7603477f866
    type: comment
  author: doberst
  content: "Thanks for your question on multiple documents in the context.   The short\
    \ answer is - \"results may vary\" and may take a little experimentation to set\
    \ up the prompt in the most effective way for the use case.     \n\nWhat will\
    \ likely work the best:   (1) If you connect together a list of unrelated sentences,\
    \ and ask a fact retrieval question contained in one of the sentences, then Yes,\
    \ the model will generally be OK to use the applicable information, or (B)  If\
    \ it is a relatively simple selection task, such as a listing of 3 sections of\
    \ an agreement, such as Section 6.3, Section 6.4 and Section 6.5, and the question\
    \ is which is the right section that addresses a particular topic, then results\
    \ should generally be OK.\n\nWhere the model will struggle:   more complex open-ended\
    \ selection and multiple choice tasks.  Models in this size range can also fail\
    \ to recognize when a question can not be answered from aa particular context.\
    \    We have found it difficult to train models in the 1B-3B range to recognize\
    \ these more complex instructions.   Please feel free to try the \"llmware/dragon\"\
    \ models which are 7B parameter models that are best suited for more complex tasks\
    \ like this.\n"
  created_at: 2023-11-17 11:23:03+00:00
  edited: false
  hidden: false
  id: 65574d173541e7603477f866
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a2536ed806274bf1a4d6bf56e740a08.svg
      fullname: RS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Practical-AI
      type: user
    createdAt: '2023-11-17T13:04:21.000Z'
    data:
      edited: false
      editors:
      - Practical-AI
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9714189767837524
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a2536ed806274bf1a4d6bf56e740a08.svg
          fullname: RS
          isHf: false
          isPro: false
          name: Practical-AI
          type: user
        html: '<p>Thanks for the reply. Is there a plan to release the training set?
          That would be immensely helpful</p>

          '
        raw: Thanks for the reply. Is there a plan to release the training set? That
          would be immensely helpful
        updatedAt: '2023-11-17T13:04:21.494Z'
      numEdits: 0
      reactions: []
    id: 655764d50f4493529799fe55
    type: comment
  author: Practical-AI
  content: Thanks for the reply. Is there a plan to release the training set? That
    would be immensely helpful
  created_at: 2023-11-17 13:04:21+00:00
  edited: false
  hidden: false
  id: 655764d50f4493529799fe55
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: llmware/bling-sheared-llama-2.7b-0.1
repo_type: model
status: open
target_branch: null
title: Are the BLING models fine-tuned on a finance/legal-specific dataset?
