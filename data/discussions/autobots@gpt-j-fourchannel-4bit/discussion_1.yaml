!!python/object:huggingface_hub.community.DiscussionWithDetails
author: FenixInDarkSolo
conflicting_files: null
created_at: 2023-04-29 11:52:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-04-29T12:52:27.000Z'
    data:
      edited: true
      editors:
      - FenixInDarkSolo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
          fullname: fenixlam
          isHf: false
          isPro: false
          name: FenixInDarkSolo
          type: user
        html: '<p>has checked the file''s sha256 to confirm the file is completely
          correct.</p>

          <pre><code>main -m ./models/ggml-gpt-fourchannel-q4_0.bin --n_parts 1 -t
          12 -n -1 -c 2048 --temp 0.8 --top_p 0.8 --top_k 160 --repeat_last_n 2048
          --repeat_penalty 1.176470588 --ignore-eos -f "../storyboard/answer_question.txt"

          main: seed = 1682772219

          llama.cpp: loading model from ./models/ggml-gpt-fourchannel-q4_0.bin

          error loading model: unexpectedly reached end of file

          llama_init_from_file: failed to load model

          main: error: failed to load model ''./models/ggml-gpt-fourchannel-q4_0.bin''

          time used: 6.1111602783203125

          </code></pre>

          '
        raw: 'has checked the file''s sha256 to confirm the file is completely correct.

          ```

          main -m ./models/ggml-gpt-fourchannel-q4_0.bin --n_parts 1 -t 12 -n -1 -c
          2048 --temp 0.8 --top_p 0.8 --top_k 160 --repeat_last_n 2048 --repeat_penalty
          1.176470588 --ignore-eos -f "../storyboard/answer_question.txt"

          main: seed = 1682772219

          llama.cpp: loading model from ./models/ggml-gpt-fourchannel-q4_0.bin

          error loading model: unexpectedly reached end of file

          llama_init_from_file: failed to load model

          main: error: failed to load model ''./models/ggml-gpt-fourchannel-q4_0.bin''

          time used: 6.1111602783203125

          ```'
        updatedAt: '2023-04-29T13:05:38.811Z'
      numEdits: 3
      reactions: []
    id: 644d130b6dfd5f8240d99207
    type: comment
  author: FenixInDarkSolo
  content: 'has checked the file''s sha256 to confirm the file is completely correct.

    ```

    main -m ./models/ggml-gpt-fourchannel-q4_0.bin --n_parts 1 -t 12 -n -1 -c 2048
    --temp 0.8 --top_p 0.8 --top_k 160 --repeat_last_n 2048 --repeat_penalty 1.176470588
    --ignore-eos -f "../storyboard/answer_question.txt"

    main: seed = 1682772219

    llama.cpp: loading model from ./models/ggml-gpt-fourchannel-q4_0.bin

    error loading model: unexpectedly reached end of file

    llama_init_from_file: failed to load model

    main: error: failed to load model ''./models/ggml-gpt-fourchannel-q4_0.bin''

    time used: 6.1111602783203125

    ```'
  created_at: 2023-04-29 11:52:27+00:00
  edited: true
  hidden: false
  id: 644d130b6dfd5f8240d99207
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-04-29T13:04:50.000Z'
    data:
      from: ggml-gpt4chan-q4_0.bin does not work on llama.cpp
      to: ggml-gpt-fourchannel-q4_0.bin does not work on llama.cpp
    id: 644d15f2fa94e93b0ec26d9e
    type: title-change
  author: FenixInDarkSolo
  created_at: 2023-04-29 12:04:50+00:00
  id: 644d15f2fa94e93b0ec26d9e
  new_title: ggml-gpt-fourchannel-q4_0.bin does not work on llama.cpp
  old_title: ggml-gpt4chan-q4_0.bin does not work on llama.cpp
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-01T13:06:38.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>Does llama CPP support GPT-J models? It is not listed on their github.
          On koboldCPP it comes up as legacy_gptj_model_load and it works.</p>

          <p>-model-q4_1.bin'' - please wait ...<br>legacy_gptj_model_load: n_vocab
          = 50400<br>legacy_gptj_model_load: n_ctx   = 2048<br>legacy_gptj_model_load:
          n_embd  = 4096<br>legacy_gptj_model_load: n_head  = 16<br>legacy_gptj_model_load:
          n_layer = 28<br>legacy_gptj_model_load: n_rot   = 64<br>legacy_gptj_model_load:
          f16     = 3<br>legacy_gptj_model_load: ggml ctx size = 5226.67 MB<br>legacy_gptj_model_load:
          memory_size =   896.00 MB, n_mem = 57344<br>legacy_gptj_model_load: ...................................
          done<br>legacy_gptj_model_load: model size =  4330.60 MB / num tensors =
          285<br>Load Model OK: True</p>

          '
        raw: 'Does llama CPP support GPT-J models? It is not listed on their github.
          On koboldCPP it comes up as legacy_gptj_model_load and it works.


          -model-q4_1.bin'' - please wait ...

          legacy_gptj_model_load: n_vocab = 50400

          legacy_gptj_model_load: n_ctx   = 2048

          legacy_gptj_model_load: n_embd  = 4096

          legacy_gptj_model_load: n_head  = 16

          legacy_gptj_model_load: n_layer = 28

          legacy_gptj_model_load: n_rot   = 64

          legacy_gptj_model_load: f16     = 3

          legacy_gptj_model_load: ggml ctx size = 5226.67 MB

          legacy_gptj_model_load: memory_size =   896.00 MB, n_mem = 57344

          legacy_gptj_model_load: ................................... done

          legacy_gptj_model_load: model size =  4330.60 MB / num tensors = 285

          Load Model OK: True'
        updatedAt: '2023-05-01T13:06:38.204Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - FenixInDarkSolo
    id: 644fb95e577838187ef88fad
    type: comment
  author: autobots
  content: 'Does llama CPP support GPT-J models? It is not listed on their github.
    On koboldCPP it comes up as legacy_gptj_model_load and it works.


    -model-q4_1.bin'' - please wait ...

    legacy_gptj_model_load: n_vocab = 50400

    legacy_gptj_model_load: n_ctx   = 2048

    legacy_gptj_model_load: n_embd  = 4096

    legacy_gptj_model_load: n_head  = 16

    legacy_gptj_model_load: n_layer = 28

    legacy_gptj_model_load: n_rot   = 64

    legacy_gptj_model_load: f16     = 3

    legacy_gptj_model_load: ggml ctx size = 5226.67 MB

    legacy_gptj_model_load: memory_size =   896.00 MB, n_mem = 57344

    legacy_gptj_model_load: ................................... done

    legacy_gptj_model_load: model size =  4330.60 MB / num tensors = 285

    Load Model OK: True'
  created_at: 2023-05-01 12:06:38+00:00
  edited: false
  hidden: false
  id: 644fb95e577838187ef88fad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-05-02T03:38:01.000Z'
    data:
      edited: false
      editors:
      - FenixInDarkSolo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
          fullname: fenixlam
          isHf: false
          isPro: false
          name: FenixInDarkSolo
          type: user
        html: '<p>Great! Thanks for the tips. I have tested it in koboldCPP and it
          works. This leads me to understand why some of the models didn''t work on
          llama.cpp. Thank you!!</p>

          '
        raw: Great! Thanks for the tips. I have tested it in koboldCPP and it works.
          This leads me to understand why some of the models didn't work on llama.cpp.
          Thank you!!
        updatedAt: '2023-05-02T03:38:01.424Z'
      numEdits: 0
      reactions: []
    id: 64508599577838187e099f45
    type: comment
  author: FenixInDarkSolo
  content: Great! Thanks for the tips. I have tested it in koboldCPP and it works.
    This leads me to understand why some of the models didn't work on llama.cpp. Thank
    you!!
  created_at: 2023-05-02 02:38:01+00:00
  edited: false
  hidden: false
  id: 64508599577838187e099f45
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: autobots/gpt-j-fourchannel-4bit
repo_type: model
status: open
target_branch: null
title: ggml-gpt-fourchannel-q4_0.bin does not work on llama.cpp
