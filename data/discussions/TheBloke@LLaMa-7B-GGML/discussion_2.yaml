!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jobenb
conflicting_files: null
created_at: 2023-07-17 14:10:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/040a2b765a5a3fd4bbe6261977c6e5f5.svg
      fullname: Joben Blom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jobenb
      type: user
    createdAt: '2023-07-17T15:10:24.000Z'
    data:
      edited: false
      editors:
      - jobenb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7829087376594543
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/040a2b765a5a3fd4bbe6261977c6e5f5.svg
          fullname: Joben Blom
          isHf: false
          isPro: false
          name: jobenb
          type: user
        html: '<p>Hi mate, I am getting the following error trying to create a tokenizer
          using AutoTokenizer:</p>

          <p>OSError: Can''t load tokenizer for ''TheBloke/LLaMa-7B-GGML''. If you
          were trying to load it from ''<a href="https://huggingface.co/models''">https://huggingface.co/models''</a>,
          make sure you don''t have a local directory with the same name. Otherwise,
          make sure ''TheBloke/LLaMa-7B-GGML'' is the correct path to a directory
          containing all relevant files for a LlamaTokenizerFast tokenizer.</p>

          <p>Thanks for all these models you upload this is awesome.</p>

          <p>Getting this model from the hub models api with config I can only see
          this:</p>

          <p>{<br>        "_id": "6464cfb34855e06b950548ca",<br>        "id": "TheBloke/LLaMa-13B-GGML",<br>        "likes":
          14,<br>        "private": false,<br>        "config": {<br>            "model_type":
          "llama"<br>        },<br>        "downloads": 79,<br>        "tags": [<br>            "llama",<br>            "transformers",<br>            "license:other",<br>            "text-generation-inference"<br>        ],<br>        "modelId":
          "TheBloke/LLaMa-13B-GGML"<br>    }</p>

          '
        raw: "Hi mate, I am getting the following error trying to create a tokenizer\
          \ using AutoTokenizer:\r\n\r\nOSError: Can't load tokenizer for 'TheBloke/LLaMa-7B-GGML'.\
          \ If you were trying to load it from 'https://huggingface.co/models', make\
          \ sure you don't have a local directory with the same name. Otherwise, make\
          \ sure 'TheBloke/LLaMa-7B-GGML' is the correct path to a directory containing\
          \ all relevant files for a LlamaTokenizerFast tokenizer.\r\n\r\nThanks for\
          \ all these models you upload this is awesome.\r\n\r\nGetting this model\
          \ from the hub models api with config I can only see this:\r\n\r\n{\r\n\t\
          \t\"_id\": \"6464cfb34855e06b950548ca\",\r\n\t\t\"id\": \"TheBloke/LLaMa-13B-GGML\"\
          ,\r\n\t\t\"likes\": 14,\r\n\t\t\"private\": false,\r\n\t\t\"config\": {\r\
          \n\t\t\t\"model_type\": \"llama\"\r\n\t\t},\r\n\t\t\"downloads\": 79,\r\n\
          \t\t\"tags\": [\r\n\t\t\t\"llama\",\r\n\t\t\t\"transformers\",\r\n\t\t\t\
          \"license:other\",\r\n\t\t\t\"text-generation-inference\"\r\n\t\t],\r\n\t\
          \t\"modelId\": \"TheBloke/LLaMa-13B-GGML\"\r\n\t}"
        updatedAt: '2023-07-17T15:10:24.857Z'
      numEdits: 0
      reactions: []
    id: 64b559e01fcc5b753cc9c92c
    type: comment
  author: jobenb
  content: "Hi mate, I am getting the following error trying to create a tokenizer\
    \ using AutoTokenizer:\r\n\r\nOSError: Can't load tokenizer for 'TheBloke/LLaMa-7B-GGML'.\
    \ If you were trying to load it from 'https://huggingface.co/models', make sure\
    \ you don't have a local directory with the same name. Otherwise, make sure 'TheBloke/LLaMa-7B-GGML'\
    \ is the correct path to a directory containing all relevant files for a LlamaTokenizerFast\
    \ tokenizer.\r\n\r\nThanks for all these models you upload this is awesome.\r\n\
    \r\nGetting this model from the hub models api with config I can only see this:\r\
    \n\r\n{\r\n\t\t\"_id\": \"6464cfb34855e06b950548ca\",\r\n\t\t\"id\": \"TheBloke/LLaMa-13B-GGML\"\
    ,\r\n\t\t\"likes\": 14,\r\n\t\t\"private\": false,\r\n\t\t\"config\": {\r\n\t\t\
    \t\"model_type\": \"llama\"\r\n\t\t},\r\n\t\t\"downloads\": 79,\r\n\t\t\"tags\"\
    : [\r\n\t\t\t\"llama\",\r\n\t\t\t\"transformers\",\r\n\t\t\t\"license:other\"\
    ,\r\n\t\t\t\"text-generation-inference\"\r\n\t\t],\r\n\t\t\"modelId\": \"TheBloke/LLaMa-13B-GGML\"\
    \r\n\t}"
  created_at: 2023-07-17 14:10:24+00:00
  edited: false
  hidden: false
  id: 64b559e01fcc5b753cc9c92c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-17T15:15:39.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8169169425964355
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This is a GGML model, it can''t be loaded directly from Huggingface
          Transformers code. Check out ctransformers for a library that can load GGML
          models from Python code: <a rel="nofollow" href="https://github.com/marella/ctransformers">https://github.com/marella/ctransformers</a></p>

          '
        raw: 'This is a GGML model, it can''t be loaded directly from Huggingface
          Transformers code. Check out ctransformers for a library that can load GGML
          models from Python code: https://github.com/marella/ctransformers'
        updatedAt: '2023-07-17T15:15:39.181Z'
      numEdits: 0
      reactions: []
    id: 64b55b1b1977b0f2460aacc6
    type: comment
  author: TheBloke
  content: 'This is a GGML model, it can''t be loaded directly from Huggingface Transformers
    code. Check out ctransformers for a library that can load GGML models from Python
    code: https://github.com/marella/ctransformers'
  created_at: 2023-07-17 14:15:39+00:00
  edited: false
  hidden: false
  id: 64b55b1b1977b0f2460aacc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/040a2b765a5a3fd4bbe6261977c6e5f5.svg
      fullname: Joben Blom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jobenb
      type: user
    createdAt: '2023-07-18T03:27:38.000Z'
    data:
      edited: false
      editors:
      - jobenb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5358421206474304
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/040a2b765a5a3fd4bbe6261977c6e5f5.svg
          fullname: Joben Blom
          isHf: false
          isPro: false
          name: jobenb
          type: user
        html: '<p>Thank you :)</p>

          '
        raw: Thank you :)
        updatedAt: '2023-07-18T03:27:38.793Z'
      numEdits: 0
      reactions: []
    id: 64b606aae6b0c53f97f81e0f
    type: comment
  author: jobenb
  content: Thank you :)
  created_at: 2023-07-18 02:27:38+00:00
  edited: false
  hidden: false
  id: 64b606aae6b0c53f97f81e0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a921e5fd1bcc2affe293f0c8d5ef2c8e.svg
      fullname: Gayathri K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gayathrik948
      type: user
    createdAt: '2023-07-18T12:21:50.000Z'
    data:
      edited: false
      editors:
      - gayathrik948
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5846487283706665
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a921e5fd1bcc2affe293f0c8d5ef2c8e.svg
          fullname: Gayathri K
          isHf: false
          isPro: false
          name: gayathrik948
          type: user
        html: "<h1 id=\"bring-in-deps\">Bring in deps</h1>\n<p>import streamlit as\
          \ st<br>from langchain.llms import LlamaCpp<br>from langchain.embeddings\
          \ import LlamaCppEmbeddings<br>from langchain.prompts import PromptTemplate<br>from\
          \ langchain.chains import LLMChain<br>from langchain.document_loaders import\
          \ TextLoader<br>from langchain.text_splitter import CharacterTextSplitter<br>from\
          \ langchain.vectorstores import Chroma</p>\n<h1 id=\"customize-the-layout\"\
          >Customize the layout</h1>\n<p>st.set_page_config(page_title=\"DOCAI\",\
          \ page_icon=\"\U0001F916\", layout=\"wide\", )<br>st.markdown(f\"\"\"<br>\
          \            <br>         \"\"\", unsafe_allow_html=True)</p>\n<h1 id=\"\
          function-for-writing-uploaded-file-in-temp\">function for writing uploaded\
          \ file in temp</h1>\n<p>def write_text_file(content, file_path):<br>   \
          \ try:<br>        with open(file_path, 'w') as file:<br>            file.write(content)<br>\
          \        return True<br>    except Exception as e:<br>        print(f\"\
          Error occurred while writing the file: {e}\")<br>        return False</p>\n\
          <h1 id=\"set-prompt-template\">set prompt template</h1>\n<p>prompt_template\
          \ = \"\"\"Use the following pieces of context to answer the question at\
          \ the end. If you don't know the answer, just say that you don't know, don't\
          \ try to make up an answer.</p>\n<p>{context}</p>\n<p>Question: {question}<br>Answer:\"\
          \"\"<br>prompt = PromptTemplate(template=prompt_template, input_variables=[\"\
          context\", \"question\"])</p>\n<h1 id=\"initialize-hte-llm--embeddings\"\
          >initialize hte LLM &amp; Embeddings</h1>\n<p>llm = LlamaCpp(model_path=\"\
          ./models/llama-7b.ggmlv3.q4_0.bin\")<br>embeddings = LlamaCppEmbeddings(model_path=\"\
          models/llama-7b.ggmlv3.q4_0.bin\")<br>llm_chain = LLMChain(llm=llm, prompt=prompt)</p>\n\
          <p>st.title(\"\U0001F4C4 Document Conversation \U0001F916\")<br>uploaded_file\
          \ = st.file_uploader(\"Upload an article\", type=\"txt\")</p>\n<p>if uploaded_file\
          \ is not None:<br>    content = uploaded_file.read().decode('utf-8')<br>\
          \    # st.write(content)<br>    file_path = \"temp/file.txt\"<br>    write_text_file(content,\
          \ file_path)   </p>\n<pre><code>loader = TextLoader(file_path)\ndocs = loader.load()\
          \    \ntext_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n\
          texts = text_splitter.split_documents(docs)\ndb = Chroma.from_documents(texts,\
          \ embeddings)    \nst.success(\"File Loaded Successfully!!\")\n\n# Query\
          \ through LLM    \nquestion = st.text_input(\"Ask something from the file\"\
          , placeholder=\"Find something similar to: ....this.... in the text?\",\
          \ disabled=not uploaded_file,)    \nif question:\n    similar_doc = db.similarity_search(question,\
          \ k=1)\n    context = similar_doc[0].page_content\n    query_llm = LLMChain(llm=llm,\
          \ prompt=prompt)\n    response = query_llm.run({\"context\": context, \"\
          question\": question})        \n    st.write(response)        this is the\
          \ come, how should i integrate GPU on this code\n</code></pre>\n"
        raw: "# Bring in deps\nimport streamlit as st \nfrom langchain.llms import\
          \ LlamaCpp\nfrom langchain.embeddings import LlamaCppEmbeddings\nfrom langchain.prompts\
          \ import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.document_loaders\
          \ import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\n\
          from langchain.vectorstores import Chroma\n\n\n# Customize the layout\n\
          st.set_page_config(page_title=\"DOCAI\", page_icon=\"\U0001F916\", layout=\"\
          wide\", )     \nst.markdown(f\"\"\"\n            <style>\n            .stApp\
          \ {{background-image: url(\"https://images.unsplash.com/photo-1509537257950-20f875b03669?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1469&q=80\"\
          ); \n                     background-attachment: fixed;\n              \
          \       background-size: cover}}\n         </style>\n         \"\"\", unsafe_allow_html=True)\n\
          \n# function for writing uploaded file in temp\ndef write_text_file(content,\
          \ file_path):\n    try:\n        with open(file_path, 'w') as file:\n  \
          \          file.write(content)\n        return True\n    except Exception\
          \ as e:\n        print(f\"Error occurred while writing the file: {e}\")\n\
          \        return False\n\n# set prompt template\nprompt_template = \"\"\"\
          Use the following pieces of context to answer the question at the end. If\
          \ you don't know the answer, just say that you don't know, don't try to\
          \ make up an answer.\n\n{context}\n\nQuestion: {question}\nAnswer:\"\"\"\
          \nprompt = PromptTemplate(template=prompt_template, input_variables=[\"\
          context\", \"question\"])\n\n# initialize hte LLM & Embeddings\nllm = LlamaCpp(model_path=\"\
          ./models/llama-7b.ggmlv3.q4_0.bin\")\nembeddings = LlamaCppEmbeddings(model_path=\"\
          models/llama-7b.ggmlv3.q4_0.bin\")\nllm_chain = LLMChain(llm=llm, prompt=prompt)\n\
          \nst.title(\"\U0001F4C4 Document Conversation \U0001F916\")\nuploaded_file\
          \ = st.file_uploader(\"Upload an article\", type=\"txt\")\n\nif uploaded_file\
          \ is not None:\n    content = uploaded_file.read().decode('utf-8')\n   \
          \ # st.write(content)\n    file_path = \"temp/file.txt\"\n    write_text_file(content,\
          \ file_path)   \n    \n    loader = TextLoader(file_path)\n    docs = loader.load()\
          \    \n    text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n\
          \    texts = text_splitter.split_documents(docs)\n    db = Chroma.from_documents(texts,\
          \ embeddings)    \n    st.success(\"File Loaded Successfully!!\")\n    \n\
          \    # Query through LLM    \n    question = st.text_input(\"Ask something\
          \ from the file\", placeholder=\"Find something similar to: ....this....\
          \ in the text?\", disabled=not uploaded_file,)    \n    if question:\n \
          \       similar_doc = db.similarity_search(question, k=1)\n        context\
          \ = similar_doc[0].page_content\n        query_llm = LLMChain(llm=llm, prompt=prompt)\n\
          \        response = query_llm.run({\"context\": context, \"question\": question})\
          \        \n        st.write(response)        this is the come, how should\
          \ i integrate GPU on this code\n"
        updatedAt: '2023-07-18T12:21:50.240Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - kerkathy
        - Yanni2
    id: 64b683de8bdd12e071f1e0d9
    type: comment
  author: gayathrik948
  content: "# Bring in deps\nimport streamlit as st \nfrom langchain.llms import LlamaCpp\n\
    from langchain.embeddings import LlamaCppEmbeddings\nfrom langchain.prompts import\
    \ PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.document_loaders\
    \ import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\n\
    from langchain.vectorstores import Chroma\n\n\n# Customize the layout\nst.set_page_config(page_title=\"\
    DOCAI\", page_icon=\"\U0001F916\", layout=\"wide\", )     \nst.markdown(f\"\"\"\
    \n            <style>\n            .stApp {{background-image: url(\"https://images.unsplash.com/photo-1509537257950-20f875b03669?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1469&q=80\"\
    ); \n                     background-attachment: fixed;\n                    \
    \ background-size: cover}}\n         </style>\n         \"\"\", unsafe_allow_html=True)\n\
    \n# function for writing uploaded file in temp\ndef write_text_file(content, file_path):\n\
    \    try:\n        with open(file_path, 'w') as file:\n            file.write(content)\n\
    \        return True\n    except Exception as e:\n        print(f\"Error occurred\
    \ while writing the file: {e}\")\n        return False\n\n# set prompt template\n\
    prompt_template = \"\"\"Use the following pieces of context to answer the question\
    \ at the end. If you don't know the answer, just say that you don't know, don't\
    \ try to make up an answer.\n\n{context}\n\nQuestion: {question}\nAnswer:\"\"\"\
    \nprompt = PromptTemplate(template=prompt_template, input_variables=[\"context\"\
    , \"question\"])\n\n# initialize hte LLM & Embeddings\nllm = LlamaCpp(model_path=\"\
    ./models/llama-7b.ggmlv3.q4_0.bin\")\nembeddings = LlamaCppEmbeddings(model_path=\"\
    models/llama-7b.ggmlv3.q4_0.bin\")\nllm_chain = LLMChain(llm=llm, prompt=prompt)\n\
    \nst.title(\"\U0001F4C4 Document Conversation \U0001F916\")\nuploaded_file = st.file_uploader(\"\
    Upload an article\", type=\"txt\")\n\nif uploaded_file is not None:\n    content\
    \ = uploaded_file.read().decode('utf-8')\n    # st.write(content)\n    file_path\
    \ = \"temp/file.txt\"\n    write_text_file(content, file_path)   \n    \n    loader\
    \ = TextLoader(file_path)\n    docs = loader.load()    \n    text_splitter = CharacterTextSplitter(chunk_size=100,\
    \ chunk_overlap=0)\n    texts = text_splitter.split_documents(docs)\n    db =\
    \ Chroma.from_documents(texts, embeddings)    \n    st.success(\"File Loaded Successfully!!\"\
    )\n    \n    # Query through LLM    \n    question = st.text_input(\"Ask something\
    \ from the file\", placeholder=\"Find something similar to: ....this.... in the\
    \ text?\", disabled=not uploaded_file,)    \n    if question:\n        similar_doc\
    \ = db.similarity_search(question, k=1)\n        context = similar_doc[0].page_content\n\
    \        query_llm = LLMChain(llm=llm, prompt=prompt)\n        response = query_llm.run({\"\
    context\": context, \"question\": question})        \n        st.write(response)\
    \        this is the come, how should i integrate GPU on this code\n"
  created_at: 2023-07-18 11:21:50+00:00
  edited: false
  hidden: false
  id: 64b683de8bdd12e071f1e0d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c3ca74f409c433a6b03d7786ee82483.svg
      fullname: prasanjith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prasanjith
      type: user
    createdAt: '2023-09-15T20:34:44.000Z'
    data:
      edited: false
      editors:
      - prasanjith
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9919756650924683
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c3ca74f409c433a6b03d7786ee82483.svg
          fullname: prasanjith
          isHf: false
          isPro: false
          name: prasanjith
          type: user
        html: '<p>i have changed the version of libraries and it worked </p>

          '
        raw: "i have changed the version of libraries and it worked \n"
        updatedAt: '2023-09-15T20:34:44.455Z'
      numEdits: 0
      reactions: []
    id: 6504bfe4c53e1a7f17811395
    type: comment
  author: prasanjith
  content: "i have changed the version of libraries and it worked \n"
  created_at: 2023-09-15 19:34:44+00:00
  edited: false
  hidden: false
  id: 6504bfe4c53e1a7f17811395
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24fc7bc36aa696921e0d8399e09a84d0.svg
      fullname: Viorel Mirea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmirea
      type: user
    createdAt: '2023-10-24T21:25:27.000Z'
    data:
      edited: false
      editors:
      - vmirea
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8728811144828796
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24fc7bc36aa696921e0d8399e09a84d0.svg
          fullname: Viorel Mirea
          isHf: false
          isPro: false
          name: vmirea
          type: user
        html: '<blockquote>

          <p>i have changed the version of libraries and it worked</p>

          </blockquote>

          <p>Did it work directly to use  tokenizer for ''TheBloke/CodeLlama-7B-Instruct-GGUF''
          or you used a library that  loads GGML models?</p>

          '
        raw: '> i have changed the version of libraries and it worked


          Did it work directly to use  tokenizer for ''TheBloke/CodeLlama-7B-Instruct-GGUF''
          or you used a library that  loads GGML models?'
        updatedAt: '2023-10-24T21:25:27.768Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - swvajanyatek
    id: 653836471b3eaa722dd051af
    type: comment
  author: vmirea
  content: '> i have changed the version of libraries and it worked


    Did it work directly to use  tokenizer for ''TheBloke/CodeLlama-7B-Instruct-GGUF''
    or you used a library that  loads GGML models?'
  created_at: 2023-10-24 20:25:27+00:00
  edited: false
  hidden: false
  id: 653836471b3eaa722dd051af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f79ab7f43fdc4bfd709017dae3a952e.svg
      fullname: Tekaya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sendatek
      type: user
    createdAt: '2023-11-25T15:29:04.000Z'
    data:
      edited: false
      editors:
      - Sendatek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8435800075531006
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f79ab7f43fdc4bfd709017dae3a952e.svg
          fullname: Tekaya
          isHf: false
          isPro: false
          name: Sendatek
          type: user
        html: '<p>Hi , i am also  getting a similar error trying to create a tokenizer
          using AutoTokenizer but for the ''TheBloke/Llama-2-7b-Chat-GGUF.<br>i would
          appreciate the help. Thank you.<br>OSError: Can''t load tokenizer for ''TheBloke/Llama-2-7b-Chat-GGUF''.
          If you were trying to load it from ''<a href="https://huggingface.co/models''">https://huggingface.co/models''</a>,
          make sure you don''t have a local directory with the same name. Otherwise,
          make sure ''TheBloke/Llama-2-7b-Chat-GGUF'' is the correct path to a directory
          containing all relevant files for a LlamaTokenizerFast tokenizer.</p>

          '
        raw: "Hi , i am also  getting a similar error trying to create a tokenizer\
          \ using AutoTokenizer but for the 'TheBloke/Llama-2-7b-Chat-GGUF. \ni would\
          \ appreciate the help. Thank you.\nOSError: Can't load tokenizer for 'TheBloke/Llama-2-7b-Chat-GGUF'.\
          \ If you were trying to load it from 'https://huggingface.co/models', make\
          \ sure you don't have a local directory with the same name. Otherwise, make\
          \ sure 'TheBloke/Llama-2-7b-Chat-GGUF' is the correct path to a directory\
          \ containing all relevant files for a LlamaTokenizerFast tokenizer."
        updatedAt: '2023-11-25T15:29:04.428Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - MariamD
        - Zaesar
    id: 656212c0ad639c6ab9abdfe4
    type: comment
  author: Sendatek
  content: "Hi , i am also  getting a similar error trying to create a tokenizer using\
    \ AutoTokenizer but for the 'TheBloke/Llama-2-7b-Chat-GGUF. \ni would appreciate\
    \ the help. Thank you.\nOSError: Can't load tokenizer for 'TheBloke/Llama-2-7b-Chat-GGUF'.\
    \ If you were trying to load it from 'https://huggingface.co/models', make sure\
    \ you don't have a local directory with the same name. Otherwise, make sure 'TheBloke/Llama-2-7b-Chat-GGUF'\
    \ is the correct path to a directory containing all relevant files for a LlamaTokenizerFast\
    \ tokenizer."
  created_at: 2023-11-25 15:29:04+00:00
  edited: false
  hidden: false
  id: 656212c0ad639c6ab9abdfe4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667771207517-634b059629bfb40824530d7a.jpeg?w=200&h=200&f=face
      fullname: Cesar Romero
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zaesar
      type: user
    createdAt: '2023-11-28T22:08:46.000Z'
    data:
      edited: false
      editors:
      - Zaesar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9492003321647644
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667771207517-634b059629bfb40824530d7a.jpeg?w=200&h=200&f=face
          fullname: Cesar Romero
          isHf: false
          isPro: false
          name: Zaesar
          type: user
        html: '<p>I''m having this issue with a lot of different models from Huggingface.
          And it happens to me with text-generation-webui, langchain and also now
          with axolotl. Something very weird...</p>

          '
        raw: 'I''m having this issue with a lot of different models from Huggingface.
          And it happens to me with text-generation-webui, langchain and also now
          with axolotl. Something very weird...

          '
        updatedAt: '2023-11-28T22:08:46.177Z'
      numEdits: 0
      reactions: []
    id: 656664ee162ad28c0485500b
    type: comment
  author: Zaesar
  content: 'I''m having this issue with a lot of different models from Huggingface.
    And it happens to me with text-generation-webui, langchain and also now with axolotl.
    Something very weird...

    '
  created_at: 2023-11-28 22:08:46+00:00
  edited: false
  hidden: false
  id: 656664ee162ad28c0485500b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/LLaMa-7B-GGML
repo_type: model
status: open
target_branch: null
title: Cannot create tokenizer
