!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Undi95
conflicting_files: null
created_at: 2023-09-03 13:14:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-09-03T14:14:18.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8540416955947876
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<p>Hello, just asking this, why CodeLlama ? Why not the OG Llama ?<br>Does
          the result of the loRA is good ? I aim to do a Lewd LLM only for that :
          Lewd<br>Your project interest me.</p>

          '
        raw: "Hello, just asking this, why CodeLlama ? Why not the OG Llama ?\r\n\
          Does the result of the loRA is good ? I aim to do a Lewd LLM only for that\
          \ : Lewd\r\nYour project interest me."
        updatedAt: '2023-09-03T14:14:18.758Z'
      numEdits: 0
      reactions: []
    id: 64f494bac904c16713020a7b
    type: comment
  author: Undi95
  content: "Hello, just asking this, why CodeLlama ? Why not the OG Llama ?\r\nDoes\
    \ the result of the loRA is good ? I aim to do a Lewd LLM only for that : Lewd\r\
    \nYour project interest me."
  created_at: 2023-09-03 13:14:18+00:00
  edited: false
  hidden: false
  id: 64f494bac904c16713020a7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9597e7a0deda0aba5e30759d46ff1d11.svg
      fullname: ricecake
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ricecake
      type: user
    createdAt: '2023-09-03T14:48:36.000Z'
    data:
      edited: false
      editors:
      - ricecake
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9822587966918945
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9597e7a0deda0aba5e30759d46ff1d11.svg
          fullname: ricecake
          isHf: false
          isPro: false
          name: ricecake
          type: user
        html: '<p>I also want to use the original LLaMA 2 33B for finetuning, but
          meta hasn''t released it, so, the only LLaMA 2 model with a 33B size released
          by meta is CodeLLaMA. And, for some reason, LLM community seems to think
          that the language model that trained the code is also good for RP. Is the
          logic of the code affecting the RP as well?</p>

          '
        raw: I also want to use the original LLaMA 2 33B for finetuning, but meta
          hasn't released it, so, the only LLaMA 2 model with a 33B size released
          by meta is CodeLLaMA. And, for some reason, LLM community seems to think
          that the language model that trained the code is also good for RP. Is the
          logic of the code affecting the RP as well?
        updatedAt: '2023-09-03T14:48:36.217Z'
      numEdits: 0
      reactions: []
    id: 64f49cc4017cb3e545cef196
    type: comment
  author: ricecake
  content: I also want to use the original LLaMA 2 33B for finetuning, but meta hasn't
    released it, so, the only LLaMA 2 model with a 33B size released by meta is CodeLLaMA.
    And, for some reason, LLM community seems to think that the language model that
    trained the code is also good for RP. Is the logic of the code affecting the RP
    as well?
  created_at: 2023-09-03 13:48:36+00:00
  edited: false
  hidden: false
  id: 64f49cc4017cb3e545cef196
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-09-03T15:06:39.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9266232252120972
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<p>Would it be possible to have the same train but done on Llama 2
          13B ? We really need this bro.<br>But I undertand your point then, thanks
          for the reply!</p>

          '
        raw: 'Would it be possible to have the same train but done on Llama 2 13B
          ? We really need this bro.

          But I undertand your point then, thanks for the reply!'
        updatedAt: '2023-09-03T15:06:39.621Z'
      numEdits: 0
      reactions: []
    id: 64f4a0ff32eaa56032d7ec5e
    type: comment
  author: Undi95
  content: 'Would it be possible to have the same train but done on Llama 2 13B ?
    We really need this bro.

    But I undertand your point then, thanks for the reply!'
  created_at: 2023-09-03 14:06:39+00:00
  edited: false
  hidden: false
  id: 64f4a0ff32eaa56032d7ec5e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ricecake/Codellama-Pygmalion-LoRA-Test
repo_type: model
status: open
target_branch: null
title: Why did you use CodeLlama ?
