!!python/object:huggingface_hub.community.DiscussionWithDetails
author: m18coppola
conflicting_files: null
created_at: 2023-10-03 21:04:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1OZiYaw-1OBSUBvTj55Ec.jpeg?w=200&h=200&f=face
      fullname: Michael Coppola
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: m18coppola
      type: user
    createdAt: '2023-10-03T22:04:03.000Z'
    data:
      edited: false
      editors:
      - m18coppola
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9561733603477478
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1OZiYaw-1OBSUBvTj55Ec.jpeg?w=200&h=200&f=face
          fullname: Michael Coppola
          isHf: false
          isPro: false
          name: m18coppola
          type: user
        html: '<p>Can you explain how you modified the files in the original repo
          in order to allow <code>llama.cpp/convert.py</code> to export to gguf?</p>

          <p>I noticed when loading the model in, the init log mentions an eos token
          of <code>&lt;dummy32000&gt;</code>. After doing some research on the ChatML
          format, I figured your goal is to not worry about using the added tokens
          and rely in the model''s ability to figure it out itself (which by spec
          is acceptable). It is mentioned that there could be some benefit to using
          these tokens though. I tried to make the most comprehensive changes I could
          to the original repo''s configs, making sure the added tokens for <code>&lt;|im_start|&gt;</code>
          and <code>&lt;|im_end|&gt;</code> made it into the gguf''s tokenizer. After
          some experimenting, I found that the <code>&lt;|im_end|&gt;</code> was being
          generated and caught by llama.cpp correctly, but the greedy nature of the
          tokenizer made it such that the <code>&lt;|im_start|&gt;</code> token was
          always ripped apart. I''m starting to see that this is more of an issue
          regarding llama.cpp/main not giving the user the control to inject a specific
          token into the prompt, but I know that more sophisticated API''s allow us
          to do so using the GGUF format. I''ve also got to ask if it any of it really
          matters at the end of the day as I haven''t seen good numbers on how much
          of a detriment it is to the model to not be using the added tokens anyway.</p>

          '
        raw: "Can you explain how you modified the files in the original repo in order\
          \ to allow `llama.cpp/convert.py` to export to gguf?\r\n\r\nI noticed when\
          \ loading the model in, the init log mentions an eos token of `<dummy32000>`.\
          \ After doing some research on the ChatML format, I figured your goal is\
          \ to not worry about using the added tokens and rely in the model's ability\
          \ to figure it out itself (which by spec is acceptable). It is mentioned\
          \ that there could be some benefit to using these tokens though. I tried\
          \ to make the most comprehensive changes I could to the original repo's\
          \ configs, making sure the added tokens for `<|im_start|>` and `<|im_end|>`\
          \ made it into the gguf's tokenizer. After some experimenting, I found that\
          \ the `<|im_end|>` was being generated and caught by llama.cpp correctly,\
          \ but the greedy nature of the tokenizer made it such that the `<|im_start|>`\
          \ token was always ripped apart. I'm starting to see that this is more of\
          \ an issue regarding llama.cpp/main not giving the user the control to inject\
          \ a specific token into the prompt, but I know that more sophisticated API's\
          \ allow us to do so using the GGUF format. I've also got to ask if it any\
          \ of it really matters at the end of the day as I haven't seen good numbers\
          \ on how much of a detriment it is to the model to not be using the added\
          \ tokens anyway."
        updatedAt: '2023-10-03T22:04:03.375Z'
      numEdits: 0
      reactions: []
    id: 651c8fd3a4e688e310bc21c1
    type: comment
  author: m18coppola
  content: "Can you explain how you modified the files in the original repo in order\
    \ to allow `llama.cpp/convert.py` to export to gguf?\r\n\r\nI noticed when loading\
    \ the model in, the init log mentions an eos token of `<dummy32000>`. After doing\
    \ some research on the ChatML format, I figured your goal is to not worry about\
    \ using the added tokens and rely in the model's ability to figure it out itself\
    \ (which by spec is acceptable). It is mentioned that there could be some benefit\
    \ to using these tokens though. I tried to make the most comprehensive changes\
    \ I could to the original repo's configs, making sure the added tokens for `<|im_start|>`\
    \ and `<|im_end|>` made it into the gguf's tokenizer. After some experimenting,\
    \ I found that the `<|im_end|>` was being generated and caught by llama.cpp correctly,\
    \ but the greedy nature of the tokenizer made it such that the `<|im_start|>`\
    \ token was always ripped apart. I'm starting to see that this is more of an issue\
    \ regarding llama.cpp/main not giving the user the control to inject a specific\
    \ token into the prompt, but I know that more sophisticated API's allow us to\
    \ do so using the GGUF format. I've also got to ask if it any of it really matters\
    \ at the end of the day as I haven't seen good numbers on how much of a detriment\
    \ it is to the model to not be using the added tokens anyway."
  created_at: 2023-10-03 21:04:03+00:00
  edited: false
  hidden: false
  id: 651c8fd3a4e688e310bc21c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1OZiYaw-1OBSUBvTj55Ec.jpeg?w=200&h=200&f=face
      fullname: Michael Coppola
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: m18coppola
      type: user
    createdAt: '2023-10-03T22:27:55.000Z'
    data:
      edited: false
      editors:
      - m18coppola
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.949085533618927
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1OZiYaw-1OBSUBvTj55Ec.jpeg?w=200&h=200&f=face
          fullname: Michael Coppola
          isHf: false
          isPro: false
          name: m18coppola
          type: user
        html: '<p>Never mind, I should''ve pulled the latest version of the Mistral
          repo. GGUF conversion seems to work nearly ootb now.</p>

          '
        raw: Never mind, I should've pulled the latest version of the Mistral repo.
          GGUF conversion seems to work nearly ootb now.
        updatedAt: '2023-10-03T22:27:55.317Z'
      numEdits: 0
      reactions: []
      relatedEventId: 651c956bfafb0777f208cd6b
    id: 651c956bfafb0777f208cd67
    type: comment
  author: m18coppola
  content: Never mind, I should've pulled the latest version of the Mistral repo.
    GGUF conversion seems to work nearly ootb now.
  created_at: 2023-10-03 21:27:55+00:00
  edited: false
  hidden: false
  id: 651c956bfafb0777f208cd67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1OZiYaw-1OBSUBvTj55Ec.jpeg?w=200&h=200&f=face
      fullname: Michael Coppola
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: m18coppola
      type: user
    createdAt: '2023-10-03T22:27:55.000Z'
    data:
      status: closed
    id: 651c956bfafb0777f208cd6b
    type: status-change
  author: m18coppola
  created_at: 2023-10-03 21:27:55+00:00
  id: 651c956bfafb0777f208cd6b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Mistral-7B-OpenOrca-GGUF
repo_type: model
status: closed
target_branch: null
title: dummy32000
