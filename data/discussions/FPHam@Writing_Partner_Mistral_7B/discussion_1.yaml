!!python/object:huggingface_hub.community.DiscussionWithDetails
author: drakosfire
conflicting_files: null
created_at: 2023-11-19 21:57:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1b67e5d84edfe97c13ac7645265b14a.svg
      fullname: Alan Meigs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: drakosfire
      type: user
    createdAt: '2023-11-19T21:57:34.000Z'
    data:
      edited: false
      editors:
      - drakosfire
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9932502508163452
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1b67e5d84edfe97c13ac7645265b14a.svg
          fullname: Alan Meigs
          isHf: false
          isPro: false
          name: drakosfire
          type: user
        html: '<p>Hello! I had downloaded your model and was playing with it, I quite
          like it and wanted to share it with someone to run locally on a 3080 (10
          Gb of VRAM). I was thinking about quantizing it, and learned that to quantize
          you need the original dataset it was trained on. Would you be open to quantizing
          and sharing the model or else sharing what data set you had fine tuned it
          on? </p>

          '
        raw: 'Hello! I had downloaded your model and was playing with it, I quite
          like it and wanted to share it with someone to run locally on a 3080 (10
          Gb of VRAM). I was thinking about quantizing it, and learned that to quantize
          you need the original dataset it was trained on. Would you be open to quantizing
          and sharing the model or else sharing what data set you had fine tuned it
          on? '
        updatedAt: '2023-11-19T21:57:34.263Z'
      numEdits: 0
      reactions: []
    id: 655a84ce16de0c93aebefaab
    type: comment
  author: drakosfire
  content: 'Hello! I had downloaded your model and was playing with it, I quite like
    it and wanted to share it with someone to run locally on a 3080 (10 Gb of VRAM).
    I was thinking about quantizing it, and learned that to quantize you need the
    original dataset it was trained on. Would you be open to quantizing and sharing
    the model or else sharing what data set you had fine tuned it on? '
  created_at: 2023-11-19 21:57:34+00:00
  edited: false
  hidden: false
  id: 655a84ce16de0c93aebefaab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91eb04f8a5a173a1bac4005bbaec7af7.svg
      fullname: D. Zaut
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dzaut
      type: user
    createdAt: '2023-12-30T10:28:35.000Z'
    data:
      edited: false
      editors:
      - dzaut
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6886634230613708
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91eb04f8a5a173a1bac4005bbaec7af7.svg
          fullname: D. Zaut
          isHf: false
          isPro: false
          name: dzaut
          type: user
        html: '<p><a href="https://huggingface.co/TheBloke">TheBloke</a> has quantized
          this model in:</p>

          <p><a href="https://huggingface.co/TheBloke/Writing_Partner_Mistral_7B-GGUF">GGUF</a>
          , <a href="https://huggingface.co/TheBloke/Writing_Partner_Mistral_7B-AWQ">AWQ</a>
          , and <a href="https://huggingface.co/TheBloke/Writing_Partner_Mistral_7B-GPTQ">GPTQ</a>
          formats.</p>

          <p>I''ve also quantized this model in EXL2 if you prefer that format at
          <a href="https://huggingface.co/dzaut/Writing_Partner_Mistral_7B-3.0bpw-exl2">3.0bpw</a>
          , and <a href="https://huggingface.co/dzaut/Writing_Partner_Mistral_7B-4.0bpw-exl2">4.0bpw</a>
          . I''ve been using the 4.0bpw EXL2 quant on my 3080 12gb GPU via oobabooga
          and the ExLlamav2_HF loader.</p>

          '
        raw: '[TheBloke](https://huggingface.co/TheBloke) has quantized this model
          in:


          [GGUF](https://huggingface.co/TheBloke/Writing_Partner_Mistral_7B-GGUF)
          , [AWQ](https://huggingface.co/TheBloke/Writing_Partner_Mistral_7B-AWQ)
          , and [GPTQ](https://huggingface.co/TheBloke/Writing_Partner_Mistral_7B-GPTQ)
          formats.


          I''ve also quantized this model in EXL2 if you prefer that format at [3.0bpw](https://huggingface.co/dzaut/Writing_Partner_Mistral_7B-3.0bpw-exl2)
          , and [4.0bpw](https://huggingface.co/dzaut/Writing_Partner_Mistral_7B-4.0bpw-exl2)
          . I''ve been using the 4.0bpw EXL2 quant on my 3080 12gb GPU via oobabooga
          and the ExLlamav2_HF loader.'
        updatedAt: '2023-12-30T10:28:35.682Z'
      numEdits: 0
      reactions: []
    id: 658ff0d3353318837529850b
    type: comment
  author: dzaut
  content: '[TheBloke](https://huggingface.co/TheBloke) has quantized this model in:


    [GGUF](https://huggingface.co/TheBloke/Writing_Partner_Mistral_7B-GGUF) , [AWQ](https://huggingface.co/TheBloke/Writing_Partner_Mistral_7B-AWQ)
    , and [GPTQ](https://huggingface.co/TheBloke/Writing_Partner_Mistral_7B-GPTQ)
    formats.


    I''ve also quantized this model in EXL2 if you prefer that format at [3.0bpw](https://huggingface.co/dzaut/Writing_Partner_Mistral_7B-3.0bpw-exl2)
    , and [4.0bpw](https://huggingface.co/dzaut/Writing_Partner_Mistral_7B-4.0bpw-exl2)
    . I''ve been using the 4.0bpw EXL2 quant on my 3080 12gb GPU via oobabooga and
    the ExLlamav2_HF loader.'
  created_at: 2023-12-30 10:28:35+00:00
  edited: false
  hidden: false
  id: 658ff0d3353318837529850b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: FPHam/Writing_Partner_Mistral_7B
repo_type: model
status: open
target_branch: null
title: Quantized version?
