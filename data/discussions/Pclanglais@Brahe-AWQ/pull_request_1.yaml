!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ybelkada
conflicting_files: []
created_at: 2023-11-04 10:44:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-11-04T11:44:09.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7673545479774475
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>This PR adds the transformers support for AWQ models. If you have
          AWQ kernels installed through <code>autoawq</code> package or <code>llm-awq</code>
          you can load this model directly through <code>AutoModelForCausalLM.from_pretrained</code>
          out of the box.<br>You can read more about the integration here: <a href="https://huggingface.co/docs/transformers/main_classes/quantization#awq-integration">https://huggingface.co/docs/transformers/main_classes/quantization#awq-integration</a>
          or this google colab demo: <a rel="nofollow" href="https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY">https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY</a>
          (we''ll announce it early next week)</p>

          '
        raw: 'This PR adds the transformers support for AWQ models. If you have AWQ
          kernels installed through `autoawq` package or `llm-awq` you can load this
          model directly through `AutoModelForCausalLM.from_pretrained` out of the
          box.

          You can read more about the integration here: https://huggingface.co/docs/transformers/main_classes/quantization#awq-integration
          or this google colab demo: https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY
          (we''ll announce it early next week)'
        updatedAt: '2023-11-04T11:44:09.497Z'
      numEdits: 0
      reactions: []
    id: 65462e892fe2a1e6865daa20
    type: comment
  author: ybelkada
  content: 'This PR adds the transformers support for AWQ models. If you have AWQ
    kernels installed through `autoawq` package or `llm-awq` you can load this model
    directly through `AutoModelForCausalLM.from_pretrained` out of the box.

    You can read more about the integration here: https://huggingface.co/docs/transformers/main_classes/quantization#awq-integration
    or this google colab demo: https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY
    (we''ll announce it early next week)'
  created_at: 2023-11-04 10:44:09+00:00
  edited: false
  hidden: false
  id: 65462e892fe2a1e6865daa20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-11-04T11:44:10.000Z'
    data:
      oid: 3f181d8a46880e5da4efcaeb1a73e513a577f329
      parents:
      - c87c4f9e8a437299c143ef21198e8701c85fd626
      subject: Update config.json
    id: 65462e8a0000000000000000
    type: commit
  author: ybelkada
  created_at: 2023-11-04 10:44:10+00:00
  id: 65462e8a0000000000000000
  oid: 3f181d8a46880e5da4efcaeb1a73e513a577f329
  summary: Update config.json
  type: commit
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-11-04T11:45:40.000Z'
    data:
      from: Update config.json
      to: Add transformers + AWQ inference support
    id: 65462ee47f43165b3dcef339
    type: title-change
  author: ybelkada
  created_at: 2023-11-04 10:45:40+00:00
  id: 65462ee47f43165b3dcef339
  new_title: Add transformers + AWQ inference support
  old_title: Update config.json
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ce091a9e9ca8123d7a42b0/OEPggp82RwigxNLL35LgT.jpeg?w=200&h=200&f=face
      fullname: Pierre-Carl Langlais
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Pclanglais
      type: user
    createdAt: '2023-11-04T17:53:10.000Z'
    data:
      status: merged
    id: 6546850662fae6b2a74abb93
    type: status-change
  author: Pclanglais
  created_at: 2023-11-04 16:53:10+00:00
  id: 6546850662fae6b2a74abb93
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: 6dbadc13237aebf09cd79dfb77d3954bb399603c
num: 1
repo_id: Pclanglais/Brahe-AWQ
repo_type: model
status: merged
target_branch: refs/heads/main
title: Add transformers + AWQ inference support
