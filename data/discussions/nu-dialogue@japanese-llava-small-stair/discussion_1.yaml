!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Labmem009
conflicting_files: null
created_at: 2024-01-16 03:04:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fcaa3bb720b9bbda1de4ff0ae3b3666f.svg
      fullname: Yongchi Zhu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Labmem009
      type: user
    createdAt: '2024-01-16T03:04:10.000Z'
    data:
      edited: false
      editors:
      - Labmem009
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9028154611587524
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fcaa3bb720b9bbda1de4ff0ae3b3666f.svg
          fullname: Yongchi Zhu
          isHf: false
          isPro: false
          name: Labmem009
          type: user
        html: '<p>I wonder what''s the base language model of the model? Because the
          weight is so small</p>

          '
        raw: I wonder what's the base language model of the model? Because the weight
          is so small
        updatedAt: '2024-01-16T03:04:10.460Z'
      numEdits: 0
      reactions: []
    id: 65a5f22ac128478d750ecedf
    type: comment
  author: Labmem009
  content: I wonder what's the base language model of the model? Because the weight
    is so small
  created_at: 2024-01-16 03:04:10+00:00
  edited: false
  hidden: false
  id: 65a5f22ac128478d750ecedf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e314ecbf1baaec5660ae0afe9820e47.svg
      fullname: Atsumoto Ohashi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ohashi56225
      type: user
    createdAt: '2024-01-16T03:46:15.000Z'
    data:
      edited: false
      editors:
      - ohashi56225
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.648608386516571
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e314ecbf1baaec5660ae0afe9820e47.svg
          fullname: Atsumoto Ohashi
          isHf: false
          isPro: false
          name: ohashi56225
          type: user
        html: '<p>Hi, Labmem009<br>We constructed our llava model with the following
          three modules (you can confirm this in <code>config.json</code>):</p>

          <ul>

          <li>vision encoder: <a href="https://huggingface.co/openai/clip-vit-large-patch14/tree/main">openai/clip-vit-large-patch14</a>
          (1.71 GB)</li>

          <li>vision language connecter: 2-layer MLP</li>

          <li>language model: <a href="https://huggingface.co/rinna/japanese-gpt-neox-small/tree/main">rinna/japanese-gpt-neox-small</a>
          (663 MB)</li>

          </ul>

          <p>The <code>model.safetensors</code> file in this repo contains the entire
          llava model above, so the total size should be 1.8GB.<br>FYI, in training
          this llava model, we only updated the MLP and the LM using the <a rel="nofollow"
          href="http://captions.stair.center/">stair dataset</a>.</p>

          '
        raw: 'Hi, Labmem009

          We constructed our llava model with the following three modules (you can
          confirm this in `config.json`):

          - vision encoder: [openai/clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14/tree/main)
          (1.71 GB)

          - vision language connecter: 2-layer MLP

          - language model: [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small/tree/main)
          (663 MB)


          The `model.safetensors` file in this repo contains the entire llava model
          above, so the total size should be 1.8GB.

          FYI, in training this llava model, we only updated the MLP and the LM using
          the [stair dataset](http://captions.stair.center/).'
        updatedAt: '2024-01-16T03:46:15.173Z'
      numEdits: 0
      reactions: []
    id: 65a5fc07b3dd88367f345005
    type: comment
  author: ohashi56225
  content: 'Hi, Labmem009

    We constructed our llava model with the following three modules (you can confirm
    this in `config.json`):

    - vision encoder: [openai/clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14/tree/main)
    (1.71 GB)

    - vision language connecter: 2-layer MLP

    - language model: [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small/tree/main)
    (663 MB)


    The `model.safetensors` file in this repo contains the entire llava model above,
    so the total size should be 1.8GB.

    FYI, in training this llava model, we only updated the MLP and the LM using the
    [stair dataset](http://captions.stair.center/).'
  created_at: 2024-01-16 03:46:15+00:00
  edited: false
  hidden: false
  id: 65a5fc07b3dd88367f345005
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: nu-dialogue/japanese-llava-small-stair
repo_type: model
status: open
target_branch: null
title: What's the base language model of the model?
