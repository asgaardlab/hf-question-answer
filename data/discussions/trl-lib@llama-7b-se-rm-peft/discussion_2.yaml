!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jvhoffbauer
conflicting_files: null
created_at: 2023-08-23 12:50:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d277dc4b5ea2663f6e82301bba1400b.svg
      fullname: Hoffbauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jvhoffbauer
      type: user
    createdAt: '2023-08-23T13:50:37.000Z'
    data:
      edited: false
      editors:
      - jvhoffbauer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4409024119377136
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d277dc4b5ea2663f6e82301bba1400b.svg
          fullname: Hoffbauer
          isHf: false
          isPro: false
          name: jvhoffbauer
          type: user
        html: "<p>Loading the model with </p>\n<pre><code>reward_model = AutoModelForSequenceClassification.from_pretrained(\n\
          \    \"trl-lib/llama-7b-se-rm-peft\", \n    num_labels=1, \n    torch_dtype=torch.bfloat16\n\
          )\n\ntokenizer = AutoTokenizer.from_pretrained(\"trl-lib/llama-7b-se-rm-peft\"\
          )\n</code></pre>\n<p>yields the following error</p>\n<pre><code>---------------------------------------------------------------------------\n\
          HTTPError                                 Traceback (most recent call last)\n\
          File /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:259,\
          \ in hf_raise_for_status(response, endpoint_name)\n    258 try:\n--&gt;\
          \ 259     response.raise_for_status()\n    260 except HTTPError as e:\n\n\
          File /workspaces/reddit_qa/venv/lib/python3.10/site-packages/requests/models.py:1021,\
          \ in Response.raise_for_status(self)\n   1020 if http_error_msg:\n-&gt;\
          \ 1021     raise HTTPError(http_error_msg, response=self)\n\nHTTPError:\
          \ 404 Client Error: Not Found for url: https://huggingface.co/trl-lib/llama-7b-se-rm-peft/resolve/main/config.json\n\
          \nThe above exception was the direct cause of the following exception:\n\
          \nEntryNotFoundError                        Traceback (most recent call\
          \ last)\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/utils/hub.py:427,\
          \ in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,\
          \ proxies, token, revision, local_files_only, subfolder, repo_type, user_agent,\
          \ _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,\
          \ _commit_hash, **deprecated_kwargs)\n    425 try:\n    426     # Load from\
          \ URL or cache if already cached\n--&gt; 427     resolved_file = hf_hub_download(\n\
          \    428         path_or_repo_id,\n    429         filename,\n    430  \
          \       subfolder=None if len(subfolder) == 0 else subfolder,\n    431 \
          \        repo_type=repo_type,\n    432         revision=revision,\n    433\
          \         cache_dir=cache_dir,\n    434         user_agent=user_agent,\n\
          \    435         force_download=force_download,\n    436         proxies=proxies,\n\
          \    437         resume_download=resume_download,\n    438         token=token,\n\
          \    439         local_files_only=local_files_only,\n    440     )\n   \
          \ 441 except GatedRepoError as e:\n\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:120,\
          \ in validate_hf_hub_args.._inner_fn(*args, **kwargs)\n    118     kwargs\
          \ = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token,\
          \ kwargs=kwargs)\n--&gt; 120 return fn(*args, **kwargs)\n\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1195,\
          \ in hf_hub_download(repo_id, filename, subfolder, repo_type, revision,\
          \ library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks,\
          \ user_agent, force_download, force_filename, proxies, etag_timeout, resume_download,\
          \ token, local_files_only, legacy_cache_layout)\n   1194 try:\n-&gt; 1195\
          \     metadata = get_hf_file_metadata(\n   1196         url=url,\n   1197\
          \         token=token,\n   1198         proxies=proxies,\n   1199      \
          \   timeout=etag_timeout,\n   1200     )\n   1201 except EntryNotFoundError\
          \ as http_error:\n   1202     # Cache the non-existence of the file and\
          \ raise\n\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:120,\
          \ in validate_hf_hub_args.._inner_fn(*args, **kwargs)\n    118     kwargs\
          \ = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token,\
          \ kwargs=kwargs)\n--&gt; 120 return fn(*args, **kwargs)\n\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1541,\
          \ in get_hf_file_metadata(url, token, proxies, timeout)\n   1532 r = _request_wrapper(\n\
          \   1533     method=\"HEAD\",\n   1534     url=url,\n   (...)\n   1539 \
          \    timeout=timeout,\n   1540 )\n-&gt; 1541 hf_raise_for_status(r)\n  \
          \ 1543 # Return\n\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:269,\
          \ in hf_raise_for_status(response, endpoint_name)\n    268     message =\
          \ f\"{response.status_code} Client Error.\" + \"\\n\\n\" + f\"Entry Not\
          \ Found for url: {response.url}.\"\n--&gt; 269     raise EntryNotFoundError(message,\
          \ response) from e\n    271 elif error_code == \"GatedRepo\":\n\nEntryNotFoundError:\
          \ 404 Client Error. (Request ID: Root=1-64e60e7f-642a805a39a142330e405e81)\n\
          \nEntry Not Found for url: https://huggingface.co/trl-lib/llama-7b-se-rm-peft/resolve/main/config.json.\n\
          \nThe above exception was the direct cause of the following exception:\n\
          \nOSError                                   Traceback (most recent call\
          \ last)\nCell In[10], line 1\n----&gt; 1 reward_model = AutoModelForSequenceClassification.from_pretrained(\n\
          \      2     \"trl-lib/llama-7b-se-rm-peft\", \n      3     num_labels=1,\
          \ \n      4     torch_dtype=torch.bfloat16\n      5 )\n      7 tokenizer\
          \ = AutoTokenizer.from_pretrained(\"trl-lib/llama-7b-se-rm-peft\")\n\nFile\
          \ /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:479,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    476 if kwargs.get(\"torch_dtype\", None) ==\
          \ \"auto\":\n    477     _ = kwargs.pop(\"torch_dtype\")\n--&gt; 479 config,\
          \ kwargs = AutoConfig.from_pretrained(\n    480     pretrained_model_name_or_path,\n\
          \    481     return_unused_kwargs=True,\n    482     trust_remote_code=trust_remote_code,\n\
          \    483     **hub_kwargs,\n    484     **kwargs,\n    485 )\n    487 #\
          \ if torch_dtype=auto was passed here, ensure to pass it on\n    488 if\
          \ kwargs_orig.get(\"torch_dtype\", None) == \"auto\":\n\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1004,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
          \   1002 kwargs[\"name_or_path\"] = pretrained_model_name_or_path\n   1003\
          \ trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n-&gt; 1004\
          \ config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)\n   1005 has_remote_code = \"auto_map\" in config_dict and \"\
          AutoConfig\" in config_dict[\"auto_map\"]\n   1006 has_local_code = \"model_type\"\
          \ in config_dict and config_dict[\"model_type\"] in CONFIG_MAPPING\n\nFile\
          \ /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:620,\
          \ in PretrainedConfig.get_config_dict(cls, pretrained_model_name_or_path,\
          \ **kwargs)\n    618 original_kwargs = copy.deepcopy(kwargs)\n    619 #\
          \ Get config dict associated with the base config file\n--&gt; 620 config_dict,\
          \ kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n\
          \    621 if \"_commit_hash\" in config_dict:\n    622     original_kwargs[\"\
          _commit_hash\"] = config_dict[\"_commit_hash\"]\n\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:675,\
          \ in PretrainedConfig._get_config_dict(cls, pretrained_model_name_or_path,\
          \ **kwargs)\n    671 configuration_file = kwargs.pop(\"_configuration_file\"\
          , CONFIG_NAME)\n    673 try:\n    674     # Load from local folder or from\
          \ cache or download from model Hub and cache\n--&gt; 675     resolved_config_file\
          \ = cached_file(\n    676         pretrained_model_name_or_path,\n    677\
          \         configuration_file,\n    678         cache_dir=cache_dir,\n  \
          \  679         force_download=force_download,\n    680         proxies=proxies,\n\
          \    681         resume_download=resume_download,\n    682         local_files_only=local_files_only,\n\
          \    683         token=token,\n    684         user_agent=user_agent,\n\
          \    685         revision=revision,\n    686         subfolder=subfolder,\n\
          \    687         _commit_hash=commit_hash,\n    688     )\n    689     commit_hash\
          \ = extract_commit_hash(resolved_config_file, commit_hash)\n    690 except\
          \ EnvironmentError:\n    691     # Raise any environment error raise by\
          \ `cached_file`. It will have a helpful error message adapted to\n    692\
          \     # the original exception.\n\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/utils/hub.py:478,\
          \ in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,\
          \ proxies, token, revision, local_files_only, subfolder, repo_type, user_agent,\
          \ _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,\
          \ _commit_hash, **deprecated_kwargs)\n    476     if revision is None:\n\
          \    477         revision = \"main\"\n--&gt; 478     raise EnvironmentError(\n\
          \    479         f\"{path_or_repo_id} does not appear to have a file named\
          \ {full_filename}. Checkout \"\n    480         f\"'https://huggingface.co/{path_or_repo_id}/{revision}'\
          \ for available files.\"\n    481     ) from e\n    482 except HTTPError\
          \ as err:\n    483     # First we try to see if we have a cached version\
          \ (not up to date):\n    484     resolved_file = try_to_load_from_cache(path_or_repo_id,\
          \ full_filename, cache_dir=cache_dir, revision=revision)\n\nOSError: trl-lib/llama-7b-se-rm-peft\
          \ does not appear to have a file named config.json. Checkout 'https://huggingface.co/trl-lib/llama-7b-se-rm-peft/main'\
          \ for available files.\n</code></pre>\n"
        raw: "Loading the model with \r\n\r\n```\r\nreward_model = AutoModelForSequenceClassification.from_pretrained(\r\
          \n    \"trl-lib/llama-7b-se-rm-peft\", \r\n    num_labels=1, \r\n    torch_dtype=torch.bfloat16\r\
          \n)\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"trl-lib/llama-7b-se-rm-peft\"\
          )\r\n```\r\n\r\nyields the following error\r\n\r\n```\r\n---------------------------------------------------------------------------\r\
          \nHTTPError                                 Traceback (most recent call\
          \ last)\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:259,\
          \ in hf_raise_for_status(response, endpoint_name)\r\n    258 try:\r\n-->\
          \ 259     response.raise_for_status()\r\n    260 except HTTPError as e:\r\
          \n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/requests/models.py:1021,\
          \ in Response.raise_for_status(self)\r\n   1020 if http_error_msg:\r\n->\
          \ 1021     raise HTTPError(http_error_msg, response=self)\r\n\r\nHTTPError:\
          \ 404 Client Error: Not Found for url: https://huggingface.co/trl-lib/llama-7b-se-rm-peft/resolve/main/config.json\r\
          \n\r\nThe above exception was the direct cause of the following exception:\r\
          \n\r\nEntryNotFoundError                        Traceback (most recent call\
          \ last)\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/utils/hub.py:427,\
          \ in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,\
          \ proxies, token, revision, local_files_only, subfolder, repo_type, user_agent,\
          \ _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,\
          \ _commit_hash, **deprecated_kwargs)\r\n    425 try:\r\n    426     # Load\
          \ from URL or cache if already cached\r\n--> 427     resolved_file = hf_hub_download(\r\
          \n    428         path_or_repo_id,\r\n    429         filename,\r\n    430\
          \         subfolder=None if len(subfolder) == 0 else subfolder,\r\n    431\
          \         repo_type=repo_type,\r\n    432         revision=revision,\r\n\
          \    433         cache_dir=cache_dir,\r\n    434         user_agent=user_agent,\r\
          \n    435         force_download=force_download,\r\n    436         proxies=proxies,\r\
          \n    437         resume_download=resume_download,\r\n    438         token=token,\r\
          \n    439         local_files_only=local_files_only,\r\n    440     )\r\n\
          \    441 except GatedRepoError as e:\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:120,\
          \ in validate_hf_hub_args.._inner_fn(*args, **kwargs)\r\n    118     kwargs\
          \ = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token,\
          \ kwargs=kwargs)\r\n--> 120 return fn(*args, **kwargs)\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1195,\
          \ in hf_hub_download(repo_id, filename, subfolder, repo_type, revision,\
          \ library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks,\
          \ user_agent, force_download, force_filename, proxies, etag_timeout, resume_download,\
          \ token, local_files_only, legacy_cache_layout)\r\n   1194 try:\r\n-> 1195\
          \     metadata = get_hf_file_metadata(\r\n   1196         url=url,\r\n \
          \  1197         token=token,\r\n   1198         proxies=proxies,\r\n   1199\
          \         timeout=etag_timeout,\r\n   1200     )\r\n   1201 except EntryNotFoundError\
          \ as http_error:\r\n   1202     # Cache the non-existence of the file and\
          \ raise\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:120,\
          \ in validate_hf_hub_args.._inner_fn(*args, **kwargs)\r\n    118     kwargs\
          \ = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token,\
          \ kwargs=kwargs)\r\n--> 120 return fn(*args, **kwargs)\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1541,\
          \ in get_hf_file_metadata(url, token, proxies, timeout)\r\n   1532 r = _request_wrapper(\r\
          \n   1533     method=\"HEAD\",\r\n   1534     url=url,\r\n   (...)\r\n \
          \  1539     timeout=timeout,\r\n   1540 )\r\n-> 1541 hf_raise_for_status(r)\r\
          \n   1543 # Return\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:269,\
          \ in hf_raise_for_status(response, endpoint_name)\r\n    268     message\
          \ = f\"{response.status_code} Client Error.\" + \"\\n\\n\" + f\"Entry Not\
          \ Found for url: {response.url}.\"\r\n--> 269     raise EntryNotFoundError(message,\
          \ response) from e\r\n    271 elif error_code == \"GatedRepo\":\r\n\r\n\
          EntryNotFoundError: 404 Client Error. (Request ID: Root=1-64e60e7f-642a805a39a142330e405e81)\r\
          \n\r\nEntry Not Found for url: https://huggingface.co/trl-lib/llama-7b-se-rm-peft/resolve/main/config.json.\r\
          \n\r\nThe above exception was the direct cause of the following exception:\r\
          \n\r\nOSError                                   Traceback (most recent call\
          \ last)\r\nCell In[10], line 1\r\n----> 1 reward_model = AutoModelForSequenceClassification.from_pretrained(\r\
          \n      2     \"trl-lib/llama-7b-se-rm-peft\", \r\n      3     num_labels=1,\
          \ \r\n      4     torch_dtype=torch.bfloat16\r\n      5 )\r\n      7 tokenizer\
          \ = AutoTokenizer.from_pretrained(\"trl-lib/llama-7b-se-rm-peft\")\r\n\r\
          \nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:479,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\r\n    476 if kwargs.get(\"torch_dtype\", None)\
          \ == \"auto\":\r\n    477     _ = kwargs.pop(\"torch_dtype\")\r\n--> 479\
          \ config, kwargs = AutoConfig.from_pretrained(\r\n    480     pretrained_model_name_or_path,\r\
          \n    481     return_unused_kwargs=True,\r\n    482     trust_remote_code=trust_remote_code,\r\
          \n    483     **hub_kwargs,\r\n    484     **kwargs,\r\n    485 )\r\n  \
          \  487 # if torch_dtype=auto was passed here, ensure to pass it on\r\n \
          \   488 if kwargs_orig.get(\"torch_dtype\", None) == \"auto\":\r\n\r\nFile\
          \ /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1004,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\
          \n   1002 kwargs[\"name_or_path\"] = pretrained_model_name_or_path\r\n \
          \  1003 trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\r\n\
          -> 1004 config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)\r\n   1005 has_remote_code = \"auto_map\" in config_dict and\
          \ \"AutoConfig\" in config_dict[\"auto_map\"]\r\n   1006 has_local_code\
          \ = \"model_type\" in config_dict and config_dict[\"model_type\"] in CONFIG_MAPPING\r\
          \n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:620,\
          \ in PretrainedConfig.get_config_dict(cls, pretrained_model_name_or_path,\
          \ **kwargs)\r\n    618 original_kwargs = copy.deepcopy(kwargs)\r\n    619\
          \ # Get config dict associated with the base config file\r\n--> 620 config_dict,\
          \ kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\
          \n    621 if \"_commit_hash\" in config_dict:\r\n    622     original_kwargs[\"\
          _commit_hash\"] = config_dict[\"_commit_hash\"]\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:675,\
          \ in PretrainedConfig._get_config_dict(cls, pretrained_model_name_or_path,\
          \ **kwargs)\r\n    671 configuration_file = kwargs.pop(\"_configuration_file\"\
          , CONFIG_NAME)\r\n    673 try:\r\n    674     # Load from local folder or\
          \ from cache or download from model Hub and cache\r\n--> 675     resolved_config_file\
          \ = cached_file(\r\n    676         pretrained_model_name_or_path,\r\n \
          \   677         configuration_file,\r\n    678         cache_dir=cache_dir,\r\
          \n    679         force_download=force_download,\r\n    680         proxies=proxies,\r\
          \n    681         resume_download=resume_download,\r\n    682         local_files_only=local_files_only,\r\
          \n    683         token=token,\r\n    684         user_agent=user_agent,\r\
          \n    685         revision=revision,\r\n    686         subfolder=subfolder,\r\
          \n    687         _commit_hash=commit_hash,\r\n    688     )\r\n    689\
          \     commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\r\
          \n    690 except EnvironmentError:\r\n    691     # Raise any environment\
          \ error raise by `cached_file`. It will have a helpful error message adapted\
          \ to\r\n    692     # the original exception.\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/utils/hub.py:478,\
          \ in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,\
          \ proxies, token, revision, local_files_only, subfolder, repo_type, user_agent,\
          \ _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,\
          \ _commit_hash, **deprecated_kwargs)\r\n    476     if revision is None:\r\
          \n    477         revision = \"main\"\r\n--> 478     raise EnvironmentError(\r\
          \n    479         f\"{path_or_repo_id} does not appear to have a file named\
          \ {full_filename}. Checkout \"\r\n    480         f\"'https://huggingface.co/{path_or_repo_id}/{revision}'\
          \ for available files.\"\r\n    481     ) from e\r\n    482 except HTTPError\
          \ as err:\r\n    483     # First we try to see if we have a cached version\
          \ (not up to date):\r\n    484     resolved_file = try_to_load_from_cache(path_or_repo_id,\
          \ full_filename, cache_dir=cache_dir, revision=revision)\r\n\r\nOSError:\
          \ trl-lib/llama-7b-se-rm-peft does not appear to have a file named config.json.\
          \ Checkout 'https://huggingface.co/trl-lib/llama-7b-se-rm-peft/main' for\
          \ available files.\r\n```"
        updatedAt: '2023-08-23T13:50:37.200Z'
      numEdits: 0
      reactions: []
    id: 64e60eadab6abc9fcfaa4dff
    type: comment
  author: jvhoffbauer
  content: "Loading the model with \r\n\r\n```\r\nreward_model = AutoModelForSequenceClassification.from_pretrained(\r\
    \n    \"trl-lib/llama-7b-se-rm-peft\", \r\n    num_labels=1, \r\n    torch_dtype=torch.bfloat16\r\
    \n)\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"trl-lib/llama-7b-se-rm-peft\"\
    )\r\n```\r\n\r\nyields the following error\r\n\r\n```\r\n---------------------------------------------------------------------------\r\
    \nHTTPError                                 Traceback (most recent call last)\r\
    \nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:259,\
    \ in hf_raise_for_status(response, endpoint_name)\r\n    258 try:\r\n--> 259 \
    \    response.raise_for_status()\r\n    260 except HTTPError as e:\r\n\r\nFile\
    \ /workspaces/reddit_qa/venv/lib/python3.10/site-packages/requests/models.py:1021,\
    \ in Response.raise_for_status(self)\r\n   1020 if http_error_msg:\r\n-> 1021\
    \     raise HTTPError(http_error_msg, response=self)\r\n\r\nHTTPError: 404 Client\
    \ Error: Not Found for url: https://huggingface.co/trl-lib/llama-7b-se-rm-peft/resolve/main/config.json\r\
    \n\r\nThe above exception was the direct cause of the following exception:\r\n\
    \r\nEntryNotFoundError                        Traceback (most recent call last)\r\
    \nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/utils/hub.py:427,\
    \ in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,\
    \ proxies, token, revision, local_files_only, subfolder, repo_type, user_agent,\
    \ _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,\
    \ _commit_hash, **deprecated_kwargs)\r\n    425 try:\r\n    426     # Load from\
    \ URL or cache if already cached\r\n--> 427     resolved_file = hf_hub_download(\r\
    \n    428         path_or_repo_id,\r\n    429         filename,\r\n    430   \
    \      subfolder=None if len(subfolder) == 0 else subfolder,\r\n    431      \
    \   repo_type=repo_type,\r\n    432         revision=revision,\r\n    433    \
    \     cache_dir=cache_dir,\r\n    434         user_agent=user_agent,\r\n    435\
    \         force_download=force_download,\r\n    436         proxies=proxies,\r\
    \n    437         resume_download=resume_download,\r\n    438         token=token,\r\
    \n    439         local_files_only=local_files_only,\r\n    440     )\r\n    441\
    \ except GatedRepoError as e:\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:120,\
    \ in validate_hf_hub_args.._inner_fn(*args, **kwargs)\r\n    118     kwargs =\
    \ smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token,\
    \ kwargs=kwargs)\r\n--> 120 return fn(*args, **kwargs)\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1195,\
    \ in hf_hub_download(repo_id, filename, subfolder, repo_type, revision, library_name,\
    \ library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download,\
    \ force_filename, proxies, etag_timeout, resume_download, token, local_files_only,\
    \ legacy_cache_layout)\r\n   1194 try:\r\n-> 1195     metadata = get_hf_file_metadata(\r\
    \n   1196         url=url,\r\n   1197         token=token,\r\n   1198        \
    \ proxies=proxies,\r\n   1199         timeout=etag_timeout,\r\n   1200     )\r\
    \n   1201 except EntryNotFoundError as http_error:\r\n   1202     # Cache the\
    \ non-existence of the file and raise\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:120,\
    \ in validate_hf_hub_args.._inner_fn(*args, **kwargs)\r\n    118     kwargs =\
    \ smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token,\
    \ kwargs=kwargs)\r\n--> 120 return fn(*args, **kwargs)\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1541,\
    \ in get_hf_file_metadata(url, token, proxies, timeout)\r\n   1532 r = _request_wrapper(\r\
    \n   1533     method=\"HEAD\",\r\n   1534     url=url,\r\n   (...)\r\n   1539\
    \     timeout=timeout,\r\n   1540 )\r\n-> 1541 hf_raise_for_status(r)\r\n   1543\
    \ # Return\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:269,\
    \ in hf_raise_for_status(response, endpoint_name)\r\n    268     message = f\"\
    {response.status_code} Client Error.\" + \"\\n\\n\" + f\"Entry Not Found for url:\
    \ {response.url}.\"\r\n--> 269     raise EntryNotFoundError(message, response)\
    \ from e\r\n    271 elif error_code == \"GatedRepo\":\r\n\r\nEntryNotFoundError:\
    \ 404 Client Error. (Request ID: Root=1-64e60e7f-642a805a39a142330e405e81)\r\n\
    \r\nEntry Not Found for url: https://huggingface.co/trl-lib/llama-7b-se-rm-peft/resolve/main/config.json.\r\
    \n\r\nThe above exception was the direct cause of the following exception:\r\n\
    \r\nOSError                                   Traceback (most recent call last)\r\
    \nCell In[10], line 1\r\n----> 1 reward_model = AutoModelForSequenceClassification.from_pretrained(\r\
    \n      2     \"trl-lib/llama-7b-se-rm-peft\", \r\n      3     num_labels=1, \r\
    \n      4     torch_dtype=torch.bfloat16\r\n      5 )\r\n      7 tokenizer = AutoTokenizer.from_pretrained(\"\
    trl-lib/llama-7b-se-rm-peft\")\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:479,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\r\n    476 if kwargs.get(\"torch_dtype\", None) == \"auto\":\r\n \
    \   477     _ = kwargs.pop(\"torch_dtype\")\r\n--> 479 config, kwargs = AutoConfig.from_pretrained(\r\
    \n    480     pretrained_model_name_or_path,\r\n    481     return_unused_kwargs=True,\r\
    \n    482     trust_remote_code=trust_remote_code,\r\n    483     **hub_kwargs,\r\
    \n    484     **kwargs,\r\n    485 )\r\n    487 # if torch_dtype=auto was passed\
    \ here, ensure to pass it on\r\n    488 if kwargs_orig.get(\"torch_dtype\", None)\
    \ == \"auto\":\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1004,\
    \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\
    \n   1002 kwargs[\"name_or_path\"] = pretrained_model_name_or_path\r\n   1003\
    \ trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\r\n-> 1004 config_dict,\
    \ unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
    \ **kwargs)\r\n   1005 has_remote_code = \"auto_map\" in config_dict and \"AutoConfig\"\
    \ in config_dict[\"auto_map\"]\r\n   1006 has_local_code = \"model_type\" in config_dict\
    \ and config_dict[\"model_type\"] in CONFIG_MAPPING\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:620,\
    \ in PretrainedConfig.get_config_dict(cls, pretrained_model_name_or_path, **kwargs)\r\
    \n    618 original_kwargs = copy.deepcopy(kwargs)\r\n    619 # Get config dict\
    \ associated with the base config file\r\n--> 620 config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path,\
    \ **kwargs)\r\n    621 if \"_commit_hash\" in config_dict:\r\n    622     original_kwargs[\"\
    _commit_hash\"] = config_dict[\"_commit_hash\"]\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:675,\
    \ in PretrainedConfig._get_config_dict(cls, pretrained_model_name_or_path, **kwargs)\r\
    \n    671 configuration_file = kwargs.pop(\"_configuration_file\", CONFIG_NAME)\r\
    \n    673 try:\r\n    674     # Load from local folder or from cache or download\
    \ from model Hub and cache\r\n--> 675     resolved_config_file = cached_file(\r\
    \n    676         pretrained_model_name_or_path,\r\n    677         configuration_file,\r\
    \n    678         cache_dir=cache_dir,\r\n    679         force_download=force_download,\r\
    \n    680         proxies=proxies,\r\n    681         resume_download=resume_download,\r\
    \n    682         local_files_only=local_files_only,\r\n    683         token=token,\r\
    \n    684         user_agent=user_agent,\r\n    685         revision=revision,\r\
    \n    686         subfolder=subfolder,\r\n    687         _commit_hash=commit_hash,\r\
    \n    688     )\r\n    689     commit_hash = extract_commit_hash(resolved_config_file,\
    \ commit_hash)\r\n    690 except EnvironmentError:\r\n    691     # Raise any\
    \ environment error raise by `cached_file`. It will have a helpful error message\
    \ adapted to\r\n    692     # the original exception.\r\n\r\nFile /workspaces/reddit_qa/venv/lib/python3.10/site-packages/transformers/utils/hub.py:478,\
    \ in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,\
    \ proxies, token, revision, local_files_only, subfolder, repo_type, user_agent,\
    \ _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,\
    \ _commit_hash, **deprecated_kwargs)\r\n    476     if revision is None:\r\n \
    \   477         revision = \"main\"\r\n--> 478     raise EnvironmentError(\r\n\
    \    479         f\"{path_or_repo_id} does not appear to have a file named {full_filename}.\
    \ Checkout \"\r\n    480         f\"'https://huggingface.co/{path_or_repo_id}/{revision}'\
    \ for available files.\"\r\n    481     ) from e\r\n    482 except HTTPError as\
    \ err:\r\n    483     # First we try to see if we have a cached version (not up\
    \ to date):\r\n    484     resolved_file = try_to_load_from_cache(path_or_repo_id,\
    \ full_filename, cache_dir=cache_dir, revision=revision)\r\n\r\nOSError: trl-lib/llama-7b-se-rm-peft\
    \ does not appear to have a file named config.json. Checkout 'https://huggingface.co/trl-lib/llama-7b-se-rm-peft/main'\
    \ for available files.\r\n```"
  created_at: 2023-08-23 12:50:37+00:00
  edited: false
  hidden: false
  id: 64e60eadab6abc9fcfaa4dff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9000f5393abf3f4b542bdfc73980649a.svg
      fullname: Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zhaorun
      type: user
    createdAt: '2023-09-01T16:22:02.000Z'
    data:
      edited: false
      editors:
      - Zhaorun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6593457460403442
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9000f5393abf3f4b542bdfc73980649a.svg
          fullname: Chen
          isHf: false
          isPro: false
          name: Zhaorun
          type: user
        html: '<p>This is an adapter version which only contain the LoRA layers parameters.
          You should merge this peft adapter layers with the base model first, using:</p>

          <p><code>python examples/stack_llama/scripts/merge_peft_adapter.py --adapter_model_name=XXX
          --base_model_name=YYY --output_name=ZZZ</code></p>

          <p>You can find the <code>merge_peft_adapter.py</code> under their <a rel="nofollow"
          href="https://github.com/huggingface/trl">repository</a>. Good luck!</p>

          '
        raw: 'This is an adapter version which only contain the LoRA layers parameters.
          You should merge this peft adapter layers with the base model first, using:


          `python examples/stack_llama/scripts/merge_peft_adapter.py --adapter_model_name=XXX
          --base_model_name=YYY --output_name=ZZZ`


          You can find the `merge_peft_adapter.py` under their [repository](https://github.com/huggingface/trl).
          Good luck!'
        updatedAt: '2023-09-01T16:22:02.469Z'
      numEdits: 0
      reactions: []
    id: 64f20faaa2fc808156d96f78
    type: comment
  author: Zhaorun
  content: 'This is an adapter version which only contain the LoRA layers parameters.
    You should merge this peft adapter layers with the base model first, using:


    `python examples/stack_llama/scripts/merge_peft_adapter.py --adapter_model_name=XXX
    --base_model_name=YYY --output_name=ZZZ`


    You can find the `merge_peft_adapter.py` under their [repository](https://github.com/huggingface/trl).
    Good luck!'
  created_at: 2023-09-01 15:22:02+00:00
  edited: false
  hidden: false
  id: 64f20faaa2fc808156d96f78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d277dc4b5ea2663f6e82301bba1400b.svg
      fullname: Hoffbauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jvhoffbauer
      type: user
    createdAt: '2023-12-26T14:35:08.000Z'
    data:
      edited: false
      editors:
      - jvhoffbauer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8076265454292297
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d277dc4b5ea2663f6e82301bba1400b.svg
          fullname: Hoffbauer
          isHf: false
          isPro: false
          name: jvhoffbauer
          type: user
        html: '<p>Thanks a lot! </p>

          '
        raw: 'Thanks a lot! '
        updatedAt: '2023-12-26T14:35:08.459Z'
      numEdits: 0
      reactions: []
      relatedEventId: 658ae49cd861072dc559138d
    id: 658ae49cd861072dc5591387
    type: comment
  author: jvhoffbauer
  content: 'Thanks a lot! '
  created_at: 2023-12-26 14:35:08+00:00
  edited: false
  hidden: false
  id: 658ae49cd861072dc5591387
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/6d277dc4b5ea2663f6e82301bba1400b.svg
      fullname: Hoffbauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jvhoffbauer
      type: user
    createdAt: '2023-12-26T14:35:08.000Z'
    data:
      status: closed
    id: 658ae49cd861072dc559138d
    type: status-change
  author: jvhoffbauer
  created_at: 2023-12-26 14:35:08+00:00
  id: 658ae49cd861072dc559138d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: trl-lib/llama-7b-se-rm-peft
repo_type: model
status: closed
target_branch: null
title: 'How to load the model? '
