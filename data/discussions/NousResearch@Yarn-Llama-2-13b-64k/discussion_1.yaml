!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Sc0urge
conflicting_files: null
created_at: 2023-09-03 11:54:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df90c855e1f66378b29d97e63ed7dcc3.svg
      fullname: Ivan Bokarev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sc0urge
      type: user
    createdAt: '2023-09-03T12:54:31.000Z'
    data:
      edited: false
      editors:
      - Sc0urge
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9215430021286011
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df90c855e1f66378b29d97e63ed7dcc3.svg
          fullname: Ivan Bokarev
          isHf: false
          isPro: false
          name: Sc0urge
          type: user
        html: '<p>Hi there, Ive been trying to do inference on a LLAMA2 with higher
          context window on a T4 GPU from google colab. Both (this and the 32k version
          from togethercompute) always crash the instance because of RAM, even with
          QLORA. Is there some kind of formula to calculate the hardware requirements
          for models with increased CW or any proven configurations that work? Thanks
          in advance</p>

          '
        raw: Hi there, Ive been trying to do inference on a LLAMA2 with higher context
          window on a T4 GPU from google colab. Both (this and the 32k version from
          togethercompute) always crash the instance because of RAM, even with QLORA.
          Is there some kind of formula to calculate the hardware requirements for
          models with increased CW or any proven configurations that work? Thanks
          in advance
        updatedAt: '2023-09-03T12:54:31.701Z'
      numEdits: 0
      reactions: []
    id: 64f482073daa796219cd8e22
    type: comment
  author: Sc0urge
  content: Hi there, Ive been trying to do inference on a LLAMA2 with higher context
    window on a T4 GPU from google colab. Both (this and the 32k version from togethercompute)
    always crash the instance because of RAM, even with QLORA. Is there some kind
    of formula to calculate the hardware requirements for models with increased CW
    or any proven configurations that work? Thanks in advance
  created_at: 2023-09-03 11:54:31+00:00
  edited: false
  hidden: false
  id: 64f482073daa796219cd8e22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cf262026c94b143173ef65/FFcC-XhG5hMjApSvfk_he.png?w=200&h=200&f=face
      fullname: Bowen Peng
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bloc97
      type: user
    createdAt: '2023-09-03T22:56:28.000Z'
    data:
      edited: false
      editors:
      - bloc97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9214249849319458
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cf262026c94b143173ef65/FFcC-XhG5hMjApSvfk_he.png?w=200&h=200&f=face
          fullname: Bowen Peng
          isHf: false
          isPro: false
          name: bloc97
          type: user
        html: '<p>QLoRA is used for training, do you mean quantization? The T4 GPU''s
          memory is rather small (16GB), thus you will be restricted to &lt;10k context.
          For the full 128k context with 13b model, it''s ~360GB of VRAM (or RAM if
          using CPU inference) for fp16 inference. Quantization doesn''t affect the
          context size memory requirements very much...</p>

          <p>At 64k context you might be looking at somewhere in the neighborhood
          of ~100GB of memory...</p>

          '
        raw: 'QLoRA is used for training, do you mean quantization? The T4 GPU''s
          memory is rather small (16GB), thus you will be restricted to <10k context.
          For the full 128k context with 13b model, it''s ~360GB of VRAM (or RAM if
          using CPU inference) for fp16 inference. Quantization doesn''t affect the
          context size memory requirements very much...


          At 64k context you might be looking at somewhere in the neighborhood of
          ~100GB of memory...'
        updatedAt: '2023-09-03T22:56:28.385Z'
      numEdits: 0
      reactions: []
    id: 64f50f1ca30d6b0a26c1534d
    type: comment
  author: bloc97
  content: 'QLoRA is used for training, do you mean quantization? The T4 GPU''s memory
    is rather small (16GB), thus you will be restricted to <10k context. For the full
    128k context with 13b model, it''s ~360GB of VRAM (or RAM if using CPU inference)
    for fp16 inference. Quantization doesn''t affect the context size memory requirements
    very much...


    At 64k context you might be looking at somewhere in the neighborhood of ~100GB
    of memory...'
  created_at: 2023-09-03 21:56:28+00:00
  edited: false
  hidden: false
  id: 64f50f1ca30d6b0a26c1534d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df90c855e1f66378b29d97e63ed7dcc3.svg
      fullname: Ivan Bokarev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sc0urge
      type: user
    createdAt: '2023-09-04T13:51:10.000Z'
    data:
      edited: false
      editors:
      - Sc0urge
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9338772892951965
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df90c855e1f66378b29d97e63ed7dcc3.svg
          fullname: Ivan Bokarev
          isHf: false
          isPro: false
          name: Sc0urge
          type: user
        html: '<blockquote>

          <p>QLoRA is used for training, do you mean quantization? The T4 GPU''s memory
          is rather small (16GB), thus you will be restricted to &lt;10k context.
          For the full 128k context with 13b model, it''s ~360GB of VRAM (or RAM if
          using CPU inference) for fp16 inference. Quantization doesn''t affect the
          context size memory requirements very much...</p>

          <p>At 64k context you might be looking at somewhere in the neighborhood
          of ~100GB of memory...</p>

          </blockquote>

          <p>My bad, what I meant was, that when I loaded the model from HF with bits
          and bytes (which uses QLoRa I believe) it already crashed. </p>

          '
        raw: "> QLoRA is used for training, do you mean quantization? The T4 GPU's\
          \ memory is rather small (16GB), thus you will be restricted to <10k context.\
          \ For the full 128k context with 13b model, it's ~360GB of VRAM (or RAM\
          \ if using CPU inference) for fp16 inference. Quantization doesn't affect\
          \ the context size memory requirements very much...\n> \n> At 64k context\
          \ you might be looking at somewhere in the neighborhood of ~100GB of memory...\n\
          \nMy bad, what I meant was, that when I loaded the model from HF with bits\
          \ and bytes (which uses QLoRa I believe) it already crashed. "
        updatedAt: '2023-09-04T13:51:10.861Z'
      numEdits: 0
      reactions: []
    id: 64f5e0ce8b8cab6e3c2c51a4
    type: comment
  author: Sc0urge
  content: "> QLoRA is used for training, do you mean quantization? The T4 GPU's memory\
    \ is rather small (16GB), thus you will be restricted to <10k context. For the\
    \ full 128k context with 13b model, it's ~360GB of VRAM (or RAM if using CPU inference)\
    \ for fp16 inference. Quantization doesn't affect the context size memory requirements\
    \ very much...\n> \n> At 64k context you might be looking at somewhere in the\
    \ neighborhood of ~100GB of memory...\n\nMy bad, what I meant was, that when I\
    \ loaded the model from HF with bits and bytes (which uses QLoRa I believe) it\
    \ already crashed. "
  created_at: 2023-09-04 12:51:10+00:00
  edited: false
  hidden: false
  id: 64f5e0ce8b8cab6e3c2c51a4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: NousResearch/Yarn-Llama-2-13b-64k
repo_type: model
status: open
target_branch: null
title: Hardware requirements for the model.
