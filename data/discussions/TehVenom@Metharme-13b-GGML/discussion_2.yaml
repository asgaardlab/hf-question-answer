!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Doctor-Shotgun
conflicting_files: null
created_at: 2023-05-23 23:50:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
      fullname: Doctor Shotgun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Doctor-Shotgun
      type: user
    createdAt: '2023-05-24T00:50:48.000Z'
    data:
      edited: false
      editors:
      - Doctor-Shotgun
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
          fullname: Doctor Shotgun
          isHf: false
          isPro: false
          name: Doctor-Shotgun
          type: user
        html: '<p>Running into an error now trying to load these models on the latest
          llama-cpp-python, and it appears that GGML had (yet another) change to the
          quant methods around the time these quants were made, that also reduces
          the size of the models slightly (now 7.32gb for Q4_0 from 8.14gb):<br><a
          rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/1508">https://github.com/ggerganov/llama.cpp/pull/1508</a></p>

          <p>I was able to find a set of updated quants for pyg13B that run on the
          current version - but as usual there was no love for metharme lol. Was wondering
          if you could do another set of quants with the new format - would be much
          appreciated!</p>

          '
        raw: "Running into an error now trying to load these models on the latest\
          \ llama-cpp-python, and it appears that GGML had (yet another) change to\
          \ the quant methods around the time these quants were made, that also reduces\
          \ the size of the models slightly (now 7.32gb for Q4_0 from 8.14gb):\r\n\
          https://github.com/ggerganov/llama.cpp/pull/1508\r\n\r\nI was able to find\
          \ a set of updated quants for pyg13B that run on the current version - but\
          \ as usual there was no love for metharme lol. Was wondering if you could\
          \ do another set of quants with the new format - would be much appreciated!"
        updatedAt: '2023-05-24T00:50:48.989Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - ozzeruk82
        - Cheuwambel
    id: 646d5f684220471ca0c765ba
    type: comment
  author: Doctor-Shotgun
  content: "Running into an error now trying to load these models on the latest llama-cpp-python,\
    \ and it appears that GGML had (yet another) change to the quant methods around\
    \ the time these quants were made, that also reduces the size of the models slightly\
    \ (now 7.32gb for Q4_0 from 8.14gb):\r\nhttps://github.com/ggerganov/llama.cpp/pull/1508\r\
    \n\r\nI was able to find a set of updated quants for pyg13B that run on the current\
    \ version - but as usual there was no love for metharme lol. Was wondering if\
    \ you could do another set of quants with the new format - would be much appreciated!"
  created_at: 2023-05-23 23:50:48+00:00
  edited: false
  hidden: false
  id: 646d5f684220471ca0c765ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
      fullname: TeH_Venom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TehVenom
      type: user
    createdAt: '2023-05-24T00:52:38.000Z'
    data:
      edited: false
      editors:
      - TehVenom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
          fullname: TeH_Venom
          isHf: false
          isPro: false
          name: TehVenom
          type: user
        html: '<p>GGML try not to break compatibility challenge... Sigh, I''ll update
          all my GGML files this week, thanks for letting me know</p>

          '
        raw: GGML try not to break compatibility challenge... Sigh, I'll update all
          my GGML files this week, thanks for letting me know
        updatedAt: '2023-05-24T00:52:38.480Z'
      numEdits: 0
      reactions: []
    id: 646d5fd62abe5323fe235d48
    type: comment
  author: TehVenom
  content: GGML try not to break compatibility challenge... Sigh, I'll update all
    my GGML files this week, thanks for letting me know
  created_at: 2023-05-23 23:52:38+00:00
  edited: false
  hidden: false
  id: 646d5fd62abe5323fe235d48
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TehVenom/Metharme-13b-GGML
repo_type: model
status: open
target_branch: null
title: Yet another change to GGML quant formats...
