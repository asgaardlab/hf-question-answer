!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gyn111
conflicting_files: null
created_at: 2023-07-25 10:54:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7daa103d8c0ead439ae7c07320d86f14.svg
      fullname: guoyuenan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gyn111
      type: user
    createdAt: '2023-07-25T11:54:06.000Z'
    data:
      edited: false
      editors:
      - gyn111
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9816417098045349
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7daa103d8c0ead439ae7c07320d86f14.svg
          fullname: guoyuenan
          isHf: false
          isPro: false
          name: gyn111
          type: user
        html: '<p>How long did you quantize your model? I have spent several hours......</p>

          '
        raw: How long did you quantize your model? I have spent several hours......
        updatedAt: '2023-07-25T11:54:06.962Z'
      numEdits: 0
      reactions: []
    id: 64bfb7de140491ca9fa00b9f
    type: comment
  author: gyn111
  content: How long did you quantize your model? I have spent several hours......
  created_at: 2023-07-25 10:54:06+00:00
  edited: false
  hidden: false
  id: 64bfb7de140491ca9fa00b9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T12:00:26.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9634039998054504
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I don''t remember exactly but I would expect about 30 minutes.  Several
          hours sounds bad.  What are you using to quantise, AutoGPTQ or GPTQ-for-Llama?
          And what stage of the process are you on?</p>

          <p>I have found some systems where the "packing model" stage seems to take
          forever, because the CPU is too weak. On those systems I usually abort it
          and start again on another system.</p>

          <p>Tell me what the HW is and show me a screenshot of the output and I can
          advise you more.</p>

          <p>Out of interest, why are you quantising your own?  Did you want different
          quantisation parameters?   This is one model where I didn''t go back and
          add extra quantisation options (like group_size + desc_act), but I could
          do that if there is demand for them</p>

          '
        raw: 'I don''t remember exactly but I would expect about 30 minutes.  Several
          hours sounds bad.  What are you using to quantise, AutoGPTQ or GPTQ-for-Llama?
          And what stage of the process are you on?


          I have found some systems where the "packing model" stage seems to take
          forever, because the CPU is too weak. On those systems I usually abort it
          and start again on another system.


          Tell me what the HW is and show me a screenshot of the output and I can
          advise you more.


          Out of interest, why are you quantising your own?  Did you want different
          quantisation parameters?   This is one model where I didn''t go back and
          add extra quantisation options (like group_size + desc_act), but I could
          do that if there is demand for them'
        updatedAt: '2023-07-25T12:00:26.909Z'
      numEdits: 0
      reactions: []
    id: 64bfb95a0b90e8652b08ec2c
    type: comment
  author: TheBloke
  content: 'I don''t remember exactly but I would expect about 30 minutes.  Several
    hours sounds bad.  What are you using to quantise, AutoGPTQ or GPTQ-for-Llama?
    And what stage of the process are you on?


    I have found some systems where the "packing model" stage seems to take forever,
    because the CPU is too weak. On those systems I usually abort it and start again
    on another system.


    Tell me what the HW is and show me a screenshot of the output and I can advise
    you more.


    Out of interest, why are you quantising your own?  Did you want different quantisation
    parameters?   This is one model where I didn''t go back and add extra quantisation
    options (like group_size + desc_act), but I could do that if there is demand for
    them'
  created_at: 2023-07-25 11:00:26+00:00
  edited: false
  hidden: false
  id: 64bfb95a0b90e8652b08ec2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7daa103d8c0ead439ae7c07320d86f14.svg
      fullname: guoyuenan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gyn111
      type: user
    createdAt: '2023-07-26T08:03:36.000Z'
    data:
      edited: true
      editors:
      - gyn111
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9549556374549866
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7daa103d8c0ead439ae7c07320d86f14.svg
          fullname: guoyuenan
          isHf: false
          isPro: false
          name: gyn111
          type: user
        html: '<p>I set bits to 4 and 8 separately,  and group_size to 16 when using
          AutoGPTQ . It seems the model has been load in cuda:0 and costs about 3
          hours to finish quantizing.</p>

          <p>To quantize different parameters, I want to compare different inference
          speed. By the way, have you test your speed of starcoderplus-GPTQ?</p>

          '
        raw: 'I set bits to 4 and 8 separately,  and group_size to 16 when using AutoGPTQ
          . It seems the model has been load in cuda:0 and costs about 3 hours to
          finish quantizing.


          To quantize different parameters, I want to compare different inference
          speed. By the way, have you test your speed of starcoderplus-GPTQ?'
        updatedAt: '2023-07-26T08:09:43.416Z'
      numEdits: 1
      reactions: []
    id: 64c0d358e56520a63d3ebb1a
    type: comment
  author: gyn111
  content: 'I set bits to 4 and 8 separately,  and group_size to 16 when using AutoGPTQ
    . It seems the model has been load in cuda:0 and costs about 3 hours to finish
    quantizing.


    To quantize different parameters, I want to compare different inference speed.
    By the way, have you test your speed of starcoderplus-GPTQ?'
  created_at: 2023-07-26 07:03:36+00:00
  edited: true
  hidden: false
  id: 64c0d358e56520a63d3ebb1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/639c5c448a34ed9a404a956b/jcypw-eh7JzKHTffd0N9l.jpeg?w=200&h=200&f=face
      fullname: Mohamad Alhajar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: malhajar
      type: user
    createdAt: '2023-09-13T11:46:37.000Z'
    data:
      edited: false
      editors:
      - malhajar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9277116656303406
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/639c5c448a34ed9a404a956b/jcypw-eh7JzKHTffd0N9l.jpeg?w=200&h=200&f=face
          fullname: Mohamad Alhajar
          isHf: false
          isPro: false
          name: malhajar
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> How did you overcome\
          \ the problem of packing the model too slow on the CPU. I am using runpod.io\
          \ to quantize my model (70b llama2) but it seems to take forever on the\
          \ packing model phase. any suggestions are widely welcomed :D</p>\n"
        raw: '@TheBloke How did you overcome the problem of packing the model too
          slow on the CPU. I am using runpod.io to quantize my model (70b llama2)
          but it seems to take forever on the packing model phase. any suggestions
          are widely welcomed :D'
        updatedAt: '2023-09-13T11:46:37.531Z'
      numEdits: 0
      reactions: []
    id: 6501a11d92dad11aa5f3a352
    type: comment
  author: malhajar
  content: '@TheBloke How did you overcome the problem of packing the model too slow
    on the CPU. I am using runpod.io to quantize my model (70b llama2) but it seems
    to take forever on the packing model phase. any suggestions are widely welcomed
    :D'
  created_at: 2023-09-13 10:46:37+00:00
  edited: false
  hidden: false
  id: 6501a11d92dad11aa5f3a352
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-13T12:01:27.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.348408967256546
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>try doing this before before running your Python quant script:</p>\n\
          <pre><code class=\"language-shell\">OMP_NUM_THREADS=16 OPENBLAS_NUM_THREADS=16\
          \ MKL_NUM_THREADS=16 VECLIB_MAXIMUM_THREADS=16 NUMEXPR_NUM_THREADS=16 \n\
          export OMP_NUM_THREADS OPENBLAS_NUM_THREADS MKL_NUM_THREADS VECLIB_MAXIMUM_THREADS\
          \ NUMEXPR_NUM_THREADS\n</code></pre>\n<p>Or add the equivalent <code>os.environ[...]\
          \ =</code> calls to the Python code.  But if you do that, you must put them\
          \ at the very top of the script, before any imports or done (except <code>os</code>);\
          \ they only take effect if set before the code is imported.</p>\n<p>If you're\
          \ using my <code>quant_autogptq.py</code> script, you can do this:</p>\n\
          <pre><code class=\"language-shell\">OMP_NUM_THREADS=16 OPENBLAS_NUM_THREADS=16\
          \ MKL_NUM_THREADS=16 VECLIB_MAXIMUM_THREADS=16 NUMEXPR_NUM_THREADS=16  CUDA_VISIBLE_DEVICES=0\
          \ python3 /workspace/ProcessQuantization/quant_autogptq.py /path/to/source/model\
          \ /path/to/output-gptq wikitext --bits 4 --group_size 128 --desc_act 1 --damp\
          \ 0.1 --dtype float16 --seqlen 4096 --num_samples 128  --use_fast --cache_examples\
          \ 1\n</code></pre>\n"
        raw: "try doing this before before running your Python quant script:\n```shell\n\
          OMP_NUM_THREADS=16 OPENBLAS_NUM_THREADS=16 MKL_NUM_THREADS=16 VECLIB_MAXIMUM_THREADS=16\
          \ NUMEXPR_NUM_THREADS=16 \nexport OMP_NUM_THREADS OPENBLAS_NUM_THREADS MKL_NUM_THREADS\
          \ VECLIB_MAXIMUM_THREADS NUMEXPR_NUM_THREADS\n```\n\nOr add the equivalent\
          \ `os.environ[...] =` calls to the Python code.  But if you do that, you\
          \ must put them at the very top of the script, before any imports or done\
          \ (except `os`); they only take effect if set before the code is imported.\n\
          \nIf you're using my `quant_autogptq.py` script, you can do this:\n\n```shell\n\
          OMP_NUM_THREADS=16 OPENBLAS_NUM_THREADS=16 MKL_NUM_THREADS=16 VECLIB_MAXIMUM_THREADS=16\
          \ NUMEXPR_NUM_THREADS=16  CUDA_VISIBLE_DEVICES=0 python3 /workspace/ProcessQuantization/quant_autogptq.py\
          \ /path/to/source/model /path/to/output-gptq wikitext --bits 4 --group_size\
          \ 128 --desc_act 1 --damp 0.1 --dtype float16 --seqlen 4096 --num_samples\
          \ 128  --use_fast --cache_examples 1\n```"
        updatedAt: '2023-09-13T12:02:27.883Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - kuotient
    id: 6501a497d4de250af9da4713
    type: comment
  author: TheBloke
  content: "try doing this before before running your Python quant script:\n```shell\n\
    OMP_NUM_THREADS=16 OPENBLAS_NUM_THREADS=16 MKL_NUM_THREADS=16 VECLIB_MAXIMUM_THREADS=16\
    \ NUMEXPR_NUM_THREADS=16 \nexport OMP_NUM_THREADS OPENBLAS_NUM_THREADS MKL_NUM_THREADS\
    \ VECLIB_MAXIMUM_THREADS NUMEXPR_NUM_THREADS\n```\n\nOr add the equivalent `os.environ[...]\
    \ =` calls to the Python code.  But if you do that, you must put them at the very\
    \ top of the script, before any imports or done (except `os`); they only take\
    \ effect if set before the code is imported.\n\nIf you're using my `quant_autogptq.py`\
    \ script, you can do this:\n\n```shell\nOMP_NUM_THREADS=16 OPENBLAS_NUM_THREADS=16\
    \ MKL_NUM_THREADS=16 VECLIB_MAXIMUM_THREADS=16 NUMEXPR_NUM_THREADS=16  CUDA_VISIBLE_DEVICES=0\
    \ python3 /workspace/ProcessQuantization/quant_autogptq.py /path/to/source/model\
    \ /path/to/output-gptq wikitext --bits 4 --group_size 128 --desc_act 1 --damp\
    \ 0.1 --dtype float16 --seqlen 4096 --num_samples 128  --use_fast --cache_examples\
    \ 1\n```"
  created_at: 2023-09-13 11:01:27+00:00
  edited: true
  hidden: false
  id: 6501a497d4de250af9da4713
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-13T12:04:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9270001649856567
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This often helps because by default the code will try to use all
          available CPU cores during the packing stage.  But on Runpod you usually
          only have a portion of the available CPUs on the host - eg if the host has
          128 CPUs and you rented 1 GPU, you will have 128/8 = 16 CPU cores available.</p>

          <p>The problem is that inside Docker, any command that tries to get the
          total number of CPU cores available will see the number of cores on the
          host, not the number available to the Docker.  </p>

          <p>The result is that the packing stage can over-saturate the CPU, by trying
          to use eg 128 threads when only 16 cores are actually available.  The above
          environment variables tell it to use a specific number of cores.</p>

          '
        raw: "This often helps because by default the code will try to use all available\
          \ CPU cores during the packing stage.  But on Runpod you usually only have\
          \ a portion of the available CPUs on the host - eg if the host has 128 CPUs\
          \ and you rented 1 GPU, you will have 128/8 = 16 CPU cores available.\n\n\
          The problem is that inside Docker, any command that tries to get the total\
          \ number of CPU cores available will see the number of cores on the host,\
          \ not the number available to the Docker.  \n\nThe result is that the packing\
          \ stage can over-saturate the CPU, by trying to use eg 128 threads when\
          \ only 16 cores are actually available.  The above environment variables\
          \ tell it to use a specific number of cores."
        updatedAt: '2023-09-13T12:04:20.820Z'
      numEdits: 0
      reactions: []
    id: 6501a544a28829909ffd060c
    type: comment
  author: TheBloke
  content: "This often helps because by default the code will try to use all available\
    \ CPU cores during the packing stage.  But on Runpod you usually only have a portion\
    \ of the available CPUs on the host - eg if the host has 128 CPUs and you rented\
    \ 1 GPU, you will have 128/8 = 16 CPU cores available.\n\nThe problem is that\
    \ inside Docker, any command that tries to get the total number of CPU cores available\
    \ will see the number of cores on the host, not the number available to the Docker.\
    \  \n\nThe result is that the packing stage can over-saturate the CPU, by trying\
    \ to use eg 128 threads when only 16 cores are actually available.  The above\
    \ environment variables tell it to use a specific number of cores."
  created_at: 2023-09-13 11:04:20+00:00
  edited: false
  hidden: false
  id: 6501a544a28829909ffd060c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/639c5c448a34ed9a404a956b/jcypw-eh7JzKHTffd0N9l.jpeg?w=200&h=200&f=face
      fullname: Mohamad Alhajar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: malhajar
      type: user
    createdAt: '2023-09-13T12:07:45.000Z'
    data:
      edited: false
      editors:
      - malhajar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9882174134254456
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/639c5c448a34ed9a404a956b/jcypw-eh7JzKHTffd0N9l.jpeg?w=200&h=200&f=face
          fullname: Mohamad Alhajar
          isHf: false
          isPro: false
          name: malhajar
          type: user
        html: '<p>Thanks alot for your help! i have spent a fortune trying to understand
          this and it makes alot of sense now. </p>

          '
        raw: 'Thanks alot for your help! i have spent a fortune trying to understand
          this and it makes alot of sense now. '
        updatedAt: '2023-09-13T12:07:45.531Z'
      numEdits: 0
      reactions: []
    id: 6501a611729c097ccd1f4b99
    type: comment
  author: malhajar
  content: 'Thanks alot for your help! i have spent a fortune trying to understand
    this and it makes alot of sense now. '
  created_at: 2023-09-13 11:07:45+00:00
  edited: false
  hidden: false
  id: 6501a611729c097ccd1f4b99
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/starcoderplus-GPTQ
repo_type: model
status: open
target_branch: null
title: Time costs
