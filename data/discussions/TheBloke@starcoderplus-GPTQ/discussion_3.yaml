!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Blevlabs
conflicting_files: null
created_at: 2023-06-10 16:16:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
      fullname: Brayden Levangie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Blevlabs
      type: user
    createdAt: '2023-06-10T17:16:16.000Z'
    data:
      edited: true
      editors:
      - Blevlabs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.585756242275238
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
          fullname: Brayden Levangie
          isHf: false
          isPro: false
          name: Blevlabs
          type: user
        html: "<p>Since the model_basename is not originally provided in the example\
          \ code, I tried this:</p>\n<pre><code>from transformers import AutoTokenizer,\
          \ pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          import argparse\n\nmodel_name_or_path = \"TheBloke/starcoderplus-GPTQ\"\n\
          model_basename = \"gptq_model-4bit--1g.safetensors\"\nuse_triton = False\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
          \nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n     \
          \   model_basename=model_basename,\n        use_safetensors=True,\n    \
          \    trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n\nprint(\"\\n\\n*** Generate:\")\n\ninputs\
          \ = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\"\
          ).to(\"cuda:0\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n\
          </code></pre>\n<p>But I always get the following:</p>\n<pre><code>FileNotFoundError:\
          \ could not find model TheBloke/starcoderplus-GPTQ\n</code></pre>\n<p>When\
          \ I remove the model_basename parameter, it downloads, but I get the following\
          \ error with generate:</p>\n<pre><code>The safetensors archive passed at\
          \ ~/.cache/huggingface/hub/models--TheBloke--starcoderplus-GPTQ/snapshots/aa67ff4fad65fc88f6281f3a2bcc0d648105ef96/gptq_model-4bit--1g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\n\n\n*** Generate:\nTypeError: generate()\
          \ takes 1 positional argument but 2 were given\n</code></pre>\n<p>I am just\
          \ using the original code provided, with no other alterations. I am able\
          \ to load other models from your HF repos with autogptq but not this one\
          \ specifically</p>\n"
        raw: "Since the model_basename is not originally provided in the example code,\
          \ I tried this:\n```\nfrom transformers import AutoTokenizer, pipeline,\
          \ logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          import argparse\n\nmodel_name_or_path = \"TheBloke/starcoderplus-GPTQ\"\n\
          model_basename = \"gptq_model-4bit--1g.safetensors\"\nuse_triton = False\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
          \nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n     \
          \   model_basename=model_basename,\n        use_safetensors=True,\n    \
          \    trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n\nprint(\"\\n\\n*** Generate:\")\n\ninputs\
          \ = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\"\
          ).to(\"cuda:0\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n\
          \n```\nBut I always get the following:\n```\nFileNotFoundError: could not\
          \ find model TheBloke/starcoderplus-GPTQ\n```\n\nWhen I remove the model_basename\
          \ parameter, it downloads, but I get the following error with generate:\n\
          ```\nThe safetensors archive passed at ~/.cache/huggingface/hub/models--TheBloke--starcoderplus-GPTQ/snapshots/aa67ff4fad65fc88f6281f3a2bcc0d648105ef96/gptq_model-4bit--1g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\n\n\n*** Generate:\nTypeError: generate()\
          \ takes 1 positional argument but 2 were given\n```\n\nI am just using the\
          \ original code provided, with no other alterations. I am able to load other\
          \ models from your HF repos with autogptq but not this one specifically"
        updatedAt: '2023-06-10T17:17:15.960Z'
      numEdits: 1
      reactions: []
    id: 6484afe082a77bbb6684dbe4
    type: comment
  author: Blevlabs
  content: "Since the model_basename is not originally provided in the example code,\
    \ I tried this:\n```\nfrom transformers import AutoTokenizer, pipeline, logging\n\
    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\
    \nmodel_name_or_path = \"TheBloke/starcoderplus-GPTQ\"\nmodel_basename = \"gptq_model-4bit--1g.safetensors\"\
    \nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \        model_basename=model_basename,\n        use_safetensors=True,\n     \
    \   trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
    \        quantize_config=None)\n\nprint(\"\\n\\n*** Generate:\")\n\ninputs = tokenizer.encode(\"\
    def print_hello_world():\", return_tensors=\"pt\").to(\"cuda:0\")\noutputs = model.generate(inputs)\n\
    print(tokenizer.decode(outputs[0]))\n\n```\nBut I always get the following:\n\
    ```\nFileNotFoundError: could not find model TheBloke/starcoderplus-GPTQ\n```\n\
    \nWhen I remove the model_basename parameter, it downloads, but I get the following\
    \ error with generate:\n```\nThe safetensors archive passed at ~/.cache/huggingface/hub/models--TheBloke--starcoderplus-GPTQ/snapshots/aa67ff4fad65fc88f6281f3a2bcc0d648105ef96/gptq_model-4bit--1g.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\n\n\n*** Generate:\nTypeError: generate()\
    \ takes 1 positional argument but 2 were given\n```\n\nI am just using the original\
    \ code provided, with no other alterations. I am able to load other models from\
    \ your HF repos with autogptq but not this one specifically"
  created_at: 2023-06-10 16:16:16+00:00
  edited: true
  hidden: false
  id: 6484afe082a77bbb6684dbe4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-10T17:28:37.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8750728964805603
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Hmm you shouldn''t need model_basename for this. Maybe that''s an
          AutoGPTQ bug.</p>

          <p>When it is required, you leave out the <code>.safetensors</code> from
          the end, so it''s<br><code>model_basename=gptq_model-4bit--1g</code></p>

          '
        raw: "Hmm you shouldn't need model_basename for this. Maybe that's an AutoGPTQ\
          \ bug.\n\nWhen it is required, you leave out the `.safetensors` from the\
          \ end, so it's \n`model_basename=gptq_model-4bit--1g`"
        updatedAt: '2023-06-10T17:28:37.310Z'
      numEdits: 0
      reactions: []
    id: 6484b2c55d8d3eeb85c828a7
    type: comment
  author: TheBloke
  content: "Hmm you shouldn't need model_basename for this. Maybe that's an AutoGPTQ\
    \ bug.\n\nWhen it is required, you leave out the `.safetensors` from the end,\
    \ so it's \n`model_basename=gptq_model-4bit--1g`"
  created_at: 2023-06-10 16:28:37+00:00
  edited: false
  hidden: false
  id: 6484b2c55d8d3eeb85c828a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
      fullname: Brayden Levangie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Blevlabs
      type: user
    createdAt: '2023-06-10T17:31:01.000Z'
    data:
      edited: false
      editors:
      - Blevlabs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5076754689216614
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
          fullname: Brayden Levangie
          isHf: false
          isPro: false
          name: Blevlabs
          type: user
        html: "<p>Thank you for the insight. Do you have an idea of why the generate()\
          \ issue is occurring when I remove the model_basename? Here is my code when\
          \ I do:</p>\n<pre><code>from transformers import AutoTokenizer, pipeline,\
          \ logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          import argparse\n\nmodel_name_or_path = \"TheBloke/starcoderplus-GPTQ\"\n\
          use_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        use_safetensors=True,\n        trust_remote_code=True,\n      \
          \  device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          \nprint(\"\\n\\n*** Generate:\")\n\ninputs = tokenizer.encode(\"def print_hello_world():\"\
          , return_tensors=\"pt\").to(\"cuda:0\")\noutputs = model.generate(inputs)\n\
          print(tokenizer.decode(outputs[0]))\n</code></pre>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/619c043362fd6a7fea1e4cdd/C_mc6kZUxnN-ROF6UB_NJ.png\"\
          ><img alt=\"Screenshot_20230610_133041.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/619c043362fd6a7fea1e4cdd/C_mc6kZUxnN-ROF6UB_NJ.png\"\
          ></a></p>\n"
        raw: "Thank you for the insight. Do you have an idea of why the generate()\
          \ issue is occurring when I remove the model_basename? Here is my code when\
          \ I do:\n```\nfrom transformers import AutoTokenizer, pipeline, logging\n\
          from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\
          \nmodel_name_or_path = \"TheBloke/starcoderplus-GPTQ\"\nuse_triton = False\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
          \nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n     \
          \   use_safetensors=True,\n        trust_remote_code=True,\n        device=\"\
          cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          \nprint(\"\\n\\n*** Generate:\")\n\ninputs = tokenizer.encode(\"def print_hello_world():\"\
          , return_tensors=\"pt\").to(\"cuda:0\")\noutputs = model.generate(inputs)\n\
          print(tokenizer.decode(outputs[0]))\n```\n![Screenshot_20230610_133041.png](https://cdn-uploads.huggingface.co/production/uploads/619c043362fd6a7fea1e4cdd/C_mc6kZUxnN-ROF6UB_NJ.png)"
        updatedAt: '2023-06-10T17:31:01.296Z'
      numEdits: 0
      reactions: []
    id: 6484b3557acbe03eb19ec000
    type: comment
  author: Blevlabs
  content: "Thank you for the insight. Do you have an idea of why the generate() issue\
    \ is occurring when I remove the model_basename? Here is my code when I do:\n\
    ```\nfrom transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
    \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\nmodel_name_or_path\
    \ = \"TheBloke/starcoderplus-GPTQ\"\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \        use_safetensors=True,\n        trust_remote_code=True,\n        device=\"\
    cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\nprint(\"\
    \\n\\n*** Generate:\")\n\ninputs = tokenizer.encode(\"def print_hello_world():\"\
    , return_tensors=\"pt\").to(\"cuda:0\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n\
    ```\n![Screenshot_20230610_133041.png](https://cdn-uploads.huggingface.co/production/uploads/619c043362fd6a7fea1e4cdd/C_mc6kZUxnN-ROF6UB_NJ.png)"
  created_at: 2023-06-10 16:31:01+00:00
  edited: false
  hidden: false
  id: 6484b3557acbe03eb19ec000
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-11T01:06:43.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8111779689788818
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>As discussed on Discord:</p>

          <p>This is caused by this bug: <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/pull/135">https://github.com/PanQiWei/AutoGPTQ/pull/135</a></p>

          <p>Workaround is <code>model.generate(inputs=inputs)</code></p>

          '
        raw: 'As discussed on Discord:


          This is caused by this bug: https://github.com/PanQiWei/AutoGPTQ/pull/135


          Workaround is `model.generate(inputs=inputs)`'
        updatedAt: '2023-06-11T01:06:43.738Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - CLRafaelR
    id: 64851e23047d72a5441054a2
    type: comment
  author: TheBloke
  content: 'As discussed on Discord:


    This is caused by this bug: https://github.com/PanQiWei/AutoGPTQ/pull/135


    Workaround is `model.generate(inputs=inputs)`'
  created_at: 2023-06-11 00:06:43+00:00
  edited: false
  hidden: false
  id: 64851e23047d72a5441054a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
      fullname: Brayden Levangie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Blevlabs
      type: user
    createdAt: '2023-06-11T01:21:58.000Z'
    data:
      edited: false
      editors:
      - Blevlabs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9000625610351562
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
          fullname: Brayden Levangie
          isHf: false
          isPro: false
          name: Blevlabs
          type: user
        html: '<p>Fix: i just needed to set inputs=inputs on the generate command,
          TheBloke submitted a fix but did not have his MR accepted yet on the autogptq
          github.<br>Ensure the script uses the no-model-basename version I provided
          above.</p>

          '
        raw: 'Fix: i just needed to set inputs=inputs on the generate command, TheBloke
          submitted a fix but did not have his MR accepted yet on the autogptq github.

          Ensure the script uses the no-model-basename version I provided above.'
        updatedAt: '2023-06-11T01:21:58.006Z'
      numEdits: 0
      reactions: []
    id: 648521b610e6b87455a0d95b
    type: comment
  author: Blevlabs
  content: 'Fix: i just needed to set inputs=inputs on the generate command, TheBloke
    submitted a fix but did not have his MR accepted yet on the autogptq github.

    Ensure the script uses the no-model-basename version I provided above.'
  created_at: 2023-06-11 00:21:58+00:00
  edited: false
  hidden: false
  id: 648521b610e6b87455a0d95b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
      fullname: Brayden Levangie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Blevlabs
      type: user
    createdAt: '2023-06-11T01:22:11.000Z'
    data:
      status: closed
    id: 648521c3171ea7b32f719819
    type: status-change
  author: Blevlabs
  created_at: 2023-06-11 00:22:11+00:00
  id: 648521c3171ea7b32f719819
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/starcoderplus-GPTQ
repo_type: model
status: closed
target_branch: null
title: Issues running model
