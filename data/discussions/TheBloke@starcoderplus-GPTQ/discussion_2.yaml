!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Boffy
conflicting_files: null
created_at: 2023-06-09 14:18:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
      fullname: Paul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boffy
      type: user
    createdAt: '2023-06-09T15:18:20.000Z'
    data:
      edited: false
      editors:
      - Boffy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8865641355514526
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
          fullname: Paul
          isHf: false
          isPro: false
          name: Boffy
          type: user
        html: '<p>using text-generation-webui with this model <a href="https://huggingface.co/spaces/HuggingFaceH4/starchat-playground">https://huggingface.co/spaces/HuggingFaceH4/starchat-playground</a>
          .. it doesn''t  even seem remotely on the same level with questions and
          responses generated.. not sure if I''m missing some setting instruct mode?
          /setup to run locally.. what model is the online version using???</p>

          '
        raw: "using text-generation-webui with this model https://huggingface.co/spaces/HuggingFaceH4/starchat-playground\
          \ .. it doesn't  even seem remotely on the same level with questions and\
          \ responses generated.. not sure if I'm missing some setting instruct mode?\
          \ /setup to run locally.. what model is the online version using???\r\n\r\
          \n"
        updatedAt: '2023-06-09T15:18:20.463Z'
      numEdits: 0
      reactions: []
    id: 648342bcca01791c4733612e
    type: comment
  author: Boffy
  content: "using text-generation-webui with this model https://huggingface.co/spaces/HuggingFaceH4/starchat-playground\
    \ .. it doesn't  even seem remotely on the same level with questions and responses\
    \ generated.. not sure if I'm missing some setting instruct mode? /setup to run\
    \ locally.. what model is the online version using???\r\n\r\n"
  created_at: 2023-06-09 14:18:20+00:00
  edited: false
  hidden: false
  id: 648342bcca01791c4733612e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-09T15:44:04.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7665431499481201
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>What model are you testing? Because you''ve posted in StarCoder
          Plus, but linked StarChat Beta, which are different models with different
          capabilities and prompting methods.</p>

          <p>I have a StarChat Beta model here: <a href="https://huggingface.co/TheBloke/starchat-beta-GPTQ">https://huggingface.co/TheBloke/starchat-beta-GPTQ</a></p>

          <p>If you are using StarChat Beta like you linked, are you using the right
          prompt template and tokens?  I just edited the README to make it clearer
          what the prompt template is:</p>

          <h2 id="prompt-template">Prompt template</h2>

          <pre><code>&lt;|system|&gt; system message goes here &lt;|end|&gt;

          &lt;|user|&gt; prompt goes here &lt;|end|&gt;

          &lt;|assistant|&gt;

          </code></pre>

          <p>Example:</p>

          <pre><code>&lt;|system|&gt; Below is a conversation between a human user
          and a helpful AI coding assistant. &lt;|end|&gt;

          &lt;|user|&gt; How do I sort a list in Python? &lt;|end|&gt;

          &lt;|assistant|&gt;

          </code></pre>

          <p>If you are using StarCoder Plus then please be aware that it is not an
          instruction tuned model.    From its README:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/l9gMEQ1w27qDiq-S9DQEz.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/l9gMEQ1w27qDiq-S9DQEz.png"></a></p>

          <p>So it should be able to auto-complete, or fill in the middle. But it''s
          not going to work with "How do I sort a list in Python?".  That''s what
          StarChat Beta is for.</p>

          '
        raw: 'What model are you testing? Because you''ve posted in StarCoder Plus,
          but linked StarChat Beta, which are different models with different capabilities
          and prompting methods.


          I have a StarChat Beta model here: https://huggingface.co/TheBloke/starchat-beta-GPTQ


          If you are using StarChat Beta like you linked, are you using the right
          prompt template and tokens?  I just edited the README to make it clearer
          what the prompt template is:


          ## Prompt template


          ```

          <|system|> system message goes here <|end|>

          <|user|> prompt goes here <|end|>

          <|assistant|>

          ```


          Example:


          ```

          <|system|> Below is a conversation between a human user and a helpful AI
          coding assistant. <|end|>

          <|user|> How do I sort a list in Python? <|end|>

          <|assistant|>

          ```



          If you are using StarCoder Plus then please be aware that it is not an instruction
          tuned model.    From its README:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/l9gMEQ1w27qDiq-S9DQEz.png)


          So it should be able to auto-complete, or fill in the middle. But it''s
          not going to work with "How do I sort a list in Python?".  That''s what
          StarChat Beta is for.'
        updatedAt: '2023-06-09T15:44:26.311Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - imoc
    id: 648348c4cd6d864c32da6827
    type: comment
  author: TheBloke
  content: 'What model are you testing? Because you''ve posted in StarCoder Plus,
    but linked StarChat Beta, which are different models with different capabilities
    and prompting methods.


    I have a StarChat Beta model here: https://huggingface.co/TheBloke/starchat-beta-GPTQ


    If you are using StarChat Beta like you linked, are you using the right prompt
    template and tokens?  I just edited the README to make it clearer what the prompt
    template is:


    ## Prompt template


    ```

    <|system|> system message goes here <|end|>

    <|user|> prompt goes here <|end|>

    <|assistant|>

    ```


    Example:


    ```

    <|system|> Below is a conversation between a human user and a helpful AI coding
    assistant. <|end|>

    <|user|> How do I sort a list in Python? <|end|>

    <|assistant|>

    ```



    If you are using StarCoder Plus then please be aware that it is not an instruction
    tuned model.    From its README:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/l9gMEQ1w27qDiq-S9DQEz.png)


    So it should be able to auto-complete, or fill in the middle. But it''s not going
    to work with "How do I sort a list in Python?".  That''s what StarChat Beta is
    for.'
  created_at: 2023-06-09 14:44:04+00:00
  edited: true
  hidden: false
  id: 648348c4cd6d864c32da6827
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
      fullname: Paul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Boffy
      type: user
    createdAt: '2023-06-09T15:50:45.000Z'
    data:
      edited: false
      editors:
      - Boffy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8960132002830505
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/665c4de91c7a4237c7f94ac2b340a176.svg
          fullname: Paul
          isHf: false
          isPro: false
          name: Boffy
          type: user
        html: '<p>ok I might have got confused on that...... downloading starchat-beta.ggmlv3.q5_1.bin
          now.. hopefully I get it working.. the online demo  <a href="https://huggingface.co/spaces/HuggingFaceH4/starchat-playground">https://huggingface.co/spaces/HuggingFaceH4/starchat-playground</a>  beta..
          worked pretty well so hopefully locally it will be the same.. definitly
          faster online (is it just much faster gpu hardware behind the scene being
          used for that?)... I''m not even sure I''m getting the speed out of my local
          setup.. 4090rtx what is the average token speed I should expect out of a
          card like that on ggml or gptq, using windows and the one-click-installers
          and text-generation-webui all upto date with git repo''s and updated ..just
          not sure if I''m missing something I assume the one-click-installer is getting
          all the correct libraries I did specifiy nvidia in the install for it and
          update.</p>

          '
        raw: ok I might have got confused on that...... downloading starchat-beta.ggmlv3.q5_1.bin
          now.. hopefully I get it working.. the online demo  https://huggingface.co/spaces/HuggingFaceH4/starchat-playground  beta..
          worked pretty well so hopefully locally it will be the same.. definitly
          faster online (is it just much faster gpu hardware behind the scene being
          used for that?)... I'm not even sure I'm getting the speed out of my local
          setup.. 4090rtx what is the average token speed I should expect out of a
          card like that on ggml or gptq, using windows and the one-click-installers
          and text-generation-webui all upto date with git repo's and updated ..just
          not sure if I'm missing something I assume the one-click-installer is getting
          all the correct libraries I did specifiy nvidia in the install for it and
          update.
        updatedAt: '2023-06-09T15:50:45.198Z'
      numEdits: 0
      reactions: []
    id: 64834a5555397d952ed3663e
    type: comment
  author: Boffy
  content: ok I might have got confused on that...... downloading starchat-beta.ggmlv3.q5_1.bin
    now.. hopefully I get it working.. the online demo  https://huggingface.co/spaces/HuggingFaceH4/starchat-playground  beta..
    worked pretty well so hopefully locally it will be the same.. definitly faster
    online (is it just much faster gpu hardware behind the scene being used for that?)...
    I'm not even sure I'm getting the speed out of my local setup.. 4090rtx what is
    the average token speed I should expect out of a card like that on ggml or gptq,
    using windows and the one-click-installers and text-generation-webui all upto
    date with git repo's and updated ..just not sure if I'm missing something I assume
    the one-click-installer is getting all the correct libraries I did specifiy nvidia
    in the install for it and update.
  created_at: 2023-06-09 14:50:45+00:00
  edited: false
  hidden: false
  id: 64834a5555397d952ed3663e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63fa476f4380ab0cb953a450/mwM-Xl70QAvYwKrvzyw_C.png?w=200&h=200&f=face
      fullname: hermes t
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: psyberm
      type: user
    createdAt: '2023-06-09T19:27:40.000Z'
    data:
      edited: true
      editors:
      - psyberm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9339571595191956
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63fa476f4380ab0cb953a450/mwM-Xl70QAvYwKrvzyw_C.png?w=200&h=200&f=face
          fullname: hermes t
          isHf: false
          isPro: false
          name: psyberm
          type: user
        html: '<blockquote>

          <p>4090rtx what is the average token speed I should expect out of a card
          like that on ggml or gptq</p>

          </blockquote>

          <p>it''s going to be slow(er) compared to something like google colab or
          hf, they''re using farms of computers with GPUs like the A1000 to run this
          infrastructure. for me (TITAN RTX) on average it takes anywhere from ~2-15
          seconds to generate a full response depending on length.</p>

          '
        raw: '> 4090rtx what is the average token speed I should expect out of a card
          like that on ggml or gptq


          it''s going to be slow(er) compared to something like google colab or hf,
          they''re using farms of computers with GPUs like the A1000 to run this infrastructure.
          for me (TITAN RTX) on average it takes anywhere from ~2-15 seconds to generate
          a full response depending on length.'
        updatedAt: '2023-06-09T19:28:01.279Z'
      numEdits: 1
      reactions: []
    id: 64837d2cce0704c450fab02b
    type: comment
  author: psyberm
  content: '> 4090rtx what is the average token speed I should expect out of a card
    like that on ggml or gptq


    it''s going to be slow(er) compared to something like google colab or hf, they''re
    using farms of computers with GPUs like the A1000 to run this infrastructure.
    for me (TITAN RTX) on average it takes anywhere from ~2-15 seconds to generate
    a full response depending on length.'
  created_at: 2023-06-09 18:27:40+00:00
  edited: true
  hidden: false
  id: 64837d2cce0704c450fab02b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/starcoderplus-GPTQ
repo_type: model
status: open
target_branch: null
title: 'seems broken.. '
