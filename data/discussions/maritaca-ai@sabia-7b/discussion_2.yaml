!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joaopaulopresa
conflicting_files: null
created_at: 2023-11-30 18:32:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/08a5d4a4b7a1c9459f3cbe1fe1671bf7.svg
      fullname: "Jo\xE3o Paulo Cavalcante Presa"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joaopaulopresa
      type: user
    createdAt: '2023-11-30T18:32:13.000Z'
    data:
      edited: false
      editors:
      - joaopaulopresa
      hidden: false
      identifiedLanguage:
        language: pt
        probability: 0.9726784825325012
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/08a5d4a4b7a1c9459f3cbe1fe1671bf7.svg
          fullname: "Jo\xE3o Paulo Cavalcante Presa"
          isHf: false
          isPro: false
          name: joaopaulopresa
          type: user
        html: "<h1 id=\"problemas-com-repeti\xE7\xF5es-indefinidas-ap\xF3s-fine-tuning-do-modelo-de-linguagem\"\
          >Problemas com Repeti\xE7\xF5es Indefinidas ap\xF3s Fine Tuning do Modelo\
          \ de Linguagem</h1>\n<p>Ol\xE1,</p>\n<p>Recentemente, venho enfrentando\
          \ um desafio com meu modelo de linguagem que persiste em repetir respostas\
          \ indefinidamente ap\xF3s o processo de <em>fine tuning</em>. Ao analisar\
          \ a configura\xE7\xE3o do modelo, notei que o <code>pad_token_id</code>\
          \ est\xE1 definido como 0, que \xE9 o mesmo valor atribu\xEDdo ao <code>&lt;unk&gt;</code>.\
          \ N\xE3o tenho certeza se isso est\xE1 afetando o desempenho.</p>\n<p>Al\xE9\
          m disso, tentei incluir o <code>eos_token</code> (<code>&lt;/s&gt;</code>)\
          \ no prompt de treino, mas parece que o modelo n\xE3o reconhece esse token\
          \ como um sinal para cessar a gera\xE7\xE3o de resposta. Verifiquei tamb\xE9\
          m o arquivo <code>special_tokens_map.json</code> e observei que n\xE3o cont\xE9\
          m o <code>pad_token</code>. Igualmente, no <code>tokenizer_config.json</code>,\
          \ o par\xE2metro <code>legacy</code> n\xE3o est\xE1 presente, n\xE3o da\
          \ pra saber se usar como <code>false</code> ou <code>true</code>.</p>\n\
          <p>Gostaria de solicitar conselhos sobre como realizar o <em>fine tuning</em>\
          \ de forma eficaz, especialmente no que diz respeito ao uso correto do <code>pad_token</code>.\
          \ Tamb\xE9m agradeceria orienta\xE7\xF5es sobre como carregar o tokenizer\
          \ apropriadamente para treino e elaborar prompts eficientes para evitar\
          \ essas repeti\xE7\xF5es indesejadas nas respostas.</p>\n<p>Qualquer ajuda\
          \ para resolver este problema seria extremamente valiosa. Obrigado.</p>\n"
        raw: "# Problemas com Repeti\xE7\xF5es Indefinidas ap\xF3s Fine Tuning do\
          \ Modelo de Linguagem\r\n\r\nOl\xE1,\r\n\r\nRecentemente, venho enfrentando\
          \ um desafio com meu modelo de linguagem que persiste em repetir respostas\
          \ indefinidamente ap\xF3s o processo de _fine tuning_. Ao analisar a configura\xE7\
          \xE3o do modelo, notei que o `pad_token_id` est\xE1 definido como 0, que\
          \ \xE9 o mesmo valor atribu\xEDdo ao `<unk>`. N\xE3o tenho certeza se isso\
          \ est\xE1 afetando o desempenho.\r\n\r\nAl\xE9m disso, tentei incluir o\
          \ `eos_token` (`</s>`) no prompt de treino, mas parece que o modelo n\xE3\
          o reconhece esse token como um sinal para cessar a gera\xE7\xE3o de resposta.\
          \ Verifiquei tamb\xE9m o arquivo `special_tokens_map.json` e observei que\
          \ n\xE3o cont\xE9m o `pad_token`. Igualmente, no `tokenizer_config.json`,\
          \ o par\xE2metro `legacy` n\xE3o est\xE1 presente, n\xE3o da pra saber se\
          \ usar como `false` ou `true`.\r\n\r\nGostaria de solicitar conselhos sobre\
          \ como realizar o _fine tuning_ de forma eficaz, especialmente no que diz\
          \ respeito ao uso correto do `pad_token`. Tamb\xE9m agradeceria orienta\xE7\
          \xF5es sobre como carregar o tokenizer apropriadamente para treino e elaborar\
          \ prompts eficientes para evitar essas repeti\xE7\xF5es indesejadas nas\
          \ respostas.\r\n\r\nQualquer ajuda para resolver este problema seria extremamente\
          \ valiosa. Obrigado."
        updatedAt: '2023-11-30T18:32:13.409Z'
      numEdits: 0
      reactions: []
    id: 6568d52d8e96268a527c9bcc
    type: comment
  author: joaopaulopresa
  content: "# Problemas com Repeti\xE7\xF5es Indefinidas ap\xF3s Fine Tuning do Modelo\
    \ de Linguagem\r\n\r\nOl\xE1,\r\n\r\nRecentemente, venho enfrentando um desafio\
    \ com meu modelo de linguagem que persiste em repetir respostas indefinidamente\
    \ ap\xF3s o processo de _fine tuning_. Ao analisar a configura\xE7\xE3o do modelo,\
    \ notei que o `pad_token_id` est\xE1 definido como 0, que \xE9 o mesmo valor atribu\xED\
    do ao `<unk>`. N\xE3o tenho certeza se isso est\xE1 afetando o desempenho.\r\n\
    \r\nAl\xE9m disso, tentei incluir o `eos_token` (`</s>`) no prompt de treino,\
    \ mas parece que o modelo n\xE3o reconhece esse token como um sinal para cessar\
    \ a gera\xE7\xE3o de resposta. Verifiquei tamb\xE9m o arquivo `special_tokens_map.json`\
    \ e observei que n\xE3o cont\xE9m o `pad_token`. Igualmente, no `tokenizer_config.json`,\
    \ o par\xE2metro `legacy` n\xE3o est\xE1 presente, n\xE3o da pra saber se usar\
    \ como `false` ou `true`.\r\n\r\nGostaria de solicitar conselhos sobre como realizar\
    \ o _fine tuning_ de forma eficaz, especialmente no que diz respeito ao uso correto\
    \ do `pad_token`. Tamb\xE9m agradeceria orienta\xE7\xF5es sobre como carregar\
    \ o tokenizer apropriadamente para treino e elaborar prompts eficientes para evitar\
    \ essas repeti\xE7\xF5es indesejadas nas respostas.\r\n\r\nQualquer ajuda para\
    \ resolver este problema seria extremamente valiosa. Obrigado."
  created_at: 2023-11-30 18:32:13+00:00
  edited: false
  hidden: false
  id: 6568d52d8e96268a527c9bcc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/695623d2efacc079b206c6ccbeb92e6f.svg
      fullname: Rodrigo Nogueira
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rodrigo-nogueira
      type: user
    createdAt: '2023-12-08T17:01:44.000Z'
    data:
      edited: true
      editors:
      - rodrigo-nogueira
      hidden: false
      identifiedLanguage:
        language: pt
        probability: 0.49774909019470215
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/695623d2efacc079b206c6ccbeb92e6f.svg
          fullname: Rodrigo Nogueira
          isHf: false
          isPro: false
          name: rodrigo-nogueira
          type: user
        html: "<p>Ol\xE1 Jo\xE3o,</p>\n<p>Isso parece ser um bug to AutoTokenizer.\
          \ Por favor tente usar use_fast=False?</p>\n<pre><code>tokenizer_sabia =\
          \ AutoTokenizer.from_pretrained('maritaca-ai/sabia-7b', use_fast=False)\n\
          tokenizer_sabia.tokenize('hello&lt;/s&gt;')\n&gt;&gt;&gt; ['\u2581hello',\
          \ '&lt;/s&gt;']\n</code></pre>\n"
        raw: "Ol\xE1 Jo\xE3o,\n\nIsso parece ser um bug to AutoTokenizer. Por favor\
          \ tente usar use_fast=False?\n```\ntokenizer_sabia = AutoTokenizer.from_pretrained('maritaca-ai/sabia-7b',\
          \ use_fast=False)\ntokenizer_sabia.tokenize('hello</s>')\n>>> ['\u2581hello',\
          \ '</s>']\n```"
        updatedAt: '2023-12-08T17:02:18.107Z'
      numEdits: 2
      reactions: []
    id: 65734bf806fdcd4ca9048048
    type: comment
  author: rodrigo-nogueira
  content: "Ol\xE1 Jo\xE3o,\n\nIsso parece ser um bug to AutoTokenizer. Por favor\
    \ tente usar use_fast=False?\n```\ntokenizer_sabia = AutoTokenizer.from_pretrained('maritaca-ai/sabia-7b',\
    \ use_fast=False)\ntokenizer_sabia.tokenize('hello</s>')\n>>> ['\u2581hello',\
    \ '</s>']\n```"
  created_at: 2023-12-08 17:01:44+00:00
  edited: true
  hidden: false
  id: 65734bf806fdcd4ca9048048
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/695623d2efacc079b206c6ccbeb92e6f.svg
      fullname: Rodrigo Nogueira
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rodrigo-nogueira
      type: user
    createdAt: '2023-12-08T17:56:29.000Z'
    data:
      edited: false
      editors:
      - rodrigo-nogueira
      hidden: false
      identifiedLanguage:
        language: pt
        probability: 0.6064165234565735
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/695623d2efacc079b206c6ccbeb92e6f.svg
          fullname: Rodrigo Nogueira
          isHf: false
          isPro: false
          name: rodrigo-nogueira
          type: user
        html: '<p>Parece que vai ser consertado nessa PR: <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/26678">https://github.com/huggingface/transformers/pull/26678</a><br>Enquanto
          isso, tem que usar o use_fast=False</p>

          '
        raw: 'Parece que vai ser consertado nessa PR: https://github.com/huggingface/transformers/pull/26678

          Enquanto isso, tem que usar o use_fast=False'
        updatedAt: '2023-12-08T17:56:29.150Z'
      numEdits: 0
      reactions: []
    id: 657358cd244aefdfc461ad2d
    type: comment
  author: rodrigo-nogueira
  content: 'Parece que vai ser consertado nessa PR: https://github.com/huggingface/transformers/pull/26678

    Enquanto isso, tem que usar o use_fast=False'
  created_at: 2023-12-08 17:56:29+00:00
  edited: false
  hidden: false
  id: 657358cd244aefdfc461ad2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/695623d2efacc079b206c6ccbeb92e6f.svg
      fullname: Rodrigo Nogueira
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rodrigo-nogueira
      type: user
    createdAt: '2023-12-08T17:58:06.000Z'
    data:
      edited: false
      editors:
      - rodrigo-nogueira
      hidden: false
      identifiedLanguage:
        language: pt
        probability: 0.5757388472557068
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/695623d2efacc079b206c6ccbeb92e6f.svg
          fullname: Rodrigo Nogueira
          isHf: false
          isPro: false
          name: rodrigo-nogueira
          type: user
        html: '<p>Quanto ao pad_token para finetuning, basta atribuir tokenizer.pad_token_id=0
          e deveria funcionar normalmente.</p>

          '
        raw: Quanto ao pad_token para finetuning, basta atribuir tokenizer.pad_token_id=0
          e deveria funcionar normalmente.
        updatedAt: '2023-12-08T17:58:06.811Z'
      numEdits: 0
      reactions: []
    id: 6573592eca03b6c5149142cc
    type: comment
  author: rodrigo-nogueira
  content: Quanto ao pad_token para finetuning, basta atribuir tokenizer.pad_token_id=0
    e deveria funcionar normalmente.
  created_at: 2023-12-08 17:58:06+00:00
  edited: false
  hidden: false
  id: 6573592eca03b6c5149142cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/695623d2efacc079b206c6ccbeb92e6f.svg
      fullname: Rodrigo Nogueira
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rodrigo-nogueira
      type: user
    createdAt: '2023-12-15T11:52:59.000Z'
    data:
      edited: false
      editors:
      - rodrigo-nogueira
      hidden: false
      identifiedLanguage:
        language: pt
        probability: 0.9979216456413269
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/695623d2efacc079b206c6ccbeb92e6f.svg
          fullname: Rodrigo Nogueira
          isHf: false
          isPro: false
          name: rodrigo-nogueira
          type: user
        html: "<p>Fechando esta discuss\xE3o, mas sinta-se a vontade para reabri-la\
          \ caso tenha mais perguntas.</p>\n"
        raw: "Fechando esta discuss\xE3o, mas sinta-se a vontade para reabri-la caso\
          \ tenha mais perguntas."
        updatedAt: '2023-12-15T11:52:59.661Z'
      numEdits: 0
      reactions: []
      relatedEventId: 657c3e1b0fe000713d3c597f
    id: 657c3e1b0fe000713d3c597c
    type: comment
  author: rodrigo-nogueira
  content: "Fechando esta discuss\xE3o, mas sinta-se a vontade para reabri-la caso\
    \ tenha mais perguntas."
  created_at: 2023-12-15 11:52:59+00:00
  edited: false
  hidden: false
  id: 657c3e1b0fe000713d3c597c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/695623d2efacc079b206c6ccbeb92e6f.svg
      fullname: Rodrigo Nogueira
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rodrigo-nogueira
      type: user
    createdAt: '2023-12-15T11:52:59.000Z'
    data:
      status: closed
    id: 657c3e1b0fe000713d3c597f
    type: status-change
  author: rodrigo-nogueira
  created_at: 2023-12-15 11:52:59+00:00
  id: 657c3e1b0fe000713d3c597f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: maritaca-ai/sabia-7b
repo_type: model
status: closed
target_branch: null
title: "Problemas com Repeti\xE7\xF5es Indefinidas Ap\xF3s Fine Tuning do Modelo de\
  \ Linguagem"
