!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Iamexperimenting
conflicting_files: null
created_at: 2023-07-03 19:48:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9c8ef28358d8ff81e6bc1dc4558de43d.svg
      fullname: IamexperimentingNow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Iamexperimenting
      type: user
    createdAt: '2023-07-03T20:48:22.000Z'
    data:
      edited: false
      editors:
      - Iamexperimenting
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7533942461013794
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9c8ef28358d8ff81e6bc1dc4558de43d.svg
          fullname: IamexperimentingNow
          isHf: false
          isPro: false
          name: Iamexperimenting
          type: user
        html: '<p>Hi,</p>

          <p>Can someone tell me how to run question answering model using LLama?
          I''m trying to build QuestionAnswering model , when I run its not giving
          correct answer even though passing context along it.</p>

          <pre><code>from langchain import PromptTemplate, HugginFacePipeline, LLMChain

          from transformers import pipeline

          import torch

          template  = """Read this article and answer the below question {context}\n
          {question} \n if you don;t know the answer please say "I don''t know" don''t
          make up on your own."""

          prompt = PromptTemplate(template=template, input_variables=["context", "question"])

          model_name = "openlm-research/llama-7b-hf"

          hf_pipe = HugginFacePipeline(pipeline=pipeline(model=model_name, device=''cuda:0'',
          torch_dtype=torch.float16, max_new_tokens=512))

          llm_chain = LLMChain(prompt=prompt, llm=hf_pipe)


          torch.cuda.empty.cache()

          result = llm_chain.run(context=input_context, question=user_question)

          </code></pre>

          <p>I''m using the above script but it is not giving correct answer even
          though answer present in the context, can someone help me here?</p>

          '
        raw: "Hi,\r\n\r\nCan someone tell me how to run question answering model using\
          \ LLama? I'm trying to build QuestionAnswering model , when I run its not\
          \ giving correct answer even though passing context along it.\r\n\r\n```\r\
          \nfrom langchain import PromptTemplate, HugginFacePipeline, LLMChain\r\n\
          from transformers import pipeline\r\nimport torch\r\ntemplate  = \"\"\"\
          Read this article and answer the below question {context}\\n {question}\
          \ \\n if you don;t know the answer please say \"I don't know\" don't make\
          \ up on your own.\"\"\"\r\nprompt = PromptTemplate(template=template, input_variables=[\"\
          context\", \"question\"])\r\nmodel_name = \"openlm-research/llama-7b-hf\"\
          \r\nhf_pipe = HugginFacePipeline(pipeline=pipeline(model=model_name, device='cuda:0',\
          \ torch_dtype=torch.float16, max_new_tokens=512))\r\nllm_chain = LLMChain(prompt=prompt,\
          \ llm=hf_pipe)\r\n\r\ntorch.cuda.empty.cache()\r\nresult = llm_chain.run(context=input_context,\
          \ question=user_question)\r\n\r\n```\r\n\r\nI'm using the above script but\
          \ it is not giving correct answer even though answer present in the context,\
          \ can someone help me here?"
        updatedAt: '2023-07-03T20:48:22.514Z'
      numEdits: 0
      reactions: []
    id: 64a3341617b9f57eaec36d2a
    type: comment
  author: Iamexperimenting
  content: "Hi,\r\n\r\nCan someone tell me how to run question answering model using\
    \ LLama? I'm trying to build QuestionAnswering model , when I run its not giving\
    \ correct answer even though passing context along it.\r\n\r\n```\r\nfrom langchain\
    \ import PromptTemplate, HugginFacePipeline, LLMChain\r\nfrom transformers import\
    \ pipeline\r\nimport torch\r\ntemplate  = \"\"\"Read this article and answer the\
    \ below question {context}\\n {question} \\n if you don;t know the answer please\
    \ say \"I don't know\" don't make up on your own.\"\"\"\r\nprompt = PromptTemplate(template=template,\
    \ input_variables=[\"context\", \"question\"])\r\nmodel_name = \"openlm-research/llama-7b-hf\"\
    \r\nhf_pipe = HugginFacePipeline(pipeline=pipeline(model=model_name, device='cuda:0',\
    \ torch_dtype=torch.float16, max_new_tokens=512))\r\nllm_chain = LLMChain(prompt=prompt,\
    \ llm=hf_pipe)\r\n\r\ntorch.cuda.empty.cache()\r\nresult = llm_chain.run(context=input_context,\
    \ question=user_question)\r\n\r\n```\r\n\r\nI'm using the above script but it\
    \ is not giving correct answer even though answer present in the context, can\
    \ someone help me here?"
  created_at: 2023-07-03 19:48:22+00:00
  edited: false
  hidden: false
  id: 64a3341617b9f57eaec36d2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64399cb9d45e8db3e28877c4/qtmSVmgH86jDkPBYVZ7B-.jpeg?w=200&h=200&f=face
      fullname: Thorold Tronrud
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ttronrud
      type: user
    createdAt: '2023-07-21T16:35:05.000Z'
    data:
      edited: false
      editors:
      - ttronrud
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9077997207641602
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64399cb9d45e8db3e28877c4/qtmSVmgH86jDkPBYVZ7B-.jpeg?w=200&h=200&f=face
          fullname: Thorold Tronrud
          isHf: false
          isPro: false
          name: ttronrud
          type: user
        html: '<p>You''re trying this with an untrained model, which is effectively
          a fancy auto-complete. The answer being present in the context doesn''t
          matter if it doesn''t know (e.g. its attention mechanism hasn''t been trained)
          to look for it there!<br>Try fine-tuning the model first:<br><a href="https://huggingface.co/docs/transformers/tasks/question_answering">https://huggingface.co/docs/transformers/tasks/question_answering</a><br><a
          rel="nofollow" href="https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing#scrollTo=XIyP_0r6zuVc">https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing#scrollTo=XIyP_0r6zuVc</a></p>

          '
        raw: 'You''re trying this with an untrained model, which is effectively a
          fancy auto-complete. The answer being present in the context doesn''t matter
          if it doesn''t know (e.g. its attention mechanism hasn''t been trained)
          to look for it there!

          Try fine-tuning the model first:

          https://huggingface.co/docs/transformers/tasks/question_answering

          https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing#scrollTo=XIyP_0r6zuVc

          '
        updatedAt: '2023-07-21T16:35:05.787Z'
      numEdits: 0
      reactions: []
    id: 64bab3b936eb058cd9b51c38
    type: comment
  author: ttronrud
  content: 'You''re trying this with an untrained model, which is effectively a fancy
    auto-complete. The answer being present in the context doesn''t matter if it doesn''t
    know (e.g. its attention mechanism hasn''t been trained) to look for it there!

    Try fine-tuning the model first:

    https://huggingface.co/docs/transformers/tasks/question_answering

    https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing#scrollTo=XIyP_0r6zuVc

    '
  created_at: 2023-07-21 15:35:05+00:00
  edited: false
  hidden: false
  id: 64bab3b936eb058cd9b51c38
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: openlm-research/open_llama_13b
repo_type: model
status: open
target_branch: null
title: question answering using llama
