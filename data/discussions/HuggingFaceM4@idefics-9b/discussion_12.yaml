!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gigant
conflicting_files: null
created_at: 2023-10-26 16:19:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg?w=200&h=200&f=face
      fullname: "Th\xE9o Gigant"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gigant
      type: user
    createdAt: '2023-10-26T17:19:41.000Z'
    data:
      edited: true
      editors:
      - gigant
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8941720128059387
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg?w=200&h=200&f=face
          fullname: "Th\xE9o Gigant"
          isHf: false
          isPro: false
          name: gigant
          type: user
        html: '<p>From my understanding, the  <code>&lt;fake_token_around_image&gt;</code>
          replaces the <code>&lt;EOC&gt;</code> end of chunk token from the original
          Flamingo paper. According to the latter, the <code>&lt;EOC&gt;</code> token
          is used at the end of a text chunk: <em>"prior to any image and at the end
          of the document"</em>.<br>However, if I am not mistaken, the <code>&lt;fake_token_around_image&gt;</code>
          is to be used before and after the image token, and is not used at the end
          of the document.<br>Why this difference? Did you do or refer to experiments/ablation
          studies on this added token?</p>

          <p>Thank you</p>

          '
        raw: 'From my understanding, the  `<fake_token_around_image>` replaces the
          `<EOC>` end of chunk token from the original Flamingo paper. According to
          the latter, the `<EOC>` token is used at the end of a text chunk: *"prior
          to any image and at the end of the document"*.

          However, if I am not mistaken, the `<fake_token_around_image>` is to be
          used before and after the image token, and is not used at the end of the
          document.

          Why this difference? Did you do or refer to experiments/ablation studies
          on this added token?


          Thank you'
        updatedAt: '2023-10-26T17:20:20.595Z'
      numEdits: 1
      reactions: []
    id: 653a9fade81b2b0dfb7b3738
    type: comment
  author: gigant
  content: 'From my understanding, the  `<fake_token_around_image>` replaces the `<EOC>`
    end of chunk token from the original Flamingo paper. According to the latter,
    the `<EOC>` token is used at the end of a text chunk: *"prior to any image and
    at the end of the document"*.

    However, if I am not mistaken, the `<fake_token_around_image>` is to be used before
    and after the image token, and is not used at the end of the document.

    Why this difference? Did you do or refer to experiments/ablation studies on this
    added token?


    Thank you'
  created_at: 2023-10-26 16:19:41+00:00
  edited: true
  hidden: false
  id: 653a9fade81b2b0dfb7b3738
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
      fullname: Victor Sanh
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: VictorSanh
      type: user
    createdAt: '2023-10-27T15:35:28.000Z'
    data:
      edited: false
      editors:
      - VictorSanh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8583572506904602
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
          fullname: Victor Sanh
          isHf: true
          isPro: true
          name: VictorSanh
          type: user
        html: '<p>Hey,<br>We use:</p>

          <ul>

          <li><code>&lt;fake_token_around_image&gt;</code> to wrap the <code>&lt;image&gt;</code>
          tokens. As such, if we have consecutive images, the token sequence will
          be <code>&lt;fake_token_around_image&gt;&lt;image&gt;&lt;fake_token_around_image&gt;&lt;image&gt;&lt;fake_token_around_image&gt;</code>,
          if you only have one image, it will be <code>&lt;fake_token_around_image&gt;&lt;image&gt;&lt;fake_token_around_image&gt;</code>.</li>

          <li><code>&lt;eos&gt;</code> tokens to mark the end of a document.</li>

          <li><code>&lt;bos&gt;</code> tokens to mark the beginning of a document.</li>

          </ul>

          <p>At the end of the day, the reason why we wrap image tokens around other
          ones is to ensure that that we always have tokens associated with all the
          images even when they are consecutive. it ensures the model can reason across
          all images. Beyond that, the exact implementation (EOC or not) depends on
          other factors.<br>For instance, we started using <code>\n\n</code> instead
          of a new learned <code>&lt;fake_token_around_image&gt;</code> token but
          the model was confusing the <code>\n\n</code> as in double line breaks and
          <code>\n\n</code> as in an image is next.</p>

          '
        raw: 'Hey,

          We use:

          - `<fake_token_around_image>` to wrap the `<image>` tokens. As such, if
          we have consecutive images, the token sequence will be `<fake_token_around_image><image><fake_token_around_image><image><fake_token_around_image>`,
          if you only have one image, it will be `<fake_token_around_image><image><fake_token_around_image>`.

          - `<eos>` tokens to mark the end of a document.

          - `<bos>` tokens to mark the beginning of a document.


          At the end of the day, the reason why we wrap image tokens around other
          ones is to ensure that that we always have tokens associated with all the
          images even when they are consecutive. it ensures the model can reason across
          all images. Beyond that, the exact implementation (EOC or not) depends on
          other factors.

          For instance, we started using `\n\n` instead of a new learned `<fake_token_around_image>`
          token but the model was confusing the `\n\n` as in double line breaks and
          `\n\n` as in an image is next.'
        updatedAt: '2023-10-27T15:35:28.987Z'
      numEdits: 0
      reactions: []
    id: 653bd8c0c6e1a87967a112a5
    type: comment
  author: VictorSanh
  content: 'Hey,

    We use:

    - `<fake_token_around_image>` to wrap the `<image>` tokens. As such, if we have
    consecutive images, the token sequence will be `<fake_token_around_image><image><fake_token_around_image><image><fake_token_around_image>`,
    if you only have one image, it will be `<fake_token_around_image><image><fake_token_around_image>`.

    - `<eos>` tokens to mark the end of a document.

    - `<bos>` tokens to mark the beginning of a document.


    At the end of the day, the reason why we wrap image tokens around other ones is
    to ensure that that we always have tokens associated with all the images even
    when they are consecutive. it ensures the model can reason across all images.
    Beyond that, the exact implementation (EOC or not) depends on other factors.

    For instance, we started using `\n\n` instead of a new learned `<fake_token_around_image>`
    token but the model was confusing the `\n\n` as in double line breaks and `\n\n`
    as in an image is next.'
  created_at: 2023-10-27 14:35:28+00:00
  edited: false
  hidden: false
  id: 653bd8c0c6e1a87967a112a5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: HuggingFaceM4/idefics-9b
repo_type: model
status: open
target_branch: null
title: Why is the use of the <fake_token_around_image> token different from Flamingo's
  <EOC> token?
