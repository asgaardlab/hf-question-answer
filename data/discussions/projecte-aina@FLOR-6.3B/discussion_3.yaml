!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Christ0pher
conflicting_files: null
created_at: 2024-01-11 19:24:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b12458f367a212c42c57/QDU6UO6HenqcR-iLvuZNj.png?w=200&h=200&f=face
      fullname: Christopher Atanasopulo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Christ0pher
      type: user
    createdAt: '2024-01-11T19:24:15.000Z'
    data:
      edited: false
      editors:
      - Christ0pher
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6744095683097839
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b4b12458f367a212c42c57/QDU6UO6HenqcR-iLvuZNj.png?w=200&h=200&f=face
          fullname: Christopher Atanasopulo
          isHf: false
          isPro: false
          name: Christ0pher
          type: user
        html: "<p>When trying to convert using the following command</p>\n<pre><code>python3\
          \ convert-hf-to-gguf.py --outtype f16 /media/psf/2TB/Software/AI/Models/FLOR-6.3B\n\
          </code></pre>\n<p>It outputs the following error message:</p>\n<pre><code>Loading\
          \ model: FLOR-6.3B\nTraceback (most recent call last):\n  File \"/home/christopher/llama.cpp/convert-hf-to-gguf.py\"\
          , line 1054, in &lt;module&gt;\n    model_class = Model.from_model_architecture(hparams[\"\
          architectures\"][0])\n                                                ~~~~~~~^^^^^^^^^^^^^^^^^\n\
          KeyError: 'architectures'\n</code></pre>\n<p>I solved it with this suggestion\
          \ <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/issues/4877\"\
          >https://github.com/ggerganov/llama.cpp/issues/4877</a></p>\n<p>Basicaly\
          \ I had to add a few lines to the model's config.json file, which should\
          \ look like this:</p>\n<pre><code>{\n    \"architectures\": [\n        \"\
          BloomForCausalLM\"\n    ],\n    \"vocab_size\": 50257,\n    \"hidden_size\"\
          : 4096,\n    \"tie_word_embeddings\": true,\n    \"n_layer\": 30,\n    \"\
          hidden_dropout\": 0.0,\n    \"layer_norm_epsilon\": 1e-05,\n    \"n_head\"\
          : 32,\n    \"attention_dropout\": 0.0,\n    \"model_type\": \"bloom\"\n\
          }\n</code></pre>\n"
        raw: "When trying to convert using the following command\r\n\r\n```\r\npython3\
          \ convert-hf-to-gguf.py --outtype f16 /media/psf/2TB/Software/AI/Models/FLOR-6.3B\r\
          \n```\r\n\r\nIt outputs the following error message:\r\n\r\n```\r\nLoading\
          \ model: FLOR-6.3B\r\nTraceback (most recent call last):\r\n  File \"/home/christopher/llama.cpp/convert-hf-to-gguf.py\"\
          , line 1054, in <module>\r\n    model_class = Model.from_model_architecture(hparams[\"\
          architectures\"][0])\r\n                                               \
          \ ~~~~~~~^^^^^^^^^^^^^^^^^\r\nKeyError: 'architectures'\r\n```\r\n\r\nI\
          \ solved it with this suggestion https://github.com/ggerganov/llama.cpp/issues/4877\r\
          \n\r\nBasicaly I had to add a few lines to the model's config.json file,\
          \ which should look like this:\r\n\r\n```\r\n{\r\n    \"architectures\"\
          : [\r\n        \"BloomForCausalLM\"\r\n    ],\r\n    \"vocab_size\": 50257,\r\
          \n    \"hidden_size\": 4096,\r\n    \"tie_word_embeddings\": true,\r\n \
          \   \"n_layer\": 30,\r\n    \"hidden_dropout\": 0.0,\r\n    \"layer_norm_epsilon\"\
          : 1e-05,\r\n    \"n_head\": 32,\r\n    \"attention_dropout\": 0.0,\r\n \
          \   \"model_type\": \"bloom\"\r\n}\r\n```"
        updatedAt: '2024-01-11T19:24:15.411Z'
      numEdits: 0
      reactions: []
    id: 65a0405f9185dcca3034604b
    type: comment
  author: Christ0pher
  content: "When trying to convert using the following command\r\n\r\n```\r\npython3\
    \ convert-hf-to-gguf.py --outtype f16 /media/psf/2TB/Software/AI/Models/FLOR-6.3B\r\
    \n```\r\n\r\nIt outputs the following error message:\r\n\r\n```\r\nLoading model:\
    \ FLOR-6.3B\r\nTraceback (most recent call last):\r\n  File \"/home/christopher/llama.cpp/convert-hf-to-gguf.py\"\
    , line 1054, in <module>\r\n    model_class = Model.from_model_architecture(hparams[\"\
    architectures\"][0])\r\n                                                ~~~~~~~^^^^^^^^^^^^^^^^^\r\
    \nKeyError: 'architectures'\r\n```\r\n\r\nI solved it with this suggestion https://github.com/ggerganov/llama.cpp/issues/4877\r\
    \n\r\nBasicaly I had to add a few lines to the model's config.json file, which\
    \ should look like this:\r\n\r\n```\r\n{\r\n    \"architectures\": [\r\n     \
    \   \"BloomForCausalLM\"\r\n    ],\r\n    \"vocab_size\": 50257,\r\n    \"hidden_size\"\
    : 4096,\r\n    \"tie_word_embeddings\": true,\r\n    \"n_layer\": 30,\r\n    \"\
    hidden_dropout\": 0.0,\r\n    \"layer_norm_epsilon\": 1e-05,\r\n    \"n_head\"\
    : 32,\r\n    \"attention_dropout\": 0.0,\r\n    \"model_type\": \"bloom\"\r\n\
    }\r\n```"
  created_at: 2024-01-11 19:24:15+00:00
  edited: false
  hidden: false
  id: 65a0405f9185dcca3034604b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: projecte-aina/FLOR-6.3B
repo_type: model
status: open
target_branch: null
title: '[SOLVED] "KeyError: ''architectures''" when trying to convert the model using
  convert-hf-to-gguf.py from llama.cpp'
