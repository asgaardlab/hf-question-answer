!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dball
conflicting_files: null
created_at: 2023-08-22 13:12:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615c231c3a60fa8486f80634/t-kcY2gsYVcwrZrsTc0Fz.jpeg?w=200&h=200&f=face
      fullname: "David Farag\xF3"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dball
      type: user
    createdAt: '2023-08-22T14:12:17.000Z'
    data:
      edited: false
      editors:
      - dball
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3053508400917053
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615c231c3a60fa8486f80634/t-kcY2gsYVcwrZrsTc0Fz.jpeg?w=200&h=200&f=face
          fullname: "David Farag\xF3"
          isHf: false
          isPro: false
          name: dball
          type: user
        html: "<p>I can neither load the tokenizer nor the model:</p>\n<pre><code>Python\
          \ 3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0] on linux\nType \"help\"\
          , \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\
          \ from transformers import AutoTokenizer, AutoModelForCausalLM\n&gt;&gt;&gt;\
          \ tokenizer = AutoTokenizer.from_pretrained(\"fxmarty/tiny-llama-fast-tokenizer\"\
          )\nDownloading (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 649/649 [00:00&lt;00:00,\
          \ 797kB/s]\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\"\
          , line 1, in &lt;module&gt;\n  File \"/home/davef/anaconda3/envs/pytorch310/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 674, in from_pretrained\n    raise ValueError(\nValueError: Tokenizer\
          \ class LlamaTokenizer does not exist or is not currently imported.\n&gt;&gt;&gt;\
          \ model = AutoModelForCausalLM.from_pretrained(\"fxmarty/tiny-llama-fast-tokenizer\"\
          )\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1,\
          \ in &lt;module&gt;\n  File \"/home/davef/anaconda3/envs/pytorch310/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 441, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\n\
          \  File \"/home/davef/anaconda3/envs/pytorch310/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 904, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]\n  File \"/home/davef/anaconda3/envs/pytorch310/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 610, in __getitem__\n    raise KeyError(key)\nKeyError: 'llama'\n\
          &gt;&gt;&gt; import transformers\n&gt;&gt;&gt; transformers.__version__\n\
          '4.27.0.dev0'\n</code></pre>\n"
        raw: "I can neither load the tokenizer nor the model:\r\n```\r\nPython 3.10.9\
          \ (main, Jan 11 2023, 15:21:40) [GCC 11.2.0] on linux\r\nType \"help\",\
          \ \"copyright\", \"credits\" or \"license\" for more information.\r\n>>>\
          \ from transformers import AutoTokenizer, AutoModelForCausalLM\r\n>>> tokenizer\
          \ = AutoTokenizer.from_pretrained(\"fxmarty/tiny-llama-fast-tokenizer\"\
          )\r\nDownloading (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 649/649 [00:00<00:00,\
          \ 797kB/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\",\
          \ line 1, in <module>\r\n  File \"/home/davef/anaconda3/envs/pytorch310/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 674, in from_pretrained\r\n    raise ValueError(\r\nValueError: Tokenizer\
          \ class LlamaTokenizer does not exist or is not currently imported.\r\n\
          >>> model = AutoModelForCausalLM.from_pretrained(\"fxmarty/tiny-llama-fast-tokenizer\"\
          )\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in\
          \ <module>\r\n  File \"/home/davef/anaconda3/envs/pytorch310/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 441, in from_pretrained\r\n    config, kwargs = AutoConfig.from_pretrained(\r\
          \n  File \"/home/davef/anaconda3/envs/pytorch310/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 904, in from_pretrained\r\n    config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]\r\n  File \"/home/davef/anaconda3/envs/pytorch310/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 610, in __getitem__\r\n    raise KeyError(key)\r\nKeyError: 'llama'\r\
          \n>>> import transformers\r\n>>> transformers.__version__\r\n'4.27.0.dev0'\r\
          \n```"
        updatedAt: '2023-08-22T14:12:17.472Z'
      numEdits: 0
      reactions: []
    id: 64e4c241ba1c93ea6d2d1c6f
    type: comment
  author: dball
  content: "I can neither load the tokenizer nor the model:\r\n```\r\nPython 3.10.9\
    \ (main, Jan 11 2023, 15:21:40) [GCC 11.2.0] on linux\r\nType \"help\", \"copyright\"\
    , \"credits\" or \"license\" for more information.\r\n>>> from transformers import\
    \ AutoTokenizer, AutoModelForCausalLM\r\n>>> tokenizer = AutoTokenizer.from_pretrained(\"\
    fxmarty/tiny-llama-fast-tokenizer\")\r\nDownloading (\u2026)okenizer_config.json:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588| 649/649 [00:00<00:00, 797kB/s]\r\nTraceback (most recent call last):\r\
    \n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/davef/anaconda3/envs/pytorch310/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 674, in from_pretrained\r\n    raise ValueError(\r\nValueError: Tokenizer\
    \ class LlamaTokenizer does not exist or is not currently imported.\r\n>>> model\
    \ = AutoModelForCausalLM.from_pretrained(\"fxmarty/tiny-llama-fast-tokenizer\"\
    )\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\
    \n  File \"/home/davef/anaconda3/envs/pytorch310/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 441, in from_pretrained\r\n    config, kwargs = AutoConfig.from_pretrained(\r\
    \n  File \"/home/davef/anaconda3/envs/pytorch310/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 904, in from_pretrained\r\n    config_class = CONFIG_MAPPING[config_dict[\"\
    model_type\"]]\r\n  File \"/home/davef/anaconda3/envs/pytorch310/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 610, in __getitem__\r\n    raise KeyError(key)\r\nKeyError: 'llama'\r\n\
    >>> import transformers\r\n>>> transformers.__version__\r\n'4.27.0.dev0'\r\n```"
  created_at: 2023-08-22 13:12:17+00:00
  edited: false
  hidden: false
  id: 64e4c241ba1c93ea6d2d1c6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1651743336129-624c60cba8ec93a7ac188b56.png?w=200&h=200&f=face
      fullname: "F\xE9lix Marty"
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: fxmarty
      type: user
    createdAt: '2023-08-24T17:49:04.000Z'
    data:
      edited: false
      editors:
      - fxmarty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9079647660255432
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1651743336129-624c60cba8ec93a7ac188b56.png?w=200&h=200&f=face
          fullname: "F\xE9lix Marty"
          isHf: true
          isPro: false
          name: fxmarty
          type: user
        html: '<p>Hi, you should upgrade transformers: <code>pip install -U transformers</code>.
          Note though that this model is just for testing, and it will produce garbage
          output.</p>

          '
        raw: 'Hi, you should upgrade transformers: `pip install -U transformers`.
          Note though that this model is just for testing, and it will produce garbage
          output.'
        updatedAt: '2023-08-24T17:49:04.498Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64e798107df33432813794ce
    id: 64e798107df33432813794cd
    type: comment
  author: fxmarty
  content: 'Hi, you should upgrade transformers: `pip install -U transformers`. Note
    though that this model is just for testing, and it will produce garbage output.'
  created_at: 2023-08-24 16:49:04+00:00
  edited: false
  hidden: false
  id: 64e798107df33432813794cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1651743336129-624c60cba8ec93a7ac188b56.png?w=200&h=200&f=face
      fullname: "F\xE9lix Marty"
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: fxmarty
      type: user
    createdAt: '2023-08-24T17:49:04.000Z'
    data:
      status: closed
    id: 64e798107df33432813794ce
    type: status-change
  author: fxmarty
  created_at: 2023-08-24 16:49:04+00:00
  id: 64e798107df33432813794ce
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: fxmarty/tiny-llama-fast-tokenizer
repo_type: model
status: closed
target_branch: null
title: Bugs when loading tokenizer or model
