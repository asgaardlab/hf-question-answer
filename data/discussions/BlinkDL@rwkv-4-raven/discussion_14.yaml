!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Raspbfox
conflicting_files: null
created_at: 2023-04-20 02:10:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a446a1dd369ddb625d991f1d72a0b239.svg
      fullname: Oleksii
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Raspbfox
      type: user
    createdAt: '2023-04-20T03:10:11.000Z'
    data:
      edited: false
      editors:
      - Raspbfox
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a446a1dd369ddb625d991f1d72a0b239.svg
          fullname: Oleksii
          isHf: false
          isPro: false
          name: Raspbfox
          type: user
        html: '<p>Hey hey, I am currently working on a mobile-friendly runtime for
          RWKV and one of the areas I am really curious about, is aggressive quantization,
          as storage and RAM spaces of mobile devices are extremely limited.</p>

          <p>At the moment, I am experimenting with dynamic INT4 quantization of your
          pre-trained models, however, it made me think: maybe we can natively train
          at least one decently sized model fully in INT4?<br>Or, at least, use the
          existing model as a "teacher" and make it train a quantized "student" model.</p>

          <p>What do you think?</p>

          '
        raw: "Hey hey, I am currently working on a mobile-friendly runtime for RWKV\
          \ and one of the areas I am really curious about, is aggressive quantization,\
          \ as storage and RAM spaces of mobile devices are extremely limited.\r\n\
          \r\nAt the moment, I am experimenting with dynamic INT4 quantization of\
          \ your pre-trained models, however, it made me think: maybe we can natively\
          \ train at least one decently sized model fully in INT4?\r\nOr, at least,\
          \ use the existing model as a \"teacher\" and make it train a quantized\
          \ \"student\" model.\r\n\r\nWhat do you think?"
        updatedAt: '2023-04-20T03:10:11.791Z'
      numEdits: 0
      reactions: []
    id: 6440ad137663594a12664773
    type: comment
  author: Raspbfox
  content: "Hey hey, I am currently working on a mobile-friendly runtime for RWKV\
    \ and one of the areas I am really curious about, is aggressive quantization,\
    \ as storage and RAM spaces of mobile devices are extremely limited.\r\n\r\nAt\
    \ the moment, I am experimenting with dynamic INT4 quantization of your pre-trained\
    \ models, however, it made me think: maybe we can natively train at least one\
    \ decently sized model fully in INT4?\r\nOr, at least, use the existing model\
    \ as a \"teacher\" and make it train a quantized \"student\" model.\r\n\r\nWhat\
    \ do you think?"
  created_at: 2023-04-20 02:10:11+00:00
  edited: false
  hidden: false
  id: 6440ad137663594a12664773
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a446a1dd369ddb625d991f1d72a0b239.svg
      fullname: Oleksii
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Raspbfox
      type: user
    createdAt: '2023-04-20T03:11:35.000Z'
    data:
      edited: false
      editors:
      - Raspbfox
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a446a1dd369ddb625d991f1d72a0b239.svg
          fullname: Oleksii
          isHf: false
          isPro: false
          name: Raspbfox
          type: user
        html: "<p>This should, in theory, reduce the size of the 7B model to ~2GBs,\
          \ which is in the real of acceptable on mobile devices \U0001F440</p>\n"
        raw: "This should, in theory, reduce the size of the 7B model to ~2GBs, which\
          \ is in the real of acceptable on mobile devices \U0001F440"
        updatedAt: '2023-04-20T03:11:35.833Z'
      numEdits: 0
      reactions: []
    id: 6440ad672113f7dfcb5f62a0
    type: comment
  author: Raspbfox
  content: "This should, in theory, reduce the size of the 7B model to ~2GBs, which\
    \ is in the real of acceptable on mobile devices \U0001F440"
  created_at: 2023-04-20 02:11:35+00:00
  edited: false
  hidden: false
  id: 6440ad672113f7dfcb5f62a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e4853966e0f93fd1a80b0fa764fac27.svg
      fullname: Cainter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cainter-73180
      type: user
    createdAt: '2023-04-23T14:14:49.000Z'
    data:
      edited: false
      editors:
      - Cainter-73180
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e4853966e0f93fd1a80b0fa764fac27.svg
          fullname: Cainter
          isHf: false
          isPro: false
          name: Cainter-73180
          type: user
        html: "<p>\u4F60\u597D</p>\n"
        raw: "\u4F60\u597D"
        updatedAt: '2023-04-23T14:14:49.063Z'
      numEdits: 0
      reactions: []
    id: 64453d59f993c804b033c7ad
    type: comment
  author: Cainter-73180
  content: "\u4F60\u597D"
  created_at: 2023-04-23 13:14:49+00:00
  edited: false
  hidden: false
  id: 64453d59f993c804b033c7ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a446a1dd369ddb625d991f1d72a0b239.svg
      fullname: Oleksii
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Raspbfox
      type: user
    createdAt: '2023-04-23T21:57:43.000Z'
    data:
      edited: false
      editors:
      - Raspbfox
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a446a1dd369ddb625d991f1d72a0b239.svg
          fullname: Oleksii
          isHf: false
          isPro: false
          name: Raspbfox
          type: user
        html: '<p>Hi!</p>

          '
        raw: Hi!
        updatedAt: '2023-04-23T21:57:43.699Z'
      numEdits: 0
      reactions: []
    id: 6445a9d7d1460e859d22d22f
    type: comment
  author: Raspbfox
  content: Hi!
  created_at: 2023-04-23 20:57:43+00:00
  edited: false
  hidden: false
  id: 6445a9d7d1460e859d22d22f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/01fb594d368702aafc91eb1d74f0db76.svg
      fullname: Andrew
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cha0tik
      type: user
    createdAt: '2023-04-24T06:34:03.000Z'
    data:
      edited: false
      editors:
      - cha0tik
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/01fb594d368702aafc91eb1d74f0db76.svg
          fullname: Andrew
          isHf: false
          isPro: false
          name: cha0tik
          type: user
        html: '<p>It is not possible to train quantized models in int4 because the
          delta of the weights during training iteration will be less than the quantum,
          and the training efficiency will become zero. That''s why quantization is
          only used for inference.</p>

          '
        raw: It is not possible to train quantized models in int4 because the delta
          of the weights during training iteration will be less than the quantum,
          and the training efficiency will become zero. That's why quantization is
          only used for inference.
        updatedAt: '2023-04-24T06:34:03.638Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - jamesbonellama
    id: 644622db2f3d84a7a8818b2d
    type: comment
  author: cha0tik
  content: It is not possible to train quantized models in int4 because the delta
    of the weights during training iteration will be less than the quantum, and the
    training efficiency will become zero. That's why quantization is only used for
    inference.
  created_at: 2023-04-24 05:34:03+00:00
  edited: false
  hidden: false
  id: 644622db2f3d84a7a8818b2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a446a1dd369ddb625d991f1d72a0b239.svg
      fullname: Oleksii
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Raspbfox
      type: user
    createdAt: '2023-04-24T15:47:35.000Z'
    data:
      edited: false
      editors:
      - Raspbfox
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a446a1dd369ddb625d991f1d72a0b239.svg
          fullname: Oleksii
          isHf: false
          isPro: false
          name: Raspbfox
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cha0tik&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cha0tik\">@<span class=\"\
          underline\">cha0tik</span></a></span>\n\n\t</span></span>, from what I understand,\
          \ it doesn't directly mean training a whole new model internally using <strong>int4</strong>\
          \ values, rather it means training (or additionally training) a model to\
          \ take issues of very-low-precision quantization into account.</p>\n<p><a\
          \ rel=\"nofollow\" href=\"https://intellabs.github.io/distiller/quantization.html#quantization-aware-training\"\
          >https://intellabs.github.io/distiller/quantization.html#quantization-aware-training</a></p>\n\
          <blockquote>\n<p>As mentioned above, in order to minimize the loss of accuracy\
          \ from \"aggressive\" quantization, many methods that target INT4 and lower\
          \ (and in some cases for INT8 as well) involve training the model in a way\
          \ that considers the quantization. This means training with quantization\
          \ of weights and activations \"baked\" into the training procedure. </p>\n\
          </blockquote>\n<p><a rel=\"nofollow\" href=\"https://arxiv.org/abs/1606.06160\"\
          >https://arxiv.org/abs/1606.06160</a></p>\n<blockquote>\n<p>As most convolutions\
          \ during forward/backward passes are now taking low bitwidth weights and<br>activations/gradients\
          \ respectively, DoReFa-Net can use the bit convolution kernels to accelerate\
          \ both<br>training and inference process. Our experiments on SVHN and ImageNet\
          \ datasets demonstrate that<br>DoReFa-Net can achieve comparable prediction\
          \ accuracy as their 32-bit counterparts. For example,<br>a DoReFa-Net derived\
          \ from AlexNet that has 1-bit weights, 2-bit activations, can be trained\
          \ from<br>scratch using 6-bit gradients to get 46.1% top-1 accuracy on ImageNet\
          \ validation set.</p>\n</blockquote>\n<p>So why not look into a 4-bit (additionally\
          \ trained) quantized set of models, or even use a combination of 1-,2- and\
          \ 4-bit quantization in appropriate places?</p>\n"
        raw: "@cha0tik, from what I understand, it doesn't directly mean training\
          \ a whole new model internally using **int4** values, rather it means training\
          \ (or additionally training) a model to take issues of very-low-precision\
          \ quantization into account.\n\nhttps://intellabs.github.io/distiller/quantization.html#quantization-aware-training\n\
          > As mentioned above, in order to minimize the loss of accuracy from \"\
          aggressive\" quantization, many methods that target INT4 and lower (and\
          \ in some cases for INT8 as well) involve training the model in a way that\
          \ considers the quantization. This means training with quantization of weights\
          \ and activations \"baked\" into the training procedure. \n\nhttps://arxiv.org/abs/1606.06160\n\
          > As most convolutions during forward/backward passes are now taking low\
          \ bitwidth weights and\nactivations/gradients respectively, DoReFa-Net can\
          \ use the bit convolution kernels to accelerate both\ntraining and inference\
          \ process. Our experiments on SVHN and ImageNet datasets demonstrate that\n\
          DoReFa-Net can achieve comparable prediction accuracy as their 32-bit counterparts.\
          \ For example,\na DoReFa-Net derived from AlexNet that has 1-bit weights,\
          \ 2-bit activations, can be trained from\nscratch using 6-bit gradients\
          \ to get 46.1% top-1 accuracy on ImageNet validation set.\n\nSo why not\
          \ look into a 4-bit (additionally trained) quantized set of models, or even\
          \ use a combination of 1-,2- and 4-bit quantization in appropriate places?"
        updatedAt: '2023-04-24T15:47:35.794Z'
      numEdits: 0
      reactions: []
    id: 6446a497f9dc06bea2a3ef7c
    type: comment
  author: Raspbfox
  content: "@cha0tik, from what I understand, it doesn't directly mean training a\
    \ whole new model internally using **int4** values, rather it means training (or\
    \ additionally training) a model to take issues of very-low-precision quantization\
    \ into account.\n\nhttps://intellabs.github.io/distiller/quantization.html#quantization-aware-training\n\
    > As mentioned above, in order to minimize the loss of accuracy from \"aggressive\"\
    \ quantization, many methods that target INT4 and lower (and in some cases for\
    \ INT8 as well) involve training the model in a way that considers the quantization.\
    \ This means training with quantization of weights and activations \"baked\" into\
    \ the training procedure. \n\nhttps://arxiv.org/abs/1606.06160\n> As most convolutions\
    \ during forward/backward passes are now taking low bitwidth weights and\nactivations/gradients\
    \ respectively, DoReFa-Net can use the bit convolution kernels to accelerate both\n\
    training and inference process. Our experiments on SVHN and ImageNet datasets\
    \ demonstrate that\nDoReFa-Net can achieve comparable prediction accuracy as their\
    \ 32-bit counterparts. For example,\na DoReFa-Net derived from AlexNet that has\
    \ 1-bit weights, 2-bit activations, can be trained from\nscratch using 6-bit gradients\
    \ to get 46.1% top-1 accuracy on ImageNet validation set.\n\nSo why not look into\
    \ a 4-bit (additionally trained) quantized set of models, or even use a combination\
    \ of 1-,2- and 4-bit quantization in appropriate places?"
  created_at: 2023-04-24 14:47:35+00:00
  edited: false
  hidden: false
  id: 6446a497f9dc06bea2a3ef7c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-02T14:29:26.000Z'
    data:
      edited: true
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Raspbfox&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Raspbfox\">@<span class=\"\
          underline\">Raspbfox</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;BlinkDL&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/BlinkDL\">@<span class=\"underline\">BlinkDL</span></a></span>\n\
          \n\t</span></span> Are there any efforts in quantizations for RWKV GPU models?\
          \ I know the ggml port of RWKV already uses ggml's quantization methods.\
          \ What about RWKV's GPTQ port?</p>\n<p>If GPTQ's compression methods apply\
          \ to RWKV, we should be able to just quantize the original model with acceptable\
          \ minimal losses. This will help deploy larger models (future 25B, 50B,\
          \ 100B RWKV models) with smaller VRAM and even faster inferencing speed.</p>\n"
        raw: '@Raspbfox @BlinkDL Are there any efforts in quantizations for RWKV GPU
          models? I know the ggml port of RWKV already uses ggml''s quantization methods.
          What about RWKV''s GPTQ port?


          If GPTQ''s compression methods apply to RWKV, we should be able to just
          quantize the original model with acceptable minimal losses. This will help
          deploy larger models (future 25B, 50B, 100B RWKV models) with smaller VRAM
          and even faster inferencing speed.'
        updatedAt: '2023-05-02T14:31:51.564Z'
      numEdits: 1
      reactions: []
    id: 64511e469d916c596e28eeeb
    type: comment
  author: Yhyu13
  content: '@Raspbfox @BlinkDL Are there any efforts in quantizations for RWKV GPU
    models? I know the ggml port of RWKV already uses ggml''s quantization methods.
    What about RWKV''s GPTQ port?


    If GPTQ''s compression methods apply to RWKV, we should be able to just quantize
    the original model with acceptable minimal losses. This will help deploy larger
    models (future 25B, 50B, 100B RWKV models) with smaller VRAM and even faster inferencing
    speed.'
  created_at: 2023-05-02 13:29:26+00:00
  edited: true
  hidden: false
  id: 64511e469d916c596e28eeeb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1655953609090-noauth.jpeg?w=200&h=200&f=face
      fullname: BlinkDL
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: BlinkDL
      type: user
    createdAt: '2023-05-02T14:47:50.000Z'
    data:
      edited: false
      editors:
      - BlinkDL
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1655953609090-noauth.jpeg?w=200&h=200&f=face
          fullname: BlinkDL
          isHf: false
          isPro: false
          name: BlinkDL
          type: user
        html: '<p>GPTQ WIP: <a rel="nofollow" href="https://github.com/BlinkDL/ChatRWKV/pull/98">https://github.com/BlinkDL/ChatRWKV/pull/98</a></p>

          '
        raw: 'GPTQ WIP: https://github.com/BlinkDL/ChatRWKV/pull/98'
        updatedAt: '2023-05-02T14:47:50.489Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Yhyu13
        - Raspbfox
    id: 6451229641f3c769b90c72b5
    type: comment
  author: BlinkDL
  content: 'GPTQ WIP: https://github.com/BlinkDL/ChatRWKV/pull/98'
  created_at: 2023-05-02 13:47:50+00:00
  edited: false
  hidden: false
  id: 6451229641f3c769b90c72b5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: BlinkDL/rwkv-4-raven
repo_type: model
status: open
target_branch: null
title: 'Training an INT4 version of the 7B model '
