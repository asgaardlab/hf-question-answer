!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fubincom
conflicting_files: null
created_at: 2023-05-23 08:12:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8dca8278a31813224ce501205a61c867.svg
      fullname: Fubin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fubincom
      type: user
    createdAt: '2023-05-23T09:12:38.000Z'
    data:
      edited: true
      editors:
      - fubincom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8dca8278a31813224ce501205a61c867.svg
          fullname: Fubin
          isHf: false
          isPro: false
          name: fubincom
          type: user
        html: '<p>Hello, we have GPU clusters to train a 7B raven model with our own
          in-domain corpus. I have 4 v100 GPUs with 32GB memory in 4 pods whose CPU
          memory is 48GB. (micro_bsz is 1, total batch size is 4). I have tried deepspeed
          2 and 3 with offload, but the CPU memory isn''t sufficient.  If I only fine-tune
          the model with LoRA, micro_bsz=8, resources are enough, and will use 23.2GB
          GPU memory and 19.2GB CPU memory in each pod (Trainable params is 20.3MB,
          with deep-speed stage2). I have set grad_cp =1, use fp16</p>

          <p>So I have several questions:</p>

          <ul>

          <li><ol>

          <li>Is it possible to share the resources you used when you train the 7B
          model</li>

          </ol>

          </li>

          <li><ol start="2">

          <li>How can I set the accumulate_grad_batches in train.py scripts. I find
          this in my log file but I can''t set it...</li>

          </ol>

          </li>

          <li><ol start="3">

          <li>I notice one phenomenon when I fine-tuned model with LoRA (actually
          w/o LoRA is same when I train a 1B5 model), at the beginning of training,
          they will occupy around 42GB CPU memory only, then split data into CPU and
          GPU memory separately, is it normal?</li>

          </ol>

          </li>

          <li><ol start="4">

          <li>What is the meaning of grad_cp, is it mean we will compute the gradient
          again, so we won''t save it in GPU memory (but computation time is 30% lower),
          save around 7G for 7B model? (7G is estimated by me, maynot be correct.)</li>

          </ol>

          </li>

          </ul>

          '
        raw: "Hello, we have GPU clusters to train a 7B raven model with our own in-domain\
          \ corpus. I have 4 v100 GPUs with 32GB memory in 4 pods whose CPU memory\
          \ is 48GB. (micro_bsz is 1, total batch size is 4). I have tried deepspeed\
          \ 2 and 3 with offload, but the CPU memory isn't sufficient.  If I only\
          \ fine-tune the model with LoRA, micro_bsz=8, resources are enough, and\
          \ will use 23.2GB GPU memory and 19.2GB CPU memory in each pod (Trainable\
          \ params is 20.3MB, with deep-speed stage2). I have set grad_cp =1, use\
          \ fp16\n\nSo I have several questions:\n- 1. Is it possible to share the\
          \ resources you used when you train the 7B model\n- 2. How can I set the\
          \ accumulate_grad_batches in train.py scripts. I find this in my log file\
          \ but I can't set it...\n- 3. I notice one phenomenon when I fine-tuned\
          \ model with LoRA (actually w/o LoRA is same when I train a 1B5 model),\
          \ at the beginning of training, they will occupy around 42GB CPU memory\
          \ only, then split data into CPU and GPU memory separately, is it normal?\
          \ \n- 4. What is the meaning of grad_cp, is it mean we will compute the\
          \ gradient again, so we won't save it in GPU memory (but computation time\
          \ is 30% lower), save around 7G for 7B model? (7G is estimated by me, maynot\
          \ be correct.)"
        updatedAt: '2023-05-23T09:19:53.913Z'
      numEdits: 2
      reactions: []
    id: 646c838649a37754bdef3696
    type: comment
  author: fubincom
  content: "Hello, we have GPU clusters to train a 7B raven model with our own in-domain\
    \ corpus. I have 4 v100 GPUs with 32GB memory in 4 pods whose CPU memory is 48GB.\
    \ (micro_bsz is 1, total batch size is 4). I have tried deepspeed 2 and 3 with\
    \ offload, but the CPU memory isn't sufficient.  If I only fine-tune the model\
    \ with LoRA, micro_bsz=8, resources are enough, and will use 23.2GB GPU memory\
    \ and 19.2GB CPU memory in each pod (Trainable params is 20.3MB, with deep-speed\
    \ stage2). I have set grad_cp =1, use fp16\n\nSo I have several questions:\n-\
    \ 1. Is it possible to share the resources you used when you train the 7B model\n\
    - 2. How can I set the accumulate_grad_batches in train.py scripts. I find this\
    \ in my log file but I can't set it...\n- 3. I notice one phenomenon when I fine-tuned\
    \ model with LoRA (actually w/o LoRA is same when I train a 1B5 model), at the\
    \ beginning of training, they will occupy around 42GB CPU memory only, then split\
    \ data into CPU and GPU memory separately, is it normal? \n- 4. What is the meaning\
    \ of grad_cp, is it mean we will compute the gradient again, so we won't save\
    \ it in GPU memory (but computation time is 30% lower), save around 7G for 7B\
    \ model? (7G is estimated by me, maynot be correct.)"
  created_at: 2023-05-23 08:12:38+00:00
  edited: true
  hidden: false
  id: 646c838649a37754bdef3696
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d9e2c96a226a28dc164b88836ae55cb5.svg
      fullname: Meet Vadher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mv9
      type: user
    createdAt: '2023-05-25T01:22:46.000Z'
    data:
      edited: false
      editors:
      - mv9
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d9e2c96a226a28dc164b88836ae55cb5.svg
          fullname: Meet Vadher
          isHf: false
          isPro: false
          name: mv9
          type: user
        html: '<p>Are you trying to fine-tune a model for your documents? I am trying
          to do something similar like train my own model to achieve a fine-tuned
          model with my data, so I can retrieve it for chat-completions</p>

          '
        raw: Are you trying to fine-tune a model for your documents? I am trying to
          do something similar like train my own model to achieve a fine-tuned model
          with my data, so I can retrieve it for chat-completions
        updatedAt: '2023-05-25T01:22:46.806Z'
      numEdits: 0
      reactions: []
    id: 646eb86634fde71fdaa17e80
    type: comment
  author: mv9
  content: Are you trying to fine-tune a model for your documents? I am trying to
    do something similar like train my own model to achieve a fine-tuned model with
    my data, so I can retrieve it for chat-completions
  created_at: 2023-05-25 00:22:46+00:00
  edited: false
  hidden: false
  id: 646eb86634fde71fdaa17e80
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: BlinkDL/rwkv-4-raven
repo_type: model
status: open
target_branch: null
title: 'How many GPU and CPU memory I need to finetune a 7B raven model '
