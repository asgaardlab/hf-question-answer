!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fubincom
conflicting_files: null
created_at: 2023-05-18 09:15:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8dca8278a31813224ce501205a61c867.svg
      fullname: Fubin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fubincom
      type: user
    createdAt: '2023-05-18T10:15:48.000Z'
    data:
      edited: false
      editors:
      - fubincom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8dca8278a31813224ce501205a61c867.svg
          fullname: Fubin
          isHf: false
          isPro: false
          name: fubincom
          type: user
        html: '<p>Hello I try to finetune the 7B rwkv model with LoRA, I have trained
          the 1.5B model successfully following the parameter you shared in train.py
          script. But for 7B model, I changed the layer to 32 and embedding dim to
          4096, but I still cant load model correctly, just told me its a bad checkpoint.
          I just want to know the correct training parameters for 7B model. Any response
          is appreciated. This is my training cmd below:</p>

          <ul>

          <li>python ${FILE_DIR}/train.py --load_model ${FILE_DIR}/rwkv_model/RWKV-4-Raven-7B-v11.pth
          --wandb "" --proj_dir "out"<br>  --data_file ${FILE_DIR}/train.npy --data_type
          "numpy" --vocab_size 50277<br>  --ctx_len 1024 --epoch_steps 100 --epoch_count
          5 --epoch_begin 0 --epoch_save 10 <br>  --micro_bsz 4  --n_layer 32 --n_embd
          4096 --pre_ffn 0 --head_qk 0 <br>  --lr_init 1e-5 --lr_final 1e-5 --warmup_steps
          0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 <br>  --precision bf16 --strategy
          deepspeed_stage_2_offload --accelerator gpu --grad_cp 0 --devices 1<br>  --lora
          --lora_r 8 --lora_alpha 16 --lora_dropout 0.05 <br>  --lora_parts=att,ffn,time,ln
          # configure which parts to fine-tune</li>

          <li>md5 of my model: a33cd94e8c6dfadde51887c796c729b0</li>

          </ul>

          '
        raw: "Hello I try to finetune the 7B rwkv model with LoRA, I have trained\
          \ the 1.5B model successfully following the parameter you shared in train.py\
          \ script. But for 7B model, I changed the layer to 32 and embedding dim\
          \ to 4096, but I still cant load model correctly, just told me its a bad\
          \ checkpoint. I just want to know the correct training parameters for 7B\
          \ model. Any response is appreciated. This is my training cmd below:\r\n\
          \r\n- python ${FILE_DIR}/train.py --load_model ${FILE_DIR}/rwkv_model/RWKV-4-Raven-7B-v11.pth\
          \ --wandb \"\" --proj_dir \"out\"\\\r\n        --data_file ${FILE_DIR}/train.npy\
          \ --data_type \"numpy\" --vocab_size 50277\\\r\n        --ctx_len 1024 --epoch_steps\
          \ 100 --epoch_count 5 --epoch_begin 0 --epoch_save 10 \\\r\n        --micro_bsz\
          \ 4  --n_layer 32 --n_embd 4096 --pre_ffn 0 --head_qk 0 \\\r\n        --lr_init\
          \ 1e-5 --lr_final 1e-5 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps\
          \ 1e-8 \\\r\n        --precision bf16 --strategy deepspeed_stage_2_offload\
          \ --accelerator gpu --grad_cp 0 --devices 1\\\r\n        --lora --lora_r\
          \ 8 --lora_alpha 16 --lora_dropout 0.05 \\\r\n        --lora_parts=att,ffn,time,ln\
          \ # configure which parts to fine-tune\r\n- md5 of my model: a33cd94e8c6dfadde51887c796c729b0"
        updatedAt: '2023-05-18T10:15:48.793Z'
      numEdits: 0
      reactions: []
    id: 6465fad463e7e09dd02f7f1f
    type: comment
  author: fubincom
  content: "Hello I try to finetune the 7B rwkv model with LoRA, I have trained the\
    \ 1.5B model successfully following the parameter you shared in train.py script.\
    \ But for 7B model, I changed the layer to 32 and embedding dim to 4096, but I\
    \ still cant load model correctly, just told me its a bad checkpoint. I just want\
    \ to know the correct training parameters for 7B model. Any response is appreciated.\
    \ This is my training cmd below:\r\n\r\n- python ${FILE_DIR}/train.py --load_model\
    \ ${FILE_DIR}/rwkv_model/RWKV-4-Raven-7B-v11.pth --wandb \"\" --proj_dir \"out\"\
    \\\r\n        --data_file ${FILE_DIR}/train.npy --data_type \"numpy\" --vocab_size\
    \ 50277\\\r\n        --ctx_len 1024 --epoch_steps 100 --epoch_count 5 --epoch_begin\
    \ 0 --epoch_save 10 \\\r\n        --micro_bsz 4  --n_layer 32 --n_embd 4096 --pre_ffn\
    \ 0 --head_qk 0 \\\r\n        --lr_init 1e-5 --lr_final 1e-5 --warmup_steps 0\
    \ --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 \\\r\n        --precision bf16 --strategy\
    \ deepspeed_stage_2_offload --accelerator gpu --grad_cp 0 --devices 1\\\r\n  \
    \      --lora --lora_r 8 --lora_alpha 16 --lora_dropout 0.05 \\\r\n        --lora_parts=att,ffn,time,ln\
    \ # configure which parts to fine-tune\r\n- md5 of my model: a33cd94e8c6dfadde51887c796c729b0"
  created_at: 2023-05-18 09:15:48+00:00
  edited: false
  hidden: false
  id: 6465fad463e7e09dd02f7f1f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/8dca8278a31813224ce501205a61c867.svg
      fullname: Fubin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fubincom
      type: user
    createdAt: '2023-05-18T10:48:02.000Z'
    data:
      status: closed
    id: 646602623b99ed9970f6a0cc
    type: status-change
  author: fubincom
  created_at: 2023-05-18 09:48:02+00:00
  id: 646602623b99ed9970f6a0cc
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1655953609090-noauth.jpeg?w=200&h=200&f=face
      fullname: BlinkDL
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: BlinkDL
      type: user
    createdAt: '2023-05-18T12:53:02.000Z'
    data:
      edited: false
      editors:
      - BlinkDL
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1655953609090-noauth.jpeg?w=200&h=200&f=face
          fullname: BlinkDL
          isHf: false
          isPro: false
          name: BlinkDL
          type: user
        html: '<p>use --grad_cp 1 to save VRAM<br>what''s the error</p>

          '
        raw: 'use --grad_cp 1 to save VRAM

          what''s the error'
        updatedAt: '2023-05-18T12:53:02.555Z'
      numEdits: 0
      reactions: []
    id: 64661fae9c627c78f86787ad
    type: comment
  author: BlinkDL
  content: 'use --grad_cp 1 to save VRAM

    what''s the error'
  created_at: 2023-05-18 11:53:02+00:00
  edited: false
  hidden: false
  id: 64661fae9c627c78f86787ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8dca8278a31813224ce501205a61c867.svg
      fullname: Fubin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fubincom
      type: user
    createdAt: '2023-05-18T13:18:35.000Z'
    data:
      edited: false
      editors:
      - fubincom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8dca8278a31813224ce501205a61c867.svg
          fullname: Fubin
          isHf: false
          isPro: false
          name: fubincom
          type: user
        html: '<blockquote>

          <p>use --grad_cp 1 to save VRAM<br>what''s the error</p>

          </blockquote>

          <p>hello, sorry I just download the model again and save it to cache dir,
          then I have solved the problems. But I find if I want to fine-tune the 7B
          model with LoRA, at least I need to have 42G CPU memory to load the model
          otherwise my training process will be killed. is it normal.</p>

          '
        raw: '> use --grad_cp 1 to save VRAM

          > what''s the error


          hello, sorry I just download the model again and save it to cache dir, then
          I have solved the problems. But I find if I want to fine-tune the 7B model
          with LoRA, at least I need to have 42G CPU memory to load the model otherwise
          my training process will be killed. is it normal.'
        updatedAt: '2023-05-18T13:18:35.371Z'
      numEdits: 0
      reactions: []
    id: 646625ab3b99ed9970f8f236
    type: comment
  author: fubincom
  content: '> use --grad_cp 1 to save VRAM

    > what''s the error


    hello, sorry I just download the model again and save it to cache dir, then I
    have solved the problems. But I find if I want to fine-tune the 7B model with
    LoRA, at least I need to have 42G CPU memory to load the model otherwise my training
    process will be killed. is it normal.'
  created_at: 2023-05-18 12:18:35+00:00
  edited: false
  hidden: false
  id: 646625ab3b99ed9970f8f236
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: BlinkDL/rwkv-4-raven
repo_type: model
status: closed
target_branch: null
title: 'What is the correct parameters to finetune the 7B rwkv model '
