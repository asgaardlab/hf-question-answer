!!python/object:huggingface_hub.community.DiscussionWithDetails
author: premsa
conflicting_files: null
created_at: 2023-07-20 09:49:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d3af2931d28ff88d3ed3927af34892d.svg
      fullname: Premtim Sahitaj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: premsa
      type: user
    createdAt: '2023-07-20T10:49:14.000Z'
    data:
      edited: false
      editors:
      - premsa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7203680872917175
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d3af2931d28ff88d3ed3927af34892d.svg
          fullname: Premtim Sahitaj
          isHf: false
          isPro: false
          name: premsa
          type: user
        html: '<p>Hello,</p>

          <p>is the code for the portation of the model available somewhere? I am
          running into problems when trying to retrieve the processor of the model.
          Any assist or pointers towards relevant code would be helpful! </p>

          <p>CODE:<br>self.processor = VisionTextDualEncoderProcessor.from_pretrained(model_name)</p>

          <p>ERROR:<br>.venv/lib/python3.10/site-packages/transformers/utils/hub.py",
          line 463, in cached_file<br>    raise EnvironmentError(<br>OSError: calpt/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k
          does not appear to have a file named preprocessor_config.json. Checkout
          ''<a href="https://huggingface.co/calpt/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k/main''">https://huggingface.co/calpt/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k/main''</a>
          for available files.</p>

          <p>ALTERNATIVE:<br>I also tried initializing the processor from:</p>

          <p>self.tokenizer = AutoTokenizer.from_pretrained(MODEL.pretrained_dual_text)<br>self.image_processor
          = AutoFeatureExtractor.from_pretrained(MODEL.pretrained_dual_image)<br>self.processor
          = VisionTextDualEncoderProcessor(self.image_processor, self.tokenizer)</p>

          <p>with</p>

          <p>pretrained_dual_text = "xlm-roberta-large"<br>pretrained_dual_image =
          "laion/CLIP-ViT-H-14-laion2B-s32B-b79K"</p>

          <p>which resulted in internal shape error which I assume comes from the
          fact that I am not using the correct preprocessors for the task?</p>

          <p>.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459,
          in _conv_forward<br>    return F.conv2d(input, weight, bias, self.stride,<br>RuntimeError:
          Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of
          size: [32, 150528]</p>

          '
        raw: "Hello,\r\n\r\nis the code for the portation of the model available somewhere?\
          \ I am running into problems when trying to retrieve the processor of the\
          \ model. Any assist or pointers towards relevant code would be helpful!\
          \ \r\n\r\nCODE:\r\nself.processor = VisionTextDualEncoderProcessor.from_pretrained(model_name)\r\
          \n\r\nERROR:\r\n.venv/lib/python3.10/site-packages/transformers/utils/hub.py\"\
          , line 463, in cached_file\r\n    raise EnvironmentError(\r\nOSError: calpt/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k\
          \ does not appear to have a file named preprocessor_config.json. Checkout\
          \ 'https://huggingface.co/calpt/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k/main'\
          \ for available files.\r\n\r\nALTERNATIVE:\r\nI also tried initializing\
          \ the processor from:\r\n\r\nself.tokenizer = AutoTokenizer.from_pretrained(MODEL.pretrained_dual_text)\r\
          \nself.image_processor = AutoFeatureExtractor.from_pretrained(MODEL.pretrained_dual_image)\r\
          \nself.processor = VisionTextDualEncoderProcessor(self.image_processor,\
          \ self.tokenizer)\r\n\r\nwith\r\n\r\npretrained_dual_text = \"xlm-roberta-large\"\
          \r\npretrained_dual_image = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\r\n\
          \r\nwhich resulted in internal shape error which I assume comes from the\
          \ fact that I am not using the correct preprocessors for the task?\r\n\r\
          \n.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 459,\
          \ in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\
          \nRuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d,\
          \ but got input of size: [32, 150528]"
        updatedAt: '2023-07-20T10:49:14.038Z'
      numEdits: 0
      reactions: []
    id: 64b9112ae0bd7b05a8522a2c
    type: comment
  author: premsa
  content: "Hello,\r\n\r\nis the code for the portation of the model available somewhere?\
    \ I am running into problems when trying to retrieve the processor of the model.\
    \ Any assist or pointers towards relevant code would be helpful! \r\n\r\nCODE:\r\
    \nself.processor = VisionTextDualEncoderProcessor.from_pretrained(model_name)\r\
    \n\r\nERROR:\r\n.venv/lib/python3.10/site-packages/transformers/utils/hub.py\"\
    , line 463, in cached_file\r\n    raise EnvironmentError(\r\nOSError: calpt/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k\
    \ does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/calpt/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k/main'\
    \ for available files.\r\n\r\nALTERNATIVE:\r\nI also tried initializing the processor\
    \ from:\r\n\r\nself.tokenizer = AutoTokenizer.from_pretrained(MODEL.pretrained_dual_text)\r\
    \nself.image_processor = AutoFeatureExtractor.from_pretrained(MODEL.pretrained_dual_image)\r\
    \nself.processor = VisionTextDualEncoderProcessor(self.image_processor, self.tokenizer)\r\
    \n\r\nwith\r\n\r\npretrained_dual_text = \"xlm-roberta-large\"\r\npretrained_dual_image\
    \ = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\r\n\r\nwhich resulted in internal\
    \ shape error which I assume comes from the fact that I am not using the correct\
    \ preprocessors for the task?\r\n\r\n.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py\"\
    , line 459, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\
    \nRuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got\
    \ input of size: [32, 150528]"
  created_at: 2023-07-20 09:49:14+00:00
  edited: false
  hidden: false
  id: 64b9112ae0bd7b05a8522a2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1c06f025919bedaa85804934e249e58.svg
      fullname: calpt
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: calpt
      type: user
    createdAt: '2023-09-24T12:31:44.000Z'
    data:
      edited: false
      editors:
      - calpt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6922547221183777
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1c06f025919bedaa85804934e249e58.svg
          fullname: calpt
          isHf: false
          isPro: false
          name: calpt
          type: user
        html: '<p>Hey,</p>

          <p>you can find the code for porting the model from OpenCLIP here: <a rel="nofollow"
          href="https://gist.github.com/calpt/8e3555bd11f1916b5169c8125117e5ee">https://gist.github.com/calpt/8e3555bd11f1916b5169c8125117e5ee</a></p>

          <p>This repo only contains the model checkpoints without tokenizer config
          or preprocessor config. The correct tokenizer/ preprocessor to use would
          be the following:</p>

          <ul>

          <li>tokenizer: <code>xlm-roberta-large</code></li>

          <li>preprocessor: <code>laion/CLIP-ViT-H-14-laion2B-s32B-b79K</code></li>

          </ul>

          '
        raw: 'Hey,


          you can find the code for porting the model from OpenCLIP here: https://gist.github.com/calpt/8e3555bd11f1916b5169c8125117e5ee


          This repo only contains the model checkpoints without tokenizer config or
          preprocessor config. The correct tokenizer/ preprocessor to use would be
          the following:

          - tokenizer: `xlm-roberta-large`

          - preprocessor: `laion/CLIP-ViT-H-14-laion2B-s32B-b79K`


          '
        updatedAt: '2023-09-24T12:31:44.411Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - premsa
    id: 65102c30e14eeb01d4700223
    type: comment
  author: calpt
  content: 'Hey,


    you can find the code for porting the model from OpenCLIP here: https://gist.github.com/calpt/8e3555bd11f1916b5169c8125117e5ee


    This repo only contains the model checkpoints without tokenizer config or preprocessor
    config. The correct tokenizer/ preprocessor to use would be the following:

    - tokenizer: `xlm-roberta-large`

    - preprocessor: `laion/CLIP-ViT-H-14-laion2B-s32B-b79K`


    '
  created_at: 2023-09-24 11:31:44+00:00
  edited: false
  hidden: false
  id: 65102c30e14eeb01d4700223
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: calpt/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k
repo_type: model
status: open
target_branch: null
title: preprocessor_config.json missing
