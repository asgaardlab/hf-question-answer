!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Srp7
conflicting_files: null
created_at: 2023-10-05 15:15:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e6afc5877bad51f74ca039a5bd5ece4.svg
      fullname: Soumya Ranjan Ranjan Patnaik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Srp7
      type: user
    createdAt: '2023-10-05T16:15:37.000Z'
    data:
      edited: false
      editors:
      - Srp7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8190744519233704
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e6afc5877bad51f74ca039a5bd5ece4.svg
          fullname: Soumya Ranjan Ranjan Patnaik
          isHf: false
          isPro: false
          name: Srp7
          type: user
        html: '<p>I created a question answer chatbot based on my pdf. The model gives
          correct answers but twice. How to fix this? and also I need some followup
          questions from the chatbot to the users. How can I use the prompt template?
          Please help me improvise the code:</p>

          <p>model_name_or_path = "TheBloke/Llama-2-7B-chat-GGUF"<br>model_basename
          = "llama-2-7b-chat.Q4_K_S.gguf"  ### <a href="https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/blob/main/llama-2-7b-chat.Q8_0.gguf">https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/blob/main/llama-2-7b-chat.Q8_0.gguf</a><br>model_path
          = hf_hub_download(repo_id=model_name_or_path,filename=model_basename)<br>n_gpu_layers=
          40  # Change this value based on your model and your GPU VRAM pool.<br>n_batch
          = 256  # Should be between 1 and n_ctx, consider the amount of VRAM in your
          GPU.</p>

          <h2 id="loading-model">Loading model</h2>

          <p>llm = LlamaCpp(<br>    model_path=model_path,<br>    max_tokens=256,<br>    n_gpu_layers=n_gpu_layers,<br>    n_batch=n_batch,<br>    callback_manager=callback_manager,<br>    n_ctx=1024,<br>    verbose=True<br>)</p>

          <p>chain = load_qa_chain(llm,chain_type = "stuff")<br>query="Can I put a
          Larger Gun into a Smaller Holster?"<br>docs=docsearch.similarity_search(query)<br>chain.run(input_documents=docs,question=query)</p>

          <p>Answer:  No, it is not recommended to try to put a larger gun into a
          smaller holster as it can be difficult to draw the firearm. No, it is not
          recommended to try to put a larger gun into a smaller holster as it can
          be difficult to draw the firearm.</p>

          <p>See above answer..correct but coming twice</p>

          '
        raw: "I created a question answer chatbot based on my pdf. The model gives\
          \ correct answers but twice. How to fix this? and also I need some followup\
          \ questions from the chatbot to the users. How can I use the prompt template?\
          \ Please help me improvise the code:\r\n\r\nmodel_name_or_path = \"TheBloke/Llama-2-7B-chat-GGUF\"\
          \r\nmodel_basename = \"llama-2-7b-chat.Q4_K_S.gguf\"  ### https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/blob/main/llama-2-7b-chat.Q8_0.gguf\r\
          \nmodel_path = hf_hub_download(repo_id=model_name_or_path,filename=model_basename)\r\
          \nn_gpu_layers= 40  # Change this value based on your model and your GPU\
          \ VRAM pool.\r\nn_batch = 256  # Should be between 1 and n_ctx, consider\
          \ the amount of VRAM in your GPU.\r\n## Loading model\r\nllm = LlamaCpp(\r\
          \n    model_path=model_path,\r\n    max_tokens=256,\r\n    n_gpu_layers=n_gpu_layers,\r\
          \n    n_batch=n_batch,\r\n    callback_manager=callback_manager,\r\n   \
          \ n_ctx=1024,\r\n    verbose=True\r\n)\r\n\r\nchain = load_qa_chain(llm,chain_type\
          \ = \"stuff\")\r\nquery=\"Can I put a Larger Gun into a Smaller Holster?\"\
          \r\ndocs=docsearch.similarity_search(query)\r\nchain.run(input_documents=docs,question=query)\r\
          \n\r\nAnswer:  No, it is not recommended to try to put a larger gun into\
          \ a smaller holster as it can be difficult to draw the firearm. No, it is\
          \ not recommended to try to put a larger gun into a smaller holster as it\
          \ can be difficult to draw the firearm.\r\n\r\nSee above answer..correct\
          \ but coming twice\r\n"
        updatedAt: '2023-10-05T16:15:37.909Z'
      numEdits: 0
      reactions: []
    id: 651ee129630a846432e90594
    type: comment
  author: Srp7
  content: "I created a question answer chatbot based on my pdf. The model gives correct\
    \ answers but twice. How to fix this? and also I need some followup questions\
    \ from the chatbot to the users. How can I use the prompt template? Please help\
    \ me improvise the code:\r\n\r\nmodel_name_or_path = \"TheBloke/Llama-2-7B-chat-GGUF\"\
    \r\nmodel_basename = \"llama-2-7b-chat.Q4_K_S.gguf\"  ### https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/blob/main/llama-2-7b-chat.Q8_0.gguf\r\
    \nmodel_path = hf_hub_download(repo_id=model_name_or_path,filename=model_basename)\r\
    \nn_gpu_layers= 40  # Change this value based on your model and your GPU VRAM\
    \ pool.\r\nn_batch = 256  # Should be between 1 and n_ctx, consider the amount\
    \ of VRAM in your GPU.\r\n## Loading model\r\nllm = LlamaCpp(\r\n    model_path=model_path,\r\
    \n    max_tokens=256,\r\n    n_gpu_layers=n_gpu_layers,\r\n    n_batch=n_batch,\r\
    \n    callback_manager=callback_manager,\r\n    n_ctx=1024,\r\n    verbose=True\r\
    \n)\r\n\r\nchain = load_qa_chain(llm,chain_type = \"stuff\")\r\nquery=\"Can I\
    \ put a Larger Gun into a Smaller Holster?\"\r\ndocs=docsearch.similarity_search(query)\r\
    \nchain.run(input_documents=docs,question=query)\r\n\r\nAnswer:  No, it is not\
    \ recommended to try to put a larger gun into a smaller holster as it can be difficult\
    \ to draw the firearm. No, it is not recommended to try to put a larger gun into\
    \ a smaller holster as it can be difficult to draw the firearm.\r\n\r\nSee above\
    \ answer..correct but coming twice\r\n"
  created_at: 2023-10-05 15:15:37+00:00
  edited: false
  hidden: false
  id: 651ee129630a846432e90594
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Llama-2-7B-Chat-GGUF
repo_type: model
status: open
target_branch: null
title: This model giving the correct answer but twice
