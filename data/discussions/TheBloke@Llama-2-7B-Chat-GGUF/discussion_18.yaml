!!python/object:huggingface_hub.community.DiscussionWithDetails
author: krishnapiya
conflicting_files: null
created_at: 2024-01-20 05:19:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04c6f7fa0ca737fc2aa509c43238a4a5.svg
      fullname: Krishnapriya s
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krishnapiya
      type: user
    createdAt: '2024-01-20T05:19:08.000Z'
    data:
      edited: false
      editors:
      - krishnapiya
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7732967734336853
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04c6f7fa0ca737fc2aa509c43238a4a5.svg
          fullname: Krishnapriya s
          isHf: false
          isPro: false
          name: krishnapiya
          type: user
        html: '<p>GGML_ASSERT: /tmp/pip-install-3q_fwex4/llama-cpp-python_520e3a5b95cc4b339cb4759635dc8a44/vendor/llama.cpp/ggml-cuda.cu:6741:
          ptr == (void *) (g_cuda_pool_addr[device] + g_cuda_pool_used[device])<br>Could
          not attach to process. If your uid matches the uid of the target<br>process,
          check the setting of /proc/sys/kernel/yama/ptrace_scope, or try<br>again
          as the root user. For more details, see /etc/sysctl.d/10-ptrace.conf<br>ptrace:
          Operation not permitted.<br>No stack.<br>The program is not being run.</p>

          <p>The above error is created while I try to process multiple request at
          a time.The error is happening from a chat bot created using Llama-2-7b chat
          GGUF file locally</p>

          '
        raw: "\r\nGGML_ASSERT: /tmp/pip-install-3q_fwex4/llama-cpp-python_520e3a5b95cc4b339cb4759635dc8a44/vendor/llama.cpp/ggml-cuda.cu:6741:\
          \ ptr == (void *) (g_cuda_pool_addr[device] + g_cuda_pool_used[device])\r\
          \nCould not attach to process. If your uid matches the uid of the target\r\
          \nprocess, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try\r\
          \nagain as the root user. For more details, see /etc/sysctl.d/10-ptrace.conf\r\
          \nptrace: Operation not permitted.\r\nNo stack.\r\nThe program is not being\
          \ run.\r\n\r\nThe above error is created while I try to process multiple\
          \ request at a time.The error is happening from a chat bot created using\
          \ Llama-2-7b chat GGUF file locally\r\n"
        updatedAt: '2024-01-20T05:19:08.650Z'
      numEdits: 0
      reactions: []
    id: 65ab57cce2a2c86356995a0f
    type: comment
  author: krishnapiya
  content: "\r\nGGML_ASSERT: /tmp/pip-install-3q_fwex4/llama-cpp-python_520e3a5b95cc4b339cb4759635dc8a44/vendor/llama.cpp/ggml-cuda.cu:6741:\
    \ ptr == (void *) (g_cuda_pool_addr[device] + g_cuda_pool_used[device])\r\nCould\
    \ not attach to process. If your uid matches the uid of the target\r\nprocess,\
    \ check the setting of /proc/sys/kernel/yama/ptrace_scope, or try\r\nagain as\
    \ the root user. For more details, see /etc/sysctl.d/10-ptrace.conf\r\nptrace:\
    \ Operation not permitted.\r\nNo stack.\r\nThe program is not being run.\r\n\r\
    \nThe above error is created while I try to process multiple request at a time.The\
    \ error is happening from a chat bot created using Llama-2-7b chat GGUF file locally\r\
    \n"
  created_at: 2024-01-20 05:19:08+00:00
  edited: false
  hidden: false
  id: 65ab57cce2a2c86356995a0f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: TheBloke/Llama-2-7B-Chat-GGUF
repo_type: model
status: open
target_branch: null
title: Problem while making multiple request at a time from seperate chat bot instances
