!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Learner
conflicting_files: null
created_at: 2023-10-22 20:10:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b347ddf55ec2288a7c8fc5d8c13a6696.svg
      fullname: Doge
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Learner
      type: user
    createdAt: '2023-10-22T21:10:13.000Z'
    data:
      edited: false
      editors:
      - Learner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7324053645133972
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b347ddf55ec2288a7c8fc5d8c13a6696.svg
          fullname: Doge
          isHf: false
          isPro: false
          name: Learner
          type: user
        html: '<pre><code>from llama_cpp import Llama


          llm = Llama(model_path="llama-2-7b-chat.Q2_K.gguf", n_ctx=512, n_batch=126)

          </code></pre>

          <p>Gives the error:</p>

          <pre><code>gguf_init_from_file: invalid magic number 4f44213c

          error loading model: llama_model_loader: failed to load model from llama-2-7b-chat.Q2_K.gguf


          llama_load_model_from_file: failed to load model

          </code></pre>

          <p>I changed from the format GGML to GGUF and thought it would resolve the
          error but it did not.</p>

          <p>I have the llama-2-7b-chat.Q2_K.gguf file completely downloaded and can
          also access it (path is correct).</p>

          <p>Any idea?</p>

          '
        raw: "```\r\nfrom llama_cpp import Llama\r\n\r\nllm = Llama(model_path=\"\
          llama-2-7b-chat.Q2_K.gguf\", n_ctx=512, n_batch=126)\r\n```\r\n\r\nGives\
          \ the error:\r\n\r\n```\r\ngguf_init_from_file: invalid magic number 4f44213c\r\
          \nerror loading model: llama_model_loader: failed to load model from llama-2-7b-chat.Q2_K.gguf\r\
          \n\r\nllama_load_model_from_file: failed to load model\r\n```\r\n\r\nI changed\
          \ from the format GGML to GGUF and thought it would resolve the error but\
          \ it did not.\r\n\r\nI have the llama-2-7b-chat.Q2_K.gguf file completely\
          \ downloaded and can also access it (path is correct).\r\n\r\nAny idea?\r\
          \n"
        updatedAt: '2023-10-22T21:10:13.995Z'
      numEdits: 0
      reactions: []
    id: 65358fb51995cee54a2727ec
    type: comment
  author: Learner
  content: "```\r\nfrom llama_cpp import Llama\r\n\r\nllm = Llama(model_path=\"llama-2-7b-chat.Q2_K.gguf\"\
    , n_ctx=512, n_batch=126)\r\n```\r\n\r\nGives the error:\r\n\r\n```\r\ngguf_init_from_file:\
    \ invalid magic number 4f44213c\r\nerror loading model: llama_model_loader: failed\
    \ to load model from llama-2-7b-chat.Q2_K.gguf\r\n\r\nllama_load_model_from_file:\
    \ failed to load model\r\n```\r\n\r\nI changed from the format GGML to GGUF and\
    \ thought it would resolve the error but it did not.\r\n\r\nI have the llama-2-7b-chat.Q2_K.gguf\
    \ file completely downloaded and can also access it (path is correct).\r\n\r\n\
    Any idea?\r\n"
  created_at: 2023-10-22 20:10:13+00:00
  edited: false
  hidden: false
  id: 65358fb51995cee54a2727ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e0aaa6982fd0beb5f792a4bf5f38e321.svg
      fullname: Ziyu Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Alexziyu
      type: user
    createdAt: '2023-11-26T08:37:49.000Z'
    data:
      edited: false
      editors:
      - Alexziyu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5754402279853821
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e0aaa6982fd0beb5f792a4bf5f38e321.svg
          fullname: Ziyu Wang
          isHf: false
          isPro: false
          name: Alexziyu
          type: user
        html: '<p>Could not load Llama model from path: E:/upenn/wrds/llama-2-7b-chat.Q3_K_M.gguf.
          Received error  (type=value_error)</p>

          '
        raw: 'Could not load Llama model from path: E:/upenn/wrds/llama-2-7b-chat.Q3_K_M.gguf.
          Received error  (type=value_error)'
        updatedAt: '2023-11-26T08:37:49.690Z'
      numEdits: 0
      reactions: []
    id: 656303dd4e74b2075a166b27
    type: comment
  author: Alexziyu
  content: 'Could not load Llama model from path: E:/upenn/wrds/llama-2-7b-chat.Q3_K_M.gguf.
    Received error  (type=value_error)'
  created_at: 2023-11-26 08:37:49+00:00
  edited: false
  hidden: false
  id: 656303dd4e74b2075a166b27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a5b86ce851cb1df6784952/FUDCk6bNUmdmRjPDE4ztB.jpeg?w=200&h=200&f=face
      fullname: Kristian Virtanen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: EkBass
      type: user
    createdAt: '2024-01-18T19:15:47.000Z'
    data:
      edited: false
      editors:
      - EkBass
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6649090051651001
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a5b86ce851cb1df6784952/FUDCk6bNUmdmRjPDE4ztB.jpeg?w=200&h=200&f=face
          fullname: Kristian Virtanen
          isHf: false
          isPro: false
          name: EkBass
          type: user
        html: '<p>This works for me: <a rel="nofollow" href="https://github.com/EkBass/console-chat-for-llama-2-7b-chat">https://github.com/EkBass/console-chat-for-llama-2-7b-chat</a></p>

          '
        raw: 'This works for me: https://github.com/EkBass/console-chat-for-llama-2-7b-chat'
        updatedAt: '2024-01-18T19:15:47.558Z'
      numEdits: 0
      reactions: []
    id: 65a978e3d6fc9616a63a18cc
    type: comment
  author: EkBass
  content: 'This works for me: https://github.com/EkBass/console-chat-for-llama-2-7b-chat'
  created_at: 2024-01-18 19:15:47+00:00
  edited: false
  hidden: false
  id: 65a978e3d6fc9616a63a18cc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: TheBloke/Llama-2-7B-Chat-GGUF
repo_type: model
status: open
target_branch: null
title: 'Running locally: Cannot load model "llama-2-7b-chat.Q2_K.gguf"'
