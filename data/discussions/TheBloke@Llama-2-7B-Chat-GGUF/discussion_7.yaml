!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gaukelkar
conflicting_files: null
created_at: 2023-10-10 08:23:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/efbba4b33d8496b6668bceb8e78f6450.svg
      fullname: Gaurav Kelkar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gaukelkar
      type: user
    createdAt: '2023-10-10T09:23:14.000Z'
    data:
      edited: false
      editors:
      - gaukelkar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.523679792881012
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/efbba4b33d8496b6668bceb8e78f6450.svg
          fullname: Gaurav Kelkar
          isHf: false
          isPro: false
          name: gaukelkar
          type: user
        html: '<p>I have copy-pasted below code from the sample provided here-</p>

          <p>llm = AutoModelForCausalLM.from_pretrained("TheBloke/Llama-2-7b-Chat-GGUF",
          model_file="llama-2-7b-chat.q4_K_M.gguf", model_type="llama", gpu_layers=50)</p>

          <p>Getting error that the model file is not found. The stack trace looks
          as follows:</p>

          <p>Traceback (most recent call last):<br>File "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/queueing.py",
          line 406, in call_prediction<br>    output = await route_utils.call_process_api(<br>  File
          "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/route_utils.py",
          line 217, in call_process_api<br>    output = await app.get_blocks().process_api(<br>  File
          "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/blocks.py",
          line 1553, in process_api<br>    result = await self.call_function(<br>  File
          "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/blocks.py",
          line 1191, in call_function<br>    prediction = await anyio.to_thread.run_sync(<br>  File
          "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/anyio/to_thread.py",
          line 33, in run_sync<br>    return await get_asynclib().run_sync_in_worker_thread(<br>  File
          "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/anyio/_backends/_asyncio.py",
          line 877, in run_sync_in_worker_thread<br>    return await future<br>  File
          "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/anyio/_backends/_asyncio.py",
          line 807, in run<br>    result = context.run(func, *args)<br>  File "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/utils.py",
          line 659, in wrapper<br>    response = f(*args, **kwargs)<br>  File "/home/user/app/app.py",
          line 11, in answer<br>    chat_agent = Llama_chat.Llama_chat()<br>  File
          "/home/user/app/Llama_chat.py", line 49, in <strong>init</strong><br>    self.llm
          = AutoModelForCausalLM.from_pretrained("TheBloke/Llama-2-7b-Chat-GGUF",
          model_file="llama-2-7b-chat.q4_K_M.gguf", model_type="llama", gpu_layers=50)<br>  File
          "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/ctransformers/hub.py",
          line 168, in from_pretrained<br>    model_path = cls._find_model_path_from_repo(<br>  File
          "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/ctransformers/hub.py",
          line 209, in _find_model_path_from_repo<br>    return cls._find_model_path_from_dir(path,
          filename=filename)<br>  File "/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/ctransformers/hub.py",
          line 242, in _find_model_path_from_dir<br>    raise ValueError(f"Model file
          ''{filename}'' not found in ''{path}''")<br>ValueError: Model file ''llama-2-7b-chat.q4_K_M.gguf''
          not found in ''/home/user/.cache/huggingface/hub/models--TheBloke--Llama-2-7b-Chat-GGUF/snapshots/9ca625120374ddaae21f067cb006517d14dc91a6''</p>

          '
        raw: "I have copy-pasted below code from the sample provided here-\r\n\r\n\
          llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GGUF\"\
          , model_file=\"llama-2-7b-chat.q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\r\
          \n\r\nGetting error that the model file is not found. The stack trace looks\
          \ as follows:\r\n\r\nTraceback (most recent call last):\r\nFile \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/queueing.py\"\
          , line 406, in call_prediction\r\n    output = await route_utils.call_process_api(\r\
          \n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/route_utils.py\"\
          , line 217, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\
          \n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/blocks.py\"\
          , line 1553, in process_api\r\n    result = await self.call_function(\r\n\
          \  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/blocks.py\"\
          , line 1191, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\
          \n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/anyio/to_thread.py\"\
          , line 33, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\
          \n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\"\
          , line 877, in run_sync_in_worker_thread\r\n    return await future\r\n\
          \  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\"\
          , line 807, in run\r\n    result = context.run(func, *args)\r\n  File \"\
          /home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/utils.py\"\
          , line 659, in wrapper\r\n    response = f(*args, **kwargs)\r\n  File \"\
          /home/user/app/app.py\", line 11, in answer\r\n    chat_agent = Llama_chat.Llama_chat()\r\
          \n  File \"/home/user/app/Llama_chat.py\", line 49, in __init__\r\n    self.llm\
          \ = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GGUF\"\
          , model_file=\"llama-2-7b-chat.q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\r\
          \n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/ctransformers/hub.py\"\
          , line 168, in from_pretrained\r\n    model_path = cls._find_model_path_from_repo(\r\
          \n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/ctransformers/hub.py\"\
          , line 209, in _find_model_path_from_repo\r\n    return cls._find_model_path_from_dir(path,\
          \ filename=filename)\r\n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/ctransformers/hub.py\"\
          , line 242, in _find_model_path_from_dir\r\n    raise ValueError(f\"Model\
          \ file '{filename}' not found in '{path}'\")\r\nValueError: Model file 'llama-2-7b-chat.q4_K_M.gguf'\
          \ not found in '/home/user/.cache/huggingface/hub/models--TheBloke--Llama-2-7b-Chat-GGUF/snapshots/9ca625120374ddaae21f067cb006517d14dc91a6'\r\
          \n"
        updatedAt: '2023-10-10T09:23:14.813Z'
      numEdits: 0
      reactions: []
    id: 652518022eff981222c1cb32
    type: comment
  author: gaukelkar
  content: "I have copy-pasted below code from the sample provided here-\r\n\r\nllm\
    \ = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GGUF\", model_file=\"\
    llama-2-7b-chat.q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\r\n\r\nGetting\
    \ error that the model file is not found. The stack trace looks as follows:\r\n\
    \r\nTraceback (most recent call last):\r\nFile \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/queueing.py\"\
    , line 406, in call_prediction\r\n    output = await route_utils.call_process_api(\r\
    \n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/route_utils.py\"\
    , line 217, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\
    \n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/blocks.py\"\
    , line 1553, in process_api\r\n    result = await self.call_function(\r\n  File\
    \ \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/blocks.py\"\
    , line 1191, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\
    \n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/anyio/to_thread.py\"\
    , line 33, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\
    \n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\"\
    , line 877, in run_sync_in_worker_thread\r\n    return await future\r\n  File\
    \ \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\"\
    , line 807, in run\r\n    result = context.run(func, *args)\r\n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gradio/utils.py\"\
    , line 659, in wrapper\r\n    response = f(*args, **kwargs)\r\n  File \"/home/user/app/app.py\"\
    , line 11, in answer\r\n    chat_agent = Llama_chat.Llama_chat()\r\n  File \"\
    /home/user/app/Llama_chat.py\", line 49, in __init__\r\n    self.llm = AutoModelForCausalLM.from_pretrained(\"\
    TheBloke/Llama-2-7b-Chat-GGUF\", model_file=\"llama-2-7b-chat.q4_K_M.gguf\", model_type=\"\
    llama\", gpu_layers=50)\r\n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/ctransformers/hub.py\"\
    , line 168, in from_pretrained\r\n    model_path = cls._find_model_path_from_repo(\r\
    \n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/ctransformers/hub.py\"\
    , line 209, in _find_model_path_from_repo\r\n    return cls._find_model_path_from_dir(path,\
    \ filename=filename)\r\n  File \"/home/user/.pyenv/versions/3.10.13/lib/python3.10/site-packages/ctransformers/hub.py\"\
    , line 242, in _find_model_path_from_dir\r\n    raise ValueError(f\"Model file\
    \ '{filename}' not found in '{path}'\")\r\nValueError: Model file 'llama-2-7b-chat.q4_K_M.gguf'\
    \ not found in '/home/user/.cache/huggingface/hub/models--TheBloke--Llama-2-7b-Chat-GGUF/snapshots/9ca625120374ddaae21f067cb006517d14dc91a6'\r\
    \n"
  created_at: 2023-10-10 08:23:14+00:00
  edited: false
  hidden: false
  id: 652518022eff981222c1cb32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-10T09:38:06.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7130492925643921
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry, this is a typo in the readme.  The filename is <code>llama-2-7b-chat.Q4_K_M.gguf</code>
          (capital Q)</p>

          '
        raw: Sorry, this is a typo in the readme.  The filename is `llama-2-7b-chat.Q4_K_M.gguf`
          (capital Q)
        updatedAt: '2023-10-10T09:38:06.918Z'
      numEdits: 0
      reactions: []
    id: 65251b7e73d871a1fc9064ca
    type: comment
  author: TheBloke
  content: Sorry, this is a typo in the readme.  The filename is `llama-2-7b-chat.Q4_K_M.gguf`
    (capital Q)
  created_at: 2023-10-10 08:38:06+00:00
  edited: false
  hidden: false
  id: 65251b7e73d871a1fc9064ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/efbba4b33d8496b6668bceb8e78f6450.svg
      fullname: Gaurav Kelkar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gaukelkar
      type: user
    createdAt: '2023-10-10T10:32:58.000Z'
    data:
      edited: false
      editors:
      - gaukelkar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9178734421730042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/efbba4b33d8496b6668bceb8e78f6450.svg
          fullname: Gaurav Kelkar
          isHf: false
          isPro: false
          name: gaukelkar
          type: user
        html: '<p>Thanks for the prompt response! That error is resolved now. Still
          getting error in ctransformers though :) </p>

          '
        raw: 'Thanks for the prompt response! That error is resolved now. Still getting
          error in ctransformers though :) '
        updatedAt: '2023-10-10T10:32:58.814Z'
      numEdits: 0
      reactions: []
    id: 6525285abd492d9b632b36fd
    type: comment
  author: gaukelkar
  content: 'Thanks for the prompt response! That error is resolved now. Still getting
    error in ctransformers though :) '
  created_at: 2023-10-10 09:32:58+00:00
  edited: false
  hidden: false
  id: 6525285abd492d9b632b36fd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/Llama-2-7B-Chat-GGUF
repo_type: model
status: open
target_branch: null
title: 'Error while using CTransformers: Model file ''llama-2-7b-chat.q4_K_M.gguf''
  not found'
