!!python/object:huggingface_hub.community.DiscussionWithDetails
author: awarity-dev
conflicting_files: null
created_at: 2023-10-26 23:22:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a9b2af19bb7cf1ae9adf5bea206f754d.svg
      fullname: awarity.ai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: awarity-dev
      type: user
    createdAt: '2023-10-27T00:22:36.000Z'
    data:
      edited: true
      editors:
      - awarity-dev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6783599853515625
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a9b2af19bb7cf1ae9adf5bea206f754d.svg
          fullname: awarity.ai
          isHf: false
          isPro: false
          name: awarity-dev
          type: user
        html: "<p>Hi there, I am trying to get this model working with a llama index\
          \ vector store.</p>\n<p>I can follow this doc, with modifications for a\
          \ local vector store of some markdown files, and get good responses.<br><a\
          \ rel=\"nofollow\" href=\"https://gpt-index.readthedocs.io/en/latest/examples/llm/llama_2_llama_cpp.html\"\
          >https://gpt-index.readthedocs.io/en/latest/examples/llm/llama_2_llama_cpp.html</a><br>That\
          \ uses this model: \"<a href=\"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf&quot;\"\
          >https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\
          </a><br>with this embedding: \"BAAI/bge-small-en-v1.5\"</p>\n<p>I see on\
          \ the model card that llama-2-7b-chat.Q4_K_M.gguf is recommended. However\
          \ when using that model with any of the \"BAAI/bge-small-en-v1.5\" variants\
          \ I get dimension mis-size errors.<br>example:<br><code>ValueError: shapes\
          \ (1536,) and (768,) not aligned: 1536 (dim 0) != 768 (dim 0)</code></p>\n\
          <p>This occurs when pinging the URL, or loading a local copy of the model.<br>Non\
          \ working example:</p>\n<pre><code>from llama_index.llms import LlamaCPP\n\
          model_url = \"https://huggingface.co/TheBloke/Llama-2-7B-chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\"\
          \nllm = LlamaCPP(\n        # You can pass in the URL to a GGML model to\
          \ download it automatically\n        model_url=model_url,\n        # optionally,\
          \ you can set the path to a pre-downloaded model instead of model_url\n\
          \        # model_path=\"./models/TheBloke/Llama-2-7B-chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf\"\
          ,\n        temperature=0.1,\n        max_new_tokens=256,\n        # llama2\
          \ has a context window of 4096 tokens, but we set it lower to allow for\
          \ some wiggle room\n        context_window=3900,\n        # kwargs to pass\
          \ to __call__()\n        generate_kwargs={},\n        # kwargs to pass to\
          \ __init__()\n        # set to at least 1 to use GPU\n        model_kwargs={\"\
          n_gpu_layers\": 1},\n        # transform inputs into Llama2 format\n   \
          \     messages_to_prompt=messages_to_prompt,\n        completion_to_prompt=completion_to_prompt,\n\
          \        # verbose=True,\n    )\n</code></pre>\n<p>Am I right that this\
          \ is some sort of mismatch between the embedding model, the storage in a\
          \ VectorStoreIndex, and the instantiation of the LLM version?</p>\n<p>Again\
          \ I am able to get the \"llama-2-13b-chat.Q4_0.gguf\" version to work, but\
          \ not the \"llama-2-7b-chat.Q4_K_M.gguf\"</p>\n<p>Thanks for reading.</p>\n"
        raw: "Hi there, I am trying to get this model working with a llama index vector\
          \ store.\n\nI can follow this doc, with modifications for a local vector\
          \ store of some markdown files, and get good responses. \nhttps://gpt-index.readthedocs.io/en/latest/examples/llm/llama_2_llama_cpp.html\n\
          That uses this model: \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\
          \nwith this embedding: \"BAAI/bge-small-en-v1.5\"\n\nI see on the model\
          \ card that llama-2-7b-chat.Q4_K_M.gguf is recommended. However when using\
          \ that model with any of the \"BAAI/bge-small-en-v1.5\" variants I get dimension\
          \ mis-size errors. \nexample:\n```ValueError: shapes (1536,) and (768,)\
          \ not aligned: 1536 (dim 0) != 768 (dim 0)```\n\nThis occurs when pinging\
          \ the URL, or loading a local copy of the model. \nNon working example:\n\
          ```\nfrom llama_index.llms import LlamaCPP\nmodel_url = \"https://huggingface.co/TheBloke/Llama-2-7B-chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\"\
          \nllm = LlamaCPP(\n        # You can pass in the URL to a GGML model to\
          \ download it automatically\n        model_url=model_url,\n        # optionally,\
          \ you can set the path to a pre-downloaded model instead of model_url\n\
          \        # model_path=\"./models/TheBloke/Llama-2-7B-chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf\"\
          ,\n        temperature=0.1,\n        max_new_tokens=256,\n        # llama2\
          \ has a context window of 4096 tokens, but we set it lower to allow for\
          \ some wiggle room\n        context_window=3900,\n        # kwargs to pass\
          \ to __call__()\n        generate_kwargs={},\n        # kwargs to pass to\
          \ __init__()\n        # set to at least 1 to use GPU\n        model_kwargs={\"\
          n_gpu_layers\": 1},\n        # transform inputs into Llama2 format\n   \
          \     messages_to_prompt=messages_to_prompt,\n        completion_to_prompt=completion_to_prompt,\n\
          \        # verbose=True,\n    )\n```\n\nAm I right that this is some sort\
          \ of mismatch between the embedding model, the storage in a VectorStoreIndex,\
          \ and the instantiation of the LLM version?\n\nAgain I am able to get the\
          \ \"llama-2-13b-chat.Q4_0.gguf\" version to work, but not the \"llama-2-7b-chat.Q4_K_M.gguf\"\
          \n\nThanks for reading."
        updatedAt: '2023-10-27T00:26:30.798Z'
      numEdits: 3
      reactions: []
    id: 653b02ccae155b92bafecfa8
    type: comment
  author: awarity-dev
  content: "Hi there, I am trying to get this model working with a llama index vector\
    \ store.\n\nI can follow this doc, with modifications for a local vector store\
    \ of some markdown files, and get good responses. \nhttps://gpt-index.readthedocs.io/en/latest/examples/llm/llama_2_llama_cpp.html\n\
    That uses this model: \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\
    \nwith this embedding: \"BAAI/bge-small-en-v1.5\"\n\nI see on the model card that\
    \ llama-2-7b-chat.Q4_K_M.gguf is recommended. However when using that model with\
    \ any of the \"BAAI/bge-small-en-v1.5\" variants I get dimension mis-size errors.\
    \ \nexample:\n```ValueError: shapes (1536,) and (768,) not aligned: 1536 (dim\
    \ 0) != 768 (dim 0)```\n\nThis occurs when pinging the URL, or loading a local\
    \ copy of the model. \nNon working example:\n```\nfrom llama_index.llms import\
    \ LlamaCPP\nmodel_url = \"https://huggingface.co/TheBloke/Llama-2-7B-chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\"\
    \nllm = LlamaCPP(\n        # You can pass in the URL to a GGML model to download\
    \ it automatically\n        model_url=model_url,\n        # optionally, you can\
    \ set the path to a pre-downloaded model instead of model_url\n        # model_path=\"\
    ./models/TheBloke/Llama-2-7B-chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf\",\n      \
    \  temperature=0.1,\n        max_new_tokens=256,\n        # llama2 has a context\
    \ window of 4096 tokens, but we set it lower to allow for some wiggle room\n \
    \       context_window=3900,\n        # kwargs to pass to __call__()\n       \
    \ generate_kwargs={},\n        # kwargs to pass to __init__()\n        # set to\
    \ at least 1 to use GPU\n        model_kwargs={\"n_gpu_layers\": 1},\n       \
    \ # transform inputs into Llama2 format\n        messages_to_prompt=messages_to_prompt,\n\
    \        completion_to_prompt=completion_to_prompt,\n        # verbose=True,\n\
    \    )\n```\n\nAm I right that this is some sort of mismatch between the embedding\
    \ model, the storage in a VectorStoreIndex, and the instantiation of the LLM version?\n\
    \nAgain I am able to get the \"llama-2-13b-chat.Q4_0.gguf\" version to work, but\
    \ not the \"llama-2-7b-chat.Q4_K_M.gguf\"\n\nThanks for reading."
  created_at: 2023-10-26 23:22:36+00:00
  edited: true
  hidden: false
  id: 653b02ccae155b92bafecfa8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea06bc606dff90b2144d740eb1d3e508.svg
      fullname: rgf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: odrec
      type: user
    createdAt: '2024-01-11T17:11:52.000Z'
    data:
      edited: false
      editors:
      - odrec
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.999070405960083
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea06bc606dff90b2144d740eb1d3e508.svg
          fullname: rgf
          isHf: false
          isPro: false
          name: odrec
          type: user
        html: '<p>How did you solve this?</p>

          '
        raw: How did you solve this?
        updatedAt: '2024-01-11T17:11:52.110Z'
      numEdits: 0
      reactions: []
    id: 65a02158074bfce8021d5c9b
    type: comment
  author: odrec
  content: How did you solve this?
  created_at: 2024-01-11 17:11:52+00:00
  edited: false
  hidden: false
  id: 65a02158074bfce8021d5c9b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: TheBloke/Llama-2-7B-Chat-GGUF
repo_type: model
status: open
target_branch: null
title: proper embedding for llama-2-7b-chat.Q4_K_M.gguf
