!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zkrider
conflicting_files: null
created_at: 2023-08-07 15:30:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cdd6f0f8d7c324bf0bae86dedd4cfaf.svg
      fullname: Zak KRider
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zkrider
      type: user
    createdAt: '2023-08-07T16:30:38.000Z'
    data:
      edited: false
      editors:
      - zkrider
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48352453112602234
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cdd6f0f8d7c324bf0bae86dedd4cfaf.svg
          fullname: Zak KRider
          isHf: false
          isPro: false
          name: zkrider
          type: user
        html: "<p>Deploying through AWS Sagemaker and running into these errors</p>\n\
          <p>Error <a href=\"/TheBloke/NewHope-GPTQ/discussions/1\">#1</a> </p>\n\
          <pre><code>#033[2m2023-08-07T15:49:27.786695Z#033[0m #033[31mERROR#033[0m\
          \ #033[1mshard-manager#033[0m: #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m\
          \ Error when initializing model\n</code></pre>\n<p>Error <a href=\"/TheBloke/NewHope-GPTQ/discussions/2\"\
          >#2</a></p>\n<pre><code>Traceback (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in &lt;module&gt;\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1130,\
          \ in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404,\
          \ in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 760, in\
          \ invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 67, in serve\n    server.serve(model_id, revision, sharded, quantize,\
          \ trust_remote_code, uds_path)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 155, in serve\n    asyncio.run(serve_inner(model_id, revision, sharded,\
          \ quantize, trust_remote_code))\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          </code></pre>\n<p>Error <a href=\"/TheBloke/NewHope-GPTQ/discussions/3\"\
          >#3</a></p>\n<pre><code>&gt; File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 124, in serve_inner\n    model = get_model(model_id, revision, sharded,\
          \ quantize, trust_remote_code)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 246, in get_model\n    return llama_cls(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 58, in __init__\n    filenames = weight_files(model_id, revision,\
          \ \".bin\")\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/hub.py\"\
          , line 95, in weight_files\n    raise e\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/hub.py\"\
          , line 92, in weight_files\n    filenames = weight_hub_files(model_id, revision,\
          \ extension)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/hub.py\"\
          , line 33, in weight_hub_files\n    raise EntryNotFoundError(\n</code></pre>\n\
          <p>Error #4</p>\n<pre><code>huggingface_hub.utils._errors.EntryNotFoundError:\
          \ No .bin weights found for model TheBloke/NewHope-GPTQ and revision None.\n\
          \ #033[2m#033[3mrank#033[0m#033[2m=#033[0m0#033[0m\n</code></pre>\n<p>Sagemaker\
          \ notebook</p>\n<pre><code>import json\nimport sagemaker\nimport boto3\n\
          from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n\
          \ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n\
          \    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\
          \n# Hub Model configuration. https://huggingface.co/models\nhub = {\n  \
          \  'HF_MODEL_ID':'TheBloke/NewHope-GPTQ',\n    'SM_NUM_GPUS': json.dumps(1),\n\
          \    'HF_API_TOKEN': '&lt;TOKEN&gt;'\n}\n\n# create Hugging Face Model Class\n\
          huggingface_model = HuggingFaceModel(\n    image_uri=get_huggingface_llm_image_uri(\"\
          huggingface\",version=\"0.8.2\"),\n    env=hub,\n    role=role, \n)\n\n\
          # deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n\
          \    initial_instance_count=1,\n    instance_type=\"ml.g5.2xlarge\",\n \
          \   container_startup_health_check_timeout=300,\n    endpoint_name=\"NewHopeGPTQ\"\
          ,\n    model_name=\"NewHopeGPTQ\"\n  )\n  \n# send request\npredictor.predict({\n\
          \    \"inputs\": \"My name is Julien and I like to\",\n})\n</code></pre>\n"
        raw: "Deploying through AWS Sagemaker and running into these errors\r\n\r\n\
          Error #1 \r\n```\r\n#033[2m2023-08-07T15:49:27.786695Z#033[0m #033[31mERROR#033[0m\
          \ #033[1mshard-manager#033[0m: #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m\
          \ Error when initializing model\r\n```\r\nError #2\r\n```\r\nTraceback (most\
          \ recent call last):\r\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/core.py\", line 778, in main\r\
          \n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\r\
          \n    return callback(**use_params)  # type: ignore\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 67, in serve\r\n    server.serve(model_id, revision, sharded, quantize,\
          \ trust_remote_code, uds_path)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 155, in serve\r\n    asyncio.run(serve_inner(model_id, revision,\
          \ sharded, quantize, trust_remote_code))\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 634, in run_until_complete\r\
          \n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\
          \n```\r\nError #3\r\n```\r\n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 124, in serve_inner\r\n    model = get_model(model_id, revision,\
          \ sharded, quantize, trust_remote_code)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 246, in get_model\r\n    return llama_cls(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 58, in __init__\r\n    filenames = weight_files(model_id, revision,\
          \ \".bin\")\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/hub.py\"\
          , line 95, in weight_files\r\n    raise e\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/hub.py\"\
          , line 92, in weight_files\r\n    filenames = weight_hub_files(model_id,\
          \ revision, extension)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/hub.py\"\
          , line 33, in weight_hub_files\r\n    raise EntryNotFoundError(\r\n```\r\
          \nError #4\r\n```\r\nhuggingface_hub.utils._errors.EntryNotFoundError: No\
          \ .bin weights found for model TheBloke/NewHope-GPTQ and revision None.\r\
          \n #033[2m#033[3mrank#033[0m#033[2m=#033[0m0#033[0m\r\n```\r\n\r\nSagemaker\
          \ notebook\r\n```\r\nimport json\r\nimport sagemaker\r\nimport boto3\r\n\
          from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\r\
          \n\r\ntry:\r\n\trole = sagemaker.get_execution_role()\r\nexcept ValueError:\r\
          \n\tiam = boto3.client('iam')\r\n\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\r\
          \n\r\n# Hub Model configuration. https://huggingface.co/models\r\nhub =\
          \ {\r\n\t'HF_MODEL_ID':'TheBloke/NewHope-GPTQ',\r\n\t'SM_NUM_GPUS': json.dumps(1),\r\
          \n    'HF_API_TOKEN': '<TOKEN>'\r\n}\r\n\r\n# create Hugging Face Model\
          \ Class\r\nhuggingface_model = HuggingFaceModel(\r\n\timage_uri=get_huggingface_llm_image_uri(\"\
          huggingface\",version=\"0.8.2\"),\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n\
          \r\n# deploy model to SageMaker Inference\r\npredictor = huggingface_model.deploy(\r\
          \n\tinitial_instance_count=1,\r\n\tinstance_type=\"ml.g5.2xlarge\",\r\n\t\
          container_startup_health_check_timeout=300,\r\n    endpoint_name=\"NewHopeGPTQ\"\
          ,\r\n    model_name=\"NewHopeGPTQ\"\r\n  )\r\n  \r\n# send request\r\npredictor.predict({\r\
          \n\t\"inputs\": \"My name is Julien and I like to\",\r\n})\r\n```"
        updatedAt: '2023-08-07T16:30:38.784Z'
      numEdits: 0
      reactions: []
    id: 64d11c2e59503263d9197a9d
    type: comment
  author: zkrider
  content: "Deploying through AWS Sagemaker and running into these errors\r\n\r\n\
    Error #1 \r\n```\r\n#033[2m2023-08-07T15:49:27.786695Z#033[0m #033[31mERROR#033[0m\
    \ #033[1mshard-manager#033[0m: #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m\
    \ Error when initializing model\r\n```\r\nError #2\r\n```\r\nTraceback (most recent\
    \ call last):\r\n  File \"/opt/conda/bin/text-generation-server\", line 8, in\
    \ <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\n \
    \ File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1130, in\
    \ __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 778, in main\r\n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1404,\
    \ in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\r\n\
    \    return __callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 67, in serve\r\n    server.serve(model_id, revision, sharded, quantize,\
    \ trust_remote_code, uds_path)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 155, in serve\r\n    asyncio.run(serve_inner(model_id, revision, sharded,\
    \ quantize, trust_remote_code))\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 634, in run_until_complete\r\n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n```\r\
    \nError #3\r\n```\r\n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 124, in serve_inner\r\n    model = get_model(model_id, revision, sharded,\
    \ quantize, trust_remote_code)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 246, in get_model\r\n    return llama_cls(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
    , line 58, in __init__\r\n    filenames = weight_files(model_id, revision, \"\
    .bin\")\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/hub.py\"\
    , line 95, in weight_files\r\n    raise e\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/hub.py\"\
    , line 92, in weight_files\r\n    filenames = weight_hub_files(model_id, revision,\
    \ extension)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/hub.py\"\
    , line 33, in weight_hub_files\r\n    raise EntryNotFoundError(\r\n```\r\nError\
    \ #4\r\n```\r\nhuggingface_hub.utils._errors.EntryNotFoundError: No .bin weights\
    \ found for model TheBloke/NewHope-GPTQ and revision None.\r\n #033[2m#033[3mrank#033[0m#033[2m=#033[0m0#033[0m\r\
    \n```\r\n\r\nSagemaker notebook\r\n```\r\nimport json\r\nimport sagemaker\r\n\
    import boto3\r\nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\r\
    \n\r\ntry:\r\n\trole = sagemaker.get_execution_role()\r\nexcept ValueError:\r\n\
    \tiam = boto3.client('iam')\r\n\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\r\
    \n\r\n# Hub Model configuration. https://huggingface.co/models\r\nhub = {\r\n\t\
    'HF_MODEL_ID':'TheBloke/NewHope-GPTQ',\r\n\t'SM_NUM_GPUS': json.dumps(1),\r\n\
    \    'HF_API_TOKEN': '<TOKEN>'\r\n}\r\n\r\n# create Hugging Face Model Class\r\
    \nhuggingface_model = HuggingFaceModel(\r\n\timage_uri=get_huggingface_llm_image_uri(\"\
    huggingface\",version=\"0.8.2\"),\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n\r\n\
    # deploy model to SageMaker Inference\r\npredictor = huggingface_model.deploy(\r\
    \n\tinitial_instance_count=1,\r\n\tinstance_type=\"ml.g5.2xlarge\",\r\n\tcontainer_startup_health_check_timeout=300,\r\
    \n    endpoint_name=\"NewHopeGPTQ\",\r\n    model_name=\"NewHopeGPTQ\"\r\n  )\r\
    \n  \r\n# send request\r\npredictor.predict({\r\n\t\"inputs\": \"My name is Julien\
    \ and I like to\",\r\n})\r\n```"
  created_at: 2023-08-07 15:30:38+00:00
  edited: false
  hidden: false
  id: 64d11c2e59503263d9197a9d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/NewHope-GPTQ
repo_type: model
status: open
target_branch: null
title: No .bin weights found
