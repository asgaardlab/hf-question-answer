!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mikehemberger
conflicting_files: null
created_at: 2023-12-02 22:28:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/vBvbquhF0Hc7MVhoh7fzl.jpeg?w=200&h=200&f=face
      fullname: Mike Hemberger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikehemberger
      type: user
    createdAt: '2023-12-02T22:28:17.000Z'
    data:
      edited: false
      editors:
      - mikehemberger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8418294787406921
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/vBvbquhF0Hc7MVhoh7fzl.jpeg?w=200&h=200&f=face
          fullname: Mike Hemberger
          isHf: false
          isPro: false
          name: mikehemberger
          type: user
        html: "<p>Hi there,<br>Love your work \U0001F44D\U0001F3FB<br>Are there any\
          \ plans to integrate timm more into the hf framework and from_pretrained()?<br>I\u2019\
          m especially interested in those fine tuned models on the iNaturalist dataset.<br>All\
          \ the best,<br>Mike </p>\n"
        raw: "Hi there,\r\nLove your work \U0001F44D\U0001F3FB\r\nAre there any plans\
          \ to integrate timm more into the hf framework and from_pretrained()?\r\n\
          I\u2019m especially interested in those fine tuned models on the iNaturalist\
          \ dataset. \r\nAll the best,\r\nMike "
        updatedAt: '2023-12-02T22:28:17.313Z'
      numEdits: 0
      reactions: []
    id: 656baf810bbc114fe6f6fa99
    type: comment
  author: mikehemberger
  content: "Hi there,\r\nLove your work \U0001F44D\U0001F3FB\r\nAre there any plans\
    \ to integrate timm more into the hf framework and from_pretrained()?\r\nI\u2019\
    m especially interested in those fine tuned models on the iNaturalist dataset.\
    \ \r\nAll the best,\r\nMike "
  created_at: 2023-12-02 22:28:17+00:00
  edited: false
  hidden: false
  id: 656baf810bbc114fe6f6fa99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
      fullname: Ross Wightman
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rwightman
      type: user
    createdAt: '2023-12-04T18:31:50.000Z'
    data:
      edited: false
      editors:
      - rwightman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9363742470741272
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
          fullname: Ross Wightman
          isHf: true
          isPro: false
          name: rwightman
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mikehemberger&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mikehemberger\"\
          >@<span class=\"underline\">mikehemberger</span></a></span>\n\n\t</span></span>\
          \ I'd say 'transformers framework' as <code>timm</code> and any other library\
          \ that integrates with the hub is integrated with the broader HF ecosystem\
          \ but in their own ways. There are timm equivalents for <code>from_pretrained()</code>,\
          \ etc but understand many users would prefer a consistent interface.</p>\n\
          <p>This has been discussed in the past, it would be possible to wrap timm\
          \ models. Probably 90% coverage of functionality with transformers only\
          \ wrapper and timm as is, and I could prioritize adding some functionality\
          \ on the timm side to close the gap and make timm wrapped models in transformers\
          \ almost indistinguishable from native transformers vision models. However,\
          \ this was shot down in the past so it'd be a matter of convincing transformers\
          \ maintainers. <span data-props=\"{&quot;user&quot;:&quot;lysandre&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lysandre\"\
          >@<span class=\"underline\">lysandre</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;ArthurZ&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ArthurZ\">@<span class=\"\
          underline\">ArthurZ</span></a></span>\n\n\t</span></span> </p>\n"
        raw: '@mikehemberger I''d say ''transformers framework'' as `timm` and any
          other library that integrates with the hub is integrated with the broader
          HF ecosystem but in their own ways. There are timm equivalents for `from_pretrained()`,
          etc but understand many users would prefer a consistent interface.


          This has been discussed in the past, it would be possible to wrap timm models.
          Probably 90% coverage of functionality with transformers only wrapper and
          timm as is, and I could prioritize adding some functionality on the timm
          side to close the gap and make timm wrapped models in transformers almost
          indistinguishable from native transformers vision models. However, this
          was shot down in the past so it''d be a matter of convincing transformers
          maintainers. @lysandre @ArthurZ '
        updatedAt: '2023-12-04T18:31:50.168Z'
      numEdits: 0
      reactions: []
    id: 656e1b16edd446c42baaaf62
    type: comment
  author: rwightman
  content: '@mikehemberger I''d say ''transformers framework'' as `timm` and any other
    library that integrates with the hub is integrated with the broader HF ecosystem
    but in their own ways. There are timm equivalents for `from_pretrained()`, etc
    but understand many users would prefer a consistent interface.


    This has been discussed in the past, it would be possible to wrap timm models.
    Probably 90% coverage of functionality with transformers only wrapper and timm
    as is, and I could prioritize adding some functionality on the timm side to close
    the gap and make timm wrapped models in transformers almost indistinguishable
    from native transformers vision models. However, this was shot down in the past
    so it''d be a matter of convincing transformers maintainers. @lysandre @ArthurZ '
  created_at: 2023-12-04 18:31:50+00:00
  edited: false
  hidden: false
  id: 656e1b16edd446c42baaaf62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/vBvbquhF0Hc7MVhoh7fzl.jpeg?w=200&h=200&f=face
      fullname: Mike Hemberger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikehemberger
      type: user
    createdAt: '2023-12-05T15:33:47.000Z'
    data:
      edited: false
      editors:
      - mikehemberger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8498218655586243
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/vBvbquhF0Hc7MVhoh7fzl.jpeg?w=200&h=200&f=face
          fullname: Mike Hemberger
          isHf: false
          isPro: false
          name: mikehemberger
          type: user
        html: "<p>Thank you for clarifying this <span data-props=\"{&quot;user&quot;:&quot;rwightman&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rwightman\"\
          >@<span class=\"underline\">rwightman</span></a></span>\n\n\t</span></span>.<br>Indeed,\
          \ I do appreciate the interface similarities and will continue to build\
          \ with those. Code turns out to be nagging me somehow, using hf-datasets,\
          \ PyTorch for dataloading (ok, off-topic) and timm for feature/embedding\
          \ extraction besides the AutoModel option.<br>While I expected that this\
          \ is not a priority I\u2019m a bit puzzled about the lack of a long-term\
          \ \u201Esolution\u201C in the making.<br>Maybe <span data-props=\"{&quot;user&quot;:&quot;ArthurZ&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ArthurZ\"\
          >@<span class=\"underline\">ArthurZ</span></a></span>\n\n\t</span></span>\
          \  or <span data-props=\"{&quot;user&quot;:&quot;lysandre&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lysandre\">@<span class=\"\
          underline\">lysandre</span></a></span>\n\n\t</span></span> can share their\
          \ thoughts or link to some previous discussion?<br>All the best,<br>M</p>\n"
        raw: "Thank you for clarifying this @rwightman.\nIndeed, I do appreciate the\
          \ interface similarities and will continue to build with those. Code turns\
          \ out to be nagging me somehow, using hf-datasets, PyTorch for dataloading\
          \ (ok, off-topic) and timm for feature/embedding extraction besides the\
          \ AutoModel option.\nWhile I expected that this is not a priority I\u2019\
          m a bit puzzled about the lack of a long-term \u201Esolution\u201C in the\
          \ making. \nMaybe @ArthurZ  or @lysandre can share their thoughts or link\
          \ to some previous discussion? \nAll the best,\nM"
        updatedAt: '2023-12-05T15:33:47.839Z'
      numEdits: 0
      reactions: []
    id: 656f42db34440bdf2ebaa2dd
    type: comment
  author: mikehemberger
  content: "Thank you for clarifying this @rwightman.\nIndeed, I do appreciate the\
    \ interface similarities and will continue to build with those. Code turns out\
    \ to be nagging me somehow, using hf-datasets, PyTorch for dataloading (ok, off-topic)\
    \ and timm for feature/embedding extraction besides the AutoModel option.\nWhile\
    \ I expected that this is not a priority I\u2019m a bit puzzled about the lack\
    \ of a long-term \u201Esolution\u201C in the making. \nMaybe @ArthurZ  or @lysandre\
    \ can share their thoughts or link to some previous discussion? \nAll the best,\n\
    M"
  created_at: 2023-12-05 15:33:47+00:00
  edited: false
  hidden: false
  id: 656f42db34440bdf2ebaa2dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/vBvbquhF0Hc7MVhoh7fzl.jpeg?w=200&h=200&f=face
      fullname: Mike Hemberger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikehemberger
      type: user
    createdAt: '2023-12-05T16:08:42.000Z'
    data:
      edited: false
      editors:
      - mikehemberger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40755608677864075
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/vBvbquhF0Hc7MVhoh7fzl.jpeg?w=200&h=200&f=face
          fullname: Mike Hemberger
          isHf: false
          isPro: false
          name: mikehemberger
          type: user
        html: "<p>Replacing the colon (\u201E:\u201C) in the string here would move\
          \ in the right direction :) :<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/sLUt-FChEGlQ4r1_nT6HT.jpeg\"\
          ><img alt=\"IMG_4979.jpeg\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/sLUt-FChEGlQ4r1_nT6HT.jpeg\"\
          ></a></p>\n"
        raw: "Replacing the colon (\u201E:\u201C) in the string here would move in\
          \ the right direction :) :\n![IMG_4979.jpeg](https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/sLUt-FChEGlQ4r1_nT6HT.jpeg)\n"
        updatedAt: '2023-12-05T16:08:42.371Z'
      numEdits: 0
      reactions: []
    id: 656f4b0a24baa8755033a4cb
    type: comment
  author: mikehemberger
  content: "Replacing the colon (\u201E:\u201C) in the string here would move in the\
    \ right direction :) :\n![IMG_4979.jpeg](https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/sLUt-FChEGlQ4r1_nT6HT.jpeg)\n"
  created_at: 2023-12-05 16:08:42+00:00
  edited: false
  hidden: false
  id: 656f4b0a24baa8755033a4cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
      fullname: Ross Wightman
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rwightman
      type: user
    createdAt: '2023-12-05T17:45:54.000Z'
    data:
      edited: true
      editors:
      - rwightman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8465061783790588
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
          fullname: Ross Wightman
          isHf: true
          isPro: false
          name: rwightman
          type: user
        html: '<p>If it''s a help, I could plan to add functionality timm side to
          make it a bit more transformers like...</p>

          <ol>

          <li>make a timm.AutoModel / AutoModelForImageClassification that''d wrap
          timm create_model and append the <code>hf_hub:</code> scheme (it''s modeled
          after URI ) so it''d make timm.AutoModel.from_pretrained(''timm/eva02...'')
          possible. The builtin model configs in timm are code based, not hub based,
          so for bwd compat the hf_hub: scheme was added to specify to source from
          hub. Aside from AutoModel, another method could also be added to assume
          hub based, like <code>timm.from_pretrained()</code></li>

          <li>make a AutoImageProcessor that wraps the appropriate transforms</li>

          <li>make a Pipeline equivalent for feat extraction and image classification</li>

          </ol>

          <p>I feel doing it transformers side so that timm is wrapped makes for a
          more seemless experience though. The above would also not address any of
          the model API differences which I feel are more significant, like <code>output_hidden_states</code>,
          etc. That would require wrapping the model and the mentioned adding support
          in timm.</p>

          '
        raw: 'If it''s a help, I could plan to add functionality timm side to make
          it a bit more transformers like...

          1. make a timm.AutoModel / AutoModelForImageClassification that''d wrap
          timm create_model and append the `hf_hub:` scheme (it''s modeled after URI
          ) so it''d make timm.AutoModel.from_pretrained(''timm/eva02...'') possible.
          The builtin model configs in timm are code based, not hub based, so for
          bwd compat the hf_hub: scheme was added to specify to source from hub. Aside
          from AutoModel, another method could also be added to assume hub based,
          like `timm.from_pretrained()`

          2. make a AutoImageProcessor that wraps the appropriate transforms

          3. make a Pipeline equivalent for feat extraction and image classification


          I feel doing it transformers side so that timm is wrapped makes for a more
          seemless experience though. The above would also not address any of the
          model API differences which I feel are more significant, like `output_hidden_states`,
          etc. That would require wrapping the model and the mentioned adding support
          in timm.'
        updatedAt: '2023-12-05T17:48:38.525Z'
      numEdits: 2
      reactions: []
    id: 656f61d26066ea8e25c3b340
    type: comment
  author: rwightman
  content: 'If it''s a help, I could plan to add functionality timm side to make it
    a bit more transformers like...

    1. make a timm.AutoModel / AutoModelForImageClassification that''d wrap timm create_model
    and append the `hf_hub:` scheme (it''s modeled after URI ) so it''d make timm.AutoModel.from_pretrained(''timm/eva02...'')
    possible. The builtin model configs in timm are code based, not hub based, so
    for bwd compat the hf_hub: scheme was added to specify to source from hub. Aside
    from AutoModel, another method could also be added to assume hub based, like `timm.from_pretrained()`

    2. make a AutoImageProcessor that wraps the appropriate transforms

    3. make a Pipeline equivalent for feat extraction and image classification


    I feel doing it transformers side so that timm is wrapped makes for a more seemless
    experience though. The above would also not address any of the model API differences
    which I feel are more significant, like `output_hidden_states`, etc. That would
    require wrapping the model and the mentioned adding support in timm.'
  created_at: 2023-12-05 17:45:54+00:00
  edited: true
  hidden: false
  id: 656f61d26066ea8e25c3b340
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/vBvbquhF0Hc7MVhoh7fzl.jpeg?w=200&h=200&f=face
      fullname: Mike Hemberger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikehemberger
      type: user
    createdAt: '2023-12-06T11:25:02.000Z'
    data:
      edited: false
      editors:
      - mikehemberger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9255445003509521
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/vBvbquhF0Hc7MVhoh7fzl.jpeg?w=200&h=200&f=face
          fullname: Mike Hemberger
          isHf: false
          isPro: false
          name: mikehemberger
          type: user
        html: "<p>Thanks for giving me a better idea of the actual complexity of such\
          \ a task.<br>I think point 1 is a bit too advanced for me. But I like idea\
          \ number 2 and can see how this would smooth over some of the edges mentioned.\
          \ Point 3 sounds great, too!</p>\n<p>I didnt think about the output_hidden_states\
          \ yet. You\u2019re right there, too. Would it help if I bring this up on\
          \ timm/transformers github? Maybe constrain the request towards the transformers\
          \ architectures? Let me know <span data-props=\"{&quot;user&quot;:&quot;rwightman&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rwightman\"\
          >@<span class=\"underline\">rwightman</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: "Thanks for giving me a better idea of the actual complexity of such\
          \ a task. \nI think point 1 is a bit too advanced for me. But I like idea\
          \ number 2 and can see how this would smooth over some of the edges mentioned.\
          \ Point 3 sounds great, too!\n\nI didnt think about the output_hidden_states\
          \ yet. You\u2019re right there, too. Would it help if I bring this up on\
          \ timm/transformers github? Maybe constrain the request towards the transformers\
          \ architectures? Let me know @rwightman "
        updatedAt: '2023-12-06T11:25:02.454Z'
      numEdits: 0
      reactions: []
    id: 65705a0eba19285f32b32278
    type: comment
  author: mikehemberger
  content: "Thanks for giving me a better idea of the actual complexity of such a\
    \ task. \nI think point 1 is a bit too advanced for me. But I like idea number\
    \ 2 and can see how this would smooth over some of the edges mentioned. Point\
    \ 3 sounds great, too!\n\nI didnt think about the output_hidden_states yet. You\u2019\
    re right there, too. Would it help if I bring this up on timm/transformers github?\
    \ Maybe constrain the request towards the transformers architectures? Let me know\
    \ @rwightman "
  created_at: 2023-12-06 11:25:02+00:00
  edited: false
  hidden: false
  id: 65705a0eba19285f32b32278
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
      fullname: Ross Wightman
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rwightman
      type: user
    createdAt: '2023-12-06T16:42:34.000Z'
    data:
      edited: false
      editors:
      - rwightman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9278415441513062
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
          fullname: Ross Wightman
          isHf: true
          isPro: false
          name: rwightman
          type: user
        html: '<p>2 would build on 1, 3 on both, etc. I think for now I could make
          a timm.from_pretrained() that mirrors AutoModel.from_pretrained() in behaviour.
          It''s still fitting the timm design but making it a bit more ''Hub first''
          for those who don''t care about builtin/offline configs (pretrained=False)
          </p>

          <p>Mirroring Auto* interfaces in timm would be a bigger change, and I feel
          it makes more sense to do it transformers side by wrapping timm for a tighter
          integration, but this needs buy in from transformers team.</p>

          '
        raw: "2 would build on 1, 3 on both, etc. I think for now I could make a timm.from_pretrained()\
          \ that mirrors AutoModel.from_pretrained() in behaviour. It's still fitting\
          \ the timm design but making it a bit more 'Hub first' for those who don't\
          \ care about builtin/offline configs (pretrained=False) \n\nMirroring Auto*\
          \ interfaces in timm would be a bigger change, and I feel it makes more\
          \ sense to do it transformers side by wrapping timm for a tighter integration,\
          \ but this needs buy in from transformers team."
        updatedAt: '2023-12-06T16:42:34.148Z'
      numEdits: 0
      reactions: []
    id: 6570a47a4f98a7a5a3b8dd48
    type: comment
  author: rwightman
  content: "2 would build on 1, 3 on both, etc. I think for now I could make a timm.from_pretrained()\
    \ that mirrors AutoModel.from_pretrained() in behaviour. It's still fitting the\
    \ timm design but making it a bit more 'Hub first' for those who don't care about\
    \ builtin/offline configs (pretrained=False) \n\nMirroring Auto* interfaces in\
    \ timm would be a bigger change, and I feel it makes more sense to do it transformers\
    \ side by wrapping timm for a tighter integration, but this needs buy in from\
    \ transformers team."
  created_at: 2023-12-06 16:42:34+00:00
  edited: false
  hidden: false
  id: 6570a47a4f98a7a5a3b8dd48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/vBvbquhF0Hc7MVhoh7fzl.jpeg?w=200&h=200&f=face
      fullname: Mike Hemberger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikehemberger
      type: user
    createdAt: '2023-12-06T21:11:32.000Z'
    data:
      edited: false
      editors:
      - mikehemberger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9413026571273804
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/vBvbquhF0Hc7MVhoh7fzl.jpeg?w=200&h=200&f=face
          fullname: Mike Hemberger
          isHf: false
          isPro: false
          name: mikehemberger
          type: user
        html: "<p>Is there an example of a wrapper for another library within transformers?\
          \ I\u2019d be happy to check it out and - if I can - draw some insights\
          \ into how this could be achieved?<br>In the meantime I think I can draw\
          \ from my code to provide an illustration of those nagging interface differences</p>\n"
        raw: "Is there an example of a wrapper for another library within transformers?\
          \ I\u2019d be happy to check it out and - if I can - draw some insights\
          \ into how this could be achieved?\nIn the meantime I think I can draw from\
          \ my code to provide an illustration of those nagging interface differences"
        updatedAt: '2023-12-06T21:11:32.652Z'
      numEdits: 0
      reactions: []
    id: 6570e3849435343c81e28aac
    type: comment
  author: mikehemberger
  content: "Is there an example of a wrapper for another library within transformers?\
    \ I\u2019d be happy to check it out and - if I can - draw some insights into how\
    \ this could be achieved?\nIn the meantime I think I can draw from my code to\
    \ provide an illustration of those nagging interface differences"
  created_at: 2023-12-06 21:11:32+00:00
  edited: false
  hidden: false
  id: 6570e3849435343c81e28aac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
      fullname: Ross Wightman
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rwightman
      type: user
    createdAt: '2023-12-06T21:28:08.000Z'
    data:
      edited: false
      editors:
      - rwightman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8429907560348511
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
          fullname: Ross Wightman
          isHf: true
          isPro: false
          name: rwightman
          type: user
        html: '<p>Someone had a sketch proposal here <a rel="nofollow" href="https://github.com/huggingface/transformers/issues/25282#issuecomment-1664960742">https://github.com/huggingface/transformers/issues/25282#issuecomment-1664960742</a>  and
          looks like it got fleshed out into a full impl in their own codebase here
          <a rel="nofollow" href="https://github.com/huggingface/optimum-intel/blob/main/optimum/intel/openvino/modeling_timm.py">https://github.com/huggingface/optimum-intel/blob/main/optimum/intel/openvino/modeling_timm.py</a></p>

          '
        raw: Someone had a sketch proposal here https://github.com/huggingface/transformers/issues/25282#issuecomment-1664960742  and
          looks like it got fleshed out into a full impl in their own codebase here
          https://github.com/huggingface/optimum-intel/blob/main/optimum/intel/openvino/modeling_timm.py
        updatedAt: '2023-12-06T21:28:08.273Z'
      numEdits: 0
      reactions: []
    id: 6570e7680ea91e592a107888
    type: comment
  author: rwightman
  content: Someone had a sketch proposal here https://github.com/huggingface/transformers/issues/25282#issuecomment-1664960742  and
    looks like it got fleshed out into a full impl in their own codebase here https://github.com/huggingface/optimum-intel/blob/main/optimum/intel/openvino/modeling_timm.py
  created_at: 2023-12-06 21:28:08+00:00
  edited: false
  hidden: false
  id: 6570e7680ea91e592a107888
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/vBvbquhF0Hc7MVhoh7fzl.jpeg?w=200&h=200&f=face
      fullname: Mike Hemberger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikehemberger
      type: user
    createdAt: '2023-12-06T22:32:23.000Z'
    data:
      edited: false
      editors:
      - mikehemberger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.922588050365448
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6265285e5f6f2e14d6ad978c/vBvbquhF0Hc7MVhoh7fzl.jpeg?w=200&h=200&f=face
          fullname: Mike Hemberger
          isHf: false
          isPro: false
          name: mikehemberger
          type: user
        html: '<p>Thanks! I will link that up :)</p>

          '
        raw: Thanks! I will link that up :)
        updatedAt: '2023-12-06T22:32:23.457Z'
      numEdits: 0
      reactions: []
    id: 6570f6770ea91e592a1329c9
    type: comment
  author: mikehemberger
  content: Thanks! I will link that up :)
  created_at: 2023-12-06 22:32:23+00:00
  edited: false
  hidden: false
  id: 6570f6770ea91e592a1329c9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: timm/eva02_large_patch14_clip_336.merged2b_ft_inat21
repo_type: model
status: open
target_branch: null
title: Timm Models as AutoModel
