!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ybelkada
conflicting_files: null
created_at: 2023-06-22 13:41:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-06-22T14:41:27.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4545559883117676
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>As the model seems to support accelerate loading, you can benefit
          from 8bit / 4bit inference out of the box by  first installing bitsandbytes
          <code>pip install --upgrade bitsandbytes</code> and run:</p>

          <p>For 8bit:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM,
          AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"baichuan-inc/baichuan-7B"</span>,
          trust_remote_code=<span class="hljs-literal">True</span>)

          model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">"baichuan-inc/baichuan-7B"</span>,
          load_in_8bit=<span class="hljs-literal">True</span>, trust_remote_code=<span
          class="hljs-literal">True</span>)

          inputs = tokenizer(<span class="hljs-string">''Hamlet-&gt;Shakespeare\nOne
          Hundred Years of Solitude-&gt;''</span>, return_tensors=<span class="hljs-string">''pt''</span>)

          inputs = inputs.to(<span class="hljs-string">''cuda:0''</span>)

          pred = model.generate(**inputs, max_new_tokens=<span class="hljs-number">64</span>,repetition_penalty=<span
          class="hljs-number">1.1</span>)

          <span class="hljs-built_in">print</span>(tokenizer.decode(pred.cpu()[<span
          class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))

          </code></pre>

          <p>For 4bit:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM,
          AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"baichuan-inc/baichuan-7B"</span>,
          trust_remote_code=<span class="hljs-literal">True</span>)

          model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">"baichuan-inc/baichuan-7B"</span>,  load_in_4bit=<span
          class="hljs-literal">True</span>, trust_remote_code=<span class="hljs-literal">True</span>)

          inputs = tokenizer(<span class="hljs-string">''Hamlet-&gt;Shakespeare\nOne
          Hundred Years of Solitude-&gt;''</span>, return_tensors=<span class="hljs-string">''pt''</span>)

          inputs = inputs.to(<span class="hljs-string">''cuda:0''</span>)

          pred = model.generate(**inputs, max_new_tokens=<span class="hljs-number">64</span>,repetition_penalty=<span
          class="hljs-number">1.1</span>)

          <span class="hljs-built_in">print</span>(tokenizer.decode(pred.cpu()[<span
          class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))

          </code></pre>

          <p>Making it possible to run the model through Google Colab for example</p>

          '
        raw: 'As the model seems to support accelerate loading, you can benefit from
          8bit / 4bit inference out of the box by  first installing bitsandbytes `pip
          install --upgrade bitsandbytes` and run:


          For 8bit:

          ```python

          from transformers import AutoModelForCausalLM, AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained("baichuan-inc/baichuan-7B", trust_remote_code=True)

          model = AutoModelForCausalLM.from_pretrained("baichuan-inc/baichuan-7B",
          load_in_8bit=True, trust_remote_code=True)

          inputs = tokenizer(''Hamlet->Shakespeare\nOne Hundred Years of Solitude->'',
          return_tensors=''pt'')

          inputs = inputs.to(''cuda:0'')

          pred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)

          print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))

          ```


          For 4bit:

          ```python

          from transformers import AutoModelForCausalLM, AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained("baichuan-inc/baichuan-7B", trust_remote_code=True)

          model = AutoModelForCausalLM.from_pretrained("baichuan-inc/baichuan-7B",  load_in_4bit=True,
          trust_remote_code=True)

          inputs = tokenizer(''Hamlet->Shakespeare\nOne Hundred Years of Solitude->'',
          return_tensors=''pt'')

          inputs = inputs.to(''cuda:0'')

          pred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)

          print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))

          ```


          Making it possible to run the model through Google Colab for example'
        updatedAt: '2023-06-22T14:41:46.118Z'
      numEdits: 1
      reactions: []
    id: 64945d9799bd5eada07e8d9d
    type: comment
  author: ybelkada
  content: 'As the model seems to support accelerate loading, you can benefit from
    8bit / 4bit inference out of the box by  first installing bitsandbytes `pip install
    --upgrade bitsandbytes` and run:


    For 8bit:

    ```python

    from transformers import AutoModelForCausalLM, AutoTokenizer


    tokenizer = AutoTokenizer.from_pretrained("baichuan-inc/baichuan-7B", trust_remote_code=True)

    model = AutoModelForCausalLM.from_pretrained("baichuan-inc/baichuan-7B", load_in_8bit=True,
    trust_remote_code=True)

    inputs = tokenizer(''Hamlet->Shakespeare\nOne Hundred Years of Solitude->'', return_tensors=''pt'')

    inputs = inputs.to(''cuda:0'')

    pred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)

    print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))

    ```


    For 4bit:

    ```python

    from transformers import AutoModelForCausalLM, AutoTokenizer


    tokenizer = AutoTokenizer.from_pretrained("baichuan-inc/baichuan-7B", trust_remote_code=True)

    model = AutoModelForCausalLM.from_pretrained("baichuan-inc/baichuan-7B",  load_in_4bit=True,
    trust_remote_code=True)

    inputs = tokenizer(''Hamlet->Shakespeare\nOne Hundred Years of Solitude->'', return_tensors=''pt'')

    inputs = inputs.to(''cuda:0'')

    pred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)

    print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))

    ```


    Making it possible to run the model through Google Colab for example'
  created_at: 2023-06-22 13:41:27+00:00
  edited: true
  hidden: false
  id: 64945d9799bd5eada07e8d9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f97419728214a3bd0d868ffef01de1cf.svg
      fullname: Reed Hs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: reedhs
      type: user
    createdAt: '2023-06-24T03:55:19.000Z'
    data:
      edited: true
      editors:
      - reedhs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.91476970911026
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f97419728214a3bd0d868ffef01de1cf.svg
          fullname: Reed Hs
          isHf: false
          isPro: false
          name: reedhs
          type: user
        html: '<p>Cannot get it work on Colab free version with the code above (8bit),
          system ran out of memory (12.7GB in total, OOM even with low_cpu_mem_usage
          =True), can you please look into it? Thanks!</p>

          '
        raw: Cannot get it work on Colab free version with the code above (8bit),
          system ran out of memory (12.7GB in total, OOM even with low_cpu_mem_usage
          =True), can you please look into it? Thanks!
        updatedAt: '2023-06-24T03:55:56.917Z'
      numEdits: 2
      reactions: []
    id: 64966927ce9ee60f635c78e5
    type: comment
  author: reedhs
  content: Cannot get it work on Colab free version with the code above (8bit), system
    ran out of memory (12.7GB in total, OOM even with low_cpu_mem_usage =True), can
    you please look into it? Thanks!
  created_at: 2023-06-24 02:55:19+00:00
  edited: true
  hidden: false
  id: 64966927ce9ee60f635c78e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-06-24T10:36:47.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5283635258674622
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>I believe you need to push the sharded checkpoints somewhere on
          the hub beforehand otherwise the colab will crash</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          torch

          <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span>
          AutoModelForCausalLM, AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"baichuan-inc/baichuan-7B"</span>,
          trust_remote_code=<span class="hljs-literal">True</span>)

          model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">"baichuan-inc/baichuan-7B"</span>,
          trust_remote_code=<span class="hljs-literal">True</span>, torch_dtype=torch.float16)

          model.push_to_hub(<span class="hljs-string">"baichuan-7b-sharded"</span>,
          max_shard_size=<span class="hljs-string">"2GB"</span>)

          </code></pre>

          <p>Then use the sharded checkpoints on the Colab</p>

          '
        raw: 'I believe you need to push the sharded checkpoints somewhere on the
          hub beforehand otherwise the colab will crash

          ```python

          import torch

          from transformers import AutoModelForCausalLM, AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained("baichuan-inc/baichuan-7B", trust_remote_code=True)

          model = AutoModelForCausalLM.from_pretrained("baichuan-inc/baichuan-7B",
          trust_remote_code=True, torch_dtype=torch.float16)

          model.push_to_hub("baichuan-7b-sharded", max_shard_size="2GB")

          ```


          Then use the sharded checkpoints on the Colab'
        updatedAt: '2023-06-24T10:37:06.761Z'
      numEdits: 1
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - reedhs
        - PenguinBIUBIUBIU
        - anakin87
        - Ali-C137
        - lewtun
      - count: 1
        reaction: "\U0001F91D"
        users:
        - PenguinBIUBIUBIU
    id: 6496c73fffedd4836d67305f
    type: comment
  author: ybelkada
  content: 'I believe you need to push the sharded checkpoints somewhere on the hub
    beforehand otherwise the colab will crash

    ```python

    import torch

    from transformers import AutoModelForCausalLM, AutoTokenizer


    tokenizer = AutoTokenizer.from_pretrained("baichuan-inc/baichuan-7B", trust_remote_code=True)

    model = AutoModelForCausalLM.from_pretrained("baichuan-inc/baichuan-7B", trust_remote_code=True,
    torch_dtype=torch.float16)

    model.push_to_hub("baichuan-7b-sharded", max_shard_size="2GB")

    ```


    Then use the sharded checkpoints on the Colab'
  created_at: 2023-06-24 09:36:47+00:00
  edited: true
  hidden: false
  id: 6496c73fffedd4836d67305f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: baichuan-inc/Baichuan-7B
repo_type: model
status: open
target_branch: null
title: Running the model in 8bit/4bit
