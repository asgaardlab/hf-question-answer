!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Hugging-ass
conflicting_files: null
created_at: 2023-06-20 05:54:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0caf49f7ea7a4ed3df47f5f04cddbb96.svg
      fullname: "\u5415\u822A"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hugging-ass
      type: user
    createdAt: '2023-06-20T06:54:33.000Z'
    data:
      edited: false
      editors:
      - Hugging-ass
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6006576418876648
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0caf49f7ea7a4ed3df47f5f04cddbb96.svg
          fullname: "\u5415\u822A"
          isHf: false
          isPro: false
          name: Hugging-ass
          type: user
        html: '<p>Here is my code:<br>from transformers import AutoModelForCausalLM,
          AutoTokenizer</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("/mnt/e/llm/baichuan", trust_remote_code=True)<br>model
          = AutoModelForCausalLM.from_pretrained("/mnt/e/llm/baichuan", device_map="auto",
          trust_remote_code=True)<br>model.tie_weights()<br>inputs = tokenizer(''Hamlet-&gt;Shakespeare\nOne
          Hundred Years of Solitude-&gt;'', return_tensors=''pt'')<br>#inputs = inputs.to(''cuda:0'')<br>pred
          = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)<br>print(tokenizer.decode(pred.cpu()[0],
          skip_special_tokens=True))</p>

          <p>error is:<br>The model weights are not tied. Please use the <code>tie_weights</code>
          method before using the <code>infer_auto_device</code> function.<br>ValueError:
          The current <code>device_map</code> had weights offloaded to the disk. Please
          provide an <code>offload_folder</code> for them. Alternatively, make sure
          you have <code>safetensors</code> installed if the<br>model you are using
          offers the weights in this format.</p>

          '
        raw: "Here is my code:\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
          \n\r\ntokenizer = AutoTokenizer.from_pretrained(\"/mnt/e/llm/baichuan\"\
          , trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          /mnt/e/llm/baichuan\", device_map=\"auto\", trust_remote_code=True)\r\n\
          model.tie_weights()\r\ninputs = tokenizer('Hamlet->Shakespeare\\nOne Hundred\
          \ Years of Solitude->', return_tensors='pt')\r\n#inputs = inputs.to('cuda:0')\r\
          \npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\r\
          \nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\r\n\r\
          \nerror is:\r\nThe model weights are not tied. Please use the `tie_weights`\
          \ method before using the `infer_auto_device` function.\r\nValueError: The\
          \ current `device_map` had weights offloaded to the disk. Please provide\
          \ an `offload_folder` for them. Alternatively, make sure you have `safetensors`\
          \ installed if the\r\nmodel you are using offers the weights in this format."
        updatedAt: '2023-06-20T06:54:33.003Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - littleevillin
    id: 64914d2971ead918eaacc89b
    type: comment
  author: Hugging-ass
  content: "Here is my code:\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
    \n\r\ntokenizer = AutoTokenizer.from_pretrained(\"/mnt/e/llm/baichuan\", trust_remote_code=True)\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(\"/mnt/e/llm/baichuan\", device_map=\"\
    auto\", trust_remote_code=True)\r\nmodel.tie_weights()\r\ninputs = tokenizer('Hamlet->Shakespeare\\\
    nOne Hundred Years of Solitude->', return_tensors='pt')\r\n#inputs = inputs.to('cuda:0')\r\
    \npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\r\n\
    print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\r\n\r\nerror\
    \ is:\r\nThe model weights are not tied. Please use the `tie_weights` method before\
    \ using the `infer_auto_device` function.\r\nValueError: The current `device_map`\
    \ had weights offloaded to the disk. Please provide an `offload_folder` for them.\
    \ Alternatively, make sure you have `safetensors` installed if the\r\nmodel you\
    \ are using offers the weights in this format."
  created_at: 2023-06-20 05:54:33+00:00
  edited: false
  hidden: false
  id: 64914d2971ead918eaacc89b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/16d7fbdfdabac1f97188ce6032afd5e2.svg
      fullname: lin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littleevillin
      type: user
    createdAt: '2023-06-28T08:58:56.000Z'
    data:
      edited: false
      editors:
      - littleevillin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8548995852470398
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/16d7fbdfdabac1f97188ce6032afd5e2.svg
          fullname: lin
          isHf: false
          isPro: false
          name: littleevillin
          type: user
        html: "<p>mac m1 have the same problem\uFF0Cmaybe don't have cuda(NVIDIA GPUs)......</p>\n"
        raw: "mac m1 have the same problem\uFF0Cmaybe don't have cuda(NVIDIA GPUs)......"
        updatedAt: '2023-06-28T08:58:56.109Z'
      numEdits: 0
      reactions: []
    id: 649bf6502a0a2094367a86c1
    type: comment
  author: littleevillin
  content: "mac m1 have the same problem\uFF0Cmaybe don't have cuda(NVIDIA GPUs)......"
  created_at: 2023-06-28 07:58:56+00:00
  edited: false
  hidden: false
  id: 649bf6502a0a2094367a86c1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: baichuan-inc/Baichuan-7B
repo_type: model
status: open
target_branch: null
title: error when doing inference
