!!python/object:huggingface_hub.community.DiscussionWithDetails
author: stonem
conflicting_files: null
created_at: 2023-08-31 07:44:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eead920a8369c79aef31a07a80fc73b4.svg
      fullname: stonem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: stonem
      type: user
    createdAt: '2023-08-31T08:44:53.000Z'
    data:
      edited: false
      editors:
      - stonem
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2679169476032257
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eead920a8369c79aef31a07a80fc73b4.svg
          fullname: stonem
          isHf: false
          isPro: false
          name: stonem
          type: user
        html: "<p>2023-08-31 11:36,FullyShardedDataParallel(<br>2023-08-31 11:36,\
          \  (_fsdp_wrapped_module): BaichuanForCausalLM(<br>2023-08-31 11:36,   \
          \ (model): BaichuanModel(<br>2023-08-31 11:36,      (embed_tokens): Embedding(64000,\
          \ 5120, padding_idx=0)<br>2023-08-31 11:36,      (layers): ModuleList(<br>2023-08-31\
          \ 11:36,        (0-39): 40 x BaichuanLayer(<br>2023-08-31 11:36,       \
          \   (self_attn): BaichuanAttention(<br>2023-08-31 11:36,            (W_pack):\
          \ Linear(in_features=5120, out_features=15360, bias=False)<br>2023-08-31\
          \ 11:36,            (o_proj): Linear(in_features=5120, out_features=5120,\
          \ bias=False)<br>2023-08-31 11:36,          )<br>2023-08-31 11:36,     \
          \     (mlp): MLP(<br>2023-08-31 11:36,            (gate_proj): Linear(in_features=5120,\
          \ out_features=13696, bias=False)<br>2023-08-31 11:36,            (down_proj):\
          \ Linear(in_features=13696, out_features=5120, bias=False)<br>2023-08-31\
          \ 11:36,            (up_proj): Linear(in_features=5120, out_features=13696,\
          \ bias=False)<br>2023-08-31 11:36,            (act_fn): SiLUActivation()<br>2023-08-31\
          \ 11:36,          )<br>2023-08-31 11:36,          (input_layernorm): RMSNorm()<br>2023-08-31\
          \ 11:36,          (post_attention_layernorm): RMSNorm()<br>2023-08-31 11:36,\
          \        )<br>2023-08-31 11:36,      )<br>2023-08-31 11:36,      (norm):\
          \ RMSNorm()<br>2023-08-31 11:36,    )<br>2023-08-31 11:36,    (lm_head):\
          \ Linear(in_features=5120, out_features=64000, bias=False)<br>2023-08-31\
          \ 11:36,  )<br>2023-08-31 11:36,)</p>\n<p>\u7ECF\u8FC7FSDP\u7684\u6A21\u578B\
          \uFF0C\u5E94\u8BE5\u53D8\u6210<br>2023-08-31 14:11,FullyShardedDataParallel(<br>2023-08-31\
          \ 14:11,  (_fsdp_wrapped_module): LlamaForCausalLM(<br>2023-08-31 14:11,\
          \    (model): LlamaModel(<br>2023-08-31 14:11,      (embed_tokens): Embedding(55296,\
          \ 5120)<br>2023-08-31 14:11,      (layers): ModuleList(<br>2023-08-31 14:11,\
          \        (0-39): 40 x FullyShardedDataParallel(<br>2023-08-31 14:11,   \
          \       (_fsdp_wrapped_module): LlamaDecoderLayer(<br>2023-08-31 14:11,\
          \            (self_attn): LlamaAttention(<br>2023-08-31 14:11,         \
          \     (q_proj): Linear(in_features=5120, out_features=5120, bias=False)<br>2023-08-31\
          \ 14:11,              (k_proj): Linear(in_features=5120, out_features=5120,\
          \ bias=False)<br>2023-08-31 14:11,              (v_proj): Linear(in_features=5120,\
          \ out_features=5120, bias=False)<br>2023-08-31 14:11,              (o_proj):\
          \ Linear(in_features=5120, out_features=5120, bias=False)<br>2023-08-31\
          \ 14:11,              (rotary_emb): LlamaRotaryEmbedding()<br>2023-08-31\
          \ 14:11,            )<br>2023-08-31 14:11,            (mlp): LlamaMLP(<br>2023-08-31\
          \ 14:11,              (gate_proj): Linear(in_features=5120, out_features=13824,\
          \ bias=False)<br>2023-08-31 14:11,              (up_proj): Linear(in_features=5120,\
          \ out_features=13824, bias=False)<br>2023-08-31 14:11,              (down_proj):\
          \ Linear(in_features=13824, out_features=5120, bias=False)<br>2023-08-31\
          \ 14:11,              (act_fn): SiLUActivation()<br>2023-08-31 14:11,  \
          \          )<br>2023-08-31 14:11,            (input_layernorm): LlamaRMSNorm()<br>2023-08-31\
          \ 14:11,            (post_attention_layernorm): LlamaRMSNorm()<br>2023-08-31\
          \ 14:11,          )<br>2023-08-31 14:11,        )<br>2023-08-31 14:11, \
          \     )<br>2023-08-31 14:11,      (norm): LlamaRMSNorm()<br>2023-08-31 14:11,\
          \    )<br>2023-08-31 14:11,    (lm_head): Linear(in_features=5120, out_features=55296,\
          \ bias=False)<br>2023-08-31 14:11,  )<br>2023-08-31 14:11,)</p>\n<p>\u5BF9\
          \u6BCF\u4E00\u4E2Alayer\u90FD\u5E94\u8BE5\u6709\u4E00\u4E2A  (_fsdp_wrapped_module)<br>\u6211\
          \u7ED9\u767E\u5DDD\u6A21\u578B\u8BBE\u7F6E\u4E86wrap_policy<br> def get_baichun_wrapper():<br>\
          \    baichuan_auto_wrap_policy = functools.partial(<br>        transformer_auto_wrap_policy,<br>\
          \        recurse=True,<br>        transformer_layer_cls={<br>          \
          \  BaichuanLayer,<br>        }<br>    )<br>    return baichuan_auto_wrap_policy</p>\n\
          <p>\u770B\u4E86modeling_baichuan.py\u8FD9\u4E2A\u6587\u4EF6\uFF0C\u6574\u4F53\
          \u5B9E\u73B0\u4E0A\u662F\u7EE7\u627Fnn.Moudle\uFF0Cllama\u6A21\u578B\u4E5F\
          \u662F\u4E00\u6837\u7684\u3002\u4E0D\u77E5\u9053\u54EA\u91CC\u51FA\u4E86\
          \u95EE\u9898\uFF0C\u4E0D\u80FD\u8FDB\u884Cfsdp\u5305\u88C5</p>\n"
        raw: "2023-08-31 11:36,FullyShardedDataParallel(\r\n2023-08-31 11:36,  (_fsdp_wrapped_module):\
          \ BaichuanForCausalLM(\r\n2023-08-31 11:36,    (model): BaichuanModel(\r\
          \n2023-08-31 11:36,      (embed_tokens): Embedding(64000, 5120, padding_idx=0)\r\
          \n2023-08-31 11:36,      (layers): ModuleList(\r\n2023-08-31 11:36,    \
          \    (0-39): 40 x BaichuanLayer(\r\n2023-08-31 11:36,          (self_attn):\
          \ BaichuanAttention(\r\n2023-08-31 11:36,            (W_pack): Linear(in_features=5120,\
          \ out_features=15360, bias=False)\r\n2023-08-31 11:36,            (o_proj):\
          \ Linear(in_features=5120, out_features=5120, bias=False)\r\n2023-08-31\
          \ 11:36,          )\r\n2023-08-31 11:36,          (mlp): MLP(\r\n2023-08-31\
          \ 11:36,            (gate_proj): Linear(in_features=5120, out_features=13696,\
          \ bias=False)\r\n2023-08-31 11:36,            (down_proj): Linear(in_features=13696,\
          \ out_features=5120, bias=False)\r\n2023-08-31 11:36,            (up_proj):\
          \ Linear(in_features=5120, out_features=13696, bias=False)\r\n2023-08-31\
          \ 11:36,            (act_fn): SiLUActivation()\r\n2023-08-31 11:36,    \
          \      )\r\n2023-08-31 11:36,          (input_layernorm): RMSNorm()\r\n\
          2023-08-31 11:36,          (post_attention_layernorm): RMSNorm()\r\n2023-08-31\
          \ 11:36,        )\r\n2023-08-31 11:36,      )\r\n2023-08-31 11:36,     \
          \ (norm): RMSNorm()\r\n2023-08-31 11:36,    )\r\n2023-08-31 11:36,    (lm_head):\
          \ Linear(in_features=5120, out_features=64000, bias=False)\r\n2023-08-31\
          \ 11:36,  )\r\n2023-08-31 11:36,)\r\n\r\n\u7ECF\u8FC7FSDP\u7684\u6A21\u578B\
          \uFF0C\u5E94\u8BE5\u53D8\u6210\r\n2023-08-31 14:11,FullyShardedDataParallel(\r\
          \n2023-08-31 14:11,  (_fsdp_wrapped_module): LlamaForCausalLM(\r\n2023-08-31\
          \ 14:11,    (model): LlamaModel(\r\n2023-08-31 14:11,      (embed_tokens):\
          \ Embedding(55296, 5120)\r\n2023-08-31 14:11,      (layers): ModuleList(\r\
          \n2023-08-31 14:11,        (0-39): 40 x FullyShardedDataParallel(\r\n2023-08-31\
          \ 14:11,          (_fsdp_wrapped_module): LlamaDecoderLayer(\r\n2023-08-31\
          \ 14:11,            (self_attn): LlamaAttention(\r\n2023-08-31 14:11,  \
          \            (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\r\
          \n2023-08-31 14:11,              (k_proj): Linear(in_features=5120, out_features=5120,\
          \ bias=False)\r\n2023-08-31 14:11,              (v_proj): Linear(in_features=5120,\
          \ out_features=5120, bias=False)\r\n2023-08-31 14:11,              (o_proj):\
          \ Linear(in_features=5120, out_features=5120, bias=False)\r\n2023-08-31\
          \ 14:11,              (rotary_emb): LlamaRotaryEmbedding()\r\n2023-08-31\
          \ 14:11,            )\r\n2023-08-31 14:11,            (mlp): LlamaMLP(\r\
          \n2023-08-31 14:11,              (gate_proj): Linear(in_features=5120, out_features=13824,\
          \ bias=False)\r\n2023-08-31 14:11,              (up_proj): Linear(in_features=5120,\
          \ out_features=13824, bias=False)\r\n2023-08-31 14:11,              (down_proj):\
          \ Linear(in_features=13824, out_features=5120, bias=False)\r\n2023-08-31\
          \ 14:11,              (act_fn): SiLUActivation()\r\n2023-08-31 14:11,  \
          \          )\r\n2023-08-31 14:11,            (input_layernorm): LlamaRMSNorm()\r\
          \n2023-08-31 14:11,            (post_attention_layernorm): LlamaRMSNorm()\r\
          \n2023-08-31 14:11,          )\r\n2023-08-31 14:11,        )\r\n2023-08-31\
          \ 14:11,      )\r\n2023-08-31 14:11,      (norm): LlamaRMSNorm()\r\n2023-08-31\
          \ 14:11,    )\r\n2023-08-31 14:11,    (lm_head): Linear(in_features=5120,\
          \ out_features=55296, bias=False)\r\n2023-08-31 14:11,  )\r\n2023-08-31\
          \ 14:11,)\r\n\r\n\r\n\u5BF9\u6BCF\u4E00\u4E2Alayer\u90FD\u5E94\u8BE5\u6709\
          \u4E00\u4E2A  (_fsdp_wrapped_module)\r\n\u6211\u7ED9\u767E\u5DDD\u6A21\u578B\
          \u8BBE\u7F6E\u4E86wrap_policy\r\n def get_baichun_wrapper():\r\n    baichuan_auto_wrap_policy\
          \ = functools.partial(\r\n        transformer_auto_wrap_policy,\r\n    \
          \    recurse=True,\r\n        transformer_layer_cls={\r\n            BaichuanLayer,\r\
          \n        }\r\n    )\r\n    return baichuan_auto_wrap_policy\r\n\r\n\u770B\
          \u4E86modeling_baichuan.py\u8FD9\u4E2A\u6587\u4EF6\uFF0C\u6574\u4F53\u5B9E\
          \u73B0\u4E0A\u662F\u7EE7\u627Fnn.Moudle\uFF0Cllama\u6A21\u578B\u4E5F\u662F\
          \u4E00\u6837\u7684\u3002\u4E0D\u77E5\u9053\u54EA\u91CC\u51FA\u4E86\u95EE\
          \u9898\uFF0C\u4E0D\u80FD\u8FDB\u884Cfsdp\u5305\u88C5"
        updatedAt: '2023-08-31T08:44:53.710Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - LemonadeXyz
    id: 64f05305cd079a253fdef442
    type: comment
  author: stonem
  content: "2023-08-31 11:36,FullyShardedDataParallel(\r\n2023-08-31 11:36,  (_fsdp_wrapped_module):\
    \ BaichuanForCausalLM(\r\n2023-08-31 11:36,    (model): BaichuanModel(\r\n2023-08-31\
    \ 11:36,      (embed_tokens): Embedding(64000, 5120, padding_idx=0)\r\n2023-08-31\
    \ 11:36,      (layers): ModuleList(\r\n2023-08-31 11:36,        (0-39): 40 x BaichuanLayer(\r\
    \n2023-08-31 11:36,          (self_attn): BaichuanAttention(\r\n2023-08-31 11:36,\
    \            (W_pack): Linear(in_features=5120, out_features=15360, bias=False)\r\
    \n2023-08-31 11:36,            (o_proj): Linear(in_features=5120, out_features=5120,\
    \ bias=False)\r\n2023-08-31 11:36,          )\r\n2023-08-31 11:36,          (mlp):\
    \ MLP(\r\n2023-08-31 11:36,            (gate_proj): Linear(in_features=5120, out_features=13696,\
    \ bias=False)\r\n2023-08-31 11:36,            (down_proj): Linear(in_features=13696,\
    \ out_features=5120, bias=False)\r\n2023-08-31 11:36,            (up_proj): Linear(in_features=5120,\
    \ out_features=13696, bias=False)\r\n2023-08-31 11:36,            (act_fn): SiLUActivation()\r\
    \n2023-08-31 11:36,          )\r\n2023-08-31 11:36,          (input_layernorm):\
    \ RMSNorm()\r\n2023-08-31 11:36,          (post_attention_layernorm): RMSNorm()\r\
    \n2023-08-31 11:36,        )\r\n2023-08-31 11:36,      )\r\n2023-08-31 11:36,\
    \      (norm): RMSNorm()\r\n2023-08-31 11:36,    )\r\n2023-08-31 11:36,    (lm_head):\
    \ Linear(in_features=5120, out_features=64000, bias=False)\r\n2023-08-31 11:36,\
    \  )\r\n2023-08-31 11:36,)\r\n\r\n\u7ECF\u8FC7FSDP\u7684\u6A21\u578B\uFF0C\u5E94\
    \u8BE5\u53D8\u6210\r\n2023-08-31 14:11,FullyShardedDataParallel(\r\n2023-08-31\
    \ 14:11,  (_fsdp_wrapped_module): LlamaForCausalLM(\r\n2023-08-31 14:11,    (model):\
    \ LlamaModel(\r\n2023-08-31 14:11,      (embed_tokens): Embedding(55296, 5120)\r\
    \n2023-08-31 14:11,      (layers): ModuleList(\r\n2023-08-31 14:11,        (0-39):\
    \ 40 x FullyShardedDataParallel(\r\n2023-08-31 14:11,          (_fsdp_wrapped_module):\
    \ LlamaDecoderLayer(\r\n2023-08-31 14:11,            (self_attn): LlamaAttention(\r\
    \n2023-08-31 14:11,              (q_proj): Linear(in_features=5120, out_features=5120,\
    \ bias=False)\r\n2023-08-31 14:11,              (k_proj): Linear(in_features=5120,\
    \ out_features=5120, bias=False)\r\n2023-08-31 14:11,              (v_proj): Linear(in_features=5120,\
    \ out_features=5120, bias=False)\r\n2023-08-31 14:11,              (o_proj): Linear(in_features=5120,\
    \ out_features=5120, bias=False)\r\n2023-08-31 14:11,              (rotary_emb):\
    \ LlamaRotaryEmbedding()\r\n2023-08-31 14:11,            )\r\n2023-08-31 14:11,\
    \            (mlp): LlamaMLP(\r\n2023-08-31 14:11,              (gate_proj): Linear(in_features=5120,\
    \ out_features=13824, bias=False)\r\n2023-08-31 14:11,              (up_proj):\
    \ Linear(in_features=5120, out_features=13824, bias=False)\r\n2023-08-31 14:11,\
    \              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\r\
    \n2023-08-31 14:11,              (act_fn): SiLUActivation()\r\n2023-08-31 14:11,\
    \            )\r\n2023-08-31 14:11,            (input_layernorm): LlamaRMSNorm()\r\
    \n2023-08-31 14:11,            (post_attention_layernorm): LlamaRMSNorm()\r\n\
    2023-08-31 14:11,          )\r\n2023-08-31 14:11,        )\r\n2023-08-31 14:11,\
    \      )\r\n2023-08-31 14:11,      (norm): LlamaRMSNorm()\r\n2023-08-31 14:11,\
    \    )\r\n2023-08-31 14:11,    (lm_head): Linear(in_features=5120, out_features=55296,\
    \ bias=False)\r\n2023-08-31 14:11,  )\r\n2023-08-31 14:11,)\r\n\r\n\r\n\u5BF9\u6BCF\
    \u4E00\u4E2Alayer\u90FD\u5E94\u8BE5\u6709\u4E00\u4E2A  (_fsdp_wrapped_module)\r\
    \n\u6211\u7ED9\u767E\u5DDD\u6A21\u578B\u8BBE\u7F6E\u4E86wrap_policy\r\n def get_baichun_wrapper():\r\
    \n    baichuan_auto_wrap_policy = functools.partial(\r\n        transformer_auto_wrap_policy,\r\
    \n        recurse=True,\r\n        transformer_layer_cls={\r\n            BaichuanLayer,\r\
    \n        }\r\n    )\r\n    return baichuan_auto_wrap_policy\r\n\r\n\u770B\u4E86\
    modeling_baichuan.py\u8FD9\u4E2A\u6587\u4EF6\uFF0C\u6574\u4F53\u5B9E\u73B0\u4E0A\
    \u662F\u7EE7\u627Fnn.Moudle\uFF0Cllama\u6A21\u578B\u4E5F\u662F\u4E00\u6837\u7684\
    \u3002\u4E0D\u77E5\u9053\u54EA\u91CC\u51FA\u4E86\u95EE\u9898\uFF0C\u4E0D\u80FD\
    \u8FDB\u884Cfsdp\u5305\u88C5"
  created_at: 2023-08-31 07:44:53+00:00
  edited: false
  hidden: false
  id: 64f05305cd079a253fdef442
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/eead920a8369c79aef31a07a80fc73b4.svg
      fullname: stonem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: stonem
      type: user
    createdAt: '2023-08-31T08:46:27.000Z'
    data:
      from: "FSDP\u8BAD\u7EC3\u6A21\u578B\uFF0C\u5E94\u8BE5\u600E\u4E48\u5BF9\u6A21\
        \u578B\u8FDB\u884Cfsdp_wrap, \u6839\u636E\u5B98\u65B9\u793A\u4F8B\u65E0\u6CD5\
        \u5BF9\u6A21\u578B\u8FDB\u884Cfsdp_wrap\u3002"
      to: "FSDP\u8BAD\u7EC3\u6A21\u578B\uFF0C\u5E94\u8BE5\u600E\u4E48\u5BF9baichuan\u6A21\
        \u578B\u8FDB\u884Cfsdp_wrap, \u6839\u636E\u5B98\u65B9\u793A\u4F8B\u65E0\u6CD5\
        \u5BF9\u6A21\u578B\u8FDB\u884Cfsdp_wrap\u3002"
    id: 64f053639a957782e222fe27
    type: title-change
  author: stonem
  created_at: 2023-08-31 07:46:27+00:00
  id: 64f053639a957782e222fe27
  new_title: "FSDP\u8BAD\u7EC3\u6A21\u578B\uFF0C\u5E94\u8BE5\u600E\u4E48\u5BF9baichuan\u6A21\
    \u578B\u8FDB\u884Cfsdp_wrap, \u6839\u636E\u5B98\u65B9\u793A\u4F8B\u65E0\u6CD5\u5BF9\
    \u6A21\u578B\u8FDB\u884Cfsdp_wrap\u3002"
  old_title: "FSDP\u8BAD\u7EC3\u6A21\u578B\uFF0C\u5E94\u8BE5\u600E\u4E48\u5BF9\u6A21\
    \u578B\u8FDB\u884Cfsdp_wrap, \u6839\u636E\u5B98\u65B9\u793A\u4F8B\u65E0\u6CD5\u5BF9\
    \u6A21\u578B\u8FDB\u884Cfsdp_wrap\u3002"
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/eead920a8369c79aef31a07a80fc73b4.svg
      fullname: stonem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: stonem
      type: user
    createdAt: '2023-08-31T09:28:36.000Z'
    data:
      status: closed
    id: 64f05d4490392cfcfaae614e
    type: status-change
  author: stonem
  created_at: 2023-08-31 08:28:36+00:00
  id: 64f05d4490392cfcfaae614e
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641744462372-noauth.jpeg?w=200&h=200&f=face
      fullname: YupeiWong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: LemonadeXyz
      type: user
    createdAt: '2023-12-29T13:29:30.000Z'
    data:
      edited: false
      editors:
      - LemonadeXyz
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.9994650483131409
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641744462372-noauth.jpeg?w=200&h=200&f=face
          fullname: YupeiWong
          isHf: false
          isPro: true
          name: LemonadeXyz
          type: user
        html: "<p>\u670B\u53CB\u8BF7\u95EE\u4F60\u73B0\u5728\u89E3\u51B3\u8FD9\u4E2A\
          \u95EE\u9898\u4E86\u5417\uFF1F</p>\n"
        raw: "\u670B\u53CB\u8BF7\u95EE\u4F60\u73B0\u5728\u89E3\u51B3\u8FD9\u4E2A\u95EE\
          \u9898\u4E86\u5417\uFF1F"
        updatedAt: '2023-12-29T13:29:30.016Z'
      numEdits: 0
      reactions: []
    id: 658ec9ba16227c7a2dea075d
    type: comment
  author: LemonadeXyz
  content: "\u670B\u53CB\u8BF7\u95EE\u4F60\u73B0\u5728\u89E3\u51B3\u8FD9\u4E2A\u95EE\
    \u9898\u4E86\u5417\uFF1F"
  created_at: 2023-12-29 13:29:30+00:00
  edited: false
  hidden: false
  id: 658ec9ba16227c7a2dea075d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: baichuan-inc/Baichuan-7B
repo_type: model
status: closed
target_branch: null
title: "FSDP\u8BAD\u7EC3\u6A21\u578B\uFF0C\u5E94\u8BE5\u600E\u4E48\u5BF9baichuan\u6A21\
  \u578B\u8FDB\u884Cfsdp_wrap, \u6839\u636E\u5B98\u65B9\u793A\u4F8B\u65E0\u6CD5\u5BF9\
  \u6A21\u578B\u8FDB\u884Cfsdp_wrap\u3002"
