!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alfredplpl
conflicting_files: null
created_at: 2023-09-01 17:24:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-09-01T18:24:14.000Z'
    data:
      edited: true
      editors:
      - alfredplpl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4594970643520355
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
          fullname: Yasunori Ozaki
          isHf: false
          isPro: false
          name: alfredplpl
          type: user
        html: "<p>I ran the code:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer\n<span class=\"hljs-keyword\">import</span>\
          \ transformers\n\nmodel = <span class=\"hljs-string\">\"NousResearch/Yarn-Llama-2-13b-128k\"\
          </span>\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline =\
          \ transformers.pipeline(\n    <span class=\"hljs-string\">\"text-generation\"\
          </span>,\n    model=model,\n    device_map=<span class=\"hljs-string\">\"\
          auto\"</span>,\n    load_in_8bit=<span class=\"hljs-literal\">True</span>\n\
          )\n</code></pre>\n<p>Then, I got the error:</p>\n<pre><code class=\"language-bash\"\
          >  File <span class=\"hljs-string\">\"/path/to/venv/lib/python3.8/site-packages/transformers/models/llama/configuration_llama.py\"\
          </span>, line 149, <span class=\"hljs-keyword\">in</span> __init__\n   \
          \ self._rope_scaling_validation()\n  File <span class=\"hljs-string\">\"\
          /path/to/venv/lib/python3.8/site-packages/transformers/models/llama/configuration_llama.py\"\
          </span>, line 167, <span class=\"hljs-keyword\">in</span> _rope_scaling_validation\n\
          \    raise ValueError(\nValueError: `rope_scaling` must be a dictionary\
          \ with with two fields, `<span class=\"hljs-built_in\">type</span>` and\
          \ `<span class=\"hljs-built_in\">factor</span>`, got {<span class=\"hljs-string\"\
          >'factor'</span>: 32.0, <span class=\"hljs-string\">'original_max_position_embeddings'</span>:\
          \ 4096, <span class=\"hljs-string\">'type'</span>: <span class=\"hljs-string\"\
          >'yarn'</span>, <span class=\"hljs-string\">'finetuned'</span>: True}\n\
          </code></pre>\n<p>But, the config is as follows:</p>\n<pre><code class=\"\
          language-json\"><span class=\"hljs-punctuation\">{</span>\n...\n  <span\
          \ class=\"hljs-attr\">\"rope_scaling\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-punctuation\">{</span>\n    <span class=\"\
          hljs-attr\">\"factor\"</span><span class=\"hljs-punctuation\">:</span> <span\
          \ class=\"hljs-number\">32.0</span><span class=\"hljs-punctuation\">,</span>\n\
          \    <span class=\"hljs-attr\">\"original_max_position_embeddings\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">4096</span><span\
          \ class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">\"\
          type\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\"\
          >\"yarn\"</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"\
          hljs-attr\">\"finetuned\"</span><span class=\"hljs-punctuation\">:</span>\
          \ <span class=\"hljs-literal\"><span class=\"hljs-keyword\">true</span></span>\n\
          \  <span class=\"hljs-punctuation\">}</span><span class=\"hljs-punctuation\"\
          >,</span>\n...\n<span class=\"hljs-punctuation\">}</span>\n</code></pre>\n\
          <p>Please fix <a href=\"https://huggingface.co/NousResearch/Yarn-Llama-2-13b-128k/blob/main/configuration_llama.py\"\
          >https://huggingface.co/NousResearch/Yarn-Llama-2-13b-128k/blob/main/configuration_llama.py</a>\
          \ .</p>\n<p>Thanks in advance.</p>\n"
        raw: "I ran the code:\n```python\nfrom transformers import AutoTokenizer\n\
          import transformers\n\nmodel = \"NousResearch/Yarn-Llama-2-13b-128k\"\n\n\
          tokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    device_map=\"auto\",\n\
          \    load_in_8bit=True\n)\n```\n\nThen, I got the error:\n```bash\n  File\
          \ \"/path/to/venv/lib/python3.8/site-packages/transformers/models/llama/configuration_llama.py\"\
          , line 149, in __init__\n    self._rope_scaling_validation()\n  File \"\
          /path/to/venv/lib/python3.8/site-packages/transformers/models/llama/configuration_llama.py\"\
          , line 167, in _rope_scaling_validation\n    raise ValueError(\nValueError:\
          \ `rope_scaling` must be a dictionary with with two fields, `type` and `factor`,\
          \ got {'factor': 32.0, 'original_max_position_embeddings': 4096, 'type':\
          \ 'yarn', 'finetuned': True}\n```\n\nBut, the config is as follows:\n```json\n\
          {\n...\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"original_max_position_embeddings\"\
          : 4096,\n    \"type\": \"yarn\",\n    \"finetuned\": true\n  },\n...\n}\n\
          ```\n\nPlease fix https://huggingface.co/NousResearch/Yarn-Llama-2-13b-128k/blob/main/configuration_llama.py\
          \ .\n\nThanks in advance."
        updatedAt: '2023-09-01T18:33:25.680Z'
      numEdits: 3
      reactions: []
    id: 64f22c4e91f426a315d9b27e
    type: comment
  author: alfredplpl
  content: "I ran the code:\n```python\nfrom transformers import AutoTokenizer\nimport\
    \ transformers\n\nmodel = \"NousResearch/Yarn-Llama-2-13b-128k\"\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    device_map=\"auto\",\n    load_in_8bit=True\n\
    )\n```\n\nThen, I got the error:\n```bash\n  File \"/path/to/venv/lib/python3.8/site-packages/transformers/models/llama/configuration_llama.py\"\
    , line 149, in __init__\n    self._rope_scaling_validation()\n  File \"/path/to/venv/lib/python3.8/site-packages/transformers/models/llama/configuration_llama.py\"\
    , line 167, in _rope_scaling_validation\n    raise ValueError(\nValueError: `rope_scaling`\
    \ must be a dictionary with with two fields, `type` and `factor`, got {'factor':\
    \ 32.0, 'original_max_position_embeddings': 4096, 'type': 'yarn', 'finetuned':\
    \ True}\n```\n\nBut, the config is as follows:\n```json\n{\n...\n  \"rope_scaling\"\
    : {\n    \"factor\": 32.0,\n    \"original_max_position_embeddings\": 4096,\n\
    \    \"type\": \"yarn\",\n    \"finetuned\": true\n  },\n...\n}\n```\n\nPlease\
    \ fix https://huggingface.co/NousResearch/Yarn-Llama-2-13b-128k/blob/main/configuration_llama.py\
    \ .\n\nThanks in advance."
  created_at: 2023-09-01 17:24:14+00:00
  edited: true
  hidden: false
  id: 64f22c4e91f426a315d9b27e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676652577978-630581db99870e13d3e0006f.jpeg?w=200&h=200&f=face
      fullname: Jeffrey Quesnelle
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: emozilla
      type: user
    createdAt: '2023-09-01T19:16:09.000Z'
    data:
      edited: false
      editors:
      - emozilla
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.817071795463562
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676652577978-630581db99870e13d3e0006f.jpeg?w=200&h=200&f=face
          fullname: Jeffrey Quesnelle
          isHf: false
          isPro: true
          name: emozilla
          type: user
        html: '<p>Pass <code>trust_remote_code=True</code> to the pipeline call --
          this is needed to run the custom modeling code</p>

          '
        raw: Pass `trust_remote_code=True` to the pipeline call -- this is needed
          to run the custom modeling code
        updatedAt: '2023-09-01T19:16:09.511Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - alfredplpl
      relatedEventId: 64f238797fafa9a685f446d3
    id: 64f238797fafa9a685f446d1
    type: comment
  author: emozilla
  content: Pass `trust_remote_code=True` to the pipeline call -- this is needed to
    run the custom modeling code
  created_at: 2023-09-01 18:16:09+00:00
  edited: false
  hidden: false
  id: 64f238797fafa9a685f446d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676652577978-630581db99870e13d3e0006f.jpeg?w=200&h=200&f=face
      fullname: Jeffrey Quesnelle
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: emozilla
      type: user
    createdAt: '2023-09-01T19:16:09.000Z'
    data:
      status: closed
    id: 64f238797fafa9a685f446d3
    type: status-change
  author: emozilla
  created_at: 2023-09-01 18:16:09+00:00
  id: 64f238797fafa9a685f446d3
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: NousResearch/Yarn-Llama-2-13b-128k
repo_type: model
status: closed
target_branch: null
title: 'Bug: fix the rope_scaling'
