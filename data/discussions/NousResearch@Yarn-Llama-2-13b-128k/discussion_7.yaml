!!python/object:huggingface_hub.community.DiscussionWithDetails
author: erikgarrison
conflicting_files: null
created_at: 2023-10-29 21:45:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0bf7a0beabd7f92c40986cae1a88324b.svg
      fullname: Erik Garrison
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: erikgarrison
      type: user
    createdAt: '2023-10-29T22:45:38.000Z'
    data:
      edited: false
      editors:
      - erikgarrison
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40566009283065796
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0bf7a0beabd7f92c40986cae1a88324b.svg
          fullname: Erik Garrison
          isHf: false
          isPro: false
          name: erikgarrison
          type: user
        html: "<p>I'm having a bit of trouble running this. Any advice? I've installed\
          \ the flash-attn library.</p>\n<pre><code># Load model directly        \
          \                                                                      \
          \                                                            \nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          NousResearch/Yarn-Llama-2-13b-128k\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          NousResearch/Yarn-Llama-2-13b-128k\")\n</code></pre>\n<pre><code>-&gt; %\
          \ python Yarn-Llama-2-13b-128k.py\nTraceback (most recent call last):\n\
          \  File \"/home/erikg/hugging/Yarn-Llama-2-13b-128k.py\", line 5, in &lt;module&gt;\n\
          \    model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Yarn-Llama-2-13b-128k\"\
          )\n  File \"/home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 525, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\n\
          \  File \"/home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 1051, in from_pretrained\n    return config_class.from_dict(config_dict,\
          \ **unused_kwargs)\n  File \"/home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/configuration_utils.py\"\
          , line 747, in from_dict\n    config = cls(**config_dict)\n  File \"/home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/models/llama/configuration_llama.py\"\
          , line 152, in __init__\n    self._rope_scaling_validation()\n  File \"\
          /home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/models/llama/configuration_llama.py\"\
          , line 171, in _rope_scaling_validation\n    raise ValueError(\nValueError:\
          \ `rope_scaling` must be a dictionary with with two fields, `type` and `factor`,\
          \ got {'factor': 32.0, 'original_max_position_embeddings': 4096, 'type':\
          \ 'yarn', 'finetuned': True}\n</code></pre>\n"
        raw: "I'm having a bit of trouble running this. Any advice? I've installed\
          \ the flash-attn library.\r\n\r\n```\r\n# Load model directly          \
          \                                                                      \
          \                                                          \r\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          NousResearch/Yarn-Llama-2-13b-128k\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          NousResearch/Yarn-Llama-2-13b-128k\")\r\n```\r\n\r\n```\r\n-> % python Yarn-Llama-2-13b-128k.py\r\
          \nTraceback (most recent call last):\r\n  File \"/home/erikg/hugging/Yarn-Llama-2-13b-128k.py\"\
          , line 5, in <module>\r\n    model = AutoModelForCausalLM.from_pretrained(\"\
          NousResearch/Yarn-Llama-2-13b-128k\")\r\n  File \"/home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 525, in from_pretrained\r\n    config, kwargs = AutoConfig.from_pretrained(\r\
          \n  File \"/home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 1051, in from_pretrained\r\n    return config_class.from_dict(config_dict,\
          \ **unused_kwargs)\r\n  File \"/home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/configuration_utils.py\"\
          , line 747, in from_dict\r\n    config = cls(**config_dict)\r\n  File \"\
          /home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/models/llama/configuration_llama.py\"\
          , line 152, in __init__\r\n    self._rope_scaling_validation()\r\n  File\
          \ \"/home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/models/llama/configuration_llama.py\"\
          , line 171, in _rope_scaling_validation\r\n    raise ValueError(\r\nValueError:\
          \ `rope_scaling` must be a dictionary with with two fields, `type` and `factor`,\
          \ got {'factor': 32.0, 'original_max_position_embeddings': 4096, 'type':\
          \ 'yarn', 'finetuned': True}\r\n```"
        updatedAt: '2023-10-29T22:45:38.567Z'
      numEdits: 0
      reactions: []
    id: 653ee092afcd26e2cbe08cbb
    type: comment
  author: erikgarrison
  content: "I'm having a bit of trouble running this. Any advice? I've installed the\
    \ flash-attn library.\r\n\r\n```\r\n# Load model directly                    \
    \                                                                            \
    \                                          \r\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Yarn-Llama-2-13b-128k\"\
    )\r\nmodel = AutoModelForCausalLM.from_pretrained(\"NousResearch/Yarn-Llama-2-13b-128k\"\
    )\r\n```\r\n\r\n```\r\n-> % python Yarn-Llama-2-13b-128k.py\r\nTraceback (most\
    \ recent call last):\r\n  File \"/home/erikg/hugging/Yarn-Llama-2-13b-128k.py\"\
    , line 5, in <module>\r\n    model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Yarn-Llama-2-13b-128k\"\
    )\r\n  File \"/home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 525, in from_pretrained\r\n    config, kwargs = AutoConfig.from_pretrained(\r\
    \n  File \"/home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 1051, in from_pretrained\r\n    return config_class.from_dict(config_dict,\
    \ **unused_kwargs)\r\n  File \"/home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/configuration_utils.py\"\
    , line 747, in from_dict\r\n    config = cls(**config_dict)\r\n  File \"/home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/models/llama/configuration_llama.py\"\
    , line 152, in __init__\r\n    self._rope_scaling_validation()\r\n  File \"/home/erikg/hugging/.env/lib/python3.10/site-packages/transformers/models/llama/configuration_llama.py\"\
    , line 171, in _rope_scaling_validation\r\n    raise ValueError(\r\nValueError:\
    \ `rope_scaling` must be a dictionary with with two fields, `type` and `factor`,\
    \ got {'factor': 32.0, 'original_max_position_embeddings': 4096, 'type': 'yarn',\
    \ 'finetuned': True}\r\n```"
  created_at: 2023-10-29 21:45:38+00:00
  edited: false
  hidden: false
  id: 653ee092afcd26e2cbe08cbb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0bf7a0beabd7f92c40986cae1a88324b.svg
      fullname: Erik Garrison
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: erikgarrison
      type: user
    createdAt: '2023-10-29T22:46:33.000Z'
    data:
      edited: false
      editors:
      - erikgarrison
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9753038883209229
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0bf7a0beabd7f92c40986cae1a88324b.svg
          fullname: Erik Garrison
          isHf: false
          isPro: false
          name: erikgarrison
          type: user
        html: '<p>I fixed it thanks to a suggestion in a prior issue.</p>

          '
        raw: I fixed it thanks to a suggestion in a prior issue.
        updatedAt: '2023-10-29T22:46:33.602Z'
      numEdits: 0
      reactions: []
    id: 653ee0c9ceb0a0f6ce4157c7
    type: comment
  author: erikgarrison
  content: I fixed it thanks to a suggestion in a prior issue.
  created_at: 2023-10-29 21:46:33+00:00
  edited: false
  hidden: false
  id: 653ee0c9ceb0a0f6ce4157c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0bf7a0beabd7f92c40986cae1a88324b.svg
      fullname: Erik Garrison
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: erikgarrison
      type: user
    createdAt: '2023-10-29T22:46:58.000Z'
    data:
      edited: false
      editors:
      - erikgarrison
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3040587604045868
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0bf7a0beabd7f92c40986cae1a88324b.svg
          fullname: Erik Garrison
          isHf: false
          isPro: false
          name: erikgarrison
          type: user
        html: '<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM


          tokenizer = AutoTokenizer.from_pretrained("NousResearch/Yarn-Llama-2-13b-128k")

          model = AutoModelForCausalLM.from_pretrained("NousResearch/Yarn-Llama-2-13b-128k",trust_remote_code=True)

          </code></pre>

          '
        raw: '```

          from transformers import AutoTokenizer, AutoModelForCausalLM


          tokenizer = AutoTokenizer.from_pretrained("NousResearch/Yarn-Llama-2-13b-128k")

          model = AutoModelForCausalLM.from_pretrained("NousResearch/Yarn-Llama-2-13b-128k",trust_remote_code=True)

          ```'
        updatedAt: '2023-10-29T22:46:58.180Z'
      numEdits: 0
      reactions: []
      relatedEventId: 653ee0e2fbe250285f1c9516
    id: 653ee0e2fbe250285f1c9514
    type: comment
  author: erikgarrison
  content: '```

    from transformers import AutoTokenizer, AutoModelForCausalLM


    tokenizer = AutoTokenizer.from_pretrained("NousResearch/Yarn-Llama-2-13b-128k")

    model = AutoModelForCausalLM.from_pretrained("NousResearch/Yarn-Llama-2-13b-128k",trust_remote_code=True)

    ```'
  created_at: 2023-10-29 21:46:58+00:00
  edited: false
  hidden: false
  id: 653ee0e2fbe250285f1c9514
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/0bf7a0beabd7f92c40986cae1a88324b.svg
      fullname: Erik Garrison
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: erikgarrison
      type: user
    createdAt: '2023-10-29T22:46:58.000Z'
    data:
      status: closed
    id: 653ee0e2fbe250285f1c9516
    type: status-change
  author: erikgarrison
  created_at: 2023-10-29 21:46:58+00:00
  id: 653ee0e2fbe250285f1c9516
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: NousResearch/Yarn-Llama-2-13b-128k
repo_type: model
status: closed
target_branch: null
title: rope_scaling must be a dictionary with with two fields
