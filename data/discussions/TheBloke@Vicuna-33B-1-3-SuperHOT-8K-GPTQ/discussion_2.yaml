!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SilverJim
conflicting_files: null
created_at: 2023-07-01 15:03:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c42ceed4647ba4c8e0f31e3bdd9e47d0.svg
      fullname: Li Ming
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SilverJim
      type: user
    createdAt: '2023-07-01T16:03:31.000Z'
    data:
      edited: true
      editors:
      - SilverJim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7294148802757263
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c42ceed4647ba4c8e0f31e3bdd9e47d0.svg
          fullname: Li Ming
          isHf: false
          isPro: false
          name: SilverJim
          type: user
        html: '<p>Hello, The Bloke</p>

          <p>SuperHOT for 7B model has been released: <a href="https://huggingface.co/kaiokendev/superhot-7b-8k-no-rlhf-test">https://huggingface.co/kaiokendev/superhot-7b-8k-no-rlhf-test</a></p>

          <p>And I need orca_mini_7B-GPTQ with 8k context size, and I am not sure
          whether SuperHOT work on orca_mini_7B (As SuperHOT is a LORA for Llama while
          orca_mini_7B is Open-Llama)</p>

          <p>If it work, could you help me create orca_mini_7B-SuperHOT-8K-GPTQ?</p>

          <p>Thank you!</p>

          '
        raw: 'Hello, The Bloke


          SuperHOT for 7B model has been released: https://huggingface.co/kaiokendev/superhot-7b-8k-no-rlhf-test


          And I need orca_mini_7B-GPTQ with 8k context size, and I am not sure whether
          SuperHOT work on orca_mini_7B (As SuperHOT is a LORA for Llama while orca_mini_7B
          is Open-Llama)


          If it work, could you help me create orca_mini_7B-SuperHOT-8K-GPTQ?


          Thank you!'
        updatedAt: '2023-07-01T16:08:44.581Z'
      numEdits: 3
      reactions: []
    id: 64a04e53d76ca0fe679f12fd
    type: comment
  author: SilverJim
  content: 'Hello, The Bloke


    SuperHOT for 7B model has been released: https://huggingface.co/kaiokendev/superhot-7b-8k-no-rlhf-test


    And I need orca_mini_7B-GPTQ with 8k context size, and I am not sure whether SuperHOT
    work on orca_mini_7B (As SuperHOT is a LORA for Llama while orca_mini_7B is Open-Llama)


    If it work, could you help me create orca_mini_7B-SuperHOT-8K-GPTQ?


    Thank you!'
  created_at: 2023-07-01 15:03:31+00:00
  edited: true
  hidden: false
  id: 64a04e53d76ca0fe679f12fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/c42ceed4647ba4c8e0f31e3bdd9e47d0.svg
      fullname: Li Ming
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SilverJim
      type: user
    createdAt: '2023-07-01T16:07:51.000Z'
    data:
      from: SuperHOT for 7B model has been released & We need merge orca_mini_7B-GPTQ
        with SuperHOT
      to: SuperHOT for 7B model has been released & I need merge orca_mini_7B-GPTQ
        with SuperHOT
    id: 64a04f57eea04b705619ddb9
    type: title-change
  author: SilverJim
  created_at: 2023-07-01 15:07:51+00:00
  id: 64a04f57eea04b705619ddb9
  new_title: SuperHOT for 7B model has been released & I need merge orca_mini_7B-GPTQ
    with SuperHOT
  old_title: SuperHOT for 7B model has been released & We need merge orca_mini_7B-GPTQ
    with SuperHOT
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ
repo_type: model
status: open
target_branch: null
title: SuperHOT for 7B model has been released & I need merge orca_mini_7B-GPTQ with
  SuperHOT
