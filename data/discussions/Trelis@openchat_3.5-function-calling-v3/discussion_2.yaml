!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kjhamilton
conflicting_files: null
created_at: 2024-01-04 02:27:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6840addccbf91294c15a5892a5aafe6a.svg
      fullname: Kevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kjhamilton
      type: user
    createdAt: '2024-01-04T02:27:57.000Z'
    data:
      edited: false
      editors:
      - kjhamilton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9069754481315613
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6840addccbf91294c15a5892a5aafe6a.svg
          fullname: Kevin
          isHf: false
          isPro: false
          name: kjhamilton
          type: user
        html: '<p>I''m looking:<br>EVGA GeForce RTX 3090 FTW3 Ultra Gaming, 24GB GDDR6X,
          iCX3 Technology, ARGB LED, Metal Backplate, 24 - pretty expensive...</p>

          <p>Or this one, quite a bit less expensive...</p>

          <p>RTX 4060 16Gb:<br>Powered by NVIDIA DLSS 3, ultra-efficient Ada Lovelace
          architechture, and full ray tracing<br>4th Generation Tensor Cores: Up to
          4x performance with DLSS 3<br>3rd Generation RT Cores: Up to 2x ray tracing
          performance<br>Powered by GeForce RTX 4060 Ti<br>Integrated with 16GB GDDR6
          128-bit memory interface<br>WINDFORCE Cooling System, Protection metal back
          plate</p>

          <p>Graphics cards are super confusing to me since the manufacturers use
          these confusing designations all with different GBs and Tensors.</p>

          <p>But since I''m putting together a build for it, do you have a recommendation?</p>

          <p>I want the best function calling response and it is important that when
          no function is required, then it just answers. I won''t be serving more
          than 10 concurrent requests and even if so it may cheaper to build more
          servers with lesser GB cards?</p>

          <p>Lastly does the rest of the computer really matter? I''ll probably have
          PCIe 3, DDR4 (32Gig) and an intel i7 9000.</p>

          '
        raw: "I'm looking:\r\nEVGA GeForce RTX 3090 FTW3 Ultra Gaming, 24GB GDDR6X,\
          \ iCX3 Technology, ARGB LED, Metal Backplate, 24 - pretty expensive...\r\
          \n\r\nOr this one, quite a bit less expensive...\r\n\r\nRTX 4060 16Gb:\r\
          \nPowered by NVIDIA DLSS 3, ultra-efficient Ada Lovelace architechture,\
          \ and full ray tracing\r\n4th Generation Tensor Cores: Up to 4x performance\
          \ with DLSS 3\r\n3rd Generation RT Cores: Up to 2x ray tracing performance\r\
          \nPowered by GeForce RTX 4060 Ti\r\nIntegrated with 16GB GDDR6 128-bit memory\
          \ interface\r\nWINDFORCE Cooling System, Protection metal back plate\r\n\
          \r\nGraphics cards are super confusing to me since the manufacturers use\
          \ these confusing designations all with different GBs and Tensors.\r\n\r\
          \nBut since I'm putting together a build for it, do you have a recommendation?\r\
          \n\r\nI want the best function calling response and it is important that\
          \ when no function is required, then it just answers. I won't be serving\
          \ more than 10 concurrent requests and even if so it may cheaper to build\
          \ more servers with lesser GB cards?\r\n\r\nLastly does the rest of the\
          \ computer really matter? I'll probably have PCIe 3, DDR4 (32Gig) and an\
          \ intel i7 9000."
        updatedAt: '2024-01-04T02:27:57.300Z'
      numEdits: 0
      reactions: []
    id: 659617ad665c29891f68495f
    type: comment
  author: kjhamilton
  content: "I'm looking:\r\nEVGA GeForce RTX 3090 FTW3 Ultra Gaming, 24GB GDDR6X,\
    \ iCX3 Technology, ARGB LED, Metal Backplate, 24 - pretty expensive...\r\n\r\n\
    Or this one, quite a bit less expensive...\r\n\r\nRTX 4060 16Gb:\r\nPowered by\
    \ NVIDIA DLSS 3, ultra-efficient Ada Lovelace architechture, and full ray tracing\r\
    \n4th Generation Tensor Cores: Up to 4x performance with DLSS 3\r\n3rd Generation\
    \ RT Cores: Up to 2x ray tracing performance\r\nPowered by GeForce RTX 4060 Ti\r\
    \nIntegrated with 16GB GDDR6 128-bit memory interface\r\nWINDFORCE Cooling System,\
    \ Protection metal back plate\r\n\r\nGraphics cards are super confusing to me\
    \ since the manufacturers use these confusing designations all with different\
    \ GBs and Tensors.\r\n\r\nBut since I'm putting together a build for it, do you\
    \ have a recommendation?\r\n\r\nI want the best function calling response and\
    \ it is important that when no function is required, then it just answers. I won't\
    \ be serving more than 10 concurrent requests and even if so it may cheaper to\
    \ build more servers with lesser GB cards?\r\n\r\nLastly does the rest of the\
    \ computer really matter? I'll probably have PCIe 3, DDR4 (32Gig) and an intel\
    \ i7 9000."
  created_at: 2024-01-04 02:27:57+00:00
  edited: false
  hidden: false
  id: 659617ad665c29891f68495f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-05T10:12:44.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8684364557266235
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Howdy! Base computer shouldn''t matter too much.</p>

          <p>If you go with a 16 GB GPU, then it will just about run a 7B model in
          16 bit precision. But if you run Text Generation Inference then you can
          run in 8bit with eetq - which is fast and will half your memory requirement.</p>

          <p>I haven''t dug too deep nor run my own gpu at home (other than mac) but
          your cheaper choice seems ok. Check out localllama on reddit for info on
          GPUS: <a rel="nofollow" href="https://www.reddit.com/r/LocalLLaMA/comments/15rwe7t/the_llm_gpu_buying_guide_august_2023/">The
          LLM GPU Buying Guide - August 2023 : r/LocalLLaMA</a></p>

          '
        raw: 'Howdy! Base computer shouldn''t matter too much.


          If you go with a 16 GB GPU, then it will just about run a 7B model in 16
          bit precision. But if you run Text Generation Inference then you can run
          in 8bit with eetq - which is fast and will half your memory requirement.


          I haven''t dug too deep nor run my own gpu at home (other than mac) but
          your cheaper choice seems ok. Check out localllama on reddit for info on
          GPUS: [The LLM GPU Buying Guide - August 2023 : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/15rwe7t/the_llm_gpu_buying_guide_august_2023/)'
        updatedAt: '2024-01-05T10:12:44.932Z'
      numEdits: 0
      reactions: []
    id: 6597d61c966fb200b4df59fa
    type: comment
  author: RonanMcGovern
  content: 'Howdy! Base computer shouldn''t matter too much.


    If you go with a 16 GB GPU, then it will just about run a 7B model in 16 bit precision.
    But if you run Text Generation Inference then you can run in 8bit with eetq -
    which is fast and will half your memory requirement.


    I haven''t dug too deep nor run my own gpu at home (other than mac) but your cheaper
    choice seems ok. Check out localllama on reddit for info on GPUS: [The LLM GPU
    Buying Guide - August 2023 : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/15rwe7t/the_llm_gpu_buying_guide_august_2023/)'
  created_at: 2024-01-05 10:12:44+00:00
  edited: false
  hidden: false
  id: 6597d61c966fb200b4df59fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6840addccbf91294c15a5892a5aafe6a.svg
      fullname: Kevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kjhamilton
      type: user
    createdAt: '2024-01-08T22:06:06.000Z'
    data:
      edited: false
      editors:
      - kjhamilton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9919714331626892
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6840addccbf91294c15a5892a5aafe6a.svg
          fullname: Kevin
          isHf: false
          isPro: false
          name: kjhamilton
          type: user
        html: '<p>This is very helpful. I''ve ordered a used 3090 from EBay.</p>

          '
        raw: This is very helpful. I've ordered a used 3090 from EBay.
        updatedAt: '2024-01-08T22:06:06.996Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - RonanMcGovern
      relatedEventId: 659c71cf8a29b41c41f5e27e
    id: 659c71ce8a29b41c41f5e270
    type: comment
  author: kjhamilton
  content: This is very helpful. I've ordered a used 3090 from EBay.
  created_at: 2024-01-08 22:06:06+00:00
  edited: false
  hidden: false
  id: 659c71ce8a29b41c41f5e270
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/6840addccbf91294c15a5892a5aafe6a.svg
      fullname: Kevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kjhamilton
      type: user
    createdAt: '2024-01-08T22:06:07.000Z'
    data:
      status: closed
    id: 659c71cf8a29b41c41f5e27e
    type: status-change
  author: kjhamilton
  created_at: 2024-01-08 22:06:07+00:00
  id: 659c71cf8a29b41c41f5e27e
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Trelis/openchat_3.5-function-calling-v3
repo_type: model
status: closed
target_branch: null
title: Looking for the right graphics card for this model
