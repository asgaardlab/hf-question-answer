!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kjhamilton
conflicting_files: null
created_at: 2024-01-08 22:11:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6840addccbf91294c15a5892a5aafe6a.svg
      fullname: Kevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kjhamilton
      type: user
    createdAt: '2024-01-08T22:11:10.000Z'
    data:
      edited: false
      editors:
      - kjhamilton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.896908164024353
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6840addccbf91294c15a5892a5aafe6a.svg
          fullname: Kevin
          isHf: false
          isPro: false
          name: kjhamilton
          type: user
        html: '<p>I understand the model will do a great job calling specific functions
          like finding the weather for a given location. How do I get it to call a
          more general knowledge search function. For example to look up features
          for a product. Right now I''m trying a function called "search_knowledge"
          and the description is "Search your knowledge for specific information about  features"
          - but it doesn''t ever call it. Is there a clever trick in the description,
          function name, or parameter that I could use to trigger it to call the function
          with a search string whenever the user asks about the product?</p>

          '
        raw: I understand the model will do a great job calling specific functions
          like finding the weather for a given location. How do I get it to call a
          more general knowledge search function. For example to look up features
          for a product. Right now I'm trying a function called "search_knowledge"
          and the description is "Search your knowledge for specific information about
          <product name> features" - but it doesn't ever call it. Is there a clever
          trick in the description, function name, or parameter that I could use to
          trigger it to call the function with a search string whenever the user asks
          about the product?
        updatedAt: '2024-01-08T22:11:10.126Z'
      numEdits: 0
      reactions: []
    id: 659c72fe1b89affadb583eb1
    type: comment
  author: kjhamilton
  content: I understand the model will do a great job calling specific functions like
    finding the weather for a given location. How do I get it to call a more general
    knowledge search function. For example to look up features for a product. Right
    now I'm trying a function called "search_knowledge" and the description is "Search
    your knowledge for specific information about <product name> features" - but it
    doesn't ever call it. Is there a clever trick in the description, function name,
    or parameter that I could use to trigger it to call the function with a search
    string whenever the user asks about the product?
  created_at: 2024-01-08 22:11:10+00:00
  edited: false
  hidden: false
  id: 659c72fe1b89affadb583eb1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-09T10:47:34.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8790034651756287
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<ol>

          <li><p>Can you share a copy of what the function metadata looks like? Hard
          for me to comment without seeing that</p>

          </li>

          <li><p>Take that metadata and ask an LLM to optimise it. That should improve
          things..</p>

          </li>

          </ol>

          <p>lmk how it goes</p>

          '
        raw: '1) Can you share a copy of what the function metadata looks like? Hard
          for me to comment without seeing that


          2) Take that metadata and ask an LLM to optimise it. That should improve
          things..


          lmk how it goes


          '
        updatedAt: '2024-01-09T10:47:34.806Z'
      numEdits: 0
      reactions: []
    id: 659d24460a4634193c07aabb
    type: comment
  author: RonanMcGovern
  content: '1) Can you share a copy of what the function metadata looks like? Hard
    for me to comment without seeing that


    2) Take that metadata and ask an LLM to optimise it. That should improve things..


    lmk how it goes


    '
  created_at: 2024-01-09 10:47:34+00:00
  edited: false
  hidden: false
  id: 659d24460a4634193c07aabb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6840addccbf91294c15a5892a5aafe6a.svg
      fullname: Kevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kjhamilton
      type: user
    createdAt: '2024-01-10T00:57:06.000Z'
    data:
      edited: false
      editors:
      - kjhamilton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9034225940704346
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6840addccbf91294c15a5892a5aafe6a.svg
          fullname: Kevin
          isHf: false
          isPro: false
          name: kjhamilton
          type: user
        html: '<p>That''s a good idea, I''ll chat with the LLM ;).</p>

          <p>A more relevant question or maybe a better one when thinking of ultimately
          architecting the solution. Would I run a RAG implementation along with function
          calling? Meaning if I wanted to pre-lookup data and attach it as another
          message for context, or is it better to get the model to make the decision
          of when it needs to go off and search or get additional information?</p>

          <p>It works exceptionally well so far with the stock and options data functions.
          It seems to get confused when I add an additional layer either by adding
          context data or having it try to lookup information to answer questions
          about the company that don''t involve the real-time data functions.</p>

          <p>Example when using RAG:</p>

          <p>messages.append({<br>            "role": "system",<br>            "content":
          "In addition to the functions, you can use the following information to
          help "<br>                       "respond: \n\n" + kb_data[0].page_content<br>         })</p>

          '
        raw: "That's a good idea, I'll chat with the LLM ;).\n\nA more relevant question\
          \ or maybe a better one when thinking of ultimately architecting the solution.\
          \ Would I run a RAG implementation along with function calling? Meaning\
          \ if I wanted to pre-lookup data and attach it as another message for context,\
          \ or is it better to get the model to make the decision of when it needs\
          \ to go off and search or get additional information?\n\nIt works exceptionally\
          \ well so far with the stock and options data functions. It seems to get\
          \ confused when I add an additional layer either by adding context data\
          \ or having it try to lookup information to answer questions about the company\
          \ that don't involve the real-time data functions.\n\nExample when using\
          \ RAG:\n\nmessages.append({\n            \"role\": \"system\",\n       \
          \     \"content\": \"In addition to the functions, you can use the following\
          \ information to help \"\n                       \"respond: \\n\\n\" + kb_data[0].page_content\n\
          \         })"
        updatedAt: '2024-01-10T00:57:06.428Z'
      numEdits: 0
      reactions: []
    id: 659deb62e7ed0764b4f4fbca
    type: comment
  author: kjhamilton
  content: "That's a good idea, I'll chat with the LLM ;).\n\nA more relevant question\
    \ or maybe a better one when thinking of ultimately architecting the solution.\
    \ Would I run a RAG implementation along with function calling? Meaning if I wanted\
    \ to pre-lookup data and attach it as another message for context, or is it better\
    \ to get the model to make the decision of when it needs to go off and search\
    \ or get additional information?\n\nIt works exceptionally well so far with the\
    \ stock and options data functions. It seems to get confused when I add an additional\
    \ layer either by adding context data or having it try to lookup information to\
    \ answer questions about the company that don't involve the real-time data functions.\n\
    \nExample when using RAG:\n\nmessages.append({\n            \"role\": \"system\"\
    ,\n            \"content\": \"In addition to the functions, you can use the following\
    \ information to help \"\n                       \"respond: \\n\\n\" + kb_data[0].page_content\n\
    \         })"
  created_at: 2024-01-10 00:57:06+00:00
  edited: false
  hidden: false
  id: 659deb62e7ed0764b4f4fbca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6840addccbf91294c15a5892a5aafe6a.svg
      fullname: Kevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kjhamilton
      type: user
    createdAt: '2024-01-10T01:18:58.000Z'
    data:
      edited: false
      editors:
      - kjhamilton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9476088881492615
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6840addccbf91294c15a5892a5aafe6a.svg
          fullname: Kevin
          isHf: false
          isPro: false
          name: kjhamilton
          type: user
        html: '<p>Would a 13b model help?</p>

          '
        raw: Would a 13b model help?
        updatedAt: '2024-01-10T01:18:58.425Z'
      numEdits: 0
      reactions: []
    id: 659df0821723f371c9e5aa5c
    type: comment
  author: kjhamilton
  content: Would a 13b model help?
  created_at: 2024-01-10 01:18:58+00:00
  edited: false
  hidden: false
  id: 659df0821723f371c9e5aa5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-10T14:59:55.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9550099968910217
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>OpenChat is based on Mixtral, and Mixtral doesn''t have a system
          message, so putting data into the system message may not work well. It may
          also throw off the function calling (because the model IS trained to expect
          function calling metadata).</p>

          <p>I think you can use RAG, and what you can do is return the data from
          RAG along with the function response. If you look at the Mixtral video,
          you can see a bit how the function response is handled by the ADVANCED-Inference
          Repo.</p>

          <p>I would probably try that before going to a bigger model. Unfortunately
          there is no 13B model (there just aren''t great open source models in that
          size) so probably the next step would be to use SUSChat (the AWQ, which
          is &lt;30 GB) or maybe deepseek coder 33B (that will be strong at function
          calling as it''s a coding model).</p>

          '
        raw: 'OpenChat is based on Mixtral, and Mixtral doesn''t have a system message,
          so putting data into the system message may not work well. It may also throw
          off the function calling (because the model IS trained to expect function
          calling metadata).


          I think you can use RAG, and what you can do is return the data from RAG
          along with the function response. If you look at the Mixtral video, you
          can see a bit how the function response is handled by the ADVANCED-Inference
          Repo.


          I would probably try that before going to a bigger model. Unfortunately
          there is no 13B model (there just aren''t great open source models in that
          size) so probably the next step would be to use SUSChat (the AWQ, which
          is <30 GB) or maybe deepseek coder 33B (that will be strong at function
          calling as it''s a coding model).'
        updatedAt: '2024-01-10T14:59:55.034Z'
      numEdits: 0
      reactions: []
    id: 659eb0ebc50ce1bffc71572e
    type: comment
  author: RonanMcGovern
  content: 'OpenChat is based on Mixtral, and Mixtral doesn''t have a system message,
    so putting data into the system message may not work well. It may also throw off
    the function calling (because the model IS trained to expect function calling
    metadata).


    I think you can use RAG, and what you can do is return the data from RAG along
    with the function response. If you look at the Mixtral video, you can see a bit
    how the function response is handled by the ADVANCED-Inference Repo.


    I would probably try that before going to a bigger model. Unfortunately there
    is no 13B model (there just aren''t great open source models in that size) so
    probably the next step would be to use SUSChat (the AWQ, which is <30 GB) or maybe
    deepseek coder 33B (that will be strong at function calling as it''s a coding
    model).'
  created_at: 2024-01-10 14:59:55+00:00
  edited: false
  hidden: false
  id: 659eb0ebc50ce1bffc71572e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6840addccbf91294c15a5892a5aafe6a.svg
      fullname: Kevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kjhamilton
      type: user
    createdAt: '2024-01-10T18:58:38.000Z'
    data:
      edited: false
      editors:
      - kjhamilton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9387713074684143
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6840addccbf91294c15a5892a5aafe6a.svg
          fullname: Kevin
          isHf: false
          isPro: false
          name: kjhamilton
          type: user
        html: '<p>So far I love it. I''ll keep working with this one. I changed the
          opening message, ditched the system message putting everything into the
          function call message and it is working pretty well. Still doing more testing.</p>

          <p>I find that if the response from the function is too complicated, it
          just spits out the function response and says: Here''s the answer . I am
          curious if that''s a training thing or if it is something where like the
          30B+ model would do a better job of interpreting a long JSON response from
          the function. For now I''m just simplifying down my responses to a few dictionary
          keys with the critical information.</p>

          '
        raw: 'So far I love it. I''ll keep working with this one. I changed the opening
          message, ditched the system message putting everything into the function
          call message and it is working pretty well. Still doing more testing.


          I find that if the response from the function is too complicated, it just
          spits out the function response and says: Here''s the answer <JSON response
          from function>. I am curious if that''s a training thing or if it is something
          where like the 30B+ model would do a better job of interpreting a long JSON
          response from the function. For now I''m just simplifying down my responses
          to a few dictionary keys with the critical information.'
        updatedAt: '2024-01-10T18:58:38.360Z'
      numEdits: 0
      reactions: []
    id: 659ee8de480c22a6e0534ac1
    type: comment
  author: kjhamilton
  content: 'So far I love it. I''ll keep working with this one. I changed the opening
    message, ditched the system message putting everything into the function call
    message and it is working pretty well. Still doing more testing.


    I find that if the response from the function is too complicated, it just spits
    out the function response and says: Here''s the answer <JSON response from function>.
    I am curious if that''s a training thing or if it is something where like the
    30B+ model would do a better job of interpreting a long JSON response from the
    function. For now I''m just simplifying down my responses to a few dictionary
    keys with the critical information.'
  created_at: 2024-01-10 18:58:38+00:00
  edited: false
  hidden: false
  id: 659ee8de480c22a6e0534ac1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-11T11:35:50.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.979679524898529
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Really nice!</p>

          <p>Yeah I suspect that with a lot of different information, a smaller model
          will have difficulty with nuance. The granularity of understanding is just
          worse, and that becomes more of an issue the more detail you have included
          (the model just averages across the detail).</p>

          '
        raw: 'Really nice!


          Yeah I suspect that with a lot of different information, a smaller model
          will have difficulty with nuance. The granularity of understanding is just
          worse, and that becomes more of an issue the more detail you have included
          (the model just averages across the detail).'
        updatedAt: '2024-01-11T11:35:50.477Z'
      numEdits: 0
      reactions: []
      relatedEventId: 659fd29628725b8dad914434
    id: 659fd29628725b8dad914432
    type: comment
  author: RonanMcGovern
  content: 'Really nice!


    Yeah I suspect that with a lot of different information, a smaller model will
    have difficulty with nuance. The granularity of understanding is just worse, and
    that becomes more of an issue the more detail you have included (the model just
    averages across the detail).'
  created_at: 2024-01-11 11:35:50+00:00
  edited: false
  hidden: false
  id: 659fd29628725b8dad914432
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-11T11:35:50.000Z'
    data:
      status: closed
    id: 659fd29628725b8dad914434
    type: status-change
  author: RonanMcGovern
  created_at: 2024-01-11 11:35:50+00:00
  id: 659fd29628725b8dad914434
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Trelis/openchat_3.5-function-calling-v3
repo_type: model
status: closed
target_branch: null
title: General search function
