!!python/object:huggingface_hub.community.DiscussionWithDetails
author: leonardlin
conflicting_files: null
created_at: 2023-08-11 13:55:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
      fullname: lhl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leonardlin
      type: user
    createdAt: '2023-08-11T14:55:14.000Z'
    data:
      edited: false
      editors:
      - leonardlin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6780239343643188
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
          fullname: lhl
          isHf: false
          isPro: false
          name: leonardlin
          type: user
        html: "<p>So, I'm able to run the sample code (although frustratingly, it\
          \ want a token even with a fully local copy, and even if I force the config\
          \ file to hardcode my local path).</p>\n<p>I'm trying to use <a rel=\"nofollow\"\
          \ href=\"https://github.com/ggerganov/ggml/tree/master/examples/gpt-neox\"\
          >GGML's GPT-NeoX support</a> to convert the model.  I've swapped the tokenizer\
          \ with the <code> LlamaTokenizer.from_pretrained(\"novelai/nerdstash-tokenizer-v1\"\
          , additional_special_tokens=['\u2581\u2581'])</code> from the example and\
          \ also swapped the AutoModelForCausalLM() as well and am able to generate\
          \ a <code>ggml-model-f16.bin</code>, however if I try to run <code>gpt-neox</code>\
          \ on the model, here's the error I get:</p>\n<pre><code>bin/gpt-neox -m\
          \ /models/llm/jp-stablelm/stabilityai_japanese-stablelm-instruct-alpha-7b/ggml-model-f16.bin\n\
          main: seed = 1691765429\ngpt_neox_model_load: loading model from '/models/llm/jp-stablelm/stabilityai_japanese-stablelm-instruct-alpha-7b/ggml-model-f16.bin'\
          \ - please wait ...\ngpt_neox_model_load: n_vocab = 65535\ngpt_neox_model_load:\
          \ n_ctx   = 1024\ngpt_neox_model_load: n_embd  = 4096\ngpt_neox_model_load:\
          \ n_head  = 32\ngpt_neox_model_load: n_layer = 32\ngpt_neox_model_load:\
          \ n_rot   = 32\ngpt_neox_model_load: par_res = 1\ngpt_neox_model_load: ftype\
          \   = 1\ngpt_neox_model_load: qntvr   = 0\ngpt_neox_model_load: ggml ctx\
          \ size = 16390.52 MB\ngpt_neox_model_load: memory_size =   512.00 MB, n_mem\
          \ = 32768\ngpt_neox_model_load: unknown tensor 'transformer.embed_in.weight'\
          \ in model file\nmain: failed to load model from '/models/llm/jp-stablelm/stabilityai_japanese-stablelm-instruct-alpha-7b/ggml-model-f16.bin'\n\
          </code></pre>\n<p>Just checking in to see if anyone has had better luck\
          \ with GGML quantize support/how different this model is from other GPT-NeoX\
          \ or StableLM models that have been quantized?</p>\n"
        raw: "So, I'm able to run the sample code (although frustratingly, it want\
          \ a token even with a fully local copy, and even if I force the config file\
          \ to hardcode my local path).\r\n\r\nI'm trying to use [GGML's GPT-NeoX\
          \ support](https://github.com/ggerganov/ggml/tree/master/examples/gpt-neox)\
          \ to convert the model.  I've swapped the tokenizer with the ` LlamaTokenizer.from_pretrained(\"\
          novelai/nerdstash-tokenizer-v1\", additional_special_tokens=['\u2581\u2581\
          '])` from the example and also swapped the AutoModelForCausalLM() as well\
          \ and am able to generate a `ggml-model-f16.bin`, however if I try to run\
          \ `gpt-neox` on the model, here's the error I get:\r\n\r\n```\r\nbin/gpt-neox\
          \ -m /models/llm/jp-stablelm/stabilityai_japanese-stablelm-instruct-alpha-7b/ggml-model-f16.bin\r\
          \nmain: seed = 1691765429\r\ngpt_neox_model_load: loading model from '/models/llm/jp-stablelm/stabilityai_japanese-stablelm-instruct-alpha-7b/ggml-model-f16.bin'\
          \ - please wait ...\r\ngpt_neox_model_load: n_vocab = 65535\r\ngpt_neox_model_load:\
          \ n_ctx   = 1024\r\ngpt_neox_model_load: n_embd  = 4096\r\ngpt_neox_model_load:\
          \ n_head  = 32\r\ngpt_neox_model_load: n_layer = 32\r\ngpt_neox_model_load:\
          \ n_rot   = 32\r\ngpt_neox_model_load: par_res = 1\r\ngpt_neox_model_load:\
          \ ftype   = 1\r\ngpt_neox_model_load: qntvr   = 0\r\ngpt_neox_model_load:\
          \ ggml ctx size = 16390.52 MB\r\ngpt_neox_model_load: memory_size =   512.00\
          \ MB, n_mem = 32768\r\ngpt_neox_model_load: unknown tensor 'transformer.embed_in.weight'\
          \ in model file\r\nmain: failed to load model from '/models/llm/jp-stablelm/stabilityai_japanese-stablelm-instruct-alpha-7b/ggml-model-f16.bin'\r\
          \n```\r\nJust checking in to see if anyone has had better luck with GGML\
          \ quantize support/how different this model is from other GPT-NeoX or StableLM\
          \ models that have been quantized?"
        updatedAt: '2023-08-11T14:55:14.989Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - sehiro
    id: 64d64bd2abf475a808b0ca65
    type: comment
  author: leonardlin
  content: "So, I'm able to run the sample code (although frustratingly, it want a\
    \ token even with a fully local copy, and even if I force the config file to hardcode\
    \ my local path).\r\n\r\nI'm trying to use [GGML's GPT-NeoX support](https://github.com/ggerganov/ggml/tree/master/examples/gpt-neox)\
    \ to convert the model.  I've swapped the tokenizer with the ` LlamaTokenizer.from_pretrained(\"\
    novelai/nerdstash-tokenizer-v1\", additional_special_tokens=['\u2581\u2581'])`\
    \ from the example and also swapped the AutoModelForCausalLM() as well and am\
    \ able to generate a `ggml-model-f16.bin`, however if I try to run `gpt-neox`\
    \ on the model, here's the error I get:\r\n\r\n```\r\nbin/gpt-neox -m /models/llm/jp-stablelm/stabilityai_japanese-stablelm-instruct-alpha-7b/ggml-model-f16.bin\r\
    \nmain: seed = 1691765429\r\ngpt_neox_model_load: loading model from '/models/llm/jp-stablelm/stabilityai_japanese-stablelm-instruct-alpha-7b/ggml-model-f16.bin'\
    \ - please wait ...\r\ngpt_neox_model_load: n_vocab = 65535\r\ngpt_neox_model_load:\
    \ n_ctx   = 1024\r\ngpt_neox_model_load: n_embd  = 4096\r\ngpt_neox_model_load:\
    \ n_head  = 32\r\ngpt_neox_model_load: n_layer = 32\r\ngpt_neox_model_load: n_rot\
    \   = 32\r\ngpt_neox_model_load: par_res = 1\r\ngpt_neox_model_load: ftype   =\
    \ 1\r\ngpt_neox_model_load: qntvr   = 0\r\ngpt_neox_model_load: ggml ctx size\
    \ = 16390.52 MB\r\ngpt_neox_model_load: memory_size =   512.00 MB, n_mem = 32768\r\
    \ngpt_neox_model_load: unknown tensor 'transformer.embed_in.weight' in model file\r\
    \nmain: failed to load model from '/models/llm/jp-stablelm/stabilityai_japanese-stablelm-instruct-alpha-7b/ggml-model-f16.bin'\r\
    \n```\r\nJust checking in to see if anyone has had better luck with GGML quantize\
    \ support/how different this model is from other GPT-NeoX or StableLM models that\
    \ have been quantized?"
  created_at: 2023-08-11 13:55:14+00:00
  edited: false
  hidden: false
  id: 64d64bd2abf475a808b0ca65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0b854a0cda41e4ea3a45c838e24a632c.svg
      fullname: Sehiro Sofoki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sehiro
      type: user
    createdAt: '2023-08-12T21:01:13.000Z'
    data:
      edited: false
      editors:
      - sehiro
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8746625185012817
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0b854a0cda41e4ea3a45c838e24a632c.svg
          fullname: Sehiro Sofoki
          isHf: false
          isPro: false
          name: sehiro
          type: user
        html: '<p>Me too...</p>

          '
        raw: Me too...
        updatedAt: '2023-08-12T21:01:13.251Z'
      numEdits: 0
      reactions: []
    id: 64d7f319c2eedf9af84c9658
    type: comment
  author: sehiro
  content: Me too...
  created_at: 2023-08-12 20:01:13+00:00
  edited: false
  hidden: false
  id: 64d7f319c2eedf9af84c9658
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678593972754-noauth.jpeg?w=200&h=200&f=face
      fullname: Metal Whale
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: metalwhale
      type: user
    createdAt: '2023-08-13T03:26:24.000Z'
    data:
      edited: true
      editors:
      - metalwhale
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7252062559127808
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678593972754-noauth.jpeg?w=200&h=200&f=face
          fullname: Metal Whale
          isHf: false
          isPro: false
          name: metalwhale
          type: user
        html: '<p>I''m not entirely certain, but I believe the reason is that in <a
          rel="nofollow" href="https://github.com/ggerganov/ggml/blob/244776a/examples/gpt-neox/main.cpp#L236"><code>main.cpp</code></a>
          file, they are currently hard-coding layer names with prefix <code>gpt_neox</code>.<br>This
          approach works well with <code>stablelm-base-alpha-7b</code> (you can check
          it at <a href="https://huggingface.co/stabilityai/stablelm-base-alpha-7b/blob/714a3dd/pytorch_model.bin.index.json#L7"><code>pytorch_model.bin.index.json</code></a>
          file). However, in the case of <code>japanese-stablelm-base-alpha-7b</code>,
          they are using prefix <code>transformer</code> (according to it''s <a href="https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b/blob/c79d6e6/pytorch_model.bin.index.json#L7">pytorch_model.bin.index.json</a>
          file).</p>

          <p>I think the simplest solution would be to modify the prefix in <code>main.cpp</code>
          file to <code>transformer</code>.</p>

          '
        raw: 'I''m not entirely certain, but I believe the reason is that in [`main.cpp`](https://github.com/ggerganov/ggml/blob/244776a/examples/gpt-neox/main.cpp#L236)
          file, they are currently hard-coding layer names with prefix `gpt_neox`.

          This approach works well with `stablelm-base-alpha-7b` (you can check it
          at [`pytorch_model.bin.index.json`](https://huggingface.co/stabilityai/stablelm-base-alpha-7b/blob/714a3dd/pytorch_model.bin.index.json#L7)
          file). However, in the case of `japanese-stablelm-base-alpha-7b`, they are
          using prefix `transformer` (according to it''s [pytorch_model.bin.index.json](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b/blob/c79d6e6/pytorch_model.bin.index.json#L7)
          file).


          I think the simplest solution would be to modify the prefix in `main.cpp`
          file to `transformer`.'
        updatedAt: '2023-08-13T03:30:40.890Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - sehiro
        - ke99L
    id: 64d84d60bb9b5ddddcad131f
    type: comment
  author: metalwhale
  content: 'I''m not entirely certain, but I believe the reason is that in [`main.cpp`](https://github.com/ggerganov/ggml/blob/244776a/examples/gpt-neox/main.cpp#L236)
    file, they are currently hard-coding layer names with prefix `gpt_neox`.

    This approach works well with `stablelm-base-alpha-7b` (you can check it at [`pytorch_model.bin.index.json`](https://huggingface.co/stabilityai/stablelm-base-alpha-7b/blob/714a3dd/pytorch_model.bin.index.json#L7)
    file). However, in the case of `japanese-stablelm-base-alpha-7b`, they are using
    prefix `transformer` (according to it''s [pytorch_model.bin.index.json](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b/blob/c79d6e6/pytorch_model.bin.index.json#L7)
    file).


    I think the simplest solution would be to modify the prefix in `main.cpp` file
    to `transformer`.'
  created_at: 2023-08-13 02:26:24+00:00
  edited: true
  hidden: false
  id: 64d84d60bb9b5ddddcad131f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: stabilityai/japanese-stablelm-instruct-alpha-7b
repo_type: model
status: open
target_branch: null
title: GGML Quantize?
