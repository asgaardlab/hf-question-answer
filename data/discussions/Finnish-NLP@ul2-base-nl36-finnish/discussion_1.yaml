!!python/object:huggingface_hub.community.DiscussionWithDetails
author: keremturgutlu
conflicting_files: null
created_at: 2023-04-09 02:49:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fc290459efb12b9fad90cb11b01c671a.svg
      fullname: Kerem Turgutlu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: keremturgutlu
      type: user
    createdAt: '2023-04-09T03:49:39.000Z'
    data:
      edited: true
      editors:
      - keremturgutlu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fc290459efb12b9fad90cb11b01c671a.svg
          fullname: Kerem Turgutlu
          isHf: false
          isPro: false
          name: keremturgutlu
          type: user
        html: '<p>Hi, I know this code snippet is mostly taken from the original paper
          but I noticed that neither this ul2_denoiser or T5x span_corruption actually
          separates documents with eos token.  Not sure if this is intended but it
          definitely made sense to me to give an idea to the model when two adjacent
          documents are different in the input.</p>

          <p>Here I created a notebook which adds eos.</p>

          <p><a rel="nofollow" href="https://www.kaggle.com/keremt/seqio-tutorial/">https://www.kaggle.com/keremt/seqio-tutorial/</a><br>
          (Search for # FIX: Added append_eos to separate different documents in encoder.)</p>

          <p>Also in your script vocabulary needs to be <code>vocabulary = seqio.SentencePieceVocabulary(''/kaggle/input/google-ul2/spiece.model'',
          extra_ids=100)</code> I guess, it is missing the extra ids. Models might
          have trained well regardless, e.g. it would be using the last 100 tokens
          in the vocab which might not be that frequent to affect the results. So
          models would learn to use those tokens as a denoising prompt token.</p>

          '
        raw: "Hi, I know this code snippet is mostly taken from the original paper\
          \ but I noticed that neither this ul2_denoiser or T5x span_corruption actually\
          \ separates documents with eos token.  Not sure if this is intended but\
          \ it definitely made sense to me to give an idea to the model when two adjacent\
          \ documents are different in the input.\n\nHere I created a notebook which\
          \ adds eos.\n\nhttps://www.kaggle.com/keremt/seqio-tutorial/\n (Search for\
          \ # FIX: Added append_eos to separate different documents in encoder.)\n\
          \n\nAlso in your script vocabulary needs to be `vocabulary = seqio.SentencePieceVocabulary('/kaggle/input/google-ul2/spiece.model',\
          \ extra_ids=100)` I guess, it is missing the extra ids. Models might have\
          \ trained well regardless, e.g. it would be using the last 100 tokens in\
          \ the vocab which might not be that frequent to affect the results. So models\
          \ would learn to use those tokens as a denoising prompt token."
        updatedAt: '2023-04-09T03:52:53.678Z'
      numEdits: 2
      reactions: []
    id: 643235d3faac480f27c89a1c
    type: comment
  author: keremturgutlu
  content: "Hi, I know this code snippet is mostly taken from the original paper but\
    \ I noticed that neither this ul2_denoiser or T5x span_corruption actually separates\
    \ documents with eos token.  Not sure if this is intended but it definitely made\
    \ sense to me to give an idea to the model when two adjacent documents are different\
    \ in the input.\n\nHere I created a notebook which adds eos.\n\nhttps://www.kaggle.com/keremt/seqio-tutorial/\n\
    \ (Search for # FIX: Added append_eos to separate different documents in encoder.)\n\
    \n\nAlso in your script vocabulary needs to be `vocabulary = seqio.SentencePieceVocabulary('/kaggle/input/google-ul2/spiece.model',\
    \ extra_ids=100)` I guess, it is missing the extra ids. Models might have trained\
    \ well regardless, e.g. it would be using the last 100 tokens in the vocab which\
    \ might not be that frequent to affect the results. So models would learn to use\
    \ those tokens as a denoising prompt token."
  created_at: 2023-04-09 02:49:39+00:00
  edited: true
  hidden: false
  id: 643235d3faac480f27c89a1c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Finnish-NLP/ul2-base-nl36-finnish
repo_type: model
status: open
target_branch: null
title: eos token to separate documents
