!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kquant03
conflicting_files: null
created_at: 2024-01-05 20:48:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6589d7e6586088fd2784a12c/WJFxNOZHe9blf89D9DXYi.png?w=200&h=200&f=face
      fullname: Stanley Sebastian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kquant03
      type: user
    createdAt: '2024-01-05T20:48:11.000Z'
    data:
      edited: false
      editors:
      - Kquant03
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9776869416236877
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6589d7e6586088fd2784a12c/WJFxNOZHe9blf89D9DXYi.png?w=200&h=200&f=face
          fullname: Stanley Sebastian
          isHf: false
          isPro: false
          name: Kquant03
          type: user
        html: '<p>This model is amazing and I''m very impressed with its performance
          in the open LLm leaderboard....it''s not that I was having trouble converting
          this base model...it''s that MoE of this model won''t convert. Thank you
          for putting the extra time into getting the conversions so people can run
          the base model, though. This base model is good enough by itself without
          MoE, it''s just fun to experiment with MoE lol</p>

          '
        raw: This model is amazing and I'm very impressed with its performance in
          the open LLm leaderboard....it's not that I was having trouble converting
          this base model...it's that MoE of this model won't convert. Thank you for
          putting the extra time into getting the conversions so people can run the
          base model, though. This base model is good enough by itself without MoE,
          it's just fun to experiment with MoE lol
        updatedAt: '2024-01-05T20:48:11.378Z'
      numEdits: 0
      reactions: []
    id: 65986b0b5f7a6d40f7d02c83
    type: comment
  author: Kquant03
  content: This model is amazing and I'm very impressed with its performance in the
    open LLm leaderboard....it's not that I was having trouble converting this base
    model...it's that MoE of this model won't convert. Thank you for putting the extra
    time into getting the conversions so people can run the base model, though. This
    base model is good enough by itself without MoE, it's just fun to experiment with
    MoE lol
  created_at: 2024-01-05 20:48:11+00:00
  edited: false
  hidden: false
  id: 65986b0b5f7a6d40f7d02c83
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: CultriX/MistralTrix-v1-GGUF
repo_type: model
status: open
target_branch: null
title: Fantastic
