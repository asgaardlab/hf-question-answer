!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wise-east
conflicting_files: null
created_at: 2022-07-06 22:52:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/33ce42f3e6fef18ecbf29ef2dde8d457.svg
      fullname: Justin Cho
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wise-east
      type: user
    createdAt: '2022-07-06T23:52:07.000Z'
    data:
      edited: false
      editors:
      - wise-east
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/33ce42f3e6fef18ecbf29ef2dde8d457.svg
          fullname: Justin Cho
          isHf: false
          isPro: false
          name: wise-east
          type: user
        html: '<p>I''d imagine that T0 has better cross-task transfer learning because
          of the instructions being more verbose and doesn''t rely on memorizing task
          names to perform certain tasks such as "cola sentence: John made Bill master
          of himself." In other words, I think apart from only zero-shot generalization,
          I think T0 might even do better for individual tasks that T5 models were
          trained with. </p>

          <p>Is there any comparison between T0 vs T5 on benchmarks that T5 report
          on? I''m surprised that I don''t find these results anywhere. </p>

          '
        raw: "I'd imagine that T0 has better cross-task transfer learning because\
          \ of the instructions being more verbose and doesn't rely on memorizing\
          \ task names to perform certain tasks such as \"cola sentence: John made\
          \ Bill master of himself.\" In other words, I think apart from only zero-shot\
          \ generalization, I think T0 might even do better for individual tasks that\
          \ T5 models were trained with. \r\n\r\nIs there any comparison between T0\
          \ vs T5 on benchmarks that T5 report on? I'm surprised that I don't find\
          \ these results anywhere. "
        updatedAt: '2022-07-06T23:52:07.348Z'
      numEdits: 0
      reactions: []
    id: 62c620276b730753e3620cf9
    type: comment
  author: wise-east
  content: "I'd imagine that T0 has better cross-task transfer learning because of\
    \ the instructions being more verbose and doesn't rely on memorizing task names\
    \ to perform certain tasks such as \"cola sentence: John made Bill master of himself.\"\
    \ In other words, I think apart from only zero-shot generalization, I think T0\
    \ might even do better for individual tasks that T5 models were trained with.\
    \ \r\n\r\nIs there any comparison between T0 vs T5 on benchmarks that T5 report\
    \ on? I'm surprised that I don't find these results anywhere. "
  created_at: 2022-07-06 22:52:07+00:00
  edited: false
  hidden: false
  id: 62c620276b730753e3620cf9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
      fullname: Victor Sanh
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: VictorSanh
      type: user
    createdAt: '2022-07-07T08:50:12.000Z'
    data:
      edited: false
      editors:
      - VictorSanh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
          fullname: Victor Sanh
          isHf: true
          isPro: true
          name: VictorSanh
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;wise-east&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/wise-east\"\
          >@<span class=\"underline\">wise-east</span></a></span>\n\n\t</span></span>!<br>In\
          \ the T0 paper, we compared against T5+LM which is T5 further pre-trained\
          \ on standard language modeling loss. This is a much more sensible baseline\
          \ to compare with given the MLM loss of T5 (IIRC, I did run some comparisons\
          \ with T5 and it performed not as good as t5+lm).</p>\n"
        raw: 'Hi @wise-east!

          In the T0 paper, we compared against T5+LM which is T5 further pre-trained
          on standard language modeling loss. This is a much more sensible baseline
          to compare with given the MLM loss of T5 (IIRC, I did run some comparisons
          with T5 and it performed not as good as t5+lm).'
        updatedAt: '2022-07-07T08:50:12.834Z'
      numEdits: 0
      reactions: []
    id: 62c69e44c24c44c981bc6fd0
    type: comment
  author: VictorSanh
  content: 'Hi @wise-east!

    In the T0 paper, we compared against T5+LM which is T5 further pre-trained on
    standard language modeling loss. This is a much more sensible baseline to compare
    with given the MLM loss of T5 (IIRC, I did run some comparisons with T5 and it
    performed not as good as t5+lm).'
  created_at: 2022-07-07 07:50:12+00:00
  edited: false
  hidden: false
  id: 62c69e44c24c44c981bc6fd0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/33ce42f3e6fef18ecbf29ef2dde8d457.svg
      fullname: Justin Cho
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wise-east
      type: user
    createdAt: '2022-07-07T17:59:15.000Z'
    data:
      edited: false
      editors:
      - wise-east
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/33ce42f3e6fef18ecbf29ef2dde8d457.svg
          fullname: Justin Cho
          isHf: false
          isPro: false
          name: wise-east
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;VictorSanh&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/VictorSanh\"\
          >@<span class=\"underline\">VictorSanh</span></a></span>\n\n\t</span></span>,\
          \ thanks for the quick response!  I'm sorry, I think my question was not\
          \ clear. It seems clear that T0 is better than T5 variants for zero-shot\
          \ generalization (i.e. not being fine-tuned on any of the training sets\
          \ of the task to be evaluated on). </p>\n<p>I'm wondering if fine-tuning\
          \ T0 with the training set of the downstream evaluation task attains better\
          \ performance for a T5 model that is also fine-tuned on the training set\
          \ of the downstream evaluation task, and how big this difference is. More\
          \ specifically, where would T0's performance be if it were to be included\
          \ in Table 14 of the T5 paper? (shown in the image)  Or has T0 been trained\
          \ with the test sets of datasets included in these benchmarks that it is\
          \ not comparable? It didn't seem to be the case based on my read of the\
          \ T0 paper. </p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1657216655204-6214a3ec8cfaecd77a128062.png\"\
          ><img alt=\"Screen Shot 2022-07-07 at 10.57.31 AM.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1657216655204-6214a3ec8cfaecd77a128062.png\"\
          ></a></p>\n"
        raw: "Hey @VictorSanh, thanks for the quick response!  I'm sorry, I think\
          \ my question was not clear. It seems clear that T0 is better than T5 variants\
          \ for zero-shot generalization (i.e. not being fine-tuned on any of the\
          \ training sets of the task to be evaluated on). \n\nI'm wondering if fine-tuning\
          \ T0 with the training set of the downstream evaluation task attains better\
          \ performance for a T5 model that is also fine-tuned on the training set\
          \ of the downstream evaluation task, and how big this difference is. More\
          \ specifically, where would T0's performance be if it were to be included\
          \ in Table 14 of the T5 paper? (shown in the image)  Or has T0 been trained\
          \ with the test sets of datasets included in these benchmarks that it is\
          \ not comparable? It didn't seem to be the case based on my read of the\
          \ T0 paper. \n\n![Screen Shot 2022-07-07 at 10.57.31 AM.png](https://cdn-uploads.huggingface.co/production/uploads/1657216655204-6214a3ec8cfaecd77a128062.png)"
        updatedAt: '2022-07-07T17:59:15.784Z'
      numEdits: 0
      reactions: []
    id: 62c71ef333aa7d875078c8f6
    type: comment
  author: wise-east
  content: "Hey @VictorSanh, thanks for the quick response!  I'm sorry, I think my\
    \ question was not clear. It seems clear that T0 is better than T5 variants for\
    \ zero-shot generalization (i.e. not being fine-tuned on any of the training sets\
    \ of the task to be evaluated on). \n\nI'm wondering if fine-tuning T0 with the\
    \ training set of the downstream evaluation task attains better performance for\
    \ a T5 model that is also fine-tuned on the training set of the downstream evaluation\
    \ task, and how big this difference is. More specifically, where would T0's performance\
    \ be if it were to be included in Table 14 of the T5 paper? (shown in the image)\
    \  Or has T0 been trained with the test sets of datasets included in these benchmarks\
    \ that it is not comparable? It didn't seem to be the case based on my read of\
    \ the T0 paper. \n\n![Screen Shot 2022-07-07 at 10.57.31 AM.png](https://cdn-uploads.huggingface.co/production/uploads/1657216655204-6214a3ec8cfaecd77a128062.png)"
  created_at: 2022-07-07 16:59:15+00:00
  edited: false
  hidden: false
  id: 62c71ef333aa7d875078c8f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
      fullname: Victor Sanh
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: VictorSanh
      type: user
    createdAt: '2022-07-11T07:55:27.000Z'
    data:
      edited: false
      editors:
      - VictorSanh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
          fullname: Victor Sanh
          isHf: true
          isPro: true
          name: VictorSanh
          type: user
        html: '<p>Oooh i see.<br>I think this is an experimental question, I don''t
          have a strong intuition on what the exact conclusion would be: papers that
          do "intermediate fine-tuning" of this sort can observe both negative or
          positive transfer depending on the task.</p>

          '
        raw: 'Oooh i see.

          I think this is an experimental question, I don''t have a strong intuition
          on what the exact conclusion would be: papers that do "intermediate fine-tuning"
          of this sort can observe both negative or positive transfer depending on
          the task.'
        updatedAt: '2022-07-11T07:55:27.689Z'
      numEdits: 0
      reactions: []
    id: 62cbd76fa5583942363347bc
    type: comment
  author: VictorSanh
  content: 'Oooh i see.

    I think this is an experimental question, I don''t have a strong intuition on
    what the exact conclusion would be: papers that do "intermediate fine-tuning"
    of this sort can observe both negative or positive transfer depending on the task.'
  created_at: 2022-07-11 06:55:27+00:00
  edited: false
  hidden: false
  id: 62cbd76fa5583942363347bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b68a8d18c616a44b4255ac7eeefeb90.svg
      fullname: Sara.Amd
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SaraAmd
      type: user
    createdAt: '2023-06-13T15:59:29.000Z'
    data:
      edited: true
      editors:
      - SaraAmd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8672729730606079
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b68a8d18c616a44b4255ac7eeefeb90.svg
          fullname: Sara.Amd
          isHf: false
          isPro: false
          name: SaraAmd
          type: user
        html: '<p>Does anyone know if the T0PP can only handle 512 tokens? I need
          a model that can handle 2048  tokens?when i print print(tokenizer.model_max_length)
          it shows 512 while the input training data is 1024. I am wondering why?</p>

          '
        raw: Does anyone know if the T0PP can only handle 512 tokens? I need a model
          that can handle 2048  tokens?when i print print(tokenizer.model_max_length)
          it shows 512 while the input training data is 1024. I am wondering why?
        updatedAt: '2023-06-13T16:30:48.872Z'
      numEdits: 1
      reactions: []
    id: 64889261495fb57b85ff63d3
    type: comment
  author: SaraAmd
  content: Does anyone know if the T0PP can only handle 512 tokens? I need a model
    that can handle 2048  tokens?when i print print(tokenizer.model_max_length) it
    shows 512 while the input training data is 1024. I am wondering why?
  created_at: 2023-06-13 14:59:29+00:00
  edited: true
  hidden: false
  id: 64889261495fb57b85ff63d3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: bigscience/T0pp
repo_type: model
status: open
target_branch: null
title: T0 vs T5
