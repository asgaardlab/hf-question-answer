!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cdani
conflicting_files: null
created_at: 2022-08-08 13:11:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e30c430989ccd1602a74889ffcdd03d8.svg
      fullname: daniel jose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cdani
      type: user
    createdAt: '2022-08-08T14:11:28.000Z'
    data:
      edited: false
      editors:
      - cdani
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e30c430989ccd1602a74889ffcdd03d8.svg
          fullname: daniel jose
          isHf: false
          isPro: false
          name: cdani
          type: user
        html: '<p>Hello. I''m trying to execute this model and it''s killed as it
          needs more memory than I have in the computer. How much memory it needs?
          I have to dedice how much to add.<br>Thanks.</p>

          '
        raw: "Hello. I'm trying to execute this model and it's killed as it needs\
          \ more memory than I have in the computer. How much memory it needs? I have\
          \ to dedice how much to add.\r\nThanks."
        updatedAt: '2022-08-08T14:11:28.368Z'
      numEdits: 0
      reactions: []
    id: 62f11990b58090c873d6968b
    type: comment
  author: cdani
  content: "Hello. I'm trying to execute this model and it's killed as it needs more\
    \ memory than I have in the computer. How much memory it needs? I have to dedice\
    \ how much to add.\r\nThanks."
  created_at: 2022-08-08 13:11:28+00:00
  edited: false
  hidden: false
  id: 62f11990b58090c873d6968b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
      fullname: Victor Sanh
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: VictorSanh
      type: user
    createdAt: '2022-08-08T14:24:19.000Z'
    data:
      edited: false
      editors:
      - VictorSanh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
          fullname: Victor Sanh
          isHf: true
          isPro: true
          name: VictorSanh
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;cdani&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cdani\">@<span class=\"\
          underline\">cdani</span></a></span>\n\n\t</span></span>, are you talking\
          \ about CPU memory or GPU memory?<br>Just to load the model, you will need\
          \ ~90GB of CPU memory.</p>\n"
        raw: 'Hi @cdani, are you talking about CPU memory or GPU memory?

          Just to load the model, you will need ~90GB of CPU memory.'
        updatedAt: '2022-08-08T14:24:19.113Z'
      numEdits: 0
      reactions: []
    id: 62f11c93d70b97c7dd39e512
    type: comment
  author: VictorSanh
  content: 'Hi @cdani, are you talking about CPU memory or GPU memory?

    Just to load the model, you will need ~90GB of CPU memory.'
  created_at: 2022-08-08 13:24:19+00:00
  edited: false
  hidden: false
  id: 62f11c93d70b97c7dd39e512
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e30c430989ccd1602a74889ffcdd03d8.svg
      fullname: daniel jose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cdani
      type: user
    createdAt: '2022-08-08T14:28:00.000Z'
    data:
      edited: false
      editors:
      - cdani
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e30c430989ccd1602a74889ffcdd03d8.svg
          fullname: daniel jose
          isHf: false
          isPro: false
          name: cdani
          type: user
        html: '<p>Ah! Ok, thanks, was just that memory. I understand that a cuda GPU
          its not necessary, isn''t it? Will it be very slow without it?</p>

          '
        raw: Ah! Ok, thanks, was just that memory. I understand that a cuda GPU its
          not necessary, isn't it? Will it be very slow without it?
        updatedAt: '2022-08-08T14:28:00.121Z'
      numEdits: 0
      reactions: []
    id: 62f11d70b711c6f2b22ab21b
    type: comment
  author: cdani
  content: Ah! Ok, thanks, was just that memory. I understand that a cuda GPU its
    not necessary, isn't it? Will it be very slow without it?
  created_at: 2022-08-08 13:28:00+00:00
  edited: false
  hidden: false
  id: 62f11d70b711c6f2b22ab21b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
      fullname: Victor Sanh
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: VictorSanh
      type: user
    createdAt: '2022-08-08T14:38:10.000Z'
    data:
      edited: false
      editors:
      - VictorSanh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
          fullname: Victor Sanh
          isHf: true
          isPro: true
          name: VictorSanh
          type: user
        html: '<p>Great!<br>Yes, technically, running inference on CPU is possible
          although it will be very slow (an order of magnitude slower than using GPU).
          There are a few pointers here: <a rel="nofollow" href="https://github.com/bigscience-workshop/t-zero/tree/master/inference">https://github.com/bigscience-workshop/t-zero/tree/master/inference</a></p>

          '
        raw: 'Great!

          Yes, technically, running inference on CPU is possible although it will
          be very slow (an order of magnitude slower than using GPU). There are a
          few pointers here: https://github.com/bigscience-workshop/t-zero/tree/master/inference'
        updatedAt: '2022-08-08T14:38:10.120Z'
      numEdits: 0
      reactions: []
    id: 62f11fd2d70b97c7dd3a0128
    type: comment
  author: VictorSanh
  content: 'Great!

    Yes, technically, running inference on CPU is possible although it will be very
    slow (an order of magnitude slower than using GPU). There are a few pointers here:
    https://github.com/bigscience-workshop/t-zero/tree/master/inference'
  created_at: 2022-08-08 13:38:10+00:00
  edited: false
  hidden: false
  id: 62f11fd2d70b97c7dd3a0128
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2c483479391614662af530f30fef6c79.svg
      fullname: Juan Luis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: juanluisdb
      type: user
    createdAt: '2022-10-21T14:37:27.000Z'
    data:
      edited: false
      editors:
      - juanluisdb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2c483479391614662af530f30fef6c79.svg
          fullname: Juan Luis
          isHf: false
          isPro: false
          name: juanluisdb
          type: user
        html: '<p>Could it be possible to upload the model weights in multiples bin
          files (as bloom does) to be able to load with less amount of memory?<br>Thanks!</p>

          '
        raw: 'Could it be possible to upload the model weights in multiples bin files
          (as bloom does) to be able to load with less amount of memory?

          Thanks!'
        updatedAt: '2022-10-21T14:37:27.286Z'
      numEdits: 0
      reactions: []
    id: 6352aea756ef05f3a1f79a07
    type: comment
  author: juanluisdb
  content: 'Could it be possible to upload the model weights in multiples bin files
    (as bloom does) to be able to load with less amount of memory?

    Thanks!'
  created_at: 2022-10-21 13:37:27+00:00
  edited: false
  hidden: false
  id: 6352aea756ef05f3a1f79a07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
      fullname: Victor Sanh
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: VictorSanh
      type: user
    createdAt: '2022-10-21T21:47:27.000Z'
    data:
      edited: false
      editors:
      - VictorSanh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
          fullname: Victor Sanh
          isHf: true
          isPro: true
          name: VictorSanh
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;juanluisdb&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/juanluisdb\"\
          >@<span class=\"underline\">juanluisdb</span></a></span>\n\n\t</span></span>,<br>yes\
          \ we can! and actually it has been done on the branch \"sharded\" -&gt;\
          \ <a href=\"https://huggingface.co/bigscience/T0pp/tree/sharded\">https://huggingface.co/bigscience/T0pp/tree/sharded</a></p>\n\
          <p>You can use the sharded ones by adding a tag in the <code>from_pretrained</code>\
          \ call: <code>AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\"\
          , revision=\"sharded\")</code>.</p>\n"
        raw: 'Hi @juanluisdb,

          yes we can! and actually it has been done on the branch "sharded" -> https://huggingface.co/bigscience/T0pp/tree/sharded


          You can use the sharded ones by adding a tag in the `from_pretrained` call:
          `AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0pp", revision="sharded")`.'
        updatedAt: '2022-10-21T21:47:27.217Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F917"
        users:
        - juanluisdb
        - lukaemon
    id: 6353136fa0da8800db97ceb8
    type: comment
  author: VictorSanh
  content: 'Hi @juanluisdb,

    yes we can! and actually it has been done on the branch "sharded" -> https://huggingface.co/bigscience/T0pp/tree/sharded


    You can use the sharded ones by adding a tag in the `from_pretrained` call: `AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0pp",
    revision="sharded")`.'
  created_at: 2022-10-21 20:47:27+00:00
  edited: false
  hidden: false
  id: 6353136fa0da8800db97ceb8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: bigscience/T0pp
repo_type: model
status: open
target_branch: null
title: memory used
