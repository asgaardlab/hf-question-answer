!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fepegar
conflicting_files: null
created_at: 2022-07-12 07:33:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669369101851-6278fbec17c069c72a11f35d.jpeg?w=200&h=200&f=face
      fullname: "Fernando P\xE9rez-Garc\xEDa"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fepegar
      type: user
    createdAt: '2022-07-12T08:33:49.000Z'
    data:
      edited: true
      editors:
      - fepegar
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669369101851-6278fbec17c069c72a11f35d.jpeg?w=200&h=200&f=face
          fullname: "Fernando P\xE9rez-Garc\xEDa"
          isHf: false
          isPro: false
          name: fepegar
          type: user
        html: '<p>The <a href="https://huggingface.co/docs/transformers/parallelism">.parallelize()</a>
          link seems to be broken.</p>

          <p>This is unfortunate, as I''ve found difficult to parallelize the model
          in a usable way. To be able to simply run inference, I''ve had to create
          my own <code>device_map</code> so the model is uniformly sharded across
          GPUs. I wonder if there''s a nicer way to do this. I''m happy to share my
          hacky solution if needed.</p>

          '
        raw: 'The [.parallelize()](https://huggingface.co/docs/transformers/parallelism)
          link seems to be broken.


          This is unfortunate, as I''ve found difficult to parallelize the model in
          a usable way. To be able to simply run inference, I''ve had to create my
          own `device_map` so the model is uniformly sharded across GPUs. I wonder
          if there''s a nicer way to do this. I''m happy to share my hacky solution
          if needed.'
        updatedAt: '2022-07-12T08:33:57.939Z'
      numEdits: 1
      reactions: []
    id: 62cd31ed4454c18f4e8d1bc4
    type: comment
  author: fepegar
  content: 'The [.parallelize()](https://huggingface.co/docs/transformers/parallelism)
    link seems to be broken.


    This is unfortunate, as I''ve found difficult to parallelize the model in a usable
    way. To be able to simply run inference, I''ve had to create my own `device_map`
    so the model is uniformly sharded across GPUs. I wonder if there''s a nicer way
    to do this. I''m happy to share my hacky solution if needed.'
  created_at: 2022-07-12 07:33:49+00:00
  edited: true
  hidden: false
  id: 62cd31ed4454c18f4e8d1bc4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: bigscience/T0pp
repo_type: model
status: open
target_branch: null
title: Broken link in model card
