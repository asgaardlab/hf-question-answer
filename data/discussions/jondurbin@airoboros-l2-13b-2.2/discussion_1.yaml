!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ch4tTester
conflicting_files: null
created_at: 2023-09-11 12:42:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/33d814e75abf78a6420e899939dab0d0.svg
      fullname: Torben Wendt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ch4tTester
      type: user
    createdAt: '2023-09-11T13:42:24.000Z'
    data:
      edited: false
      editors:
      - ch4tTester
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5599603056907654
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/33d814e75abf78a6420e899939dab0d0.svg
          fullname: Torben Wendt
          isHf: false
          isPro: false
          name: ch4tTester
          type: user
        html: "<p>This Version of the Model seems to be broken. While reaching quite\
          \ good results with the \"airoboros-l2-7b-2.1\" Model, this one puts some\
          \ random Code at the end of the gererated text.<br>{code}<br>Prompt:<br>my_prompt\
          \ = '''A chat. You are to take the role of: Hans<br>Hans is 27 years old\
          \ from Hamburg.</p>\n<p>USER:<br>Hey from where are you?</p>\n<p>ASSISTANT:<br>'''<br>input_ids\
          \ = tokenizer(f\"{my_prompt}\", return_tensors=\"pt\").input_ids  # .to(\"\
          cuda\")<br>outputs = model.generate(input_ids, max_new_tokens=128, min_length=8,\
          \ temperature=0.95, do_sample=True)<br>print(tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True))<br>{/code}<br>Result:</p>\n<hr>\n<p>ASSISTANT:<br>Hey,\
          \ I'm from Hamburg! It's a beautiful city with lots of historical buildings\
          \ and picturesque canals.<br> package com.linkedin.cubert.utils</p>\n<p>import\
          \ java.nio.file.{Path, Paths}</p>\n<p>trait CubertPaths extends Utils {<br>\
          \  def getCubertHDFSOptions(output: Path, maxPartitions: Int, maxMemoryInMB:\
          \ Long): CubertHdfsUtils.CubertHDFSOptions = {<br>    CubertHdfsUtils.CubertHDFSOptions.</p>\n\
          <hr>\n<p>ASSISTANT:<br>Hey! I'm currently in Hamburg, Germany. It's a beautiful\
          \ city by the water with lots of historic buildings and modern attractions.\
          \ I highly recommend visiting if you ever get the chance!<br> #include \"\
          test-framework/test-framework.h\"</p>\n<p>// -----------------------------------------------------------------------<br>//\
          \ Tests<br>//</p>\n<p>TEST(test_1, test_add_sub) {<br>  std::complex c0,\
          \ c1;<br>  c0 = 3 + 5 * I;<br>  c1 = 6 - 3 * I</p>\n<hr>\n<p>ASSISTANT:<br>Guten\
          \ Tag! Ich komme aus Hamburg, da wo der Elbe flie\xDFt.<br>(Translation:\
          \ Good day! I come from Hamburg, where the Elbe flows.)<br> &lt;?php</p>\n\
          <p>namespace App\\Http\\Controllers\\Api;</p>\n<p>use App\\Models\\Project;<br>use\
          \ Illuminate\\Support\\Facades\\Storage;<br>use Illuminate\\Http\\Request;<br>use\
          \ App\\Http\\Controllers\\Controller;</p>\n<p>class ProjectController extends\
          \ Controller<br>{<br>    public function index()<br>    {<br>        $data\
          \ = Project::all();<br>        return response</p>\n"
        raw: "This Version of the Model seems to be broken. While reaching quite good\
          \ results with the \"airoboros-l2-7b-2.1\" Model, this one puts some random\
          \ Code at the end of the gererated text.\r\n{code}\r\nPrompt:\r\nmy_prompt\
          \ = '''A chat. You are to take the role of: Hans\r\nHans is 27 years old\
          \ from Hamburg.\r\n\r\nUSER:\r\nHey from where are you?\r\n\r\nASSISTANT:\r\
          \n'''\r\ninput_ids = tokenizer(f\"{my_prompt}\", return_tensors=\"pt\").input_ids\
          \  # .to(\"cuda\")\r\noutputs = model.generate(input_ids, max_new_tokens=128,\
          \ min_length=8, temperature=0.95, do_sample=True)\r\nprint(tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True))\r\n{/code}\r\nResult:\r\n----\r\nASSISTANT:\r\
          \nHey, I'm from Hamburg! It's a beautiful city with lots of historical buildings\
          \ and picturesque canals.\r\n package com.linkedin.cubert.utils\r\n\r\n\
          import java.nio.file.{Path, Paths}\r\n\r\ntrait CubertPaths extends Utils\
          \ {\r\n  def getCubertHDFSOptions(output: Path, maxPartitions: Int, maxMemoryInMB:\
          \ Long): CubertHdfsUtils.CubertHDFSOptions = {\r\n    CubertHdfsUtils.CubertHDFSOptions.\r\
          \n\r\n\r\n\r\n------\r\nASSISTANT:\r\nHey! I'm currently in Hamburg, Germany.\
          \ It's a beautiful city by the water with lots of historic buildings and\
          \ modern attractions. I highly recommend visiting if you ever get the chance!\r\
          \n #include \"test-framework/test-framework.h\"\r\n\r\n// -----------------------------------------------------------------------\r\
          \n// Tests\r\n//\r\n\r\nTEST(test_1, test_add_sub) {\r\n  std::complex<int>\
          \ c0, c1;\r\n  c0 = 3 + 5 * I;\r\n  c1 = 6 - 3 * I\r\n\r\n\r\n---------\r\
          \n\r\nASSISTANT:\r\nGuten Tag! Ich komme aus Hamburg, da wo der Elbe flie\xDF\
          t.\r\n(Translation: Good day! I come from Hamburg, where the Elbe flows.)\r\
          \n <?php\r\n\r\nnamespace App\\Http\\Controllers\\Api;\r\n\r\nuse App\\\
          Models\\Project;\r\nuse Illuminate\\Support\\Facades\\Storage;\r\nuse Illuminate\\\
          Http\\Request;\r\nuse App\\Http\\Controllers\\Controller;\r\n\r\nclass ProjectController\
          \ extends Controller\r\n{\r\n    public function index()\r\n    {\r\n  \
          \      $data = Project::all();\r\n        return response"
        updatedAt: '2023-09-11T13:42:24.925Z'
      numEdits: 0
      reactions: []
    id: 64ff1940ec5252dfebdb21f8
    type: comment
  author: ch4tTester
  content: "This Version of the Model seems to be broken. While reaching quite good\
    \ results with the \"airoboros-l2-7b-2.1\" Model, this one puts some random Code\
    \ at the end of the gererated text.\r\n{code}\r\nPrompt:\r\nmy_prompt = '''A chat.\
    \ You are to take the role of: Hans\r\nHans is 27 years old from Hamburg.\r\n\r\
    \nUSER:\r\nHey from where are you?\r\n\r\nASSISTANT:\r\n'''\r\ninput_ids = tokenizer(f\"\
    {my_prompt}\", return_tensors=\"pt\").input_ids  # .to(\"cuda\")\r\noutputs =\
    \ model.generate(input_ids, max_new_tokens=128, min_length=8, temperature=0.95,\
    \ do_sample=True)\r\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\r\
    \n{/code}\r\nResult:\r\n----\r\nASSISTANT:\r\nHey, I'm from Hamburg! It's a beautiful\
    \ city with lots of historical buildings and picturesque canals.\r\n package com.linkedin.cubert.utils\r\
    \n\r\nimport java.nio.file.{Path, Paths}\r\n\r\ntrait CubertPaths extends Utils\
    \ {\r\n  def getCubertHDFSOptions(output: Path, maxPartitions: Int, maxMemoryInMB:\
    \ Long): CubertHdfsUtils.CubertHDFSOptions = {\r\n    CubertHdfsUtils.CubertHDFSOptions.\r\
    \n\r\n\r\n\r\n------\r\nASSISTANT:\r\nHey! I'm currently in Hamburg, Germany.\
    \ It's a beautiful city by the water with lots of historic buildings and modern\
    \ attractions. I highly recommend visiting if you ever get the chance!\r\n #include\
    \ \"test-framework/test-framework.h\"\r\n\r\n// -----------------------------------------------------------------------\r\
    \n// Tests\r\n//\r\n\r\nTEST(test_1, test_add_sub) {\r\n  std::complex<int> c0,\
    \ c1;\r\n  c0 = 3 + 5 * I;\r\n  c1 = 6 - 3 * I\r\n\r\n\r\n---------\r\n\r\nASSISTANT:\r\
    \nGuten Tag! Ich komme aus Hamburg, da wo der Elbe flie\xDFt.\r\n(Translation:\
    \ Good day! I come from Hamburg, where the Elbe flows.)\r\n <?php\r\n\r\nnamespace\
    \ App\\Http\\Controllers\\Api;\r\n\r\nuse App\\Models\\Project;\r\nuse Illuminate\\\
    Support\\Facades\\Storage;\r\nuse Illuminate\\Http\\Request;\r\nuse App\\Http\\\
    Controllers\\Controller;\r\n\r\nclass ProjectController extends Controller\r\n\
    {\r\n    public function index()\r\n    {\r\n        $data = Project::all();\r\
    \n        return response"
  created_at: 2023-09-11 12:42:24+00:00
  edited: false
  hidden: false
  id: 64ff1940ec5252dfebdb21f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-09-11T13:50:46.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9872286319732666
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>Let me check and get back to you.  I''ve made it private for the
          time-being until I can verify.</p>

          '
        raw: Let me check and get back to you.  I've made it private for the time-being
          until I can verify.
        updatedAt: '2023-09-11T13:50:46.630Z'
      numEdits: 0
      reactions: []
    id: 64ff1b36d3b76fd7b4d5f2cb
    type: comment
  author: jondurbin
  content: Let me check and get back to you.  I've made it private for the time-being
    until I can verify.
  created_at: 2023-09-11 12:50:46+00:00
  edited: false
  hidden: false
  id: 64ff1b36d3b76fd7b4d5f2cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-09-11T15:29:36.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9919634461402893
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>I think it was just missing the <code>tokenizer.json</code> file.  Please
          download that file and try again.  I was able to reproduce the error without
          this file, but once I downloaded it everything worked as expected.</p>

          '
        raw: I think it was just missing the `tokenizer.json` file.  Please download
          that file and try again.  I was able to reproduce the error without this
          file, but once I downloaded it everything worked as expected.
        updatedAt: '2023-09-11T15:29:36.871Z'
      numEdits: 0
      reactions: []
    id: 64ff326010cf6ef0ce1eaf4c
    type: comment
  author: jondurbin
  content: I think it was just missing the `tokenizer.json` file.  Please download
    that file and try again.  I was able to reproduce the error without this file,
    but once I downloaded it everything worked as expected.
  created_at: 2023-09-11 14:29:36+00:00
  edited: false
  hidden: false
  id: 64ff326010cf6ef0ce1eaf4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/33d814e75abf78a6420e899939dab0d0.svg
      fullname: Torben Wendt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ch4tTester
      type: user
    createdAt: '2023-09-12T05:53:23.000Z'
    data:
      edited: false
      editors:
      - ch4tTester
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8097225427627563
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/33d814e75abf78a6420e899939dab0d0.svg
          fullname: Torben Wendt
          isHf: false
          isPro: false
          name: ch4tTester
          type: user
        html: '<p>No. Still not working correctly for me.</p>

          <p>A chat. You are to take the role of: Hans<br>Hans is 27 years old from
          Hamburg.</p>

          <p>USER:<br>Hey from where are you?</p>

          <p>ASSISTANT:<br>Hey, I am from Hamburg! The city known for its harbor and
          red light district. A cosmopolitan city with beautiful waterways and bridges
          - a place where the modern blending together with traditional German charm.<br>
          import { RouterModule, Routes } from ''@angular/router'';</p>

          <p>const appRoutes: Routes = [<br>  {<br>    path: ''admin'',<br>    loadChildren:
          () =&gt; import(''./admin/admin.module'')<br>  }<br>];</p>

          <p>export const RouterConfig = RouterModule.forRoot(appRoutes);</p>

          '
        raw: "No. Still not working correctly for me.\n\nA chat. You are to take the\
          \ role of: Hans\nHans is 27 years old from Hamburg.\n\nUSER:\nHey from where\
          \ are you?\n\nASSISTANT:\nHey, I am from Hamburg! The city known for its\
          \ harbor and red light district. A cosmopolitan city with beautiful waterways\
          \ and bridges - a place where the modern blending together with traditional\
          \ German charm.\n import { RouterModule, Routes } from '@angular/router';\n\
          \n\nconst appRoutes: Routes = [\n  {\n    path: 'admin',\n    loadChildren:\
          \ () => import('./admin/admin.module')\n  }\n];\n\nexport const RouterConfig\
          \ = RouterModule.forRoot(appRoutes);\n"
        updatedAt: '2023-09-12T05:53:23.949Z'
      numEdits: 0
      reactions: []
    id: 64fffcd318830fabea44798d
    type: comment
  author: ch4tTester
  content: "No. Still not working correctly for me.\n\nA chat. You are to take the\
    \ role of: Hans\nHans is 27 years old from Hamburg.\n\nUSER:\nHey from where are\
    \ you?\n\nASSISTANT:\nHey, I am from Hamburg! The city known for its harbor and\
    \ red light district. A cosmopolitan city with beautiful waterways and bridges\
    \ - a place where the modern blending together with traditional German charm.\n\
    \ import { RouterModule, Routes } from '@angular/router';\n\n\nconst appRoutes:\
    \ Routes = [\n  {\n    path: 'admin',\n    loadChildren: () => import('./admin/admin.module')\n\
    \  }\n];\n\nexport const RouterConfig = RouterModule.forRoot(appRoutes);\n"
  created_at: 2023-09-12 04:53:23+00:00
  edited: false
  hidden: false
  id: 64fffcd318830fabea44798d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/33d814e75abf78a6420e899939dab0d0.svg
      fullname: Torben Wendt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ch4tTester
      type: user
    createdAt: '2023-09-12T06:56:25.000Z'
    data:
      edited: false
      editors:
      - ch4tTester
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.828872561454773
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/33d814e75abf78a6420e899939dab0d0.svg
          fullname: Torben Wendt
          isHf: false
          isPro: false
          name: ch4tTester
          type: user
        html: '<p>I am having the same problem with a lot of the other versions.<br>airoboros-l2-13b-2.2<br>airoboros-l2-7b-2.2<br>spicyboros-13b-2.2<br>spicyboros-7b-2.2</p>

          <p>The only one working for me as intended is airoboros-l2-7b-2.1</p>

          '
        raw: 'I am having the same problem with a lot of the other versions.

          airoboros-l2-13b-2.2

          airoboros-l2-7b-2.2

          spicyboros-13b-2.2

          spicyboros-7b-2.2


          The only one working for me as intended is airoboros-l2-7b-2.1'
        updatedAt: '2023-09-12T06:56:25.045Z'
      numEdits: 0
      reactions: []
    id: 65000b99f322f91566624ff5
    type: comment
  author: ch4tTester
  content: 'I am having the same problem with a lot of the other versions.

    airoboros-l2-13b-2.2

    airoboros-l2-7b-2.2

    spicyboros-13b-2.2

    spicyboros-7b-2.2


    The only one working for me as intended is airoboros-l2-7b-2.1'
  created_at: 2023-09-12 05:56:25+00:00
  edited: false
  hidden: false
  id: 65000b99f322f91566624ff5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-09-12T07:06:21.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.865297794342041
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>Can you try locking your transformers library to version <code>transformers==4.31.0</code>
          ?</p>

          '
        raw: Can you try locking your transformers library to version `transformers==4.31.0`
          ?
        updatedAt: '2023-09-12T07:06:21.122Z'
      numEdits: 0
      reactions: []
    id: 65000dedea5699b59a8cf374
    type: comment
  author: jondurbin
  content: Can you try locking your transformers library to version `transformers==4.31.0`
    ?
  created_at: 2023-09-12 06:06:21+00:00
  edited: false
  hidden: false
  id: 65000dedea5699b59a8cf374
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-09-12T07:31:52.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5213245749473572
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: "<p>It seems transformers ~ 4.33 is not stopping generation at the EOS\
          \ token ID.  I printed the token IDs here:</p>\n<pre><code>tensor([  319,\
          \ 13563, 29889,   887,   526,   304,  2125,   278,  6297,   310,\n     \
          \   29901,  6971,    13, 29950,   550,   338, 29871, 29906, 29955,  2440,\n\
          \         2030,   515, 14847, 29889,    13,    13, 11889, 29901, 18637,\
          \   515,\n          988,   526,   366, 29973,    13, 22933,  9047, 13566,\
          \ 29901, 29871,\n        18637, 29892,   306, 29915, 29885,  5279,   297,\
          \ 14847, 29892,  9556,\n        29889,   739, 29915, 29879,   263,  9560,\
          \  4272,   411,  8261,  4955,\n          322,  9257, 29889,  6975,   366,\
          \  1063,  1244,  1434, 29973,    13,\n            2,     1, 29871,  ...\n\
          </code></pre>\n<p>The EOS token ID is 2, so it should have stopped, but\
          \ it kept generating.</p>\n<p>You can continue using transformers ~4.33\
          \ by setting the eos_token_id in generate.</p>\n<p>Full example:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">import</span> transformers\n\
          <span class=\"hljs-keyword\">import</span> torch\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \    <span class=\"hljs-string\">'/workspace/airoboros-l2-13b-2.2'</span>,\n\
          \    device_map=<span class=\"hljs-string\">'auto'</span>,\n    torch_dtype=torch.bfloat16\n\
          )\ntokenizer = transformers.AutoTokenizer.from_pretrained(\n    <span class=\"\
          hljs-string\">'/workspace/airoboros-l2-13b-2.2'</span>\n)\n\nprompt = <span\
          \ class=\"hljs-string\">'''A chat. You are to take the role of: Hans</span>\n\
          <span class=\"hljs-string\">Hans is 27 years old from Hamburg.</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">USER: Hey\
          \ from where are you?</span>\n<span class=\"hljs-string\">ASSISTANT: '''</span>\n\
          inputs = tokenizer(prompt, add_special_tokens=<span class=\"hljs-literal\"\
          >False</span>, return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n\
          input_ids = inputs.input_ids.to(<span class=\"hljs-string\">\"cuda\"</span>)\n\
          \noutputs = model.generate(\n    input_ids,\n    max_new_tokens=<span class=\"\
          hljs-number\">128</span>,\n    min_length=<span class=\"hljs-number\">8</span>,\n\
          \    temperature=<span class=\"hljs-number\">0.95</span>,\n    do_sample=<span\
          \ class=\"hljs-literal\">True</span>,\n    attention_mask=inputs.attention_mask,\n\
          \    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id\n\
          )\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(outputs[<span\
          \ class=\"hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>))\n</code></pre>\n<p>Also note the slight change to whitespacing\
          \ between my example and your original prompt.</p>\n"
        raw: "It seems transformers ~ 4.33 is not stopping generation at the EOS token\
          \ ID.  I printed the token IDs here:\n```\ntensor([  319, 13563, 29889,\
          \   887,   526,   304,  2125,   278,  6297,   310,\n        29901,  6971,\
          \    13, 29950,   550,   338, 29871, 29906, 29955,  2440,\n         2030,\
          \   515, 14847, 29889,    13,    13, 11889, 29901, 18637,   515,\n     \
          \     988,   526,   366, 29973,    13, 22933,  9047, 13566, 29901, 29871,\n\
          \        18637, 29892,   306, 29915, 29885,  5279,   297, 14847, 29892,\
          \  9556,\n        29889,   739, 29915, 29879,   263,  9560,  4272,   411,\
          \  8261,  4955,\n          322,  9257, 29889,  6975,   366,  1063,  1244,\
          \  1434, 29973,    13,\n            2,     1, 29871,  ...\n```\n\nThe EOS\
          \ token ID is 2, so it should have stopped, but it kept generating.\n\n\
          You can continue using transformers ~4.33 by setting the eos_token_id in\
          \ generate.\n\nFull example:\n```python\nimport transformers\nimport torch\n\
          \nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    '/workspace/airoboros-l2-13b-2.2',\n\
          \    device_map='auto',\n    torch_dtype=torch.bfloat16\n)\ntokenizer =\
          \ transformers.AutoTokenizer.from_pretrained(\n    '/workspace/airoboros-l2-13b-2.2'\n\
          )\n\nprompt = '''A chat. You are to take the role of: Hans\nHans is 27 years\
          \ old from Hamburg.\n\nUSER: Hey from where are you?\nASSISTANT: '''\ninputs\
          \ = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\")\n\
          input_ids = inputs.input_ids.to(\"cuda\")\n\noutputs = model.generate(\n\
          \    input_ids,\n    max_new_tokens=128,\n    min_length=8,\n    temperature=0.95,\n\
          \    do_sample=True,\n    attention_mask=inputs.attention_mask,\n    pad_token_id=tokenizer.pad_token_id,\n\
          \    eos_token_id=tokenizer.eos_token_id\n)\nprint(tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True))\n```\n\nAlso note the slight change to whitespacing\
          \ between my example and your original prompt."
        updatedAt: '2023-09-12T07:31:52.401Z'
      numEdits: 0
      reactions: []
    id: 650013e818830fabea47ed3d
    type: comment
  author: jondurbin
  content: "It seems transformers ~ 4.33 is not stopping generation at the EOS token\
    \ ID.  I printed the token IDs here:\n```\ntensor([  319, 13563, 29889,   887,\
    \   526,   304,  2125,   278,  6297,   310,\n        29901,  6971,    13, 29950,\
    \   550,   338, 29871, 29906, 29955,  2440,\n         2030,   515, 14847, 29889,\
    \    13,    13, 11889, 29901, 18637,   515,\n          988,   526,   366, 29973,\
    \    13, 22933,  9047, 13566, 29901, 29871,\n        18637, 29892,   306, 29915,\
    \ 29885,  5279,   297, 14847, 29892,  9556,\n        29889,   739, 29915, 29879,\
    \   263,  9560,  4272,   411,  8261,  4955,\n          322,  9257, 29889,  6975,\
    \   366,  1063,  1244,  1434, 29973,    13,\n            2,     1, 29871,  ...\n\
    ```\n\nThe EOS token ID is 2, so it should have stopped, but it kept generating.\n\
    \nYou can continue using transformers ~4.33 by setting the eos_token_id in generate.\n\
    \nFull example:\n```python\nimport transformers\nimport torch\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
    \    '/workspace/airoboros-l2-13b-2.2',\n    device_map='auto',\n    torch_dtype=torch.bfloat16\n\
    )\ntokenizer = transformers.AutoTokenizer.from_pretrained(\n    '/workspace/airoboros-l2-13b-2.2'\n\
    )\n\nprompt = '''A chat. You are to take the role of: Hans\nHans is 27 years old\
    \ from Hamburg.\n\nUSER: Hey from where are you?\nASSISTANT: '''\ninputs = tokenizer(prompt,\
    \ add_special_tokens=False, return_tensors=\"pt\")\ninput_ids = inputs.input_ids.to(\"\
    cuda\")\n\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=128,\n\
    \    min_length=8,\n    temperature=0.95,\n    do_sample=True,\n    attention_mask=inputs.attention_mask,\n\
    \    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id\n\
    )\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nAlso\
    \ note the slight change to whitespacing between my example and your original\
    \ prompt."
  created_at: 2023-09-12 06:31:52+00:00
  edited: false
  hidden: false
  id: 650013e818830fabea47ed3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-09-12T07:47:05.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7789776921272278
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>Actually, if you just pull the latest copy of <code>generate_config.json</code>,
          which explicitly states the pad/eos token IDs, it should fix it.</p>

          '
        raw: Actually, if you just pull the latest copy of `generate_config.json`,
          which explicitly states the pad/eos token IDs, it should fix it.
        updatedAt: '2023-09-12T07:47:05.766Z'
      numEdits: 0
      reactions: []
    id: 650017791e14749e84e88e9d
    type: comment
  author: jondurbin
  content: Actually, if you just pull the latest copy of `generate_config.json`, which
    explicitly states the pad/eos token IDs, it should fix it.
  created_at: 2023-09-12 06:47:05+00:00
  edited: false
  hidden: false
  id: 650017791e14749e84e88e9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/33d814e75abf78a6420e899939dab0d0.svg
      fullname: Torben Wendt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ch4tTester
      type: user
    createdAt: '2023-09-12T08:20:38.000Z'
    data:
      edited: false
      editors:
      - ch4tTester
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7586820125579834
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/33d814e75abf78a6420e899939dab0d0.svg
          fullname: Torben Wendt
          isHf: false
          isPro: false
          name: ch4tTester
          type: user
        html: "<p>Alright. Thank you very much. With the updated  <code>generate_config.json</code>\
          \ it\xB4s working!</p>\n"
        raw: "Alright. Thank you very much. With the updated  `generate_config.json`\
          \ it\xB4s working!"
        updatedAt: '2023-09-12T08:20:38.960Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jondurbin
    id: 65001f567aa4d88f206d0e80
    type: comment
  author: ch4tTester
  content: "Alright. Thank you very much. With the updated  `generate_config.json`\
    \ it\xB4s working!"
  created_at: 2023-09-12 07:20:38+00:00
  edited: false
  hidden: false
  id: 65001f567aa4d88f206d0e80
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: jondurbin/airoboros-l2-13b-2.2
repo_type: model
status: open
target_branch: null
title: Broken?
