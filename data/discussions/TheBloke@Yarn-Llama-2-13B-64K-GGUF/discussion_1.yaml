!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MichaelBui
conflicting_files: null
created_at: 2023-09-01 08:12:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8790ed4b203b1fd78e650e06596e27f5.svg
      fullname: Michael Bui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MichaelBui
      type: user
    createdAt: '2023-09-01T09:12:17.000Z'
    data:
      edited: false
      editors:
      - MichaelBui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9341803789138794
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8790ed4b203b1fd78e650e06596e27f5.svg
          fullname: Michael Bui
          isHf: false
          isPro: false
          name: MichaelBui
          type: user
        html: '<p>Sorry if this is a dumb question but I''ve heard that 360GB RAM
          is required for a full 128k context of this 13B model, how much does it
          require for a full 64k context? Is the <code>10.37 GB</code> requirement
          for the <code>Q4_K_M</code> quantized version still valid?</p>

          '
        raw: Sorry if this is a dumb question but I've heard that 360GB RAM is required
          for a full 128k context of this 13B model, how much does it require for
          a full 64k context? Is the `10.37 GB` requirement for the `Q4_K_M` quantized
          version still valid?
        updatedAt: '2023-09-01T09:12:17.313Z'
      numEdits: 0
      reactions: []
    id: 64f1aaf1e35f2c694bfc1ddf
    type: comment
  author: MichaelBui
  content: Sorry if this is a dumb question but I've heard that 360GB RAM is required
    for a full 128k context of this 13B model, how much does it require for a full
    64k context? Is the `10.37 GB` requirement for the `Q4_K_M` quantized version
    still valid?
  created_at: 2023-09-01 08:12:17+00:00
  edited: false
  hidden: false
  id: 64f1aaf1e35f2c694bfc1ddf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-01T09:24:17.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9371935129165649
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Good question. That RAM requirement is set statically according
          to the size of the model. I have not measured RAM requirements at extended
          context.</p>

          <p>I''ll do a quick test now</p>

          '
        raw: 'Good question. That RAM requirement is set statically according to the
          size of the model. I have not measured RAM requirements at extended context.


          I''ll do a quick test now'
        updatedAt: '2023-09-01T09:24:17.366Z'
      numEdits: 0
      reactions: []
    id: 64f1adc18f45e5f6ee19ec1b
    type: comment
  author: TheBloke
  content: 'Good question. That RAM requirement is set statically according to the
    size of the model. I have not measured RAM requirements at extended context.


    I''ll do a quick test now'
  created_at: 2023-09-01 08:24:17+00:00
  edited: false
  hidden: false
  id: 64f1adc18f45e5f6ee19ec1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-01T10:07:13.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.963748037815094
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK, I tested Llama-2-7B-64K with <code>-c 65736</code> and an input
          file containing plain text which I generated from Transformers <code>tokenizer.decode(..)</code>
          from 64K wikitext tokens.  (I set <code>-c 65736</code> to allow 200 tokens
          of reply, not that I got that far.)</p>

          <p>RAM usage quickly went to 80GB - this was shortly after I started the
          command:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/EqzH1KJ65FMMLAe_Y8Ntx.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/EqzH1KJ65FMMLAe_Y8Ntx.png"></a></p>

          <p>At about 20% through the prompt , it was using 87GB RAM, so a growth
          of 7GB over 20% prompt ingestion.  At 30%, it was at 96GB.  At 45%, 107GB.  At
          51%, 115G.</p>

          <p>It was also using VRAM at the same time, and eventually went OOM at 60%
          through the prompt.  I didn''t tell it to use the GPU, I had <code>-ngl
          0</code>, so no weights were cached.  But I guess it was using cuBLAS for
          prompt ingestion.</p>

          <p>So I don''t know the final RAM figure, and I don''t know if RAM usage
          would have been even higher without cuBLAS on a 48GB GPU.</p>

          <p>But getting some very rough figures: </p>

          <p>It used an additional 3.5G RAM per 10% of the prompt at 20% through,  then
          5.3GB per 10% at 30%, and 7GB per 10% at 50% of the prompt.  So if by 100%
          it were using 14GB per 10%, total RAM usage would be 220GB for 7B 64k.  Though
          maybe it''d be even higher than that.</p>

          <p>I probably don''t have those figures right, but we can definitely see
          that RAM usage per token is increasing as it gets further, which is expected
          without flash attention; quadratic RAM/VRAM growth as context increases.</p>

          <p>So yeah I could definitely believe 360GB for 13B at 128k context!</p>

          '
        raw: "OK, I tested Llama-2-7B-64K with `-c 65736` and an input file containing\
          \ plain text which I generated from Transformers `tokenizer.decode(..)`\
          \ from 64K wikitext tokens.  (I set `-c 65736` to allow 200 tokens of reply,\
          \ not that I got that far.)\n\nRAM usage quickly went to 80GB - this was\
          \ shortly after I started the command:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/EqzH1KJ65FMMLAe_Y8Ntx.png)\n\
          \nAt about 20% through the prompt , it was using 87GB RAM, so a growth of\
          \ 7GB over 20% prompt ingestion.  At 30%, it was at 96GB.  At 45%, 107GB.\
          \  At 51%, 115G.\n\nIt was also using VRAM at the same time, and eventually\
          \ went OOM at 60% through the prompt.  I didn't tell it to use the GPU,\
          \ I had `-ngl 0`, so no weights were cached.  But I guess it was using cuBLAS\
          \ for prompt ingestion.\n\nSo I don't know the final RAM figure, and I don't\
          \ know if RAM usage would have been even higher without cuBLAS on a 48GB\
          \ GPU.\n\nBut getting some very rough figures: \n\nIt used an additional\
          \ 3.5G RAM per 10% of the prompt at 20% through,  then 5.3GB per 10% at\
          \ 30%, and 7GB per 10% at 50% of the prompt.  So if by 100% it were using\
          \ 14GB per 10%, total RAM usage would be 220GB for 7B 64k.  Though maybe\
          \ it'd be even higher than that.\n\nI probably don't have those figures\
          \ right, but we can definitely see that RAM usage per token is increasing\
          \ as it gets further, which is expected without flash attention; quadratic\
          \ RAM/VRAM growth as context increases.\n\nSo yeah I could definitely believe\
          \ 360GB for 13B at 128k context!"
        updatedAt: '2023-09-01T10:13:39.361Z'
      numEdits: 3
      reactions:
      - count: 7
        reaction: "\u2764\uFE0F"
        users:
        - MichaelBui
        - SergiusFlavius
        - shafiqalibhai
        - Raspbfox
        - BDARUI2
        - PrimeD
        - GabrielZhu
      - count: 2
        reaction: "\U0001F44D"
        users:
        - shafiqalibhai
        - Raspbfox
      - count: 1
        reaction: "\U0001F92F"
        users:
        - felipelo
    id: 64f1b7d10b27861b2cb6e956
    type: comment
  author: TheBloke
  content: "OK, I tested Llama-2-7B-64K with `-c 65736` and an input file containing\
    \ plain text which I generated from Transformers `tokenizer.decode(..)` from 64K\
    \ wikitext tokens.  (I set `-c 65736` to allow 200 tokens of reply, not that I\
    \ got that far.)\n\nRAM usage quickly went to 80GB - this was shortly after I\
    \ started the command:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/EqzH1KJ65FMMLAe_Y8Ntx.png)\n\
    \nAt about 20% through the prompt , it was using 87GB RAM, so a growth of 7GB\
    \ over 20% prompt ingestion.  At 30%, it was at 96GB.  At 45%, 107GB.  At 51%,\
    \ 115G.\n\nIt was also using VRAM at the same time, and eventually went OOM at\
    \ 60% through the prompt.  I didn't tell it to use the GPU, I had `-ngl 0`, so\
    \ no weights were cached.  But I guess it was using cuBLAS for prompt ingestion.\n\
    \nSo I don't know the final RAM figure, and I don't know if RAM usage would have\
    \ been even higher without cuBLAS on a 48GB GPU.\n\nBut getting some very rough\
    \ figures: \n\nIt used an additional 3.5G RAM per 10% of the prompt at 20% through,\
    \  then 5.3GB per 10% at 30%, and 7GB per 10% at 50% of the prompt.  So if by\
    \ 100% it were using 14GB per 10%, total RAM usage would be 220GB for 7B 64k.\
    \  Though maybe it'd be even higher than that.\n\nI probably don't have those\
    \ figures right, but we can definitely see that RAM usage per token is increasing\
    \ as it gets further, which is expected without flash attention; quadratic RAM/VRAM\
    \ growth as context increases.\n\nSo yeah I could definitely believe 360GB for\
    \ 13B at 128k context!"
  created_at: 2023-09-01 09:07:13+00:00
  edited: true
  hidden: false
  id: 64f1b7d10b27861b2cb6e956
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8790ed4b203b1fd78e650e06596e27f5.svg
      fullname: Michael Bui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MichaelBui
      type: user
    createdAt: '2023-09-01T10:17:03.000Z'
    data:
      edited: false
      editors:
      - MichaelBui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9705951809883118
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8790ed4b203b1fd78e650e06596e27f5.svg
          fullname: Michael Bui
          isHf: false
          isPro: false
          name: MichaelBui
          type: user
        html: '<p>Thanks for the detailed info. So if I have about 24GB RAM, I won''t
          be able to load this (since your initial RAM is already 80GB)?<br>I think
          I will stick with the original Llama2 with 4k context for now.</p>

          '
        raw: "Thanks for the detailed info. So if I have about 24GB RAM, I won't be\
          \ able to load this (since your initial RAM is already 80GB)? \nI think\
          \ I will stick with the original Llama2 with 4k context for now."
        updatedAt: '2023-09-01T10:17:03.648Z'
      numEdits: 0
      reactions: []
    id: 64f1ba1f83d5256f7488fa51
    type: comment
  author: MichaelBui
  content: "Thanks for the detailed info. So if I have about 24GB RAM, I won't be\
    \ able to load this (since your initial RAM is already 80GB)? \nI think I will\
    \ stick with the original Llama2 with 4k context for now."
  created_at: 2023-09-01 09:17:03+00:00
  edited: false
  hidden: false
  id: 64f1ba1f83d5256f7488fa51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
      fullname: Rajesh Kumar V
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RajeshkumarV
      type: user
    createdAt: '2023-09-06T18:09:55.000Z'
    data:
      edited: false
      editors:
      - RajeshkumarV
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7121718525886536
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
          fullname: Rajesh Kumar V
          isHf: false
          isPro: false
          name: RajeshkumarV
          type: user
        html: '<p>Is there  a 4k context version of the model in the GGUF format pls?
          llama2</p>

          '
        raw: Is there  a 4k context version of the model in the GGUF format pls? llama2
        updatedAt: '2023-09-06T18:09:55.307Z'
      numEdits: 0
      reactions: []
    id: 64f8c0738a234f114e351818
    type: comment
  author: RajeshkumarV
  content: Is there  a 4k context version of the model in the GGUF format pls? llama2
  created_at: 2023-09-06 17:09:55+00:00
  edited: false
  hidden: false
  id: 64f8c0738a234f114e351818
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Yarn-Llama-2-13B-64K-GGUF
repo_type: model
status: open
target_branch: null
title: RAM required for full 64k context?
