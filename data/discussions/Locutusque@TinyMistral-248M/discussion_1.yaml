!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Jcuhfehl
conflicting_files: null
created_at: 2023-11-18 21:41:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/740c430a1ed06b65995cba494ca8b299.svg
      fullname: Freya Winters
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jcuhfehl
      type: user
    createdAt: '2023-11-18T21:41:12.000Z'
    data:
      edited: false
      editors:
      - Jcuhfehl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9626490473747253
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/740c430a1ed06b65995cba494ca8b299.svg
          fullname: Freya Winters
          isHf: false
          isPro: false
          name: Jcuhfehl
          type: user
        html: '<p>Hi, I wanted to know how this model was created. Is it a sheared/pruned
          version of mistral 7b, or is it a from-scratch model that shares the same
          architecture?</p>

          '
        raw: Hi, I wanted to know how this model was created. Is it a sheared/pruned
          version of mistral 7b, or is it a from-scratch model that shares the same
          architecture?
        updatedAt: '2023-11-18T21:41:12.211Z'
      numEdits: 0
      reactions: []
    id: 65592f78efc0fb7bed84defc
    type: comment
  author: Jcuhfehl
  content: Hi, I wanted to know how this model was created. Is it a sheared/pruned
    version of mistral 7b, or is it a from-scratch model that shares the same architecture?
  created_at: 2023-11-18 21:41:12+00:00
  edited: false
  hidden: false
  id: 65592f78efc0fb7bed84defc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-11-18T23:57:42.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9702292084693909
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<p>Hey there!<br>This model is trained from scratch with the same architecture
          as Mistral. I''ve done this as an attempt to demonstrate that trillion-scale
          datasets are not absolutely necessary to pretrain language models, and as
          a result, they can be trained on a single GPU.</p>

          '
        raw: 'Hey there!

          This model is trained from scratch with the same architecture as Mistral.
          I''ve done this as an attempt to demonstrate that trillion-scale datasets
          are not absolutely necessary to pretrain language models, and as a result,
          they can be trained on a single GPU.'
        updatedAt: '2023-11-18T23:57:42.542Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Sigmally
        - afrideva
    id: 65594f763aff9efaad325528
    type: comment
  author: Locutusque
  content: 'Hey there!

    This model is trained from scratch with the same architecture as Mistral. I''ve
    done this as an attempt to demonstrate that trillion-scale datasets are not absolutely
    necessary to pretrain language models, and as a result, they can be trained on
    a single GPU.'
  created_at: 2023-11-18 23:57:42+00:00
  edited: false
  hidden: false
  id: 65594f763aff9efaad325528
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1cbf678175545f00f626febd1c87e11.svg
      fullname: soupslurpr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: soupslurpr
      type: user
    createdAt: '2023-11-19T03:47:16.000Z'
    data:
      edited: false
      editors:
      - soupslurpr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9958178400993347
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1cbf678175545f00f626febd1c87e11.svg
          fullname: soupslurpr
          isHf: false
          isPro: false
          name: soupslurpr
          type: user
        html: '<p>I''m interested, what GPU was this trained on and how long did it
          take?</p>

          '
        raw: I'm interested, what GPU was this trained on and how long did it take?
        updatedAt: '2023-11-19T03:47:16.045Z'
      numEdits: 0
      reactions: []
    id: 6559854451ebd26dbc24e2d3
    type: comment
  author: soupslurpr
  content: I'm interested, what GPU was this trained on and how long did it take?
  created_at: 2023-11-19 03:47:16+00:00
  edited: false
  hidden: false
  id: 6559854451ebd26dbc24e2d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ddca2b56096411db44768195289aa0d.svg
      fullname: ai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: artificialgenerations4gsdfg
      type: user
    createdAt: '2023-11-19T04:09:05.000Z'
    data:
      edited: false
      editors:
      - artificialgenerations4gsdfg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9264684915542603
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ddca2b56096411db44768195289aa0d.svg
          fullname: ai
          isHf: false
          isPro: false
          name: artificialgenerations4gsdfg
          type: user
        html: '<p>I''m also interested. Cool model!</p>

          '
        raw: 'I''m also interested. Cool model!

          '
        updatedAt: '2023-11-19T04:09:05.493Z'
      numEdits: 0
      reactions: []
    id: 65598a613fc7998474786b51
    type: comment
  author: artificialgenerations4gsdfg
  content: 'I''m also interested. Cool model!

    '
  created_at: 2023-11-19 04:09:05+00:00
  edited: false
  hidden: false
  id: 65598a613fc7998474786b51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-11-19T04:09:49.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9780672192573547
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<p>I actually have not finished training it yet, and I''m estimating
          around 5 more training days/sessions.<br>I''ve been using a single Titan
          V (which is a budget V100 if you don''t know what it is)</p>

          '
        raw: 'I actually have not finished training it yet, and I''m estimating around
          5 more training days/sessions.

          I''ve been using a single Titan V (which is a budget V100 if you don''t
          know what it is)'
        updatedAt: '2023-11-19T04:09:49.357Z'
      numEdits: 0
      reactions: []
    id: 65598a8d1c419bb072619eea
    type: comment
  author: Locutusque
  content: 'I actually have not finished training it yet, and I''m estimating around
    5 more training days/sessions.

    I''ve been using a single Titan V (which is a budget V100 if you don''t know what
    it is)'
  created_at: 2023-11-19 04:09:49+00:00
  edited: false
  hidden: false
  id: 65598a8d1c419bb072619eea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-11-19T04:11:49.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9894964098930359
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<p>So far it has taken about 48-72 GPU hours to train, which in comparison
          to other models, is very good.</p>

          '
        raw: So far it has taken about 48-72 GPU hours to train, which in comparison
          to other models, is very good.
        updatedAt: '2023-11-19T04:11:49.105Z'
      numEdits: 0
      reactions: []
    id: 65598b0523e43ac218cdc966
    type: comment
  author: Locutusque
  content: So far it has taken about 48-72 GPU hours to train, which in comparison
    to other models, is very good.
  created_at: 2023-11-19 04:11:49+00:00
  edited: false
  hidden: false
  id: 65598b0523e43ac218cdc966
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1cbf678175545f00f626febd1c87e11.svg
      fullname: soupslurpr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: soupslurpr
      type: user
    createdAt: '2023-11-19T04:14:00.000Z'
    data:
      edited: false
      editors:
      - soupslurpr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9865978360176086
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1cbf678175545f00f626febd1c87e11.svg
          fullname: soupslurpr
          isHf: false
          isPro: false
          name: soupslurpr
          type: user
        html: '<p>Thanks, do you think it would be possible to do it on a rtx 4080?
          Or is it too weak</p>

          '
        raw: Thanks, do you think it would be possible to do it on a rtx 4080? Or
          is it too weak
        updatedAt: '2023-11-19T04:14:00.333Z'
      numEdits: 0
      reactions: []
    id: 65598b888762c448a110a37d
    type: comment
  author: soupslurpr
  content: Thanks, do you think it would be possible to do it on a rtx 4080? Or is
    it too weak
  created_at: 2023-11-19 04:14:00+00:00
  edited: false
  hidden: false
  id: 65598b888762c448a110a37d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1cbf678175545f00f626febd1c87e11.svg
      fullname: soupslurpr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: soupslurpr
      type: user
    createdAt: '2023-11-19T04:14:20.000Z'
    data:
      edited: false
      editors:
      - soupslurpr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8624343872070312
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1cbf678175545f00f626febd1c87e11.svg
          fullname: soupslurpr
          isHf: false
          isPro: false
          name: soupslurpr
          type: user
        html: '<p>Also interested what framework your using to train it</p>

          '
        raw: Also interested what framework your using to train it
        updatedAt: '2023-11-19T04:14:20.170Z'
      numEdits: 0
      reactions: []
    id: 65598b9c86fbe7506ee4d6f7
    type: comment
  author: soupslurpr
  content: Also interested what framework your using to train it
  created_at: 2023-11-19 04:14:20+00:00
  edited: false
  hidden: false
  id: 65598b9c86fbe7506ee4d6f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-11-19T04:19:52.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9682359099388123
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<blockquote>

          <p>Thanks, do you think it would be possible to do it on a rtx 4080? Or
          is it too weak</p>

          </blockquote>

          <p>The RTX 4080 should for sure be enough for it, you may be able to train
          a model that is significantly better than this one with the RTX 4080</p>

          <blockquote>

          <p>Also interested what framework your using to train it</p>

          </blockquote>

          <p>I''m using PyTorch to train this model.</p>

          '
        raw: '> Thanks, do you think it would be possible to do it on a rtx 4080?
          Or is it too weak


          The RTX 4080 should for sure be enough for it, you may be able to train
          a model that is significantly better than this one with the RTX 4080


          > Also interested what framework your using to train it


          I''m using PyTorch to train this model.'
        updatedAt: '2023-11-19T04:19:52.308Z'
      numEdits: 0
      reactions: []
    id: 65598ce851ebd26dbc25d98e
    type: comment
  author: Locutusque
  content: '> Thanks, do you think it would be possible to do it on a rtx 4080? Or
    is it too weak


    The RTX 4080 should for sure be enough for it, you may be able to train a model
    that is significantly better than this one with the RTX 4080


    > Also interested what framework your using to train it


    I''m using PyTorch to train this model.'
  created_at: 2023-11-19 04:19:52+00:00
  edited: false
  hidden: false
  id: 65598ce851ebd26dbc25d98e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1cbf678175545f00f626febd1c87e11.svg
      fullname: soupslurpr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: soupslurpr
      type: user
    createdAt: '2023-11-19T04:22:41.000Z'
    data:
      edited: false
      editors:
      - soupslurpr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9049432873725891
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1cbf678175545f00f626febd1c87e11.svg
          fullname: soupslurpr
          isHf: false
          isPro: false
          name: soupslurpr
          type: user
        html: '<p>Any specific library or your own written training code?</p>

          '
        raw: Any specific library or your own written training code?
        updatedAt: '2023-11-19T04:22:41.133Z'
      numEdits: 0
      reactions: []
    id: 65598d91bf604a773248f85a
    type: comment
  author: soupslurpr
  content: Any specific library or your own written training code?
  created_at: 2023-11-19 04:22:41+00:00
  edited: false
  hidden: false
  id: 65598d91bf604a773248f85a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-11-19T04:31:57.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9520339965820312
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<blockquote>

          <p>Any specific library or your own written training code?</p>

          </blockquote>

          <p>I only use wandb, transformers (to load the model and pick up where I
          left off), and maybe a few others for utils. I write my own training and
          evaluation loop.</p>

          '
        raw: '> Any specific library or your own written training code?


          I only use wandb, transformers (to load the model and pick up where I left
          off), and maybe a few others for utils. I write my own training and evaluation
          loop.'
        updatedAt: '2023-11-19T04:31:57.508Z'
      numEdits: 0
      reactions: []
    id: 65598fbda75de9cef1a1447d
    type: comment
  author: Locutusque
  content: '> Any specific library or your own written training code?


    I only use wandb, transformers (to load the model and pick up where I left off),
    and maybe a few others for utils. I write my own training and evaluation loop.'
  created_at: 2023-11-19 04:31:57+00:00
  edited: false
  hidden: false
  id: 65598fbda75de9cef1a1447d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1cbf678175545f00f626febd1c87e11.svg
      fullname: soupslurpr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: soupslurpr
      type: user
    createdAt: '2023-11-19T04:33:58.000Z'
    data:
      edited: false
      editors:
      - soupslurpr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9563722610473633
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1cbf678175545f00f626febd1c87e11.svg
          fullname: soupslurpr
          isHf: false
          isPro: false
          name: soupslurpr
          type: user
        html: '<p>Would you please consider open sourcing the code? I want to mess
          with it :D</p>

          '
        raw: Would you please consider open sourcing the code? I want to mess with
          it :D
        updatedAt: '2023-11-19T04:33:58.988Z'
      numEdits: 0
      reactions: []
    id: 65599036da4acab10a79316e
    type: comment
  author: soupslurpr
  content: Would you please consider open sourcing the code? I want to mess with it
    :D
  created_at: 2023-11-19 04:33:58+00:00
  edited: false
  hidden: false
  id: 65599036da4acab10a79316e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-11-19T04:35:07.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9889770746231079
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<blockquote>

          <p>Would you please consider open sourcing the code? I want to mess with
          it :D</p>

          </blockquote>

          <p>I''ll consider it, I don''t see why I shouldn''t other than the fact
          it''s super messy and is probably difficult to read.</p>

          '
        raw: '> Would you please consider open sourcing the code? I want to mess with
          it :D


          I''ll consider it, I don''t see why I shouldn''t other than the fact it''s
          super messy and is probably difficult to read.'
        updatedAt: '2023-11-19T04:35:07.002Z'
      numEdits: 0
      reactions: []
    id: 6559907bcafc48de36366391
    type: comment
  author: Locutusque
  content: '> Would you please consider open sourcing the code? I want to mess with
    it :D


    I''ll consider it, I don''t see why I shouldn''t other than the fact it''s super
    messy and is probably difficult to read.'
  created_at: 2023-11-19 04:35:07+00:00
  edited: false
  hidden: false
  id: 6559907bcafc48de36366391
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-11-19T04:40:44.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9894888997077942
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Would you please consider open sourcing the code? I want to mess with
          it :D</p>

          </blockquote>

          <p>I''ll consider it, I don''t see why I shouldn''t other than the fact
          it''s super messy and is probably difficult to read.</p>

          </blockquote>

          <p>I''ll open source it, I''ll also make sure to clean it up a bit. I don''t
          think you''ll be able to see it until Wednesday or Thursday because I am
          not home at the moment (I don''t have the code uploaded anywhere, only at
          an external drive).</p>

          '
        raw: "> > Would you please consider open sourcing the code? I want to mess\
          \ with it :D\n> \n> I'll consider it, I don't see why I shouldn't other\
          \ than the fact it's super messy and is probably difficult to read.\n\n\
          I'll open source it, I'll also make sure to clean it up a bit. I don't think\
          \ you'll be able to see it until Wednesday or Thursday because I am not\
          \ home at the moment (I don't have the code uploaded anywhere, only at an\
          \ external drive)."
        updatedAt: '2023-11-19T04:40:44.088Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - Felladrin
        - artificialgenerations4gsdfg
        - Sigmally
        - afrideva
    id: 655991ccb9fcdeff51a0b911
    type: comment
  author: Locutusque
  content: "> > Would you please consider open sourcing the code? I want to mess with\
    \ it :D\n> \n> I'll consider it, I don't see why I shouldn't other than the fact\
    \ it's super messy and is probably difficult to read.\n\nI'll open source it,\
    \ I'll also make sure to clean it up a bit. I don't think you'll be able to see\
    \ it until Wednesday or Thursday because I am not home at the moment (I don't\
    \ have the code uploaded anywhere, only at an external drive)."
  created_at: 2023-11-19 04:40:44+00:00
  edited: false
  hidden: false
  id: 655991ccb9fcdeff51a0b911
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1cbf678175545f00f626febd1c87e11.svg
      fullname: soupslurpr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: soupslurpr
      type: user
    createdAt: '2023-11-19T04:41:39.000Z'
    data:
      edited: false
      editors:
      - soupslurpr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.898610532283783
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1cbf678175545f00f626febd1c87e11.svg
          fullname: soupslurpr
          isHf: false
          isPro: false
          name: soupslurpr
          type: user
        html: "<p>Thank you \u2764 I'll be patient, take your time</p>\n"
        raw: "Thank you \u2764 I'll be patient, take your time"
        updatedAt: '2023-11-19T04:41:39.899Z'
      numEdits: 0
      reactions: []
    id: 65599203180749bf32df0240
    type: comment
  author: soupslurpr
  content: "Thank you \u2764 I'll be patient, take your time"
  created_at: 2023-11-19 04:41:39+00:00
  edited: false
  hidden: false
  id: 65599203180749bf32df0240
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-11-19T04:45:32.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9725815653800964
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: "<blockquote>\n<p>Thank you \u2764 I'll be patient, take your time</p>\n\
          </blockquote>\n<p>Yeah of course! I'll reply to this letting you know that\
          \ I've uploaded it here on this repository, and I'll also create a github\
          \ repo.</p>\n"
        raw: "> Thank you \u2764 I'll be patient, take your time\n\nYeah of course!\
          \ I'll reply to this letting you know that I've uploaded it here on this\
          \ repository, and I'll also create a github repo."
        updatedAt: '2023-11-19T04:45:32.844Z'
      numEdits: 0
      reactions: []
    id: 655992ec7f246643395e0e4e
    type: comment
  author: Locutusque
  content: "> Thank you \u2764 I'll be patient, take your time\n\nYeah of course!\
    \ I'll reply to this letting you know that I've uploaded it here on this repository,\
    \ and I'll also create a github repo."
  created_at: 2023-11-19 04:45:32+00:00
  edited: false
  hidden: false
  id: 655992ec7f246643395e0e4e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-11-21T00:18:47.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9287648797035217
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: "<blockquote>\n<p>Thank you \u2764 I'll be patient, take your time</p>\n\
          </blockquote>\n<p>It's your lucky day, turns out I actually had the training\
          \ script lying in the depths of github.<br><a rel=\"nofollow\" href=\"https://github.com/Locutusque/TinyMistral-train-eval\"\
          >https://github.com/Locutusque/TinyMistral-train-eval</a><br>You can find\
          \ the notebooks on this github. Please create an issue if you find problems\
          \ with it.</p>\n"
        raw: "> Thank you \u2764 I'll be patient, take your time\n\nIt's your lucky\
          \ day, turns out I actually had the training script lying in the depths\
          \ of github.\nhttps://github.com/Locutusque/TinyMistral-train-eval\nYou\
          \ can find the notebooks on this github. Please create an issue if you find\
          \ problems with it."
        updatedAt: '2023-11-21T00:18:47.175Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - soupslurpr
        - Felladrin
        - Sigmally
        - afrideva
      - count: 1
        reaction: "\U0001F917"
        users:
        - Sigmally
    id: 655bf767cfe086853e3223fc
    type: comment
  author: Locutusque
  content: "> Thank you \u2764 I'll be patient, take your time\n\nIt's your lucky\
    \ day, turns out I actually had the training script lying in the depths of github.\n\
    https://github.com/Locutusque/TinyMistral-train-eval\nYou can find the notebooks\
    \ on this github. Please create an issue if you find problems with it."
  created_at: 2023-11-21 00:18:47+00:00
  edited: false
  hidden: false
  id: 655bf767cfe086853e3223fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6929d6f17e3ba91a764ab5d38164fd59.svg
      fullname: IO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sigmally
      type: user
    createdAt: '2023-11-24T10:05:17.000Z'
    data:
      edited: false
      editors:
      - Sigmally
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.954546332359314
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6929d6f17e3ba91a764ab5d38164fd59.svg
          fullname: IO
          isHf: false
          isPro: false
          name: Sigmally
          type: user
        html: '<p>Hello, I am very curious about the results your experiment. Could
          you share more details about the efficiency and performance of this approach?<br>I
          have also looked through your code on GitHub (<a rel="nofollow" href="https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb">https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb</a>),
          which, as I understand, is designed to create a base model for subsequent
          fine-tuning. I noticed that the Mistral-7B model uses attention mechanisms
          such as grouped-query attention (GQA) and sliding window attention (SWA).
          As I am just beginning my journey with language models and primarily have
          theoretical knowledge (there is not much of it so far), I find these techniques
          very interesting. Unfortunately, I was unable to locate in your code where
          these aforementioned mechanisms (GQA. SWA) used in Mistral-7B are defined.
          Could you indicate where they can be found? I would greatly appreciate your
          help.</p>

          '
        raw: 'Hello, I am very curious about the results your experiment. Could you
          share more details about the efficiency and performance of this approach?

          I have also looked through your code on GitHub (https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb),
          which, as I understand, is designed to create a base model for subsequent
          fine-tuning. I noticed that the Mistral-7B model uses attention mechanisms
          such as grouped-query attention (GQA) and sliding window attention (SWA).
          As I am just beginning my journey with language models and primarily have
          theoretical knowledge (there is not much of it so far), I find these techniques
          very interesting. Unfortunately, I was unable to locate in your code where
          these aforementioned mechanisms (GQA. SWA) used in Mistral-7B are defined.
          Could you indicate where they can be found? I would greatly appreciate your
          help.'
        updatedAt: '2023-11-24T10:05:17.410Z'
      numEdits: 0
      reactions: []
    id: 6560755d5f13eb6efa3fdd81
    type: comment
  author: Sigmally
  content: 'Hello, I am very curious about the results your experiment. Could you
    share more details about the efficiency and performance of this approach?

    I have also looked through your code on GitHub (https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb),
    which, as I understand, is designed to create a base model for subsequent fine-tuning.
    I noticed that the Mistral-7B model uses attention mechanisms such as grouped-query
    attention (GQA) and sliding window attention (SWA). As I am just beginning my
    journey with language models and primarily have theoretical knowledge (there is
    not much of it so far), I find these techniques very interesting. Unfortunately,
    I was unable to locate in your code where these aforementioned mechanisms (GQA.
    SWA) used in Mistral-7B are defined. Could you indicate where they can be found?
    I would greatly appreciate your help.'
  created_at: 2023-11-24 10:05:17+00:00
  edited: false
  hidden: false
  id: 6560755d5f13eb6efa3fdd81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-11-24T16:42:32.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9373063445091248
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<blockquote>

          <p>Hello, I am very curious about the results your experiment. Could you
          share more details about the efficiency and performance of this approach?<br>I
          have also looked through your code on GitHub (<a rel="nofollow" href="https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb">https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb</a>),
          which, as I understand, is designed to create a base model for subsequent
          fine-tuning. I noticed that the Mistral-7B model uses attention mechanisms
          such as grouped-query attention (GQA) and sliding window attention (SWA).
          As I am just beginning my journey with language models and primarily have
          theoretical knowledge (there is not much of it so far), I find these techniques
          very interesting. Unfortunately, I was unable to locate in your code where
          these aforementioned mechanisms (GQA. SWA) used in Mistral-7B are defined.
          Could you indicate where they can be found? I would greatly appreciate your
          help.</p>

          </blockquote>

          <p>Hello,</p>

          <p>I am not the individual who originally coded the attention mechanisms.
          Instead, I am developing a new base model that mirrors the architecture
          of Mistral-7B. The corresponding code is accessible on the Hugging Face
          Transformers GitHub repository at <a rel="nofollow" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py">https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py</a>.
          Within this code, you should find two distinct attention mechanisms; however,
          I may have the names slightly incorrect. I believe the names are something
          like"MistralAttention" and "MistralFlashAttention."Regarding the efficiency
          and performance, I am not yet able to come up with a valid conclusion; the
          evaluation that you see on the model card is outdated. That evaluation was
          based on the 2-million-example model. The current one has not been evaluated
          yet, and I am hopeful that the performance has improved. I do invite you
          to do some human-based evaluations to help make a conclusion though! (You
          can make a pull request if you decide to do this)<br>I hope this helps!</p>

          '
        raw: '> Hello, I am very curious about the results your experiment. Could
          you share more details about the efficiency and performance of this approach?

          > I have also looked through your code on GitHub (https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb),
          which, as I understand, is designed to create a base model for subsequent
          fine-tuning. I noticed that the Mistral-7B model uses attention mechanisms
          such as grouped-query attention (GQA) and sliding window attention (SWA).
          As I am just beginning my journey with language models and primarily have
          theoretical knowledge (there is not much of it so far), I find these techniques
          very interesting. Unfortunately, I was unable to locate in your code where
          these aforementioned mechanisms (GQA. SWA) used in Mistral-7B are defined.
          Could you indicate where they can be found? I would greatly appreciate your
          help.


          Hello,


          I am not the individual who originally coded the attention mechanisms. Instead,
          I am developing a new base model that mirrors the architecture of Mistral-7B.
          The corresponding code is accessible on the Hugging Face Transformers GitHub
          repository at https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py.
          Within this code, you should find two distinct attention mechanisms; however,
          I may have the names slightly incorrect. I believe the names are something
          like"MistralAttention" and "MistralFlashAttention."Regarding the efficiency
          and performance, I am not yet able to come up with a valid conclusion; the
          evaluation that you see on the model card is outdated. That evaluation was
          based on the 2-million-example model. The current one has not been evaluated
          yet, and I am hopeful that the performance has improved. I do invite you
          to do some human-based evaluations to help make a conclusion though! (You
          can make a pull request if you decide to do this)

          I hope this helps!'
        updatedAt: '2023-11-24T16:42:32.686Z'
      numEdits: 0
      reactions: []
    id: 6560d278e0a7720b6addcf8c
    type: comment
  author: Locutusque
  content: '> Hello, I am very curious about the results your experiment. Could you
    share more details about the efficiency and performance of this approach?

    > I have also looked through your code on GitHub (https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb),
    which, as I understand, is designed to create a base model for subsequent fine-tuning.
    I noticed that the Mistral-7B model uses attention mechanisms such as grouped-query
    attention (GQA) and sliding window attention (SWA). As I am just beginning my
    journey with language models and primarily have theoretical knowledge (there is
    not much of it so far), I find these techniques very interesting. Unfortunately,
    I was unable to locate in your code where these aforementioned mechanisms (GQA.
    SWA) used in Mistral-7B are defined. Could you indicate where they can be found?
    I would greatly appreciate your help.


    Hello,


    I am not the individual who originally coded the attention mechanisms. Instead,
    I am developing a new base model that mirrors the architecture of Mistral-7B.
    The corresponding code is accessible on the Hugging Face Transformers GitHub repository
    at https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py.
    Within this code, you should find two distinct attention mechanisms; however,
    I may have the names slightly incorrect. I believe the names are something like"MistralAttention"
    and "MistralFlashAttention."Regarding the efficiency and performance, I am not
    yet able to come up with a valid conclusion; the evaluation that you see on the
    model card is outdated. That evaluation was based on the 2-million-example model.
    The current one has not been evaluated yet, and I am hopeful that the performance
    has improved. I do invite you to do some human-based evaluations to help make
    a conclusion though! (You can make a pull request if you decide to do this)

    I hope this helps!'
  created_at: 2023-11-24 16:42:32+00:00
  edited: false
  hidden: false
  id: 6560d278e0a7720b6addcf8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6929d6f17e3ba91a764ab5d38164fd59.svg
      fullname: IO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sigmally
      type: user
    createdAt: '2023-11-25T08:48:37.000Z'
    data:
      edited: false
      editors:
      - Sigmally
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9142735004425049
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6929d6f17e3ba91a764ab5d38164fd59.svg
          fullname: IO
          isHf: false
          isPro: false
          name: Sigmally
          type: user
        html: '<p>Thank you for link. I didn''t know where to find the code for the
          Mistral model''s mechanisms; it clarified a lot for me. I have another question
          about the code (<a rel="nofollow" href="https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb">https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb</a>).
          In the main function, the model is loaded using <code>AutoModelForCausalLM.from_pretrained(args.model)</code>,
          where args.model is set to <code>"Locutusque/TinyMistral-248M"</code>.<br>Does
          this <code>from_pretrained</code> method retrieve a pre-trained model from
          the Hugging Face repository? After the model is loaded, the code adapts
          it to a specific dataset and task. I''m not sure if I understand correctly
          (If it''s otherwise, please correct me), but it seems that the code does
          not create a new model architecture from scratch (a randomly initiated set
          of weights), but rather relies on a pre-trained model provided by Hugging
          Face from your repository, which I believe includes a file named <code>model.safetensors</code>.
          This file is loaded before the training process, and the file in your repository
          is not empty and has a size of about 900 MB. Is there a way to initialize
          this model with random weights so that it starts ''clean''?</p>

          '
        raw: 'Thank you for link. I didn''t know where to find the code for the Mistral
          model''s mechanisms; it clarified a lot for me. I have another question
          about the code (https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb).
          In the main function, the model is loaded using `AutoModelForCausalLM.from_pretrained(args.model)`,
          where args.model is set to `"Locutusque/TinyMistral-248M"`.

          Does this `from_pretrained` method retrieve a pre-trained model from the
          Hugging Face repository? After the model is loaded, the code adapts it to
          a specific dataset and task. I''m not sure if I understand correctly (If
          it''s otherwise, please correct me), but it seems that the code does not
          create a new model architecture from scratch (a randomly initiated set of
          weights), but rather relies on a pre-trained model provided by Hugging Face
          from your repository, which I believe includes a file named `model.safetensors`.
          This file is loaded before the training process, and the file in your repository
          is not empty and has a size of about 900 MB. Is there a way to initialize
          this model with random weights so that it starts ''clean''?'
        updatedAt: '2023-11-25T08:48:37.473Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - maywell
    id: 6561b4e5d5e3c35c79fd2060
    type: comment
  author: Sigmally
  content: 'Thank you for link. I didn''t know where to find the code for the Mistral
    model''s mechanisms; it clarified a lot for me. I have another question about
    the code (https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb).
    In the main function, the model is loaded using `AutoModelForCausalLM.from_pretrained(args.model)`,
    where args.model is set to `"Locutusque/TinyMistral-248M"`.

    Does this `from_pretrained` method retrieve a pre-trained model from the Hugging
    Face repository? After the model is loaded, the code adapts it to a specific dataset
    and task. I''m not sure if I understand correctly (If it''s otherwise, please
    correct me), but it seems that the code does not create a new model architecture
    from scratch (a randomly initiated set of weights), but rather relies on a pre-trained
    model provided by Hugging Face from your repository, which I believe includes
    a file named `model.safetensors`. This file is loaded before the training process,
    and the file in your repository is not empty and has a size of about 900 MB. Is
    there a way to initialize this model with random weights so that it starts ''clean''?'
  created_at: 2023-11-25 08:48:37+00:00
  edited: false
  hidden: false
  id: 6561b4e5d5e3c35c79fd2060
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-11-25T17:55:53.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8954533934593201
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<blockquote>

          <p>Thank you for link. I didn''t know where to find the code for the Mistral
          model''s mechanisms; it clarified a lot for me. I have another question
          about the code (<a rel="nofollow" href="https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb">https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb</a>).
          In the main function, the model is loaded using <code>AutoModelForCausalLM.from_pretrained(args.model)</code>,
          where args.model is set to <code>"Locutusque/TinyMistral-248M"</code>.<br>Does
          this <code>from_pretrained</code> method retrieve a pre-trained model from
          the Hugging Face repository? After the model is loaded, the code adapts
          it to a specific dataset and task. I''m not sure if I understand correctly
          (If it''s otherwise, please correct me), but it seems that the code does
          not create a new model architecture from scratch (a randomly initiated set
          of weights), but rather relies on a pre-trained model provided by Hugging
          Face from your repository, which I believe includes a file named <code>model.safetensors</code>.
          This file is loaded before the training process, and the file in your repository
          is not empty and has a size of about 900 MB. Is there a way to initialize
          this model with random weights so that it starts ''clean''?</p>

          </blockquote>

          <p>Yes and no to your question. The model is loaded from scratch, but that
          file you mentioned is a state dictionary that contains all of the weights
          and biases of the pretrained model, and then the <code>model.safetensors</code>
          file is applied to the weights and biases of the randomly initialized model.</p>

          <p>To load a model with random weights, you can choose a model class, such
          as MistralForCausalLM, and the corresponding config class (in this case
          it would be MistralConfig). Here''s an example:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> MistralConfig, MistralForCausalLM


          config = MistralConfig(...) <span class="hljs-comment"># Define model parameters</span>


          model = MistralForCausalLM(config) <span class="hljs-comment"># Load the
          model with the config</span>

          </code></pre>

          '
        raw: '> Thank you for link. I didn''t know where to find the code for the
          Mistral model''s mechanisms; it clarified a lot for me. I have another question
          about the code (https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb).
          In the main function, the model is loaded using `AutoModelForCausalLM.from_pretrained(args.model)`,
          where args.model is set to `"Locutusque/TinyMistral-248M"`.

          > Does this `from_pretrained` method retrieve a pre-trained model from the
          Hugging Face repository? After the model is loaded, the code adapts it to
          a specific dataset and task. I''m not sure if I understand correctly (If
          it''s otherwise, please correct me), but it seems that the code does not
          create a new model architecture from scratch (a randomly initiated set of
          weights), but rather relies on a pre-trained model provided by Hugging Face
          from your repository, which I believe includes a file named `model.safetensors`.
          This file is loaded before the training process, and the file in your repository
          is not empty and has a size of about 900 MB. Is there a way to initialize
          this model with random weights so that it starts ''clean''?


          Yes and no to your question. The model is loaded from scratch, but that
          file you mentioned is a state dictionary that contains all of the weights
          and biases of the pretrained model, and then the `model.safetensors` file
          is applied to the weights and biases of the randomly initialized model.


          To load a model with random weights, you can choose a model class, such
          as MistralForCausalLM, and the corresponding config class (in this case
          it would be MistralConfig). Here''s an example:


          ```python

          from transformers import MistralConfig, MistralForCausalLM


          config = MistralConfig(...) # Define model parameters


          model = MistralForCausalLM(config) # Load the model with the config


          ```'
        updatedAt: '2023-11-25T17:55:53.977Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - Sigmally
        - maywell
        - afrideva
    id: 656235296573ea070841f83c
    type: comment
  author: Locutusque
  content: '> Thank you for link. I didn''t know where to find the code for the Mistral
    model''s mechanisms; it clarified a lot for me. I have another question about
    the code (https://github.com/Locutusque/TinyMistral-train-eval/blob/main/locutusque-s-train-eval.ipynb).
    In the main function, the model is loaded using `AutoModelForCausalLM.from_pretrained(args.model)`,
    where args.model is set to `"Locutusque/TinyMistral-248M"`.

    > Does this `from_pretrained` method retrieve a pre-trained model from the Hugging
    Face repository? After the model is loaded, the code adapts it to a specific dataset
    and task. I''m not sure if I understand correctly (If it''s otherwise, please
    correct me), but it seems that the code does not create a new model architecture
    from scratch (a randomly initiated set of weights), but rather relies on a pre-trained
    model provided by Hugging Face from your repository, which I believe includes
    a file named `model.safetensors`. This file is loaded before the training process,
    and the file in your repository is not empty and has a size of about 900 MB. Is
    there a way to initialize this model with random weights so that it starts ''clean''?


    Yes and no to your question. The model is loaded from scratch, but that file you
    mentioned is a state dictionary that contains all of the weights and biases of
    the pretrained model, and then the `model.safetensors` file is applied to the
    weights and biases of the randomly initialized model.


    To load a model with random weights, you can choose a model class, such as MistralForCausalLM,
    and the corresponding config class (in this case it would be MistralConfig). Here''s
    an example:


    ```python

    from transformers import MistralConfig, MistralForCausalLM


    config = MistralConfig(...) # Define model parameters


    model = MistralForCausalLM(config) # Load the model with the config


    ```'
  created_at: 2023-11-25 17:55:53+00:00
  edited: false
  hidden: false
  id: 656235296573ea070841f83c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6929d6f17e3ba91a764ab5d38164fd59.svg
      fullname: IO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sigmally
      type: user
    createdAt: '2023-11-27T13:49:24.000Z'
    data:
      edited: false
      editors:
      - Sigmally
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9854015707969666
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6929d6f17e3ba91a764ab5d38164fd59.svg
          fullname: IO
          isHf: false
          isPro: false
          name: Sigmally
          type: user
        html: '<p>How long did the model training process take, could you share that
          information?</p>

          '
        raw: How long did the model training process take, could you share that information?
        updatedAt: '2023-11-27T13:49:24.736Z'
      numEdits: 0
      reactions: []
    id: 65649e647579f424d923c82f
    type: comment
  author: Sigmally
  content: How long did the model training process take, could you share that information?
  created_at: 2023-11-27 13:49:24+00:00
  edited: false
  hidden: false
  id: 65649e647579f424d923c82f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-11-27T15:59:57.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9908430576324463
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<blockquote>

          <p>How long did the model training process take, could you share that information?</p>

          </blockquote>

          <p>The training process took about 5-6 days of effective training time.</p>

          '
        raw: '> How long did the model training process take, could you share that
          information?


          The training process took about 5-6 days of effective training time.'
        updatedAt: '2023-11-27T15:59:57.164Z'
      numEdits: 0
      reactions: []
    id: 6564bcfd3640137c1a91f508
    type: comment
  author: Locutusque
  content: '> How long did the model training process take, could you share that information?


    The training process took about 5-6 days of effective training time.'
  created_at: 2023-11-27 15:59:57+00:00
  edited: false
  hidden: false
  id: 6564bcfd3640137c1a91f508
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7e1c6a14d79eb7fe7b64eed7127a0ca.svg
      fullname: Lucifer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LuciferYagami
      type: user
    createdAt: '2023-12-02T15:44:44.000Z'
    data:
      edited: false
      editors:
      - LuciferYagami
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9617316126823425
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7e1c6a14d79eb7fe7b64eed7127a0ca.svg
          fullname: Lucifer
          isHf: false
          isPro: false
          name: LuciferYagami
          type: user
        html: '<p>what are your thoughts on finetuning this model on a RTX3060 12GB
          VRAM. I don''t plan it to run all day. Can I run it in batches? I mean like
          how much time does it take for a single epoch? Maybe I can do 1 epoch a
          day?</p>

          '
        raw: what are your thoughts on finetuning this model on a RTX3060 12GB VRAM.
          I don't plan it to run all day. Can I run it in batches? I mean like how
          much time does it take for a single epoch? Maybe I can do 1 epoch a day?
        updatedAt: '2023-12-02T15:44:44.311Z'
      numEdits: 0
      reactions: []
    id: 656b50ec09d123964b9d8ab7
    type: comment
  author: LuciferYagami
  content: what are your thoughts on finetuning this model on a RTX3060 12GB VRAM.
    I don't plan it to run all day. Can I run it in batches? I mean like how much
    time does it take for a single epoch? Maybe I can do 1 epoch a day?
  created_at: 2023-12-02 15:44:44+00:00
  edited: false
  hidden: false
  id: 656b50ec09d123964b9d8ab7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-12-02T15:59:45.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9707939028739929
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<blockquote>

          <p>what are your thoughts on finetuning this model on a RTX3060 12GB VRAM.
          I don''t plan it to run all day. Can I run it in batches? I mean like how
          much time does it take for a single epoch? Maybe I can do 1 epoch a day?</p>

          </blockquote>

          <p>You can certainly do that, although the RTX 3060 has less compute power
          than the Titan V, you can indeed train in batches that you described.</p>

          '
        raw: '> what are your thoughts on finetuning this model on a RTX3060 12GB
          VRAM. I don''t plan it to run all day. Can I run it in batches? I mean like
          how much time does it take for a single epoch? Maybe I can do 1 epoch a
          day?


          You can certainly do that, although the RTX 3060 has less compute power
          than the Titan V, you can indeed train in batches that you described.'
        updatedAt: '2023-12-02T15:59:45.943Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - LuciferYagami
    id: 656b54719c8778992f76f9d1
    type: comment
  author: Locutusque
  content: '> what are your thoughts on finetuning this model on a RTX3060 12GB VRAM.
    I don''t plan it to run all day. Can I run it in batches? I mean like how much
    time does it take for a single epoch? Maybe I can do 1 epoch a day?


    You can certainly do that, although the RTX 3060 has less compute power than the
    Titan V, you can indeed train in batches that you described.'
  created_at: 2023-12-02 15:59:45+00:00
  edited: false
  hidden: false
  id: 656b54719c8778992f76f9d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7e1c6a14d79eb7fe7b64eed7127a0ca.svg
      fullname: Lucifer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LuciferYagami
      type: user
    createdAt: '2023-12-02T16:45:36.000Z'
    data:
      edited: true
      editors:
      - LuciferYagami
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9912279844284058
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7e1c6a14d79eb7fe7b64eed7127a0ca.svg
          fullname: Lucifer
          isHf: false
          isPro: false
          name: LuciferYagami
          type: user
        html: '<p>How many epochs was Titan V able to achieve in a day? I mean it
          surely depends the data your training on, but just want to know the figure.
          And how do you rate the model performance. Being 248M parameter model, like
          how''s it performance?</p>

          '
        raw: How many epochs was Titan V able to achieve in a day? I mean it surely
          depends the data your training on, but just want to know the figure. And
          how do you rate the model performance. Being 248M parameter model, like
          how's it performance?
        updatedAt: '2023-12-02T16:46:52.852Z'
      numEdits: 1
      reactions: []
    id: 656b5f3002a56b531a714367
    type: comment
  author: LuciferYagami
  content: How many epochs was Titan V able to achieve in a day? I mean it surely
    depends the data your training on, but just want to know the figure. And how do
    you rate the model performance. Being 248M parameter model, like how's it performance?
  created_at: 2023-12-02 16:45:36+00:00
  edited: true
  hidden: false
  id: 656b5f3002a56b531a714367
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-12-02T17:37:37.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9689553380012512
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<blockquote>

          <p>How many epochs was Titan V able to achieve in a day? I mean it surely
          depends the data your training on, but just want to know the figure. And
          how do you rate the model performance. Being 248M parameter model, like
          how''s it performance?</p>

          </blockquote>

          <p>It depends on the amount of examples I was doing. On average, it would
          do around 200k steps per day, which equates to around 1 million examples.</p>

          '
        raw: '> How many epochs was Titan V able to achieve in a day? I mean it surely
          depends the data your training on, but just want to know the figure. And
          how do you rate the model performance. Being 248M parameter model, like
          how''s it performance?


          It depends on the amount of examples I was doing. On average, it would do
          around 200k steps per day, which equates to around 1 million examples.'
        updatedAt: '2023-12-02T17:37:37.689Z'
      numEdits: 0
      reactions: []
    id: 656b6b61801ed9952f7f278b
    type: comment
  author: Locutusque
  content: '> How many epochs was Titan V able to achieve in a day? I mean it surely
    depends the data your training on, but just want to know the figure. And how do
    you rate the model performance. Being 248M parameter model, like how''s it performance?


    It depends on the amount of examples I was doing. On average, it would do around
    200k steps per day, which equates to around 1 million examples.'
  created_at: 2023-12-02 17:37:37+00:00
  edited: false
  hidden: false
  id: 656b6b61801ed9952f7f278b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-12-02T17:38:26.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9559279084205627
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<blockquote>

          <p>How many epochs was Titan V able to achieve in a day? I mean it surely
          depends the data your training on, but just want to know the figure. And
          how do you rate the model performance. Being 248M parameter model, like
          how''s it performance?</p>

          </blockquote>

          <p>For the amount of parameters it has, I consider it to have good performance,
          especially on the amount of training data it was trained on.</p>

          '
        raw: '> How many epochs was Titan V able to achieve in a day? I mean it surely
          depends the data your training on, but just want to know the figure. And
          how do you rate the model performance. Being 248M parameter model, like
          how''s it performance?


          For the amount of parameters it has, I consider it to have good performance,
          especially on the amount of training data it was trained on.'
        updatedAt: '2023-12-02T17:38:26.968Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - LuciferYagami
        - Sigmally
    id: 656b6b921f8d9b618d8a11a1
    type: comment
  author: Locutusque
  content: '> How many epochs was Titan V able to achieve in a day? I mean it surely
    depends the data your training on, but just want to know the figure. And how do
    you rate the model performance. Being 248M parameter model, like how''s it performance?


    For the amount of parameters it has, I consider it to have good performance, especially
    on the amount of training data it was trained on.'
  created_at: 2023-12-02 17:38:26+00:00
  edited: false
  hidden: false
  id: 656b6b921f8d9b618d8a11a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7e1c6a14d79eb7fe7b64eed7127a0ca.svg
      fullname: Lucifer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LuciferYagami
      type: user
    createdAt: '2023-12-03T05:34:45.000Z'
    data:
      edited: false
      editors:
      - LuciferYagami
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9873216152191162
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7e1c6a14d79eb7fe7b64eed7127a0ca.svg
          fullname: Lucifer
          isHf: false
          isPro: false
          name: LuciferYagami
          type: user
        html: '<p>Cool, thats really good to know. Will try it out. What would be
          a good starting point to get started with Finetuning in general? Any suggestions?</p>

          '
        raw: Cool, thats really good to know. Will try it out. What would be a good
          starting point to get started with Finetuning in general? Any suggestions?
        updatedAt: '2023-12-03T05:34:45.570Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Sigmally
    id: 656c13750a5ec499aad47f74
    type: comment
  author: LuciferYagami
  content: Cool, thats really good to know. Will try it out. What would be a good
    starting point to get started with Finetuning in general? Any suggestions?
  created_at: 2023-12-03 05:34:45+00:00
  edited: false
  hidden: false
  id: 656c13750a5ec499aad47f74
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-12-03T06:25:47.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9406576752662659
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: '<blockquote>

          <p>Cool, thats really good to know. Will try it out. What would be a good
          starting point to get started with Finetuning in general? Any suggestions?</p>

          </blockquote>

          <p>You could consider fine-tuning it on a dataset like Alpaca or OpenPlatypus
          with LoRA. If you want to do more intense fine-tuning, you could fine-tune
          all of the model parameters on a more difficult dataset like Orca, or maybe
          even my InstructMix datasets (7 - 14 million examples depending on which
          you use).</p>

          '
        raw: '> Cool, thats really good to know. Will try it out. What would be a
          good starting point to get started with Finetuning in general? Any suggestions?


          You could consider fine-tuning it on a dataset like Alpaca or OpenPlatypus
          with LoRA. If you want to do more intense fine-tuning, you could fine-tune
          all of the model parameters on a more difficult dataset like Orca, or maybe
          even my InstructMix datasets (7 - 14 million examples depending on which
          you use).'
        updatedAt: '2023-12-03T06:25:47.455Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - LuciferYagami
        - Sigmally
    id: 656c1f6b9ced9d5ff54ebe9f
    type: comment
  author: Locutusque
  content: '> Cool, thats really good to know. Will try it out. What would be a good
    starting point to get started with Finetuning in general? Any suggestions?


    You could consider fine-tuning it on a dataset like Alpaca or OpenPlatypus with
    LoRA. If you want to do more intense fine-tuning, you could fine-tune all of the
    model parameters on a more difficult dataset like Orca, or maybe even my InstructMix
    datasets (7 - 14 million examples depending on which you use).'
  created_at: 2023-12-03 06:25:47+00:00
  edited: false
  hidden: false
  id: 656c1f6b9ced9d5ff54ebe9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7e1c6a14d79eb7fe7b64eed7127a0ca.svg
      fullname: Lucifer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LuciferYagami
      type: user
    createdAt: '2023-12-03T06:58:48.000Z'
    data:
      edited: false
      editors:
      - LuciferYagami
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9393269419670105
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7e1c6a14d79eb7fe7b64eed7127a0ca.svg
          fullname: Lucifer
          isHf: false
          isPro: false
          name: LuciferYagami
          type: user
        html: '<p>Sure sure, will try it out with LoRA first, thanks for the suggestions</p>

          '
        raw: Sure sure, will try it out with LoRA first, thanks for the suggestions
        updatedAt: '2023-12-03T06:58:48.732Z'
      numEdits: 0
      reactions: []
    id: 656c2728996819a8288f6898
    type: comment
  author: LuciferYagami
  content: Sure sure, will try it out with LoRA first, thanks for the suggestions
  created_at: 2023-12-03 06:58:48+00:00
  edited: false
  hidden: false
  id: 656c2728996819a8288f6898
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Locutusque/TinyMistral-248M
repo_type: model
status: open
target_branch: null
title: How was this made?
