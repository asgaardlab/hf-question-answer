!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LuciferianInk
conflicting_files: null
created_at: 2023-11-29 19:10:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c675514e7dbb75a7ab5cfb215a3960d0.svg
      fullname: Ryan Brooks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LuciferianInk
      type: user
    createdAt: '2023-11-29T19:10:57.000Z'
    data:
      edited: false
      editors:
      - LuciferianInk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6840211749076843
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c675514e7dbb75a7ab5cfb215a3960d0.svg
          fullname: Ryan Brooks
          isHf: false
          isPro: false
          name: LuciferianInk
          type: user
        html: "<p>Somewhere between the conversion from <code>model.safetensors</code>\
          \ to <code>pytorch_model.bin</code>, this model stopped working for me:</p>\n\
          <pre><code>Traceback (most recent call last):\n  File \"/src/harness.py\"\
          , line 553, in &lt;module&gt;\n    main()\n  File \"/src/harness.py\", line\
          \ 223, in main\n    prototype = aigen(\n  File \"/src/aigen/aigen/aigen.py\"\
          , line 185, in __init__\n    self.model = AutoModelForCausalLM.from_pretrained(\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\"\
          , line 565, in from_pretrained\n    return model_class.from_pretrained(\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 3307, in from_pretrained\n    ) = cls._load_pretrained_model(\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 3695, in _load_pretrained_model\n    new_error_msgs, offload_index,\
          \ state_dict_index = _load_state_dict_into_meta_model(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 741, in _load_state_dict_into_meta_model\n    set_module_tensor_to_device(model,\
          \ param_name, param_device, **set_module_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\"\
          , line 285, in set_module_tensor_to_device\n    raise ValueError(\nValueError:\
          \ Trying to set a tensor of shape torch.Size([32005, 1024]) in \"weight\"\
          \ (which has shape torch.Size([32003, 1024])), this look incorrect.\n</code></pre>\n\
          <p>I don't know where the extra tokens are coming from, because I'm definitely\
          \ not adding them. I can use the exact same code, and just swap in any other\
          \ model, and it will load just fine (not to mention, the old version of\
          \ THIS one also worked fine). I'm not sure what changed, but it looks like\
          \ you broke something :(</p>\n"
        raw: "Somewhere between the conversion from `model.safetensors` to `pytorch_model.bin`,\
          \ this model stopped working for me:\r\n```\r\nTraceback (most recent call\
          \ last):\r\n  File \"/src/harness.py\", line 553, in <module>\r\n    main()\r\
          \n  File \"/src/harness.py\", line 223, in main\r\n    prototype = aigen(\r\
          \n  File \"/src/aigen/aigen/aigen.py\", line 185, in __init__\r\n    self.model\
          \ = AutoModelForCausalLM.from_pretrained(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\"\
          , line 565, in from_pretrained\r\n    return model_class.from_pretrained(\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 3307, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 3695, in _load_pretrained_model\r\n    new_error_msgs, offload_index,\
          \ state_dict_index = _load_state_dict_into_meta_model(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 741, in _load_state_dict_into_meta_model\r\n    set_module_tensor_to_device(model,\
          \ param_name, param_device, **set_module_kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\"\
          , line 285, in set_module_tensor_to_device\r\n    raise ValueError(\r\n\
          ValueError: Trying to set a tensor of shape torch.Size([32005, 1024]) in\
          \ \"weight\" (which has shape torch.Size([32003, 1024])), this look incorrect.\r\
          \n```\r\nI don't know where the extra tokens are coming from, because I'm\
          \ definitely not adding them. I can use the exact same code, and just swap\
          \ in any other model, and it will load just fine (not to mention, the old\
          \ version of THIS one also worked fine). I'm not sure what changed, but\
          \ it looks like you broke something :("
        updatedAt: '2023-11-29T19:10:57.705Z'
      numEdits: 0
      reactions: []
    id: 65678cc13fd0bf1f82246548
    type: comment
  author: LuciferianInk
  content: "Somewhere between the conversion from `model.safetensors` to `pytorch_model.bin`,\
    \ this model stopped working for me:\r\n```\r\nTraceback (most recent call last):\r\
    \n  File \"/src/harness.py\", line 553, in <module>\r\n    main()\r\n  File \"\
    /src/harness.py\", line 223, in main\r\n    prototype = aigen(\r\n  File \"/src/aigen/aigen/aigen.py\"\
    , line 185, in __init__\r\n    self.model = AutoModelForCausalLM.from_pretrained(\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\"\
    , line 565, in from_pretrained\r\n    return model_class.from_pretrained(\r\n\
    \  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
    , line 3307, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\",\
    \ line 3695, in _load_pretrained_model\r\n    new_error_msgs, offload_index, state_dict_index\
    \ = _load_state_dict_into_meta_model(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
    , line 741, in _load_state_dict_into_meta_model\r\n    set_module_tensor_to_device(model,\
    \ param_name, param_device, **set_module_kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\"\
    , line 285, in set_module_tensor_to_device\r\n    raise ValueError(\r\nValueError:\
    \ Trying to set a tensor of shape torch.Size([32005, 1024]) in \"weight\" (which\
    \ has shape torch.Size([32003, 1024])), this look incorrect.\r\n```\r\nI don't\
    \ know where the extra tokens are coming from, because I'm definitely not adding\
    \ them. I can use the exact same code, and just swap in any other model, and it\
    \ will load just fine (not to mention, the old version of THIS one also worked\
    \ fine). I'm not sure what changed, but it looks like you broke something :("
  created_at: 2023-11-29 19:10:57+00:00
  edited: false
  hidden: false
  id: 65678cc13fd0bf1f82246548
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-11-29T23:39:08.000Z'
    data:
      edited: false
      editors:
      - Locutusque
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7138546705245972
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
          fullname: Sebastian Gabarain
          isHf: false
          isPro: false
          name: Locutusque
          type: user
        html: "<blockquote>\n<p>Somewhere between the conversion from <code>model.safetensors</code>\
          \ to <code>pytorch_model.bin</code>, this model stopped working for me:</p>\n\
          <pre><code>Traceback (most recent call last):\n  File \"/src/harness.py\"\
          , line 553, in &lt;module&gt;\n    main()\n  File \"/src/harness.py\", line\
          \ 223, in main\n    prototype = aigen(\n  File \"/src/aigen/aigen/aigen.py\"\
          , line 185, in __init__\n    self.model = AutoModelForCausalLM.from_pretrained(\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\"\
          , line 565, in from_pretrained\n    return model_class.from_pretrained(\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 3307, in from_pretrained\n    ) = cls._load_pretrained_model(\n \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 3695, in _load_pretrained_model\n    new_error_msgs, offload_index,\
          \ state_dict_index = _load_state_dict_into_meta_model(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 741, in _load_state_dict_into_meta_model\n    set_module_tensor_to_device(model,\
          \ param_name, param_device, **set_module_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\"\
          , line 285, in set_module_tensor_to_device\n    raise ValueError(\nValueError:\
          \ Trying to set a tensor of shape torch.Size([32005, 1024]) in \"weight\"\
          \ (which has shape torch.Size([32003, 1024])), this look incorrect.\n</code></pre>\n\
          <p>I don't know where the extra tokens are coming from, because I'm definitely\
          \ not adding them. I can use the exact same code, and just swap in any other\
          \ model, and it will load just fine (not to mention, the old version of\
          \ THIS one also worked fine). I'm not sure what changed, but it looks like\
          \ you broke something :(</p>\n</blockquote>\n<p>Hello,<br>It seems I have\
          \ accidentally loaded the wrong tokenizer when further training this model.\
          \ I will upload this new tokenizer now.</p>\n"
        raw: '> Somewhere between the conversion from `model.safetensors` to `pytorch_model.bin`,
          this model stopped working for me:

          > ```

          > Traceback (most recent call last):

          >   File "/src/harness.py", line 553, in <module>

          >     main()

          >   File "/src/harness.py", line 223, in main

          >     prototype = aigen(

          >   File "/src/aigen/aigen/aigen.py", line 185, in __init__

          >     self.model = AutoModelForCausalLM.from_pretrained(

          >   File "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py",
          line 565, in from_pretrained

          >     return model_class.from_pretrained(

          >   File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py",
          line 3307, in from_pretrained

          >     ) = cls._load_pretrained_model(

          >   File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py",
          line 3695, in _load_pretrained_model

          >     new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(

          >   File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py",
          line 741, in _load_state_dict_into_meta_model

          >     set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)

          >   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py",
          line 285, in set_module_tensor_to_device

          >     raise ValueError(

          > ValueError: Trying to set a tensor of shape torch.Size([32005, 1024])
          in "weight" (which has shape torch.Size([32003, 1024])), this look incorrect.

          > ```

          > I don''t know where the extra tokens are coming from, because I''m definitely
          not adding them. I can use the exact same code, and just swap in any other
          model, and it will load just fine (not to mention, the old version of THIS
          one also worked fine). I''m not sure what changed, but it looks like you
          broke something :(


          Hello,

          It seems I have accidentally loaded the wrong tokenizer when further training
          this model. I will upload this new tokenizer now.'
        updatedAt: '2023-11-29T23:39:08.335Z'
      numEdits: 0
      reactions: []
    id: 6567cb9c178ee07f04e3b047
    type: comment
  author: Locutusque
  content: '> Somewhere between the conversion from `model.safetensors` to `pytorch_model.bin`,
    this model stopped working for me:

    > ```

    > Traceback (most recent call last):

    >   File "/src/harness.py", line 553, in <module>

    >     main()

    >   File "/src/harness.py", line 223, in main

    >     prototype = aigen(

    >   File "/src/aigen/aigen/aigen.py", line 185, in __init__

    >     self.model = AutoModelForCausalLM.from_pretrained(

    >   File "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py",
    line 565, in from_pretrained

    >     return model_class.from_pretrained(

    >   File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py",
    line 3307, in from_pretrained

    >     ) = cls._load_pretrained_model(

    >   File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py",
    line 3695, in _load_pretrained_model

    >     new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(

    >   File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py",
    line 741, in _load_state_dict_into_meta_model

    >     set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)

    >   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py",
    line 285, in set_module_tensor_to_device

    >     raise ValueError(

    > ValueError: Trying to set a tensor of shape torch.Size([32005, 1024]) in "weight"
    (which has shape torch.Size([32003, 1024])), this look incorrect.

    > ```

    > I don''t know where the extra tokens are coming from, because I''m definitely
    not adding them. I can use the exact same code, and just swap in any other model,
    and it will load just fine (not to mention, the old version of THIS one also worked
    fine). I''m not sure what changed, but it looks like you broke something :(


    Hello,

    It seems I have accidentally loaded the wrong tokenizer when further training
    this model. I will upload this new tokenizer now.'
  created_at: 2023-11-29 23:39:08+00:00
  edited: false
  hidden: false
  id: 6567cb9c178ee07f04e3b047
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/YeFyz1AZVcCRsyNHHtwJG.jpeg?w=200&h=200&f=face
      fullname: Sebastian Gabarain
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Locutusque
      type: user
    createdAt: '2023-11-29T23:55:00.000Z'
    data:
      status: closed
    id: 6567cf5457c58ae7f849ffa7
    type: status-change
  author: Locutusque
  created_at: 2023-11-29 23:55:00+00:00
  id: 6567cf5457c58ae7f849ffa7
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Locutusque/TinyMistral-248M
repo_type: model
status: closed
target_branch: null
title: 'ValueError: Trying to set a tensor of shape torch.Size([32005, 1024]) in "weight"
  (which has shape torch.Size([32003, 1024])), this look incorrect.'
