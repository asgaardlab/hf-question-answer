!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pradpalnis
conflicting_files: null
created_at: 2023-10-12 15:34:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6f323b294b167e22932341860267cd40.svg
      fullname: Pradeep Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pradpalnis
      type: user
    createdAt: '2023-10-12T16:34:20.000Z'
    data:
      edited: false
      editors:
      - pradpalnis
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4003666639328003
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6f323b294b167e22932341860267cd40.svg
          fullname: Pradeep Kumar
          isHf: false
          isPro: false
          name: pradpalnis
          type: user
        html: '<p>logged as "PMC_LLaMA_13B" model &amp; while loading the registered
          model got into RecursionError: maximum recursion depth exceeded<br>transformers==4.30.2
          </p>

          <pre><code> 1 # loaded_model = mlflow.pyfunc.load_model(f"models:/{registered_name}/2",dst_path=''/local_disk0'')

          </code></pre>

          <p>----&gt; 3 loaded_model = mlflow.pyfunc.load_model(f"models:/")</p>

          <p>File /databricks/python/lib/python3.10/site-packages/mlflow/pyfunc/<strong>init</strong>.py:597,
          in load_model(model_uri, suppress_warnings, dst_path)<br>    595 _add_code_from_conf_to_system_path(local_path,
          conf, code_key=CODE)<br>    596 data_path = os.path.join(local_path, conf[DATA])
          if (DATA in conf) else local_path<br>--&gt; 597 model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)<br>    598
          predict_fn = conf.get("predict_fn", "predict")<br>    599 return PyFuncModel(model_meta=model_meta,
          model_impl=model_impl, predict_fn=predict_fn)</p>

          <p>File /databricks/python/lib/python3.10/site-packages/mlflow/pyfunc/model.py:302,
          in _load_pyfunc(model_path)<br>    297     artifacts[saved_artifact_name]
          = os.path.join(<br>    298         model_path, saved_artifact_info[CONFIG_KEY_ARTIFACT_RELATIVE_PATH]<br>    299     )<br>    301
          context = PythonModelContext(artifacts=artifacts)<br>--&gt; 302 python_model.load_context(context=context)<br>    303
          signature = mlflow.models.Model.load(model_path).signature<br>    304 return
          _PythonModelPyfuncWrapper(<br>    305     python_model=python_model, context=context,
          signature=signature<br>    306 )</p>

          <p>File ~/.ipykernel/3009/command-3731584185085058-309509596:58, in load_context(self,
          context)</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:727,
          in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,
          **kwargs)<br>    723     if tokenizer_class is None:<br>    724         raise
          ValueError(<br>    725             f"Tokenizer class {tokenizer_class_candidate}
          does not exist or is not currently imported."<br>    726         )<br>--&gt;
          727     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,
          *inputs, **kwargs)<br>    729 # Otherwise we have to be creative.<br>    730
          # if model is an encoder decoder, the encoder tokenizer class is used by
          default<br>    731 if isinstance(config, EncoderDecoderConfig):</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1854,
          in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,
          cache_dir, force_download, local_files_only, token, revision, *init_inputs,
          **kwargs)<br>   1851     else:<br>   1852         logger.info(f"loading
          file {file_path} from cache at {resolved_vocab_files[file_id]}")<br>-&gt;
          1854 return cls._from_pretrained(<br>   1855     resolved_vocab_files,<br>   1856     pretrained_model_name_or_path,<br>   1857     init_configuration,<br>   1858     *init_inputs,<br>   1859     token=token,<br>   1860     cache_dir=cache_dir,<br>   1861     local_files_only=local_files_only,<br>   1862     _commit_hash=commit_hash,<br>   1863     _is_local=is_local,<br>   1864     **kwargs,<br>   1865
          )</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2017,
          in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,
          init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local,
          *init_inputs, **kwargs)<br>   2015 # Instantiate tokenizer.<br>   2016 try:<br>-&gt;
          2017     tokenizer = cls(*init_inputs, **init_kwargs)<br>   2018 except
          OSError:<br>   2019     raise OSError(<br>   2020         "Unable to load
          vocabulary from file. "<br>   2021         "Please check that the provided
          vocabulary is accessible and not corrupted."<br>   2022     )</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py:128,
          in LlamaTokenizerFast.<strong>init</strong>(self, vocab_file, tokenizer_file,
          clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token,
          add_eos_token, use_default_system_prompt, **kwargs)<br>    126 self._add_bos_token
          = add_bos_token<br>    127 self._add_eos_token = add_eos_token<br>--&gt;
          128 self.update_post_processor()<br>    129 self.use_default_system_prompt
          = use_default_system_prompt<br>    130 self.vocab_file = vocab_file</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py:138,
          in LlamaTokenizerFast.update_post_processor(self)<br>    134 """<br>    135
          Updates the underlying post processor with the current <code>bos_token</code>
          and <code>eos_token</code>.<br>    136 """<br>    137 bos = self.bos_token<br>--&gt;
          138 bos_token_id = self.bos_token_id<br>    140 eos = self.eos_token<br>    141
          eos_token_id = self.eos_token_id</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1141,
          in SpecialTokensMixin.bos_token_id(self)<br>   1139 if self._bos_token is
          None:<br>   1140     return None<br>-&gt; 1141 return self.convert_tokens_to_ids(self.bos_token)</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:270,
          in PreTrainedTokenizerFast.convert_tokens_to_ids(self, tokens)<br>    267     return
          None<br>    269 if isinstance(tokens, str):<br>--&gt; 270     return self._convert_token_to_id_with_added_voc(tokens)<br>    272
          return [self._convert_token_to_id_with_added_voc(token) for token in tokens]</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:277,
          in PreTrainedTokenizerFast._convert_token_to_id_with_added_voc(self, token)<br>    275
          index = self._tokenizer.token_to_id(token)<br>    276 if index is None:<br>--&gt;
          277     return self.unk_token_id<br>    278 return index</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1160,
          in SpecialTokensMixin.unk_token_id(self)<br>   1158 if self._unk_token is
          None:<br>   1159     return None<br>-&gt; 1160 return self.convert_tokens_to_ids(self.unk_token)</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:270,
          in PreTrainedTokenizerFast.convert_tokens_to_ids(self, tokens)<br>    267     return
          None<br>    269 if isinstance(tokens, str):<br>--&gt; 270     return self._convert_token_to_id_with_added_voc(tokens)<br>    272
          return [self._convert_token_to_id_with_added_voc(token) for token in tokens]</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:277,
          in PreTrainedTokenizerFast._convert_token_to_id_with_added_voc(self, token)<br>    275
          index = self._tokenizer.token_to_id(token)<br>    276 if index is None:<br>--&gt;
          277     return self.unk_token_id<br>    278 return index</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1160,
          in SpecialTokensMixin.unk_token_id(self)<br>   1158 if self._unk_token is
          None:<br>   1159     return None<br>-&gt; 1160 return self.convert_tokens_to_ids(self.unk_token)</p>

          <pre><code>[... skipping similar frames: PreTrainedTokenizerFast._convert_token_to_id_with_added_voc
          at line 277 (986 times), PreTrainedTokenizerFast.convert_tokens_to_ids at
          line 270 (986 times), SpecialTokensMixin.unk_token_id at line 1160 (986
          times)]

          </code></pre>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:270,
          in PreTrainedTokenizerFast.convert_tokens_to_ids(self, tokens)<br>    267     return
          None<br>    269 if isinstance(tokens, str):<br>--&gt; 270     return self._convert_token_to_id_with_added_voc(tokens)<br>    272
          return [self._convert_token_to_id_with_added_voc(token) for token in tokens]</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:277,
          in PreTrainedTokenizerFast._convert_token_to_id_with_added_voc(self, token)<br>    275
          index = self._tokenizer.token_to_id(token)<br>    276 if index is None:<br>--&gt;
          277     return self.unk_token_id<br>    278 return index</p>

          <p>File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1160,
          in SpecialTokensMixin.unk_token_id(self)<br>   1158 if self._unk_token is
          None:<br>   1159     return None<br>-&gt; 1160 return self.convert_tokens_to_ids(self.unk_token)</p>

          <p>RecursionError: maximum recursion depth exceeded</p>

          '
        raw: "logged as \"PMC_LLaMA_13B\" model & while loading the registered model\
          \ got into RecursionError: maximum recursion depth exceeded\r\ntransformers==4.30.2\
          \ \r\n\r\n     1 # loaded_model = mlflow.pyfunc.load_model(f\"models:/{registered_name}/2\"\
          ,dst_path='/local_disk0')\r\n----> 3 loaded_model = mlflow.pyfunc.load_model(f\"\
          models:/\")\r\n\r\nFile /databricks/python/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py:597,\
          \ in load_model(model_uri, suppress_warnings, dst_path)\r\n    595 _add_code_from_conf_to_system_path(local_path,\
          \ conf, code_key=CODE)\r\n    596 data_path = os.path.join(local_path, conf[DATA])\
          \ if (DATA in conf) else local_path\r\n--> 597 model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\r\
          \n    598 predict_fn = conf.get(\"predict_fn\", \"predict\")\r\n    599\
          \ return PyFuncModel(model_meta=model_meta, model_impl=model_impl, predict_fn=predict_fn)\r\
          \n\r\nFile /databricks/python/lib/python3.10/site-packages/mlflow/pyfunc/model.py:302,\
          \ in _load_pyfunc(model_path)\r\n    297     artifacts[saved_artifact_name]\
          \ = os.path.join(\r\n    298         model_path, saved_artifact_info[CONFIG_KEY_ARTIFACT_RELATIVE_PATH]\r\
          \n    299     )\r\n    301 context = PythonModelContext(artifacts=artifacts)\r\
          \n--> 302 python_model.load_context(context=context)\r\n    303 signature\
          \ = mlflow.models.Model.load(model_path).signature\r\n    304 return _PythonModelPyfuncWrapper(\r\
          \n    305     python_model=python_model, context=context, signature=signature\r\
          \n    306 )\r\n\r\nFile ~/.ipykernel/3009/command-3731584185085058-309509596:58,\
          \ in load_context(self, context)\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:727,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\r\n    723     if tokenizer_class is None:\r\n    724      \
          \   raise ValueError(\r\n    725             f\"Tokenizer class {tokenizer_class_candidate}\
          \ does not exist or is not currently imported.\"\r\n    726         )\r\n\
          --> 727     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    729 # Otherwise we have to be creative.\r\n\
          \    730 # if model is an encoder decoder, the encoder tokenizer class is\
          \ used by default\r\n    731 if isinstance(config, EncoderDecoderConfig):\r\
          \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1854,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
          \ **kwargs)\r\n   1851     else:\r\n   1852         logger.info(f\"loading\
          \ file {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n\
          -> 1854 return cls._from_pretrained(\r\n   1855     resolved_vocab_files,\r\
          \n   1856     pretrained_model_name_or_path,\r\n   1857     init_configuration,\r\
          \n   1858     *init_inputs,\r\n   1859     token=token,\r\n   1860     cache_dir=cache_dir,\r\
          \n   1861     local_files_only=local_files_only,\r\n   1862     _commit_hash=commit_hash,\r\
          \n   1863     _is_local=is_local,\r\n   1864     **kwargs,\r\n   1865 )\r\
          \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2017,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only,\
          \ _commit_hash, _is_local, *init_inputs, **kwargs)\r\n   2015 # Instantiate\
          \ tokenizer.\r\n   2016 try:\r\n-> 2017     tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\r\n   2018 except OSError:\r\n   2019     raise OSError(\r\
          \n   2020         \"Unable to load vocabulary from file. \"\r\n   2021 \
          \        \"Please check that the provided vocabulary is accessible and not\
          \ corrupted.\"\r\n   2022     )\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py:128,\
          \ in LlamaTokenizerFast.__init__(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces,\
          \ unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt,\
          \ **kwargs)\r\n    126 self._add_bos_token = add_bos_token\r\n    127 self._add_eos_token\
          \ = add_eos_token\r\n--> 128 self.update_post_processor()\r\n    129 self.use_default_system_prompt\
          \ = use_default_system_prompt\r\n    130 self.vocab_file = vocab_file\r\n\
          \r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py:138,\
          \ in LlamaTokenizerFast.update_post_processor(self)\r\n    134 \"\"\"\r\n\
          \    135 Updates the underlying post processor with the current `bos_token`\
          \ and `eos_token`.\r\n    136 \"\"\"\r\n    137 bos = self.bos_token\r\n\
          --> 138 bos_token_id = self.bos_token_id\r\n    140 eos = self.eos_token\r\
          \n    141 eos_token_id = self.eos_token_id\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1141,\
          \ in SpecialTokensMixin.bos_token_id(self)\r\n   1139 if self._bos_token\
          \ is None:\r\n   1140     return None\r\n-> 1141 return self.convert_tokens_to_ids(self.bos_token)\r\
          \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:270,\
          \ in PreTrainedTokenizerFast.convert_tokens_to_ids(self, tokens)\r\n   \
          \ 267     return None\r\n    269 if isinstance(tokens, str):\r\n--> 270\
          \     return self._convert_token_to_id_with_added_voc(tokens)\r\n    272\
          \ return [self._convert_token_to_id_with_added_voc(token) for token in tokens]\r\
          \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:277,\
          \ in PreTrainedTokenizerFast._convert_token_to_id_with_added_voc(self, token)\r\
          \n    275 index = self._tokenizer.token_to_id(token)\r\n    276 if index\
          \ is None:\r\n--> 277     return self.unk_token_id\r\n    278 return index\r\
          \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1160,\
          \ in SpecialTokensMixin.unk_token_id(self)\r\n   1158 if self._unk_token\
          \ is None:\r\n   1159     return None\r\n-> 1160 return self.convert_tokens_to_ids(self.unk_token)\r\
          \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:270,\
          \ in PreTrainedTokenizerFast.convert_tokens_to_ids(self, tokens)\r\n   \
          \ 267     return None\r\n    269 if isinstance(tokens, str):\r\n--> 270\
          \     return self._convert_token_to_id_with_added_voc(tokens)\r\n    272\
          \ return [self._convert_token_to_id_with_added_voc(token) for token in tokens]\r\
          \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:277,\
          \ in PreTrainedTokenizerFast._convert_token_to_id_with_added_voc(self, token)\r\
          \n    275 index = self._tokenizer.token_to_id(token)\r\n    276 if index\
          \ is None:\r\n--> 277     return self.unk_token_id\r\n    278 return index\r\
          \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1160,\
          \ in SpecialTokensMixin.unk_token_id(self)\r\n   1158 if self._unk_token\
          \ is None:\r\n   1159     return None\r\n-> 1160 return self.convert_tokens_to_ids(self.unk_token)\r\
          \n\r\n    [... skipping similar frames: PreTrainedTokenizerFast._convert_token_to_id_with_added_voc\
          \ at line 277 (986 times), PreTrainedTokenizerFast.convert_tokens_to_ids\
          \ at line 270 (986 times), SpecialTokensMixin.unk_token_id at line 1160\
          \ (986 times)]\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:270,\
          \ in PreTrainedTokenizerFast.convert_tokens_to_ids(self, tokens)\r\n   \
          \ 267     return None\r\n    269 if isinstance(tokens, str):\r\n--> 270\
          \     return self._convert_token_to_id_with_added_voc(tokens)\r\n    272\
          \ return [self._convert_token_to_id_with_added_voc(token) for token in tokens]\r\
          \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:277,\
          \ in PreTrainedTokenizerFast._convert_token_to_id_with_added_voc(self, token)\r\
          \n    275 index = self._tokenizer.token_to_id(token)\r\n    276 if index\
          \ is None:\r\n--> 277     return self.unk_token_id\r\n    278 return index\r\
          \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1160,\
          \ in SpecialTokensMixin.unk_token_id(self)\r\n   1158 if self._unk_token\
          \ is None:\r\n   1159     return None\r\n-> 1160 return self.convert_tokens_to_ids(self.unk_token)\r\
          \n\r\nRecursionError: maximum recursion depth exceeded"
        updatedAt: '2023-10-12T16:34:20.173Z'
      numEdits: 0
      reactions: []
    id: 6528200cd3b2ada94d9bee0c
    type: comment
  author: pradpalnis
  content: "logged as \"PMC_LLaMA_13B\" model & while loading the registered model\
    \ got into RecursionError: maximum recursion depth exceeded\r\ntransformers==4.30.2\
    \ \r\n\r\n     1 # loaded_model = mlflow.pyfunc.load_model(f\"models:/{registered_name}/2\"\
    ,dst_path='/local_disk0')\r\n----> 3 loaded_model = mlflow.pyfunc.load_model(f\"\
    models:/\")\r\n\r\nFile /databricks/python/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py:597,\
    \ in load_model(model_uri, suppress_warnings, dst_path)\r\n    595 _add_code_from_conf_to_system_path(local_path,\
    \ conf, code_key=CODE)\r\n    596 data_path = os.path.join(local_path, conf[DATA])\
    \ if (DATA in conf) else local_path\r\n--> 597 model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\r\
    \n    598 predict_fn = conf.get(\"predict_fn\", \"predict\")\r\n    599 return\
    \ PyFuncModel(model_meta=model_meta, model_impl=model_impl, predict_fn=predict_fn)\r\
    \n\r\nFile /databricks/python/lib/python3.10/site-packages/mlflow/pyfunc/model.py:302,\
    \ in _load_pyfunc(model_path)\r\n    297     artifacts[saved_artifact_name] =\
    \ os.path.join(\r\n    298         model_path, saved_artifact_info[CONFIG_KEY_ARTIFACT_RELATIVE_PATH]\r\
    \n    299     )\r\n    301 context = PythonModelContext(artifacts=artifacts)\r\
    \n--> 302 python_model.load_context(context=context)\r\n    303 signature = mlflow.models.Model.load(model_path).signature\r\
    \n    304 return _PythonModelPyfuncWrapper(\r\n    305     python_model=python_model,\
    \ context=context, signature=signature\r\n    306 )\r\n\r\nFile ~/.ipykernel/3009/command-3731584185085058-309509596:58,\
    \ in load_context(self, context)\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:727,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    723     if tokenizer_class is None:\r\n    724         raise\
    \ ValueError(\r\n    725             f\"Tokenizer class {tokenizer_class_candidate}\
    \ does not exist or is not currently imported.\"\r\n    726         )\r\n--> 727\
    \     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    729 # Otherwise we have to be creative.\r\n    730 # if model\
    \ is an encoder decoder, the encoder tokenizer class is used by default\r\n  \
    \  731 if isinstance(config, EncoderDecoderConfig):\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1854,\
    \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
    \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
    \ **kwargs)\r\n   1851     else:\r\n   1852         logger.info(f\"loading file\
    \ {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n-> 1854 return\
    \ cls._from_pretrained(\r\n   1855     resolved_vocab_files,\r\n   1856     pretrained_model_name_or_path,\r\
    \n   1857     init_configuration,\r\n   1858     *init_inputs,\r\n   1859    \
    \ token=token,\r\n   1860     cache_dir=cache_dir,\r\n   1861     local_files_only=local_files_only,\r\
    \n   1862     _commit_hash=commit_hash,\r\n   1863     _is_local=is_local,\r\n\
    \   1864     **kwargs,\r\n   1865 )\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2017,\
    \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local,\
    \ *init_inputs, **kwargs)\r\n   2015 # Instantiate tokenizer.\r\n   2016 try:\r\
    \n-> 2017     tokenizer = cls(*init_inputs, **init_kwargs)\r\n   2018 except OSError:\r\
    \n   2019     raise OSError(\r\n   2020         \"Unable to load vocabulary from\
    \ file. \"\r\n   2021         \"Please check that the provided vocabulary is accessible\
    \ and not corrupted.\"\r\n   2022     )\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py:128,\
    \ in LlamaTokenizerFast.__init__(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces,\
    \ unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt,\
    \ **kwargs)\r\n    126 self._add_bos_token = add_bos_token\r\n    127 self._add_eos_token\
    \ = add_eos_token\r\n--> 128 self.update_post_processor()\r\n    129 self.use_default_system_prompt\
    \ = use_default_system_prompt\r\n    130 self.vocab_file = vocab_file\r\n\r\n\
    File /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py:138,\
    \ in LlamaTokenizerFast.update_post_processor(self)\r\n    134 \"\"\"\r\n    135\
    \ Updates the underlying post processor with the current `bos_token` and `eos_token`.\r\
    \n    136 \"\"\"\r\n    137 bos = self.bos_token\r\n--> 138 bos_token_id = self.bos_token_id\r\
    \n    140 eos = self.eos_token\r\n    141 eos_token_id = self.eos_token_id\r\n\
    \r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1141,\
    \ in SpecialTokensMixin.bos_token_id(self)\r\n   1139 if self._bos_token is None:\r\
    \n   1140     return None\r\n-> 1141 return self.convert_tokens_to_ids(self.bos_token)\r\
    \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:270,\
    \ in PreTrainedTokenizerFast.convert_tokens_to_ids(self, tokens)\r\n    267  \
    \   return None\r\n    269 if isinstance(tokens, str):\r\n--> 270     return self._convert_token_to_id_with_added_voc(tokens)\r\
    \n    272 return [self._convert_token_to_id_with_added_voc(token) for token in\
    \ tokens]\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:277,\
    \ in PreTrainedTokenizerFast._convert_token_to_id_with_added_voc(self, token)\r\
    \n    275 index = self._tokenizer.token_to_id(token)\r\n    276 if index is None:\r\
    \n--> 277     return self.unk_token_id\r\n    278 return index\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1160,\
    \ in SpecialTokensMixin.unk_token_id(self)\r\n   1158 if self._unk_token is None:\r\
    \n   1159     return None\r\n-> 1160 return self.convert_tokens_to_ids(self.unk_token)\r\
    \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:270,\
    \ in PreTrainedTokenizerFast.convert_tokens_to_ids(self, tokens)\r\n    267  \
    \   return None\r\n    269 if isinstance(tokens, str):\r\n--> 270     return self._convert_token_to_id_with_added_voc(tokens)\r\
    \n    272 return [self._convert_token_to_id_with_added_voc(token) for token in\
    \ tokens]\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:277,\
    \ in PreTrainedTokenizerFast._convert_token_to_id_with_added_voc(self, token)\r\
    \n    275 index = self._tokenizer.token_to_id(token)\r\n    276 if index is None:\r\
    \n--> 277     return self.unk_token_id\r\n    278 return index\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1160,\
    \ in SpecialTokensMixin.unk_token_id(self)\r\n   1158 if self._unk_token is None:\r\
    \n   1159     return None\r\n-> 1160 return self.convert_tokens_to_ids(self.unk_token)\r\
    \n\r\n    [... skipping similar frames: PreTrainedTokenizerFast._convert_token_to_id_with_added_voc\
    \ at line 277 (986 times), PreTrainedTokenizerFast.convert_tokens_to_ids at line\
    \ 270 (986 times), SpecialTokensMixin.unk_token_id at line 1160 (986 times)]\r\
    \n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:270,\
    \ in PreTrainedTokenizerFast.convert_tokens_to_ids(self, tokens)\r\n    267  \
    \   return None\r\n    269 if isinstance(tokens, str):\r\n--> 270     return self._convert_token_to_id_with_added_voc(tokens)\r\
    \n    272 return [self._convert_token_to_id_with_added_voc(token) for token in\
    \ tokens]\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:277,\
    \ in PreTrainedTokenizerFast._convert_token_to_id_with_added_voc(self, token)\r\
    \n    275 index = self._tokenizer.token_to_id(token)\r\n    276 if index is None:\r\
    \n--> 277     return self.unk_token_id\r\n    278 return index\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1160,\
    \ in SpecialTokensMixin.unk_token_id(self)\r\n   1158 if self._unk_token is None:\r\
    \n   1159     return None\r\n-> 1160 return self.convert_tokens_to_ids(self.unk_token)\r\
    \n\r\nRecursionError: maximum recursion depth exceeded"
  created_at: 2023-10-12 15:34:20+00:00
  edited: false
  hidden: false
  id: 6528200cd3b2ada94d9bee0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/6f323b294b167e22932341860267cd40.svg
      fullname: Pradeep Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pradpalnis
      type: user
    createdAt: '2023-10-12T16:34:54.000Z'
    data:
      status: closed
    id: 6528202e6860a2c294bfa789
    type: status-change
  author: pradpalnis
  created_at: 2023-10-12 15:34:54+00:00
  id: 6528202e6860a2c294bfa789
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: axiong/PMC_LLaMA_13B
repo_type: model
status: closed
target_branch: null
title: 'RecursionError: maximum recursion depth exceeded'
