!!python/object:huggingface_hub.community.DiscussionWithDetails
author: akshay1943
conflicting_files: null
created_at: 2023-09-07 15:57:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd0be5342e32ac3289c38b55031c8e35.svg
      fullname: Akshay Valsaraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akshay1943
      type: user
    createdAt: '2023-09-07T16:57:45.000Z'
    data:
      edited: false
      editors:
      - akshay1943
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7746928930282593
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd0be5342e32ac3289c38b55031c8e35.svg
          fullname: Akshay Valsaraj
          isHf: false
          isPro: false
          name: akshay1943
          type: user
        html: "<pre><code>import transformers\nimport torch\n\ntokenizer = transformers.LlamaTokenizer.from_pretrained('axiong/PMC_LLaMA_13B')\n\
          model = transformers.LlamaForCausalLM.from_pretrained('axiong/PMC_LLaMA_13B')\n\
          \nsentence = 'Hello, doctor' \nbatch = tokenizer(\n    sentence,\n    return_tensors=\"\
          pt\", \n    add_special_tokens=False\n)\n\nwith torch.no_grad():\n    generated\
          \ = model.generate(\n        inputs = batch[\"input_ids\"],\n        max_length=200,\n\
          \        do_sample=True,\n        top_k=50\n    )\n    print('model predict:\
          \ ',tokenizer.decode(generated[0]))\n</code></pre>\n<p>You are using the\
          \ legacy behaviour of the &lt;class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'&gt;.\
          \ This means that tokens that come after special tokens will not be properly\
          \ handled. We recommend you to read the related pull request available at\
          \ <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/pull/24565\"\
          >https://github.com/huggingface/transformers/pull/24565</a><br>[2023-09-07\
          \ 16:22:27,715] [INFO] [real_accelerator.py:158:get_accelerator] Setting\
          \ ds_accelerator to cuda (auto detect)<br>Loading checkpoint shards: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [02:08&lt;00:00, 21.40s/it]/home/users/aks/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1270:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use a generation configuration\
          \ file (see <a href=\"https://huggingface.co/docs/transformers/main_classes/text_generation\"\
          >https://huggingface.co/docs/transformers/main_classes/text_generation</a>\
          \ )<br>  warnings.warn(<br>model predict:  Hello, doctor can you hear voices\
          \ as well Hello, Hello, doctor can you hear me now patient No, you are a\
          \ doctor, you are a doctor Hello, can I go outside and buy a sandwich? patient\
          \ No, not now, you have to take your medications first. Hello, doctor can\
          \ you hear me now Hello, Hello, can I leave the room now patient No, you\
          \ have to wait for the medications. Hello, doctor can you hear me now patient\
          \ Hello, no, you are a patient. You are ill, why are you doing this Hello,\
          \ can I go outside and buy a sandwich? patient No, you need to take your\
          \ medications. Hello, can I leave the room now patient No, you have to wait\
          \ for the medications. patient You are a doctor, why are you doing this\
          \ Hello, can I go outside and buy a sandwich? patient No, not now, take\
          \ your medications first. patient You are a doctor, why</p>\n"
        raw: "```\r\nimport transformers\r\nimport torch\r\n\r\ntokenizer = transformers.LlamaTokenizer.from_pretrained('axiong/PMC_LLaMA_13B')\r\
          \nmodel = transformers.LlamaForCausalLM.from_pretrained('axiong/PMC_LLaMA_13B')\r\
          \n\r\nsentence = 'Hello, doctor' \r\nbatch = tokenizer(\r\n    sentence,\r\
          \n    return_tensors=\"pt\", \r\n    add_special_tokens=False\r\n)\r\n\r\
          \nwith torch.no_grad():\r\n    generated = model.generate(\r\n        inputs\
          \ = batch[\"input_ids\"],\r\n        max_length=200,\r\n        do_sample=True,\r\
          \n        top_k=50\r\n    )\r\n    print('model predict: ',tokenizer.decode(generated[0]))\r\
          \n```\r\n\r\n\r\nYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>.\
          \ This means that tokens that come after special tokens will not be properly\
          \ handled. We recommend you to read the related pull request available at\
          \ https://github.com/huggingface/transformers/pull/24565\r\n[2023-09-07\
          \ 16:22:27,715] [INFO] [real_accelerator.py:158:get_accelerator] Setting\
          \ ds_accelerator to cuda (auto detect)\r\nLoading checkpoint shards: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [02:08<00:00, 21.40s/it]/home/users/aks/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1270:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use a generation configuration\
          \ file (see https://huggingface.co/docs/transformers/main_classes/text_generation\
          \ )\r\n  warnings.warn(\r\nmodel predict:  Hello, doctor can you hear voices\
          \ as well Hello, Hello, doctor can you hear me now patient No, you are a\
          \ doctor, you are a doctor Hello, can I go outside and buy a sandwich? patient\
          \ No, not now, you have to take your medications first. Hello, doctor can\
          \ you hear me now Hello, Hello, can I leave the room now patient No, you\
          \ have to wait for the medications. Hello, doctor can you hear me now patient\
          \ Hello, no, you are a patient. You are ill, why are you doing this Hello,\
          \ can I go outside and buy a sandwich? patient No, you need to take your\
          \ medications. Hello, can I leave the room now patient No, you have to wait\
          \ for the medications. patient You are a doctor, why are you doing this\
          \ Hello, can I go outside and buy a sandwich? patient No, not now, take\
          \ your medications first. patient You are a doctor, why"
        updatedAt: '2023-09-07T16:57:45.785Z'
      numEdits: 0
      reactions: []
    id: 64fa01099132c7f62ad31d13
    type: comment
  author: akshay1943
  content: "```\r\nimport transformers\r\nimport torch\r\n\r\ntokenizer = transformers.LlamaTokenizer.from_pretrained('axiong/PMC_LLaMA_13B')\r\
    \nmodel = transformers.LlamaForCausalLM.from_pretrained('axiong/PMC_LLaMA_13B')\r\
    \n\r\nsentence = 'Hello, doctor' \r\nbatch = tokenizer(\r\n    sentence,\r\n \
    \   return_tensors=\"pt\", \r\n    add_special_tokens=False\r\n)\r\n\r\nwith torch.no_grad():\r\
    \n    generated = model.generate(\r\n        inputs = batch[\"input_ids\"],\r\n\
    \        max_length=200,\r\n        do_sample=True,\r\n        top_k=50\r\n  \
    \  )\r\n    print('model predict: ',tokenizer.decode(generated[0]))\r\n```\r\n\
    \r\n\r\nYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>.\
    \ This means that tokens that come after special tokens will not be properly handled.\
    \ We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\r\
    \n[2023-09-07 16:22:27,715] [INFO] [real_accelerator.py:158:get_accelerator] Setting\
    \ ds_accelerator to cuda (auto detect)\r\nLoading checkpoint shards: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588| 6/6 [02:08<00:00, 21.40s/it]/home/users/aks/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1270:\
    \ UserWarning: You have modified the pretrained model configuration to control\
    \ generation. This is a deprecated strategy to control generation and will be\
    \ removed soon, in a future version. Please use a generation configuration file\
    \ (see https://huggingface.co/docs/transformers/main_classes/text_generation )\r\
    \n  warnings.warn(\r\nmodel predict:  Hello, doctor can you hear voices as well\
    \ Hello, Hello, doctor can you hear me now patient No, you are a doctor, you are\
    \ a doctor Hello, can I go outside and buy a sandwich? patient No, not now, you\
    \ have to take your medications first. Hello, doctor can you hear me now Hello,\
    \ Hello, can I leave the room now patient No, you have to wait for the medications.\
    \ Hello, doctor can you hear me now patient Hello, no, you are a patient. You\
    \ are ill, why are you doing this Hello, can I go outside and buy a sandwich?\
    \ patient No, you need to take your medications. Hello, can I leave the room now\
    \ patient No, you have to wait for the medications. patient You are a doctor,\
    \ why are you doing this Hello, can I go outside and buy a sandwich? patient No,\
    \ not now, take your medications first. patient You are a doctor, why"
  created_at: 2023-09-07 15:57:45+00:00
  edited: false
  hidden: false
  id: 64fa01099132c7f62ad31d13
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: axiong/PMC_LLaMA_13B
repo_type: model
status: open
target_branch: null
title: Why is the loading very slow and there are some legacy loading issues and the
  output is not correct
