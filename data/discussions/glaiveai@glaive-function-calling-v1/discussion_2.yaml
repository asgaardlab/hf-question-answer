!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JulienSantiago
conflicting_files: null
created_at: 2023-09-20 08:33:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0258712835638ac7b6f1e80fb31334d4.svg
      fullname: Julien Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JulienSantiago
      type: user
    createdAt: '2023-09-20T09:33:31.000Z'
    data:
      edited: false
      editors:
      - JulienSantiago
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6653153300285339
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0258712835638ac7b6f1e80fb31334d4.svg
          fullname: Julien Santiago
          isHf: false
          isPro: false
          name: JulienSantiago
          type: user
        html: '<p>When I try to load the model, I have the following error:<br>"File
          ~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:733,
          in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,
          **kwargs)<br>    731         tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate)<br>    732     if
          tokenizer_class is None:<br>--&gt; 733         raise ValueError(<br>    734             f"Tokenizer
          class {tokenizer_class_candidate} does not exist or is not currently imported."<br>    735         )<br>    736     return
          tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs,
          **kwargs)<br>    738 # Otherwise we have to be creative.<br>    739 # if
          model is an encoder decoder, the encoder tokenizer class is used by default</p>

          <p>ValueError: Tokenizer class ReplitLMTokenizer does not exist or is not
          currently imported."</p>

          <p>I tried to upgrade transformers but it did not work</p>

          '
        raw: "When I try to load the model, I have the following error: \r\n\"File\
          \ ~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:733,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\r\n    731         tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate)\r\
          \n    732     if tokenizer_class is None:\r\n--> 733         raise ValueError(\r\
          \n    734             f\"Tokenizer class {tokenizer_class_candidate} does\
          \ not exist or is not currently imported.\"\r\n    735         )\r\n   \
          \ 736     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    738 # Otherwise we have to be creative.\r\n\
          \    739 # if model is an encoder decoder, the encoder tokenizer class is\
          \ used by default\r\n\r\nValueError: Tokenizer class ReplitLMTokenizer does\
          \ not exist or is not currently imported.\"\r\n\r\nI tried to upgrade transformers\
          \ but it did not work"
        updatedAt: '2023-09-20T09:33:31.414Z'
      numEdits: 0
      reactions: []
    id: 650abc6bc19e5b4c8a7692e8
    type: comment
  author: JulienSantiago
  content: "When I try to load the model, I have the following error: \r\n\"File ~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:733,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    731         tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate)\r\
    \n    732     if tokenizer_class is None:\r\n--> 733         raise ValueError(\r\
    \n    734             f\"Tokenizer class {tokenizer_class_candidate} does not\
    \ exist or is not currently imported.\"\r\n    735         )\r\n    736     return\
    \ tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\r\
    \n    738 # Otherwise we have to be creative.\r\n    739 # if model is an encoder\
    \ decoder, the encoder tokenizer class is used by default\r\n\r\nValueError: Tokenizer\
    \ class ReplitLMTokenizer does not exist or is not currently imported.\"\r\n\r\
    \nI tried to upgrade transformers but it did not work"
  created_at: 2023-09-20 08:33:31+00:00
  edited: false
  hidden: false
  id: 650abc6bc19e5b4c8a7692e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0258712835638ac7b6f1e80fb31334d4.svg
      fullname: Julien Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JulienSantiago
      type: user
    createdAt: '2023-09-20T09:57:01.000Z'
    data:
      edited: false
      editors:
      - JulienSantiago
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8480601906776428
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0258712835638ac7b6f1e80fb31334d4.svg
          fullname: Julien Santiago
          isHf: false
          isPro: false
          name: JulienSantiago
          type: user
        html: '<p>If someone one day has the same issue, just use "trust_remote_code=True"
          as an argument of AutoTokenizer.from_pretrained</p>

          '
        raw: If someone one day has the same issue, just use "trust_remote_code=True"
          as an argument of AutoTokenizer.from_pretrained
        updatedAt: '2023-09-20T09:57:01.305Z'
      numEdits: 0
      reactions: []
    id: 650ac1edd26103b6eeeadf06
    type: comment
  author: JulienSantiago
  content: If someone one day has the same issue, just use "trust_remote_code=True"
    as an argument of AutoTokenizer.from_pretrained
  created_at: 2023-09-20 08:57:01+00:00
  edited: false
  hidden: false
  id: 650ac1edd26103b6eeeadf06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64cabba5300d22ba20b05b32c09dee6c.svg
      fullname: Boddu Surya Venkat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MLconArtist
      type: user
    createdAt: '2023-10-09T11:05:22.000Z'
    data:
      edited: true
      editors:
      - MLconArtist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4268585741519928
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64cabba5300d22ba20b05b32c09dee6c.svg
          fullname: Boddu Surya Venkat
          isHf: false
          isPro: false
          name: MLconArtist
          type: user
        html: "<p>I kind of have a similar issue. I am getting this error. I have\
          \ the argument 'trust_remote_code' set to True</p>\n<pre><code> ~/.cache/huggingface/modules/transformers_modules/glaiveai/glaive-function-calling-\
          \ \nv1/d94dd60c9ceaff581cf7e6b5f9982b4b1716ae16/replit_lm_tokenizer.py in\
          \ vocab_size(self)\n 71     @property\n 72     def vocab_size(self):\n---&gt;\
          \ 73         return self.sp_model.get_piece_size()\n 74 \n 75     def get_vocab(self):\n\
          \nAttributeError: 'ReplitLMTokenizer' object has no attribute 'sp_model'\n\
          </code></pre>\n"
        raw: "I kind of have a similar issue. I am getting this error. I have the\
          \ argument 'trust_remote_code' set to True\n\n     ~/.cache/huggingface/modules/transformers_modules/glaiveai/glaive-function-calling-\
          \ \n    v1/d94dd60c9ceaff581cf7e6b5f9982b4b1716ae16/replit_lm_tokenizer.py\
          \ in vocab_size(self)\n     71     @property\n     72     def vocab_size(self):\n\
          \    ---> 73         return self.sp_model.get_piece_size()\n     74 \n \
          \    75     def get_vocab(self):\n\n    AttributeError: 'ReplitLMTokenizer'\
          \ object has no attribute 'sp_model'"
        updatedAt: '2023-10-09T11:06:38.755Z'
      numEdits: 2
      reactions: []
    id: 6523de7273a0f19d06e92266
    type: comment
  author: MLconArtist
  content: "I kind of have a similar issue. I am getting this error. I have the argument\
    \ 'trust_remote_code' set to True\n\n     ~/.cache/huggingface/modules/transformers_modules/glaiveai/glaive-function-calling-\
    \ \n    v1/d94dd60c9ceaff581cf7e6b5f9982b4b1716ae16/replit_lm_tokenizer.py in\
    \ vocab_size(self)\n     71     @property\n     72     def vocab_size(self):\n\
    \    ---> 73         return self.sp_model.get_piece_size()\n     74 \n     75\
    \     def get_vocab(self):\n\n    AttributeError: 'ReplitLMTokenizer' object has\
    \ no attribute 'sp_model'"
  created_at: 2023-10-09 10:05:22+00:00
  edited: true
  hidden: false
  id: 6523de7273a0f19d06e92266
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b4fa6c0ca4363d2645d8f92c5456d500.svg
      fullname: wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fireshooter
      type: user
    createdAt: '2023-12-12T14:54:36.000Z'
    data:
      edited: false
      editors:
      - fireshooter
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47042372822761536
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b4fa6c0ca4363d2645d8f92c5456d500.svg
          fullname: wang
          isHf: false
          isPro: false
          name: fireshooter
          type: user
        html: "<blockquote>\n<p>I kind of have a similar issue. I am getting this\
          \ error. I have the argument 'trust_remote_code' set to True</p>\n<pre><code>\
          \ ~/.cache/huggingface/modules/transformers_modules/glaiveai/glaive-function-calling-\
          \ \nv1/d94dd60c9ceaff581cf7e6b5f9982b4b1716ae16/replit_lm_tokenizer.py in\
          \ vocab_size(self)\n 71     @property\n 72     def vocab_size(self):\n---&gt;\
          \ 73         return self.sp_model.get_piece_size()\n 74 \n 75     def get_vocab(self):\n\
          \nAttributeError: 'ReplitLMTokenizer' object has no attribute 'sp_model'\n\
          </code></pre>\n</blockquote>\n<p>Same issue on colab</p>\n"
        raw: "> I kind of have a similar issue. I am getting this error. I have the\
          \ argument 'trust_remote_code' set to True\n> \n>      ~/.cache/huggingface/modules/transformers_modules/glaiveai/glaive-function-calling-\
          \ \n>     v1/d94dd60c9ceaff581cf7e6b5f9982b4b1716ae16/replit_lm_tokenizer.py\
          \ in vocab_size(self)\n>      71     @property\n>      72     def vocab_size(self):\n\
          >     ---> 73         return self.sp_model.get_piece_size()\n>      74 \n\
          >      75     def get_vocab(self):\n> \n>     AttributeError: 'ReplitLMTokenizer'\
          \ object has no attribute 'sp_model'\n\nSame issue on colab"
        updatedAt: '2023-12-12T14:54:36.023Z'
      numEdits: 0
      reactions: []
    id: 6578742c411e14898ba02d0a
    type: comment
  author: fireshooter
  content: "> I kind of have a similar issue. I am getting this error. I have the\
    \ argument 'trust_remote_code' set to True\n> \n>      ~/.cache/huggingface/modules/transformers_modules/glaiveai/glaive-function-calling-\
    \ \n>     v1/d94dd60c9ceaff581cf7e6b5f9982b4b1716ae16/replit_lm_tokenizer.py in\
    \ vocab_size(self)\n>      71     @property\n>      72     def vocab_size(self):\n\
    >     ---> 73         return self.sp_model.get_piece_size()\n>      74 \n>   \
    \   75     def get_vocab(self):\n> \n>     AttributeError: 'ReplitLMTokenizer'\
    \ object has no attribute 'sp_model'\n\nSame issue on colab"
  created_at: 2023-12-12 14:54:36+00:00
  edited: false
  hidden: false
  id: 6578742c411e14898ba02d0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c2f0ef71477678d46fb8bccd1aa4f1b.svg
      fullname: luke newey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lmnewey
      type: user
    createdAt: '2023-12-14T07:53:29.000Z'
    data:
      edited: true
      editors:
      - lmnewey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9890740513801575
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c2f0ef71477678d46fb8bccd1aa4f1b.svg
          fullname: luke newey
          isHf: false
          isPro: false
          name: lmnewey
          type: user
        html: "<p>Mine has \"trust remote code \" set to true but its still having\
          \ the error</p>\n<p>Edit: my mistake i was passing the argument to Vllm,\
          \ ive modified the init method to reflect that, will try again and let you\
          \ know </p>\n<pre><code>def __init__(\n    self,\n    model: str,\n    tokenizer:\
          \ Optional[str] = None,\n    tokenizer_mode: str = \"auto\",\n    trust_remote_code:\
          \ bool = True, # modified to run replit\n    tensor_parallel_size: int =\
          \ 1,\n    dtype: str = \"auto\",\n    quantization: Optional[str] = None,\n\
          \    revision: Optional[str] = None,\n    tokenizer_revision: Optional[str]\
          \ = None,\n    seed: int = 0,\n    gpu_memory_utilization: float = 0.9,\n\
          \    swap_space: int = 4,\n    **kwargs,\n) -&gt; None:\n</code></pre>\n"
        raw: "Mine has \"trust remote code \" set to true but its still having the\
          \ error\n\n\nEdit: my mistake i was passing the argument to Vllm, ive modified\
          \ the init method to reflect that, will try again and let you know \n\n\
          \    def __init__(\n        self,\n        model: str,\n        tokenizer:\
          \ Optional[str] = None,\n        tokenizer_mode: str = \"auto\",\n     \
          \   trust_remote_code: bool = True, # modified to run replit\n        tensor_parallel_size:\
          \ int = 1,\n        dtype: str = \"auto\",\n        quantization: Optional[str]\
          \ = None,\n        revision: Optional[str] = None,\n        tokenizer_revision:\
          \ Optional[str] = None,\n        seed: int = 0,\n        gpu_memory_utilization:\
          \ float = 0.9,\n        swap_space: int = 4,\n        **kwargs,\n    ) ->\
          \ None:\n"
        updatedAt: '2023-12-14T07:55:23.663Z'
      numEdits: 1
      reactions: []
    id: 657ab479270ef0b785f1eb96
    type: comment
  author: lmnewey
  content: "Mine has \"trust remote code \" set to true but its still having the error\n\
    \n\nEdit: my mistake i was passing the argument to Vllm, ive modified the init\
    \ method to reflect that, will try again and let you know \n\n    def __init__(\n\
    \        self,\n        model: str,\n        tokenizer: Optional[str] = None,\n\
    \        tokenizer_mode: str = \"auto\",\n        trust_remote_code: bool = True,\
    \ # modified to run replit\n        tensor_parallel_size: int = 1,\n        dtype:\
    \ str = \"auto\",\n        quantization: Optional[str] = None,\n        revision:\
    \ Optional[str] = None,\n        tokenizer_revision: Optional[str] = None,\n \
    \       seed: int = 0,\n        gpu_memory_utilization: float = 0.9,\n       \
    \ swap_space: int = 4,\n        **kwargs,\n    ) -> None:\n"
  created_at: 2023-12-14 07:53:29+00:00
  edited: true
  hidden: false
  id: 657ab479270ef0b785f1eb96
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c2f0ef71477678d46fb8bccd1aa4f1b.svg
      fullname: luke newey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lmnewey
      type: user
    createdAt: '2023-12-14T07:56:38.000Z'
    data:
      edited: false
      editors:
      - lmnewey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8233715891838074
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c2f0ef71477678d46fb8bccd1aa4f1b.svg
          fullname: luke newey
          isHf: false
          isPro: false
          name: lmnewey
          type: user
        html: '<p>Still same issue here for me</p>

          '
        raw: Still same issue here for me
        updatedAt: '2023-12-14T07:56:38.585Z'
      numEdits: 0
      reactions: []
    id: 657ab53606e44e4565574d8f
    type: comment
  author: lmnewey
  content: Still same issue here for me
  created_at: 2023-12-14 07:56:38+00:00
  edited: false
  hidden: false
  id: 657ab53606e44e4565574d8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c2f0ef71477678d46fb8bccd1aa4f1b.svg
      fullname: luke newey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lmnewey
      type: user
    createdAt: '2023-12-14T08:16:38.000Z'
    data:
      edited: false
      editors:
      - lmnewey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.24789905548095703
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c2f0ef71477678d46fb8bccd1aa4f1b.svg
          fullname: luke newey
          isHf: false
          isPro: false
          name: lmnewey
          type: user
        html: '<p>When the error happens, </p>

          <p>it looks like self.sp_model is not yet set, in replit tokenizer. I got
          a bit further by swapping it around so that the super().__init line comes
          after</p>

          <p>Original:<br>    def <strong>init</strong>(self, vocab_file, bos_token=None,
          eos_token=''&lt;|endoftext|&gt;'', unk_token=''&lt;|unk|&gt;'', pad_token=''&lt;|pad|&gt;'',
          sep_token=None, sp_model_kwargs: Optional[Dict[str, Any]]=None, **kwargs)
          -&gt; None:<br>        self.sp_model_kwargs = {} if sp_model_kwargs is None
          else sp_model_kwargs<br>        super().<strong>init</strong>(bos_token=bos_token,
          eos_token=eos_token, unk_token=unk_token, pad_token=pad_token, sep_token=sep_token,
          sp_model_kwargs=self.sp_model_kwargs, **kwargs)<br>        self.vocab_file
          = vocab_file<br>        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)<br>        self.sp_model.Load(vocab_file)</p>

          <p>Edited version:<br>    def <strong>init</strong>(self, vocab_file, bos_token=None,
          eos_token=''&lt;|endoftext|&gt;'', unk_token=''&lt;|unk|&gt;'', pad_token=''&lt;|pad|&gt;'',
          sep_token=None, sp_model_kwargs: Optional[Dict[str, Any]]=None, **kwargs)
          -&gt; None:<br>        self.sp_model_kwargs = {} if sp_model_kwargs is None
          else sp_model_kwargs<br>        self.vocab_file = vocab_file<br>        self.sp_model
          = spm.SentencePieceProcessor(**self.sp_model_kwargs)<br>        self.sp_model.Load(vocab_file)<br>        super().<strong>init</strong>(bos_token=bos_token,
          eos_token=eos_token, unk_token=unk_token, pad_token=pad_token, sep_token=sep_token,
          sp_model_kwargs=self.sp_model_kwargs, **kwargs)</p>

          '
        raw: "When the error happens, \n\nit looks like self.sp_model is not yet set,\
          \ in replit tokenizer. I got a bit further by swapping it around so that\
          \ the super().__init line comes after\n\nOriginal:\n    def __init__(self,\
          \ vocab_file, bos_token=None, eos_token='<|endoftext|>', unk_token='<|unk|>',\
          \ pad_token='<|pad|>', sep_token=None, sp_model_kwargs: Optional[Dict[str,\
          \ Any]]=None, **kwargs) -> None:\n        self.sp_model_kwargs = {} if sp_model_kwargs\
          \ is None else sp_model_kwargs\n        super().__init__(bos_token=bos_token,\
          \ eos_token=eos_token, unk_token=unk_token, pad_token=pad_token, sep_token=sep_token,\
          \ sp_model_kwargs=self.sp_model_kwargs, **kwargs)\n        self.vocab_file\
          \ = vocab_file\n        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n\
          \        self.sp_model.Load(vocab_file)\n\n\nEdited version:\n    def __init__(self,\
          \ vocab_file, bos_token=None, eos_token='<|endoftext|>', unk_token='<|unk|>',\
          \ pad_token='<|pad|>', sep_token=None, sp_model_kwargs: Optional[Dict[str,\
          \ Any]]=None, **kwargs) -> None:\n        self.sp_model_kwargs = {} if sp_model_kwargs\
          \ is None else sp_model_kwargs\n        self.vocab_file = vocab_file\n \
          \       self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n\
          \        self.sp_model.Load(vocab_file)\n        super().__init__(bos_token=bos_token,\
          \ eos_token=eos_token, unk_token=unk_token, pad_token=pad_token, sep_token=sep_token,\
          \ sp_model_kwargs=self.sp_model_kwargs, **kwargs)"
        updatedAt: '2023-12-14T08:16:38.232Z'
      numEdits: 0
      reactions: []
    id: 657ab9e6eaef97ff4c1e1608
    type: comment
  author: lmnewey
  content: "When the error happens, \n\nit looks like self.sp_model is not yet set,\
    \ in replit tokenizer. I got a bit further by swapping it around so that the super().__init\
    \ line comes after\n\nOriginal:\n    def __init__(self, vocab_file, bos_token=None,\
    \ eos_token='<|endoftext|>', unk_token='<|unk|>', pad_token='<|pad|>', sep_token=None,\
    \ sp_model_kwargs: Optional[Dict[str, Any]]=None, **kwargs) -> None:\n       \
    \ self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n\
    \        super().__init__(bos_token=bos_token, eos_token=eos_token, unk_token=unk_token,\
    \ pad_token=pad_token, sep_token=sep_token, sp_model_kwargs=self.sp_model_kwargs,\
    \ **kwargs)\n        self.vocab_file = vocab_file\n        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n\
    \        self.sp_model.Load(vocab_file)\n\n\nEdited version:\n    def __init__(self,\
    \ vocab_file, bos_token=None, eos_token='<|endoftext|>', unk_token='<|unk|>',\
    \ pad_token='<|pad|>', sep_token=None, sp_model_kwargs: Optional[Dict[str, Any]]=None,\
    \ **kwargs) -> None:\n        self.sp_model_kwargs = {} if sp_model_kwargs is\
    \ None else sp_model_kwargs\n        self.vocab_file = vocab_file\n        self.sp_model\
    \ = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n        self.sp_model.Load(vocab_file)\n\
    \        super().__init__(bos_token=bos_token, eos_token=eos_token, unk_token=unk_token,\
    \ pad_token=pad_token, sep_token=sep_token, sp_model_kwargs=self.sp_model_kwargs,\
    \ **kwargs)"
  created_at: 2023-12-14 08:16:38+00:00
  edited: false
  hidden: false
  id: 657ab9e6eaef97ff4c1e1608
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c2f0ef71477678d46fb8bccd1aa4f1b.svg
      fullname: luke newey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lmnewey
      type: user
    createdAt: '2023-12-14T08:28:17.000Z'
    data:
      edited: false
      editors:
      - lmnewey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5867184996604919
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c2f0ef71477678d46fb8bccd1aa4f1b.svg
          fullname: luke newey
          isHf: false
          isPro: false
          name: lmnewey
          type: user
        html: '<p>The next issue i faced was with the HF_prefixlm_converter</p>

          <p>15 and 16<br>I needed to comment out line 15,16,23 and 24<br>#from transformers.models.bloom.modeling_bloom
          import _expand_mask as _expand_mask_bloom<br>#from transformers.models.bloom.modeling_bloom
          import _make_causal_mask as _make_causal_mask_bloom</p>

          <p>23 and 24<br>#from transformers.models.opt.modeling_opt import _expand_mask
          as _expand_mask_opt<br>#from transformers.models.opt.modeling_opt import
          _make_causal_mask as _make_causal_mask_opt</p>

          '
        raw: 'The next issue i faced was with the HF_prefixlm_converter


          15 and 16

          I needed to comment out line 15,16,23 and 24

          #from transformers.models.bloom.modeling_bloom import _expand_mask as _expand_mask_bloom

          #from transformers.models.bloom.modeling_bloom import _make_causal_mask
          as _make_causal_mask_bloom


          23 and 24

          #from transformers.models.opt.modeling_opt import _expand_mask as _expand_mask_opt

          #from transformers.models.opt.modeling_opt import _make_causal_mask as _make_causal_mask_opt'
        updatedAt: '2023-12-14T08:28:17.164Z'
      numEdits: 0
      reactions: []
    id: 657abca1f55b7314c3e2d260
    type: comment
  author: lmnewey
  content: 'The next issue i faced was with the HF_prefixlm_converter


    15 and 16

    I needed to comment out line 15,16,23 and 24

    #from transformers.models.bloom.modeling_bloom import _expand_mask as _expand_mask_bloom

    #from transformers.models.bloom.modeling_bloom import _make_causal_mask as _make_causal_mask_bloom


    23 and 24

    #from transformers.models.opt.modeling_opt import _expand_mask as _expand_mask_opt

    #from transformers.models.opt.modeling_opt import _make_causal_mask as _make_causal_mask_opt'
  created_at: 2023-12-14 08:28:17+00:00
  edited: false
  hidden: false
  id: 657abca1f55b7314c3e2d260
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: glaiveai/glaive-function-calling-v1
repo_type: model
status: open
target_branch: null
title: Tokenizer problem
