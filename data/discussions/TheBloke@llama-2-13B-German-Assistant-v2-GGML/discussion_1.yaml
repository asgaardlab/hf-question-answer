!!python/object:huggingface_hub.community.DiscussionWithDetails
author: appliedstuff
conflicting_files: null
created_at: 2023-07-20 19:25:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/beabf70201af07b24a5fa5681b44a6f6.svg
      fullname: PK
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appliedstuff
      type: user
    createdAt: '2023-07-20T20:25:20.000Z'
    data:
      edited: true
      editors:
      - appliedstuff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9230016469955444
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/beabf70201af07b24a5fa5681b44a6f6.svg
          fullname: PK
          isHf: false
          isPro: false
          name: appliedstuff
          type: user
        html: '<p>I just want to know if someone was able to use them sucessfully?
          I only get empty responses. </p>

          <p>I am using llama-cpp-python (latest versions). The english llma-2-7b-chat.ggmlv3.g4_0.bin
          works fine. Any suggestion or recommendations what I should test or what
          the reason could be?</p>

          <p>I tried original quant and new quant models.</p>

          '
        raw: "I just want to know if someone was able to use them sucessfully? I only\
          \ get empty responses. \n\nI am using llama-cpp-python (latest versions).\
          \ The english llma-2-7b-chat.ggmlv3.g4_0.bin works fine. Any suggestion\
          \ or recommendations what I should test or what the reason could be?\n\n\
          I tried original quant and new quant models."
        updatedAt: '2023-07-20T20:50:21.833Z'
      numEdits: 2
      reactions: []
    id: 64b998300290090bb9a9ea7e
    type: comment
  author: appliedstuff
  content: "I just want to know if someone was able to use them sucessfully? I only\
    \ get empty responses. \n\nI am using llama-cpp-python (latest versions). The\
    \ english llma-2-7b-chat.ggmlv3.g4_0.bin works fine. Any suggestion or recommendations\
    \ what I should test or what the reason could be?\n\nI tried original quant and\
    \ new quant models."
  created_at: 2023-07-20 19:25:20+00:00
  edited: true
  hidden: false
  id: 64b998300290090bb9a9ea7e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/beabf70201af07b24a5fa5681b44a6f6.svg
      fullname: PK
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appliedstuff
      type: user
    createdAt: '2023-07-20T20:34:16.000Z'
    data:
      from: Get only empty string '' response back - Any one used them successfully?
      to: Get only empty string '' response back - Anyone used them successfully?
    id: 64b99a482cfce8f7b3ce946c
    type: title-change
  author: appliedstuff
  created_at: 2023-07-20 19:34:16+00:00
  id: 64b99a482cfce8f7b3ce946c
  new_title: Get only empty string '' response back - Anyone used them successfully?
  old_title: Get only empty string '' response back - Any one used them successfully?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-20T20:53:55.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.897632896900177
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Hmm yeah I saw the same with the q4_0 model.  But then I tried q5_1
          and it works.. but won''t shut up:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/2HCnd_ESzRBUEKIkDrtLW.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/2HCnd_ESzRBUEKIkDrtLW.png"></a></p>

          <p>Maybe this model doesn''t work very well in GGML :(</p>

          '
        raw: 'Hmm yeah I saw the same with the q4_0 model.  But then I tried q5_1
          and it works.. but won''t shut up:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/2HCnd_ESzRBUEKIkDrtLW.png)


          Maybe this model doesn''t work very well in GGML :('
        updatedAt: '2023-07-20T20:53:55.828Z'
      numEdits: 0
      reactions: []
    id: 64b99ee382975e7c601c9272
    type: comment
  author: TheBloke
  content: 'Hmm yeah I saw the same with the q4_0 model.  But then I tried q5_1 and
    it works.. but won''t shut up:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/2HCnd_ESzRBUEKIkDrtLW.png)


    Maybe this model doesn''t work very well in GGML :('
  created_at: 2023-07-20 19:53:55+00:00
  edited: false
  hidden: false
  id: 64b99ee382975e7c601c9272
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03e26ad44894f94eb5ed3a0b1a9cc7a7.svg
      fullname: Uwe G.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Silverspoon7
      type: user
    createdAt: '2023-07-20T21:40:27.000Z'
    data:
      edited: true
      editors:
      - Silverspoon7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9138000011444092
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03e26ad44894f94eb5ed3a0b1a9cc7a7.svg
          fullname: Uwe G.
          isHf: false
          isPro: false
          name: Silverspoon7
          type: user
        html: '<p>Yes, I tried q4K and q6k variant, and it does gibberish output in
          both English and German, maybe depending on temperature settings a little
          bit. I tried the Open Assistant instruction template on chat-instruct, but
          without success (except of doing output instead of giving empty results).
          Maybe it is not really compatible with oobabooga or need completely own
          hf settings.</p>

          '
        raw: Yes, I tried q4K and q6k variant, and it does gibberish output in both
          English and German, maybe depending on temperature settings a little bit.
          I tried the Open Assistant instruction template on chat-instruct, but without
          success (except of doing output instead of giving empty results). Maybe
          it is not really compatible with oobabooga or need completely own hf settings.
        updatedAt: '2023-07-20T22:06:09.153Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Pawloo
    id: 64b9a9cb6a7c71f0046482f9
    type: comment
  author: Silverspoon7
  content: Yes, I tried q4K and q6k variant, and it does gibberish output in both
    English and German, maybe depending on temperature settings a little bit. I tried
    the Open Assistant instruction template on chat-instruct, but without success
    (except of doing output instead of giving empty results). Maybe it is not really
    compatible with oobabooga or need completely own hf settings.
  created_at: 2023-07-20 20:40:27+00:00
  edited: true
  hidden: false
  id: 64b9a9cb6a7c71f0046482f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-07-20T22:20:33.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8129807710647583
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: '<p>Same here, doesn''t work. Used llama-2-13b-german-assistant-v2.ggmlv3.q5_K_M.bin
          and got only the beginning of a response, then it started outputting only
          blanks.</p>

          '
        raw: Same here, doesn't work. Used llama-2-13b-german-assistant-v2.ggmlv3.q5_K_M.bin
          and got only the beginning of a response, then it started outputting only
          blanks.
        updatedAt: '2023-07-20T22:20:33.617Z'
      numEdits: 0
      reactions: []
    id: 64b9b33144ade326860d91db
    type: comment
  author: wolfram
  content: Same here, doesn't work. Used llama-2-13b-german-assistant-v2.ggmlv3.q5_K_M.bin
    and got only the beginning of a response, then it started outputting only blanks.
  created_at: 2023-07-20 21:20:33+00:00
  edited: false
  hidden: false
  id: 64b9b33144ade326860d91db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03e26ad44894f94eb5ed3a0b1a9cc7a7.svg
      fullname: Uwe G.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Silverspoon7
      type: user
    createdAt: '2023-07-20T23:20:13.000Z'
    data:
      edited: false
      editors:
      - Silverspoon7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9046751856803894
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03e26ad44894f94eb5ed3a0b1a9cc7a7.svg
          fullname: Uwe G.
          isHf: false
          isPro: false
          name: Silverspoon7
          type: user
        html: '<p>When you got blanks, prompt template is empty or wrong. See model
          card for correct prompt. Turn down temperature to get better results. As
          explained before, results are not good with my tests.</p>

          '
        raw: When you got blanks, prompt template is empty or wrong. See model card
          for correct prompt. Turn down temperature to get better results. As explained
          before, results are not good with my tests.
        updatedAt: '2023-07-20T23:20:13.529Z'
      numEdits: 0
      reactions: []
    id: 64b9c12da6ccf0f64b3e5b8a
    type: comment
  author: Silverspoon7
  content: When you got blanks, prompt template is empty or wrong. See model card
    for correct prompt. Turn down temperature to get better results. As explained
    before, results are not good with my tests.
  created_at: 2023-07-20 22:20:13+00:00
  edited: false
  hidden: false
  id: 64b9c12da6ccf0f64b3e5b8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/beabf70201af07b24a5fa5681b44a6f6.svg
      fullname: PK
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appliedstuff
      type: user
    createdAt: '2023-07-28T13:34:22.000Z'
    data:
      edited: false
      editors:
      - appliedstuff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9607735276222229
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/beabf70201af07b24a5fa5681b44a6f6.svg
          fullname: PK
          isHf: false
          isPro: false
          name: appliedstuff
          type: user
        html: '<p>Unfourtanetly, the results are also not good for me and the q5_1
          works. So what makes that model so worse?<br>Is it the conversion to GGML?<br>Or
          the model itself?</p>

          '
        raw: 'Unfourtanetly, the results are also not good for me and the q5_1 works.
          So what makes that model so worse?

          Is it the conversion to GGML?

          Or the model itself?'
        updatedAt: '2023-07-28T13:34:22.375Z'
      numEdits: 0
      reactions: []
    id: 64c3c3de5e5bc55a92d8c9ab
    type: comment
  author: appliedstuff
  content: 'Unfourtanetly, the results are also not good for me and the q5_1 works.
    So what makes that model so worse?

    Is it the conversion to GGML?

    Or the model itself?'
  created_at: 2023-07-28 12:34:22+00:00
  edited: false
  hidden: false
  id: 64c3c3de5e5bc55a92d8c9ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03e26ad44894f94eb5ed3a0b1a9cc7a7.svg
      fullname: Uwe G.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Silverspoon7
      type: user
    createdAt: '2023-07-31T13:01:32.000Z'
    data:
      edited: false
      editors:
      - Silverspoon7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9541211128234863
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03e26ad44894f94eb5ed3a0b1a9cc7a7.svg
          fullname: Uwe G.
          isHf: false
          isPro: false
          name: Silverspoon7
          type: user
        html: "<p>That model is completely overfitted I think. I trained a blank llama2\
          \ 13b model and got better results. But I haven\u2018t used that instruction\
          \ sets, which are not bad at all. But I don\u2018t need them. I simply apply\
          \ my Lora file to an orca llama2 after training the blank model. That works\
          \ good for me. I am using the text generation ui for training, and it seems\
          \ there are some compatibility issues, so training the orca-llama2 doesn\u2018\
          t start\u2026 </p>\n<p>My results are these so long: Use a Lora rank around\
          \ 32, learning rate of 2-e5 and two epochs. Meanwhile, I trained 10 MB mixed\
          \ books and 10 MB of specialized stories, the model keeps in context, but\
          \ German is still poor, but build up bias on specialized stories. I think\
          \ that this is an awesome result for a private hobby research with one simple\
          \ RTX 4090 GPU and just five GPU hours. The hardest work is to clean the\
          \ German books. Or getting some. I cleaned up text in more than ten hours.\
          \ With my own hands. :-D</p>\n"
        raw: "That model is completely overfitted I think. I trained a blank llama2\
          \ 13b model and got better results. But I haven\u2018t used that instruction\
          \ sets, which are not bad at all. But I don\u2018t need them. I simply apply\
          \ my Lora file to an orca llama2 after training the blank model. That works\
          \ good for me. I am using the text generation ui for training, and it seems\
          \ there are some compatibility issues, so training the orca-llama2 doesn\u2018\
          t start\u2026 \n\nMy results are these so long: Use a Lora rank around 32,\
          \ learning rate of 2-e5 and two epochs. Meanwhile, I trained 10 MB mixed\
          \ books and 10 MB of specialized stories, the model keeps in context, but\
          \ German is still poor, but build up bias on specialized stories. I think\
          \ that this is an awesome result for a private hobby research with one simple\
          \ RTX 4090 GPU and just five GPU hours. The hardest work is to clean the\
          \ German books. Or getting some. I cleaned up text in more than ten hours.\
          \ With my own hands. :-D"
        updatedAt: '2023-07-31T13:01:32.253Z'
      numEdits: 0
      reactions: []
    id: 64c7b0acbaa60107a67a6e8b
    type: comment
  author: Silverspoon7
  content: "That model is completely overfitted I think. I trained a blank llama2\
    \ 13b model and got better results. But I haven\u2018t used that instruction sets,\
    \ which are not bad at all. But I don\u2018t need them. I simply apply my Lora\
    \ file to an orca llama2 after training the blank model. That works good for me.\
    \ I am using the text generation ui for training, and it seems there are some\
    \ compatibility issues, so training the orca-llama2 doesn\u2018t start\u2026 \n\
    \nMy results are these so long: Use a Lora rank around 32, learning rate of 2-e5\
    \ and two epochs. Meanwhile, I trained 10 MB mixed books and 10 MB of specialized\
    \ stories, the model keeps in context, but German is still poor, but build up\
    \ bias on specialized stories. I think that this is an awesome result for a private\
    \ hobby research with one simple RTX 4090 GPU and just five GPU hours. The hardest\
    \ work is to clean the German books. Or getting some. I cleaned up text in more\
    \ than ten hours. With my own hands. :-D"
  created_at: 2023-07-31 12:01:32+00:00
  edited: false
  hidden: false
  id: 64c7b0acbaa60107a67a6e8b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/174c0144cebdd3fd2e12c4b15c166d5f.svg
      fullname: 'Timo Pawlowski '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pawloo
      type: user
    createdAt: '2023-08-18T08:34:33.000Z'
    data:
      edited: false
      editors:
      - Pawloo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.947715163230896
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/174c0144cebdd3fd2e12c4b15c166d5f.svg
          fullname: 'Timo Pawlowski '
          isHf: false
          isPro: false
          name: Pawloo
          type: user
        html: "<blockquote>\n<p>That model is completely overfitted I think. I trained\
          \ a blank llama2 13b model and got better results. But I haven\u2018t used\
          \ that instruction sets, which are not bad at all. But I don\u2018t need\
          \ them. I simply apply my Lora file to an orca llama2 after training the\
          \ blank model. That works good for me. I am using the text generation ui\
          \ for training, and it seems there are some compatibility issues, so training\
          \ the orca-llama2 doesn\u2018t start\u2026 </p>\n<p>My results are these\
          \ so long: Use a Lora rank around 32, learning rate of 2-e5 and two epochs.\
          \ Meanwhile, I trained 10 MB mixed books and 10 MB of specialized stories,\
          \ the model keeps in context, but German is still poor, but build up bias\
          \ on specialized stories. I think that this is an awesome result for a private\
          \ hobby research with one simple RTX 4090 GPU and just five GPU hours. The\
          \ hardest work is to clean the German books. Or getting some. I cleaned\
          \ up text in more than ten hours. With my own hands. :-D</p>\n</blockquote>\n\
          <p>Sounds really interesting! Do you mind sharing your dataset? I understand\
          \ if you can not share the dataset since there might be issues 'redistributing'\
          \ those books.</p>\n"
        raw: "> That model is completely overfitted I think. I trained a blank llama2\
          \ 13b model and got better results. But I haven\u2018t used that instruction\
          \ sets, which are not bad at all. But I don\u2018t need them. I simply apply\
          \ my Lora file to an orca llama2 after training the blank model. That works\
          \ good for me. I am using the text generation ui for training, and it seems\
          \ there are some compatibility issues, so training the orca-llama2 doesn\u2018\
          t start\u2026 \n> \n> My results are these so long: Use a Lora rank around\
          \ 32, learning rate of 2-e5 and two epochs. Meanwhile, I trained 10 MB mixed\
          \ books and 10 MB of specialized stories, the model keeps in context, but\
          \ German is still poor, but build up bias on specialized stories. I think\
          \ that this is an awesome result for a private hobby research with one simple\
          \ RTX 4090 GPU and just five GPU hours. The hardest work is to clean the\
          \ German books. Or getting some. I cleaned up text in more than ten hours.\
          \ With my own hands. :-D\n\nSounds really interesting! Do you mind sharing\
          \ your dataset? I understand if you can not share the dataset since there\
          \ might be issues 'redistributing' those books."
        updatedAt: '2023-08-18T08:34:33.308Z'
      numEdits: 0
      reactions: []
    id: 64df2d195a8a9efea803b399
    type: comment
  author: Pawloo
  content: "> That model is completely overfitted I think. I trained a blank llama2\
    \ 13b model and got better results. But I haven\u2018t used that instruction sets,\
    \ which are not bad at all. But I don\u2018t need them. I simply apply my Lora\
    \ file to an orca llama2 after training the blank model. That works good for me.\
    \ I am using the text generation ui for training, and it seems there are some\
    \ compatibility issues, so training the orca-llama2 doesn\u2018t start\u2026 \n\
    > \n> My results are these so long: Use a Lora rank around 32, learning rate of\
    \ 2-e5 and two epochs. Meanwhile, I trained 10 MB mixed books and 10 MB of specialized\
    \ stories, the model keeps in context, but German is still poor, but build up\
    \ bias on specialized stories. I think that this is an awesome result for a private\
    \ hobby research with one simple RTX 4090 GPU and just five GPU hours. The hardest\
    \ work is to clean the German books. Or getting some. I cleaned up text in more\
    \ than ten hours. With my own hands. :-D\n\nSounds really interesting! Do you\
    \ mind sharing your dataset? I understand if you can not share the dataset since\
    \ there might be issues 'redistributing' those books."
  created_at: 2023-08-18 07:34:33+00:00
  edited: false
  hidden: false
  id: 64df2d195a8a9efea803b399
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03e26ad44894f94eb5ed3a0b1a9cc7a7.svg
      fullname: Uwe G.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Silverspoon7
      type: user
    createdAt: '2023-08-18T20:31:49.000Z'
    data:
      edited: false
      editors:
      - Silverspoon7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9552950263023376
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03e26ad44894f94eb5ed3a0b1a9cc7a7.svg
          fullname: Uwe G.
          isHf: false
          isPro: false
          name: Silverspoon7
          type: user
        html: "<p>Yes, i have trained 50 mb books meanwhile, Lora rank 64, but used\
          \ the model llama2 model from John p. (Jphme). German is getting better,\
          \ but is still poor. Data is a raw text collection from books, I am not\
          \ allowed to share\u2026 sorry for that\u2026 I think I will need another\
          \ 300-500 mb and 200 gpu hours\u2026 this is really hard. I am switching\
          \ now to use huggingface dataset, but scripting that costs some time, too.\
          \ Then I will be able to use a filtered oasst dataset for instruct training,\
          \ which could be a little bit more efficient. I also figured out to train\
          \ some stories twice in German and English. Loss value curve was incredible.\
          \ But results are hard to evaluate on the poor amount of translations (200kb\
          \ text, used Google translate). But it seems like the English pretrained\
          \ model can profit from that. </p>\n"
        raw: "Yes, i have trained 50 mb books meanwhile, Lora rank 64, but used the\
          \ model llama2 model from John p. (Jphme). German is getting better, but\
          \ is still poor. Data is a raw text collection from books, I am not allowed\
          \ to share\u2026 sorry for that\u2026 I think I will need another 300-500\
          \ mb and 200 gpu hours\u2026 this is really hard. I am switching now to\
          \ use huggingface dataset, but scripting that costs some time, too. Then\
          \ I will be able to use a filtered oasst dataset for instruct training,\
          \ which could be a little bit more efficient. I also figured out to train\
          \ some stories twice in German and English. Loss value curve was incredible.\
          \ But results are hard to evaluate on the poor amount of translations (200kb\
          \ text, used Google translate). But it seems like the English pretrained\
          \ model can profit from that. "
        updatedAt: '2023-08-18T20:31:49.131Z'
      numEdits: 0
      reactions: []
    id: 64dfd535c20c27fcc8abbbcd
    type: comment
  author: Silverspoon7
  content: "Yes, i have trained 50 mb books meanwhile, Lora rank 64, but used the\
    \ model llama2 model from John p. (Jphme). German is getting better, but is still\
    \ poor. Data is a raw text collection from books, I am not allowed to share\u2026\
    \ sorry for that\u2026 I think I will need another 300-500 mb and 200 gpu hours\u2026\
    \ this is really hard. I am switching now to use huggingface dataset, but scripting\
    \ that costs some time, too. Then I will be able to use a filtered oasst dataset\
    \ for instruct training, which could be a little bit more efficient. I also figured\
    \ out to train some stories twice in German and English. Loss value curve was\
    \ incredible. But results are hard to evaluate on the poor amount of translations\
    \ (200kb text, used Google translate). But it seems like the English pretrained\
    \ model can profit from that. "
  created_at: 2023-08-18 19:31:49+00:00
  edited: false
  hidden: false
  id: 64dfd535c20c27fcc8abbbcd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03e26ad44894f94eb5ed3a0b1a9cc7a7.svg
      fullname: Uwe G.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Silverspoon7
      type: user
    createdAt: '2023-08-18T20:35:11.000Z'
    data:
      edited: false
      editors:
      - Silverspoon7
      hidden: false
      identifiedLanguage:
        language: de
        probability: 0.9875631332397461
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03e26ad44894f94eb5ed3a0b1a9cc7a7.svg
          fullname: Uwe G.
          isHf: false
          isPro: false
          name: Silverspoon7
          type: user
        html: "<p>Here is a sample output from my model:</p>\n<p>Erz\xE4hl mir das\
          \ M\xE4rchen H\xE4nsel und Gretel der Gebr\xFCder grimm</p>\n<p>Chiharu\
          \ Yamada<br>Die beiden Kinder werden von ihrem Vater in eine Wildnis geschickt;\
          \ dort geraten sie in gro\xDFe Gefahr, weil sie den Wolf nicht finden k\xF6\
          nnen. Am Ende wird sie beide fast fressen, aber durch List entkommen sie\
          \ dem Tod.</p>\n"
        raw: "Here is a sample output from my model:\n\nErz\xE4hl mir das M\xE4rchen\
          \ H\xE4nsel und Gretel der Gebr\xFCder grimm\n\nChiharu Yamada\nDie beiden\
          \ Kinder werden von ihrem Vater in eine Wildnis geschickt; dort geraten\
          \ sie in gro\xDFe Gefahr, weil sie den Wolf nicht finden k\xF6nnen. Am Ende\
          \ wird sie beide fast fressen, aber durch List entkommen sie dem Tod."
        updatedAt: '2023-08-18T20:35:11.475Z'
      numEdits: 0
      reactions: []
    id: 64dfd5ff4ab6962b21aaa9e6
    type: comment
  author: Silverspoon7
  content: "Here is a sample output from my model:\n\nErz\xE4hl mir das M\xE4rchen\
    \ H\xE4nsel und Gretel der Gebr\xFCder grimm\n\nChiharu Yamada\nDie beiden Kinder\
    \ werden von ihrem Vater in eine Wildnis geschickt; dort geraten sie in gro\xDF\
    e Gefahr, weil sie den Wolf nicht finden k\xF6nnen. Am Ende wird sie beide fast\
    \ fressen, aber durch List entkommen sie dem Tod."
  created_at: 2023-08-18 19:35:11+00:00
  edited: false
  hidden: false
  id: 64dfd5ff4ab6962b21aaa9e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8d8f8470cf1a901f9f10966c5596dcc1.svg
      fullname: Blake Jones
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: blakepeerlogic
      type: user
    createdAt: '2023-10-05T17:14:03.000Z'
    data:
      edited: false
      editors:
      - blakepeerlogic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9638432264328003
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8d8f8470cf1a901f9f10966c5596dcc1.svg
          fullname: Blake Jones
          isHf: false
          isPro: false
          name: blakepeerlogic
          type: user
        html: '<p>For what it''s worth, I''ve done a lot of experimentation with this
          and what I found is the following:</p>

          <ul>

          <li>if you implement retry logic, it''ll generally output something non-blank
          eventually</li>

          <li>the rate at which it outputs blanks is directly correlated to the token-length
          of the prompt. I found that it seems to fully hang up after about 400 tokens
          on my machine (M1 MacBook Pro). I''m modifying my prompts in my project
          to be broken up at as fine a level as possible.</li>

          </ul>

          '
        raw: 'For what it''s worth, I''ve done a lot of experimentation with this
          and what I found is the following:

          - if you implement retry logic, it''ll generally output something non-blank
          eventually

          - the rate at which it outputs blanks is directly correlated to the token-length
          of the prompt. I found that it seems to fully hang up after about 400 tokens
          on my machine (M1 MacBook Pro). I''m modifying my prompts in my project
          to be broken up at as fine a level as possible.'
        updatedAt: '2023-10-05T17:14:03.941Z'
      numEdits: 0
      reactions: []
    id: 651eeedb82dbe82d3f5575df
    type: comment
  author: blakepeerlogic
  content: 'For what it''s worth, I''ve done a lot of experimentation with this and
    what I found is the following:

    - if you implement retry logic, it''ll generally output something non-blank eventually

    - the rate at which it outputs blanks is directly correlated to the token-length
    of the prompt. I found that it seems to fully hang up after about 400 tokens on
    my machine (M1 MacBook Pro). I''m modifying my prompts in my project to be broken
    up at as fine a level as possible.'
  created_at: 2023-10-05 16:14:03+00:00
  edited: false
  hidden: false
  id: 651eeedb82dbe82d3f5575df
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/llama-2-13B-German-Assistant-v2-GGML
repo_type: model
status: open
target_branch: null
title: Get only empty string '' response back - Anyone used them successfully?
