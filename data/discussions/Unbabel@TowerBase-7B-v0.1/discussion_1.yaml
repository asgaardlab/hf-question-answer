!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bpop
conflicting_files: null
created_at: 2024-01-15 22:28:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ec70d96c0e78538fc8dd9153bb4220f.svg
      fullname: Ben Peters
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bpop
      type: user
    createdAt: '2024-01-15T22:28:28.000Z'
    data:
      edited: false
      editors:
      - bpop
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8557373881340027
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ec70d96c0e78538fc8dd9153bb4220f.svg
          fullname: Ben Peters
          isHf: false
          isPro: false
          name: bpop
          type: user
        html: "<p>Hello everyone,</p>\n<p>Using pipelines for inference is currently\
          \ broken for batch sizes greater than 1. For example, you cannot do this:</p>\n\
          <pre><code>model = AutoModelForCausalLM.from_pretrained(\"Unbabel/TowerBase-7B-v0.1\"\
          )\ntokenizer = AutoTokenizer.from_pretrained(\"Unbabel/TowerBase-7B-v0.1\"\
          )\npipe = pipeline(\n            \"text-generation\",\n            model=model,\n\
          \            tokenizer=tokenizer,\n            batch_size=2\n)\nexamples\
          \ = [\"English: My name is TowerBase.\\nPortuguese:\", \"English: These\
          \ are my friends, NLLB, GPT, and wFST.\\nPortuguese:\"]\nout = pipe(examples)\n\
          </code></pre>\n<p>The issue seems to be related to a mismatch between the\
          \ vocabulary the tokenizer uses and what is expected by the model. The tokenizer\
          \ uses 32004 as its pad_id, but the vocab size is only 32000. Passing a\
          \ padded batch of mixed-length sequences consequently produces an indexing\
          \ error in TowerBase's embedding layer.</p>\n<p>I worked around this issue\
          \ by setting <code>tokenizer.pad_token_id = tokenizer.eos_token_id</code>.\
          \ I assume that TowerInstruct has the same bug and short-term fix, but I\
          \ haven't tested it yet.</p>\n"
        raw: "Hello everyone,\r\n\r\nUsing pipelines for inference is currently broken\
          \ for batch sizes greater than 1. For example, you cannot do this:\r\n\r\
          \n```\r\nmodel = AutoModelForCausalLM.from_pretrained(\"Unbabel/TowerBase-7B-v0.1\"\
          )\r\ntokenizer = AutoTokenizer.from_pretrained(\"Unbabel/TowerBase-7B-v0.1\"\
          )\r\npipe = pipeline(\r\n            \"text-generation\",\r\n          \
          \  model=model,\r\n            tokenizer=tokenizer,\r\n            batch_size=2\r\
          \n)\r\nexamples = [\"English: My name is TowerBase.\\nPortuguese:\", \"\
          English: These are my friends, NLLB, GPT, and wFST.\\nPortuguese:\"]\r\n\
          out = pipe(examples)\r\n```\r\n\r\nThe issue seems to be related to a mismatch\
          \ between the vocabulary the tokenizer uses and what is expected by the\
          \ model. The tokenizer uses 32004 as its pad_id, but the vocab size is only\
          \ 32000. Passing a padded batch of mixed-length sequences consequently produces\
          \ an indexing error in TowerBase's embedding layer.\r\n\r\nI worked around\
          \ this issue by setting `tokenizer.pad_token_id = tokenizer.eos_token_id`.\
          \ I assume that TowerInstruct has the same bug and short-term fix, but I\
          \ haven't tested it yet."
        updatedAt: '2024-01-15T22:28:28.256Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jshnaoko
    id: 65a5b18cc980ec22966b4a8c
    type: comment
  author: bpop
  content: "Hello everyone,\r\n\r\nUsing pipelines for inference is currently broken\
    \ for batch sizes greater than 1. For example, you cannot do this:\r\n\r\n```\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(\"Unbabel/TowerBase-7B-v0.1\")\r\
    \ntokenizer = AutoTokenizer.from_pretrained(\"Unbabel/TowerBase-7B-v0.1\")\r\n\
    pipe = pipeline(\r\n            \"text-generation\",\r\n            model=model,\r\
    \n            tokenizer=tokenizer,\r\n            batch_size=2\r\n)\r\nexamples\
    \ = [\"English: My name is TowerBase.\\nPortuguese:\", \"English: These are my\
    \ friends, NLLB, GPT, and wFST.\\nPortuguese:\"]\r\nout = pipe(examples)\r\n```\r\
    \n\r\nThe issue seems to be related to a mismatch between the vocabulary the tokenizer\
    \ uses and what is expected by the model. The tokenizer uses 32004 as its pad_id,\
    \ but the vocab size is only 32000. Passing a padded batch of mixed-length sequences\
    \ consequently produces an indexing error in TowerBase's embedding layer.\r\n\r\
    \nI worked around this issue by setting `tokenizer.pad_token_id = tokenizer.eos_token_id`.\
    \ I assume that TowerInstruct has the same bug and short-term fix, but I haven't\
    \ tested it yet."
  created_at: 2024-01-15 22:28:28+00:00
  edited: false
  hidden: false
  id: 65a5b18cc980ec22966b4a8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/23a28a84d1d465c3beb2f2fbb6723a75.svg
      fullname: "Jos\xE9 Maria Pombal"
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jmprcp
      type: user
    createdAt: '2024-01-16T10:28:59.000Z'
    data:
      edited: false
      editors:
      - jmprcp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5364904403686523
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/23a28a84d1d465c3beb2f2fbb6723a75.svg
          fullname: "Jos\xE9 Maria Pombal"
          isHf: false
          isPro: false
          name: jmprcp
          type: user
        html: '<p>Thanks for raising this. It should now be fixed, without needing
          to explicitly set <code>tokenizer.pad_token_id = tokenizer.eos_token_id</code>.
          (<a href="https://huggingface.co/Unbabel/TowerBase-7B-v0.1/commit/2837006f6f8e9ed6e637ab8fcf9a6bf22e31e4d8">https://huggingface.co/Unbabel/TowerBase-7B-v0.1/commit/2837006f6f8e9ed6e637ab8fcf9a6bf22e31e4d8</a>)</p>

          '
        raw: 'Thanks for raising this. It should now be fixed, without needing to
          explicitly set `tokenizer.pad_token_id = tokenizer.eos_token_id`. (https://huggingface.co/Unbabel/TowerBase-7B-v0.1/commit/2837006f6f8e9ed6e637ab8fcf9a6bf22e31e4d8)

          '
        updatedAt: '2024-01-16T10:28:59.887Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65a65a6bd927fd04c348216c
    id: 65a65a6bd927fd04c3482168
    type: comment
  author: jmprcp
  content: 'Thanks for raising this. It should now be fixed, without needing to explicitly
    set `tokenizer.pad_token_id = tokenizer.eos_token_id`. (https://huggingface.co/Unbabel/TowerBase-7B-v0.1/commit/2837006f6f8e9ed6e637ab8fcf9a6bf22e31e4d8)

    '
  created_at: 2024-01-16 10:28:59+00:00
  edited: false
  hidden: false
  id: 65a65a6bd927fd04c3482168
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/23a28a84d1d465c3beb2f2fbb6723a75.svg
      fullname: "Jos\xE9 Maria Pombal"
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jmprcp
      type: user
    createdAt: '2024-01-16T10:28:59.000Z'
    data:
      status: closed
    id: 65a65a6bd927fd04c348216c
    type: status-change
  author: jmprcp
  created_at: 2024-01-16 10:28:59+00:00
  id: 65a65a6bd927fd04c348216c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Unbabel/TowerBase-7B-v0.1
repo_type: model
status: closed
target_branch: null
title: Pipelines fail with batch > 1
