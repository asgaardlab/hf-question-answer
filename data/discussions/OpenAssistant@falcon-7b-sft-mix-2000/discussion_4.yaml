!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Eduardo-AC
conflicting_files: null
created_at: 2023-07-20 08:48:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8f3e448b0b5caaabe62bb5e01182f0c5.svg
      fullname: Eduardo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Eduardo-AC
      type: user
    createdAt: '2023-07-20T09:48:51.000Z'
    data:
      edited: false
      editors:
      - Eduardo-AC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6600995659828186
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8f3e448b0b5caaabe62bb5e01182f0c5.svg
          fullname: Eduardo
          isHf: false
          isPro: false
          name: Eduardo-AC
          type: user
        html: "<p>Good morning, </p>\n<p>I am writing to ask how to overcome the issue\
          \ popping up while trying to run the pipeline described in the model card.</p>\n\
          <pre><code>from transformers import AutoTokenizer\nimport transformers\n\
          import torch\n\nmodel = \"OpenAssistant/falcon-7b-sft-mix-2000\"\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
          auto\",\n)\n\ninput_text=\"&lt;|prompter|&gt;What is a meme, and what's\
          \ the history behind this word?&lt;|endoftext|&gt;&lt;|assistant|&gt;\"\n\
          \nsequences = pipeline(\n    input_text,\n    max_length=500,\n    do_sample=True,\n\
          \    return_full_text=False,\n    top_k=10,\n    num_return_sequences=1,\n\
          \    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n  \
          \  print(f\"Result: {seq['generated_text']}\")\n</code></pre>\n<p>Which\
          \ triggers the following error</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64b901e4296e311ff6677325/s589H8v7KlvOszucZCSmc.png\"\
          ><img alt=\"Screenshot 2023-07-20 at 10.47.22.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64b901e4296e311ff6677325/s589H8v7KlvOszucZCSmc.png\"\
          ></a></p>\n<p>How did you manage to solve this issue to run the pipeline\
          \ successfully? </p>\n"
        raw: "Good morning, \r\n\r\nI am writing to ask how to overcome the issue\
          \ popping up while trying to run the pipeline described in the model card.\r\
          \n\r\n```\r\nfrom transformers import AutoTokenizer\r\nimport transformers\r\
          \nimport torch\r\n\r\nmodel = \"OpenAssistant/falcon-7b-sft-mix-2000\"\r\
          \n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\npipeline = transformers.pipeline(\r\
          \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
          \n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n   \
          \ device_map=\"auto\",\r\n)\r\n\r\ninput_text=\"<|prompter|>What is a meme,\
          \ and what's the history behind this word?<|endoftext|><|assistant|>\"\r\
          \n\r\nsequences = pipeline(\r\n    input_text,\r\n    max_length=500,\r\n\
          \    do_sample=True,\r\n    return_full_text=False,\r\n    top_k=10,\r\n\
          \    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
          \n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
          )\r\n```\r\n\r\nWhich triggers the following error\r\n\r\n\r\n![Screenshot\
          \ 2023-07-20 at 10.47.22.png](https://cdn-uploads.huggingface.co/production/uploads/64b901e4296e311ff6677325/s589H8v7KlvOszucZCSmc.png)\r\
          \n\r\nHow did you manage to solve this issue to run the pipeline successfully?\
          \ \r\n"
        updatedAt: '2023-07-20T09:48:51.113Z'
      numEdits: 0
      reactions: []
    id: 64b90303ace99c0723ab2cdf
    type: comment
  author: Eduardo-AC
  content: "Good morning, \r\n\r\nI am writing to ask how to overcome the issue popping\
    \ up while trying to run the pipeline described in the model card.\r\n\r\n```\r\
    \nfrom transformers import AutoTokenizer\r\nimport transformers\r\nimport torch\r\
    \n\r\nmodel = \"OpenAssistant/falcon-7b-sft-mix-2000\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\
    \npipeline = transformers.pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
    \n    tokenizer=tokenizer,\r\n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\
    \n    device_map=\"auto\",\r\n)\r\n\r\ninput_text=\"<|prompter|>What is a meme,\
    \ and what's the history behind this word?<|endoftext|><|assistant|>\"\r\n\r\n\
    sequences = pipeline(\r\n    input_text,\r\n    max_length=500,\r\n    do_sample=True,\r\
    \n    return_full_text=False,\r\n    top_k=10,\r\n    num_return_sequences=1,\r\
    \n    eos_token_id=tokenizer.eos_token_id,\r\n)\r\nfor seq in sequences:\r\n \
    \   print(f\"Result: {seq['generated_text']}\")\r\n```\r\n\r\nWhich triggers the\
    \ following error\r\n\r\n\r\n![Screenshot 2023-07-20 at 10.47.22.png](https://cdn-uploads.huggingface.co/production/uploads/64b901e4296e311ff6677325/s589H8v7KlvOszucZCSmc.png)\r\
    \n\r\nHow did you manage to solve this issue to run the pipeline successfully?\
    \ \r\n"
  created_at: 2023-07-20 08:48:51+00:00
  edited: false
  hidden: false
  id: 64b90303ace99c0723ab2cdf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c48bdd59b89cfe7405c14e7797bfcf17.svg
      fullname: Braxton Brown
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThatOneShortGuy
      type: user
    createdAt: '2023-07-21T10:10:36.000Z'
    data:
      edited: false
      editors:
      - ThatOneShortGuy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8858980536460876
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c48bdd59b89cfe7405c14e7797bfcf17.svg
          fullname: Braxton Brown
          isHf: false
          isPro: false
          name: ThatOneShortGuy
          type: user
        html: '<p>Is everything updated? Especially check that <code>transformers</code>
          is updated. </p>

          '
        raw: 'Is everything updated? Especially check that `transformers` is updated. '
        updatedAt: '2023-07-21T10:10:36.229Z'
      numEdits: 0
      reactions: []
    id: 64ba599c4a5c90676c581974
    type: comment
  author: ThatOneShortGuy
  content: 'Is everything updated? Especially check that `transformers` is updated. '
  created_at: 2023-07-21 09:10:36+00:00
  edited: false
  hidden: false
  id: 64ba599c4a5c90676c581974
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8f3e448b0b5caaabe62bb5e01182f0c5.svg
      fullname: Eduardo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Eduardo-AC
      type: user
    createdAt: '2023-07-24T08:25:13.000Z'
    data:
      edited: false
      editors:
      - Eduardo-AC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9215355515480042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8f3e448b0b5caaabe62bb5e01182f0c5.svg
          fullname: Eduardo
          isHf: false
          isPro: false
          name: Eduardo-AC
          type: user
        html: '<p>Yes, everything was installed from Scrath for a Hackathon project
          using Python 3.11</p>

          '
        raw: Yes, everything was installed from Scrath for a Hackathon project using
          Python 3.11
        updatedAt: '2023-07-24T08:25:13.506Z'
      numEdits: 0
      reactions: []
    id: 64be356994c0e3be4a02ba6f
    type: comment
  author: Eduardo-AC
  content: Yes, everything was installed from Scrath for a Hackathon project using
    Python 3.11
  created_at: 2023-07-24 07:25:13+00:00
  edited: false
  hidden: false
  id: 64be356994c0e3be4a02ba6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f91ee12ddd76fb9aabf4f8e8cb538889.svg
      fullname: Patrice Ferlet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Metal3d
      type: user
    createdAt: '2023-08-14T12:49:28.000Z'
    data:
      edited: false
      editors:
      - Metal3d
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.41712120175361633
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f91ee12ddd76fb9aabf4f8e8cb538889.svg
          fullname: Patrice Ferlet
          isHf: false
          isPro: false
          name: Metal3d
          type: user
        html: '<p>Exactly the same error for me<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6337ebed171879571956a971/3Nq8q1Ls9HV21svmRz4DR.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6337ebed171879571956a971/3Nq8q1Ls9HV21svmRz4DR.png"></a></p>

          '
        raw: "Exactly the same error for me \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6337ebed171879571956a971/3Nq8q1Ls9HV21svmRz4DR.png)\n"
        updatedAt: '2023-08-14T12:49:28.121Z'
      numEdits: 0
      reactions: []
    id: 64da22d8e7bc8544f97027dc
    type: comment
  author: Metal3d
  content: "Exactly the same error for me \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6337ebed171879571956a971/3Nq8q1Ls9HV21svmRz4DR.png)\n"
  created_at: 2023-08-14 11:49:28+00:00
  edited: false
  hidden: false
  id: 64da22d8e7bc8544f97027dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c48bdd59b89cfe7405c14e7797bfcf17.svg
      fullname: Braxton Brown
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThatOneShortGuy
      type: user
    createdAt: '2023-08-14T14:29:11.000Z'
    data:
      edited: true
      editors:
      - ThatOneShortGuy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7007825374603271
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c48bdd59b89cfe7405c14e7797bfcf17.svg
          fullname: Braxton Brown
          isHf: false
          isPro: false
          name: ThatOneShortGuy
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Metal3d&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Metal3d\">@<span class=\"\
          underline\">Metal3d</span></a></span>\n\n\t</span></span> This is strange.\
          \ It seems to work fine for me with transformers 4.31.0. Are you sure you\
          \ copied the sample code right?</p>\n<p>The first guy was also using a flask\
          \ server, so his code couldn't be verified.</p>\n<p>If you really want to\
          \ use the model, try loading in without the pipeline:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">import</span> torch\n<span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = <span\
          \ class=\"hljs-string\">\"OpenAssistant/falcon-7b-sft-mix-2000\"</span>\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model_name, padding_size=<span\
          \ class=\"hljs-string\">'left'</span>)\ntokenizer.pad_token = tokenizer.eos_token\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n           \
          \                                  torch_dtype=torch.bfloat16,\n       \
          \                                      device_map=<span class=\"hljs-string\"\
          >\"auto\"</span>,\n                                             trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>\n                                 \
          \            ).half()\n\ntokens = tokenizer.encode(<span class=\"hljs-string\"\
          >\"Hello, my dog is cute\"</span>, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>, padding=<span class=\"hljs-literal\">True</span>, truncation=<span\
          \ class=\"hljs-literal\">True</span>).to(model.device)\ngen = model.generate(tokens,\
          \ do_sample=<span class=\"hljs-literal\">True</span>, max_length=<span class=\"\
          hljs-number\">100</span>, top_p=<span class=\"hljs-number\">0.95</span>,\
          \ top_k=<span class=\"hljs-number\">50</span>, num_return_sequences=<span\
          \ class=\"hljs-number\">1</span>, no_repeat_ngram_size=<span class=\"hljs-number\"\
          >2</span>, early_stopping=<span class=\"hljs-literal\">True</span>)\n<span\
          \ class=\"hljs-built_in\">print</span>(tokenizer.decode(gen[<span class=\"\
          hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>))\n</code></pre>\n"
        raw: "@Metal3d This is strange. It seems to work fine for me with transformers\
          \ 4.31.0. Are you sure you copied the sample code right?\n\nThe first guy\
          \ was also using a flask server, so his code couldn't be verified.\n\nIf\
          \ you really want to use the model, try loading in without the pipeline:\n\
          ```python\nimport torch\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\nmodel_name = \"OpenAssistant/falcon-7b-sft-mix-2000\"\
          \n\ntokenizer = AutoTokenizer.from_pretrained(model_name, padding_size='left')\n\
          tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n\
          \                                             torch_dtype=torch.bfloat16,\n\
          \                                             device_map=\"auto\",\n   \
          \                                          trust_remote_code=True\n    \
          \                                         ).half()\n\ntokens = tokenizer.encode(\"\
          Hello, my dog is cute\", return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n\
          gen = model.generate(tokens, do_sample=True, max_length=100, top_p=0.95,\
          \ top_k=50, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n\
          print(tokenizer.decode(gen[0], skip_special_tokens=True))\n```\n\n\n"
        updatedAt: '2023-08-14T14:36:12.896Z'
      numEdits: 3
      reactions: []
    id: 64da3a37d0645cd60c6f3c08
    type: comment
  author: ThatOneShortGuy
  content: "@Metal3d This is strange. It seems to work fine for me with transformers\
    \ 4.31.0. Are you sure you copied the sample code right?\n\nThe first guy was\
    \ also using a flask server, so his code couldn't be verified.\n\nIf you really\
    \ want to use the model, try loading in without the pipeline:\n```python\nimport\
    \ torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name\
    \ = \"OpenAssistant/falcon-7b-sft-mix-2000\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name,\
    \ padding_size='left')\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n\
    \                                             torch_dtype=torch.bfloat16,\n  \
    \                                           device_map=\"auto\",\n           \
    \                                  trust_remote_code=True\n                  \
    \                           ).half()\n\ntokens = tokenizer.encode(\"Hello, my\
    \ dog is cute\", return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n\
    gen = model.generate(tokens, do_sample=True, max_length=100, top_p=0.95, top_k=50,\
    \ num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\nprint(tokenizer.decode(gen[0],\
    \ skip_special_tokens=True))\n```\n\n\n"
  created_at: 2023-08-14 13:29:11+00:00
  edited: true
  hidden: false
  id: 64da3a37d0645cd60c6f3c08
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
      fullname: Deepak  Kaura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkaura26
      type: user
    createdAt: '2023-08-16T15:43:13.000Z'
    data:
      edited: false
      editors:
      - deepakkaura26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8568074107170105
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
          fullname: Deepak  Kaura
          isHf: false
          isPro: false
          name: deepakkaura26
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ThatOneShortGuy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ThatOneShortGuy\"\
          >@<span class=\"underline\">ThatOneShortGuy</span></a></span>\n\n\t</span></span>\
          \ can I run this model on CPU ? </p>\n"
        raw: '@ThatOneShortGuy can I run this model on CPU ? '
        updatedAt: '2023-08-16T15:43:13.982Z'
      numEdits: 0
      reactions: []
    id: 64dcee91c336f18d54faf2a9
    type: comment
  author: deepakkaura26
  content: '@ThatOneShortGuy can I run this model on CPU ? '
  created_at: 2023-08-16 14:43:13+00:00
  edited: false
  hidden: false
  id: 64dcee91c336f18d54faf2a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c48bdd59b89cfe7405c14e7797bfcf17.svg
      fullname: Braxton Brown
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThatOneShortGuy
      type: user
    createdAt: '2023-08-16T19:44:21.000Z'
    data:
      edited: false
      editors:
      - ThatOneShortGuy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9643129706382751
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c48bdd59b89cfe7405c14e7797bfcf17.svg
          fullname: Braxton Brown
          isHf: false
          isPro: false
          name: ThatOneShortGuy
          type: user
        html: "<p>Yes, you theoretically can, but it is <em>extremely</em> slow.</p>\n\
          <p>Practically speaking, no.</p>\n<p>It's been 16 minutes and not a single\
          \ token has been generated on my 10600k. It may be worth noting that it\
          \ is single threaded, but \U0001F937\u200D\u2642\uFE0F</p>\n"
        raw: "Yes, you theoretically can, but it is *extremely* slow.\n\nPractically\
          \ speaking, no.\n\nIt's been 16 minutes and not a single token has been\
          \ generated on my 10600k. It may be worth noting that it is single threaded,\
          \ but \U0001F937\u200D\u2642\uFE0F"
        updatedAt: '2023-08-16T19:44:21.042Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Eduardo-AC
    id: 64dd271530bd3fef69714b83
    type: comment
  author: ThatOneShortGuy
  content: "Yes, you theoretically can, but it is *extremely* slow.\n\nPractically\
    \ speaking, no.\n\nIt's been 16 minutes and not a single token has been generated\
    \ on my 10600k. It may be worth noting that it is single threaded, but \U0001F937\
    \u200D\u2642\uFE0F"
  created_at: 2023-08-16 18:44:21+00:00
  edited: false
  hidden: false
  id: 64dd271530bd3fef69714b83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8f3e448b0b5caaabe62bb5e01182f0c5.svg
      fullname: Eduardo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Eduardo-AC
      type: user
    createdAt: '2023-08-17T17:59:44.000Z'
    data:
      edited: false
      editors:
      - Eduardo-AC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7636494040489197
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8f3e448b0b5caaabe62bb5e01182f0c5.svg
          fullname: Eduardo
          isHf: false
          isPro: false
          name: Eduardo-AC
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;Metal3d&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Metal3d\"\
          >@<span class=\"underline\">Metal3d</span></a></span>\n\n\t</span></span>\
          \ This is strange. It seems to work fine for me with transformers 4.31.0.\
          \ Are you sure you copied the sample code right?</p>\n<p>The first guy was\
          \ also using a flask server, so his code couldn't be verified.</p>\n<p>If\
          \ you really want to use the model, try loading in without the pipeline:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span>\
          \ torch\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"\
          hljs-keyword\">import</span> AutoModelForCausalLM, AutoTokenizer\n\nmodel_name\
          \ = <span class=\"hljs-string\">\"OpenAssistant/falcon-7b-sft-mix-2000\"\
          </span>\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, padding_size=<span\
          \ class=\"hljs-string\">'left'</span>)\ntokenizer.pad_token = tokenizer.eos_token\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n           \
          \                                  torch_dtype=torch.bfloat16,\n       \
          \                                      device_map=<span class=\"hljs-string\"\
          >\"auto\"</span>,\n                                             trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>\n                                 \
          \            ).half()\n\ntokens = tokenizer.encode(<span class=\"hljs-string\"\
          >\"Hello, my dog is cute\"</span>, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>, padding=<span class=\"hljs-literal\">True</span>, truncation=<span\
          \ class=\"hljs-literal\">True</span>).to(model.device)\ngen = model.generate(tokens,\
          \ do_sample=<span class=\"hljs-literal\">True</span>, max_length=<span class=\"\
          hljs-number\">100</span>, top_p=<span class=\"hljs-number\">0.95</span>,\
          \ top_k=<span class=\"hljs-number\">50</span>, num_return_sequences=<span\
          \ class=\"hljs-number\">1</span>, no_repeat_ngram_size=<span class=\"hljs-number\"\
          >2</span>, early_stopping=<span class=\"hljs-literal\">True</span>)\n<span\
          \ class=\"hljs-built_in\">print</span>(tokenizer.decode(gen[<span class=\"\
          hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>))\n</code></pre>\n</blockquote>\n<p>Yes, everything following\
          \ the instructions give. I will give a try to 4.31.0 but as one  model works\
          \ and another not. I believe the problem must be in the instructions or.\
          \ The model itself</p>\n"
        raw: "> @Metal3d This is strange. It seems to work fine for me with transformers\
          \ 4.31.0. Are you sure you copied the sample code right?\n> \n> The first\
          \ guy was also using a flask server, so his code couldn't be verified.\n\
          > \n> If you really want to use the model, try loading in without the pipeline:\n\
          > ```python\n> import torch\n> from transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n> \n> model_name = \"OpenAssistant/falcon-7b-sft-mix-2000\"\
          \n> \n> tokenizer = AutoTokenizer.from_pretrained(model_name, padding_size='left')\n\
          > tokenizer.pad_token = tokenizer.eos_token\n> \n> model = AutoModelForCausalLM.from_pretrained(model_name,\n\
          >                                              torch_dtype=torch.bfloat16,\n\
          >                                              device_map=\"auto\",\n> \
          \                                             trust_remote_code=True\n>\
          \                                              ).half()\n> \n> tokens =\
          \ tokenizer.encode(\"Hello, my dog is cute\", return_tensors=\"pt\", padding=True,\
          \ truncation=True).to(model.device)\n> gen = model.generate(tokens, do_sample=True,\
          \ max_length=100, top_p=0.95, top_k=50, num_return_sequences=1, no_repeat_ngram_size=2,\
          \ early_stopping=True)\n> print(tokenizer.decode(gen[0], skip_special_tokens=True))\n\
          > ```\n\nYes, everything following the instructions give. I will give a\
          \ try to 4.31.0 but as one  model works and another not. I believe the problem\
          \ must be in the instructions or. The model itself"
        updatedAt: '2023-08-17T17:59:44.404Z'
      numEdits: 0
      reactions: []
    id: 64de6010842e7056587eda41
    type: comment
  author: Eduardo-AC
  content: "> @Metal3d This is strange. It seems to work fine for me with transformers\
    \ 4.31.0. Are you sure you copied the sample code right?\n> \n> The first guy\
    \ was also using a flask server, so his code couldn't be verified.\n> \n> If you\
    \ really want to use the model, try loading in without the pipeline:\n> ```python\n\
    > import torch\n> from transformers import AutoModelForCausalLM, AutoTokenizer\n\
    > \n> model_name = \"OpenAssistant/falcon-7b-sft-mix-2000\"\n> \n> tokenizer =\
    \ AutoTokenizer.from_pretrained(model_name, padding_size='left')\n> tokenizer.pad_token\
    \ = tokenizer.eos_token\n> \n> model = AutoModelForCausalLM.from_pretrained(model_name,\n\
    >                                              torch_dtype=torch.bfloat16,\n>\
    \                                              device_map=\"auto\",\n>       \
    \                                       trust_remote_code=True\n>            \
    \                                  ).half()\n> \n> tokens = tokenizer.encode(\"\
    Hello, my dog is cute\", return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n\
    > gen = model.generate(tokens, do_sample=True, max_length=100, top_p=0.95, top_k=50,\
    \ num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n> print(tokenizer.decode(gen[0],\
    \ skip_special_tokens=True))\n> ```\n\nYes, everything following the instructions\
    \ give. I will give a try to 4.31.0 but as one  model works and another not. I\
    \ believe the problem must be in the instructions or. The model itself"
  created_at: 2023-08-17 16:59:44+00:00
  edited: false
  hidden: false
  id: 64de6010842e7056587eda41
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: OpenAssistant/falcon-7b-sft-mix-2000
repo_type: model
status: open
target_branch: null
title: Error using example code in model card
