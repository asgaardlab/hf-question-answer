!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YarKo69
conflicting_files: null
created_at: 2023-11-21 16:03:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aeaf6a78dcb30edd8431a30781938989.svg
      fullname: Yaroslav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YarKo69
      type: user
    createdAt: '2023-11-21T16:03:58.000Z'
    data:
      edited: false
      editors:
      - YarKo69
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.1923779398202896
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aeaf6a78dcb30edd8431a30781938989.svg
          fullname: Yaroslav
          isHf: false
          isPro: false
          name: YarKo69
          type: user
        html: "<p>\u041F\u043E\u0441\u043B\u0435 \u0437\u0430\u0433\u0440\u0443\u0437\
          \u043A\u0438 \u043C\u043E\u0434\u0435\u043B\u0438, \u043F\u0440\u0438 \u043B\
          \u044E\u0431\u043E\u043C \u0437\u0430\u043F\u0440\u043E\u0441\u0435 \u0432\
          \u044B\u043B\u0435\u0437\u0430\u0435\u0442 \u043E\u0448\u0438\u0431\u043A\
          \u0430 (\u0441\u043C. \u043A\u043E\u0434 \u043D\u0438\u0436\u0435). \u041F\
          \u043E\u0434\u0441\u043A\u0430\u0436\u0438\u0442\u0435 \u043F\u043E\u0436\
          \u0430\u043B\u0443\u0439\u0441\u0442\u0430 \u0441 \u0447\u0435\u043C \u044D\
          \u0442\u043E \u043C\u043E\u0436\u0435\u0442 \u0431\u044B\u0442\u044C \u0441\
          \u0432\u044F\u0437\u0430\u043D\u043E \u0438 \u043A\u0430\u043A \u044D\u0442\
          \u043E \u0438\u0441\u043F\u0440\u0430\u0432\u0438\u0442\u044C. \u041E\u0441\
          \u0442\u0430\u043B\u044C\u043D\u044B\u0435 \u043C\u043E\u0434\u0435\u043B\
          \u0438 saiga \u0440\u0430\u0431\u043E\u0442\u0430\u044E\u0442 \u043E\u0442\
          \u043B\u0438\u0447\u043D\u043E \u043F\u0440\u0438 \u0442\u0435\u0445 \u0436\
          \u0435 \u0437\u0430\u043F\u0440\u043E\u0441\u0430\u0445.<br><a rel=\"nofollow\"\
          \ href=\"https://www.kaggle.com/code/yarasslove/extraction-saiga\">\u041A\
          \u043E\u0434 \u0437\u0430\u043F\u0443\u0441\u043A\u0430 \u043C\u043E\u0434\
          \u0435\u043B\u0438 \u043D\u0430 Kaggle</a></p>\n<pre><code class=\"language-py\"\
          >RuntimeError                              Traceback (most recent call last)\n\
          Cell In[<span class=\"hljs-number\">15</span>], line <span class=\"hljs-number\"\
          >1</span>\n----&gt; <span class=\"hljs-number\">1</span> get_message([<span\
          \ class=\"hljs-string\">'\u041F\u0440\u0438\u0432\u0435\u0442, \u0441\u043A\
          \u043E\u043B\u044C\u043A\u043E \u0432\u0435\u0441\u0438\u0442 \u0447\u0435\
          \u043B\u043E\u0432\u0435\u043A \u0432 \u0441\u0440\u0435\u0434\u043D\u0435\
          \u043C?'</span>])\n\nFile &lt;timed <span class=\"hljs-built_in\">exec</span>&gt;:<span\
          \ class=\"hljs-number\">10</span>, <span class=\"hljs-keyword\">in</span>\
          \ get_message(inputs)\n\nCell In[<span class=\"hljs-number\">7</span>],\
          \ line <span class=\"hljs-number\">54</span>, <span class=\"hljs-keyword\"\
          >in</span> generate(model, tokenizer, prompt, generation_config)\n     <span\
          \ class=\"hljs-number\">51</span> <span class=\"hljs-comment\">#print(data)</span>\n\
          \     <span class=\"hljs-number\">52</span> data = {k: v.to(device) <span\
          \ class=\"hljs-keyword\">for</span> k, v <span class=\"hljs-keyword\">in</span>\
          \ data.items()}\n---&gt; <span class=\"hljs-number\">54</span> output_ids\
          \ = model.generate(\n     <span class=\"hljs-number\">55</span>     **data,\n\
          \     <span class=\"hljs-number\">56</span>     generation_config = generation_config\n\
          \     <span class=\"hljs-number\">57</span> <span class=\"hljs-comment\"\
          >#         remove_invalid_values = True</span>\n     <span class=\"hljs-number\"\
          >58</span> )[<span class=\"hljs-number\">0</span>]\n     <span class=\"\
          hljs-number\">59</span> output_ids = output_ids[<span class=\"hljs-built_in\"\
          >len</span>(data[<span class=\"hljs-string\">\"input_ids\"</span>][<span\
          \ class=\"hljs-number\">0</span>]):]\n     <span class=\"hljs-number\">60</span>\
          \ output = tokenizer.decode(output_ids, skip_special_tokens=<span class=\"\
          hljs-literal\">True</span>)\n\nFile /opt/conda/lib/python3<span class=\"\
          hljs-number\">.10</span>/site-packages/peft/peft_model.py:<span class=\"\
          hljs-number\">975</span>, <span class=\"hljs-keyword\">in</span> PeftModelForCausalLM.generate(self,\
          \ **kwargs)\n    <span class=\"hljs-number\">973</span>     self.base_model.generation_config\
          \ = self.generation_config\n    <span class=\"hljs-number\">974</span> <span\
          \ class=\"hljs-keyword\">try</span>:\n--&gt; <span class=\"hljs-number\"\
          >975</span>     outputs = self.base_model.generate(**kwargs)\n    <span\
          \ class=\"hljs-number\">976</span> <span class=\"hljs-keyword\">except</span>:\n\
          \    <span class=\"hljs-number\">977</span>     self.base_model.prepare_inputs_for_generation\
          \ = self.base_model_prepare_inputs_for_generation\n\nFile /opt/conda/lib/python3<span\
          \ class=\"hljs-number\">.10</span>/site-packages/torch/utils/_contextlib.py:<span\
          \ class=\"hljs-number\">115</span>, <span class=\"hljs-keyword\">in</span>\
          \ context_decorator.&lt;<span class=\"hljs-built_in\">locals</span>&gt;.decorate_context(*args,\
          \ **kwargs)\n    <span class=\"hljs-number\">112</span> @functools.wraps(func)\n\
          \    <span class=\"hljs-number\">113</span> <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">decorate_context</span>(<span\
          \ class=\"hljs-params\">*args, **kwargs</span>):\n    <span class=\"hljs-number\"\
          >114</span>     <span class=\"hljs-keyword\">with</span> ctx_factory():\n\
          --&gt; <span class=\"hljs-number\">115</span>         <span class=\"hljs-keyword\"\
          >return</span> func(*args, **kwargs)\n\nFile /opt/conda/lib/python3<span\
          \ class=\"hljs-number\">.10</span>/site-packages/transformers/generation/utils.py:<span\
          \ class=\"hljs-number\">1719</span>, <span class=\"hljs-keyword\">in</span>\
          \ GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\n\
          \   <span class=\"hljs-number\">1711</span>     input_ids, model_kwargs\
          \ = self._expand_inputs_for_generation(\n   <span class=\"hljs-number\"\
          >1712</span>         input_ids=input_ids,\n   <span class=\"hljs-number\"\
          >1713</span>         expand_size=generation_config.num_return_sequences,\n\
          \   <span class=\"hljs-number\">1714</span>         is_encoder_decoder=self.config.is_encoder_decoder,\n\
          \   <span class=\"hljs-number\">1715</span>         **model_kwargs,\n  \
          \ <span class=\"hljs-number\">1716</span>     )\n   <span class=\"hljs-number\"\
          >1718</span>     <span class=\"hljs-comment\"># 13. run sample</span>\n\
          -&gt; <span class=\"hljs-number\">1719</span>     <span class=\"hljs-keyword\"\
          >return</span> self.sample(\n   <span class=\"hljs-number\">1720</span>\
          \         input_ids,\n   <span class=\"hljs-number\">1721</span>       \
          \  logits_processor=logits_processor,\n   <span class=\"hljs-number\">1722</span>\
          \         logits_warper=logits_warper,\n   <span class=\"hljs-number\">1723</span>\
          \         stopping_criteria=stopping_criteria,\n   <span class=\"hljs-number\"\
          >1724</span>         pad_token_id=generation_config.pad_token_id,\n   <span\
          \ class=\"hljs-number\">1725</span>         eos_token_id=generation_config.eos_token_id,\n\
          \   <span class=\"hljs-number\">1726</span>         output_scores=generation_config.output_scores,\n\
          \   <span class=\"hljs-number\">1727</span>         return_dict_in_generate=generation_config.return_dict_in_generate,\n\
          \   <span class=\"hljs-number\">1728</span>         synced_gpus=synced_gpus,\n\
          \   <span class=\"hljs-number\">1729</span>         streamer=streamer,\n\
          \   <span class=\"hljs-number\">1730</span>         **model_kwargs,\n  \
          \ <span class=\"hljs-number\">1731</span>     )\n   <span class=\"hljs-number\"\
          >1733</span> <span class=\"hljs-keyword\">elif</span> generation_mode ==\
          \ GenerationMode.BEAM_SEARCH:\n   <span class=\"hljs-number\">1734</span>\
          \     <span class=\"hljs-comment\"># 11. prepare beam search scorer</span>\n\
          \   <span class=\"hljs-number\">1735</span>     beam_scorer = BeamSearchScorer(\n\
          \   <span class=\"hljs-number\">1736</span>         batch_size=batch_size,\n\
          \   <span class=\"hljs-number\">1737</span>         num_beams=generation_config.num_beams,\n\
          \   (...)\n   <span class=\"hljs-number\">1742</span>         max_length=generation_config.max_length,\n\
          \   <span class=\"hljs-number\">1743</span>     )\n\nFile /opt/conda/lib/python3<span\
          \ class=\"hljs-number\">.10</span>/site-packages/transformers/generation/utils.py:<span\
          \ class=\"hljs-number\">2837</span>, <span class=\"hljs-keyword\">in</span>\
          \ GenerationMixin.sample(self, input_ids, logits_processor, stopping_criteria,\
          \ logits_warper, max_length, pad_token_id, eos_token_id, output_attentions,\
          \ output_hidden_states, output_scores, return_dict_in_generate, synced_gpus,\
          \ streamer, **model_kwargs)\n   <span class=\"hljs-number\">2835</span>\
          \ <span class=\"hljs-comment\"># sample</span>\n   <span class=\"hljs-number\"\
          >2836</span> probs = nn.functional.softmax(next_token_scores, dim=-<span\
          \ class=\"hljs-number\">1</span>)\n-&gt; <span class=\"hljs-number\">2837</span>\
          \ next_tokens = torch.multinomial(probs, num_samples=<span class=\"hljs-number\"\
          >1</span>).squeeze(<span class=\"hljs-number\">1</span>)\n   <span class=\"\
          hljs-number\">2839</span> <span class=\"hljs-comment\"># finished sentences\
          \ should have their next token be a padding token</span>\n   <span class=\"\
          hljs-number\">2840</span> <span class=\"hljs-keyword\">if</span> eos_token_id\
          \ <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span>\
          \ <span class=\"hljs-literal\">None</span>:\n\nRuntimeError: probability\
          \ tensor contains either `inf`, `nan` <span class=\"hljs-keyword\">or</span>\
          \ element &lt; <span class=\"hljs-number\">0</span>\n</code></pre>\n"
        raw: "\u041F\u043E\u0441\u043B\u0435 \u0437\u0430\u0433\u0440\u0443\u0437\u043A\
          \u0438 \u043C\u043E\u0434\u0435\u043B\u0438, \u043F\u0440\u0438 \u043B\u044E\
          \u0431\u043E\u043C \u0437\u0430\u043F\u0440\u043E\u0441\u0435 \u0432\u044B\
          \u043B\u0435\u0437\u0430\u0435\u0442 \u043E\u0448\u0438\u0431\u043A\u0430\
          \ (\u0441\u043C. \u043A\u043E\u0434 \u043D\u0438\u0436\u0435). \u041F\u043E\
          \u0434\u0441\u043A\u0430\u0436\u0438\u0442\u0435 \u043F\u043E\u0436\u0430\
          \u043B\u0443\u0439\u0441\u0442\u0430 \u0441 \u0447\u0435\u043C \u044D\u0442\
          \u043E \u043C\u043E\u0436\u0435\u0442 \u0431\u044B\u0442\u044C \u0441\u0432\
          \u044F\u0437\u0430\u043D\u043E \u0438 \u043A\u0430\u043A \u044D\u0442\u043E\
          \ \u0438\u0441\u043F\u0440\u0430\u0432\u0438\u0442\u044C. \u041E\u0441\u0442\
          \u0430\u043B\u044C\u043D\u044B\u0435 \u043C\u043E\u0434\u0435\u043B\u0438\
          \ saiga \u0440\u0430\u0431\u043E\u0442\u0430\u044E\u0442 \u043E\u0442\u043B\
          \u0438\u0447\u043D\u043E \u043F\u0440\u0438 \u0442\u0435\u0445 \u0436\u0435\
          \ \u0437\u0430\u043F\u0440\u043E\u0441\u0430\u0445. \r\n[\u041A\u043E\u0434\
          \ \u0437\u0430\u043F\u0443\u0441\u043A\u0430 \u043C\u043E\u0434\u0435\u043B\
          \u0438 \u043D\u0430 Kaggle](https://www.kaggle.com/code/yarasslove/extraction-saiga)\r\
          \n```py \r\nRuntimeError                              Traceback (most recent\
          \ call last)\r\nCell In[15], line 1\r\n----> 1 get_message(['\u041F\u0440\
          \u0438\u0432\u0435\u0442, \u0441\u043A\u043E\u043B\u044C\u043A\u043E \u0432\
          \u0435\u0441\u0438\u0442 \u0447\u0435\u043B\u043E\u0432\u0435\u043A \u0432\
          \ \u0441\u0440\u0435\u0434\u043D\u0435\u043C?'])\r\n\r\nFile <timed exec>:10,\
          \ in get_message(inputs)\r\n\r\nCell In[7], line 54, in generate(model,\
          \ tokenizer, prompt, generation_config)\r\n     51 #print(data)\r\n    \
          \ 52 data = {k: v.to(device) for k, v in data.items()}\r\n---> 54 output_ids\
          \ = model.generate(\r\n     55     **data,\r\n     56     generation_config\
          \ = generation_config\r\n     57 #         remove_invalid_values = True\r\
          \n     58 )[0]\r\n     59 output_ids = output_ids[len(data[\"input_ids\"\
          ][0]):]\r\n     60 output = tokenizer.decode(output_ids, skip_special_tokens=True)\r\
          \n\r\nFile /opt/conda/lib/python3.10/site-packages/peft/peft_model.py:975,\
          \ in PeftModelForCausalLM.generate(self, **kwargs)\r\n    973     self.base_model.generation_config\
          \ = self.generation_config\r\n    974 try:\r\n--> 975     outputs = self.base_model.generate(**kwargs)\r\
          \n    976 except:\r\n    977     self.base_model.prepare_inputs_for_generation\
          \ = self.base_model_prepare_inputs_for_generation\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115,\
          \ in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n  \
          \  112 @functools.wraps(func)\r\n    113 def decorate_context(*args, **kwargs):\r\
          \n    114     with ctx_factory():\r\n--> 115         return func(*args,\
          \ **kwargs)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1719,\
          \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\r\
          \n   1711     input_ids, model_kwargs = self._expand_inputs_for_generation(\r\
          \n   1712         input_ids=input_ids,\r\n   1713         expand_size=generation_config.num_return_sequences,\r\
          \n   1714         is_encoder_decoder=self.config.is_encoder_decoder,\r\n\
          \   1715         **model_kwargs,\r\n   1716     )\r\n   1718     # 13. run\
          \ sample\r\n-> 1719     return self.sample(\r\n   1720         input_ids,\r\
          \n   1721         logits_processor=logits_processor,\r\n   1722        \
          \ logits_warper=logits_warper,\r\n   1723         stopping_criteria=stopping_criteria,\r\
          \n   1724         pad_token_id=generation_config.pad_token_id,\r\n   1725\
          \         eos_token_id=generation_config.eos_token_id,\r\n   1726      \
          \   output_scores=generation_config.output_scores,\r\n   1727         return_dict_in_generate=generation_config.return_dict_in_generate,\r\
          \n   1728         synced_gpus=synced_gpus,\r\n   1729         streamer=streamer,\r\
          \n   1730         **model_kwargs,\r\n   1731     )\r\n   1733 elif generation_mode\
          \ == GenerationMode.BEAM_SEARCH:\r\n   1734     # 11. prepare beam search\
          \ scorer\r\n   1735     beam_scorer = BeamSearchScorer(\r\n   1736     \
          \    batch_size=batch_size,\r\n   1737         num_beams=generation_config.num_beams,\r\
          \n   (...)\r\n   1742         max_length=generation_config.max_length,\r\
          \n   1743     )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2837,\
          \ in GenerationMixin.sample(self, input_ids, logits_processor, stopping_criteria,\
          \ logits_warper, max_length, pad_token_id, eos_token_id, output_attentions,\
          \ output_hidden_states, output_scores, return_dict_in_generate, synced_gpus,\
          \ streamer, **model_kwargs)\r\n   2835 # sample\r\n   2836 probs = nn.functional.softmax(next_token_scores,\
          \ dim=-1)\r\n-> 2837 next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\
          \n   2839 # finished sentences should have their next token be a padding\
          \ token\r\n   2840 if eos_token_id is not None:\r\n\r\nRuntimeError: probability\
          \ tensor contains either `inf`, `nan` or element < 0\r\n```\r\n\r\n"
        updatedAt: '2023-11-21T16:03:58.176Z'
      numEdits: 0
      reactions: []
    id: 655cd4eecaa21067cd798758
    type: comment
  author: YarKo69
  content: "\u041F\u043E\u0441\u043B\u0435 \u0437\u0430\u0433\u0440\u0443\u0437\u043A\
    \u0438 \u043C\u043E\u0434\u0435\u043B\u0438, \u043F\u0440\u0438 \u043B\u044E\u0431\
    \u043E\u043C \u0437\u0430\u043F\u0440\u043E\u0441\u0435 \u0432\u044B\u043B\u0435\
    \u0437\u0430\u0435\u0442 \u043E\u0448\u0438\u0431\u043A\u0430 (\u0441\u043C. \u043A\
    \u043E\u0434 \u043D\u0438\u0436\u0435). \u041F\u043E\u0434\u0441\u043A\u0430\u0436\
    \u0438\u0442\u0435 \u043F\u043E\u0436\u0430\u043B\u0443\u0439\u0441\u0442\u0430\
    \ \u0441 \u0447\u0435\u043C \u044D\u0442\u043E \u043C\u043E\u0436\u0435\u0442\
    \ \u0431\u044B\u0442\u044C \u0441\u0432\u044F\u0437\u0430\u043D\u043E \u0438 \u043A\
    \u0430\u043A \u044D\u0442\u043E \u0438\u0441\u043F\u0440\u0430\u0432\u0438\u0442\
    \u044C. \u041E\u0441\u0442\u0430\u043B\u044C\u043D\u044B\u0435 \u043C\u043E\u0434\
    \u0435\u043B\u0438 saiga \u0440\u0430\u0431\u043E\u0442\u0430\u044E\u0442 \u043E\
    \u0442\u043B\u0438\u0447\u043D\u043E \u043F\u0440\u0438 \u0442\u0435\u0445 \u0436\
    \u0435 \u0437\u0430\u043F\u0440\u043E\u0441\u0430\u0445. \r\n[\u041A\u043E\u0434\
    \ \u0437\u0430\u043F\u0443\u0441\u043A\u0430 \u043C\u043E\u0434\u0435\u043B\u0438\
    \ \u043D\u0430 Kaggle](https://www.kaggle.com/code/yarasslove/extraction-saiga)\r\
    \n```py \r\nRuntimeError                              Traceback (most recent call\
    \ last)\r\nCell In[15], line 1\r\n----> 1 get_message(['\u041F\u0440\u0438\u0432\
    \u0435\u0442, \u0441\u043A\u043E\u043B\u044C\u043A\u043E \u0432\u0435\u0441\u0438\
    \u0442 \u0447\u0435\u043B\u043E\u0432\u0435\u043A \u0432 \u0441\u0440\u0435\u0434\
    \u043D\u0435\u043C?'])\r\n\r\nFile <timed exec>:10, in get_message(inputs)\r\n\
    \r\nCell In[7], line 54, in generate(model, tokenizer, prompt, generation_config)\r\
    \n     51 #print(data)\r\n     52 data = {k: v.to(device) for k, v in data.items()}\r\
    \n---> 54 output_ids = model.generate(\r\n     55     **data,\r\n     56     generation_config\
    \ = generation_config\r\n     57 #         remove_invalid_values = True\r\n  \
    \   58 )[0]\r\n     59 output_ids = output_ids[len(data[\"input_ids\"][0]):]\r\
    \n     60 output = tokenizer.decode(output_ids, skip_special_tokens=True)\r\n\r\
    \nFile /opt/conda/lib/python3.10/site-packages/peft/peft_model.py:975, in PeftModelForCausalLM.generate(self,\
    \ **kwargs)\r\n    973     self.base_model.generation_config = self.generation_config\r\
    \n    974 try:\r\n--> 975     outputs = self.base_model.generate(**kwargs)\r\n\
    \    976 except:\r\n    977     self.base_model.prepare_inputs_for_generation\
    \ = self.base_model_prepare_inputs_for_generation\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115,\
    \ in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\
    \n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\
    \n--> 115         return func(*args, **kwargs)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1719,\
    \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
    \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer,\
    \ negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\r\n   1711  \
    \   input_ids, model_kwargs = self._expand_inputs_for_generation(\r\n   1712 \
    \        input_ids=input_ids,\r\n   1713         expand_size=generation_config.num_return_sequences,\r\
    \n   1714         is_encoder_decoder=self.config.is_encoder_decoder,\r\n   1715\
    \         **model_kwargs,\r\n   1716     )\r\n   1718     # 13. run sample\r\n\
    -> 1719     return self.sample(\r\n   1720         input_ids,\r\n   1721     \
    \    logits_processor=logits_processor,\r\n   1722         logits_warper=logits_warper,\r\
    \n   1723         stopping_criteria=stopping_criteria,\r\n   1724         pad_token_id=generation_config.pad_token_id,\r\
    \n   1725         eos_token_id=generation_config.eos_token_id,\r\n   1726    \
    \     output_scores=generation_config.output_scores,\r\n   1727         return_dict_in_generate=generation_config.return_dict_in_generate,\r\
    \n   1728         synced_gpus=synced_gpus,\r\n   1729         streamer=streamer,\r\
    \n   1730         **model_kwargs,\r\n   1731     )\r\n   1733 elif generation_mode\
    \ == GenerationMode.BEAM_SEARCH:\r\n   1734     # 11. prepare beam search scorer\r\
    \n   1735     beam_scorer = BeamSearchScorer(\r\n   1736         batch_size=batch_size,\r\
    \n   1737         num_beams=generation_config.num_beams,\r\n   (...)\r\n   1742\
    \         max_length=generation_config.max_length,\r\n   1743     )\r\n\r\nFile\
    \ /opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2837,\
    \ in GenerationMixin.sample(self, input_ids, logits_processor, stopping_criteria,\
    \ logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states,\
    \ output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\r\
    \n   2835 # sample\r\n   2836 probs = nn.functional.softmax(next_token_scores,\
    \ dim=-1)\r\n-> 2837 next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\r\
    \n   2839 # finished sentences should have their next token be a padding token\r\
    \n   2840 if eos_token_id is not None:\r\n\r\nRuntimeError: probability tensor\
    \ contains either `inf`, `nan` or element < 0\r\n```\r\n\r\n"
  created_at: 2023-11-21 16:03:58+00:00
  edited: false
  hidden: false
  id: 655cd4eecaa21067cd798758
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
      fullname: Ilya Gusev
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: IlyaGusev
      type: user
    createdAt: '2023-11-24T00:05:47.000Z'
    data:
      edited: false
      editors:
      - IlyaGusev
      hidden: false
      identifiedLanguage:
        language: ru
        probability: 0.9865778684616089
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
          fullname: Ilya Gusev
          isHf: false
          isPro: true
          name: IlyaGusev
          type: user
        html: "<p>\u041C\u0435\u0436\u0434\u0443 \u043E\u0441\u0442\u0430\u043B\u044C\
          \u043D\u044B\u043C\u0438 \u043C\u043E\u0434\u0435\u043B\u044F\u043C\u0438\
          \ \u0438 \u044D\u0442\u043E\u0439 \u0440\u0430\u0437\u043D\u0430\u044F \u0442\
          \u043E\u043A\u0435\u043D\u0438\u0437\u0430\u0446\u0438\u044F. bot_token_id,\
          \ \u043D\u0430\u043F\u0440\u0438\u043C\u0435\u0440, \u0431\u0443\u0434\u0435\
          \u0442 \u0440\u0430\u0437\u043D\u044B\u0439. \u0410 \u043F\u043E \u043A\u043E\
          \u0434\u0443 \u0432\u044B\u0433\u043B\u044F\u0434\u0438\u0442, \u0447\u0442\
          \u043E \u043E\u043D \u0432\u0435\u0437\u0434\u0435 \u043E\u0434\u0438\u043D\
          \ \u0438 \u0442\u043E\u0442 \u0436\u0435.</p>\n<p>\u0418 torch_dtype \u043B\
          \u0443\u0447\u0448\u0435 \u0441\u0442\u0430\u0432\u0438\u0442\u044C bfloat16.</p>\n"
        raw: "\u041C\u0435\u0436\u0434\u0443 \u043E\u0441\u0442\u0430\u043B\u044C\u043D\
          \u044B\u043C\u0438 \u043C\u043E\u0434\u0435\u043B\u044F\u043C\u0438 \u0438\
          \ \u044D\u0442\u043E\u0439 \u0440\u0430\u0437\u043D\u0430\u044F \u0442\u043E\
          \u043A\u0435\u043D\u0438\u0437\u0430\u0446\u0438\u044F. bot_token_id, \u043D\
          \u0430\u043F\u0440\u0438\u043C\u0435\u0440, \u0431\u0443\u0434\u0435\u0442\
          \ \u0440\u0430\u0437\u043D\u044B\u0439. \u0410 \u043F\u043E \u043A\u043E\
          \u0434\u0443 \u0432\u044B\u0433\u043B\u044F\u0434\u0438\u0442, \u0447\u0442\
          \u043E \u043E\u043D \u0432\u0435\u0437\u0434\u0435 \u043E\u0434\u0438\u043D\
          \ \u0438 \u0442\u043E\u0442 \u0436\u0435.\n\n\u0418 torch_dtype \u043B\u0443\
          \u0447\u0448\u0435 \u0441\u0442\u0430\u0432\u0438\u0442\u044C bfloat16."
        updatedAt: '2023-11-24T00:05:47.873Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - YarKo69
    id: 655fe8db9cf1eef99d56ee4a
    type: comment
  author: IlyaGusev
  content: "\u041C\u0435\u0436\u0434\u0443 \u043E\u0441\u0442\u0430\u043B\u044C\u043D\
    \u044B\u043C\u0438 \u043C\u043E\u0434\u0435\u043B\u044F\u043C\u0438 \u0438 \u044D\
    \u0442\u043E\u0439 \u0440\u0430\u0437\u043D\u0430\u044F \u0442\u043E\u043A\u0435\
    \u043D\u0438\u0437\u0430\u0446\u0438\u044F. bot_token_id, \u043D\u0430\u043F\u0440\
    \u0438\u043C\u0435\u0440, \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u0437\u043D\
    \u044B\u0439. \u0410 \u043F\u043E \u043A\u043E\u0434\u0443 \u0432\u044B\u0433\u043B\
    \u044F\u0434\u0438\u0442, \u0447\u0442\u043E \u043E\u043D \u0432\u0435\u0437\u0434\
    \u0435 \u043E\u0434\u0438\u043D \u0438 \u0442\u043E\u0442 \u0436\u0435.\n\n\u0418\
    \ torch_dtype \u043B\u0443\u0447\u0448\u0435 \u0441\u0442\u0430\u0432\u0438\u0442\
    \u044C bfloat16."
  created_at: 2023-11-24 00:05:47+00:00
  edited: false
  hidden: false
  id: 655fe8db9cf1eef99d56ee4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/aeaf6a78dcb30edd8431a30781938989.svg
      fullname: Yaroslav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YarKo69
      type: user
    createdAt: '2023-12-14T09:30:23.000Z'
    data:
      status: closed
    id: 657acb2fdb35eccb837214b9
    type: status-change
  author: YarKo69
  created_at: 2023-12-14 09:30:23+00:00
  id: 657acb2fdb35eccb837214b9
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: IlyaGusev/saiga_mistral_7b_lora
repo_type: model
status: closed
target_branch: null
title: 'RuntimeError: probability tensor contains either `inf`, `nan` or element <
  0'
