!!python/object:huggingface_hub.community.DiscussionWithDetails
author: henrywang0314
conflicting_files: null
created_at: 2023-11-29 05:58:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/07549653becdaa4e10f7a3971168998d.svg
      fullname: hungren wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: henrywang0314
      type: user
    createdAt: '2023-11-29T05:58:55.000Z'
    data:
      edited: true
      editors:
      - henrywang0314
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3370089828968048
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/07549653becdaa4e10f7a3971168998d.svg
          fullname: hungren wang
          isHf: false
          isPro: false
          name: henrywang0314
          type: user
        html: '<p>randomly occur this error</p>

          <p>File "c:\Users\vic\Desktop\chatGLM\Vitual_pet_GLM\chatGPTAPI.py", line
          77, in role_play_qa<br>    role_response = conversation_with_summary({"input":
          question, "role": role})["response"]<br>  File "C:\Users\vic\Desktop\chatGLM.conda\lib\site-packages\langchain\chains\base.py",
          line 310, in <strong>call</strong><br>    raise e<br>  File "C:\Users\vic\Desktop\chatGLM.conda\lib\site-packages\langchain\chains\base.py",
          line 304, in <strong>call</strong><br>    self._call(inputs, run_manager=run_manager)<br>  File
          "C:\Users\vic\Desktop\chatGLM.conda\lib\site-packages\langchain\chains\llm.py",
          line 108, in _call<br>    response = self.generate([inputs], run_manager=run_manager)<br>  File
          "C:\Users\vic\Desktop\chatGLM.conda\lib\site-packages\langchain\chains\llm.py",
          line 120, in generate<br>    return self.llm.generate_prompt(<br>  File
          "C:\Users\vic\Desktop\chatGLM.conda\lib\site-packages\langchain\llms\base.py",
          line 507, in generate_prompt<br>    return self.generate(prompt_strings,
          stop=stop, callbacks=callbacks, **kwargs)<br>  File "C:\Users\vic\Desktop\chatGLM.conda\lib\site-packages\langchain\llms\base.py",
          line 656, in generate<br>    output = self._generate_helper(<br>  File "C:\Users\vic\Desktop\chatGLM.conda\lib\site-packages\langchain\llms\base.py",
          line 544, in _generate_helper<br>    raise e<br>  File "C:\Users\vic\Desktop\chatGLM.conda\lib\site-packages\langchain\llms\base.py",
          line 531, in _generate_helper<br>    self._generate(<br>  File "C:\Users\vic\Desktop\chatGLM.conda\lib\site-packages\langchain\llms\base.py",
          line 1055, in _generate<br>    else self._call(prompt, stop=stop, **kwargs)<br>  File
          "c:\Users\vic\Desktop\chatGLM\Vitual_pet_GLM\ChatGLM.py", line 30, in _call<br>    response,
          _ = self.model.chat(<br>  File "C:\Users\vic\Desktop\chatGLM.conda\lib\site-packages\torch\utils_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "C:\Users\vic/.cache\huggingface\modules\transformers_modules\THUDM\chatglm3-6b\e46a14881eae613281abbd266ee918e93a56018f\modeling_chatglm.py",
          line 1039, in chat<br>    response, history = self.process_response(response,
          history)<br>  File "C:\Users\vic/.cache\huggingface\modules\transformers_modules\THUDM\chatglm3-6b\e46a14881eae613281abbd266ee918e93a56018f\modeling_chatglm.py",
          line 1003, in process_response<br>    metadata, content = response.split("\n",
          maxsplit=1)</p>

          '
        raw: "randomly occur this error\n\nFile \"c:\\Users\\vic\\Desktop\\chatGLM\\\
          Vitual_pet_GLM\\chatGPTAPI.py\", line 77, in role_play_qa\n    role_response\
          \ = conversation_with_summary({\"input\": question, \"role\": role})[\"\
          response\"]\n  File \"C:\\Users\\vic\\Desktop\\chatGLM\\.conda\\lib\\site-packages\\\
          langchain\\chains\\base.py\", line 310, in __call__\n    raise e\n  File\
          \ \"C:\\Users\\vic\\Desktop\\chatGLM\\.conda\\lib\\site-packages\\langchain\\\
          chains\\base.py\", line 304, in __call__\n    self._call(inputs, run_manager=run_manager)\n\
          \  File \"C:\\Users\\vic\\Desktop\\chatGLM\\.conda\\lib\\site-packages\\\
          langchain\\chains\\llm.py\", line 108, in _call\n    response = self.generate([inputs],\
          \ run_manager=run_manager)\n  File \"C:\\Users\\vic\\Desktop\\chatGLM\\\
          .conda\\lib\\site-packages\\langchain\\chains\\llm.py\", line 120, in generate\n\
          \    return self.llm.generate_prompt(\n  File \"C:\\Users\\vic\\Desktop\\\
          chatGLM\\.conda\\lib\\site-packages\\langchain\\llms\\base.py\", line 507,\
          \ in generate_prompt\n    return self.generate(prompt_strings, stop=stop,\
          \ callbacks=callbacks, **kwargs)\n  File \"C:\\Users\\vic\\Desktop\\chatGLM\\\
          .conda\\lib\\site-packages\\langchain\\llms\\base.py\", line 656, in generate\n\
          \    output = self._generate_helper(\n  File \"C:\\Users\\vic\\Desktop\\\
          chatGLM\\.conda\\lib\\site-packages\\langchain\\llms\\base.py\", line 544,\
          \ in _generate_helper\n    raise e\n  File \"C:\\Users\\vic\\Desktop\\chatGLM\\\
          .conda\\lib\\site-packages\\langchain\\llms\\base.py\", line 531, in _generate_helper\n\
          \    self._generate(\n  File \"C:\\Users\\vic\\Desktop\\chatGLM\\.conda\\\
          lib\\site-packages\\langchain\\llms\\base.py\", line 1055, in _generate\n\
          \    else self._call(prompt, stop=stop, **kwargs)\n  File \"c:\\Users\\\
          vic\\Desktop\\chatGLM\\Vitual_pet_GLM\\ChatGLM.py\", line 30, in _call\n\
          \    response, _ = self.model.chat(\n  File \"C:\\Users\\vic\\Desktop\\\
          chatGLM\\.conda\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line\
          \ 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"\
          C:\\Users\\vic/.cache\\huggingface\\modules\\transformers_modules\\THUDM\\\
          chatglm3-6b\\e46a14881eae613281abbd266ee918e93a56018f\\modeling_chatglm.py\"\
          , line 1039, in chat\n    response, history = self.process_response(response,\
          \ history)\n  File \"C:\\Users\\vic/.cache\\huggingface\\modules\\transformers_modules\\\
          THUDM\\chatglm3-6b\\e46a14881eae613281abbd266ee918e93a56018f\\modeling_chatglm.py\"\
          , line 1003, in process_response  \n    metadata, content = response.split(\"\
          \\n\", maxsplit=1)"
        updatedAt: '2023-11-29T06:04:14.327Z'
      numEdits: 1
      reactions: []
    id: 6566d31f162ad28c049da775
    type: comment
  author: henrywang0314
  content: "randomly occur this error\n\nFile \"c:\\Users\\vic\\Desktop\\chatGLM\\\
    Vitual_pet_GLM\\chatGPTAPI.py\", line 77, in role_play_qa\n    role_response =\
    \ conversation_with_summary({\"input\": question, \"role\": role})[\"response\"\
    ]\n  File \"C:\\Users\\vic\\Desktop\\chatGLM\\.conda\\lib\\site-packages\\langchain\\\
    chains\\base.py\", line 310, in __call__\n    raise e\n  File \"C:\\Users\\vic\\\
    Desktop\\chatGLM\\.conda\\lib\\site-packages\\langchain\\chains\\base.py\", line\
    \ 304, in __call__\n    self._call(inputs, run_manager=run_manager)\n  File \"\
    C:\\Users\\vic\\Desktop\\chatGLM\\.conda\\lib\\site-packages\\langchain\\chains\\\
    llm.py\", line 108, in _call\n    response = self.generate([inputs], run_manager=run_manager)\n\
    \  File \"C:\\Users\\vic\\Desktop\\chatGLM\\.conda\\lib\\site-packages\\langchain\\\
    chains\\llm.py\", line 120, in generate\n    return self.llm.generate_prompt(\n\
    \  File \"C:\\Users\\vic\\Desktop\\chatGLM\\.conda\\lib\\site-packages\\langchain\\\
    llms\\base.py\", line 507, in generate_prompt\n    return self.generate(prompt_strings,\
    \ stop=stop, callbacks=callbacks, **kwargs)\n  File \"C:\\Users\\vic\\Desktop\\\
    chatGLM\\.conda\\lib\\site-packages\\langchain\\llms\\base.py\", line 656, in\
    \ generate\n    output = self._generate_helper(\n  File \"C:\\Users\\vic\\Desktop\\\
    chatGLM\\.conda\\lib\\site-packages\\langchain\\llms\\base.py\", line 544, in\
    \ _generate_helper\n    raise e\n  File \"C:\\Users\\vic\\Desktop\\chatGLM\\.conda\\\
    lib\\site-packages\\langchain\\llms\\base.py\", line 531, in _generate_helper\n\
    \    self._generate(\n  File \"C:\\Users\\vic\\Desktop\\chatGLM\\.conda\\lib\\\
    site-packages\\langchain\\llms\\base.py\", line 1055, in _generate\n    else self._call(prompt,\
    \ stop=stop, **kwargs)\n  File \"c:\\Users\\vic\\Desktop\\chatGLM\\Vitual_pet_GLM\\\
    ChatGLM.py\", line 30, in _call\n    response, _ = self.model.chat(\n  File \"\
    C:\\Users\\vic\\Desktop\\chatGLM\\.conda\\lib\\site-packages\\torch\\utils\\_contextlib.py\"\
    , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"C:\\\
    Users\\vic/.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm3-6b\\\
    e46a14881eae613281abbd266ee918e93a56018f\\modeling_chatglm.py\", line 1039, in\
    \ chat\n    response, history = self.process_response(response, history)\n  File\
    \ \"C:\\Users\\vic/.cache\\huggingface\\modules\\transformers_modules\\THUDM\\\
    chatglm3-6b\\e46a14881eae613281abbd266ee918e93a56018f\\modeling_chatglm.py\",\
    \ line 1003, in process_response  \n    metadata, content = response.split(\"\\\
    n\", maxsplit=1)"
  created_at: 2023-11-29 05:58:55+00:00
  edited: true
  hidden: false
  id: 6566d31f162ad28c049da775
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/RQJQZouD-Qnt70k0dVjHU.png?w=200&h=200&f=face
      fullname: Yuxuan Zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zRzRzRzRzRzRzR
      type: user
    createdAt: '2023-12-07T09:02:45.000Z'
    data:
      edited: false
      editors:
      - zRzRzRzRzRzRzR
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.9772824645042419
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/RQJQZouD-Qnt70k0dVjHU.png?w=200&h=200&f=face
          fullname: Yuxuan Zhang
          isHf: false
          isPro: false
          name: zRzRzRzRzRzRzR
          type: user
        html: "<p>\u95EE\u9898\u5DF2\u7ECF\u89E3\u51B3\uFF0C\u66F4\u65B0\u4E00\u4E0B\
          \u6A21\u578B\u914D\u7F6E\u6587\u4EF6</p>\n"
        raw: "\u95EE\u9898\u5DF2\u7ECF\u89E3\u51B3\uFF0C\u66F4\u65B0\u4E00\u4E0B\u6A21\
          \u578B\u914D\u7F6E\u6587\u4EF6"
        updatedAt: '2023-12-07T09:02:45.525Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65718a35fc313225f1853fb4
    id: 65718a35fc313225f1853fb3
    type: comment
  author: zRzRzRzRzRzRzR
  content: "\u95EE\u9898\u5DF2\u7ECF\u89E3\u51B3\uFF0C\u66F4\u65B0\u4E00\u4E0B\u6A21\
    \u578B\u914D\u7F6E\u6587\u4EF6"
  created_at: 2023-12-07 09:02:45+00:00
  edited: false
  hidden: false
  id: 65718a35fc313225f1853fb3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/RQJQZouD-Qnt70k0dVjHU.png?w=200&h=200&f=face
      fullname: Yuxuan Zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zRzRzRzRzRzRzR
      type: user
    createdAt: '2023-12-07T09:02:45.000Z'
    data:
      status: closed
    id: 65718a35fc313225f1853fb4
    type: status-change
  author: zRzRzRzRzRzRzR
  created_at: 2023-12-07 09:02:45+00:00
  id: 65718a35fc313225f1853fb4
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: THUDM/chatglm3-6b
repo_type: model
status: closed
target_branch: null
title: ValueError:not enough values to unpack (expected 2, got 1)
