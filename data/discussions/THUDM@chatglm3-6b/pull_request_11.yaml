!!python/object:huggingface_hub.community.DiscussionWithDetails
author: chielo
conflicting_files:
- tokenization_chatglm.py
created_at: 2023-11-10 09:48:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b682e0f86dfe57da8a0e016f1f9d9979.svg
      fullname: Chielo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chielo
      type: user
    createdAt: '2023-11-10T09:48:28.000Z'
    data:
      edited: true
      editors:
      - chielo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2852921783924103
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b682e0f86dfe57da8a0e016f1f9d9979.svg
          fullname: Chielo
          isHf: false
          isPro: false
          name: chielo
          type: user
        html: "<h1 id=\"usage\">Usage</h1>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">import</span> transformers, tokenizers\n\ntransformers.__version__,\
          \ tokenizers.__version__\n<span class=\"hljs-comment\"># &gt;&gt;&gt; ('4.35.0',\
          \ '0.14.1')</span>\n\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer\n\nfast_tokenizer\
          \ = AutoTokenizer.from_pretrained(\n    <span class=\"hljs-string\">\"./\"\
          </span>, trust_remote_code=<span class=\"hljs-literal\">True</span>, use_fast=<span\
          \ class=\"hljs-literal\">True</span>\n)\nslow_tokenizer = AutoTokenizer.from_pretrained(\n\
          \    <span class=\"hljs-string\">\"./\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>, use_fast=<span class=\"hljs-literal\"\
          >False</span>\n)\n\ncontent = <span class=\"hljs-string\">\"\u662F\u54EA\
          \u4E2A\u661F\u7403\u7684\u5462\uFF1F\"</span>\nhistory = [\n    {\n    \
          \    <span class=\"hljs-string\">\"role\"</span>: <span class=\"hljs-string\"\
          >\"user\"</span>,\n        <span class=\"hljs-string\">\"content\"</span>:\
          \ <span class=\"hljs-string\">\"\u8FD9\u662F\u4EC0\u4E48\u8BED\u8A00\uFF1F\
          \u201Caburaka    dabura   \u201D\"</span>,\n        <span class=\"hljs-string\"\
          >\"metadata\"</span>: {<span class=\"hljs-string\">\"username\"</span>:\
          \ <span class=\"hljs-string\">\"Chielo\"</span>},\n    },\n    {<span class=\"\
          hljs-string\">\"role\"</span>: <span class=\"hljs-string\">\"assistant\"\
          </span>, <span class=\"hljs-string\">\"content\"</span>: <span class=\"\
          hljs-string\">\"\u8FD9\u662F\u6765\u81EA\u5916\u661F\u7684\u8BED\u8A00\u3002\
          \"</span>},\n]\n\nold_inputs = slow_tokenizer.build_chat_input(content,\
          \ history=history)\nnew_inputs = fast_tokenizer.build_chat_input(content,\
          \ history=history)\nnew_text = fast_tokenizer.build_chat_text(content, history=history)\n\
          \nold_input_ids = old_inputs[<span class=\"hljs-string\">\"input_ids\"</span>][<span\
          \ class=\"hljs-number\">0</span>].tolist()\n\nnew_inputs[<span class=\"\
          hljs-string\">\"input_ids\"</span>][<span class=\"hljs-number\">0</span>].tolist()\
          \ == old_input_ids\n<span class=\"hljs-comment\"># &gt;&gt;&gt; True</span>\n\
          \nfast_tokenizer.encode(new_text) == old_input_ids\n<span class=\"hljs-comment\"\
          ># &gt;&gt;&gt; True</span>\n\nfast_tokenizer.decode(old_input_ids) == slow_tokenizer.decode(old_input_ids)\n\
          <span class=\"hljs-comment\"># &gt;&gt;&gt; True</span>\n\nnew_text\n<span\
          \ class=\"hljs-comment\"># &gt;&gt;&gt; \"&lt;|user|&gt;&lt;!encode-sep!&gt;{'username':\
          \ 'Chielo'}\\n&lt;!encode-sep!&gt;\u8FD9\u662F\u4EC0\u4E48\u8BED\u8A00\uFF1F\
          \u201Caburaka    dabura   \u201D&lt;|assistant|&gt;&lt;!encode-sep!&gt;\\\
          n&lt;!encode-sep!&gt;\u8FD9\u662F\u6765\u81EA\u5916\u661F\u7684\u8BED\u8A00\
          \u3002&lt;|user|&gt;&lt;!encode-sep!&gt;\\n&lt;!encode-sep!&gt;\u662F\u54EA\
          \u4E2A\u661F\u7403\u7684\u5462\uFF1F&lt;|assistant|&gt;\"</span>\n</code></pre>\n"
        raw: "# Usage\n\n```python\nimport transformers, tokenizers\n\ntransformers.__version__,\
          \ tokenizers.__version__\n# >>> ('4.35.0', '0.14.1')\n\nfrom transformers\
          \ import AutoTokenizer\n\nfast_tokenizer = AutoTokenizer.from_pretrained(\n\
          \    \"./\", trust_remote_code=True, use_fast=True\n)\nslow_tokenizer =\
          \ AutoTokenizer.from_pretrained(\n    \"./\", trust_remote_code=True, use_fast=False\n\
          )\n\ncontent = \"\u662F\u54EA\u4E2A\u661F\u7403\u7684\u5462\uFF1F\"\nhistory\
          \ = [\n    {\n        \"role\": \"user\",\n        \"content\": \"\u8FD9\
          \u662F\u4EC0\u4E48\u8BED\u8A00\uFF1F\u201Caburaka    dabura   \u201D\",\n\
          \        \"metadata\": {\"username\": \"Chielo\"},\n    },\n    {\"role\"\
          : \"assistant\", \"content\": \"\u8FD9\u662F\u6765\u81EA\u5916\u661F\u7684\
          \u8BED\u8A00\u3002\"},\n]\n\nold_inputs = slow_tokenizer.build_chat_input(content,\
          \ history=history)\nnew_inputs = fast_tokenizer.build_chat_input(content,\
          \ history=history)\nnew_text = fast_tokenizer.build_chat_text(content, history=history)\n\
          \nold_input_ids = old_inputs[\"input_ids\"][0].tolist()\n\nnew_inputs[\"\
          input_ids\"][0].tolist() == old_input_ids\n# >>> True\n\nfast_tokenizer.encode(new_text)\
          \ == old_input_ids\n# >>> True\n\nfast_tokenizer.decode(old_input_ids) ==\
          \ slow_tokenizer.decode(old_input_ids)\n# >>> True\n\nnew_text\n# >>> \"\
          <|user|><!encode-sep!>{'username': 'Chielo'}\\n<!encode-sep!>\u8FD9\u662F\
          \u4EC0\u4E48\u8BED\u8A00\uFF1F\u201Caburaka    dabura   \u201D<|assistant|><!encode-sep!>\\\
          n<!encode-sep!>\u8FD9\u662F\u6765\u81EA\u5916\u661F\u7684\u8BED\u8A00\u3002\
          <|user|><!encode-sep!>\\n<!encode-sep!>\u662F\u54EA\u4E2A\u661F\u7403\u7684\
          \u5462\uFF1F<|assistant|>\"\n```"
        updatedAt: '2023-11-10T09:49:17.436Z'
      numEdits: 1
      reactions: []
    id: 654dfc6cf5297ada0ba75f27
    type: comment
  author: chielo
  content: "# Usage\n\n```python\nimport transformers, tokenizers\n\ntransformers.__version__,\
    \ tokenizers.__version__\n# >>> ('4.35.0', '0.14.1')\n\nfrom transformers import\
    \ AutoTokenizer\n\nfast_tokenizer = AutoTokenizer.from_pretrained(\n    \"./\"\
    , trust_remote_code=True, use_fast=True\n)\nslow_tokenizer = AutoTokenizer.from_pretrained(\n\
    \    \"./\", trust_remote_code=True, use_fast=False\n)\n\ncontent = \"\u662F\u54EA\
    \u4E2A\u661F\u7403\u7684\u5462\uFF1F\"\nhistory = [\n    {\n        \"role\":\
    \ \"user\",\n        \"content\": \"\u8FD9\u662F\u4EC0\u4E48\u8BED\u8A00\uFF1F\
    \u201Caburaka    dabura   \u201D\",\n        \"metadata\": {\"username\": \"Chielo\"\
    },\n    },\n    {\"role\": \"assistant\", \"content\": \"\u8FD9\u662F\u6765\u81EA\
    \u5916\u661F\u7684\u8BED\u8A00\u3002\"},\n]\n\nold_inputs = slow_tokenizer.build_chat_input(content,\
    \ history=history)\nnew_inputs = fast_tokenizer.build_chat_input(content, history=history)\n\
    new_text = fast_tokenizer.build_chat_text(content, history=history)\n\nold_input_ids\
    \ = old_inputs[\"input_ids\"][0].tolist()\n\nnew_inputs[\"input_ids\"][0].tolist()\
    \ == old_input_ids\n# >>> True\n\nfast_tokenizer.encode(new_text) == old_input_ids\n\
    # >>> True\n\nfast_tokenizer.decode(old_input_ids) == slow_tokenizer.decode(old_input_ids)\n\
    # >>> True\n\nnew_text\n# >>> \"<|user|><!encode-sep!>{'username': 'Chielo'}\\\
    n<!encode-sep!>\u8FD9\u662F\u4EC0\u4E48\u8BED\u8A00\uFF1F\u201Caburaka    dabura\
    \   \u201D<|assistant|><!encode-sep!>\\n<!encode-sep!>\u8FD9\u662F\u6765\u81EA\
    \u5916\u661F\u7684\u8BED\u8A00\u3002<|user|><!encode-sep!>\\n<!encode-sep!>\u662F\
    \u54EA\u4E2A\u661F\u7403\u7684\u5462\uFF1F<|assistant|>\"\n```"
  created_at: 2023-11-10 09:48:28+00:00
  edited: true
  hidden: false
  id: 654dfc6cf5297ada0ba75f27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/b682e0f86dfe57da8a0e016f1f9d9979.svg
      fullname: Chielo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chielo
      type: user
    createdAt: '2023-11-10T09:48:29.000Z'
    data:
      oid: 92670adc065f32dce5465960086f392b0e8a0ca4
      parents:
      - e46a14881eae613281abbd266ee918e93a56018f
      subject: Add a fast tokenizer implementation and converter
    id: 654dfc6d0000000000000000
    type: commit
  author: chielo
  created_at: 2023-11-10 09:48:29+00:00
  id: 654dfc6d0000000000000000
  oid: 92670adc065f32dce5465960086f392b0e8a0ca4
  summary: Add a fast tokenizer implementation and converter
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b682e0f86dfe57da8a0e016f1f9d9979.svg
      fullname: Chielo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chielo
      type: user
    createdAt: '2023-11-10T09:48:43.000Z'
    data:
      edited: false
      editors:
      - chielo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8260871767997742
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b682e0f86dfe57da8a0e016f1f9d9979.svg
          fullname: Chielo
          isHf: false
          isPro: false
          name: chielo
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/ModelTC/lightllm/pull/207">We
          added support to ChatGLM3 in LightLLM.</a></p>

          '
        raw: '[We added support to ChatGLM3 in LightLLM.](https://github.com/ModelTC/lightllm/pull/207)'
        updatedAt: '2023-11-10T09:48:43.812Z'
      numEdits: 0
      reactions: []
    id: 654dfc7bd61e57476c3d4ee5
    type: comment
  author: chielo
  content: '[We added support to ChatGLM3 in LightLLM.](https://github.com/ModelTC/lightllm/pull/207)'
  created_at: 2023-11-10 09:48:43+00:00
  edited: false
  hidden: false
  id: 654dfc7bd61e57476c3d4ee5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/b682e0f86dfe57da8a0e016f1f9d9979.svg
      fullname: Chielo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chielo
      type: user
    createdAt: '2023-11-10T10:21:59.000Z'
    data:
      from: Add a fast tokenizer implementation and converter
      to: 'DRAFT: Add a fast tokenizer implementation and converter'
    id: 654e04470011607c312e020d
    type: title-change
  author: chielo
  created_at: 2023-11-10 10:21:59+00:00
  id: 654e04470011607c312e020d
  new_title: 'DRAFT: Add a fast tokenizer implementation and converter'
  old_title: Add a fast tokenizer implementation and converter
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b682e0f86dfe57da8a0e016f1f9d9979.svg
      fullname: Chielo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chielo
      type: user
    createdAt: '2023-11-10T10:51:57.000Z'
    data:
      status: closed
    id: 654e0b4d1b922725af8c1d8e
    type: status-change
  author: chielo
  created_at: 2023-11-10 10:51:57+00:00
  id: 654e0b4d1b922725af8c1d8e
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b682e0f86dfe57da8a0e016f1f9d9979.svg
      fullname: Chielo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chielo
      type: user
    createdAt: '2023-11-10T10:53:41.000Z'
    data:
      status: open
    id: 654e0bb5e1a1ce4341d3feaf
    type: status-change
  author: chielo
  created_at: 2023-11-10 10:53:41+00:00
  id: 654e0bb5e1a1ce4341d3feaf
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b682e0f86dfe57da8a0e016f1f9d9979.svg
      fullname: Chielo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chielo
      type: user
    createdAt: '2023-11-10T11:09:16.000Z'
    data:
      status: closed
    id: 654e0f5ca1014d8a8514fab1
    type: status-change
  author: chielo
  created_at: 2023-11-10 11:09:16+00:00
  id: 654e0f5ca1014d8a8514fab1
  new_status: closed
  type: status-change
is_pull_request: true
merge_commit_oid: null
num: 11
repo_id: THUDM/chatglm3-6b
repo_type: model
status: closed
target_branch: refs/heads/main
title: 'DRAFT: Add a fast tokenizer implementation and converter'
