!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ybelkada
conflicting_files: []
created_at: 2024-01-10 12:08:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-10T12:08:24.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.11044929921627045
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>This is a draft PR! Do not merge it until <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers/pull/27883\">https://github.com/huggingface/transformers/pull/27883</a>\
          \ is merged and we test everything<br>This PR refactors ChatGLM so that\
          \ both the trust remote code and the transformers code are inter-compatible.\
          \ Since some keys had to be renamed, therefore we had to change the safetensors\
          \ weights as well<br>In order to try this PR you need to first:</p>\n<pre><code\
          \ class=\"language-bash\">pip install -U git+https://github.com/huggingface/transformers@add-chat-glm\n\
          </code></pre>\n<p>Then pass <code>revision=\"refs/pr/28\"</code> in <code>from_pretrained</code>.\
          \ The modeling code will work both with <code>trust_remote_code=True</code>\
          \ and <code>trust_remote_code=False</code>. For example, users that use\
          \ <code>quantize</code> should still use <code>trust_remote_code=True</code>.</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span>\
          \ torch\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"\
          hljs-keyword\">import</span> AutoTokenizer, ChatGlmForCausalLM\n\nmodel_id\
          \ = <span class=\"hljs-string\">\"THUDM/chatglm3-6b\"</span>\n\nmodel =\
          \ ChatGlmForCausalLM.from_pretrained(\n    model_id, \n    torch_dtype=torch.float16,\
          \ \n    low_cpu_mem_usage=<span class=\"hljs-literal\">True</span>,\n  \
          \  revision=<span class=\"hljs-string\">\"refs/pr/28\"</span>,\n).to(<span\
          \ class=\"hljs-number\">0</span>)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id,\
          \ trust_remote_code=<span class=\"hljs-literal\">True</span>)\n\ninputs\
          \ = tokenizer(<span class=\"hljs-string\">\"\u4F60\u597D\"</span>, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>).to(<span class=\"hljs-number\">0</span>)\n\
          output = model.generate(**inputs, max_new_tokens=<span class=\"hljs-number\"\
          >10</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>))\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span>[gMASK]sop\
          \ \u4F60\u597D\uFF0C\u6211\u662F\u4EBA\u5DE5\u667A\u80FD\u52A9\u624B\u3002\
          \u5F88\u9AD8\u5174\u8BA4\u8BC6\u4F60\u53EB\u4EC0\u4E48\n</code></pre>\n\
          <p>When I tested it myself, all seemed good so far, if you can cross-test\
          \ this implementation with me, it would be great.<br>If you want to convert\
          \ other chatGLM checkpoints, have a look at this script (that currently\
          \ support 6b only): <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/add-chat-glm/src/transformers/models/chatglm/convert_chatglm_weights_to_hf.py\"\
          >https://github.com/huggingface/transformers/blob/add-chat-glm/src/transformers/models/chatglm/convert_chatglm_weights_to_hf.py</a></p>\n"
        raw: "This is a draft PR! Do not merge it until https://github.com/huggingface/transformers/pull/27883\
          \ is merged and we test everything\nThis PR refactors ChatGLM so that both\
          \ the trust remote code and the transformers code are inter-compatible.\
          \ Since some keys had to be renamed, therefore we had to change the safetensors\
          \ weights as well \nIn order to try this PR you need to first:\n```bash\n\
          pip install -U git+https://github.com/huggingface/transformers@add-chat-glm\n\
          ```\nThen pass `revision=\"refs/pr/28\"` in `from_pretrained`. The modeling\
          \ code will work both with `trust_remote_code=True` and `trust_remote_code=False`.\
          \ For example, users that use `quantize` should still use `trust_remote_code=True`.\n\
          \n```python\nimport torch\nfrom transformers import AutoTokenizer, ChatGlmForCausalLM\n\
          \nmodel_id = \"THUDM/chatglm3-6b\"\n\nmodel = ChatGlmForCausalLM.from_pretrained(\n\
          \    model_id, \n    torch_dtype=torch.float16, \n    low_cpu_mem_usage=True,\n\
          \    revision=\"refs/pr/28\",\n).to(0)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id,\
          \ trust_remote_code=True)\n\ninputs = tokenizer(\"\u4F60\u597D\", return_tensors=\"\
          pt\").to(0)\noutput = model.generate(**inputs, max_new_tokens=10)\nprint(tokenizer.decode(output[0],\
          \ skip_special_tokens=True))\n>>> [gMASK]sop \u4F60\u597D\uFF0C\u6211\u662F\
          \u4EBA\u5DE5\u667A\u80FD\u52A9\u624B\u3002\u5F88\u9AD8\u5174\u8BA4\u8BC6\
          \u4F60\u53EB\u4EC0\u4E48\n```\nWhen I tested it myself, all seemed good\
          \ so far, if you can cross-test this implementation with me, it would be\
          \ great.\nIf you want to convert other chatGLM checkpoints, have a look\
          \ at this script (that currently support 6b only): https://github.com/huggingface/transformers/blob/add-chat-glm/src/transformers/models/chatglm/convert_chatglm_weights_to_hf.py"
        updatedAt: '2024-01-10T12:25:25.423Z'
      numEdits: 4
      reactions: []
    id: 659e88b8bf4807368c14facf
    type: comment
  author: ybelkada
  content: "This is a draft PR! Do not merge it until https://github.com/huggingface/transformers/pull/27883\
    \ is merged and we test everything\nThis PR refactors ChatGLM so that both the\
    \ trust remote code and the transformers code are inter-compatible. Since some\
    \ keys had to be renamed, therefore we had to change the safetensors weights as\
    \ well \nIn order to try this PR you need to first:\n```bash\npip install -U git+https://github.com/huggingface/transformers@add-chat-glm\n\
    ```\nThen pass `revision=\"refs/pr/28\"` in `from_pretrained`. The modeling code\
    \ will work both with `trust_remote_code=True` and `trust_remote_code=False`.\
    \ For example, users that use `quantize` should still use `trust_remote_code=True`.\n\
    \n```python\nimport torch\nfrom transformers import AutoTokenizer, ChatGlmForCausalLM\n\
    \nmodel_id = \"THUDM/chatglm3-6b\"\n\nmodel = ChatGlmForCausalLM.from_pretrained(\n\
    \    model_id, \n    torch_dtype=torch.float16, \n    low_cpu_mem_usage=True,\n\
    \    revision=\"refs/pr/28\",\n).to(0)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id,\
    \ trust_remote_code=True)\n\ninputs = tokenizer(\"\u4F60\u597D\", return_tensors=\"\
    pt\").to(0)\noutput = model.generate(**inputs, max_new_tokens=10)\nprint(tokenizer.decode(output[0],\
    \ skip_special_tokens=True))\n>>> [gMASK]sop \u4F60\u597D\uFF0C\u6211\u662F\u4EBA\
    \u5DE5\u667A\u80FD\u52A9\u624B\u3002\u5F88\u9AD8\u5174\u8BA4\u8BC6\u4F60\u53EB\
    \u4EC0\u4E48\n```\nWhen I tested it myself, all seemed good so far, if you can\
    \ cross-test this implementation with me, it would be great.\nIf you want to convert\
    \ other chatGLM checkpoints, have a look at this script (that currently support\
    \ 6b only): https://github.com/huggingface/transformers/blob/add-chat-glm/src/transformers/models/chatglm/convert_chatglm_weights_to_hf.py"
  created_at: 2024-01-10 12:08:24+00:00
  edited: true
  hidden: false
  id: 659e88b8bf4807368c14facf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-10T12:08:24.000Z'
    data:
      oid: cd99ecfc7973215203cea5b6b0b38b79ce63f666
      parents:
      - b098244a71fbe69ce149682d9072a7629f7e908c
      subject: Upload ChatGlmForCausalLM
    id: 659e88b80000000000000000
    type: commit
  author: ybelkada
  created_at: 2024-01-10 12:08:24+00:00
  id: 659e88b80000000000000000
  oid: cd99ecfc7973215203cea5b6b0b38b79ce63f666
  summary: Upload ChatGlmForCausalLM
  type: commit
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-10T12:09:01.000Z'
    data:
      from: Upload ChatGlmForCausalLM
      to: HF transformers integration
    id: 659e88dd346c8c84bf897539
    type: title-change
  author: ybelkada
  created_at: 2024-01-10 12:09:01+00:00
  id: 659e88dd346c8c84bf897539
  new_title: HF transformers integration
  old_title: Upload ChatGlmForCausalLM
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-10T12:14:41.000Z'
    data:
      oid: 83c91c4a0c389f80b0df1db6fced9938490ecc17
      parents:
      - cd99ecfc7973215203cea5b6b0b38b79ce63f666
      subject: Update config.json
    id: 659e8a310000000000000000
    type: commit
  author: ybelkada
  created_at: 2024-01-10 12:14:41+00:00
  id: 659e8a310000000000000000
  oid: 83c91c4a0c389f80b0df1db6fced9938490ecc17
  summary: Update config.json
  type: commit
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-10T12:15:08.000Z'
    data:
      oid: 26072b41ab4556ebfddc2f537362a720b6098d20
      parents:
      - 83c91c4a0c389f80b0df1db6fced9938490ecc17
      subject: Update modeling_chatglm.py
    id: 659e8a4c0000000000000000
    type: commit
  author: ybelkada
  created_at: 2024-01-10 12:15:08+00:00
  id: 659e8a4c0000000000000000
  oid: 26072b41ab4556ebfddc2f537362a720b6098d20
  summary: Update modeling_chatglm.py
  type: commit
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-10T12:16:35.000Z'
    data:
      oid: 15e9135462661b46a0d1445fb93bed8a2443ab34
      parents:
      - 26072b41ab4556ebfddc2f537362a720b6098d20
      subject: Update modeling_chatglm.py
    id: 659e8aa30000000000000000
    type: commit
  author: ybelkada
  created_at: 2024-01-10 12:16:35+00:00
  id: 659e8aa30000000000000000
  oid: 15e9135462661b46a0d1445fb93bed8a2443ab34
  summary: Update modeling_chatglm.py
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-10T12:18:02.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: pl
        probability: 0.8323667645454407
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;zRzRzRzRzRzRzR&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/zRzRzRzRzRzRzR\"\
          >@<span class=\"underline\">zRzRzRzRzRzRzR</span></a></span>\n\n\t</span></span>\
          \ it would be great if you can help testing this branch, especially with\
          \ respect to quantization and other features I might not be aware of. Regarding\
          \ <code>.bin</code> files, I'll submit another PR once <a rel=\"nofollow\"\
          \ href=\"https://github.com/huggingface/transformers/pull/27883\">https://github.com/huggingface/transformers/pull/27883</a>\
          \  and this PR gets merged</p>\n"
        raw: cc @zRzRzRzRzRzRzR it would be great if you can help testing this branch,
          especially with respect to quantization and other features I might not be
          aware of. Regarding `.bin` files, I'll submit another PR once https://github.com/huggingface/transformers/pull/27883  and
          this PR gets merged
        updatedAt: '2024-01-10T12:18:48.218Z'
      numEdits: 1
      reactions: []
    id: 659e8afae05f29a29c90b860
    type: comment
  author: ybelkada
  content: cc @zRzRzRzRzRzRzR it would be great if you can help testing this branch,
    especially with respect to quantization and other features I might not be aware
    of. Regarding `.bin` files, I'll submit another PR once https://github.com/huggingface/transformers/pull/27883  and
    this PR gets merged
  created_at: 2024-01-10 12:18:02+00:00
  edited: true
  hidden: false
  id: 659e8afae05f29a29c90b860
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-10T12:30:46.000Z'
    data:
      oid: 7db386114716cf55c1c4aded8ca92cd58345ed2e
      parents:
      - 15e9135462661b46a0d1445fb93bed8a2443ab34
      subject: Update config.json
    id: 659e8df60000000000000000
    type: commit
  author: ybelkada
  created_at: 2024-01-10 12:30:46+00:00
  id: 659e8df60000000000000000
  oid: 7db386114716cf55c1c4aded8ca92cd58345ed2e
  summary: Update config.json
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d0d99192204adf3da935c5336de238b5.svg
      fullname: Chenhui Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhangch
      type: user
    createdAt: '2024-01-18T04:02:34.000Z'
    data:
      edited: true
      editors:
      - zhangch
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9960355162620544
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d0d99192204adf3da935c5336de238b5.svg
          fullname: Chenhui Zhang
          isHf: false
          isPro: false
          name: zhangch
          type: user
        html: "<p>Hi, <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>,\
          \ thank you very much for the work. It seems that the tokenizer is not included\
          \ in this PR. I would like to add them. Let me know how to work on it.</p>\n"
        raw: Hi, @ybelkada, thank you very much for the work. It seems that the tokenizer
          is not included in this PR. I would like to add them. Let me know how to
          work on it.
        updatedAt: '2024-01-18T06:59:10.150Z'
      numEdits: 1
      reactions: []
    id: 65a8a2da91ec5d1ec6e4cfd3
    type: comment
  author: zhangch
  content: Hi, @ybelkada, thank you very much for the work. It seems that the tokenizer
    is not included in this PR. I would like to add them. Let me know how to work
    on it.
  created_at: 2024-01-18 04:02:34+00:00
  edited: true
  hidden: false
  id: 65a8a2da91ec5d1ec6e4cfd3
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 28
repo_id: THUDM/chatglm3-6b
repo_type: model
status: open
target_branch: refs/heads/main
title: HF transformers integration
