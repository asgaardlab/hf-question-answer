!!python/object:huggingface_hub.community.DiscussionWithDetails
author: p208p2002
conflicting_files: []
created_at: 2023-12-18 06:10:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620006622342-606296de21cc4e7bf31dc5b4.jpeg?w=200&h=200&f=face
      fullname: Philip Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: p208p2002
      type: user
    createdAt: '2023-12-18T06:10:57.000Z'
    data:
      edited: true
      editors:
      - p208p2002
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.398041307926178
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620006622342-606296de21cc4e7bf31dc5b4.jpeg?w=200&h=200&f=face
          fullname: Philip Huang
          isHf: false
          isPro: false
          name: p208p2002
          type: user
        html: "<h3 id=\"\u7C21\u77ED\">\u7C21\u77ED</h3>\n<p>\u8A2D\u7F6E <code>tokenizer.chat_template</code>\
          \ \u4F86\u4F7F\u7528<code>tokenizer.apply_chat_template</code>, <code>model.generate</code>\u548C\
          \ <code>pipeline</code>(text-gen) \u7B49transformer\u63D0\u4F9B\u4E4B\u529F\
          \u80FD\uFF0C\u6E1B\u5C11\u4E0D\u4E00\u81F4\u6027\u53EF\u80FD\u3002</p>\n\
          <pre><code class=\"language-python\">model_id_or_path = <span class=\"hljs-string\"\
          >\"THUDM/chatglm3-6b\"</span>\ntokenizer = AutoTokenizer.from_pretrained(model_id_or_path,trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\ntokenizer.chat_template = <span class=\"\
          hljs-string\">\"{% for message in messages %}{% if loop.first %}[gMASK]sop&lt;|{{\
          \ message['role'] }}|&gt; \\n {{ message['content'] }}{% else %}&lt;|{{\
          \ message['role'] }}|&gt; \\n {{ message['content'] }}{% endif %}{% endfor\
          \ %}{% if add_generation_prompt %}&lt;|assistant|&gt;{% endif %}\"</span>\n\
          </code></pre>\n<h3 id=\"\u6B64pr\u76EE\u7684\">\u6B64PR\u76EE\u7684</h3>\n\
          <p>\u65B0\u7248\u672C\u7684 transformer tokenizer \u5DF2\u7D93\u652F\u63F4\
          \ chat_template \u5C6C\u6027 <a href=\"https://huggingface.co/blog/zh/chat-templates\"\
          >(https://huggingface.co/blog/zh/chat-templates)</a>\uFF0C\u5176\u76EE\u7684\
          \u5728\u66F4\u597D\u7684\u8B93\u4F7F\u7528\u8005\u9075\u5B88\u804A\u5929\
          \u6A21\u578B\u9700\u8981\u7684\u5C0D\u8A71\u683C\u5F0F\u3002</p>\n<p>\u4E00\
          \u4E9B\u7B2C\u4E09\u65B9\u90E8\u5C6C\u6846\u67B6\u4E5F\u958B\u59CB\u63A1\
          \u7528\u9019\u500B\u8A2D\u5B9A(\u5982<a rel=\"nofollow\" href=\"https://github.com/bentoml/OpenLLM\"\
          >OpenLLM</a>)\uFF0C\u4F46\u662F\u5C0D\u65BC\u6C92\u6709\u8A2D\u7F6E<code>tokenizer.chat_template</code>\u7684\
          \u5206\u8A5E\u5668\u6703\u81EA\u52D5fallback\u63A1\u7528<code>tokenizer.default_chat_template</code>\u5C0E\
          \u81F4\u683C\u5F0F<strong>\u4E0D\u6B63\u78BA</strong>:</p>\n<h4 id=\"\u76EE\
          \u524D\u6C92\u6709\u8A2D\u7F6E-chat_template\">\u76EE\u524D\u6C92\u6709\u8A2D\
          \u7F6E chat_template</h4>\n<pre><code class=\"language-python\">chatglm_tokenizer.chat_template\n\
          <span class=\"hljs-comment\"># None</span>\n</code></pre>\n<h4 id=\"\u9810\
          \u8A2D\u7684-chat_template-chatml\u683C\u5F0F\">\u9810\u8A2D\u7684 chat_template\
          \ (ChatML\u683C\u5F0F)</h4>\n<pre><code class=\"language-python\">chatglm_tokenizer.default_chat_template\n\
          <span class=\"hljs-comment\"># {% for message in messages %}{{'&lt;|im_start|&gt;'\
          \ + message['role'] + '</span>\n<span class=\"hljs-comment\"># ' + message['content']\
          \ + '&lt;|im_end|&gt;' + '</span>\n<span class=\"hljs-comment\"># '}}{%\
          \ endfor %}{% if add_generation_prompt %}{{ '&lt;|im_start|&gt;assistant</span>\n\
          <span class=\"hljs-comment\"># ' }}{% endif %}</span>\n</code></pre>\n<h4\
          \ id=\"chat_template-fot-chatglm\">chat_template fot chatglm</h4>\n<p>\u9019\
          \u500BPR\u63D0\u4F9B jinja template \u8B93\u65B0\u7684 tokenzier.chat_template\
          \ feature \u80FD\u5920 work:</p>\n<pre><code class=\"language-jinja\"><span\
          \ class=\"hljs-template-tag\">{% <span class=\"hljs-name\"><span class=\"\
          hljs-name\">for</span></span> message <span class=\"hljs-keyword\">in</span>\
          \ messages %}</span><span class=\"hljs-template-tag\">{% <span class=\"\
          hljs-name\"><span class=\"hljs-name\">if</span></span> loop.first %}</span><span\
          \ class=\"language-xml\">[gMASK]sop&lt;|</span><span class=\"hljs-template-variable\"\
          >{{ message['role'] }}</span><span class=\"language-xml\">|&gt; \\n </span><span\
          \ class=\"hljs-template-variable\">{{ message['content'] }}</span><span\
          \ class=\"hljs-template-tag\">{% <span class=\"hljs-name\"><span class=\"\
          hljs-name\">else</span></span> %}</span><span class=\"language-xml\">&lt;|</span><span\
          \ class=\"hljs-template-variable\">{{ message['role'] }}</span><span class=\"\
          language-xml\">|&gt; \\n </span><span class=\"hljs-template-variable\">{{\
          \ message['content'] }}</span><span class=\"hljs-template-tag\">{% <span\
          \ class=\"hljs-name\"><span class=\"hljs-name\">endif</span></span> %}</span><span\
          \ class=\"hljs-template-tag\">{% <span class=\"hljs-name\"><span class=\"\
          hljs-name\">endfor</span></span> %}</span><span class=\"hljs-template-tag\"\
          >{% <span class=\"hljs-name\"><span class=\"hljs-name\">if</span></span>\
          \ add_generation_prompt %}</span><span class=\"language-xml\">&lt;|assistant|&gt;</span><span\
          \ class=\"hljs-template-tag\">{% <span class=\"hljs-name\"><span class=\"\
          hljs-name\">endif</span></span> %}</span>\n</code></pre>\n<h3 id=\"\u4F7F\
          \u7528-chat_template\">\u4F7F\u7528 chat_template</h3>\n<p>\u73FE\u5728\u53EF\
          \u4EE5\u76F4\u63A5\u4F7F\u7528 <code>model.generate</code>\uFF0C\u5177\u9AD4\
          \u6548\u679C\u5982\u4E0B:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer,AutoModelForCausalLM\n\nmodel_id_or_path =\
          \ <span class=\"hljs-string\">\"THUDM/chatglm3-6b\"</span>\ntokenizer =\
          \ AutoTokenizer.from_pretrained(model_id_or_path,trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\ntokenizer.chat_template = <span class=\"\
          hljs-string\">\"{% for message in messages %}{% if loop.first %}[gMASK]sop&lt;|{{\
          \ message['role'] }}|&gt; \\n {{ message['content'] }}{% else %}&lt;|{{\
          \ message['role'] }}|&gt; \\n {{ message['content'] }}{% endif %}{% endfor\
          \ %}{% if add_generation_prompt %}&lt;|assistant|&gt;{% endif %}\"</span>\n\
          model = AutoModelForCausalLM.from_pretrained(model_id_or_path,device_map=<span\
          \ class=\"hljs-string\">\"auto\"</span>,trust_remote_code=<span class=\"\
          hljs-literal\">True</span>)\ninputs = tokenizer.apply_chat_template([\n\
          \    {<span class=\"hljs-string\">\"role\"</span>:<span class=\"hljs-string\"\
          >\"system\"</span>,<span class=\"hljs-string\">\"content\"</span>:<span\
          \ class=\"hljs-string\">\"\u4F60\u662F\u4E00\u4F4D\u6A02\u65BC\u52A9\u4EBA\
          \u3001\u5C0A\u91CD\u4ED6\u4EBA\u4E14\u8AA0\u5BE6\u7684\u52A9\u7406\u3002\
          \u8ACB\u59CB\u7D42\u4EE5\u6700\u6709\u5E6B\u52A9\u7684\u65B9\u5F0F\u56DE\
          \u7B54\u554F\u984C\u3002\u5982\u679C\u4F60\u5C0D\u67D0\u500B\u554F\u984C\
          \u4E0D\u77E5\u9053\u7B54\u6848\uFF0C\u8ACB\u4E0D\u8981\u63D0\u4F9B\u865B\
          \u5047\u4FE1\u606F\u3002\"</span>},\n    {<span class=\"hljs-string\">\"\
          role\"</span>:<span class=\"hljs-string\">\"user\"</span>,<span class=\"\
          hljs-string\">\"content\"</span>:<span class=\"hljs-string\">\"\u5982\u4F55\
          \u6E1B\u7DE9\u5730\u7403\u6696\u5316\uFF1F\"</span>}\n],add_generation_prompt=<span\
          \ class=\"hljs-literal\">True</span>,tokenize=<span class=\"hljs-literal\"\
          >True</span>,return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n\
          \nout = model.generate(inputs,max_new_tokens=<span class=\"hljs-number\"\
          >256</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(out[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n<pre><code>[gMASK]sop&lt;|system|&gt;\
          \ \n \u4F60\u662F\u4E00\u4F4D\u6A02\u65BC\u52A9\u4EBA\u3001\u5C0A\u91CD\u4ED6\
          \u4EBA\u4E14\u8AA0\u5BE6\u7684\u52A9\u7406\u3002\u8ACB\u59CB\u7D42\u4EE5\
          \u6700\u6709\u5E6B\u52A9\u7684\u65B9\u5F0F\u56DE\u7B54\u554F\u984C\u3002\
          \u5982\u679C\u4F60\u5C0D\u67D0\u500B\u554F\u984C\u4E0D\u77E5\u9053\u7B54\
          \u6848\uFF0C\u8ACB\u4E0D\u8981\u63D0\u4F9B\u865B\u5047\u4FE1\u606F\u3002\
          &lt;|user|&gt; \n \u5982\u4F55\u6E1B\u7DE9\u5730\u7403\u6696\u5316\uFF1F\
          &lt;|assistant|&gt; \n \u6E1B\u7DE9\u5730\u7403\u6696\u5316\u6709\u8A31\u591A\
          \u65B9\u6CD5\uFF0C\u4EE5\u4E0B\u662F\u4E00\u4E9B\u4E3B\u8981\u7684\u63AA\
          \u65BD\uFF1A\n\n1. \u6E1B\u5C11\u4E8C\u6C27\u5316\u78B3\u6392\u653E\uFF1A\
          \u9019\u5305\u62EC\u6E1B\u5C11\u5DE5\u696D\u548C\u4EA4\u901A\u78B3\u6392\
          \u653E\uFF0C\u4EE5\u53CA\u63D0\u9AD8\u80FD\u6E90\u6548\u7387\u3002\n2. \u63A1\
          \u7528\u53EF\u518D\u751F\u80FD\u6E90\uFF1A\u5982\u592A\u967D\u80FD\u3001\
          \u98A8\u80FD\u548C\u6C34\u80FD\u7B49\u3002\n3. \u4FDD\u8B77\u68EE\u6797\uFF1A\
          \u68EE\u6797\u53EF\u4EE5\u5438\u6536\u4E8C\u6C27\u5316\u78B3\uFF0C\u5982\
          \u679C\u68EE\u6797\u88AB\u780D\u4F10\u6216\u88AB\u71D2\u6BC0\uFF0C\u6703\
          \u589E\u52A0\u4E8C\u6C27\u5316\u78B3\u7684\u6392\u653E\u3002\n4. \u6E1B\u5C11\
          \u6EAB\u5BA4\u6C23\u9AD4\u6392\u653E\uFF1A\u9019\u5305\u62EC\u6E1B\u5C11\
          \u8FB2\u696D\u548C\u5DE5\u696D\u6EAB\u5BA4\u6C23\u9AD4\u6392\u653E\uFF0C\
          \u4EE5\u53CA\u63D0\u9AD8\u80FD\u6E90\u6548\u7387\u3002\n5. \u6539\u8B8A\u98F2\
          \u98DF\u7FD2\u6163\uFF1A\u6E1B\u5C11\u8089\u985E\u548C\u4E73\u88FD\u54C1\
          \ consumption\uFF0C\u56E0\u70BA\u5B83\u5011\u7522\u751F\u7684\u5927\u6C23\
          \u78B3\u6392}&gt;\n</code></pre>\n<h3 id=\"\u683C\u5F0F\u6B63\u78BA\u6027\
          \u9A57\u8B49\">\u683C\u5F0F\u6B63\u78BA\u6027\u9A57\u8B49</h3>\n<p>\u5DF2\
          \u7D93\u900F\u904E<code>difflab</code>\u8207<code>tokenizer.build_chat_input</code>\u6BD4\
          \u8F03\u78BA\u8F38\u51FA\u4E00\u81F4\u6027</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-comment\">#from difflib import ndiff</span>\n<span class=\"\
          hljs-comment\"># \u5982\u679C\u6A23\u677F\u7D50\u679C\u8207\u5B98\u65B9\u7248\
          \u672C\u4E0D\u540C\uFF0C\u6BD4\u8F03\u5DEE\u7570</span>\n<span class=\"\
          hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span> jinja_template_result\
          \ == official_result:\n    str1 = jinja_template_result\n    str2 = official_result\n\
          \    diff = ndiff(str1.splitlines(), str2.splitlines())\n    <span class=\"\
          hljs-keyword\">for</span> line <span class=\"hljs-keyword\">in</span> diff:\n\
          \        <span class=\"hljs-built_in\">print</span>(line)\n</code></pre>\n"
        raw: "### \u7C21\u77ED\n\u8A2D\u7F6E `tokenizer.chat_template` \u4F86\u4F7F\
          \u7528`tokenizer.apply_chat_template`, `model.generate`\u548C `pipeline`(text-gen)\
          \ \u7B49transformer\u63D0\u4F9B\u4E4B\u529F\u80FD\uFF0C\u6E1B\u5C11\u4E0D\
          \u4E00\u81F4\u6027\u53EF\u80FD\u3002\n```python\nmodel_id_or_path = \"THUDM/chatglm3-6b\"\
          \ntokenizer = AutoTokenizer.from_pretrained(model_id_or_path,trust_remote_code=True)\n\
          tokenizer.chat_template = \"{% for message in messages %}{% if loop.first\
          \ %}[gMASK]sop<|{{ message['role'] }}|> \\n {{ message['content'] }}{% else\
          \ %}<|{{ message['role'] }}|> \\n {{ message['content'] }}{% endif %}{%\
          \ endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}\"\n```\n\
          ### \u6B64PR\u76EE\u7684\n\u65B0\u7248\u672C\u7684 transformer tokenizer\
          \ \u5DF2\u7D93\u652F\u63F4 chat_template \u5C6C\u6027 [(https://huggingface.co/blog/zh/chat-templates)](https://huggingface.co/blog/zh/chat-templates)\uFF0C\
          \u5176\u76EE\u7684\u5728\u66F4\u597D\u7684\u8B93\u4F7F\u7528\u8005\u9075\
          \u5B88\u804A\u5929\u6A21\u578B\u9700\u8981\u7684\u5C0D\u8A71\u683C\u5F0F\
          \u3002\n\n\u4E00\u4E9B\u7B2C\u4E09\u65B9\u90E8\u5C6C\u6846\u67B6\u4E5F\u958B\
          \u59CB\u63A1\u7528\u9019\u500B\u8A2D\u5B9A(\u5982[OpenLLM](https://github.com/bentoml/OpenLLM))\uFF0C\
          \u4F46\u662F\u5C0D\u65BC\u6C92\u6709\u8A2D\u7F6E`tokenizer.chat_template`\u7684\
          \u5206\u8A5E\u5668\u6703\u81EA\u52D5fallback\u63A1\u7528`tokenizer.default_chat_template`\u5C0E\
          \u81F4\u683C\u5F0F**\u4E0D\u6B63\u78BA**:\n\n#### \u76EE\u524D\u6C92\u6709\
          \u8A2D\u7F6E chat_template\n```python\nchatglm_tokenizer.chat_template\n\
          # None\n```\n\n#### \u9810\u8A2D\u7684 chat_template (ChatML\u683C\u5F0F\
          )\n```python\nchatglm_tokenizer.default_chat_template\n# {% for message\
          \ in messages %}{{'<|im_start|>' + message['role'] + '\n# ' + message['content']\
          \ + '<|im_end|>' + '\n# '}}{% endfor %}{% if add_generation_prompt %}{{\
          \ '<|im_start|>assistant\n# ' }}{% endif %}\n```\n\n#### chat_template fot\
          \ chatglm\n\u9019\u500BPR\u63D0\u4F9B jinja template \u8B93\u65B0\u7684\
          \ tokenzier.chat_template feature \u80FD\u5920 work:\n```jinja\n{% for message\
          \ in messages %}{% if loop.first %}[gMASK]sop<|{{ message['role'] }}|> \\\
          n {{ message['content'] }}{% else %}<|{{ message['role'] }}|> \\n {{ message['content']\
          \ }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{%\
          \ endif %}\n```\n### \u4F7F\u7528 chat_template\n\u73FE\u5728\u53EF\u4EE5\
          \u76F4\u63A5\u4F7F\u7528 `model.generate`\uFF0C\u5177\u9AD4\u6548\u679C\u5982\
          \u4E0B:\n```python\nfrom transformers import AutoTokenizer,AutoModelForCausalLM\n\
          \nmodel_id_or_path = \"THUDM/chatglm3-6b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id_or_path,trust_remote_code=True)\n\
          tokenizer.chat_template = \"{% for message in messages %}{% if loop.first\
          \ %}[gMASK]sop<|{{ message['role'] }}|> \\n {{ message['content'] }}{% else\
          \ %}<|{{ message['role'] }}|> \\n {{ message['content'] }}{% endif %}{%\
          \ endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}\"\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(model_id_or_path,device_map=\"\
          auto\",trust_remote_code=True)\ninputs = tokenizer.apply_chat_template([\n\
          \    {\"role\":\"system\",\"content\":\"\u4F60\u662F\u4E00\u4F4D\u6A02\u65BC\
          \u52A9\u4EBA\u3001\u5C0A\u91CD\u4ED6\u4EBA\u4E14\u8AA0\u5BE6\u7684\u52A9\
          \u7406\u3002\u8ACB\u59CB\u7D42\u4EE5\u6700\u6709\u5E6B\u52A9\u7684\u65B9\
          \u5F0F\u56DE\u7B54\u554F\u984C\u3002\u5982\u679C\u4F60\u5C0D\u67D0\u500B\
          \u554F\u984C\u4E0D\u77E5\u9053\u7B54\u6848\uFF0C\u8ACB\u4E0D\u8981\u63D0\
          \u4F9B\u865B\u5047\u4FE1\u606F\u3002\"},\n    {\"role\":\"user\",\"content\"\
          :\"\u5982\u4F55\u6E1B\u7DE9\u5730\u7403\u6696\u5316\uFF1F\"}\n],add_generation_prompt=True,tokenize=True,return_tensors=\"\
          pt\")\n\nout = model.generate(inputs,max_new_tokens=256)\nprint(tokenizer.decode(out[0]))\n\
          ```\n```\n[gMASK]sop<|system|> \n \u4F60\u662F\u4E00\u4F4D\u6A02\u65BC\u52A9\
          \u4EBA\u3001\u5C0A\u91CD\u4ED6\u4EBA\u4E14\u8AA0\u5BE6\u7684\u52A9\u7406\
          \u3002\u8ACB\u59CB\u7D42\u4EE5\u6700\u6709\u5E6B\u52A9\u7684\u65B9\u5F0F\
          \u56DE\u7B54\u554F\u984C\u3002\u5982\u679C\u4F60\u5C0D\u67D0\u500B\u554F\
          \u984C\u4E0D\u77E5\u9053\u7B54\u6848\uFF0C\u8ACB\u4E0D\u8981\u63D0\u4F9B\
          \u865B\u5047\u4FE1\u606F\u3002<|user|> \n \u5982\u4F55\u6E1B\u7DE9\u5730\
          \u7403\u6696\u5316\uFF1F<|assistant|> \n \u6E1B\u7DE9\u5730\u7403\u6696\u5316\
          \u6709\u8A31\u591A\u65B9\u6CD5\uFF0C\u4EE5\u4E0B\u662F\u4E00\u4E9B\u4E3B\
          \u8981\u7684\u63AA\u65BD\uFF1A\n\n1. \u6E1B\u5C11\u4E8C\u6C27\u5316\u78B3\
          \u6392\u653E\uFF1A\u9019\u5305\u62EC\u6E1B\u5C11\u5DE5\u696D\u548C\u4EA4\
          \u901A\u78B3\u6392\u653E\uFF0C\u4EE5\u53CA\u63D0\u9AD8\u80FD\u6E90\u6548\
          \u7387\u3002\n2. \u63A1\u7528\u53EF\u518D\u751F\u80FD\u6E90\uFF1A\u5982\u592A\
          \u967D\u80FD\u3001\u98A8\u80FD\u548C\u6C34\u80FD\u7B49\u3002\n3. \u4FDD\u8B77\
          \u68EE\u6797\uFF1A\u68EE\u6797\u53EF\u4EE5\u5438\u6536\u4E8C\u6C27\u5316\
          \u78B3\uFF0C\u5982\u679C\u68EE\u6797\u88AB\u780D\u4F10\u6216\u88AB\u71D2\
          \u6BC0\uFF0C\u6703\u589E\u52A0\u4E8C\u6C27\u5316\u78B3\u7684\u6392\u653E\
          \u3002\n4. \u6E1B\u5C11\u6EAB\u5BA4\u6C23\u9AD4\u6392\u653E\uFF1A\u9019\u5305\
          \u62EC\u6E1B\u5C11\u8FB2\u696D\u548C\u5DE5\u696D\u6EAB\u5BA4\u6C23\u9AD4\
          \u6392\u653E\uFF0C\u4EE5\u53CA\u63D0\u9AD8\u80FD\u6E90\u6548\u7387\u3002\
          \n5. \u6539\u8B8A\u98F2\u98DF\u7FD2\u6163\uFF1A\u6E1B\u5C11\u8089\u985E\u548C\
          \u4E73\u88FD\u54C1 consumption\uFF0C\u56E0\u70BA\u5B83\u5011\u7522\u751F\
          \u7684\u5927\u6C23\u78B3\u6392}>\n```\n\n### \u683C\u5F0F\u6B63\u78BA\u6027\
          \u9A57\u8B49\n\u5DF2\u7D93\u900F\u904E`difflab`\u8207`tokenizer.build_chat_input`\u6BD4\
          \u8F03\u78BA\u8F38\u51FA\u4E00\u81F4\u6027\n```python\n#from difflib import\
          \ ndiff\n# \u5982\u679C\u6A23\u677F\u7D50\u679C\u8207\u5B98\u65B9\u7248\u672C\
          \u4E0D\u540C\uFF0C\u6BD4\u8F03\u5DEE\u7570\nif not jinja_template_result\
          \ == official_result:\n    str1 = jinja_template_result\n    str2 = official_result\n\
          \    diff = ndiff(str1.splitlines(), str2.splitlines())\n    for line in\
          \ diff:\n        print(line)\n```"
        updatedAt: '2023-12-18T06:17:17.197Z'
      numEdits: 4
      reactions: []
    id: 657fe271e1116d68e9e1e97e
    type: comment
  author: p208p2002
  content: "### \u7C21\u77ED\n\u8A2D\u7F6E `tokenizer.chat_template` \u4F86\u4F7F\u7528\
    `tokenizer.apply_chat_template`, `model.generate`\u548C `pipeline`(text-gen) \u7B49\
    transformer\u63D0\u4F9B\u4E4B\u529F\u80FD\uFF0C\u6E1B\u5C11\u4E0D\u4E00\u81F4\u6027\
    \u53EF\u80FD\u3002\n```python\nmodel_id_or_path = \"THUDM/chatglm3-6b\"\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_id_or_path,trust_remote_code=True)\ntokenizer.chat_template\
    \ = \"{% for message in messages %}{% if loop.first %}[gMASK]sop<|{{ message['role']\
    \ }}|> \\n {{ message['content'] }}{% else %}<|{{ message['role'] }}|> \\n {{\
    \ message['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{%\
    \ endif %}\"\n```\n### \u6B64PR\u76EE\u7684\n\u65B0\u7248\u672C\u7684 transformer\
    \ tokenizer \u5DF2\u7D93\u652F\u63F4 chat_template \u5C6C\u6027 [(https://huggingface.co/blog/zh/chat-templates)](https://huggingface.co/blog/zh/chat-templates)\uFF0C\
    \u5176\u76EE\u7684\u5728\u66F4\u597D\u7684\u8B93\u4F7F\u7528\u8005\u9075\u5B88\
    \u804A\u5929\u6A21\u578B\u9700\u8981\u7684\u5C0D\u8A71\u683C\u5F0F\u3002\n\n\u4E00\
    \u4E9B\u7B2C\u4E09\u65B9\u90E8\u5C6C\u6846\u67B6\u4E5F\u958B\u59CB\u63A1\u7528\
    \u9019\u500B\u8A2D\u5B9A(\u5982[OpenLLM](https://github.com/bentoml/OpenLLM))\uFF0C\
    \u4F46\u662F\u5C0D\u65BC\u6C92\u6709\u8A2D\u7F6E`tokenizer.chat_template`\u7684\
    \u5206\u8A5E\u5668\u6703\u81EA\u52D5fallback\u63A1\u7528`tokenizer.default_chat_template`\u5C0E\
    \u81F4\u683C\u5F0F**\u4E0D\u6B63\u78BA**:\n\n#### \u76EE\u524D\u6C92\u6709\u8A2D\
    \u7F6E chat_template\n```python\nchatglm_tokenizer.chat_template\n# None\n```\n\
    \n#### \u9810\u8A2D\u7684 chat_template (ChatML\u683C\u5F0F)\n```python\nchatglm_tokenizer.default_chat_template\n\
    # {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n# ' + message['content']\
    \ + '<|im_end|>' + '\n# '}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n\
    # ' }}{% endif %}\n```\n\n#### chat_template fot chatglm\n\u9019\u500BPR\u63D0\
    \u4F9B jinja template \u8B93\u65B0\u7684 tokenzier.chat_template feature \u80FD\
    \u5920 work:\n```jinja\n{% for message in messages %}{% if loop.first %}[gMASK]sop<|{{\
    \ message['role'] }}|> \\n {{ message['content'] }}{% else %}<|{{ message['role']\
    \ }}|> \\n {{ message['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt\
    \ %}<|assistant|>{% endif %}\n```\n### \u4F7F\u7528 chat_template\n\u73FE\u5728\
    \u53EF\u4EE5\u76F4\u63A5\u4F7F\u7528 `model.generate`\uFF0C\u5177\u9AD4\u6548\u679C\
    \u5982\u4E0B:\n```python\nfrom transformers import AutoTokenizer,AutoModelForCausalLM\n\
    \nmodel_id_or_path = \"THUDM/chatglm3-6b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id_or_path,trust_remote_code=True)\n\
    tokenizer.chat_template = \"{% for message in messages %}{% if loop.first %}[gMASK]sop<|{{\
    \ message['role'] }}|> \\n {{ message['content'] }}{% else %}<|{{ message['role']\
    \ }}|> \\n {{ message['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt\
    \ %}<|assistant|>{% endif %}\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id_or_path,device_map=\"\
    auto\",trust_remote_code=True)\ninputs = tokenizer.apply_chat_template([\n   \
    \ {\"role\":\"system\",\"content\":\"\u4F60\u662F\u4E00\u4F4D\u6A02\u65BC\u52A9\
    \u4EBA\u3001\u5C0A\u91CD\u4ED6\u4EBA\u4E14\u8AA0\u5BE6\u7684\u52A9\u7406\u3002\
    \u8ACB\u59CB\u7D42\u4EE5\u6700\u6709\u5E6B\u52A9\u7684\u65B9\u5F0F\u56DE\u7B54\
    \u554F\u984C\u3002\u5982\u679C\u4F60\u5C0D\u67D0\u500B\u554F\u984C\u4E0D\u77E5\
    \u9053\u7B54\u6848\uFF0C\u8ACB\u4E0D\u8981\u63D0\u4F9B\u865B\u5047\u4FE1\u606F\
    \u3002\"},\n    {\"role\":\"user\",\"content\":\"\u5982\u4F55\u6E1B\u7DE9\u5730\
    \u7403\u6696\u5316\uFF1F\"}\n],add_generation_prompt=True,tokenize=True,return_tensors=\"\
    pt\")\n\nout = model.generate(inputs,max_new_tokens=256)\nprint(tokenizer.decode(out[0]))\n\
    ```\n```\n[gMASK]sop<|system|> \n \u4F60\u662F\u4E00\u4F4D\u6A02\u65BC\u52A9\u4EBA\
    \u3001\u5C0A\u91CD\u4ED6\u4EBA\u4E14\u8AA0\u5BE6\u7684\u52A9\u7406\u3002\u8ACB\
    \u59CB\u7D42\u4EE5\u6700\u6709\u5E6B\u52A9\u7684\u65B9\u5F0F\u56DE\u7B54\u554F\
    \u984C\u3002\u5982\u679C\u4F60\u5C0D\u67D0\u500B\u554F\u984C\u4E0D\u77E5\u9053\
    \u7B54\u6848\uFF0C\u8ACB\u4E0D\u8981\u63D0\u4F9B\u865B\u5047\u4FE1\u606F\u3002\
    <|user|> \n \u5982\u4F55\u6E1B\u7DE9\u5730\u7403\u6696\u5316\uFF1F<|assistant|>\
    \ \n \u6E1B\u7DE9\u5730\u7403\u6696\u5316\u6709\u8A31\u591A\u65B9\u6CD5\uFF0C\u4EE5\
    \u4E0B\u662F\u4E00\u4E9B\u4E3B\u8981\u7684\u63AA\u65BD\uFF1A\n\n1. \u6E1B\u5C11\
    \u4E8C\u6C27\u5316\u78B3\u6392\u653E\uFF1A\u9019\u5305\u62EC\u6E1B\u5C11\u5DE5\
    \u696D\u548C\u4EA4\u901A\u78B3\u6392\u653E\uFF0C\u4EE5\u53CA\u63D0\u9AD8\u80FD\
    \u6E90\u6548\u7387\u3002\n2. \u63A1\u7528\u53EF\u518D\u751F\u80FD\u6E90\uFF1A\u5982\
    \u592A\u967D\u80FD\u3001\u98A8\u80FD\u548C\u6C34\u80FD\u7B49\u3002\n3. \u4FDD\u8B77\
    \u68EE\u6797\uFF1A\u68EE\u6797\u53EF\u4EE5\u5438\u6536\u4E8C\u6C27\u5316\u78B3\
    \uFF0C\u5982\u679C\u68EE\u6797\u88AB\u780D\u4F10\u6216\u88AB\u71D2\u6BC0\uFF0C\
    \u6703\u589E\u52A0\u4E8C\u6C27\u5316\u78B3\u7684\u6392\u653E\u3002\n4. \u6E1B\u5C11\
    \u6EAB\u5BA4\u6C23\u9AD4\u6392\u653E\uFF1A\u9019\u5305\u62EC\u6E1B\u5C11\u8FB2\
    \u696D\u548C\u5DE5\u696D\u6EAB\u5BA4\u6C23\u9AD4\u6392\u653E\uFF0C\u4EE5\u53CA\
    \u63D0\u9AD8\u80FD\u6E90\u6548\u7387\u3002\n5. \u6539\u8B8A\u98F2\u98DF\u7FD2\u6163\
    \uFF1A\u6E1B\u5C11\u8089\u985E\u548C\u4E73\u88FD\u54C1 consumption\uFF0C\u56E0\
    \u70BA\u5B83\u5011\u7522\u751F\u7684\u5927\u6C23\u78B3\u6392}>\n```\n\n### \u683C\
    \u5F0F\u6B63\u78BA\u6027\u9A57\u8B49\n\u5DF2\u7D93\u900F\u904E`difflab`\u8207\
    `tokenizer.build_chat_input`\u6BD4\u8F03\u78BA\u8F38\u51FA\u4E00\u81F4\u6027\n\
    ```python\n#from difflib import ndiff\n# \u5982\u679C\u6A23\u677F\u7D50\u679C\u8207\
    \u5B98\u65B9\u7248\u672C\u4E0D\u540C\uFF0C\u6BD4\u8F03\u5DEE\u7570\nif not jinja_template_result\
    \ == official_result:\n    str1 = jinja_template_result\n    str2 = official_result\n\
    \    diff = ndiff(str1.splitlines(), str2.splitlines())\n    for line in diff:\n\
    \        print(line)\n```"
  created_at: 2023-12-18 06:10:57+00:00
  edited: true
  hidden: false
  id: 657fe271e1116d68e9e1e97e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620006622342-606296de21cc4e7bf31dc5b4.jpeg?w=200&h=200&f=face
      fullname: Philip Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: p208p2002
      type: user
    createdAt: '2023-12-18T06:10:58.000Z'
    data:
      oid: 8c4bd56f3f760417ee95b6d20b9f78302f80e78d
      parents:
      - 456aa875cf1f46623006edaa23103774ea9c0eae
      subject: Update tokenizer_config.json
    id: 657fe2720000000000000000
    type: commit
  author: p208p2002
  created_at: 2023-12-18 06:10:58+00:00
  id: 657fe2720000000000000000
  oid: 8c4bd56f3f760417ee95b6d20b9f78302f80e78d
  summary: Update tokenizer_config.json
  type: commit
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620006622342-606296de21cc4e7bf31dc5b4.jpeg?w=200&h=200&f=face
      fullname: Philip Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: p208p2002
      type: user
    createdAt: '2023-12-18T06:16:00.000Z'
    data:
      from: Update tokenizer_config.json
      to: "\u589E\u52A0\u5C0Dtokenizer.chat_template\u7684\u652F\u63F4"
    id: 657fe3a0d70b7308f3b7f52d
    type: title-change
  author: p208p2002
  created_at: 2023-12-18 06:16:00+00:00
  id: 657fe3a0d70b7308f3b7f52d
  new_title: "\u589E\u52A0\u5C0Dtokenizer.chat_template\u7684\u652F\u63F4"
  old_title: Update tokenizer_config.json
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/RQJQZouD-Qnt70k0dVjHU.png?w=200&h=200&f=face
      fullname: Yuxuan Zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zRzRzRzRzRzRzR
      type: user
    createdAt: '2023-12-29T12:53:08.000Z'
    data:
      status: merged
    id: 658ec134cdc0c4099b0a9acc
    type: status-change
  author: zRzRzRzRzRzRzR
  created_at: 2023-12-29 12:53:08+00:00
  id: 658ec134cdc0c4099b0a9acc
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: 1cfbbfdaeafef01da8e07b5be24a14bc7cbcf6f0
num: 22
repo_id: THUDM/chatglm3-6b
repo_type: model
status: merged
target_branch: refs/heads/main
title: "\u589E\u52A0\u5C0Dtokenizer.chat_template\u7684\u652F\u63F4"
