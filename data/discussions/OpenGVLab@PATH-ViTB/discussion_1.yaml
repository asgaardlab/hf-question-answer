!!python/object:huggingface_hub.community.DiscussionWithDetails
author: luomingshuang
conflicting_files: null
created_at: 2023-07-18 05:26:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1636104707106-6178ea05267320abb99df778.jpeg?w=200&h=200&f=face
      fullname: Mingshuang Luo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luomingshuang
      type: user
    createdAt: '2023-07-18T06:26:56.000Z'
    data:
      edited: false
      editors:
      - luomingshuang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.13244149088859558
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1636104707106-6178ea05267320abb99df778.jpeg?w=200&h=200&f=face
          fullname: Mingshuang Luo
          isHf: false
          isPro: false
          name: luomingshuang
          type: user
        html: '<p>Hi, thanks for your great job. I am trying to reproduce your results
          based on this url pretrained weights. But when I load the pretrained weights
          model, I find the keys are not same. The pretrained weights keys are as
          follows:</p>

          <pre><code>odict_keys([''cls_token'', ''cls_token_pos_embed'', ''pos_embed'',
          ''patch_embed.proj.weight'', ''patch_embed.proj.bias'', ''blocks.0.norm1.weight'',
          ''blocks.0.norm1.bias'', ''blocks.0.attn.qkv.weight'', ''blocks.0.attn.qkv.bias'',
          ''blocks.0.attn.proj.weight'', ''blocks.0.attn.proj.bias'', ''blocks.0.norm2.weight'',
          ''blocks.0.norm2.bias'', ''blocks.0.mlp.fc1.weight'', ''blocks.0.mlp.fc1.bias'',
          ''blocks.0.mlp.fc2.weight'', ''blocks.0.mlp.fc2.bias'', ''blocks.1.norm1.weight'',
          ''blocks.1.norm1.bias'', ''blocks.1.attn.qkv.weight'', ''blocks.1.attn.qkv.bias'',
          ''blocks.1.attn.proj.weight'', ''blocks.1.attn.proj.bias'', ''blocks.1.norm2.weight'',
          ''blocks.1.norm2.bias'', ''blocks.1.mlp.fc1.weight'', ''blocks.1.mlp.fc1.bias'',
          ''blocks.1.mlp.fc2.weight'', ''blocks.1.mlp.fc2.bias'', ''blocks.2.norm1.weight'',
          ''blocks.2.norm1.bias'', ''blocks.2.attn.qkv.weight'', ''blocks.2.attn.qkv.bias'',
          ''blocks.2.attn.proj.weight'', ''blocks.2.attn.proj.bias'', ''blocks.2.norm2.weight'',
          ''blocks.2.norm2.bias'', ''blocks.2.mlp.fc1.weight'', ''blocks.2.mlp.fc1.bias'',
          ''blocks.2.mlp.fc2.weight'', ''blocks.2.mlp.fc2.bias'', ''blocks.3.norm1.weight'',
          ''blocks.3.norm1.bias'', ''blocks.3.attn.qkv.weight'', ''blocks.3.attn.qkv.bias'',
          ''blocks.3.attn.proj.weight'', ''blocks.3.attn.proj.bias'', ''blocks.3.norm2.weight'',
          ''blocks.3.norm2.bias'', ''blocks.3.mlp.fc1.weight'', ''blocks.3.mlp.fc1.bias'',
          ''blocks.3.mlp.fc2.weight'', ''blocks.3.mlp.fc2.bias'', ''blocks.4.norm1.weight'',
          ''blocks.4.norm1.bias'', ''blocks.4.attn.qkv.weight'', ''blocks.4.attn.qkv.bias'',
          ''blocks.4.attn.proj.weight'', ''blocks.4.attn.proj.bias'', ''blocks.4.norm2.weight'',
          ''blocks.4.norm2.bias'', ''blocks.4.mlp.fc1.weight'', ''blocks.4.mlp.fc1.bias'',
          ''blocks.4.mlp.fc2.weight'', ''blocks.4.mlp.fc2.bias'', ''blocks.5.norm1.weight'',
          ''blocks.5.norm1.bias'', ''blocks.5.attn.qkv.weight'', ''blocks.5.attn.qkv.bias'',
          ''blocks.5.attn.proj.weight'', ''blocks.5.attn.proj.bias'', ''blocks.5.norm2.weight'',
          ''blocks.5.norm2.bias'', ''blocks.5.mlp.fc1.weight'', ''blocks.5.mlp.fc1.bias'',
          ''blocks.5.mlp.fc2.weight'', ''blocks.5.mlp.fc2.bias'', ''blocks.6.norm1.weight'',
          ''blocks.6.norm1.bias'', ''blocks.6.attn.qkv.weight'', ''blocks.6.attn.qkv.bias'',
          ''blocks.6.attn.proj.weight'', ''blocks.6.attn.proj.bias'', ''blocks.6.norm2.weight'',
          ''blocks.6.norm2.bias'', ''blocks.6.mlp.fc1.weight'', ''blocks.6.mlp.fc1.bias'',
          ''blocks.6.mlp.fc2.weight'', ''blocks.6.mlp.fc2.bias'', ''blocks.7.norm1.weight'',
          ''blocks.7.norm1.bias'', ''blocks.7.attn.qkv.weight'', ''blocks.7.attn.qkv.bias'',
          ''blocks.7.attn.proj.weight'', ''blocks.7.attn.proj.bias'', ''blocks.7.norm2.weight'',
          ''blocks.7.norm2.bias'', ''blocks.7.mlp.fc1.weight'', ''blocks.7.mlp.fc1.bias'',
          ''blocks.7.mlp.fc2.weight'', ''blocks.7.mlp.fc2.bias'', ''blocks.8.norm1.weight'',
          ''blocks.8.norm1.bias'', ''blocks.8.attn.qkv.weight'', ''blocks.8.attn.qkv.bias'',
          ''blocks.8.attn.proj.weight'', ''blocks.8.attn.proj.bias'', ''blocks.8.norm2.weight'',
          ''blocks.8.norm2.bias'', ''blocks.8.mlp.fc1.weight'', ''blocks.8.mlp.fc1.bias'',
          ''blocks.8.mlp.fc2.weight'', ''blocks.8.mlp.fc2.bias'', ''blocks.9.norm1.weight'',
          ''blocks.9.norm1.bias'', ''blocks.9.attn.qkv.weight'', ''blocks.9.attn.qkv.bias'',
          ''blocks.9.attn.proj.weight'', ''blocks.9.attn.proj.bias'', ''blocks.9.norm2.weight'',
          ''blocks.9.norm2.bias'', ''blocks.9.mlp.fc1.weight'', ''blocks.9.mlp.fc1.bias'',
          ''blocks.9.mlp.fc2.weight'', ''blocks.9.mlp.fc2.bias'', ''blocks.10.norm1.weight'',
          ''blocks.10.norm1.bias'', ''blocks.10.attn.qkv.weight'', ''blocks.10.attn.qkv.bias'',
          ''blocks.10.attn.proj.weight'', ''blocks.10.attn.proj.bias'', ''blocks.10.norm2.weight'',
          ''blocks.10.norm2.bias'', ''blocks.10.mlp.fc1.weight'', ''blocks.10.mlp.fc1.bias'',
          ''blocks.10.mlp.fc2.weight'', ''blocks.10.mlp.fc2.bias'', ''blocks.11.norm1.weight'',
          ''blocks.11.norm1.bias'', ''blocks.11.attn.qkv.weight'', ''blocks.11.attn.qkv.bias'',
          ''blocks.11.attn.proj.weight'', ''blocks.11.attn.proj.bias'', ''blocks.11.norm2.weight'',
          ''blocks.11.norm2.bias'', ''blocks.11.mlp.fc1.weight'', ''blocks.11.mlp.fc1.bias'',
          ''blocks.11.mlp.fc2.weight'', ''blocks.11.mlp.fc2.bias'', ''blocks.12.norm1.weight'',
          ''blocks.12.norm1.bias'', ''blocks.12.attn.qkv.weight'', ''blocks.12.attn.qkv.bias'',
          ''blocks.12.attn.proj.weight'', ''blocks.12.attn.proj.bias'', ''blocks.12.norm2.weight'',
          ''blocks.12.norm2.bias'', ''blocks.12.mlp.fc1.weight'', ''blocks.12.mlp.fc1.bias'',
          ''blocks.12.mlp.fc2.weight'', ''blocks.12.mlp.fc2.bias'', ''blocks.13.norm1.weight'',
          ''blocks.13.norm1.bias'', ''blocks.13.attn.qkv.weight'', ''blocks.13.attn.qkv.bias'',
          ''blocks.13.attn.proj.weight'', ''blocks.13.attn.proj.bias'', ''blocks.13.norm2.weight'',
          ''blocks.13.norm2.bias'', ''blocks.13.mlp.fc1.weight'', ''blocks.13.mlp.fc1.bias'',
          ''blocks.13.mlp.fc2.weight'', ''blocks.13.mlp.fc2.bias'', ''blocks.14.norm1.weight'',
          ''blocks.14.norm1.bias'', ''blocks.14.attn.qkv.weight'', ''blocks.14.attn.qkv.bias'',
          ''blocks.14.attn.proj.weight'', ''blocks.14.attn.proj.bias'', ''blocks.14.norm2.weight'',
          ''blocks.14.norm2.bias'', ''blocks.14.mlp.fc1.weight'', ''blocks.14.mlp.fc1.bias'',
          ''blocks.14.mlp.fc2.weight'', ''blocks.14.mlp.fc2.bias'', ''blocks.15.norm1.weight'',
          ''blocks.15.norm1.bias'', ''blocks.15.attn.qkv.weight'', ''blocks.15.attn.qkv.bias'',
          ''blocks.15.attn.proj.weight'', ''blocks.15.attn.proj.bias'', ''blocks.15.norm2.weight'',
          ''blocks.15.norm2.bias'', ''blocks.15.mlp.fc1.weight'', ''blocks.15.mlp.fc1.bias'',
          ''blocks.15.mlp.fc2.weight'', ''blocks.15.mlp.fc2.bias'', ''blocks.16.norm1.weight'',
          ''blocks.16.norm1.bias'', ''blocks.16.attn.qkv.weight'', ''blocks.16.attn.qkv.bias'',
          ''blocks.16.attn.proj.weight'', ''blocks.16.attn.proj.bias'', ''blocks.16.norm2.weight'',
          ''blocks.16.norm2.bias'', ''blocks.16.mlp.fc1.weight'', ''blocks.16.mlp.fc1.bias'',
          ''blocks.16.mlp.fc2.weight'', ''blocks.16.mlp.fc2.bias'', ''blocks.17.norm1.weight'',
          ''blocks.17.norm1.bias'', ''blocks.17.attn.qkv.weight'', ''blocks.17.attn.qkv.bias'',
          ''blocks.17.attn.proj.weight'', ''blocks.17.attn.proj.bias'', ''blocks.17.norm2.weight'',
          ''blocks.17.norm2.bias'', ''blocks.17.mlp.fc1.weight'', ''blocks.17.mlp.fc1.bias'',
          ''blocks.17.mlp.fc2.weight'', ''blocks.17.mlp.fc2.bias'', ''blocks.18.norm1.weight'',
          ''blocks.18.norm1.bias'', ''blocks.18.attn.qkv.weight'', ''blocks.18.attn.qkv.bias'',
          ''blocks.18.attn.proj.weight'', ''blocks.18.attn.proj.bias'', ''blocks.18.norm2.weight'',
          ''blocks.18.norm2.bias'', ''blocks.18.mlp.fc1.weight'', ''blocks.18.mlp.fc1.bias'',
          ''blocks.18.mlp.fc2.weight'', ''blocks.18.mlp.fc2.bias'', ''blocks.19.norm1.weight'',
          ''blocks.19.norm1.bias'', ''blocks.19.attn.qkv.weight'', ''blocks.19.attn.qkv.bias'',
          ''blocks.19.attn.proj.weight'', ''blocks.19.attn.proj.bias'', ''blocks.19.norm2.weight'',
          ''blocks.19.norm2.bias'', ''blocks.19.mlp.fc1.weight'', ''blocks.19.mlp.fc1.bias'',
          ''blocks.19.mlp.fc2.weight'', ''blocks.19.mlp.fc2.bias'', ''blocks.20.norm1.weight'',
          ''blocks.20.norm1.bias'', ''blocks.20.attn.qkv.weight'', ''blocks.20.attn.qkv.bias'',
          ''blocks.20.attn.proj.weight'', ''blocks.20.attn.proj.bias'', ''blocks.20.norm2.weight'',
          ''blocks.20.norm2.bias'', ''blocks.20.mlp.fc1.weight'', ''blocks.20.mlp.fc1.bias'',
          ''blocks.20.mlp.fc2.weight'', ''blocks.20.mlp.fc2.bias'', ''blocks.21.norm1.weight'',
          ''blocks.21.norm1.bias'', ''blocks.21.attn.qkv.weight'', ''blocks.21.attn.qkv.bias'',
          ''blocks.21.attn.proj.weight'', ''blocks.21.attn.proj.bias'', ''blocks.21.norm2.weight'',
          ''blocks.21.norm2.bias'', ''blocks.21.mlp.fc1.weight'', ''blocks.21.mlp.fc1.bias'',
          ''blocks.21.mlp.fc2.weight'', ''blocks.21.mlp.fc2.bias'', ''blocks.22.norm1.weight'',
          ''blocks.22.norm1.bias'', ''blocks.22.attn.qkv.weight'', ''blocks.22.attn.qkv.bias'',
          ''blocks.22.attn.proj.weight'', ''blocks.22.attn.proj.bias'', ''blocks.22.norm2.weight'',
          ''blocks.22.norm2.bias'', ''blocks.22.mlp.fc1.weight'', ''blocks.22.mlp.fc1.bias'',
          ''blocks.22.mlp.fc2.weight'', ''blocks.22.mlp.fc2.bias'', ''blocks.23.norm1.weight'',
          ''blocks.23.norm1.bias'', ''blocks.23.attn.qkv.weight'', ''blocks.23.attn.qkv.bias'',
          ''blocks.23.attn.proj.weight'', ''blocks.23.attn.proj.bias'', ''blocks.23.norm2.weight'',
          ''blocks.23.norm2.bias'', ''blocks.23.mlp.fc1.weight'', ''blocks.23.mlp.fc1.bias'',
          ''blocks.23.mlp.fc2.weight'', ''blocks.23.mlp.fc2.bias'', ''norm.weight'',
          ''norm.bias''])

          </code></pre>

          <p>But the keys for  the model in PATH  are as follows:</p>

          <pre><code>odict_keys([''module.backbone_module.pos_embed'', ''module.backbone_module.patch_embed.proj.weight'',
          ''module.backbone_module.patch_embed.proj.bias'', ''module.backbone_module.blocks.0.norm1.weight'',
          ''module.backbone_module.blocks.0.norm1.bias'', ''module.backbone_module.blocks.0.attn.qkv.weight'',
          ''module.backbone_module.blocks.0.attn.qkv.bias'', ''module.backbone_module.blocks.0.attn.proj.weight'',
          ''module.backbone_module.blocks.0.attn.proj.bias'', ''module.backbone_module.blocks.0.norm2.weight'',
          ''module.backbone_module.blocks.0.norm2.bias'', ''module.backbone_module.blocks.0.mlp.fc1.weight'',
          ''module.backbone_module.blocks.0.mlp.fc1.bias'', ''module.backbone_module.blocks.0.mlp.fc2.weight'',
          ''module.backbone_module.blocks.0.mlp.fc2.bias'', ''module.backbone_module.blocks.1.norm1.weight'',
          ''module.backbone_module.blocks.1.norm1.bias'', ''module.backbone_module.blocks.1.attn.qkv.weight'',
          ''module.backbone_module.blocks.1.attn.qkv.bias'', ''module.backbone_module.blocks.1.attn.proj.weight'',
          ''module.backbone_module.blocks.1.attn.proj.bias'', ''module.backbone_module.blocks.1.norm2.weight'',
          ''module.backbone_module.blocks.1.norm2.bias'', ''module.backbone_module.blocks.1.mlp.fc1.weight'',
          ''module.backbone_module.blocks.1.mlp.fc1.bias'', ''module.backbone_module.blocks.1.mlp.fc2.weight'',
          ''module.backbone_module.blocks.1.mlp.fc2.bias'', ''module.backbone_module.blocks.2.norm1.weight'',
          ''module.backbone_module.blocks.2.norm1.bias'', ''module.backbone_module.blocks.2.attn.qkv.weight'',
          ''module.backbone_module.blocks.2.attn.qkv.bias'', ''module.backbone_module.blocks.2.attn.proj.weight'',
          ''module.backbone_module.blocks.2.attn.proj.bias'', ''module.backbone_module.blocks.2.norm2.weight'',
          ''module.backbone_module.blocks.2.norm2.bias'', ''module.backbone_module.blocks.2.mlp.fc1.weight'',
          ''module.backbone_module.blocks.2.mlp.fc1.bias'', ''module.backbone_module.blocks.2.mlp.fc2.weight'',
          ''module.backbone_module.blocks.2.mlp.fc2.bias'', ''module.backbone_module.blocks.3.norm1.weight'',
          ''module.backbone_module.blocks.3.norm1.bias'', ''module.backbone_module.blocks.3.attn.qkv.weight'',
          ''module.backbone_module.blocks.3.attn.qkv.bias'', ''module.backbone_module.blocks.3.attn.proj.weight'',
          ''module.backbone_module.blocks.3.attn.proj.bias'', ''module.backbone_module.blocks.3.norm2.weight'',
          ''module.backbone_module.blocks.3.norm2.bias'', ''module.backbone_module.blocks.3.mlp.fc1.weight'',
          ''module.backbone_module.blocks.3.mlp.fc1.bias'', ''module.backbone_module.blocks.3.mlp.fc2.weight'',
          ''module.backbone_module.blocks.3.mlp.fc2.bias'', ''module.backbone_module.blocks.4.norm1.weight'',
          ''module.backbone_module.blocks.4.norm1.bias'', ''module.backbone_module.blocks.4.attn.qkv.weight'',
          ''module.backbone_module.blocks.4.attn.qkv.bias'', ''module.backbone_module.blocks.4.attn.proj.weight'',
          ''module.backbone_module.blocks.4.attn.proj.bias'', ''module.backbone_module.blocks.4.norm2.weight'',
          ''module.backbone_module.blocks.4.norm2.bias'', ''module.backbone_module.blocks.4.mlp.fc1.weight'',
          ''module.backbone_module.blocks.4.mlp.fc1.bias'', ''module.backbone_module.blocks.4.mlp.fc2.weight'',
          ''module.backbone_module.blocks.4.mlp.fc2.bias'', ''module.backbone_module.blocks.5.norm1.weight'',
          ''module.backbone_module.blocks.5.norm1.bias'', ''module.backbone_module.blocks.5.attn.qkv.weight'',
          ''module.backbone_module.blocks.5.attn.qkv.bias'', ''module.backbone_module.blocks.5.attn.proj.weight'',
          ''module.backbone_module.blocks.5.attn.proj.bias'', ''module.backbone_module.blocks.5.norm2.weight'',
          ''module.backbone_module.blocks.5.norm2.bias'', ''module.backbone_module.blocks.5.mlp.fc1.weight'',
          ''module.backbone_module.blocks.5.mlp.fc1.bias'', ''module.backbone_module.blocks.5.mlp.fc2.weight'',
          ''module.backbone_module.blocks.5.mlp.fc2.bias'', ''module.backbone_module.blocks.6.norm1.weight'',
          ''module.backbone_module.blocks.6.norm1.bias'', ''module.backbone_module.blocks.6.attn.qkv.weight'',
          ''module.backbone_module.blocks.6.attn.qkv.bias'', ''module.backbone_module.blocks.6.attn.proj.weight'',
          ''module.backbone_module.blocks.6.attn.proj.bias'', ''module.backbone_module.blocks.6.norm2.weight'',
          ''module.backbone_module.blocks.6.norm2.bias'', ''module.backbone_module.blocks.6.mlp.fc1.weight'',
          ''module.backbone_module.blocks.6.mlp.fc1.bias'', ''module.backbone_module.blocks.6.mlp.fc2.weight'',
          ''module.backbone_module.blocks.6.mlp.fc2.bias'', ''module.backbone_module.blocks.7.norm1.weight'',
          ''module.backbone_module.blocks.7.norm1.bias'', ''module.backbone_module.blocks.7.attn.qkv.weight'',
          ''module.backbone_module.blocks.7.attn.qkv.bias'', ''module.backbone_module.blocks.7.attn.proj.weight'',
          ''module.backbone_module.blocks.7.attn.proj.bias'', ''module.backbone_module.blocks.7.norm2.weight'',
          ''module.backbone_module.blocks.7.norm2.bias'', ''module.backbone_module.blocks.7.mlp.fc1.weight'',
          ''module.backbone_module.blocks.7.mlp.fc1.bias'', ''module.backbone_module.blocks.7.mlp.fc2.weight'',
          ''module.backbone_module.blocks.7.mlp.fc2.bias'', ''module.backbone_module.blocks.8.norm1.weight'',
          ''module.backbone_module.blocks.8.norm1.bias'', ''module.backbone_module.blocks.8.attn.qkv.weight'',
          ''module.backbone_module.blocks.8.attn.qkv.bias'', ''module.backbone_module.blocks.8.attn.proj.weight'',
          ''module.backbone_module.blocks.8.attn.proj.bias'', ''module.backbone_module.blocks.8.norm2.weight'',
          ''module.backbone_module.blocks.8.norm2.bias'', ''module.backbone_module.blocks.8.mlp.fc1.weight'',
          ''module.backbone_module.blocks.8.mlp.fc1.bias'', ''module.backbone_module.blocks.8.mlp.fc2.weight'',
          ''module.backbone_module.blocks.8.mlp.fc2.bias'', ''module.backbone_module.blocks.9.norm1.weight'',
          ''module.backbone_module.blocks.9.norm1.bias'', ''module.backbone_module.blocks.9.attn.qkv.weight'',
          ''module.backbone_module.blocks.9.attn.qkv.bias'', ''module.backbone_module.blocks.9.attn.proj.weight'',
          ''module.backbone_module.blocks.9.attn.proj.bias'', ''module.backbone_module.blocks.9.norm2.weight'',
          ''module.backbone_module.blocks.9.norm2.bias'', ''module.backbone_module.blocks.9.mlp.fc1.weight'',
          ''module.backbone_module.blocks.9.mlp.fc1.bias'', ''module.backbone_module.blocks.9.mlp.fc2.weight'',
          ''module.backbone_module.blocks.9.mlp.fc2.bias'', ''module.backbone_module.blocks.10.norm1.weight'',
          ''module.backbone_module.blocks.10.norm1.bias'', ''module.backbone_module.blocks.10.attn.qkv.weight'',
          ''module.backbone_module.blocks.10.attn.qkv.bias'', ''module.backbone_module.blocks.10.attn.proj.weight'',
          ''module.backbone_module.blocks.10.attn.proj.bias'', ''module.backbone_module.blocks.10.norm2.weight'',
          ''module.backbone_module.blocks.10.norm2.bias'', ''module.backbone_module.blocks.10.mlp.fc1.weight'',
          ''module.backbone_module.blocks.10.mlp.fc1.bias'', ''module.backbone_module.blocks.10.mlp.fc2.weight'',
          ''module.backbone_module.blocks.10.mlp.fc2.bias'', ''module.backbone_module.blocks.11.norm1.weight'',
          ''module.backbone_module.blocks.11.norm1.bias'', ''module.backbone_module.blocks.11.attn.qkv.weight'',
          ''module.backbone_module.blocks.11.attn.qkv.bias'', ''module.backbone_module.blocks.11.attn.proj.weight'',
          ''module.backbone_module.blocks.11.attn.proj.bias'', ''module.backbone_module.blocks.11.norm2.weight'',
          ''module.backbone_module.blocks.11.norm2.bias'', ''module.backbone_module.blocks.11.mlp.fc1.weight'',
          ''module.backbone_module.blocks.11.mlp.fc1.bias'', ''module.backbone_module.blocks.11.mlp.fc2.weight'',
          ''module.backbone_module.blocks.11.mlp.fc2.bias'', ''module.backbone_module.norm.weight'',
          ''module.backbone_module.norm.bias'', ''module.neck_module.reduction_layers.0.weight'',
          ''module.neck_module.reduction_layers.0.bias'', ''module.neck_module.reduction_layers.1.weight'',
          ''module.neck_module.reduction_layers.1.bias'', ''module.neck_module.reduction_layers.2.weight'',
          ''module.neck_module.reduction_layers.2.bias'', ''module.neck_module.reduction_layers.3.weight'',
          ''module.neck_module.reduction_layers.3.bias'', ''module.neck_module.reduction_layers.4.weight'',
          ''module.neck_module.reduction_layers.4.bias'', ''module.neck_module.reduction_layers.5.weight'',
          ''module.neck_module.reduction_layers.5.bias'', ''module.neck_module.reduction_layers.6.weight'',
          ''module.neck_module.reduction_layers.6.bias'', ''module.neck_module.reduction_layers.7.weight'',
          ''module.neck_module.reduction_layers.7.bias'', ''module.neck_module.reduction_layers.8.weight'',
          ''module.neck_module.reduction_layers.8.bias'', ''module.neck_module.reduction_layers.9.weight'',
          ''module.neck_module.reduction_layers.9.bias'', ''module.neck_module.reduction_layers.10.weight'',
          ''module.neck_module.reduction_layers.10.bias'', ''module.neck_module.reduction_layers.11.weight'',
          ''module.neck_module.reduction_layers.11.bias'', ''module.neck_module.reduction_layers.12.weight'',
          ''module.neck_module.reduction_layers.12.bias'', ''module.neck_module.side_gate_params.0'',
          ''module.neck_module.side_gate_params.1'', ''module.neck_module.side_gate_params.2'',
          ''module.neck_module.side_gate_params.3'', ''module.neck_module.side_gate_params.4'',
          ''module.neck_module.side_gate_params.5'', ''module.neck_module.side_gate_params.6'',
          ''module.neck_module.side_gate_params.7'', ''module.neck_module.side_gate_params.8'',
          ''module.neck_module.side_gate_params.9'', ''module.neck_module.side_gate_params.10'',
          ''module.neck_module.side_gate_params.11'', ''module.neck_module.transformer_blocks.0.0.norm1.weight'',
          ''module.neck_module.transformer_blocks.0.0.norm1.bias'', ''module.neck_module.transformer_blocks.0.0.attn.qkv.weight'',
          ''module.neck_module.transformer_blocks.0.0.attn.proj.weight'', ''module.neck_module.transformer_blocks.0.0.attn.proj.bias'',
          ''module.neck_module.transformer_blocks.0.0.norm2.weight'', ''module.neck_module.transformer_blocks.0.0.norm2.bias'',
          ''module.neck_module.transformer_blocks.0.0.mlp.fc1.weight'', ''module.neck_module.transformer_blocks.0.0.mlp.fc1.bias'',
          ''module.neck_module.transformer_blocks.0.0.mlp.fc2.weight'', ''module.neck_module.transformer_blocks.0.0.mlp.fc2.bias'',
          ''module.neck_module.transformer_blocks.1.0.norm1.weight'', ''module.neck_module.transformer_blocks.1.0.norm1.bias'',
          ''module.neck_module.transformer_blocks.1.0.attn.qkv.weight'', ''module.neck_module.transformer_blocks.1.0.attn.proj.weight'',
          ''module.neck_module.transformer_blocks.1.0.attn.proj.bias'', ''module.neck_module.transformer_blocks.1.0.norm2.weight'',
          ''module.neck_module.transformer_blocks.1.0.norm2.bias'', ''module.neck_module.transformer_blocks.1.0.mlp.fc1.weight'',
          ''module.neck_module.transformer_blocks.1.0.mlp.fc1.bias'', ''module.neck_module.transformer_blocks.1.0.mlp.fc2.weight'',
          ''module.neck_module.transformer_blocks.1.0.mlp.fc2.bias'', ''module.neck_module.transformer_blocks.2.0.norm1.weight'',
          ''module.neck_module.transformer_blocks.2.0.norm1.bias'', ''module.neck_module.transformer_blocks.2.0.attn.qkv.weight'',
          ''module.neck_module.transformer_blocks.2.0.attn.proj.weight'', ''module.neck_module.transformer_blocks.2.0.attn.proj.bias'',
          ''module.neck_module.transformer_blocks.2.0.norm2.weight'', ''module.neck_module.transformer_blocks.2.0.norm2.bias'',
          ''module.neck_module.transformer_blocks.2.0.mlp.fc1.weight'', ''module.neck_module.transformer_blocks.2.0.mlp.fc1.bias'',
          ''module.neck_module.transformer_blocks.2.0.mlp.fc2.weight'', ''module.neck_module.transformer_blocks.2.0.mlp.fc2.bias'',
          ''module.neck_module.transformer_blocks.3.0.norm1.weight'', ''module.neck_module.transformer_blocks.3.0.norm1.bias'',
          ''module.neck_module.transformer_blocks.3.0.attn.qkv.weight'', ''module.neck_module.transformer_blocks.3.0.attn.proj.weight'',
          ''module.neck_module.transformer_blocks.3.0.attn.proj.bias'', ''module.neck_module.transformer_blocks.3.0.norm2.weight'',
          ''module.neck_module.transformer_blocks.3.0.norm2.bias'', ''module.neck_module.transformer_blocks.3.0.mlp.fc1.weight'',
          ''module.neck_module.transformer_blocks.3.0.mlp.fc1.bias'', ''module.neck_module.transformer_blocks.3.0.mlp.fc2.weight'',
          ''module.neck_module.transformer_blocks.3.0.mlp.fc2.bias'', ''module.neck_module.transformer_blocks.4.0.norm1.weight'',
          ''module.neck_module.transformer_blocks.4.0.norm1.bias'', ''module.neck_module.transformer_blocks.4.0.attn.qkv.weight'',
          ''module.neck_module.transformer_blocks.4.0.attn.proj.weight'', ''module.neck_module.transformer_blocks.4.0.attn.proj.bias'',
          ''module.neck_module.transformer_blocks.4.0.norm2.weight'', ''module.neck_module.transformer_blocks.4.0.norm2.bias'',
          ''module.neck_module.transformer_blocks.4.0.mlp.fc1.weight'', ''module.neck_module.transformer_blocks.4.0.mlp.fc1.bias'',
          ''module.neck_module.transformer_blocks.4.0.mlp.fc2.weight'', ''module.neck_module.transformer_blocks.4.0.mlp.fc2.bias'',
          ''module.neck_module.transformer_blocks.5.0.norm1.weight'', ''module.neck_module.transformer_blocks.5.0.norm1.bias'',
          ''module.neck_module.transformer_blocks.5.0.attn.qkv.weight'', ''module.neck_module.transformer_blocks.5.0.attn.proj.weight'',
          ''module.neck_module.transformer_blocks.5.0.attn.proj.bias'', ''module.neck_module.transformer_blocks.5.0.norm2.weight'',
          ''module.neck_module.transformer_blocks.5.0.norm2.bias'', ''module.neck_module.transformer_blocks.5.0.mlp.fc1.weight'',
          ''module.neck_module.transformer_blocks.5.0.mlp.fc1.bias'', ''module.neck_module.transformer_blocks.5.0.mlp.fc2.weight'',
          ''module.neck_module.transformer_blocks.5.0.mlp.fc2.bias'', ''module.neck_module.transformer_blocks.6.0.norm1.weight'',
          ''module.neck_module.transformer_blocks.6.0.norm1.bias'', ''module.neck_module.transformer_blocks.6.0.attn.qkv.weight'',
          ''module.neck_module.transformer_blocks.6.0.attn.proj.weight'', ''module.neck_module.transformer_blocks.6.0.attn.proj.bias'',
          ''module.neck_module.transformer_blocks.6.0.norm2.weight'', ''module.neck_module.transformer_blocks.6.0.norm2.bias'',
          ''module.neck_module.transformer_blocks.6.0.mlp.fc1.weight'', ''module.neck_module.transformer_blocks.6.0.mlp.fc1.bias'',
          ''module.neck_module.transformer_blocks.6.0.mlp.fc2.weight'', ''module.neck_module.transformer_blocks.6.0.mlp.fc2.bias'',
          ''module.neck_module.transformer_blocks.7.0.norm1.weight'', ''module.neck_module.transformer_blocks.7.0.norm1.bias'',
          ''module.neck_module.transformer_blocks.7.0.attn.qkv.weight'', ''module.neck_module.transformer_blocks.7.0.attn.proj.weight'',
          ''module.neck_module.transformer_blocks.7.0.attn.proj.bias'', ''module.neck_module.transformer_blocks.7.0.norm2.weight'',
          ''module.neck_module.transformer_blocks.7.0.norm2.bias'', ''module.neck_module.transformer_blocks.7.0.mlp.fc1.weight'',
          ''module.neck_module.transformer_blocks.7.0.mlp.fc1.bias'', ''module.neck_module.transformer_blocks.7.0.mlp.fc2.weight'',
          ''module.neck_module.transformer_blocks.7.0.mlp.fc2.bias'', ''module.neck_module.transformer_blocks.8.0.norm1.weight'',
          ''module.neck_module.transformer_blocks.8.0.norm1.bias'', ''module.neck_module.transformer_blocks.8.0.attn.qkv.weight'',
          ''module.neck_module.transformer_blocks.8.0.attn.proj.weight'', ''module.neck_module.transformer_blocks.8.0.attn.proj.bias'',
          ''module.neck_module.transformer_blocks.8.0.norm2.weight'', ''module.neck_module.transformer_blocks.8.0.norm2.bias'',
          ''module.neck_module.transformer_blocks.8.0.mlp.fc1.weight'', ''module.neck_module.transformer_blocks.8.0.mlp.fc1.bias'',
          ''module.neck_module.transformer_blocks.8.0.mlp.fc2.weight'', ''module.neck_module.transformer_blocks.8.0.mlp.fc2.bias'',
          ''module.neck_module.transformer_blocks.9.0.norm1.weight'', ''module.neck_module.transformer_blocks.9.0.norm1.bias'',
          ''module.neck_module.transformer_blocks.9.0.attn.qkv.weight'', ''module.neck_module.transformer_blocks.9.0.attn.proj.weight'',
          ''module.neck_module.transformer_blocks.9.0.attn.proj.bias'', ''module.neck_module.transformer_blocks.9.0.norm2.weight'',
          ''module.neck_module.transformer_blocks.9.0.norm2.bias'', ''module.neck_module.transformer_blocks.9.0.mlp.fc1.weight'',
          ''module.neck_module.transformer_blocks.9.0.mlp.fc1.bias'', ''module.neck_module.transformer_blocks.9.0.mlp.fc2.weight'',
          ''module.neck_module.transformer_blocks.9.0.mlp.fc2.bias'', ''module.neck_module.transformer_blocks.10.0.norm1.weight'',
          ''module.neck_module.transformer_blocks.10.0.norm1.bias'', ''module.neck_module.transformer_blocks.10.0.attn.qkv.weight'',
          ''module.neck_module.transformer_blocks.10.0.attn.proj.weight'', ''module.neck_module.transformer_blocks.10.0.attn.proj.bias'',
          ''module.neck_module.transformer_blocks.10.0.norm2.weight'', ''module.neck_module.transformer_blocks.10.0.norm2.bias'',
          ''module.neck_module.transformer_blocks.10.0.mlp.fc1.weight'', ''module.neck_module.transformer_blocks.10.0.mlp.fc1.bias'',
          ''module.neck_module.transformer_blocks.10.0.mlp.fc2.weight'', ''module.neck_module.transformer_blocks.10.0.mlp.fc2.bias'',
          ''module.neck_module.transformer_blocks.11.0.norm1.weight'', ''module.neck_module.transformer_blocks.11.0.norm1.bias'',
          ''module.neck_module.transformer_blocks.11.0.attn.qkv.weight'', ''module.neck_module.transformer_blocks.11.0.attn.proj.weight'',
          ''module.neck_module.transformer_blocks.11.0.attn.proj.bias'', ''module.neck_module.transformer_blocks.11.0.norm2.weight'',
          ''module.neck_module.transformer_blocks.11.0.norm2.bias'', ''module.neck_module.transformer_blocks.11.0.mlp.fc1.weight'',
          ''module.neck_module.transformer_blocks.11.0.mlp.fc1.bias'', ''module.neck_module.transformer_blocks.11.0.mlp.fc2.weight'',
          ''module.neck_module.transformer_blocks.11.0.mlp.fc2.bias'', ''module.neck_module.last_proj.weight'',
          ''module.neck_module.last_proj.bias'', ''module.decoder_module.logits.0.weight'',
          ''module.decoder_module.logits.0.bias'', ''module.decoder_module.logits.1.weight'',
          ''module.decoder_module.logits.1.bias'', ''module.decoder_module.logits.1.running_mean'',
          ''module.decoder_module.logits.1.running_var'', ''module.decoder_module.logits.1.num_batches_tracked''])

          </code></pre>

          <p>So the pretrained weights keys can not adapt to the model, can you give
          me some suggestions about how to load the pretrained weights for the PATH
          model? </p>

          <p>Thanks a lot!!</p>

          '
        raw: "Hi, thanks for your great job. I am trying to reproduce your results\
          \ based on this url pretrained weights. But when I load the pretrained weights\
          \ model, I find the keys are not same. The pretrained weights keys are as\
          \ follows:\r\n```\r\nodict_keys(['cls_token', 'cls_token_pos_embed', 'pos_embed',\
          \ 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight',\
          \ 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias',\
          \ 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight',\
          \ 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias',\
          \ 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight',\
          \ 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias',\
          \ 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight',\
          \ 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias',\
          \ 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight',\
          \ 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias',\
          \ 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight',\
          \ 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias',\
          \ 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight',\
          \ 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias',\
          \ 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight',\
          \ 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias',\
          \ 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight',\
          \ 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias',\
          \ 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight',\
          \ 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias',\
          \ 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight',\
          \ 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias',\
          \ 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight',\
          \ 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias',\
          \ 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight',\
          \ 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias',\
          \ 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight',\
          \ 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias',\
          \ 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight',\
          \ 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias',\
          \ 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight',\
          \ 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias',\
          \ 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight',\
          \ 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias',\
          \ 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight',\
          \ 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias',\
          \ 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight',\
          \ 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias',\
          \ 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight',\
          \ 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias',\
          \ 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight',\
          \ 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias',\
          \ 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight',\
          \ 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias',\
          \ 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight',\
          \ 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias',\
          \ 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight',\
          \ 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias',\
          \ 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.12.norm1.weight',\
          \ 'blocks.12.norm1.bias', 'blocks.12.attn.qkv.weight', 'blocks.12.attn.qkv.bias',\
          \ 'blocks.12.attn.proj.weight', 'blocks.12.attn.proj.bias', 'blocks.12.norm2.weight',\
          \ 'blocks.12.norm2.bias', 'blocks.12.mlp.fc1.weight', 'blocks.12.mlp.fc1.bias',\
          \ 'blocks.12.mlp.fc2.weight', 'blocks.12.mlp.fc2.bias', 'blocks.13.norm1.weight',\
          \ 'blocks.13.norm1.bias', 'blocks.13.attn.qkv.weight', 'blocks.13.attn.qkv.bias',\
          \ 'blocks.13.attn.proj.weight', 'blocks.13.attn.proj.bias', 'blocks.13.norm2.weight',\
          \ 'blocks.13.norm2.bias', 'blocks.13.mlp.fc1.weight', 'blocks.13.mlp.fc1.bias',\
          \ 'blocks.13.mlp.fc2.weight', 'blocks.13.mlp.fc2.bias', 'blocks.14.norm1.weight',\
          \ 'blocks.14.norm1.bias', 'blocks.14.attn.qkv.weight', 'blocks.14.attn.qkv.bias',\
          \ 'blocks.14.attn.proj.weight', 'blocks.14.attn.proj.bias', 'blocks.14.norm2.weight',\
          \ 'blocks.14.norm2.bias', 'blocks.14.mlp.fc1.weight', 'blocks.14.mlp.fc1.bias',\
          \ 'blocks.14.mlp.fc2.weight', 'blocks.14.mlp.fc2.bias', 'blocks.15.norm1.weight',\
          \ 'blocks.15.norm1.bias', 'blocks.15.attn.qkv.weight', 'blocks.15.attn.qkv.bias',\
          \ 'blocks.15.attn.proj.weight', 'blocks.15.attn.proj.bias', 'blocks.15.norm2.weight',\
          \ 'blocks.15.norm2.bias', 'blocks.15.mlp.fc1.weight', 'blocks.15.mlp.fc1.bias',\
          \ 'blocks.15.mlp.fc2.weight', 'blocks.15.mlp.fc2.bias', 'blocks.16.norm1.weight',\
          \ 'blocks.16.norm1.bias', 'blocks.16.attn.qkv.weight', 'blocks.16.attn.qkv.bias',\
          \ 'blocks.16.attn.proj.weight', 'blocks.16.attn.proj.bias', 'blocks.16.norm2.weight',\
          \ 'blocks.16.norm2.bias', 'blocks.16.mlp.fc1.weight', 'blocks.16.mlp.fc1.bias',\
          \ 'blocks.16.mlp.fc2.weight', 'blocks.16.mlp.fc2.bias', 'blocks.17.norm1.weight',\
          \ 'blocks.17.norm1.bias', 'blocks.17.attn.qkv.weight', 'blocks.17.attn.qkv.bias',\
          \ 'blocks.17.attn.proj.weight', 'blocks.17.attn.proj.bias', 'blocks.17.norm2.weight',\
          \ 'blocks.17.norm2.bias', 'blocks.17.mlp.fc1.weight', 'blocks.17.mlp.fc1.bias',\
          \ 'blocks.17.mlp.fc2.weight', 'blocks.17.mlp.fc2.bias', 'blocks.18.norm1.weight',\
          \ 'blocks.18.norm1.bias', 'blocks.18.attn.qkv.weight', 'blocks.18.attn.qkv.bias',\
          \ 'blocks.18.attn.proj.weight', 'blocks.18.attn.proj.bias', 'blocks.18.norm2.weight',\
          \ 'blocks.18.norm2.bias', 'blocks.18.mlp.fc1.weight', 'blocks.18.mlp.fc1.bias',\
          \ 'blocks.18.mlp.fc2.weight', 'blocks.18.mlp.fc2.bias', 'blocks.19.norm1.weight',\
          \ 'blocks.19.norm1.bias', 'blocks.19.attn.qkv.weight', 'blocks.19.attn.qkv.bias',\
          \ 'blocks.19.attn.proj.weight', 'blocks.19.attn.proj.bias', 'blocks.19.norm2.weight',\
          \ 'blocks.19.norm2.bias', 'blocks.19.mlp.fc1.weight', 'blocks.19.mlp.fc1.bias',\
          \ 'blocks.19.mlp.fc2.weight', 'blocks.19.mlp.fc2.bias', 'blocks.20.norm1.weight',\
          \ 'blocks.20.norm1.bias', 'blocks.20.attn.qkv.weight', 'blocks.20.attn.qkv.bias',\
          \ 'blocks.20.attn.proj.weight', 'blocks.20.attn.proj.bias', 'blocks.20.norm2.weight',\
          \ 'blocks.20.norm2.bias', 'blocks.20.mlp.fc1.weight', 'blocks.20.mlp.fc1.bias',\
          \ 'blocks.20.mlp.fc2.weight', 'blocks.20.mlp.fc2.bias', 'blocks.21.norm1.weight',\
          \ 'blocks.21.norm1.bias', 'blocks.21.attn.qkv.weight', 'blocks.21.attn.qkv.bias',\
          \ 'blocks.21.attn.proj.weight', 'blocks.21.attn.proj.bias', 'blocks.21.norm2.weight',\
          \ 'blocks.21.norm2.bias', 'blocks.21.mlp.fc1.weight', 'blocks.21.mlp.fc1.bias',\
          \ 'blocks.21.mlp.fc2.weight', 'blocks.21.mlp.fc2.bias', 'blocks.22.norm1.weight',\
          \ 'blocks.22.norm1.bias', 'blocks.22.attn.qkv.weight', 'blocks.22.attn.qkv.bias',\
          \ 'blocks.22.attn.proj.weight', 'blocks.22.attn.proj.bias', 'blocks.22.norm2.weight',\
          \ 'blocks.22.norm2.bias', 'blocks.22.mlp.fc1.weight', 'blocks.22.mlp.fc1.bias',\
          \ 'blocks.22.mlp.fc2.weight', 'blocks.22.mlp.fc2.bias', 'blocks.23.norm1.weight',\
          \ 'blocks.23.norm1.bias', 'blocks.23.attn.qkv.weight', 'blocks.23.attn.qkv.bias',\
          \ 'blocks.23.attn.proj.weight', 'blocks.23.attn.proj.bias', 'blocks.23.norm2.weight',\
          \ 'blocks.23.norm2.bias', 'blocks.23.mlp.fc1.weight', 'blocks.23.mlp.fc1.bias',\
          \ 'blocks.23.mlp.fc2.weight', 'blocks.23.mlp.fc2.bias', 'norm.weight', 'norm.bias'])\r\
          \n```\r\nBut the keys for  the model in PATH  are as follows:\r\n```\r\n\
          odict_keys(['module.backbone_module.pos_embed', 'module.backbone_module.patch_embed.proj.weight',\
          \ 'module.backbone_module.patch_embed.proj.bias', 'module.backbone_module.blocks.0.norm1.weight',\
          \ 'module.backbone_module.blocks.0.norm1.bias', 'module.backbone_module.blocks.0.attn.qkv.weight',\
          \ 'module.backbone_module.blocks.0.attn.qkv.bias', 'module.backbone_module.blocks.0.attn.proj.weight',\
          \ 'module.backbone_module.blocks.0.attn.proj.bias', 'module.backbone_module.blocks.0.norm2.weight',\
          \ 'module.backbone_module.blocks.0.norm2.bias', 'module.backbone_module.blocks.0.mlp.fc1.weight',\
          \ 'module.backbone_module.blocks.0.mlp.fc1.bias', 'module.backbone_module.blocks.0.mlp.fc2.weight',\
          \ 'module.backbone_module.blocks.0.mlp.fc2.bias', 'module.backbone_module.blocks.1.norm1.weight',\
          \ 'module.backbone_module.blocks.1.norm1.bias', 'module.backbone_module.blocks.1.attn.qkv.weight',\
          \ 'module.backbone_module.blocks.1.attn.qkv.bias', 'module.backbone_module.blocks.1.attn.proj.weight',\
          \ 'module.backbone_module.blocks.1.attn.proj.bias', 'module.backbone_module.blocks.1.norm2.weight',\
          \ 'module.backbone_module.blocks.1.norm2.bias', 'module.backbone_module.blocks.1.mlp.fc1.weight',\
          \ 'module.backbone_module.blocks.1.mlp.fc1.bias', 'module.backbone_module.blocks.1.mlp.fc2.weight',\
          \ 'module.backbone_module.blocks.1.mlp.fc2.bias', 'module.backbone_module.blocks.2.norm1.weight',\
          \ 'module.backbone_module.blocks.2.norm1.bias', 'module.backbone_module.blocks.2.attn.qkv.weight',\
          \ 'module.backbone_module.blocks.2.attn.qkv.bias', 'module.backbone_module.blocks.2.attn.proj.weight',\
          \ 'module.backbone_module.blocks.2.attn.proj.bias', 'module.backbone_module.blocks.2.norm2.weight',\
          \ 'module.backbone_module.blocks.2.norm2.bias', 'module.backbone_module.blocks.2.mlp.fc1.weight',\
          \ 'module.backbone_module.blocks.2.mlp.fc1.bias', 'module.backbone_module.blocks.2.mlp.fc2.weight',\
          \ 'module.backbone_module.blocks.2.mlp.fc2.bias', 'module.backbone_module.blocks.3.norm1.weight',\
          \ 'module.backbone_module.blocks.3.norm1.bias', 'module.backbone_module.blocks.3.attn.qkv.weight',\
          \ 'module.backbone_module.blocks.3.attn.qkv.bias', 'module.backbone_module.blocks.3.attn.proj.weight',\
          \ 'module.backbone_module.blocks.3.attn.proj.bias', 'module.backbone_module.blocks.3.norm2.weight',\
          \ 'module.backbone_module.blocks.3.norm2.bias', 'module.backbone_module.blocks.3.mlp.fc1.weight',\
          \ 'module.backbone_module.blocks.3.mlp.fc1.bias', 'module.backbone_module.blocks.3.mlp.fc2.weight',\
          \ 'module.backbone_module.blocks.3.mlp.fc2.bias', 'module.backbone_module.blocks.4.norm1.weight',\
          \ 'module.backbone_module.blocks.4.norm1.bias', 'module.backbone_module.blocks.4.attn.qkv.weight',\
          \ 'module.backbone_module.blocks.4.attn.qkv.bias', 'module.backbone_module.blocks.4.attn.proj.weight',\
          \ 'module.backbone_module.blocks.4.attn.proj.bias', 'module.backbone_module.blocks.4.norm2.weight',\
          \ 'module.backbone_module.blocks.4.norm2.bias', 'module.backbone_module.blocks.4.mlp.fc1.weight',\
          \ 'module.backbone_module.blocks.4.mlp.fc1.bias', 'module.backbone_module.blocks.4.mlp.fc2.weight',\
          \ 'module.backbone_module.blocks.4.mlp.fc2.bias', 'module.backbone_module.blocks.5.norm1.weight',\
          \ 'module.backbone_module.blocks.5.norm1.bias', 'module.backbone_module.blocks.5.attn.qkv.weight',\
          \ 'module.backbone_module.blocks.5.attn.qkv.bias', 'module.backbone_module.blocks.5.attn.proj.weight',\
          \ 'module.backbone_module.blocks.5.attn.proj.bias', 'module.backbone_module.blocks.5.norm2.weight',\
          \ 'module.backbone_module.blocks.5.norm2.bias', 'module.backbone_module.blocks.5.mlp.fc1.weight',\
          \ 'module.backbone_module.blocks.5.mlp.fc1.bias', 'module.backbone_module.blocks.5.mlp.fc2.weight',\
          \ 'module.backbone_module.blocks.5.mlp.fc2.bias', 'module.backbone_module.blocks.6.norm1.weight',\
          \ 'module.backbone_module.blocks.6.norm1.bias', 'module.backbone_module.blocks.6.attn.qkv.weight',\
          \ 'module.backbone_module.blocks.6.attn.qkv.bias', 'module.backbone_module.blocks.6.attn.proj.weight',\
          \ 'module.backbone_module.blocks.6.attn.proj.bias', 'module.backbone_module.blocks.6.norm2.weight',\
          \ 'module.backbone_module.blocks.6.norm2.bias', 'module.backbone_module.blocks.6.mlp.fc1.weight',\
          \ 'module.backbone_module.blocks.6.mlp.fc1.bias', 'module.backbone_module.blocks.6.mlp.fc2.weight',\
          \ 'module.backbone_module.blocks.6.mlp.fc2.bias', 'module.backbone_module.blocks.7.norm1.weight',\
          \ 'module.backbone_module.blocks.7.norm1.bias', 'module.backbone_module.blocks.7.attn.qkv.weight',\
          \ 'module.backbone_module.blocks.7.attn.qkv.bias', 'module.backbone_module.blocks.7.attn.proj.weight',\
          \ 'module.backbone_module.blocks.7.attn.proj.bias', 'module.backbone_module.blocks.7.norm2.weight',\
          \ 'module.backbone_module.blocks.7.norm2.bias', 'module.backbone_module.blocks.7.mlp.fc1.weight',\
          \ 'module.backbone_module.blocks.7.mlp.fc1.bias', 'module.backbone_module.blocks.7.mlp.fc2.weight',\
          \ 'module.backbone_module.blocks.7.mlp.fc2.bias', 'module.backbone_module.blocks.8.norm1.weight',\
          \ 'module.backbone_module.blocks.8.norm1.bias', 'module.backbone_module.blocks.8.attn.qkv.weight',\
          \ 'module.backbone_module.blocks.8.attn.qkv.bias', 'module.backbone_module.blocks.8.attn.proj.weight',\
          \ 'module.backbone_module.blocks.8.attn.proj.bias', 'module.backbone_module.blocks.8.norm2.weight',\
          \ 'module.backbone_module.blocks.8.norm2.bias', 'module.backbone_module.blocks.8.mlp.fc1.weight',\
          \ 'module.backbone_module.blocks.8.mlp.fc1.bias', 'module.backbone_module.blocks.8.mlp.fc2.weight',\
          \ 'module.backbone_module.blocks.8.mlp.fc2.bias', 'module.backbone_module.blocks.9.norm1.weight',\
          \ 'module.backbone_module.blocks.9.norm1.bias', 'module.backbone_module.blocks.9.attn.qkv.weight',\
          \ 'module.backbone_module.blocks.9.attn.qkv.bias', 'module.backbone_module.blocks.9.attn.proj.weight',\
          \ 'module.backbone_module.blocks.9.attn.proj.bias', 'module.backbone_module.blocks.9.norm2.weight',\
          \ 'module.backbone_module.blocks.9.norm2.bias', 'module.backbone_module.blocks.9.mlp.fc1.weight',\
          \ 'module.backbone_module.blocks.9.mlp.fc1.bias', 'module.backbone_module.blocks.9.mlp.fc2.weight',\
          \ 'module.backbone_module.blocks.9.mlp.fc2.bias', 'module.backbone_module.blocks.10.norm1.weight',\
          \ 'module.backbone_module.blocks.10.norm1.bias', 'module.backbone_module.blocks.10.attn.qkv.weight',\
          \ 'module.backbone_module.blocks.10.attn.qkv.bias', 'module.backbone_module.blocks.10.attn.proj.weight',\
          \ 'module.backbone_module.blocks.10.attn.proj.bias', 'module.backbone_module.blocks.10.norm2.weight',\
          \ 'module.backbone_module.blocks.10.norm2.bias', 'module.backbone_module.blocks.10.mlp.fc1.weight',\
          \ 'module.backbone_module.blocks.10.mlp.fc1.bias', 'module.backbone_module.blocks.10.mlp.fc2.weight',\
          \ 'module.backbone_module.blocks.10.mlp.fc2.bias', 'module.backbone_module.blocks.11.norm1.weight',\
          \ 'module.backbone_module.blocks.11.norm1.bias', 'module.backbone_module.blocks.11.attn.qkv.weight',\
          \ 'module.backbone_module.blocks.11.attn.qkv.bias', 'module.backbone_module.blocks.11.attn.proj.weight',\
          \ 'module.backbone_module.blocks.11.attn.proj.bias', 'module.backbone_module.blocks.11.norm2.weight',\
          \ 'module.backbone_module.blocks.11.norm2.bias', 'module.backbone_module.blocks.11.mlp.fc1.weight',\
          \ 'module.backbone_module.blocks.11.mlp.fc1.bias', 'module.backbone_module.blocks.11.mlp.fc2.weight',\
          \ 'module.backbone_module.blocks.11.mlp.fc2.bias', 'module.backbone_module.norm.weight',\
          \ 'module.backbone_module.norm.bias', 'module.neck_module.reduction_layers.0.weight',\
          \ 'module.neck_module.reduction_layers.0.bias', 'module.neck_module.reduction_layers.1.weight',\
          \ 'module.neck_module.reduction_layers.1.bias', 'module.neck_module.reduction_layers.2.weight',\
          \ 'module.neck_module.reduction_layers.2.bias', 'module.neck_module.reduction_layers.3.weight',\
          \ 'module.neck_module.reduction_layers.3.bias', 'module.neck_module.reduction_layers.4.weight',\
          \ 'module.neck_module.reduction_layers.4.bias', 'module.neck_module.reduction_layers.5.weight',\
          \ 'module.neck_module.reduction_layers.5.bias', 'module.neck_module.reduction_layers.6.weight',\
          \ 'module.neck_module.reduction_layers.6.bias', 'module.neck_module.reduction_layers.7.weight',\
          \ 'module.neck_module.reduction_layers.7.bias', 'module.neck_module.reduction_layers.8.weight',\
          \ 'module.neck_module.reduction_layers.8.bias', 'module.neck_module.reduction_layers.9.weight',\
          \ 'module.neck_module.reduction_layers.9.bias', 'module.neck_module.reduction_layers.10.weight',\
          \ 'module.neck_module.reduction_layers.10.bias', 'module.neck_module.reduction_layers.11.weight',\
          \ 'module.neck_module.reduction_layers.11.bias', 'module.neck_module.reduction_layers.12.weight',\
          \ 'module.neck_module.reduction_layers.12.bias', 'module.neck_module.side_gate_params.0',\
          \ 'module.neck_module.side_gate_params.1', 'module.neck_module.side_gate_params.2',\
          \ 'module.neck_module.side_gate_params.3', 'module.neck_module.side_gate_params.4',\
          \ 'module.neck_module.side_gate_params.5', 'module.neck_module.side_gate_params.6',\
          \ 'module.neck_module.side_gate_params.7', 'module.neck_module.side_gate_params.8',\
          \ 'module.neck_module.side_gate_params.9', 'module.neck_module.side_gate_params.10',\
          \ 'module.neck_module.side_gate_params.11', 'module.neck_module.transformer_blocks.0.0.norm1.weight',\
          \ 'module.neck_module.transformer_blocks.0.0.norm1.bias', 'module.neck_module.transformer_blocks.0.0.attn.qkv.weight',\
          \ 'module.neck_module.transformer_blocks.0.0.attn.proj.weight', 'module.neck_module.transformer_blocks.0.0.attn.proj.bias',\
          \ 'module.neck_module.transformer_blocks.0.0.norm2.weight', 'module.neck_module.transformer_blocks.0.0.norm2.bias',\
          \ 'module.neck_module.transformer_blocks.0.0.mlp.fc1.weight', 'module.neck_module.transformer_blocks.0.0.mlp.fc1.bias',\
          \ 'module.neck_module.transformer_blocks.0.0.mlp.fc2.weight', 'module.neck_module.transformer_blocks.0.0.mlp.fc2.bias',\
          \ 'module.neck_module.transformer_blocks.1.0.norm1.weight', 'module.neck_module.transformer_blocks.1.0.norm1.bias',\
          \ 'module.neck_module.transformer_blocks.1.0.attn.qkv.weight', 'module.neck_module.transformer_blocks.1.0.attn.proj.weight',\
          \ 'module.neck_module.transformer_blocks.1.0.attn.proj.bias', 'module.neck_module.transformer_blocks.1.0.norm2.weight',\
          \ 'module.neck_module.transformer_blocks.1.0.norm2.bias', 'module.neck_module.transformer_blocks.1.0.mlp.fc1.weight',\
          \ 'module.neck_module.transformer_blocks.1.0.mlp.fc1.bias', 'module.neck_module.transformer_blocks.1.0.mlp.fc2.weight',\
          \ 'module.neck_module.transformer_blocks.1.0.mlp.fc2.bias', 'module.neck_module.transformer_blocks.2.0.norm1.weight',\
          \ 'module.neck_module.transformer_blocks.2.0.norm1.bias', 'module.neck_module.transformer_blocks.2.0.attn.qkv.weight',\
          \ 'module.neck_module.transformer_blocks.2.0.attn.proj.weight', 'module.neck_module.transformer_blocks.2.0.attn.proj.bias',\
          \ 'module.neck_module.transformer_blocks.2.0.norm2.weight', 'module.neck_module.transformer_blocks.2.0.norm2.bias',\
          \ 'module.neck_module.transformer_blocks.2.0.mlp.fc1.weight', 'module.neck_module.transformer_blocks.2.0.mlp.fc1.bias',\
          \ 'module.neck_module.transformer_blocks.2.0.mlp.fc2.weight', 'module.neck_module.transformer_blocks.2.0.mlp.fc2.bias',\
          \ 'module.neck_module.transformer_blocks.3.0.norm1.weight', 'module.neck_module.transformer_blocks.3.0.norm1.bias',\
          \ 'module.neck_module.transformer_blocks.3.0.attn.qkv.weight', 'module.neck_module.transformer_blocks.3.0.attn.proj.weight',\
          \ 'module.neck_module.transformer_blocks.3.0.attn.proj.bias', 'module.neck_module.transformer_blocks.3.0.norm2.weight',\
          \ 'module.neck_module.transformer_blocks.3.0.norm2.bias', 'module.neck_module.transformer_blocks.3.0.mlp.fc1.weight',\
          \ 'module.neck_module.transformer_blocks.3.0.mlp.fc1.bias', 'module.neck_module.transformer_blocks.3.0.mlp.fc2.weight',\
          \ 'module.neck_module.transformer_blocks.3.0.mlp.fc2.bias', 'module.neck_module.transformer_blocks.4.0.norm1.weight',\
          \ 'module.neck_module.transformer_blocks.4.0.norm1.bias', 'module.neck_module.transformer_blocks.4.0.attn.qkv.weight',\
          \ 'module.neck_module.transformer_blocks.4.0.attn.proj.weight', 'module.neck_module.transformer_blocks.4.0.attn.proj.bias',\
          \ 'module.neck_module.transformer_blocks.4.0.norm2.weight', 'module.neck_module.transformer_blocks.4.0.norm2.bias',\
          \ 'module.neck_module.transformer_blocks.4.0.mlp.fc1.weight', 'module.neck_module.transformer_blocks.4.0.mlp.fc1.bias',\
          \ 'module.neck_module.transformer_blocks.4.0.mlp.fc2.weight', 'module.neck_module.transformer_blocks.4.0.mlp.fc2.bias',\
          \ 'module.neck_module.transformer_blocks.5.0.norm1.weight', 'module.neck_module.transformer_blocks.5.0.norm1.bias',\
          \ 'module.neck_module.transformer_blocks.5.0.attn.qkv.weight', 'module.neck_module.transformer_blocks.5.0.attn.proj.weight',\
          \ 'module.neck_module.transformer_blocks.5.0.attn.proj.bias', 'module.neck_module.transformer_blocks.5.0.norm2.weight',\
          \ 'module.neck_module.transformer_blocks.5.0.norm2.bias', 'module.neck_module.transformer_blocks.5.0.mlp.fc1.weight',\
          \ 'module.neck_module.transformer_blocks.5.0.mlp.fc1.bias', 'module.neck_module.transformer_blocks.5.0.mlp.fc2.weight',\
          \ 'module.neck_module.transformer_blocks.5.0.mlp.fc2.bias', 'module.neck_module.transformer_blocks.6.0.norm1.weight',\
          \ 'module.neck_module.transformer_blocks.6.0.norm1.bias', 'module.neck_module.transformer_blocks.6.0.attn.qkv.weight',\
          \ 'module.neck_module.transformer_blocks.6.0.attn.proj.weight', 'module.neck_module.transformer_blocks.6.0.attn.proj.bias',\
          \ 'module.neck_module.transformer_blocks.6.0.norm2.weight', 'module.neck_module.transformer_blocks.6.0.norm2.bias',\
          \ 'module.neck_module.transformer_blocks.6.0.mlp.fc1.weight', 'module.neck_module.transformer_blocks.6.0.mlp.fc1.bias',\
          \ 'module.neck_module.transformer_blocks.6.0.mlp.fc2.weight', 'module.neck_module.transformer_blocks.6.0.mlp.fc2.bias',\
          \ 'module.neck_module.transformer_blocks.7.0.norm1.weight', 'module.neck_module.transformer_blocks.7.0.norm1.bias',\
          \ 'module.neck_module.transformer_blocks.7.0.attn.qkv.weight', 'module.neck_module.transformer_blocks.7.0.attn.proj.weight',\
          \ 'module.neck_module.transformer_blocks.7.0.attn.proj.bias', 'module.neck_module.transformer_blocks.7.0.norm2.weight',\
          \ 'module.neck_module.transformer_blocks.7.0.norm2.bias', 'module.neck_module.transformer_blocks.7.0.mlp.fc1.weight',\
          \ 'module.neck_module.transformer_blocks.7.0.mlp.fc1.bias', 'module.neck_module.transformer_blocks.7.0.mlp.fc2.weight',\
          \ 'module.neck_module.transformer_blocks.7.0.mlp.fc2.bias', 'module.neck_module.transformer_blocks.8.0.norm1.weight',\
          \ 'module.neck_module.transformer_blocks.8.0.norm1.bias', 'module.neck_module.transformer_blocks.8.0.attn.qkv.weight',\
          \ 'module.neck_module.transformer_blocks.8.0.attn.proj.weight', 'module.neck_module.transformer_blocks.8.0.attn.proj.bias',\
          \ 'module.neck_module.transformer_blocks.8.0.norm2.weight', 'module.neck_module.transformer_blocks.8.0.norm2.bias',\
          \ 'module.neck_module.transformer_blocks.8.0.mlp.fc1.weight', 'module.neck_module.transformer_blocks.8.0.mlp.fc1.bias',\
          \ 'module.neck_module.transformer_blocks.8.0.mlp.fc2.weight', 'module.neck_module.transformer_blocks.8.0.mlp.fc2.bias',\
          \ 'module.neck_module.transformer_blocks.9.0.norm1.weight', 'module.neck_module.transformer_blocks.9.0.norm1.bias',\
          \ 'module.neck_module.transformer_blocks.9.0.attn.qkv.weight', 'module.neck_module.transformer_blocks.9.0.attn.proj.weight',\
          \ 'module.neck_module.transformer_blocks.9.0.attn.proj.bias', 'module.neck_module.transformer_blocks.9.0.norm2.weight',\
          \ 'module.neck_module.transformer_blocks.9.0.norm2.bias', 'module.neck_module.transformer_blocks.9.0.mlp.fc1.weight',\
          \ 'module.neck_module.transformer_blocks.9.0.mlp.fc1.bias', 'module.neck_module.transformer_blocks.9.0.mlp.fc2.weight',\
          \ 'module.neck_module.transformer_blocks.9.0.mlp.fc2.bias', 'module.neck_module.transformer_blocks.10.0.norm1.weight',\
          \ 'module.neck_module.transformer_blocks.10.0.norm1.bias', 'module.neck_module.transformer_blocks.10.0.attn.qkv.weight',\
          \ 'module.neck_module.transformer_blocks.10.0.attn.proj.weight', 'module.neck_module.transformer_blocks.10.0.attn.proj.bias',\
          \ 'module.neck_module.transformer_blocks.10.0.norm2.weight', 'module.neck_module.transformer_blocks.10.0.norm2.bias',\
          \ 'module.neck_module.transformer_blocks.10.0.mlp.fc1.weight', 'module.neck_module.transformer_blocks.10.0.mlp.fc1.bias',\
          \ 'module.neck_module.transformer_blocks.10.0.mlp.fc2.weight', 'module.neck_module.transformer_blocks.10.0.mlp.fc2.bias',\
          \ 'module.neck_module.transformer_blocks.11.0.norm1.weight', 'module.neck_module.transformer_blocks.11.0.norm1.bias',\
          \ 'module.neck_module.transformer_blocks.11.0.attn.qkv.weight', 'module.neck_module.transformer_blocks.11.0.attn.proj.weight',\
          \ 'module.neck_module.transformer_blocks.11.0.attn.proj.bias', 'module.neck_module.transformer_blocks.11.0.norm2.weight',\
          \ 'module.neck_module.transformer_blocks.11.0.norm2.bias', 'module.neck_module.transformer_blocks.11.0.mlp.fc1.weight',\
          \ 'module.neck_module.transformer_blocks.11.0.mlp.fc1.bias', 'module.neck_module.transformer_blocks.11.0.mlp.fc2.weight',\
          \ 'module.neck_module.transformer_blocks.11.0.mlp.fc2.bias', 'module.neck_module.last_proj.weight',\
          \ 'module.neck_module.last_proj.bias', 'module.decoder_module.logits.0.weight',\
          \ 'module.decoder_module.logits.0.bias', 'module.decoder_module.logits.1.weight',\
          \ 'module.decoder_module.logits.1.bias', 'module.decoder_module.logits.1.running_mean',\
          \ 'module.decoder_module.logits.1.running_var', 'module.decoder_module.logits.1.num_batches_tracked'])\r\
          \n```\r\nSo the pretrained weights keys can not adapt to the model, can\
          \ you give me some suggestions about how to load the pretrained weights\
          \ for the PATH model? \r\n\r\nThanks a lot!!"
        updatedAt: '2023-07-18T06:26:56.530Z'
      numEdits: 0
      reactions: []
    id: 64b630b0db5ccb303c6dc2b8
    type: comment
  author: luomingshuang
  content: "Hi, thanks for your great job. I am trying to reproduce your results based\
    \ on this url pretrained weights. But when I load the pretrained weights model,\
    \ I find the keys are not same. The pretrained weights keys are as follows:\r\n\
    ```\r\nodict_keys(['cls_token', 'cls_token_pos_embed', 'pos_embed', 'patch_embed.proj.weight',\
    \ 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight',\
    \ 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias',\
    \ 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias',\
    \ 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight',\
    \ 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias',\
    \ 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight',\
    \ 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight',\
    \ 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight',\
    \ 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias',\
    \ 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias',\
    \ 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight',\
    \ 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias',\
    \ 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight',\
    \ 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight',\
    \ 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight',\
    \ 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias',\
    \ 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias',\
    \ 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight',\
    \ 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias',\
    \ 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight',\
    \ 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight',\
    \ 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight',\
    \ 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias',\
    \ 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias',\
    \ 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight',\
    \ 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias',\
    \ 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight',\
    \ 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight',\
    \ 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight',\
    \ 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias',\
    \ 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias',\
    \ 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight',\
    \ 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias',\
    \ 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight',\
    \ 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight',\
    \ 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight',\
    \ 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias',\
    \ 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight',\
    \ 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias',\
    \ 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight',\
    \ 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias',\
    \ 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight',\
    \ 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias',\
    \ 'blocks.12.norm1.weight', 'blocks.12.norm1.bias', 'blocks.12.attn.qkv.weight',\
    \ 'blocks.12.attn.qkv.bias', 'blocks.12.attn.proj.weight', 'blocks.12.attn.proj.bias',\
    \ 'blocks.12.norm2.weight', 'blocks.12.norm2.bias', 'blocks.12.mlp.fc1.weight',\
    \ 'blocks.12.mlp.fc1.bias', 'blocks.12.mlp.fc2.weight', 'blocks.12.mlp.fc2.bias',\
    \ 'blocks.13.norm1.weight', 'blocks.13.norm1.bias', 'blocks.13.attn.qkv.weight',\
    \ 'blocks.13.attn.qkv.bias', 'blocks.13.attn.proj.weight', 'blocks.13.attn.proj.bias',\
    \ 'blocks.13.norm2.weight', 'blocks.13.norm2.bias', 'blocks.13.mlp.fc1.weight',\
    \ 'blocks.13.mlp.fc1.bias', 'blocks.13.mlp.fc2.weight', 'blocks.13.mlp.fc2.bias',\
    \ 'blocks.14.norm1.weight', 'blocks.14.norm1.bias', 'blocks.14.attn.qkv.weight',\
    \ 'blocks.14.attn.qkv.bias', 'blocks.14.attn.proj.weight', 'blocks.14.attn.proj.bias',\
    \ 'blocks.14.norm2.weight', 'blocks.14.norm2.bias', 'blocks.14.mlp.fc1.weight',\
    \ 'blocks.14.mlp.fc1.bias', 'blocks.14.mlp.fc2.weight', 'blocks.14.mlp.fc2.bias',\
    \ 'blocks.15.norm1.weight', 'blocks.15.norm1.bias', 'blocks.15.attn.qkv.weight',\
    \ 'blocks.15.attn.qkv.bias', 'blocks.15.attn.proj.weight', 'blocks.15.attn.proj.bias',\
    \ 'blocks.15.norm2.weight', 'blocks.15.norm2.bias', 'blocks.15.mlp.fc1.weight',\
    \ 'blocks.15.mlp.fc1.bias', 'blocks.15.mlp.fc2.weight', 'blocks.15.mlp.fc2.bias',\
    \ 'blocks.16.norm1.weight', 'blocks.16.norm1.bias', 'blocks.16.attn.qkv.weight',\
    \ 'blocks.16.attn.qkv.bias', 'blocks.16.attn.proj.weight', 'blocks.16.attn.proj.bias',\
    \ 'blocks.16.norm2.weight', 'blocks.16.norm2.bias', 'blocks.16.mlp.fc1.weight',\
    \ 'blocks.16.mlp.fc1.bias', 'blocks.16.mlp.fc2.weight', 'blocks.16.mlp.fc2.bias',\
    \ 'blocks.17.norm1.weight', 'blocks.17.norm1.bias', 'blocks.17.attn.qkv.weight',\
    \ 'blocks.17.attn.qkv.bias', 'blocks.17.attn.proj.weight', 'blocks.17.attn.proj.bias',\
    \ 'blocks.17.norm2.weight', 'blocks.17.norm2.bias', 'blocks.17.mlp.fc1.weight',\
    \ 'blocks.17.mlp.fc1.bias', 'blocks.17.mlp.fc2.weight', 'blocks.17.mlp.fc2.bias',\
    \ 'blocks.18.norm1.weight', 'blocks.18.norm1.bias', 'blocks.18.attn.qkv.weight',\
    \ 'blocks.18.attn.qkv.bias', 'blocks.18.attn.proj.weight', 'blocks.18.attn.proj.bias',\
    \ 'blocks.18.norm2.weight', 'blocks.18.norm2.bias', 'blocks.18.mlp.fc1.weight',\
    \ 'blocks.18.mlp.fc1.bias', 'blocks.18.mlp.fc2.weight', 'blocks.18.mlp.fc2.bias',\
    \ 'blocks.19.norm1.weight', 'blocks.19.norm1.bias', 'blocks.19.attn.qkv.weight',\
    \ 'blocks.19.attn.qkv.bias', 'blocks.19.attn.proj.weight', 'blocks.19.attn.proj.bias',\
    \ 'blocks.19.norm2.weight', 'blocks.19.norm2.bias', 'blocks.19.mlp.fc1.weight',\
    \ 'blocks.19.mlp.fc1.bias', 'blocks.19.mlp.fc2.weight', 'blocks.19.mlp.fc2.bias',\
    \ 'blocks.20.norm1.weight', 'blocks.20.norm1.bias', 'blocks.20.attn.qkv.weight',\
    \ 'blocks.20.attn.qkv.bias', 'blocks.20.attn.proj.weight', 'blocks.20.attn.proj.bias',\
    \ 'blocks.20.norm2.weight', 'blocks.20.norm2.bias', 'blocks.20.mlp.fc1.weight',\
    \ 'blocks.20.mlp.fc1.bias', 'blocks.20.mlp.fc2.weight', 'blocks.20.mlp.fc2.bias',\
    \ 'blocks.21.norm1.weight', 'blocks.21.norm1.bias', 'blocks.21.attn.qkv.weight',\
    \ 'blocks.21.attn.qkv.bias', 'blocks.21.attn.proj.weight', 'blocks.21.attn.proj.bias',\
    \ 'blocks.21.norm2.weight', 'blocks.21.norm2.bias', 'blocks.21.mlp.fc1.weight',\
    \ 'blocks.21.mlp.fc1.bias', 'blocks.21.mlp.fc2.weight', 'blocks.21.mlp.fc2.bias',\
    \ 'blocks.22.norm1.weight', 'blocks.22.norm1.bias', 'blocks.22.attn.qkv.weight',\
    \ 'blocks.22.attn.qkv.bias', 'blocks.22.attn.proj.weight', 'blocks.22.attn.proj.bias',\
    \ 'blocks.22.norm2.weight', 'blocks.22.norm2.bias', 'blocks.22.mlp.fc1.weight',\
    \ 'blocks.22.mlp.fc1.bias', 'blocks.22.mlp.fc2.weight', 'blocks.22.mlp.fc2.bias',\
    \ 'blocks.23.norm1.weight', 'blocks.23.norm1.bias', 'blocks.23.attn.qkv.weight',\
    \ 'blocks.23.attn.qkv.bias', 'blocks.23.attn.proj.weight', 'blocks.23.attn.proj.bias',\
    \ 'blocks.23.norm2.weight', 'blocks.23.norm2.bias', 'blocks.23.mlp.fc1.weight',\
    \ 'blocks.23.mlp.fc1.bias', 'blocks.23.mlp.fc2.weight', 'blocks.23.mlp.fc2.bias',\
    \ 'norm.weight', 'norm.bias'])\r\n```\r\nBut the keys for  the model in PATH \
    \ are as follows:\r\n```\r\nodict_keys(['module.backbone_module.pos_embed', 'module.backbone_module.patch_embed.proj.weight',\
    \ 'module.backbone_module.patch_embed.proj.bias', 'module.backbone_module.blocks.0.norm1.weight',\
    \ 'module.backbone_module.blocks.0.norm1.bias', 'module.backbone_module.blocks.0.attn.qkv.weight',\
    \ 'module.backbone_module.blocks.0.attn.qkv.bias', 'module.backbone_module.blocks.0.attn.proj.weight',\
    \ 'module.backbone_module.blocks.0.attn.proj.bias', 'module.backbone_module.blocks.0.norm2.weight',\
    \ 'module.backbone_module.blocks.0.norm2.bias', 'module.backbone_module.blocks.0.mlp.fc1.weight',\
    \ 'module.backbone_module.blocks.0.mlp.fc1.bias', 'module.backbone_module.blocks.0.mlp.fc2.weight',\
    \ 'module.backbone_module.blocks.0.mlp.fc2.bias', 'module.backbone_module.blocks.1.norm1.weight',\
    \ 'module.backbone_module.blocks.1.norm1.bias', 'module.backbone_module.blocks.1.attn.qkv.weight',\
    \ 'module.backbone_module.blocks.1.attn.qkv.bias', 'module.backbone_module.blocks.1.attn.proj.weight',\
    \ 'module.backbone_module.blocks.1.attn.proj.bias', 'module.backbone_module.blocks.1.norm2.weight',\
    \ 'module.backbone_module.blocks.1.norm2.bias', 'module.backbone_module.blocks.1.mlp.fc1.weight',\
    \ 'module.backbone_module.blocks.1.mlp.fc1.bias', 'module.backbone_module.blocks.1.mlp.fc2.weight',\
    \ 'module.backbone_module.blocks.1.mlp.fc2.bias', 'module.backbone_module.blocks.2.norm1.weight',\
    \ 'module.backbone_module.blocks.2.norm1.bias', 'module.backbone_module.blocks.2.attn.qkv.weight',\
    \ 'module.backbone_module.blocks.2.attn.qkv.bias', 'module.backbone_module.blocks.2.attn.proj.weight',\
    \ 'module.backbone_module.blocks.2.attn.proj.bias', 'module.backbone_module.blocks.2.norm2.weight',\
    \ 'module.backbone_module.blocks.2.norm2.bias', 'module.backbone_module.blocks.2.mlp.fc1.weight',\
    \ 'module.backbone_module.blocks.2.mlp.fc1.bias', 'module.backbone_module.blocks.2.mlp.fc2.weight',\
    \ 'module.backbone_module.blocks.2.mlp.fc2.bias', 'module.backbone_module.blocks.3.norm1.weight',\
    \ 'module.backbone_module.blocks.3.norm1.bias', 'module.backbone_module.blocks.3.attn.qkv.weight',\
    \ 'module.backbone_module.blocks.3.attn.qkv.bias', 'module.backbone_module.blocks.3.attn.proj.weight',\
    \ 'module.backbone_module.blocks.3.attn.proj.bias', 'module.backbone_module.blocks.3.norm2.weight',\
    \ 'module.backbone_module.blocks.3.norm2.bias', 'module.backbone_module.blocks.3.mlp.fc1.weight',\
    \ 'module.backbone_module.blocks.3.mlp.fc1.bias', 'module.backbone_module.blocks.3.mlp.fc2.weight',\
    \ 'module.backbone_module.blocks.3.mlp.fc2.bias', 'module.backbone_module.blocks.4.norm1.weight',\
    \ 'module.backbone_module.blocks.4.norm1.bias', 'module.backbone_module.blocks.4.attn.qkv.weight',\
    \ 'module.backbone_module.blocks.4.attn.qkv.bias', 'module.backbone_module.blocks.4.attn.proj.weight',\
    \ 'module.backbone_module.blocks.4.attn.proj.bias', 'module.backbone_module.blocks.4.norm2.weight',\
    \ 'module.backbone_module.blocks.4.norm2.bias', 'module.backbone_module.blocks.4.mlp.fc1.weight',\
    \ 'module.backbone_module.blocks.4.mlp.fc1.bias', 'module.backbone_module.blocks.4.mlp.fc2.weight',\
    \ 'module.backbone_module.blocks.4.mlp.fc2.bias', 'module.backbone_module.blocks.5.norm1.weight',\
    \ 'module.backbone_module.blocks.5.norm1.bias', 'module.backbone_module.blocks.5.attn.qkv.weight',\
    \ 'module.backbone_module.blocks.5.attn.qkv.bias', 'module.backbone_module.blocks.5.attn.proj.weight',\
    \ 'module.backbone_module.blocks.5.attn.proj.bias', 'module.backbone_module.blocks.5.norm2.weight',\
    \ 'module.backbone_module.blocks.5.norm2.bias', 'module.backbone_module.blocks.5.mlp.fc1.weight',\
    \ 'module.backbone_module.blocks.5.mlp.fc1.bias', 'module.backbone_module.blocks.5.mlp.fc2.weight',\
    \ 'module.backbone_module.blocks.5.mlp.fc2.bias', 'module.backbone_module.blocks.6.norm1.weight',\
    \ 'module.backbone_module.blocks.6.norm1.bias', 'module.backbone_module.blocks.6.attn.qkv.weight',\
    \ 'module.backbone_module.blocks.6.attn.qkv.bias', 'module.backbone_module.blocks.6.attn.proj.weight',\
    \ 'module.backbone_module.blocks.6.attn.proj.bias', 'module.backbone_module.blocks.6.norm2.weight',\
    \ 'module.backbone_module.blocks.6.norm2.bias', 'module.backbone_module.blocks.6.mlp.fc1.weight',\
    \ 'module.backbone_module.blocks.6.mlp.fc1.bias', 'module.backbone_module.blocks.6.mlp.fc2.weight',\
    \ 'module.backbone_module.blocks.6.mlp.fc2.bias', 'module.backbone_module.blocks.7.norm1.weight',\
    \ 'module.backbone_module.blocks.7.norm1.bias', 'module.backbone_module.blocks.7.attn.qkv.weight',\
    \ 'module.backbone_module.blocks.7.attn.qkv.bias', 'module.backbone_module.blocks.7.attn.proj.weight',\
    \ 'module.backbone_module.blocks.7.attn.proj.bias', 'module.backbone_module.blocks.7.norm2.weight',\
    \ 'module.backbone_module.blocks.7.norm2.bias', 'module.backbone_module.blocks.7.mlp.fc1.weight',\
    \ 'module.backbone_module.blocks.7.mlp.fc1.bias', 'module.backbone_module.blocks.7.mlp.fc2.weight',\
    \ 'module.backbone_module.blocks.7.mlp.fc2.bias', 'module.backbone_module.blocks.8.norm1.weight',\
    \ 'module.backbone_module.blocks.8.norm1.bias', 'module.backbone_module.blocks.8.attn.qkv.weight',\
    \ 'module.backbone_module.blocks.8.attn.qkv.bias', 'module.backbone_module.blocks.8.attn.proj.weight',\
    \ 'module.backbone_module.blocks.8.attn.proj.bias', 'module.backbone_module.blocks.8.norm2.weight',\
    \ 'module.backbone_module.blocks.8.norm2.bias', 'module.backbone_module.blocks.8.mlp.fc1.weight',\
    \ 'module.backbone_module.blocks.8.mlp.fc1.bias', 'module.backbone_module.blocks.8.mlp.fc2.weight',\
    \ 'module.backbone_module.blocks.8.mlp.fc2.bias', 'module.backbone_module.blocks.9.norm1.weight',\
    \ 'module.backbone_module.blocks.9.norm1.bias', 'module.backbone_module.blocks.9.attn.qkv.weight',\
    \ 'module.backbone_module.blocks.9.attn.qkv.bias', 'module.backbone_module.blocks.9.attn.proj.weight',\
    \ 'module.backbone_module.blocks.9.attn.proj.bias', 'module.backbone_module.blocks.9.norm2.weight',\
    \ 'module.backbone_module.blocks.9.norm2.bias', 'module.backbone_module.blocks.9.mlp.fc1.weight',\
    \ 'module.backbone_module.blocks.9.mlp.fc1.bias', 'module.backbone_module.blocks.9.mlp.fc2.weight',\
    \ 'module.backbone_module.blocks.9.mlp.fc2.bias', 'module.backbone_module.blocks.10.norm1.weight',\
    \ 'module.backbone_module.blocks.10.norm1.bias', 'module.backbone_module.blocks.10.attn.qkv.weight',\
    \ 'module.backbone_module.blocks.10.attn.qkv.bias', 'module.backbone_module.blocks.10.attn.proj.weight',\
    \ 'module.backbone_module.blocks.10.attn.proj.bias', 'module.backbone_module.blocks.10.norm2.weight',\
    \ 'module.backbone_module.blocks.10.norm2.bias', 'module.backbone_module.blocks.10.mlp.fc1.weight',\
    \ 'module.backbone_module.blocks.10.mlp.fc1.bias', 'module.backbone_module.blocks.10.mlp.fc2.weight',\
    \ 'module.backbone_module.blocks.10.mlp.fc2.bias', 'module.backbone_module.blocks.11.norm1.weight',\
    \ 'module.backbone_module.blocks.11.norm1.bias', 'module.backbone_module.blocks.11.attn.qkv.weight',\
    \ 'module.backbone_module.blocks.11.attn.qkv.bias', 'module.backbone_module.blocks.11.attn.proj.weight',\
    \ 'module.backbone_module.blocks.11.attn.proj.bias', 'module.backbone_module.blocks.11.norm2.weight',\
    \ 'module.backbone_module.blocks.11.norm2.bias', 'module.backbone_module.blocks.11.mlp.fc1.weight',\
    \ 'module.backbone_module.blocks.11.mlp.fc1.bias', 'module.backbone_module.blocks.11.mlp.fc2.weight',\
    \ 'module.backbone_module.blocks.11.mlp.fc2.bias', 'module.backbone_module.norm.weight',\
    \ 'module.backbone_module.norm.bias', 'module.neck_module.reduction_layers.0.weight',\
    \ 'module.neck_module.reduction_layers.0.bias', 'module.neck_module.reduction_layers.1.weight',\
    \ 'module.neck_module.reduction_layers.1.bias', 'module.neck_module.reduction_layers.2.weight',\
    \ 'module.neck_module.reduction_layers.2.bias', 'module.neck_module.reduction_layers.3.weight',\
    \ 'module.neck_module.reduction_layers.3.bias', 'module.neck_module.reduction_layers.4.weight',\
    \ 'module.neck_module.reduction_layers.4.bias', 'module.neck_module.reduction_layers.5.weight',\
    \ 'module.neck_module.reduction_layers.5.bias', 'module.neck_module.reduction_layers.6.weight',\
    \ 'module.neck_module.reduction_layers.6.bias', 'module.neck_module.reduction_layers.7.weight',\
    \ 'module.neck_module.reduction_layers.7.bias', 'module.neck_module.reduction_layers.8.weight',\
    \ 'module.neck_module.reduction_layers.8.bias', 'module.neck_module.reduction_layers.9.weight',\
    \ 'module.neck_module.reduction_layers.9.bias', 'module.neck_module.reduction_layers.10.weight',\
    \ 'module.neck_module.reduction_layers.10.bias', 'module.neck_module.reduction_layers.11.weight',\
    \ 'module.neck_module.reduction_layers.11.bias', 'module.neck_module.reduction_layers.12.weight',\
    \ 'module.neck_module.reduction_layers.12.bias', 'module.neck_module.side_gate_params.0',\
    \ 'module.neck_module.side_gate_params.1', 'module.neck_module.side_gate_params.2',\
    \ 'module.neck_module.side_gate_params.3', 'module.neck_module.side_gate_params.4',\
    \ 'module.neck_module.side_gate_params.5', 'module.neck_module.side_gate_params.6',\
    \ 'module.neck_module.side_gate_params.7', 'module.neck_module.side_gate_params.8',\
    \ 'module.neck_module.side_gate_params.9', 'module.neck_module.side_gate_params.10',\
    \ 'module.neck_module.side_gate_params.11', 'module.neck_module.transformer_blocks.0.0.norm1.weight',\
    \ 'module.neck_module.transformer_blocks.0.0.norm1.bias', 'module.neck_module.transformer_blocks.0.0.attn.qkv.weight',\
    \ 'module.neck_module.transformer_blocks.0.0.attn.proj.weight', 'module.neck_module.transformer_blocks.0.0.attn.proj.bias',\
    \ 'module.neck_module.transformer_blocks.0.0.norm2.weight', 'module.neck_module.transformer_blocks.0.0.norm2.bias',\
    \ 'module.neck_module.transformer_blocks.0.0.mlp.fc1.weight', 'module.neck_module.transformer_blocks.0.0.mlp.fc1.bias',\
    \ 'module.neck_module.transformer_blocks.0.0.mlp.fc2.weight', 'module.neck_module.transformer_blocks.0.0.mlp.fc2.bias',\
    \ 'module.neck_module.transformer_blocks.1.0.norm1.weight', 'module.neck_module.transformer_blocks.1.0.norm1.bias',\
    \ 'module.neck_module.transformer_blocks.1.0.attn.qkv.weight', 'module.neck_module.transformer_blocks.1.0.attn.proj.weight',\
    \ 'module.neck_module.transformer_blocks.1.0.attn.proj.bias', 'module.neck_module.transformer_blocks.1.0.norm2.weight',\
    \ 'module.neck_module.transformer_blocks.1.0.norm2.bias', 'module.neck_module.transformer_blocks.1.0.mlp.fc1.weight',\
    \ 'module.neck_module.transformer_blocks.1.0.mlp.fc1.bias', 'module.neck_module.transformer_blocks.1.0.mlp.fc2.weight',\
    \ 'module.neck_module.transformer_blocks.1.0.mlp.fc2.bias', 'module.neck_module.transformer_blocks.2.0.norm1.weight',\
    \ 'module.neck_module.transformer_blocks.2.0.norm1.bias', 'module.neck_module.transformer_blocks.2.0.attn.qkv.weight',\
    \ 'module.neck_module.transformer_blocks.2.0.attn.proj.weight', 'module.neck_module.transformer_blocks.2.0.attn.proj.bias',\
    \ 'module.neck_module.transformer_blocks.2.0.norm2.weight', 'module.neck_module.transformer_blocks.2.0.norm2.bias',\
    \ 'module.neck_module.transformer_blocks.2.0.mlp.fc1.weight', 'module.neck_module.transformer_blocks.2.0.mlp.fc1.bias',\
    \ 'module.neck_module.transformer_blocks.2.0.mlp.fc2.weight', 'module.neck_module.transformer_blocks.2.0.mlp.fc2.bias',\
    \ 'module.neck_module.transformer_blocks.3.0.norm1.weight', 'module.neck_module.transformer_blocks.3.0.norm1.bias',\
    \ 'module.neck_module.transformer_blocks.3.0.attn.qkv.weight', 'module.neck_module.transformer_blocks.3.0.attn.proj.weight',\
    \ 'module.neck_module.transformer_blocks.3.0.attn.proj.bias', 'module.neck_module.transformer_blocks.3.0.norm2.weight',\
    \ 'module.neck_module.transformer_blocks.3.0.norm2.bias', 'module.neck_module.transformer_blocks.3.0.mlp.fc1.weight',\
    \ 'module.neck_module.transformer_blocks.3.0.mlp.fc1.bias', 'module.neck_module.transformer_blocks.3.0.mlp.fc2.weight',\
    \ 'module.neck_module.transformer_blocks.3.0.mlp.fc2.bias', 'module.neck_module.transformer_blocks.4.0.norm1.weight',\
    \ 'module.neck_module.transformer_blocks.4.0.norm1.bias', 'module.neck_module.transformer_blocks.4.0.attn.qkv.weight',\
    \ 'module.neck_module.transformer_blocks.4.0.attn.proj.weight', 'module.neck_module.transformer_blocks.4.0.attn.proj.bias',\
    \ 'module.neck_module.transformer_blocks.4.0.norm2.weight', 'module.neck_module.transformer_blocks.4.0.norm2.bias',\
    \ 'module.neck_module.transformer_blocks.4.0.mlp.fc1.weight', 'module.neck_module.transformer_blocks.4.0.mlp.fc1.bias',\
    \ 'module.neck_module.transformer_blocks.4.0.mlp.fc2.weight', 'module.neck_module.transformer_blocks.4.0.mlp.fc2.bias',\
    \ 'module.neck_module.transformer_blocks.5.0.norm1.weight', 'module.neck_module.transformer_blocks.5.0.norm1.bias',\
    \ 'module.neck_module.transformer_blocks.5.0.attn.qkv.weight', 'module.neck_module.transformer_blocks.5.0.attn.proj.weight',\
    \ 'module.neck_module.transformer_blocks.5.0.attn.proj.bias', 'module.neck_module.transformer_blocks.5.0.norm2.weight',\
    \ 'module.neck_module.transformer_blocks.5.0.norm2.bias', 'module.neck_module.transformer_blocks.5.0.mlp.fc1.weight',\
    \ 'module.neck_module.transformer_blocks.5.0.mlp.fc1.bias', 'module.neck_module.transformer_blocks.5.0.mlp.fc2.weight',\
    \ 'module.neck_module.transformer_blocks.5.0.mlp.fc2.bias', 'module.neck_module.transformer_blocks.6.0.norm1.weight',\
    \ 'module.neck_module.transformer_blocks.6.0.norm1.bias', 'module.neck_module.transformer_blocks.6.0.attn.qkv.weight',\
    \ 'module.neck_module.transformer_blocks.6.0.attn.proj.weight', 'module.neck_module.transformer_blocks.6.0.attn.proj.bias',\
    \ 'module.neck_module.transformer_blocks.6.0.norm2.weight', 'module.neck_module.transformer_blocks.6.0.norm2.bias',\
    \ 'module.neck_module.transformer_blocks.6.0.mlp.fc1.weight', 'module.neck_module.transformer_blocks.6.0.mlp.fc1.bias',\
    \ 'module.neck_module.transformer_blocks.6.0.mlp.fc2.weight', 'module.neck_module.transformer_blocks.6.0.mlp.fc2.bias',\
    \ 'module.neck_module.transformer_blocks.7.0.norm1.weight', 'module.neck_module.transformer_blocks.7.0.norm1.bias',\
    \ 'module.neck_module.transformer_blocks.7.0.attn.qkv.weight', 'module.neck_module.transformer_blocks.7.0.attn.proj.weight',\
    \ 'module.neck_module.transformer_blocks.7.0.attn.proj.bias', 'module.neck_module.transformer_blocks.7.0.norm2.weight',\
    \ 'module.neck_module.transformer_blocks.7.0.norm2.bias', 'module.neck_module.transformer_blocks.7.0.mlp.fc1.weight',\
    \ 'module.neck_module.transformer_blocks.7.0.mlp.fc1.bias', 'module.neck_module.transformer_blocks.7.0.mlp.fc2.weight',\
    \ 'module.neck_module.transformer_blocks.7.0.mlp.fc2.bias', 'module.neck_module.transformer_blocks.8.0.norm1.weight',\
    \ 'module.neck_module.transformer_blocks.8.0.norm1.bias', 'module.neck_module.transformer_blocks.8.0.attn.qkv.weight',\
    \ 'module.neck_module.transformer_blocks.8.0.attn.proj.weight', 'module.neck_module.transformer_blocks.8.0.attn.proj.bias',\
    \ 'module.neck_module.transformer_blocks.8.0.norm2.weight', 'module.neck_module.transformer_blocks.8.0.norm2.bias',\
    \ 'module.neck_module.transformer_blocks.8.0.mlp.fc1.weight', 'module.neck_module.transformer_blocks.8.0.mlp.fc1.bias',\
    \ 'module.neck_module.transformer_blocks.8.0.mlp.fc2.weight', 'module.neck_module.transformer_blocks.8.0.mlp.fc2.bias',\
    \ 'module.neck_module.transformer_blocks.9.0.norm1.weight', 'module.neck_module.transformer_blocks.9.0.norm1.bias',\
    \ 'module.neck_module.transformer_blocks.9.0.attn.qkv.weight', 'module.neck_module.transformer_blocks.9.0.attn.proj.weight',\
    \ 'module.neck_module.transformer_blocks.9.0.attn.proj.bias', 'module.neck_module.transformer_blocks.9.0.norm2.weight',\
    \ 'module.neck_module.transformer_blocks.9.0.norm2.bias', 'module.neck_module.transformer_blocks.9.0.mlp.fc1.weight',\
    \ 'module.neck_module.transformer_blocks.9.0.mlp.fc1.bias', 'module.neck_module.transformer_blocks.9.0.mlp.fc2.weight',\
    \ 'module.neck_module.transformer_blocks.9.0.mlp.fc2.bias', 'module.neck_module.transformer_blocks.10.0.norm1.weight',\
    \ 'module.neck_module.transformer_blocks.10.0.norm1.bias', 'module.neck_module.transformer_blocks.10.0.attn.qkv.weight',\
    \ 'module.neck_module.transformer_blocks.10.0.attn.proj.weight', 'module.neck_module.transformer_blocks.10.0.attn.proj.bias',\
    \ 'module.neck_module.transformer_blocks.10.0.norm2.weight', 'module.neck_module.transformer_blocks.10.0.norm2.bias',\
    \ 'module.neck_module.transformer_blocks.10.0.mlp.fc1.weight', 'module.neck_module.transformer_blocks.10.0.mlp.fc1.bias',\
    \ 'module.neck_module.transformer_blocks.10.0.mlp.fc2.weight', 'module.neck_module.transformer_blocks.10.0.mlp.fc2.bias',\
    \ 'module.neck_module.transformer_blocks.11.0.norm1.weight', 'module.neck_module.transformer_blocks.11.0.norm1.bias',\
    \ 'module.neck_module.transformer_blocks.11.0.attn.qkv.weight', 'module.neck_module.transformer_blocks.11.0.attn.proj.weight',\
    \ 'module.neck_module.transformer_blocks.11.0.attn.proj.bias', 'module.neck_module.transformer_blocks.11.0.norm2.weight',\
    \ 'module.neck_module.transformer_blocks.11.0.norm2.bias', 'module.neck_module.transformer_blocks.11.0.mlp.fc1.weight',\
    \ 'module.neck_module.transformer_blocks.11.0.mlp.fc1.bias', 'module.neck_module.transformer_blocks.11.0.mlp.fc2.weight',\
    \ 'module.neck_module.transformer_blocks.11.0.mlp.fc2.bias', 'module.neck_module.last_proj.weight',\
    \ 'module.neck_module.last_proj.bias', 'module.decoder_module.logits.0.weight',\
    \ 'module.decoder_module.logits.0.bias', 'module.decoder_module.logits.1.weight',\
    \ 'module.decoder_module.logits.1.bias', 'module.decoder_module.logits.1.running_mean',\
    \ 'module.decoder_module.logits.1.running_var', 'module.decoder_module.logits.1.num_batches_tracked'])\r\
    \n```\r\nSo the pretrained weights keys can not adapt to the model, can you give\
    \ me some suggestions about how to load the pretrained weights for the PATH model?\
    \ \r\n\r\nThanks a lot!!"
  created_at: 2023-07-18 05:26:56+00:00
  edited: false
  hidden: false
  id: 64b630b0db5ccb303c6dc2b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5d9c3d47073e71e4cea124d9c17356d.svg
      fullname: SHIXIANG TANG
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: tangshixiang
      type: user
    createdAt: '2023-07-19T10:12:46.000Z'
    data:
      edited: false
      editors:
      - tangshixiang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4694310426712036
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5d9c3d47073e71e4cea124d9c17356d.svg
          fullname: SHIXIANG TANG
          isHf: false
          isPro: false
          name: tangshixiang
          type: user
        html: "<p>Thanks for your interests to our work!</p>\n<p>import os<br>import\
          \ sys<br>import collections<br>import torch<br>import numpy as np</p>\n\
          <h2 id=\"please-use-the-transferred-model-with_cls_token-reid-for-reid-tasks-and-the-transferred-model-wo_cls_token-reid-for-other-tasks\"\
          >Please use the transferred model (with_cls_token-reid) for ReID tasks and\
          \ the transferred model (wo_cls_token-reid) for other tasks!</h2>\n<p>sys.path.append('/mnt/cache/chencheng1/vitruvian/vitruvian-multitask')</p>\n\
          <p>mae_pretrain_path = '/mnt/cache/chencheng1/vitruvian/vitruvian-multitask/core/models/backbones/pretrain_weights/mae_pretrain_vit_base.pth'\
          \ # load mae model<br>mae_model = torch.load(mae_pretrain_path)</p>\n<p>save_root\
          \ = '/mnt/lustre/share_data/chencheng1/vitruvian/L2_final_base' #  folder\
          \ of  save_path of the transferred model</p>\n<p>root = '/mnt/lustre/chencheng1/expr_files/vitruvian/L2_full_setting_joint/checkpoints'\
          \ # folder of model_path of the pretrained model<br>config_lists = [<br>\
          \    'v100_32g_vitbase_size224_lr1e3_stepLRx3_bmp1_adafactor_wd01_clip05_layerdecay075_lpe_peddet_citypersons_LSA_reduct8_tbn1_heads2_gate1_peddetShareDecoder_exp3_setting_SharePosEmbed'<br>]</p>\n\
          <p>for config in config_lists:<br>    trained_ckpt_root = os.path.join(root,\
          \ config) # Please take note trained_ckpt_root is the place the pretrained\
          \ model is saved<br>    expr_name = trained_ckpt_root.split('/')[-1]</p>\n\
          <pre><code>wo_cls_token_index = 0  # coco\nwith_cls_token_index = 20  #\
          \ reid_4set\n\n\n# with_cls_token-reid\nwith_cls_token_train_model_path\
          \ = os.path.join(trained_ckpt_root, 'ckpt_task{}_iter_newest.pth.tar'.format(with_cls_token_index))\n\
          with_cls_token_transed_ckpt_save_path = os.path.join(save_root, 'with_cls_token',\
          \ expr_name+'.pth')\nwith_cls_token_train_model = torch.load(with_cls_token_train_model_path,\
          \ map_location=torch.device('cpu'))\n\ncnt = 0\ntraned_ckpt = collections.OrderedDict()\n\
          for name, param in mae_model['model'].items():\n    trained_model_name =\
          \ 'module.backbone_module.' + name\n    if trained_model_name in with_cls_token_train_model['state_dict']:\n\
          \        if name == 'pos_embed':\n            cnt += 1\n            traned_ckpt[name]\
          \ = torch.cat([with_cls_token_train_model['state_dict']['module.backbone_module.cls_token_pos_embed'],\
          \ with_cls_token_train_model['state_dict'][trained_model_name]], dim=1)\n\
          \        else:\n            cnt += 1\n            traned_ckpt[name] = with_cls_token_train_model['state_dict'][trained_model_name]\n\
          \    else:\n        traned_ckpt[name] = mae_model['model'][name]\n\ntorch.save({'model':\
          \ traned_ckpt}, with_cls_token_transed_ckpt_save_path)\nprint('done! transed\
          \ ckpt saved at: {}'.format(with_cls_token_transed_ckpt_save_path))\n\n\n\
          # wo_cls_token-reid\nwo_cls_token_train_model_path = os.path.join(trained_ckpt_root,\
          \ 'ckpt_task{}_iter_newest.pth.tar'.format(wo_cls_token_index))\nwo_cls_token_transed_ckpt_save_path\
          \ = os.path.join(save_root, 'wo_cls_token', expr_name+'.pth')\nwo_cls_token_train_model\
          \ = torch.load(wo_cls_token_train_model_path, map_location=torch.device('cpu'))\n\
          \ncnt = 0\ntraned_ckpt = collections.OrderedDict()\nfor name, param in mae_model['model'].items():\n\
          \    trained_model_name = 'module.backbone_module.' + name\n    if trained_model_name\
          \ in wo_cls_token_train_model['state_dict']:\n        cnt += 1\n       \
          \ traned_ckpt[name] = wo_cls_token_train_model['state_dict'][trained_model_name]\n\
          \    else:\n        traned_ckpt[name] = mae_model['model'][name]\n\ntorch.save({'model':\
          \ traned_ckpt}, wo_cls_token_transed_ckpt_save_path)\nprint('done! transed\
          \ ckpt saved at: {}'.format(wo_cls_token_transed_ckpt_save_path))\n</code></pre>\n\
          <hr>\n"
        raw: "Thanks for your interests to our work!\n\nimport os\nimport sys\nimport\
          \ collections\nimport torch\nimport numpy as np\n\nPlease use the transferred\
          \ model (with_cls_token-reid) for ReID tasks and the transferred model (wo_cls_token-reid)\
          \ for other tasks!\n-----------------------------------------------------------------------------------------------\n\
          sys.path.append('/mnt/cache/chencheng1/vitruvian/vitruvian-multitask')\n\
          \nmae_pretrain_path = '/mnt/cache/chencheng1/vitruvian/vitruvian-multitask/core/models/backbones/pretrain_weights/mae_pretrain_vit_base.pth'\
          \ # load mae model\nmae_model = torch.load(mae_pretrain_path)\n\nsave_root\
          \ = '/mnt/lustre/share_data/chencheng1/vitruvian/L2_final_base' #  folder\
          \ of  save_path of the transferred model\n\nroot = '/mnt/lustre/chencheng1/expr_files/vitruvian/L2_full_setting_joint/checkpoints'\
          \ # folder of model_path of the pretrained model\nconfig_lists = [\n   \
          \ 'v100_32g_vitbase_size224_lr1e3_stepLRx3_bmp1_adafactor_wd01_clip05_layerdecay075_lpe_peddet_citypersons_LSA_reduct8_tbn1_heads2_gate1_peddetShareDecoder_exp3_setting_SharePosEmbed'\n\
          ]\n\nfor config in config_lists:\n    trained_ckpt_root = os.path.join(root,\
          \ config) # Please take note trained_ckpt_root is the place the pretrained\
          \ model is saved\n    expr_name = trained_ckpt_root.split('/')[-1]\n\n \
          \   wo_cls_token_index = 0  # coco\n    with_cls_token_index = 20  # reid_4set\n\
          \n\n    # with_cls_token-reid\n    with_cls_token_train_model_path = os.path.join(trained_ckpt_root,\
          \ 'ckpt_task{}_iter_newest.pth.tar'.format(with_cls_token_index))\n    with_cls_token_transed_ckpt_save_path\
          \ = os.path.join(save_root, 'with_cls_token', expr_name+'.pth')\n    with_cls_token_train_model\
          \ = torch.load(with_cls_token_train_model_path, map_location=torch.device('cpu'))\n\
          \n    cnt = 0\n    traned_ckpt = collections.OrderedDict()\n    for name,\
          \ param in mae_model['model'].items():\n        trained_model_name = 'module.backbone_module.'\
          \ + name\n        if trained_model_name in with_cls_token_train_model['state_dict']:\n\
          \            if name == 'pos_embed':\n                cnt += 1\n       \
          \         traned_ckpt[name] = torch.cat([with_cls_token_train_model['state_dict']['module.backbone_module.cls_token_pos_embed'],\
          \ with_cls_token_train_model['state_dict'][trained_model_name]], dim=1)\n\
          \            else:\n                cnt += 1\n                traned_ckpt[name]\
          \ = with_cls_token_train_model['state_dict'][trained_model_name]\n     \
          \   else:\n            traned_ckpt[name] = mae_model['model'][name]\n\n\
          \    torch.save({'model': traned_ckpt}, with_cls_token_transed_ckpt_save_path)\n\
          \    print('done! transed ckpt saved at: {}'.format(with_cls_token_transed_ckpt_save_path))\n\
          \n\n    # wo_cls_token-reid\n    wo_cls_token_train_model_path = os.path.join(trained_ckpt_root,\
          \ 'ckpt_task{}_iter_newest.pth.tar'.format(wo_cls_token_index))\n    wo_cls_token_transed_ckpt_save_path\
          \ = os.path.join(save_root, 'wo_cls_token', expr_name+'.pth')\n    wo_cls_token_train_model\
          \ = torch.load(wo_cls_token_train_model_path, map_location=torch.device('cpu'))\n\
          \n    cnt = 0\n    traned_ckpt = collections.OrderedDict()\n    for name,\
          \ param in mae_model['model'].items():\n        trained_model_name = 'module.backbone_module.'\
          \ + name\n        if trained_model_name in wo_cls_token_train_model['state_dict']:\n\
          \            cnt += 1\n            traned_ckpt[name] = wo_cls_token_train_model['state_dict'][trained_model_name]\n\
          \        else:\n            traned_ckpt[name] = mae_model['model'][name]\n\
          \n    torch.save({'model': traned_ckpt}, wo_cls_token_transed_ckpt_save_path)\n\
          \    print('done! transed ckpt saved at: {}'.format(wo_cls_token_transed_ckpt_save_path))\n\
          -------------------------------------------------------"
        updatedAt: '2023-07-19T10:12:46.093Z'
      numEdits: 0
      reactions: []
    id: 64b7b71efa7eabaae504cdf3
    type: comment
  author: tangshixiang
  content: "Thanks for your interests to our work!\n\nimport os\nimport sys\nimport\
    \ collections\nimport torch\nimport numpy as np\n\nPlease use the transferred\
    \ model (with_cls_token-reid) for ReID tasks and the transferred model (wo_cls_token-reid)\
    \ for other tasks!\n-----------------------------------------------------------------------------------------------\n\
    sys.path.append('/mnt/cache/chencheng1/vitruvian/vitruvian-multitask')\n\nmae_pretrain_path\
    \ = '/mnt/cache/chencheng1/vitruvian/vitruvian-multitask/core/models/backbones/pretrain_weights/mae_pretrain_vit_base.pth'\
    \ # load mae model\nmae_model = torch.load(mae_pretrain_path)\n\nsave_root = '/mnt/lustre/share_data/chencheng1/vitruvian/L2_final_base'\
    \ #  folder of  save_path of the transferred model\n\nroot = '/mnt/lustre/chencheng1/expr_files/vitruvian/L2_full_setting_joint/checkpoints'\
    \ # folder of model_path of the pretrained model\nconfig_lists = [\n    'v100_32g_vitbase_size224_lr1e3_stepLRx3_bmp1_adafactor_wd01_clip05_layerdecay075_lpe_peddet_citypersons_LSA_reduct8_tbn1_heads2_gate1_peddetShareDecoder_exp3_setting_SharePosEmbed'\n\
    ]\n\nfor config in config_lists:\n    trained_ckpt_root = os.path.join(root, config)\
    \ # Please take note trained_ckpt_root is the place the pretrained model is saved\n\
    \    expr_name = trained_ckpt_root.split('/')[-1]\n\n    wo_cls_token_index =\
    \ 0  # coco\n    with_cls_token_index = 20  # reid_4set\n\n\n    # with_cls_token-reid\n\
    \    with_cls_token_train_model_path = os.path.join(trained_ckpt_root, 'ckpt_task{}_iter_newest.pth.tar'.format(with_cls_token_index))\n\
    \    with_cls_token_transed_ckpt_save_path = os.path.join(save_root, 'with_cls_token',\
    \ expr_name+'.pth')\n    with_cls_token_train_model = torch.load(with_cls_token_train_model_path,\
    \ map_location=torch.device('cpu'))\n\n    cnt = 0\n    traned_ckpt = collections.OrderedDict()\n\
    \    for name, param in mae_model['model'].items():\n        trained_model_name\
    \ = 'module.backbone_module.' + name\n        if trained_model_name in with_cls_token_train_model['state_dict']:\n\
    \            if name == 'pos_embed':\n                cnt += 1\n             \
    \   traned_ckpt[name] = torch.cat([with_cls_token_train_model['state_dict']['module.backbone_module.cls_token_pos_embed'],\
    \ with_cls_token_train_model['state_dict'][trained_model_name]], dim=1)\n    \
    \        else:\n                cnt += 1\n                traned_ckpt[name] =\
    \ with_cls_token_train_model['state_dict'][trained_model_name]\n        else:\n\
    \            traned_ckpt[name] = mae_model['model'][name]\n\n    torch.save({'model':\
    \ traned_ckpt}, with_cls_token_transed_ckpt_save_path)\n    print('done! transed\
    \ ckpt saved at: {}'.format(with_cls_token_transed_ckpt_save_path))\n\n\n    #\
    \ wo_cls_token-reid\n    wo_cls_token_train_model_path = os.path.join(trained_ckpt_root,\
    \ 'ckpt_task{}_iter_newest.pth.tar'.format(wo_cls_token_index))\n    wo_cls_token_transed_ckpt_save_path\
    \ = os.path.join(save_root, 'wo_cls_token', expr_name+'.pth')\n    wo_cls_token_train_model\
    \ = torch.load(wo_cls_token_train_model_path, map_location=torch.device('cpu'))\n\
    \n    cnt = 0\n    traned_ckpt = collections.OrderedDict()\n    for name, param\
    \ in mae_model['model'].items():\n        trained_model_name = 'module.backbone_module.'\
    \ + name\n        if trained_model_name in wo_cls_token_train_model['state_dict']:\n\
    \            cnt += 1\n            traned_ckpt[name] = wo_cls_token_train_model['state_dict'][trained_model_name]\n\
    \        else:\n            traned_ckpt[name] = mae_model['model'][name]\n\n \
    \   torch.save({'model': traned_ckpt}, wo_cls_token_transed_ckpt_save_path)\n\
    \    print('done! transed ckpt saved at: {}'.format(wo_cls_token_transed_ckpt_save_path))\n\
    -------------------------------------------------------"
  created_at: 2023-07-19 09:12:46+00:00
  edited: false
  hidden: false
  id: 64b7b71efa7eabaae504cdf3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: OpenGVLab/PATH-ViTB
repo_type: model
status: open
target_branch: null
title: The keys names are not same to the model state dict keys in PATH
