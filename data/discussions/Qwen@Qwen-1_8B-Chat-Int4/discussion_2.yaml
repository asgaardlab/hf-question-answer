!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-12-02 13:13:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-02T13:13:18.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9456629157066345
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Hi,</p>

          <p>Recent release of Qwen models all claim that we can use AutoGPTQ to load,
          especially metnioned in the readme. But this is not true at all. None of
          them is GPTQ quantized.</p>

          <p>All Unquantized and int4 quantized version can be loaded by HF Transformer
          with trust remote code=True. There is nothing to do with AutoGPTQ loader,
          except that AutoGPTQ can in fact making quantized version of Qwen.</p>

          <p>But you don''t have to make AutoGPTQ a dependent package.</p>

          <p>PS:<br>Also can confirm that latest transformers 4.35.2 would work well
          for Qwen models, too. 4.32.0 is not enforced.</p>

          '
        raw: "Hi,\r\n\r\nRecent release of Qwen models all claim that we can use AutoGPTQ\
          \ to load, especially metnioned in the readme. But this is not true at all.\
          \ None of them is GPTQ quantized.\r\n\r\nAll Unquantized and int4 quantized\
          \ version can be loaded by HF Transformer with trust remote code=True. There\
          \ is nothing to do with AutoGPTQ loader, except that AutoGPTQ can in fact\
          \ making quantized version of Qwen.\r\n\r\nBut you don't have to make AutoGPTQ\
          \ a dependent package.\r\n\r\nPS:\r\nAlso can confirm that latest transformers\
          \ 4.35.2 would work well for Qwen models, too. 4.32.0 is not enforced."
        updatedAt: '2023-12-02T13:13:18.544Z'
      numEdits: 0
      reactions: []
    id: 656b2d6e7069c2390a0c95bb
    type: comment
  author: Yhyu13
  content: "Hi,\r\n\r\nRecent release of Qwen models all claim that we can use AutoGPTQ\
    \ to load, especially metnioned in the readme. But this is not true at all. None\
    \ of them is GPTQ quantized.\r\n\r\nAll Unquantized and int4 quantized version\
    \ can be loaded by HF Transformer with trust remote code=True. There is nothing\
    \ to do with AutoGPTQ loader, except that AutoGPTQ can in fact making quantized\
    \ version of Qwen.\r\n\r\nBut you don't have to make AutoGPTQ a dependent package.\r\
    \n\r\nPS:\r\nAlso can confirm that latest transformers 4.35.2 would work well\
    \ for Qwen models, too. 4.32.0 is not enforced."
  created_at: 2023-12-02 13:13:18+00:00
  edited: false
  hidden: false
  id: 656b2d6e7069c2390a0c95bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9921ca8568f9d53c919681d8960ad802.svg
      fullname: jklj077
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jklj077
      type: user
    createdAt: '2023-12-21T13:11:24.000Z'
    data:
      edited: false
      editors:
      - jklj077
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8203617334365845
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9921ca8568f9d53c919681d8960ad802.svg
          fullname: jklj077
          isHf: false
          isPro: false
          name: jklj077
          type: user
        html: '<ol>

          <li><code>transformers</code> relies on the <code>auto-gptq</code> to load
          GPTQ quantized models. You indeed need <code>auto-gptq</code> as a dependency
          to load this model (you also need <code>optimum</code>). Otherwise, a ValueError
          is raised. For example, in version 4.35.2, the related code is at <a rel="nofollow"
          href="https://github.com/huggingface/transformers/blob/514de24abfd4416aeba6a6455ad5920f57f3567d/src/transformers/modeling_utils.py#L2802-L2805">https://github.com/huggingface/transformers/blob/514de24abfd4416aeba6a6455ad5920f57f3567d/src/transformers/modeling_utils.py#L2802-L2805</a>.</li>

          <li>Please refer to the GitHub README at <a rel="nofollow" href="https://github.com/QwenLM/Qwen#gptq">https://github.com/QwenLM/Qwen#gptq</a>
          for more updated instructions on the versions of <code>transformers</code>
          and <code>auto-gptq</code>. They are complicated.</li>

          </ol>

          '
        raw: '1. `transformers` relies on the `auto-gptq` to load GPTQ quantized models.
          You indeed need `auto-gptq` as a dependency to load this model (you also
          need `optimum`). Otherwise, a ValueError is raised. For example, in version
          4.35.2, the related code is at <https://github.com/huggingface/transformers/blob/514de24abfd4416aeba6a6455ad5920f57f3567d/src/transformers/modeling_utils.py#L2802-L2805>.

          2. Please refer to the GitHub README at <https://github.com/QwenLM/Qwen#gptq>
          for more updated instructions on the versions of `transformers` and `auto-gptq`.
          They are complicated.'
        updatedAt: '2023-12-21T13:11:24.806Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6584397ca4f176a57fb5b98f
    id: 6584397ca4f176a57fb5b98a
    type: comment
  author: jklj077
  content: '1. `transformers` relies on the `auto-gptq` to load GPTQ quantized models.
    You indeed need `auto-gptq` as a dependency to load this model (you also need
    `optimum`). Otherwise, a ValueError is raised. For example, in version 4.35.2,
    the related code is at <https://github.com/huggingface/transformers/blob/514de24abfd4416aeba6a6455ad5920f57f3567d/src/transformers/modeling_utils.py#L2802-L2805>.

    2. Please refer to the GitHub README at <https://github.com/QwenLM/Qwen#gptq>
    for more updated instructions on the versions of `transformers` and `auto-gptq`.
    They are complicated.'
  created_at: 2023-12-21 13:11:24+00:00
  edited: false
  hidden: false
  id: 6584397ca4f176a57fb5b98a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9921ca8568f9d53c919681d8960ad802.svg
      fullname: jklj077
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jklj077
      type: user
    createdAt: '2023-12-21T13:11:24.000Z'
    data:
      status: closed
    id: 6584397ca4f176a57fb5b98f
    type: status-change
  author: jklj077
  created_at: 2023-12-21 13:11:24+00:00
  id: 6584397ca4f176a57fb5b98f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Qwen/Qwen-1_8B-Chat-Int4
repo_type: model
status: closed
target_branch: null
title: AutoGPTQ does not apply to this int quantization!
