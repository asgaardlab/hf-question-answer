!!python/object:huggingface_hub.community.DiscussionWithDetails
author: george713
conflicting_files: null
created_at: 2024-01-11 14:38:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50073644021e4bb4b2c4e4a0e912d23e.svg
      fullname: "Friedrich Fr\xF6bel"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: george713
      type: user
    createdAt: '2024-01-11T14:38:27.000Z'
    data:
      edited: false
      editors:
      - george713
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7459409236907959
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50073644021e4bb4b2c4e4a0e912d23e.svg
          fullname: "Friedrich Fr\xF6bel"
          isHf: false
          isPro: false
          name: george713
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>I ran into a problem using \"mixtral-8x7b-instruct-v0.1-limarp-zloss-dare-ties.Q5_K_M.gguf\"\
          \ with <code>ctransformers</code> resulting in this error:</p>\n<p><code>RuntimeError:\
          \ Failed to create LLM 'mistral' from 'mixtral-8x7b-instruct-v0.1-limarp-zloss-dare-ties.Q5_K_M.gguf'.</code><br>Exchanging\
          \ 'mixtral' for 'mistral' as model_type did not resolve this.</p>\n<p>Could\
          \ this be a similar error to the previous one here: <a href=\"https://huggingface.co/TheBloke/CausalLM-7B-GGUF/discussions/3\"\
          >https://huggingface.co/TheBloke/CausalLM-7B-GGUF/discussions/3</a></p>\n\
          <p>Best<br>George</p>\n"
        raw: "Hey @TheBloke,\r\n\r\nI ran into a problem using \"mixtral-8x7b-instruct-v0.1-limarp-zloss-dare-ties.Q5_K_M.gguf\"\
          \ with `ctransformers` resulting in this error:\r\n\r\n```RuntimeError:\
          \ Failed to create LLM 'mistral' from 'mixtral-8x7b-instruct-v0.1-limarp-zloss-dare-ties.Q5_K_M.gguf'.```\r\
          \nExchanging 'mixtral' for 'mistral' as model_type did not resolve this.\r\
          \n\r\nCould this be a similar error to the previous one here: https://huggingface.co/TheBloke/CausalLM-7B-GGUF/discussions/3\r\
          \n\r\nBest\r\nGeorge"
        updatedAt: '2024-01-11T14:38:27.184Z'
      numEdits: 0
      reactions: []
    id: 659ffd63c9873890d1e6a8d8
    type: comment
  author: george713
  content: "Hey @TheBloke,\r\n\r\nI ran into a problem using \"mixtral-8x7b-instruct-v0.1-limarp-zloss-dare-ties.Q5_K_M.gguf\"\
    \ with `ctransformers` resulting in this error:\r\n\r\n```RuntimeError: Failed\
    \ to create LLM 'mistral' from 'mixtral-8x7b-instruct-v0.1-limarp-zloss-dare-ties.Q5_K_M.gguf'.```\r\
    \nExchanging 'mixtral' for 'mistral' as model_type did not resolve this.\r\n\r\
    \nCould this be a similar error to the previous one here: https://huggingface.co/TheBloke/CausalLM-7B-GGUF/discussions/3\r\
    \n\r\nBest\r\nGeorge"
  created_at: 2024-01-11 14:38:27+00:00
  edited: false
  hidden: false
  id: 659ffd63c9873890d1e6a8d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2024-01-11T14:53:34.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8384222388267517
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;george713&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/george713\">@<span class=\"\
          underline\">george713</span></a></span>\n\n\t</span></span> I dont think\
          \ ctransformers supports mixtral yet so use llama cpp or llama cpp python\
          \ since that supports it and its considerably faster.</p>\n"
        raw: '@george713 I dont think ctransformers supports mixtral yet so use llama
          cpp or llama cpp python since that supports it and its considerably faster.'
        updatedAt: '2024-01-11T14:53:34.035Z'
      numEdits: 0
      reactions: []
    id: 65a000ee07184d32fae425ad
    type: comment
  author: YaTharThShaRma999
  content: '@george713 I dont think ctransformers supports mixtral yet so use llama
    cpp or llama cpp python since that supports it and its considerably faster.'
  created_at: 2024-01-11 14:53:34+00:00
  edited: false
  hidden: false
  id: 65a000ee07184d32fae425ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50073644021e4bb4b2c4e4a0e912d23e.svg
      fullname: "Friedrich Fr\xF6bel"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: george713
      type: user
    createdAt: '2024-01-11T14:57:08.000Z'
    data:
      edited: false
      editors:
      - george713
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8487913608551025
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50073644021e4bb4b2c4e4a0e912d23e.svg
          fullname: "Friedrich Fr\xF6bel"
          isHf: false
          isPro: false
          name: george713
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;george713&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/george713\"\
          >@<span class=\"underline\">george713</span></a></span>\n\n\t</span></span>\
          \ I dont think ctransformers supports mixtral yet so use llama cpp or llama\
          \ cpp python since that supports it and its considerably faster.</p>\n</blockquote>\n\
          <p>I successfully used a <code>dolphin-2.0-mistral</code> model with ctransformers.\
          \ Is that different from the 'mixtral' models?</p>\n"
        raw: '> @george713 I dont think ctransformers supports mixtral yet so use
          llama cpp or llama cpp python since that supports it and its considerably
          faster.


          I successfully used a `dolphin-2.0-mistral` model with ctransformers. Is
          that different from the ''mixtral'' models?'
        updatedAt: '2024-01-11T14:57:08.754Z'
      numEdits: 0
      reactions: []
    id: 65a001c426d1e9df4faddb77
    type: comment
  author: george713
  content: '> @george713 I dont think ctransformers supports mixtral yet so use llama
    cpp or llama cpp python since that supports it and its considerably faster.


    I successfully used a `dolphin-2.0-mistral` model with ctransformers. Is that
    different from the ''mixtral'' models?'
  created_at: 2024-01-11 14:57:08+00:00
  edited: false
  hidden: false
  id: 65a001c426d1e9df4faddb77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2024-01-11T16:55:08.000Z'
    data:
      edited: true
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9405201077461243
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;george713&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/george713\">@<span class=\"\
          underline\">george713</span></a></span>\n\n\t</span></span> yes very different<br>mixtral\
          \ models actually are like a completely different architecture then mistral\
          \ models.</p>\n<p>mixtral is a moe(mixture of experts) where basically there\
          \ are 8 experts(each 7b in size) and each token, 2 of them are chosen to\
          \ do inference.<br>This results in 13b speed but takes the same vram as\
          \ a 46b.</p>\n<p>unlike mistral and llama models where its just 1 model\
          \ instead of 8 different expert models.</p>\n<p>thats why it says 8x7b.\
          \ the actual original mixtral had 8 experts, each 7b in size.</p>\n"
        raw: '@george713 yes very different

          mixtral models actually are like a completely different architecture then
          mistral models.


          mixtral is a moe(mixture of experts) where basically there are 8 experts(each
          7b in size) and each token, 2 of them are chosen to do inference.

          This results in 13b speed but takes the same vram as a 46b.


          unlike mistral and llama models where its just 1 model instead of 8 different
          expert models.


          thats why it says 8x7b. the actual original mixtral had 8 experts, each
          7b in size.'
        updatedAt: '2024-01-11T16:56:09.324Z'
      numEdits: 1
      reactions: []
    id: 65a01d6cc5770b27ae846608
    type: comment
  author: YaTharThShaRma999
  content: '@george713 yes very different

    mixtral models actually are like a completely different architecture then mistral
    models.


    mixtral is a moe(mixture of experts) where basically there are 8 experts(each
    7b in size) and each token, 2 of them are chosen to do inference.

    This results in 13b speed but takes the same vram as a 46b.


    unlike mistral and llama models where its just 1 model instead of 8 different
    expert models.


    thats why it says 8x7b. the actual original mixtral had 8 experts, each 7b in
    size.'
  created_at: 2024-01-11 16:55:08+00:00
  edited: true
  hidden: false
  id: 65a01d6cc5770b27ae846608
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-LimaRP-ZLoss-DARE-TIES-GGUF
repo_type: model
status: open
target_branch: null
title: Error loading Q5_K_M with ctransformers
