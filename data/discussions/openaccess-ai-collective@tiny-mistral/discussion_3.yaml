!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LaferriereJC
conflicting_files: null
created_at: 2023-10-22 11:59:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-10-22T12:59:20.000Z'
    data:
      edited: false
      editors:
      - LaferriereJC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9921055436134338
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
          fullname: Joshua Laferriere
          isHf: false
          isPro: false
          name: LaferriereJC
          type: user
        html: '<p>would love to train one from scratch.</p>

          '
        raw: would love to train one from scratch.
        updatedAt: '2023-10-22T12:59:20.355Z'
      numEdits: 0
      reactions: []
    id: 65351ca81995cee54a18937f
    type: comment
  author: LaferriereJC
  content: would love to train one from scratch.
  created_at: 2023-10-22 11:59:20+00:00
  edited: false
  hidden: false
  id: 65351ca81995cee54a18937f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-10-22T13:08:17.000Z'
    data:
      edited: false
      editors:
      - LaferriereJC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3961009383201599
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
          fullname: Joshua Laferriere
          isHf: false
          isPro: false
          name: LaferriereJC
          type: user
        html: '<p>would this work?<br>from transformers import MistralConfig, AutoModelForCausalLM</p>

          <p>import torch</p>

          <p>import sys</p>

          <p>config = MistralConfig(<br>hidden_size = 4096,<br>intermediate_size =
          14336,<br>num_hidden_layers = 16,<br>num_attention_heads = 32,<br>num_key_value_heads
          = 8,<br>)</p>

          <p>model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.bfloat16)</p>

          <p>print(f''Created a new model with {model.num_parameters()} parameters.'')</p>

          <p>#this config results in 3.75B parameters.<br>with torch.no_grad():<br>    for
          name, param in model.named_parameters():<br>        param.data = torch.zeros(size=param.size(),
          dtype=param.dtype)<br>        model.save_pretrained(sys.argv[1])</p>

          '
        raw: "would this work?\nfrom transformers import MistralConfig, AutoModelForCausalLM\n\
          \nimport torch\n\nimport sys\n\nconfig = MistralConfig(\nhidden_size = 4096,\n\
          intermediate_size = 14336,\nnum_hidden_layers = 16,\nnum_attention_heads\
          \ = 32,\nnum_key_value_heads = 8,\n)\n\nmodel = AutoModelForCausalLM.from_config(config,\
          \ torch_dtype=torch.bfloat16)\n\nprint(f'Created a new model with {model.num_parameters()}\
          \ parameters.')\n\n#this config results in 3.75B parameters.\nwith torch.no_grad():\n\
          \    for name, param in model.named_parameters():\n        param.data =\
          \ torch.zeros(size=param.size(), dtype=param.dtype)\n        model.save_pretrained(sys.argv[1])"
        updatedAt: '2023-10-22T13:08:17.041Z'
      numEdits: 0
      reactions: []
    id: 65351ec1e7601058463bb1a9
    type: comment
  author: LaferriereJC
  content: "would this work?\nfrom transformers import MistralConfig, AutoModelForCausalLM\n\
    \nimport torch\n\nimport sys\n\nconfig = MistralConfig(\nhidden_size = 4096,\n\
    intermediate_size = 14336,\nnum_hidden_layers = 16,\nnum_attention_heads = 32,\n\
    num_key_value_heads = 8,\n)\n\nmodel = AutoModelForCausalLM.from_config(config,\
    \ torch_dtype=torch.bfloat16)\n\nprint(f'Created a new model with {model.num_parameters()}\
    \ parameters.')\n\n#this config results in 3.75B parameters.\nwith torch.no_grad():\n\
    \    for name, param in model.named_parameters():\n        param.data = torch.zeros(size=param.size(),\
    \ dtype=param.dtype)\n        model.save_pretrained(sys.argv[1])"
  created_at: 2023-10-22 12:08:17+00:00
  edited: false
  hidden: false
  id: 65351ec1e7601058463bb1a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-10-22T17:31:55.000Z'
    data:
      edited: false
      editors:
      - LaferriereJC
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7209296822547913
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
          fullname: Joshua Laferriere
          isHf: false
          isPro: false
          name: LaferriereJC
          type: user
        html: '<p><a rel="nofollow" href="https://www.reddit.com/r/LocalLLaMA/comments/17djecw/mistral_3b/">https://www.reddit.com/r/LocalLLaMA/comments/17djecw/mistral_3b/</a><br><a
          rel="nofollow" href="https://gist.github.com/thistleknot/48b4551737e72c039abf20b6b913b043">https://gist.github.com/thistleknot/48b4551737e72c039abf20b6b913b043</a></p>

          '
        raw: 'https://www.reddit.com/r/LocalLLaMA/comments/17djecw/mistral_3b/

          https://gist.github.com/thistleknot/48b4551737e72c039abf20b6b913b043'
        updatedAt: '2023-10-22T17:31:55.311Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65355c8bc65f2e7588bb1ea6
    id: 65355c8bc65f2e7588bb1ea4
    type: comment
  author: LaferriereJC
  content: 'https://www.reddit.com/r/LocalLLaMA/comments/17djecw/mistral_3b/

    https://gist.github.com/thistleknot/48b4551737e72c039abf20b6b913b043'
  created_at: 2023-10-22 16:31:55+00:00
  edited: false
  hidden: false
  id: 65355c8bc65f2e7588bb1ea4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-10-22T17:31:55.000Z'
    data:
      status: closed
    id: 65355c8bc65f2e7588bb1ea6
    type: status-change
  author: LaferriereJC
  created_at: 2023-10-22 16:31:55+00:00
  id: 65355c8bc65f2e7588bb1ea6
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: openaccess-ai-collective/tiny-mistral
repo_type: model
status: closed
target_branch: null
title: possible to get a 1.1 and 3b version?
