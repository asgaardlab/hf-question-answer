!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AngledLuffa
conflicting_files: null
created_at: 2022-05-25 18:05:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
      fullname: John Bauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AngledLuffa
      type: user
    createdAt: '2022-05-25T19:05:07.000Z'
    data:
      edited: false
      editors:
      - AngledLuffa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
          fullname: John Bauer
          isHf: false
          isPro: false
          name: AngledLuffa
          type: user
        html: '<p>My workflow, at Stanford as part of our Stanza software, has been
          to load transformers &amp; tokenizers in a generic manner so that they can
          be used for various tasks in a variety of languages.  Part of that workflow
          is figuring out how long the input is by checking <code>model_max_length</code>.  Unfortunately,
          the config file has an extremely large value set for that field.  Can we
          get this updated?  Thanks.</p>

          '
        raw: My workflow, at Stanford as part of our Stanza software, has been to
          load transformers & tokenizers in a generic manner so that they can be used
          for various tasks in a variety of languages.  Part of that workflow is figuring
          out how long the input is by checking `model_max_length`.  Unfortunately,
          the config file has an extremely large value set for that field.  Can we
          get this updated?  Thanks.
        updatedAt: '2022-05-25T19:05:07.000Z'
      numEdits: 0
      reactions: []
    id: 628e7de3a9a3c754c1f887be
    type: comment
  author: AngledLuffa
  content: My workflow, at Stanford as part of our Stanza software, has been to load
    transformers & tokenizers in a generic manner so that they can be used for various
    tasks in a variety of languages.  Part of that workflow is figuring out how long
    the input is by checking `model_max_length`.  Unfortunately, the config file has
    an extremely large value set for that field.  Can we get this updated?  Thanks.
  created_at: 2022-05-25 18:05:07+00:00
  edited: false
  hidden: false
  id: 628e7de3a9a3c754c1f887be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2022-05-26T01:07:54.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>Great remark <span data-props=\"{&quot;user&quot;:&quot;AngledLuffa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/AngledLuffa\"\
          >@<span class=\"underline\">AngledLuffa</span></a></span>\n\n\t</span></span>\
          \ !</p>\n<p>Due to backwards breaking compability, I think it'll be difficult\
          \ to change <code>model_max_length</code> in the config as the users would\
          \ silently get shorter <code>input_ids</code>.<br>What do you think <span\
          \ data-props=\"{&quot;user&quot;:&quot;lysandre&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lysandre\">@<span class=\"\
          underline\">lysandre</span></a></span>\n\n\t</span></span> ?</p>\n<p>Note\
          \ that you can however, just set this variable in your script as follows:</p>\n\
          <pre><code class=\"language-py\">tok = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"google/muril-base-cased\"</span>, model_max_length=<span\
          \ class=\"hljs-number\">250</span>)\n</code></pre>\n<p>Would this work for\
          \ you? </p>\n"
        raw: 'Great remark @AngledLuffa !


          Due to backwards breaking compability, I think it''ll be difficult to change
          `model_max_length` in the config as the users would silently get shorter
          `input_ids`.

          What do you think @lysandre ?


          Note that you can however, just set this variable in your script as follows:


          ```py

          tok = AutoTokenizer.from_pretrained("google/muril-base-cased", model_max_length=250)

          ```


          Would this work for you? '
        updatedAt: '2022-05-26T01:07:54.000Z'
      numEdits: 0
      reactions: []
    id: 628ed2eae0994038b5f63ef1
    type: comment
  author: patrickvonplaten
  content: 'Great remark @AngledLuffa !


    Due to backwards breaking compability, I think it''ll be difficult to change `model_max_length`
    in the config as the users would silently get shorter `input_ids`.

    What do you think @lysandre ?


    Note that you can however, just set this variable in your script as follows:


    ```py

    tok = AutoTokenizer.from_pretrained("google/muril-base-cased", model_max_length=250)

    ```


    Would this work for you? '
  created_at: 2022-05-26 00:07:54+00:00
  edited: false
  hidden: false
  id: 628ed2eae0994038b5f63ef1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
      fullname: John Bauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AngledLuffa
      type: user
    createdAt: '2022-05-26T01:15:50.000Z'
    data:
      edited: false
      editors:
      - AngledLuffa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
          fullname: John Bauer
          isHf: false
          isPro: false
          name: AngledLuffa
          type: user
        html: '<p>I don''t believe there''s a circumstance under which the <code>input_ids</code>
          would be shorter.  If the input goes beyond the maximum length of the model,
          there''s actually an exception thrown.   This was how I first noticed the
          problem, actually; I had given it an input of ~900, and <code>transformers</code>
          reported an error with the position encoding.</p>

          '
        raw: I don't believe there's a circumstance under which the `input_ids` would
          be shorter.  If the input goes beyond the maximum length of the model, there's
          actually an exception thrown.   This was how I first noticed the problem,
          actually; I had given it an input of ~900, and `transformers` reported an
          error with the position encoding.
        updatedAt: '2022-05-26T01:15:50.000Z'
      numEdits: 0
      reactions: []
    id: 628ed4c68ce0f6815b394878
    type: comment
  author: AngledLuffa
  content: I don't believe there's a circumstance under which the `input_ids` would
    be shorter.  If the input goes beyond the maximum length of the model, there's
    actually an exception thrown.   This was how I first noticed the problem, actually;
    I had given it an input of ~900, and `transformers` reported an error with the
    position encoding.
  created_at: 2022-05-26 00:15:50+00:00
  edited: false
  hidden: false
  id: 628ed4c68ce0f6815b394878
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2022-05-26T01:18:49.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>Gottcha - that's a very valid reason then to update the tokenizer\
          \ to the model's max length.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;lysandre&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lysandre\"\
          >@<span class=\"underline\">lysandre</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;sgugger&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sgugger\">@<span class=\"\
          underline\">sgugger</span></a></span>\n\n\t</span></span> what do you think?\
          \ Update the tokenizer max length to the model max length at least here?</p>\n"
        raw: 'Gottcha - that''s a very valid reason then to update the tokenizer to
          the model''s max length.


          @lysandre and @sgugger what do you think? Update the tokenizer max length
          to the model max length at least here?'
        updatedAt: '2022-05-26T01:18:49.000Z'
      numEdits: 0
      reactions: []
    id: 628ed57954698ce61d1e9f15
    type: comment
  author: patrickvonplaten
  content: 'Gottcha - that''s a very valid reason then to update the tokenizer to
    the model''s max length.


    @lysandre and @sgugger what do you think? Update the tokenizer max length to the
    model max length at least here?'
  created_at: 2022-05-26 00:18:49+00:00
  edited: false
  hidden: false
  id: 628ed57954698ce61d1e9f15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2022-05-30T07:17:41.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: '<p>I think updating the tokenizer max length to the model max length
          makes sense in this situation given the model cannot handle a sequence longer
          than that number of tokens.</p>

          '
        raw: I think updating the tokenizer max length to the model max length makes
          sense in this situation given the model cannot handle a sequence longer
          than that number of tokens.
        updatedAt: '2022-05-30T07:17:41.000Z'
      numEdits: 0
      reactions: []
    id: 62946f95b570e95329128bb4
    type: comment
  author: lysandre
  content: I think updating the tokenizer max length to the model max length makes
    sense in this situation given the model cannot handle a sequence longer than that
    number of tokens.
  created_at: 2022-05-30 06:17:41+00:00
  edited: false
  hidden: false
  id: 62946f95b570e95329128bb4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2022-05-30T09:26:59.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>Cool! <span data-props=\"{&quot;user&quot;:&quot;AngledLuffa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/AngledLuffa\"\
          >@<span class=\"underline\">AngledLuffa</span></a></span>\n\n\t</span></span>\
          \ would you like to open a PR to do this or would you like us to help you\
          \ on it? :-) </p>\n"
        raw: 'Cool! @AngledLuffa would you like to open a PR to do this or would you
          like us to help you on it? :-) '
        updatedAt: '2022-05-30T09:26:59.000Z'
      numEdits: 0
      reactions: []
    id: 62948de3a1e875b26e929ef8
    type: comment
  author: patrickvonplaten
  content: 'Cool! @AngledLuffa would you like to open a PR to do this or would you
    like us to help you on it? :-) '
  created_at: 2022-05-30 08:26:59+00:00
  edited: false
  hidden: false
  id: 62948de3a1e875b26e929ef8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
      fullname: John Bauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AngledLuffa
      type: user
    createdAt: '2022-05-30T17:42:14.000Z'
    data:
      edited: false
      editors:
      - AngledLuffa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60fc7ae7eb985843aabf0775e9b95cef.svg
          fullname: John Bauer
          isHf: false
          isPro: false
          name: AngledLuffa
          type: user
        html: '<p>I am happy to defer to the experts on this model :)</p>

          '
        raw: I am happy to defer to the experts on this model :)
        updatedAt: '2022-05-30T17:42:14.000Z'
      numEdits: 0
      reactions: []
    id: 629501f6ed6dba733377852c
    type: comment
  author: AngledLuffa
  content: I am happy to defer to the experts on this model :)
  created_at: 2022-05-30 16:42:14+00:00
  edited: false
  hidden: false
  id: 629501f6ed6dba733377852c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: google/muril-base-cased
repo_type: model
status: open
target_branch: null
title: model_max_length effectively infinite
