!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MoritzLaurer
conflicting_files: null
created_at: 2022-12-21 08:22:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1613511937628-5fb15d1e84389b139cf3b508.jpeg?w=200&h=200&f=face
      fullname: Moritz Laurer
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MoritzLaurer
      type: user
    createdAt: '2022-12-21T08:22:13.000Z'
    data:
      edited: false
      editors:
      - MoritzLaurer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1613511937628-5fb15d1e84389b139cf3b508.jpeg?w=200&h=200&f=face
          fullname: Moritz Laurer
          isHf: true
          isPro: false
          name: MoritzLaurer
          type: user
        html: '<p>Really great paper and fascinating to see instruction finetuning
          also work for dense embedders. If I understand correctly, table 2 in the
          paper only displays the metrics for the two large models. can you also display
          the same metrics for this base sized model? table 2 only compares the large
          335M instructor model with other smaller 111M models, which seems like an
          unfair comparison. (Adding compute times for generating embeddings would
          also be interesting to assess practical utility)</p>

          '
        raw: Really great paper and fascinating to see instruction finetuning also
          work for dense embedders. If I understand correctly, table 2 in the paper
          only displays the metrics for the two large models. can you also display
          the same metrics for this base sized model? table 2 only compares the large
          335M instructor model with other smaller 111M models, which seems like an
          unfair comparison. (Adding compute times for generating embeddings would
          also be interesting to assess practical utility)
        updatedAt: '2022-12-21T08:22:13.268Z'
      numEdits: 0
      reactions: []
    id: 63a2c235c8a2aa5d9e8067a7
    type: comment
  author: MoritzLaurer
  content: Really great paper and fascinating to see instruction finetuning also work
    for dense embedders. If I understand correctly, table 2 in the paper only displays
    the metrics for the two large models. can you also display the same metrics for
    this base sized model? table 2 only compares the large 335M instructor model with
    other smaller 111M models, which seems like an unfair comparison. (Adding compute
    times for generating embeddings would also be interesting to assess practical
    utility)
  created_at: 2022-12-21 08:22:13+00:00
  edited: false
  hidden: false
  id: 63a2c235c8a2aa5d9e8067a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2022-12-21T11:12:46.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>Hi, Thanks a lot for your interests and comments on our paper!</p>

          <p>In table 2, we also provide metrics for large models, e.g., Sent-T5-XXL
          (4.8B), GTR-XXL (4.8B), etc, and our INSTRUCTOR (335M) outperforms both
          of them in terms of the average score across 9 categories (last column).
          </p>

          <p>In the next version, we will provide more results for the base sized
          model and compute times for generating embeddings!</p>

          '
        raw: "Hi, Thanks a lot for your interests and comments on our paper!\n\nIn\
          \ table 2, we also provide metrics for large models, e.g., Sent-T5-XXL (4.8B),\
          \ GTR-XXL (4.8B), etc, and our INSTRUCTOR (335M) outperforms both of them\
          \ in terms of the average score across 9 categories (last column). \n\n\
          In the next version, we will provide more results for the base sized model\
          \ and compute times for generating embeddings!"
        updatedAt: '2022-12-21T11:12:46.991Z'
      numEdits: 0
      reactions: []
    id: 63a2ea2e528eba15b13b3bc6
    type: comment
  author: multi-train
  content: "Hi, Thanks a lot for your interests and comments on our paper!\n\nIn table\
    \ 2, we also provide metrics for large models, e.g., Sent-T5-XXL (4.8B), GTR-XXL\
    \ (4.8B), etc, and our INSTRUCTOR (335M) outperforms both of them in terms of\
    \ the average score across 9 categories (last column). \n\nIn the next version,\
    \ we will provide more results for the base sized model and compute times for\
    \ generating embeddings!"
  created_at: 2022-12-21 11:12:46+00:00
  edited: false
  hidden: false
  id: 63a2ea2e528eba15b13b3bc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1613511937628-5fb15d1e84389b139cf3b508.jpeg?w=200&h=200&f=face
      fullname: Moritz Laurer
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MoritzLaurer
      type: user
    createdAt: '2022-12-21T12:25:15.000Z'
    data:
      edited: true
      editors:
      - MoritzLaurer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1613511937628-5fb15d1e84389b139cf3b508.jpeg?w=200&h=200&f=face
          fullname: Moritz Laurer
          isHf: true
          isPro: false
          name: MoritzLaurer
          type: user
        html: '<p>great to hear that you will add results on the base-sized model
          and compute times in the next paper version. Model size (and ensuing memory
          requirements) and embedding speed is quite important for large corpora and
          directly comparable metrics of roughly 111M sized-models would be great.
          in your paper you write that instructor-base has 0.1B params, which seems
          much more comparable to the 111M sized-models than instrucutor-large with
          335M params. Otherwise it feels a bit like you are comparing your own large
          apples with other people''s small oranges.</p>

          '
        raw: great to hear that you will add results on the base-sized model and compute
          times in the next paper version. Model size (and ensuing memory requirements)
          and embedding speed is quite important for large corpora and directly comparable
          metrics of roughly 111M sized-models would be great. in your paper you write
          that instructor-base has 0.1B params, which seems much more comparable to
          the 111M sized-models than instrucutor-large with 335M params. Otherwise
          it feels a bit like you are comparing your own large apples with other people's
          small oranges.
        updatedAt: '2022-12-21T12:26:08.200Z'
      numEdits: 1
      reactions: []
    id: 63a2fb2b528eba15b13d2ba4
    type: comment
  author: MoritzLaurer
  content: great to hear that you will add results on the base-sized model and compute
    times in the next paper version. Model size (and ensuing memory requirements)
    and embedding speed is quite important for large corpora and directly comparable
    metrics of roughly 111M sized-models would be great. in your paper you write that
    instructor-base has 0.1B params, which seems much more comparable to the 111M
    sized-models than instrucutor-large with 335M params. Otherwise it feels a bit
    like you are comparing your own large apples with other people's small oranges.
  created_at: 2022-12-21 12:25:15+00:00
  edited: true
  hidden: false
  id: 63a2fb2b528eba15b13d2ba4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
      fullname: Hongjin SU
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: multi-train
      type: user
    createdAt: '2022-12-21T12:50:05.000Z'
    data:
      edited: false
      editors:
      - multi-train
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7d0a895e669bcd1303c4716b5401c36.svg
          fullname: Hongjin SU
          isHf: false
          isPro: false
          name: multi-train
          type: user
        html: '<p>Hi, thanks a lot for your constructive feedbacks!</p>

          <p>We will have more discussions about efficiency (in both memory and embedding
          speed) in our next version. For your reference, it takes about 90 seconds
          for INSTRUCTOR-base to encode 40,000 documents in the natural question dataset
          on single 40GB A100 GPU. </p>

          <p>In table 2, our focus is to directly compare INSTRUCTOR(335M/1.5B) to
          GTR-Large/GTR-XL, as they have the same model size and architecture. In
          this case, I think they are fair comparison. In the next version, we will
          add more comparison on the base-sized models.</p>

          '
        raw: "Hi, thanks a lot for your constructive feedbacks!\n\nWe will have more\
          \ discussions about efficiency (in both memory and embedding speed) in our\
          \ next version. For your reference, it takes about 90 seconds for INSTRUCTOR-base\
          \ to encode 40,000 documents in the natural question dataset on single 40GB\
          \ A100 GPU. \n\nIn table 2, our focus is to directly compare INSTRUCTOR(335M/1.5B)\
          \ to GTR-Large/GTR-XL, as they have the same model size and architecture.\
          \ In this case, I think they are fair comparison. In the next version, we\
          \ will add more comparison on the base-sized models."
        updatedAt: '2022-12-21T12:50:05.911Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MoritzLaurer
    id: 63a300fdcc2140620596f95e
    type: comment
  author: multi-train
  content: "Hi, thanks a lot for your constructive feedbacks!\n\nWe will have more\
    \ discussions about efficiency (in both memory and embedding speed) in our next\
    \ version. For your reference, it takes about 90 seconds for INSTRUCTOR-base to\
    \ encode 40,000 documents in the natural question dataset on single 40GB A100\
    \ GPU. \n\nIn table 2, our focus is to directly compare INSTRUCTOR(335M/1.5B)\
    \ to GTR-Large/GTR-XL, as they have the same model size and architecture. In this\
    \ case, I think they are fair comparison. In the next version, we will add more\
    \ comparison on the base-sized models."
  created_at: 2022-12-21 12:50:05+00:00
  edited: false
  hidden: false
  id: 63a300fdcc2140620596f95e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: hkunlp/instructor-base
repo_type: model
status: open
target_branch: null
title: display benchmark metrics for base sized model
