!!python/object:huggingface_hub.community.DiscussionWithDetails
author: buchylx
conflicting_files: null
created_at: 2023-07-03 11:35:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/39f2c95919d07814c9ce2ceefa579006.svg
      fullname: buchylx
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: buchylx
      type: user
    createdAt: '2023-07-03T12:35:27.000Z'
    data:
      edited: false
      editors:
      - buchylx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8981550931930542
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/39f2c95919d07814c9ce2ceefa579006.svg
          fullname: buchylx
          isHf: false
          isPro: false
          name: buchylx
          type: user
        html: '<p>Hi mate, when I use the example code, I realized that the output
          of the model is [prompt + response], how can I force the model only output
          its response?<br>Thank you</p>

          '
        raw: "Hi mate, when I use the example code, I realized that the output of\
          \ the model is [prompt + response], how can I force the model only output\
          \ its response? \r\nThank you"
        updatedAt: '2023-07-03T12:35:27.375Z'
      numEdits: 0
      reactions: []
    id: 64a2c08f635bebfdbd593fbb
    type: comment
  author: buchylx
  content: "Hi mate, when I use the example code, I realized that the output of the\
    \ model is [prompt + response], how can I force the model only output its response?\
    \ \r\nThank you"
  created_at: 2023-07-03 11:35:27+00:00
  edited: false
  hidden: false
  id: 64a2c08f635bebfdbd593fbb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/39f2c95919d07814c9ce2ceefa579006.svg
      fullname: buchylx
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: buchylx
      type: user
    createdAt: '2023-07-03T12:50:31.000Z'
    data:
      edited: false
      editors:
      - buchylx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8585147857666016
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/39f2c95919d07814c9ce2ceefa579006.svg
          fullname: buchylx
          isHf: false
          isPro: false
          name: buchylx
          type: user
        html: '<p>And also, is it possible to get stream output?</p>

          '
        raw: And also, is it possible to get stream output?
        updatedAt: '2023-07-03T12:50:31.293Z'
      numEdits: 0
      reactions: []
    id: 64a2c41734ae4da8c16e2c2e
    type: comment
  author: buchylx
  content: And also, is it possible to get stream output?
  created_at: 2023-07-03 11:50:31+00:00
  edited: false
  hidden: false
  id: 64a2c41734ae4da8c16e2c2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/39f2c95919d07814c9ce2ceefa579006.svg
      fullname: buchylx
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: buchylx
      type: user
    createdAt: '2023-07-04T03:00:48.000Z'
    data:
      edited: false
      editors:
      - buchylx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2804199457168579
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/39f2c95919d07814c9ce2ceefa579006.svg
          fullname: buchylx
          isHf: false
          isPro: false
          name: buchylx
          type: user
        html: '<p>Answer to Q1: we can add a parameter "return_full_text=False" when
          we call it. </p>

          '
        raw: 'Answer to Q1: we can add a parameter "return_full_text=False" when we
          call it. '
        updatedAt: '2023-07-04T03:00:48.015Z'
      numEdits: 0
      reactions: []
    id: 64a38b6078edd17f964e563c
    type: comment
  author: buchylx
  content: 'Answer to Q1: we can add a parameter "return_full_text=False" when we
    call it. '
  created_at: 2023-07-04 02:00:48+00:00
  edited: false
  hidden: false
  id: 64a38b6078edd17f964e563c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-08T09:28:50.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8373875021934509
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah return_full_text is what I''d recommend.</p>

          <p>For streaming output, that''s a bit more complex.  Have a look at how
          text-generation-webui does it, in files <code>modules/text_generation.py</code>.
          starting line 281 in the current file.  It''s not too complex. </p>

          '
        raw: 'Yeah return_full_text is what I''d recommend.


          For streaming output, that''s a bit more complex.  Have a look at how text-generation-webui
          does it, in files `modules/text_generation.py`. starting line 281 in the
          current file.  It''s not too complex. '
        updatedAt: '2023-07-08T09:28:50.322Z'
      numEdits: 0
      reactions: []
    id: 64a92c5204e7b379feb3170c
    type: comment
  author: TheBloke
  content: 'Yeah return_full_text is what I''d recommend.


    For streaming output, that''s a bit more complex.  Have a look at how text-generation-webui
    does it, in files `modules/text_generation.py`. starting line 281 in the current
    file.  It''s not too complex. '
  created_at: 2023-07-08 08:28:50+00:00
  edited: false
  hidden: false
  id: 64a92c5204e7b379feb3170c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Guanaco-33B-SuperHOT-8K-GPTQ
repo_type: model
status: open
target_branch: null
title: How to make the model only output the "response" part?
