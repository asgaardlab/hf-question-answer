!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bishmdl
conflicting_files: null
created_at: 2023-03-12 19:19:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0558066c300bc0c8098b160551254ede.svg
      fullname: Bishwas Mandal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bishmdl
      type: user
    createdAt: '2023-03-12T20:19:10.000Z'
    data:
      edited: false
      editors:
      - bishmdl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0558066c300bc0c8098b160551254ede.svg
          fullname: Bishwas Mandal
          isHf: false
          isPro: false
          name: bishmdl
          type: user
        html: '<p>I tried to use the hosted inference API for Align model, but it
          is not working. Error message received: The model_type ''align'' is not
          recognized. It could be a bleeding edge model, or incorrect.</p>

          <p>I have tried to import AlignModel and AlignProcessor from the transformers
          library, and i get Import Error there as well. There seems to be some error
          in the model. Any updates/help will be highly appreciated! </p>

          '
        raw: "I tried to use the hosted inference API for Align model, but it is not\
          \ working. Error message received: The model_type 'align' is not recognized.\
          \ It could be a bleeding edge model, or incorrect.\r\n\r\nI have tried to\
          \ import AlignModel and AlignProcessor from the transformers library, and\
          \ i get Import Error there as well. There seems to be some error in the\
          \ model. Any updates/help will be highly appreciated! "
        updatedAt: '2023-03-12T20:19:10.119Z'
      numEdits: 0
      reactions: []
    id: 640e33be8512ec51d7f36563
    type: comment
  author: bishmdl
  content: "I tried to use the hosted inference API for Align model, but it is not\
    \ working. Error message received: The model_type 'align' is not recognized. It\
    \ could be a bleeding edge model, or incorrect.\r\n\r\nI have tried to import\
    \ AlignModel and AlignProcessor from the transformers library, and i get Import\
    \ Error there as well. There seems to be some error in the model. Any updates/help\
    \ will be highly appreciated! "
  created_at: 2023-03-12 19:19:10+00:00
  edited: false
  hidden: false
  id: 640e33be8512ec51d7f36563
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661316320697-6305aafd85c9f507e355d4a0.png?w=200&h=200&f=face
      fullname: Minwoo Byeon
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dylan-m
      type: user
    createdAt: '2023-03-13T10:30:23.000Z'
    data:
      edited: false
      editors:
      - dylan-m
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661316320697-6305aafd85c9f507e355d4a0.png?w=200&h=200&f=face
          fullname: Minwoo Byeon
          isHf: false
          isPro: false
          name: dylan-m
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;bishmdl&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/bishmdl\">@<span class=\"\
          underline\">bishmdl</span></a></span>\n\n\t</span></span> </p>\n<p>The transformers\
          \ library with the ALIGN model is not yet released and is only on the main\
          \ branch,<br>so to use the current ALIGN model, do <code>pip install git+https://github.com/huggingface/transformers</code><br>\
          \ command to install and use transformers.</p>\n<pre><code class=\"language-py\"\
          >\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"\
          hljs-keyword\">import</span> AlignProcessor, AlignModel\n\nprocessor = AlignProcessor.from_pretrained(<span\
          \ class=\"hljs-string\">\"kakaobrain/align-base\"</span>)\nmodel = AlignModel.from_pretrained(<span\
          \ class=\"hljs-string\">\"kakaobrain/align-base\"</span>)\n\n<span class=\"\
          hljs-string\">\"\"\" </span>\n<span class=\"hljs-string\">Downloading (\u2026\
          )rocessor_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 508/508 [00:00&lt;00:00, 59.5kB/s]</span>\n\
          <span class=\"hljs-string\">Downloading (\u2026)okenizer_config.json: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 399/399 [00:00&lt;00:00, 53.4kB/s]</span>\n<span class=\"hljs-string\"\
          >Downloading (\u2026)solve/main/vocab.txt: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 232k/232k [00:00&lt;00:00,\
          \ 279kB/s]</span>\n<span class=\"hljs-string\">Downloading (\u2026)cial_tokens_map.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 125/125 [00:00&lt;00:00, 49.1kB/s]</span>\n<span class=\"\
          hljs-string\">Downloading (\u2026)lve/main/config.json: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.25k/5.25k [00:00&lt;00:00,\
          \ 660kB/s]</span>\n<span class=\"hljs-string\">Downloading pytorch_model.bin:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 690M/690M [00:31&lt;00:00, 22.1MB/s]</span>\n\
          <span class=\"hljs-string\">\"\"\"</span>\n</code></pre>\n"
        raw: "@bishmdl \n\nThe transformers library with the ALIGN model is not yet\
          \ released and is only on the main branch, \nso to use the current ALIGN\
          \ model, do `pip install git+https://github.com/huggingface/transformers`\n\
          \ command to install and use transformers.\n\n```py\n\nfrom transformers\
          \ import AlignProcessor, AlignModel\n\nprocessor = AlignProcessor.from_pretrained(\"\
          kakaobrain/align-base\")\nmodel = AlignModel.from_pretrained(\"kakaobrain/align-base\"\
          )\n\n\"\"\" \nDownloading (\u2026)rocessor_config.json: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 508/508 [00:00<00:00, 59.5kB/s]\nDownloading (\u2026)okenizer_config.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 399/399 [00:00<00:00, 53.4kB/s]\nDownloading (\u2026\
          )solve/main/vocab.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 232k/232k [00:00<00:00, 279kB/s]\nDownloading\
          \ (\u2026)cial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [00:00<00:00,\
          \ 49.1kB/s]\nDownloading (\u2026)lve/main/config.json: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.25k/5.25k [00:00<00:00,\
          \ 660kB/s]\nDownloading pytorch_model.bin: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 690M/690M [00:31<00:00, 22.1MB/s]\n\"\"\"\n```"
        updatedAt: '2023-03-13T10:30:23.711Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - adirik
        - bishmdl
    id: 640efb3f10460f5e6238395b
    type: comment
  author: dylan-m
  content: "@bishmdl \n\nThe transformers library with the ALIGN model is not yet\
    \ released and is only on the main branch, \nso to use the current ALIGN model,\
    \ do `pip install git+https://github.com/huggingface/transformers`\n command to\
    \ install and use transformers.\n\n```py\n\nfrom transformers import AlignProcessor,\
    \ AlignModel\n\nprocessor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\"\
    )\nmodel = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n\n\"\"\" \n\
    Downloading (\u2026)rocessor_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 508/508 [00:00<00:00, 59.5kB/s]\nDownloading (\u2026\
    )okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588| 399/399 [00:00<00:00, 53.4kB/s]\nDownloading (\u2026)solve/main/vocab.txt:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 232k/232k\
    \ [00:00<00:00, 279kB/s]\nDownloading (\u2026)cial_tokens_map.json: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [00:00<00:00,\
    \ 49.1kB/s]\nDownloading (\u2026)lve/main/config.json: 100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 5.25k/5.25k [00:00<00:00, 660kB/s]\nDownloading\
    \ pytorch_model.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 690M/690M [00:31<00:00, 22.1MB/s]\n\"\"\"\
    \n```"
  created_at: 2023-03-13 09:30:23+00:00
  edited: false
  hidden: false
  id: 640efb3f10460f5e6238395b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af369d95e9b4a8a3ab90c3c854ee423e.svg
      fullname: Rabiul Awal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rabiulawal
      type: user
    createdAt: '2023-03-15T06:55:11.000Z'
    data:
      edited: false
      editors:
      - rabiulawal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af369d95e9b4a8a3ab90c3c854ee423e.svg
          fullname: Rabiul Awal
          isHf: false
          isPro: false
          name: rabiulawal
          type: user
        html: '<p>I think something is wrong with the weight initialization of the
          ALIGN model class. It shows me a warning to train all the layers since they
          are not initialized!</p>

          '
        raw: I think something is wrong with the weight initialization of the ALIGN
          model class. It shows me a warning to train all the layers since they are
          not initialized!
        updatedAt: '2023-03-15T06:55:11.152Z'
      numEdits: 0
      reactions: []
    id: 64116bcfc100524d3ce24775
    type: comment
  author: rabiulawal
  content: I think something is wrong with the weight initialization of the ALIGN
    model class. It shows me a warning to train all the layers since they are not
    initialized!
  created_at: 2023-03-15 05:55:11+00:00
  edited: false
  hidden: false
  id: 64116bcfc100524d3ce24775
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0e07353a8db97b3bef3b1d411c2d9fac.svg
      fullname: Rosenberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sersh
      type: user
    createdAt: '2023-03-25T14:06:23.000Z'
    data:
      edited: false
      editors:
      - Sersh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0e07353a8db97b3bef3b1d411c2d9fac.svg
          fullname: Rosenberg
          isHf: false
          isPro: false
          name: Sersh
          type: user
        html: '<p>How to fine-tune this model?</p>

          '
        raw: How to fine-tune this model?
        updatedAt: '2023-03-25T14:06:23.259Z'
      numEdits: 0
      reactions: []
    id: 641effdf62b95850b7f81517
    type: comment
  author: Sersh
  content: How to fine-tune this model?
  created_at: 2023-03-25 13:06:23+00:00
  edited: false
  hidden: false
  id: 641effdf62b95850b7f81517
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0558066c300bc0c8098b160551254ede.svg
      fullname: Bishwas Mandal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bishmdl
      type: user
    createdAt: '2023-03-25T15:38:33.000Z'
    data:
      edited: true
      editors:
      - bishmdl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0558066c300bc0c8098b160551254ede.svg
          fullname: Bishwas Mandal
          isHf: false
          isPro: false
          name: bishmdl
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Sersh&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sersh\">@<span class=\"\
          underline\">Sersh</span></a></span>\n\n\t</span></span> You can fine-tune\
          \ this model in the same way you fine-tune other PyTorch models. Here's\
          \ one way you can do it:</p>\n<pre><code class=\"language-py\"><span class=\"\
          hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">import</span>\
          \ torch.nn <span class=\"hljs-keyword\">as</span> nn\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AlignModel\n\
          \n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\"\
          >AlignClassifier</span>(nn.module):\n    <span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\"\
          >self, num_classes</span>):\n        <span class=\"hljs-built_in\">super</span>(AlignClassifier,\
          \ self).__init__()\n        self.model = AlignModel.from_pretrained(<span\
          \ class=\"hljs-string\">\"kakaobrain/align-base\"</span>)\n        <span\
          \ class=\"hljs-comment\"># embedding size of Align Model is 640 for each\
          \ modality.</span>\n        hidden_size = <span class=\"hljs-number\">640</span>\
          \ + <span class=\"hljs-number\">640</span>\n        self.fc = nn.Linear(hidden_size,\
          \ num_classes)\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">forward</span>(<span class=\"hljs-params\">self,\
          \ **out</span>):\n        outputs = self.model(**out)\n        image_embeds\
          \ = outputs.image_embeds\n        text_embeds = outputs.text_embeds\n  \
          \      <span class=\"hljs-comment\"># concatenate both embeddings</span>\n\
          \        embeds = torch.cat((image_embeds, text_embeds), dim=<span class=\"\
          hljs-number\">1</span>)\n        outputs = self.fc(embeds)\n        <span\
          \ class=\"hljs-keyword\">return</span> outputs\n</code></pre>\n"
        raw: "@Sersh You can fine-tune this model in the same way you fine-tune other\
          \ PyTorch models. Here's one way you can do it:\n\n```py\nimport torch\n\
          import torch.nn as nn\nfrom transformers import AlignModel\n\nclass AlignClassifier(nn.module):\n\
          \    def __init__(self, num_classes):\n        super(AlignClassifier, self).__init__()\n\
          \        self.model = AlignModel.from_pretrained(\"kakaobrain/align-base\"\
          )\n        # embedding size of Align Model is 640 for each modality.\n \
          \       hidden_size = 640 + 640\n        self.fc = nn.Linear(hidden_size,\
          \ num_classes)\n\n    def forward(self, **out):\n        outputs = self.model(**out)\n\
          \        image_embeds = outputs.image_embeds\n        text_embeds = outputs.text_embeds\n\
          \        # concatenate both embeddings\n        embeds = torch.cat((image_embeds,\
          \ text_embeds), dim=1)\n        outputs = self.fc(embeds)\n        return\
          \ outputs\n```"
        updatedAt: '2023-03-25T15:40:05.031Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Sersh
        - dylan-m
    id: 641f1579485c37e54d6b01d7
    type: comment
  author: bishmdl
  content: "@Sersh You can fine-tune this model in the same way you fine-tune other\
    \ PyTorch models. Here's one way you can do it:\n\n```py\nimport torch\nimport\
    \ torch.nn as nn\nfrom transformers import AlignModel\n\nclass AlignClassifier(nn.module):\n\
    \    def __init__(self, num_classes):\n        super(AlignClassifier, self).__init__()\n\
    \        self.model = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n\
    \        # embedding size of Align Model is 640 for each modality.\n        hidden_size\
    \ = 640 + 640\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def\
    \ forward(self, **out):\n        outputs = self.model(**out)\n        image_embeds\
    \ = outputs.image_embeds\n        text_embeds = outputs.text_embeds\n        #\
    \ concatenate both embeddings\n        embeds = torch.cat((image_embeds, text_embeds),\
    \ dim=1)\n        outputs = self.fc(embeds)\n        return outputs\n```"
  created_at: 2023-03-25 14:38:33+00:00
  edited: true
  hidden: false
  id: 641f1579485c37e54d6b01d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0558066c300bc0c8098b160551254ede.svg
      fullname: Bishwas Mandal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bishmdl
      type: user
    createdAt: '2023-03-25T15:44:03.000Z'
    data:
      edited: false
      editors:
      - bishmdl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0558066c300bc0c8098b160551254ede.svg
          fullname: Bishwas Mandal
          isHf: false
          isPro: false
          name: bishmdl
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;rabiulawal&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rabiulawal\">@<span class=\"\
          underline\">rabiulawal</span></a></span>\n\n\t</span></span> If you use\
          \ the <code>from_pretrained</code> method correctly, I think the model should\
          \ have trained weights. Make sure you are not initializing using <code>config</code>\
          \ as that will essentially give you only the architecture as per your configurations\
          \ and the weights are randomly initialized.</p>\n"
        raw: '@rabiulawal If you use the ```from_pretrained``` method correctly, I
          think the model should have trained weights. Make sure you are not initializing
          using ```config``` as that will essentially give you only the architecture
          as per your configurations and the weights are randomly initialized.'
        updatedAt: '2023-03-25T15:44:03.233Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dylan-m
    id: 641f16c3485c37e54d6b0e5a
    type: comment
  author: bishmdl
  content: '@rabiulawal If you use the ```from_pretrained``` method correctly, I think
    the model should have trained weights. Make sure you are not initializing using
    ```config``` as that will essentially give you only the architecture as per your
    configurations and the weights are randomly initialized.'
  created_at: 2023-03-25 14:44:03+00:00
  edited: false
  hidden: false
  id: 641f16c3485c37e54d6b0e5a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: kakaobrain/align-base
repo_type: model
status: open
target_branch: null
title: Model not working
