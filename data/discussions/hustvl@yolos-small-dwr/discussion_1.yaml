!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mhyatt000
conflicting_files: null
created_at: 2022-09-01 13:51:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658862186149-62b09fe1a14cbd64386c042d.jpeg?w=200&h=200&f=face
      fullname: Matt Hyatt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mhyatt000
      type: user
    createdAt: '2022-09-01T14:51:40.000Z'
    data:
      edited: false
      editors:
      - mhyatt000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658862186149-62b09fe1a14cbd64386c042d.jpeg?w=200&h=200&f=face
          fullname: Matt Hyatt
          isHf: false
          isPro: false
          name: mhyatt000
          type: user
        html: "<p>I tried to reproduce the results mentioned on this model card. Seems\
          \ like my results do not match the claimed mAP in the model card. I cannot\
          \ figure out how to get the correct numbers, can you help me find my mistake?</p>\n\
          <ul>\n<li>Claimed mAP: 37.6</li>\n<li>Recieved mAP: 33.2</li>\n</ul>\n<p>Here\
          \ are the details for my validation:</p>\n<ul>\n<li>I instantiate pre-trained\
          \ model with <code>transformers.pipeline()</code> and use COCO API to calculate\
          \ AP from detection bboxes. </li>\n<li>Evaluation was performed on macOS\
          \ CPU.</li>\n<li>Dataset was downloaded from cocodataset.org</li>\n</ul>\n\
          <hr>\n<pre><code> Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all\
          \ | maxDets=100 ] = 0.332\n Average Precision  (AP) @[ IoU=0.50      | area=\
          \   all | maxDets=100 ] = 0.530\n Average Precision  (AP) @[ IoU=0.75  \
          \    | area=   all | maxDets=100 ] = 0.340\n Average Precision  (AP) @[\
          \ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.115\n Average Precision\
          \  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.352\n Average\
          \ Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.538\n\
          \ Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ]\
          \ = 0.282\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=\
          \ 10 ] = 0.411\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all\
          \ | maxDets=100 ] = 0.423\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=\
          \ small | maxDets=100 ] = 0.161\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area=medium | maxDets=100 ] = 0.454\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area= large | maxDets=100 ] = 0.661\n</code></pre>\n"
        raw: "I tried to reproduce the results mentioned on this model card. Seems\
          \ like my results do not match the claimed mAP in the model card. I cannot\
          \ figure out how to get the correct numbers, can you help me find my mistake?\r\
          \n\r\n- Claimed mAP: 37.6\r\n- Recieved mAP: 33.2\r\n\r\nHere are the details\
          \ for my validation:\r\n\r\n- I instantiate pre-trained model with `transformers.pipeline()`\
          \ and use COCO API to calculate AP from detection bboxes. \r\n- Evaluation\
          \ was performed on macOS CPU.\r\n- Dataset was downloaded from cocodataset.org\r\
          \n---\r\n\r\n```\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=  \
          \ all | maxDets=100 ] = 0.332\r\n Average Precision  (AP) @[ IoU=0.50  \
          \    | area=   all | maxDets=100 ] = 0.530\r\n Average Precision  (AP) @[\
          \ IoU=0.75      | area=   all | maxDets=100 ] = 0.340\r\n Average Precision\
          \  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.115\r\n Average\
          \ Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.352\r\
          \n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100\
          \ ] = 0.538\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all |\
          \ maxDets=  1 ] = 0.282\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=\
          \   all | maxDets= 10 ] = 0.411\r\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area=   all | maxDets=100 ] = 0.423\r\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area= small | maxDets=100 ] = 0.161\r\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area=medium | maxDets=100 ] = 0.454\r\n Average Recall     (AR) @[ IoU=0.50:0.95\
          \ | area= large | maxDets=100 ] = 0.661\r\n```"
        updatedAt: '2022-09-01T14:51:40.763Z'
      numEdits: 0
      reactions: []
    id: 6310c6fce87051f3e3e3faa4
    type: comment
  author: mhyatt000
  content: "I tried to reproduce the results mentioned on this model card. Seems like\
    \ my results do not match the claimed mAP in the model card. I cannot figure out\
    \ how to get the correct numbers, can you help me find my mistake?\r\n\r\n- Claimed\
    \ mAP: 37.6\r\n- Recieved mAP: 33.2\r\n\r\nHere are the details for my validation:\r\
    \n\r\n- I instantiate pre-trained model with `transformers.pipeline()` and use\
    \ COCO API to calculate AP from detection bboxes. \r\n- Evaluation was performed\
    \ on macOS CPU.\r\n- Dataset was downloaded from cocodataset.org\r\n---\r\n\r\n\
    ```\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ]\
    \ = 0.332\r\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100\
    \ ] = 0.530\r\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100\
    \ ] = 0.340\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100\
    \ ] = 0.115\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100\
    \ ] = 0.352\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100\
    \ ] = 0.538\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=\
    \  1 ] = 0.282\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=\
    \ 10 ] = 0.411\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100\
    \ ] = 0.423\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100\
    \ ] = 0.161\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100\
    \ ] = 0.454\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100\
    \ ] = 0.661\r\n```"
  created_at: 2022-09-01 13:51:40+00:00
  edited: false
  hidden: false
  id: 6310c6fce87051f3e3e3faa4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: hustvl/yolos-small-dwr
repo_type: model
status: open
target_branch: null
title: mAP Drop
