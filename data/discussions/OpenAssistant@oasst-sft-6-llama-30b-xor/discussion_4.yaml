!!python/object:huggingface_hub.community.DiscussionWithDetails
author: daryl149
conflicting_files: null
created_at: 2023-04-23 08:54:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-04-23T09:54:14.000Z'
    data:
      edited: true
      editors:
      - daryl149
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
          fullname: Daryl Autar
          isHf: false
          isPro: false
          name: daryl149
          type: user
        html: '<p>To convert the original llama 30B weights to HF format, it needs
          to fit the entire model into RAM (PC, not GPU VRAM, see <a href="https://huggingface.co/docs/transformers/main/model_doc/llama">https://huggingface.co/docs/transformers/main/model_doc/llama</a>).<br>That''s
          more than 65GB of RAM. Chances are as a hobbyist you don''t have that in
          your PC. If you: </p>

          <ul>

          <li>run <code>python src/transformers/models/llama/convert_llama_weights_to_hf.py
          --input_dir /path/to/LLaMA  --output_dir /path/to/converted_llama_hf --model_size
          30B</code> </li>

          <li>see your RAM go up to 100% in the activity monitor,</li>

          <li>your PC freezes for a bit,  </li>

          <li>and then you see <code>killed</code> in the terminal,</li>

          </ul>

          <p>that''s your cue.<br>Instead of buying and upgrading the total RAM in
          your system for this one time, you can instead increase the swap file of
          your PC to offload objects that are larger than your RAM onto your SSD/HDD.
          So this method does require you have at least a spare 100GB of disk space
          available. (More likely than having 128GB RAM sitting in your PC.)</p>

          <p>Here''s how to increase swap for Ubuntu:</p>

          <pre><code>#check your current swap size

          free -h

          #turn off your current swap

          sudo swapoff -a

          #increase swap to 100GB to be able to offload the entire model from RAM
          to disk

          sudo fallocate -l 100G /swapfile

          #make sure swapfile permissions are set, then activate

          sudo chmod 600 /swapfile

          sudo mkswap /swapfile

          sudo swapon /swapfile

          #check new swap size (should say something like 97Gi)

          free -h

          </code></pre>

          <p>Congrats, you can now run <code>python src/transformers/models/llama/convert_llama_weights_to_hf.py
          --input_dir /path/to/LLaMA  --output_dir /path/to/converted_llama_hf --model_size
          30B</code> without buying more RAM.<br>It should be done in an hour or so,
          which is slower than usual due to swapping between RAM and disk, but still
          very manageable for a one time operation.<br>(If you''d like, you can decrease
          your swap partition and free up disk space again.)</p>

          '
        raw: "To convert the original llama 30B weights to HF format, it needs to\
          \ fit the entire model into RAM (PC, not GPU VRAM, see https://huggingface.co/docs/transformers/main/model_doc/llama).\n\
          That's more than 65GB of RAM. Chances are as a hobbyist you don't have that\
          \ in your PC. If you: \n- run `python src/transformers/models/llama/convert_llama_weights_to_hf.py\
          \ --input_dir /path/to/LLaMA  --output_dir /path/to/converted_llama_hf --model_size\
          \ 30B` \n- see your RAM go up to 100% in the activity monitor,\n- your PC\
          \ freezes for a bit,  \n- and then you see `killed` in the terminal, \n\n\
          that's your cue.\nInstead of buying and upgrading the total RAM in your\
          \ system for this one time, you can instead increase the swap file of your\
          \ PC to offload objects that are larger than your RAM onto your SSD/HDD.\
          \ So this method does require you have at least a spare 100GB of disk space\
          \ available. (More likely than having 128GB RAM sitting in your PC.)\n\n\
          Here's how to increase swap for Ubuntu:\n```\n#check your current swap size\n\
          free -h\n#turn off your current swap\nsudo swapoff -a\n#increase swap to\
          \ 100GB to be able to offload the entire model from RAM to disk\nsudo fallocate\
          \ -l 100G /swapfile\n#make sure swapfile permissions are set, then activate\n\
          sudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n\
          #check new swap size (should say something like 97Gi)\nfree -h\n```\n\n\
          Congrats, you can now run `python src/transformers/models/llama/convert_llama_weights_to_hf.py\
          \ --input_dir /path/to/LLaMA  --output_dir /path/to/converted_llama_hf --model_size\
          \ 30B` without buying more RAM.\nIt should be done in an hour or so, which\
          \ is slower than usual due to swapping between RAM and disk, but still very\
          \ manageable for a one time operation.\n(If you'd like, you can decrease\
          \ your swap partition and free up disk space again.)"
        updatedAt: '2023-04-23T09:57:33.318Z'
      numEdits: 2
      reactions:
      - count: 9
        reaction: "\U0001F44D"
        users:
        - andreaskoepf
        - Blacky372
        - apoorvagni
        - nikolab
        - MatthewK
        - Darkhand-786
        - GabbyJay
        - andreasxPol
        - rjtmehta99
    id: 644500467a7b94ddc2d1ff9b
    type: comment
  author: daryl149
  content: "To convert the original llama 30B weights to HF format, it needs to fit\
    \ the entire model into RAM (PC, not GPU VRAM, see https://huggingface.co/docs/transformers/main/model_doc/llama).\n\
    That's more than 65GB of RAM. Chances are as a hobbyist you don't have that in\
    \ your PC. If you: \n- run `python src/transformers/models/llama/convert_llama_weights_to_hf.py\
    \ --input_dir /path/to/LLaMA  --output_dir /path/to/converted_llama_hf --model_size\
    \ 30B` \n- see your RAM go up to 100% in the activity monitor,\n- your PC freezes\
    \ for a bit,  \n- and then you see `killed` in the terminal, \n\nthat's your cue.\n\
    Instead of buying and upgrading the total RAM in your system for this one time,\
    \ you can instead increase the swap file of your PC to offload objects that are\
    \ larger than your RAM onto your SSD/HDD. So this method does require you have\
    \ at least a spare 100GB of disk space available. (More likely than having 128GB\
    \ RAM sitting in your PC.)\n\nHere's how to increase swap for Ubuntu:\n```\n#check\
    \ your current swap size\nfree -h\n#turn off your current swap\nsudo swapoff -a\n\
    #increase swap to 100GB to be able to offload the entire model from RAM to disk\n\
    sudo fallocate -l 100G /swapfile\n#make sure swapfile permissions are set, then\
    \ activate\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n\
    #check new swap size (should say something like 97Gi)\nfree -h\n```\n\nCongrats,\
    \ you can now run `python src/transformers/models/llama/convert_llama_weights_to_hf.py\
    \ --input_dir /path/to/LLaMA  --output_dir /path/to/converted_llama_hf --model_size\
    \ 30B` without buying more RAM.\nIt should be done in an hour or so, which is\
    \ slower than usual due to swapping between RAM and disk, but still very manageable\
    \ for a one time operation.\n(If you'd like, you can decrease your swap partition\
    \ and free up disk space again.)"
  created_at: 2023-04-23 08:54:14+00:00
  edited: true
  hidden: false
  id: 644500467a7b94ddc2d1ff9b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-04-23T10:20:19.000Z'
    data:
      from: Trying to convert the LlaMa weight to HF and running out of RAM, but don't
        want to buy more RAM?
      to: Trying to convert LlaMa weights to HF and running out of RAM, but don't
        want to buy more RAM?
    id: 644506630f2fc80feb1ff428
    type: title-change
  author: daryl149
  created_at: 2023-04-23 09:20:19+00:00
  id: 644506630f2fc80feb1ff428
  new_title: Trying to convert LlaMa weights to HF and running out of RAM, but don't
    want to buy more RAM?
  old_title: Trying to convert the LlaMa weight to HF and running out of RAM, but
    don't want to buy more RAM?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8bc783f10a9763a9198e93daeaee87f.svg
      fullname: Samir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mahmoud1234
      type: user
    createdAt: '2023-04-23T13:15:51.000Z'
    data:
      edited: false
      editors:
      - Mahmoud1234
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8bc783f10a9763a9198e93daeaee87f.svg
          fullname: Samir
          isHf: false
          isPro: false
          name: Mahmoud1234
          type: user
        html: "<p>please <span data-props=\"{&quot;user&quot;:&quot;daryl149&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/daryl149\"\
          >@<span class=\"underline\">daryl149</span></a></span>\n\n\t</span></span>,\
          \ To run this command</p>\n<p>python src/transformers/models/llama/convert_llama_weights_to_hf.py\
          \ --input_dir /path/to/LLaMA --output_dir /path/to/converted_llama_hf --model_size\
          \ 30B</p>\n<p>how to know input_dir, output_dir  exactly in my system, or\
          \ should I create them ?</p>\n"
        raw: 'please @daryl149, To run this command


          python src/transformers/models/llama/convert_llama_weights_to_hf.py --input_dir
          /path/to/LLaMA --output_dir /path/to/converted_llama_hf --model_size 30B


          how to know input_dir, output_dir  exactly in my system, or should I create
          them ?'
        updatedAt: '2023-04-23T13:15:51.147Z'
      numEdits: 0
      reactions: []
    id: 64452f87f993c804b032c563
    type: comment
  author: Mahmoud1234
  content: 'please @daryl149, To run this command


    python src/transformers/models/llama/convert_llama_weights_to_hf.py --input_dir
    /path/to/LLaMA --output_dir /path/to/converted_llama_hf --model_size 30B


    how to know input_dir, output_dir  exactly in my system, or should I create them
    ?'
  created_at: 2023-04-23 12:15:51+00:00
  edited: false
  hidden: false
  id: 64452f87f993c804b032c563
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-04-23T13:53:47.000Z'
    data:
      edited: false
      editors:
      - daryl149
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
          fullname: Daryl Autar
          isHf: false
          isPro: false
          name: daryl149
          type: user
        html: '<p>The input_dir is the folder called LLaMA where you saved the original
          llama weights from meta.<br>The output dir can be anywhere you want to store
          it.</p>

          '
        raw: 'The input_dir is the folder called LLaMA where you saved the original
          llama weights from meta.

          The output dir can be anywhere you want to store it.'
        updatedAt: '2023-04-23T13:53:47.314Z'
      numEdits: 0
      reactions: []
    id: 6445386b0f2fc80feb2386fd
    type: comment
  author: daryl149
  content: 'The input_dir is the folder called LLaMA where you saved the original
    llama weights from meta.

    The output dir can be anywhere you want to store it.'
  created_at: 2023-04-23 12:53:47+00:00
  edited: false
  hidden: false
  id: 6445386b0f2fc80feb2386fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/521116a47e6629414bdd7ead0c366749.svg
      fullname: M. Lemoyne
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mlemoyne
      type: user
    createdAt: '2023-04-24T00:45:07.000Z'
    data:
      edited: false
      editors:
      - Mlemoyne
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/521116a47e6629414bdd7ead0c366749.svg
          fullname: M. Lemoyne
          isHf: false
          isPro: false
          name: Mlemoyne
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;daryl149&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/daryl149\">@<span class=\"\
          underline\">daryl149</span></a></span>\n\n\t</span></span>  Even if you\
          \ managed to convert it, would you be able to run the model for inference\
          \ with your RAM?</p>\n"
        raw: '@daryl149  Even if you managed to convert it, would you be able to run
          the model for inference with your RAM?'
        updatedAt: '2023-04-24T00:45:07.370Z'
      numEdits: 0
      reactions: []
    id: 6445d113f993c804b03e6aed
    type: comment
  author: Mlemoyne
  content: '@daryl149  Even if you managed to convert it, would you be able to run
    the model for inference with your RAM?'
  created_at: 2023-04-23 23:45:07+00:00
  edited: false
  hidden: false
  id: 6445d113f993c804b03e6aed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-04-24T09:32:24.000Z'
    data:
      edited: true
      editors:
      - daryl149
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
          fullname: Daryl Autar
          isHf: false
          isPro: false
          name: daryl149
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Mlemoyne&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Mlemoyne\">@<span class=\"\
          underline\">Mlemoyne</span></a></span>\n\n\t</span></span><br>Yes! For inference,\
          \ PC RAM usage is not a bottleneck. You have these options:</p>\n<ul>\n\
          <li>if you have a combined GPU VRAM of at least 40GB, you can run it in\
          \ 8-bit mode (35GB to host the model and 5 in reserve for inference). The\
          \ PC RAM usage is negligible (&lt;10GB).</li>\n<li>if you have less than\
          \ 40GB VRAM, then you can use the <code>offload_folder='offload'</code>\
          \ parameter in your model call. It will offload every layer that does not\
          \ fit in GPU VRAM to a folder you create called offload on your SSD/HDD.\
          \ It will be slow, but it will run (2 minutes inference time or so)</li>\n\
          <li>if you have no notable GPU VRAM to speak of, you can convert the weights\
          \ to ggml and run it on CPU. See this thread: <a href=\"https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/2\"\
          >https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/2</a>.</li>\n\
          </ul>\n"
        raw: "@Mlemoyne \nYes! For inference, PC RAM usage is not a bottleneck. You\
          \ have these options:\n- if you have a combined GPU VRAM of at least 40GB,\
          \ you can run it in 8-bit mode (35GB to host the model and 5 in reserve\
          \ for inference). The PC RAM usage is negligible (<10GB).\n- if you have\
          \ less than 40GB VRAM, then you can use the `offload_folder='offload'` parameter\
          \ in your model call. It will offload every layer that does not fit in GPU\
          \ VRAM to a folder you create called offload on your SSD/HDD. It will be\
          \ slow, but it will run (2 minutes inference time or so)\n- if you have\
          \ no notable GPU VRAM to speak of, you can convert the weights to ggml and\
          \ run it on CPU. See this thread: https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/2."
        updatedAt: '2023-04-24T09:34:03.447Z'
      numEdits: 1
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - Mlemoyne
        - shmalex
        - purehate
        - McYang
        - MariusSheppard
    id: 64464ca82f3d84a7a88574af
    type: comment
  author: daryl149
  content: "@Mlemoyne \nYes! For inference, PC RAM usage is not a bottleneck. You\
    \ have these options:\n- if you have a combined GPU VRAM of at least 40GB, you\
    \ can run it in 8-bit mode (35GB to host the model and 5 in reserve for inference).\
    \ The PC RAM usage is negligible (<10GB).\n- if you have less than 40GB VRAM,\
    \ then you can use the `offload_folder='offload'` parameter in your model call.\
    \ It will offload every layer that does not fit in GPU VRAM to a folder you create\
    \ called offload on your SSD/HDD. It will be slow, but it will run (2 minutes\
    \ inference time or so)\n- if you have no notable GPU VRAM to speak of, you can\
    \ convert the weights to ggml and run it on CPU. See this thread: https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/2."
  created_at: 2023-04-24 08:32:24+00:00
  edited: true
  hidden: false
  id: 64464ca82f3d84a7a88574af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a545728872bc7cc9d8f77dd67cd8090.svg
      fullname: Aviral Agarwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JackReacher23
      type: user
    createdAt: '2023-04-26T05:23:50.000Z'
    data:
      edited: true
      editors:
      - JackReacher23
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a545728872bc7cc9d8f77dd67cd8090.svg
          fullname: Aviral Agarwal
          isHf: false
          isPro: false
          name: JackReacher23
          type: user
        html: '<p>Can''t we directly use the converted model present --&gt; <a href="https://huggingface.co/decapoda-research/llama-30b-hf">https://huggingface.co/decapoda-research/llama-30b-hf</a>
          and then run the xor_codec.py on it?<br>Also, for inference, what are you
          guys using? I saw a similar inference-related thread --&gt; <a href="https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/5">https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/5</a>
          I am simply getting killed just by loading the model even after adding offload_folder=''offload''<br>It
          would be really helpful if anyone could tell.</p>

          '
        raw: 'Can''t we directly use the converted model present --> https://huggingface.co/decapoda-research/llama-30b-hf
          and then run the xor_codec.py on it?

          Also, for inference, what are you guys using? I saw a similar inference-related
          thread --> https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/5
          I am simply getting killed just by loading the model even after adding offload_folder=''offload''

          It would be really helpful if anyone could tell.'
        updatedAt: '2023-04-26T05:50:59.790Z'
      numEdits: 3
      reactions: []
    id: 6448b5663e498d6691a48be3
    type: comment
  author: JackReacher23
  content: 'Can''t we directly use the converted model present --> https://huggingface.co/decapoda-research/llama-30b-hf
    and then run the xor_codec.py on it?

    Also, for inference, what are you guys using? I saw a similar inference-related
    thread --> https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/5
    I am simply getting killed just by loading the model even after adding offload_folder=''offload''

    It would be really helpful if anyone could tell.'
  created_at: 2023-04-26 04:23:50+00:00
  edited: true
  hidden: false
  id: 6448b5663e498d6691a48be3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-04-26T20:34:03.000Z'
    data:
      edited: true
      editors:
      - daryl149
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
          fullname: Daryl Autar
          isHf: false
          isPro: false
          name: daryl149
          type: user
        html: '<blockquote>

          <p>Can''t we directly use the converted model present --&gt; <a href="https://huggingface.co/decapoda-research/llama-30b-hf">https://huggingface.co/decapoda-research/llama-30b-hf</a>
          and then run the xor_codec.py on it?<br>Also, for inference, what are you
          guys using? I saw a similar inference-related thread --&gt; <a href="https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/5">https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/5</a>
          I am simply getting killed just by loading the model even after adding offload_folder=''offload''<br>It
          would be really helpful if anyone could tell.</p>

          </blockquote>

          <p>Apparently not, see: <a href="https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/15#644911e8e988635a3d6be312">https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/15#644911e8e988635a3d6be312</a><br>I''m
          running a system with an RTX A6000, so that fits most model weights in 8-bit
          without offloading.<br>What''s your VRAM, RAM and swap size? Are you running
          out of all 3? If so, try to still increase the swap size combined with the
          offload folder.</p>

          '
        raw: '> Can''t we directly use the converted model present --> https://huggingface.co/decapoda-research/llama-30b-hf
          and then run the xor_codec.py on it?

          > Also, for inference, what are you guys using? I saw a similar inference-related
          thread --> https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/5
          I am simply getting killed just by loading the model even after adding offload_folder=''offload''

          > It would be really helpful if anyone could tell.


          Apparently not, see: https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/15#644911e8e988635a3d6be312

          I''m running a system with an RTX A6000, so that fits most model weights
          in 8-bit without offloading.

          What''s your VRAM, RAM and swap size? Are you running out of all 3? If so,
          try to still increase the swap size combined with the offload folder.'
        updatedAt: '2023-04-26T20:43:23.536Z'
      numEdits: 2
      reactions: []
    id: 64498abb111b3bf687802543
    type: comment
  author: daryl149
  content: '> Can''t we directly use the converted model present --> https://huggingface.co/decapoda-research/llama-30b-hf
    and then run the xor_codec.py on it?

    > Also, for inference, what are you guys using? I saw a similar inference-related
    thread --> https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/5
    I am simply getting killed just by loading the model even after adding offload_folder=''offload''

    > It would be really helpful if anyone could tell.


    Apparently not, see: https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/15#644911e8e988635a3d6be312

    I''m running a system with an RTX A6000, so that fits most model weights in 8-bit
    without offloading.

    What''s your VRAM, RAM and swap size? Are you running out of all 3? If so, try
    to still increase the swap size combined with the offload folder.'
  created_at: 2023-04-26 19:34:03+00:00
  edited: true
  hidden: false
  id: 64498abb111b3bf687802543
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662326188075-noauth.png?w=200&h=200&f=face
      fullname: Matthew
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MatthewK
      type: user
    createdAt: '2023-04-27T00:49:57.000Z'
    data:
      edited: false
      editors:
      - MatthewK
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662326188075-noauth.png?w=200&h=200&f=face
          fullname: Matthew
          isHf: false
          isPro: false
          name: MatthewK
          type: user
        html: '<p>If using WSL on Windows you can alternatively create a ".wslconfig"
          file in your C:\Users%USERNAME%\ directory with the following (replace 55GB
          with something a little less than your total RAM):</p>

          <p>[wsl2]<br>memory=55GB<br>swap=100GB</p>

          '
        raw: 'If using WSL on Windows you can alternatively create a ".wslconfig"
          file in your C:\Users\%USERNAME%\ directory with the following (replace
          55GB with something a little less than your total RAM):


          [wsl2]

          memory=55GB

          swap=100GB'
        updatedAt: '2023-04-27T00:49:57.392Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - daryl149
        - JackReacher23
        - rjtmehta99
    id: 6449c6b51af713976c36e480
    type: comment
  author: MatthewK
  content: 'If using WSL on Windows you can alternatively create a ".wslconfig" file
    in your C:\Users\%USERNAME%\ directory with the following (replace 55GB with something
    a little less than your total RAM):


    [wsl2]

    memory=55GB

    swap=100GB'
  created_at: 2023-04-26 23:49:57+00:00
  edited: false
  hidden: false
  id: 6449c6b51af713976c36e480
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
      fullname: Mukesh Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MukeshSharma
      type: user
    createdAt: '2023-04-27T05:46:39.000Z'
    data:
      edited: false
      editors:
      - MukeshSharma
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
          fullname: Mukesh Sharma
          isHf: false
          isPro: false
          name: MukeshSharma
          type: user
        html: "<p>can we use the model to finetune it on our specific dataset , like\
          \ how other models hosted on hugging face is done <span data-props=\"{&quot;user&quot;:&quot;daryl149&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/daryl149\"\
          >@<span class=\"underline\">daryl149</span></a></span>\n\n\t</span></span>\
          \ please let me know whats the actual process.</p>\n"
        raw: can we use the model to finetune it on our specific dataset , like how
          other models hosted on hugging face is done @daryl149 please let me know
          whats the actual process.
        updatedAt: '2023-04-27T05:46:39.050Z'
      numEdits: 0
      reactions: []
    id: 644a0c3f111b3bf6878a5cab
    type: comment
  author: MukeshSharma
  content: can we use the model to finetune it on our specific dataset , like how
    other models hosted on hugging face is done @daryl149 please let me know whats
    the actual process.
  created_at: 2023-04-27 04:46:39+00:00
  edited: false
  hidden: false
  id: 644a0c3f111b3bf6878a5cab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a545728872bc7cc9d8f77dd67cd8090.svg
      fullname: Aviral Agarwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JackReacher23
      type: user
    createdAt: '2023-04-27T05:49:12.000Z'
    data:
      edited: true
      editors:
      - JackReacher23
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a545728872bc7cc9d8f77dd67cd8090.svg
          fullname: Aviral Agarwal
          isHf: false
          isPro: false
          name: JackReacher23
          type: user
        html: "<blockquote>\n<blockquote>\n<p>Can't we directly use the converted\
          \ model present --&gt; <a href=\"https://huggingface.co/decapoda-research/llama-30b-hf\"\
          >https://huggingface.co/decapoda-research/llama-30b-hf</a> and then run\
          \ the xor_codec.py on it?<br>Also, for inference, what are you guys using?\
          \ I saw a similar inference-related thread --&gt; <a href=\"https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/5\"\
          >https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/5</a>\
          \ I am simply getting killed just by loading the model even after adding\
          \ offload_folder='offload'<br>It would be really helpful if anyone could\
          \ tell.</p>\n</blockquote>\n<p>Apparently not, see: <a href=\"https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/15#644911e8e988635a3d6be312\"\
          >https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/15#644911e8e988635a3d6be312</a><br>I'm\
          \ running a system with an RTX A6000, so that fits most model weights in\
          \ 8-bit without offloading.<br>What's your VRAM, RAM and swap size? Are\
          \ you running out of all 3? If so, try to still increase the swap size combined\
          \ with the offload folder.</p>\n</blockquote>\n<p>I have 24GB of A10G GPU\
          \ and 32GB Ram, I have yet not set up the swap size, and hence it is 0.\
          \ I think I was trying everything with <a href=\"https://huggingface.co/decapoda-research/llama-30b-hf\"\
          >https://huggingface.co/decapoda-research/llama-30b-hf</a>, and hence things\
          \ might not be working. I will try with the original Llama weights once.\
          \ I will have to use both the method you suggested:</p>\n<ol>\n<li>Transforming\
          \ the model to HF compatible (as there is less RAM)</li>\n<li>Even with\
          \ 8-bit, I will have to do offloading (48GB GPU would have been good, haha).<br>Thanks\
          \ for the response!</li>\n</ol>\n<p><span data-props=\"{&quot;user&quot;:&quot;MatthewK&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MatthewK\"\
          >@<span class=\"underline\">MatthewK</span></a></span>\n\n\t</span></span>\
          \ I am on Linux only so setting up swap size directly should be straight\
          \ forward, thanks though!</p>\n"
        raw: "> > Can't we directly use the converted model present --> https://huggingface.co/decapoda-research/llama-30b-hf\
          \ and then run the xor_codec.py on it?\n> > Also, for inference, what are\
          \ you guys using? I saw a similar inference-related thread --> https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/5\
          \ I am simply getting killed just by loading the model even after adding\
          \ offload_folder='offload'\n> > It would be really helpful if anyone could\
          \ tell.\n> \n> Apparently not, see: https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/15#644911e8e988635a3d6be312\n\
          > I'm running a system with an RTX A6000, so that fits most model weights\
          \ in 8-bit without offloading.\n> What's your VRAM, RAM and swap size? Are\
          \ you running out of all 3? If so, try to still increase the swap size combined\
          \ with the offload folder.\n\nI have 24GB of A10G GPU and 32GB Ram, I have\
          \ yet not set up the swap size, and hence it is 0. I think I was trying\
          \ everything with https://huggingface.co/decapoda-research/llama-30b-hf,\
          \ and hence things might not be working. I will try with the original Llama\
          \ weights once. I will have to use both the method you suggested:\n1) Transforming\
          \ the model to HF compatible (as there is less RAM)\n2) Even with 8-bit,\
          \ I will have to do offloading (48GB GPU would have been good, haha).\n\
          Thanks for the response!\n\n\n@MatthewK I am on Linux only so setting up\
          \ swap size directly should be straight forward, thanks though!"
        updatedAt: '2023-04-27T05:50:21.884Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - daryl149
        - shmalex
        - MatthewK
    id: 644a0cd81af713976c3ce85c
    type: comment
  author: JackReacher23
  content: "> > Can't we directly use the converted model present --> https://huggingface.co/decapoda-research/llama-30b-hf\
    \ and then run the xor_codec.py on it?\n> > Also, for inference, what are you\
    \ guys using? I saw a similar inference-related thread --> https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/5\
    \ I am simply getting killed just by loading the model even after adding offload_folder='offload'\n\
    > > It would be really helpful if anyone could tell.\n> \n> Apparently not, see:\
    \ https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/15#644911e8e988635a3d6be312\n\
    > I'm running a system with an RTX A6000, so that fits most model weights in 8-bit\
    \ without offloading.\n> What's your VRAM, RAM and swap size? Are you running\
    \ out of all 3? If so, try to still increase the swap size combined with the offload\
    \ folder.\n\nI have 24GB of A10G GPU and 32GB Ram, I have yet not set up the swap\
    \ size, and hence it is 0. I think I was trying everything with https://huggingface.co/decapoda-research/llama-30b-hf,\
    \ and hence things might not be working. I will try with the original Llama weights\
    \ once. I will have to use both the method you suggested:\n1) Transforming the\
    \ model to HF compatible (as there is less RAM)\n2) Even with 8-bit, I will have\
    \ to do offloading (48GB GPU would have been good, haha).\nThanks for the response!\n\
    \n\n@MatthewK I am on Linux only so setting up swap size directly should be straight\
    \ forward, thanks though!"
  created_at: 2023-04-27 04:49:12+00:00
  edited: true
  hidden: false
  id: 644a0cd81af713976c3ce85c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-04-27T11:42:07.000Z'
    data:
      edited: false
      editors:
      - daryl149
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
          fullname: Daryl Autar
          isHf: false
          isPro: false
          name: daryl149
          type: user
        html: "<blockquote>\n<p>can we use the model to finetune it on our specific\
          \ dataset , like how other models hosted on hugging face is done <span data-props=\"\
          {&quot;user&quot;:&quot;daryl149&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/daryl149\">@<span class=\"underline\">daryl149</span></a></span>\n\
          \n\t</span></span> please let me know whats the actual process.</p>\n</blockquote>\n\
          <p>Have not tried, but also this is a question you can ask in a new topic.</p>\n"
        raw: '> can we use the model to finetune it on our specific dataset , like
          how other models hosted on hugging face is done @daryl149 please let me
          know whats the actual process.


          Have not tried, but also this is a question you can ask in a new topic.'
        updatedAt: '2023-04-27T11:42:07.611Z'
      numEdits: 0
      reactions: []
    id: 644a5f8f1b4d976445ad71ee
    type: comment
  author: daryl149
  content: '> can we use the model to finetune it on our specific dataset , like how
    other models hosted on hugging face is done @daryl149 please let me know whats
    the actual process.


    Have not tried, but also this is a question you can ask in a new topic.'
  created_at: 2023-04-27 10:42:07+00:00
  edited: false
  hidden: false
  id: 644a5f8f1b4d976445ad71ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a545728872bc7cc9d8f77dd67cd8090.svg
      fullname: Aviral Agarwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JackReacher23
      type: user
    createdAt: '2023-04-27T14:53:40.000Z'
    data:
      edited: false
      editors:
      - JackReacher23
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a545728872bc7cc9d8f77dd67cd8090.svg
          fullname: Aviral Agarwal
          isHf: false
          isPro: false
          name: JackReacher23
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;daryl149&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/daryl149\">@<span class=\"\
          underline\">daryl149</span></a></span>\n\n\t</span></span> Sorry to bother\
          \ you again, but all my checksums after the convert_to_hf are the same except\
          \ for the following:</p>\n<p>8bc8ad3b8256b780d0917e72d257c176  ./tokenizer.json</p>\n\
          <p>Was this the same case for you? If not, any idea what might have happened?\
          \ Because of the rest of the things. match perfectly.</p>\n<p>After this,\
          \ I used xor-codec.py, and none of the checksums matches now :(<br>If possible,\
          \ can you please share your tokenizer.json? I think because of that, the\
          \ whole thing got messed up..</p>\n"
        raw: '@daryl149 Sorry to bother you again, but all my checksums after the
          convert_to_hf are the same except for the following:


          8bc8ad3b8256b780d0917e72d257c176  ./tokenizer.json


          Was this the same case for you? If not, any idea what might have happened?
          Because of the rest of the things. match perfectly.


          After this, I used xor-codec.py, and none of the checksums matches now :(

          If possible, can you please share your tokenizer.json? I think because of
          that, the whole thing got messed up..'
        updatedAt: '2023-04-27T14:53:40.521Z'
      numEdits: 0
      reactions: []
    id: 644a8c74e3a902dbe004ed54
    type: comment
  author: JackReacher23
  content: '@daryl149 Sorry to bother you again, but all my checksums after the convert_to_hf
    are the same except for the following:


    8bc8ad3b8256b780d0917e72d257c176  ./tokenizer.json


    Was this the same case for you? If not, any idea what might have happened? Because
    of the rest of the things. match perfectly.


    After this, I used xor-codec.py, and none of the checksums matches now :(

    If possible, can you please share your tokenizer.json? I think because of that,
    the whole thing got messed up..'
  created_at: 2023-04-27 13:53:40+00:00
  edited: false
  hidden: false
  id: 644a8c74e3a902dbe004ed54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-04-27T16:10:59.000Z'
    data:
      edited: true
      editors:
      - daryl149
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
          fullname: Daryl Autar
          isHf: false
          isPro: false
          name: daryl149
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;daryl149&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/daryl149\"\
          >@<span class=\"underline\">daryl149</span></a></span>\n\n\t</span></span>\
          \ Sorry to bother you again, but all my checksums after the convert_to_hf\
          \ are the same except for the following:</p>\n<p>8bc8ad3b8256b780d0917e72d257c176\
          \  ./tokenizer.json</p>\n<p>Was this the same case for you? If not, any\
          \ idea what might have happened? Because of the rest of the things. match\
          \ perfectly.</p>\n<p>After this, I used xor-codec.py, and none of the checksums\
          \ matches now :(<br>If possible, can you please share your tokenizer.json?\
          \ I think because of that, the whole thing got messed up..</p>\n</blockquote>\n\
          <p>I'd first try to call the model and see if it works for you anyway. Maybe\
          \ you used a windows edited version of the file, which replaces the CRLF\
          \ with CR, see <a href=\"https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/1#64444613c63001ae6355dda7\"\
          >https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/1#64444613c63001ae6355dda7</a>.\
          \ Or maybe you opened it manually once and it converted some line breaks\
          \ automatically.</p>\n<p>Probably best to open a new topic for your issue\
          \ or redownload the file from where you got it. Can't share any files, due\
          \ to not wanting to incur the wrath of meta.</p>\n"
        raw: "> @daryl149 Sorry to bother you again, but all my checksums after the\
          \ convert_to_hf are the same except for the following:\n> \n> 8bc8ad3b8256b780d0917e72d257c176\
          \  ./tokenizer.json\n> \n> Was this the same case for you? If not, any idea\
          \ what might have happened? Because of the rest of the things. match perfectly.\n\
          > \n> After this, I used xor-codec.py, and none of the checksums matches\
          \ now :(\n> If possible, can you please share your tokenizer.json? I think\
          \ because of that, the whole thing got messed up..\n\nI'd first try to call\
          \ the model and see if it works for you anyway. Maybe you used a windows\
          \ edited version of the file, which replaces the CRLF with CR, see https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/1#64444613c63001ae6355dda7.\
          \ Or maybe you opened it manually once and it converted some line breaks\
          \ automatically.\n\nProbably best to open a new topic for your issue or\
          \ redownload the file from where you got it. Can't share any files, due\
          \ to not wanting to incur the wrath of meta."
        updatedAt: '2023-04-27T16:11:16.278Z'
      numEdits: 1
      reactions: []
    id: 644a9e93d4483bfaa06bcb38
    type: comment
  author: daryl149
  content: "> @daryl149 Sorry to bother you again, but all my checksums after the\
    \ convert_to_hf are the same except for the following:\n> \n> 8bc8ad3b8256b780d0917e72d257c176\
    \  ./tokenizer.json\n> \n> Was this the same case for you? If not, any idea what\
    \ might have happened? Because of the rest of the things. match perfectly.\n>\
    \ \n> After this, I used xor-codec.py, and none of the checksums matches now :(\n\
    > If possible, can you please share your tokenizer.json? I think because of that,\
    \ the whole thing got messed up..\n\nI'd first try to call the model and see if\
    \ it works for you anyway. Maybe you used a windows edited version of the file,\
    \ which replaces the CRLF with CR, see https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/1#64444613c63001ae6355dda7.\
    \ Or maybe you opened it manually once and it converted some line breaks automatically.\n\
    \nProbably best to open a new topic for your issue or redownload the file from\
    \ where you got it. Can't share any files, due to not wanting to incur the wrath\
    \ of meta."
  created_at: 2023-04-27 15:10:59+00:00
  edited: true
  hidden: false
  id: 644a9e93d4483bfaa06bcb38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a545728872bc7cc9d8f77dd67cd8090.svg
      fullname: Aviral Agarwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JackReacher23
      type: user
    createdAt: '2023-04-29T10:51:01.000Z'
    data:
      edited: false
      editors:
      - JackReacher23
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a545728872bc7cc9d8f77dd67cd8090.svg
          fullname: Aviral Agarwal
          isHf: false
          isPro: false
          name: JackReacher23
          type: user
        html: "<blockquote>\n<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;daryl149&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/daryl149\"\
          >@<span class=\"underline\">daryl149</span></a></span>\n\n\t</span></span>\
          \ Sorry to bother you again, but all my checksums after the convert_to_hf\
          \ are the same except for the following:</p>\n<p>8bc8ad3b8256b780d0917e72d257c176\
          \  ./tokenizer.json</p>\n<p>Was this the same case for you? If not, any\
          \ idea what might have happened? Because of the rest of the things. match\
          \ perfectly.</p>\n<p>After this, I used xor-codec.py, and none of the checksums\
          \ matches now :(<br>If possible, can you please share your tokenizer.json?\
          \ I think because of that, the whole thing got messed up..</p>\n</blockquote>\n\
          <p>I'd first try to call the model and see if it works for you anyway. Maybe\
          \ you used a windows edited version of the file, which replaces the CRLF\
          \ with CR, see <a href=\"https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/1#64444613c63001ae6355dda7\"\
          >https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/1#64444613c63001ae6355dda7</a>.\
          \ Or maybe you opened it manually once and it converted some line breaks\
          \ automatically.</p>\n<p>Probably best to open a new topic for your issue\
          \ or redownload the file from where you got it. Can't share any files, due\
          \ to not wanting to incur the wrath of meta.</p>\n</blockquote>\n<p>I completely\
          \ understand, it started working for me. I was in the wrong virtual env,\
          \ and that just messed up one file somehow. Either way thanks a lot for\
          \ your help and to this thread. Wouldn't have been able to run the model\
          \ without it :)</p>\n"
        raw: "> > @daryl149 Sorry to bother you again, but all my checksums after\
          \ the convert_to_hf are the same except for the following:\n> > \n> > 8bc8ad3b8256b780d0917e72d257c176\
          \  ./tokenizer.json\n> > \n> > Was this the same case for you? If not, any\
          \ idea what might have happened? Because of the rest of the things. match\
          \ perfectly.\n> > \n> > After this, I used xor-codec.py, and none of the\
          \ checksums matches now :(\n> > If possible, can you please share your tokenizer.json?\
          \ I think because of that, the whole thing got messed up..\n> \n> I'd first\
          \ try to call the model and see if it works for you anyway. Maybe you used\
          \ a windows edited version of the file, which replaces the CRLF with CR,\
          \ see https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/1#64444613c63001ae6355dda7.\
          \ Or maybe you opened it manually once and it converted some line breaks\
          \ automatically.\n> \n> Probably best to open a new topic for your issue\
          \ or redownload the file from where you got it. Can't share any files, due\
          \ to not wanting to incur the wrath of meta.\n\nI completely understand,\
          \ it started working for me. I was in the wrong virtual env, and that just\
          \ messed up one file somehow. Either way thanks a lot for your help and\
          \ to this thread. Wouldn't have been able to run the model without it :)"
        updatedAt: '2023-04-29T10:51:01.248Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - daryl149
    id: 644cf6950dc952d245985de2
    type: comment
  author: JackReacher23
  content: "> > @daryl149 Sorry to bother you again, but all my checksums after the\
    \ convert_to_hf are the same except for the following:\n> > \n> > 8bc8ad3b8256b780d0917e72d257c176\
    \  ./tokenizer.json\n> > \n> > Was this the same case for you? If not, any idea\
    \ what might have happened? Because of the rest of the things. match perfectly.\n\
    > > \n> > After this, I used xor-codec.py, and none of the checksums matches now\
    \ :(\n> > If possible, can you please share your tokenizer.json? I think because\
    \ of that, the whole thing got messed up..\n> \n> I'd first try to call the model\
    \ and see if it works for you anyway. Maybe you used a windows edited version\
    \ of the file, which replaces the CRLF with CR, see https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/1#64444613c63001ae6355dda7.\
    \ Or maybe you opened it manually once and it converted some line breaks automatically.\n\
    > \n> Probably best to open a new topic for your issue or redownload the file\
    \ from where you got it. Can't share any files, due to not wanting to incur the\
    \ wrath of meta.\n\nI completely understand, it started working for me. I was\
    \ in the wrong virtual env, and that just messed up one file somehow. Either way\
    \ thanks a lot for your help and to this thread. Wouldn't have been able to run\
    \ the model without it :)"
  created_at: 2023-04-29 09:51:01+00:00
  edited: false
  hidden: false
  id: 644cf6950dc952d245985de2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/349ab2d71ef4226f7d1c492162317467.svg
      fullname: HO CHUN HO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmpasswd2000
      type: user
    createdAt: '2023-04-29T14:52:47.000Z'
    data:
      edited: false
      editors:
      - cmpasswd2000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/349ab2d71ef4226f7d1c492162317467.svg
          fullname: HO CHUN HO
          isHf: false
          isPro: false
          name: cmpasswd2000
          type: user
        html: '<p>want do you want from any help I don''t know programmer and I networking
          and security</p>

          '
        raw: want do you want from any help I don't know programmer and I networking
          and security
        updatedAt: '2023-04-29T14:52:47.507Z'
      numEdits: 0
      reactions: []
    id: 644d2f3ffa94e93b0ec5809a
    type: comment
  author: cmpasswd2000
  content: want do you want from any help I don't know programmer and I networking
    and security
  created_at: 2023-04-29 13:52:47+00:00
  edited: false
  hidden: false
  id: 644d2f3ffa94e93b0ec5809a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: OpenAssistant/oasst-sft-6-llama-30b-xor
repo_type: model
status: open
target_branch: null
title: Trying to convert LlaMa weights to HF and running out of RAM, but don't want
  to buy more RAM?
