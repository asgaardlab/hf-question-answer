!!python/object:huggingface_hub.community.DiscussionWithDetails
author: PierrePeng
conflicting_files: null
created_at: 2023-05-15 15:20:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5a46beee8dbf270a94d3b6287291411.svg
      fullname: PierrePeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PierrePeng
      type: user
    createdAt: '2023-05-15T16:20:19.000Z'
    data:
      edited: false
      editors:
      - PierrePeng
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5a46beee8dbf270a94d3b6287291411.svg
          fullname: PierrePeng
          isHf: false
          isPro: false
          name: PierrePeng
          type: user
        html: '<p>As the title mentioned above, the inference speed is about 1 word
          per second.  There is enough GPU RAM and all the weights are loaded on GPU
          without offload.  But the utility of the GPU is only up to about 40%.</p>

          <p>I wanna if the inference speed is normal based on my hardware resource.<br>If
          not, what''s the reason and how can I improve the speed?<br>If yes, any
          recommendations for the hardware resources?</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/5dck4oBEyEie5W8NQGjqz.png"><img
          alt="b902f531dc92e78b4feeedb114350f8.png" src="https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/5dck4oBEyEie5W8NQGjqz.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/lshSoPsiEs8NMRi7iTKOP.png"><img
          alt="39859f7fef57c341efa87d8fa89bbdf.png" src="https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/lshSoPsiEs8NMRi7iTKOP.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/6sS-kfQV1DdTjuByKIDpA.png"><img
          alt="1ed3601a7aef010ea5f2994385d9fde.png" src="https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/6sS-kfQV1DdTjuByKIDpA.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/Cym6pVsvpBohyfxToXoHr.png"><img
          alt="72f3491699048d69e4dba86113b2ed3.png" src="https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/Cym6pVsvpBohyfxToXoHr.png"></a></p>

          '
        raw: "As the title mentioned above, the inference speed is about 1 word per\
          \ second.  There is enough GPU RAM and all the weights are loaded on GPU\
          \ without offload.  But the utility of the GPU is only up to about 40%.\r\
          \n\r\nI wanna if the inference speed is normal based on my hardware resource.\r\
          \nIf not, what's the reason and how can I improve the speed?\r\nIf yes,\
          \ any recommendations for the hardware resources?\r\n\r\n\r\n![b902f531dc92e78b4feeedb114350f8.png](https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/5dck4oBEyEie5W8NQGjqz.png)\r\
          \n\r\n![39859f7fef57c341efa87d8fa89bbdf.png](https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/lshSoPsiEs8NMRi7iTKOP.png)\r\
          \n\r\n![1ed3601a7aef010ea5f2994385d9fde.png](https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/6sS-kfQV1DdTjuByKIDpA.png)\r\
          \n\r\n![72f3491699048d69e4dba86113b2ed3.png](https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/Cym6pVsvpBohyfxToXoHr.png)\r\
          \n"
        updatedAt: '2023-05-15T16:20:19.847Z'
      numEdits: 0
      reactions: []
    id: 64625bc377f63463fc5e81e7
    type: comment
  author: PierrePeng
  content: "As the title mentioned above, the inference speed is about 1 word per\
    \ second.  There is enough GPU RAM and all the weights are loaded on GPU without\
    \ offload.  But the utility of the GPU is only up to about 40%.\r\n\r\nI wanna\
    \ if the inference speed is normal based on my hardware resource.\r\nIf not, what's\
    \ the reason and how can I improve the speed?\r\nIf yes, any recommendations for\
    \ the hardware resources?\r\n\r\n\r\n![b902f531dc92e78b4feeedb114350f8.png](https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/5dck4oBEyEie5W8NQGjqz.png)\r\
    \n\r\n![39859f7fef57c341efa87d8fa89bbdf.png](https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/lshSoPsiEs8NMRi7iTKOP.png)\r\
    \n\r\n![1ed3601a7aef010ea5f2994385d9fde.png](https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/6sS-kfQV1DdTjuByKIDpA.png)\r\
    \n\r\n![72f3491699048d69e4dba86113b2ed3.png](https://cdn-uploads.huggingface.co/production/uploads/6434462f7b824748010b1fb7/Cym6pVsvpBohyfxToXoHr.png)\r\
    \n"
  created_at: 2023-05-15 15:20:19+00:00
  edited: false
  hidden: false
  id: 64625bc377f63463fc5e81e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d5a46beee8dbf270a94d3b6287291411.svg
      fullname: PierrePeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PierrePeng
      type: user
    createdAt: '2023-05-25T02:51:49.000Z'
    data:
      status: closed
    id: 646ecd451a427e2630473639
    type: status-change
  author: PierrePeng
  created_at: 2023-05-25 01:51:49+00:00
  id: 646ecd451a427e2630473639
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f612a93e1d25633212a00c0fc16093b2.svg
      fullname: Yufei Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yufei
      type: user
    createdAt: '2023-06-15T20:40:41.000Z'
    data:
      edited: false
      editors:
      - Yufei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9497206807136536
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f612a93e1d25633212a00c0fc16093b2.svg
          fullname: Yufei Huang
          isHf: false
          isPro: false
          name: Yufei
          type: user
        html: '<p>Hi. I''m facing the same issue. Do you have any tips that can speed
          up the model inference? Thanks!</p>

          '
        raw: Hi. I'm facing the same issue. Do you have any tips that can speed up
          the model inference? Thanks!
        updatedAt: '2023-06-15T20:40:41.857Z'
      numEdits: 0
      reactions: []
    id: 648b7749df4710674c686b79
    type: comment
  author: Yufei
  content: Hi. I'm facing the same issue. Do you have any tips that can speed up the
    model inference? Thanks!
  created_at: 2023-06-15 19:40:41+00:00
  edited: false
  hidden: false
  id: 648b7749df4710674c686b79
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5a46beee8dbf270a94d3b6287291411.svg
      fullname: PierrePeng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PierrePeng
      type: user
    createdAt: '2023-06-16T05:01:14.000Z'
    data:
      edited: false
      editors:
      - PierrePeng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8976953029632568
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5a46beee8dbf270a94d3b6287291411.svg
          fullname: PierrePeng
          isHf: false
          isPro: false
          name: PierrePeng
          type: user
        html: '<blockquote>

          <p>Hi. I''m facing the same issue. Do you have any tips that can speed up
          the model inference? Thanks!</p>

          </blockquote>

          <p>Following the tutorial in the link will meet your question.</p>

          <p> <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</a></p>

          '
        raw: "> Hi. I'm facing the same issue. Do you have any tips that can speed\
          \ up the model inference? Thanks!\n\nFollowing the tutorial in the link\
          \ will meet your question.\n\n https://github.com/huggingface/text-generation-inference"
        updatedAt: '2023-06-16T05:01:14.213Z'
      numEdits: 0
      reactions: []
    id: 648bec9a9dbb6e41f7c2229d
    type: comment
  author: PierrePeng
  content: "> Hi. I'm facing the same issue. Do you have any tips that can speed up\
    \ the model inference? Thanks!\n\nFollowing the tutorial in the link will meet\
    \ your question.\n\n https://github.com/huggingface/text-generation-inference"
  created_at: 2023-06-16 04:01:14+00:00
  edited: false
  hidden: false
  id: 648bec9a9dbb6e41f7c2229d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f612a93e1d25633212a00c0fc16093b2.svg
      fullname: Yufei Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yufei
      type: user
    createdAt: '2023-06-16T06:39:10.000Z'
    data:
      edited: false
      editors:
      - Yufei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.896908164024353
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f612a93e1d25633212a00c0fc16093b2.svg
          fullname: Yufei Huang
          isHf: false
          isPro: false
          name: Yufei
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Hi. I''m facing the same issue. Do you have any tips that can speed up
          the model inference? Thanks!</p>

          </blockquote>

          <p>Following the tutorial in the link will meet your question.</p>

          <p> <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</a></p>

          </blockquote>

          <p>Thank you! Let me take a try.</p>

          '
        raw: "> > Hi. I'm facing the same issue. Do you have any tips that can speed\
          \ up the model inference? Thanks!\n> \n> Following the tutorial in the link\
          \ will meet your question.\n> \n>  https://github.com/huggingface/text-generation-inference\n\
          \nThank you! Let me take a try."
        updatedAt: '2023-06-16T06:39:10.023Z'
      numEdits: 0
      reactions: []
    id: 648c038ed00b66436ca3a705
    type: comment
  author: Yufei
  content: "> > Hi. I'm facing the same issue. Do you have any tips that can speed\
    \ up the model inference? Thanks!\n> \n> Following the tutorial in the link will\
    \ meet your question.\n> \n>  https://github.com/huggingface/text-generation-inference\n\
    \nThank you! Let me take a try."
  created_at: 2023-06-16 05:39:10+00:00
  edited: false
  hidden: false
  id: 648c038ed00b66436ca3a705
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c154b4a89945329978f1ae0f31379d6.svg
      fullname: Icyrockton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Icyrockton
      type: user
    createdAt: '2023-06-17T01:12:19.000Z'
    data:
      edited: false
      editors:
      - Icyrockton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7646471858024597
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c154b4a89945329978f1ae0f31379d6.svg
          fullname: Icyrockton
          isHf: false
          isPro: false
          name: Icyrockton
          type: user
        html: '<p>I am facing the same problem, GPU usage is only up to about 50%.<br>I
          use 2 3090 to load the 13B LLM model, the model is evenly distributed across
          gpus.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62d8a3aabf50eca8389b7ca9/exEIpUGeiWmq5_mMyXDvv.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/62d8a3aabf50eca8389b7ca9/exEIpUGeiWmq5_mMyXDvv.png"></a></p>

          '
        raw: 'I am facing the same problem, GPU usage is only up to about 50%.

          I use 2 3090 to load the 13B LLM model, the model is evenly distributed
          across gpus.

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/62d8a3aabf50eca8389b7ca9/exEIpUGeiWmq5_mMyXDvv.png)

          '
        updatedAt: '2023-06-17T01:12:19.402Z'
      numEdits: 0
      reactions: []
    id: 648d08734e46c331f0795e07
    type: comment
  author: Icyrockton
  content: 'I am facing the same problem, GPU usage is only up to about 50%.

    I use 2 3090 to load the 13B LLM model, the model is evenly distributed across
    gpus.

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/62d8a3aabf50eca8389b7ca9/exEIpUGeiWmq5_mMyXDvv.png)

    '
  created_at: 2023-06-17 00:12:19+00:00
  edited: false
  hidden: false
  id: 648d08734e46c331f0795e07
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 47
repo_id: OpenAssistant/oasst-sft-6-llama-30b-xor
repo_type: model
status: closed
target_branch: null
title: The inference speed is too slow on 4*A10G GPU
