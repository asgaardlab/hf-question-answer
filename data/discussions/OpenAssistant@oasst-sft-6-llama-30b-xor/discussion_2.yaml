!!python/object:huggingface_hub.community.DiscussionWithDetails
author: akiselev
conflicting_files: null
created_at: 2023-04-22 19:36:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0b983dd7d0dd047b6dd9ab41e24ca6fd.svg
      fullname: A K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akiselev
      type: user
    createdAt: '2023-04-22T20:36:26.000Z'
    data:
      edited: true
      editors:
      - akiselev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0b983dd7d0dd047b6dd9ab41e24ca6fd.svg
          fullname: A K
          isHf: false
          isPro: false
          name: akiselev
          type: user
        html: "<p>After applying the XOR deltas, I tried to convert the weights to\
          \ GGML format using the latest llama.cpp (<code>0e018fe008eacebdbcfa2d61b6c988c245c961cd</code>)\
          \ <code>convert.py</code> using this command:</p>\n<pre><code class=\"language-sh\"\
          >python3 convert.py --outfile ~/models/oasst-sft-6-llama-30B-float.bin ~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/\n\
          </code></pre>\n<p>which resulted in the following error:</p>\n<pre><code>Loading\
          \ vocab file ~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/tokenizer.model\n\
          Traceback (most recent call last):\n  File \"~/models/llama.cpp/convert.py\"\
          , line 1149, in &lt;module&gt;\n    main()\n  File \"~/models/llama.cpp/convert.py\"\
          , line 1144, in main\n    OutputFile.write_all(outfile, params, model, vocab)\n\
          \  File \"~/models/llama.cpp/convert.py\", line 942, in write_all\n    check_vocab_size(params,\
          \ vocab)\n  File \"~/models/llama.cpp/convert.py\", line 896, in check_vocab_size\n\
          \    raise Exception(msg)\nException: Vocab size mismatch (model has 32016,\
          \ but ~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/tokenizer.model\
          \ combined with ~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/added_tokens.json\
          \ has 32005).\n</code></pre>\n<p>I updated <code>~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/added_tokens.json</code>\
          \ to add the tokens:</p>\n<pre><code class=\"language-json\"><span class=\"\
          hljs-punctuation\">{</span>\n  <span class=\"hljs-attr\">\"&lt;|assistant|&gt;\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\"\
          >32004</span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"\
          hljs-attr\">\"&lt;|prefix_begin|&gt;\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">32000</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"&lt;|prefix_end|&gt;\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">32003</span><span\
          \ class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"&lt;|prompter|&gt;\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\"\
          >32002</span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"\
          hljs-attr\">\"&lt;|system|&gt;\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">32001</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"&lt;|babychonk|&gt;\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">32015</span><span\
          \ class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"&lt;|superchonk|&gt;\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\"\
          >32014</span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"\
          hljs-attr\">\"&lt;|megachonk|&gt;\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">32013</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"&lt;|ohlawdhecomin|&gt;\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">32012</span><span\
          \ class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"&lt;|baby_chonk|&gt;\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\"\
          >32011</span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"\
          hljs-attr\">\"&lt;|super_chonk|&gt;\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">32010</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"&lt;|mega_chonk|&gt;\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">32009</span><span\
          \ class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"&lt;|oh_lawd_he_comin|&gt;\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\"\
          >32008</span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"\
          hljs-attr\">\"&lt;|BABYCHONK|&gt;\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">32007</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"&lt;|SUPERCHONK|&gt;\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">32006</span><span\
          \ class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"&lt;|MEGACHONK|&gt;\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\"\
          >32005</span>\n<span class=\"hljs-punctuation\">}</span>\n</code></pre>\n\
          <p>This fixed the error and conversion worked.</p>\n<p>Quantized successfully\
          \ using this command:</p>\n<pre><code class=\"language-sh\">./llama.cpp/quantize\
          \ ~/models/oasst-sft-6-llama-30b-float.bin ~/models/ggml-oasst-sft-6-llama-30b-q4_0.bin\
          \ 2\n</code></pre>\n<p>Then, running command:</p>\n<pre><code class=\"language-sh\"\
          >./llama.cpp/main  -m ggml-oasst-sft-6-llama-30b-q4_0.bin -p <span class=\"\
          hljs-string\">\"&lt;|prompter|&gt;: Suppose I have a cabbage, a goat and\
          \ a lion, and I need to get them across a river. I have a boat that can\
          \ only carry myself and a single other item. I am not allowed to leave the\
          \ cabbage and lion alone together, and I am not allowed to leave the lion\
          \ and goat alone together. How can I safely get all three across? &lt;|assistant|&gt;:\
          \ \"</span> -n 1000\n</code></pre>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/631c3f8a38ede3a38d3dc359/q9QYKH59VAdlcs6o4I0GQ.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/631c3f8a38ede3a38d3dc359/q9QYKH59VAdlcs6o4I0GQ.png\"\
          ></a></p>\n"
        raw: "After applying the XOR deltas, I tried to convert the weights to GGML\
          \ format using the latest llama.cpp (`0e018fe008eacebdbcfa2d61b6c988c245c961cd`)\
          \ `convert.py` using this command:\n\n```sh\npython3 convert.py --outfile\
          \ ~/models/oasst-sft-6-llama-30B-float.bin ~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/\n\
          ```\n\nwhich resulted in the following error:\n\n```\nLoading vocab file\
          \ ~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/tokenizer.model\n\
          Traceback (most recent call last):\n  File \"~/models/llama.cpp/convert.py\"\
          , line 1149, in <module>\n    main()\n  File \"~/models/llama.cpp/convert.py\"\
          , line 1144, in main\n    OutputFile.write_all(outfile, params, model, vocab)\n\
          \  File \"~/models/llama.cpp/convert.py\", line 942, in write_all\n    check_vocab_size(params,\
          \ vocab)\n  File \"~/models/llama.cpp/convert.py\", line 896, in check_vocab_size\n\
          \    raise Exception(msg)\nException: Vocab size mismatch (model has 32016,\
          \ but ~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/tokenizer.model\
          \ combined with ~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/added_tokens.json\
          \ has 32005).\n```\n\nI updated `~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/added_tokens.json`\
          \ to add the tokens:\n\n```json\n{\n  \"<|assistant|>\": 32004,\n  \"<|prefix_begin|>\"\
          : 32000,\n  \"<|prefix_end|>\": 32003,\n  \"<|prompter|>\": 32002,\n  \"\
          <|system|>\": 32001,\n  \"<|babychonk|>\": 32015,\n  \"<|superchonk|>\"\
          : 32014,\n  \"<|megachonk|>\": 32013,\n  \"<|ohlawdhecomin|>\": 32012,\n\
          \  \"<|baby_chonk|>\": 32011,\n  \"<|super_chonk|>\": 32010,\n  \"<|mega_chonk|>\"\
          : 32009,\n  \"<|oh_lawd_he_comin|>\": 32008,\n  \"<|BABYCHONK|>\": 32007,\n\
          \  \"<|SUPERCHONK|>\": 32006,\n  \"<|MEGACHONK|>\": 32005\n}\n```\n\nThis\
          \ fixed the error and conversion worked.\n\nQuantized successfully using\
          \ this command:\n\n```sh\n./llama.cpp/quantize ~/models/oasst-sft-6-llama-30b-float.bin\
          \ ~/models/ggml-oasst-sft-6-llama-30b-q4_0.bin 2\n```\n\nThen, running command:\n\
          \n```sh\n./llama.cpp/main  -m ggml-oasst-sft-6-llama-30b-q4_0.bin -p \"\
          <|prompter|>: Suppose I have a cabbage, a goat and a lion, and I need to\
          \ get them across a river. I have a boat that can only carry myself and\
          \ a single other item. I am not allowed to leave the cabbage and lion alone\
          \ together, and I am not allowed to leave the lion and goat alone together.\
          \ How can I safely get all three across? <|assistant|>: \" -n 1000\n```\n\
          \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/631c3f8a38ede3a38d3dc359/q9QYKH59VAdlcs6o4I0GQ.png)"
        updatedAt: '2023-04-22T20:36:55.932Z'
      numEdits: 1
      reactions:
      - count: 8
        reaction: "\U0001F44D"
        users:
        - andreaskoepf
        - spanielrassler
        - Cube6288
        - daryl149
        - EmmaTew
        - opcode0x90
        - Jysdy
        - jack2-5
    id: 6444454a92c17f01a171b179
    type: comment
  author: akiselev
  content: "After applying the XOR deltas, I tried to convert the weights to GGML\
    \ format using the latest llama.cpp (`0e018fe008eacebdbcfa2d61b6c988c245c961cd`)\
    \ `convert.py` using this command:\n\n```sh\npython3 convert.py --outfile ~/models/oasst-sft-6-llama-30B-float.bin\
    \ ~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/\n```\n\nwhich resulted\
    \ in the following error:\n\n```\nLoading vocab file ~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/tokenizer.model\n\
    Traceback (most recent call last):\n  File \"~/models/llama.cpp/convert.py\",\
    \ line 1149, in <module>\n    main()\n  File \"~/models/llama.cpp/convert.py\"\
    , line 1144, in main\n    OutputFile.write_all(outfile, params, model, vocab)\n\
    \  File \"~/models/llama.cpp/convert.py\", line 942, in write_all\n    check_vocab_size(params,\
    \ vocab)\n  File \"~/models/llama.cpp/convert.py\", line 896, in check_vocab_size\n\
    \    raise Exception(msg)\nException: Vocab size mismatch (model has 32016, but\
    \ ~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/tokenizer.model combined\
    \ with ~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/added_tokens.json\
    \ has 32005).\n```\n\nI updated `~/models/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b/added_tokens.json`\
    \ to add the tokens:\n\n```json\n{\n  \"<|assistant|>\": 32004,\n  \"<|prefix_begin|>\"\
    : 32000,\n  \"<|prefix_end|>\": 32003,\n  \"<|prompter|>\": 32002,\n  \"<|system|>\"\
    : 32001,\n  \"<|babychonk|>\": 32015,\n  \"<|superchonk|>\": 32014,\n  \"<|megachonk|>\"\
    : 32013,\n  \"<|ohlawdhecomin|>\": 32012,\n  \"<|baby_chonk|>\": 32011,\n  \"\
    <|super_chonk|>\": 32010,\n  \"<|mega_chonk|>\": 32009,\n  \"<|oh_lawd_he_comin|>\"\
    : 32008,\n  \"<|BABYCHONK|>\": 32007,\n  \"<|SUPERCHONK|>\": 32006,\n  \"<|MEGACHONK|>\"\
    : 32005\n}\n```\n\nThis fixed the error and conversion worked.\n\nQuantized successfully\
    \ using this command:\n\n```sh\n./llama.cpp/quantize ~/models/oasst-sft-6-llama-30b-float.bin\
    \ ~/models/ggml-oasst-sft-6-llama-30b-q4_0.bin 2\n```\n\nThen, running command:\n\
    \n```sh\n./llama.cpp/main  -m ggml-oasst-sft-6-llama-30b-q4_0.bin -p \"<|prompter|>:\
    \ Suppose I have a cabbage, a goat and a lion, and I need to get them across a\
    \ river. I have a boat that can only carry myself and a single other item. I am\
    \ not allowed to leave the cabbage and lion alone together, and I am not allowed\
    \ to leave the lion and goat alone together. How can I safely get all three across?\
    \ <|assistant|>: \" -n 1000\n```\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/631c3f8a38ede3a38d3dc359/q9QYKH59VAdlcs6o4I0GQ.png)"
  created_at: 2023-04-22 19:36:26+00:00
  edited: true
  hidden: false
  id: 6444454a92c17f01a171b179
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0d146e4b922a325b69d3d509965ed60.svg
      fullname: Frankie G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spanielrassler
      type: user
    createdAt: '2023-04-22T22:58:32.000Z'
    data:
      edited: true
      editors:
      - spanielrassler
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0d146e4b922a325b69d3d509965ed60.svg
          fullname: Frankie G
          isHf: false
          isPro: false
          name: spanielrassler
          type: user
        html: '<p>That''s great news!! Can you post the ggml weights on HF, pretty
          please? :)</p>

          '
        raw: That's great news!! Can you post the ggml weights on HF, pretty please?
          :)
        updatedAt: '2023-04-22T23:01:00.672Z'
      numEdits: 1
      reactions: []
    id: 644466988f795c936d072122
    type: comment
  author: spanielrassler
  content: That's great news!! Can you post the ggml weights on HF, pretty please?
    :)
  created_at: 2023-04-22 21:58:32+00:00
  edited: true
  hidden: false
  id: 644466988f795c936d072122
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41ada123197e21cdd5b091a81364ec9f.svg
      fullname: Crum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: artemcrum
      type: user
    createdAt: '2023-04-23T05:36:09.000Z'
    data:
      edited: false
      editors:
      - artemcrum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41ada123197e21cdd5b091a81364ec9f.svg
          fullname: Crum
          isHf: false
          isPro: false
          name: artemcrum
          type: user
        html: '<p>Share the weights please!</p>

          '
        raw: Share the weights please!
        updatedAt: '2023-04-23T05:36:09.007Z'
      numEdits: 0
      reactions: []
    id: 6444c3c93dc283776339eac8
    type: comment
  author: artemcrum
  content: Share the weights please!
  created_at: 2023-04-23 04:36:09+00:00
  edited: false
  hidden: false
  id: 6444c3c93dc283776339eac8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-04-23T15:41:43.000Z'
    data:
      edited: false
      editors:
      - daryl149
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
          fullname: Daryl Autar
          isHf: false
          isPro: false
          name: daryl149
          type: user
        html: '<p>So about 2 minutes total response time on cpu for this prompt with
          ggml? Is this an M1/M2 chip or intel/AMD?</p>

          '
        raw: So about 2 minutes total response time on cpu for this prompt with ggml?
          Is this an M1/M2 chip or intel/AMD?
        updatedAt: '2023-04-23T15:41:43.414Z'
      numEdits: 0
      reactions: []
    id: 644551b7f993c804b0356b72
    type: comment
  author: daryl149
  content: So about 2 minutes total response time on cpu for this prompt with ggml?
    Is this an M1/M2 chip or intel/AMD?
  created_at: 2023-04-23 14:41:43+00:00
  edited: false
  hidden: false
  id: 644551b7f993c804b0356b72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0d146e4b922a325b69d3d509965ed60.svg
      fullname: Frankie G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spanielrassler
      type: user
    createdAt: '2023-04-23T16:02:41.000Z'
    data:
      edited: false
      editors:
      - spanielrassler
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0d146e4b922a325b69d3d509965ed60.svg
          fullname: Frankie G
          isHf: false
          isPro: false
          name: spanielrassler
          type: user
        html: '<p>I''m seeing decently fast response on an M2 pro with 32gb ram so
          it''s not bad.</p>

          '
        raw: I'm seeing decently fast response on an M2 pro with 32gb ram so it's
          not bad.
        updatedAt: '2023-04-23T16:02:41.886Z'
      numEdits: 0
      reactions: []
    id: 644556a1d1460e859d1d0a0b
    type: comment
  author: spanielrassler
  content: I'm seeing decently fast response on an M2 pro with 32gb ram so it's not
    bad.
  created_at: 2023-04-23 15:02:41+00:00
  edited: false
  hidden: false
  id: 644556a1d1460e859d1d0a0b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d0cfd2485856cd71131c31/kbmt5cO6Gu3iDyJxhYiJ8.jpeg?w=200&h=200&f=face
      fullname: Hanif Yuli Abdillah P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hanifabdlh
      type: user
    createdAt: '2023-05-02T03:08:45.000Z'
    data:
      edited: false
      editors:
      - hanifabdlh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d0cfd2485856cd71131c31/kbmt5cO6Gu3iDyJxhYiJ8.jpeg?w=200&h=200&f=face
          fullname: Hanif Yuli Abdillah P
          isHf: false
          isPro: false
          name: hanifabdlh
          type: user
        html: '<p>where i can get the bin file of the <code>~/models/oasst-sft-6-llama-30b-float.bin</code>
          model?</p>

          '
        raw: where i can get the bin file of the `~/models/oasst-sft-6-llama-30b-float.bin`
          model?
        updatedAt: '2023-05-02T03:08:45.138Z'
      numEdits: 0
      reactions: []
    id: 64507ebd577838187e091fbe
    type: comment
  author: hanifabdlh
  content: where i can get the bin file of the `~/models/oasst-sft-6-llama-30b-float.bin`
    model?
  created_at: 2023-05-02 02:08:45+00:00
  edited: false
  hidden: false
  id: 64507ebd577838187e091fbe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: OpenAssistant/oasst-sft-6-llama-30b-xor
repo_type: model
status: open
target_branch: null
title: Converting to ggml and quantizing with llama.cpp
