!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vgoklani
conflicting_files: null
created_at: 2023-04-25 17:43:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f04e9625d08220171a0ad41/8jySP30xsYKCyEiqMN3Dm.jpeg?w=200&h=200&f=face
      fullname: VG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vgoklani
      type: user
    createdAt: '2023-04-25T18:43:37.000Z'
    data:
      edited: false
      editors:
      - vgoklani
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f04e9625d08220171a0ad41/8jySP30xsYKCyEiqMN3Dm.jpeg?w=200&h=200&f=face
          fullname: VG
          isHf: false
          isPro: false
          name: vgoklani
          type: user
        html: '<p>I read through the code and I didn''t follow what you were trying
          to do. thanks!</p>

          '
        raw: I read through the code and I didn't follow what you were trying to do.
          thanks!
        updatedAt: '2023-04-25T18:43:37.425Z'
      numEdits: 0
      reactions: []
    id: 64481f59e21484883405e19d
    type: comment
  author: vgoklani
  content: I read through the code and I didn't follow what you were trying to do.
    thanks!
  created_at: 2023-04-25 17:43:37+00:00
  edited: false
  hidden: false
  id: 64481f59e21484883405e19d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd6ec96b8ccb2d52a8e413a2aae20b13.svg
      fullname: brianm
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: b-mc2
      type: user
    createdAt: '2023-04-26T01:24:33.000Z'
    data:
      edited: true
      editors:
      - b-mc2
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd6ec96b8ccb2d52a8e413a2aae20b13.svg
          fullname: brianm
          isHf: false
          isPro: false
          name: b-mc2
          type: user
        html: "<p>I won't speak for all the parts of it, but the general idea appears\
          \ to be OpenAssistant ships weights that aren't actually the weights, but\
          \ become the weights when <a rel=\"nofollow\" href=\"https://en.wikipedia.org/wiki/XOR_gate\"\
          >XOR'd</a> against the original LLaMa weights. This one uses numpy <a rel=\"\
          nofollow\" href=\"https://numpy.org/doc/stable/reference/generated/numpy.bitwise_xor.html\"\
          >bitwise_xor()</a>.</p>\n<p><code>Due to the license attached to LLaMa models\
          \ by Meta AI it is not possible to directly distribute LLaMa-based models.\
          \ Instead we provide XOR weights for the OA models....To use OpenAssistant\
          \ LLaMa-Based Models, you need to have a copy of the original LLaMa model\
          \ weights and add them to a llama subdirectory here.</code></p>\n<p>This\
          \ is basically saying <code>we aren't allowed to give you the list [1,2,3,4,5]\
          \ or our version of it [1,2,5,6,9] but we can give you [0, 0, 6, 2, 12]\
          \ and if you XOR it against the original list...which we won't give you,\
          \ you'll get our modified version</code></p>\n<p>In other words, they won't\
          \ give you LLaMa weights or the LLaMa based oasst-sft-6-llama-30b weights,\
          \ but they will give you a way to create them yourself if you already have\
          \ the LLaMa weights.</p>\n<p>Toy example:</p>\n<pre><code class=\"language-python\"\
          >llama_weights = [<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\"\
          >2</span>, <span class=\"hljs-number\">3</span>, <span class=\"hljs-number\"\
          >4</span>, <span class=\"hljs-number\">5</span>]\noa_xor_weights = [<span\
          \ class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>,\
          \ <span class=\"hljs-number\">6</span>, <span class=\"hljs-number\">2</span>,\
          \ <span class=\"hljs-number\">12</span>]\n<span class=\"hljs-comment\">#\
          \ the actual desired weights of the model are [1, 2, 5, 6, 9]</span>\n<span\
          \ class=\"hljs-comment\"># zip the two weights and perform an XOR, 1^0,\
          \ 2^0, 3^6, 4^2, 5^12</span>\n\noasst_sft_6_llama_30b = [a^b <span class=\"\
          hljs-keyword\">for</span> a, b <span class=\"hljs-keyword\">in</span> <span\
          \ class=\"hljs-built_in\">zip</span>(llama_weights, oa_xor_weights)]\n<span\
          \ class=\"hljs-comment\"># oasst_sft_6_llama_30b = [1, 2, 5, 6, 9]</span>\n\
          </code></pre>\n<p>Back to the actual code...so after all the initial steps\
          \ in the model card instructions, the <code>xor_codec.py</code> file gets\
          \ run from the command line like this <code>python xor_codec.py oasst-sft-6-llama-30b/\
          \ oasst-sft-6-llama-30b-xor/ llama30b_hf/</code> This passes the paths to\
          \ both the original LLaMa weights and the OA \"weights\" to the <code>xor_decode()</code>\
          \ function <a href=\"https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/blob/main/xor_codec.py#L40\"\
          >here</a> unless one of the flags is change from their defaults.</p>\n<p>I've\
          \ added some comments below</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >xor_decode</span>(<span class=\"hljs-params\">dst, src_payload, src_base,\
          \ block_size=<span class=\"hljs-number\">4096</span></span>):\n    <span\
          \ class=\"hljs-comment\"># open both the LLaMa weights and the OA xor weights\
          \ as bytes</span>\n    fp_payload = gzip.<span class=\"hljs-built_in\">open</span>(src_payload,\
          \ <span class=\"hljs-string\">'rb'</span>)\n    fp_base = <span class=\"\
          hljs-built_in\">open</span>(src_base, <span class=\"hljs-string\">'rb'</span>)\n\
          \n    <span class=\"hljs-comment\"># create/open a file at the destination\
          \ path </span>\n    <span class=\"hljs-keyword\">with</span> <span class=\"\
          hljs-built_in\">open</span>(dst, <span class=\"hljs-string\">'wb'</span>)\
          \ <span class=\"hljs-keyword\">as</span> fp:\n        <span class=\"hljs-keyword\"\
          >while</span> <span class=\"hljs-literal\">True</span>:\n\n            <span\
          \ class=\"hljs-comment\"># loop through both weights in block_size/4096\
          \ byte increments, create numpy arrays for each</span>\n            buf1\
          \ = numpy.array(<span class=\"hljs-built_in\">bytearray</span>(fp_payload.read(block_size)),\
          \ dtype=numpy.uint8)\n            buf2 = numpy.array(<span class=\"hljs-built_in\"\
          >bytearray</span>(fp_base.read(block_size)), dtype=numpy.uint8)\n\n    \
          \        <span class=\"hljs-comment\"># Padding seems to just check that\
          \ buf1 and buf2 are the same length and adds zeros or trims an array to\
          \ make them same length if they're not.</span>\n            padding = <span\
          \ class=\"hljs-built_in\">len</span>(buf1) - <span class=\"hljs-built_in\"\
          >len</span>(buf2)\n            <span class=\"hljs-keyword\">if</span> padding\
          \ &gt; <span class=\"hljs-number\">0</span>: buf2 = numpy.pad(buf2, (<span\
          \ class=\"hljs-number\">0</span>, padding), <span class=\"hljs-string\"\
          >'constant'</span>, constant_values=(<span class=\"hljs-number\">0</span>,))\n\
          \            <span class=\"hljs-keyword\">if</span> padding &lt; <span class=\"\
          hljs-number\">0</span>: buf2 = buf2[:<span class=\"hljs-built_in\">len</span>(buf1)]\n\
          \n            <span class=\"hljs-comment\"># Then uses numpy bitwise_xor\
          \ which performs an XOR between the chunks of the two weights. The resulting\
          \ array is assigned to `buf` variable</span>\n            buf = numpy.bitwise_xor(buf1,\
          \ buf2)\n\n            <span class=\"hljs-comment\"># then write that array\
          \ to the file and continue with the next loop until both files are exhausted</span>\n\
          \            fp.write(buf)\n            <span class=\"hljs-keyword\">if</span>\
          \ <span class=\"hljs-built_in\">len</span>(buf1) &lt; block_size: <span\
          \ class=\"hljs-keyword\">break</span>\n    fp_payload.close()\n    fp_base.close()\n\
          </code></pre>\n<p>The end result is the XOR operation between the original\
          \ LLaMA weights and the OA XOR weights, which means the XOR of the two spits\
          \ out the <strong>real</strong> weights for the model. </p>\n<p>Hopefully\
          \ that helps and I didn't misinterpret it</p>\n"
        raw: "I won't speak for all the parts of it, but the general idea appears\
          \ to be OpenAssistant ships weights that aren't actually the weights, but\
          \ become the weights when [XOR'd](https://en.wikipedia.org/wiki/XOR_gate)\
          \ against the original LLaMa weights. This one uses numpy [bitwise_xor()](https://numpy.org/doc/stable/reference/generated/numpy.bitwise_xor.html).\n\
          \n\n`Due to the license attached to LLaMa models by Meta AI it is not possible\
          \ to directly distribute LLaMa-based models. Instead we provide XOR weights\
          \ for the OA models....To use OpenAssistant LLaMa-Based Models, you need\
          \ to have a copy of the original LLaMa model weights and add them to a llama\
          \ subdirectory here.`\n\nThis is basically saying `we aren't allowed to\
          \ give you the list [1,2,3,4,5] or our version of it [1,2,5,6,9] but we\
          \ can give you [0, 0, 6, 2, 12] and if you XOR it against the original list...which\
          \ we won't give you, you'll get our modified version`\n\nIn other words,\
          \ they won't give you LLaMa weights or the LLaMa based oasst-sft-6-llama-30b\
          \ weights, but they will give you a way to create them yourself if you already\
          \ have the LLaMa weights.\n\nToy example:\n```python\nllama_weights = [1,\
          \ 2, 3, 4, 5]\noa_xor_weights = [0, 0, 6, 2, 12]\n# the actual desired weights\
          \ of the model are [1, 2, 5, 6, 9]\n# zip the two weights and perform an\
          \ XOR, 1^0, 2^0, 3^6, 4^2, 5^12\n\noasst_sft_6_llama_30b = [a^b for a, b\
          \ in zip(llama_weights, oa_xor_weights)]\n# oasst_sft_6_llama_30b = [1,\
          \ 2, 5, 6, 9]\n```\n\nBack to the actual code...so after all the initial\
          \ steps in the model card instructions, the `xor_codec.py` file gets run\
          \ from the command line like this `python xor_codec.py oasst-sft-6-llama-30b/\
          \ oasst-sft-6-llama-30b-xor/ llama30b_hf/` This passes the paths to both\
          \ the original LLaMa weights and the OA \"weights\" to the `xor_decode()`\
          \ function [here](https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/blob/main/xor_codec.py#L40)\
          \ unless one of the flags is change from their defaults.\n\nI've added some\
          \ comments below\n```python\ndef xor_decode(dst, src_payload, src_base,\
          \ block_size=4096):\n    # open both the LLaMa weights and the OA xor weights\
          \ as bytes\n    fp_payload = gzip.open(src_payload, 'rb')\n    fp_base =\
          \ open(src_base, 'rb')\n\n    # create/open a file at the destination path\
          \ \n    with open(dst, 'wb') as fp:\n        while True:\n\n           \
          \ # loop through both weights in block_size/4096 byte increments, create\
          \ numpy arrays for each\n            buf1 = numpy.array(bytearray(fp_payload.read(block_size)),\
          \ dtype=numpy.uint8)\n            buf2 = numpy.array(bytearray(fp_base.read(block_size)),\
          \ dtype=numpy.uint8)\n\n            # Padding seems to just check that buf1\
          \ and buf2 are the same length and adds zeros or trims an array to make\
          \ them same length if they're not.\n            padding = len(buf1) - len(buf2)\n\
          \            if padding > 0: buf2 = numpy.pad(buf2, (0, padding), 'constant',\
          \ constant_values=(0,))\n            if padding < 0: buf2 = buf2[:len(buf1)]\n\
          \n            # Then uses numpy bitwise_xor which performs an XOR between\
          \ the chunks of the two weights. The resulting array is assigned to `buf`\
          \ variable\n            buf = numpy.bitwise_xor(buf1, buf2)\n\n        \
          \    # then write that array to the file and continue with the next loop\
          \ until both files are exhausted\n            fp.write(buf)\n          \
          \  if len(buf1) < block_size: break\n    fp_payload.close()\n    fp_base.close()\n\
          ```\n\nThe end result is the XOR operation between the original LLaMA weights\
          \ and the OA XOR weights, which means the XOR of the two spits out the **real**\
          \ weights for the model. \n\nHopefully that helps and I didn't misinterpret\
          \ it"
        updatedAt: '2023-04-26T01:31:23.651Z'
      numEdits: 2
      reactions:
      - count: 7
        reaction: "\u2764\uFE0F"
        users:
        - shmalex
        - evanLF
        - passthebutter
        - atulloona
        - AayushShah
        - abdnelly
        - versae
    id: 64487d513e498d66919facba
    type: comment
  author: b-mc2
  content: "I won't speak for all the parts of it, but the general idea appears to\
    \ be OpenAssistant ships weights that aren't actually the weights, but become\
    \ the weights when [XOR'd](https://en.wikipedia.org/wiki/XOR_gate) against the\
    \ original LLaMa weights. This one uses numpy [bitwise_xor()](https://numpy.org/doc/stable/reference/generated/numpy.bitwise_xor.html).\n\
    \n\n`Due to the license attached to LLaMa models by Meta AI it is not possible\
    \ to directly distribute LLaMa-based models. Instead we provide XOR weights for\
    \ the OA models....To use OpenAssistant LLaMa-Based Models, you need to have a\
    \ copy of the original LLaMa model weights and add them to a llama subdirectory\
    \ here.`\n\nThis is basically saying `we aren't allowed to give you the list [1,2,3,4,5]\
    \ or our version of it [1,2,5,6,9] but we can give you [0, 0, 6, 2, 12] and if\
    \ you XOR it against the original list...which we won't give you, you'll get our\
    \ modified version`\n\nIn other words, they won't give you LLaMa weights or the\
    \ LLaMa based oasst-sft-6-llama-30b weights, but they will give you a way to create\
    \ them yourself if you already have the LLaMa weights.\n\nToy example:\n```python\n\
    llama_weights = [1, 2, 3, 4, 5]\noa_xor_weights = [0, 0, 6, 2, 12]\n# the actual\
    \ desired weights of the model are [1, 2, 5, 6, 9]\n# zip the two weights and\
    \ perform an XOR, 1^0, 2^0, 3^6, 4^2, 5^12\n\noasst_sft_6_llama_30b = [a^b for\
    \ a, b in zip(llama_weights, oa_xor_weights)]\n# oasst_sft_6_llama_30b = [1, 2,\
    \ 5, 6, 9]\n```\n\nBack to the actual code...so after all the initial steps in\
    \ the model card instructions, the `xor_codec.py` file gets run from the command\
    \ line like this `python xor_codec.py oasst-sft-6-llama-30b/ oasst-sft-6-llama-30b-xor/\
    \ llama30b_hf/` This passes the paths to both the original LLaMa weights and the\
    \ OA \"weights\" to the `xor_decode()` function [here](https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/blob/main/xor_codec.py#L40)\
    \ unless one of the flags is change from their defaults.\n\nI've added some comments\
    \ below\n```python\ndef xor_decode(dst, src_payload, src_base, block_size=4096):\n\
    \    # open both the LLaMa weights and the OA xor weights as bytes\n    fp_payload\
    \ = gzip.open(src_payload, 'rb')\n    fp_base = open(src_base, 'rb')\n\n    #\
    \ create/open a file at the destination path \n    with open(dst, 'wb') as fp:\n\
    \        while True:\n\n            # loop through both weights in block_size/4096\
    \ byte increments, create numpy arrays for each\n            buf1 = numpy.array(bytearray(fp_payload.read(block_size)),\
    \ dtype=numpy.uint8)\n            buf2 = numpy.array(bytearray(fp_base.read(block_size)),\
    \ dtype=numpy.uint8)\n\n            # Padding seems to just check that buf1 and\
    \ buf2 are the same length and adds zeros or trims an array to make them same\
    \ length if they're not.\n            padding = len(buf1) - len(buf2)\n      \
    \      if padding > 0: buf2 = numpy.pad(buf2, (0, padding), 'constant', constant_values=(0,))\n\
    \            if padding < 0: buf2 = buf2[:len(buf1)]\n\n            # Then uses\
    \ numpy bitwise_xor which performs an XOR between the chunks of the two weights.\
    \ The resulting array is assigned to `buf` variable\n            buf = numpy.bitwise_xor(buf1,\
    \ buf2)\n\n            # then write that array to the file and continue with the\
    \ next loop until both files are exhausted\n            fp.write(buf)\n      \
    \      if len(buf1) < block_size: break\n    fp_payload.close()\n    fp_base.close()\n\
    ```\n\nThe end result is the XOR operation between the original LLaMA weights\
    \ and the OA XOR weights, which means the XOR of the two spits out the **real**\
    \ weights for the model. \n\nHopefully that helps and I didn't misinterpret it"
  created_at: 2023-04-26 00:24:33+00:00
  edited: true
  hidden: false
  id: 64487d513e498d66919facba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672994786123-63b78c85bd2d15352280d265.png?w=200&h=200&f=face
      fullname: J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: razodactyl
      type: user
    createdAt: '2023-04-26T02:13:53.000Z'
    data:
      edited: true
      editors:
      - razodactyl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672994786123-63b78c85bd2d15352280d265.png?w=200&h=200&f=face
          fullname: J
          isHf: false
          isPro: false
          name: razodactyl
          type: user
        html: '<p>Or you know... Meta could just support open research.</p>

          '
        raw: Or you know... Meta could just support open research.
        updatedAt: '2023-04-26T02:14:34.765Z'
      numEdits: 1
      reactions: []
    id: 644888e1058f3572dd1eaec0
    type: comment
  author: razodactyl
  content: Or you know... Meta could just support open research.
  created_at: 2023-04-26 01:13:53+00:00
  edited: true
  hidden: false
  id: 644888e1058f3572dd1eaec0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678892702901-6303f5f37b50dd9d0a371b28.jpeg?w=200&h=200&f=face
      fullname: Oliver Stanley
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: OllieStanley
      type: user
    createdAt: '2023-04-26T12:49:32.000Z'
    data:
      edited: false
      editors:
      - OllieStanley
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678892702901-6303f5f37b50dd9d0a371b28.jpeg?w=200&h=200&f=face
          fullname: Oliver Stanley
          isHf: false
          isPro: false
          name: OllieStanley
          type: user
        html: '<blockquote>

          <p>Or you know... Meta could just support open research.</p>

          </blockquote>

          <p>Yep, but they don''t, so for now we distribute XORs to comply with their
          license :)</p>

          '
        raw: '> Or you know... Meta could just support open research.


          Yep, but they don''t, so for now we distribute XORs to comply with their
          license :)'
        updatedAt: '2023-04-26T12:49:32.912Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64491ddcd5d86def91cf4266
    id: 64491ddcd5d86def91cf4265
    type: comment
  author: OllieStanley
  content: '> Or you know... Meta could just support open research.


    Yep, but they don''t, so for now we distribute XORs to comply with their license
    :)'
  created_at: 2023-04-26 11:49:32+00:00
  edited: false
  hidden: false
  id: 64491ddcd5d86def91cf4265
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678892702901-6303f5f37b50dd9d0a371b28.jpeg?w=200&h=200&f=face
      fullname: Oliver Stanley
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: OllieStanley
      type: user
    createdAt: '2023-04-26T12:49:32.000Z'
    data:
      status: closed
    id: 64491ddcd5d86def91cf4266
    type: status-change
  author: OllieStanley
  created_at: 2023-04-26 11:49:32+00:00
  id: 64491ddcd5d86def91cf4266
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: OpenAssistant/oasst-sft-6-llama-30b-xor
repo_type: model
status: closed
target_branch: null
title: 'What exactly does xor_codec.py do? '
