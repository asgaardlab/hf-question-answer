!!python/object:huggingface_hub.community.DiscussionWithDetails
author: InsafQ
conflicting_files: null
created_at: 2023-05-01 11:42:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df1fc2d8721f54f8a76e0dabc9ef7014.svg
      fullname: Ashrapov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: InsafQ
      type: user
    createdAt: '2023-05-01T12:42:43.000Z'
    data:
      edited: false
      editors:
      - InsafQ
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df1fc2d8721f54f8a76e0dabc9ef7014.svg
          fullname: Ashrapov
          isHf: false
          isPro: false
          name: InsafQ
          type: user
        html: '<p>How to chat with model via python API?</p>

          '
        raw: How to chat with model via python API?
        updatedAt: '2023-05-01T12:42:43.265Z'
      numEdits: 0
      reactions: []
    id: 644fb3c3d43fedb824b9274b
    type: comment
  author: InsafQ
  content: How to chat with model via python API?
  created_at: 2023-05-01 11:42:43+00:00
  edited: false
  hidden: false
  id: 644fb3c3d43fedb824b9274b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d169d873818023a3659dae0cb86be2c.svg
      fullname: c c
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aszfcxcgszdx
      type: user
    createdAt: '2023-05-01T14:19:14.000Z'
    data:
      edited: false
      editors:
      - aszfcxcgszdx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d169d873818023a3659dae0cb86be2c.svg
          fullname: c c
          isHf: false
          isPro: false
          name: aszfcxcgszdx
          type: user
        html: '<p>You can''t. These are delta weights, and you need to apply them
          to the actual LLama weights to get the OpenAssisant weights.</p>

          '
        raw: You can't. These are delta weights, and you need to apply them to the
          actual LLama weights to get the OpenAssisant weights.
        updatedAt: '2023-05-01T14:19:14.493Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - halilergul1
    id: 644fca6220ba3e3e4be94122
    type: comment
  author: aszfcxcgszdx
  content: You can't. These are delta weights, and you need to apply them to the actual
    LLama weights to get the OpenAssisant weights.
  created_at: 2023-05-01 13:19:14+00:00
  edited: false
  hidden: false
  id: 644fca6220ba3e3e4be94122
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6166b9b36289524ce2e3f441366c9ac2.svg
      fullname: "Halil \u0130brahim Erg\xFCl"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: halilergul1
      type: user
    createdAt: '2023-05-01T17:45:47.000Z'
    data:
      edited: true
      editors:
      - halilergul1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6166b9b36289524ce2e3f441366c9ac2.svg
          fullname: "Halil \u0130brahim Erg\xFCl"
          isHf: false
          isPro: false
          name: halilergul1
          type: user
        html: '<p>So when we apply them to the actual LLama weights, can we run it
          in local to get a kind of chat engine similar to web UI. Or is it totally
          another type of engineering stuff to do ?</p>

          '
        raw: So when we apply them to the actual LLama weights, can we run it in local
          to get a kind of chat engine similar to web UI. Or is it totally another
          type of engineering stuff to do ?
        updatedAt: '2023-05-01T17:46:13.522Z'
      numEdits: 1
      reactions: []
    id: 644ffacbd5f7dafcfa64eca6
    type: comment
  author: halilergul1
  content: So when we apply them to the actual LLama weights, can we run it in local
    to get a kind of chat engine similar to web UI. Or is it totally another type
    of engineering stuff to do ?
  created_at: 2023-05-01 16:45:47+00:00
  edited: true
  hidden: false
  id: 644ffacbd5f7dafcfa64eca6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d169d873818023a3659dae0cb86be2c.svg
      fullname: c c
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aszfcxcgszdx
      type: user
    createdAt: '2023-05-02T12:46:10.000Z'
    data:
      edited: false
      editors:
      - aszfcxcgszdx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d169d873818023a3659dae0cb86be2c.svg
          fullname: c c
          isHf: false
          isPro: false
          name: aszfcxcgszdx
          type: user
        html: '<p>I think you can run it with transformers'' automodel, as transformers
          supports LLama inference</p>

          '
        raw: I think you can run it with transformers' automodel, as transformers
          supports LLama inference
        updatedAt: '2023-05-02T12:46:10.666Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - halilergul1
    id: 645106129d916c596e261e01
    type: comment
  author: aszfcxcgszdx
  content: I think you can run it with transformers' automodel, as transformers supports
    LLama inference
  created_at: 2023-05-02 11:46:10+00:00
  edited: false
  hidden: false
  id: 645106129d916c596e261e01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb12bf78ebd9bad77b26cb72dcfdb8db.svg
      fullname: Julian Cai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tztsai
      type: user
    createdAt: '2023-05-02T15:23:05.000Z'
    data:
      edited: true
      editors:
      - tztsai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb12bf78ebd9bad77b26cb72dcfdb8db.svg
          fullname: Julian Cai
          isHf: false
          isPro: false
          name: tztsai
          type: user
        html: '<p>Try <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui">text
          generation web UI</a>. It can load this model quite simply if you have about
          70GB of GPU memory. It also allows you to load the weights in 8-bit quantization,
          which reduces the memory requirement approximately by half.</p>

          '
        raw: Try [text generation web UI](https://github.com/oobabooga/text-generation-webui).
          It can load this model quite simply if you have about 70GB of GPU memory.
          It also allows you to load the weights in 8-bit quantization, which reduces
          the memory requirement approximately by half.
        updatedAt: '2023-05-02T15:24:08.778Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - halilergul1
    id: 64512ad99d916c596e2a8bfd
    type: comment
  author: tztsai
  content: Try [text generation web UI](https://github.com/oobabooga/text-generation-webui).
    It can load this model quite simply if you have about 70GB of GPU memory. It also
    allows you to load the weights in 8-bit quantization, which reduces the memory
    requirement approximately by half.
  created_at: 2023-05-02 14:23:05+00:00
  edited: true
  hidden: false
  id: 64512ad99d916c596e2a8bfd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c799879489c0b6c6bea48ff8f5deb592.svg
      fullname: Aaron B
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kaxon
      type: user
    createdAt: '2023-05-22T00:23:09.000Z'
    data:
      edited: false
      editors:
      - Kaxon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c799879489c0b6c6bea48ff8f5deb592.svg
          fullname: Aaron B
          isHf: false
          isPro: false
          name: Kaxon
          type: user
        html: '<blockquote>

          <p>Try <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui">text
          generation web UI</a>. It can load this model quite simply if you have about
          70GB of GPU memory. It also allows you to load the weights in 8-bit quantization,
          which reduces the memory requirement approximately by half.</p>

          </blockquote>

          <p>70GB of VRAM? What rig do you have?</p>

          '
        raw: '> Try [text generation web UI](https://github.com/oobabooga/text-generation-webui).
          It can load this model quite simply if you have about 70GB of GPU memory.
          It also allows you to load the weights in 8-bit quantization, which reduces
          the memory requirement approximately by half.


          70GB of VRAM? What rig do you have?'
        updatedAt: '2023-05-22T00:23:09.465Z'
      numEdits: 0
      reactions: []
    id: 646ab5ed96cfe72aef814d6f
    type: comment
  author: Kaxon
  content: '> Try [text generation web UI](https://github.com/oobabooga/text-generation-webui).
    It can load this model quite simply if you have about 70GB of GPU memory. It also
    allows you to load the weights in 8-bit quantization, which reduces the memory
    requirement approximately by half.


    70GB of VRAM? What rig do you have?'
  created_at: 2023-05-21 23:23:09+00:00
  edited: false
  hidden: false
  id: 646ab5ed96cfe72aef814d6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb12bf78ebd9bad77b26cb72dcfdb8db.svg
      fullname: Julian Cai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tztsai
      type: user
    createdAt: '2023-05-22T09:27:05.000Z'
    data:
      edited: false
      editors:
      - tztsai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb12bf78ebd9bad77b26cb72dcfdb8db.svg
          fullname: Julian Cai
          isHf: false
          isPro: false
          name: tztsai
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Try <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui">text
          generation web UI</a>. It can load this model quite simply if you have about
          70GB of GPU memory. It also allows you to load the weights in 8-bit quantization,
          which reduces the memory requirement approximately by half.</p>

          </blockquote>

          <p>70GB of VRAM? What rig do you have?</p>

          </blockquote>

          <p>My work station has 3 GPUs with 24 GB memory each.</p>

          '
        raw: "> > Try [text generation web UI](https://github.com/oobabooga/text-generation-webui).\
          \ It can load this model quite simply if you have about 70GB of GPU memory.\
          \ It also allows you to load the weights in 8-bit quantization, which reduces\
          \ the memory requirement approximately by half.\n> \n> 70GB of VRAM? What\
          \ rig do you have?\n\nMy work station has 3 GPUs with 24 GB memory each."
        updatedAt: '2023-05-22T09:27:05.096Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Kaxon
    id: 646b35695d68f5c15a181f5d
    type: comment
  author: tztsai
  content: "> > Try [text generation web UI](https://github.com/oobabooga/text-generation-webui).\
    \ It can load this model quite simply if you have about 70GB of GPU memory. It\
    \ also allows you to load the weights in 8-bit quantization, which reduces the\
    \ memory requirement approximately by half.\n> \n> 70GB of VRAM? What rig do you\
    \ have?\n\nMy work station has 3 GPUs with 24 GB memory each."
  created_at: 2023-05-22 08:27:05+00:00
  edited: false
  hidden: false
  id: 646b35695d68f5c15a181f5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645aad59c4acfcf664022df5/yd-2hSU7bjcY1zEbe_Hul.jpeg?w=200&h=200&f=face
      fullname: Owen Welfel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CodeSoft
      type: user
    createdAt: '2023-06-06T22:50:00.000Z'
    data:
      edited: false
      editors:
      - CodeSoft
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9094982147216797
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645aad59c4acfcf664022df5/yd-2hSU7bjcY1zEbe_Hul.jpeg?w=200&h=200&f=face
          fullname: Owen Welfel
          isHf: false
          isPro: false
          name: CodeSoft
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>Try <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui">text
          generation web UI</a>. It can load this model quite simply if you have about
          70GB of GPU memory. It also allows you to load the weights in 8-bit quantization,
          which reduces the memory requirement approximately by half.</p>

          </blockquote>

          <p>70GB of VRAM? What rig do you have?</p>

          </blockquote>

          <p>My work station has 3 GPUs with 24 GB memory each.</p>

          </blockquote>

          <p>THREE GPUs? I only have a 1650! It takes so long to train models :sob:</p>

          '
        raw: "> > > Try [text generation web UI](https://github.com/oobabooga/text-generation-webui).\
          \ It can load this model quite simply if you have about 70GB of GPU memory.\
          \ It also allows you to load the weights in 8-bit quantization, which reduces\
          \ the memory requirement approximately by half.\n> > \n> > 70GB of VRAM?\
          \ What rig do you have?\n> \n> My work station has 3 GPUs with 24 GB memory\
          \ each.\n\nTHREE GPUs? I only have a 1650! It takes so long to train models\
          \ :sob:"
        updatedAt: '2023-06-06T22:50:00.727Z'
      numEdits: 0
      reactions: []
    id: 647fb818cbb8294ed80b03bf
    type: comment
  author: CodeSoft
  content: "> > > Try [text generation web UI](https://github.com/oobabooga/text-generation-webui).\
    \ It can load this model quite simply if you have about 70GB of GPU memory. It\
    \ also allows you to load the weights in 8-bit quantization, which reduces the\
    \ memory requirement approximately by half.\n> > \n> > 70GB of VRAM? What rig\
    \ do you have?\n> \n> My work station has 3 GPUs with 24 GB memory each.\n\nTHREE\
    \ GPUs? I only have a 1650! It takes so long to train models :sob:"
  created_at: 2023-06-06 21:50:00+00:00
  edited: false
  hidden: false
  id: 647fb818cbb8294ed80b03bf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 33
repo_id: OpenAssistant/oasst-sft-6-llama-30b-xor
repo_type: model
status: open
target_branch: null
title: How to chat with model via API?
