!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xiangdal
conflicting_files: null
created_at: 2023-04-27 01:18:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ffa0b99a9752a897dcbfabf85f70ff3.svg
      fullname: Xiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xiangdal
      type: user
    createdAt: '2023-04-27T02:18:15.000Z'
    data:
      edited: false
      editors:
      - xiangdal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ffa0b99a9752a897dcbfabf85f70ff3.svg
          fullname: Xiang
          isHf: false
          isPro: false
          name: xiangdal
          type: user
        html: "<p>Where can I locate this params.json file?</p>\n<pre><code>python\
          \ src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n--input_dir\
          \  /home/ec2-user/llm/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor\
          \ \\\n--output_dir /home/ec2-user/llm/oasst-sft-6-llama-30b \\\n--model_size\
          \ 30B\nTraceback (most recent call last):\n  File \"/home/ec2-user/llm-endpoints/xor-transformation/transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py\"\
          , line 278, in &lt;module&gt;\n    main()\n  File \"/home/ec2-user/llm-endpoints/xor-transformation/transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py\"\
          , line 268, in main\n    write_model(\n  File \"/home/ec2-user/llm-endpoints/xor-transformation/transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py\"\
          , line 90, in write_model\n    params = read_json(os.path.join(input_base_path,\
          \ \"params.json\"))\n  File \"/home/ec2-user/llm-endpoints/xor-transformation/transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py\"\
          , line 76, in read_json\n    with open(path, \"r\") as f:\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: '/home/ec2-user/llm/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor/30B/params.json'\n\
          </code></pre>\n"
        raw: "Where can I locate this params.json file?\r\n```\r\npython src/transformers/models/llama/convert_llama_weights_to_hf.py\
          \ \\\r\n--input_dir  /home/ec2-user/llm/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor\
          \ \\\r\n--output_dir /home/ec2-user/llm/oasst-sft-6-llama-30b \\\r\n--model_size\
          \ 30B\r\nTraceback (most recent call last):\r\n  File \"/home/ec2-user/llm-endpoints/xor-transformation/transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py\"\
          , line 278, in <module>\r\n    main()\r\n  File \"/home/ec2-user/llm-endpoints/xor-transformation/transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py\"\
          , line 268, in main\r\n    write_model(\r\n  File \"/home/ec2-user/llm-endpoints/xor-transformation/transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py\"\
          , line 90, in write_model\r\n    params = read_json(os.path.join(input_base_path,\
          \ \"params.json\"))\r\n  File \"/home/ec2-user/llm-endpoints/xor-transformation/transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py\"\
          , line 76, in read_json\r\n    with open(path, \"r\") as f:\r\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: '/home/ec2-user/llm/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor/30B/params.json'\r\
          \n```"
        updatedAt: '2023-04-27T02:18:15.657Z'
      numEdits: 0
      reactions: []
    id: 6449db6786e837e3d58725c9
    type: comment
  author: xiangdal
  content: "Where can I locate this params.json file?\r\n```\r\npython src/transformers/models/llama/convert_llama_weights_to_hf.py\
    \ \\\r\n--input_dir  /home/ec2-user/llm/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor\
    \ \\\r\n--output_dir /home/ec2-user/llm/oasst-sft-6-llama-30b \\\r\n--model_size\
    \ 30B\r\nTraceback (most recent call last):\r\n  File \"/home/ec2-user/llm-endpoints/xor-transformation/transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py\"\
    , line 278, in <module>\r\n    main()\r\n  File \"/home/ec2-user/llm-endpoints/xor-transformation/transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py\"\
    , line 268, in main\r\n    write_model(\r\n  File \"/home/ec2-user/llm-endpoints/xor-transformation/transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py\"\
    , line 90, in write_model\r\n    params = read_json(os.path.join(input_base_path,\
    \ \"params.json\"))\r\n  File \"/home/ec2-user/llm-endpoints/xor-transformation/transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py\"\
    , line 76, in read_json\r\n    with open(path, \"r\") as f:\r\nFileNotFoundError:\
    \ [Errno 2] No such file or directory: '/home/ec2-user/llm/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor/30B/params.json'\r\
    \n```"
  created_at: 2023-04-27 01:18:15+00:00
  edited: false
  hidden: false
  id: 6449db6786e837e3d58725c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6de2cd3373e5d64185c8429c18e438e5.svg
      fullname: Mahesh Sinha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mahesh00000
      type: user
    createdAt: '2023-04-27T05:06:51.000Z'
    data:
      edited: false
      editors:
      - mahesh00000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6de2cd3373e5d64185c8429c18e438e5.svg
          fullname: Mahesh Sinha
          isHf: false
          isPro: false
          name: mahesh00000
          type: user
        html: '<p>What I can understand is that for this model conversion we need
          llama weights which have params.json.</p>

          '
        raw: What I can understand is that for this model conversion we need llama
          weights which have params.json.
        updatedAt: '2023-04-27T05:06:51.307Z'
      numEdits: 0
      reactions: []
    id: 644a02eb111b3bf687898459
    type: comment
  author: mahesh00000
  content: What I can understand is that for this model conversion we need llama weights
    which have params.json.
  created_at: 2023-04-27 04:06:51+00:00
  edited: false
  hidden: false
  id: 644a02eb111b3bf687898459
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/28687ed08eaf63496dfe9cbaaa752554.svg
      fullname: Spam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DontLike
      type: user
    createdAt: '2023-04-27T05:46:56.000Z'
    data:
      edited: false
      editors:
      - DontLike
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/28687ed08eaf63496dfe9cbaaa752554.svg
          fullname: Spam
          isHf: false
          isPro: false
          name: DontLike
          type: user
        html: '<p>Each Llama model has params.json in it''s folder so you get the
          file from the place you downloaded Llama from... Or just find a new place
          where they have it. Google is quite successful at finding that.</p>

          '
        raw: Each Llama model has params.json in it's folder so you get the file from
          the place you downloaded Llama from... Or just find a new place where they
          have it. Google is quite successful at finding that.
        updatedAt: '2023-04-27T05:46:56.789Z'
      numEdits: 0
      reactions: []
    id: 644a0c50a281f51a62bb7d7c
    type: comment
  author: DontLike
  content: Each Llama model has params.json in it's folder so you get the file from
    the place you downloaded Llama from... Or just find a new place where they have
    it. Google is quite successful at finding that.
  created_at: 2023-04-27 04:46:56+00:00
  edited: false
  hidden: false
  id: 644a0c50a281f51a62bb7d7c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: OpenAssistant/oasst-sft-6-llama-30b-xor
repo_type: model
status: open
target_branch: null
title: 'XOR conversion fails: missing params.json'
