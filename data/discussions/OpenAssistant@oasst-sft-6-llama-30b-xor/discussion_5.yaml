!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Grambel
conflicting_files: null
created_at: 2023-04-23 15:18:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f8aa181f5389b3f06c301dcf2f10973.svg
      fullname: Gram Bel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Grambel
      type: user
    createdAt: '2023-04-23T16:18:54.000Z'
    data:
      edited: false
      editors:
      - Grambel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f8aa181f5389b3f06c301dcf2f10973.svg
          fullname: Gram Bel
          isHf: false
          isPro: false
          name: Grambel
          type: user
        html: "<p>I followed all the steps and I now have the transformed weights.\
          \ How do I use these weights to run inference with a quantized model on\
          \ GPU?</p>\n<p>I tried <code>model = LlamaForCausalLM.from_pretrained('&lt;path_to_weights&gt;/oasst-sft-6-llama-30b/',\
          \ load_in_8bit=True, device_map='auto')</code>, but this gives the following\
          \ error:</p>\n<pre><code>ValueError: \n    Some modules are dispatched on\
          \ the CPU or the disk. Make sure you have enough GPU RAM to fit\n    the\
          \ quantized model. If you want to dispatch the model on the CPU or the disk\
          \ while keeping\n    these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True`\
          \ and pass a custom\n    `device_map` to `from_pretrained`. Check\n    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n\
          \    for more details.\n</code></pre>\n<p>Any suggestions on how to run\
          \ this on a 48 GB GPU?</p>\n"
        raw: "I followed all the steps and I now have the transformed weights. How\
          \ do I use these weights to run inference with a quantized model on GPU?\r\
          \n\r\nI tried `model = LlamaForCausalLM.from_pretrained('<path_to_weights>/oasst-sft-6-llama-30b/',\
          \ load_in_8bit=True, device_map='auto')`, but this gives the following error:\r\
          \n\r\n```\r\nValueError: \r\n    Some modules are dispatched on the CPU\
          \ or the disk. Make sure you have enough GPU RAM to fit\r\n    the quantized\
          \ model. If you want to dispatch the model on the CPU or the disk while\
          \ keeping\r\n    these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True`\
          \ and pass a custom\r\n    `device_map` to `from_pretrained`. Check\r\n\
          \    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\r\
          \n    for more details.\r\n```\r\n\r\nAny suggestions on how to run this\
          \ on a 48 GB GPU?"
        updatedAt: '2023-04-23T16:18:54.376Z'
      numEdits: 0
      reactions: []
    id: 64455a6eb272430bdbf3754f
    type: comment
  author: Grambel
  content: "I followed all the steps and I now have the transformed weights. How do\
    \ I use these weights to run inference with a quantized model on GPU?\r\n\r\n\
    I tried `model = LlamaForCausalLM.from_pretrained('<path_to_weights>/oasst-sft-6-llama-30b/',\
    \ load_in_8bit=True, device_map='auto')`, but this gives the following error:\r\
    \n\r\n```\r\nValueError: \r\n    Some modules are dispatched on the CPU or the\
    \ disk. Make sure you have enough GPU RAM to fit\r\n    the quantized model. If\
    \ you want to dispatch the model on the CPU or the disk while keeping\r\n    these\
    \ modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and\
    \ pass a custom\r\n    `device_map` to `from_pretrained`. Check\r\n    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\r\
    \n    for more details.\r\n```\r\n\r\nAny suggestions on how to run this on a\
    \ 48 GB GPU?"
  created_at: 2023-04-23 15:18:54+00:00
  edited: false
  hidden: false
  id: 64455a6eb272430bdbf3754f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-04-23T19:22:33.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<p>I faced this error with vicuna-13b model. I saw that upgrading accelerate
          to 0.18.0 would resolve this error.</p>

          '
        raw: I faced this error with vicuna-13b model. I saw that upgrading accelerate
          to 0.18.0 would resolve this error.
        updatedAt: '2023-04-23T19:22:33.818Z'
      numEdits: 0
      reactions: []
    id: 6445857953ecc52f50f3df1b
    type: comment
  author: gsaivinay
  content: I faced this error with vicuna-13b model. I saw that upgrading accelerate
    to 0.18.0 would resolve this error.
  created_at: 2023-04-23 18:22:33+00:00
  edited: false
  hidden: false
  id: 6445857953ecc52f50f3df1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f8aa181f5389b3f06c301dcf2f10973.svg
      fullname: Gram Bel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Grambel
      type: user
    createdAt: '2023-04-23T19:53:32.000Z'
    data:
      edited: true
      editors:
      - Grambel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f8aa181f5389b3f06c301dcf2f10973.svg
          fullname: Gram Bel
          isHf: false
          isPro: false
          name: Grambel
          type: user
        html: '<p>accelerate is already on 0.18.0.</p>

          '
        raw: accelerate is already on 0.18.0.
        updatedAt: '2023-04-23T19:53:43.945Z'
      numEdits: 1
      reactions: []
    id: 64458cbc0f2fc80feb29d2ef
    type: comment
  author: Grambel
  content: accelerate is already on 0.18.0.
  created_at: 2023-04-23 18:53:32+00:00
  edited: true
  hidden: false
  id: 64458cbc0f2fc80feb29d2ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ccba54c8cb198bed3851d046b74b8cd.svg
      fullname: patrick siyou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: psiyou
      type: user
    createdAt: '2023-04-27T07:24:05.000Z'
    data:
      edited: false
      editors:
      - psiyou
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ccba54c8cb198bed3851d046b74b8cd.svg
          fullname: patrick siyou
          isHf: false
          isPro: false
          name: psiyou
          type: user
        html: '<p>has anyone found the solution? I''m facing the same issue.</p>

          '
        raw: has anyone found the solution? I'm facing the same issue.
        updatedAt: '2023-04-27T07:24:05.256Z'
      numEdits: 0
      reactions: []
    id: 644a2315ca8809b635d9da93
    type: comment
  author: psiyou
  content: has anyone found the solution? I'm facing the same issue.
  created_at: 2023-04-27 06:24:05+00:00
  edited: false
  hidden: false
  id: 644a2315ca8809b635d9da93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba822d01be17abcacf8d43bf1496f022.svg
      fullname: Muhammad Ahtesham Ul Haq
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ahtesham1
      type: user
    createdAt: '2023-04-27T22:43:33.000Z'
    data:
      edited: true
      editors:
      - ahtesham1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba822d01be17abcacf8d43bf1496f022.svg
          fullname: Muhammad Ahtesham Ul Haq
          isHf: false
          isPro: false
          name: ahtesham1
          type: user
        html: '<p>Following is the inference code.<br>I am importing the train.py
          from Stanford alpaca repo.</p>

          <p>My Env:</p>

          <p>absl-py==1.4.0<br>accelerate==0.18.0<br>aiohttp==3.8.4<br>aiosignal==1.3.1<br>appdirs==1.4.4<br>async-timeout==4.0.2<br>attrs==23.1.0<br>bitsandbytes==0.38.1<br>certifi==2022.12.7<br>charset-normalizer==3.1.0<br>click==8.1.3<br>cmake==3.26.3<br>docker-pycreds==0.4.0<br>filelock==3.12.0<br>fire==0.5.0<br>frozenlist==1.3.3<br>fsspec==2023.4.0<br>gitdb==4.0.10<br>GitPython==3.1.31<br>huggingface-hub==0.14.1<br>idna==3.4<br>Jinja2==3.1.2<br>joblib==1.2.0<br>lit==16.0.2<br>MarkupSafe==2.1.2<br>mpmath==1.3.0<br>multidict==6.0.4<br>networkx==3.1<br>nltk==3.8.1<br>numpy==1.24.3<br>nvidia-cublas-cu11==11.10.3.66<br>nvidia-cuda-cupti-cu11==11.7.101<br>nvidia-cuda-nvrtc-cu11==11.7.99<br>nvidia-cuda-runtime-cu11==11.7.99<br>nvidia-cudnn-cu11==8.5.0.96<br>nvidia-cufft-cu11==10.9.0.58<br>nvidia-curand-cu11==10.2.10.91<br>nvidia-cusolver-cu11==11.4.0.1<br>nvidia-cusparse-cu11==11.7.4.91<br>nvidia-nccl-cu11==2.14.3<br>nvidia-nvtx-cu11==11.7.91<br>openai==0.27.4<br>packaging==23.1<br>pathtools==0.1.2<br>protobuf==4.22.3<br>psutil==5.9.5<br>PyYAML==6.0<br>regex==2023.3.23<br>requests==2.28.2<br>rouge-score==0.1.2<br>sentencepiece==0.1.98<br>sentry-sdk==1.21.0<br>setproctitle==1.3.2<br>six==1.16.0<br>smmap==5.0.0<br>sympy==1.11.1<br>termcolor==2.3.0<br>tokenizers==0.13.3<br>torch==2.0.0<br>tqdm==4.65.0<br>transformers==4.28.1<br>triton==2.0.0<br>typing_extensions==4.5.0<br>urllib3==1.26.15<br>wandb==0.15.0<br>yarl==1.9.1</p>

          <p>from dataclasses import dataclass, field</p>

          <p>import numpy as np<br>import torch<br>import transformers<br>from transformers
          import GenerationConfig</p>

          <p>from train import ModelArguments, smart_tokenizer_and_embedding_resize,
          DEFAULT_PAD_TOKEN, DEFAULT_EOS_TOKEN, <br>  DEFAULT_BOS_TOKEN, DEFAULT_UNK_TOKEN,
          PROMPT_DICT</p>

          <p>@dataclass<br>class InferenceArguments:<br>  model_max_length: int =
          field(<br>    default=512,<br>    metadata={"help": "Maximum sequence length.
          Sequences will be right padded (and possibly truncated)."},<br>  )<br>  load_in_8bit:
          bool = field(<br>    default=False,<br>    metadata={"help": "Load the model
          in 8-bit mode."},<br>  )<br>  inference_dtype: torch.dtype = field(<br>    default=torch.float32,<br>    metadata={"help":
          "The dtype to use for inference."},<br>  )</p>

          <p>def generate_prompt(instruction, input=None):<br>  if input:<br>    return
          PROMPT_DICT["prompt_input"].format(instruction=instruction, input=input)<br>  else:<br>    return
          PROMPT_DICT["prompt_no_input"].format(instruction=instruction)</p>

          <p>def inference():<br>  parser = transformers.HfArgumentParser((ModelArguments,
          InferenceArguments))<br>  model_args, inference_args = parser.parse_args_into_dataclasses()</p>

          <p>  model = transformers.AutoModelForCausalLM.from_pretrained(<br>    "./oasst-sft-6-llama-30b",<br>    load_in_8bit=inference_args.load_in_8bit,<br>    torch_dtype=inference_args.inference_dtype,<br>    device_map="auto",<br>  )<br>  model.cpu()<br>  model.eval()</p>

          <p>  generation_config = GenerationConfig(<br>    temperature=0.1,<br>    top_p=0.75,<br>    num_beams=4,<br>  )</p>

          <p>  tokenizer = transformers.AutoTokenizer.from_pretrained(<br>    "./oasst-sft-6-llama-30b",<br>    use_fast=False,<br>    model_max_length=inference_args.model_max_length,<br>  )</p>

          <p>  if tokenizer.pad_token is None:<br>    smart_tokenizer_and_embedding_resize(<br>      special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),<br>      tokenizer=tokenizer,<br>      model=model,<br>    )<br>  tokenizer.add_special_tokens(<br>    {<br>      "eos_token":
          DEFAULT_EOS_TOKEN,<br>      "bos_token": DEFAULT_BOS_TOKEN,<br>      "unk_token":
          DEFAULT_UNK_TOKEN,<br>    }<br>  )</p>

          <p>  ctx = ""<br>  for instruction in ["Whats your objective?","Who are
          you?","Write me a poem about birds?"]:<br>    print("Instruction:", instruction)<br>    inputs
          = tokenizer(generate_prompt(instruction, None), return_tensors="pt")<br>    outputs
          = model.generate(input_ids=inputs["input_ids"].cpu(),<br>                             generation_config=generation_config,<br>                             max_new_tokens=inference_args.model_max_length,<br>                             return_dict_in_generate=True,<br>                             output_scores=True)<br>    input_length
          = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]<br>    generated_tokens
          = outputs.sequences[:, input_length:]</p>

          <pre><code>ctx += f"Instruction: {instruction}\n" + f"Response: {generated_tokens[0]}\n"

          print("Response:", tokenizer.decode(generated_tokens[0]))

          print()

          </code></pre>

          <p>if <strong>name</strong> == "<strong>main</strong>":<br>  inference()</p>

          '
        raw: "Following is the inference code.\nI am importing the train.py from Stanford\
          \ alpaca repo.\n\nMy Env:\n\nabsl-py==1.4.0\naccelerate==0.18.0\naiohttp==3.8.4\n\
          aiosignal==1.3.1\nappdirs==1.4.4\nasync-timeout==4.0.2\nattrs==23.1.0\n\
          bitsandbytes==0.38.1\ncertifi==2022.12.7\ncharset-normalizer==3.1.0\nclick==8.1.3\n\
          cmake==3.26.3\ndocker-pycreds==0.4.0\nfilelock==3.12.0\nfire==0.5.0\nfrozenlist==1.3.3\n\
          fsspec==2023.4.0\ngitdb==4.0.10\nGitPython==3.1.31\nhuggingface-hub==0.14.1\n\
          idna==3.4\nJinja2==3.1.2\njoblib==1.2.0\nlit==16.0.2\nMarkupSafe==2.1.2\n\
          mpmath==1.3.0\nmultidict==6.0.4\nnetworkx==3.1\nnltk==3.8.1\nnumpy==1.24.3\n\
          nvidia-cublas-cu11==11.10.3.66\nnvidia-cuda-cupti-cu11==11.7.101\nnvidia-cuda-nvrtc-cu11==11.7.99\n\
          nvidia-cuda-runtime-cu11==11.7.99\nnvidia-cudnn-cu11==8.5.0.96\nnvidia-cufft-cu11==10.9.0.58\n\
          nvidia-curand-cu11==10.2.10.91\nnvidia-cusolver-cu11==11.4.0.1\nnvidia-cusparse-cu11==11.7.4.91\n\
          nvidia-nccl-cu11==2.14.3\nnvidia-nvtx-cu11==11.7.91\nopenai==0.27.4\npackaging==23.1\n\
          pathtools==0.1.2\nprotobuf==4.22.3\npsutil==5.9.5\nPyYAML==6.0\nregex==2023.3.23\n\
          requests==2.28.2\nrouge-score==0.1.2\nsentencepiece==0.1.98\nsentry-sdk==1.21.0\n\
          setproctitle==1.3.2\nsix==1.16.0\nsmmap==5.0.0\nsympy==1.11.1\ntermcolor==2.3.0\n\
          tokenizers==0.13.3\ntorch==2.0.0\ntqdm==4.65.0\ntransformers==4.28.1\ntriton==2.0.0\n\
          typing_extensions==4.5.0\nurllib3==1.26.15\nwandb==0.15.0\nyarl==1.9.1\n\
          \n\n\n\n\nfrom dataclasses import dataclass, field\n\nimport numpy as np\n\
          import torch\nimport transformers\nfrom transformers import GenerationConfig\n\
          \nfrom train import ModelArguments, smart_tokenizer_and_embedding_resize,\
          \ DEFAULT_PAD_TOKEN, DEFAULT_EOS_TOKEN, \\\n  DEFAULT_BOS_TOKEN, DEFAULT_UNK_TOKEN,\
          \ PROMPT_DICT\n\n\n@dataclass\nclass InferenceArguments:\n  model_max_length:\
          \ int = field(\n    default=512,\n    metadata={\"help\": \"Maximum sequence\
          \ length. Sequences will be right padded (and possibly truncated).\"},\n\
          \  )\n  load_in_8bit: bool = field(\n    default=False,\n    metadata={\"\
          help\": \"Load the model in 8-bit mode.\"},\n  )\n  inference_dtype: torch.dtype\
          \ = field(\n    default=torch.float32,\n    metadata={\"help\": \"The dtype\
          \ to use for inference.\"},\n  )\n\n\ndef generate_prompt(instruction, input=None):\n\
          \  if input:\n    return PROMPT_DICT[\"prompt_input\"].format(instruction=instruction,\
          \ input=input)\n  else:\n    return PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)\n\
          \n\ndef inference():\n  parser = transformers.HfArgumentParser((ModelArguments,\
          \ InferenceArguments))\n  model_args, inference_args = parser.parse_args_into_dataclasses()\n\
          \n  model = transformers.AutoModelForCausalLM.from_pretrained(\n    \"./oasst-sft-6-llama-30b\"\
          ,\n    load_in_8bit=inference_args.load_in_8bit,\n    torch_dtype=inference_args.inference_dtype,\n\
          \    device_map=\"auto\",\n  )\n  model.cpu()\n  model.eval()\n\n  generation_config\
          \ = GenerationConfig(\n    temperature=0.1,\n    top_p=0.75,\n    num_beams=4,\n\
          \  )\n\n  tokenizer = transformers.AutoTokenizer.from_pretrained(\n    \"\
          ./oasst-sft-6-llama-30b\",\n    use_fast=False,\n    model_max_length=inference_args.model_max_length,\n\
          \  )\n\n  if tokenizer.pad_token is None:\n    smart_tokenizer_and_embedding_resize(\n\
          \      special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n      tokenizer=tokenizer,\n\
          \      model=model,\n    )\n  tokenizer.add_special_tokens(\n    {\n   \
          \   \"eos_token\": DEFAULT_EOS_TOKEN,\n      \"bos_token\": DEFAULT_BOS_TOKEN,\n\
          \      \"unk_token\": DEFAULT_UNK_TOKEN,\n    }\n  )\n\n  ctx = \"\"\n \
          \ for instruction in [\"Whats your objective?\",\"Who are you?\",\"Write\
          \ me a poem about birds?\"]:\n    print(\"Instruction:\", instruction)\n\
          \    inputs = tokenizer(generate_prompt(instruction, None), return_tensors=\"\
          pt\")\n    outputs = model.generate(input_ids=inputs[\"input_ids\"].cpu(),\n\
          \                             generation_config=generation_config,\n   \
          \                          max_new_tokens=inference_args.model_max_length,\n\
          \                             return_dict_in_generate=True,\n          \
          \                   output_scores=True)\n    input_length = 1 if model.config.is_encoder_decoder\
          \ else inputs.input_ids.shape[1]\n    generated_tokens = outputs.sequences[:,\
          \ input_length:]\n\n    ctx += f\"Instruction: {instruction}\\n\" + f\"\
          Response: {generated_tokens[0]}\\n\"\n    print(\"Response:\", tokenizer.decode(generated_tokens[0]))\n\
          \    print()\n\n\nif __name__ == \"__main__\":\n  inference()"
        updatedAt: '2023-04-27T22:44:55.664Z'
      numEdits: 1
      reactions: []
    id: 644afa95af97dfd24c14cc7a
    type: comment
  author: ahtesham1
  content: "Following is the inference code.\nI am importing the train.py from Stanford\
    \ alpaca repo.\n\nMy Env:\n\nabsl-py==1.4.0\naccelerate==0.18.0\naiohttp==3.8.4\n\
    aiosignal==1.3.1\nappdirs==1.4.4\nasync-timeout==4.0.2\nattrs==23.1.0\nbitsandbytes==0.38.1\n\
    certifi==2022.12.7\ncharset-normalizer==3.1.0\nclick==8.1.3\ncmake==3.26.3\ndocker-pycreds==0.4.0\n\
    filelock==3.12.0\nfire==0.5.0\nfrozenlist==1.3.3\nfsspec==2023.4.0\ngitdb==4.0.10\n\
    GitPython==3.1.31\nhuggingface-hub==0.14.1\nidna==3.4\nJinja2==3.1.2\njoblib==1.2.0\n\
    lit==16.0.2\nMarkupSafe==2.1.2\nmpmath==1.3.0\nmultidict==6.0.4\nnetworkx==3.1\n\
    nltk==3.8.1\nnumpy==1.24.3\nnvidia-cublas-cu11==11.10.3.66\nnvidia-cuda-cupti-cu11==11.7.101\n\
    nvidia-cuda-nvrtc-cu11==11.7.99\nnvidia-cuda-runtime-cu11==11.7.99\nnvidia-cudnn-cu11==8.5.0.96\n\
    nvidia-cufft-cu11==10.9.0.58\nnvidia-curand-cu11==10.2.10.91\nnvidia-cusolver-cu11==11.4.0.1\n\
    nvidia-cusparse-cu11==11.7.4.91\nnvidia-nccl-cu11==2.14.3\nnvidia-nvtx-cu11==11.7.91\n\
    openai==0.27.4\npackaging==23.1\npathtools==0.1.2\nprotobuf==4.22.3\npsutil==5.9.5\n\
    PyYAML==6.0\nregex==2023.3.23\nrequests==2.28.2\nrouge-score==0.1.2\nsentencepiece==0.1.98\n\
    sentry-sdk==1.21.0\nsetproctitle==1.3.2\nsix==1.16.0\nsmmap==5.0.0\nsympy==1.11.1\n\
    termcolor==2.3.0\ntokenizers==0.13.3\ntorch==2.0.0\ntqdm==4.65.0\ntransformers==4.28.1\n\
    triton==2.0.0\ntyping_extensions==4.5.0\nurllib3==1.26.15\nwandb==0.15.0\nyarl==1.9.1\n\
    \n\n\n\n\nfrom dataclasses import dataclass, field\n\nimport numpy as np\nimport\
    \ torch\nimport transformers\nfrom transformers import GenerationConfig\n\nfrom\
    \ train import ModelArguments, smart_tokenizer_and_embedding_resize, DEFAULT_PAD_TOKEN,\
    \ DEFAULT_EOS_TOKEN, \\\n  DEFAULT_BOS_TOKEN, DEFAULT_UNK_TOKEN, PROMPT_DICT\n\
    \n\n@dataclass\nclass InferenceArguments:\n  model_max_length: int = field(\n\
    \    default=512,\n    metadata={\"help\": \"Maximum sequence length. Sequences\
    \ will be right padded (and possibly truncated).\"},\n  )\n  load_in_8bit: bool\
    \ = field(\n    default=False,\n    metadata={\"help\": \"Load the model in 8-bit\
    \ mode.\"},\n  )\n  inference_dtype: torch.dtype = field(\n    default=torch.float32,\n\
    \    metadata={\"help\": \"The dtype to use for inference.\"},\n  )\n\n\ndef generate_prompt(instruction,\
    \ input=None):\n  if input:\n    return PROMPT_DICT[\"prompt_input\"].format(instruction=instruction,\
    \ input=input)\n  else:\n    return PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)\n\
    \n\ndef inference():\n  parser = transformers.HfArgumentParser((ModelArguments,\
    \ InferenceArguments))\n  model_args, inference_args = parser.parse_args_into_dataclasses()\n\
    \n  model = transformers.AutoModelForCausalLM.from_pretrained(\n    \"./oasst-sft-6-llama-30b\"\
    ,\n    load_in_8bit=inference_args.load_in_8bit,\n    torch_dtype=inference_args.inference_dtype,\n\
    \    device_map=\"auto\",\n  )\n  model.cpu()\n  model.eval()\n\n  generation_config\
    \ = GenerationConfig(\n    temperature=0.1,\n    top_p=0.75,\n    num_beams=4,\n\
    \  )\n\n  tokenizer = transformers.AutoTokenizer.from_pretrained(\n    \"./oasst-sft-6-llama-30b\"\
    ,\n    use_fast=False,\n    model_max_length=inference_args.model_max_length,\n\
    \  )\n\n  if tokenizer.pad_token is None:\n    smart_tokenizer_and_embedding_resize(\n\
    \      special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n      tokenizer=tokenizer,\n\
    \      model=model,\n    )\n  tokenizer.add_special_tokens(\n    {\n      \"eos_token\"\
    : DEFAULT_EOS_TOKEN,\n      \"bos_token\": DEFAULT_BOS_TOKEN,\n      \"unk_token\"\
    : DEFAULT_UNK_TOKEN,\n    }\n  )\n\n  ctx = \"\"\n  for instruction in [\"Whats\
    \ your objective?\",\"Who are you?\",\"Write me a poem about birds?\"]:\n    print(\"\
    Instruction:\", instruction)\n    inputs = tokenizer(generate_prompt(instruction,\
    \ None), return_tensors=\"pt\")\n    outputs = model.generate(input_ids=inputs[\"\
    input_ids\"].cpu(),\n                             generation_config=generation_config,\n\
    \                             max_new_tokens=inference_args.model_max_length,\n\
    \                             return_dict_in_generate=True,\n                \
    \             output_scores=True)\n    input_length = 1 if model.config.is_encoder_decoder\
    \ else inputs.input_ids.shape[1]\n    generated_tokens = outputs.sequences[:,\
    \ input_length:]\n\n    ctx += f\"Instruction: {instruction}\\n\" + f\"Response:\
    \ {generated_tokens[0]}\\n\"\n    print(\"Response:\", tokenizer.decode(generated_tokens[0]))\n\
    \    print()\n\n\nif __name__ == \"__main__\":\n  inference()"
  created_at: 2023-04-27 21:43:33+00:00
  edited: true
  hidden: false
  id: 644afa95af97dfd24c14cc7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c8887be945ad848da1c07b9853b26e13.svg
      fullname: Stijn Dreezen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Marburps
      type: user
    createdAt: '2023-05-05T12:28:40.000Z'
    data:
      edited: false
      editors:
      - Marburps
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c8887be945ad848da1c07b9853b26e13.svg
          fullname: Stijn Dreezen
          isHf: false
          isPro: false
          name: Marburps
          type: user
        html: '<p>you''ll just have to offload to cpu or disk, because the model is
          too big.<br>This works for me:</p>

          <p>model = transformers.LlamaForCausalLM.from_pretrained(PATH, local_files_only=True,
          torch_dtype=torch.float16, device_map=''auto'', low_cpu_mem_usage=True,
          offload_folder=''models_hf'')</p>

          '
        raw: 'you''ll just have to offload to cpu or disk, because the model is too
          big.

          This works for me:


          model = transformers.LlamaForCausalLM.from_pretrained(PATH, local_files_only=True,
          torch_dtype=torch.float16, device_map=''auto'', low_cpu_mem_usage=True,
          offload_folder=''models_hf'')'
        updatedAt: '2023-05-05T12:28:40.610Z'
      numEdits: 0
      reactions: []
    id: 6454f678a473375be56d82f7
    type: comment
  author: Marburps
  content: 'you''ll just have to offload to cpu or disk, because the model is too
    big.

    This works for me:


    model = transformers.LlamaForCausalLM.from_pretrained(PATH, local_files_only=True,
    torch_dtype=torch.float16, device_map=''auto'', low_cpu_mem_usage=True, offload_folder=''models_hf'')'
  created_at: 2023-05-05 11:28:40+00:00
  edited: false
  hidden: false
  id: 6454f678a473375be56d82f7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: OpenAssistant/oasst-sft-6-llama-30b-xor
repo_type: model
status: open
target_branch: null
title: How can I do inference with this model?
