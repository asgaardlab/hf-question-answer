!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HennyWong
conflicting_files: null
created_at: 2023-06-12 07:26:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6109be449e1d0abc2506328831854aba.svg
      fullname: Yuxuan Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HennyWong
      type: user
    createdAt: '2023-06-12T08:26:52.000Z'
    data:
      edited: false
      editors:
      - HennyWong
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.9738458395004272
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6109be449e1d0abc2506328831854aba.svg
          fullname: Yuxuan Wang
          isHf: false
          isPro: false
          name: HennyWong
          type: user
        html: "<p>\u6211\u5C1D\u8BD5\u7528\u5229\u7528torch\u52A0\u8F7D\u7684\u65B9\
          \u6CD5\u66F4\u6539\u4EE3\u7801\u3002\u7136\u540E\u5BF9\u6BD4\u4E86\u4E00\
          \u4E0B\u6743\u91CD\uFF0C\u53D1\u73B0\u597D\u50CF\u548C\u539F\u751F\u6743\
          \u91CD\u6709\u533A\u522B\uFF1F</p>\n"
        raw: "\u6211\u5C1D\u8BD5\u7528\u5229\u7528torch\u52A0\u8F7D\u7684\u65B9\u6CD5\
          \u66F4\u6539\u4EE3\u7801\u3002\u7136\u540E\u5BF9\u6BD4\u4E86\u4E00\u4E0B\
          \u6743\u91CD\uFF0C\u53D1\u73B0\u597D\u50CF\u548C\u539F\u751F\u6743\u91CD\
          \u6709\u533A\u522B\uFF1F"
        updatedAt: '2023-06-12T08:26:52.318Z'
      numEdits: 0
      reactions: []
    id: 6486d6ccfd80431590872c56
    type: comment
  author: HennyWong
  content: "\u6211\u5C1D\u8BD5\u7528\u5229\u7528torch\u52A0\u8F7D\u7684\u65B9\u6CD5\
    \u66F4\u6539\u4EE3\u7801\u3002\u7136\u540E\u5BF9\u6BD4\u4E86\u4E00\u4E0B\u6743\
    \u91CD\uFF0C\u53D1\u73B0\u597D\u50CF\u548C\u539F\u751F\u6743\u91CD\u6709\u533A\
    \u522B\uFF1F"
  created_at: 2023-06-12 07:26:52+00:00
  edited: false
  hidden: false
  id: 6486d6ccfd80431590872c56
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1863bfbc90f003ff3f446ec09e2806f9.svg
      fullname: wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: vanewu
      type: user
    createdAt: '2023-06-12T09:22:33.000Z'
    data:
      edited: false
      editors:
      - vanewu
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.9817731380462646
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1863bfbc90f003ff3f446ec09e2806f9.svg
          fullname: wu
          isHf: false
          isPro: false
          name: vanewu
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;HennyWong&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/HennyWong\">@<span class=\"\
          underline\">HennyWong</span></a></span>\n\n\t</span></span> Hi\uFF0C\u8FD9\
          \u91CC\u6CA1\u6709\u5F88\u660E\u767D\u4F60\u7684\u610F\u601D\u3002\u662F\
          \u8BF4\u6743\u91CD\u53C2\u6570\u6709\u533A\u522B\u4E48\uFF1F\u6211\u4EEC\
          \u5F00\u53D1\u7684\u65F6\u5019\uFF0C\u8BBE\u5B9A\u8FC7\u53C2\u6570\u5BF9\
          \u6BD4\uFF0C\u548C \u6A21\u578B\u6BCF\u5C42 transformer \u7684\u8F93\u51FA\
          \uFF0C\u5BF9\u4E8E\u540C\u4E00\u4E2A\u6570\u503C\u8F93\u5165\uFF0C\u6BCF\
          \u5C42\u7684 transformer \u6570\u503C\u8F93\u51FA\u548C torch \u7248\u672C\
          \u662F\u53EF\u4EE5\u5BF9\u9F50\u7684\u3002\u8FD9\u610F\u5473\u7740\u53C2\
          \u6570\u5E94\u8BE5\u662F\u5BF9\u9F50\u7684\u3002\u4F60\u53EF\u4EE5\u8D34\
          \u4E0B\u4F60\u7684\u6539\u52A8\u7247\u6BB5\u4E48\uFF1F</p>\n"
        raw: "@HennyWong Hi\uFF0C\u8FD9\u91CC\u6CA1\u6709\u5F88\u660E\u767D\u4F60\u7684\
          \u610F\u601D\u3002\u662F\u8BF4\u6743\u91CD\u53C2\u6570\u6709\u533A\u522B\
          \u4E48\uFF1F\u6211\u4EEC\u5F00\u53D1\u7684\u65F6\u5019\uFF0C\u8BBE\u5B9A\
          \u8FC7\u53C2\u6570\u5BF9\u6BD4\uFF0C\u548C \u6A21\u578B\u6BCF\u5C42 transformer\
          \ \u7684\u8F93\u51FA\uFF0C\u5BF9\u4E8E\u540C\u4E00\u4E2A\u6570\u503C\u8F93\
          \u5165\uFF0C\u6BCF\u5C42\u7684 transformer \u6570\u503C\u8F93\u51FA\u548C\
          \ torch \u7248\u672C\u662F\u53EF\u4EE5\u5BF9\u9F50\u7684\u3002\u8FD9\u610F\
          \u5473\u7740\u53C2\u6570\u5E94\u8BE5\u662F\u5BF9\u9F50\u7684\u3002\u4F60\
          \u53EF\u4EE5\u8D34\u4E0B\u4F60\u7684\u6539\u52A8\u7247\u6BB5\u4E48\uFF1F"
        updatedAt: '2023-06-12T09:22:33.570Z'
      numEdits: 0
      reactions: []
    id: 6486e3d9c9d77c91be1f138a
    type: comment
  author: vanewu
  content: "@HennyWong Hi\uFF0C\u8FD9\u91CC\u6CA1\u6709\u5F88\u660E\u767D\u4F60\u7684\
    \u610F\u601D\u3002\u662F\u8BF4\u6743\u91CD\u53C2\u6570\u6709\u533A\u522B\u4E48\
    \uFF1F\u6211\u4EEC\u5F00\u53D1\u7684\u65F6\u5019\uFF0C\u8BBE\u5B9A\u8FC7\u53C2\
    \u6570\u5BF9\u6BD4\uFF0C\u548C \u6A21\u578B\u6BCF\u5C42 transformer \u7684\u8F93\
    \u51FA\uFF0C\u5BF9\u4E8E\u540C\u4E00\u4E2A\u6570\u503C\u8F93\u5165\uFF0C\u6BCF\
    \u5C42\u7684 transformer \u6570\u503C\u8F93\u51FA\u548C torch \u7248\u672C\u662F\
    \u53EF\u4EE5\u5BF9\u9F50\u7684\u3002\u8FD9\u610F\u5473\u7740\u53C2\u6570\u5E94\
    \u8BE5\u662F\u5BF9\u9F50\u7684\u3002\u4F60\u53EF\u4EE5\u8D34\u4E0B\u4F60\u7684\
    \u6539\u52A8\u7247\u6BB5\u4E48\uFF1F"
  created_at: 2023-06-12 08:22:33+00:00
  edited: false
  hidden: false
  id: 6486e3d9c9d77c91be1f138a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6109be449e1d0abc2506328831854aba.svg
      fullname: Yuxuan Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HennyWong
      type: user
    createdAt: '2023-06-12T09:28:55.000Z'
    data:
      edited: false
      editors:
      - HennyWong
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2847798466682434
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6109be449e1d0abc2506328831854aba.svg
          fullname: Yuxuan Wang
          isHf: false
          isPro: false
          name: HennyWong
          type: user
        html: "<p>\u60A8\u597D\uFF1A\u611F\u8C22\u60A8\u7684\u56DE\u590D\u3002\u6211\
          \u4E3B\u8981\u4F5C\u4E86\u5982\u4E0B\u7684\u7247\u6BB5\u4FEE\u6539\uFF0C\
          \u4E0D\u77E5\u9053\u662F\u5426\u6709\u9057\u6F0F\uFF1F</p>\n<pre><code>\
          \    from transformers import AutoModel\n    torch_model = AutoModel.from_pretrained(ckpt_path,\
          \ trust_remote_code=True).half().cuda()torch_model\n    \n    w.extend([load_to_torch(torch_model.transformer.layers[i].input_layernorm.weight,\
          \ is_load(i))\n              for i in range(self.layer_num)])\n    w.extend([load_to_torch(torch_model.transformer.layers[i].input_layernorm.bias,\
          \ is_load(i))\n             for i in range(self.layer_num)])\n    w.extend([load_to_torch(torch_model.transformer.layers[i].attention.query_key_value.weight,\
          \ is_load(i))\n             for i in range(self.layer_num)])\n    w.extend([load_to_torch(torch_model.transformer.layers[i].attention.query_key_value.bias,\
          \ is_load(i))\n             for i in range(self.layer_num)])\n    w.extend([load_to_torch(torch_model.transformer.layers[i].attention.dense.weight,\
          \ is_load(i))\n             for i in range(self.layer_num)])\n    w.extend([load_to_torch(torch_model.transformer.layers[i].attention.dense.bias,\
          \ is_load(i))\n             for i in range(self.layer_num)])\n    w.extend([load_to_torch(torch_model.transformer.layers[i].post_attention_layernorm.weight,\
          \ is_load(i))\n             for i in range(self.layer_num)])\n    w.extend([load_to_torch(torch_model.transformer.layers[i].post_attention_layernorm.bias,\
          \ is_load(i))\n             for i in range(self.layer_num)])\n    w.extend([load_to_torch(torch_model.transformer.layers[i].mlp.dense_h_to_4h.weight,\
          \ is_load(i))\n             for i in range(self.layer_num)])\n    w.extend([load_to_torch(torch_model.transformer.layers[i].mlp.dense_h_to_4h.bias,\
          \ is_load(i))\n             for i in range(self.layer_num)])\n    w.extend([load_to_torch(torch_model.transformer.layers[i].mlp.dense_4h_to_h.weight,\
          \ is_load(i))\n             for i in range(self.layer_num)])\n    w.extend([load_to_torch(torch_model.transformer.layers[i].mlp.dense_4h_to_h.bias,\
          \ is_load(i))\n             for i in range(self.layer_num)])\n\n    if self.has_pre_decoder_layernorm:\n\
          \        w.append(load_to_torch(torch_model.transformer.pre_decoder_layernorm.weight,\
          \ True))\n        w.append(load_to_torch(torch_model.transformer.pre_decoder_layernorm.bias,\
          \ True))\n\n    if self.has_post_decoder_layernorm:\n        w.append(load_to_torch(torch_model.transformer.final_layernorm.weight,\
          \ True))\n        w.append(load_to_torch(torch_model.transformer.final_layernorm.bias,\
          \ True))\n\n    if self.has_positional_encoding:\n        wpe = load_to_torch(f\"\
          model.wpe\", True).reshape(-1, self.global_hidden_units)\n        assert\
          \ self.max_seq_len &lt;= wpe.size(0), (\n            f\"max_seq_len ({self.max_seq_len}\
          \ must not exceed \"\n            f\"the value of maximum sequence length\
          \ during training ({wpe.size(0)}).\"\n        )\n        w.append(wpe)\n\
          \    w.append(load_to_torch(torch_model.transformer.word_embeddings.weight,\
          \ True))\n</code></pre>\n<p>\u6700\u540E\u5BF9\u6BD4\u4E86embedding\u5C42\
          \u7684\u6743\u91CD\uFF0C\u611F\u89C9\u548Chf \u4E0B\u8F7D\u7684\u7248\u672C\
          \u6709\u70B9\u533A\u522B\u3002\u53EF\u4EE5\u5E2E\u5FD9\u770B\u770B\u5417\
          \uFF1F</p>\n"
        raw: "\u60A8\u597D\uFF1A\u611F\u8C22\u60A8\u7684\u56DE\u590D\u3002\u6211\u4E3B\
          \u8981\u4F5C\u4E86\u5982\u4E0B\u7684\u7247\u6BB5\u4FEE\u6539\uFF0C\u4E0D\
          \u77E5\u9053\u662F\u5426\u6709\u9057\u6F0F\uFF1F\n\n        from transformers\
          \ import AutoModel\n        torch_model = AutoModel.from_pretrained(ckpt_path,\
          \ trust_remote_code=True).half().cuda()torch_model\n        \n        w.extend([load_to_torch(torch_model.transformer.layers[i].input_layernorm.weight,\
          \ is_load(i))\n                  for i in range(self.layer_num)])\n    \
          \    w.extend([load_to_torch(torch_model.transformer.layers[i].input_layernorm.bias,\
          \ is_load(i))\n                 for i in range(self.layer_num)])\n     \
          \   w.extend([load_to_torch(torch_model.transformer.layers[i].attention.query_key_value.weight,\
          \ is_load(i))\n                 for i in range(self.layer_num)])\n     \
          \   w.extend([load_to_torch(torch_model.transformer.layers[i].attention.query_key_value.bias,\
          \ is_load(i))\n                 for i in range(self.layer_num)])\n     \
          \   w.extend([load_to_torch(torch_model.transformer.layers[i].attention.dense.weight,\
          \ is_load(i))\n                 for i in range(self.layer_num)])\n     \
          \   w.extend([load_to_torch(torch_model.transformer.layers[i].attention.dense.bias,\
          \ is_load(i))\n                 for i in range(self.layer_num)])\n     \
          \   w.extend([load_to_torch(torch_model.transformer.layers[i].post_attention_layernorm.weight,\
          \ is_load(i))\n                 for i in range(self.layer_num)])\n     \
          \   w.extend([load_to_torch(torch_model.transformer.layers[i].post_attention_layernorm.bias,\
          \ is_load(i))\n                 for i in range(self.layer_num)])\n     \
          \   w.extend([load_to_torch(torch_model.transformer.layers[i].mlp.dense_h_to_4h.weight,\
          \ is_load(i))\n                 for i in range(self.layer_num)])\n     \
          \   w.extend([load_to_torch(torch_model.transformer.layers[i].mlp.dense_h_to_4h.bias,\
          \ is_load(i))\n                 for i in range(self.layer_num)])\n     \
          \   w.extend([load_to_torch(torch_model.transformer.layers[i].mlp.dense_4h_to_h.weight,\
          \ is_load(i))\n                 for i in range(self.layer_num)])\n     \
          \   w.extend([load_to_torch(torch_model.transformer.layers[i].mlp.dense_4h_to_h.bias,\
          \ is_load(i))\n                 for i in range(self.layer_num)])\n\n   \
          \     if self.has_pre_decoder_layernorm:\n            w.append(load_to_torch(torch_model.transformer.pre_decoder_layernorm.weight,\
          \ True))\n            w.append(load_to_torch(torch_model.transformer.pre_decoder_layernorm.bias,\
          \ True))\n\n        if self.has_post_decoder_layernorm:\n            w.append(load_to_torch(torch_model.transformer.final_layernorm.weight,\
          \ True))\n            w.append(load_to_torch(torch_model.transformer.final_layernorm.bias,\
          \ True))\n\n        if self.has_positional_encoding:\n            wpe =\
          \ load_to_torch(f\"model.wpe\", True).reshape(-1, self.global_hidden_units)\n\
          \            assert self.max_seq_len <= wpe.size(0), (\n               \
          \ f\"max_seq_len ({self.max_seq_len} must not exceed \"\n              \
          \  f\"the value of maximum sequence length during training ({wpe.size(0)}).\"\
          \n            )\n            w.append(wpe)\n        w.append(load_to_torch(torch_model.transformer.word_embeddings.weight,\
          \ True))\n\n\u6700\u540E\u5BF9\u6BD4\u4E86embedding\u5C42\u7684\u6743\u91CD\
          \uFF0C\u611F\u89C9\u548Chf \u4E0B\u8F7D\u7684\u7248\u672C\u6709\u70B9\u533A\
          \u522B\u3002\u53EF\u4EE5\u5E2E\u5FD9\u770B\u770B\u5417\uFF1F"
        updatedAt: '2023-06-12T09:28:55.607Z'
      numEdits: 0
      reactions: []
    id: 6486e55777ab5e8f9935de5b
    type: comment
  author: HennyWong
  content: "\u60A8\u597D\uFF1A\u611F\u8C22\u60A8\u7684\u56DE\u590D\u3002\u6211\u4E3B\
    \u8981\u4F5C\u4E86\u5982\u4E0B\u7684\u7247\u6BB5\u4FEE\u6539\uFF0C\u4E0D\u77E5\
    \u9053\u662F\u5426\u6709\u9057\u6F0F\uFF1F\n\n        from transformers import\
    \ AutoModel\n        torch_model = AutoModel.from_pretrained(ckpt_path, trust_remote_code=True).half().cuda()torch_model\n\
    \        \n        w.extend([load_to_torch(torch_model.transformer.layers[i].input_layernorm.weight,\
    \ is_load(i))\n                  for i in range(self.layer_num)])\n        w.extend([load_to_torch(torch_model.transformer.layers[i].input_layernorm.bias,\
    \ is_load(i))\n                 for i in range(self.layer_num)])\n        w.extend([load_to_torch(torch_model.transformer.layers[i].attention.query_key_value.weight,\
    \ is_load(i))\n                 for i in range(self.layer_num)])\n        w.extend([load_to_torch(torch_model.transformer.layers[i].attention.query_key_value.bias,\
    \ is_load(i))\n                 for i in range(self.layer_num)])\n        w.extend([load_to_torch(torch_model.transformer.layers[i].attention.dense.weight,\
    \ is_load(i))\n                 for i in range(self.layer_num)])\n        w.extend([load_to_torch(torch_model.transformer.layers[i].attention.dense.bias,\
    \ is_load(i))\n                 for i in range(self.layer_num)])\n        w.extend([load_to_torch(torch_model.transformer.layers[i].post_attention_layernorm.weight,\
    \ is_load(i))\n                 for i in range(self.layer_num)])\n        w.extend([load_to_torch(torch_model.transformer.layers[i].post_attention_layernorm.bias,\
    \ is_load(i))\n                 for i in range(self.layer_num)])\n        w.extend([load_to_torch(torch_model.transformer.layers[i].mlp.dense_h_to_4h.weight,\
    \ is_load(i))\n                 for i in range(self.layer_num)])\n        w.extend([load_to_torch(torch_model.transformer.layers[i].mlp.dense_h_to_4h.bias,\
    \ is_load(i))\n                 for i in range(self.layer_num)])\n        w.extend([load_to_torch(torch_model.transformer.layers[i].mlp.dense_4h_to_h.weight,\
    \ is_load(i))\n                 for i in range(self.layer_num)])\n        w.extend([load_to_torch(torch_model.transformer.layers[i].mlp.dense_4h_to_h.bias,\
    \ is_load(i))\n                 for i in range(self.layer_num)])\n\n        if\
    \ self.has_pre_decoder_layernorm:\n            w.append(load_to_torch(torch_model.transformer.pre_decoder_layernorm.weight,\
    \ True))\n            w.append(load_to_torch(torch_model.transformer.pre_decoder_layernorm.bias,\
    \ True))\n\n        if self.has_post_decoder_layernorm:\n            w.append(load_to_torch(torch_model.transformer.final_layernorm.weight,\
    \ True))\n            w.append(load_to_torch(torch_model.transformer.final_layernorm.bias,\
    \ True))\n\n        if self.has_positional_encoding:\n            wpe = load_to_torch(f\"\
    model.wpe\", True).reshape(-1, self.global_hidden_units)\n            assert self.max_seq_len\
    \ <= wpe.size(0), (\n                f\"max_seq_len ({self.max_seq_len} must not\
    \ exceed \"\n                f\"the value of maximum sequence length during training\
    \ ({wpe.size(0)}).\"\n            )\n            w.append(wpe)\n        w.append(load_to_torch(torch_model.transformer.word_embeddings.weight,\
    \ True))\n\n\u6700\u540E\u5BF9\u6BD4\u4E86embedding\u5C42\u7684\u6743\u91CD\uFF0C\
    \u611F\u89C9\u548Chf \u4E0B\u8F7D\u7684\u7248\u672C\u6709\u70B9\u533A\u522B\u3002\
    \u53EF\u4EE5\u5E2E\u5FD9\u770B\u770B\u5417\uFF1F"
  created_at: 2023-06-12 08:28:55+00:00
  edited: false
  hidden: false
  id: 6486e55777ab5e8f9935de5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6109be449e1d0abc2506328831854aba.svg
      fullname: Yuxuan Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HennyWong
      type: user
    createdAt: '2023-06-12T09:31:49.000Z'
    data:
      edited: false
      editors:
      - HennyWong
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.5956446528434753
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6109be449e1d0abc2506328831854aba.svg
          fullname: Yuxuan Wang
          isHf: false
          isPro: false
          name: HennyWong
          type: user
        html: "<p>\u4E3B\u8981\u9488\u5BF9model.py\u91CC\u9762257-311\u884C\u91CC\u7684\
          \u4FEE\u6539</p>\n"
        raw: "\u4E3B\u8981\u9488\u5BF9model.py\u91CC\u9762257-311\u884C\u91CC\u7684\
          \u4FEE\u6539"
        updatedAt: '2023-06-12T09:31:49.165Z'
      numEdits: 0
      reactions: []
    id: 6486e605c9d77c91be1fd6a4
    type: comment
  author: HennyWong
  content: "\u4E3B\u8981\u9488\u5BF9model.py\u91CC\u9762257-311\u884C\u91CC\u7684\u4FEE\
    \u6539"
  created_at: 2023-06-12 08:31:49+00:00
  edited: false
  hidden: false
  id: 6486e605c9d77c91be1fd6a4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 33
repo_id: TMElyralab/lyraChatGLM
repo_type: model
status: open
target_branch: null
title: "\u6743\u91CD\u4E0Eoriginal\u6743\u91CD\u6709\u533A\u522B\uFF1F"
