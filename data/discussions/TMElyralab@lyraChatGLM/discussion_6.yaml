!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lchustc
conflicting_files: null
created_at: 2023-05-16 11:22:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aef43e3055005ae6f6c6dcdf6b514de5.svg
      fullname: lch
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lchustc
      type: user
    createdAt: '2023-05-16T12:22:30.000Z'
    data:
      edited: false
      editors:
      - lchustc
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aef43e3055005ae6f6c6dcdf6b514de5.svg
          fullname: lch
          isHf: false
          isPro: false
          name: lchustc
          type: user
        html: "<p>Hi\uFF0Ci reproduced the LyraChatGLM in my A100 and got the speed\
          \ of 71 tokens/s.<br>I wonder under what settings you achieved your 310\
          \ tokens/s?<br>I have tested the speed of the LyraChatGLM and the raw ChatGLM\
          \ under  condition batch_size=1(31 tokens/s and 71 tokens/s).</p>\n"
        raw: "Hi\uFF0Ci reproduced the LyraChatGLM in my A100 and got the speed of\
          \ 71 tokens/s.\r\nI wonder under what settings you achieved your 310 tokens/s?\r\
          \nI have tested the speed of the LyraChatGLM and the raw ChatGLM under \
          \ condition batch_size=1(31 tokens/s and 71 tokens/s)."
        updatedAt: '2023-05-16T12:22:30.999Z'
      numEdits: 0
      reactions: []
    id: 646375867e9025b09bd70347
    type: comment
  author: lchustc
  content: "Hi\uFF0Ci reproduced the LyraChatGLM in my A100 and got the speed of 71\
    \ tokens/s.\r\nI wonder under what settings you achieved your 310 tokens/s?\r\n\
    I have tested the speed of the LyraChatGLM and the raw ChatGLM under  condition\
    \ batch_size=1(31 tokens/s and 71 tokens/s)."
  created_at: 2023-05-16 11:22:30+00:00
  edited: false
  hidden: false
  id: 646375867e9025b09bd70347
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/n0RQ_qmhz6_MvTE3gVHNr.jpeg?w=200&h=200&f=face
      fullname: moyanwang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bigmoyan
      type: user
    createdAt: '2023-05-17T03:25:06.000Z'
    data:
      edited: true
      editors:
      - bigmoyan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/n0RQ_qmhz6_MvTE3gVHNr.jpeg?w=200&h=200&f=face
          fullname: moyanwang
          isHf: false
          isPro: false
          name: bigmoyan
          type: user
        html: '<p>@lchwhut Short answer: try batch_size = 8</p>

          <p>to get full speed, we need to improve computation parallelism to fully
          use the whole available resources. We modified original chatGLM batch preparation
          method to make it works correctly under KV cache optimization, thus in batch
          mode we can run much faster than original version. ( actually original chatGLM
          doesn''t support batch inference at all, it cannot inference correctly)</p>

          '
        raw: '@lchwhut Short answer: try batch_size = 8


          to get full speed, we need to improve computation parallelism to fully use
          the whole available resources. We modified original chatGLM batch preparation
          method to make it works correctly under KV cache optimization, thus in batch
          mode we can run much faster than original version. ( actually original chatGLM
          doesn''t support batch inference at all, it cannot inference correctly)'
        updatedAt: '2023-05-17T03:51:55.089Z'
      numEdits: 2
      reactions: []
    id: 64644912dae3c8a327c3eebb
    type: comment
  author: bigmoyan
  content: '@lchwhut Short answer: try batch_size = 8


    to get full speed, we need to improve computation parallelism to fully use the
    whole available resources. We modified original chatGLM batch preparation method
    to make it works correctly under KV cache optimization, thus in batch mode we
    can run much faster than original version. ( actually original chatGLM doesn''t
    support batch inference at all, it cannot inference correctly)'
  created_at: 2023-05-17 02:25:06+00:00
  edited: true
  hidden: false
  id: 64644912dae3c8a327c3eebb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aef43e3055005ae6f6c6dcdf6b514de5.svg
      fullname: lch
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lchustc
      type: user
    createdAt: '2023-05-17T04:32:32.000Z'
    data:
      edited: true
      editors:
      - lchustc
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aef43e3055005ae6f6c6dcdf6b514de5.svg
          fullname: lch
          isHf: false
          isPro: false
          name: lchustc
          type: user
        html: "<blockquote>\n<p>@lchwhut Short answer: try batch_size = 8</p>\n<p>to\
          \ get full speed, we need to improve computation parallelism to fully use\
          \ the whole available resources. We modified original chatGLM batch preparation\
          \ method to make it works correctly under KV cache optimization, thus in\
          \ batch mode we can run much faster than original version. ( actually original\
          \ chatGLM doesn't support batch inference at all, it cannot inference correctly)</p>\n\
          </blockquote>\n<p>Hi\uFF0CI have achieved correct batch(bs=8) inference\
          \ of original ChatGLM and got the speed of 137 token/s in my A100.<br>You\
          \ can see this issue <a rel=\"nofollow\" href=\"https://github.com/THUDM/ChatGLM-6B/issues/745\"\
          >https://github.com/THUDM/ChatGLM-6B/issues/745</a></p>\n"
        raw: "> @lchwhut Short answer: try batch_size = 8\n> \n> to get full speed,\
          \ we need to improve computation parallelism to fully use the whole available\
          \ resources. We modified original chatGLM batch preparation method to make\
          \ it works correctly under KV cache optimization, thus in batch mode we\
          \ can run much faster than original version. ( actually original chatGLM\
          \ doesn't support batch inference at all, it cannot inference correctly)\n\
          \nHi\uFF0CI have achieved correct batch(bs=8) inference of original ChatGLM\
          \ and got the speed of 137 token/s in my A100.\nYou can see this issue https://github.com/THUDM/ChatGLM-6B/issues/745"
        updatedAt: '2023-05-17T04:35:21.494Z'
      numEdits: 2
      reactions: []
    id: 646458e0dae3c8a327c475f9
    type: comment
  author: lchustc
  content: "> @lchwhut Short answer: try batch_size = 8\n> \n> to get full speed,\
    \ we need to improve computation parallelism to fully use the whole available\
    \ resources. We modified original chatGLM batch preparation method to make it\
    \ works correctly under KV cache optimization, thus in batch mode we can run much\
    \ faster than original version. ( actually original chatGLM doesn't support batch\
    \ inference at all, it cannot inference correctly)\n\nHi\uFF0CI have achieved\
    \ correct batch(bs=8) inference of original ChatGLM and got the speed of 137 token/s\
    \ in my A100.\nYou can see this issue https://github.com/THUDM/ChatGLM-6B/issues/745"
  created_at: 2023-05-17 03:32:32+00:00
  edited: true
  hidden: false
  id: 646458e0dae3c8a327c475f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/n0RQ_qmhz6_MvTE3gVHNr.jpeg?w=200&h=200&f=face
      fullname: moyanwang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bigmoyan
      type: user
    createdAt: '2023-05-17T06:26:34.000Z'
    data:
      edited: false
      editors:
      - bigmoyan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/n0RQ_qmhz6_MvTE3gVHNr.jpeg?w=200&h=200&f=face
          fullname: moyanwang
          isHf: false
          isPro: false
          name: bigmoyan
          type: user
        html: '<p>@lchwhut good job! we started this project based on old version
          and didn''t notice this update. </p>

          <p>I''ll update readme to make it clearer</p>

          '
        raw: "@lchwhut good job! we started this project based on old version and\
          \ didn't notice this update. \n\nI'll update readme to make it clearer"
        updatedAt: '2023-05-17T06:26:34.326Z'
      numEdits: 0
      reactions: []
    id: 6464739a956e83990922325f
    type: comment
  author: bigmoyan
  content: "@lchwhut good job! we started this project based on old version and didn't\
    \ notice this update. \n\nI'll update readme to make it clearer"
  created_at: 2023-05-17 05:26:34+00:00
  edited: false
  hidden: false
  id: 6464739a956e83990922325f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31a99ddaa713e51b3c4c39fd58e923f7.svg
      fullname: Xiang Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xiangli
      type: user
    createdAt: '2023-05-25T01:59:19.000Z'
    data:
      edited: false
      editors:
      - xiangli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31a99ddaa713e51b3c4c39fd58e923f7.svg
          fullname: Xiang Li
          isHf: false
          isPro: false
          name: xiangli
          type: user
        html: "<p>Hi, <span data-props=\"{&quot;user&quot;:&quot;bigmoyan&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/bigmoyan\"\
          >@<span class=\"underline\">bigmoyan</span></a></span>\n\n\t</span></span><br>what's\
          \ the input length for 310 tokens/s?<br>Thanks.</p>\n"
        raw: "Hi, @bigmoyan \nwhat's the input length for 310 tokens/s?\nThanks."
        updatedAt: '2023-05-25T01:59:19.916Z'
      numEdits: 0
      reactions: []
    id: 646ec0f77942c36e9db17dec
    type: comment
  author: xiangli
  content: "Hi, @bigmoyan \nwhat's the input length for 310 tokens/s?\nThanks."
  created_at: 2023-05-25 00:59:19+00:00
  edited: false
  hidden: false
  id: 646ec0f77942c36e9db17dec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7b3fea2f318ddf0c41fbeb23528af23e.svg
      fullname: felix.han
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: feilix
      type: user
    createdAt: '2023-06-02T05:30:35.000Z'
    data:
      edited: true
      editors:
      - feilix
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7b3fea2f318ddf0c41fbeb23528af23e.svg
          fullname: felix.han
          isHf: false
          isPro: false
          name: feilix
          type: user
        html: "<blockquote>\n<p>@lchwhut Short answer: try batch_size = 8</p>\n<p>to\
          \ get full speed, we need to improve computation parallelism to fully use\
          \ the whole available resources. We modified original chatGLM batch preparation\
          \ method to make it works correctly under KV cache optimization, thus in\
          \ batch mode we can run much faster than original version. ( actually original\
          \ chatGLM doesn't support batch inference at all, it cannot inference correctly)<br><span\
          \ data-props=\"{&quot;user&quot;:&quot;bigmoyan&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/bigmoyan\">@<span class=\"\
          underline\">bigmoyan</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;lchustc&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/lchustc\">@<span class=\"underline\">lchustc</span></a></span>\n\
          \n\t</span></span> I got 70 tokens/s  in my A100 with batch_size = 8.<br>Can\
          \ you share your demo code with batch_size=8?</p>\n</blockquote>\n"
        raw: "> @lchwhut Short answer: try batch_size = 8\n> \n> to get full speed,\
          \ we need to improve computation parallelism to fully use the whole available\
          \ resources. We modified original chatGLM batch preparation method to make\
          \ it works correctly under KV cache optimization, thus in batch mode we\
          \ can run much faster than original version. ( actually original chatGLM\
          \ doesn't support batch inference at all, it cannot inference correctly)\n\
          @bigmoyan @lchustc I got 70 tokens/s  in my A100 with batch_size = 8.\n\
          Can you share your demo code with batch_size=8?"
        updatedAt: '2023-06-02T05:30:53.361Z'
      numEdits: 1
      reactions: []
    id: 64797e7b8c0a0bb3b1fb2b14
    type: comment
  author: feilix
  content: "> @lchwhut Short answer: try batch_size = 8\n> \n> to get full speed,\
    \ we need to improve computation parallelism to fully use the whole available\
    \ resources. We modified original chatGLM batch preparation method to make it\
    \ works correctly under KV cache optimization, thus in batch mode we can run much\
    \ faster than original version. ( actually original chatGLM doesn't support batch\
    \ inference at all, it cannot inference correctly)\n@bigmoyan @lchustc I got 70\
    \ tokens/s  in my A100 with batch_size = 8.\nCan you share your demo code with\
    \ batch_size=8?"
  created_at: 2023-06-02 04:30:35+00:00
  edited: true
  hidden: false
  id: 64797e7b8c0a0bb3b1fb2b14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1863bfbc90f003ff3f446ec09e2806f9.svg
      fullname: wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: vanewu
      type: user
    createdAt: '2023-06-02T09:07:36.000Z'
    data:
      edited: false
      editors:
      - vanewu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1863bfbc90f003ff3f446ec09e2806f9.svg
          fullname: wu
          isHf: false
          isPro: false
          name: vanewu
          type: user
        html: '<p>Everything is updated. Please try the new version.</p>

          '
        raw: Everything is updated. Please try the new version.
        updatedAt: '2023-06-02T09:07:36.376Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6479b158a84498f2af44994e
    id: 6479b158a84498f2af44994d
    type: comment
  author: vanewu
  content: Everything is updated. Please try the new version.
  created_at: 2023-06-02 08:07:36+00:00
  edited: false
  hidden: false
  id: 6479b158a84498f2af44994d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/1863bfbc90f003ff3f446ec09e2806f9.svg
      fullname: wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: vanewu
      type: user
    createdAt: '2023-06-02T09:07:36.000Z'
    data:
      status: closed
    id: 6479b158a84498f2af44994e
    type: status-change
  author: vanewu
  created_at: 2023-06-02 08:07:36+00:00
  id: 6479b158a84498f2af44994e
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TMElyralab/lyraChatGLM
repo_type: model
status: closed
target_branch: null
title: about 310 tokens/s
