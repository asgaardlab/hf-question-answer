!!python/object:huggingface_hub.community.DiscussionWithDetails
author: qfalex12
conflicting_files: null
created_at: 2023-09-15 02:08:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba8f7b84499ee36f9115eee8d0c747f3.svg
      fullname: Qifang Yin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: qfalex12
      type: user
    createdAt: '2023-09-15T03:08:32.000Z'
    data:
      edited: false
      editors:
      - qfalex12
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46841681003570557
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba8f7b84499ee36f9115eee8d0c747f3.svg
          fullname: Qifang Yin
          isHf: false
          isPro: false
          name: qfalex12
          type: user
        html: '<p>Under input 2000 and output 64, a cuda error occurs when batch_size
          &gt; 16. </p>

          <p>Environment:<br>    nvcr.io/nvidia/pytorch:23.02-py3<br>    A100-SXM4-80GB</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6478ab7e60410176020b07ad/Meg3ychAJaBj7ewu8E_nI.png"><img
          alt="glm_error_bs17.png" src="https://cdn-uploads.huggingface.co/production/uploads/6478ab7e60410176020b07ad/Meg3ychAJaBj7ewu8E_nI.png"></a></p>

          <p>Thanks.</p>

          '
        raw: "Under input 2000 and output 64, a cuda error occurs when batch_size\
          \ > 16. \r\n\r\nEnvironment: \r\n    nvcr.io/nvidia/pytorch:23.02-py3 \r\
          \n    A100-SXM4-80GB\r\n\r\n![glm_error_bs17.png](https://cdn-uploads.huggingface.co/production/uploads/6478ab7e60410176020b07ad/Meg3ychAJaBj7ewu8E_nI.png)\r\
          \n\r\nThanks."
        updatedAt: '2023-09-15T03:08:32.895Z'
      numEdits: 0
      reactions: []
    id: 6503cab093574a89717f7145
    type: comment
  author: qfalex12
  content: "Under input 2000 and output 64, a cuda error occurs when batch_size >\
    \ 16. \r\n\r\nEnvironment: \r\n    nvcr.io/nvidia/pytorch:23.02-py3 \r\n    A100-SXM4-80GB\r\
    \n\r\n![glm_error_bs17.png](https://cdn-uploads.huggingface.co/production/uploads/6478ab7e60410176020b07ad/Meg3ychAJaBj7ewu8E_nI.png)\r\
    \n\r\nThanks."
  created_at: 2023-09-15 02:08:32+00:00
  edited: false
  hidden: false
  id: 6503cab093574a89717f7145
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/95a9bf5b470adf1359bd14a4b825e837.svg
      fullname: Haoxiong Su
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Carson
      type: user
    createdAt: '2023-10-16T06:51:26.000Z'
    data:
      edited: false
      editors:
      - Carson
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.978569746017456
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/95a9bf5b470adf1359bd14a4b825e837.svg
          fullname: Haoxiong Su
          isHf: false
          isPro: false
          name: Carson
          type: user
        html: '<p>Seems like an OOM. It''s reasonable since both the seq length and
          batch size are large.</p>

          '
        raw: Seems like an OOM. It's reasonable since both the seq length and batch
          size are large.
        updatedAt: '2023-10-16T06:51:26.910Z'
      numEdits: 0
      reactions: []
    id: 652cdd6e62a4885189b0778e
    type: comment
  author: Carson
  content: Seems like an OOM. It's reasonable since both the seq length and batch
    size are large.
  created_at: 2023-10-16 05:51:26+00:00
  edited: false
  hidden: false
  id: 652cdd6e62a4885189b0778e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 48
repo_id: TMElyralab/lyraChatGLM
repo_type: model
status: open
target_branch: null
title: 'CUDA runtime error: an illegal memory access was encountered.'
