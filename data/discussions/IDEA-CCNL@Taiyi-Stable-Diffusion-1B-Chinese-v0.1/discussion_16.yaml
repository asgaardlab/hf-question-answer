!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wanghaofan
conflicting_files: null
created_at: 2022-11-28 10:59:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg?w=200&h=200&f=face
      fullname: Haofan Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wanghaofan
      type: user
    createdAt: '2022-11-28T10:59:10.000Z'
    data:
      edited: false
      editors:
      - wanghaofan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg?w=200&h=200&f=face
          fullname: Haofan Wang
          isHf: false
          isPro: false
          name: wanghaofan
          type: user
        html: "<p>To reproduce the issue, run following lines in the command</p>\n\
          <pre><code>from transformers import BertForSequenceClassification, BertConfig,\
          \ BertTokenizer\n\npretrained_model_name_or_path = \"IDEA-CCNL/Taiyi-CLIP-Roberta-large-326M-Chinese\"\
          \ntokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n\
          \ncaptions = [\"\u4E00\u53EA\u732B\", '\u6D4B\u8BD5']\n\ninputs = tokenizer(captions,\
          \ max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True)\n\
          </code></pre>\n<p>If padding mode is set to 'max_length', it will raise\
          \ overflow. Actually, I think this is a typo once you print out <code>tokenizer.model_max_len</code>,\
          \ which is quite abnormal. To solve it, just set <code>tokenizer.model_max_len</code>\
          \ to a small number such as <code>77</code>.</p>\n"
        raw: "To reproduce the issue, run following lines in the command\r\n\r\n```\r\
          \nfrom transformers import BertForSequenceClassification, BertConfig, BertTokenizer\r\
          \n\r\npretrained_model_name_or_path = \"IDEA-CCNL/Taiyi-CLIP-Roberta-large-326M-Chinese\"\
          \r\ntokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\r\
          \n\r\ncaptions = [\"\u4E00\u53EA\u732B\", '\u6D4B\u8BD5']\r\n\r\ninputs\
          \ = tokenizer(captions, max_length=tokenizer.model_max_length, padding=\"\
          max_length\", truncation=True)\r\n```\r\n\r\nIf padding mode is set to 'max_length',\
          \ it will raise overflow. Actually, I think this is a typo once you print\
          \ out `tokenizer.model_max_len`, which is quite abnormal. To solve it, just\
          \ set `tokenizer.model_max_len` to a small number such as `77`.\r\n"
        updatedAt: '2022-11-28T10:59:10.536Z'
      numEdits: 0
      reactions: []
    id: 6384947e4d32ba5e1facadf8
    type: comment
  author: wanghaofan
  content: "To reproduce the issue, run following lines in the command\r\n\r\n```\r\
    \nfrom transformers import BertForSequenceClassification, BertConfig, BertTokenizer\r\
    \n\r\npretrained_model_name_or_path = \"IDEA-CCNL/Taiyi-CLIP-Roberta-large-326M-Chinese\"\
    \r\ntokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\r\
    \n\r\ncaptions = [\"\u4E00\u53EA\u732B\", '\u6D4B\u8BD5']\r\n\r\ninputs = tokenizer(captions,\
    \ max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True)\r\
    \n```\r\n\r\nIf padding mode is set to 'max_length', it will raise overflow. Actually,\
    \ I think this is a typo once you print out `tokenizer.model_max_len`, which is\
    \ quite abnormal. To solve it, just set `tokenizer.model_max_len` to a small number\
    \ such as `77`.\r\n"
  created_at: 2022-11-28 10:59:10+00:00
  edited: false
  hidden: false
  id: 6384947e4d32ba5e1facadf8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a7fc03ac97233f161c2433/CO3JDQVBZCnQxsgmnQHc1.jpeg?w=200&h=200&f=face
      fullname: weifeng
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wf-genius
      type: user
    createdAt: '2022-11-29T08:12:45.000Z'
    data:
      edited: false
      editors:
      - wf-genius
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a7fc03ac97233f161c2433/CO3JDQVBZCnQxsgmnQHc1.jpeg?w=200&h=200&f=face
          fullname: weifeng
          isHf: false
          isPro: false
          name: wf-genius
          type: user
        html: '<p>we did not set the model_max_length in the config file of "IDEA-CCNL/Taiyi-CLIP-Roberta-large-326M-Chinese"
          , so by default it will be a very large number.<br>We fix it when training
          stable diffusion and manually set it to 512. you can try this:</p>

          <pre><code>tokenizer = BertTokenizer.from_pretrained("IDEA-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-v0.1",
          subfolder="tokenizer")

          tokenizer.model_max_length

          </code></pre>

          '
        raw: "we did not set the model_max_length in the config file of \"IDEA-CCNL/Taiyi-CLIP-Roberta-large-326M-Chinese\"\
          \ , so by default it will be a very large number. \nWe fix it when training\
          \ stable diffusion and manually set it to 512. you can try this:\n```\n\
          tokenizer = BertTokenizer.from_pretrained(\"IDEA-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-v0.1\"\
          , subfolder=\"tokenizer\")\ntokenizer.model_max_length\n```"
        updatedAt: '2022-11-29T08:12:45.583Z'
      numEdits: 0
      reactions: []
    id: 6385befd988cb440e7cc8065
    type: comment
  author: wf-genius
  content: "we did not set the model_max_length in the config file of \"IDEA-CCNL/Taiyi-CLIP-Roberta-large-326M-Chinese\"\
    \ , so by default it will be a very large number. \nWe fix it when training stable\
    \ diffusion and manually set it to 512. you can try this:\n```\ntokenizer = BertTokenizer.from_pretrained(\"\
    IDEA-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-v0.1\", subfolder=\"tokenizer\")\n\
    tokenizer.model_max_length\n```"
  created_at: 2022-11-29 08:12:45+00:00
  edited: false
  hidden: false
  id: 6385befd988cb440e7cc8065
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667442736294-6352637d0f9bdb641c44e52d.jpeg?w=200&h=200&f=face
      fullname: wuxiaojun
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wuxiaojun
      type: user
    createdAt: '2023-06-13T02:41:17.000Z'
    data:
      status: closed
    id: 6487d74de8fc845078d039c4
    type: status-change
  author: wuxiaojun
  created_at: 2023-06-13 01:41:17+00:00
  id: 6487d74de8fc845078d039c4
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: IDEA-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-v0.1
repo_type: model
status: closed
target_branch: null
title: Overflow of tokenizer caused by problematic model_max_len
