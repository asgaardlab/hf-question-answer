!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Goldenblood56
conflicting_files: null
created_at: 2023-07-30 02:44:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-07-30T03:44:12.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9299612641334534
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Will this work at 4K or 8K context in oobabooga yet?  I noticed
          Oobabooga seems to now support 70B GGML  at up to 4K is that right? With
          the Llama 2? Does this update apply to using models like this at either
          4K or 8K? Just wondering. </p>

          <p>And if so does anyone know optimal settings in Oobabooga? </p>

          <p>For<br>n_gqa and rms_norm_eps?</p>

          <p>I know how to do the rest of the settings including "compress_pos_emb"
          but the above two I''m not sure about for 33B. I know what to set them for
          70B llama 2.<br>I feel like this model is not supported yet but maybe it
          is? </p>

          '
        raw: "Will this work at 4K or 8K context in oobabooga yet?  I noticed Oobabooga\
          \ seems to now support 70B GGML  at up to 4K is that right? With the Llama\
          \ 2? Does this update apply to using models like this at either 4K or 8K?\
          \ Just wondering. \n\nAnd if so does anyone know optimal settings in Oobabooga?\
          \ \n\nFor\nn_gqa and rms_norm_eps?\n\nI know how to do the rest of the settings\
          \ including \"compress_pos_emb\" but the above two I'm not sure about for\
          \ 33B. I know what to set them for 70B llama 2. \nI feel like this model\
          \ is not supported yet but maybe it is? "
        updatedAt: '2023-07-30T03:45:49.203Z'
      numEdits: 3
      reactions: []
    id: 64c5dc8ced521f27a4191904
    type: comment
  author: Goldenblood56
  content: "Will this work at 4K or 8K context in oobabooga yet?  I noticed Oobabooga\
    \ seems to now support 70B GGML  at up to 4K is that right? With the Llama 2?\
    \ Does this update apply to using models like this at either 4K or 8K? Just wondering.\
    \ \n\nAnd if so does anyone know optimal settings in Oobabooga? \n\nFor\nn_gqa\
    \ and rms_norm_eps?\n\nI know how to do the rest of the settings including \"\
    compress_pos_emb\" but the above two I'm not sure about for 33B. I know what to\
    \ set them for 70B llama 2. \nI feel like this model is not supported yet but\
    \ maybe it is? "
  created_at: 2023-07-30 02:44:12+00:00
  edited: true
  hidden: false
  id: 64c5dc8ced521f27a4191904
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Guanaco-33B-SuperHOT-8K-GGML
repo_type: model
status: open
target_branch: null
title: 'Will this work at 4K or 8K context in oobabooga yet? '
