!!python/object:huggingface_hub.community.DiscussionWithDetails
author: benfy
conflicting_files: null
created_at: 2023-05-03 02:46:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7725d2bb59f8f8d89149dd59b96381d1.svg
      fullname: Ben Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: benfy
      type: user
    createdAt: '2023-05-03T03:46:47.000Z'
    data:
      edited: false
      editors:
      - benfy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7725d2bb59f8f8d89149dd59b96381d1.svg
          fullname: Ben Ong
          isHf: false
          isPro: false
          name: benfy
          type: user
        html: '<p>As title, I am currently running RTX 3090 24GB, but this model exhaust
          my VRAM after 10-15 lines of prompt, is there any other setting should I
          change to optimize the balance between VRAM consumption and performance?
          Really enjoyed this model but sadly I can''t really get the most out of
          it because of out of memory errors.</p>

          <p>My current setting is oobabooga as following:<br>-auto-devices -wbits
          4 -groupsize 128 -model_type llama ~ this setting exhaust VRAM around 10
          lines of prompt, generation speed is acceptable<br>-auto-devices -wbits
          4 -groupsize 128 -model_type llama -gpu-memory 23 -pre_layer 50 ~ this setting
          will prevent out of memory errors at the moment but the generation speed
          is very slow</p>

          <p>Any other suggestions?</p>

          '
        raw: "As title, I am currently running RTX 3090 24GB, but this model exhaust\
          \ my VRAM after 10-15 lines of prompt, is there any other setting should\
          \ I change to optimize the balance between VRAM consumption and performance?\
          \ Really enjoyed this model but sadly I can't really get the most out of\
          \ it because of out of memory errors.\r\n\r\nMy current setting is oobabooga\
          \ as following:\r\n-auto-devices -wbits 4 -groupsize 128 -model_type llama\
          \ ~ this setting exhaust VRAM around 10 lines of prompt, generation speed\
          \ is acceptable\r\n-auto-devices -wbits 4 -groupsize 128 -model_type llama\
          \ -gpu-memory 23 -pre_layer 50 ~ this setting will prevent out of memory\
          \ errors at the moment but the generation speed is very slow\r\n\r\nAny\
          \ other suggestions?"
        updatedAt: '2023-05-03T03:46:47.714Z'
      numEdits: 0
      reactions: []
    id: 6451d92704397681bcfe32b3
    type: comment
  author: benfy
  content: "As title, I am currently running RTX 3090 24GB, but this model exhaust\
    \ my VRAM after 10-15 lines of prompt, is there any other setting should I change\
    \ to optimize the balance between VRAM consumption and performance? Really enjoyed\
    \ this model but sadly I can't really get the most out of it because of out of\
    \ memory errors.\r\n\r\nMy current setting is oobabooga as following:\r\n-auto-devices\
    \ -wbits 4 -groupsize 128 -model_type llama ~ this setting exhaust VRAM around\
    \ 10 lines of prompt, generation speed is acceptable\r\n-auto-devices -wbits 4\
    \ -groupsize 128 -model_type llama -gpu-memory 23 -pre_layer 50 ~ this setting\
    \ will prevent out of memory errors at the moment but the generation speed is\
    \ very slow\r\n\r\nAny other suggestions?"
  created_at: 2023-05-03 02:46:47+00:00
  edited: false
  hidden: false
  id: 6451d92704397681bcfe32b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7725d2bb59f8f8d89149dd59b96381d1.svg
      fullname: Ben Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: benfy
      type: user
    createdAt: '2023-05-03T07:53:37.000Z'
    data:
      edited: false
      editors:
      - benfy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7725d2bb59f8f8d89149dd59b96381d1.svg
          fullname: Ben Ong
          isHf: false
          isPro: false
          name: benfy
          type: user
        html: '<p>Sorry, my bad. I have found the solution, I switched the model to
          "gpt4-x-alpasta-30b-4bit.safetensors" solved the performance and out of
          VRAM issue (previously the 128g variant cause me alot of issues), with parameters:<br>-auto-devices
          -wbits 4 -model_type llama</p>

          <p>Now it works like a charm.</p>

          '
        raw: 'Sorry, my bad. I have found the solution, I switched the model to "gpt4-x-alpasta-30b-4bit.safetensors"
          solved the performance and out of VRAM issue (previously the 128g variant
          cause me alot of issues), with parameters:

          -auto-devices -wbits 4 -model_type llama


          Now it works like a charm.'
        updatedAt: '2023-05-03T07:53:37.937Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nostromostro
    id: 6452130194a54195ce4f5d43
    type: comment
  author: benfy
  content: 'Sorry, my bad. I have found the solution, I switched the model to "gpt4-x-alpasta-30b-4bit.safetensors"
    solved the performance and out of VRAM issue (previously the 128g variant cause
    me alot of issues), with parameters:

    -auto-devices -wbits 4 -model_type llama


    Now it works like a charm.'
  created_at: 2023-05-03 06:53:37+00:00
  edited: false
  hidden: false
  id: 6452130194a54195ce4f5d43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
      fullname: Brett S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cleverest
      type: user
    createdAt: '2023-05-11T22:22:00.000Z'
    data:
      edited: false
      editors:
      - cleverest
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
          fullname: Brett S
          isHf: false
          isPro: false
          name: cleverest
          type: user
        html: '<p>What does -auto-devices DO?</p>

          '
        raw: What does -auto-devices DO?
        updatedAt: '2023-05-11T22:22:00.942Z'
      numEdits: 0
      reactions: []
    id: 645d6a884438da4fcc210ea5
    type: comment
  author: cleverest
  content: What does -auto-devices DO?
  created_at: 2023-05-11 21:22:00+00:00
  edited: false
  hidden: false
  id: 645d6a884438da4fcc210ea5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df932694e10a6ef0e8d78aebc2ce253c.svg
      fullname: Peter Beks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kwissbeats
      type: user
    createdAt: '2023-05-15T20:57:14.000Z'
    data:
      edited: false
      editors:
      - Kwissbeats
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df932694e10a6ef0e8d78aebc2ce253c.svg
          fullname: Peter Beks
          isHf: false
          isPro: false
          name: Kwissbeats
          type: user
        html: '<blockquote>

          <p>What does -auto-devices DO?</p>

          </blockquote>

          <p>My understanding, nothing when using GPTQ.</p>

          '
        raw: '> What does -auto-devices DO?


          My understanding, nothing when using GPTQ.'
        updatedAt: '2023-05-15T20:57:14.373Z'
      numEdits: 0
      reactions: []
    id: 64629caad290a75bd98f8fd0
    type: comment
  author: Kwissbeats
  content: '> What does -auto-devices DO?


    My understanding, nothing when using GPTQ.'
  created_at: 2023-05-15 19:57:14+00:00
  edited: false
  hidden: false
  id: 64629caad290a75bd98f8fd0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: MetaIX/GPT4-X-Alpasta-30b-4bit
repo_type: model
status: open
target_branch: null
title: Best optimized settings for 24GB VRAM video card?
