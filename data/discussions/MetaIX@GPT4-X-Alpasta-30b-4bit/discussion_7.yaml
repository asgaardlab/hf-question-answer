!!python/object:huggingface_hub.community.DiscussionWithDetails
author: droidriz
conflicting_files: null
created_at: 2023-05-19 16:26:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a10178e034382f046922bba6cdc3d8d8.svg
      fullname: Rizwan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: droidriz
      type: user
    createdAt: '2023-05-19T17:26:13.000Z'
    data:
      edited: false
      editors:
      - droidriz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a10178e034382f046922bba6cdc3d8d8.svg
          fullname: Rizwan
          isHf: false
          isPro: false
          name: droidriz
          type: user
        html: '<p>hi i am getting "AttributeError: ''LlamaForCausalLM'' object has
          no attribute ''generate_with_streaming''" error with following run </p>

          <p> python server.py --model MetaIX_GPT4-X-Alpasta-30b-4bit --model_type
          llama --wbits 4 --groupsize 128 --auto-devices<br>INFO:Loading MetaIX_GPT4-X-Alpasta-30b-4bit...<br>INFO:Found
          the following quantized model: models/MetaIX_GPT4-X-Alpasta-30b-4bit/gpt4-x-alpasta-30b-128g-4bit.safetensors<br>INFO:Loaded
          the model in 29.45 seconds.</p>

          '
        raw: "hi i am getting \"AttributeError: 'LlamaForCausalLM' object has no attribute\
          \ 'generate_with_streaming'\" error with following run \r\n\r\n python server.py\
          \ --model MetaIX_GPT4-X-Alpasta-30b-4bit --model_type llama --wbits 4 --groupsize\
          \ 128 --auto-devices\r\nINFO:Loading MetaIX_GPT4-X-Alpasta-30b-4bit...\r\
          \nINFO:Found the following quantized model: models/MetaIX_GPT4-X-Alpasta-30b-4bit/gpt4-x-alpasta-30b-128g-4bit.safetensors\r\
          \nINFO:Loaded the model in 29.45 seconds."
        updatedAt: '2023-05-19T17:26:13.980Z'
      numEdits: 0
      reactions: []
    id: 6467b135fc6f6da8b11ccac5
    type: comment
  author: droidriz
  content: "hi i am getting \"AttributeError: 'LlamaForCausalLM' object has no attribute\
    \ 'generate_with_streaming'\" error with following run \r\n\r\n python server.py\
    \ --model MetaIX_GPT4-X-Alpasta-30b-4bit --model_type llama --wbits 4 --groupsize\
    \ 128 --auto-devices\r\nINFO:Loading MetaIX_GPT4-X-Alpasta-30b-4bit...\r\nINFO:Found\
    \ the following quantized model: models/MetaIX_GPT4-X-Alpasta-30b-4bit/gpt4-x-alpasta-30b-128g-4bit.safetensors\r\
    \nINFO:Loaded the model in 29.45 seconds."
  created_at: 2023-05-19 16:26:13+00:00
  edited: false
  hidden: false
  id: 6467b135fc6f6da8b11ccac5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a10178e034382f046922bba6cdc3d8d8.svg
      fullname: Rizwan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: droidriz
      type: user
    createdAt: '2023-05-19T17:31:42.000Z'
    data:
      edited: false
      editors:
      - droidriz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a10178e034382f046922bba6cdc3d8d8.svg
          fullname: Rizwan
          isHf: false
          isPro: false
          name: droidriz
          type: user
        html: '<p>--no-stream gives this error<br>    raise ValueError(<br>ValueError:
          The following <code>model_kwargs</code> are not used by the model: [''context'',
          ''token_count''] (note: typos in the generate arguments will also show up
          in this list)</p>

          '
        raw: "--no-stream gives this error\n    raise ValueError(\nValueError: The\
          \ following `model_kwargs` are not used by the model: ['context', 'token_count']\
          \ (note: typos in the generate arguments will also show up in this list)"
        updatedAt: '2023-05-19T17:31:42.555Z'
      numEdits: 0
      reactions: []
    id: 6467b27eab75d9cb3c4ae488
    type: comment
  author: droidriz
  content: "--no-stream gives this error\n    raise ValueError(\nValueError: The following\
    \ `model_kwargs` are not used by the model: ['context', 'token_count'] (note:\
    \ typos in the generate arguments will also show up in this list)"
  created_at: 2023-05-19 16:31:42+00:00
  edited: false
  hidden: false
  id: 6467b27eab75d9cb3c4ae488
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657830933687-noauth.jpeg?w=200&h=200&f=face
      fullname: Gero Doll
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Limbicnation
      type: user
    createdAt: '2023-06-14T13:00:28.000Z'
    data:
      edited: false
      editors:
      - Limbicnation
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6994001269340515
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657830933687-noauth.jpeg?w=200&h=200&f=face
          fullname: Gero Doll
          isHf: false
          isPro: false
          name: Limbicnation
          type: user
        html: '<p>Hi I am getting the same error when trying to run the model with
          oobabooga webui</p>

          <p><code>    raise ValueError( ValueError: The following `model_kwargs`
          are not used by the model: [''context'', ''token_count''] (note: typos in
          the generate arguments will also show up in this list) Output generated
          in 0.01 seconds (0.00 tokens/s, 0 tokens, context 36, seed 16471360)</code></p>

          '
        raw: 'Hi I am getting the same error when trying to run the model with oobabooga
          webui

          ```    raise ValueError(

          ValueError: The following `model_kwargs` are not used by the model: [''context'',
          ''token_count''] (note: typos in the generate arguments will also show up
          in this list)

          Output generated in 0.01 seconds (0.00 tokens/s, 0 tokens, context 36, seed
          16471360)```'
        updatedAt: '2023-06-14T13:00:28.356Z'
      numEdits: 0
      reactions: []
    id: 6489b9ec08b6a836a6d9fc14
    type: comment
  author: Limbicnation
  content: 'Hi I am getting the same error when trying to run the model with oobabooga
    webui

    ```    raise ValueError(

    ValueError: The following `model_kwargs` are not used by the model: [''context'',
    ''token_count''] (note: typos in the generate arguments will also show up in this
    list)

    Output generated in 0.01 seconds (0.00 tokens/s, 0 tokens, context 36, seed 16471360)```'
  created_at: 2023-06-14 12:00:28+00:00
  edited: false
  hidden: false
  id: 6489b9ec08b6a836a6d9fc14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671579102189-noauth.jpeg?w=200&h=200&f=face
      fullname: Bruno Kreiner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brunokreiner
      type: user
    createdAt: '2023-06-15T00:29:54.000Z'
    data:
      edited: false
      editors:
      - brunokreiner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5912063717842102
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671579102189-noauth.jpeg?w=200&h=200&f=face
          fullname: Bruno Kreiner
          isHf: false
          isPro: false
          name: brunokreiner
          type: user
        html: '<p>Same error for me with the MetaIX_OpenAssistant-Llama-30b-4bit model</p>

          '
        raw: Same error for me with the MetaIX_OpenAssistant-Llama-30b-4bit model
        updatedAt: '2023-06-15T00:29:54.847Z'
      numEdits: 0
      reactions: []
    id: 648a5b82ff0f4d8a545aca15
    type: comment
  author: brunokreiner
  content: Same error for me with the MetaIX_OpenAssistant-Llama-30b-4bit model
  created_at: 2023-06-14 23:29:54+00:00
  edited: false
  hidden: false
  id: 648a5b82ff0f4d8a545aca15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671579102189-noauth.jpeg?w=200&h=200&f=face
      fullname: Bruno Kreiner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brunokreiner
      type: user
    createdAt: '2023-06-15T00:57:57.000Z'
    data:
      edited: false
      editors:
      - brunokreiner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8811894059181213
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671579102189-noauth.jpeg?w=200&h=200&f=face
          fullname: Bruno Kreiner
          isHf: false
          isPro: false
          name: brunokreiner
          type: user
        html: '<p>Ok, so for the other model I''m using I got it to work again. I
          downgraded the transformers_version in "config.json" to "4.28.0.dev0" based
          on another working model. Then i got the error that not all tensors are
          on the same device, so i disabled autodevice, cpu and disk settings in textgeneration
          webui. Just a quick fix.</p>

          '
        raw: Ok, so for the other model I'm using I got it to work again. I downgraded
          the transformers_version in "config.json" to "4.28.0.dev0" based on another
          working model. Then i got the error that not all tensors are on the same
          device, so i disabled autodevice, cpu and disk settings in textgeneration
          webui. Just a quick fix.
        updatedAt: '2023-06-15T00:57:57.131Z'
      numEdits: 0
      reactions: []
    id: 648a62150e2624e9779ec04a
    type: comment
  author: brunokreiner
  content: Ok, so for the other model I'm using I got it to work again. I downgraded
    the transformers_version in "config.json" to "4.28.0.dev0" based on another working
    model. Then i got the error that not all tensors are on the same device, so i
    disabled autodevice, cpu and disk settings in textgeneration webui. Just a quick
    fix.
  created_at: 2023-06-14 23:57:57+00:00
  edited: false
  hidden: false
  id: 648a62150e2624e9779ec04a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: MetaIX/GPT4-X-Alpasta-30b-4bit
repo_type: model
status: open
target_branch: null
title: Error when running generate in text generation ui
