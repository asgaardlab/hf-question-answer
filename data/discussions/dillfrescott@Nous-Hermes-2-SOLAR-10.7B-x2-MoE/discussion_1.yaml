!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Lolkid654
conflicting_files: null
created_at: 2024-01-03 06:34:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/80c8ae4a89932319b6e304a5c26e6335.svg
      fullname: Hector Gonzalez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lolkid654
      type: user
    createdAt: '2024-01-03T06:34:45.000Z'
    data:
      edited: false
      editors:
      - Lolkid654
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9628671407699585
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/80c8ae4a89932319b6e304a5c26e6335.svg
          fullname: Hector Gonzalez
          isHf: false
          isPro: false
          name: Lolkid654
          type: user
        html: '<p>Hey i downloaded both of the Sonya and Silicone-medium models you
          merged and i have to say i am impressed with their performance. I find both
          funny and mind blowing that by merging the same model, it can be smarter.
          But i actually have a few questions regarding the process you do with it.</p>

          <p>is the process "Black magic"? Like is there a limit to how many times
          one can merge models to end up with something like a 100B parameter model?</p>

          <p>Does the model deteriorate with more consecutive merges?</p>

          <p>How does this work exactly and why does it work?</p>

          <p>How did you find this? Where you experimenting with merging models?</p>

          <p>Does this process work with every model? even old ones, smaller ones
          or bigger models like 20B and above?</p>

          <p>and lastly, would it be possible to do this with for example Pygmalion
          6B and create a 10 or 11B version of that model?</p>

          '
        raw: "Hey i downloaded both of the Sonya and Silicone-medium models you merged\
          \ and i have to say i am impressed with their performance. I find both funny\
          \ and mind blowing that by merging the same model, it can be smarter. But\
          \ i actually have a few questions regarding the process you do with it.\r\
          \n\r\nis the process \"Black magic\"? Like is there a limit to how many\
          \ times one can merge models to end up with something like a 100B parameter\
          \ model?\r\n\r\nDoes the model deteriorate with more consecutive merges?\r\
          \n\r\nHow does this work exactly and why does it work?\r\n\r\nHow did you\
          \ find this? Where you experimenting with merging models?\r\n\r\nDoes this\
          \ process work with every model? even old ones, smaller ones or bigger models\
          \ like 20B and above?\r\n\r\nand lastly, would it be possible to do this\
          \ with for example Pygmalion 6B and create a 10 or 11B version of that model?"
        updatedAt: '2024-01-03T06:34:45.276Z'
      numEdits: 0
      reactions: []
    id: 65950005a33889b27f4d6a50
    type: comment
  author: Lolkid654
  content: "Hey i downloaded both of the Sonya and Silicone-medium models you merged\
    \ and i have to say i am impressed with their performance. I find both funny and\
    \ mind blowing that by merging the same model, it can be smarter. But i actually\
    \ have a few questions regarding the process you do with it.\r\n\r\nis the process\
    \ \"Black magic\"? Like is there a limit to how many times one can merge models\
    \ to end up with something like a 100B parameter model?\r\n\r\nDoes the model\
    \ deteriorate with more consecutive merges?\r\n\r\nHow does this work exactly\
    \ and why does it work?\r\n\r\nHow did you find this? Where you experimenting\
    \ with merging models?\r\n\r\nDoes this process work with every model? even old\
    \ ones, smaller ones or bigger models like 20B and above?\r\n\r\nand lastly, would\
    \ it be possible to do this with for example Pygmalion 6B and create a 10 or 11B\
    \ version of that model?"
  created_at: 2024-01-03 06:34:45+00:00
  edited: false
  hidden: false
  id: 65950005a33889b27f4d6a50
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
      fullname: Cross Nastasi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: dillfrescott
      type: user
    createdAt: '2024-01-04T08:19:33.000Z'
    data:
      edited: true
      editors:
      - dillfrescott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9807031750679016
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
          fullname: Cross Nastasi
          isHf: false
          isPro: false
          name: dillfrescott
          type: user
        html: '<p>I''m glad you like the results!</p>

          <p>I''m not positive about a lot of these questions, for the simple reason
          of: These things are kind of unpredictable, so I''d like to say that, yes
          you could merge a 7B over itself x amount of times to create a 100B model
          that would perform x times better. But I haven''t attempted it myself, so
          I could be very wrong about the outcome.</p>

          <p>With my MoE experiments, I wasn''t expecting a model that is better than
          the base it was created from, but it seems, in some sort of way, that the
          final product is a bit better from my testings, even with a temperature
          of 0 set during inference.</p>

          <p>This was discovered purely from experimentation. I wanted to avoid tokenizer
          issues from differing models, so I attempted to merge the same model, in
          some experiments upon itself via a passthrough method, and also into an
          MoE. Most of the time it works.</p>

          <p>I have no clue if it works with older models or even if it will work
          with models of the future. My saying is that experimentation is best. You''ll
          never truly know until you attempt it.</p>

          <p>I really wish I had all the answers to these questions and could say
          that doing this or that will work best, but, again, these things tend to
          work in mysterious ways from my evaluations. Maybe someone much smarter
          than me would be able to explain it better.</p>

          <p>And yes, I could attempt it with the Pygmalion 6B model and see how it
          fares. Thats a good idea.</p>

          <p>So overall, I may be able to give better answers in the future after
          more experimentation, or you may be able to do something like join Eric
          Hartford''s discord server and ask someone much smarter than I, such as
          Fernando, why and how these things work!</p>

          '
        raw: 'I''m glad you like the results!


          I''m not positive about a lot of these questions, for the simple reason
          of: These things are kind of unpredictable, so I''d like to say that, yes
          you could merge a 7B over itself x amount of times to create a 100B model
          that would perform x times better. But I haven''t attempted it myself, so
          I could be very wrong about the outcome.


          With my MoE experiments, I wasn''t expecting a model that is better than
          the base it was created from, but it seems, in some sort of way, that the
          final product is a bit better from my testings, even with a temperature
          of 0 set during inference.


          This was discovered purely from experimentation. I wanted to avoid tokenizer
          issues from differing models, so I attempted to merge the same model, in
          some experiments upon itself via a passthrough method, and also into an
          MoE. Most of the time it works.


          I have no clue if it works with older models or even if it will work with
          models of the future. My saying is that experimentation is best. You''ll
          never truly know until you attempt it.


          I really wish I had all the answers to these questions and could say that
          doing this or that will work best, but, again, these things tend to work
          in mysterious ways from my evaluations. Maybe someone much smarter than
          me would be able to explain it better.


          And yes, I could attempt it with the Pygmalion 6B model and see how it fares.
          Thats a good idea.


          So overall, I may be able to give better answers in the future after more
          experimentation, or you may be able to do something like join Eric Hartford''s
          discord server and ask someone much smarter than I, such as Fernando, why
          and how these things work!'
        updatedAt: '2024-01-04T08:20:46.154Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PrimeD
    id: 65966a1546624a86ff45ab80
    type: comment
  author: dillfrescott
  content: 'I''m glad you like the results!


    I''m not positive about a lot of these questions, for the simple reason of: These
    things are kind of unpredictable, so I''d like to say that, yes you could merge
    a 7B over itself x amount of times to create a 100B model that would perform x
    times better. But I haven''t attempted it myself, so I could be very wrong about
    the outcome.


    With my MoE experiments, I wasn''t expecting a model that is better than the base
    it was created from, but it seems, in some sort of way, that the final product
    is a bit better from my testings, even with a temperature of 0 set during inference.


    This was discovered purely from experimentation. I wanted to avoid tokenizer issues
    from differing models, so I attempted to merge the same model, in some experiments
    upon itself via a passthrough method, and also into an MoE. Most of the time it
    works.


    I have no clue if it works with older models or even if it will work with models
    of the future. My saying is that experimentation is best. You''ll never truly
    know until you attempt it.


    I really wish I had all the answers to these questions and could say that doing
    this or that will work best, but, again, these things tend to work in mysterious
    ways from my evaluations. Maybe someone much smarter than me would be able to
    explain it better.


    And yes, I could attempt it with the Pygmalion 6B model and see how it fares.
    Thats a good idea.


    So overall, I may be able to give better answers in the future after more experimentation,
    or you may be able to do something like join Eric Hartford''s discord server and
    ask someone much smarter than I, such as Fernando, why and how these things work!'
  created_at: 2024-01-04 08:19:33+00:00
  edited: true
  hidden: false
  id: 65966a1546624a86ff45ab80
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/80c8ae4a89932319b6e304a5c26e6335.svg
      fullname: Hector Gonzalez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lolkid654
      type: user
    createdAt: '2024-01-04T08:28:12.000Z'
    data:
      edited: false
      editors:
      - Lolkid654
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9659172296524048
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/80c8ae4a89932319b6e304a5c26e6335.svg
          fullname: Hector Gonzalez
          isHf: false
          isPro: false
          name: Lolkid654
          type: user
        html: '<blockquote>

          <p>So overall, I may be able to give better answers in the future after
          more experimentation, or you may be able to do something like join Eric
          Hartford''s discord server and as someone much smarter than I, such as Fernando,
          why and how these things work!</p>

          </blockquote>

          <p>Thanks for the reply!<br>Sorry if i was asking many questions, i just
          find it both fascinating and kinda intriguing at the same time. I never
          thought this sort of method could work or even be possible at all. XD<br>If
          you ever try it on older modes such as the Pygmalion or future ones, i''d
          love to download them and check them out. I''ve got a feeling, people will
          start doing this from now on in one way or another, merging their models
          together like this if it brings any sort of performance boost. About the
          discord server, if you have an invite or know where i can get it, i''ll
          join in.</p>

          <p>Best of luck in the future!</p>

          '
        raw: '>So overall, I may be able to give better answers in the future after
          more experimentation, or you may be able to do something like join Eric
          Hartford''s discord server and as someone much smarter than I, such as Fernando,
          why and how these things work!


          Thanks for the reply!

          Sorry if i was asking many questions, i just find it both fascinating and
          kinda intriguing at the same time. I never thought this sort of method could
          work or even be possible at all. XD

          If you ever try it on older modes such as the Pygmalion or future ones,
          i''d love to download them and check them out. I''ve got a feeling, people
          will start doing this from now on in one way or another, merging their models
          together like this if it brings any sort of performance boost. About the
          discord server, if you have an invite or know where i can get it, i''ll
          join in.


          Best of luck in the future!'
        updatedAt: '2024-01-04T08:28:12.133Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - dillfrescott
    id: 65966c1c580e8d6da145fc2d
    type: comment
  author: Lolkid654
  content: '>So overall, I may be able to give better answers in the future after
    more experimentation, or you may be able to do something like join Eric Hartford''s
    discord server and as someone much smarter than I, such as Fernando, why and how
    these things work!


    Thanks for the reply!

    Sorry if i was asking many questions, i just find it both fascinating and kinda
    intriguing at the same time. I never thought this sort of method could work or
    even be possible at all. XD

    If you ever try it on older modes such as the Pygmalion or future ones, i''d love
    to download them and check them out. I''ve got a feeling, people will start doing
    this from now on in one way or another, merging their models together like this
    if it brings any sort of performance boost. About the discord server, if you have
    an invite or know where i can get it, i''ll join in.


    Best of luck in the future!'
  created_at: 2024-01-04 08:28:12+00:00
  edited: false
  hidden: false
  id: 65966c1c580e8d6da145fc2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
      fullname: Cross Nastasi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: dillfrescott
      type: user
    createdAt: '2024-01-04T08:36:46.000Z'
    data:
      edited: false
      editors:
      - dillfrescott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.930377185344696
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
          fullname: Cross Nastasi
          isHf: false
          isPro: false
          name: dillfrescott
          type: user
        html: '<p>Yes dont worry!</p>

          <p><a rel="nofollow" href="https://discord.gg/TcKjjXPy">https://discord.gg/TcKjjXPy</a></p>

          <p>Thats a link to his server!</p>

          <p>Thank you so much for the questions I actually really appreciate them!</p>

          '
        raw: 'Yes dont worry!


          https://discord.gg/TcKjjXPy


          Thats a link to his server!


          Thank you so much for the questions I actually really appreciate them!'
        updatedAt: '2024-01-04T08:36:46.471Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Lolkid654
    id: 65966e1e7cda5685b34efb88
    type: comment
  author: dillfrescott
  content: 'Yes dont worry!


    https://discord.gg/TcKjjXPy


    Thats a link to his server!


    Thank you so much for the questions I actually really appreciate them!'
  created_at: 2024-01-04 08:36:46+00:00
  edited: false
  hidden: false
  id: 65966e1e7cda5685b34efb88
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: dillfrescott/Nous-Hermes-2-SOLAR-10.7B-x2-MoE
repo_type: model
status: open
target_branch: null
title: Pygmalion merge like Sonya and Silicone-medium?
