!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GayCodeGal
conflicting_files: null
created_at: 2022-06-18 00:31:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8efc5c054bb60e8a1b0d0dd4b46decbb.svg
      fullname: Sarah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GayCodeGal
      type: user
    createdAt: '2022-06-18T01:31:04.000Z'
    data:
      edited: false
      editors:
      - GayCodeGal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8efc5c054bb60e8a1b0d0dd4b46decbb.svg
          fullname: Sarah
          isHf: false
          isPro: false
          name: GayCodeGal
          type: user
        html: '<p>I tried to recreate your tflite model using the following code I
          took from an android project</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          tensorflow <span class="hljs-keyword">as</span> tf

          <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span>
          TFGPT2LMHeadModel


          model = TFGPT2LMHeadModel.from_pretrained(<span class="hljs-string">''distilgpt2''</span>)


          input_spec = tf.TensorSpec([<span class="hljs-number">1</span>, <span class="hljs-number">64</span>],
          tf.int32)

          model._set_inputs(input_spec, training=<span class="hljs-literal">False</span>)


          <span class="hljs-built_in">print</span>(model.inputs)

          <span class="hljs-built_in">print</span>(model.outputs)


          converter = tf.lite.TFLiteConverter.from_keras_model(model)


          <span class="hljs-comment"># For FP16 quantization:</span>

          <span class="hljs-comment">#converter.optimizations = [tf.lite.Optimize.DEFAULT]</span>

          <span class="hljs-comment">#converter.target_spec.supported_types = [tf.float16]</span>


          tflite_model = converter.convert()


          <span class="hljs-built_in">open</span>(<span class="hljs-string">"distilgpt2-64.tflite"</span>,
          <span class="hljs-string">"wb"</span>).write(tflite_model)

          </code></pre>

          <p>and it did not run correctly (it produced a tflite file that didn''t
          work with <a rel="nofollow" href="https://github.com/huggingface/tflite-android-transformers">https://github.com/huggingface/tflite-android-transformers</a>),
          whereas your model works with the android app great.</p>

          <p>It was slightly smaller 324626324 bytes and had a different hash ef5bf0a1dbbf640a1b1b3f03e1c7d43cd99e19f1f2dd2568cda91511f72da38d  distilgpt2-64.tflite</p>

          <p>I want to convert your model and make it smaller and try it as the 16
          bit floating point or 8bit versions, as that tflite android project does
          with the real gpt2, but I don''t know what you did to create your tflite
          model. Is there a repo somewhere? Or could you upload the script that did
          it? Thanks!</p>

          '
        raw: "I tried to recreate your tflite model using the following code I took\
          \ from an android project\r\n\r\n```python\r\nimport tensorflow as tf\r\n\
          from transformers import TFGPT2LMHeadModel\r\n\r\nmodel = TFGPT2LMHeadModel.from_pretrained('distilgpt2')\r\
          \n\r\ninput_spec = tf.TensorSpec([1, 64], tf.int32)\r\nmodel._set_inputs(input_spec,\
          \ training=False)\r\n\r\nprint(model.inputs)\r\nprint(model.outputs)\r\n\
          \r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n\
          # For FP16 quantization:\r\n#converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\
          \n#converter.target_spec.supported_types = [tf.float16]\r\n\r\ntflite_model\
          \ = converter.convert()\r\n\r\nopen(\"distilgpt2-64.tflite\", \"wb\").write(tflite_model)\r\
          \n```\r\n\r\nand it did not run correctly (it produced a tflite file that\
          \ didn't work with https://github.com/huggingface/tflite-android-transformers),\
          \ whereas your model works with the android app great.\r\n\r\nIt was slightly\
          \ smaller 324626324 bytes and had a different hash ef5bf0a1dbbf640a1b1b3f03e1c7d43cd99e19f1f2dd2568cda91511f72da38d\
          \  distilgpt2-64.tflite\r\n\r\nI want to convert your model and make it\
          \ smaller and try it as the 16 bit floating point or 8bit versions, as that\
          \ tflite android project does with the real gpt2, but I don't know what\
          \ you did to create your tflite model. Is there a repo somewhere? Or could\
          \ you upload the script that did it? Thanks!"
        updatedAt: '2022-06-18T01:31:04.042Z'
      numEdits: 0
      reactions: []
    id: 62ad2ad87bd21e09d03b759a
    type: comment
  author: GayCodeGal
  content: "I tried to recreate your tflite model using the following code I took\
    \ from an android project\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom\
    \ transformers import TFGPT2LMHeadModel\r\n\r\nmodel = TFGPT2LMHeadModel.from_pretrained('distilgpt2')\r\
    \n\r\ninput_spec = tf.TensorSpec([1, 64], tf.int32)\r\nmodel._set_inputs(input_spec,\
    \ training=False)\r\n\r\nprint(model.inputs)\r\nprint(model.outputs)\r\n\r\nconverter\
    \ = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n# For FP16 quantization:\r\
    \n#converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n#converter.target_spec.supported_types\
    \ = [tf.float16]\r\n\r\ntflite_model = converter.convert()\r\n\r\nopen(\"distilgpt2-64.tflite\"\
    , \"wb\").write(tflite_model)\r\n```\r\n\r\nand it did not run correctly (it produced\
    \ a tflite file that didn't work with https://github.com/huggingface/tflite-android-transformers),\
    \ whereas your model works with the android app great.\r\n\r\nIt was slightly\
    \ smaller 324626324 bytes and had a different hash ef5bf0a1dbbf640a1b1b3f03e1c7d43cd99e19f1f2dd2568cda91511f72da38d\
    \  distilgpt2-64.tflite\r\n\r\nI want to convert your model and make it smaller\
    \ and try it as the 16 bit floating point or 8bit versions, as that tflite android\
    \ project does with the real gpt2, but I don't know what you did to create your\
    \ tflite model. Is there a repo somewhere? Or could you upload the script that\
    \ did it? Thanks!"
  created_at: 2022-06-18 00:31:04+00:00
  edited: false
  hidden: false
  id: 62ad2ad87bd21e09d03b759a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2022-06-20T12:38:59.000Z'
    data:
      edited: false
      editors:
      - julien-c
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: "<p>maybe for <span data-props=\"{&quot;user&quot;:&quot;pierric&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/pierric\"\
          >@<span class=\"underline\">pierric</span></a></span>\n\n\t</span></span></p>\n"
        raw: maybe for @pierric
        updatedAt: '2022-06-20T12:38:59.550Z'
      numEdits: 0
      reactions: []
    id: 62b06a6385a75c9bed82f362
    type: comment
  author: julien-c
  content: maybe for @pierric
  created_at: 2022-06-20 11:38:59+00:00
  edited: false
  hidden: false
  id: 62b06a6385a75c9bed82f362
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624630689857-5e67de201009063689407481.jpeg?w=200&h=200&f=face
      fullname: Pierric Cistac
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: true
      name: pierric
      type: user
    createdAt: '2022-08-09T22:14:19.000Z'
    data:
      edited: false
      editors:
      - pierric
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624630689857-5e67de201009063689407481.jpeg?w=200&h=200&f=face
          fullname: Pierric Cistac
          isHf: true
          isPro: true
          name: pierric
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;GayCodeGal&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/GayCodeGal\"\
          >@<span class=\"underline\">GayCodeGal</span></a></span>\n\n\t</span></span>,\
          \ sorry for the late reply!</p>\n<p>Your code seems indeed to be the same\
          \ as <a rel=\"nofollow\" href=\"https://github.com/huggingface/tflite-android-transformers/blob/master/models_generation/gpt2.py\"\
          >https://github.com/huggingface/tflite-android-transformers/blob/master/models_generation/gpt2.py</a>,\
          \ which is the script that was used to generate the TFLite version for both\
          \ gpt2 and distilgpt2. My guess here is that it has to do with the versions\
          \ of TFLite you're using - if you don't have any error during the generation,\
          \ the generated model is probably correct, but it's possible that it's incompatible\
          \ with the android app because of a different TFLite version used in the\
          \ app (it seems to be TFLite 2.0, see <a rel=\"nofollow\" href=\"https://github.com/huggingface/tflite-android-transformers/blob/master/gpt2/build.gradle#L56\"\
          >https://github.com/huggingface/tflite-android-transformers/blob/master/gpt2/build.gradle#L56</a>).</p>\n"
        raw: 'Hi @GayCodeGal, sorry for the late reply!


          Your code seems indeed to be the same as https://github.com/huggingface/tflite-android-transformers/blob/master/models_generation/gpt2.py,
          which is the script that was used to generate the TFLite version for both
          gpt2 and distilgpt2. My guess here is that it has to do with the versions
          of TFLite you''re using - if you don''t have any error during the generation,
          the generated model is probably correct, but it''s possible that it''s incompatible
          with the android app because of a different TFLite version used in the app
          (it seems to be TFLite 2.0, see https://github.com/huggingface/tflite-android-transformers/blob/master/gpt2/build.gradle#L56).'
        updatedAt: '2022-08-09T22:14:19.292Z'
      numEdits: 0
      reactions: []
    id: 62f2dc3b62c5483a00236e0d
    type: comment
  author: pierric
  content: 'Hi @GayCodeGal, sorry for the late reply!


    Your code seems indeed to be the same as https://github.com/huggingface/tflite-android-transformers/blob/master/models_generation/gpt2.py,
    which is the script that was used to generate the TFLite version for both gpt2
    and distilgpt2. My guess here is that it has to do with the versions of TFLite
    you''re using - if you don''t have any error during the generation, the generated
    model is probably correct, but it''s possible that it''s incompatible with the
    android app because of a different TFLite version used in the app (it seems to
    be TFLite 2.0, see https://github.com/huggingface/tflite-android-transformers/blob/master/gpt2/build.gradle#L56).'
  created_at: 2022-08-09 21:14:19+00:00
  edited: false
  hidden: false
  id: 62f2dc3b62c5483a00236e0d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: distilgpt2
repo_type: model
status: open
target_branch: null
title: How did you make the tflite model?
