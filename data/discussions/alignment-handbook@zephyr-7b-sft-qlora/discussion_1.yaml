!!python/object:huggingface_hub.community.DiscussionWithDetails
author: brekk
conflicting_files: null
created_at: 2023-12-07 02:09:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648fdcb4cb9b9578a7e53bad/K4zQCoKvjtDD0NKDkQrM6.png?w=200&h=200&f=face
      fullname: Chung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brekk
      type: user
    createdAt: '2023-12-07T02:09:36.000Z'
    data:
      edited: false
      editors:
      - brekk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6771703362464905
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648fdcb4cb9b9578a7e53bad/K4zQCoKvjtDD0NKDkQrM6.png?w=200&h=200&f=face
          fullname: Chung
          isHf: false
          isPro: false
          name: brekk
          type: user
        html: '<p>Issue:<br>I cannot start inference endpoint, the log says:<br>2023/12/07
          10:53:21 ~ Error: ShardCannotStart<br>2023/12/07 10:53:21 ~ {"timestamp":"2023-12-07T01:53:21.369939Z","level":"ERROR","fields":{"message":"Shard
          0 failed to start"},"target":"text_generation_launcher"}<br>2023/12/07 10:53:21
          ~ {"timestamp":"2023-12-07T01:53:21.369962Z","level":"INFO","fields":{"message":"Shutting
          down shards"},"target":"text_generation_launcher"}</p>

          <p>Steps for reproduce:<br><code>Deploy</code> &gt; <code>Inference Endpoint</code>
          &gt; Select A10G AWS instance </p>

          <p>Is there a way to use inference endpoint with this lora model?</p>

          <p>Thanks in advance!</p>

          '
        raw: "Issue:\r\nI cannot start inference endpoint, the log says:\r\n2023/12/07\
          \ 10:53:21 ~ Error: ShardCannotStart\r\n2023/12/07 10:53:21 ~ {\"timestamp\"\
          :\"2023-12-07T01:53:21.369939Z\",\"level\":\"ERROR\",\"fields\":{\"message\"\
          :\"Shard 0 failed to start\"},\"target\":\"text_generation_launcher\"}\r\
          \n2023/12/07 10:53:21 ~ {\"timestamp\":\"2023-12-07T01:53:21.369962Z\",\"\
          level\":\"INFO\",\"fields\":{\"message\":\"Shutting down shards\"},\"target\"\
          :\"text_generation_launcher\"}\r\n\r\nSteps for reproduce:\r\n`Deploy` >\
          \ `Inference Endpoint` > Select A10G AWS instance \r\n\r\nIs there a way\
          \ to use inference endpoint with this lora model?\r\n\r\nThanks in advance!"
        updatedAt: '2023-12-07T02:09:36.829Z'
      numEdits: 0
      reactions: []
    id: 65712960d8b22ac39ff263bc
    type: comment
  author: brekk
  content: "Issue:\r\nI cannot start inference endpoint, the log says:\r\n2023/12/07\
    \ 10:53:21 ~ Error: ShardCannotStart\r\n2023/12/07 10:53:21 ~ {\"timestamp\":\"\
    2023-12-07T01:53:21.369939Z\",\"level\":\"ERROR\",\"fields\":{\"message\":\"Shard\
    \ 0 failed to start\"},\"target\":\"text_generation_launcher\"}\r\n2023/12/07\
    \ 10:53:21 ~ {\"timestamp\":\"2023-12-07T01:53:21.369962Z\",\"level\":\"INFO\"\
    ,\"fields\":{\"message\":\"Shutting down shards\"},\"target\":\"text_generation_launcher\"\
    }\r\n\r\nSteps for reproduce:\r\n`Deploy` > `Inference Endpoint` > Select A10G\
    \ AWS instance \r\n\r\nIs there a way to use inference endpoint with this lora\
    \ model?\r\n\r\nThanks in advance!"
  created_at: 2023-12-07 02:09:36+00:00
  edited: false
  hidden: false
  id: 65712960d8b22ac39ff263bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-07T09:23:17.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.655369758605957
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;brekk&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/brekk\">@<span class=\"\
          underline\">brekk</span></a></span>\n\n\t</span></span><br>I am not sure\
          \ the inference endpoints support Lora, you should consider use the merged\
          \ model (which I believe is: <a href=\"https://huggingface.co/alignment-handbook/zephyr-7b-sft-full\"\
          >https://huggingface.co/alignment-handbook/zephyr-7b-sft-full</a> right\
          \ <span data-props=\"{&quot;user&quot;:&quot;lewtun&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lewtun\">@<span class=\"\
          underline\">lewtun</span></a></span>\n\n\t</span></span> ?) - if not, you\
          \ can merge the model yourself, please have a look at: <a href=\"https://huggingface.co/docs/peft/v0.7.0/en/package_reference/lora#peft.LoraModel.merge_and_unload\"\
          >https://huggingface.co/docs/peft/v0.7.0/en/package_reference/lora#peft.LoraModel.merge_and_unload</a>\
          \ but to merge the lora model you can just:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> peft <span class=\"hljs-keyword\"\
          >import</span> AutoPeftModelForCausalLM\n\nmerged_model_id = YOUR_NEW_MODEL_ID\n\
          \nmodel = AutoPeftModelForCausalLM.from_pretrained(peft_model_id)\nmerged_model\
          \ = model.merge_and_unload()\nmerged_model.push_to_hub(YOUR_NEW_MODEL_ID)\n\
          </code></pre>\n"
        raw: "Hi @brekk \nI am not sure the inference endpoints support Lora, you\
          \ should consider use the merged model (which I believe is: https://huggingface.co/alignment-handbook/zephyr-7b-sft-full\
          \ right @lewtun ?) - if not, you can merge the model yourself, please have\
          \ a look at: https://huggingface.co/docs/peft/v0.7.0/en/package_reference/lora#peft.LoraModel.merge_and_unload\
          \ but to merge the lora model you can just:\n```python\nfrom peft import\
          \ AutoPeftModelForCausalLM\n\nmerged_model_id = YOUR_NEW_MODEL_ID\n\nmodel\
          \ = AutoPeftModelForCausalLM.from_pretrained(peft_model_id)\nmerged_model\
          \ = model.merge_and_unload()\nmerged_model.push_to_hub(YOUR_NEW_MODEL_ID)\n\
          ```"
        updatedAt: '2023-12-07T09:23:23.582Z'
      numEdits: 1
      reactions: []
    id: 65718f05f3fc330b8d778559
    type: comment
  author: ybelkada
  content: "Hi @brekk \nI am not sure the inference endpoints support Lora, you should\
    \ consider use the merged model (which I believe is: https://huggingface.co/alignment-handbook/zephyr-7b-sft-full\
    \ right @lewtun ?) - if not, you can merge the model yourself, please have a look\
    \ at: https://huggingface.co/docs/peft/v0.7.0/en/package_reference/lora#peft.LoraModel.merge_and_unload\
    \ but to merge the lora model you can just:\n```python\nfrom peft import AutoPeftModelForCausalLM\n\
    \nmerged_model_id = YOUR_NEW_MODEL_ID\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(peft_model_id)\n\
    merged_model = model.merge_and_unload()\nmerged_model.push_to_hub(YOUR_NEW_MODEL_ID)\n\
    ```"
  created_at: 2023-12-07 09:23:17+00:00
  edited: true
  hidden: false
  id: 65718f05f3fc330b8d778559
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648fdcb4cb9b9578a7e53bad/K4zQCoKvjtDD0NKDkQrM6.png?w=200&h=200&f=face
      fullname: Chung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brekk
      type: user
    createdAt: '2023-12-08T07:05:04.000Z'
    data:
      edited: false
      editors:
      - brekk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8963662981987
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648fdcb4cb9b9578a7e53bad/K4zQCoKvjtDD0NKDkQrM6.png?w=200&h=200&f=face
          fullname: Chung
          isHf: false
          isPro: false
          name: brekk
          type: user
        html: "<p>Thank you for the reply <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>.<br>I\
          \ will give it a try!</p>\n"
        raw: 'Thank you for the reply @ybelkada.

          I will give it a try!'
        updatedAt: '2023-12-08T07:05:04.038Z'
      numEdits: 0
      reactions: []
    id: 6572c020569d7ce0bcaa2a4f
    type: comment
  author: brekk
  content: 'Thank you for the reply @ybelkada.

    I will give it a try!'
  created_at: 2023-12-08 07:05:04+00:00
  edited: false
  hidden: false
  id: 6572c020569d7ce0bcaa2a4f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: alignment-handbook/zephyr-7b-sft-qlora
repo_type: model
status: open
target_branch: null
title: Failed to create inference endpoint
