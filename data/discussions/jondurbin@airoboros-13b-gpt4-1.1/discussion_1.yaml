!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hussainwali1
conflicting_files: null
created_at: 2023-06-12 07:32:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/305ffe97659628082e04ca17949cba89.svg
      fullname: Wali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hussainwali1
      type: user
    createdAt: '2023-06-12T08:32:09.000Z'
    data:
      edited: false
      editors:
      - hussainwali1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9321704506874084
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/305ffe97659628082e04ca17949cba89.svg
          fullname: Wali
          isHf: false
          isPro: false
          name: hussainwali1
          type: user
        html: '<p>Thank you for the great model,<br>I want to train a model like this
          but smaller one, I have the dataset prepared, can you please give me few
          pointers how to start with the training. I am complete noob rn.<br>My system:<br>RTX
          3090<br>64G Ram</p>

          '
        raw: "Thank you for the great model, \r\nI want to train a model like this\
          \ but smaller one, I have the dataset prepared, can you please give me few\
          \ pointers how to start with the training. I am complete noob rn.\r\nMy\
          \ system:\r\nRTX 3090\r\n64G Ram"
        updatedAt: '2023-06-12T08:32:09.530Z'
      numEdits: 0
      reactions: []
    id: 6486d8090fa3d9e574f87aca
    type: comment
  author: hussainwali1
  content: "Thank you for the great model, \r\nI want to train a model like this but\
    \ smaller one, I have the dataset prepared, can you please give me few pointers\
    \ how to start with the training. I am complete noob rn.\r\nMy system:\r\nRTX\
    \ 3090\r\n64G Ram"
  created_at: 2023-06-12 07:32:09+00:00
  edited: false
  hidden: false
  id: 6486d8090fa3d9e574f87aca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-06-12T08:51:43.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5368211269378662
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: "<p>If your dataset is formatted as JSONL (one json string per line,\
          \ newline separated) and has keys \"instruction\", \"response\", you should\
          \ be able to fine-tune a model with either FastChat (full-fine-tune) or\
          \ qlora.</p>\n<p>In either approach, you'll first want to get the llama\
          \ 13b base model:<br><a href=\"https://huggingface.co/decapoda-research/llama-13b-hf\"\
          >https://huggingface.co/decapoda-research/llama-13b-hf</a></p>\n<p>Then,\
          \ make sure you replace these two files in that base model:<br>special_tokens_map.json\
          \ (replace it with: <a href=\"https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.1/resolve/main/special_tokens_map.json\"\
          >https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.1/resolve/main/special_tokens_map.json</a>)<br>tokenizer_config.json\
          \ (replace it with: <a href=\"https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.1/resolve/main/tokenizer_config.json\"\
          >https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.1/resolve/main/tokenizer_config.json</a>)</p>\n\
          <p>qlora may be the easiest, and should theoretically work on a 3090.  To\
          \ do this, you'll want to use my. fork of qlora here: <a rel=\"nofollow\"\
          \ href=\"https://github.com/jondurbin/qlora\">https://github.com/jondurbin/qlora</a><br>Be\
          \ sure to install the dependencies: <code>pip install -r ./qlora/requirements.txt</code>,\
          \ then run something like:</p>\n<pre><code>export WANDB_API_KEY=[replace\
          \ with your key]\nexport WANDB_PROJECT=[replace with your project name]\n\
          \npython qlora.py \\\n    --model_name_or_path /path/to/llama-13b-hf \\\n\
          \    --output_dir /path/to/output_dir \\\n    --max_steps 900 \\\n    --logging_steps\
          \ 1 \\\n    --save_strategy steps \\\n    --data_seed 11422 \\\n    --save_steps\
          \ 25 \\\n    --save_total_limit 5 \\\n    --evaluation_strategy \"no\" \\\
          \n    --eval_dataset_size 2 \\\n    --max_new_tokens 2048 \\\n    --dataloader_num_workers\
          \ 3 \\\n    --logging_strategy steps \\\n    --remove_unused_columns False\
          \ \\\n    --do_train \\\n    --lora_r 64 \\\n    --lora_alpha 16 \\\n  \
          \  --lora_modules all \\\n    --double_quant \\\n    --quant_type nf4 \\\
          \n    --bf16 \\\n    --bits 4 \\\n    --warmup_ratio 0.03 \\\n    --lr_scheduler_type\
          \ constant \\\n    --gradient_checkpointing \\\n    --dataset /path/to/instructions.jsonl\
          \ \\\n    --dataset_format airoboros \\\n    --model_max_len 2048 \\\n \
          \   --per_device_train_batch_size 2 \\\n    --gradient_accumulation_steps\
          \ 16 \\\n    --learning_rate 0.0001 \\\n    --adam_beta2 0.999 \\\n    --max_grad_norm\
          \ 0.3 \\\n    --lora_dropout 0.05 \\\n    --weight_decay 0.0 \\\n    --seed\
          \ 11422 \\\n    --report_to wandb\n</code></pre>\n<p>You may be able to\
          \ increase per_device_train_batch_size a bit depending on how much vram\
          \ it's taking per pass, or you may need to set it to 1, just experiment\
          \ and check vram usage.  Make sure you replace the various paths, and update\
          \ the <code>--max_steps</code> value based on the number of instructions\
          \ you have in the training data.  I typically use 3 epochs, so you want\
          \ <code>steps = epochs * size of training data / (train batch size * gradient\
          \ accumulation steps)</code>, so say you had 500 instructions, you'd want\
          \ steps ~ <code>3 * 500 / (2 * 16) = 47</code></p>\n<p>I don't know if the\
          \ 3090 will have enough memory to do a full fine-tune of a 13b model, but\
          \ you can try with my fork of the fastchat repo and flash attention, e.g.\
          \  <code>git clone https://github.com/jondurbin/FastChat &amp;&amp; pip\
          \ install ./FastChat &amp;&amp; pip install flash_attn==1.0.5</code></p>\n\
          <p>FastChat expects the training data in conversation format, so you'd want\
          \ to convert your instruction/response jsonl file via:</p>\n<pre><code>import\
          \ json\nimport uuid\ninputs = [json.loads(line) for line in open(\"instructions.jsonl\"\
          ).readlines()]\nconversations = []\nfor row in inputs:\n    inputs = row['instruction']\n\
          \    conversations.append({\n        \"id\": str(uuid.uuid4()),\n      \
          \  \"conversations\": [\n            {\n                \"from\": \"human\"\
          ,\n                \"value\": inputs,\n            },\n            {\n \
          \               \"from\": \"gpt\",\n                \"value\": row['response']\n\
          \            },\n        ],\n    })\nwith open(\"as_conversations.json\"\
          , \"w\") as outfile:\n    outfile.write(json.dumps(conversations, indent=2))\n\
          </code></pre>\n<p>Then, after installing and getting your dataset prepped,\
          \ run something like:</p>\n<pre><code>export WANDB_API_KEY=[replace with\
          \ your key from wandb.ai, free to signup]\nexport WANDB_PROJECT=[name of\
          \ your project]\n\ntorchrun --nproc_per_node=1 --master_port=20001 ./FastChat/fastchat/train/train_mem.py\
          \ \\\n  --model_name_or_path /path/to/llama-13b-hf  \\\n  --data_path /path/to/as_conversations.json\
          \ \\\n  --output_dir /path/to/output_directory \\\n  --num_train_epochs\
          \ 3 \\\n  --per_device_train_batch_size 2 \\\n  --gradient_accumulation_steps\
          \ 4 \\\n  --evaluation_strategy \"no\" \\\n  --save_steps 25 \\\n  --save_total_limit\
          \ 3 \\\n  --learning_rate 1e-5 \\\n  --weight_decay 0. \\\n  --warmup_ratio\
          \ 0.04 \\\n  --lr_scheduler_type \"cosine\" \\\n  --logging_steps 1 \\\n\
          \  --fsdp \"full_shard auto_wrap offload\" \\\n  --fsdp_transformer_layer_cls_to_wrap\
          \ 'LlamaDecoderLayer' \\\n  --bf16 True \\\n  --tf32 True \\\n  --model_max_length\
          \ 2048 \\\n  --gradient_checkpointing True \\\n  --lazy_preprocess True\n\
          </code></pre>\n<p>Play around with per_device_train_batch_size and gradient\
          \ accumulation steps.</p>\n"
        raw: "If your dataset is formatted as JSONL (one json string per line, newline\
          \ separated) and has keys \"instruction\", \"response\", you should be able\
          \ to fine-tune a model with either FastChat (full-fine-tune) or qlora.\n\
          \nIn either approach, you'll first want to get the llama 13b base model:\n\
          https://huggingface.co/decapoda-research/llama-13b-hf\n\nThen, make sure\
          \ you replace these two files in that base model:\nspecial_tokens_map.json\
          \ (replace it with: https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.1/resolve/main/special_tokens_map.json)\n\
          tokenizer_config.json (replace it with: https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.1/resolve/main/tokenizer_config.json)\n\
          \nqlora may be the easiest, and should theoretically work on a 3090.  To\
          \ do this, you'll want to use my. fork of qlora here: https://github.com/jondurbin/qlora\n\
          Be sure to install the dependencies: `pip install -r ./qlora/requirements.txt`,\
          \ then run something like:\n\n```\nexport WANDB_API_KEY=[replace with your\
          \ key]\nexport WANDB_PROJECT=[replace with your project name]\n\npython\
          \ qlora.py \\\n    --model_name_or_path /path/to/llama-13b-hf \\\n    --output_dir\
          \ /path/to/output_dir \\\n    --max_steps 900 \\\n    --logging_steps 1\
          \ \\\n    --save_strategy steps \\\n    --data_seed 11422 \\\n    --save_steps\
          \ 25 \\\n    --save_total_limit 5 \\\n    --evaluation_strategy \"no\" \\\
          \n    --eval_dataset_size 2 \\\n    --max_new_tokens 2048 \\\n    --dataloader_num_workers\
          \ 3 \\\n    --logging_strategy steps \\\n    --remove_unused_columns False\
          \ \\\n    --do_train \\\n    --lora_r 64 \\\n    --lora_alpha 16 \\\n  \
          \  --lora_modules all \\\n    --double_quant \\\n    --quant_type nf4 \\\
          \n    --bf16 \\\n    --bits 4 \\\n    --warmup_ratio 0.03 \\\n    --lr_scheduler_type\
          \ constant \\\n    --gradient_checkpointing \\\n    --dataset /path/to/instructions.jsonl\
          \ \\\n    --dataset_format airoboros \\\n    --model_max_len 2048 \\\n \
          \   --per_device_train_batch_size 2 \\\n    --gradient_accumulation_steps\
          \ 16 \\\n    --learning_rate 0.0001 \\\n    --adam_beta2 0.999 \\\n    --max_grad_norm\
          \ 0.3 \\\n    --lora_dropout 0.05 \\\n    --weight_decay 0.0 \\\n    --seed\
          \ 11422 \\\n    --report_to wandb\n```\n\nYou may be able to increase per_device_train_batch_size\
          \ a bit depending on how much vram it's taking per pass, or you may need\
          \ to set it to 1, just experiment and check vram usage.  Make sure you replace\
          \ the various paths, and update the `--max_steps` value based on the number\
          \ of instructions you have in the training data.  I typically use 3 epochs,\
          \ so you want `steps = epochs * size of training data / (train batch size\
          \ * gradient accumulation steps)`, so say you had 500 instructions, you'd\
          \ want steps ~ `3 * 500 / (2 * 16) = 47`\n\nI don't know if the 3090 will\
          \ have enough memory to do a full fine-tune of a 13b model, but you can\
          \ try with my fork of the fastchat repo and flash attention, e.g.  `git\
          \ clone https://github.com/jondurbin/FastChat && pip install ./FastChat\
          \ && pip install flash_attn==1.0.5`\n\nFastChat expects the training data\
          \ in conversation format, so you'd want to convert your instruction/response\
          \ jsonl file via:\n\n```\nimport json\nimport uuid\ninputs = [json.loads(line)\
          \ for line in open(\"instructions.jsonl\").readlines()]\nconversations =\
          \ []\nfor row in inputs:\n    inputs = row['instruction']\n    conversations.append({\n\
          \        \"id\": str(uuid.uuid4()),\n        \"conversations\": [\n    \
          \        {\n                \"from\": \"human\",\n                \"value\"\
          : inputs,\n            },\n            {\n                \"from\": \"gpt\"\
          ,\n                \"value\": row['response']\n            },\n        ],\n\
          \    })\nwith open(\"as_conversations.json\", \"w\") as outfile:\n    outfile.write(json.dumps(conversations,\
          \ indent=2))\n```\n\nThen, after installing and getting your dataset prepped,\
          \ run something like:\n```\nexport WANDB_API_KEY=[replace with your key\
          \ from wandb.ai, free to signup]\nexport WANDB_PROJECT=[name of your project]\n\
          \ntorchrun --nproc_per_node=1 --master_port=20001 ./FastChat/fastchat/train/train_mem.py\
          \ \\\n  --model_name_or_path /path/to/llama-13b-hf  \\\n  --data_path /path/to/as_conversations.json\
          \ \\\n  --output_dir /path/to/output_directory \\\n  --num_train_epochs\
          \ 3 \\\n  --per_device_train_batch_size 2 \\\n  --gradient_accumulation_steps\
          \ 4 \\\n  --evaluation_strategy \"no\" \\\n  --save_steps 25 \\\n  --save_total_limit\
          \ 3 \\\n  --learning_rate 1e-5 \\\n  --weight_decay 0. \\\n  --warmup_ratio\
          \ 0.04 \\\n  --lr_scheduler_type \"cosine\" \\\n  --logging_steps 1 \\\n\
          \  --fsdp \"full_shard auto_wrap offload\" \\\n  --fsdp_transformer_layer_cls_to_wrap\
          \ 'LlamaDecoderLayer' \\\n  --bf16 True \\\n  --tf32 True \\\n  --model_max_length\
          \ 2048 \\\n  --gradient_checkpointing True \\\n  --lazy_preprocess True\n\
          ```\n\nPlay around with per_device_train_batch_size and gradient accumulation\
          \ steps."
        updatedAt: '2023-06-12T08:51:43.566Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - hussainwali1
    id: 6486dc9f9773a131834762a4
    type: comment
  author: jondurbin
  content: "If your dataset is formatted as JSONL (one json string per line, newline\
    \ separated) and has keys \"instruction\", \"response\", you should be able to\
    \ fine-tune a model with either FastChat (full-fine-tune) or qlora.\n\nIn either\
    \ approach, you'll first want to get the llama 13b base model:\nhttps://huggingface.co/decapoda-research/llama-13b-hf\n\
    \nThen, make sure you replace these two files in that base model:\nspecial_tokens_map.json\
    \ (replace it with: https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.1/resolve/main/special_tokens_map.json)\n\
    tokenizer_config.json (replace it with: https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.1/resolve/main/tokenizer_config.json)\n\
    \nqlora may be the easiest, and should theoretically work on a 3090.  To do this,\
    \ you'll want to use my. fork of qlora here: https://github.com/jondurbin/qlora\n\
    Be sure to install the dependencies: `pip install -r ./qlora/requirements.txt`,\
    \ then run something like:\n\n```\nexport WANDB_API_KEY=[replace with your key]\n\
    export WANDB_PROJECT=[replace with your project name]\n\npython qlora.py \\\n\
    \    --model_name_or_path /path/to/llama-13b-hf \\\n    --output_dir /path/to/output_dir\
    \ \\\n    --max_steps 900 \\\n    --logging_steps 1 \\\n    --save_strategy steps\
    \ \\\n    --data_seed 11422 \\\n    --save_steps 25 \\\n    --save_total_limit\
    \ 5 \\\n    --evaluation_strategy \"no\" \\\n    --eval_dataset_size 2 \\\n  \
    \  --max_new_tokens 2048 \\\n    --dataloader_num_workers 3 \\\n    --logging_strategy\
    \ steps \\\n    --remove_unused_columns False \\\n    --do_train \\\n    --lora_r\
    \ 64 \\\n    --lora_alpha 16 \\\n    --lora_modules all \\\n    --double_quant\
    \ \\\n    --quant_type nf4 \\\n    --bf16 \\\n    --bits 4 \\\n    --warmup_ratio\
    \ 0.03 \\\n    --lr_scheduler_type constant \\\n    --gradient_checkpointing \\\
    \n    --dataset /path/to/instructions.jsonl \\\n    --dataset_format airoboros\
    \ \\\n    --model_max_len 2048 \\\n    --per_device_train_batch_size 2 \\\n  \
    \  --gradient_accumulation_steps 16 \\\n    --learning_rate 0.0001 \\\n    --adam_beta2\
    \ 0.999 \\\n    --max_grad_norm 0.3 \\\n    --lora_dropout 0.05 \\\n    --weight_decay\
    \ 0.0 \\\n    --seed 11422 \\\n    --report_to wandb\n```\n\nYou may be able to\
    \ increase per_device_train_batch_size a bit depending on how much vram it's taking\
    \ per pass, or you may need to set it to 1, just experiment and check vram usage.\
    \  Make sure you replace the various paths, and update the `--max_steps` value\
    \ based on the number of instructions you have in the training data.  I typically\
    \ use 3 epochs, so you want `steps = epochs * size of training data / (train batch\
    \ size * gradient accumulation steps)`, so say you had 500 instructions, you'd\
    \ want steps ~ `3 * 500 / (2 * 16) = 47`\n\nI don't know if the 3090 will have\
    \ enough memory to do a full fine-tune of a 13b model, but you can try with my\
    \ fork of the fastchat repo and flash attention, e.g.  `git clone https://github.com/jondurbin/FastChat\
    \ && pip install ./FastChat && pip install flash_attn==1.0.5`\n\nFastChat expects\
    \ the training data in conversation format, so you'd want to convert your instruction/response\
    \ jsonl file via:\n\n```\nimport json\nimport uuid\ninputs = [json.loads(line)\
    \ for line in open(\"instructions.jsonl\").readlines()]\nconversations = []\n\
    for row in inputs:\n    inputs = row['instruction']\n    conversations.append({\n\
    \        \"id\": str(uuid.uuid4()),\n        \"conversations\": [\n          \
    \  {\n                \"from\": \"human\",\n                \"value\": inputs,\n\
    \            },\n            {\n                \"from\": \"gpt\",\n         \
    \       \"value\": row['response']\n            },\n        ],\n    })\nwith open(\"\
    as_conversations.json\", \"w\") as outfile:\n    outfile.write(json.dumps(conversations,\
    \ indent=2))\n```\n\nThen, after installing and getting your dataset prepped,\
    \ run something like:\n```\nexport WANDB_API_KEY=[replace with your key from wandb.ai,\
    \ free to signup]\nexport WANDB_PROJECT=[name of your project]\n\ntorchrun --nproc_per_node=1\
    \ --master_port=20001 ./FastChat/fastchat/train/train_mem.py \\\n  --model_name_or_path\
    \ /path/to/llama-13b-hf  \\\n  --data_path /path/to/as_conversations.json \\\n\
    \  --output_dir /path/to/output_directory \\\n  --num_train_epochs 3 \\\n  --per_device_train_batch_size\
    \ 2 \\\n  --gradient_accumulation_steps 4 \\\n  --evaluation_strategy \"no\" \\\
    \n  --save_steps 25 \\\n  --save_total_limit 3 \\\n  --learning_rate 1e-5 \\\n\
    \  --weight_decay 0. \\\n  --warmup_ratio 0.04 \\\n  --lr_scheduler_type \"cosine\"\
    \ \\\n  --logging_steps 1 \\\n  --fsdp \"full_shard auto_wrap offload\" \\\n \
    \ --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\n  --bf16 True \\\
    \n  --tf32 True \\\n  --model_max_length 2048 \\\n  --gradient_checkpointing True\
    \ \\\n  --lazy_preprocess True\n```\n\nPlay around with per_device_train_batch_size\
    \ and gradient accumulation steps."
  created_at: 2023-06-12 07:51:43+00:00
  edited: false
  hidden: false
  id: 6486dc9f9773a131834762a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6331a50d4bd187e9ce06a91b/kMm7tdb0d0PsTznLBNnpE.png?w=200&h=200&f=face
      fullname: Saifeddine ALOUI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ParisNeo
      type: user
    createdAt: '2023-06-28T09:04:23.000Z'
    data:
      edited: true
      editors:
      - ParisNeo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9297985434532166
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6331a50d4bd187e9ce06a91b/kMm7tdb0d0PsTznLBNnpE.png?w=200&h=200&f=face
          fullname: Saifeddine ALOUI
          isHf: false
          isPro: false
          name: ParisNeo
          type: user
        html: '<p>Hi and thanks for all this.</p>

          <p>Is there a way to fine tune other open source models like the MPT or
          Falcon?<br>I am making a training UI on my lollms-webui interface and want
          to make it as easy as possible. Like the user selects the base model he
          wants to use, the database, the training parameters, and press train.<br>Also,
          does qlora run as lora? Can we fuse the model with the newly generated lora
          delta? And can we restart traning a model that has already been trained
          using qlora?</p>

          <p>Thank you for all this</p>

          '
        raw: 'Hi and thanks for all this.


          Is there a way to fine tune other open source models like the MPT or Falcon?

          I am making a training UI on my lollms-webui interface and want to make
          it as easy as possible. Like the user selects the base model he wants to
          use, the database, the training parameters, and press train.

          Also, does qlora run as lora? Can we fuse the model with the newly generated
          lora delta? And can we restart traning a model that has already been trained
          using qlora?


          Thank you for all this

          '
        updatedAt: '2023-06-28T09:40:51.323Z'
      numEdits: 1
      reactions: []
    id: 649bf797d960e569b998894c
    type: comment
  author: ParisNeo
  content: 'Hi and thanks for all this.


    Is there a way to fine tune other open source models like the MPT or Falcon?

    I am making a training UI on my lollms-webui interface and want to make it as
    easy as possible. Like the user selects the base model he wants to use, the database,
    the training parameters, and press train.

    Also, does qlora run as lora? Can we fuse the model with the newly generated lora
    delta? And can we restart traning a model that has already been trained using
    qlora?


    Thank you for all this

    '
  created_at: 2023-06-28 08:04:23+00:00
  edited: true
  hidden: false
  id: 649bf797d960e569b998894c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/305ffe97659628082e04ca17949cba89.svg
      fullname: Wali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hussainwali1
      type: user
    createdAt: '2023-07-03T06:24:40.000Z'
    data:
      edited: false
      editors:
      - hussainwali1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46829426288604736
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/305ffe97659628082e04ca17949cba89.svg
          fullname: Wali
          isHf: false
          isPro: false
          name: hussainwali1
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ParisNeo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ParisNeo\">@<span class=\"\
          underline\">ParisNeo</span></a></span>\n\n\t</span></span> Here is a version\
          \ using the QLora method:<br>  <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1GrCda2sT0nE25fN-vOoMdpU-a93HQJg1?usp=sharing\"\
          >https://colab.research.google.com/drive/1GrCda2sT0nE25fN-vOoMdpU-a93HQJg1?usp=sharing</a></p>\n"
        raw: "@ParisNeo Here is a version using the QLora method:\n  https://colab.research.google.com/drive/1GrCda2sT0nE25fN-vOoMdpU-a93HQJg1?usp=sharing"
        updatedAt: '2023-07-03T06:24:40.451Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ParisNeo
    id: 64a269a8d3149e05bc748d2d
    type: comment
  author: hussainwali1
  content: "@ParisNeo Here is a version using the QLora method:\n  https://colab.research.google.com/drive/1GrCda2sT0nE25fN-vOoMdpU-a93HQJg1?usp=sharing"
  created_at: 2023-07-03 05:24:40+00:00
  edited: false
  hidden: false
  id: 64a269a8d3149e05bc748d2d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: jondurbin/airoboros-13b-gpt4-1.1
repo_type: model
status: open
target_branch: null
title: instructions
