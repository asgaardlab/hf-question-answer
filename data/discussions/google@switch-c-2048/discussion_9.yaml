!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ehartford
conflicting_files: null
created_at: 2023-11-20 19:20:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-11-20T19:20:50.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9878402352333069
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>This is beautiful.</p>

          '
        raw: This is beautiful.
        updatedAt: '2023-11-20T19:20:50.206Z'
      numEdits: 0
      reactions: []
    id: 655bb192fb74ac37d3eafda4
    type: comment
  author: ehartford
  content: This is beautiful.
  created_at: 2023-11-20 19:20:50+00:00
  edited: false
  hidden: false
  id: 655bb192fb74ac37d3eafda4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6462813577f63463fc5f554a/i5-MquBS0RB9nR8Y6Wn1K.png?w=200&h=200&f=face
      fullname: Karim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KarimJedda
      type: user
    createdAt: '2023-11-20T19:27:05.000Z'
    data:
      edited: false
      editors:
      - KarimJedda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.983329713344574
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6462813577f63463fc5f554a/i5-MquBS0RB9nR8Y6Wn1K.png?w=200&h=200&f=face
          fullname: Karim
          isHf: false
          isPro: false
          name: KarimJedda
          type: user
        html: '<p>I concur. </p>

          <p>What''s weird though is that some files show up as "commited on Nov 15,
          2022", glitch? o_o</p>

          '
        raw: "I concur. \n\nWhat's weird though is that some files show up as \"commited\
          \ on Nov 15, 2022\", glitch? o_o"
        updatedAt: '2023-11-20T19:27:05.105Z'
      numEdits: 0
      reactions: []
    id: 655bb309c80e332db2c9d24c
    type: comment
  author: KarimJedda
  content: "I concur. \n\nWhat's weird though is that some files show up as \"commited\
    \ on Nov 15, 2022\", glitch? o_o"
  created_at: 2023-11-20 19:27:05+00:00
  edited: false
  hidden: false
  id: 655bb309c80e332db2c9d24c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
      fullname: bread null
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: breadlicker45
      type: user
    createdAt: '2023-11-21T01:07:23.000Z'
    data:
      edited: false
      editors:
      - breadlicker45
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9801055192947388
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
          fullname: bread null
          isHf: false
          isPro: false
          name: breadlicker45
          type: user
        html: '<blockquote>

          <p>This is beautiful.</p>

          </blockquote>

          <p>this is 5 years old you know</p>

          '
        raw: '> This is beautiful.


          this is 5 years old you know'
        updatedAt: '2023-11-21T01:07:23.551Z'
      numEdits: 0
      reactions: []
    id: 655c02cb935d0f9a75e06a3d
    type: comment
  author: breadlicker45
  content: '> This is beautiful.


    this is 5 years old you know'
  created_at: 2023-11-21 01:07:23+00:00
  edited: false
  hidden: false
  id: 655c02cb935d0f9a75e06a3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
      fullname: Alpin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alpindale
      type: user
    createdAt: '2023-11-21T17:05:47.000Z'
    data:
      edited: false
      editors:
      - alpindale
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9578229188919067
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
          fullname: Alpin
          isHf: false
          isPro: false
          name: alpindale
          type: user
        html: '<blockquote>

          <blockquote>

          <p>This is beautiful.</p>

          </blockquote>

          <p>this is 5 years old you know</p>

          </blockquote>

          <p>Does not make it any less beautiful.</p>

          '
        raw: "> > This is beautiful.\n> \n> this is 5 years old you know\n\nDoes not\
          \ make it any less beautiful."
        updatedAt: '2023-11-21T17:05:47.116Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F91D"
        users:
        - alpayariyak
        - pszemraj
    id: 655ce36b6e0cdbd945a7c5bc
    type: comment
  author: alpindale
  content: "> > This is beautiful.\n> \n> this is 5 years old you know\n\nDoes not\
    \ make it any less beautiful."
  created_at: 2023-11-21 17:05:47+00:00
  edited: false
  hidden: false
  id: 655ce36b6e0cdbd945a7c5bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
      fullname: Jeremy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JuLuComputing
      type: user
    createdAt: '2023-11-21T19:11:40.000Z'
    data:
      edited: true
      editors:
      - JuLuComputing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9748470187187195
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
          fullname: Jeremy
          isHf: false
          isPro: false
          name: JuLuComputing
          type: user
        html: '<blockquote>

          <p>This is beautiful.</p>

          </blockquote>

          <p>Yes, I''m very impressed as well.  Are you going to do something cool
          with this, like fine tuning or swapping out some components for smaller
          (and more competent/contemporary) models?  My vote is:  Yes, please!</p>

          <p>I could see a system of special Samanthas tearin'' it up.  ;-)</p>

          '
        raw: '> This is beautiful.


          Yes, I''m very impressed as well.  Are you going to do something cool with
          this, like fine tuning or swapping out some components for smaller (and
          more competent/contemporary) models?  My vote is:  Yes, please!


          I could see a system of special Samanthas tearin'' it up.  ;-)'
        updatedAt: '2023-11-21T19:19:04.892Z'
      numEdits: 2
      reactions: []
    id: 655d00ec62531252742fdf76
    type: comment
  author: JuLuComputing
  content: '> This is beautiful.


    Yes, I''m very impressed as well.  Are you going to do something cool with this,
    like fine tuning or swapping out some components for smaller (and more competent/contemporary)
    models?  My vote is:  Yes, please!


    I could see a system of special Samanthas tearin'' it up.  ;-)'
  created_at: 2023-11-21 19:11:40+00:00
  edited: true
  hidden: false
  id: 655d00ec62531252742fdf76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
      fullname: bread null
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: breadlicker45
      type: user
    createdAt: '2023-11-22T16:55:58.000Z'
    data:
      edited: false
      editors:
      - breadlicker45
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9440359473228455
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
          fullname: bread null
          isHf: false
          isPro: false
          name: breadlicker45
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>This is beautiful.</p>

          </blockquote>

          <p>this is 5 years old you know</p>

          </blockquote>

          <p>Does not make it any less beautiful.</p>

          </blockquote>

          <p>agreed</p>

          '
        raw: "> > > This is beautiful.\n> > \n> > this is 5 years old you know\n>\
          \ \n> Does not make it any less beautiful.\n\nagreed"
        updatedAt: '2023-11-22T16:55:58.789Z'
      numEdits: 0
      reactions: []
    id: 655e329e1b960c5c616e69a8
    type: comment
  author: breadlicker45
  content: "> > > This is beautiful.\n> > \n> > this is 5 years old you know\n> \n\
    > Does not make it any less beautiful.\n\nagreed"
  created_at: 2023-11-22 16:55:58+00:00
  edited: false
  hidden: false
  id: 655e329e1b960c5c616e69a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
      fullname: mrfakename
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrfakename
      type: user
    createdAt: '2023-11-22T22:12:42.000Z'
    data:
      edited: false
      editors:
      - mrfakename
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9882591962814331
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
          fullname: mrfakename
          isHf: false
          isPro: false
          name: mrfakename
          type: user
        html: '<p>How does anyone run this</p>

          '
        raw: How does anyone run this
        updatedAt: '2023-11-22T22:12:42.124Z'
      numEdits: 0
      reactions: []
    id: 655e7cda5afa950b64c449c3
    type: comment
  author: mrfakename
  content: How does anyone run this
  created_at: 2023-11-22 22:12:42+00:00
  edited: false
  hidden: false
  id: 655e7cda5afa950b64c449c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
      fullname: mrfakename
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrfakename
      type: user
    createdAt: '2023-11-22T22:12:54.000Z'
    data:
      edited: false
      editors:
      - mrfakename
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5100818276405334
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
          fullname: mrfakename
          isHf: false
          isPro: false
          name: mrfakename
          type: user
        html: "<p>Is there a gguf version <span data-props=\"{&quot;user&quot;:&quot;thebloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/thebloke\"\
          >@<span class=\"underline\">thebloke</span></a></span>\n\n\t</span></span></p>\n"
        raw: Is there a gguf version @thebloke
        updatedAt: '2023-11-22T22:12:54.012Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - pszemraj
    id: 655e7ce60523479b97f68c8e
    type: comment
  author: mrfakename
  content: Is there a gguf version @thebloke
  created_at: 2023-11-22 22:12:54+00:00
  edited: false
  hidden: false
  id: 655e7ce60523479b97f68c8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
      fullname: bread null
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: breadlicker45
      type: user
    createdAt: '2023-11-23T12:10:37.000Z'
    data:
      edited: true
      editors:
      - breadlicker45
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9860609173774719
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
          fullname: bread null
          isHf: false
          isPro: false
          name: breadlicker45
          type: user
        html: '<blockquote>

          <p>How does anyone run this</p>

          </blockquote>

          <p>you would need 4 h100 gpus to run it, that''s what my math says</p>

          '
        raw: '> How does anyone run this


          you would need 4 h100 gpus to run it, that''s what my math says'
        updatedAt: '2023-11-23T12:11:21.843Z'
      numEdits: 1
      reactions: []
    id: 655f413d60f1a93e90319ff8
    type: comment
  author: breadlicker45
  content: '> How does anyone run this


    you would need 4 h100 gpus to run it, that''s what my math says'
  created_at: 2023-11-23 12:10:37+00:00
  edited: true
  hidden: false
  id: 655f413d60f1a93e90319ff8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
      fullname: Jeremy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JuLuComputing
      type: user
    createdAt: '2023-11-23T12:35:09.000Z'
    data:
      edited: false
      editors:
      - JuLuComputing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9084140658378601
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
          fullname: Jeremy
          isHf: false
          isPro: false
          name: JuLuComputing
          type: user
        html: '<blockquote>

          <blockquote>

          <p>How does anyone run this</p>

          </blockquote>

          <p>you would need 4 h100 gpus to run it, that''s what my math says</p>

          </blockquote>

          <p>The Model Card gives another option.  It gives examples for GPU and CPU,
          a GPU-poor person could run it on a machine with a lot of ram and no GPU:</p>

          <p><a href="https://huggingface.co/google/switch-c-2048#running-the-model-on-a-cpu">https://huggingface.co/google/switch-c-2048#running-the-model-on-a-cpu</a></p>

          '
        raw: "> > How does anyone run this\n> \n> you would need 4 h100 gpus to run\
          \ it, that's what my math says\n\nThe Model Card gives another option. \
          \ It gives examples for GPU and CPU, a GPU-poor person could run it on a\
          \ machine with a lot of ram and no GPU:\n\nhttps://huggingface.co/google/switch-c-2048#running-the-model-on-a-cpu"
        updatedAt: '2023-11-23T12:35:09.714Z'
      numEdits: 0
      reactions: []
    id: 655f46fd0bda1e8ff83f0b30
    type: comment
  author: JuLuComputing
  content: "> > How does anyone run this\n> \n> you would need 4 h100 gpus to run\
    \ it, that's what my math says\n\nThe Model Card gives another option.  It gives\
    \ examples for GPU and CPU, a GPU-poor person could run it on a machine with a\
    \ lot of ram and no GPU:\n\nhttps://huggingface.co/google/switch-c-2048#running-the-model-on-a-cpu"
  created_at: 2023-11-23 12:35:09+00:00
  edited: false
  hidden: false
  id: 655f46fd0bda1e8ff83f0b30
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
      fullname: mrfakename
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrfakename
      type: user
    createdAt: '2023-11-23T16:43:53.000Z'
    data:
      edited: false
      editors:
      - mrfakename
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9718195199966431
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
          fullname: mrfakename
          isHf: false
          isPro: false
          name: mrfakename
          type: user
        html: '<blockquote>

          <blockquote>

          <p>How does anyone run this</p>

          </blockquote>

          <p>you would need 4 h100 gpus to run it, that''s what my math says</p>

          </blockquote>

          <p>Have any extras :)</p>

          '
        raw: '> > How does anyone run this

          >

          > you would need 4 h100 gpus to run it, that''s what my math says


          Have any extras :)'
        updatedAt: '2023-11-23T16:43:53.680Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - susnato
    id: 655f8149a7c531e282173de4
    type: comment
  author: mrfakename
  content: '> > How does anyone run this

    >

    > you would need 4 h100 gpus to run it, that''s what my math says


    Have any extras :)'
  created_at: 2023-11-23 16:43:53+00:00
  edited: false
  hidden: false
  id: 655f8149a7c531e282173de4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-11-25T23:56:06.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.945827841758728
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>+1 for the GGUF version. Personally, I am very excited to be able
          to mask and then unmask tokens on the CPU </p>

          '
        raw: '+1 for the GGUF version. Personally, I am very excited to be able to
          mask and then unmask tokens on the CPU '
        updatedAt: '2023-11-25T23:56:06.021Z'
      numEdits: 0
      reactions: []
    id: 65628996f231380a6cfc4326
    type: comment
  author: pszemraj
  content: '+1 for the GGUF version. Personally, I am very excited to be able to mask
    and then unmask tokens on the CPU '
  created_at: 2023-11-25 23:56:06+00:00
  edited: false
  hidden: false
  id: 65628996f231380a6cfc4326
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
      fullname: mrfakename
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrfakename
      type: user
    createdAt: '2023-11-26T01:18:39.000Z'
    data:
      edited: false
      editors:
      - mrfakename
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9407491683959961
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
          fullname: mrfakename
          isHf: false
          isPro: false
          name: mrfakename
          type: user
        html: "<p>\U0001F92F How many GB of memory do you have to run this <span data-props=\"\
          {&quot;user&quot;:&quot;pszemraj&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/pszemraj\">@<span class=\"underline\">pszemraj</span></a></span>\n\
          \n\t</span></span> </p>\n"
        raw: "\U0001F92F How many GB of memory do you have to run this @pszemraj "
        updatedAt: '2023-11-26T01:18:39.135Z'
      numEdits: 0
      reactions: []
    id: 65629cefcf4f078533b14447
    type: comment
  author: mrfakename
  content: "\U0001F92F How many GB of memory do you have to run this @pszemraj "
  created_at: 2023-11-26 01:18:39+00:00
  edited: false
  hidden: false
  id: 65629cefcf4f078533b14447
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
      fullname: Jeremy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JuLuComputing
      type: user
    createdAt: '2023-11-26T02:09:05.000Z'
    data:
      edited: false
      editors:
      - JuLuComputing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9444994926452637
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
          fullname: Jeremy
          isHf: false
          isPro: false
          name: JuLuComputing
          type: user
        html: '<blockquote>

          <p>+1 for the GGUF version. Personally, I am very excited to be able to
          mask and then unmask tokens on the CPU</p>

          </blockquote>

          <p>It''s quite the beast.  Have you looked at Switch 128 or 256?  You may
          not need the GGUF.</p>

          '
        raw: '> +1 for the GGUF version. Personally, I am very excited to be able
          to mask and then unmask tokens on the CPU


          It''s quite the beast.  Have you looked at Switch 128 or 256?  You may not
          need the GGUF.'
        updatedAt: '2023-11-26T02:09:05.060Z'
      numEdits: 0
      reactions: []
    id: 6562a8c1ec7e239899cf7ff0
    type: comment
  author: JuLuComputing
  content: '> +1 for the GGUF version. Personally, I am very excited to be able to
    mask and then unmask tokens on the CPU


    It''s quite the beast.  Have you looked at Switch 128 or 256?  You may not need
    the GGUF.'
  created_at: 2023-11-26 02:09:05+00:00
  edited: false
  hidden: false
  id: 6562a8c1ec7e239899cf7ff0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
      fullname: Jeremy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JuLuComputing
      type: user
    createdAt: '2023-11-26T02:11:19.000Z'
    data:
      edited: false
      editors:
      - JuLuComputing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9736827611923218
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
          fullname: Jeremy
          isHf: false
          isPro: false
          name: JuLuComputing
          type: user
        html: "<blockquote>\n<p>\U0001F92F How many GB of memory do you have to run\
          \ this <span data-props=\"{&quot;user&quot;:&quot;pszemraj&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pszemraj\">@<span class=\"\
          underline\">pszemraj</span></a></span>\n\n\t</span></span></p>\n</blockquote>\n\
          <p>I was curious, too.  I just added the the switch 2048, 256, and 128 XXL\
          \ to my download list.  I'll report back in a few days after the turtle\
          \ speed downloads on my end are complete.</p>\n"
        raw: "> \U0001F92F How many GB of memory do you have to run this @pszemraj\n\
          \nI was curious, too.  I just added the the switch 2048, 256, and 128 XXL\
          \ to my download list.  I'll report back in a few days after the turtle\
          \ speed downloads on my end are complete."
        updatedAt: '2023-11-26T02:11:19.132Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - susnato
    id: 6562a9471338610184b0bb5f
    type: comment
  author: JuLuComputing
  content: "> \U0001F92F How many GB of memory do you have to run this @pszemraj\n\
    \nI was curious, too.  I just added the the switch 2048, 256, and 128 XXL to my\
    \ download list.  I'll report back in a few days after the turtle speed downloads\
    \ on my end are complete."
  created_at: 2023-11-26 02:11:19+00:00
  edited: false
  hidden: false
  id: 6562a9471338610184b0bb5f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
      fullname: mrfakename
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrfakename
      type: user
    createdAt: '2023-11-26T03:19:18.000Z'
    data:
      edited: false
      editors:
      - mrfakename
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9706647992134094
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
          fullname: mrfakename
          isHf: false
          isPro: false
          name: mrfakename
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;JuLuComputing&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/JuLuComputing\"\
          >@<span class=\"underline\">JuLuComputing</span></a></span>\n\n\t</span></span>\
          \ If downloading speed is an issue, have you considered pget? I think it\
          \ uses multithreading or something by downloading different chunks of the\
          \ model at once to speed things up. Seems a bit faster for HF models than\
          \ wget, however I haven't benchmarked it</p>\n"
        raw: '@JuLuComputing If downloading speed is an issue, have you considered
          pget? I think it uses multithreading or something by downloading different
          chunks of the model at once to speed things up. Seems a bit faster for HF
          models than wget, however I haven''t benchmarked it'
        updatedAt: '2023-11-26T03:19:18.741Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - JuLuComputing
    id: 6562b936a72f05d2eaaf3e8d
    type: comment
  author: mrfakename
  content: '@JuLuComputing If downloading speed is an issue, have you considered pget?
    I think it uses multithreading or something by downloading different chunks of
    the model at once to speed things up. Seems a bit faster for HF models than wget,
    however I haven''t benchmarked it'
  created_at: 2023-11-26 03:19:18+00:00
  edited: false
  hidden: false
  id: 6562b936a72f05d2eaaf3e8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
      fullname: Jeremy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JuLuComputing
      type: user
    createdAt: '2023-11-26T04:14:44.000Z'
    data:
      edited: false
      editors:
      - JuLuComputing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9571164846420288
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
          fullname: Jeremy
          isHf: false
          isPro: false
          name: JuLuComputing
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;JuLuComputing&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/JuLuComputing\"\
          >@<span class=\"underline\">JuLuComputing</span></a></span>\n\n\t</span></span>\
          \ If downloading speed is an issue, have you considered pget? I think it\
          \ uses multithreading or something by downloading different chunks of the\
          \ model at once to speed things up. Seems a bit faster for HF models than\
          \ wget, however I haven't benchmarked it</p>\n</blockquote>\n<p>Thanks for\
          \ the suggestion!  I'm going to check it out and see if how well it works.\
          \  I gave up on wget a long time ago.  Currently, I use a modified version\
          \ of github/bodaay. It has some issues with sometimes thinking a download\
          \ is complete when there are certain types of data errors, so I built an\
          \ error handling script around it to make up for its deficiencies.  It does\
          \ 5 threads by default and has some level of data error handling and SHA\
          \ checking.  As for speed, I think it's negligible, if not slower that just\
          \ using a browser download due to some Huggingface algorithm, I presume.\
          \  Nonetheless, the advantages of a downloader is for handling Huggingface\
          \ folders with large numbers of files and the ability to resume downloads.</p>\n"
        raw: '> @JuLuComputing If downloading speed is an issue, have you considered
          pget? I think it uses multithreading or something by downloading different
          chunks of the model at once to speed things up. Seems a bit faster for HF
          models than wget, however I haven''t benchmarked it


          Thanks for the suggestion!  I''m going to check it out and see if how well
          it works.  I gave up on wget a long time ago.  Currently, I use a modified
          version of github/bodaay. It has some issues with sometimes thinking a download
          is complete when there are certain types of data errors, so I built an error
          handling script around it to make up for its deficiencies.  It does 5 threads
          by default and has some level of data error handling and SHA checking.  As
          for speed, I think it''s negligible, if not slower that just using a browser
          download due to some Huggingface algorithm, I presume.  Nonetheless, the
          advantages of a downloader is for handling Huggingface folders with large
          numbers of files and the ability to resume downloads.'
        updatedAt: '2023-11-26T04:14:44.731Z'
      numEdits: 0
      reactions: []
    id: 6562c6342bdaccfcd5f6f15a
    type: comment
  author: JuLuComputing
  content: '> @JuLuComputing If downloading speed is an issue, have you considered
    pget? I think it uses multithreading or something by downloading different chunks
    of the model at once to speed things up. Seems a bit faster for HF models than
    wget, however I haven''t benchmarked it


    Thanks for the suggestion!  I''m going to check it out and see if how well it
    works.  I gave up on wget a long time ago.  Currently, I use a modified version
    of github/bodaay. It has some issues with sometimes thinking a download is complete
    when there are certain types of data errors, so I built an error handling script
    around it to make up for its deficiencies.  It does 5 threads by default and has
    some level of data error handling and SHA checking.  As for speed, I think it''s
    negligible, if not slower that just using a browser download due to some Huggingface
    algorithm, I presume.  Nonetheless, the advantages of a downloader is for handling
    Huggingface folders with large numbers of files and the ability to resume downloads.'
  created_at: 2023-11-26 04:14:44+00:00
  edited: false
  hidden: false
  id: 6562c6342bdaccfcd5f6f15a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-11-26T06:57:19.000Z'
    data:
      edited: true
      editors:
      - vmajor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9903982877731323
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>This would need to be fine tuned. It has no direct utility as it
          is, as far as I can understand.</p>

          <p>Thus "simply" running it will not achieve anything. It needs to be trained
          on further data in order for it to do anything beyond interesting.</p>

          <p>EDIT: actually no... it can do a lot as an encoder model. Can even use
          it as a super tokenizer of sorts. I wonder if that would help the causal
          language models perform better since they would be fed tokens generated
          by a presumably very capable MoE.</p>

          '
        raw: 'This would need to be fine tuned. It has no direct utility as it is,
          as far as I can understand.


          Thus "simply" running it will not achieve anything. It needs to be trained
          on further data in order for it to do anything beyond interesting.


          EDIT: actually no... it can do a lot as an encoder model. Can even use it
          as a super tokenizer of sorts. I wonder if that would help the causal language
          models perform better since they would be fed tokens generated by a presumably
          very capable MoE.'
        updatedAt: '2023-11-26T07:36:21.528Z'
      numEdits: 1
      reactions: []
    id: 6562ec4fec7e239899d888d9
    type: comment
  author: vmajor
  content: 'This would need to be fine tuned. It has no direct utility as it is, as
    far as I can understand.


    Thus "simply" running it will not achieve anything. It needs to be trained on
    further data in order for it to do anything beyond interesting.


    EDIT: actually no... it can do a lot as an encoder model. Can even use it as a
    super tokenizer of sorts. I wonder if that would help the causal language models
    perform better since they would be fed tokens generated by a presumably very capable
    MoE.'
  created_at: 2023-11-26 06:57:19+00:00
  edited: true
  hidden: false
  id: 6562ec4fec7e239899d888d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-11-26T07:49:13.000Z'
    data:
      edited: true
      editors:
      - vmajor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9825780391693115
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<blockquote>

          <blockquote>

          <p>How does anyone run this</p>

          </blockquote>

          <p>you would need 4 h100 gpus to run it, that''s what my math says</p>

          </blockquote>

          <p>the model is 3.1 TB in size. How does your math work that out, or am
          I wrong, or missing something? Even at INT8 the model would need approx
          780GB of VRAM to load and 4 x H100 NVL would supply 752 GB.</p>

          '
        raw: "> > How does anyone run this\n> \n> you would need 4 h100 gpus to run\
          \ it, that's what my math says\n\nthe model is 3.1 TB in size. How does\
          \ your math work that out, or am I wrong, or missing something? Even at\
          \ INT8 the model would need approx 780GB of VRAM to load and 4 x H100 NVL\
          \ would supply 752 GB."
        updatedAt: '2023-11-26T07:53:03.387Z'
      numEdits: 1
      reactions: []
    id: 6562f8791338610184bb9cd8
    type: comment
  author: vmajor
  content: "> > How does anyone run this\n> \n> you would need 4 h100 gpus to run\
    \ it, that's what my math says\n\nthe model is 3.1 TB in size. How does your math\
    \ work that out, or am I wrong, or missing something? Even at INT8 the model would\
    \ need approx 780GB of VRAM to load and 4 x H100 NVL would supply 752 GB."
  created_at: 2023-11-26 07:49:13+00:00
  edited: true
  hidden: false
  id: 6562f8791338610184bb9cd8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-11-26T20:39:32.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9693233966827393
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: "<blockquote>\n<p>EDIT: actually no... it can do a lot as an encoder\
          \ model. Can even use it as a super tokenizer of sorts. I wonder if that\
          \ would help the causal language models perform better since they would\
          \ be fed tokens generated by a presumably very capable MoE.</p>\n</blockquote>\n\
          <p>Congrats \U0001F389 you have discovered the encoder-decoder architecture\
          \ with a slight twist </p>\n"
        raw: "> EDIT: actually no... it can do a lot as an encoder model. Can even\
          \ use it as a super tokenizer of sorts. I wonder if that would help the\
          \ causal language models perform better since they would be fed tokens generated\
          \ by a presumably very capable MoE.\n\n\nCongrats \U0001F389 you have discovered\
          \ the encoder-decoder architecture with a slight twist "
        updatedAt: '2023-11-26T20:39:32.801Z'
      numEdits: 0
      reactions: []
    id: 6563ad047a465cdcb397e656
    type: comment
  author: pszemraj
  content: "> EDIT: actually no... it can do a lot as an encoder model. Can even use\
    \ it as a super tokenizer of sorts. I wonder if that would help the causal language\
    \ models perform better since they would be fed tokens generated by a presumably\
    \ very capable MoE.\n\n\nCongrats \U0001F389 you have discovered the encoder-decoder\
    \ architecture with a slight twist "
  created_at: 2023-11-26 20:39:32+00:00
  edited: false
  hidden: false
  id: 6563ad047a465cdcb397e656
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
      fullname: bread null
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: breadlicker45
      type: user
    createdAt: '2023-11-28T13:08:29.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
          fullname: bread null
          isHf: false
          isPro: false
          name: breadlicker45
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-11-28T13:18:56.243Z'
      numEdits: 1
      reactions: []
    id: 6565e64d995cc49553fe36fb
    type: comment
  author: breadlicker45
  content: This comment has been hidden
  created_at: 2023-11-28 13:08:29+00:00
  edited: true
  hidden: true
  id: 6565e64d995cc49553fe36fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
      fullname: bread null
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: breadlicker45
      type: user
    createdAt: '2023-11-28T13:11:27.000Z'
    data:
      edited: true
      editors:
      - breadlicker45
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9669317603111267
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
          fullname: bread null
          isHf: false
          isPro: false
          name: breadlicker45
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>How does anyone run this</p>

          </blockquote>

          <p>you would need 4 h100 gpus to run it, that''s what my math says</p>

          </blockquote>

          <p>the model is 3.1 TB in size. How does your math work that out, or am
          I wrong, or missing something? Even at INT8 the model would need approx
          780GB of VRAM to load and 4 x H100 NVL would supply 752 GB.</p>

          </blockquote>

          <p>Wait let me check my math again, you would need around 24800gb of vram
          assuming it has a max length of 2048 tokens. The math is (3100 * 4) * (2048/1024)</p>

          '
        raw: "> > > How does anyone run this\n> > \n> > you would need 4 h100 gpus\
          \ to run it, that's what my math says\n> \n> the model is 3.1 TB in size.\
          \ How does your math work that out, or am I wrong, or missing something?\
          \ Even at INT8 the model would need approx 780GB of VRAM to load and 4 x\
          \ H100 NVL would supply 752 GB.\n\nWait let me check my math again, you\
          \ would need around 24800gb of vram assuming it has a max length of 2048\
          \ tokens. The math is (3100 * 4) * (2048/1024)"
        updatedAt: '2023-11-28T13:19:12.030Z'
      numEdits: 6
      reactions: []
    id: 6565e6ff4a33bf3d440b38a6
    type: comment
  author: breadlicker45
  content: "> > > How does anyone run this\n> > \n> > you would need 4 h100 gpus to\
    \ run it, that's what my math says\n> \n> the model is 3.1 TB in size. How does\
    \ your math work that out, or am I wrong, or missing something? Even at INT8 the\
    \ model would need approx 780GB of VRAM to load and 4 x H100 NVL would supply\
    \ 752 GB.\n\nWait let me check my math again, you would need around 24800gb of\
    \ vram assuming it has a max length of 2048 tokens. The math is (3100 * 4) * (2048/1024)"
  created_at: 2023-11-28 13:11:27+00:00
  edited: true
  hidden: false
  id: 6565e6ff4a33bf3d440b38a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
      fullname: Jeremy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JuLuComputing
      type: user
    createdAt: '2023-11-28T16:30:02.000Z'
    data:
      edited: true
      editors:
      - JuLuComputing
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8560720086097717
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50f65f0537965a35090cefb31251ade6.svg
          fullname: Jeremy
          isHf: false
          isPro: false
          name: JuLuComputing
          type: user
        html: "<p>Alright, <span data-props=\"{&quot;user&quot;:&quot;ehartford&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ehartford\"\
          >@<span class=\"underline\">ehartford</span></a></span>\n\n\t</span></span>,\
          \ <span data-props=\"{&quot;user&quot;:&quot;pszemraj&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pszemraj\">@<span class=\"\
          underline\">pszemraj</span></a></span>\n\n\t</span></span>, <span data-props=\"\
          {&quot;user&quot;:&quot;breadlicker45&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/breadlicker45\">@<span class=\"underline\"\
          >breadlicker45</span></a></span>\n\n\t</span></span>, and anyone else reading\
          \ this, I have tested the smaller '256' version of this model and posted\
          \ a script I made for running it.  For comparison, the 256 model used 60GB\
          \ of ram while working.  I'm not sure I'll be able to load this big 2048\
          \ model on any machine I have without ram caching to disk.... a lot of caching\
          \ to disk....</p>\n<p>Check out the script, I hope this work is useful for\
          \ you guys!<br><a href=\"https://huggingface.co/google/switch-base-256/discussions/6#6566121866d5f87c6297fbeb\"\
          >https://huggingface.co/google/switch-base-256/discussions/6#6566121866d5f87c6297fbeb</a></p>\n\
          <p>My script reported that on a Dell R820, quad Xeon E5-4657L v2  CPUs,\
          \ 1TB DDR3 1866 ram, I got a consistent 2 tokens/sec.</p>\n"
        raw: 'Alright, @ehartford, @pszemraj, @breadlicker45, and anyone else reading
          this, I have tested the smaller ''256'' version of this model and posted
          a script I made for running it.  For comparison, the 256 model used 60GB
          of ram while working.  I''m not sure I''ll be able to load this big 2048
          model on any machine I have without ram caching to disk.... a lot of caching
          to disk....


          Check out the script, I hope this work is useful for you guys!

          https://huggingface.co/google/switch-base-256/discussions/6#6566121866d5f87c6297fbeb


          My script reported that on a Dell R820, quad Xeon E5-4657L v2  CPUs, 1TB
          DDR3 1866 ram, I got a consistent 2 tokens/sec.'
        updatedAt: '2023-11-28T16:57:15.971Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - susnato
    id: 6566158a4f6ec720173a13ab
    type: comment
  author: JuLuComputing
  content: 'Alright, @ehartford, @pszemraj, @breadlicker45, and anyone else reading
    this, I have tested the smaller ''256'' version of this model and posted a script
    I made for running it.  For comparison, the 256 model used 60GB of ram while working.  I''m
    not sure I''ll be able to load this big 2048 model on any machine I have without
    ram caching to disk.... a lot of caching to disk....


    Check out the script, I hope this work is useful for you guys!

    https://huggingface.co/google/switch-base-256/discussions/6#6566121866d5f87c6297fbeb


    My script reported that on a Dell R820, quad Xeon E5-4657L v2  CPUs, 1TB DDR3
    1866 ram, I got a consistent 2 tokens/sec.'
  created_at: 2023-11-28 16:30:02+00:00
  edited: true
  hidden: false
  id: 6566158a4f6ec720173a13ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
      fullname: bread null
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: breadlicker45
      type: user
    createdAt: '2023-11-29T11:17:13.000Z'
    data:
      edited: false
      editors:
      - breadlicker45
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8561954498291016
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
          fullname: bread null
          isHf: false
          isPro: false
          name: breadlicker45
          type: user
        html: "<blockquote>\n<p>Alright, <span data-props=\"{&quot;user&quot;:&quot;ehartford&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ehartford\"\
          >@<span class=\"underline\">ehartford</span></a></span>\n\n\t</span></span>,\
          \ <span data-props=\"{&quot;user&quot;:&quot;pszemraj&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pszemraj\">@<span class=\"\
          underline\">pszemraj</span></a></span>\n\n\t</span></span>, <span data-props=\"\
          {&quot;user&quot;:&quot;breadlicker45&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/breadlicker45\">@<span class=\"underline\"\
          >breadlicker45</span></a></span>\n\n\t</span></span>, and anyone else reading\
          \ this, I have tested the smaller '256' version of this model and posted\
          \ a script I made for running it.  For comparison, the 256 model used 60GB\
          \ of ram while working.  I'm not sure I'll be able to load this big 2048\
          \ model on any machine I have without ram caching to disk.... a lot of caching\
          \ to disk....</p>\n<p>Check out the script, I hope this work is useful for\
          \ you guys!<br><a href=\"https://huggingface.co/google/switch-base-256/discussions/6#6566121866d5f87c6297fbeb\"\
          >https://huggingface.co/google/switch-base-256/discussions/6#6566121866d5f87c6297fbeb</a></p>\n\
          <p>My script reported that on a Dell R820, quad Xeon E5-4657L v2  CPUs,\
          \ 1TB DDR3 1866 ram, I got a consistent 2 tokens/sec.</p>\n</blockquote>\n\
          <p>your going to need 24tb of ram to run switch-c-2048</p>\n"
        raw: "> Alright, @ehartford, @pszemraj, @breadlicker45, and anyone else reading\
          \ this, I have tested the smaller '256' version of this model and posted\
          \ a script I made for running it.  For comparison, the 256 model used 60GB\
          \ of ram while working.  I'm not sure I'll be able to load this big 2048\
          \ model on any machine I have without ram caching to disk.... a lot of caching\
          \ to disk....\n> \n> Check out the script, I hope this work is useful for\
          \ you guys!\n> https://huggingface.co/google/switch-base-256/discussions/6#6566121866d5f87c6297fbeb\n\
          > \n> My script reported that on a Dell R820, quad Xeon E5-4657L v2  CPUs,\
          \ 1TB DDR3 1866 ram, I got a consistent 2 tokens/sec.\n\nyour going to need\
          \ 24tb of ram to run switch-c-2048\n"
        updatedAt: '2023-11-29T11:17:13.557Z'
      numEdits: 0
      reactions: []
    id: 65671db950386bbde7464c72
    type: comment
  author: breadlicker45
  content: "> Alright, @ehartford, @pszemraj, @breadlicker45, and anyone else reading\
    \ this, I have tested the smaller '256' version of this model and posted a script\
    \ I made for running it.  For comparison, the 256 model used 60GB of ram while\
    \ working.  I'm not sure I'll be able to load this big 2048 model on any machine\
    \ I have without ram caching to disk.... a lot of caching to disk....\n> \n> Check\
    \ out the script, I hope this work is useful for you guys!\n> https://huggingface.co/google/switch-base-256/discussions/6#6566121866d5f87c6297fbeb\n\
    > \n> My script reported that on a Dell R820, quad Xeon E5-4657L v2  CPUs, 1TB\
    \ DDR3 1866 ram, I got a consistent 2 tokens/sec.\n\nyour going to need 24tb of\
    \ ram to run switch-c-2048\n"
  created_at: 2023-11-29 11:17:13+00:00
  edited: false
  hidden: false
  id: 65671db950386bbde7464c72
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: google/switch-c-2048
repo_type: model
status: open
target_branch: null
title: Thank you
