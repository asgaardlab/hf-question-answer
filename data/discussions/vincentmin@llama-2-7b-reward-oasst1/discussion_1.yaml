!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wlgq
conflicting_files: null
created_at: 2023-08-03 06:43:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/72b9ff786e4ff405c240a85bdedf4fb4.svg
      fullname: hei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wlgq
      type: user
    createdAt: '2023-08-03T07:43:28.000Z'
    data:
      edited: false
      editors:
      - wlgq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4486846923828125
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/72b9ff786e4ff405c240a85bdedf4fb4.svg
          fullname: hei
          isHf: false
          isPro: false
          name: wlgq
          type: user
        html: '<p>import torch<br>from peft import PeftModel, PeftConfig<br>from transformers
          import AutoModelForSequenceClassification, AutoTokenizer</p>

          <p>peft_model_id = "vincentmin/llama-2-7b-reward-oasst1"<br>config = PeftConfig.from_pretrained(peft_model_id)<br>model
          = AutoModelForSequenceClassification.from_pretrained("data/llama-2-7b-chat-hf",    num_labels=1,    load_in_4bit=True,  torch_dtype=torch.float16)<br>model
          = PeftModel.from_pretrained(model, peft_model_id)<br>tokenizer = AutoTokenizer.from_pretrained("data/llama-2-7b-chat-hf",
          use_auth_token=True)<br>model.eval()<br>with torch.no_grad():<br>  reward
          = model(**tokenizer("prompter: hello world. assistant: foo bar", return_tensors=''pt'')).logits</p>

          <p>when i run "reward = model(**tokenizer("prompter: hello world. assistant:
          foo bar", return_tensors=''pt'')).logits",<br>got a error : RuntimeError:
          mat1 and mat2 shapes cannot be multiplied (12x4096 and 1x8388608)</p>

          <p>I dont know which steps were wrong, could you help check it? thanks</p>

          <p>load model:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/648ad7b0ff2fb2428b26def5/c7N7IXmr9dK_avgYJhiJB.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/648ad7b0ff2fb2428b26def5/c7N7IXmr9dK_avgYJhiJB.png"></a></p>

          <p>model.eval()<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/648ad7b0ff2fb2428b26def5/7u2WWSy9ptUInOKV1yFti.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/648ad7b0ff2fb2428b26def5/7u2WWSy9ptUInOKV1yFti.png"></a></p>

          '
        raw: "import torch\r\nfrom peft import PeftModel, PeftConfig\r\nfrom transformers\
          \ import AutoModelForSequenceClassification, AutoTokenizer\r\n\r\npeft_model_id\
          \ = \"vincentmin/llama-2-7b-reward-oasst1\"\r\nconfig = PeftConfig.from_pretrained(peft_model_id)\r\
          \nmodel = AutoModelForSequenceClassification.from_pretrained(\"data/llama-2-7b-chat-hf\"\
          ,    num_labels=1,    load_in_4bit=True,  torch_dtype=torch.float16)\r\n\
          model = PeftModel.from_pretrained(model, peft_model_id)\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          data/llama-2-7b-chat-hf\", use_auth_token=True)\r\nmodel.eval()\r\nwith\
          \ torch.no_grad():\r\n  reward = model(**tokenizer(\"prompter: hello world.\
          \ assistant: foo bar\", return_tensors='pt')).logits\r\n\r\nwhen i run \"\
          reward = model(**tokenizer(\"prompter: hello world. assistant: foo bar\"\
          , return_tensors='pt')).logits\", \r\ngot a error : RuntimeError: mat1 and\
          \ mat2 shapes cannot be multiplied (12x4096 and 1x8388608)\r\n\r\nI dont\
          \ know which steps were wrong, could you help check it? thanks\r\n\r\nload\
          \ model:\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/648ad7b0ff2fb2428b26def5/c7N7IXmr9dK_avgYJhiJB.png)\r\
          \n\r\nmodel.eval()\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/648ad7b0ff2fb2428b26def5/7u2WWSy9ptUInOKV1yFti.png)\r\
          \n"
        updatedAt: '2023-08-03T07:43:28.736Z'
      numEdits: 0
      reactions: []
    id: 64cb5aa08a16b1748fffc182
    type: comment
  author: wlgq
  content: "import torch\r\nfrom peft import PeftModel, PeftConfig\r\nfrom transformers\
    \ import AutoModelForSequenceClassification, AutoTokenizer\r\n\r\npeft_model_id\
    \ = \"vincentmin/llama-2-7b-reward-oasst1\"\r\nconfig = PeftConfig.from_pretrained(peft_model_id)\r\
    \nmodel = AutoModelForSequenceClassification.from_pretrained(\"data/llama-2-7b-chat-hf\"\
    ,    num_labels=1,    load_in_4bit=True,  torch_dtype=torch.float16)\r\nmodel\
    \ = PeftModel.from_pretrained(model, peft_model_id)\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    data/llama-2-7b-chat-hf\", use_auth_token=True)\r\nmodel.eval()\r\nwith torch.no_grad():\r\
    \n  reward = model(**tokenizer(\"prompter: hello world. assistant: foo bar\",\
    \ return_tensors='pt')).logits\r\n\r\nwhen i run \"reward = model(**tokenizer(\"\
    prompter: hello world. assistant: foo bar\", return_tensors='pt')).logits\", \r\
    \ngot a error : RuntimeError: mat1 and mat2 shapes cannot be multiplied (12x4096\
    \ and 1x8388608)\r\n\r\nI dont know which steps were wrong, could you help check\
    \ it? thanks\r\n\r\nload model:\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/648ad7b0ff2fb2428b26def5/c7N7IXmr9dK_avgYJhiJB.png)\r\
    \n\r\nmodel.eval()\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/648ad7b0ff2fb2428b26def5/7u2WWSy9ptUInOKV1yFti.png)\r\
    \n"
  created_at: 2023-08-03 06:43:28+00:00
  edited: false
  hidden: false
  id: 64cb5aa08a16b1748fffc182
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64306c3b84f3ed1ce62c0e00/sc3JZ_cZW7pcF8fPIARFI.jpeg?w=200&h=200&f=face
      fullname: Vincent Min
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: vincentmin
      type: user
    createdAt: '2023-08-03T15:51:37.000Z'
    data:
      edited: false
      editors:
      - vincentmin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6296225190162659
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64306c3b84f3ed1ce62c0e00/sc3JZ_cZW7pcF8fPIARFI.jpeg?w=200&h=200&f=face
          fullname: Vincent Min
          isHf: false
          isPro: false
          name: vincentmin
          type: user
        html: "<p>I don't recognise the error. I just tried running</p>\n<pre><code>!pip\
          \ install -q transformers git+https://github.com/huggingface/peft.git accelerate\
          \ trl bitsandbytes einops`\n\nfrom huggingface_hub import notebook_login\n\
          notebook_login()\n\nimport torch\nfrom peft import PeftModel, PeftConfig\n\
          from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\
          \npeft_model_id = \"vincentmin/llama-2-7b-reward-oasst1\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\n\
          model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path,\
          \ num_labels=1, load_in_4bit=True, torch_dtype=torch.float16)\nmodel = PeftModel.from_pretrained(model,\
          \ peft_model_id)\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,\
          \ use_auth_token=True)\nmodel.eval()\nwith torch.no_grad():\n  reward =\
          \ model(**tokenizer(\"prompter: hello world. assistant: foo bar\", return_tensors='pt')).logits\n\
          </code></pre>\n<p>in a Google Colab and it ran without issues. Can you try\
          \ to install the latest version of these packages: <code>transformers git+https://github.com/huggingface/peft.git\
          \ accelerate trl bitsandbytes einops</code>?</p>\n<p>How did you download\
          \ your model to \"data/llama-2-7b-chat-hf\"?</p>\n<p>Note that it is important\
          \ to install <code>peft</code> from git as an important patch was made with\
          \ PR #755. This shouldn't affect your current issue though.</p>\n"
        raw: "I don't recognise the error. I just tried running\n```\n!pip install\
          \ -q transformers git+https://github.com/huggingface/peft.git accelerate\
          \ trl bitsandbytes einops`\n\nfrom huggingface_hub import notebook_login\n\
          notebook_login()\n\nimport torch\nfrom peft import PeftModel, PeftConfig\n\
          from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\
          \npeft_model_id = \"vincentmin/llama-2-7b-reward-oasst1\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\n\
          model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path,\
          \ num_labels=1, load_in_4bit=True, torch_dtype=torch.float16)\nmodel = PeftModel.from_pretrained(model,\
          \ peft_model_id)\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,\
          \ use_auth_token=True)\nmodel.eval()\nwith torch.no_grad():\n  reward =\
          \ model(**tokenizer(\"prompter: hello world. assistant: foo bar\", return_tensors='pt')).logits\n\
          ```\nin a Google Colab and it ran without issues. Can you try to install\
          \ the latest version of these packages: `transformers git+https://github.com/huggingface/peft.git\
          \ accelerate trl bitsandbytes einops`?\n\nHow did you download your model\
          \ to \"data/llama-2-7b-chat-hf\"?\n\nNote that it is important to install\
          \ `peft` from git as an important patch was made with PR #755. This shouldn't\
          \ affect your current issue though."
        updatedAt: '2023-08-03T15:51:37.829Z'
      numEdits: 0
      reactions: []
    id: 64cbcd09942890af935fc8cb
    type: comment
  author: vincentmin
  content: "I don't recognise the error. I just tried running\n```\n!pip install -q\
    \ transformers git+https://github.com/huggingface/peft.git accelerate trl bitsandbytes\
    \ einops`\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n\
    import torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import\
    \ AutoModelForSequenceClassification, AutoTokenizer\n\npeft_model_id = \"vincentmin/llama-2-7b-reward-oasst1\"\
    \nconfig = PeftConfig.from_pretrained(peft_model_id)\nmodel = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path,\
    \ num_labels=1, load_in_4bit=True, torch_dtype=torch.float16)\nmodel = PeftModel.from_pretrained(model,\
    \ peft_model_id)\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,\
    \ use_auth_token=True)\nmodel.eval()\nwith torch.no_grad():\n  reward = model(**tokenizer(\"\
    prompter: hello world. assistant: foo bar\", return_tensors='pt')).logits\n```\n\
    in a Google Colab and it ran without issues. Can you try to install the latest\
    \ version of these packages: `transformers git+https://github.com/huggingface/peft.git\
    \ accelerate trl bitsandbytes einops`?\n\nHow did you download your model to \"\
    data/llama-2-7b-chat-hf\"?\n\nNote that it is important to install `peft` from\
    \ git as an important patch was made with PR #755. This shouldn't affect your\
    \ current issue though."
  created_at: 2023-08-03 14:51:37+00:00
  edited: false
  hidden: false
  id: 64cbcd09942890af935fc8cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/72b9ff786e4ff405c240a85bdedf4fb4.svg
      fullname: hei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wlgq
      type: user
    createdAt: '2023-08-04T03:33:14.000Z'
    data:
      edited: false
      editors:
      - wlgq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.822363555431366
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/72b9ff786e4ff405c240a85bdedf4fb4.svg
          fullname: hei
          isHf: false
          isPro: false
          name: wlgq
          type: user
        html: "<p>thanks\uFF01<br>After installing the latest version of those packages,\
          \ it worked!</p>\n"
        raw: "thanks\uFF01\nAfter installing the latest version of those packages,\
          \ it worked!\n"
        updatedAt: '2023-08-04T03:33:14.745Z'
      numEdits: 0
      reactions: []
    id: 64cc717a71b435a75ddd5141
    type: comment
  author: wlgq
  content: "thanks\uFF01\nAfter installing the latest version of those packages, it\
    \ worked!\n"
  created_at: 2023-08-04 02:33:14+00:00
  edited: false
  hidden: false
  id: 64cc717a71b435a75ddd5141
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: vincentmin/llama-2-7b-reward-oasst1
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: mat1 and mat2 shapes cannot be multiplied (12x4096 and 1x8388608)'
