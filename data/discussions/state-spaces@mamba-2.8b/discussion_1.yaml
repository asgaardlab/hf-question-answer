!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Prakh24s
conflicting_files: null
created_at: 2023-12-05 09:44:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61d4ee18d9a5acebdb4f86eb/AM03YJbtAo2MVSSbHAIXz.png?w=200&h=200&f=face
      fullname: Prakhar Saxena
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Prakh24s
      type: user
    createdAt: '2023-12-05T09:44:41.000Z'
    data:
      edited: false
      editors:
      - Prakh24s
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9343820810317993
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61d4ee18d9a5acebdb4f86eb/AM03YJbtAo2MVSSbHAIXz.png?w=200&h=200&f=face
          fullname: Prakhar Saxena
          isHf: false
          isPro: false
          name: Prakh24s
          type: user
        html: '<p>Thank you for the amazing paper and model weights.</p>

          <p>The model seems to be twice the size compared transformer based model
          for the same size (~5.9 GB for 3b transformer model vs 11.1GB Mamba model).<br>Is
          is expected?</p>

          '
        raw: "Thank you for the amazing paper and model weights.\r\n\r\nThe model\
          \ seems to be twice the size compared transformer based model for the same\
          \ size (~5.9 GB for 3b transformer model vs 11.1GB Mamba model). \r\nIs\
          \ is expected?"
        updatedAt: '2023-12-05T09:44:41.681Z'
      numEdits: 0
      reactions: []
    id: 656ef109baa95d8b8cc2dd66
    type: comment
  author: Prakh24s
  content: "Thank you for the amazing paper and model weights.\r\n\r\nThe model seems\
    \ to be twice the size compared transformer based model for the same size (~5.9\
    \ GB for 3b transformer model vs 11.1GB Mamba model). \r\nIs is expected?"
  created_at: 2023-12-05 09:44:41+00:00
  edited: false
  hidden: false
  id: 656ef109baa95d8b8cc2dd66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0e5ec55ca725a5d4fff2937bfaa6640e.svg
      fullname: Mamba
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mamba-checkpoints
      type: user
    createdAt: '2023-12-05T09:53:27.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/0e5ec55ca725a5d4fff2937bfaa6640e.svg
          fullname: Mamba
          isHf: false
          isPro: false
          name: mamba-checkpoints
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-05T09:53:44.468Z'
      numEdits: 0
      reactions: []
    id: 656ef317a2076764c8e9274f
    type: comment
  author: mamba-checkpoints
  content: This comment has been hidden
  created_at: 2023-12-05 09:53:27+00:00
  edited: true
  hidden: true
  id: 656ef317a2076764c8e9274f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0815a74c9c962e6043e07d1af005d975.svg
      fullname: B
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: benxh
      type: user
    createdAt: '2023-12-05T11:36:14.000Z'
    data:
      edited: false
      editors:
      - benxh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8557453155517578
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0815a74c9c962e6043e07d1af005d975.svg
          fullname: B
          isHf: false
          isPro: false
          name: benxh
          type: user
        html: '<p>It''s a float32 model, hence the size difference. Transformers are
          usually float16 or bfloat16.</p>

          '
        raw: It's a float32 model, hence the size difference. Transformers are usually
          float16 or bfloat16.
        updatedAt: '2023-12-05T11:36:14.256Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Prakh24s
    id: 656f0b2ea2076764c8edeb89
    type: comment
  author: benxh
  content: It's a float32 model, hence the size difference. Transformers are usually
    float16 or bfloat16.
  created_at: 2023-12-05 11:36:14+00:00
  edited: false
  hidden: false
  id: 656f0b2ea2076764c8edeb89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61d4ee18d9a5acebdb4f86eb/AM03YJbtAo2MVSSbHAIXz.png?w=200&h=200&f=face
      fullname: Prakhar Saxena
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Prakh24s
      type: user
    createdAt: '2023-12-06T07:51:56.000Z'
    data:
      edited: false
      editors:
      - Prakh24s
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8543881177902222
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61d4ee18d9a5acebdb4f86eb/AM03YJbtAo2MVSSbHAIXz.png?w=200&h=200&f=face
          fullname: Prakhar Saxena
          isHf: false
          isPro: false
          name: Prakh24s
          type: user
        html: '<p>Thank you for the answer!<br>Very excited for bigger/quantized models!</p>

          '
        raw: 'Thank you for the answer!

          Very excited for bigger/quantized models!'
        updatedAt: '2023-12-06T07:51:56.198Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6570281cec366e93ca3d67f1
    id: 6570281cec366e93ca3d67f0
    type: comment
  author: Prakh24s
  content: 'Thank you for the answer!

    Very excited for bigger/quantized models!'
  created_at: 2023-12-06 07:51:56+00:00
  edited: false
  hidden: false
  id: 6570281cec366e93ca3d67f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61d4ee18d9a5acebdb4f86eb/AM03YJbtAo2MVSSbHAIXz.png?w=200&h=200&f=face
      fullname: Prakhar Saxena
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Prakh24s
      type: user
    createdAt: '2023-12-06T07:51:56.000Z'
    data:
      status: closed
    id: 6570281cec366e93ca3d67f1
    type: status-change
  author: Prakh24s
  created_at: 2023-12-06 07:51:56+00:00
  id: 6570281cec366e93ca3d67f1
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: state-spaces/mamba-2.8b
repo_type: model
status: closed
target_branch: null
title: Regarding the Model size
