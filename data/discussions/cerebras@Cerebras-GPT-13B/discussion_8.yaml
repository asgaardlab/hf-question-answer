!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nikodevv
conflicting_files: null
created_at: 2023-04-16 04:43:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/363116f55cb2f1581b97a20b659dc14a.svg
      fullname: niko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nikodevv
      type: user
    createdAt: '2023-04-16T05:43:46.000Z'
    data:
      edited: false
      editors:
      - nikodevv
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/363116f55cb2f1581b97a20b659dc14a.svg
          fullname: niko
          isHf: false
          isPro: false
          name: nikodevv
          type: user
        html: "<p>Sagemaker based deployment will fail with the following error:</p>\n\
          <pre><code>Will error out with:\n\n---------------------------------------------------------------------------\n\
          ModelError                                Traceback (most recent call last)\n\
          &lt;ipython-input-4-e72b8b1a6621&gt; in &lt;module&gt;\n     25 \n     26\
          \ predictor.predict({\n---&gt; 27         'inputs': \"Can you please let\
          \ us know more details about your \"\n     28 })\n\n/opt/conda/lib/python3.7/site-packages/sagemaker/predictor.py\
          \ in predict(self, data, initial_args, target_model, target_variant, inference_id)\n\
          \    159             data, initial_args, target_model, target_variant, inference_id\n\
          \    160         )\n--&gt; 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n\
          \    162         return self._handle_response(response)\n    163 \n\n/opt/conda/lib/python3.7/site-packages/botocore/client.py\
          \ in _api_call(self, *args, **kwargs)\n    528                 )\n    529\
          \             # The \"self\" in this scope is referring to the BaseClient.\n\
          --&gt; 530             return self._make_api_call(operation_name, kwargs)\n\
          \    531 \n    532         _api_call.__name__ = str(py_operation_name)\n\
          \n/opt/conda/lib/python3.7/site-packages/botocore/client.py in _make_api_call(self,\
          \ operation_name, api_params)\n    958             error_code = parsed_response.get(\"\
          Error\", {}).get(\"Code\")\n    959             error_class = self.exceptions.from_code(error_code)\n\
          --&gt; 960             raise error_class(parsed_response, operation_name)\n\
          \    961         else:\n    962             return parsed_response\n\nModelError:\
          \ An error occurred (ModelError) when calling the InvokeEndpoint operation:\
          \ Received client error (400) from primary with message \"{\n  \"code\"\
          : 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Could\
          \ not load model /.sagemaker/mms/models/cerebras__Cerebras-GPT-13B with\
          \ any of the following classes: (\\u003cclass \\u0027transformers.models.auto.modeling_auto.AutoModelForCausalLM\\\
          u0027\\u003e, \\u003cclass \\u0027transformers.models.gpt2.modeling_gpt2.GPT2Model\\\
          u0027\\u003e).\"\n}\n\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-04-16-04-42-22-119\
          \ in account XXX for more information.\n</code></pre>\n<pre><code>\n\nInput:\n\
          </code></pre>\n<p>from sagemaker.huggingface import HuggingFaceModel<br>import\
          \ sagemaker</p>\n<p>role = sagemaker.get_execution_role()</p>\n<h1 id=\"\
          hub-model-configuration-httpshuggingfacecomodels\">Hub Model configuration.\
          \ <a href=\"https://huggingface.co/models\">https://huggingface.co/models</a></h1>\n\
          <p>hub = {<br>    'HF_MODEL_ID':'cerebras/Cerebras-GPT-13B',<br>    'HF_TASK':'text-generation'<br>}</p>\n\
          <h1 id=\"create-hugging-face-model-class\">create Hugging Face Model Class</h1>\n\
          <p>huggingface_model = HuggingFaceModel(<br>    transformers_version='4.17.0',<br>\
          \    pytorch_version='1.10.2',<br>    py_version='py38',<br>    env=hub,<br>\
          \    role=role,<br>)</p>\n<h1 id=\"deploy-model-to-sagemaker-inference\"\
          >deploy model to SageMaker Inference</h1>\n<p>predictor = huggingface_model.deploy(<br>\
          \    initial_instance_count=1, # number of instances<br>    instance_type='ml.m5.xlarge'\
          \ # ec2 instance type<br>)</p>\n<p>predictor.predict({<br>    'inputs':\
          \ \"Can you please let us know more details about your \"<br>})<br>```</p>\n"
        raw: "Sagemaker based deployment will fail with the following error:\r\n\r\
          \n```\r\nWill error out with:\r\n\r\n---------------------------------------------------------------------------\r\
          \nModelError                                Traceback (most recent call\
          \ last)\r\n<ipython-input-4-e72b8b1a6621> in <module>\r\n     25 \r\n  \
          \   26 predictor.predict({\r\n---> 27         'inputs': \"Can you please\
          \ let us know more details about your \"\r\n     28 })\r\n\r\n/opt/conda/lib/python3.7/site-packages/sagemaker/predictor.py\
          \ in predict(self, data, initial_args, target_model, target_variant, inference_id)\r\
          \n    159             data, initial_args, target_model, target_variant,\
          \ inference_id\r\n    160         )\r\n--> 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\r\
          \n    162         return self._handle_response(response)\r\n    163 \r\n\
          \r\n/opt/conda/lib/python3.7/site-packages/botocore/client.py in _api_call(self,\
          \ *args, **kwargs)\r\n    528                 )\r\n    529             #\
          \ The \"self\" in this scope is referring to the BaseClient.\r\n--> 530\
          \             return self._make_api_call(operation_name, kwargs)\r\n   \
          \ 531 \r\n    532         _api_call.__name__ = str(py_operation_name)\r\n\
          \r\n/opt/conda/lib/python3.7/site-packages/botocore/client.py in _make_api_call(self,\
          \ operation_name, api_params)\r\n    958             error_code = parsed_response.get(\"\
          Error\", {}).get(\"Code\")\r\n    959             error_class = self.exceptions.from_code(error_code)\r\
          \n--> 960             raise error_class(parsed_response, operation_name)\r\
          \n    961         else:\r\n    962             return parsed_response\r\n\
          \r\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint\
          \ operation: Received client error (400) from primary with message \"{\r\
          \n  \"code\": 400,\r\n  \"type\": \"InternalServerException\",\r\n  \"message\"\
          : \"Could not load model /.sagemaker/mms/models/cerebras__Cerebras-GPT-13B\
          \ with any of the following classes: (\\u003cclass \\u0027transformers.models.auto.modeling_auto.AutoModelForCausalLM\\\
          u0027\\u003e, \\u003cclass \\u0027transformers.models.gpt2.modeling_gpt2.GPT2Model\\\
          u0027\\u003e).\"\r\n}\r\n\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-04-16-04-42-22-119\
          \ in account XXX for more information.\r\n```\r\n```\r\n\r\n\r\nInput:\r\
          \n\r\n```\r\nfrom sagemaker.huggingface import HuggingFaceModel\r\nimport\
          \ sagemaker\r\n\r\nrole = sagemaker.get_execution_role()\r\n# Hub Model\
          \ configuration. https://huggingface.co/models\r\nhub = {\r\n\t'HF_MODEL_ID':'cerebras/Cerebras-GPT-13B',\r\
          \n\t'HF_TASK':'text-generation'\r\n}\r\n\r\n# create Hugging Face Model\
          \ Class\r\nhuggingface_model = HuggingFaceModel(\r\n\ttransformers_version='4.17.0',\r\
          \n\tpytorch_version='1.10.2',\r\n\tpy_version='py38',\r\n\tenv=hub,\r\n\t\
          role=role, \r\n)\r\n\r\n# deploy model to SageMaker Inference\r\npredictor\
          \ = huggingface_model.deploy(\r\n\tinitial_instance_count=1, # number of\
          \ instances\r\n\tinstance_type='ml.m5.xlarge' # ec2 instance type\r\n)\r\
          \n\r\npredictor.predict({\r\n\t'inputs': \"Can you please let us know more\
          \ details about your \"\r\n})\r\n```"
        updatedAt: '2023-04-16T05:43:46.974Z'
      numEdits: 0
      reactions: []
    id: 643b8b12065961b2252bdfde
    type: comment
  author: nikodevv
  content: "Sagemaker based deployment will fail with the following error:\r\n\r\n\
    ```\r\nWill error out with:\r\n\r\n---------------------------------------------------------------------------\r\
    \nModelError                                Traceback (most recent call last)\r\
    \n<ipython-input-4-e72b8b1a6621> in <module>\r\n     25 \r\n     26 predictor.predict({\r\
    \n---> 27         'inputs': \"Can you please let us know more details about your\
    \ \"\r\n     28 })\r\n\r\n/opt/conda/lib/python3.7/site-packages/sagemaker/predictor.py\
    \ in predict(self, data, initial_args, target_model, target_variant, inference_id)\r\
    \n    159             data, initial_args, target_model, target_variant, inference_id\r\
    \n    160         )\r\n--> 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\r\
    \n    162         return self._handle_response(response)\r\n    163 \r\n\r\n/opt/conda/lib/python3.7/site-packages/botocore/client.py\
    \ in _api_call(self, *args, **kwargs)\r\n    528                 )\r\n    529\
    \             # The \"self\" in this scope is referring to the BaseClient.\r\n\
    --> 530             return self._make_api_call(operation_name, kwargs)\r\n   \
    \ 531 \r\n    532         _api_call.__name__ = str(py_operation_name)\r\n\r\n\
    /opt/conda/lib/python3.7/site-packages/botocore/client.py in _make_api_call(self,\
    \ operation_name, api_params)\r\n    958             error_code = parsed_response.get(\"\
    Error\", {}).get(\"Code\")\r\n    959             error_class = self.exceptions.from_code(error_code)\r\
    \n--> 960             raise error_class(parsed_response, operation_name)\r\n \
    \   961         else:\r\n    962             return parsed_response\r\n\r\nModelError:\
    \ An error occurred (ModelError) when calling the InvokeEndpoint operation: Received\
    \ client error (400) from primary with message \"{\r\n  \"code\": 400,\r\n  \"\
    type\": \"InternalServerException\",\r\n  \"message\": \"Could not load model\
    \ /.sagemaker/mms/models/cerebras__Cerebras-GPT-13B with any of the following\
    \ classes: (\\u003cclass \\u0027transformers.models.auto.modeling_auto.AutoModelForCausalLM\\\
    u0027\\u003e, \\u003cclass \\u0027transformers.models.gpt2.modeling_gpt2.GPT2Model\\\
    u0027\\u003e).\"\r\n}\r\n\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-04-16-04-42-22-119\
    \ in account XXX for more information.\r\n```\r\n```\r\n\r\n\r\nInput:\r\n\r\n\
    ```\r\nfrom sagemaker.huggingface import HuggingFaceModel\r\nimport sagemaker\r\
    \n\r\nrole = sagemaker.get_execution_role()\r\n# Hub Model configuration. https://huggingface.co/models\r\
    \nhub = {\r\n\t'HF_MODEL_ID':'cerebras/Cerebras-GPT-13B',\r\n\t'HF_TASK':'text-generation'\r\
    \n}\r\n\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\
    \n\ttransformers_version='4.17.0',\r\n\tpytorch_version='1.10.2',\r\n\tpy_version='py38',\r\
    \n\tenv=hub,\r\n\trole=role, \r\n)\r\n\r\n# deploy model to SageMaker Inference\r\
    \npredictor = huggingface_model.deploy(\r\n\tinitial_instance_count=1, # number\
    \ of instances\r\n\tinstance_type='ml.m5.xlarge' # ec2 instance type\r\n)\r\n\r\
    \npredictor.predict({\r\n\t'inputs': \"Can you please let us know more details\
    \ about your \"\r\n})\r\n```"
  created_at: 2023-04-16 04:43:46+00:00
  edited: false
  hidden: false
  id: 643b8b12065961b2252bdfde
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/34ff435e90c3846663c4ade5a4802c25.svg
      fullname: Paul Lam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: quantisan
      type: user
    createdAt: '2023-04-16T06:28:40.000Z'
    data:
      edited: false
      editors:
      - quantisan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/34ff435e90c3846663c4ade5a4802c25.svg
          fullname: Paul Lam
          isHf: false
          isPro: false
          name: quantisan
          type: user
        html: '<p>what does your log at <a rel="nofollow" href="https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-04-16-04-42-22-119">https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-04-16-04-42-22-119</a>
          say?</p>

          '
        raw: what does your log at https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-04-16-04-42-22-119
          say?
        updatedAt: '2023-04-16T06:28:40.813Z'
      numEdits: 0
      reactions: []
    id: 643b95982d67f0ed22f53487
    type: comment
  author: quantisan
  content: what does your log at https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-04-16-04-42-22-119
    say?
  created_at: 2023-04-16 05:28:40+00:00
  edited: false
  hidden: false
  id: 643b95982d67f0ed22f53487
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: cerebras/Cerebras-GPT-13B
repo_type: model
status: open
target_branch: null
title: Deployment instructions for sagemaker do not work
