!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hiarcs
conflicting_files: null
created_at: 2023-04-03 15:23:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/87d0e28e9160ecb0683d812a68545e54.svg
      fullname: Yang, Fei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiarcs
      type: user
    createdAt: '2023-04-03T16:23:07.000Z'
    data:
      edited: false
      editors:
      - hiarcs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/87d0e28e9160ecb0683d812a68545e54.svg
          fullname: Yang, Fei
          isHf: false
          isPro: false
          name: hiarcs
          type: user
        html: '<p>When I tried to load the model, I received an error:<br>404 Client
          Error: Entry Not Found for url: <a href="https://huggingface.co/cerebras/Cerebras-GPT-13B/resolve/main/pytorch_model.bin">https://huggingface.co/cerebras/Cerebras-GPT-13B/resolve/main/pytorch_model.bin</a><br>I
          then downloaded all files to local folder and tried to load the model from
          local files, but got another OSError:<br>OSError: Error no file named pytorch_model.bin
          found in directory GPT-13B but there is a file for Flax weights. Use <code>from_flax=True</code>
          to load this model from those weights.</p>

          <p>I have already tested both way on 111M, and they both work, so I assume
          my environment is ok.<br>I copied the code directly from the mdoel card
          and made no modifications.</p>

          <p>Is there anything else I should try?</p>

          '
        raw: "When I tried to load the model, I received an error:\r\n404 Client Error:\
          \ Entry Not Found for url: https://huggingface.co/cerebras/Cerebras-GPT-13B/resolve/main/pytorch_model.bin\r\
          \nI then downloaded all files to local folder and tried to load the model\
          \ from local files, but got another OSError:\r\nOSError: Error no file named\
          \ pytorch_model.bin found in directory GPT-13B but there is a file for Flax\
          \ weights. Use `from_flax=True` to load this model from those weights.\r\
          \n\r\nI have already tested both way on 111M, and they both work, so I assume\
          \ my environment is ok.\r\nI copied the code directly from the mdoel card\
          \ and made no modifications.\r\n\r\nIs there anything else I should try?\r\
          \n"
        updatedAt: '2023-04-03T16:23:07.531Z'
      numEdits: 0
      reactions: []
    id: 642afd6ba096201096ee353f
    type: comment
  author: hiarcs
  content: "When I tried to load the model, I received an error:\r\n404 Client Error:\
    \ Entry Not Found for url: https://huggingface.co/cerebras/Cerebras-GPT-13B/resolve/main/pytorch_model.bin\r\
    \nI then downloaded all files to local folder and tried to load the model from\
    \ local files, but got another OSError:\r\nOSError: Error no file named pytorch_model.bin\
    \ found in directory GPT-13B but there is a file for Flax weights. Use `from_flax=True`\
    \ to load this model from those weights.\r\n\r\nI have already tested both way\
    \ on 111M, and they both work, so I assume my environment is ok.\r\nI copied the\
    \ code directly from the mdoel card and made no modifications.\r\n\r\nIs there\
    \ anything else I should try?\r\n"
  created_at: 2023-04-03 15:23:07+00:00
  edited: false
  hidden: false
  id: 642afd6ba096201096ee353f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60c908f5d3f493296bae29fb/vlczac90Je1dd826vObeS.jpeg?w=200&h=200&f=face
      fullname: Richard Kuzma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rskuzma
      type: user
    createdAt: '2023-04-03T22:21:35.000Z'
    data:
      edited: false
      editors:
      - rskuzma
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60c908f5d3f493296bae29fb/vlczac90Je1dd826vObeS.jpeg?w=200&h=200&f=face
          fullname: Richard Kuzma
          isHf: false
          isPro: false
          name: rskuzma
          type: user
        html: "<p>Hi, thanks for sharing this issue.</p>\n<p>I believe this model\
          \ is sharded by the <code>save_pretrained()</code> method into two <code>pytorch_model_0000#-of-0000#+1.bin</code>\
          \ files, so you're correct\u2014there is no <a href=\"https://huggingface.co/cerebras/Cerebras-GPT-13B/resolve/main/pytorch_model.bin\"\
          >https://huggingface.co/cerebras/Cerebras-GPT-13B/resolve/main/pytorch_model.bin</a>\
          \ as there is with smaller models. You may need to load both models and\
          \ save as one <code>.bin</code> file.</p>\n<p>Passing the directory name\
          \ to the <code>load_pretrained()</code> function may solve the loading issue\
          \ with multiple checkpoint shards. Could you check out <a href=\"https://huggingface.co/docs/transformers/big_models\"\
          >https://huggingface.co/docs/transformers/big_models</a> and see if it is\
          \ useful?</p>\n"
        raw: "Hi, thanks for sharing this issue.\n\nI believe this model is sharded\
          \ by the `save_pretrained()` method into two `pytorch_model_0000#-of-0000#+1.bin`\
          \ files, so you're correct\u2014there is no https://huggingface.co/cerebras/Cerebras-GPT-13B/resolve/main/pytorch_model.bin\
          \ as there is with smaller models. You may need to load both models and\
          \ save as one `.bin` file.\n\nPassing the directory name to the `load_pretrained()`\
          \ function may solve the loading issue with multiple checkpoint shards.\
          \ Could you check out https://huggingface.co/docs/transformers/big_models\
          \ and see if it is useful?"
        updatedAt: '2023-04-03T22:21:35.079Z'
      numEdits: 0
      reactions: []
    id: 642b516f74600bc5d4770dd9
    type: comment
  author: rskuzma
  content: "Hi, thanks for sharing this issue.\n\nI believe this model is sharded\
    \ by the `save_pretrained()` method into two `pytorch_model_0000#-of-0000#+1.bin`\
    \ files, so you're correct\u2014there is no https://huggingface.co/cerebras/Cerebras-GPT-13B/resolve/main/pytorch_model.bin\
    \ as there is with smaller models. You may need to load both models and save as\
    \ one `.bin` file.\n\nPassing the directory name to the `load_pretrained()` function\
    \ may solve the loading issue with multiple checkpoint shards. Could you check\
    \ out https://huggingface.co/docs/transformers/big_models and see if it is useful?"
  created_at: 2023-04-03 21:21:35+00:00
  edited: false
  hidden: false
  id: 642b516f74600bc5d4770dd9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/87d0e28e9160ecb0683d812a68545e54.svg
      fullname: Yang, Fei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiarcs
      type: user
    createdAt: '2023-04-04T04:25:19.000Z'
    data:
      edited: false
      editors:
      - hiarcs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/87d0e28e9160ecb0683d812a68545e54.svg
          fullname: Yang, Fei
          isHf: false
          isPro: false
          name: hiarcs
          type: user
        html: '<p>Firstly, I''ve already downloaded both models and other files as
          well. I passed the local folder name to load_pretrained(), and I received
          an OSError, as I mentioned before. This way works for 111M, as well as in
          other multi model-files cases.<br>Sencondly, whether using a single model
          file or multi model files, the way to use load_pretrained() is the same,
          and the code in the model card should work regardless. There is no reason
          for a 404 client error to occur.</p>

          '
        raw: 'Firstly, I''ve already downloaded both models and other files as well.
          I passed the local folder name to load_pretrained(), and I received an OSError,
          as I mentioned before. This way works for 111M, as well as in other multi
          model-files cases.

          Sencondly, whether using a single model file or multi model files, the way
          to use load_pretrained() is the same, and the code in the model card should
          work regardless. There is no reason for a 404 client error to occur.'
        updatedAt: '2023-04-04T04:25:19.553Z'
      numEdits: 0
      reactions: []
    id: 642ba6aff8692638b03bd7f1
    type: comment
  author: hiarcs
  content: 'Firstly, I''ve already downloaded both models and other files as well.
    I passed the local folder name to load_pretrained(), and I received an OSError,
    as I mentioned before. This way works for 111M, as well as in other multi model-files
    cases.

    Sencondly, whether using a single model file or multi model files, the way to
    use load_pretrained() is the same, and the code in the model card should work
    regardless. There is no reason for a 404 client error to occur.'
  created_at: 2023-04-04 03:25:19+00:00
  edited: false
  hidden: false
  id: 642ba6aff8692638b03bd7f1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: cerebras/Cerebras-GPT-13B
repo_type: model
status: open
target_branch: null
title: '"Entry Not Found" for online loading, and OSError for offline loading'
