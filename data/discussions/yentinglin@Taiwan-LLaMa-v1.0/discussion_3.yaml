!!python/object:huggingface_hub.community.DiscussionWithDetails
author: johannchu
conflicting_files: null
created_at: 2023-09-09 03:33:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb9f19bebb94a86bc96c728fdcc7b51a.svg
      fullname: Johann Chu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johannchu
      type: user
    createdAt: '2023-09-09T04:33:12.000Z'
    data:
      edited: false
      editors:
      - johannchu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9399806261062622
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb9f19bebb94a86bc96c728fdcc7b51a.svg
          fullname: Johann Chu
          isHf: false
          isPro: false
          name: johannchu
          type: user
        html: '<p>Hi all, I had successfully run inference with TaiwanLLaMa by loading
          the model directly on Jupyter notebook. Nevertheless, when I tried to host
          the model as an API with Gradio (on NVIDIA A100 GPU * 4), the status wheel
          would just keep spinning and never provide any output. Since the same code
          worked with Llama-2-7b-chat-hf, I wonder if there''s any architectural change
          in TaiwanLLaMa that prohibits us from using the same code to get the output.
          Thanks!</p>

          '
        raw: Hi all, I had successfully run inference with TaiwanLLaMa by loading
          the model directly on Jupyter notebook. Nevertheless, when I tried to host
          the model as an API with Gradio (on NVIDIA A100 GPU * 4), the status wheel
          would just keep spinning and never provide any output. Since the same code
          worked with Llama-2-7b-chat-hf, I wonder if there's any architectural change
          in TaiwanLLaMa that prohibits us from using the same code to get the output.
          Thanks!
        updatedAt: '2023-09-09T04:33:12.878Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - maggie-lee
    id: 64fbf58814636d417ab70b52
    type: comment
  author: johannchu
  content: Hi all, I had successfully run inference with TaiwanLLaMa by loading the
    model directly on Jupyter notebook. Nevertheless, when I tried to host the model
    as an API with Gradio (on NVIDIA A100 GPU * 4), the status wheel would just keep
    spinning and never provide any output. Since the same code worked with Llama-2-7b-chat-hf,
    I wonder if there's any architectural change in TaiwanLLaMa that prohibits us
    from using the same code to get the output. Thanks!
  created_at: 2023-09-09 03:33:12+00:00
  edited: false
  hidden: false
  id: 64fbf58814636d417ab70b52
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c369aeb1af9de197fe7652ef73ff228.svg
      fullname: LEE CHIEH YU
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maggie-lee
      type: user
    createdAt: '2023-09-13T06:45:25.000Z'
    data:
      edited: false
      editors:
      - maggie-lee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9745980501174927
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c369aeb1af9de197fe7652ef73ff228.svg
          fullname: LEE CHIEH YU
          isHf: false
          isPro: false
          name: maggie-lee
          type: user
        html: '<p>I have same question, Thanks!</p>

          '
        raw: I have same question, Thanks!
        updatedAt: '2023-09-13T06:45:25.733Z'
      numEdits: 0
      reactions: []
    id: 65015a8513f1546526cd0766
    type: comment
  author: maggie-lee
  content: I have same question, Thanks!
  created_at: 2023-09-13 05:45:25+00:00
  edited: false
  hidden: false
  id: 65015a8513f1546526cd0766
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5df9c78eda6d0311fd3d541f/K8m3JEmIhH8WfwrgW-3l8.jpeg?w=200&h=200&f=face
      fullname: Yen-Ting Lin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: yentinglin
      type: user
    createdAt: '2023-09-28T16:18:19.000Z'
    data:
      edited: false
      editors:
      - yentinglin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8599473237991333
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5df9c78eda6d0311fd3d541f/K8m3JEmIhH8WfwrgW-3l8.jpeg?w=200&h=200&f=face
          fullname: Yen-Ting Lin
          isHf: false
          isPro: false
          name: yentinglin
          type: user
        html: '<p>No architectural change in TW LLM. Could you provide the gradio
          scripts?</p>

          '
        raw: No architectural change in TW LLM. Could you provide the gradio scripts?
        updatedAt: '2023-09-28T16:18:19.017Z'
      numEdits: 0
      reactions: []
    id: 6515a74b5b041d575d0e53c3
    type: comment
  author: yentinglin
  content: No architectural change in TW LLM. Could you provide the gradio scripts?
  created_at: 2023-09-28 15:18:19+00:00
  edited: false
  hidden: false
  id: 6515a74b5b041d575d0e53c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb9f19bebb94a86bc96c728fdcc7b51a.svg
      fullname: Johann Chu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johannchu
      type: user
    createdAt: '2023-10-18T04:02:37.000Z'
    data:
      edited: false
      editors:
      - johannchu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8410285115242004
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb9f19bebb94a86bc96c728fdcc7b51a.svg
          fullname: Johann Chu
          isHf: false
          isPro: false
          name: johannchu
          type: user
        html: '<p>The script is listed here:<br><a rel="nofollow" href="https://github.com/johannchu/taiwan-llama-on-gradio/blob/main/demo.py">https://github.com/johannchu/taiwan-llama-on-gradio/blob/main/demo.py</a></p>

          <p>To avoid downloading the model every time, we save the model checkpoint
          and load it locally. For the sake of bandwidth, we did not upload the model
          to the repo.<br>Also, to make sure there are no mistake in Gradio app itself,
          we added a "get_random_asnwer()" function in the script for testing. When
          we switched to using "get_random_answer()" under "respond()", the web app
          can display chat content normally. Nevertheless, when we switched to "get_reply_from_llm()"
          (i.e. the function running LlaMa inference), the web app would just keep
          spinning and never respond.</p>

          <p>What''s interesting is that if we use "get_reply_from_llm()" in a script
          with prompt hardcoded in (instead of hosting the whole thing as an app),
          the script can function normally, albeit the longer waiting time.</p>

          '
        raw: "The script is listed here: \nhttps://github.com/johannchu/taiwan-llama-on-gradio/blob/main/demo.py\n\
          \nTo avoid downloading the model every time, we save the model checkpoint\
          \ and load it locally. For the sake of bandwidth, we did not upload the\
          \ model to the repo. \nAlso, to make sure there are no mistake in Gradio\
          \ app itself, we added a \"get_random_asnwer()\" function in the script\
          \ for testing. When we switched to using \"get_random_answer()\" under \"\
          respond()\", the web app can display chat content normally. Nevertheless,\
          \ when we switched to \"get_reply_from_llm()\" (i.e. the function running\
          \ LlaMa inference), the web app would just keep spinning and never respond.\n\
          \nWhat's interesting is that if we use \"get_reply_from_llm()\" in a script\
          \ with prompt hardcoded in (instead of hosting the whole thing as an app),\
          \ the script can function normally, albeit the longer waiting time."
        updatedAt: '2023-10-18T04:02:37.794Z'
      numEdits: 0
      reactions: []
    id: 652f58dd329f988fe2eb340e
    type: comment
  author: johannchu
  content: "The script is listed here: \nhttps://github.com/johannchu/taiwan-llama-on-gradio/blob/main/demo.py\n\
    \nTo avoid downloading the model every time, we save the model checkpoint and\
    \ load it locally. For the sake of bandwidth, we did not upload the model to the\
    \ repo. \nAlso, to make sure there are no mistake in Gradio app itself, we added\
    \ a \"get_random_asnwer()\" function in the script for testing. When we switched\
    \ to using \"get_random_answer()\" under \"respond()\", the web app can display\
    \ chat content normally. Nevertheless, when we switched to \"get_reply_from_llm()\"\
    \ (i.e. the function running LlaMa inference), the web app would just keep spinning\
    \ and never respond.\n\nWhat's interesting is that if we use \"get_reply_from_llm()\"\
    \ in a script with prompt hardcoded in (instead of hosting the whole thing as\
    \ an app), the script can function normally, albeit the longer waiting time."
  created_at: 2023-10-18 03:02:37+00:00
  edited: false
  hidden: false
  id: 652f58dd329f988fe2eb340e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: yentinglin/Taiwan-LLaMa-v1.0
repo_type: model
status: open
target_branch: null
title: Failure to run inference when running on Gradio
