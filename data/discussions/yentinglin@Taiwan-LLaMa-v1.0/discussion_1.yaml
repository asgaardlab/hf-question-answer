!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nps798
conflicting_files: null
created_at: 2023-08-18 15:32:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6550b60a4863272fef6c45c3602d9269.svg
      fullname: YuJuLin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nps798
      type: user
    createdAt: '2023-08-18T16:32:59.000Z'
    data:
      edited: true
      editors:
      - nps798
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.975400984287262
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6550b60a4863272fef6c45c3602d9269.svg
          fullname: YuJuLin
          isHf: false
          isPro: false
          name: nps798
          type: user
        html: '<p>First, I''d like to thank you and your team for developing such
          a powerful model that has greatly contributed to the Traditional Chinese
          LLM community.</p>

          <p>Not really a question about bugs or errors. I''m just newbie in this
          field. but.... I''ve heard that LLAMA-2 from Meta was predominantly trained
          on English corpus. Consequently, the vanilla Meta LLAMA-2 struggles with
          Chinese reasoning and responses. It would be very grateful if you could
          share some of your experience or tips for training the English-based LLM
          to learn Chinese?<br>And, if it''s like going through the pre-training again.
          What''s the advantage of using LLAMA2 rather than previous published model
          like Falcon or MPT ?</p>

          <p>Thank you !</p>

          '
        raw: 'First, I''d like to thank you and your team for developing such a powerful
          model that has greatly contributed to the Traditional Chinese LLM community.


          Not really a question about bugs or errors. I''m just newbie in this field.
          but.... I''ve heard that LLAMA-2 from Meta was predominantly trained on
          English corpus. Consequently, the vanilla Meta LLAMA-2 struggles with Chinese
          reasoning and responses. It would be very grateful if you could share some
          of your experience or tips for training the English-based LLM to learn Chinese?

          And, if it''s like going through the pre-training again. What''s the advantage
          of using LLAMA2 rather than previous published model like Falcon or MPT
          ?


          Thank you !'
        updatedAt: '2023-08-23T12:10:21.548Z'
      numEdits: 1
      reactions: []
    id: 64df9d3bf6c2311e7ecc0724
    type: comment
  author: nps798
  content: 'First, I''d like to thank you and your team for developing such a powerful
    model that has greatly contributed to the Traditional Chinese LLM community.


    Not really a question about bugs or errors. I''m just newbie in this field. but....
    I''ve heard that LLAMA-2 from Meta was predominantly trained on English corpus.
    Consequently, the vanilla Meta LLAMA-2 struggles with Chinese reasoning and responses.
    It would be very grateful if you could share some of your experience or tips for
    training the English-based LLM to learn Chinese?

    And, if it''s like going through the pre-training again. What''s the advantage
    of using LLAMA2 rather than previous published model like Falcon or MPT ?


    Thank you !'
  created_at: 2023-08-18 15:32:59+00:00
  edited: true
  hidden: false
  id: 64df9d3bf6c2311e7ecc0724
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5df9c78eda6d0311fd3d541f/K8m3JEmIhH8WfwrgW-3l8.jpeg?w=200&h=200&f=face
      fullname: Yen-Ting Lin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: yentinglin
      type: user
    createdAt: '2023-11-12T03:25:34.000Z'
    data:
      edited: false
      editors:
      - yentinglin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.950697660446167
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5df9c78eda6d0311fd3d541f/K8m3JEmIhH8WfwrgW-3l8.jpeg?w=200&h=200&f=face
          fullname: Yen-Ting Lin
          isHf: false
          isPro: false
          name: yentinglin
          type: user
        html: '<p>Thanks for your kind words and sorry for my late reply.</p>

          <p>The choice of LLaMa-2 is based on the good performance (in English) and
          its relatively open license (compared to llama1)</p>

          <p>Taiwan-LLMs were all continue-pretrained on a massive amount of Traditional
          Chinese and then instruction-tuned.</p>

          <p>In additional to zh-tw, I kept a small portion of English and programming
          languages in the continue-pretraining in v2 for retraining the original
          capability in coding and english conversation.</p>

          '
        raw: 'Thanks for your kind words and sorry for my late reply.


          The choice of LLaMa-2 is based on the good performance (in English) and
          its relatively open license (compared to llama1)


          Taiwan-LLMs were all continue-pretrained on a massive amount of Traditional
          Chinese and then instruction-tuned.


          In additional to zh-tw, I kept a small portion of English and programming
          languages in the continue-pretraining in v2 for retraining the original
          capability in coding and english conversation.'
        updatedAt: '2023-11-12T03:25:34.485Z'
      numEdits: 0
      reactions: []
    id: 655045aea12178dd75b9e7eb
    type: comment
  author: yentinglin
  content: 'Thanks for your kind words and sorry for my late reply.


    The choice of LLaMa-2 is based on the good performance (in English) and its relatively
    open license (compared to llama1)


    Taiwan-LLMs were all continue-pretrained on a massive amount of Traditional Chinese
    and then instruction-tuned.


    In additional to zh-tw, I kept a small portion of English and programming languages
    in the continue-pretraining in v2 for retraining the original capability in coding
    and english conversation.'
  created_at: 2023-11-12 03:25:34+00:00
  edited: false
  hidden: false
  id: 655045aea12178dd75b9e7eb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: yentinglin/Taiwan-LLaMa-v1.0
repo_type: model
status: open
target_branch: null
title: Curious about how the model was trained to support Taiwan Chinese so well
