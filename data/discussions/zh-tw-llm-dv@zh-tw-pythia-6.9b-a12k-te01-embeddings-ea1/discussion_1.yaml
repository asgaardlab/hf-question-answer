!!python/object:huggingface_hub.community.DiscussionWithDetails
author: phamvantoan
conflicting_files: null
created_at: 2023-11-09 01:21:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/464b102ee1912be62d619abe295d62d2.svg
      fullname: Pham Van Toan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phamvantoan
      type: user
    createdAt: '2023-11-09T01:21:33.000Z'
    data:
      edited: true
      editors:
      - phamvantoan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9595143795013428
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/464b102ee1912be62d619abe295d62d2.svg
          fullname: Pham Van Toan
          isHf: false
          isPro: false
          name: phamvantoan
          type: user
        html: '<p>Hi,</p>

          <p>Firstly, thank you very much for your work!</p>

          <p>Could you please post more information to explain clearly for each LLM
          uploaded in "zh-tw-llm-dv"?</p>

          <p>For example, what does a12k, te01, v1-a_2, v1-b_1,  ea1, ... mean? What
          model is the latest/best one for chat/instruct/base model now?</p>

          <p>Besides, do I understand correctly that the model name with "tw" was
          trained on pure Traditional Chinese while "zh-tw" was trained on both Simplified
          and Traditional Chinese?</p>

          <p>I''m very appreciated about that because it will help us know what LLM
          is suitable for our applications before downloading.</p>

          '
        raw: 'Hi,


          Firstly, thank you very much for your work!


          Could you please post more information to explain clearly for each LLM uploaded
          in "zh-tw-llm-dv"?


          For example, what does a12k, te01, v1-a_2, v1-b_1,  ea1, ... mean? What
          model is the latest/best one for chat/instruct/base model now?


          Besides, do I understand correctly that the model name with "tw" was trained
          on pure Traditional Chinese while "zh-tw" was trained on both Simplified
          and Traditional Chinese?


          I''m very appreciated about that because it will help us know what LLM is
          suitable for our applications before downloading.'
        updatedAt: '2023-11-09T01:25:06.552Z'
      numEdits: 1
      reactions: []
    id: 654c341d6217c5e2f8e1a56f
    type: comment
  author: phamvantoan
  content: 'Hi,


    Firstly, thank you very much for your work!


    Could you please post more information to explain clearly for each LLM uploaded
    in "zh-tw-llm-dv"?


    For example, what does a12k, te01, v1-a_2, v1-b_1,  ea1, ... mean? What model
    is the latest/best one for chat/instruct/base model now?


    Besides, do I understand correctly that the model name with "tw" was trained on
    pure Traditional Chinese while "zh-tw" was trained on both Simplified and Traditional
    Chinese?


    I''m very appreciated about that because it will help us know what LLM is suitable
    for our applications before downloading.'
  created_at: 2023-11-09 01:21:33+00:00
  edited: true
  hidden: false
  id: 654c341d6217c5e2f8e1a56f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: zh-tw-llm-dv/zh-tw-pythia-6.9b-a12k-te01-embeddings-ea1
repo_type: model
status: open
target_branch: null
title: Request detailed information for each model in your repository "zh-tw-llm-dv"
