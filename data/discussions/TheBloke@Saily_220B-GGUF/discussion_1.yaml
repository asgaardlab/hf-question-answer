!!python/object:huggingface_hub.community.DiscussionWithDetails
author: uti24
conflicting_files: null
created_at: 2023-12-25 17:55:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c88c851e5783305deb4990e51242ac74.svg
      fullname: uti24
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: uti24
      type: user
    createdAt: '2023-12-25T17:55:05.000Z'
    data:
      edited: false
      editors:
      - uti24
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.18131986260414124
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c88c851e5783305deb4990e51242ac74.svg
          fullname: uti24
          isHf: false
          isPro: false
          name: uti24
          type: user
        html: '<p>When I try loading saily_220b.Q3_K_M.gguf using text-generation-webui
          I got error:<br>Traceback (most recent call last):</p>

          <p>File "S:\text-generation-webui\modules\ui_model_menu.py", line 214, in
          load_model_wrapper</p>

          <p>shared.model, shared.tokenizer = load_model(selected_model, loader)</p>

          <pre><code>                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "S:\text-generation-webui\modules\models.py", line 90, in load_model</p>

          <p>output = load_func_map<a rel="nofollow" href="model_name">loader</a></p>

          <pre><code>     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "S:\text-generation-webui\modules\models.py", line 260, in llamacpp_loader</p>

          <p>model, tokenizer = LlamaCppModel.from_pretrained(model_file)</p>

          <pre><code>               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "S:\text-generation-webui\modules\llamacpp_model.py", line 101,
          in from_pretrained</p>

          <p>result.model = Llama(**params)</p>

          <pre><code>           ^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "S:\text-generation-webui\installer_files\env\Lib\site-packages\llama_cpp_cuda\llama.py",
          line 932, in init</p>

          <p>self._ctx = _LlamaContext(</p>

          <pre><code>        ^^^^^^^^^^^^^^

          </code></pre>

          <p>File "S:\text-generation-webui\installer_files\env\Lib\site-packages\llama_cpp_cuda\llama.py",
          line 420, in init</p>

          <p>self.ctx = llama_cpp.llama_new_context_with_model(</p>

          <pre><code>       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "S:\text-generation-webui\installer_files\env\Lib\site-packages\llama_cpp_cuda\llama_cpp.py",
          line 667, in llama_new_context_with_model</p>

          <p>return _lib.llama_new_context_with_model(model, params)</p>

          <pre><code>   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>OSError: exception: access violation reading 0x0000000000000000</p>

          '
        raw: "When I try loading saily_220b.Q3_K_M.gguf using text-generation-webui\
          \ I got error:\r\nTraceback (most recent call last):\r\n\r\nFile \"S:\\\
          text-generation-webui\\modules\\ui_model_menu.py\", line 214, in load_model_wrapper\r\
          \n\r\n\r\nshared.model, shared.tokenizer = load_model(selected_model, loader)\r\
          \n\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \nFile \"S:\\text-generation-webui\\modules\\models.py\", line 90, in load_model\r\
          \n\r\n\r\noutput = load_func_map[loader](model_name)\r\n\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \nFile \"S:\\text-generation-webui\\modules\\models.py\", line 260, in llamacpp_loader\r\
          \n\r\n\r\nmodel, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
          \n\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile\
          \ \"S:\\text-generation-webui\\modules\\llamacpp_model.py\", line 101, in\
          \ from_pretrained\r\n\r\n\r\nresult.model = Llama(**params)\r\n\r\n    \
          \           ^^^^^^^^^^^^^^^\r\nFile \"S:\\text-generation-webui\\installer_files\\\
          env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 932, in init\r\
          \n\r\n\r\nself._ctx = _LlamaContext(\r\n\r\n            ^^^^^^^^^^^^^^\r\
          \nFile \"S:\\text-generation-webui\\installer_files\\env\\Lib\\site-packages\\\
          llama_cpp_cuda\\llama.py\", line 420, in init\r\n\r\n\r\nself.ctx = llama_cpp.llama_new_context_with_model(\r\
          \n\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"S:\\\
          text-generation-webui\\installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\\
          llama_cpp.py\", line 667, in llama_new_context_with_model\r\n\r\n\r\nreturn\
          \ _lib.llama_new_context_with_model(model, params)\r\n\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \nOSError: exception: access violation reading 0x0000000000000000"
        updatedAt: '2023-12-25T17:55:05.903Z'
      numEdits: 0
      reactions: []
    id: 6589c1f92021ba68d7c0452d
    type: comment
  author: uti24
  content: "When I try loading saily_220b.Q3_K_M.gguf using text-generation-webui\
    \ I got error:\r\nTraceback (most recent call last):\r\n\r\nFile \"S:\\text-generation-webui\\\
    modules\\ui_model_menu.py\", line 214, in load_model_wrapper\r\n\r\n\r\nshared.model,\
    \ shared.tokenizer = load_model(selected_model, loader)\r\n\r\n              \
    \                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"S:\\text-generation-webui\\\
    modules\\models.py\", line 90, in load_model\r\n\r\n\r\noutput = load_func_map[loader](model_name)\r\
    \n\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"S:\\text-generation-webui\\\
    modules\\models.py\", line 260, in llamacpp_loader\r\n\r\n\r\nmodel, tokenizer\
    \ = LlamaCppModel.from_pretrained(model_file)\r\n\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \nFile \"S:\\text-generation-webui\\modules\\llamacpp_model.py\", line 101, in\
    \ from_pretrained\r\n\r\n\r\nresult.model = Llama(**params)\r\n\r\n          \
    \     ^^^^^^^^^^^^^^^\r\nFile \"S:\\text-generation-webui\\installer_files\\env\\\
    Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 932, in init\r\n\r\n\r\n\
    self._ctx = _LlamaContext(\r\n\r\n            ^^^^^^^^^^^^^^\r\nFile \"S:\\text-generation-webui\\\
    installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 420,\
    \ in init\r\n\r\n\r\nself.ctx = llama_cpp.llama_new_context_with_model(\r\n\r\n\
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"S:\\text-generation-webui\\\
    installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\llama_cpp.py\", line\
    \ 667, in llama_new_context_with_model\r\n\r\n\r\nreturn _lib.llama_new_context_with_model(model,\
    \ params)\r\n\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nOSError:\
    \ exception: access violation reading 0x0000000000000000"
  created_at: 2023-12-25 17:55:05+00:00
  edited: false
  hidden: false
  id: 6589c1f92021ba68d7c0452d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Saily_220B-GGUF
repo_type: model
status: open
target_branch: null
title: Won't load on text-generation-webui
