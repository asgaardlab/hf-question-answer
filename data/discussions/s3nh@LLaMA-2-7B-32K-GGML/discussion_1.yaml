!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pszemraj
conflicting_files: null
created_at: 2023-07-30 06:25:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-07-30T07:25:21.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8546798825263977
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: "<p>hi! thanks for posting this. could you share more details about\
          \ your setup and what you did when you played around with the model and\
          \ tested? running your example (albeit on CPU <code>ctransformers</code>)\
          \ I get the very interesting dream response of:</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://i.imgur.com/iQgObVh.png\"><img alt=\"dream\" src=\"https://i.imgur.com/iQgObVh.png\"\
          ></a></p>\n<p>does it work normally/what am I doing wrong?</p>\n<p>A replicate-able\
          \ example:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >from</span> ctransformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM\n\
          \nllm = AutoModelForCausalLM.from_pretrained(\n    <span class=\"hljs-string\"\
          >\"s3nh/LLaMA-2-7B-32K-GGML\"</span>,\n    model_file=<span class=\"hljs-string\"\
          >\"LLaMA-2-7B-32K.ggmlv3.q4_1.bin\"</span>,\n    model_type=<span class=\"\
          hljs-string\">\"llama\"</span>,\n)\n\n<span class=\"hljs-built_in\">print</span>(llm(<span\
          \ class=\"hljs-string\">\"AI is going to\"</span>))\n</code></pre>\n<p><a\
          \ rel=\"nofollow\" href=\"https://i.imgur.com/GJ7Eejg.png\"><img alt=\"\
          ai\" src=\"https://i.imgur.com/GJ7Eejg.png\"></a></p>\n"
        raw: "hi! thanks for posting this. could you share more details about your\
          \ setup and what you did when you played around with the model and tested?\
          \ running your example (albeit on CPU `ctransformers`) I get the very interesting\
          \ dream response of:\r\n\r\n![dream](https://i.imgur.com/iQgObVh.png)\r\n\
          \r\ndoes it work normally/what am I doing wrong?\r\n\r\nA replicate-able\
          \ example:\r\n\r\n```python\r\nfrom ctransformers import AutoModelForCausalLM\r\
          \n\r\nllm = AutoModelForCausalLM.from_pretrained(\r\n    \"s3nh/LLaMA-2-7B-32K-GGML\"\
          ,\r\n    model_file=\"LLaMA-2-7B-32K.ggmlv3.q4_1.bin\",\r\n    model_type=\"\
          llama\",\r\n)\r\n\r\nprint(llm(\"AI is going to\"))\r\n```\r\n\r\n![ai](https://i.imgur.com/GJ7Eejg.png)"
        updatedAt: '2023-07-30T07:25:21.980Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - s3nh
    id: 64c61061aedc4337781c5b1e
    type: comment
  author: pszemraj
  content: "hi! thanks for posting this. could you share more details about your setup\
    \ and what you did when you played around with the model and tested? running your\
    \ example (albeit on CPU `ctransformers`) I get the very interesting dream response\
    \ of:\r\n\r\n![dream](https://i.imgur.com/iQgObVh.png)\r\n\r\ndoes it work normally/what\
    \ am I doing wrong?\r\n\r\nA replicate-able example:\r\n\r\n```python\r\nfrom\
    \ ctransformers import AutoModelForCausalLM\r\n\r\nllm = AutoModelForCausalLM.from_pretrained(\r\
    \n    \"s3nh/LLaMA-2-7B-32K-GGML\",\r\n    model_file=\"LLaMA-2-7B-32K.ggmlv3.q4_1.bin\"\
    ,\r\n    model_type=\"llama\",\r\n)\r\n\r\nprint(llm(\"AI is going to\"))\r\n\
    ```\r\n\r\n![ai](https://i.imgur.com/GJ7Eejg.png)"
  created_at: 2023-07-30 06:25:21+00:00
  edited: false
  hidden: false
  id: 64c61061aedc4337781c5b1e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
      fullname: s3nh
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: s3nh
      type: user
    createdAt: '2023-07-30T07:44:07.000Z'
    data:
      edited: false
      editors:
      - s3nh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9520867466926575
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
          fullname: s3nh
          isHf: false
          isPro: false
          name: s3nh
          type: user
        html: '<p>Hi, yes I am aware of that. The quantization goes well but output
          is not acceptable at all. I am working on this right new, will update when
          got this solved. </p>

          '
        raw: 'Hi, yes I am aware of that. The quantization goes well but output is
          not acceptable at all. I am working on this right new, will update when
          got this solved. '
        updatedAt: '2023-07-30T07:44:07.841Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - yovizzle
        - pszemraj
    id: 64c614c711354a0eca125467
    type: comment
  author: s3nh
  content: 'Hi, yes I am aware of that. The quantization goes well but output is not
    acceptable at all. I am working on this right new, will update when got this solved. '
  created_at: 2023-07-30 06:44:07+00:00
  edited: false
  hidden: false
  id: 64c614c711354a0eca125467
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
      fullname: Robert Dzupin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dzupin
      type: user
    createdAt: '2023-07-30T13:32:26.000Z'
    data:
      edited: false
      editors:
      - dzupin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9189862012863159
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
          fullname: Robert Dzupin
          isHf: false
          isPro: false
          name: dzupin
          type: user
        html: "<p>You probably need to use rope-frequency parameters with correct\
          \ values, considering this is 32K model .</p>\n<p>I am also trying to figure\
          \ out how to prompt it properly. Here is what I am getting so far:</p>\n\
          <p>main.exe  -t 16 -ngl 51 -m C:\\AI\\WIP\\32K\\LLaMA-2-7B-32K.ggmlv3.q4_1.bin\
          \ --color -c 32768 --rope-freq-scale 0.0625 --rope-freq-base 30000  -s 7\
          \ --keep -1 --mlock --temp 0.72 --top_p 0.73 --repeat_last_n 256 --batch_size\
          \ 1024 --repeat_penalty 1.1 -p \"Tell me about your last dream, please.\"\
          </p>\n<p>...<br>...<br>generate: n_ctx = 32768, n_batch = 512, n_predict\
          \ = -1, n_keep = 10</p>\n<p> Tell me about your last dream, please.</p>\n\
          <p>Do you remember any of the following: color, clothing, and/or accessories?<br>Ans:\
          \ Yes<br>What is the name of person in your dream that you are talking to?<br>Ans:\
          \ my dad<br>Tell me about a place in your dream, please?<br>Ans: I am in\
          \ an old house with my friend<br>What was happening before the event?<br>Ans:\
          \ we were getting ready for school<br>What happened after you got there?<br>Ans:\
          \ i fell down from stairs<br>Give me some info on what is going on in your\
          \ dream<br>Ans: I was climbing up the steps but then i saw a big black dog\
          \ jumping towards me so I started running<br>Is this a good or bad dream?<br>Ans:\
          \ Bad<br>Why did you have this dream?<br>Ans: It's a scary one<br>Does anything\
          \ happen when you wake up from this dream?<br>Ans: yes<br>Did you get hurt\
          \ in your dream?<br>Ans: no<br>What was the emotion you felt during the\
          \ dream?<br>Ans: Scared<br>Was anyone injured in your dream?<br>Ans:^C\n\
          \ </p>\n"
        raw: "You probably need to use rope-frequency parameters with correct values,\
          \ considering this is 32K model .\n\n\nI am also trying to figure out how\
          \ to prompt it properly. Here is what I am getting so far:\n\nmain.exe \
          \ -t 16 -ngl 51 -m C:\\AI\\WIP\\32K\\LLaMA-2-7B-32K.ggmlv3.q4_1.bin --color\
          \ -c 32768 --rope-freq-scale 0.0625 --rope-freq-base 30000  -s 7 --keep\
          \ -1 --mlock --temp 0.72 --top_p 0.73 --repeat_last_n 256 --batch_size 1024\
          \ --repeat_penalty 1.1 -p \"Tell me about your last dream, please.\"\n\n\
          \n...\n...\ngenerate: n_ctx = 32768, n_batch = 512, n_predict = -1, n_keep\
          \ = 10\n\n\n Tell me about your last dream, please.\n\nDo you remember any\
          \ of the following: color, clothing, and/or accessories?\nAns: Yes\nWhat\
          \ is the name of person in your dream that you are talking to?\nAns: my\
          \ dad\nTell me about a place in your dream, please?\nAns: I am in an old\
          \ house with my friend\nWhat was happening before the event?\nAns: we were\
          \ getting ready for school\nWhat happened after you got there?\nAns: i fell\
          \ down from stairs\nGive me some info on what is going on in your dream\n\
          Ans: I was climbing up the steps but then i saw a big black dog jumping\
          \ towards me so I started running\nIs this a good or bad dream?\nAns: Bad\n\
          Why did you have this dream?\nAns: It's a scary one\nDoes anything happen\
          \ when you wake up from this dream?\nAns: yes\nDid you get hurt in your\
          \ dream?\nAns: no\nWhat was the emotion you felt during the dream?\nAns:\
          \ Scared\nWas anyone injured in your dream?\nAns:^C\n "
        updatedAt: '2023-07-30T13:32:26.299Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - pszemraj
        - shafiqalibhai
    id: 64c6666aa684146b1c02389b
    type: comment
  author: dzupin
  content: "You probably need to use rope-frequency parameters with correct values,\
    \ considering this is 32K model .\n\n\nI am also trying to figure out how to prompt\
    \ it properly. Here is what I am getting so far:\n\nmain.exe  -t 16 -ngl 51 -m\
    \ C:\\AI\\WIP\\32K\\LLaMA-2-7B-32K.ggmlv3.q4_1.bin --color -c 32768 --rope-freq-scale\
    \ 0.0625 --rope-freq-base 30000  -s 7 --keep -1 --mlock --temp 0.72 --top_p 0.73\
    \ --repeat_last_n 256 --batch_size 1024 --repeat_penalty 1.1 -p \"Tell me about\
    \ your last dream, please.\"\n\n\n...\n...\ngenerate: n_ctx = 32768, n_batch =\
    \ 512, n_predict = -1, n_keep = 10\n\n\n Tell me about your last dream, please.\n\
    \nDo you remember any of the following: color, clothing, and/or accessories?\n\
    Ans: Yes\nWhat is the name of person in your dream that you are talking to?\n\
    Ans: my dad\nTell me about a place in your dream, please?\nAns: I am in an old\
    \ house with my friend\nWhat was happening before the event?\nAns: we were getting\
    \ ready for school\nWhat happened after you got there?\nAns: i fell down from\
    \ stairs\nGive me some info on what is going on in your dream\nAns: I was climbing\
    \ up the steps but then i saw a big black dog jumping towards me so I started\
    \ running\nIs this a good or bad dream?\nAns: Bad\nWhy did you have this dream?\n\
    Ans: It's a scary one\nDoes anything happen when you wake up from this dream?\n\
    Ans: yes\nDid you get hurt in your dream?\nAns: no\nWhat was the emotion you felt\
    \ during the dream?\nAns: Scared\nWas anyone injured in your dream?\nAns:^C\n "
  created_at: 2023-07-30 12:32:26+00:00
  edited: false
  hidden: false
  id: 64c6666aa684146b1c02389b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
      fullname: s3nh
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: s3nh
      type: user
    createdAt: '2023-07-30T13:37:16.000Z'
    data:
      edited: false
      editors:
      - s3nh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9583007097244263
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
          fullname: s3nh
          isHf: false
          isPro: false
          name: s3nh
          type: user
        html: '<p>Cool, so quantization is not an issue I guess. That help a lot!</p>

          '
        raw: Cool, so quantization is not an issue I guess. That help a lot!
        updatedAt: '2023-07-30T13:37:16.773Z'
      numEdits: 0
      reactions: []
    id: 64c6678cd43e4dee5195af71
    type: comment
  author: s3nh
  content: Cool, so quantization is not an issue I guess. That help a lot!
  created_at: 2023-07-30 12:37:16+00:00
  edited: false
  hidden: false
  id: 64c6678cd43e4dee5195af71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
      fullname: Robert Dzupin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dzupin
      type: user
    createdAt: '2023-07-30T13:43:56.000Z'
    data:
      edited: false
      editors:
      - dzupin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9629451036453247
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f712cd2d350562dfe5f84525f492c42.svg
          fullname: Robert Dzupin
          isHf: false
          isPro: false
          name: dzupin
          type: user
        html: '<p>Yes, quantization  looks to be working well.<br>Main problem I can
          see, is to figure out on how to use/prompt  32K model correctly. Yours is
          the first GGML model of that size I found so far, so not much info yet.   </p>

          '
        raw: "Yes, quantization  looks to be working well. \nMain problem I can see,\
          \ is to figure out on how to use/prompt  32K model correctly. Yours is the\
          \ first GGML model of that size I found so far, so not much info yet.   "
        updatedAt: '2023-07-30T13:43:56.520Z'
      numEdits: 0
      reactions: []
    id: 64c6691cc097c6c2b3c65d4c
    type: comment
  author: dzupin
  content: "Yes, quantization  looks to be working well. \nMain problem I can see,\
    \ is to figure out on how to use/prompt  32K model correctly. Yours is the first\
    \ GGML model of that size I found so far, so not much info yet.   "
  created_at: 2023-07-30 12:43:56+00:00
  edited: false
  hidden: false
  id: 64c6691cc097c6c2b3c65d4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1064f4b0fb98ac86cdff06c778b2c46c.svg
      fullname: Aptha K S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aptha
      type: user
    createdAt: '2023-07-31T07:25:41.000Z'
    data:
      edited: false
      editors:
      - aptha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8530066609382629
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1064f4b0fb98ac86cdff06c778b2c46c.svg
          fullname: Aptha K S
          isHf: false
          isPro: false
          name: aptha
          type: user
        html: '<p>Any update on the issue?</p>

          '
        raw: 'Any update on the issue?

          '
        updatedAt: '2023-07-31T07:25:41.377Z'
      numEdits: 0
      reactions: []
    id: 64c761f515bd12e579a12cdc
    type: comment
  author: aptha
  content: 'Any update on the issue?

    '
  created_at: 2023-07-31 06:25:41+00:00
  edited: false
  hidden: false
  id: 64c761f515bd12e579a12cdc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-08-02T22:08:31.000Z'
    data:
      edited: true
      editors:
      - pszemraj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.934812605381012
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: "<p>good stuff! <span data-props=\"{&quot;user&quot;:&quot;s3nh&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/s3nh\"\
          >@<span class=\"underline\">s3nh</span></a></span>\n\n\t</span></span> feel\
          \ free to close this :) I'm just leaving it open still in case anyone else\
          \ finds this and learns </p>\n"
        raw: 'good stuff! @s3nh feel free to close this :) I''m just leaving it open
          still in case anyone else finds this and learns '
        updatedAt: '2023-08-02T22:08:44.274Z'
      numEdits: 1
      reactions: []
    id: 64cad3df7a4f2363570fba6f
    type: comment
  author: pszemraj
  content: 'good stuff! @s3nh feel free to close this :) I''m just leaving it open
    still in case anyone else finds this and learns '
  created_at: 2023-08-02 21:08:31+00:00
  edited: true
  hidden: false
  id: 64cad3df7a4f2363570fba6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df90c855e1f66378b29d97e63ed7dcc3.svg
      fullname: Ivan Bokarev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sc0urge
      type: user
    createdAt: '2023-08-26T08:23:35.000Z'
    data:
      edited: false
      editors:
      - Sc0urge
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9032828211784363
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df90c855e1f66378b29d97e63ed7dcc3.svg
          fullname: Ivan Bokarev
          isHf: false
          isPro: false
          name: Sc0urge
          type: user
        html: '<blockquote>

          <p>You probably need to use rope-frequency parameters with correct values,
          considering this is 32K model .</p>

          <p>I am also trying to figure out how to prompt it properly. Here is what
          I am getting so far:</p>

          <p>main.exe  -t 16 -ngl 51 -m C:\AI\WIP\32K\LLaMA-2-7B-32K.ggmlv3.q4_1.bin
          --color -c 32768 --rope-freq-scale 0.0625 --rope-freq-base 30000  -s 7 --keep
          -1 --mlock --temp 0.72 --top_p 0.73 --repeat_last_n 256 --batch_size 1024
          --repeat_penalty 1.1 -p "Tell me about your last dream, please."</p>

          <p>...<br>...<br>generate: n_ctx = 32768, n_batch = 512, n_predict = -1,
          n_keep = 10</p>

          <p> Tell me about your last dream, please.</p>

          <p>Do you remember any of the following: color, clothing, and/or accessories?<br>Ans:
          Yes<br>What is the name of person in your dream that you are talking to?<br>Ans:
          my dad<br>Tell me about a place in your dream, please?<br>Ans: I am in an
          old house with my friend<br>What was happening before the event?<br>Ans:
          we were getting ready for school<br>What happened after you got there?<br>Ans:
          i fell down from stairs<br>Give me some info on what is going on in your
          dream<br>Ans: I was climbing up the steps but then i saw a big black dog
          jumping towards me so I started running<br>Is this a good or bad dream?<br>Ans:
          Bad<br>Why did you have this dream?<br>Ans: It''s a scary one<br>Does anything
          happen when you wake up from this dream?<br>Ans: yes<br>Did you get hurt
          in your dream?<br>Ans: no<br>What was the emotion you felt during the dream?<br>Ans:
          Scared<br>Was anyone injured in your dream?<br>Ans:^C</p>

          </blockquote>

          <p>I am trying to use the model in google colab and just pasting  your command
          gives me /bin/bash: line 1: main.exe: command not found. Is there a way
          to pass the commands in the python code instead of using the command line?</p>

          '
        raw: "> You probably need to use rope-frequency parameters with correct values,\
          \ considering this is 32K model .\n> \n> \n> I am also trying to figure\
          \ out how to prompt it properly. Here is what I am getting so far:\n> \n\
          > main.exe  -t 16 -ngl 51 -m C:\\AI\\WIP\\32K\\LLaMA-2-7B-32K.ggmlv3.q4_1.bin\
          \ --color -c 32768 --rope-freq-scale 0.0625 --rope-freq-base 30000  -s 7\
          \ --keep -1 --mlock --temp 0.72 --top_p 0.73 --repeat_last_n 256 --batch_size\
          \ 1024 --repeat_penalty 1.1 -p \"Tell me about your last dream, please.\"\
          \n> \n> \n> ...\n> ...\n> generate: n_ctx = 32768, n_batch = 512, n_predict\
          \ = -1, n_keep = 10\n> \n> \n>  Tell me about your last dream, please.\n\
          > \n> Do you remember any of the following: color, clothing, and/or accessories?\n\
          > Ans: Yes\n> What is the name of person in your dream that you are talking\
          \ to?\n> Ans: my dad\n> Tell me about a place in your dream, please?\n>\
          \ Ans: I am in an old house with my friend\n> What was happening before\
          \ the event?\n> Ans: we were getting ready for school\n> What happened after\
          \ you got there?\n> Ans: i fell down from stairs\n> Give me some info on\
          \ what is going on in your dream\n> Ans: I was climbing up the steps but\
          \ then i saw a big black dog jumping towards me so I started running\n>\
          \ Is this a good or bad dream?\n> Ans: Bad\n> Why did you have this dream?\n\
          > Ans: It's a scary one\n> Does anything happen when you wake up from this\
          \ dream?\n> Ans: yes\n> Did you get hurt in your dream?\n> Ans: no\n> What\
          \ was the emotion you felt during the dream?\n> Ans: Scared\n> Was anyone\
          \ injured in your dream?\n> Ans:^C\n\nI am trying to use the model in google\
          \ colab and just pasting  your command gives me /bin/bash: line 1: main.exe:\
          \ command not found. Is there a way to pass the commands in the python code\
          \ instead of using the command line?\n"
        updatedAt: '2023-08-26T08:23:35.834Z'
      numEdits: 0
      reactions: []
    id: 64e9b687b96ff0e175273cd5
    type: comment
  author: Sc0urge
  content: "> You probably need to use rope-frequency parameters with correct values,\
    \ considering this is 32K model .\n> \n> \n> I am also trying to figure out how\
    \ to prompt it properly. Here is what I am getting so far:\n> \n> main.exe  -t\
    \ 16 -ngl 51 -m C:\\AI\\WIP\\32K\\LLaMA-2-7B-32K.ggmlv3.q4_1.bin --color -c 32768\
    \ --rope-freq-scale 0.0625 --rope-freq-base 30000  -s 7 --keep -1 --mlock --temp\
    \ 0.72 --top_p 0.73 --repeat_last_n 256 --batch_size 1024 --repeat_penalty 1.1\
    \ -p \"Tell me about your last dream, please.\"\n> \n> \n> ...\n> ...\n> generate:\
    \ n_ctx = 32768, n_batch = 512, n_predict = -1, n_keep = 10\n> \n> \n>  Tell me\
    \ about your last dream, please.\n> \n> Do you remember any of the following:\
    \ color, clothing, and/or accessories?\n> Ans: Yes\n> What is the name of person\
    \ in your dream that you are talking to?\n> Ans: my dad\n> Tell me about a place\
    \ in your dream, please?\n> Ans: I am in an old house with my friend\n> What was\
    \ happening before the event?\n> Ans: we were getting ready for school\n> What\
    \ happened after you got there?\n> Ans: i fell down from stairs\n> Give me some\
    \ info on what is going on in your dream\n> Ans: I was climbing up the steps but\
    \ then i saw a big black dog jumping towards me so I started running\n> Is this\
    \ a good or bad dream?\n> Ans: Bad\n> Why did you have this dream?\n> Ans: It's\
    \ a scary one\n> Does anything happen when you wake up from this dream?\n> Ans:\
    \ yes\n> Did you get hurt in your dream?\n> Ans: no\n> What was the emotion you\
    \ felt during the dream?\n> Ans: Scared\n> Was anyone injured in your dream?\n\
    > Ans:^C\n\nI am trying to use the model in google colab and just pasting  your\
    \ command gives me /bin/bash: line 1: main.exe: command not found. Is there a\
    \ way to pass the commands in the python code instead of using the command line?\n"
  created_at: 2023-08-26 07:23:35+00:00
  edited: false
  hidden: false
  id: 64e9b687b96ff0e175273cd5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-08-26T19:58:24.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9499165415763855
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: "<p>AFAIK <code>ctransformers</code> doesn't support it yet (would be\
          \ good if someone created an issue btw \U0001F642), but you can  use <a\
          \ rel=\"nofollow\" href=\"https://github.com/abetlen/llama-cpp-python\"\
          >llama-cpp-python</a> which has similar bindings and does support the needed\
          \ params</p>\n"
        raw: "AFAIK `ctransformers` doesn't support it yet (would be good if someone\
          \ created an issue btw \U0001F642), but you can  use [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)\
          \ which has similar bindings and does support the needed params"
        updatedAt: '2023-08-26T19:58:24.665Z'
      numEdits: 0
      reactions: []
    id: 64ea59609e53684e6e833fbf
    type: comment
  author: pszemraj
  content: "AFAIK `ctransformers` doesn't support it yet (would be good if someone\
    \ created an issue btw \U0001F642), but you can  use [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)\
    \ which has similar bindings and does support the needed params"
  created_at: 2023-08-26 18:58:24+00:00
  edited: false
  hidden: false
  id: 64ea59609e53684e6e833fbf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: s3nh/LLaMA-2-7B-32K-GGML
repo_type: model
status: open
target_branch: null
title: 'model outputs with ctransformers '
