!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AmazingTurtle
conflicting_files: null
created_at: 2023-07-30 20:03:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3080d12b7f8fe10beb517bfeed349c10.svg
      fullname: Felix Faust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AmazingTurtle
      type: user
    createdAt: '2023-07-30T21:03:57.000Z'
    data:
      edited: true
      editors:
      - AmazingTurtle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48108088970184326
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3080d12b7f8fe10beb517bfeed349c10.svg
          fullname: Felix Faust
          isHf: false
          isPro: false
          name: AmazingTurtle
          type: user
        html: "<p>I'm running latest llama.cpp (<code>11f3ca06b8c66b0427aab0a472479da22553b472</code>)\
          \ on with <code>LLAMA_CUBLAS=1</code> and get the following error when loading\
          \ the model</p>\n<pre><code class=\"language-ShellSession\"><span class=\"\
          hljs-meta prompt_\">$ </span><span class=\"language-bash\">./main -m ./models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin</span>\n\
          main: build = 928 (11f3ca0)\nmain: seed  = 1690751016\nggml_init_cublas:\
          \ found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3080, compute capability\
          \ 8.6\nllama.cpp: loading model from ./models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin\n\
          error loading model: unexpectedly reached end of file\nllama_load_model_from_file:\
          \ failed to load model\nllama_init_from_gpt_params: error: failed to load\
          \ model './models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin'\n\
          main: error: unable to load model\n</code></pre>\n<p>I have confirmed that\
          \ the downloaded ggml models sha256sum is identical to remote.</p>\n"
        raw: "I'm running latest llama.cpp (`11f3ca06b8c66b0427aab0a472479da22553b472`)\
          \ on with `LLAMA_CUBLAS=1` and get the following error when loading the\
          \ model\n\n```ShellSession\n$ ./main -m ./models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin\n\
          main: build = 928 (11f3ca0)\nmain: seed  = 1690751016\nggml_init_cublas:\
          \ found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3080, compute capability\
          \ 8.6\nllama.cpp: loading model from ./models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin\n\
          error loading model: unexpectedly reached end of file\nllama_load_model_from_file:\
          \ failed to load model\nllama_init_from_gpt_params: error: failed to load\
          \ model './models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin'\n\
          main: error: unable to load model\n```\n\nI have confirmed that the downloaded\
          \ ggml models sha256sum is identical to remote."
        updatedAt: '2023-07-30T21:04:47.889Z'
      numEdits: 1
      reactions: []
    id: 64c6d03d720d494bb29cf5dc
    type: comment
  author: AmazingTurtle
  content: "I'm running latest llama.cpp (`11f3ca06b8c66b0427aab0a472479da22553b472`)\
    \ on with `LLAMA_CUBLAS=1` and get the following error when loading the model\n\
    \n```ShellSession\n$ ./main -m ./models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin\n\
    main: build = 928 (11f3ca0)\nmain: seed  = 1690751016\nggml_init_cublas: found\
    \ 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3080, compute capability 8.6\n\
    llama.cpp: loading model from ./models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin\n\
    error loading model: unexpectedly reached end of file\nllama_load_model_from_file:\
    \ failed to load model\nllama_init_from_gpt_params: error: failed to load model\
    \ './models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin'\nmain: error:\
    \ unable to load model\n```\n\nI have confirmed that the downloaded ggml models\
    \ sha256sum is identical to remote."
  created_at: 2023-07-30 20:03:57+00:00
  edited: true
  hidden: false
  id: 64c6d03d720d494bb29cf5dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
      fullname: s3nh
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: s3nh
      type: user
    createdAt: '2023-07-31T07:17:56.000Z'
    data:
      edited: false
      editors:
      - s3nh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7939971685409546
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
          fullname: s3nh
          isHf: false
          isPro: false
          name: s3nh
          type: user
        html: '<p>Hi, Could you please try to run it with  parameters mentioned in
          linked discussion?<br> <a href="https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML/discussions/1#64c6666aa684146b1c02389b">https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML/discussions/1#64c6666aa684146b1c02389b</a><br>To
          convert I was using ggml v3 version.<br>Thanks.</p>

          '
        raw: "Hi, Could you please try to run it with  parameters mentioned in linked\
          \ discussion? \n https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML/discussions/1#64c6666aa684146b1c02389b\n\
          To convert I was using ggml v3 version.\nThanks."
        updatedAt: '2023-07-31T07:17:56.417Z'
      numEdits: 0
      reactions: []
    id: 64c7602419565937fb6690e7
    type: comment
  author: s3nh
  content: "Hi, Could you please try to run it with  parameters mentioned in linked\
    \ discussion? \n https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML/discussions/1#64c6666aa684146b1c02389b\n\
    To convert I was using ggml v3 version.\nThanks."
  created_at: 2023-07-31 06:17:56+00:00
  edited: false
  hidden: false
  id: 64c7602419565937fb6690e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3080d12b7f8fe10beb517bfeed349c10.svg
      fullname: Felix Faust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AmazingTurtle
      type: user
    createdAt: '2023-07-31T09:38:03.000Z'
    data:
      edited: false
      editors:
      - AmazingTurtle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.41310590505599976
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3080d12b7f8fe10beb517bfeed349c10.svg
          fullname: Felix Faust
          isHf: false
          isPro: false
          name: AmazingTurtle
          type: user
        html: "<p>apparently llama.cpp does not support ggmlv3. I tried the rope frequency\
          \ params etc. but no success.</p>\n<pre><code class=\"language-ShellSession\"\
          ><span class=\"hljs-meta prompt_\">$ </span><span class=\"language-bash\"\
          >./main -t 8 -ngl 0 -m ./models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin\
          \ --grammar-file ./grammars/doc-describe.gbnf -n 1024 -f prompts/classify.txt\
          \ -c 32768 --temp 0 --top-k 1024 --top-p 0.9 --color -b 1024  --rope-freq-scale\
          \ 0.0625 --rope-freq-base 30000 --keep 1</span>\nmain: warning: changing\
          \ RoPE frequency base to 30000 (default 10000.0)\nmain: warning: scaling\
          \ RoPE frequency by 0,0625 (default 1.0)\nmain: warning: base model only\
          \ supports context sizes no greater than 2048 tokens (32768 specified)\n\
          main: build = 928 (11f3ca0)\nmain: seed  = 1690796156\nggml_init_cublas:\
          \ found 1 CUDA devices:\n  Device 0: NVIDIA GeForce RTX 3080, compute capability\
          \ 8.6\nllama.cpp: loading model from ./models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin\n\
          error loading model: unexpectedly reached end of file\nllama_load_model_from_file:\
          \ failed to load model\nllama_init_from_gpt_params: error: failed to load\
          \ model './models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin'\n\
          main: error: unable to load model\n</code></pre>\n"
        raw: "apparently llama.cpp does not support ggmlv3. I tried the rope frequency\
          \ params etc. but no success.\n\n```ShellSession\n$ ./main -t 8 -ngl 0 -m\
          \ ./models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin --grammar-file\
          \ ./grammars/doc-describe.gbnf -n 1024 -f prompts/classify.txt -c 32768\
          \ --temp 0 --top-k 1024 --top-p 0.9 --color -b 1024  --rope-freq-scale 0.0625\
          \ --rope-freq-base 30000 --keep 1\nmain: warning: changing RoPE frequency\
          \ base to 30000 (default 10000.0)\nmain: warning: scaling RoPE frequency\
          \ by 0,0625 (default 1.0)\nmain: warning: base model only supports context\
          \ sizes no greater than 2048 tokens (32768 specified)\nmain: build = 928\
          \ (11f3ca0)\nmain: seed  = 1690796156\nggml_init_cublas: found 1 CUDA devices:\n\
          \  Device 0: NVIDIA GeForce RTX 3080, compute capability 8.6\nllama.cpp:\
          \ loading model from ./models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin\n\
          error loading model: unexpectedly reached end of file\nllama_load_model_from_file:\
          \ failed to load model\nllama_init_from_gpt_params: error: failed to load\
          \ model './models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin'\n\
          main: error: unable to load model\n```"
        updatedAt: '2023-07-31T09:38:03.283Z'
      numEdits: 0
      reactions: []
    id: 64c780fbf3d2a59a43f3fcfe
    type: comment
  author: AmazingTurtle
  content: "apparently llama.cpp does not support ggmlv3. I tried the rope frequency\
    \ params etc. but no success.\n\n```ShellSession\n$ ./main -t 8 -ngl 0 -m ./models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin\
    \ --grammar-file ./grammars/doc-describe.gbnf -n 1024 -f prompts/classify.txt\
    \ -c 32768 --temp 0 --top-k 1024 --top-p 0.9 --color -b 1024  --rope-freq-scale\
    \ 0.0625 --rope-freq-base 30000 --keep 1\nmain: warning: changing RoPE frequency\
    \ base to 30000 (default 10000.0)\nmain: warning: scaling RoPE frequency by 0,0625\
    \ (default 1.0)\nmain: warning: base model only supports context sizes no greater\
    \ than 2048 tokens (32768 specified)\nmain: build = 928 (11f3ca0)\nmain: seed\
    \  = 1690796156\nggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA GeForce\
    \ RTX 3080, compute capability 8.6\nllama.cpp: loading model from ./models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin\n\
    error loading model: unexpectedly reached end of file\nllama_load_model_from_file:\
    \ failed to load model\nllama_init_from_gpt_params: error: failed to load model\
    \ './models/llama-2-7b-32k-ggml/LLaMA-2-7B-32K.ggmlv3.q5_1.bin'\nmain: error:\
    \ unable to load model\n```"
  created_at: 2023-07-31 08:38:03+00:00
  edited: false
  hidden: false
  id: 64c780fbf3d2a59a43f3fcfe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67b5096fda259b8503d2c98eed09c1f6.svg
      fullname: Stefan Alenius
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Darlanio
      type: user
    createdAt: '2023-08-02T08:11:35.000Z'
    data:
      edited: false
      editors:
      - Darlanio
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6823082566261292
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67b5096fda259b8503d2c98eed09c1f6.svg
          fullname: Stefan Alenius
          isHf: false
          isPro: false
          name: Darlanio
          type: user
        html: '<p>Seems so, yes,</p>

          <p>main.exe -t 1 -n 65536 -c 32768 -m d:\Files\LLAMA2-7B-32k\LLaMA-2-7B-32K.ggmlv3.q8_0.bin
          -p "Once upon a time,"<br>main: warning: base model only supports context
          sizes no greater than 2048 tokens (32768 specified)</p>

          '
        raw: 'Seems so, yes,


          main.exe -t 1 -n 65536 -c 32768 -m d:\Files\LLAMA2-7B-32k\LLaMA-2-7B-32K.ggmlv3.q8_0.bin
          -p "Once upon a time,"

          main: warning: base model only supports context sizes no greater than 2048
          tokens (32768 specified)'
        updatedAt: '2023-08-02T08:11:35.193Z'
      numEdits: 0
      reactions: []
    id: 64ca0fb7547f59248fd13e70
    type: comment
  author: Darlanio
  content: 'Seems so, yes,


    main.exe -t 1 -n 65536 -c 32768 -m d:\Files\LLAMA2-7B-32k\LLaMA-2-7B-32K.ggmlv3.q8_0.bin
    -p "Once upon a time,"

    main: warning: base model only supports context sizes no greater than 2048 tokens
    (32768 specified)'
  created_at: 2023-08-02 07:11:35+00:00
  edited: false
  hidden: false
  id: 64ca0fb7547f59248fd13e70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1aeb668c702e4846ae73b561e5ead14.svg
      fullname: Jose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aldarisbm
      type: user
    createdAt: '2023-08-02T23:13:37.000Z'
    data:
      edited: false
      editors:
      - aldarisbm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9322071671485901
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1aeb668c702e4846ae73b561e5ead14.svg
          fullname: Jose
          isHf: false
          isPro: false
          name: aldarisbm
          type: user
        html: '<p>I am running this with llama.cpp on a macbook m2 successfully.</p>

          '
        raw: I am running this with llama.cpp on a macbook m2 successfully.
        updatedAt: '2023-08-02T23:13:37.726Z'
      numEdits: 0
      reactions: []
    id: 64cae321942890af934088e8
    type: comment
  author: aldarisbm
  content: I am running this with llama.cpp on a macbook m2 successfully.
  created_at: 2023-08-02 22:13:37+00:00
  edited: false
  hidden: false
  id: 64cae321942890af934088e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/812f44c598fdaf5962badd54ccde651e.svg
      fullname: Robert M Arnold
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rmarnold
      type: user
    createdAt: '2023-08-02T23:39:05.000Z'
    data:
      edited: false
      editors:
      - rmarnold
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8701545596122742
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/812f44c598fdaf5962badd54ccde651e.svg
          fullname: Robert M Arnold
          isHf: false
          isPro: false
          name: rmarnold
          type: user
        html: '<p>it looks like the context size is still only 2048 for llama.cpp...</p>

          '
        raw: 'it looks like the context size is still only 2048 for llama.cpp...

          '
        updatedAt: '2023-08-02T23:39:05.918Z'
      numEdits: 0
      reactions: []
    id: 64cae91938837b12d50395dc
    type: comment
  author: rmarnold
  content: 'it looks like the context size is still only 2048 for llama.cpp...

    '
  created_at: 2023-08-02 22:39:05+00:00
  edited: false
  hidden: false
  id: 64cae91938837b12d50395dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d581a78a256b2a52e7fcef7e8272cb9.svg
      fullname: Peter Kotsch
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LinkuStarto
      type: user
    createdAt: '2023-08-10T08:06:22.000Z'
    data:
      edited: false
      editors:
      - LinkuStarto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3943784236907959
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d581a78a256b2a52e7fcef7e8272cb9.svg
          fullname: Peter Kotsch
          isHf: false
          isPro: false
          name: LinkuStarto
          type: user
        html: '<blockquote>

          <p>it looks like the context size is still only 2048 for llama.cpp...</p>

          </blockquote>

          <p>actually it supports 4096 already. Just need to tweak the RoPE parameters
          depending on the context size of the model.<br>That being said, the model
          doesn''t seem to be compatible with llama.cpp or llama-cpp-python.<br>Are
          there any other specific parameters needed for this model ?</p>

          <p>This is my output when I try to load the model:</p>

          <p> llama.cpp: loading model from models/LLaMA-2-7B-32K.ggmlv3.q4_1.bin<br>llama_model_load_internal:
          format     = ggjt v3 (latest)<br>llama_model_load_internal: n_vocab    =
          32000<br>llama_model_load_internal: n_ctx      = 32768<br>llama_model_load_internal:
          n_embd     = 4096<br>llama_model_load_internal: n_mult     = 256<br>llama_model_load_internal:
          n_head     = 32<br>llama_model_load_internal: n_head_kv  = 32<br>llama_model_load_internal:
          n_layer    = 32<br>llama_model_load_internal: n_rot      = 128<br>llama_model_load_internal:
          n_gqa      = 1<br>llama_model_load_internal: rnorm_eps  = 1.0e-06<br>llama_model_load_internal:
          n_ff       = 11008<br>llama_model_load_internal: freq_base  = 10000.0<br>llama_model_load_internal:
          freq_scale = 0.125<br>llama_model_load_internal: ftype      = 3 (mostly
          Q4_1)<br>llama_model_load_internal: model size = 7B<br>llama_model_load_internal:
          ggml ctx size =    0.08 MB<br>llama_model_load_internal: mem required  =
          6359.77 MB (+ 16384.00 MB per state)<br>terminate called after throwing
          an instance of ''std::bad_alloc''<br>  what():  std::bad_alloc</p>

          '
        raw: "> it looks like the context size is still only 2048 for llama.cpp...\n\
          \nactually it supports 4096 already. Just need to tweak the RoPE parameters\
          \ depending on the context size of the model. \nThat being said, the model\
          \ doesn't seem to be compatible with llama.cpp or llama-cpp-python.\nAre\
          \ there any other specific parameters needed for this model ?\n\nThis is\
          \ my output when I try to load the model:\n\n\n llama.cpp: loading model\
          \ from models/LLaMA-2-7B-32K.ggmlv3.q4_1.bin\nllama_model_load_internal:\
          \ format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    =\
          \ 32000\nllama_model_load_internal: n_ctx      = 32768\nllama_model_load_internal:\
          \ n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
          \ n_head     = 32\nllama_model_load_internal: n_head_kv  = 32\nllama_model_load_internal:\
          \ n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal:\
          \ n_gqa      = 1\nllama_model_load_internal: rnorm_eps  = 1.0e-06\nllama_model_load_internal:\
          \ n_ff       = 11008\nllama_model_load_internal: freq_base  = 10000.0\n\
          llama_model_load_internal: freq_scale = 0.125\nllama_model_load_internal:\
          \ ftype      = 3 (mostly Q4_1)\nllama_model_load_internal: model size =\
          \ 7B\nllama_model_load_internal: ggml ctx size =    0.08 MB\nllama_model_load_internal:\
          \ mem required  = 6359.77 MB (+ 16384.00 MB per state)\nterminate called\
          \ after throwing an instance of 'std::bad_alloc'\n  what():  std::bad_alloc"
        updatedAt: '2023-08-10T08:06:22.213Z'
      numEdits: 0
      reactions: []
    id: 64d49a7ef20597d0f508fae9
    type: comment
  author: LinkuStarto
  content: "> it looks like the context size is still only 2048 for llama.cpp...\n\
    \nactually it supports 4096 already. Just need to tweak the RoPE parameters depending\
    \ on the context size of the model. \nThat being said, the model doesn't seem\
    \ to be compatible with llama.cpp or llama-cpp-python.\nAre there any other specific\
    \ parameters needed for this model ?\n\nThis is my output when I try to load the\
    \ model:\n\n\n llama.cpp: loading model from models/LLaMA-2-7B-32K.ggmlv3.q4_1.bin\n\
    llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
    \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 32768\nllama_model_load_internal:\
    \ n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
    \ n_head     = 32\nllama_model_load_internal: n_head_kv  = 32\nllama_model_load_internal:\
    \ n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal:\
    \ n_gqa      = 1\nllama_model_load_internal: rnorm_eps  = 1.0e-06\nllama_model_load_internal:\
    \ n_ff       = 11008\nllama_model_load_internal: freq_base  = 10000.0\nllama_model_load_internal:\
    \ freq_scale = 0.125\nllama_model_load_internal: ftype      = 3 (mostly Q4_1)\n\
    llama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx\
    \ size =    0.08 MB\nllama_model_load_internal: mem required  = 6359.77 MB (+\
    \ 16384.00 MB per state)\nterminate called after throwing an instance of 'std::bad_alloc'\n\
    \  what():  std::bad_alloc"
  created_at: 2023-08-10 07:06:22+00:00
  edited: false
  hidden: false
  id: 64d49a7ef20597d0f508fae9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67f2bb5bcd5cc125efc6d07f4af00ea5.svg
      fullname: Jun Xu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: askool
      type: user
    createdAt: '2023-08-13T13:01:00.000Z'
    data:
      edited: false
      editors:
      - askool
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.366608589887619
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67f2bb5bcd5cc125efc6d07f4af00ea5.svg
          fullname: Jun Xu
          isHf: false
          isPro: false
          name: askool
          type: user
        html: '<p>it is able to run, but it simply gave the following message:</p>

          <h3 id="instruction-write-a-story-about-llamasn-response">Instruction: Write
          a story about llamas\n### Response:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::</h3>

          <p>----- more information -----<br>./main -t 10 -m "LLaMA-2-7B-32K.ggmlv3.q8_0.bin"
          --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "### Instruction:
          Write a story about llamas\n### Response:"<br>main: warning: base model
          only supports context sizes no greater than 2048 tokens (4096 specified)<br>main:
          build = 977 (b19edd5)<br>main: seed  = 1691930243<br>llama.cpp: loading
          model from /home/xujun/Downloads/LLaMA-2-7B-32K.ggmlv3.q8_0.bin<br>llama_model_load_internal:
          format     = ggjt v3 (latest)<br>llama_model_load_internal: n_vocab    =
          32000<br>llama_model_load_internal: n_ctx      = 4096<br>llama_model_load_internal:
          n_embd     = 4096<br>llama_model_load_internal: n_mult     = 256<br>llama_model_load_internal:
          n_head     = 32<br>llama_model_load_internal: n_head_kv  = 32<br>llama_model_load_internal:
          n_layer    = 32<br>llama_model_load_internal: n_rot      = 128<br>llama_model_load_internal:
          n_gqa      = 1<br>llama_model_load_internal: rnorm_eps  = 5.0e-06<br>llama_model_load_internal:
          n_ff       = 11008<br>llama_model_load_internal: freq_base  = 10000.0<br>llama_model_load_internal:
          freq_scale = 1<br>llama_model_load_internal: ftype      = 7 (mostly Q8_0)<br>llama_model_load_internal:
          model size = 7B<br>llama_model_load_internal: ggml ctx size =    0.08 MB<br>llama_model_load_internal:
          mem required  = 6798.46 MB (+ 2048.00 MB per state)<br>llama_new_context_with_model:
          kv self size  = 2048.00 MB<br>llama_new_context_with_model: compute buffer
          total size =  281.35 MB</p>

          <p>system_info: n_threads = 10 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 |
          AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C
          = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |<br>sampling:
          repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000,
          frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000,
          typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000,
          mirostat_ent = 5.000000<br>generate: n_ctx = 4096, n_batch = 512, n_predict
          = -1, n_keep = 0</p>

          '
        raw: "it is able to run, but it simply gave the following message:\n\n###\
          \ Instruction: Write a story about llamas\\n### Response:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n\
          \n\n\n----- more information -----\n./main -t 10 -m \"LLaMA-2-7B-32K.ggmlv3.q8_0.bin\"\
          \ --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"### Instruction:\
          \ Write a story about llamas\\n### Response:\"\nmain: warning: base model\
          \ only supports context sizes no greater than 2048 tokens (4096 specified)\n\
          main: build = 977 (b19edd5)\nmain: seed  = 1691930243\nllama.cpp: loading\
          \ model from /home/xujun/Downloads/LLaMA-2-7B-32K.ggmlv3.q8_0.bin\nllama_model_load_internal:\
          \ format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    =\
          \ 32000\nllama_model_load_internal: n_ctx      = 4096\nllama_model_load_internal:\
          \ n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
          \ n_head     = 32\nllama_model_load_internal: n_head_kv  = 32\nllama_model_load_internal:\
          \ n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal:\
          \ n_gqa      = 1\nllama_model_load_internal: rnorm_eps  = 5.0e-06\nllama_model_load_internal:\
          \ n_ff       = 11008\nllama_model_load_internal: freq_base  = 10000.0\n\
          llama_model_load_internal: freq_scale = 1\nllama_model_load_internal: ftype\
          \      = 7 (mostly Q8_0)\nllama_model_load_internal: model size = 7B\nllama_model_load_internal:\
          \ ggml ctx size =    0.08 MB\nllama_model_load_internal: mem required  =\
          \ 6798.46 MB (+ 2048.00 MB per state)\nllama_new_context_with_model: kv\
          \ self size  = 2048.00 MB\nllama_new_context_with_model: compute buffer\
          \ total size =  281.35 MB\n\nsystem_info: n_threads = 10 / 12 | AVX = 1\
          \ | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1\
          \ | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS\
          \ = 0 | SSE3 = 1 | VSX = 0 | \nsampling: repeat_last_n = 64, repeat_penalty\
          \ = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000,\
          \ top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000,\
          \ temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent =\
          \ 5.000000\ngenerate: n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep\
          \ = 0\n"
        updatedAt: '2023-08-13T13:01:00.601Z'
      numEdits: 0
      reactions: []
    id: 64d8d40c67c967b015026fde
    type: comment
  author: askool
  content: "it is able to run, but it simply gave the following message:\n\n### Instruction:\
    \ Write a story about llamas\\n### Response:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n\
    \n\n\n----- more information -----\n./main -t 10 -m \"LLaMA-2-7B-32K.ggmlv3.q8_0.bin\"\
    \ --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"### Instruction:\
    \ Write a story about llamas\\n### Response:\"\nmain: warning: base model only\
    \ supports context sizes no greater than 2048 tokens (4096 specified)\nmain: build\
    \ = 977 (b19edd5)\nmain: seed  = 1691930243\nllama.cpp: loading model from /home/xujun/Downloads/LLaMA-2-7B-32K.ggmlv3.q8_0.bin\n\
    llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
    \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 4096\nllama_model_load_internal:\
    \ n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
    \ n_head     = 32\nllama_model_load_internal: n_head_kv  = 32\nllama_model_load_internal:\
    \ n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal:\
    \ n_gqa      = 1\nllama_model_load_internal: rnorm_eps  = 5.0e-06\nllama_model_load_internal:\
    \ n_ff       = 11008\nllama_model_load_internal: freq_base  = 10000.0\nllama_model_load_internal:\
    \ freq_scale = 1\nllama_model_load_internal: ftype      = 7 (mostly Q8_0)\nllama_model_load_internal:\
    \ model size = 7B\nllama_model_load_internal: ggml ctx size =    0.08 MB\nllama_model_load_internal:\
    \ mem required  = 6798.46 MB (+ 2048.00 MB per state)\nllama_new_context_with_model:\
    \ kv self size  = 2048.00 MB\nllama_new_context_with_model: compute buffer total\
    \ size =  281.35 MB\n\nsystem_info: n_threads = 10 / 12 | AVX = 1 | AVX2 = 1 |\
    \ AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA\
    \ = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0\
    \ | \nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty\
    \ = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p\
    \ = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr\
    \ = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx = 4096, n_batch = 512,\
    \ n_predict = -1, n_keep = 0\n"
  created_at: 2023-08-13 12:01:00+00:00
  edited: false
  hidden: false
  id: 64d8d40c67c967b015026fde
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67f2bb5bcd5cc125efc6d07f4af00ea5.svg
      fullname: Jun Xu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: askool
      type: user
    createdAt: '2023-08-13T13:02:56.000Z'
    data:
      edited: true
      editors:
      - askool
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8787634968757629
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67f2bb5bcd5cc125efc6d07f4af00ea5.svg
          fullname: Jun Xu
          isHf: false
          isPro: false
          name: askool
          type: user
        html: '<p>it works with some non-default parameters as stated in <a href="https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML/discussions/1">https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML/discussions/1</a></p>

          '
        raw: 'it works with some non-default parameters as stated in https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML/discussions/1





          '
        updatedAt: '2023-08-13T13:19:42.304Z'
      numEdits: 1
      reactions: []
    id: 64d8d480c9def36c56cc5f2b
    type: comment
  author: askool
  content: 'it works with some non-default parameters as stated in https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML/discussions/1





    '
  created_at: 2023-08-13 12:02:56+00:00
  edited: true
  hidden: false
  id: 64d8d480c9def36c56cc5f2b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: s3nh/LLaMA-2-7B-32K-GGML
repo_type: model
status: open
target_branch: null
title: Not compatible with llama.cpp
