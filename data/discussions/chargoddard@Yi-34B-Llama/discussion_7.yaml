!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Q-bert
conflicting_files: null
created_at: 2023-11-28 17:57:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
      fullname: "Talha R\xFCzgar Akku\u015F"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Q-bert
      type: user
    createdAt: '2023-11-28T17:57:37.000Z'
    data:
      edited: false
      editors:
      - Q-bert
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9905953407287598
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
          fullname: "Talha R\xFCzgar Akku\u015F"
          isHf: false
          isPro: false
          name: Q-bert
          type: user
        html: '<p>I couldn''t write converting code but you did it. Can you share
          code with me pls ?</p>

          '
        raw: I couldn't write converting code but you did it. Can you share code with
          me pls ?
        updatedAt: '2023-11-28T17:57:37.046Z'
      numEdits: 0
      reactions: []
    id: 65662a116443f1b315ba7891
    type: comment
  author: Q-bert
  content: I couldn't write converting code but you did it. Can you share code with
    me pls ?
  created_at: 2023-11-28 17:57:37+00:00
  edited: false
  hidden: false
  id: 65662a116443f1b315ba7891
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630495b0ce6b12280b193c25/bT61kBtQPhDYk00AI0o0g.png?w=200&h=200&f=face
      fullname: Charles Goddard
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: chargoddard
      type: user
    createdAt: '2023-12-10T02:03:11.000Z'
    data:
      edited: false
      editors:
      - chargoddard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.43793681263923645
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630495b0ce6b12280b193c25/bT61kBtQPhDYk00AI0o0g.png?w=200&h=200&f=face
          fullname: Charles Goddard
          isHf: false
          isPro: false
          name: chargoddard
          type: user
        html: "<p>Not sure if this is still useful, but sure - here's the script I\
          \ wrote to convert this originally.</p>\n<pre><code class=\"language-py\"\
          ><span class=\"hljs-keyword\">import</span> copy\n<span class=\"hljs-keyword\"\
          >import</span> os\n<span class=\"hljs-keyword\">import</span> safetensors.torch\n\
          <span class=\"hljs-keyword\">import</span> glob\n<span class=\"hljs-keyword\"\
          >import</span> json\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">transform_st</span>(<span class=\"hljs-params\">path:\
          \ <span class=\"hljs-built_in\">str</span>, out_dir: <span class=\"hljs-built_in\"\
          >str</span></span>):\n    data = safetensors.torch.load_file(path)\n   \
          \ old_keys = <span class=\"hljs-built_in\">list</span>(data.keys())\n  \
          \  <span class=\"hljs-keyword\">for</span> key <span class=\"hljs-keyword\"\
          >in</span> old_keys:\n        old_key = key\n        <span class=\"hljs-keyword\"\
          >if</span> <span class=\"hljs-string\">\".ln1.\"</span> <span class=\"hljs-keyword\"\
          >in</span> key:\n            key = key.replace(<span class=\"hljs-string\"\
          >\".ln1.\"</span>, <span class=\"hljs-string\">\".input_layernorm.\"</span>)\n\
          \        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\"\
          >\".ln2.\"</span> <span class=\"hljs-keyword\">in</span> key:\n        \
          \    key = key.replace(<span class=\"hljs-string\">\".ln2.\"</span>, <span\
          \ class=\"hljs-string\">\".post_attention_layernorm.\"</span>)\n       \
          \ <span class=\"hljs-keyword\">if</span> key != old_key:\n            data[key]\
          \ = data[old_key]\n            <span class=\"hljs-keyword\">del</span> data[old_key]\n\
          \    safetensors.torch.save_file(\n        data, os.path.join(out_dir, os.path.basename(path)),\
          \ metadata={<span class=\"hljs-string\">\"format\"</span>: <span class=\"\
          hljs-string\">\"pt\"</span>}\n    )\n\n\n<span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">process_model</span>(<span class=\"\
          hljs-params\">path: <span class=\"hljs-built_in\">str</span>, out_path:\
          \ <span class=\"hljs-built_in\">str</span></span>):\n    <span class=\"\
          hljs-keyword\">for</span> p <span class=\"hljs-keyword\">in</span> glob.glob(os.path.join(path,\
          \ <span class=\"hljs-string\">\"model-*.safetensors\"</span>)):\n      \
          \  transform_st(p, out_path)\n\n    <span class=\"hljs-keyword\">with</span>\
          \ <span class=\"hljs-built_in\">open</span>(os.path.join(path, <span class=\"\
          hljs-string\">\"model.safetensors.index.json\"</span>, <span class=\"hljs-string\"\
          >\"r\"</span>)) <span class=\"hljs-keyword\">as</span> fd:\n        index_data\
          \ = json.load(fd)\n\n    new_index = {<span class=\"hljs-string\">\"metadata\"\
          </span>: copy.copy(index_data[<span class=\"hljs-string\">\"metadata\"</span>]),\
          \ <span class=\"hljs-string\">\"weight_map\"</span>: {}}\n    <span class=\"\
          hljs-keyword\">for</span> key <span class=\"hljs-keyword\">in</span> index_data[<span\
          \ class=\"hljs-string\">\"weight_map\"</span>]:\n        new_key = key.replace(<span\
          \ class=\"hljs-string\">\".ln1.\"</span>, <span class=\"hljs-string\">\"\
          .input_layernorm.\"</span>).replace(\n            <span class=\"hljs-string\"\
          >\".ln2.\"</span>, <span class=\"hljs-string\">\".post_attention_layernorm.\"\
          </span>\n        )\n        new_index[<span class=\"hljs-string\">\"weight_map\"\
          </span>][new_key] = index_data[<span class=\"hljs-string\">\"weight_map\"\
          </span>][key]\n\n    <span class=\"hljs-keyword\">with</span> <span class=\"\
          hljs-built_in\">open</span>(\n        os.path.join(out_path, <span class=\"\
          hljs-string\">\"model.safetensors.index.json\"</span>, <span class=\"hljs-string\"\
          >\"w\"</span>, encoding=<span class=\"hljs-string\">\"utf-8\"</span>)\n\
          \    ) <span class=\"hljs-keyword\">as</span> fd:\n        json.dump(new_index,\
          \ fd)\n\n\nprocess_model(<span class=\"hljs-string\">\"/workspace/Yi-34B\"\
          </span>, <span class=\"hljs-string\">\"/workspace/Yi-34B-Llama\"</span>)\n\
          </code></pre>\n"
        raw: "Not sure if this is still useful, but sure - here's the script I wrote\
          \ to convert this originally.\n\n```py\nimport copy\nimport os\nimport safetensors.torch\n\
          import glob\nimport json\n\n\ndef transform_st(path: str, out_dir: str):\n\
          \    data = safetensors.torch.load_file(path)\n    old_keys = list(data.keys())\n\
          \    for key in old_keys:\n        old_key = key\n        if \".ln1.\" in\
          \ key:\n            key = key.replace(\".ln1.\", \".input_layernorm.\")\n\
          \        if \".ln2.\" in key:\n            key = key.replace(\".ln2.\",\
          \ \".post_attention_layernorm.\")\n        if key != old_key:\n        \
          \    data[key] = data[old_key]\n            del data[old_key]\n    safetensors.torch.save_file(\n\
          \        data, os.path.join(out_dir, os.path.basename(path)), metadata={\"\
          format\": \"pt\"}\n    )\n\n\ndef process_model(path: str, out_path: str):\n\
          \    for p in glob.glob(os.path.join(path, \"model-*.safetensors\")):\n\
          \        transform_st(p, out_path)\n\n    with open(os.path.join(path, \"\
          model.safetensors.index.json\", \"r\")) as fd:\n        index_data = json.load(fd)\n\
          \n    new_index = {\"metadata\": copy.copy(index_data[\"metadata\"]), \"\
          weight_map\": {}}\n    for key in index_data[\"weight_map\"]:\n        new_key\
          \ = key.replace(\".ln1.\", \".input_layernorm.\").replace(\n           \
          \ \".ln2.\", \".post_attention_layernorm.\"\n        )\n        new_index[\"\
          weight_map\"][new_key] = index_data[\"weight_map\"][key]\n\n    with open(\n\
          \        os.path.join(out_path, \"model.safetensors.index.json\", \"w\"\
          , encoding=\"utf-8\")\n    ) as fd:\n        json.dump(new_index, fd)\n\n\
          \nprocess_model(\"/workspace/Yi-34B\", \"/workspace/Yi-34B-Llama\")\n```"
        updatedAt: '2023-12-10T02:03:11.743Z'
      numEdits: 0
      reactions: []
    id: 65751c5f6da136b50f9ca2d6
    type: comment
  author: chargoddard
  content: "Not sure if this is still useful, but sure - here's the script I wrote\
    \ to convert this originally.\n\n```py\nimport copy\nimport os\nimport safetensors.torch\n\
    import glob\nimport json\n\n\ndef transform_st(path: str, out_dir: str):\n   \
    \ data = safetensors.torch.load_file(path)\n    old_keys = list(data.keys())\n\
    \    for key in old_keys:\n        old_key = key\n        if \".ln1.\" in key:\n\
    \            key = key.replace(\".ln1.\", \".input_layernorm.\")\n        if \"\
    .ln2.\" in key:\n            key = key.replace(\".ln2.\", \".post_attention_layernorm.\"\
    )\n        if key != old_key:\n            data[key] = data[old_key]\n       \
    \     del data[old_key]\n    safetensors.torch.save_file(\n        data, os.path.join(out_dir,\
    \ os.path.basename(path)), metadata={\"format\": \"pt\"}\n    )\n\n\ndef process_model(path:\
    \ str, out_path: str):\n    for p in glob.glob(os.path.join(path, \"model-*.safetensors\"\
    )):\n        transform_st(p, out_path)\n\n    with open(os.path.join(path, \"\
    model.safetensors.index.json\", \"r\")) as fd:\n        index_data = json.load(fd)\n\
    \n    new_index = {\"metadata\": copy.copy(index_data[\"metadata\"]), \"weight_map\"\
    : {}}\n    for key in index_data[\"weight_map\"]:\n        new_key = key.replace(\"\
    .ln1.\", \".input_layernorm.\").replace(\n            \".ln2.\", \".post_attention_layernorm.\"\
    \n        )\n        new_index[\"weight_map\"][new_key] = index_data[\"weight_map\"\
    ][key]\n\n    with open(\n        os.path.join(out_path, \"model.safetensors.index.json\"\
    , \"w\", encoding=\"utf-8\")\n    ) as fd:\n        json.dump(new_index, fd)\n\
    \n\nprocess_model(\"/workspace/Yi-34B\", \"/workspace/Yi-34B-Llama\")\n```"
  created_at: 2023-12-10 02:03:11+00:00
  edited: false
  hidden: false
  id: 65751c5f6da136b50f9ca2d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
      fullname: "Talha R\xFCzgar Akku\u015F"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Q-bert
      type: user
    createdAt: '2023-12-10T19:19:31.000Z'
    data:
      edited: false
      editors:
      - Q-bert
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45940202474594116
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675246771355-noauth.jpeg?w=200&h=200&f=face
          fullname: "Talha R\xFCzgar Akku\u015F"
          isHf: false
          isPro: false
          name: Q-bert
          type: user
        html: "<blockquote>\n<p>Not sure if this is still useful, but sure - here's\
          \ the script I wrote to convert this originally.</p>\n<pre><code class=\"\
          language-py\"><span class=\"hljs-keyword\">import</span> copy\n<span class=\"\
          hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">import</span>\
          \ safetensors.torch\n<span class=\"hljs-keyword\">import</span> glob\n<span\
          \ class=\"hljs-keyword\">import</span> json\n\n\n<span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">transform_st</span>(<span\
          \ class=\"hljs-params\">path: <span class=\"hljs-built_in\">str</span>,\
          \ out_dir: <span class=\"hljs-built_in\">str</span></span>):\n    data =\
          \ safetensors.torch.load_file(path)\n    old_keys = <span class=\"hljs-built_in\"\
          >list</span>(data.keys())\n    <span class=\"hljs-keyword\">for</span> key\
          \ <span class=\"hljs-keyword\">in</span> old_keys:\n        old_key = key\n\
          \        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-string\"\
          >\".ln1.\"</span> <span class=\"hljs-keyword\">in</span> key:\n        \
          \    key = key.replace(<span class=\"hljs-string\">\".ln1.\"</span>, <span\
          \ class=\"hljs-string\">\".input_layernorm.\"</span>)\n        <span class=\"\
          hljs-keyword\">if</span> <span class=\"hljs-string\">\".ln2.\"</span> <span\
          \ class=\"hljs-keyword\">in</span> key:\n            key = key.replace(<span\
          \ class=\"hljs-string\">\".ln2.\"</span>, <span class=\"hljs-string\">\"\
          .post_attention_layernorm.\"</span>)\n        <span class=\"hljs-keyword\"\
          >if</span> key != old_key:\n            data[key] = data[old_key]\n    \
          \        <span class=\"hljs-keyword\">del</span> data[old_key]\n    safetensors.torch.save_file(\n\
          \        data, os.path.join(out_dir, os.path.basename(path)), metadata={<span\
          \ class=\"hljs-string\">\"format\"</span>: <span class=\"hljs-string\">\"\
          pt\"</span>}\n    )\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">process_model</span>(<span class=\"hljs-params\"\
          >path: <span class=\"hljs-built_in\">str</span>, out_path: <span class=\"\
          hljs-built_in\">str</span></span>):\n    <span class=\"hljs-keyword\">for</span>\
          \ p <span class=\"hljs-keyword\">in</span> glob.glob(os.path.join(path,\
          \ <span class=\"hljs-string\">\"model-*.safetensors\"</span>)):\n      \
          \  transform_st(p, out_path)\n\n    <span class=\"hljs-keyword\">with</span>\
          \ <span class=\"hljs-built_in\">open</span>(os.path.join(path, <span class=\"\
          hljs-string\">\"model.safetensors.index.json\"</span>, <span class=\"hljs-string\"\
          >\"r\"</span>)) <span class=\"hljs-keyword\">as</span> fd:\n        index_data\
          \ = json.load(fd)\n\n    new_index = {<span class=\"hljs-string\">\"metadata\"\
          </span>: copy.copy(index_data[<span class=\"hljs-string\">\"metadata\"</span>]),\
          \ <span class=\"hljs-string\">\"weight_map\"</span>: {}}\n    <span class=\"\
          hljs-keyword\">for</span> key <span class=\"hljs-keyword\">in</span> index_data[<span\
          \ class=\"hljs-string\">\"weight_map\"</span>]:\n        new_key = key.replace(<span\
          \ class=\"hljs-string\">\".ln1.\"</span>, <span class=\"hljs-string\">\"\
          .input_layernorm.\"</span>).replace(\n            <span class=\"hljs-string\"\
          >\".ln2.\"</span>, <span class=\"hljs-string\">\".post_attention_layernorm.\"\
          </span>\n        )\n        new_index[<span class=\"hljs-string\">\"weight_map\"\
          </span>][new_key] = index_data[<span class=\"hljs-string\">\"weight_map\"\
          </span>][key]\n\n    <span class=\"hljs-keyword\">with</span> <span class=\"\
          hljs-built_in\">open</span>(\n        os.path.join(out_path, <span class=\"\
          hljs-string\">\"model.safetensors.index.json\"</span>, <span class=\"hljs-string\"\
          >\"w\"</span>, encoding=<span class=\"hljs-string\">\"utf-8\"</span>)\n\
          \    ) <span class=\"hljs-keyword\">as</span> fd:\n        json.dump(new_index,\
          \ fd)\n\n\nprocess_model(<span class=\"hljs-string\">\"/workspace/Yi-34B\"\
          </span>, <span class=\"hljs-string\">\"/workspace/Yi-34B-Llama\"</span>)\n\
          </code></pre>\n</blockquote>\n<p>Can we do the same for this model? It is\
          \ almost exactly the same as the stablelm llama. </p>\n<p><a href=\"https://huggingface.co/stabilityai/stablelm-zephyr-3b\"\
          >https://huggingface.co/stabilityai/stablelm-zephyr-3b</a></p>\n"
        raw: "> Not sure if this is still useful, but sure - here's the script I wrote\
          \ to convert this originally.\n> \n> ```py\n> import copy\n> import os\n\
          > import safetensors.torch\n> import glob\n> import json\n> \n> \n> def\
          \ transform_st(path: str, out_dir: str):\n>     data = safetensors.torch.load_file(path)\n\
          >     old_keys = list(data.keys())\n>     for key in old_keys:\n>      \
          \   old_key = key\n>         if \".ln1.\" in key:\n>             key = key.replace(\"\
          .ln1.\", \".input_layernorm.\")\n>         if \".ln2.\" in key:\n>     \
          \        key = key.replace(\".ln2.\", \".post_attention_layernorm.\")\n\
          >         if key != old_key:\n>             data[key] = data[old_key]\n\
          >             del data[old_key]\n>     safetensors.torch.save_file(\n> \
          \        data, os.path.join(out_dir, os.path.basename(path)), metadata={\"\
          format\": \"pt\"}\n>     )\n> \n> \n> def process_model(path: str, out_path:\
          \ str):\n>     for p in glob.glob(os.path.join(path, \"model-*.safetensors\"\
          )):\n>         transform_st(p, out_path)\n> \n>     with open(os.path.join(path,\
          \ \"model.safetensors.index.json\", \"r\")) as fd:\n>         index_data\
          \ = json.load(fd)\n> \n>     new_index = {\"metadata\": copy.copy(index_data[\"\
          metadata\"]), \"weight_map\": {}}\n>     for key in index_data[\"weight_map\"\
          ]:\n>         new_key = key.replace(\".ln1.\", \".input_layernorm.\").replace(\n\
          >             \".ln2.\", \".post_attention_layernorm.\"\n>         )\n>\
          \         new_index[\"weight_map\"][new_key] = index_data[\"weight_map\"\
          ][key]\n> \n>     with open(\n>         os.path.join(out_path, \"model.safetensors.index.json\"\
          , \"w\", encoding=\"utf-8\")\n>     ) as fd:\n>         json.dump(new_index,\
          \ fd)\n> \n> \n> process_model(\"/workspace/Yi-34B\", \"/workspace/Yi-34B-Llama\"\
          )\n> ```\n\nCan we do the same for this model? It is almost exactly the\
          \ same as the stablelm llama. \n\nhttps://huggingface.co/stabilityai/stablelm-zephyr-3b"
        updatedAt: '2023-12-10T19:19:31.347Z'
      numEdits: 0
      reactions: []
    id: 65760f43a90ae2daaef7c1d2
    type: comment
  author: Q-bert
  content: "> Not sure if this is still useful, but sure - here's the script I wrote\
    \ to convert this originally.\n> \n> ```py\n> import copy\n> import os\n> import\
    \ safetensors.torch\n> import glob\n> import json\n> \n> \n> def transform_st(path:\
    \ str, out_dir: str):\n>     data = safetensors.torch.load_file(path)\n>     old_keys\
    \ = list(data.keys())\n>     for key in old_keys:\n>         old_key = key\n>\
    \         if \".ln1.\" in key:\n>             key = key.replace(\".ln1.\", \"\
    .input_layernorm.\")\n>         if \".ln2.\" in key:\n>             key = key.replace(\"\
    .ln2.\", \".post_attention_layernorm.\")\n>         if key != old_key:\n>    \
    \         data[key] = data[old_key]\n>             del data[old_key]\n>     safetensors.torch.save_file(\n\
    >         data, os.path.join(out_dir, os.path.basename(path)), metadata={\"format\"\
    : \"pt\"}\n>     )\n> \n> \n> def process_model(path: str, out_path: str):\n>\
    \     for p in glob.glob(os.path.join(path, \"model-*.safetensors\")):\n>    \
    \     transform_st(p, out_path)\n> \n>     with open(os.path.join(path, \"model.safetensors.index.json\"\
    , \"r\")) as fd:\n>         index_data = json.load(fd)\n> \n>     new_index =\
    \ {\"metadata\": copy.copy(index_data[\"metadata\"]), \"weight_map\": {}}\n> \
    \    for key in index_data[\"weight_map\"]:\n>         new_key = key.replace(\"\
    .ln1.\", \".input_layernorm.\").replace(\n>             \".ln2.\", \".post_attention_layernorm.\"\
    \n>         )\n>         new_index[\"weight_map\"][new_key] = index_data[\"weight_map\"\
    ][key]\n> \n>     with open(\n>         os.path.join(out_path, \"model.safetensors.index.json\"\
    , \"w\", encoding=\"utf-8\")\n>     ) as fd:\n>         json.dump(new_index, fd)\n\
    > \n> \n> process_model(\"/workspace/Yi-34B\", \"/workspace/Yi-34B-Llama\")\n\
    > ```\n\nCan we do the same for this model? It is almost exactly the same as the\
    \ stablelm llama. \n\nhttps://huggingface.co/stabilityai/stablelm-zephyr-3b"
  created_at: 2023-12-10 19:19:31+00:00
  edited: false
  hidden: false
  id: 65760f43a90ae2daaef7c1d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630495b0ce6b12280b193c25/bT61kBtQPhDYk00AI0o0g.png?w=200&h=200&f=face
      fullname: Charles Goddard
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: chargoddard
      type: user
    createdAt: '2023-12-12T06:37:06.000Z'
    data:
      edited: false
      editors:
      - chargoddard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9534143805503845
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630495b0ce6b12280b193c25/bT61kBtQPhDYk00AI0o0g.png?w=200&h=200&f=face
          fullname: Charles Goddard
          isHf: false
          isPro: false
          name: chargoddard
          type: user
        html: '<blockquote>

          <p>Can we do the same for this model? It is almost exactly the same as the
          stablelm llama. </p>

          <p><a href="https://huggingface.co/stabilityai/stablelm-zephyr-3b">https://huggingface.co/stabilityai/stablelm-zephyr-3b</a></p>

          </blockquote>

          <p>StableLM has a bias term in their norm layers, which Llama does not.
          You could strip them out and try to use the rest of the weights with LlamaForCausalLM,
          but it''s pretty likely it won''t be coherent without fine-tuning.</p>

          '
        raw: "> Can we do the same for this model? It is almost exactly the same as\
          \ the stablelm llama. \n> \n> https://huggingface.co/stabilityai/stablelm-zephyr-3b\n\
          \nStableLM has a bias term in their norm layers, which Llama does not. You\
          \ could strip them out and try to use the rest of the weights with LlamaForCausalLM,\
          \ but it's pretty likely it won't be coherent without fine-tuning."
        updatedAt: '2023-12-12T06:37:06.194Z'
      numEdits: 0
      reactions: []
    id: 6577ff9217166d821e08e0ce
    type: comment
  author: chargoddard
  content: "> Can we do the same for this model? It is almost exactly the same as\
    \ the stablelm llama. \n> \n> https://huggingface.co/stabilityai/stablelm-zephyr-3b\n\
    \nStableLM has a bias term in their norm layers, which Llama does not. You could\
    \ strip them out and try to use the rest of the weights with LlamaForCausalLM,\
    \ but it's pretty likely it won't be coherent without fine-tuning."
  created_at: 2023-12-12 06:37:06+00:00
  edited: false
  hidden: false
  id: 6577ff9217166d821e08e0ce
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: chargoddard/Yi-34B-Llama
repo_type: model
status: open
target_branch: null
title: Can you share the converting code ?
