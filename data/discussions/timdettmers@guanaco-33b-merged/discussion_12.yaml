!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xiaojinchuan
conflicting_files: null
created_at: 2023-06-08 07:11:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/667e46d953482c66d4bb63620e76fb4d.svg
      fullname: xiaojinchuan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xiaojinchuan
      type: user
    createdAt: '2023-06-08T08:11:30.000Z'
    data:
      edited: false
      editors:
      - xiaojinchuan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7328541278839111
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/667e46d953482c66d4bb63620e76fb4d.svg
          fullname: xiaojinchuan
          isHf: false
          isPro: false
          name: xiaojinchuan
          type: user
        html: '<p>I converted this model to ggml, and quantized it to 4bit using <a
          rel="nofollow" href="https://github.com/ggerganov/llama.cpp/blob/master/convert.py">https://github.com/ggerganov/llama.cpp/blob/master/convert.py</a><br>and
          run the quantized model with llama-cpp-python, get gibberish output as below.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/647ef5cbaa8c04bbf9360844/TAhD048KPoyf85ARFT8tq.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/647ef5cbaa8c04bbf9360844/TAhD048KPoyf85ARFT8tq.png"></a></p>

          '
        raw: "I converted this model to ggml, and quantized it to 4bit using https://github.com/ggerganov/llama.cpp/blob/master/convert.py\r\
          \nand run the quantized model with llama-cpp-python, get gibberish output\
          \ as below.\r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/647ef5cbaa8c04bbf9360844/TAhD048KPoyf85ARFT8tq.png)\r\
          \n"
        updatedAt: '2023-06-08T08:11:30.098Z'
      numEdits: 0
      reactions: []
    id: 64818d32578646b5c2362e36
    type: comment
  author: xiaojinchuan
  content: "I converted this model to ggml, and quantized it to 4bit using https://github.com/ggerganov/llama.cpp/blob/master/convert.py\r\
    \nand run the quantized model with llama-cpp-python, get gibberish output as below.\r\
    \n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/647ef5cbaa8c04bbf9360844/TAhD048KPoyf85ARFT8tq.png)\r\
    \n"
  created_at: 2023-06-08 07:11:30+00:00
  edited: false
  hidden: false
  id: 64818d32578646b5c2362e36
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: timdettmers/guanaco-33b-merged
repo_type: model
status: open
target_branch: null
title: Run this model with llama.cpp, get gibberish output
