!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ehartford
conflicting_files: null
created_at: 2023-05-24 19:18:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-05-24T20:18:00.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>Whats the difference between<br>guanaco-33b-merged and<br>guanaco-33b
          ?</p>

          <p>Also thanks for setting precedent, henceforth I''ll be calling it 33b
          instead of 30b too</p>

          '
        raw: "Whats the difference between \r\nguanaco-33b-merged and \r\nguanaco-33b\
          \ ?\r\n\r\nAlso thanks for setting precedent, henceforth I'll be calling\
          \ it 33b instead of 30b too\r\n"
        updatedAt: '2023-05-24T20:18:00.777Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - thecuriousnewbie
    id: 646e70f8ee36629c15ba50bd
    type: comment
  author: ehartford
  content: "Whats the difference between \r\nguanaco-33b-merged and \r\nguanaco-33b\
    \ ?\r\n\r\nAlso thanks for setting precedent, henceforth I'll be calling it 33b\
    \ instead of 30b too\r\n"
  created_at: 2023-05-24 19:18:00+00:00
  edited: false
  hidden: false
  id: 646e70f8ee36629c15ba50bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-25T00:22:17.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>guanaco-33b = LoRA only<br>guanaco-33b-merged = full model</p>

          <p>?</p>

          '
        raw: 'guanaco-33b = LoRA only

          guanaco-33b-merged = full model


          ?'
        updatedAt: '2023-05-25T00:22:17.594Z'
      numEdits: 0
      reactions: []
    id: 646eaa391a427e263041b921
    type: comment
  author: mancub
  content: 'guanaco-33b = LoRA only

    guanaco-33b-merged = full model


    ?'
  created_at: 2023-05-24 23:22:17+00:00
  edited: false
  hidden: false
  id: 646eaa391a427e263041b921
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-05-25T00:33:02.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>ok thank you :D</p>

          '
        raw: ok thank you :D
        updatedAt: '2023-05-25T00:33:02.080Z'
      numEdits: 0
      reactions: []
    id: 646eacbe7a376d3010cb625e
    type: comment
  author: ehartford
  content: ok thank you :D
  created_at: 2023-05-24 23:33:02+00:00
  edited: false
  hidden: false
  id: 646eacbe7a376d3010cb625e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-05-25T01:33:59.000Z'
    data:
      edited: true
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: '<p>These merged weights are the size of the full precision weights?</p>

          <p>I thought QLORA was done against the 4bit weights, or did I misunderstand?</p>

          <p>Please fill my empty brain~</p>

          <p>edit: ah, I see.. the fine-tuning is actually against the unquantized
          HF model.</p>

          '
        raw: 'These merged weights are the size of the full precision weights?


          I thought QLORA was done against the 4bit weights, or did I misunderstand?


          Please fill my empty brain~


          edit: ah, I see.. the fine-tuning is actually against the unquantized HF
          model.'
        updatedAt: '2023-05-25T05:11:40.792Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - tensiondriven
    id: 646ebb077a376d3010cd599b
    type: comment
  author: disarmyouwitha
  content: 'These merged weights are the size of the full precision weights?


    I thought QLORA was done against the 4bit weights, or did I misunderstand?


    Please fill my empty brain~


    edit: ah, I see.. the fine-tuning is actually against the unquantized HF model.'
  created_at: 2023-05-25 00:33:59+00:00
  edited: true
  hidden: false
  id: 646ebb077a376d3010cd599b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-05-25T17:22:13.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>The model seems very promising, but I can''t seem to get good inference
          speed compared to other GPTQ 4-bit quantized models.  I''m running this
          model on a 4090 and I''m getting only a few tokens per second following
          these instructions:<br><a rel="nofollow" href="https://github.com/artidoro/qlora#quantization">https://github.com/artidoro/qlora#quantization</a></p>

          <p>When I run the same prompt on the HF Spaces, I get much, much faster
          inference (probably 20-40 tokens per second):<br><a href="https://huggingface.co/spaces/uwnlp/guanaco-playground-tgi">https://huggingface.co/spaces/uwnlp/guanaco-playground-tgi</a></p>

          <p>Any suggestions for speeding things up?</p>

          <p>Thanks!</p>

          '
        raw: 'The model seems very promising, but I can''t seem to get good inference
          speed compared to other GPTQ 4-bit quantized models.  I''m running this
          model on a 4090 and I''m getting only a few tokens per second following
          these instructions:

          https://github.com/artidoro/qlora#quantization


          When I run the same prompt on the HF Spaces, I get much, much faster inference
          (probably 20-40 tokens per second):

          https://huggingface.co/spaces/uwnlp/guanaco-playground-tgi


          Any suggestions for speeding things up?


          Thanks!'
        updatedAt: '2023-05-25T17:22:13.034Z'
      numEdits: 0
      reactions: []
    id: 646f9945bc42f4b00231652e
    type: comment
  author: LoneStriker
  content: 'The model seems very promising, but I can''t seem to get good inference
    speed compared to other GPTQ 4-bit quantized models.  I''m running this model
    on a 4090 and I''m getting only a few tokens per second following these instructions:

    https://github.com/artidoro/qlora#quantization


    When I run the same prompt on the HF Spaces, I get much, much faster inference
    (probably 20-40 tokens per second):

    https://huggingface.co/spaces/uwnlp/guanaco-playground-tgi


    Any suggestions for speeding things up?


    Thanks!'
  created_at: 2023-05-25 16:22:13+00:00
  edited: false
  hidden: false
  id: 646f9945bc42f4b00231652e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-27T14:53:19.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Maybe give TheBloke/guanaco-33B-GPTQ or TheBloke/guanaco-33B-GGML\
          \ a try instead?</p>\n<p>I would not expect my 3090, or your 4090, to ever\
          \ match HF Spaces inference speed as they most likely run better hardware.</p>\n"
        raw: "@LoneStriker \n\nMaybe give TheBloke/guanaco-33B-GPTQ or TheBloke/guanaco-33B-GGML\
          \ a try instead?\n\nI would not expect my 3090, or your 4090, to ever match\
          \ HF Spaces inference speed as they most likely run better hardware."
        updatedAt: '2023-05-27T14:53:19.352Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - LoneStriker
        - Chris126
    id: 6472195f5afd6a696583034a
    type: comment
  author: mancub
  content: "@LoneStriker \n\nMaybe give TheBloke/guanaco-33B-GPTQ or TheBloke/guanaco-33B-GGML\
    \ a try instead?\n\nI would not expect my 3090, or your 4090, to ever match HF\
    \ Spaces inference speed as they most likely run better hardware."
  created_at: 2023-05-27 13:53:19+00:00
  edited: false
  hidden: false
  id: 6472195f5afd6a696583034a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-05-27T16:10:09.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mancub&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mancub\">@<span class=\"\
          underline\">mancub</span></a></span>\n\n\t</span></span> thanks for the\
          \ reference, I somehow missed TheBloke's release of this model (I use his\
          \ other quantized models predominantly).  The inference speed of TheBloke's\
          \ quantized model is as fast or faster than the HF Spaces demo, so problem\
          \ solved!  The consumer cards are usually as fast or faster than the enterprise\
          \ versions, they just don't have the same VRAM capacity.  So, NVidia can\
          \ charge enterprise customers a lot more money and not have them using much\
          \ cheaper consumer cards.  Tim's qLora library, though, will let us lowly\
          \ consumers fine-tune and run much large models now, so we will have the\
          \ best of both worlds.</p>\n"
        raw: '@mancub thanks for the reference, I somehow missed TheBloke''s release
          of this model (I use his other quantized models predominantly).  The inference
          speed of TheBloke''s quantized model is as fast or faster than the HF Spaces
          demo, so problem solved!  The consumer cards are usually as fast or faster
          than the enterprise versions, they just don''t have the same VRAM capacity.  So,
          NVidia can charge enterprise customers a lot more money and not have them
          using much cheaper consumer cards.  Tim''s qLora library, though, will let
          us lowly consumers fine-tune and run much large models now, so we will have
          the best of both worlds.'
        updatedAt: '2023-05-27T16:10:09.009Z'
      numEdits: 0
      reactions: []
    id: 64722b610211f8527005bdad
    type: comment
  author: LoneStriker
  content: '@mancub thanks for the reference, I somehow missed TheBloke''s release
    of this model (I use his other quantized models predominantly).  The inference
    speed of TheBloke''s quantized model is as fast or faster than the HF Spaces demo,
    so problem solved!  The consumer cards are usually as fast or faster than the
    enterprise versions, they just don''t have the same VRAM capacity.  So, NVidia
    can charge enterprise customers a lot more money and not have them using much
    cheaper consumer cards.  Tim''s qLora library, though, will let us lowly consumers
    fine-tune and run much large models now, so we will have the best of both worlds.'
  created_at: 2023-05-27 15:10:09+00:00
  edited: false
  hidden: false
  id: 64722b610211f8527005bdad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-28T01:25:22.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I don''t know, maybe it''s my hardware setup then...but I can maybe
          get 5-6 t/s if I''m lucky on my 3090. I got a boatload of RAM as well as
          a dual Xeon v3 system, and load the entire model into VRAM.</p>

          <p>I find it that GGML performs much better here than GPTQ though (6-7 t/s
          vs 4-5 t/s respectively).</p>

          '
        raw: 'I don''t know, maybe it''s my hardware setup then...but I can maybe
          get 5-6 t/s if I''m lucky on my 3090. I got a boatload of RAM as well as
          a dual Xeon v3 system, and load the entire model into VRAM.


          I find it that GGML performs much better here than GPTQ though (6-7 t/s
          vs 4-5 t/s respectively).'
        updatedAt: '2023-05-28T01:25:22.112Z'
      numEdits: 0
      reactions: []
    id: 6472ad82c27f74a0ebaf90af
    type: comment
  author: mancub
  content: 'I don''t know, maybe it''s my hardware setup then...but I can maybe get
    5-6 t/s if I''m lucky on my 3090. I got a boatload of RAM as well as a dual Xeon
    v3 system, and load the entire model into VRAM.


    I find it that GGML performs much better here than GPTQ though (6-7 t/s vs 4-5
    t/s respectively).'
  created_at: 2023-05-28 00:25:22+00:00
  edited: false
  hidden: false
  id: 6472ad82c27f74a0ebaf90af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-05-28T04:54:41.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: "<blockquote>\n<p>I don't know, maybe it's my hardware setup then...but\
          \ I can maybe get 5-6 t/s if I'm lucky on my 3090. I got a boatload of RAM\
          \ as well as a dual Xeon v3 system, and load the entire model into VRAM.</p>\n\
          <p>I find it that GGML performs much better here than GPTQ though (6-7 t/s\
          \ vs 4-5 t/s respectively).</p>\n</blockquote>\n<p>Chatting with <span data-props=\"\
          {&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/TheBloke\">@<span class=\"underline\">TheBloke</span></a></span>\n\
          \n\t</span></span> on his Discord, he indicated that performance depends\
          \ primarily on single-thread CPU performance along with GPU speed.  I'm\
          \ running on an 13900K CPU.</p>\n"
        raw: "> I don't know, maybe it's my hardware setup then...but I can maybe\
          \ get 5-6 t/s if I'm lucky on my 3090. I got a boatload of RAM as well as\
          \ a dual Xeon v3 system, and load the entire model into VRAM.\n> \n> I find\
          \ it that GGML performs much better here than GPTQ though (6-7 t/s vs 4-5\
          \ t/s respectively).\n\nChatting with @TheBloke on his Discord, he indicated\
          \ that performance depends primarily on single-thread CPU performance along\
          \ with GPU speed.  I'm running on an 13900K CPU."
        updatedAt: '2023-05-28T04:54:41.983Z'
      numEdits: 0
      reactions: []
    id: 6472de9197a75cc77ac15b64
    type: comment
  author: LoneStriker
  content: "> I don't know, maybe it's my hardware setup then...but I can maybe get\
    \ 5-6 t/s if I'm lucky on my 3090. I got a boatload of RAM as well as a dual Xeon\
    \ v3 system, and load the entire model into VRAM.\n> \n> I find it that GGML performs\
    \ much better here than GPTQ though (6-7 t/s vs 4-5 t/s respectively).\n\nChatting\
    \ with @TheBloke on his Discord, he indicated that performance depends primarily\
    \ on single-thread CPU performance along with GPU speed.  I'm running on an 13900K\
    \ CPU."
  created_at: 2023-05-28 03:54:41+00:00
  edited: false
  hidden: false
  id: 6472de9197a75cc77ac15b64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-28T12:50:01.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah, unfortunately single core CPU performance is a bottleneck
          for pytorch / GPTQ inference.  At least when running one prompt at a time,
          as most of us do.</p>

          <p>For example I was getting 28 tokens/s with a 7B model on a 4090 GPU on
          an AMD EPYC 24-core server CPU.  Then I tried the same model on a 4090 on
          an i9-13900K, and got 98 tokens/s!</p>

          <p>The CPUs that I know that perform similarly well are:</p>

          <ul>

          <li>Intel i9-13900K</li>

          <li>Intel i7-13700k</li>

          <li>Ryzen 7950X and 7900X</li>

          </ul>

          <p>If you google "single core CPU benchmark" you''ll find that all these
          CPUs are right at the top.  And unfortunately that''s currently very important.</p>

          <p>Maybe in future there will be a way to do multithreading with pytorch
          models, even for single prompts.</p>

          <p>So yeah, it''s definitely worth testing GGML + GPU acceleration to see
          how it performs in comparison.</p>

          '
        raw: 'Yeah, unfortunately single core CPU performance is a bottleneck for
          pytorch / GPTQ inference.  At least when running one prompt at a time, as
          most of us do.


          For example I was getting 28 tokens/s with a 7B model on a 4090 GPU on an
          AMD EPYC 24-core server CPU.  Then I tried the same model on a 4090 on an
          i9-13900K, and got 98 tokens/s!


          The CPUs that I know that perform similarly well are:

          * Intel i9-13900K

          * Intel i7-13700k

          * Ryzen 7950X and 7900X


          If you google "single core CPU benchmark" you''ll find that all these CPUs
          are right at the top.  And unfortunately that''s currently very important.


          Maybe in future there will be a way to do multithreading with pytorch models,
          even for single prompts.


          So yeah, it''s definitely worth testing GGML + GPU acceleration to see how
          it performs in comparison.'
        updatedAt: '2023-05-28T12:50:01.727Z'
      numEdits: 0
      reactions: []
    id: 64734df963001a0002c95771
    type: comment
  author: TheBloke
  content: 'Yeah, unfortunately single core CPU performance is a bottleneck for pytorch
    / GPTQ inference.  At least when running one prompt at a time, as most of us do.


    For example I was getting 28 tokens/s with a 7B model on a 4090 GPU on an AMD
    EPYC 24-core server CPU.  Then I tried the same model on a 4090 on an i9-13900K,
    and got 98 tokens/s!


    The CPUs that I know that perform similarly well are:

    * Intel i9-13900K

    * Intel i7-13700k

    * Ryzen 7950X and 7900X


    If you google "single core CPU benchmark" you''ll find that all these CPUs are
    right at the top.  And unfortunately that''s currently very important.


    Maybe in future there will be a way to do multithreading with pytorch models,
    even for single prompts.


    So yeah, it''s definitely worth testing GGML + GPU acceleration to see how it
    performs in comparison.'
  created_at: 2023-05-28 11:50:01+00:00
  edited: false
  hidden: false
  id: 64734df963001a0002c95771
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-28T13:49:39.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>But is the issue here CPU/PCI bandwidth, clock frequency, or something
          else? </p>

          <p>It seems like it''s all about brute force, if anything. There''s no sophistication
          of any kind.</p>

          <p>What would be a benefit of running multiple prompts at the same time,
          and how could that even be done with a single GPU?</p>

          <p>I have two 3090s though using only one atm. I guess NVlinking them would
          make no difference with inference, and all I might get is more VRAM (48GB).
          And with QLoRA that does not matter as much now, does it?</p>

          '
        raw: "But is the issue here CPU/PCI bandwidth, clock frequency, or something\
          \ else? \n\nIt seems like it's all about brute force, if anything. There's\
          \ no sophistication of any kind.\n\nWhat would be a benefit of running multiple\
          \ prompts at the same time, and how could that even be done with a single\
          \ GPU?\n\nI have two 3090s though using only one atm. I guess NVlinking\
          \ them would make no difference with inference, and all I might get is more\
          \ VRAM (48GB). And with QLoRA that does not matter as much now, does it?"
        updatedAt: '2023-05-28T13:49:39.184Z'
      numEdits: 0
      reactions: []
    id: 64735bf3352c94a20dd48164
    type: comment
  author: mancub
  content: "But is the issue here CPU/PCI bandwidth, clock frequency, or something\
    \ else? \n\nIt seems like it's all about brute force, if anything. There's no\
    \ sophistication of any kind.\n\nWhat would be a benefit of running multiple prompts\
    \ at the same time, and how could that even be done with a single GPU?\n\nI have\
    \ two 3090s though using only one atm. I guess NVlinking them would make no difference\
    \ with inference, and all I might get is more VRAM (48GB). And with QLoRA that\
    \ does not matter as much now, does it?"
  created_at: 2023-05-28 12:49:39+00:00
  edited: false
  hidden: false
  id: 64735bf3352c94a20dd48164
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-05-28T14:01:48.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<blockquote>

          <p>But is the issue here CPU/PCI bandwidth, clock frequency, or something
          else? </p>

          <p>It seems like it''s all about brute force, if anything. There''s no sophistication
          of any kind.</p>

          <p>What would be a benefit of running multiple prompts at the same time,
          and how could that even be done with a single GPU?</p>

          <p>I have two 3090s though using only one atm. I guess NVlinking them would
          make no difference with inference, and all I might get is more VRAM (48GB).
          And with QLoRA that does not matter as much now, does it?</p>

          </blockquote>

          <p>Python is single-core by default because of the GIL.  At its heart, it''s
          an inherent Python limitation.  So, the best CPU in this case is one that
          can run a single, non-multithreaded application as fast as possible.</p>

          <p>You can run different prompts on different cores, but they would have
          to scheduled serially on the GPU, so limited value there.  NVlinking two
          3090s just gives you a fast interconnect between GPUs, but they are still
          two distinct GPUs.  You don''t get 48 GB VRAM for free without doing additional
          work to split your model across GPUs as far as I''m aware.  A real 48 GB
          VRAM GPU can run bigger models much faster than 2x 24 GB VRAM GPUs. I can''t
          run the 65B 4-bit GPTQ Guanaco model for example on my 2x 4090s at any reasonable
          speed (only get 1-2 tokens/second.)  If you want to run inference or train
          the 65B model using QLoRA, you''ll need a real 48 GB VRAM GPU.</p>

          '
        raw: "> But is the issue here CPU/PCI bandwidth, clock frequency, or something\
          \ else? \n> \n> It seems like it's all about brute force, if anything. There's\
          \ no sophistication of any kind.\n> \n> What would be a benefit of running\
          \ multiple prompts at the same time, and how could that even be done with\
          \ a single GPU?\n> \n> I have two 3090s though using only one atm. I guess\
          \ NVlinking them would make no difference with inference, and all I might\
          \ get is more VRAM (48GB). And with QLoRA that does not matter as much now,\
          \ does it?\n\nPython is single-core by default because of the GIL.  At its\
          \ heart, it's an inherent Python limitation.  So, the best CPU in this case\
          \ is one that can run a single, non-multithreaded application as fast as\
          \ possible.\n\nYou can run different prompts on different cores, but they\
          \ would have to scheduled serially on the GPU, so limited value there. \
          \ NVlinking two 3090s just gives you a fast interconnect between GPUs, but\
          \ they are still two distinct GPUs.  You don't get 48 GB VRAM for free without\
          \ doing additional work to split your model across GPUs as far as I'm aware.\
          \  A real 48 GB VRAM GPU can run bigger models much faster than 2x 24 GB\
          \ VRAM GPUs. I can't run the 65B 4-bit GPTQ Guanaco model for example on\
          \ my 2x 4090s at any reasonable speed (only get 1-2 tokens/second.)  If\
          \ you want to run inference or train the 65B model using QLoRA, you'll need\
          \ a real 48 GB VRAM GPU."
        updatedAt: '2023-05-28T14:01:48.867Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - mancub
        - luketow
    id: 64735ecc2a74fb43ccdd613a
    type: comment
  author: LoneStriker
  content: "> But is the issue here CPU/PCI bandwidth, clock frequency, or something\
    \ else? \n> \n> It seems like it's all about brute force, if anything. There's\
    \ no sophistication of any kind.\n> \n> What would be a benefit of running multiple\
    \ prompts at the same time, and how could that even be done with a single GPU?\n\
    > \n> I have two 3090s though using only one atm. I guess NVlinking them would\
    \ make no difference with inference, and all I might get is more VRAM (48GB).\
    \ And with QLoRA that does not matter as much now, does it?\n\nPython is single-core\
    \ by default because of the GIL.  At its heart, it's an inherent Python limitation.\
    \  So, the best CPU in this case is one that can run a single, non-multithreaded\
    \ application as fast as possible.\n\nYou can run different prompts on different\
    \ cores, but they would have to scheduled serially on the GPU, so limited value\
    \ there.  NVlinking two 3090s just gives you a fast interconnect between GPUs,\
    \ but they are still two distinct GPUs.  You don't get 48 GB VRAM for free without\
    \ doing additional work to split your model across GPUs as far as I'm aware. \
    \ A real 48 GB VRAM GPU can run bigger models much faster than 2x 24 GB VRAM GPUs.\
    \ I can't run the 65B 4-bit GPTQ Guanaco model for example on my 2x 4090s at any\
    \ reasonable speed (only get 1-2 tokens/second.)  If you want to run inference\
    \ or train the 65B model using QLoRA, you'll need a real 48 GB VRAM GPU."
  created_at: 2023-05-28 13:01:48+00:00
  edited: false
  hidden: false
  id: 64735ecc2a74fb43ccdd613a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-28T14:39:49.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Regarding multiple prompts: that's something you might do if you\
          \ were processing data in bulk.  For example, if you were wanted an LLM\
          \ to summarise or write replies to 1000 emails, or summarise articles for\
          \ you, or whatever.  That sort of thing.  It's not relevant to the average\
          \ user who wants to do ChatGPT style chatting.</p>\n<p>Here's some example\
          \ code that makes use of that:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >pipeline</span>(<span class=\"hljs-params\">self, prompts, batch_size=<span\
          \ class=\"hljs-number\">1</span></span>):\n        <span class=\"hljs-keyword\"\
          >if</span> <span class=\"hljs-keyword\">not</span> self.pipe:\n        \
          \    <span class=\"hljs-comment\"># Prevent printing spurious transformers\
          \ error when using pipeline with AutoGPTQ</span>\n            logging.set_verbosity(logging.CRITICAL)\n\
          \            self.pipe = pipeline(\n                <span class=\"hljs-string\"\
          >\"text-generation\"</span>,\n                model=self.model,\n      \
          \          tokenizer=self.tokenizer,\n                generation_config=self.generation_config,\n\
          \                device=self.device\n            )\n        self.update_seed()\n\
          \        answers = []\n        <span class=\"hljs-keyword\">with</span>\
          \ self.do_timing(<span class=\"hljs-literal\">True</span>) <span class=\"\
          hljs-keyword\">as</span> timing:\n            <span class=\"hljs-keyword\"\
          >with</span> torch.no_grad():\n                <span class=\"hljs-comment\"\
          ># <span class=\"hljs-doctag\">TODO:</span> batch_size &gt;1 causes gibberish\
          \ output, investigate</span>\n                output = self.pipe(prompts,\
          \ return_tensors=<span class=\"hljs-literal\">True</span>, batch_size=batch_size)\n\
          \n            <span class=\"hljs-keyword\">for</span> index, gen <span class=\"\
          hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(output):\n\
          \                tokens = gen[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">'generated_token_ids'</span>]\n                input_ids,\
          \ len_input_ids = self.encode(prompts[index])\n                len_reply\
          \ = <span class=\"hljs-built_in\">len</span>(tokens) + <span class=\"hljs-number\"\
          >1</span> - len_input_ids\n                response = self.decode(tokens)\n\
          \                reply_tokens = tokens[-len_reply:]\n                reply\
          \ = self.tokenizer.decode(reply_tokens)\n\n                result = {\n\
          \                    <span class=\"hljs-string\">'response'</span>: response,\
          \   <span class=\"hljs-comment\"># The response in full, including prompt</span>\n\
          \                    <span class=\"hljs-string\">'reply'</span>: reply,\
          \         <span class=\"hljs-comment\"># Just the reply, no prompt</span>\n\
          \                    <span class=\"hljs-string\">'len_reply'</span>: len_reply,\
          \ <span class=\"hljs-comment\"># The length of the reply tokens</span>\n\
          \                    <span class=\"hljs-string\">'seed'</span>: self.seed,\
          \      <span class=\"hljs-comment\"># The seed used to generate this response</span>\n\
          \                    <span class=\"hljs-string\">'time'</span>: timing[<span\
          \ class=\"hljs-string\">'time'</span>]  <span class=\"hljs-comment\"># The\
          \ time in seconds to generate the response</span>\n                }\n \
          \               answers.append(result)\n\n        <span class=\"hljs-keyword\"\
          >return</span> answers\n</code></pre>\n<p>With <code>batch_size</code> set\
          \ to  X (where X is &gt;1), and with <code>prompts</code> being a List of\
          \ multiple prompts, this will process them X prompts at a time.  This enables\
          \ using 100% of the GPU in situations where a single prompt would use only\
          \ a fraction of that.</p>\n<p>There are some complexities though.  If you\
          \ run that code as-is with a bunch of varied prompts, you will likely find\
          \ that the outputs are partially gibberish.  At least that's what I found.</p>\n\
          <p>In order for it to work properly, the prompts have to be padded to all\
          \ be the same length.  I never got as far as writing code to do that, but\
          \ there are examples in the Hugging Face docs and elsewhere.</p>\n<p>If\
          \ that's done correctly the result should be much faster performance, despite\
          \ the single-core performance limit.</p>\n<p>But again, it doesn't really\
          \ help for the average use case of just wanting to infer one prompt at a\
          \ time.</p>\n"
        raw: "Regarding multiple prompts: that's something you might do if you were\
          \ processing data in bulk.  For example, if you were wanted an LLM to summarise\
          \ or write replies to 1000 emails, or summarise articles for you, or whatever.\
          \  That sort of thing.  It's not relevant to the average user who wants\
          \ to do ChatGPT style chatting.\n\nHere's some example code that makes use\
          \ of that:\n```python\ndef pipeline(self, prompts, batch_size=1):\n    \
          \    if not self.pipe:\n            # Prevent printing spurious transformers\
          \ error when using pipeline with AutoGPTQ\n            logging.set_verbosity(logging.CRITICAL)\n\
          \            self.pipe = pipeline(\n                \"text-generation\"\
          ,\n                model=self.model,\n                tokenizer=self.tokenizer,\n\
          \                generation_config=self.generation_config,\n           \
          \     device=self.device\n            )\n        self.update_seed()\n  \
          \      answers = []\n        with self.do_timing(True) as timing:\n    \
          \        with torch.no_grad():\n                # TODO: batch_size >1 causes\
          \ gibberish output, investigate\n                output = self.pipe(prompts,\
          \ return_tensors=True, batch_size=batch_size)\n\n            for index,\
          \ gen in enumerate(output):\n                tokens = gen[0]['generated_token_ids']\n\
          \                input_ids, len_input_ids = self.encode(prompts[index])\n\
          \                len_reply = len(tokens) + 1 - len_input_ids\n         \
          \       response = self.decode(tokens)\n                reply_tokens = tokens[-len_reply:]\n\
          \                reply = self.tokenizer.decode(reply_tokens)\n\n       \
          \         result = {\n                    'response': response,   # The\
          \ response in full, including prompt\n                    'reply': reply,\
          \         # Just the reply, no prompt\n                    'len_reply':\
          \ len_reply, # The length of the reply tokens\n                    'seed':\
          \ self.seed,      # The seed used to generate this response\n          \
          \          'time': timing['time']  # The time in seconds to generate the\
          \ response\n                }\n                answers.append(result)\n\n\
          \        return answers\n```\n\nWith `batch_size` set to  X (where X is\
          \ >1), and with `prompts` being a List of multiple prompts, this will process\
          \ them X prompts at a time.  This enables using 100% of the GPU in situations\
          \ where a single prompt would use only a fraction of that.\n\nThere are\
          \ some complexities though.  If you run that code as-is with a bunch of\
          \ varied prompts, you will likely find that the outputs are partially gibberish.\
          \  At least that's what I found.\n\nIn order for it to work properly, the\
          \ prompts have to be padded to all be the same length.  I never got as far\
          \ as writing code to do that, but there are examples in the Hugging Face\
          \ docs and elsewhere.\n\nIf that's done correctly the result should be much\
          \ faster performance, despite the single-core performance limit.\n\nBut\
          \ again, it doesn't really help for the average use case of just wanting\
          \ to infer one prompt at a time."
        updatedAt: '2023-05-28T14:39:49.814Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - mancub
        - abdullah
        - luketow
    id: 647367b52a74fb43ccddffc0
    type: comment
  author: TheBloke
  content: "Regarding multiple prompts: that's something you might do if you were\
    \ processing data in bulk.  For example, if you were wanted an LLM to summarise\
    \ or write replies to 1000 emails, or summarise articles for you, or whatever.\
    \  That sort of thing.  It's not relevant to the average user who wants to do\
    \ ChatGPT style chatting.\n\nHere's some example code that makes use of that:\n\
    ```python\ndef pipeline(self, prompts, batch_size=1):\n        if not self.pipe:\n\
    \            # Prevent printing spurious transformers error when using pipeline\
    \ with AutoGPTQ\n            logging.set_verbosity(logging.CRITICAL)\n       \
    \     self.pipe = pipeline(\n                \"text-generation\",\n          \
    \      model=self.model,\n                tokenizer=self.tokenizer,\n        \
    \        generation_config=self.generation_config,\n                device=self.device\n\
    \            )\n        self.update_seed()\n        answers = []\n        with\
    \ self.do_timing(True) as timing:\n            with torch.no_grad():\n       \
    \         # TODO: batch_size >1 causes gibberish output, investigate\n       \
    \         output = self.pipe(prompts, return_tensors=True, batch_size=batch_size)\n\
    \n            for index, gen in enumerate(output):\n                tokens = gen[0]['generated_token_ids']\n\
    \                input_ids, len_input_ids = self.encode(prompts[index])\n    \
    \            len_reply = len(tokens) + 1 - len_input_ids\n                response\
    \ = self.decode(tokens)\n                reply_tokens = tokens[-len_reply:]\n\
    \                reply = self.tokenizer.decode(reply_tokens)\n\n             \
    \   result = {\n                    'response': response,   # The response in\
    \ full, including prompt\n                    'reply': reply,         # Just the\
    \ reply, no prompt\n                    'len_reply': len_reply, # The length of\
    \ the reply tokens\n                    'seed': self.seed,      # The seed used\
    \ to generate this response\n                    'time': timing['time']  # The\
    \ time in seconds to generate the response\n                }\n              \
    \  answers.append(result)\n\n        return answers\n```\n\nWith `batch_size`\
    \ set to  X (where X is >1), and with `prompts` being a List of multiple prompts,\
    \ this will process them X prompts at a time.  This enables using 100% of the\
    \ GPU in situations where a single prompt would use only a fraction of that.\n\
    \nThere are some complexities though.  If you run that code as-is with a bunch\
    \ of varied prompts, you will likely find that the outputs are partially gibberish.\
    \  At least that's what I found.\n\nIn order for it to work properly, the prompts\
    \ have to be padded to all be the same length.  I never got as far as writing\
    \ code to do that, but there are examples in the Hugging Face docs and elsewhere.\n\
    \nIf that's done correctly the result should be much faster performance, despite\
    \ the single-core performance limit.\n\nBut again, it doesn't really help for\
    \ the average use case of just wanting to infer one prompt at a time."
  created_at: 2023-05-28 13:39:49+00:00
  edited: false
  hidden: false
  id: 647367b52a74fb43ccddffc0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-29T00:14:37.000Z'
    data:
      edited: true
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Bah humbug...well A6000 is a bit out of reach for me. :)</p>

          <p>Sounds like that QLoRA will be the only saving grace for single consumer
          GPU setups, coupled with the latest generation CPUs and a fair amount of
          RAM.</p>

          <p>As an aside question, what should I be seeing my CPU/GPU usage be during
          inference?</p>

          <p>Right now with both GPTQ and GGML models I see &lt; 7% use on either
          CPU or GPU, is that normal?</p>

          '
        raw: 'Bah humbug...well A6000 is a bit out of reach for me. :)


          Sounds like that QLoRA will be the only saving grace for single consumer
          GPU setups, coupled with the latest generation CPUs and a fair amount of
          RAM.


          As an aside question, what should I be seeing my CPU/GPU usage be during
          inference?


          Right now with both GPTQ and GGML models I see < 7% use on either CPU or
          GPU, is that normal?'
        updatedAt: '2023-05-29T00:14:46.187Z'
      numEdits: 1
      reactions: []
    id: 6473ee6d63001a0002d3c10a
    type: comment
  author: mancub
  content: 'Bah humbug...well A6000 is a bit out of reach for me. :)


    Sounds like that QLoRA will be the only saving grace for single consumer GPU setups,
    coupled with the latest generation CPUs and a fair amount of RAM.


    As an aside question, what should I be seeing my CPU/GPU usage be during inference?


    Right now with both GPTQ and GGML models I see < 7% use on either CPU or GPU,
    is that normal?'
  created_at: 2023-05-28 23:14:37+00:00
  edited: true
  hidden: false
  id: 6473ee6d63001a0002d3c10a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-29T02:26:15.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>By the way, have you seen this: <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/pull/1607\"\
          >https://github.com/ggerganov/llama.cpp/pull/1607</a> - might be worth testing\
          \ on your dual 4090.</p>\n"
        raw: "@LoneStriker \n\nBy the way, have you seen this: https://github.com/ggerganov/llama.cpp/pull/1607\
          \ - might be worth testing on your dual 4090."
        updatedAt: '2023-05-29T02:26:15.015Z'
      numEdits: 0
      reactions: []
    id: 64740d472a74fb43cce92eba
    type: comment
  author: mancub
  content: "@LoneStriker \n\nBy the way, have you seen this: https://github.com/ggerganov/llama.cpp/pull/1607\
    \ - might be worth testing on your dual 4090."
  created_at: 2023-05-29 01:26:15+00:00
  edited: false
  hidden: false
  id: 64740d472a74fb43cce92eba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-05-29T04:23:34.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<blockquote>

          <p>Bah humbug...well A6000 is a bit out of reach for me. :)</p>

          <p>Sounds like that QLoRA will be the only saving grace for single consumer
          GPU setups, coupled with the latest generation CPUs and a fair amount of
          RAM.</p>

          <p>As an aside question, what should I be seeing my CPU/GPU usage be during
          inference?</p>

          <p>Right now with both GPTQ and GGML models I see &lt; 7% use on either
          CPU or GPU, is that normal?</p>

          </blockquote>

          <p>CPU is generally always 100% on at least one core for gptq inference.
          I don''t usually use ggml as it''s slower than gptq models by a factor of
          2x using GPU. On my box with Intel 13900K CPU, the 4090 is running at 100%.
          For my box with AMD 3700X, the 3090 only gets to 60-75% GPU. So I''m bottlenecked
          on the AMD box by the slower CPU.</p>

          '
        raw: "> Bah humbug...well A6000 is a bit out of reach for me. :)\n> \n> Sounds\
          \ like that QLoRA will be the only saving grace for single consumer GPU\
          \ setups, coupled with the latest generation CPUs and a fair amount of RAM.\n\
          > \n> As an aside question, what should I be seeing my CPU/GPU usage be\
          \ during inference?\n> \n> Right now with both GPTQ and GGML models I see\
          \ < 7% use on either CPU or GPU, is that normal?\n\nCPU is generally always\
          \ 100% on at least one core for gptq inference. I don't usually use ggml\
          \ as it's slower than gptq models by a factor of 2x using GPU. On my box\
          \ with Intel 13900K CPU, the 4090 is running at 100%. For my box with AMD\
          \ 3700X, the 3090 only gets to 60-75% GPU. So I'm bottlenecked on the AMD\
          \ box by the slower CPU."
        updatedAt: '2023-05-29T04:23:34.600Z'
      numEdits: 0
      reactions: []
    id: 647428c663001a0002d8064b
    type: comment
  author: LoneStriker
  content: "> Bah humbug...well A6000 is a bit out of reach for me. :)\n> \n> Sounds\
    \ like that QLoRA will be the only saving grace for single consumer GPU setups,\
    \ coupled with the latest generation CPUs and a fair amount of RAM.\n> \n> As\
    \ an aside question, what should I be seeing my CPU/GPU usage be during inference?\n\
    > \n> Right now with both GPTQ and GGML models I see < 7% use on either CPU or\
    \ GPU, is that normal?\n\nCPU is generally always 100% on at least one core for\
    \ gptq inference. I don't usually use ggml as it's slower than gptq models by\
    \ a factor of 2x using GPU. On my box with Intel 13900K CPU, the 4090 is running\
    \ at 100%. For my box with AMD 3700X, the 3090 only gets to 60-75% GPU. So I'm\
    \ bottlenecked on the AMD box by the slower CPU."
  created_at: 2023-05-29 03:23:34+00:00
  edited: false
  hidden: false
  id: 647428c663001a0002d8064b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-05-29T04:25:47.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>By the way, have you seen this: <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/pull/1607\"\
          >https://github.com/ggerganov/llama.cpp/pull/1607</a> - might be worth testing\
          \ on your dual 4090.</p>\n</blockquote>\n<p>Thanks for the reference. I\
          \ think llama.cpp will be a serious contender in the future. But it currently\
          \ can't beat gptq if your model fits in VRAM. It's faster though when it\
          \ doesn't fit, but the inference speed on a 65B model on LLaMA.cpp is too\
          \ slow to be useful by an order of magnitude.</p>\n"
        raw: "> @LoneStriker \n> \n> By the way, have you seen this: https://github.com/ggerganov/llama.cpp/pull/1607\
          \ - might be worth testing on your dual 4090.\n\nThanks for the reference.\
          \ I think llama.cpp will be a serious contender in the future. But it currently\
          \ can't beat gptq if your model fits in VRAM. It's faster though when it\
          \ doesn't fit, but the inference speed on a 65B model on LLaMA.cpp is too\
          \ slow to be useful by an order of magnitude."
        updatedAt: '2023-05-29T04:25:47.559Z'
      numEdits: 0
      reactions: []
    id: 6474294b63001a0002d80cee
    type: comment
  author: LoneStriker
  content: "> @LoneStriker \n> \n> By the way, have you seen this: https://github.com/ggerganov/llama.cpp/pull/1607\
    \ - might be worth testing on your dual 4090.\n\nThanks for the reference. I think\
    \ llama.cpp will be a serious contender in the future. But it currently can't\
    \ beat gptq if your model fits in VRAM. It's faster though when it doesn't fit,\
    \ but the inference speed on a 65B model on LLaMA.cpp is too slow to be useful\
    \ by an order of magnitude."
  created_at: 2023-05-29 03:25:47+00:00
  edited: false
  hidden: false
  id: 6474294b63001a0002d80cee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-29T23:52:34.000Z'
    data:
      edited: true
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I guess I''m seriously bottlenecked by the Xeon E5v3 then but llama.cpp
          is getting better and better.  GPTQ might not have a future for me in that
          case, as it won''t be worth while for me to upgrade to a latest gen CPU.
          It''s hard to tell when so many things are in motion at the moment and new
          improvements are coming out every day (QLoRA for example).</p>

          <p>Or maybe what''s bottlenecking me even more is Windows and WSL.</p>

          '
        raw: 'I guess I''m seriously bottlenecked by the Xeon E5v3 then but llama.cpp
          is getting better and better.  GPTQ might not have a future for me in that
          case, as it won''t be worth while for me to upgrade to a latest gen CPU.
          It''s hard to tell when so many things are in motion at the moment and new
          improvements are coming out every day (QLoRA for example).


          Or maybe what''s bottlenecking me even more is Windows and WSL.'
        updatedAt: '2023-05-29T23:53:19.334Z'
      numEdits: 1
      reactions: []
    id: 64753ac2f9e3e0b312f6b2b3
    type: comment
  author: mancub
  content: 'I guess I''m seriously bottlenecked by the Xeon E5v3 then but llama.cpp
    is getting better and better.  GPTQ might not have a future for me in that case,
    as it won''t be worth while for me to upgrade to a latest gen CPU. It''s hard
    to tell when so many things are in motion at the moment and new improvements are
    coming out every day (QLoRA for example).


    Or maybe what''s bottlenecking me even more is Windows and WSL.'
  created_at: 2023-05-29 22:52:34+00:00
  edited: true
  hidden: false
  id: 64753ac2f9e3e0b312f6b2b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-30T00:18:35.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah WSL is definitely bottlenecking you I''m afraid, based on what
          i''ve heard from several people using it.</p>

          '
        raw: Yeah WSL is definitely bottlenecking you I'm afraid, based on what i've
          heard from several people using it.
        updatedAt: '2023-05-30T00:18:35.608Z'
      numEdits: 0
      reactions: []
    id: 647540dbf9e3e0b312f709c7
    type: comment
  author: TheBloke
  content: Yeah WSL is definitely bottlenecking you I'm afraid, based on what i've
    heard from several people using it.
  created_at: 2023-05-29 23:18:35+00:00
  edited: false
  hidden: false
  id: 647540dbf9e3e0b312f709c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-05-30T00:26:06.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>I''m having no problem with WSL besides that Windows only gives
          it half the system RAM</p>

          '
        raw: I'm having no problem with WSL besides that Windows only gives it half
          the system RAM
        updatedAt: '2023-05-30T00:26:06.306Z'
      numEdits: 0
      reactions: []
    id: 6475429e5ada8510bc4b1727
    type: comment
  author: ehartford
  content: I'm having no problem with WSL besides that Windows only gives it half
    the system RAM
  created_at: 2023-05-29 23:26:06+00:00
  edited: false
  hidden: false
  id: 6475429e5ada8510bc4b1727
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-31T00:10:27.000Z'
    data:
      edited: true
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ehartford&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ehartford\">@<span class=\"\
          underline\">ehartford</span></a></span>\n\n\t</span></span> </p>\n<p>Have\
          \ you increased the available RAM to WSL via the .wslconfig file. I wrote\
          \ about that in another post here on HF - by creating C:\\Users\\[username]\\\
          .wslconfig you can give it more RAM, more cores, etc if needed, but remember\
          \ that it needs a full restart of the service for the changes to take effect.</p>\n\
          <p>Here's mine for example:</p>\n<p>[wsl2]<br>memory=128GB<br>swap=0<br>localhostForwarding=true<br>processors=16</p>\n\
          <p>EDIT: had to edit the post couple of times because backslashes were missing.\
          \ My WSL2 is in Windows 10, maybe it works better in Windows 11?</p>\n"
        raw: "@ehartford \n\nHave you increased the available RAM to WSL via the .wslconfig\
          \ file. I wrote about that in another post here on HF - by creating C:\\\
          \\Users\\\\[username]\\\\.wslconfig you can give it more RAM, more cores,\
          \ etc if needed, but remember that it needs a full restart of the service\
          \ for the changes to take effect.\n\nHere's mine for example:\n\n[wsl2]\n\
          memory=128GB\nswap=0\nlocalhostForwarding=true\nprocessors=16\n\n\nEDIT:\
          \ had to edit the post couple of times because backslashes were missing.\
          \ My WSL2 is in Windows 10, maybe it works better in Windows 11?"
        updatedAt: '2023-05-31T00:13:48.662Z'
      numEdits: 8
      reactions: []
    id: 647690736f0553f1b4fa8b07
    type: comment
  author: mancub
  content: "@ehartford \n\nHave you increased the available RAM to WSL via the .wslconfig\
    \ file. I wrote about that in another post here on HF - by creating C:\\\\Users\\\
    \\[username]\\\\.wslconfig you can give it more RAM, more cores, etc if needed,\
    \ but remember that it needs a full restart of the service for the changes to\
    \ take effect.\n\nHere's mine for example:\n\n[wsl2]\nmemory=128GB\nswap=0\nlocalhostForwarding=true\n\
    processors=16\n\n\nEDIT: had to edit the post couple of times because backslashes\
    \ were missing. My WSL2 is in Windows 10, maybe it works better in Windows 11?"
  created_at: 2023-05-30 23:10:27+00:00
  edited: true
  hidden: false
  id: 647690736f0553f1b4fa8b07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6de9698f57bbf9ed46a2823a94caf44.svg
      fullname: Surguchev Alexander
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xsanf
      type: user
    createdAt: '2023-06-02T21:47:37.000Z'
    data:
      edited: false
      editors:
      - Xsanf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9543178081512451
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6de9698f57bbf9ed46a2823a94caf44.svg
          fullname: Surguchev Alexander
          isHf: false
          isPro: false
          name: Xsanf
          type: user
        html: '<p>To the question about llamacpp. I tried <a rel="nofollow" href="https://github.com/SciSharp/LLamaSharp">https://github.com/SciSharp/LLamaSharp</a>.
          Under Visual Studio C#  it works fine with GPU. Also uses all CPU cores.
          It was possible to use it in Unity, although crutches were required, since
          Unity uses NET 4.2 and LLamaSharp NET 6.0.<br>This is much more convenient
          for development than linking an application to an HTTP server in Python.
          Unfortunately, LLamaSharp does not yet support all the models that interest
          me, but the approach itself is interesting.</p>

          '
        raw: 'To the question about llamacpp. I tried https://github.com/SciSharp/LLamaSharp.
          Under Visual Studio C#  it works fine with GPU. Also uses all CPU cores.
          It was possible to use it in Unity, although crutches were required, since
          Unity uses NET 4.2 and LLamaSharp NET 6.0.

          This is much more convenient for development than linking an application
          to an HTTP server in Python. Unfortunately, LLamaSharp does not yet support
          all the models that interest me, but the approach itself is interesting.'
        updatedAt: '2023-06-02T21:47:37.871Z'
      numEdits: 0
      reactions: []
    id: 647a637942abe277475d4cbd
    type: comment
  author: Xsanf
  content: 'To the question about llamacpp. I tried https://github.com/SciSharp/LLamaSharp.
    Under Visual Studio C#  it works fine with GPU. Also uses all CPU cores. It was
    possible to use it in Unity, although crutches were required, since Unity uses
    NET 4.2 and LLamaSharp NET 6.0.

    This is much more convenient for development than linking an application to an
    HTTP server in Python. Unfortunately, LLamaSharp does not yet support all the
    models that interest me, but the approach itself is interesting.'
  created_at: 2023-06-02 20:47:37+00:00
  edited: false
  hidden: false
  id: 647a637942abe277475d4cbd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: timdettmers/guanaco-33b-merged
repo_type: model
status: open
target_branch: null
title: Whats the difference?
