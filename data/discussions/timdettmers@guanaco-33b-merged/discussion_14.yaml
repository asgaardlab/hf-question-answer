!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AIReach
conflicting_files: null
created_at: 2023-06-19 08:10:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
      fullname: andrewxu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIReach
      type: user
    createdAt: '2023-06-19T09:10:20.000Z'
    data:
      edited: false
      editors:
      - AIReach
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.680149257183075
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
          fullname: andrewxu
          isHf: false
          isPro: false
          name: AIReach
          type: user
        html: '<p>thanks for your work. I compared a lot llms and find this one is
          has good performance with low cost. but when i try to generate 512 tokens
          or 1024 tokens, both 33b and 65b versions will failed and my linux system
          will freeze.<br>i am currently using two 4090 with newest driver 525. any
          comments will be appreciated.</p>

          <p>following is my inference config</p>

          <p>nf4_config = BitsAndBytesConfig(<br>   load_in_4bit=True,<br>   bnb_4bit_quant_type="nf4",<br>   bnb_4bit_use_double_quant=True,<br>   bnb_4bit_compute_dtype=torch.float16<br>)</p>

          <p>config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)<br>model
          = AutoModelForCausalLM.from_pretrained(model_path,<br>                                            quantization_config=nf4_config,<br>                                            trust_remote_code=True,<br>                                            torch_dtype=torch.float16,
          # additional option to lower RAM consumtion<br>                                            device_map={"":
          1} #used for 33b version<br>                                            #device_map=''auto''
          #used for 65b version<br>                                            )</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)</p>

          <p>input_text = ''system: you are AI model. help user and responde carefully
          with long generation.\n <br>user:what is y-90 liver treatment. responde
          with 1000 words. \n <br>AI:''<br>input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to(''cuda'')</p>

          <p>outputs = model.generate(input_ids, max_length=1024, top_k=10, temperature=0.5)<br>print(tokenizer.decode(outputs[0],
          skip_special_tokens=True))</p>

          '
        raw: "thanks for your work. I compared a lot llms and find this one is has\
          \ good performance with low cost. but when i try to generate 512 tokens\
          \ or 1024 tokens, both 33b and 65b versions will failed and my linux system\
          \ will freeze. \r\ni am currently using two 4090 with newest driver 525.\
          \ any comments will be appreciated.\r\n\r\nfollowing is my inference config\r\
          \n\r\n\r\nnf4_config = BitsAndBytesConfig(\r\n   load_in_4bit=True,\r\n\
          \   bnb_4bit_quant_type=\"nf4\",\r\n   bnb_4bit_use_double_quant=True,\r\
          \n   bnb_4bit_compute_dtype=torch.float16\r\n)\r\n\r\nconfig = AutoConfig.from_pretrained(model_path,\
          \ trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\r\
          \n                                            quantization_config=nf4_config,\r\
          \n                                            trust_remote_code=True,\r\n\
          \                                            torch_dtype=torch.float16,\
          \ # additional option to lower RAM consumtion\r\n                      \
          \                      device_map={\"\": 1} #used for 33b version\r\n  \
          \                                          #device_map='auto' #used for\
          \ 65b version\r\n                                            )\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_path, use_fast=False)\r\n\r\ninput_text\
          \ = 'system: you are AI model. help user and responde carefully with long\
          \ generation.\\n \\\r\nuser:what is y-90 liver treatment. responde with\
          \ 1000 words. \\n \\\r\nAI:'\r\ninput_ids = tokenizer(input_text, return_tensors=\"\
          pt\").input_ids.to('cuda')\r\n                \r\noutputs = model.generate(input_ids,\
          \ max_length=1024, top_k=10, temperature=0.5)\r\nprint(tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True))"
        updatedAt: '2023-06-19T09:10:20.523Z'
      numEdits: 0
      reactions: []
    id: 64901b7c4d63d2ff65bfc9a2
    type: comment
  author: AIReach
  content: "thanks for your work. I compared a lot llms and find this one is has good\
    \ performance with low cost. but when i try to generate 512 tokens or 1024 tokens,\
    \ both 33b and 65b versions will failed and my linux system will freeze. \r\n\
    i am currently using two 4090 with newest driver 525. any comments will be appreciated.\r\
    \n\r\nfollowing is my inference config\r\n\r\n\r\nnf4_config = BitsAndBytesConfig(\r\
    \n   load_in_4bit=True,\r\n   bnb_4bit_quant_type=\"nf4\",\r\n   bnb_4bit_use_double_quant=True,\r\
    \n   bnb_4bit_compute_dtype=torch.float16\r\n)\r\n\r\nconfig = AutoConfig.from_pretrained(model_path,\
    \ trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\r\
    \n                                            quantization_config=nf4_config,\r\
    \n                                            trust_remote_code=True,\r\n    \
    \                                        torch_dtype=torch.float16, # additional\
    \ option to lower RAM consumtion\r\n                                         \
    \   device_map={\"\": 1} #used for 33b version\r\n                           \
    \                 #device_map='auto' #used for 65b version\r\n               \
    \                             )\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_path,\
    \ use_fast=False)\r\n\r\ninput_text = 'system: you are AI model. help user and\
    \ responde carefully with long generation.\\n \\\r\nuser:what is y-90 liver treatment.\
    \ responde with 1000 words. \\n \\\r\nAI:'\r\ninput_ids = tokenizer(input_text,\
    \ return_tensors=\"pt\").input_ids.to('cuda')\r\n                \r\noutputs =\
    \ model.generate(input_ids, max_length=1024, top_k=10, temperature=0.5)\r\nprint(tokenizer.decode(outputs[0],\
    \ skip_special_tokens=True))"
  created_at: 2023-06-19 08:10:20+00:00
  edited: false
  hidden: false
  id: 64901b7c4d63d2ff65bfc9a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
      fullname: andrewxu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIReach
      type: user
    createdAt: '2023-06-20T06:38:51.000Z'
    data:
      edited: false
      editors:
      - AIReach
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9243264198303223
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
          fullname: andrewxu
          isHf: false
          isPro: false
          name: AIReach
          type: user
        html: '<p>I tried apply guanaco-33b adapter to llama-30b on my machine locally.
          Same issue happened. By a forced long generation, the system will freeze.<br>Good
          news is the issue could be prevented by using only 8bit quantization. Avoid
          NF4 quantization could solve this problem. bad news, in another word 33b
          model could not be deployed on single 24GB GPU. 48GB will be suggested.</p>

          '
        raw: "I tried apply guanaco-33b adapter to llama-30b on my machine locally.\
          \ Same issue happened. By a forced long generation, the system will freeze.\
          \ \nGood news is the issue could be prevented by using only 8bit quantization.\
          \ Avoid NF4 quantization could solve this problem. bad news, in another\
          \ word 33b model could not be deployed on single 24GB GPU. 48GB will be\
          \ suggested."
        updatedAt: '2023-06-20T06:38:51.943Z'
      numEdits: 0
      reactions: []
    id: 6491497b990d53d751188d22
    type: comment
  author: AIReach
  content: "I tried apply guanaco-33b adapter to llama-30b on my machine locally.\
    \ Same issue happened. By a forced long generation, the system will freeze. \n\
    Good news is the issue could be prevented by using only 8bit quantization. Avoid\
    \ NF4 quantization could solve this problem. bad news, in another word 33b model\
    \ could not be deployed on single 24GB GPU. 48GB will be suggested."
  created_at: 2023-06-20 05:38:51+00:00
  edited: false
  hidden: false
  id: 6491497b990d53d751188d22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
      fullname: andrewxu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIReach
      type: user
    createdAt: '2023-06-20T06:40:08.000Z'
    data:
      edited: false
      editors:
      - AIReach
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8698171973228455
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
          fullname: andrewxu
          isHf: false
          isPro: false
          name: AIReach
          type: user
        html: '<p>another test. 4bit on vicuna-13b works fast without this issue.</p>

          '
        raw: another test. 4bit on vicuna-13b works fast without this issue.
        updatedAt: '2023-06-20T06:40:08.000Z'
      numEdits: 0
      reactions: []
    id: 649149c8990d53d751189705
    type: comment
  author: AIReach
  content: another test. 4bit on vicuna-13b works fast without this issue.
  created_at: 2023-06-20 05:40:08+00:00
  edited: false
  hidden: false
  id: 649149c8990d53d751189705
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
      fullname: andrewxu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIReach
      type: user
    createdAt: '2023-06-23T07:38:18.000Z'
    data:
      edited: false
      editors:
      - AIReach
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8058164715766907
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e26655778171f4dd01fb0fd0482a149.svg
          fullname: andrewxu
          isHf: false
          isPro: false
          name: AIReach
          type: user
        html: '<p>I have found the problem. The second 4090 is hardware borken. 4bit
          inference works well on a single 4090.<br>If someone found the system hangs/freeze/crash
          via inference llm or stable diffusion webui. You should consider your 4090
          is broken.</p>

          '
        raw: 'I have found the problem. The second 4090 is hardware borken. 4bit inference
          works well on a single 4090.

          If someone found the system hangs/freeze/crash via inference llm or stable
          diffusion webui. You should consider your 4090 is broken.'
        updatedAt: '2023-06-23T07:38:18.405Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - SudipThomas
    id: 64954bea44fda5890231b478
    type: comment
  author: AIReach
  content: 'I have found the problem. The second 4090 is hardware borken. 4bit inference
    works well on a single 4090.

    If someone found the system hangs/freeze/crash via inference llm or stable diffusion
    webui. You should consider your 4090 is broken.'
  created_at: 2023-06-23 06:38:18+00:00
  edited: false
  hidden: false
  id: 64954bea44fda5890231b478
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: timdettmers/guanaco-33b-merged
repo_type: model
status: open
target_branch: null
title: 'good performance but failed for long generation '
