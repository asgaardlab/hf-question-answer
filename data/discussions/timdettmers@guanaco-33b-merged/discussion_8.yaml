!!python/object:huggingface_hub.community.DiscussionWithDetails
author: carlosbdw
conflicting_files: null
created_at: 2023-06-01 06:39:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/10a0d62af0788d6533722fd18d1d47c7.svg
      fullname: dawei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: carlosbdw
      type: user
    createdAt: '2023-06-01T07:39:49.000Z'
    data:
      edited: false
      editors:
      - carlosbdw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/10a0d62af0788d6533722fd18d1d47c7.svg
          fullname: dawei
          isHf: false
          isPro: false
          name: carlosbdw
          type: user
        html: '<p>Hi , I run it on a A100 x4 station , and it generates very slow
          , no matter  load_in_4bit=False or True , is it normal? </p>

          '
        raw: 'Hi , I run it on a A100 x4 station , and it generates very slow , no
          matter  load_in_4bit=False or True , is it normal? '
        updatedAt: '2023-06-01T07:39:49.892Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - wjw1998
    id: 64784b458315f87514515ed4
    type: comment
  author: carlosbdw
  content: 'Hi , I run it on a A100 x4 station , and it generates very slow , no matter  load_in_4bit=False
    or True , is it normal? '
  created_at: 2023-06-01 06:39:49+00:00
  edited: false
  hidden: false
  id: 64784b458315f87514515ed4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d97b20672975167a8355f54b5edf248.svg
      fullname: Patrick Shechet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kajuberdut
      type: user
    createdAt: '2023-06-02T21:34:15.000Z'
    data:
      edited: false
      editors:
      - kajuberdut
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9616894125938416
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d97b20672975167a8355f54b5edf248.svg
          fullname: Patrick Shechet
          isHf: false
          isPro: false
          name: kajuberdut
          type: user
        html: '<p>I''m running a single 3090 and getting between 8 and 9 tokens per
          second. Not sure what "very slow" means but I would expect A100x4 to be
          at least double that... Maybe it''s not using GPU with the way you have
          it set up. Are you using oobabooga/text-generation-webui or something else?</p>

          '
        raw: I'm running a single 3090 and getting between 8 and 9 tokens per second.
          Not sure what "very slow" means but I would expect A100x4 to be at least
          double that... Maybe it's not using GPU with the way you have it set up.
          Are you using oobabooga/text-generation-webui or something else?
        updatedAt: '2023-06-02T21:34:15.595Z'
      numEdits: 0
      reactions: []
    id: 647a6057822b7e8ccbd83fb0
    type: comment
  author: kajuberdut
  content: I'm running a single 3090 and getting between 8 and 9 tokens per second.
    Not sure what "very slow" means but I would expect A100x4 to be at least double
    that... Maybe it's not using GPU with the way you have it set up. Are you using
    oobabooga/text-generation-webui or something else?
  created_at: 2023-06-02 20:34:15+00:00
  edited: false
  hidden: false
  id: 647a6057822b7e8ccbd83fb0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef441eccf2c2bed775a6c7fe475dc726.svg
      fullname: Christophe Protat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Chris126
      type: user
    createdAt: '2023-06-11T20:05:33.000Z'
    data:
      edited: true
      editors:
      - Chris126
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9680801033973694
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef441eccf2c2bed775a6c7fe475dc726.svg
          fullname: Christophe Protat
          isHf: false
          isPro: false
          name: Chris126
          type: user
        html: '<p>same issue. Running with load_in_4bits on single 4090 and is around
          2.5tps. I would have considered it is normal and not that bad for my specs.
          But I am curious w.r.t the  comment from giblesnot. 9 tps for a 33B mode
          on a 3090 sounds really good.</p>

          '
        raw: same issue. Running with load_in_4bits on single 4090 and is around 2.5tps.
          I would have considered it is normal and not that bad for my specs. But
          I am curious w.r.t the  comment from giblesnot. 9 tps for a 33B mode on
          a 3090 sounds really good.
        updatedAt: '2023-06-11T20:06:19.867Z'
      numEdits: 1
      reactions: []
    id: 6486290d19b74d6d646a8710
    type: comment
  author: Chris126
  content: same issue. Running with load_in_4bits on single 4090 and is around 2.5tps.
    I would have considered it is normal and not that bad for my specs. But I am curious
    w.r.t the  comment from giblesnot. 9 tps for a 33B mode on a 3090 sounds really
    good.
  created_at: 2023-06-11 19:05:33+00:00
  edited: true
  hidden: false
  id: 6486290d19b74d6d646a8710
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d97b20672975167a8355f54b5edf248.svg
      fullname: Patrick Shechet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kajuberdut
      type: user
    createdAt: '2023-06-12T02:11:26.000Z'
    data:
      edited: false
      editors:
      - kajuberdut
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7624590396881104
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d97b20672975167a8355f54b5edf248.svg
          fullname: Patrick Shechet
          isHf: false
          isPro: false
          name: kajuberdut
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Chris126&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Chris126\">@<span class=\"\
          underline\">Chris126</span></a></span>\n\n\t</span></span><br>I'm currently\
          \ using The Bloke's GPTQ quantization, and with the recent improvements\
          \ to text-gen web ui (and all the related ecosystem) I'm now getting 15+\
          \ tokens per second even though I've set a power limit of 250 watts on my\
          \ 3090. <a href=\"https://huggingface.co/TheBloke/guanaco-33B-GPTQ\">https://huggingface.co/TheBloke/guanaco-33B-GPTQ</a></p>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/631612c195b55e2621d0900b/XqBV0tmDxagwrs8W_kmax.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/631612c195b55e2621d0900b/XqBV0tmDxagwrs8W_kmax.png\"\
          ></a></p>\n"
        raw: "@Chris126 \nI'm currently using The Bloke's GPTQ quantization, and with\
          \ the recent improvements to text-gen web ui (and all the related ecosystem)\
          \ I'm now getting 15+ tokens per second even though I've set a power limit\
          \ of 250 watts on my 3090. https://huggingface.co/TheBloke/guanaco-33B-GPTQ\n\
          \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/631612c195b55e2621d0900b/XqBV0tmDxagwrs8W_kmax.png)"
        updatedAt: '2023-06-12T02:11:26.017Z'
      numEdits: 0
      reactions: []
    id: 64867ece494e71ca720132aa
    type: comment
  author: kajuberdut
  content: "@Chris126 \nI'm currently using The Bloke's GPTQ quantization, and with\
    \ the recent improvements to text-gen web ui (and all the related ecosystem) I'm\
    \ now getting 15+ tokens per second even though I've set a power limit of 250\
    \ watts on my 3090. https://huggingface.co/TheBloke/guanaco-33B-GPTQ\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/631612c195b55e2621d0900b/XqBV0tmDxagwrs8W_kmax.png)"
  created_at: 2023-06-12 01:11:26+00:00
  edited: false
  hidden: false
  id: 64867ece494e71ca720132aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef441eccf2c2bed775a6c7fe475dc726.svg
      fullname: Christophe Protat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Chris126
      type: user
    createdAt: '2023-06-26T10:29:01.000Z'
    data:
      edited: true
      editors:
      - Chris126
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9559256434440613
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef441eccf2c2bed775a6c7fe475dc726.svg
          fullname: Christophe Protat
          isHf: false
          isPro: false
          name: Chris126
          type: user
        html: '<p>yeah, GPTQ is blazingly fast indeed. But I think did not mention
          this in your first answer and probably was the reason of the misunderstanding.  Running
          inference directly on guanaco-33b-merged is slow (based on  your hardware)
          But switching to GPTQ version solve the problem. </p>

          <p>Anyway, thank you @giblesnot for the precision. </p>

          '
        raw: "yeah, GPTQ is blazingly fast indeed. But I think did not mention this\
          \ in your first answer and probably was the reason of the misunderstanding.\
          \  Running inference directly on guanaco-33b-merged is slow (based on  your\
          \ hardware) But switching to GPTQ version solve the problem. \n\nAnyway,\
          \ thank you @giblesnot for the precision. "
        updatedAt: '2023-06-26T10:29:40.349Z'
      numEdits: 1
      reactions: []
    id: 6499686d8e438ed8e792d774
    type: comment
  author: Chris126
  content: "yeah, GPTQ is blazingly fast indeed. But I think did not mention this\
    \ in your first answer and probably was the reason of the misunderstanding.  Running\
    \ inference directly on guanaco-33b-merged is slow (based on  your hardware) But\
    \ switching to GPTQ version solve the problem. \n\nAnyway, thank you @giblesnot\
    \ for the precision. "
  created_at: 2023-06-26 09:29:01+00:00
  edited: true
  hidden: false
  id: 6499686d8e438ed8e792d774
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: timdettmers/guanaco-33b-merged
repo_type: model
status: open
target_branch: null
title: Is it very slow , correct?
