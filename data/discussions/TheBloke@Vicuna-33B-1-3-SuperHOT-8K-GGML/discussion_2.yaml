!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2023-06-30 19:33:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-06-30T20:33:19.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9108883738517761
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>So im running this model in text gen, and its partially working
          with llamaccp, but the models glitch out after about 6 tokens and start
          repeating the same words, and if you increase repeat penaly they will start
          spewing out random words. Any idea how to fix this?</p>

          '
        raw: So im running this model in text gen, and its partially working with
          llamaccp, but the models glitch out after about 6 tokens and start repeating
          the same words, and if you increase repeat penaly they will start spewing
          out random words. Any idea how to fix this?
        updatedAt: '2023-06-30T20:33:19.072Z'
      numEdits: 0
      reactions: []
    id: 649f3c0f8347efe800261f06
    type: comment
  author: rombodawg
  content: So im running this model in text gen, and its partially working with llamaccp,
    but the models glitch out after about 6 tokens and start repeating the same words,
    and if you increase repeat penaly they will start spewing out random words. Any
    idea how to fix this?
  created_at: 2023-06-30 19:33:19+00:00
  edited: false
  hidden: false
  id: 649f3c0f8347efe800261f06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-06-30T21:30:41.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9776520133018494
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>Im seeing now that support for text gen is still slowly being worked
          on, in the mean time what command do we use for this in window for kobaldcpp?</p>

          '
        raw: Im seeing now that support for text gen is still slowly being worked
          on, in the mean time what command do we use for this in window for kobaldcpp?
        updatedAt: '2023-06-30T21:30:41.095Z'
      numEdits: 0
      reactions: []
    id: 649f498153158a718050c45e
    type: comment
  author: rombodawg
  content: Im seeing now that support for text gen is still slowly being worked on,
    in the mean time what command do we use for this in window for kobaldcpp?
  created_at: 2023-06-30 20:30:41+00:00
  edited: false
  hidden: false
  id: 649f498153158a718050c45e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-30T21:51:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7318457365036011
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>The same as I show in the Readme, just you run <code>koboldcpp.exe</code>
          instead of <code>python koboldcpp.py</code> - the rest of the arguments
          should be the same</p>

          '
        raw: The same as I show in the Readme, just you run `koboldcpp.exe` instead
          of `python koboldcpp.py` - the rest of the arguments should be the same
        updatedAt: '2023-06-30T21:51:52.861Z'
      numEdits: 0
      reactions: []
    id: 649f4e78ca03a1a35e3b9c1c
    type: comment
  author: TheBloke
  content: The same as I show in the Readme, just you run `koboldcpp.exe` instead
    of `python koboldcpp.py` - the rest of the arguments should be the same
  created_at: 2023-06-30 20:51:52+00:00
  edited: false
  hidden: false
  id: 649f4e78ca03a1a35e3b9c1c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-06-30T22:12:41.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8725132942199707
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>Ok i was able to get it to run, however still have the issue of
          the models glitch out after about 6 tokens and start repeating the same
          words, here is what im running on windows<br>koboldcpp.exe --stream --unbantokens
          --threads 8  --noblas vicuna-33b-1.3-superhot-8k.ggmlv3.q6_K.bin<br>Im running
          on cpu exclusively because i only have enough ram on cpu to run the model.
          Is there something im doing wrong that causing the glitch? what settings
          do you run on kobaldcpp so the model behaves normally?</p>

          '
        raw: "Ok i was able to get it to run, however still have the issue of the\
          \ models glitch out after about 6 tokens and start repeating the same words,\
          \ here is what im running on windows\nkoboldcpp.exe --stream --unbantokens\
          \ --threads 8  --noblas vicuna-33b-1.3-superhot-8k.ggmlv3.q6_K.bin \nIm\
          \ running on cpu exclusively because i only have enough ram on cpu to run\
          \ the model. Is there something im doing wrong that causing the glitch?\
          \ what settings do you run on kobaldcpp so the model behaves normally?"
        updatedAt: '2023-06-30T22:12:41.722Z'
      numEdits: 0
      reactions: []
    id: 649f5359e634fdbf5d23caaf
    type: comment
  author: rombodawg
  content: "Ok i was able to get it to run, however still have the issue of the models\
    \ glitch out after about 6 tokens and start repeating the same words, here is\
    \ what im running on windows\nkoboldcpp.exe --stream --unbantokens --threads 8\
    \  --noblas vicuna-33b-1.3-superhot-8k.ggmlv3.q6_K.bin \nIm running on cpu exclusively\
    \ because i only have enough ram on cpu to run the model. Is there something im\
    \ doing wrong that causing the glitch? what settings do you run on kobaldcpp so\
    \ the model behaves normally?"
  created_at: 2023-06-30 21:12:41+00:00
  edited: false
  hidden: false
  id: 649f5359e634fdbf5d23caaf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-30T22:13:46.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8613290786743164
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You need to set <code>--contextsize</code> ,  eg <code>--contextsize
          4096</code>.  These SuperHOT models seem to perform very poorly at the default
          2048 context, but are OK at higher context sizes.</p>

          <p>This should be resolved in future as the context-increasing algorithm
          improves.</p>

          '
        raw: 'You need to set `--contextsize` ,  eg `--contextsize 4096`.  These SuperHOT
          models seem to perform very poorly at the default 2048 context, but are
          OK at higher context sizes.


          This should be resolved in future as the context-increasing algorithm improves.'
        updatedAt: '2023-06-30T22:13:46.290Z'
      numEdits: 0
      reactions: []
    id: 649f539a850cf879852bf29a
    type: comment
  author: TheBloke
  content: 'You need to set `--contextsize` ,  eg `--contextsize 4096`.  These SuperHOT
    models seem to perform very poorly at the default 2048 context, but are OK at
    higher context sizes.


    This should be resolved in future as the context-increasing algorithm improves.'
  created_at: 2023-06-30 21:13:46+00:00
  edited: false
  hidden: false
  id: 649f539a850cf879852bf29a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/42648bec3fc52ddb3b8a11d485d4548c.svg
      fullname: King  Bull
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nichedreams
      type: user
    createdAt: '2023-07-01T02:33:43.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/42648bec3fc52ddb3b8a11d485d4548c.svg
          fullname: King  Bull
          isHf: false
          isPro: false
          name: nichedreams
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-07-01T02:35:32.039Z'
      numEdits: 1
      reactions: []
    id: 649f90877712e05292421de4
    type: comment
  author: nichedreams
  content: This comment has been hidden
  created_at: 2023-07-01 01:33:43+00:00
  edited: true
  hidden: true
  id: 649f90877712e05292421de4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-07-01T19:28:07.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5798337459564209
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>Honestly i dont know if im using the wrong version of koboldcpp.exe,
          but that program only allows you to generate up to 500 tokens, even with
          the --contextsize 4096 flag enabled. The version im using is here on windows:<br><a
          rel="nofollow" href="https://github.com/LostRuins/koboldcpp">https://github.com/LostRuins/koboldcpp</a></p>

          '
        raw: 'Honestly i dont know if im using the wrong version of koboldcpp.exe,
          but that program only allows you to generate up to 500 tokens, even with
          the --contextsize 4096 flag enabled. The version im using is here on windows:

          https://github.com/LostRuins/koboldcpp'
        updatedAt: '2023-07-01T19:28:07.364Z'
      numEdits: 0
      reactions: []
    id: 64a07e4749c020745c9c5d04
    type: comment
  author: rombodawg
  content: 'Honestly i dont know if im using the wrong version of koboldcpp.exe, but
    that program only allows you to generate up to 500 tokens, even with the --contextsize
    4096 flag enabled. The version im using is here on windows:

    https://github.com/LostRuins/koboldcpp'
  created_at: 2023-07-01 18:28:07+00:00
  edited: false
  hidden: false
  id: 64a07e4749c020745c9c5d04
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-07-02T00:54:51.000Z'
    data:
      edited: true
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9444235563278198
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>Hey i finally got it working, using  the command lines to change
          the context size in koboldcpp doesnt work for generation, maybe it helps
          for loading the model but you have to do into settings, and select the actual
          number that represents Max_tokens and also amount_to_generate and set those
          manually to 8k and 4k respectively. thats the only way it works, even though
          the sliders dont go that far you can edit the numbers yourself and the models
          will work with it. </p>

          '
        raw: 'Hey i finally got it working, using  the command lines to change the
          context size in koboldcpp doesnt work for generation, maybe it helps for
          loading the model but you have to do into settings, and select the actual
          number that represents Max_tokens and also amount_to_generate and set those
          manually to 8k and 4k respectively. thats the only way it works, even though
          the sliders dont go that far you can edit the numbers yourself and the models
          will work with it. '
        updatedAt: '2023-07-02T00:55:32.116Z'
      numEdits: 1
      reactions: []
    id: 64a0cadbf152445823437189
    type: comment
  author: rombodawg
  content: 'Hey i finally got it working, using  the command lines to change the context
    size in koboldcpp doesnt work for generation, maybe it helps for loading the model
    but you have to do into settings, and select the actual number that represents
    Max_tokens and also amount_to_generate and set those manually to 8k and 4k respectively.
    thats the only way it works, even though the sliders dont go that far you can
    edit the numbers yourself and the models will work with it. '
  created_at: 2023-07-01 23:54:51+00:00
  edited: true
  hidden: false
  id: 64a0cadbf152445823437189
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-08T09:50:25.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9895386695861816
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ah yeah, I used to have a note about that in my READMEs but it''s
          got lost somewhere along the way. I''ll make sure to add it in future!</p>

          '
        raw: Ah yeah, I used to have a note about that in my READMEs but it's got
          lost somewhere along the way. I'll make sure to add it in future!
        updatedAt: '2023-07-08T09:50:25.544Z'
      numEdits: 0
      reactions: []
    id: 64a9316104e7b379feb3708f
    type: comment
  author: TheBloke
  content: Ah yeah, I used to have a note about that in my READMEs but it's got lost
    somewhere along the way. I'll make sure to add it in future!
  created_at: 2023-07-08 08:50:25+00:00
  edited: false
  hidden: false
  id: 64a9316104e7b379feb3708f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GGML
repo_type: model
status: open
target_branch: null
title: Issues in Text-Generation-Wev-ui
