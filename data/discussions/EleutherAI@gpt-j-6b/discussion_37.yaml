!!python/object:huggingface_hub.community.DiscussionWithDetails
author: avstinpaxton
conflicting_files: null
created_at: 2023-09-15 05:29:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0448a2bc45e794f7ba8a0eeedb697853.svg
      fullname: Austin Paxton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: avstinpaxton
      type: user
    createdAt: '2023-09-15T06:29:22.000Z'
    data:
      edited: false
      editors:
      - avstinpaxton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9400983452796936
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0448a2bc45e794f7ba8a0eeedb697853.svg
          fullname: Austin Paxton
          isHf: false
          isPro: false
          name: avstinpaxton
          type: user
        html: '<p>Hi there,<br>I am new to transformers and working with larger models
          and could use some help.  I fined tuned the model using adapters after converting
          it to 8-bit using the same setup as the notebook on the model page.  My
          model is in a .bin format and ~6 Gb; is there a way to reload the model
          without reperforming 8 bit quantization again?</p>

          <p>I am working on my flask application on my laptop and attempting to load
          the model onto my cpu. However, the bitnbytes library only works for GPU
          bc it is a cuda wrapper. Any ideas on how to use the model on a cpu while
          I am developing my flask application?</p>

          <p>Any help is greatly appreciated. (:</p>

          '
        raw: "Hi there, \r\nI am new to transformers and working with larger models\
          \ and could use some help.  I fined tuned the model using adapters after\
          \ converting it to 8-bit using the same setup as the notebook on the model\
          \ page.  My model is in a .bin format and ~6 Gb; is there a way to reload\
          \ the model without reperforming 8 bit quantization again?\r\n\r\nI am working\
          \ on my flask application on my laptop and attempting to load the model\
          \ onto my cpu. However, the bitnbytes library only works for GPU bc it is\
          \ a cuda wrapper. Any ideas on how to use the model on a cpu while I am\
          \ developing my flask application?\r\n\r\nAny help is greatly appreciated.\
          \ (:"
        updatedAt: '2023-09-15T06:29:22.815Z'
      numEdits: 0
      reactions: []
    id: 6503f9c206e7b98952fd8045
    type: comment
  author: avstinpaxton
  content: "Hi there, \r\nI am new to transformers and working with larger models\
    \ and could use some help.  I fined tuned the model using adapters after converting\
    \ it to 8-bit using the same setup as the notebook on the model page.  My model\
    \ is in a .bin format and ~6 Gb; is there a way to reload the model without reperforming\
    \ 8 bit quantization again?\r\n\r\nI am working on my flask application on my\
    \ laptop and attempting to load the model onto my cpu. However, the bitnbytes\
    \ library only works for GPU bc it is a cuda wrapper. Any ideas on how to use\
    \ the model on a cpu while I am developing my flask application?\r\n\r\nAny help\
    \ is greatly appreciated. (:"
  created_at: 2023-09-15 05:29:22+00:00
  edited: false
  hidden: false
  id: 6503f9c206e7b98952fd8045
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-09-22T11:38:35.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9994029402732849
      isReport: false
      latest:
        html: '<p>did you manage to do that?</p>

          '
        raw: did you manage to do that?
        updatedAt: '2023-09-22T11:38:35.930Z'
      numEdits: 0
      reactions: []
    id: 650d7cbb03ce54bd11beb297
    type: comment
  author: deleted
  content: did you manage to do that?
  created_at: 2023-09-22 10:38:35+00:00
  edited: false
  hidden: false
  id: 650d7cbb03ce54bd11beb297
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c0fb208c60f5c816210d0c06e271cf55.svg
      fullname: Aug Chan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: augchan42
      type: user
    createdAt: '2023-11-28T00:17:51.000Z'
    data:
      edited: false
      editors:
      - augchan42
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7515410780906677
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c0fb208c60f5c816210d0c06e271cf55.svg
          fullname: Aug Chan
          isHf: false
          isPro: false
          name: augchan42
          type: user
        html: '<p>Seems the default way of loading and running these pytorch models
          is awfully slow.  I wrote up how I used ggml to inference a gptj-6-b model
          here:<br><a rel="nofollow" href="https://augchan42.github.io/2023/11/26/Its-Hard-to-find-an-Uncensored-Model.html">https://augchan42.github.io/2023/11/26/Its-Hard-to-find-an-Uncensored-Model.html</a><br>But
          basically, convert the pytorch_model.bin to ggml format, then use the ggml
          gpt-j bin to run inference.  No need to quantize to 8bit, I used a float16
          version</p>

          '
        raw: 'Seems the default way of loading and running these pytorch models is
          awfully slow.  I wrote up how I used ggml to inference a gptj-6-b model
          here:

          https://augchan42.github.io/2023/11/26/Its-Hard-to-find-an-Uncensored-Model.html

          But basically, convert the pytorch_model.bin to ggml format, then use the
          ggml gpt-j bin to run inference.  No need to quantize to 8bit, I used a
          float16 version'
        updatedAt: '2023-11-28T00:17:51.025Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - avstinpaxton
    id: 656531aff483a5f0c6b6ae57
    type: comment
  author: augchan42
  content: 'Seems the default way of loading and running these pytorch models is awfully
    slow.  I wrote up how I used ggml to inference a gptj-6-b model here:

    https://augchan42.github.io/2023/11/26/Its-Hard-to-find-an-Uncensored-Model.html

    But basically, convert the pytorch_model.bin to ggml format, then use the ggml
    gpt-j bin to run inference.  No need to quantize to 8bit, I used a float16 version'
  created_at: 2023-11-28 00:17:51+00:00
  edited: false
  hidden: false
  id: 656531aff483a5f0c6b6ae57
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 37
repo_id: EleutherAI/gpt-j-6b
repo_type: model
status: open
target_branch: null
title: Best Way to Load a Model After Training w/o Requantizing
