!!python/object:huggingface_hub.community.DiscussionWithDetails
author: moshi
conflicting_files: null
created_at: 2023-03-23 12:33:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/527dce45ddf2b87edf97fb390aa62032.svg
      fullname: Moshe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: moshi
      type: user
    createdAt: '2023-03-23T13:33:40.000Z'
    data:
      edited: true
      editors:
      - moshi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/527dce45ddf2b87edf97fb390aa62032.svg
          fullname: Moshe
          isHf: false
          isPro: false
          name: moshi
          type: user
        html: '<p>Hi, I''m trying to load GPT-J in 8-bit mode for fine-tuning using
          LoRA with the new PEFT library.<br>This is the part of the code I use for
          loading the model in 8-bit and the tokenizer:</p>

          <p>def tokenize(element):<br>  inputs = tokenizer(<br>      element[''text''],<br>      truncation=True,<br>      padding=True,<br>      max_length=MAX_LEN,<br>  )<br>  return
          {''input_ids'': inputs.input_ids,<br>          ''attention_mask'':inputs.attention_mask,<br>          ''labels'':inputs.input_ids}</p>

          <p>model = GPTJForCausalLM.from_pretrained("EleutherAI/gpt-j-6B",<br>                                        device_map=device_map,<br>                                        load_in_8bit=True,<br>                                        )<br>model
          = prepare_model_for_int8_training(model)</p>

          <p>config = LoraConfig(<br>    r=LORA_R,<br>    lora_alpha=LORA_ALPHA,<br>    target_modules=TARGET_MODULES,<br>    lora_dropout=LORA_DROPOUT,<br>    bias="none",<br>    task_type=TaskType.CAUSAL_LM,<br>)<br>model
          = get_peft_model(model, config)</p>

          <p>When I try to run the training using the Trainer API I get the following
          error:<br>File "/home/azureuser/.local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py",
          line 456, in backward<br>    grad_A = torch.matmul(grad_output, CB).view(ctx.grad_shape).to(ctx.dtype_A)<br>RuntimeError:
          expected scalar type Half but found Float</p>

          <p>It should be noted that I ran a very similar script with a different
          model (CodeT5-Large) and it worked just fine so there isn''t suppose to
          be something missing in the rest of the code. (I obviously changed parameters
          and the data collator for a CasualLM one instead of Seq2Seq aswell with
          the data itself having the prompt and completion appended into one text
          column to be ran CLM training on).</p>

          '
        raw: "Hi, I'm trying to load GPT-J in 8-bit mode for fine-tuning using LoRA\
          \ with the new PEFT library. \nThis is the part of the code I use for loading\
          \ the model in 8-bit and the tokenizer:\n\ndef tokenize(element):\n  inputs\
          \ = tokenizer(\n      element['text'],\n      truncation=True,\n      padding=True,\n\
          \      max_length=MAX_LEN,\n  )\n  return {'input_ids': inputs.input_ids,\n\
          \          'attention_mask':inputs.attention_mask,\n          'labels':inputs.input_ids}\n\
          \nmodel = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\",\n   \
          \                                     device_map=device_map,\n         \
          \                               load_in_8bit=True,\n                   \
          \                     )\nmodel = prepare_model_for_int8_training(model)\n\
          \nconfig = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=TARGET_MODULES,\n\
          \    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n\
          )\nmodel = get_peft_model(model, config)\n \nWhen I try to run the training\
          \ using the Trainer API I get the following error:\nFile \"/home/azureuser/.local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py\"\
          , line 456, in backward\n    grad_A = torch.matmul(grad_output, CB).view(ctx.grad_shape).to(ctx.dtype_A)\n\
          RuntimeError: expected scalar type Half but found Float\n\nIt should be\
          \ noted that I ran a very similar script with a different model (CodeT5-Large)\
          \ and it worked just fine so there isn't suppose to be something missing\
          \ in the rest of the code. (I obviously changed parameters and the data\
          \ collator for a CasualLM one instead of Seq2Seq aswell with the data itself\
          \ having the prompt and completion appended into one text column to be ran\
          \ CLM training on)."
        updatedAt: '2023-03-23T13:35:11.043Z'
      numEdits: 1
      reactions: []
    id: 641c5534f0b71a9743638d1c
    type: comment
  author: moshi
  content: "Hi, I'm trying to load GPT-J in 8-bit mode for fine-tuning using LoRA\
    \ with the new PEFT library. \nThis is the part of the code I use for loading\
    \ the model in 8-bit and the tokenizer:\n\ndef tokenize(element):\n  inputs =\
    \ tokenizer(\n      element['text'],\n      truncation=True,\n      padding=True,\n\
    \      max_length=MAX_LEN,\n  )\n  return {'input_ids': inputs.input_ids,\n  \
    \        'attention_mask':inputs.attention_mask,\n          'labels':inputs.input_ids}\n\
    \nmodel = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\",\n         \
    \                               device_map=device_map,\n                     \
    \                   load_in_8bit=True,\n                                     \
    \   )\nmodel = prepare_model_for_int8_training(model)\n\nconfig = LoraConfig(\n\
    \    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=TARGET_MODULES,\n\
    \    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n\
    )\nmodel = get_peft_model(model, config)\n \nWhen I try to run the training using\
    \ the Trainer API I get the following error:\nFile \"/home/azureuser/.local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py\"\
    , line 456, in backward\n    grad_A = torch.matmul(grad_output, CB).view(ctx.grad_shape).to(ctx.dtype_A)\n\
    RuntimeError: expected scalar type Half but found Float\n\nIt should be noted\
    \ that I ran a very similar script with a different model (CodeT5-Large) and it\
    \ worked just fine so there isn't suppose to be something missing in the rest\
    \ of the code. (I obviously changed parameters and the data collator for a CasualLM\
    \ one instead of Seq2Seq aswell with the data itself having the prompt and completion\
    \ appended into one text column to be ran CLM training on)."
  created_at: 2023-03-23 12:33:40+00:00
  edited: true
  hidden: false
  id: 641c5534f0b71a9743638d1c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9a410137446bd0eb74d072b60595c1de.svg
      fullname: zhou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nathan0
      type: user
    createdAt: '2023-04-08T01:21:56.000Z'
    data:
      edited: false
      editors:
      - nathan0
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9a410137446bd0eb74d072b60595c1de.svg
          fullname: zhou
          isHf: false
          isPro: false
          name: nathan0
          type: user
        html: '<p>I met the same problem, did you fix it now? BTW, what kind of GPU
          did you use?</p>

          '
        raw: I met the same problem, did you fix it now? BTW, what kind of GPU did
          you use?
        updatedAt: '2023-04-08T01:21:56.497Z'
      numEdits: 0
      reactions: []
    id: 6430c1b4eece62b995dbb772
    type: comment
  author: nathan0
  content: I met the same problem, did you fix it now? BTW, what kind of GPU did you
    use?
  created_at: 2023-04-08 00:21:56+00:00
  edited: false
  hidden: false
  id: 6430c1b4eece62b995dbb772
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: EleutherAI/gpt-j-6b
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: expected scalar type Half but found Float'
