!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mrmartin
conflicting_files: null
created_at: 2022-11-14 15:29:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1ae843e211d409a6e763993c6ce8ce9.svg
      fullname: Martin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrmartin
      type: user
    createdAt: '2022-11-14T15:29:42.000Z'
    data:
      edited: false
      editors:
      - mrmartin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1ae843e211d409a6e763993c6ce8ce9.svg
          fullname: Martin
          isHf: false
          isPro: false
          name: mrmartin
          type: user
        html: "<p>The model works fine when loaded as follows:<br><code>model = GPTJForCausalLM.from_pretrained(\"\
          EleutherAI/gpt-j-6B\", revision=\"float16\", low_cpu_mem_usage=True)</code><br>but\
          \ after executing a few successful evaluations like this<br><code>generated_ids\
          \ = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=200)</code><br>I\
          \ get the error<br><code>CUDA out of memory. Tried to allocate 214.00 MiB\
          \ (GPU 0; 14.76 GiB total capacity; 13.45 GiB already allocated; 95.75 MiB\
          \ free; 13.66 GiB reserved in total by PyTorch)</code><br>Basically, the\
          \ free memory keeps going down, it never clears, and it clogs up the GPU\
          \ until I need to kill the process.</p>\n<p>Any better solution?</p>\n<p>I've\
          \ had a look at<br><code>print(torch.cuda.memory_summary())</code><br>and\
          \ tried<br><code>torch.cuda.empty_cache()</code><br>but no luck \U0001F937\
          </p>\n"
        raw: "The model works fine when loaded as follows:\r\n```model = GPTJForCausalLM.from_pretrained(\"\
          EleutherAI/gpt-j-6B\", revision=\"float16\", low_cpu_mem_usage=True)```\r\
          \nbut after executing a few successful evaluations like this\r\n```generated_ids\
          \ = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=200)```\r\
          \nI get the error\r\n```CUDA out of memory. Tried to allocate 214.00 MiB\
          \ (GPU 0; 14.76 GiB total capacity; 13.45 GiB already allocated; 95.75 MiB\
          \ free; 13.66 GiB reserved in total by PyTorch)```\r\nBasically, the free\
          \ memory keeps going down, it never clears, and it clogs up the GPU until\
          \ I need to kill the process.\r\n\r\nAny better solution?\r\n\r\nI've had\
          \ a look at\r\n```print(torch.cuda.memory_summary())```\r\nand tried\r\n\
          ```torch.cuda.empty_cache()```\r\nbut no luck \U0001F937"
        updatedAt: '2022-11-14T15:29:42.542Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - 0xnurl
    id: 63725ee63010b4dd30890feb
    type: comment
  author: mrmartin
  content: "The model works fine when loaded as follows:\r\n```model = GPTJForCausalLM.from_pretrained(\"\
    EleutherAI/gpt-j-6B\", revision=\"float16\", low_cpu_mem_usage=True)```\r\nbut\
    \ after executing a few successful evaluations like this\r\n```generated_ids =\
    \ model.generate(input_ids, do_sample=True, temperature=0.9, max_length=200)```\r\
    \nI get the error\r\n```CUDA out of memory. Tried to allocate 214.00 MiB (GPU\
    \ 0; 14.76 GiB total capacity; 13.45 GiB already allocated; 95.75 MiB free; 13.66\
    \ GiB reserved in total by PyTorch)```\r\nBasically, the free memory keeps going\
    \ down, it never clears, and it clogs up the GPU until I need to kill the process.\r\
    \n\r\nAny better solution?\r\n\r\nI've had a look at\r\n```print(torch.cuda.memory_summary())```\r\
    \nand tried\r\n```torch.cuda.empty_cache()```\r\nbut no luck \U0001F937"
  created_at: 2022-11-14 15:29:42+00:00
  edited: false
  hidden: false
  id: 63725ee63010b4dd30890feb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
      fullname: Mukesh Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MukeshSharma
      type: user
    createdAt: '2022-12-20T17:50:50.000Z'
    data:
      edited: false
      editors:
      - MukeshSharma
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
          fullname: Mukesh Sharma
          isHf: false
          isPro: false
          name: MukeshSharma
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;mrmartin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mrmartin\"\
          >@<span class=\"underline\">mrmartin</span></a></span>\n\n\t</span></span>\
          \ you should use pip install parallelformers</p>\n<p>from transformers import\
          \ AutoModelForCausalLM, AutoTokenizer</p>\n<p>model = AutoModelForCausalLM.from_pretrained(\"\
          Model Name\")<br>tokenizer = AutoTokenizer.from_pretrained(\"Model Name\"\
          )</p>\n<p>from parallelformers import parallelize</p>\n<p>parallelize(model,\
          \ num_gpus=2, fp16=True, verbose='detail')</p>\n<p>inputs = tokenizer(\"\
          My Name is Mukesh \", return_tensors=\"pt\")</p>\n<p>outputs = model.generate(<br>\
          \    **inputs,<br>    num_beams=5,<br>    no_repeat_ngram_size=4,<br>  \
          \  max_length=15,<br>)</p>\n<p>print(f\"Output: {tokenizer.batch_decode(outputs)[0]}\"\
          )</p>\n<p>It will surely solve ur problem.</p>\n"
        raw: "Hi @mrmartin you should use pip install parallelformers\n\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          Model Name\")\ntokenizer = AutoTokenizer.from_pretrained(\"Model Name\"\
          )\n\nfrom parallelformers import parallelize\n\nparallelize(model, num_gpus=2,\
          \ fp16=True, verbose='detail')\n\ninputs = tokenizer(\"My Name is Mukesh\
          \ \", return_tensors=\"pt\")\n\noutputs = model.generate(\n    **inputs,\n\
          \    num_beams=5,\n    no_repeat_ngram_size=4,\n    max_length=15,\n)\n\n\
          print(f\"Output: {tokenizer.batch_decode(outputs)[0]}\")\n\n\nIt will surely\
          \ solve ur problem."
        updatedAt: '2022-12-20T17:50:50.260Z'
      numEdits: 0
      reactions: []
    id: 63a1f5fa3c003e409316e24b
    type: comment
  author: MukeshSharma
  content: "Hi @mrmartin you should use pip install parallelformers\n\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    Model Name\")\ntokenizer = AutoTokenizer.from_pretrained(\"Model Name\")\n\nfrom\
    \ parallelformers import parallelize\n\nparallelize(model, num_gpus=2, fp16=True,\
    \ verbose='detail')\n\ninputs = tokenizer(\"My Name is Mukesh \", return_tensors=\"\
    pt\")\n\noutputs = model.generate(\n    **inputs,\n    num_beams=5,\n    no_repeat_ngram_size=4,\n\
    \    max_length=15,\n)\n\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\"\
    )\n\n\nIt will surely solve ur problem."
  created_at: 2022-12-20 17:50:50+00:00
  edited: false
  hidden: false
  id: 63a1f5fa3c003e409316e24b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c0fb208c60f5c816210d0c06e271cf55.svg
      fullname: Aug Chan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: augchan42
      type: user
    createdAt: '2023-11-28T00:20:38.000Z'
    data:
      edited: false
      editors:
      - augchan42
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6360530853271484
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c0fb208c60f5c816210d0c06e271cf55.svg
          fullname: Aug Chan
          isHf: false
          isPro: false
          name: augchan42
          type: user
        html: '<p>Or just convert the model to ggml and use ggml for inference<br><a
          rel="nofollow" href="https://augchan42.github.io/2023/11/26/Its-Hard-to-find-an-Uncensored-Model.html">https://augchan42.github.io/2023/11/26/Its-Hard-to-find-an-Uncensored-Model.html</a></p>

          '
        raw: 'Or just convert the model to ggml and use ggml for inference

          https://augchan42.github.io/2023/11/26/Its-Hard-to-find-an-Uncensored-Model.html

          '
        updatedAt: '2023-11-28T00:20:38.440Z'
      numEdits: 0
      reactions: []
    id: 6565325681fa36db359c3a60
    type: comment
  author: augchan42
  content: 'Or just convert the model to ggml and use ggml for inference

    https://augchan42.github.io/2023/11/26/Its-Hard-to-find-an-Uncensored-Model.html

    '
  created_at: 2023-11-28 00:20:38+00:00
  edited: false
  hidden: false
  id: 6565325681fa36db359c3a60
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: EleutherAI/gpt-j-6b
repo_type: model
status: open
target_branch: null
title: GPTJForCausalLM hogs memory - inference only
