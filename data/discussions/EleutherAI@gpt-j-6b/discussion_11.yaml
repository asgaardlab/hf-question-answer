!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Baicai003
conflicting_files: null
created_at: 2023-01-06 06:30:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/213a574236d78b7a5cad3b6adde6a6b8.svg
      fullname: baicai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Baicai003
      type: user
    createdAt: '2023-01-06T06:30:38.000Z'
    data:
      edited: false
      editors:
      - Baicai003
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/213a574236d78b7a5cad3b6adde6a6b8.svg
          fullname: baicai
          isHf: false
          isPro: false
          name: Baicai003
          type: user
        html: "<p>with the <a href=\"https://huggingface.co/EleutherAI/gpt-j-6B/tree/float16\"\
          >https://huggingface.co/EleutherAI/gpt-j-6B/tree/float16</a> model<br>I\
          \ test:</p>\n<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\
          import time\nimport torch\n\nprint(\"start load\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          ./gpt-j-6B\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\nprint(\"\
          end 1\")\ntokenizer = AutoTokenizer.from_pretrained(\"./gpt-j-6B\", low_cpu_mem_usage=True,\
          \ torch_dtype=torch.float16)\nprint(\"end 2\")\n\n# from parallelformers\
          \ import parallelize\n\n# parallelize(model, num_gpus=1, fp16=True, verbose='detail')\n\
          \nstart = time.time()\ninputs = tokenizer(\"My Name is Mukesh \", return_tensors=\"\
          pt\")\noutputs = model.generate(\n**inputs,\nnum_beams=5,\nno_repeat_ngram_size=4,\n\
          max_length=200,\n)\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\"\
          )\nend = time.time()\nprint(end - start)\n\nstart = time.time()\ninputs\
          \ = tokenizer(\"Q:what is AI\\nA:AI is a cat.\\nQ:why?\\nA:\", return_tensors=\"\
          pt\")\noutputs = model.generate(\n**inputs,\nnum_beams=5,\nno_repeat_ngram_size=4,\n\
          max_length=200,\n)\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\"\
          )\nend = time.time()\nprint(end - start)\n\nstart = time.time()\ninputs\
          \ = tokenizer(\"Q:\u4E16\u754C\u4E0A\u6700\u5927\u7684\u5730\u65B9\u662F\
          \u54EA\\nA:\u662F\u592A\u9633.\\nQ:\u592A\u9633\u662F\u4E16\u754C\u4E0A\u6700\
          \u5927\u7684\u5730\u65B9\u5417\uFF1F\\nA:\", return_tensors=\"pt\")\noutputs\
          \ = model.generate(\n**inputs,\nnum_beams=5,\nno_repeat_ngram_size=4,\n\
          max_length=200,\n)\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\"\
          )\nend = time.time()\nprint(end - start)\n</code></pre>\n<p>but got an error\
          \ with:</p>\n<pre><code>start load\nend 1\nend 2\nSetting `pad_token_id`\
          \ to `eos_token_id`:50256 for open-end generation.\nTraceback (most recent\
          \ call last):\n  File \"test.py\", line 17, in &lt;module&gt;\n    outputs\
          \ = model.generate(\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/root/miniconda3/lib/python3.8/site-packages/transformers/generation/utils.py\"\
          , line 1608, in generate\n    return self.beam_search(\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/generation/utils.py\"\
          , line 2799, in beam_search\n    outputs = self(\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py\"\
          , line 821, in forward\n    transformer_outputs = self.transformer(\n  File\
          \ \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py\"\
          , line 676, in forward\n    outputs = block(\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py\"\
          , line 309, in forward\n    hidden_states = self.ln_1(hidden_states)\n \
          \ File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/normalization.py\"\
          , line 189, in forward\n    return F.layer_norm(\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\"\
          , line 2503, in layer_norm\n    return torch.layer_norm(input, normalized_shape,\
          \ weight, bias, eps, torch.backends.cudnn.enabled)\nRuntimeError: \"LayerNormKernelImpl\"\
          \ not implemented for 'Half'\n</code></pre>\n"
        raw: "with the https://huggingface.co/EleutherAI/gpt-j-6B/tree/float16 model\r\
          \nI test:\r\n```\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
          \nimport time\r\nimport torch\r\n\r\nprint(\"start load\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          ./gpt-j-6B\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\r\nprint(\"\
          end 1\")\r\ntokenizer = AutoTokenizer.from_pretrained(\"./gpt-j-6B\", low_cpu_mem_usage=True,\
          \ torch_dtype=torch.float16)\r\nprint(\"end 2\")\r\n\r\n# from parallelformers\
          \ import parallelize\r\n\r\n# parallelize(model, num_gpus=1, fp16=True,\
          \ verbose='detail')\r\n\r\nstart = time.time()\r\ninputs = tokenizer(\"\
          My Name is Mukesh \", return_tensors=\"pt\")\r\noutputs = model.generate(\r\
          \n**inputs,\r\nnum_beams=5,\r\nno_repeat_ngram_size=4,\r\nmax_length=200,\r\
          \n)\r\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\")\r\nend =\
          \ time.time()\r\nprint(end - start)\r\n\r\nstart = time.time()\r\ninputs\
          \ = tokenizer(\"Q:what is AI\\nA:AI is a cat.\\nQ:why?\\nA:\", return_tensors=\"\
          pt\")\r\noutputs = model.generate(\r\n**inputs,\r\nnum_beams=5,\r\nno_repeat_ngram_size=4,\r\
          \nmax_length=200,\r\n)\r\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\"\
          )\r\nend = time.time()\r\nprint(end - start)\r\n\r\nstart = time.time()\r\
          \ninputs = tokenizer(\"Q:\u4E16\u754C\u4E0A\u6700\u5927\u7684\u5730\u65B9\
          \u662F\u54EA\\nA:\u662F\u592A\u9633.\\nQ:\u592A\u9633\u662F\u4E16\u754C\u4E0A\
          \u6700\u5927\u7684\u5730\u65B9\u5417\uFF1F\\nA:\", return_tensors=\"pt\"\
          )\r\noutputs = model.generate(\r\n**inputs,\r\nnum_beams=5,\r\nno_repeat_ngram_size=4,\r\
          \nmax_length=200,\r\n)\r\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\"\
          )\r\nend = time.time()\r\nprint(end - start)\r\n```\r\nbut got an error\
          \ with:\r\n```\r\nstart load\r\nend 1\r\nend 2\r\nSetting `pad_token_id`\
          \ to `eos_token_id`:50256 for open-end generation.\r\nTraceback (most recent\
          \ call last):\r\n  File \"test.py\", line 17, in <module>\r\n    outputs\
          \ = model.generate(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n \
          \ File \"/root/miniconda3/lib/python3.8/site-packages/transformers/generation/utils.py\"\
          , line 1608, in generate\r\n    return self.beam_search(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/generation/utils.py\"\
          , line 2799, in beam_search\r\n    outputs = self(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\
          \n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py\"\
          , line 821, in forward\r\n    transformer_outputs = self.transformer(\r\n\
          \  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\
          \n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py\"\
          , line 676, in forward\r\n    outputs = block(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\
          \n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py\"\
          , line 309, in forward\r\n    hidden_states = self.ln_1(hidden_states)\r\
          \n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\
          \n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/normalization.py\"\
          , line 189, in forward\r\n    return F.layer_norm(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\"\
          , line 2503, in layer_norm\r\n    return torch.layer_norm(input, normalized_shape,\
          \ weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: \"LayerNormKernelImpl\"\
          \ not implemented for 'Half'\r\n```"
        updatedAt: '2023-01-06T06:30:38.928Z'
      numEdits: 0
      reactions: []
    id: 63b7c00eca2f378e7103e612
    type: comment
  author: Baicai003
  content: "with the https://huggingface.co/EleutherAI/gpt-j-6B/tree/float16 model\r\
    \nI test:\r\n```\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
    \nimport time\r\nimport torch\r\n\r\nprint(\"start load\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    ./gpt-j-6B\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\r\nprint(\"end\
    \ 1\")\r\ntokenizer = AutoTokenizer.from_pretrained(\"./gpt-j-6B\", low_cpu_mem_usage=True,\
    \ torch_dtype=torch.float16)\r\nprint(\"end 2\")\r\n\r\n# from parallelformers\
    \ import parallelize\r\n\r\n# parallelize(model, num_gpus=1, fp16=True, verbose='detail')\r\
    \n\r\nstart = time.time()\r\ninputs = tokenizer(\"My Name is Mukesh \", return_tensors=\"\
    pt\")\r\noutputs = model.generate(\r\n**inputs,\r\nnum_beams=5,\r\nno_repeat_ngram_size=4,\r\
    \nmax_length=200,\r\n)\r\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\"\
    )\r\nend = time.time()\r\nprint(end - start)\r\n\r\nstart = time.time()\r\ninputs\
    \ = tokenizer(\"Q:what is AI\\nA:AI is a cat.\\nQ:why?\\nA:\", return_tensors=\"\
    pt\")\r\noutputs = model.generate(\r\n**inputs,\r\nnum_beams=5,\r\nno_repeat_ngram_size=4,\r\
    \nmax_length=200,\r\n)\r\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\"\
    )\r\nend = time.time()\r\nprint(end - start)\r\n\r\nstart = time.time()\r\ninputs\
    \ = tokenizer(\"Q:\u4E16\u754C\u4E0A\u6700\u5927\u7684\u5730\u65B9\u662F\u54EA\
    \\nA:\u662F\u592A\u9633.\\nQ:\u592A\u9633\u662F\u4E16\u754C\u4E0A\u6700\u5927\u7684\
    \u5730\u65B9\u5417\uFF1F\\nA:\", return_tensors=\"pt\")\r\noutputs = model.generate(\r\
    \n**inputs,\r\nnum_beams=5,\r\nno_repeat_ngram_size=4,\r\nmax_length=200,\r\n\
    )\r\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\")\r\nend = time.time()\r\
    \nprint(end - start)\r\n```\r\nbut got an error with:\r\n```\r\nstart load\r\n\
    end 1\r\nend 2\r\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end\
    \ generation.\r\nTraceback (most recent call last):\r\n  File \"test.py\", line\
    \ 17, in <module>\r\n    outputs = model.generate(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\"\
    , line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"\
    /root/miniconda3/lib/python3.8/site-packages/transformers/generation/utils.py\"\
    , line 1608, in generate\r\n    return self.beam_search(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/generation/utils.py\"\
    , line 2799, in beam_search\r\n    outputs = self(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
    , line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File\
    \ \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py\"\
    , line 821, in forward\r\n    transformer_outputs = self.transformer(\r\n  File\
    \ \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
    , line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File\
    \ \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py\"\
    , line 676, in forward\r\n    outputs = block(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
    , line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File\
    \ \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/gptj/modeling_gptj.py\"\
    , line 309, in forward\r\n    hidden_states = self.ln_1(hidden_states)\r\n  File\
    \ \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
    , line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File\
    \ \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/normalization.py\"\
    , line 189, in forward\r\n    return F.layer_norm(\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\"\
    , line 2503, in layer_norm\r\n    return torch.layer_norm(input, normalized_shape,\
    \ weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: \"LayerNormKernelImpl\"\
    \ not implemented for 'Half'\r\n```"
  created_at: 2023-01-06 06:30:38+00:00
  edited: false
  hidden: false
  id: 63b7c00eca2f378e7103e612
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/213a574236d78b7a5cad3b6adde6a6b8.svg
      fullname: baicai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Baicai003
      type: user
    createdAt: '2023-01-06T17:50:51.000Z'
    data:
      edited: false
      editors:
      - Baicai003
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/213a574236d78b7a5cad3b6adde6a6b8.svg
          fullname: baicai
          isHf: false
          isPro: false
          name: Baicai003
          type: user
        html: "<p>this code works fine for me:</p>\n<pre><code>from transformers import\
          \ AutoModelForCausalLM, AutoTokenizer\nimport time\nimport torch\n\nprint(\"\
          start load\")\nmodel = AutoModelForCausalLM.from_pretrained(\"./gpt-j-6B\"\
          , low_cpu_mem_usage=True, torch_dtype=torch.float16)\nmodel = model.to(torch.device(\"\
          cuda:0\"))\nprint(\"end 1\")\ntokenizer = AutoTokenizer.from_pretrained(\"\
          ./gpt-j-6B\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\n# tokenizer\
          \ = tokenizer.to(torch.device(\"cuda:0\"))\nprint(\"end 2\")\n\n# from parallelformers\
          \ import parallelize\n\n# parallelize(model, num_gpus=1, fp16=True, verbose='detail')\n\
          \nstart = time.time()\n# \u4F7F\u7528float16\ninputs = tokenizer(\"My Name\
          \ is Mukesh \", return_tensors=\"pt\")\ninputs = inputs.to(torch.device(\"\
          cuda:0\"))\n\noutputs = model.generate(\n**inputs,\nnum_beams=5,\nno_repeat_ngram_size=4,\n\
          max_length=100,\n)\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\"\
          )\nend = time.time()\nprint(end - start)\n\nstart = time.time()\ninputs\
          \ = tokenizer(\"Q:what is AI\\nA:AI is a cat.\\nQ:why?\\nA:\", return_tensors=\"\
          pt\")\ninputs = inputs.to(torch.device(\"cuda:0\"))\noutputs = model.generate(\n\
          **inputs,\nnum_beams=5,\nno_repeat_ngram_size=4,\nmax_length=100,\n)\nprint(f\"\
          Output: {tokenizer.batch_decode(outputs)[0]}\")\nend = time.time()\nprint(end\
          \ - start)\n\nstart = time.time()\ninputs = tokenizer(\"Q:\u8BF7\u7528python\u5B9E\
          \u73B0CNN\\nA:\", return_tensors=\"pt\")\ninputs = inputs.to(torch.device(\"\
          cuda:0\"))\noutputs = model.generate(\n**inputs,\nnum_beams=5,\nno_repeat_ngram_size=4,\n\
          max_length=100,\n)\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\"\
          )\nend = time.time()\nprint(end - start)\n</code></pre>\n"
        raw: "this code works fine for me:\n```\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\nimport time\nimport torch\n\nprint(\"start load\")\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(\"./gpt-j-6B\", low_cpu_mem_usage=True,\
          \ torch_dtype=torch.float16)\nmodel = model.to(torch.device(\"cuda:0\"))\n\
          print(\"end 1\")\ntokenizer = AutoTokenizer.from_pretrained(\"./gpt-j-6B\"\
          , low_cpu_mem_usage=True, torch_dtype=torch.float16)\n# tokenizer = tokenizer.to(torch.device(\"\
          cuda:0\"))\nprint(\"end 2\")\n\n# from parallelformers import parallelize\n\
          \n# parallelize(model, num_gpus=1, fp16=True, verbose='detail')\n\nstart\
          \ = time.time()\n# \u4F7F\u7528float16\ninputs = tokenizer(\"My Name is\
          \ Mukesh \", return_tensors=\"pt\")\ninputs = inputs.to(torch.device(\"\
          cuda:0\"))\n\noutputs = model.generate(\n**inputs,\nnum_beams=5,\nno_repeat_ngram_size=4,\n\
          max_length=100,\n)\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\"\
          )\nend = time.time()\nprint(end - start)\n\nstart = time.time()\ninputs\
          \ = tokenizer(\"Q:what is AI\\nA:AI is a cat.\\nQ:why?\\nA:\", return_tensors=\"\
          pt\")\ninputs = inputs.to(torch.device(\"cuda:0\"))\noutputs = model.generate(\n\
          **inputs,\nnum_beams=5,\nno_repeat_ngram_size=4,\nmax_length=100,\n)\nprint(f\"\
          Output: {tokenizer.batch_decode(outputs)[0]}\")\nend = time.time()\nprint(end\
          \ - start)\n\nstart = time.time()\ninputs = tokenizer(\"Q:\u8BF7\u7528python\u5B9E\
          \u73B0CNN\\nA:\", return_tensors=\"pt\")\ninputs = inputs.to(torch.device(\"\
          cuda:0\"))\noutputs = model.generate(\n**inputs,\nnum_beams=5,\nno_repeat_ngram_size=4,\n\
          max_length=100,\n)\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\"\
          )\nend = time.time()\nprint(end - start)\n```"
        updatedAt: '2023-01-06T17:50:51.820Z'
      numEdits: 0
      reactions: []
      relatedEventId: 63b85f7b3e8441704866acae
    id: 63b85f7b3e8441704866acad
    type: comment
  author: Baicai003
  content: "this code works fine for me:\n```\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\nimport time\nimport torch\n\nprint(\"start load\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    ./gpt-j-6B\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\nmodel = model.to(torch.device(\"\
    cuda:0\"))\nprint(\"end 1\")\ntokenizer = AutoTokenizer.from_pretrained(\"./gpt-j-6B\"\
    , low_cpu_mem_usage=True, torch_dtype=torch.float16)\n# tokenizer = tokenizer.to(torch.device(\"\
    cuda:0\"))\nprint(\"end 2\")\n\n# from parallelformers import parallelize\n\n\
    # parallelize(model, num_gpus=1, fp16=True, verbose='detail')\n\nstart = time.time()\n\
    # \u4F7F\u7528float16\ninputs = tokenizer(\"My Name is Mukesh \", return_tensors=\"\
    pt\")\ninputs = inputs.to(torch.device(\"cuda:0\"))\n\noutputs = model.generate(\n\
    **inputs,\nnum_beams=5,\nno_repeat_ngram_size=4,\nmax_length=100,\n)\nprint(f\"\
    Output: {tokenizer.batch_decode(outputs)[0]}\")\nend = time.time()\nprint(end\
    \ - start)\n\nstart = time.time()\ninputs = tokenizer(\"Q:what is AI\\nA:AI is\
    \ a cat.\\nQ:why?\\nA:\", return_tensors=\"pt\")\ninputs = inputs.to(torch.device(\"\
    cuda:0\"))\noutputs = model.generate(\n**inputs,\nnum_beams=5,\nno_repeat_ngram_size=4,\n\
    max_length=100,\n)\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\")\n\
    end = time.time()\nprint(end - start)\n\nstart = time.time()\ninputs = tokenizer(\"\
    Q:\u8BF7\u7528python\u5B9E\u73B0CNN\\nA:\", return_tensors=\"pt\")\ninputs = inputs.to(torch.device(\"\
    cuda:0\"))\noutputs = model.generate(\n**inputs,\nnum_beams=5,\nno_repeat_ngram_size=4,\n\
    max_length=100,\n)\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\")\n\
    end = time.time()\nprint(end - start)\n```"
  created_at: 2023-01-06 17:50:51+00:00
  edited: false
  hidden: false
  id: 63b85f7b3e8441704866acad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/213a574236d78b7a5cad3b6adde6a6b8.svg
      fullname: baicai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Baicai003
      type: user
    createdAt: '2023-01-06T17:50:51.000Z'
    data:
      status: closed
    id: 63b85f7b3e8441704866acae
    type: status-change
  author: Baicai003
  created_at: 2023-01-06 17:50:51+00:00
  id: 63b85f7b3e8441704866acae
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: EleutherAI/gpt-j-6b
repo_type: model
status: closed
target_branch: null
title: error to run in float16
