!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LycheeX
conflicting_files: null
created_at: 2023-02-22 06:10:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63aa66da7930fa8c7de0c40a/tc1uIfN9HD5gnECQ9sEQV.jpeg?w=200&h=200&f=face
      fullname: TsuLychee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LycheeX
      type: user
    createdAt: '2023-02-22T06:10:29.000Z'
    data:
      edited: false
      editors:
      - LycheeX
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63aa66da7930fa8c7de0c40a/tc1uIfN9HD5gnECQ9sEQV.jpeg?w=200&h=200&f=face
          fullname: TsuLychee
          isHf: false
          isPro: false
          name: LycheeX
          type: user
        html: '<p>as said in title</p>

          '
        raw: as said in title
        updatedAt: '2023-02-22T06:10:29.628Z'
      numEdits: 0
      reactions: []
    id: 63f5b1d571a5d395c72543db
    type: comment
  author: LycheeX
  content: as said in title
  created_at: 2023-02-22 06:10:29+00:00
  edited: false
  hidden: false
  id: 63f5b1d571a5d395c72543db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63e85cbdfdb4097ef66887e8/Aq9D4znfBJGSbL5vgPU_v.jpeg?w=200&h=200&f=face
      fullname: Arnaud
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Aruno
      type: user
    createdAt: '2023-04-25T05:52:28.000Z'
    data:
      edited: false
      editors:
      - Aruno
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63e85cbdfdb4097ef66887e8/Aq9D4znfBJGSbL5vgPU_v.jpeg?w=200&h=200&f=face
          fullname: Arnaud
          isHf: false
          isPro: false
          name: Aruno
          type: user
        html: "<p>Response <a href=\"https://huggingface.co/docs/transformers/model_doc/t5#inference\"\
          >here</a>.</p>\n<p>My sample code:</p>\n<pre><code>from transformers import\
          \ AutoTokenizer, T5ForConditionalGeneration\nimport torch\n\ndevice:str\
          \ = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\"\
          ).to(device)\n\nfor prompt in [\"Hello, How are you?\", \"My name is Arnaud\"\
          ]:\n    print(\"Input:\", prompt)\n    inputTokens = tokenizer(\"translate\
          \ English to French: {}\".format(prompt), return_tensors=\"pt\").to(device)\n\
          \    outputs = model.generate(inputTokens['input_ids'], attention_mask=inputTokens['attention_mask'],\
          \ max_new_tokens=50)\n    print(\"Output:\", tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True))\n</code></pre>\n"
        raw: "Response [here](https://huggingface.co/docs/transformers/model_doc/t5#inference).\n\
          \nMy sample code:\n```\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\
          import torch\n\ndevice:str = 'cuda' if torch.cuda.is_available() else 'cpu'\n\
          \ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"\
          t5-small\").to(device)\n\nfor prompt in [\"Hello, How are you?\", \"My name\
          \ is Arnaud\"]:\n    print(\"Input:\", prompt)\n    inputTokens = tokenizer(\"\
          translate English to French: {}\".format(prompt), return_tensors=\"pt\"\
          ).to(device)\n    outputs = model.generate(inputTokens['input_ids'], attention_mask=inputTokens['attention_mask'],\
          \ max_new_tokens=50)\n    print(\"Output:\", tokenizer.decode(outputs[0],\
          \ skip_special_tokens=True))\n```"
        updatedAt: '2023-04-25T05:52:28.544Z'
      numEdits: 0
      reactions: []
    id: 64476a9c257773309c7949a3
    type: comment
  author: Aruno
  content: "Response [here](https://huggingface.co/docs/transformers/model_doc/t5#inference).\n\
    \nMy sample code:\n```\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\
    import torch\n\ndevice:str = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\
    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"\
    t5-small\").to(device)\n\nfor prompt in [\"Hello, How are you?\", \"My name is\
    \ Arnaud\"]:\n    print(\"Input:\", prompt)\n    inputTokens = tokenizer(\"translate\
    \ English to French: {}\".format(prompt), return_tensors=\"pt\").to(device)\n\
    \    outputs = model.generate(inputTokens['input_ids'], attention_mask=inputTokens['attention_mask'],\
    \ max_new_tokens=50)\n    print(\"Output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n\
    ```"
  created_at: 2023-04-25 04:52:28+00:00
  edited: false
  hidden: false
  id: 64476a9c257773309c7949a3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: t5-small
repo_type: model
status: open
target_branch: null
title: how to use the trained model to infer ?
