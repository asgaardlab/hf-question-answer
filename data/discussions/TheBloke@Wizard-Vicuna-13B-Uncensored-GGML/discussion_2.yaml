!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alphaprime90
conflicting_files: null
created_at: 2023-05-13 12:36:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66944b2b07a6b618f8fa46bff44ba427.svg
      fullname: 'alpha '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alphaprime90
      type: user
    createdAt: '2023-05-13T13:36:58.000Z'
    data:
      edited: false
      editors:
      - alphaprime90
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66944b2b07a6b618f8fa46bff44ba427.svg
          fullname: 'alpha '
          isHf: false
          isPro: false
          name: alphaprime90
          type: user
        html: '<p>Thank you TheBloke for the various quants options. How is 8bit treating
          you?</p>

          '
        raw: Thank you TheBloke for the various quants options. How is 8bit treating
          you?
        updatedAt: '2023-05-13T13:36:58.989Z'
      numEdits: 0
      reactions: []
    id: 645f927aa3e00932c9efd007
    type: comment
  author: alphaprime90
  content: Thank you TheBloke for the various quants options. How is 8bit treating
    you?
  created_at: 2023-05-13 12:36:58+00:00
  edited: false
  hidden: false
  id: 645f927aa3e00932c9efd007
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3dd8ff0e53b517d1dbf7f34ef625189a.svg
      fullname: Jakub Strnad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: creative420
      type: user
    createdAt: '2023-05-13T18:41:04.000Z'
    data:
      edited: true
      editors:
      - creative420
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3dd8ff0e53b517d1dbf7f34ef625189a.svg
          fullname: Jakub Strnad
          isHf: false
          isPro: false
          name: creative420
          type: user
        html: '<p>in my case 8 bit is actually around 30% faster than 4 or 5 bit.
          But it uses more RAM.<br>Windows 11, Ryzen 5900x</p>

          '
        raw: 'in my case 8 bit is actually around 30% faster than 4 or 5 bit. But
          it uses more RAM.

          Windows 11, Ryzen 5900x'
        updatedAt: '2023-05-13T18:41:59.044Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - matthoffner
        - alphaprime90
      - count: 1
        reaction: "\U0001F92F"
        users:
        - Psychopatz
    id: 645fd9c06990e1208526e7fd
    type: comment
  author: creative420
  content: 'in my case 8 bit is actually around 30% faster than 4 or 5 bit. But it
    uses more RAM.

    Windows 11, Ryzen 5900x'
  created_at: 2023-05-13 17:41:04+00:00
  edited: true
  hidden: false
  id: 645fd9c06990e1208526e7fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T18:48:59.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Wow faster? That''s surprising.</p>

          <p>I''ve not tested 8bit at all yet to be honest. I just threw it in there
          for completeness. I I''ll give it a go later to see how it goes</p>

          '
        raw: 'Wow faster? That''s surprising.


          I''ve not tested 8bit at all yet to be honest. I just threw it in there
          for completeness. I I''ll give it a go later to see how it goes'
        updatedAt: '2023-05-13T18:48:59.596Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - alphaprime90
        - Hanssep123
        - mirek190
    id: 645fdb9bd27fa024aa871321
    type: comment
  author: TheBloke
  content: 'Wow faster? That''s surprising.


    I''ve not tested 8bit at all yet to be honest. I just threw it in there for completeness.
    I I''ll give it a go later to see how it goes'
  created_at: 2023-05-13 17:48:59+00:00
  edited: false
  hidden: false
  id: 645fdb9bd27fa024aa871321
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/547aadaef26772dbacfe1fe97ce9d06d.svg
      fullname: Patrick Oliver Bustamante
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Psychopatz
      type: user
    createdAt: '2023-05-14T01:05:29.000Z'
    data:
      edited: false
      editors:
      - Psychopatz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/547aadaef26772dbacfe1fe97ce9d06d.svg
          fullname: Patrick Oliver Bustamante
          isHf: false
          isPro: false
          name: Psychopatz
          type: user
        html: '<blockquote>

          <p>in my case 8 bit is actually around 30% faster than 4 or 5 bit. But it
          uses more RAM.<br>Windows 11, Ryzen 5900x</p>

          </blockquote>

          <p>Sir, may I know how much ram does it consume? I have 16gb, will that
          be enough? thanks!</p>

          '
        raw: '> in my case 8 bit is actually around 30% faster than 4 or 5 bit. But
          it uses more RAM.

          > Windows 11, Ryzen 5900x


          Sir, may I know how much ram does it consume? I have 16gb, will that be
          enough? thanks!'
        updatedAt: '2023-05-14T01:05:29.971Z'
      numEdits: 0
      reactions: []
    id: 646033d96990e12085292673
    type: comment
  author: Psychopatz
  content: '> in my case 8 bit is actually around 30% faster than 4 or 5 bit. But
    it uses more RAM.

    > Windows 11, Ryzen 5900x


    Sir, may I know how much ram does it consume? I have 16gb, will that be enough?
    thanks!'
  created_at: 2023-05-14 00:05:29+00:00
  edited: false
  hidden: false
  id: 646033d96990e12085292673
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e51bfbed966ae1408d2c3ec26f2e4d4b.svg
      fullname: SR Jee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SRJee
      type: user
    createdAt: '2023-05-14T03:29:08.000Z'
    data:
      edited: false
      editors:
      - SRJee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e51bfbed966ae1408d2c3ec26f2e4d4b.svg
          fullname: SR Jee
          isHf: false
          isPro: false
          name: SRJee
          type: user
        html: '<p>I have a doubt.<br>Are 4_0  models faster than 5_0 models ?</p>

          '
        raw: "I have a doubt. \nAre 4_0  models faster than 5_0 models ?"
        updatedAt: '2023-05-14T03:29:08.331Z'
      numEdits: 0
      reactions: []
    id: 6460558472397238b23317f8
    type: comment
  author: SRJee
  content: "I have a doubt. \nAre 4_0  models faster than 5_0 models ?"
  created_at: 2023-05-14 02:29:08+00:00
  edited: false
  hidden: false
  id: 6460558472397238b23317f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e51bfbed966ae1408d2c3ec26f2e4d4b.svg
      fullname: SR Jee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SRJee
      type: user
    createdAt: '2023-05-14T03:30:04.000Z'
    data:
      edited: false
      editors:
      - SRJee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e51bfbed966ae1408d2c3ec26f2e4d4b.svg
          fullname: SR Jee
          isHf: false
          isPro: false
          name: SRJee
          type: user
        html: '<blockquote>

          <p>in my case 8 bit is actually around 30% faster than 4 or 5 bit. But it
          uses more RAM.<br>Windows 11, Ryzen 5900x</p>

          </blockquote>

          <p>Nice. I have a doubt. Which one is faster 4 bit or 5 bit?</p>

          '
        raw: '> in my case 8 bit is actually around 30% faster than 4 or 5 bit. But
          it uses more RAM.

          > Windows 11, Ryzen 5900x


          Nice. I have a doubt. Which one is faster 4 bit or 5 bit?'
        updatedAt: '2023-05-14T03:30:04.049Z'
      numEdits: 0
      reactions: []
    id: 646055bc72397238b233195a
    type: comment
  author: SRJee
  content: '> in my case 8 bit is actually around 30% faster than 4 or 5 bit. But
    it uses more RAM.

    > Windows 11, Ryzen 5900x


    Nice. I have a doubt. Which one is faster 4 bit or 5 bit?'
  created_at: 2023-05-14 02:30:04+00:00
  edited: false
  hidden: false
  id: 646055bc72397238b233195a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3dd8ff0e53b517d1dbf7f34ef625189a.svg
      fullname: Jakub Strnad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: creative420
      type: user
    createdAt: '2023-05-14T18:08:29.000Z'
    data:
      edited: true
      editors:
      - creative420
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3dd8ff0e53b517d1dbf7f34ef625189a.svg
          fullname: Jakub Strnad
          isHf: false
          isPro: false
          name: creative420
          type: user
        html: '<p>unfortunatelly i was trying different versions of llama-cpp-python
          and llama.cpp (mainly because of the new ggml version and gpu offloading)
          and i am not able to reproduce it on my main computer anymore, now the q8_0
          is slower. But on laptop i still have older version of oobabooga webui and
          there the q8_0 is faster than q5_1 . Even the readme of llama.cpp say that
          q8_0 is faster in some cases.  Although there is one good thing on q8_0,
          unlike q5_1 (i am not sure about the other types), even the older models
          works with newest llama.cpp<br>pip list: <a rel="nofollow" href="https://pastebin.com/sLkgSe2s">https://pastebin.com/sLkgSe2s</a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/637a9533806b18943e48c995/OnDRH10mRd39cntm7A4oV.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/637a9533806b18943e48c995/OnDRH10mRd39cntm7A4oV.png"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/637a9533806b18943e48c995/IhR_7odHeLmdD8tXfLzxe.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/637a9533806b18943e48c995/IhR_7odHeLmdD8tXfLzxe.png"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/637a9533806b18943e48c995/vA_tBPWvLR3llt2B0L5Vh.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/637a9533806b18943e48c995/vA_tBPWvLR3llt2B0L5Vh.png"></a></p>

          '
        raw: "unfortunatelly i was trying different versions of llama-cpp-python and\
          \ llama.cpp (mainly because of the new ggml version and gpu offloading)\
          \ and i am not able to reproduce it on my main computer anymore, now the\
          \ q8_0 is slower. But on laptop i still have older version of oobabooga\
          \ webui and there the q8_0 is faster than q5_1 . Even the readme of llama.cpp\
          \ say that q8_0 is faster in some cases.  Although there is one good thing\
          \ on q8_0, unlike q5_1 (i am not sure about the other types), even the older\
          \ models works with newest llama.cpp\npip list: https://pastebin.com/sLkgSe2s\
          \ \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/637a9533806b18943e48c995/OnDRH10mRd39cntm7A4oV.png)\n\
          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/637a9533806b18943e48c995/IhR_7odHeLmdD8tXfLzxe.png)\n\
          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/637a9533806b18943e48c995/vA_tBPWvLR3llt2B0L5Vh.png)"
        updatedAt: '2023-05-14T18:57:45.146Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - SRJee
        - dzupin
    id: 6461239d3cc3259ce05a9a31
    type: comment
  author: creative420
  content: "unfortunatelly i was trying different versions of llama-cpp-python and\
    \ llama.cpp (mainly because of the new ggml version and gpu offloading) and i\
    \ am not able to reproduce it on my main computer anymore, now the q8_0 is slower.\
    \ But on laptop i still have older version of oobabooga webui and there the q8_0\
    \ is faster than q5_1 . Even the readme of llama.cpp say that q8_0 is faster in\
    \ some cases.  Although there is one good thing on q8_0, unlike q5_1 (i am not\
    \ sure about the other types), even the older models works with newest llama.cpp\n\
    pip list: https://pastebin.com/sLkgSe2s \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/637a9533806b18943e48c995/OnDRH10mRd39cntm7A4oV.png)\n\
    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/637a9533806b18943e48c995/IhR_7odHeLmdD8tXfLzxe.png)\n\
    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/637a9533806b18943e48c995/vA_tBPWvLR3llt2B0L5Vh.png)"
  created_at: 2023-05-14 17:08:29+00:00
  edited: true
  hidden: false
  id: 6461239d3cc3259ce05a9a31
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e51bfbed966ae1408d2c3ec26f2e4d4b.svg
      fullname: SR Jee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SRJee
      type: user
    createdAt: '2023-05-14T21:27:11.000Z'
    data:
      edited: false
      editors:
      - SRJee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e51bfbed966ae1408d2c3ec26f2e4d4b.svg
          fullname: SR Jee
          isHf: false
          isPro: false
          name: SRJee
          type: user
        html: '<p>Nice! Thanks for the explanation.<br>I am also glad that this model
          is working with my old laptop. I will keep on using that version.<br>This
          new Wiz-Vic 13B Uncensored model is really amazing.</p>

          '
        raw: "Nice! Thanks for the explanation. \nI am also glad that this model is\
          \ working with my old laptop. I will keep on using that version.\nThis new\
          \ Wiz-Vic 13B Uncensored model is really amazing."
        updatedAt: '2023-05-14T21:27:11.596Z'
      numEdits: 0
      reactions: []
    id: 6461522f7735f76a4a525532
    type: comment
  author: SRJee
  content: "Nice! Thanks for the explanation. \nI am also glad that this model is\
    \ working with my old laptop. I will keep on using that version.\nThis new Wiz-Vic\
    \ 13B Uncensored model is really amazing."
  created_at: 2023-05-14 20:27:11+00:00
  edited: false
  hidden: false
  id: 6461522f7735f76a4a525532
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8790ed4b203b1fd78e650e06596e27f5.svg
      fullname: Michael Bui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MichaelBui
      type: user
    createdAt: '2023-05-15T15:27:01.000Z'
    data:
      edited: false
      editors:
      - MichaelBui
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8790ed4b203b1fd78e650e06596e27f5.svg
          fullname: Michael Bui
          isHf: false
          isPro: false
          name: MichaelBui
          type: user
        html: "<p>I'm running on Dell R520 (2 x CPU E5-2470 v2 @ 2.40GHz - 40 threads\
          \ in total). However, I installed Proxmox &amp; created a VM just for generative\
          \ AI experiments.</p>\n<p>This is the VM specs:</p>\n<ul>\n<li>32GB RAM</li>\n\
          <li>16 vCPU (threads) with only AVX enabled, not AVX2, AVX512,...</li>\n\
          </ul>\n<p>I'm running q8_0 with these args:</p>\n<pre><code>--cpu-memory\
          \ 20\n--threads 16 \n--wbits 8 \n--groupsize 128 \n--cpu \n--listen \n--chat\
          \ \n--verbose \n--no-stream \n--extensions long_term_memory sd_api_pictures\
          \ send_pictures \n--model Wizard-Vicuna-13B-Uncensored.ggml.q8_0\n</code></pre>\n\
          <p>Normally, I will get a performance ~ 1.0 ~ 1.2 tokens/s (~4-5m for 200\
          \ tokens)</p>\n"
        raw: "I'm running on Dell R520 (2 x CPU E5-2470 v2 @ 2.40GHz - 40 threads\
          \ in total). However, I installed Proxmox & created a VM just for generative\
          \ AI experiments.\n\nThis is the VM specs:\n- 32GB RAM\n- 16 vCPU (threads)\
          \ with only AVX enabled, not AVX2, AVX512,...\n\nI'm running q8_0 with these\
          \ args:\n```\n--cpu-memory 20\n--threads 16 \n--wbits 8 \n--groupsize 128\
          \ \n--cpu \n--listen \n--chat \n--verbose \n--no-stream \n--extensions long_term_memory\
          \ sd_api_pictures send_pictures \n--model Wizard-Vicuna-13B-Uncensored.ggml.q8_0\n\
          ```\nNormally, I will get a performance ~ 1.0 ~ 1.2 tokens/s (~4-5m for\
          \ 200 tokens)"
        updatedAt: '2023-05-15T15:27:01.980Z'
      numEdits: 0
      reactions: []
    id: 64624f45cce92c7d882c7016
    type: comment
  author: MichaelBui
  content: "I'm running on Dell R520 (2 x CPU E5-2470 v2 @ 2.40GHz - 40 threads in\
    \ total). However, I installed Proxmox & created a VM just for generative AI experiments.\n\
    \nThis is the VM specs:\n- 32GB RAM\n- 16 vCPU (threads) with only AVX enabled,\
    \ not AVX2, AVX512,...\n\nI'm running q8_0 with these args:\n```\n--cpu-memory\
    \ 20\n--threads 16 \n--wbits 8 \n--groupsize 128 \n--cpu \n--listen \n--chat \n\
    --verbose \n--no-stream \n--extensions long_term_memory sd_api_pictures send_pictures\
    \ \n--model Wizard-Vicuna-13B-Uncensored.ggml.q8_0\n```\nNormally, I will get\
    \ a performance ~ 1.0 ~ 1.2 tokens/s (~4-5m for 200 tokens)"
  created_at: 2023-05-15 14:27:01+00:00
  edited: false
  hidden: false
  id: 64624f45cce92c7d882c7016
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63037b895c70c21d0ea80b0e/myu35DuQn9io_HhxNLYR4.png?w=200&h=200&f=face
      fullname: Pooodle Shmith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: Tom9000
      type: user
    createdAt: '2023-05-15T23:32:40.000Z'
    data:
      edited: true
      editors:
      - Tom9000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63037b895c70c21d0ea80b0e/myu35DuQn9io_HhxNLYR4.png?w=200&h=200&f=face
          fullname: Pooodle Shmith
          isHf: false
          isPro: true
          name: Tom9000
          type: user
        html: '<p>Seems like AVX2 makes a bit difference.<br>With 13B models, I get
          around 360ms per token with q8_0, on only 6 threads, on my budget R7 5700G.<br>Although
          I still prefer q4_0 for that extra speed (200ms per token).</p>

          '
        raw: 'Seems like AVX2 makes a bit difference.

          With 13B models, I get around 360ms per token with q8_0, on only 6 threads,
          on my budget R7 5700G.

          Although I still prefer q4_0 for that extra speed (200ms per token).'
        updatedAt: '2023-05-15T23:33:15.524Z'
      numEdits: 1
      reactions: []
    id: 6462c1188e12f9ab9999a40c
    type: comment
  author: Tom9000
  content: 'Seems like AVX2 makes a bit difference.

    With 13B models, I get around 360ms per token with q8_0, on only 6 threads, on
    my budget R7 5700G.

    Although I still prefer q4_0 for that extra speed (200ms per token).'
  created_at: 2023-05-15 22:32:40+00:00
  edited: true
  hidden: false
  id: 6462c1188e12f9ab9999a40c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-17T20:31:38.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>I have with i9 9900 and q5_1 312 ms/token but with  -ngl 20  getting
          190 ms/token   :)</p>

          '
        raw: I have with i9 9900 and q5_1 312 ms/token but with  -ngl 20  getting
          190 ms/token   :)
        updatedAt: '2023-05-17T20:31:38.072Z'
      numEdits: 0
      reactions: []
    id: 646539aaa0748f9aa4c88c96
    type: comment
  author: mirek190
  content: I have with i9 9900 and q5_1 312 ms/token but with  -ngl 20  getting 190
    ms/token   :)
  created_at: 2023-05-17 19:31:38+00:00
  edited: false
  hidden: false
  id: 646539aaa0748f9aa4c88c96
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-18T01:17:41.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I can''t get -ngl to work for the life of me, keep getting out of
          memory... :(</p>

          '
        raw: I can't get -ngl to work for the life of me, keep getting out of memory...
          :(
        updatedAt: '2023-05-18T01:17:41.762Z'
      numEdits: 0
      reactions: []
    id: 64657cb5a0748f9aa4cc22e6
    type: comment
  author: mancub
  content: I can't get -ngl to work for the life of me, keep getting out of memory...
    :(
  created_at: 2023-05-18 00:17:41+00:00
  edited: false
  hidden: false
  id: 64657cb5a0748f9aa4cc22e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8790ed4b203b1fd78e650e06596e27f5.svg
      fullname: Michael Bui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MichaelBui
      type: user
    createdAt: '2023-05-18T01:41:04.000Z'
    data:
      edited: false
      editors:
      - MichaelBui
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8790ed4b203b1fd78e650e06596e27f5.svg
          fullname: Michael Bui
          isHf: false
          isPro: false
          name: MichaelBui
          type: user
        html: '<p>Interesting, which tool are you using to run this model? I''m using
          oobabooga but don''t see that flag</p>

          '
        raw: Interesting, which tool are you using to run this model? I'm using oobabooga
          but don't see that flag
        updatedAt: '2023-05-18T01:41:04.504Z'
      numEdits: 0
      reactions: []
    id: 64658230e8e31202cb557e1a
    type: comment
  author: MichaelBui
  content: Interesting, which tool are you using to run this model? I'm using oobabooga
    but don't see that flag
  created_at: 2023-05-18 00:41:04+00:00
  edited: false
  hidden: false
  id: 64658230e8e31202cb557e1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-18T06:50:50.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Llama.cpp using that -ngl<br>I have only 8GB card  etc 3060 so for
          13B model i can fit only 20 layers .<br>But 7B models I can fit all layers
          on my GPU ( 32 ) and models works 3x faster ...180ms vsc 58 ms </p>

          <p>Newest kobold as well but that parameter has different name .</p>

          <p>I think it''s time to buy etc 3090 with 24 GB as are very cheap  now
          - second hand I can buy for 600 euro :D .</p>

          '
        raw: "Llama.cpp using that -ngl \nI have only 8GB card  etc 3060 so for 13B\
          \ model i can fit only 20 layers .\nBut 7B models I can fit all layers on\
          \ my GPU ( 32 ) and models works 3x faster ...180ms vsc 58 ms \n\n\nNewest\
          \ kobold as well but that parameter has different name .\n\nI think it's\
          \ time to buy etc 3090 with 24 GB as are very cheap  now - second hand I\
          \ can buy for 600 euro :D ."
        updatedAt: '2023-05-18T06:50:50.306Z'
      numEdits: 0
      reactions: []
    id: 6465caca9c627c78f8616000
    type: comment
  author: mirek190
  content: "Llama.cpp using that -ngl \nI have only 8GB card  etc 3060 so for 13B\
    \ model i can fit only 20 layers .\nBut 7B models I can fit all layers on my GPU\
    \ ( 32 ) and models works 3x faster ...180ms vsc 58 ms \n\n\nNewest kobold as\
    \ well but that parameter has different name .\n\nI think it's time to buy etc\
    \ 3090 with 24 GB as are very cheap  now - second hand I can buy for 600 euro\
    \ :D ."
  created_at: 2023-05-18 05:50:50+00:00
  edited: false
  hidden: false
  id: 6465caca9c627c78f8616000
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
      fullname: Erik Scholz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Green-Sky
      type: user
    createdAt: '2023-05-18T12:56:11.000Z'
    data:
      edited: false
      editors:
      - Green-Sky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
          fullname: Erik Scholz
          isHf: false
          isPro: false
          name: Green-Sky
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mirek190&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mirek190\">@<span class=\"\
          underline\">mirek190</span></a></span>\n\n\t</span></span> you can put 33\
          \ layers into vram (+1 output layer), might get even faster :)</p>\n"
        raw: '@mirek190 you can put 33 layers into vram (+1 output layer), might get
          even faster :)'
        updatedAt: '2023-05-18T12:56:11.597Z'
      numEdits: 0
      reactions: []
    id: 6466206b119ad94383cd5259
    type: comment
  author: Green-Sky
  content: '@mirek190 you can put 33 layers into vram (+1 output layer), might get
    even faster :)'
  created_at: 2023-05-18 11:56:11+00:00
  edited: false
  hidden: false
  id: 6466206b119ad94383cd5259
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3dd8ff0e53b517d1dbf7f34ef625189a.svg
      fullname: Jakub Strnad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: creative420
      type: user
    createdAt: '2023-05-18T13:20:48.000Z'
    data:
      edited: false
      editors:
      - creative420
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3dd8ff0e53b517d1dbf7f34ef625189a.svg
          fullname: Jakub Strnad
          isHf: false
          isPro: false
          name: creative420
          type: user
        html: '<blockquote>

          <p>I can''t get -ngl to work for the life of me, keep getting out of memory...
          :(</p>

          </blockquote>

          <p>if you are using oobabooga it could be because llama.cpp doesnt free
          vram, i always have to close the app completely and turn again to free the
          vram after i use ngl</p>

          '
        raw: '> I can''t get -ngl to work for the life of me, keep getting out of
          memory... :(


          if you are using oobabooga it could be because llama.cpp doesnt free vram,
          i always have to close the app completely and turn again to free the vram
          after i use ngl'
        updatedAt: '2023-05-18T13:20:48.000Z'
      numEdits: 0
      reactions: []
    id: 64662630e0fe831b478f0c70
    type: comment
  author: creative420
  content: '> I can''t get -ngl to work for the life of me, keep getting out of memory...
    :(


    if you are using oobabooga it could be because llama.cpp doesnt free vram, i always
    have to close the app completely and turn again to free the vram after i use ngl'
  created_at: 2023-05-18 12:20:48+00:00
  edited: false
  hidden: false
  id: 64662630e0fe831b478f0c70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-18T23:12:12.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;mirek190&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mirek190\"\
          >@<span class=\"underline\">mirek190</span></a></span>\n\n\t</span></span>\
          \ you can put 33 layers into vram (+1 output layer), might get even faster\
          \ :)</p>\n</blockquote>\n<p>Omg ... you're right ... adding one extra layer\
          \ giving even better speed \U0001F604</p>\n"
        raw: "> @mirek190 you can put 33 layers into vram (+1 output layer), might\
          \ get even faster :)\n\nOmg ... you're right ... adding one extra layer\
          \ giving even better speed \U0001F604"
        updatedAt: '2023-05-18T23:12:12.377Z'
      numEdits: 0
      reactions: []
    id: 6466b0cce0fe831b4798ed0e
    type: comment
  author: mirek190
  content: "> @mirek190 you can put 33 layers into vram (+1 output layer), might get\
    \ even faster :)\n\nOmg ... you're right ... adding one extra layer giving even\
    \ better speed \U0001F604"
  created_at: 2023-05-18 22:12:12+00:00
  edited: false
  hidden: false
  id: 6466b0cce0fe831b4798ed0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-19T00:27:59.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I can''t get -ngl to work for the life of me, keep getting out of memory...
          :(</p>

          </blockquote>

          <p>if you are using oobabooga it could be because llama.cpp doesnt free
          vram, i always have to close the app completely and turn again to free the
          vram after i use ngl</p>

          </blockquote>

          <p>I''m actually using <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a>
          directly, have not tried this with oobabooga.</p>

          <p>I have a 3090 here and with 24GB I''d think any 13B 4bit model should
          be working without any issues. Yet no matter the number of layers I set,
          I always get OOM. It works without it, but then what''s the point, no? :)</p>

          <p>There was some issue logged in the llama.cpp repo about this, but they
          marked it as resolved. Guess it hasn''t been truly resolved, and I''m stumped.</p>

          '
        raw: "> > I can't get -ngl to work for the life of me, keep getting out of\
          \ memory... :(\n> \n> if you are using oobabooga it could be because llama.cpp\
          \ doesnt free vram, i always have to close the app completely and turn again\
          \ to free the vram after i use ngl\n\nI'm actually using https://github.com/ggerganov/llama.cpp\
          \ directly, have not tried this with oobabooga.\n\nI have a 3090 here and\
          \ with 24GB I'd think any 13B 4bit model should be working without any issues.\
          \ Yet no matter the number of layers I set, I always get OOM. It works without\
          \ it, but then what's the point, no? :)\n\nThere was some issue logged in\
          \ the llama.cpp repo about this, but they marked it as resolved. Guess it\
          \ hasn't been truly resolved, and I'm stumped."
        updatedAt: '2023-05-19T00:27:59.301Z'
      numEdits: 0
      reactions: []
    id: 6466c28fe65dce8e4330fc22
    type: comment
  author: mancub
  content: "> > I can't get -ngl to work for the life of me, keep getting out of memory...\
    \ :(\n> \n> if you are using oobabooga it could be because llama.cpp doesnt free\
    \ vram, i always have to close the app completely and turn again to free the vram\
    \ after i use ngl\n\nI'm actually using https://github.com/ggerganov/llama.cpp\
    \ directly, have not tried this with oobabooga.\n\nI have a 3090 here and with\
    \ 24GB I'd think any 13B 4bit model should be working without any issues. Yet\
    \ no matter the number of layers I set, I always get OOM. It works without it,\
    \ but then what's the point, no? :)\n\nThere was some issue logged in the llama.cpp\
    \ repo about this, but they marked it as resolved. Guess it hasn't been truly\
    \ resolved, and I'm stumped."
  created_at: 2023-05-18 23:27:59+00:00
  edited: false
  hidden: false
  id: 6466c28fe65dce8e4330fc22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-19T07:59:23.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Strange.. for me -ngl works even with 65b model but I can fit omly
          7 layers on 8 GB card :)</p>

          <p>It looks like that for me with -ngl<br>7B - 32<br>13B - 25<br>30B - 14<br>65B
          - 7</p>

          '
        raw: "Strange.. for me -ngl works even with 65b model but I can fit omly 7\
          \ layers on 8 GB card :)\n\nIt looks like that for me with -ngl\n7B - 32\
          \ \n13B - 25\n30B - 14\n65B - 7"
        updatedAt: '2023-05-19T07:59:23.835Z'
      numEdits: 0
      reactions: []
    id: 64672c5be92e2372d5c8ee9e
    type: comment
  author: mirek190
  content: "Strange.. for me -ngl works even with 65b model but I can fit omly 7 layers\
    \ on 8 GB card :)\n\nIt looks like that for me with -ngl\n7B - 32 \n13B - 25\n\
    30B - 14\n65B - 7"
  created_at: 2023-05-19 06:59:23+00:00
  edited: false
  hidden: false
  id: 64672c5be92e2372d5c8ee9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3dd8ff0e53b517d1dbf7f34ef625189a.svg
      fullname: Jakub Strnad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: creative420
      type: user
    createdAt: '2023-05-19T09:02:51.000Z'
    data:
      edited: false
      editors:
      - creative420
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3dd8ff0e53b517d1dbf7f34ef625189a.svg
          fullname: Jakub Strnad
          isHf: false
          isPro: false
          name: creative420
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>I can''t get -ngl to work for the life of me, keep getting out of memory...
          :(</p>

          </blockquote>

          <p>if you are using oobabooga it could be because llama.cpp doesnt free
          vram, i always have to close the app completely and turn again to free the
          vram after i use ngl</p>

          </blockquote>

          <p>I''m actually using <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a>
          directly, have not tried this with oobabooga.</p>

          <p>I have a 3090 here and with 24GB I''d think any 13B 4bit model should
          be working without any issues. Yet no matter the number of layers I set,
          I always get OOM. It works without it, but then what''s the point, no? :)</p>

          <p>There was some issue logged in the llama.cpp repo about this, but they
          marked it as resolved. Guess it hasn''t been truly resolved, and I''m stumped.</p>

          </blockquote>

          <p>i see, i haven''t tried it directly. I am using oobabooga and it works
          well for me. the only downside is that i have to close oobabooga completely
          before i switch to new model because of the memory releasing problem. I
          also have 3090 but i havent found any benefit in using ngl for 13b models,
          in that case, GPTQ is nearly 3x faster. I use ngl for 65B models, i can
          offload half of the model in gpu and the speed is about 66% faster than
          purely on cpu. 1.66t/s in my case (3090ti, ryzen 5900x, 64GB RAM, linux).
          on windows i had just 1t/s in same settings</p>

          '
        raw: "> > > I can't get -ngl to work for the life of me, keep getting out\
          \ of memory... :(\n> > \n> > if you are using oobabooga it could be because\
          \ llama.cpp doesnt free vram, i always have to close the app completely\
          \ and turn again to free the vram after i use ngl\n> \n> I'm actually using\
          \ https://github.com/ggerganov/llama.cpp directly, have not tried this with\
          \ oobabooga.\n> \n> I have a 3090 here and with 24GB I'd think any 13B 4bit\
          \ model should be working without any issues. Yet no matter the number of\
          \ layers I set, I always get OOM. It works without it, but then what's the\
          \ point, no? :)\n> \n> There was some issue logged in the llama.cpp repo\
          \ about this, but they marked it as resolved. Guess it hasn't been truly\
          \ resolved, and I'm stumped.\n\ni see, i haven't tried it directly. I am\
          \ using oobabooga and it works well for me. the only downside is that i\
          \ have to close oobabooga completely before i switch to new model because\
          \ of the memory releasing problem. I also have 3090 but i havent found any\
          \ benefit in using ngl for 13b models, in that case, GPTQ is nearly 3x faster.\
          \ I use ngl for 65B models, i can offload half of the model in gpu and the\
          \ speed is about 66% faster than purely on cpu. 1.66t/s in my case (3090ti,\
          \ ryzen 5900x, 64GB RAM, linux). on windows i had just 1t/s in same settings"
        updatedAt: '2023-05-19T09:02:51.082Z'
      numEdits: 0
      reactions: []
    id: 64673b3be92e2372d5ca0d25
    type: comment
  author: creative420
  content: "> > > I can't get -ngl to work for the life of me, keep getting out of\
    \ memory... :(\n> > \n> > if you are using oobabooga it could be because llama.cpp\
    \ doesnt free vram, i always have to close the app completely and turn again to\
    \ free the vram after i use ngl\n> \n> I'm actually using https://github.com/ggerganov/llama.cpp\
    \ directly, have not tried this with oobabooga.\n> \n> I have a 3090 here and\
    \ with 24GB I'd think any 13B 4bit model should be working without any issues.\
    \ Yet no matter the number of layers I set, I always get OOM. It works without\
    \ it, but then what's the point, no? :)\n> \n> There was some issue logged in\
    \ the llama.cpp repo about this, but they marked it as resolved. Guess it hasn't\
    \ been truly resolved, and I'm stumped.\n\ni see, i haven't tried it directly.\
    \ I am using oobabooga and it works well for me. the only downside is that i have\
    \ to close oobabooga completely before i switch to new model because of the memory\
    \ releasing problem. I also have 3090 but i havent found any benefit in using\
    \ ngl for 13b models, in that case, GPTQ is nearly 3x faster. I use ngl for 65B\
    \ models, i can offload half of the model in gpu and the speed is about 66% faster\
    \ than purely on cpu. 1.66t/s in my case (3090ti, ryzen 5900x, 64GB RAM, linux).\
    \ on windows i had just 1t/s in same settings"
  created_at: 2023-05-19 08:02:51+00:00
  edited: false
  hidden: false
  id: 64673b3be92e2372d5ca0d25
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-19T10:48:48.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Wait ...you have 1.6t/s with half layers on rtx 3090 ?<br>That''s
          actually slow .<br>I have on CPU ( i9 9900 ) 64 GB ram 1.3 t/s ....<br>But
          I''m using llama.cpp</p>

          '
        raw: 'Wait ...you have 1.6t/s with half layers on rtx 3090 ?

          That''s actually slow .

          I have on CPU ( i9 9900 ) 64 GB ram 1.3 t/s ....

          But I''m using llama.cpp'
        updatedAt: '2023-05-19T10:49:10.095Z'
      numEdits: 1
      reactions: []
    id: 64675410e92e2372d5cbd723
    type: comment
  author: mirek190
  content: 'Wait ...you have 1.6t/s with half layers on rtx 3090 ?

    That''s actually slow .

    I have on CPU ( i9 9900 ) 64 GB ram 1.3 t/s ....

    But I''m using llama.cpp'
  created_at: 2023-05-19 09:48:48+00:00
  edited: true
  hidden: false
  id: 64675410e92e2372d5cbd723
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3dd8ff0e53b517d1dbf7f34ef625189a.svg
      fullname: Jakub Strnad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: creative420
      type: user
    createdAt: '2023-05-19T11:22:18.000Z'
    data:
      edited: false
      editors:
      - creative420
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3dd8ff0e53b517d1dbf7f34ef625189a.svg
          fullname: Jakub Strnad
          isHf: false
          isPro: false
          name: creative420
          type: user
        html: '<p>i am talking about 65B model.. oobabooga+llama.cpp with 40 layers
          offloaded to gpu. after some tweaks i have 1.7t/s. you say that it can be
          improved?</p>

          '
        raw: i am talking about 65B model.. oobabooga+llama.cpp with 40 layers offloaded
          to gpu. after some tweaks i have 1.7t/s. you say that it can be improved?
        updatedAt: '2023-05-19T11:22:18.873Z'
      numEdits: 0
      reactions: []
    id: 64675beae92e2372d5cc6026
    type: comment
  author: creative420
  content: i am talking about 65B model.. oobabooga+llama.cpp with 40 layers offloaded
    to gpu. after some tweaks i have 1.7t/s. you say that it can be improved?
  created_at: 2023-05-19 10:22:18+00:00
  edited: false
  hidden: false
  id: 64675beae92e2372d5cc6026
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-19T16:08:14.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: "<p>Sorry that is my mistake ... I thought you said  1.6 s/t  ...lol<br>You\
          \ actually said 1.6t/s with rtx 3090 \U0001F605.</p>\n"
        raw: "Sorry that is my mistake ... I thought you said  1.6 s/t  ...lol \n\
          You actually said 1.6t/s with rtx 3090 \U0001F605."
        updatedAt: '2023-05-19T16:08:14.359Z'
      numEdits: 0
      reactions: []
    id: 64679eeea48c2b6f0d63540f
    type: comment
  author: mirek190
  content: "Sorry that is my mistake ... I thought you said  1.6 s/t  ...lol \nYou\
    \ actually said 1.6t/s with rtx 3090 \U0001F605."
  created_at: 2023-05-19 15:08:14+00:00
  edited: false
  hidden: false
  id: 64679eeea48c2b6f0d63540f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-20T02:11:54.000Z'
    data:
      edited: true
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>I went ahead and installed CUDA-12.1 thinking that llama.cpp might
          need the latest CUDA to compile with CUBLAS, but even with that I''m getting
          OOM. And this is with the latest git pull as of now.</p>

          <p>Starting llama.cpp with the following line:</p>

          <p>./main -t 20 --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -m models/wizard-mega-13B.ggml.q5_0.bin
          -r "user " --interactive-first -ngl 40</p>

          <p>This works in native Windows (~ 6t/s) using the pre-compiled binary with
          CUBLAS support, but in WSL (which I prefer to run in) it doesn''t and gives
          OOM.<br>....<br>WARNING: failed to allocate 1602.00 MB of pinned memory:
          out of memory<br>llama_init_from_file: kv self size  = 1600.00 MB<br>WARNING:
          failed to allocate 512.00 MB of pinned memory: out of memory<br>WARNING:
          failed to allocate 512.00 MB of pinned memory: out of memory<br>...<br>CUDA
          error 2 at ggml-cuda.cu:693: out of memory</p>

          <p>So....</p>

          <p>For anyone who is trying this in WSL, it appears Pinned Memory is limited,
          as per <a rel="nofollow" href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html#known-limitations-for-linux-cuda-applications">https://docs.nvidia.com/cuda/wsl-user-guide/index.html#known-limitations-for-linux-cuda-applications</a></p>

          <p>The solution for this is to add enviroment variable GGML_CUDA_NO_PINNED=1,
          as per <a rel="nofollow" href="https://github.com/abetlen/llama-cpp-python/issues/229#issuecomment-1553800926">https://github.com/abetlen/llama-cpp-python/issues/229#issuecomment-1553800926</a></p>

          '
        raw: 'I went ahead and installed CUDA-12.1 thinking that llama.cpp might need
          the latest CUDA to compile with CUBLAS, but even with that I''m getting
          OOM. And this is with the latest git pull as of now.


          Starting llama.cpp with the following line:


          ./main -t 20 --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -m models/wizard-mega-13B.ggml.q5_0.bin
          -r "user " --interactive-first -ngl 40


          This works in native Windows (~ 6t/s) using the pre-compiled binary with
          CUBLAS support, but in WSL (which I prefer to run in) it doesn''t and gives
          OOM.

          ....

          WARNING: failed to allocate 1602.00 MB of pinned memory: out of memory

          llama_init_from_file: kv self size  = 1600.00 MB

          WARNING: failed to allocate 512.00 MB of pinned memory: out of memory

          WARNING: failed to allocate 512.00 MB of pinned memory: out of memory

          ...

          CUDA error 2 at ggml-cuda.cu:693: out of memory


          So....


          For anyone who is trying this in WSL, it appears Pinned Memory is limited,
          as per https://docs.nvidia.com/cuda/wsl-user-guide/index.html#known-limitations-for-linux-cuda-applications


          The solution for this is to add enviroment variable GGML_CUDA_NO_PINNED=1,
          as per https://github.com/abetlen/llama-cpp-python/issues/229#issuecomment-1553800926'
        updatedAt: '2023-05-20T02:23:14.157Z'
      numEdits: 1
      reactions: []
    id: 64682c6ae92e2372d5d90301
    type: comment
  author: mancub
  content: 'I went ahead and installed CUDA-12.1 thinking that llama.cpp might need
    the latest CUDA to compile with CUBLAS, but even with that I''m getting OOM. And
    this is with the latest git pull as of now.


    Starting llama.cpp with the following line:


    ./main -t 20 --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -m models/wizard-mega-13B.ggml.q5_0.bin
    -r "user " --interactive-first -ngl 40


    This works in native Windows (~ 6t/s) using the pre-compiled binary with CUBLAS
    support, but in WSL (which I prefer to run in) it doesn''t and gives OOM.

    ....

    WARNING: failed to allocate 1602.00 MB of pinned memory: out of memory

    llama_init_from_file: kv self size  = 1600.00 MB

    WARNING: failed to allocate 512.00 MB of pinned memory: out of memory

    WARNING: failed to allocate 512.00 MB of pinned memory: out of memory

    ...

    CUDA error 2 at ggml-cuda.cu:693: out of memory


    So....


    For anyone who is trying this in WSL, it appears Pinned Memory is limited, as
    per https://docs.nvidia.com/cuda/wsl-user-guide/index.html#known-limitations-for-linux-cuda-applications


    The solution for this is to add enviroment variable GGML_CUDA_NO_PINNED=1, as
    per https://github.com/abetlen/llama-cpp-python/issues/229#issuecomment-1553800926'
  created_at: 2023-05-20 01:11:54+00:00
  edited: true
  hidden: false
  id: 64682c6ae92e2372d5d90301
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-20T02:22:31.000Z'
    data:
      edited: true
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Actually, I realized I made a mistake...I started the command with
          20 threads but only had 12 enabled in WSL, duh.</p>

          <p>With -t 12 and -ngl 40 I''m getting a very usable speed, finally, ~ 10t/s</p>

          <p>EDIT:</p>

          <p>Not sure if I''m doing this right (running a test), but for example,
          running:</p>

          <p>GGML_CUDA_NO_PINNED=1 ./main -t 12 -ngl 40 --color -c 2048 --temp 0.7
          --repeat_penalty 1.1 -m models/wizard-mega-13B.ggml.q5_0.bin -n 2048 --mtest
          -p "### Instruction: write a story about llamas ### Response:"</p>

          <p>I get this result back:</p>

          <p>llama_print_timings:        load time = 11020.03 ms<br>llama_print_timings:      sample
          time =     0.00 ms /     1 runs   (    0.00 ms per token)<br>llama_print_timings:
          prompt eval time =  4942.57 ms /   512 tokens (    9.65 ms per token)<br>llama_print_timings:        eval
          time =   136.34 ms /     1 runs   (  136.34 ms per token)<br>llama_print_timings:       total
          time = 11156.38 ms</p>

          <p>Is there a better way to get some measurements of performance?</p>

          '
        raw: 'Actually, I realized I made a mistake...I started the command with 20
          threads but only had 12 enabled in WSL, duh.


          With -t 12 and -ngl 40 I''m getting a very usable speed, finally, ~ 10t/s


          EDIT:


          Not sure if I''m doing this right (running a test), but for example, running:


          GGML_CUDA_NO_PINNED=1 ./main -t 12 -ngl 40 --color -c 2048 --temp 0.7 --repeat_penalty
          1.1 -m models/wizard-mega-13B.ggml.q5_0.bin -n 2048 --mtest -p "### Instruction:
          write a story about llamas ### Response:"


          I get this result back:


          llama_print_timings:        load time = 11020.03 ms

          llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00
          ms per token)

          llama_print_timings: prompt eval time =  4942.57 ms /   512 tokens (    9.65
          ms per token)

          llama_print_timings:        eval time =   136.34 ms /     1 runs   (  136.34
          ms per token)

          llama_print_timings:       total time = 11156.38 ms


          Is there a better way to get some measurements of performance?'
        updatedAt: '2023-05-20T02:30:54.211Z'
      numEdits: 3
      reactions: []
    id: 64682ee7a48c2b6f0d6b3797
    type: comment
  author: mancub
  content: 'Actually, I realized I made a mistake...I started the command with 20
    threads but only had 12 enabled in WSL, duh.


    With -t 12 and -ngl 40 I''m getting a very usable speed, finally, ~ 10t/s


    EDIT:


    Not sure if I''m doing this right (running a test), but for example, running:


    GGML_CUDA_NO_PINNED=1 ./main -t 12 -ngl 40 --color -c 2048 --temp 0.7 --repeat_penalty
    1.1 -m models/wizard-mega-13B.ggml.q5_0.bin -n 2048 --mtest -p "### Instruction:
    write a story about llamas ### Response:"


    I get this result back:


    llama_print_timings:        load time = 11020.03 ms

    llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms
    per token)

    llama_print_timings: prompt eval time =  4942.57 ms /   512 tokens (    9.65 ms
    per token)

    llama_print_timings:        eval time =   136.34 ms /     1 runs   (  136.34 ms
    per token)

    llama_print_timings:       total time = 11156.38 ms


    Is there a better way to get some measurements of performance?'
  created_at: 2023-05-20 01:22:31+00:00
  edited: true
  hidden: false
  id: 64682ee7a48c2b6f0d6b3797
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Wizard-Vicuna-13B-Uncensored-GGML
repo_type: model
status: open
target_branch: null
title: New quant 8bit method, how is it performing on your CPU? (share your token/s,
  CPU model and -- thread)
