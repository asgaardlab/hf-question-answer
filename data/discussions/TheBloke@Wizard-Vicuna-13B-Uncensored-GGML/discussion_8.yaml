!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YaTharThShaRma999
conflicting_files: null
created_at: 2023-06-20 09:31:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-06-20T10:31:44.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.641221284866333
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>Awesome model but when i use it, it doesnt generate token by token
          but only when the whole response is complete. Is there a way to do token
          by token?<br>This is my current code and im using it with llama cpp right
          now and colab.</p>

          <p>!pip install llama-cpp-python<br>from llama_cpp import Llama<br>llm =
          Llama(model_path="/content/Wizard-Vicuna-13B-Uncensored.ggmlv3.q2_K.bin")<br>message
          = "Hello, how are you?"<br>output = llm("USER:" + message +  "ASSISTANT:",
          max_tokens=32, stop=["USER:", "\n"], echo=True)<br>print(output)</p>

          '
        raw: "Awesome model but when i use it, it doesnt generate token by token but\
          \ only when the whole response is complete. Is there a way to do token by\
          \ token? \r\nThis is my current code and im using it with llama cpp right\
          \ now and colab.\r\n\r\n!pip install llama-cpp-python \r\nfrom llama_cpp\
          \ import Llama\r\nllm = Llama(model_path=\"/content/Wizard-Vicuna-13B-Uncensored.ggmlv3.q2_K.bin\"\
          )\r\nmessage = \"Hello, how are you?\"\r\noutput = llm(\"USER:\" + message\
          \ +  \"ASSISTANT:\", max_tokens=32, stop=[\"USER:\", \"\\n\"], echo=True)\r\
          \nprint(output)"
        updatedAt: '2023-06-20T10:31:44.655Z'
      numEdits: 0
      reactions: []
    id: 649180106cadae13f22e3b0b
    type: comment
  author: YaTharThShaRma999
  content: "Awesome model but when i use it, it doesnt generate token by token but\
    \ only when the whole response is complete. Is there a way to do token by token?\
    \ \r\nThis is my current code and im using it with llama cpp right now and colab.\r\
    \n\r\n!pip install llama-cpp-python \r\nfrom llama_cpp import Llama\r\nllm = Llama(model_path=\"\
    /content/Wizard-Vicuna-13B-Uncensored.ggmlv3.q2_K.bin\")\r\nmessage = \"Hello,\
    \ how are you?\"\r\noutput = llm(\"USER:\" + message +  \"ASSISTANT:\", max_tokens=32,\
    \ stop=[\"USER:\", \"\\n\"], echo=True)\r\nprint(output)"
  created_at: 2023-06-20 09:31:44+00:00
  edited: false
  hidden: false
  id: 649180106cadae13f22e3b0b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-20T10:37:01.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6137391328811646
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Here's a llama-cpp-python script that shows the response word by\
          \ word, as well as all at once at the end (you can remove that line):</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ llama_cpp <span class=\"hljs-keyword\">import</span> Llama\n<span class=\"\
          hljs-keyword\">import</span> random\nllm = Llama(model_path=<span class=\"\
          hljs-string\">\"tulu-7b.ggmlv3.q2_K.bin\"</span>, n_gpu_layers=<span class=\"\
          hljs-number\">40</span>, seed=random.randint(<span class=\"hljs-number\"\
          >1</span>, <span class=\"hljs-number\">2</span>**<span class=\"hljs-number\"\
          >31</span>))\ntokens = llm.tokenize(<span class=\"hljs-string\">b\"### Instruction:\
          \ write a story about llamas\\n### Response:\"</span>)\n\noutput = <span\
          \ class=\"hljs-string\">b\"\"</span>\ncount = <span class=\"hljs-number\"\
          >0</span>\n<span class=\"hljs-keyword\">for</span> token <span class=\"\
          hljs-keyword\">in</span> llm.generate(tokens, top_k=<span class=\"hljs-number\"\
          >50</span>, top_p=<span class=\"hljs-number\">0.73</span>, temp=<span class=\"\
          hljs-number\">0.72</span>, repeat_penalty=<span class=\"hljs-number\">1.1</span>):\n\
          \     text = llm.detokenize([token])\n     <span class=\"hljs-built_in\"\
          >print</span>(text.decode(), end=<span class=\"hljs-string\">''</span>,\
          \ flush=<span class=\"hljs-literal\">True</span>)\n     output += text\n\
          \n     count +=<span class=\"hljs-number\">1</span>\n     <span class=\"\
          hljs-keyword\">if</span> count &gt;= <span class=\"hljs-number\">500</span>\
          \ <span class=\"hljs-keyword\">or</span> (token == llm.token_eos()):\n \
          \        <span class=\"hljs-keyword\">break</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\nFull response:\"</span>,\
          \ output.decode())\n</code></pre>\n"
        raw: "Here's a llama-cpp-python script that shows the response word by word,\
          \ as well as all at once at the end (you can remove that line):\n\n```python\n\
          from llama_cpp import Llama\nimport random\nllm = Llama(model_path=\"tulu-7b.ggmlv3.q2_K.bin\"\
          , n_gpu_layers=40, seed=random.randint(1, 2**31))\ntokens = llm.tokenize(b\"\
          ### Instruction: write a story about llamas\\n### Response:\")\n\noutput\
          \ = b\"\"\ncount = 0\nfor token in llm.generate(tokens, top_k=50, top_p=0.73,\
          \ temp=0.72, repeat_penalty=1.1):\n     text = llm.detokenize([token])\n\
          \     print(text.decode(), end='', flush=True)\n     output += text\n\n\
          \     count +=1\n     if count >= 500 or (token == llm.token_eos()):\n \
          \        break\n\nprint(\"\\n\\nFull response:\", output.decode())\n```"
        updatedAt: '2023-06-20T10:41:07.743Z'
      numEdits: 2
      reactions: []
    id: 6491814d4f91f411046d86d4
    type: comment
  author: TheBloke
  content: "Here's a llama-cpp-python script that shows the response word by word,\
    \ as well as all at once at the end (you can remove that line):\n\n```python\n\
    from llama_cpp import Llama\nimport random\nllm = Llama(model_path=\"tulu-7b.ggmlv3.q2_K.bin\"\
    , n_gpu_layers=40, seed=random.randint(1, 2**31))\ntokens = llm.tokenize(b\"###\
    \ Instruction: write a story about llamas\\n### Response:\")\n\noutput = b\"\"\
    \ncount = 0\nfor token in llm.generate(tokens, top_k=50, top_p=0.73, temp=0.72,\
    \ repeat_penalty=1.1):\n     text = llm.detokenize([token])\n     print(text.decode(),\
    \ end='', flush=True)\n     output += text\n\n     count +=1\n     if count >=\
    \ 500 or (token == llm.token_eos()):\n         break\n\nprint(\"\\n\\nFull response:\"\
    , output.decode())\n```"
  created_at: 2023-06-20 09:37:01+00:00
  edited: true
  hidden: false
  id: 6491814d4f91f411046d86d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-06-20T11:48:10.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8864230513572693
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>Awesome, it seems to work perfectly! however I just did llm = Llama(model_path="/content/Wizard-Vicuna-13B-Uncensored.ggmlv3.q2_K.bin")
          but  what does the n_gpu_layers do? and seed=random.randint(1, 2**31)? I
          am also doing cpu right now so should i add something?</p>

          '
        raw: Awesome, it seems to work perfectly! however I just did llm = Llama(model_path="/content/Wizard-Vicuna-13B-Uncensored.ggmlv3.q2_K.bin")
          but  what does the n_gpu_layers do? and seed=random.randint(1, 2**31)? I
          am also doing cpu right now so should i add something?
        updatedAt: '2023-06-20T11:48:10.027Z'
      numEdits: 0
      reactions: []
    id: 649191fa5f1f2e2c3b462c8c
    type: comment
  author: YaTharThShaRma999
  content: Awesome, it seems to work perfectly! however I just did llm = Llama(model_path="/content/Wizard-Vicuna-13B-Uncensored.ggmlv3.q2_K.bin")
    but  what does the n_gpu_layers do? and seed=random.randint(1, 2**31)? I am also
    doing cpu right now so should i add something?
  created_at: 2023-06-20 10:48:10+00:00
  edited: false
  hidden: false
  id: 649191fa5f1f2e2c3b462c8c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/Wizard-Vicuna-13B-Uncensored-GGML
repo_type: model
status: open
target_branch: null
title: How to generate token by token?
