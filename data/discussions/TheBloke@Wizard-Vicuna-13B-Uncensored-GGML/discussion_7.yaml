!!python/object:huggingface_hub.community.DiscussionWithDetails
author: florestankorp
conflicting_files: null
created_at: 2023-06-10 06:31:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a2e261407e4f40b9d76ce8a3c0d2d171.svg
      fullname: Florestan Korp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: florestankorp
      type: user
    createdAt: '2023-06-10T07:31:19.000Z'
    data:
      edited: false
      editors:
      - florestankorp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8125283122062683
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a2e261407e4f40b9d76ce8a3c0d2d171.svg
          fullname: Florestan Korp
          isHf: false
          isPro: false
          name: florestankorp
          type: user
        html: "<p>Hi all! </p>\n<p>After a few weeks of leaving my system dormant\
          \ I decided to jump back into the local LLM frenzy and am pleased to say\
          \ that after some tinkering with Conda environments and Python packages\
          \ yesterday I managed to run <code>Wizard-Vicuna-13B-Uncensored</code> on\
          \ my <strong>Apple M2 Pro - 16 GB RAM</strong>.</p>\n<p>Now, when I first\
          \ tried inspecting the files on Hugging Face I saw that there were a bunch\
          \ of new files that I didn't encounter the last time I played with LLMs.\
          \ As I have a machine that is not ideal for running LLMs locally, my understanding\
          \ is that I need 4-bit quantized GGML models, so here we are.</p>\n<p>Because\
          \ I didn't have the time to download all the files I started with these\
          \ two:</p>\n<ul>\n<li>Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_0.bin</li>\n\
          <li>Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_1.bin</li>\n</ul>\n<p>By the\
          \ way, I assumed that both files are needed to complete the model, similar\
          \ to a <code>.zip</code> file. Is that accurate or can I get away with only\
          \ using one? How will that affect output quality and speed?</p>\n<p>Leaving\
          \ only the <code>q4_0.bin</code> and <code>q4_1.bin</code> files in my <code>/models</code>\
          \ folder and then startimg the chat gave me a decent 5 tokens/s and I've\
          \ been chatting away with this wonderful model. But now my question is what\
          \ are all the other files for then and could they help me improve the quality\
          \ and/or speed of my chat experience?</p>\n<p>For example when I have only\
          \ <code>*_K_M.bin</code> files, then I can't even get the model to load.\
          \ So what are these files even for? As I've gathered from the commit messages\
          \ from TheBloke's Hugging Face page it's something pertaining to k-quant,\
          \ but I can't find any information as to what they're used for...</p>\n\
          <p>Then, would I benefit from ONLY running the <code>q8_0.bin</code> for\
          \ example or is my hardware not equipped to handle that load or am I better\
          \ off running <code>q4_0.bin</code> or maybe I should go for the \"newer\"\
          \ <code>q5_0.bin</code>?</p>\n<p>Here are all files available for download\
          \ in the \"Files and versions\" tab:</p>\n<pre><code>    Wizard-Vicuna-13B-Uncensored.ggmlv3.q2_K.bin\n\
          \    Wizard-Vicuna-13B-Uncensored.ggmlv3.q3_K_L.bin\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q3_K_M.bin\n\
          \    Wizard-Vicuna-13B-Uncensored.ggmlv3.q3_K_S.bin\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_0.bin\n\
          \    Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_1.bin\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_K_M.bin\n\
          \    Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_K_S.bin\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_0.bin\n\
          \    Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_1.bin\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_K_M.bin\n\
          \    Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_K_S.bin\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q6_K.bin\n\
          \    Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin\n</code></pre>\n<p>PS:\
          \ I run the model through <code>oobabooga/text-generation-webui</code> with\
          \ the following command:</p>\n<pre><code class=\"language-bash\">python\
          \ server.py --threads=8 --gpu-memory=10 --mlock --chat --model=TheBloke_Wizard-Vicuna-13B-Uncensored-GGML\n\
          </code></pre>\n"
        raw: "Hi all! \r\n\r\nAfter a few weeks of leaving my system dormant I decided\
          \ to jump back into the local LLM frenzy and am pleased to say that after\
          \ some tinkering with Conda environments and Python packages yesterday I\
          \ managed to run `Wizard-Vicuna-13B-Uncensored` on my **Apple M2 Pro - 16\
          \ GB RAM**.\r\n\r\nNow, when I first tried inspecting the files on Hugging\
          \ Face I saw that there were a bunch of new files that I didn't encounter\
          \ the last time I played with LLMs. As I have a machine that is not ideal\
          \ for running LLMs locally, my understanding is that I need 4-bit quantized\
          \ GGML models, so here we are.\r\n\r\nBecause I didn't have the time to\
          \ download all the files I started with these two:\r\n* Wizard-Vicuna-13B-Uncensored.ggmlv3.q4\\\
          _0.bin\r\n* Wizard-Vicuna-13B-Uncensored.ggmlv3.q4\\_1.bin\r\n\r\nBy the\
          \ way, I assumed that both files are needed to complete the model, similar\
          \ to a `.zip` file. Is that accurate or can I get away with only using one?\
          \ How will that affect output quality and speed?\r\n\r\nLeaving only the\
          \ `q4_0.bin` and `q4_1.bin` files in my `/models` folder and then startimg\
          \ the chat gave me a decent 5 tokens/s and I've been chatting away with\
          \ this wonderful model. But now my question is what are all the other files\
          \ for then and could they help me improve the quality and/or speed of my\
          \ chat experience?\r\n\r\nFor example when I have only `*_K_M.bin` files,\
          \ then I can't even get the model to load. So what are these files even\
          \ for? As I've gathered from the commit messages from TheBloke's Hugging\
          \ Face page it's something pertaining to k-quant, but I can't find any information\
          \ as to what they're used for...\r\n\r\nThen, would I benefit from ONLY\
          \ running the `q8_0.bin` for example or is my hardware not equipped to handle\
          \ that load or am I better off running `q4_0.bin` or maybe I should go for\
          \ the \"newer\" `q5_0.bin`?\r\n\r\nHere are all files available for download\
          \ in the \"Files and versions\" tab:\r\n```\r\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q2_K.bin\r\
          \n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q3_K_L.bin\r\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q3_K_M.bin\r\
          \n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q3_K_S.bin\r\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_0.bin\r\
          \n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_1.bin\r\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_K_M.bin\r\
          \n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_K_S.bin\r\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_0.bin\r\
          \n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_1.bin\r\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_K_M.bin\r\
          \n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_K_S.bin\r\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q6_K.bin\r\
          \n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin\r\n```\r\n\r\nPS: I run\
          \ the model through `oobabooga/text-generation-webui` with the following\
          \ command:\r\n```bash\r\npython server.py --threads=8 --gpu-memory=10 --mlock\
          \ --chat --model=TheBloke_Wizard-Vicuna-13B-Uncensored-GGML\r\n```"
        updatedAt: '2023-06-10T07:31:19.741Z'
      numEdits: 0
      reactions: []
    id: 648426c73594eb92885eed8b
    type: comment
  author: florestankorp
  content: "Hi all! \r\n\r\nAfter a few weeks of leaving my system dormant I decided\
    \ to jump back into the local LLM frenzy and am pleased to say that after some\
    \ tinkering with Conda environments and Python packages yesterday I managed to\
    \ run `Wizard-Vicuna-13B-Uncensored` on my **Apple M2 Pro - 16 GB RAM**.\r\n\r\
    \nNow, when I first tried inspecting the files on Hugging Face I saw that there\
    \ were a bunch of new files that I didn't encounter the last time I played with\
    \ LLMs. As I have a machine that is not ideal for running LLMs locally, my understanding\
    \ is that I need 4-bit quantized GGML models, so here we are.\r\n\r\nBecause I\
    \ didn't have the time to download all the files I started with these two:\r\n\
    * Wizard-Vicuna-13B-Uncensored.ggmlv3.q4\\_0.bin\r\n* Wizard-Vicuna-13B-Uncensored.ggmlv3.q4\\\
    _1.bin\r\n\r\nBy the way, I assumed that both files are needed to complete the\
    \ model, similar to a `.zip` file. Is that accurate or can I get away with only\
    \ using one? How will that affect output quality and speed?\r\n\r\nLeaving only\
    \ the `q4_0.bin` and `q4_1.bin` files in my `/models` folder and then startimg\
    \ the chat gave me a decent 5 tokens/s and I've been chatting away with this wonderful\
    \ model. But now my question is what are all the other files for then and could\
    \ they help me improve the quality and/or speed of my chat experience?\r\n\r\n\
    For example when I have only `*_K_M.bin` files, then I can't even get the model\
    \ to load. So what are these files even for? As I've gathered from the commit\
    \ messages from TheBloke's Hugging Face page it's something pertaining to k-quant,\
    \ but I can't find any information as to what they're used for...\r\n\r\nThen,\
    \ would I benefit from ONLY running the `q8_0.bin` for example or is my hardware\
    \ not equipped to handle that load or am I better off running `q4_0.bin` or maybe\
    \ I should go for the \"newer\" `q5_0.bin`?\r\n\r\nHere are all files available\
    \ for download in the \"Files and versions\" tab:\r\n```\r\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q2_K.bin\r\
    \n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q3_K_L.bin\r\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q3_K_M.bin\r\
    \n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q3_K_S.bin\r\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_0.bin\r\
    \n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_1.bin\r\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_K_M.bin\r\
    \n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_K_S.bin\r\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_0.bin\r\
    \n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_1.bin\r\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_K_M.bin\r\
    \n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_K_S.bin\r\n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q6_K.bin\r\
    \n    Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin\r\n```\r\n\r\nPS: I run the\
    \ model through `oobabooga/text-generation-webui` with the following command:\r\
    \n```bash\r\npython server.py --threads=8 --gpu-memory=10 --mlock --chat --model=TheBloke_Wizard-Vicuna-13B-Uncensored-GGML\r\
    \n```"
  created_at: 2023-06-10 06:31:19+00:00
  edited: false
  hidden: false
  id: 648426c73594eb92885eed8b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-10T10:33:25.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9758490324020386
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You only need one file.  q4_0 and q4_1 are different models, and
          you can use either but don''t need both.</p>

          <p>As for the different sizes: the larger the file the better the accuracy,
          but the more resources required and the slower the speed.  q4_0 and q5_0
          are good compromises.</p>

          <p>The formats with the letter K in their name are a new type of quantisation.
          They''re generally better than the old types, but don''t yet have as wide
          support.  They don''t yet work in text-generation-webui for example, I think.</p>

          <p>As you only have 16GB RAM, I would limit yourself to 13B models in q4_0
          for now.</p>

          <p>FYI, llama.cpp recently added Metal acceleration which should give you
          much better performance.  Again I don''t think that''s yet supported in
          text-generation-webui, but I would expect it to come in the next week or
          so.  It won''t enable you to run larger models (you''re still limited by
          that 16GB), but it will give you much faster performance.</p>

          '
        raw: 'You only need one file.  q4_0 and q4_1 are different models, and you
          can use either but don''t need both.


          As for the different sizes: the larger the file the better the accuracy,
          but the more resources required and the slower the speed.  q4_0 and q5_0
          are good compromises.


          The formats with the letter K in their name are a new type of quantisation.
          They''re generally better than the old types, but don''t yet have as wide
          support.  They don''t yet work in text-generation-webui for example, I think.


          As you only have 16GB RAM, I would limit yourself to 13B models in q4_0
          for now.


          FYI, llama.cpp recently added Metal acceleration which should give you much
          better performance.  Again I don''t think that''s yet supported in text-generation-webui,
          but I would expect it to come in the next week or so.  It won''t enable
          you to run larger models (you''re still limited by that 16GB), but it will
          give you much faster performance.'
        updatedAt: '2023-06-10T10:33:25.984Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - florestankorp
        - edertef
        - Mykee
        - mikeee
    id: 6484517550e57f3530ed2b34
    type: comment
  author: TheBloke
  content: 'You only need one file.  q4_0 and q4_1 are different models, and you can
    use either but don''t need both.


    As for the different sizes: the larger the file the better the accuracy, but the
    more resources required and the slower the speed.  q4_0 and q5_0 are good compromises.


    The formats with the letter K in their name are a new type of quantisation. They''re
    generally better than the old types, but don''t yet have as wide support.  They
    don''t yet work in text-generation-webui for example, I think.


    As you only have 16GB RAM, I would limit yourself to 13B models in q4_0 for now.


    FYI, llama.cpp recently added Metal acceleration which should give you much better
    performance.  Again I don''t think that''s yet supported in text-generation-webui,
    but I would expect it to come in the next week or so.  It won''t enable you to
    run larger models (you''re still limited by that 16GB), but it will give you much
    faster performance.'
  created_at: 2023-06-10 09:33:25+00:00
  edited: false
  hidden: false
  id: 6484517550e57f3530ed2b34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/975f3c34076f0f075d1867849c0a6389.svg
      fullname: BERKUT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: berkut1
      type: user
    createdAt: '2023-06-13T21:41:14.000Z'
    data:
      edited: true
      editors:
      - berkut1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9587783217430115
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/975f3c34076f0f075d1867849c0a6389.svg
          fullname: BERKUT
          isHf: false
          isPro: false
          name: berkut1
          type: user
        html: '<p>I have tested some new quantization..<br>It looks like models with
          4bit, especially q4_K_S better work with Nvidia 2000 (I have 2080). If other
          bit models (i tried everything, except 2bit), finished with not completed
          sentences, when q4_K_S finished all of them in right context. It''s weird
          I think because 4 bit is half of 8 bit and it helps to finish the generation
          without cutting them off.<br>P.S I mean when I try to generate 512 tokens.
          If about performance, they all work similar without noticeable boost.</p>

          '
        raw: 'I have tested some new quantization..

          It looks like models with 4bit, especially q4_K_S better work with Nvidia
          2000 (I have 2080). If other bit models (i tried everything, except 2bit),
          finished with not completed sentences, when q4_K_S finished all of them
          in right context. It''s weird I think because 4 bit is half of 8 bit and
          it helps to finish the generation without cutting them off.

          P.S I mean when I try to generate 512 tokens. If about performance, they
          all work similar without noticeable boost.


          '
        updatedAt: '2023-06-13T22:32:06.953Z'
      numEdits: 4
      reactions: []
    id: 6488e27adf7ff9ce07aad771
    type: comment
  author: berkut1
  content: 'I have tested some new quantization..

    It looks like models with 4bit, especially q4_K_S better work with Nvidia 2000
    (I have 2080). If other bit models (i tried everything, except 2bit), finished
    with not completed sentences, when q4_K_S finished all of them in right context.
    It''s weird I think because 4 bit is half of 8 bit and it helps to finish the
    generation without cutting them off.

    P.S I mean when I try to generate 512 tokens. If about performance, they all work
    similar without noticeable boost.


    '
  created_at: 2023-06-13 20:41:14+00:00
  edited: true
  hidden: false
  id: 6488e27adf7ff9ce07aad771
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/68AChNdt_w63AUqBYELrO.jpeg?w=200&h=200&f=face
      fullname: Andre Romanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sytross
      type: user
    createdAt: '2023-07-25T15:35:58.000Z'
    data:
      edited: false
      editors:
      - sytross
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9088386297225952
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/68AChNdt_w63AUqBYELrO.jpeg?w=200&h=200&f=face
          fullname: Andre Romanov
          isHf: false
          isPro: false
          name: sytross
          type: user
        html: '<p>A question to TheBlock: which is better in performance: ggmlv3.q5_K_S.bin  OR  ggmlv3.q5_K_M.bin
          ?? Any difference in accuracy?</p>

          '
        raw: 'A question to TheBlock: which is better in performance: ggmlv3.q5_K_S.bin  OR  ggmlv3.q5_K_M.bin
          ?? Any difference in accuracy?

          '
        updatedAt: '2023-07-25T15:35:58.855Z'
      numEdits: 0
      reactions: []
    id: 64bfebde89592d4f21c1662a
    type: comment
  author: sytross
  content: 'A question to TheBlock: which is better in performance: ggmlv3.q5_K_S.bin  OR  ggmlv3.q5_K_M.bin
    ?? Any difference in accuracy?

    '
  created_at: 2023-07-25 14:35:58+00:00
  edited: false
  hidden: false
  id: 64bfebde89592d4f21c1662a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T15:37:13.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9015849232673645
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>q5_K_M will be slightly better in accuracy. The bigger the file,
          the better the accuracy, but also the more RAM/VRAM needed and the slower
          inference will be.  So you pick the right compromise for your needs/hardware.</p>

          '
        raw: q5_K_M will be slightly better in accuracy. The bigger the file, the
          better the accuracy, but also the more RAM/VRAM needed and the slower inference
          will be.  So you pick the right compromise for your needs/hardware.
        updatedAt: '2023-07-25T15:37:13.566Z'
      numEdits: 0
      reactions: []
    id: 64bfec29505a40085bcbc94c
    type: comment
  author: TheBloke
  content: q5_K_M will be slightly better in accuracy. The bigger the file, the better
    the accuracy, but also the more RAM/VRAM needed and the slower inference will
    be.  So you pick the right compromise for your needs/hardware.
  created_at: 2023-07-25 14:37:13+00:00
  edited: false
  hidden: false
  id: 64bfec29505a40085bcbc94c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/Wizard-Vicuna-13B-Uncensored-GGML
repo_type: model
status: open
target_branch: null
title: Question about which .bin file to use and quantization
