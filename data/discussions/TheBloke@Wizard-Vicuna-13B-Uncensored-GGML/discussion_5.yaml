!!python/object:huggingface_hub.community.DiscussionWithDetails
author: supercharge19
conflicting_files: null
created_at: 2023-06-06 08:37:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bbfe7f8c8e7ce8b50e48a4a2164a3c2e.svg
      fullname: Jawad Mansoor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: supercharge19
      type: user
    createdAt: '2023-06-06T09:37:09.000Z'
    data:
      edited: false
      editors:
      - supercharge19
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8528716564178467
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bbfe7f8c8e7ce8b50e48a4a2164a3c2e.svg
          fullname: Jawad Mansoor
          isHf: false
          isPro: false
          name: supercharge19
          type: user
        html: "<p>I am struggling with the issue of models not following instructions\
          \ at all when they are used in python, however, they work much better when\
          \ they are used in a shell (like cmd, or powershell).</p>\n<p>python examples:<br>Q:\
          \ llm(\"Can you solve math questions?\")<br>R: '\\nCan you solve these math\
          \ questions?'</p>\n<p>Q: llm(\"what is (4.5*2.1)^2.2?\")<br>A: Long text\
          \ output omitted. It was just not related to question. It just asked more\
          \ questions instead of answering the question.</p>\n<p>I am trying to use\
          \ it with langchain as llm for agent, however, models are acting too dumb.\
          \ I should be able to get a correct answer of the following:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">from</span> langchain.agents\
          \ <span class=\"hljs-keyword\">import</span> load_tools\n\ntools = load_tools(\n\
          \    [<span class=\"hljs-string\">'llm-math'</span>],\n    llm=llm\n)\n\n\
          <span class=\"hljs-keyword\">from</span> langchain.agents <span class=\"\
          hljs-keyword\">import</span> initialize_agent\n\nzero_shot_agent = initialize_agent(\n\
          \    agent=<span class=\"hljs-string\">\"zero-shot-react-description\"</span>,\n\
          \    tools=tools,\n    llm=llm,\n    verbose=<span class=\"hljs-literal\"\
          >True</span>,\n    max_iterations=<span class=\"hljs-number\">3</span>\n\
          )\nzero_shot_agent(<span class=\"hljs-string\">\"what is (4.5*2.1)^2.2?\"\
          </span>)\n</code></pre>\n<p>The response I get:</p>\n<pre><code class=\"\
          language-python\">Entering new AgentExecutor chain...\nLlama.generate: prefix-<span\
          \ class=\"hljs-keyword\">match</span> hit\n let<span class=\"hljs-string\"\
          >'s get the calculator out!</span>\n<span class=\"hljs-string\">Action:\
          \ [Calculator]</span>\n<span class=\"hljs-string\">Action Input: 4.5 and\
          \ 2.1 as a ratio</span>\n<span class=\"hljs-string\">Observation: [Calculator]\
          \ is not a valid tool, try another one.</span>\n<span class=\"hljs-string\"\
          >Thought:Llama.generate: prefix-match hit</span>\n</code></pre>\n<blockquote>\n\
          <p>omitting large output</p>\n</blockquote>\n<pre><code>OutputParserException:\
          \ Could not parse LLM output: ` I will use the power rule for exponents\
          \ to do this by hand.\nAction: (4.5*2.1)^2.2 = 4.5*2.1^2.2`\n</code></pre>\n\
          <p>Is there a way to overcome this problem, but I want to use GGML model\
          \ (or any model that can be run on cpu locally). Model that I got this outputs\
          \ as above is manticore 13b q4_0. (though I am sure that larger models i.e.\
          \ more bits eg 5 or 8 would not be any better). Also, this kind of error\
          \ (OutputParserException only occours when I use a notebook (ipynb or google\
          \ colab) I usually encounter a different problem when the code is run in\
          \ python REPL (through cmd or powershell). The problem I encounter when\
          \ running code in REPL is that langchain just can't use my tools. For example\
          \ for my quetion <code>zero_shot_agent(\"what is (4.5*2.1)^2.2?\")</code>\
          \ I get outputs like </p>\n<pre><code> I should use the calculator for this\
          \ math problem.\nAction: [Calculator]\nAction Input: press the equals button\
          \ and type in 4.5 and 2.1, then press the square root button twice\nObservation:\
          \ [Calculator] is not a valid tool, try another one.\n\n I will use a regular\
          \ calculator.\nAction: [Regular Calculator]\nAction Input: turn on the calculator\
          \ and input the problem: (4.5*2.1)^2.2\nObservation: [Regular Calculator]\
          \ is not a valid tool, try another one.\n\n I will use my phone's calculator\
          \ app.\nAction: [Phone Calculator]\nAction Input: open the app and input\
          \ the problem: (4.5*2.1)^2.2\nObservation: [Phone Calculator] is not a valid\
          \ tool, try another one.\nThought:\n\n&gt; Finished chain.\n{'input': 'what\
          \ is (4.5*2.1)^2.2?', 'output': 'Agent stopped due to iteration limit or\
          \ time limit.'}\n</code></pre>\n<p>Though it stopped at third iteration\
          \ (try) to solve the problem, however, I don't see any value for letting\
          \ it run longer.</p>\n"
        raw: "I am struggling with the issue of models not following instructions\
          \ at all when they are used in python, however, they work much better when\
          \ they are used in a shell (like cmd, or powershell).\r\n\r\npython examples:\r\
          \nQ: llm(\"Can you solve math questions?\")\r\nR: '\\nCan you solve these\
          \ math questions?'\r\n\r\nQ: llm(\"what is (4.5*2.1)^2.2?\")\r\nA: Long\
          \ text output omitted. It was just not related to question. It just asked\
          \ more questions instead of answering the question.\r\n\r\nI am trying to\
          \ use it with langchain as llm for agent, however, models are acting too\
          \ dumb. I should be able to get a correct answer of the following:\r\n```python\r\
          \nfrom langchain.agents import load_tools\r\n\r\ntools = load_tools(\r\n\
          \    ['llm-math'],\r\n    llm=llm\r\n)\r\n\r\nfrom langchain.agents import\
          \ initialize_agent\r\n\r\nzero_shot_agent = initialize_agent(\r\n    agent=\"\
          zero-shot-react-description\",\r\n    tools=tools,\r\n    llm=llm,\r\n \
          \   verbose=True,\r\n    max_iterations=3\r\n)\r\nzero_shot_agent(\"what\
          \ is (4.5*2.1)^2.2?\")\r\n```\r\nThe response I get:\r\n```python\r\nEntering\
          \ new AgentExecutor chain...\r\nLlama.generate: prefix-match hit\r\n let's\
          \ get the calculator out!\r\nAction: [Calculator]\r\nAction Input: 4.5 and\
          \ 2.1 as a ratio\r\nObservation: [Calculator] is not a valid tool, try another\
          \ one.\r\nThought:Llama.generate: prefix-match hit\r\n```\r\n> omitting\
          \ large output\r\n```\r\nOutputParserException: Could not parse LLM output:\
          \ ` I will use the power rule for exponents to do this by hand.\r\nAction:\
          \ (4.5*2.1)^2.2 = 4.5*2.1^2.2`\r\n```\r\n\r\nIs there a way to overcome\
          \ this problem, but I want to use GGML model (or any model that can be run\
          \ on cpu locally). Model that I got this outputs as above is manticore 13b\
          \ q4_0. (though I am sure that larger models i.e. more bits eg 5 or 8 would\
          \ not be any better). Also, this kind of error (OutputParserException only\
          \ occours when I use a notebook (ipynb or google colab) I usually encounter\
          \ a different problem when the code is run in python REPL (through cmd or\
          \ powershell). The problem I encounter when running code in REPL is that\
          \ langchain just can't use my tools. For example for my quetion `zero_shot_agent(\"\
          what is (4.5*2.1)^2.2?\")` I get outputs like \r\n```\r\n I should use the\
          \ calculator for this math problem.\r\nAction: [Calculator]\r\nAction Input:\
          \ press the equals button and type in 4.5 and 2.1, then press the square\
          \ root button twice\r\nObservation: [Calculator] is not a valid tool, try\
          \ another one.\r\n\r\n I will use a regular calculator.\r\nAction: [Regular\
          \ Calculator]\r\nAction Input: turn on the calculator and input the problem:\
          \ (4.5*2.1)^2.2\r\nObservation: [Regular Calculator] is not a valid tool,\
          \ try another one.\r\n\r\n I will use my phone's calculator app.\r\nAction:\
          \ [Phone Calculator]\r\nAction Input: open the app and input the problem:\
          \ (4.5*2.1)^2.2\r\nObservation: [Phone Calculator] is not a valid tool,\
          \ try another one.\r\nThought:\r\n\r\n> Finished chain.\r\n{'input': 'what\
          \ is (4.5*2.1)^2.2?', 'output': 'Agent stopped due to iteration limit or\
          \ time limit.'}\r\n```\r\nThough it stopped at third iteration (try) to\
          \ solve the problem, however, I don't see any value for letting it run longer."
        updatedAt: '2023-06-06T09:37:09.263Z'
      numEdits: 0
      reactions: []
    id: 647efe4545baf21ad707ece9
    type: comment
  author: supercharge19
  content: "I am struggling with the issue of models not following instructions at\
    \ all when they are used in python, however, they work much better when they are\
    \ used in a shell (like cmd, or powershell).\r\n\r\npython examples:\r\nQ: llm(\"\
    Can you solve math questions?\")\r\nR: '\\nCan you solve these math questions?'\r\
    \n\r\nQ: llm(\"what is (4.5*2.1)^2.2?\")\r\nA: Long text output omitted. It was\
    \ just not related to question. It just asked more questions instead of answering\
    \ the question.\r\n\r\nI am trying to use it with langchain as llm for agent,\
    \ however, models are acting too dumb. I should be able to get a correct answer\
    \ of the following:\r\n```python\r\nfrom langchain.agents import load_tools\r\n\
    \r\ntools = load_tools(\r\n    ['llm-math'],\r\n    llm=llm\r\n)\r\n\r\nfrom langchain.agents\
    \ import initialize_agent\r\n\r\nzero_shot_agent = initialize_agent(\r\n    agent=\"\
    zero-shot-react-description\",\r\n    tools=tools,\r\n    llm=llm,\r\n    verbose=True,\r\
    \n    max_iterations=3\r\n)\r\nzero_shot_agent(\"what is (4.5*2.1)^2.2?\")\r\n\
    ```\r\nThe response I get:\r\n```python\r\nEntering new AgentExecutor chain...\r\
    \nLlama.generate: prefix-match hit\r\n let's get the calculator out!\r\nAction:\
    \ [Calculator]\r\nAction Input: 4.5 and 2.1 as a ratio\r\nObservation: [Calculator]\
    \ is not a valid tool, try another one.\r\nThought:Llama.generate: prefix-match\
    \ hit\r\n```\r\n> omitting large output\r\n```\r\nOutputParserException: Could\
    \ not parse LLM output: ` I will use the power rule for exponents to do this by\
    \ hand.\r\nAction: (4.5*2.1)^2.2 = 4.5*2.1^2.2`\r\n```\r\n\r\nIs there a way to\
    \ overcome this problem, but I want to use GGML model (or any model that can be\
    \ run on cpu locally). Model that I got this outputs as above is manticore 13b\
    \ q4_0. (though I am sure that larger models i.e. more bits eg 5 or 8 would not\
    \ be any better). Also, this kind of error (OutputParserException only occours\
    \ when I use a notebook (ipynb or google colab) I usually encounter a different\
    \ problem when the code is run in python REPL (through cmd or powershell). The\
    \ problem I encounter when running code in REPL is that langchain just can't use\
    \ my tools. For example for my quetion `zero_shot_agent(\"what is (4.5*2.1)^2.2?\"\
    )` I get outputs like \r\n```\r\n I should use the calculator for this math problem.\r\
    \nAction: [Calculator]\r\nAction Input: press the equals button and type in 4.5\
    \ and 2.1, then press the square root button twice\r\nObservation: [Calculator]\
    \ is not a valid tool, try another one.\r\n\r\n I will use a regular calculator.\r\
    \nAction: [Regular Calculator]\r\nAction Input: turn on the calculator and input\
    \ the problem: (4.5*2.1)^2.2\r\nObservation: [Regular Calculator] is not a valid\
    \ tool, try another one.\r\n\r\n I will use my phone's calculator app.\r\nAction:\
    \ [Phone Calculator]\r\nAction Input: open the app and input the problem: (4.5*2.1)^2.2\r\
    \nObservation: [Phone Calculator] is not a valid tool, try another one.\r\nThought:\r\
    \n\r\n> Finished chain.\r\n{'input': 'what is (4.5*2.1)^2.2?', 'output': 'Agent\
    \ stopped due to iteration limit or time limit.'}\r\n```\r\nThough it stopped\
    \ at third iteration (try) to solve the problem, however, I don't see any value\
    \ for letting it run longer."
  created_at: 2023-06-06 08:37:09+00:00
  edited: false
  hidden: false
  id: 647efe4545baf21ad707ece9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/281d5c575f426ef23fd207221f38d263.svg
      fullname: Brian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brian-exe
      type: user
    createdAt: '2023-09-09T23:37:19.000Z'
    data:
      edited: true
      editors:
      - brian-exe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8845036625862122
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/281d5c575f426ef23fd207221f38d263.svg
          fullname: Brian
          isHf: false
          isPro: false
          name: brian-exe
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;supercharge19&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/supercharge19\"\
          >@<span class=\"underline\">supercharge19</span></a></span>\n\n\t</span></span>\
          \ I have nothing to help you with this issue  but I am kind of starting\
          \ in this worlds an I would like to try this models local.<br>I have a code\
          \ snippet that stole from other tutorial, I managed to make an agent respond\
          \ using OpenAI model and also others but I was trying to use now Vicuna\
          \ and I get an error while trying to load the model.</p>\n<p>I have the\
          \ following code:</p>\n<p>checkpoint = \"./wizard-vicuna-13B-uncensored/\"\
          <br>tokenizer = AutoTokenizer.from_pretrained(checkpoint)<br>base_model\
          \ = AutoModelForSeq2SeqLM.from_pretrained(checkpoint,device_map='auto',torch_dtype=torch.float32)<br>llm\
          \ = HuggingFacePipeline.from_model_id(model_id=checkpoint,task = 'text2text-generation',model_kwargs={\"\
          temperature\":0.60,\"min_length\":30, \"max_length\":600, \"repetition_penalty\"\
          : 5.0})</p>\n<p>Could you point out what is wrong here? It is trying to\
          \ find a \"config.json\" file which is not there but not sure what to change\
          \ to make it work.</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64d7e65fa3c9b92761aabcc0/5V8jDmWDVu7fvmdf66-hK.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64d7e65fa3c9b92761aabcc0/5V8jDmWDVu7fvmdf66-hK.png\"\
          ></a></p>\n<p>inside folder \"wizard-vicuna-13B-uncensored\" is the model\
          \ file named as \"pytorch_model\"</p>\n<p>if you can share me how you are\
          \ using it would be enough also</p>\n"
        raw: 'Hey @supercharge19 I have nothing to help you with this issue  but I
          am kind of starting in this worlds an I would like to try this models local.

          I have a code snippet that stole from other tutorial, I managed to make
          an agent respond using OpenAI model and also others but I was trying to
          use now Vicuna and I get an error while trying to load the model.


          I have the following code:



          checkpoint = "./wizard-vicuna-13B-uncensored/"

          tokenizer = AutoTokenizer.from_pretrained(checkpoint)

          base_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint,device_map=''auto'',torch_dtype=torch.float32)

          llm = HuggingFacePipeline.from_model_id(model_id=checkpoint,task = ''text2text-generation'',model_kwargs={"temperature":0.60,"min_length":30,
          "max_length":600, "repetition_penalty": 5.0})



          Could you point out what is wrong here? It is trying to find a "config.json"
          file which is not there but not sure what to change to make it work.


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64d7e65fa3c9b92761aabcc0/5V8jDmWDVu7fvmdf66-hK.png)


          inside folder "wizard-vicuna-13B-uncensored" is the model file named as
          "pytorch_model"


          if you can share me how you are using it would be enough also'
        updatedAt: '2023-09-09T23:38:07.391Z'
      numEdits: 1
      reactions: []
    id: 64fd01af9a62bb2791d85f65
    type: comment
  author: brian-exe
  content: 'Hey @supercharge19 I have nothing to help you with this issue  but I am
    kind of starting in this worlds an I would like to try this models local.

    I have a code snippet that stole from other tutorial, I managed to make an agent
    respond using OpenAI model and also others but I was trying to use now Vicuna
    and I get an error while trying to load the model.


    I have the following code:



    checkpoint = "./wizard-vicuna-13B-uncensored/"

    tokenizer = AutoTokenizer.from_pretrained(checkpoint)

    base_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint,device_map=''auto'',torch_dtype=torch.float32)

    llm = HuggingFacePipeline.from_model_id(model_id=checkpoint,task = ''text2text-generation'',model_kwargs={"temperature":0.60,"min_length":30,
    "max_length":600, "repetition_penalty": 5.0})



    Could you point out what is wrong here? It is trying to find a "config.json" file
    which is not there but not sure what to change to make it work.


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64d7e65fa3c9b92761aabcc0/5V8jDmWDVu7fvmdf66-hK.png)


    inside folder "wizard-vicuna-13B-uncensored" is the model file named as "pytorch_model"


    if you can share me how you are using it would be enough also'
  created_at: 2023-09-09 22:37:19+00:00
  edited: true
  hidden: false
  id: 64fd01af9a62bb2791d85f65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bbfe7f8c8e7ce8b50e48a4a2164a3c2e.svg
      fullname: Jawad Mansoor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: supercharge19
      type: user
    createdAt: '2023-09-12T06:33:01.000Z'
    data:
      edited: false
      editors:
      - supercharge19
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8865400552749634
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bbfe7f8c8e7ce8b50e48a4a2164a3c2e.svg
          fullname: Jawad Mansoor
          isHf: false
          isPro: false
          name: supercharge19
          type: user
        html: '<p>Where did you get the model from? If you obtained from ehartford
          (ehartford/Wizard-Vicuna-13B-Uncensored) then this contains required json
          file. Also, the code you are using works only with hugging face models i.e.
          uploaded to huggingface directly after training these models are not gguf
          or ggml files, first ensure that you are downloading correct format for
          your code. Otherwise, you can try using ctranformers or llama_cpp_python
          code (look them up on github).</p>

          '
        raw: Where did you get the model from? If you obtained from ehartford (ehartford/Wizard-Vicuna-13B-Uncensored)
          then this contains required json file. Also, the code you are using works
          only with hugging face models i.e. uploaded to huggingface directly after
          training these models are not gguf or ggml files, first ensure that you
          are downloading correct format for your code. Otherwise, you can try using
          ctranformers or llama_cpp_python code (look them up on github).
        updatedAt: '2023-09-12T06:33:01.309Z'
      numEdits: 0
      reactions: []
    id: 6500061ddadb8c628f244a79
    type: comment
  author: supercharge19
  content: Where did you get the model from? If you obtained from ehartford (ehartford/Wizard-Vicuna-13B-Uncensored)
    then this contains required json file. Also, the code you are using works only
    with hugging face models i.e. uploaded to huggingface directly after training
    these models are not gguf or ggml files, first ensure that you are downloading
    correct format for your code. Otherwise, you can try using ctranformers or llama_cpp_python
    code (look them up on github).
  created_at: 2023-09-12 05:33:01+00:00
  edited: false
  hidden: false
  id: 6500061ddadb8c628f244a79
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/Wizard-Vicuna-13B-Uncensored-GGML
repo_type: model
status: open
target_branch: null
title: GGML models become dumb when used in python.
