!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Phil337
conflicting_files: null
created_at: 2023-09-12 15:32:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-09-12T16:32:35.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.953109085559845
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: '<p>I''ve created and used a set of 50 somewhat tricky questions to
          test LLMs, and your previous WizardLM 13B v1.1 scored better than any other
          13B Q4 or lower LLM. However, every LLM based on Llama 2 went completely
          off the rails on several of my questions.</p>

          <p>For example, when I asked Meta''s full 70B "aligned" version of Llama
          2 about the Meghan Trainor song "Dear Future Husband" using the lyric "And
          don''t forget the flowers every anniversary" it said the song didn''t exist,
          then lectured me about spreading misinformation online. Then I referred
          to the song by name in my follow up question "Dear Future Husband". Llama
          2 refused to answer and lectured me about using potentially hurtful gender
          stereotypes. I then tricked it into answering my question. Proving it was
          aware of the lyric and the song.</p>

          <p>When I went through the same series of questions on uncensored 13b Llama
          2 LLMs, not just yours, the lecturing about spreading misinformation and
          gender stereotypes are gone, but weird hallucinations start occurring. For
          example, the song''s name is "All About That Bass". Yet I can get the information
          by rephrasing the questions so they can''t be interpreted as contentious
          (e.g. a potentially hurtful gender stereotype).</p>

          <p>This is a pattern I found in my testing, and further confirmed by making
          up about a dozen more tricky questions that can be misinterpreted as contentious.
          On questions were Llama 2 censures and lectures by misidentifying contentious
          questions, the uncensored models, while they don''t lecture, consistently
          hallucinate, yet theyt perform very well on similar non-contentious questions.
          Proving that the "unaligned" LLM are still censored via the suppression
          of potentially contentious, but completely harmless, information.</p>

          <p>This is so bad it even happened with some of my non-contentious questions.
          For example, when I asked about the phenomenon that occurs when particles
          travel faster than light in a medium like water (Cherenkov radiation), the
          censored version got to "air bubble" and immediately switched to warning
          about scuba diving and paying close attention to your instructor. And your
          uncensored model hallucinated.</p>

          <p>In short, Llama 2 is technically a very good LLM, and better than Llama
          1 at answering questions free of any conceivable contention, but it''s not
          just overly censored through alignment. The source LLM is also censored
          by suppressing potentially contentious data, resulting in a notably spike
          in hallucinations when answering harmless popular knowledge questions with
          unfortunate ambiguity. This makes any uncensored LLM based on Llama 2 clearly
          inferior to the ones based on Llama 1 for use as a general purpose chat
          bot. And even as programming assistant you have to be careful how your phrase
          things (e.g. don''t ask it to "kill" a windows process). </p>

          '
        raw: "I've created and used a set of 50 somewhat tricky questions to test\
          \ LLMs, and your previous WizardLM 13B v1.1 scored better than any other\
          \ 13B Q4 or lower LLM. However, every LLM based on Llama 2 went completely\
          \ off the rails on several of my questions.\r\n\r\nFor example, when I asked\
          \ Meta's full 70B \"aligned\" version of Llama 2 about the Meghan Trainor\
          \ song \"Dear Future Husband\" using the lyric \"And don't forget the flowers\
          \ every anniversary\" it said the song didn't exist, then lectured me about\
          \ spreading misinformation online. Then I referred to the song by name in\
          \ my follow up question \"Dear Future Husband\". Llama 2 refused to answer\
          \ and lectured me about using potentially hurtful gender stereotypes. I\
          \ then tricked it into answering my question. Proving it was aware of the\
          \ lyric and the song.\r\n\r\nWhen I went through the same series of questions\
          \ on uncensored 13b Llama 2 LLMs, not just yours, the lecturing about spreading\
          \ misinformation and gender stereotypes are gone, but weird hallucinations\
          \ start occurring. For example, the song's name is \"All About That Bass\"\
          . Yet I can get the information by rephrasing the questions so they can't\
          \ be interpreted as contentious (e.g. a potentially hurtful gender stereotype).\r\
          \n\r\nThis is a pattern I found in my testing, and further confirmed by\
          \ making up about a dozen more tricky questions that can be misinterpreted\
          \ as contentious. On questions were Llama 2 censures and lectures by misidentifying\
          \ contentious questions, the uncensored models, while they don't lecture,\
          \ consistently hallucinate, yet theyt perform very well on similar non-contentious\
          \ questions. Proving that the \"unaligned\" LLM are still censored via the\
          \ suppression of potentially contentious, but completely harmless, information.\r\
          \n\r\nThis is so bad it even happened with some of my non-contentious questions.\
          \ For example, when I asked about the phenomenon that occurs when particles\
          \ travel faster than light in a medium like water (Cherenkov radiation),\
          \ the censored version got to \"air bubble\" and immediately switched to\
          \ warning about scuba diving and paying close attention to your instructor.\
          \ And your uncensored model hallucinated.\r\n\r\nIn short, Llama 2 is technically\
          \ a very good LLM, and better than Llama 1 at answering questions free of\
          \ any conceivable contention, but it's not just overly censored through\
          \ alignment. The source LLM is also censored by suppressing potentially\
          \ contentious data, resulting in a notably spike in hallucinations when\
          \ answering harmless popular knowledge questions with unfortunate ambiguity.\
          \ This makes any uncensored LLM based on Llama 2 clearly inferior to the\
          \ ones based on Llama 1 for use as a general purpose chat bot. And even\
          \ as programming assistant you have to be careful how your phrase things\
          \ (e.g. don't ask it to \"kill\" a windows process). "
        updatedAt: '2023-09-12T16:32:35.340Z'
      numEdits: 0
      reactions: []
    id: 650092a37f91f940400816a6
    type: comment
  author: Phil337
  content: "I've created and used a set of 50 somewhat tricky questions to test LLMs,\
    \ and your previous WizardLM 13B v1.1 scored better than any other 13B Q4 or lower\
    \ LLM. However, every LLM based on Llama 2 went completely off the rails on several\
    \ of my questions.\r\n\r\nFor example, when I asked Meta's full 70B \"aligned\"\
    \ version of Llama 2 about the Meghan Trainor song \"Dear Future Husband\" using\
    \ the lyric \"And don't forget the flowers every anniversary\" it said the song\
    \ didn't exist, then lectured me about spreading misinformation online. Then I\
    \ referred to the song by name in my follow up question \"Dear Future Husband\"\
    . Llama 2 refused to answer and lectured me about using potentially hurtful gender\
    \ stereotypes. I then tricked it into answering my question. Proving it was aware\
    \ of the lyric and the song.\r\n\r\nWhen I went through the same series of questions\
    \ on uncensored 13b Llama 2 LLMs, not just yours, the lecturing about spreading\
    \ misinformation and gender stereotypes are gone, but weird hallucinations start\
    \ occurring. For example, the song's name is \"All About That Bass\". Yet I can\
    \ get the information by rephrasing the questions so they can't be interpreted\
    \ as contentious (e.g. a potentially hurtful gender stereotype).\r\n\r\nThis is\
    \ a pattern I found in my testing, and further confirmed by making up about a\
    \ dozen more tricky questions that can be misinterpreted as contentious. On questions\
    \ were Llama 2 censures and lectures by misidentifying contentious questions,\
    \ the uncensored models, while they don't lecture, consistently hallucinate, yet\
    \ theyt perform very well on similar non-contentious questions. Proving that the\
    \ \"unaligned\" LLM are still censored via the suppression of potentially contentious,\
    \ but completely harmless, information.\r\n\r\nThis is so bad it even happened\
    \ with some of my non-contentious questions. For example, when I asked about the\
    \ phenomenon that occurs when particles travel faster than light in a medium like\
    \ water (Cherenkov radiation), the censored version got to \"air bubble\" and\
    \ immediately switched to warning about scuba diving and paying close attention\
    \ to your instructor. And your uncensored model hallucinated.\r\n\r\nIn short,\
    \ Llama 2 is technically a very good LLM, and better than Llama 1 at answering\
    \ questions free of any conceivable contention, but it's not just overly censored\
    \ through alignment. The source LLM is also censored by suppressing potentially\
    \ contentious data, resulting in a notably spike in hallucinations when answering\
    \ harmless popular knowledge questions with unfortunate ambiguity. This makes\
    \ any uncensored LLM based on Llama 2 clearly inferior to the ones based on Llama\
    \ 1 for use as a general purpose chat bot. And even as programming assistant you\
    \ have to be careful how your phrase things (e.g. don't ask it to \"kill\" a windows\
    \ process). "
  created_at: 2023-09-12 15:32:35+00:00
  edited: false
  hidden: false
  id: 650092a37f91f940400816a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/79ac6ccc5a3564452aa028dec1497957.svg
      fullname: Anthony Guijarro
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Anthonyg5005
      type: user
    createdAt: '2023-10-20T20:20:28.000Z'
    data:
      edited: false
      editors:
      - Anthonyg5005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.974518358707428
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/79ac6ccc5a3564452aa028dec1497957.svg
          fullname: Anthony Guijarro
          isHf: false
          isPro: false
          name: Anthonyg5005
          type: user
        html: '<p>The "uncensored" models are unofficial fine-tunes and not related
          to WizardLM. I made the mistake of testing the uncensored versions first
          and they performed horribly. I assume the reason is because finetuning a
          model overwrites part of it which causes a loss in information. As for the
          problem you are having  with the model refusing to answer, that may be a
          problem with the system prompt you are using. I haven''t had a problem with
          that while testing this model on the LMSYS chatbot arena.</p>

          '
        raw: The "uncensored" models are unofficial fine-tunes and not related to
          WizardLM. I made the mistake of testing the uncensored versions first and
          they performed horribly. I assume the reason is because finetuning a model
          overwrites part of it which causes a loss in information. As for the problem
          you are having  with the model refusing to answer, that may be a problem
          with the system prompt you are using. I haven't had a problem with that
          while testing this model on the LMSYS chatbot arena.
        updatedAt: '2023-10-20T20:20:28.702Z'
      numEdits: 0
      reactions: []
    id: 6532e10cae42162a1eb02396
    type: comment
  author: Anthonyg5005
  content: The "uncensored" models are unofficial fine-tunes and not related to WizardLM.
    I made the mistake of testing the uncensored versions first and they performed
    horribly. I assume the reason is because finetuning a model overwrites part of
    it which causes a loss in information. As for the problem you are having  with
    the model refusing to answer, that may be a problem with the system prompt you
    are using. I haven't had a problem with that while testing this model on the LMSYS
    chatbot arena.
  created_at: 2023-10-20 19:20:28+00:00
  edited: false
  hidden: false
  id: 6532e10cae42162a1eb02396
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: WizardLM/WizardLM-13B-V1.2
repo_type: model
status: open
target_branch: null
title: Something's Wrong, It's Not Your Fault, Llama 2 Is Fundamentally Flawed
