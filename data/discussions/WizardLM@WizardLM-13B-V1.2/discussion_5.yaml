!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nicolasbo
conflicting_files: null
created_at: 2023-07-28 03:22:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9bb7d8e2f902f4d1b3ba3887208643b3.svg
      fullname: Nicolas Bougie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicolasbo
      type: user
    createdAt: '2023-07-28T04:22:20.000Z'
    data:
      edited: false
      editors:
      - nicolasbo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5869339108467102
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9bb7d8e2f902f4d1b3ba3887208643b3.svg
          fullname: Nicolas Bougie
          isHf: false
          isPro: false
          name: nicolasbo
          type: user
        html: "<h3 id=\"system-info\">System Info</h3>\n<p>Error when loading LLM\
          \ with 8 bit quantization.</p>\n<p><strong>Versions:</strong><br>tokenizers\
          \                    0.13.3<br>transformers                  4.31.0</p>\n\
          <p><strong>Error message:</strong></p>\n<pre><code>  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 408, in forward\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\",\
          \ line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 295, in forward\n    query_states = [F.linear(hidden_states, query_slices[i])\
          \ for i in range(self.pretraining_tp)]\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 295, in &lt;listcomp&gt;\n    query_states = [F.linear(hidden_states,\
          \ query_slices[i]) for i in range(self.pretraining_tp)]\nRuntimeError: expected\
          \ scalar type Half but found Char\n</code></pre>\n<h3 id=\"who-can-help\"\
          >Who can help?</h3>\n<p><em>No response</em></p>\n<h3 id=\"information\"\
          >Information</h3>\n<ul>\n<li><input type=\"checkbox\" disabled=\"\"> The\
          \ official example scripts</li>\n<li><input type=\"checkbox\" disabled=\"\
          \"> My own modified scripts</li>\n</ul>\n<h3 id=\"tasks\">Tasks</h3>\n<ul>\n\
          <li><input type=\"checkbox\" disabled=\"\" checked=\"\"> An officially supported\
          \ task in the <code>examples</code> folder (such as GLUE/SQuAD, ...)</li>\n\
          <li><input type=\"checkbox\" disabled=\"\"> My own task or dataset (give\
          \ details below)</li>\n</ul>\n<h3 id=\"reproduction\">Reproduction</h3>\n\
          <p><strong>To reproduce the issue:</strong></p>\n<pre><code>import torch\n\
          from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig,\
          \ LlamaConfig\n\nmodel_id=\"WizardLM/WizardLM-13B-V1.2\"\ntokenizer = LlamaTokenizer.from_pretrained(model_id)\n\
          model = LlamaForCausalLM.from_pretrained(\n        model_id,\n        load_in_8bit=True,\n\
          \        torch_dtype=torch.float16,\n        device_map=\"auto\",\n)\n\n\
          model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\nmodel.config.bos_token_id\
          \ = 1\nmodel.config.eos_token_id = 2\nmodel.eval()\n</code></pre>\n<p><strong>Inference:</strong></p>\n\
          <pre><code>prompt_ = \"What is the difference between fusion and fission?\"\
          \nprompts = f\"\"\"A chat between a curious user and an artificial intelligence\
          \ assistant. The assistant gives helpful, detailed, and polite answers to\
          \ the user's questions. USER: {prompt_} ASSISTANT:\"\"\"\ninputs = tokenizer(prompts,\
          \ return_tensors=\"pt\")\ndevice = \"cuda\"\ninput_ids = inputs[\"input_ids\"\
          ].to(device)\nmax_new_tokens= 2048\nwith torch.no_grad():\n    generation_output\
          \ = model.generate(\n                input_ids=input_ids,\n            \
          \    return_dict_in_generate=True,\n                output_scores=True,\n\
          \                max_new_tokens=max_new_tokens\n    )\n</code></pre>\n<h3\
          \ id=\"expected-behavior\">Expected behavior</h3>\n<p>Reply to the prompt.</p>\n"
        raw: "### System Info\r\n\r\nError when loading LLM with 8 bit quantization.\r\
          \n\r\n**Versions:**\r\ntokenizers                    0.13.3\r\ntransformers\
          \                  4.31.0\r\n\r\n**Error message:**\r\n```\r\n  File \"\
          /opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 165,\
          \ in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 408, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 295, in forward\r\n    query_states = [F.linear(hidden_states, query_slices[i])\
          \ for i in range(self.pretraining_tp)]\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 295, in <listcomp>\r\n    query_states = [F.linear(hidden_states,\
          \ query_slices[i]) for i in range(self.pretraining_tp)]\r\nRuntimeError:\
          \ expected scalar type Half but found Char\r\n```\r\n\r\n\r\n### Who can\
          \ help?\r\n\r\n_No response_\r\n\r\n### Information\r\n\r\n- [ ] The official\
          \ example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\
          \n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD,\
          \ ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\
          \n\r\n**To reproduce the issue:**\r\n\r\n```\r\nimport torch\r\nfrom transformers\
          \ import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, LlamaConfig\r\
          \n\r\nmodel_id=\"WizardLM/WizardLM-13B-V1.2\"\r\ntokenizer = LlamaTokenizer.from_pretrained(model_id)\r\
          \nmodel = LlamaForCausalLM.from_pretrained(\r\n        model_id,\r\n   \
          \     load_in_8bit=True,\r\n        torch_dtype=torch.float16,\r\n     \
          \   device_map=\"auto\",\r\n)\r\n\r\nmodel.config.pad_token_id = tokenizer.pad_token_id\
          \ = 0  # unk\r\nmodel.config.bos_token_id = 1\r\nmodel.config.eos_token_id\
          \ = 2\r\nmodel.eval()\r\n```\r\n\r\n\r\n**Inference:**\r\n\r\n```\r\nprompt_\
          \ = \"What is the difference between fusion and fission?\"\r\nprompts =\
          \ f\"\"\"A chat between a curious user and an artificial intelligence assistant.\
          \ The assistant gives helpful, detailed, and polite answers to the user's\
          \ questions. USER: {prompt_} ASSISTANT:\"\"\"\r\ninputs = tokenizer(prompts,\
          \ return_tensors=\"pt\")\r\ndevice = \"cuda\"\r\ninput_ids = inputs[\"input_ids\"\
          ].to(device)\r\nmax_new_tokens= 2048\r\nwith torch.no_grad():\r\n    generation_output\
          \ = model.generate(\r\n                input_ids=input_ids,\r\n        \
          \        return_dict_in_generate=True,\r\n                output_scores=True,\r\
          \n                max_new_tokens=max_new_tokens\r\n    )\r\n```\r\n\r\n\r\
          \n### Expected behavior\r\n\r\nReply to the prompt."
        updatedAt: '2023-07-28T04:22:20.023Z'
      numEdits: 0
      reactions: []
    id: 64c3427cf8632fe837e0f512
    type: comment
  author: nicolasbo
  content: "### System Info\r\n\r\nError when loading LLM with 8 bit quantization.\r\
    \n\r\n**Versions:**\r\ntokenizers                    0.13.3\r\ntransformers  \
    \                4.31.0\r\n\r\n**Error message:**\r\n```\r\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 408, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
    \ = self.self_attn(\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in\
    \ new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 295, in forward\r\n    query_states = [F.linear(hidden_states, query_slices[i])\
    \ for i in range(self.pretraining_tp)]\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 295, in <listcomp>\r\n    query_states = [F.linear(hidden_states, query_slices[i])\
    \ for i in range(self.pretraining_tp)]\r\nRuntimeError: expected scalar type Half\
    \ but found Char\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\n_No response_\r\n\r\
    \n### Information\r\n\r\n- [ ] The official example scripts\r\n- [ ] My own modified\
    \ scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples`\
    \ folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details\
    \ below)\r\n\r\n### Reproduction\r\n\r\n**To reproduce the issue:**\r\n\r\n```\r\
    \nimport torch\r\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig,\
    \ LlamaConfig\r\n\r\nmodel_id=\"WizardLM/WizardLM-13B-V1.2\"\r\ntokenizer = LlamaTokenizer.from_pretrained(model_id)\r\
    \nmodel = LlamaForCausalLM.from_pretrained(\r\n        model_id,\r\n        load_in_8bit=True,\r\
    \n        torch_dtype=torch.float16,\r\n        device_map=\"auto\",\r\n)\r\n\r\
    \nmodel.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\r\nmodel.config.bos_token_id\
    \ = 1\r\nmodel.config.eos_token_id = 2\r\nmodel.eval()\r\n```\r\n\r\n\r\n**Inference:**\r\
    \n\r\n```\r\nprompt_ = \"What is the difference between fusion and fission?\"\r\
    \nprompts = f\"\"\"A chat between a curious user and an artificial intelligence\
    \ assistant. The assistant gives helpful, detailed, and polite answers to the\
    \ user's questions. USER: {prompt_} ASSISTANT:\"\"\"\r\ninputs = tokenizer(prompts,\
    \ return_tensors=\"pt\")\r\ndevice = \"cuda\"\r\ninput_ids = inputs[\"input_ids\"\
    ].to(device)\r\nmax_new_tokens= 2048\r\nwith torch.no_grad():\r\n    generation_output\
    \ = model.generate(\r\n                input_ids=input_ids,\r\n              \
    \  return_dict_in_generate=True,\r\n                output_scores=True,\r\n  \
    \              max_new_tokens=max_new_tokens\r\n    )\r\n```\r\n\r\n\r\n### Expected\
    \ behavior\r\n\r\nReply to the prompt."
  created_at: 2023-07-28 03:22:20+00:00
  edited: false
  hidden: false
  id: 64c3427cf8632fe837e0f512
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/28ec1f893712a422aa6973084cd6d59f.svg
      fullname: 'Anand Trivedi '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Aavenir
      type: user
    createdAt: '2023-08-08T11:41:04.000Z'
    data:
      edited: false
      editors:
      - Aavenir
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.1823035031557083
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/28ec1f893712a422aa6973084cd6d59f.svg
          fullname: 'Anand Trivedi '
          isHf: false
          isPro: false
          name: Aavenir
          type: user
        html: '<p>i get same error .</p>

          '
        raw: i get same error .
        updatedAt: '2023-08-08T11:41:04.057Z'
      numEdits: 0
      reactions: []
    id: 64d229d0c2bd2354220166f1
    type: comment
  author: Aavenir
  content: i get same error .
  created_at: 2023-08-08 10:41:04+00:00
  edited: false
  hidden: false
  id: 64d229d0c2bd2354220166f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5b57d22236d909beb3976da000d9328d.svg
      fullname: Matt Burton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TripleExclam
      type: user
    createdAt: '2023-08-10T03:26:04.000Z'
    data:
      edited: false
      editors:
      - TripleExclam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4446384310722351
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5b57d22236d909beb3976da000d9328d.svg
          fullname: Matt Burton
          isHf: false
          isPro: false
          name: TripleExclam
          type: user
        html: "<pre><code>tokenizer = LlamaTokenizer.from_pretrained(\"WizardLM/WizardLM-13B-V1.2\"\
          , model_max_length=2048)\nmodel = LlamaForCausalLM.from_pretrained(\"WizardLM/WizardLM-13B-V1.2\"\
          , pretraining_tp=1,\n                                         load_in_8bit=True,\
          \ torch_dtype=torch.float16, device_map=\"auto\")\n</code></pre>\n<p>Load\
          \ the model with <code>pretraining_tp=1</code></p>\n"
        raw: "```\ntokenizer = LlamaTokenizer.from_pretrained(\"WizardLM/WizardLM-13B-V1.2\"\
          , model_max_length=2048)\nmodel = LlamaForCausalLM.from_pretrained(\"WizardLM/WizardLM-13B-V1.2\"\
          , pretraining_tp=1,\n                                         load_in_8bit=True,\
          \ torch_dtype=torch.float16, device_map=\"auto\")\n```\n\nLoad the model\
          \ with `pretraining_tp=1`"
        updatedAt: '2023-08-10T03:26:04.162Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Boss302
    id: 64d458ccd0cfd876d93eb87a
    type: comment
  author: TripleExclam
  content: "```\ntokenizer = LlamaTokenizer.from_pretrained(\"WizardLM/WizardLM-13B-V1.2\"\
    , model_max_length=2048)\nmodel = LlamaForCausalLM.from_pretrained(\"WizardLM/WizardLM-13B-V1.2\"\
    , pretraining_tp=1,\n                                         load_in_8bit=True,\
    \ torch_dtype=torch.float16, device_map=\"auto\")\n```\n\nLoad the model with\
    \ `pretraining_tp=1`"
  created_at: 2023-08-10 02:26:04+00:00
  edited: false
  hidden: false
  id: 64d458ccd0cfd876d93eb87a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: WizardLM/WizardLM-13B-V1.2
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: expected scalar type Half but found Char'
