!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheBloke
conflicting_files: null
created_at: 2023-07-26 11:48:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-26T12:48:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9466009140014648
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Thanks for the great release!</p>\n<p>Could you please clarify the\
          \ prompt template?</p>\n<p>For WizardLM V1.0 13B and 30B you had Vicuna\
          \ style:</p>\n<pre><code>A chat between a curious user and an artificial\
          \ intelligence assistant. The assistant gives helpful, detailed, and polite\
          \ answers to the user's questions. USER: hello, who are you? ASSISTANT:\
          \ \n</code></pre>\n<p>For the earlier WizardLM V1.0 7B you had Alpaca style:</p>\n\
          <pre><code>\"Below is an instruction that describes a task. Write a response\
          \ that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\\
          n\\n### Response:\"\n</code></pre>\n<p>I assumed that this was Vicuna, as\
          \ that is what you used most recently.</p>\n<p>But someone is telling me\
          \ that Alpaca is working better.</p>\n<p>It would be great to get confirmation\
          \ of the official intended prompt template?</p>\n<p>Thanks again for all\
          \ your great models.</p>\n"
        raw: "Thanks for the great release!\r\n\r\nCould you please clarify the prompt\
          \ template?\r\n\r\nFor WizardLM V1.0 13B and 30B you had Vicuna style:\r\
          \n```\r\nA chat between a curious user and an artificial intelligence assistant.\
          \ The assistant gives helpful, detailed, and polite answers to the user's\
          \ questions. USER: hello, who are you? ASSISTANT: \r\n```\r\n\r\nFor the\
          \ earlier WizardLM V1.0 7B you had Alpaca style:\r\n```\r\n\"Below is an\
          \ instruction that describes a task. Write a response that appropriately\
          \ completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n###\
          \ Response:\"\r\n```\r\n\r\nI assumed that this was Vicuna, as that is what\
          \ you used most recently.\r\n\r\nBut someone is telling me that Alpaca is\
          \ working better.\r\n\r\nIt would be great to get confirmation of the official\
          \ intended prompt template?\r\n\r\nThanks again for all your great models."
        updatedAt: '2023-07-26T12:48:27.867Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - jonfairbanks
        - LittleTitBoy
        - zhenpingfeng
    id: 64c1161babe5f855b7ad92c6
    type: comment
  author: TheBloke
  content: "Thanks for the great release!\r\n\r\nCould you please clarify the prompt\
    \ template?\r\n\r\nFor WizardLM V1.0 13B and 30B you had Vicuna style:\r\n```\r\
    \nA chat between a curious user and an artificial intelligence assistant. The\
    \ assistant gives helpful, detailed, and polite answers to the user's questions.\
    \ USER: hello, who are you? ASSISTANT: \r\n```\r\n\r\nFor the earlier WizardLM\
    \ V1.0 7B you had Alpaca style:\r\n```\r\n\"Below is an instruction that describes\
    \ a task. Write a response that appropriately completes the request.\\n\\n###\
    \ Instruction:\\n{instruction}\\n\\n### Response:\"\r\n```\r\n\r\nI assumed that\
    \ this was Vicuna, as that is what you used most recently.\r\n\r\nBut someone\
    \ is telling me that Alpaca is working better.\r\n\r\nIt would be great to get\
    \ confirmation of the official intended prompt template?\r\n\r\nThanks again for\
    \ all your great models."
  created_at: 2023-07-26 11:48:27+00:00
  edited: false
  hidden: false
  id: 64c1161babe5f855b7ad92c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4cd276aaa319b12d0beaf23c65630769.svg
      fullname: "DAN\u2122"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dranger003
      type: user
    createdAt: '2023-07-27T23:59:42.000Z'
    data:
      edited: false
      editors:
      - dranger003
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8584494590759277
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4cd276aaa319b12d0beaf23c65630769.svg
          fullname: "DAN\u2122"
          isHf: false
          isPro: false
          name: dranger003
          type: user
        html: "<p>Looks like you have the right prompt <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ as it is the same in their repo:<br><a rel=\"nofollow\" href=\"https://github.com/nlpxucan/WizardLM/blob/main/WizardLM/src/infer_wizardlm13b.py#L79\"\
          >https://github.com/nlpxucan/WizardLM/blob/main/WizardLM/src/infer_wizardlm13b.py#L79</a></p>\n"
        raw: 'Looks like you have the right prompt @TheBloke as it is the same in
          their repo:

          https://github.com/nlpxucan/WizardLM/blob/main/WizardLM/src/infer_wizardlm13b.py#L79'
        updatedAt: '2023-07-27T23:59:42.392Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - aastha6
    id: 64c304ee027e6d96666f4cf1
    type: comment
  author: dranger003
  content: 'Looks like you have the right prompt @TheBloke as it is the same in their
    repo:

    https://github.com/nlpxucan/WizardLM/blob/main/WizardLM/src/infer_wizardlm13b.py#L79'
  created_at: 2023-07-27 22:59:42+00:00
  edited: false
  hidden: false
  id: 64c304ee027e6d96666f4cf1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: WizardLM/WizardLM-13B-V1.2
repo_type: model
status: open
target_branch: null
title: What is the prompt format?
