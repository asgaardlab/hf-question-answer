!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hwasiti
conflicting_files: null
created_at: 2022-11-30 07:21:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
      fullname: Haider Alwasiti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hwasiti
      type: user
    createdAt: '2022-11-30T07:21:00.000Z'
    data:
      edited: false
      editors:
      - hwasiti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
          fullname: Haider Alwasiti
          isHf: false
          isPro: false
          name: hwasiti
          type: user
        html: "<p>Tried this model on CPU only with float16 and the following code\
          \ gave me this error:<br><code>RuntimeError: \"LayerNormKernelImpl\" not\
          \ implemented for 'Half'</code></p>\n<pre><code>from transformers import\
          \ AutoTokenizer, OPTForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          facebook/galactica-6.7b\")\nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-6.7b\"\
          , torch_dtype=torch.float16)\n\ninput_text = \"\"\"\n# The benefits of deadlifting\n\
          \n## INTRODUCTION\n\"\"\"\n\nrandomizer_value = 0\nrepititions = 1\n\n\n\
          # set seed to reproduce results. Feel free to change the seed though to\
          \ get different results\ntorch.manual_seed(randomizer_value)\n\ninput_ids\
          \ = tokenizer(input_text, return_tensors=\"pt\").input_ids \n\n# set top_k\
          \ = 50 and set top_p = 0.95 and num_return_sequences = 3\nsample_outputs\
          \ = model.generate(\n    input_ids,\n    do_sample=True, \n    max_length=2000,\
          \ \n    top_k=50, \n    top_p=0.95, \n    num_return_sequences=1\n)\n</code></pre>\n"
        raw: "Tried this model on CPU only with float16 and the following code gave\
          \ me this error:\r\n`RuntimeError: \"LayerNormKernelImpl\" not implemented\
          \ for 'Half'`\r\n\r\n```\r\nfrom transformers import AutoTokenizer, OPTForCausalLM\r\
          \n\r\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-6.7b\"\
          )\r\nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-6.7b\"\
          , torch_dtype=torch.float16)\r\n\r\ninput_text = \"\"\"\r\n# The benefits\
          \ of deadlifting\r\n\r\n## INTRODUCTION\r\n\"\"\"\r\n\r\nrandomizer_value\
          \ = 0\r\nrepititions = 1\r\n\r\n\r\n# set seed to reproduce results. Feel\
          \ free to change the seed though to get different results\r\ntorch.manual_seed(randomizer_value)\r\
          \n\r\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\
          \ \r\n\r\n# set top_k = 50 and set top_p = 0.95 and num_return_sequences\
          \ = 3\r\nsample_outputs = model.generate(\r\n    input_ids,\r\n    do_sample=True,\
          \ \r\n    max_length=2000, \r\n    top_k=50, \r\n    top_p=0.95, \r\n  \
          \  num_return_sequences=1\r\n)\r\n\r\n```"
        updatedAt: '2022-11-30T07:21:00.427Z'
      numEdits: 0
      reactions: []
    id: 6387045c2ee2261893d84d79
    type: comment
  author: hwasiti
  content: "Tried this model on CPU only with float16 and the following code gave\
    \ me this error:\r\n`RuntimeError: \"LayerNormKernelImpl\" not implemented for\
    \ 'Half'`\r\n\r\n```\r\nfrom transformers import AutoTokenizer, OPTForCausalLM\r\
    \n\r\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-6.7b\")\r\
    \nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-6.7b\", torch_dtype=torch.float16)\r\
    \n\r\ninput_text = \"\"\"\r\n# The benefits of deadlifting\r\n\r\n## INTRODUCTION\r\
    \n\"\"\"\r\n\r\nrandomizer_value = 0\r\nrepititions = 1\r\n\r\n\r\n# set seed\
    \ to reproduce results. Feel free to change the seed though to get different results\r\
    \ntorch.manual_seed(randomizer_value)\r\n\r\ninput_ids = tokenizer(input_text,\
    \ return_tensors=\"pt\").input_ids \r\n\r\n# set top_k = 50 and set top_p = 0.95\
    \ and num_return_sequences = 3\r\nsample_outputs = model.generate(\r\n    input_ids,\r\
    \n    do_sample=True, \r\n    max_length=2000, \r\n    top_k=50, \r\n    top_p=0.95,\
    \ \r\n    num_return_sequences=1\r\n)\r\n\r\n```"
  created_at: 2022-11-30 07:21:00+00:00
  edited: false
  hidden: false
  id: 6387045c2ee2261893d84d79
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
      fullname: Haider Alwasiti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hwasiti
      type: user
    createdAt: '2022-12-07T18:07:36.000Z'
    data:
      from: No support for float16?
      to: No support for float16 on CPU?
    id: 6390d6689f5aad7a7e6e5d14
    type: title-change
  author: hwasiti
  created_at: 2022-12-07 18:07:36+00:00
  id: 6390d6689f5aad7a7e6e5d14
  new_title: No support for float16 on CPU?
  old_title: No support for float16?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
      fullname: Haider Alwasiti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hwasiti
      type: user
    createdAt: '2022-12-07T21:12:41.000Z'
    data:
      edited: true
      editors:
      - hwasiti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
          fullname: Haider Alwasiti
          isHf: false
          isPro: false
          name: hwasiti
          type: user
        html: "<p>I think this post answers my question:<br><a rel=\"nofollow\" href=\"\
          https://twitter.com/pytorch/status/1450502321838960641?lang=en\">https://twitter.com/pytorch/status/1450502321838960641?lang=en</a></p>\n\
          <pre><code>FP16 is only supported in CUDA, BF16 has support on newer CPUs\
          \ and TPUs\nCalling .half() on your network and tensors explicitly casts\
          \ them to FP16, but not all ops are safe to run in half-precision. \n</code></pre>\n"
        raw: "I think this post answers my question:\nhttps://twitter.com/pytorch/status/1450502321838960641?lang=en\n\
          \n```\nFP16 is only supported in CUDA, BF16 has support on newer CPUs and\
          \ TPUs\nCalling .half() on your network and tensors explicitly casts them\
          \ to FP16, but not all ops are safe to run in half-precision. \n```"
        updatedAt: '2023-01-05T03:00:16.768Z'
      numEdits: 1
      reactions: []
    id: 639101c921dc990662596da2
    type: comment
  author: hwasiti
  content: "I think this post answers my question:\nhttps://twitter.com/pytorch/status/1450502321838960641?lang=en\n\
    \n```\nFP16 is only supported in CUDA, BF16 has support on newer CPUs and TPUs\n\
    Calling .half() on your network and tensors explicitly casts them to FP16, but\
    \ not all ops are safe to run in half-precision. \n```"
  created_at: 2022-12-07 21:12:41+00:00
  edited: true
  hidden: false
  id: 639101c921dc990662596da2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
      fullname: Haider Alwasiti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hwasiti
      type: user
    createdAt: '2022-12-07T21:15:50.000Z'
    data:
      edited: false
      editors:
      - hwasiti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
          fullname: Haider Alwasiti
          isHf: false
          isPro: false
          name: hwasiti
          type: user
        html: "<p>For CPU BF16 is supported:</p>\n<pre><code>1.10 onwards, PyTorch\
          \ has a generic API `torch. autocast()` that automatically casts\n\n*  CUDA\
          \ tensors to FP16, and\n*  CPU tensors to BF16. \n</code></pre>\n<p>source:\
          \ <a rel=\"nofollow\" href=\"https://twitter.com/PyTorch/status/1450502326834368516\"\
          >https://twitter.com/PyTorch/status/1450502326834368516</a></p>\n<p>Now\
          \ the question is can we use BF16 instead of FP16?</p>\n"
        raw: "For CPU BF16 is supported:\n\n\n```\n1.10 onwards, PyTorch has a generic\
          \ API `torch. autocast()` that automatically casts\n\n*  CUDA tensors to\
          \ FP16, and\n*  CPU tensors to BF16. \n```\nsource: https://twitter.com/PyTorch/status/1450502326834368516\n\
          \nNow the question is can we use BF16 instead of FP16?"
        updatedAt: '2022-12-07T21:15:50.893Z'
      numEdits: 0
      reactions: []
    id: 6391028621dc990662598162
    type: comment
  author: hwasiti
  content: "For CPU BF16 is supported:\n\n\n```\n1.10 onwards, PyTorch has a generic\
    \ API `torch. autocast()` that automatically casts\n\n*  CUDA tensors to FP16,\
    \ and\n*  CPU tensors to BF16. \n```\nsource: https://twitter.com/PyTorch/status/1450502326834368516\n\
    \nNow the question is can we use BF16 instead of FP16?"
  created_at: 2022-12-07 21:15:50+00:00
  edited: false
  hidden: false
  id: 6391028621dc990662598162
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
      fullname: Haider Alwasiti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hwasiti
      type: user
    createdAt: '2022-12-07T21:35:33.000Z'
    data:
      edited: true
      editors:
      - hwasiti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
          fullname: Haider Alwasiti
          isHf: false
          isPro: false
          name: hwasiti
          type: user
        html: "<p>Again answering myself:<br>We\u2019re empowering PyTorch 1.12 on\
          \ the 3rd gen @Intel<br> Xeon\xAE Scalable processor (codename Cooper Lake).\
          \ It\u2019s the first general purpose x86 CPU with native bfloat16 support,\
          \ showing a 1.4x to 2.2x performance gain over float32 on the TorchVision\
          \ models</p>\n<p>source:<br><a rel=\"nofollow\" href=\"https://twitter.com/pytorch/status/1559611043273375746?lang=en\"\
          >https://twitter.com/pytorch/status/1559611043273375746?lang=en</a></p>\n\
          <p>Conclusion:<br>Only recent XEON processors support BFLOAT16 natively.\
          \ (Cooperlake introduced in June 2020)</p>\n"
        raw: "Again answering myself:\nWe\u2019re empowering PyTorch 1.12 on the 3rd\
          \ gen @Intel\n Xeon\xAE Scalable processor (codename Cooper Lake). It\u2019\
          s the first general purpose x86 CPU with native bfloat16 support, showing\
          \ a 1.4x to 2.2x performance gain over float32 on the TorchVision models\n\
          \nsource:\nhttps://twitter.com/pytorch/status/1559611043273375746?lang=en\n\
          \nConclusion:\nOnly recent XEON processors support BFLOAT16 natively. (Cooperlake\
          \ introduced in June 2020)"
        updatedAt: '2022-12-07T21:35:42.349Z'
      numEdits: 1
      reactions: []
    id: 63910725e6d656eb421cb6e5
    type: comment
  author: hwasiti
  content: "Again answering myself:\nWe\u2019re empowering PyTorch 1.12 on the 3rd\
    \ gen @Intel\n Xeon\xAE Scalable processor (codename Cooper Lake). It\u2019s the\
    \ first general purpose x86 CPU with native bfloat16 support, showing a 1.4x to\
    \ 2.2x performance gain over float32 on the TorchVision models\n\nsource:\nhttps://twitter.com/pytorch/status/1559611043273375746?lang=en\n\
    \nConclusion:\nOnly recent XEON processors support BFLOAT16 natively. (Cooperlake\
    \ introduced in June 2020)"
  created_at: 2022-12-07 21:35:33+00:00
  edited: true
  hidden: false
  id: 63910725e6d656eb421cb6e5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: facebook/galactica-6.7b
repo_type: model
status: open
target_branch: null
title: No support for float16 on CPU?
