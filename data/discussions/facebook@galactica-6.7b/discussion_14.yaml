!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sxx123
conflicting_files: null
created_at: 2023-06-24 11:45:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c78c814712bccca7bdf05bd46bd3cb6a.svg
      fullname: shixiangxiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sxx123
      type: user
    createdAt: '2023-06-24T12:45:47.000Z'
    data:
      edited: false
      editors:
      - sxx123
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.16913136839866638
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c78c814712bccca7bdf05bd46bd3cb6a.svg
          fullname: shixiangxiang
          isHf: false
          isPro: false
          name: sxx123
          type: user
        html: "<p>from transformers import AutoTokenizer, OPTForCausalLM</p>\n<p>tokenizer\
          \ = AutoTokenizer.from_pretrained(\"facebook/galactica-6.7b\")<br>model\
          \ = OPTForCausalLM.from_pretrained(\"facebook/galactica-6.7b\", device_map={\"\
          cuda:0\": \"cuda\"})</p>\n<p>input_text = \"The Transformer architecture\
          \ [START_REF]\"<br>input_ids = tokenizer(input_text, return_tensors=\"pt\"\
          ).input_ids.to(\"cuda:0\")</p>\n<p>outputs = model.generate(input_ids)<br>print(tokenizer.decode(outputs[0]))<br>\u6709\
          \u4EBA\u5E2E\u6211\u770B\u770B\u8FD9\u4E2A\u95EE\u9898\u5417\uFF1F\u611F\
          \u8C22\U0001F647\u200D</p>\n"
        raw: "from transformers import AutoTokenizer, OPTForCausalLM\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"facebook/galactica-6.7b\")\r\nmodel\
          \ = OPTForCausalLM.from_pretrained(\"facebook/galactica-6.7b\", device_map={\"\
          cuda:0\": \"cuda\"})\r\n\r\ninput_text = \"The Transformer architecture\
          \ [START_REF]\"\r\ninput_ids = tokenizer(input_text, return_tensors=\"pt\"\
          ).input_ids.to(\"cuda:0\")\r\n\r\noutputs = model.generate(input_ids)\r\n\
          print(tokenizer.decode(outputs[0]))\r\n\u6709\u4EBA\u5E2E\u6211\u770B\u770B\
          \u8FD9\u4E2A\u95EE\u9898\u5417\uFF1F\u611F\u8C22\U0001F647\u200D"
        updatedAt: '2023-06-24T12:45:47.785Z'
      numEdits: 0
      reactions: []
    id: 6496e57b28a5c2a030f5db4c
    type: comment
  author: sxx123
  content: "from transformers import AutoTokenizer, OPTForCausalLM\r\n\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"facebook/galactica-6.7b\")\r\nmodel = OPTForCausalLM.from_pretrained(\"\
    facebook/galactica-6.7b\", device_map={\"cuda:0\": \"cuda\"})\r\n\r\ninput_text\
    \ = \"The Transformer architecture [START_REF]\"\r\ninput_ids = tokenizer(input_text,\
    \ return_tensors=\"pt\").input_ids.to(\"cuda:0\")\r\n\r\noutputs = model.generate(input_ids)\r\
    \nprint(tokenizer.decode(outputs[0]))\r\n\u6709\u4EBA\u5E2E\u6211\u770B\u770B\u8FD9\
    \u4E2A\u95EE\u9898\u5417\uFF1F\u611F\u8C22\U0001F647\u200D"
  created_at: 2023-06-24 11:45:47+00:00
  edited: false
  hidden: false
  id: 6496e57b28a5c2a030f5db4c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: facebook/galactica-6.7b
repo_type: model
status: open
target_branch: null
title: 'ValueError: model.decoder.embed_tokens.weight doesn''t have any device set.'
