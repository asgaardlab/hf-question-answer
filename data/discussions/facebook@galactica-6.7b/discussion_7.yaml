!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hwasiti
conflicting_files: null
created_at: 2022-11-30 08:36:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
      fullname: Haider Alwasiti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hwasiti
      type: user
    createdAt: '2022-11-30T08:36:25.000Z'
    data:
      edited: false
      editors:
      - hwasiti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
          fullname: Haider Alwasiti
          isHf: false
          isPro: false
          name: hwasiti
          type: user
        html: "<p>I am trying to use my 2 GPUs + RAM mapped by <code> device_map=\"\
          auto\"</code><br>However, I am getting CUDA OOM error.<br>Using only 1 GPU\
          \ gave the same error by specifying:</p>\n<pre><code>import torch\nimport\
          \ os\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"\
          ]=\"1\"\n</code></pre>\n<p>If only CPU used it will take 30 min. for <code>max_length=2000</code>\
          \ on my corei9-9900K/64GB which is too much. I just hoped that my 2x1080Ti\
          \ (11GB) could help to speed up the text generation.</p>\n<pre><code>tokenizer\
          \ = AutoTokenizer.from_pretrained(\"facebook/galactica-6.7b\")    \nmodel\
          \ = OPTForCausalLM.from_pretrained(\"facebook/galactica-6.7b\", device_map=\"\
          auto\",  offload_state_dict = True)  #  no disk offloading\n\ninput_text\
          \ = \"\"\"\n# The benefits of deadlifting\n\n## INTRODUCTION\n\"\"\"\n\n\
          randomizer_value = 0\nrepititions = 1\n\n# set seed to reproduce results.\
          \ Feel free to change the seed though to get different results\ntorch.manual_seed(randomizer_value)\n\
          \n# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids \
          \  ############### CPU only\ninput_ids = tokenizer(input_text, return_tensors=\"\
          pt\").input_ids.to(\"cuda\")\n\n# set top_k = 50 and set top_p = 0.95 and\
          \ num_return_sequences = 3\nsample_outputs = model.generate(\n    input_ids,\n\
          \    do_sample=True, \n    max_length=2000, \n    top_k=50, \n    top_p=0.95,\
          \ \n    num_return_sequences=1\n)\n</code></pre>\n<p><code>OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 20.00 MiB (GPU 1; 10.92 GiB total\
          \ capacity; 9.80 GiB already allocated; 9.75 MiB free; 9.81 GiB reserved\
          \ in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try\
          \ setting max_split_size_mb to avoid fragmentation.  See documentation for\
          \ Memory Management and PYTORCH_CUDA_ALLOC_CONF</code></p>\n"
        raw: "I am trying to use my 2 GPUs + RAM mapped by ` device_map=\"auto\"`\r\
          \nHowever, I am getting CUDA OOM error.\r\nUsing only 1 GPU gave the same\
          \ error by specifying:\r\n```\r\nimport torch\r\nimport os\r\nos.environ[\"\
          CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"\
          ]=\"1\"\r\n```\r\n\r\nIf only CPU used it will take 30 min. for `max_length=2000`\
          \ on my corei9-9900K/64GB which is too much. I just hoped that my 2x1080Ti\
          \ (11GB) could help to speed up the text generation.\r\n\r\n\r\n```\r\n\
          tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-6.7b\") \
          \   \r\nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-6.7b\"\
          , device_map=\"auto\",  offload_state_dict = True)  #  no disk offloading\r\
          \n\r\ninput_text = \"\"\"\r\n# The benefits of deadlifting\r\n\r\n## INTRODUCTION\r\
          \n\"\"\"\r\n\r\nrandomizer_value = 0\r\nrepititions = 1\r\n\r\n# set seed\
          \ to reproduce results. Feel free to change the seed though to get different\
          \ results\r\ntorch.manual_seed(randomizer_value)\r\n\r\n# input_ids = tokenizer(input_text,\
          \ return_tensors=\"pt\").input_ids   ############### CPU only\r\ninput_ids\
          \ = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\r\
          \n\r\n# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\r\
          \nsample_outputs = model.generate(\r\n    input_ids,\r\n    do_sample=True,\
          \ \r\n    max_length=2000, \r\n    top_k=50, \r\n    top_p=0.95, \r\n  \
          \  num_return_sequences=1\r\n)\r\n\r\n```\r\n\r\n`OutOfMemoryError: CUDA\
          \ out of memory. Tried to allocate 20.00 MiB (GPU 1; 10.92 GiB total capacity;\
          \ 9.80 GiB already allocated; 9.75 MiB free; 9.81 GiB reserved in total\
          \ by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF`"
        updatedAt: '2022-11-30T08:36:25.295Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - EnesDS
    id: 63871609189d6915f1d37204
    type: comment
  author: hwasiti
  content: "I am trying to use my 2 GPUs + RAM mapped by ` device_map=\"auto\"`\r\n\
    However, I am getting CUDA OOM error.\r\nUsing only 1 GPU gave the same error\
    \ by specifying:\r\n```\r\nimport torch\r\nimport os\r\nos.environ[\"CUDA_DEVICE_ORDER\"\
    ]=\"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\r\n```\r\n\r\n\
    If only CPU used it will take 30 min. for `max_length=2000` on my corei9-9900K/64GB\
    \ which is too much. I just hoped that my 2x1080Ti (11GB) could help to speed\
    \ up the text generation.\r\n\r\n\r\n```\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    facebook/galactica-6.7b\")    \r\nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-6.7b\"\
    , device_map=\"auto\",  offload_state_dict = True)  #  no disk offloading\r\n\r\
    \ninput_text = \"\"\"\r\n# The benefits of deadlifting\r\n\r\n## INTRODUCTION\r\
    \n\"\"\"\r\n\r\nrandomizer_value = 0\r\nrepititions = 1\r\n\r\n# set seed to reproduce\
    \ results. Feel free to change the seed though to get different results\r\ntorch.manual_seed(randomizer_value)\r\
    \n\r\n# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids   ###############\
    \ CPU only\r\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"\
    cuda\")\r\n\r\n# set top_k = 50 and set top_p = 0.95 and num_return_sequences\
    \ = 3\r\nsample_outputs = model.generate(\r\n    input_ids,\r\n    do_sample=True,\
    \ \r\n    max_length=2000, \r\n    top_k=50, \r\n    top_p=0.95, \r\n    num_return_sequences=1\r\
    \n)\r\n\r\n```\r\n\r\n`OutOfMemoryError: CUDA out of memory. Tried to allocate\
    \ 20.00 MiB (GPU 1; 10.92 GiB total capacity; 9.80 GiB already allocated; 9.75\
    \ MiB free; 9.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated\
    \ memory try setting max_split_size_mb to avoid fragmentation.  See documentation\
    \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF`"
  created_at: 2022-11-30 08:36:25+00:00
  edited: false
  hidden: false
  id: 63871609189d6915f1d37204
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
      fullname: Haider Alwasiti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hwasiti
      type: user
    createdAt: '2022-12-07T20:13:48.000Z'
    data:
      edited: false
      editors:
      - hwasiti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e415d32515656cad0d8fb48e2f7298b3.svg
          fullname: Haider Alwasiti
          isHf: false
          isPro: false
          name: hwasiti
          type: user
        html: "<p>Answering my own question:</p>\n<p>We should remap the model a little\
          \ bit to make the part of the map that occupies the GPU a little smaller.\
          \ One leyr offloaded to another device like a CPU or another GPU.</p>\n\
          <p>Here is an example how I did it for the galactica-30b model:<br>Run the\
          \ following code to explore the current mapping that gives the CUDA error</p>\n\
          <pre><code>tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-30b\"\
          )    \nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-30b\"\
          , device_map=device_map,  torch_dtype=torch.float16)  \n\nmodel.hf_device_map\n\
          </code></pre>\n<p>It will output a dictionary of the map like:</p>\n<pre><code>{'model.decoder.embed_tokens':\
          \ 0,\n 'lm_head': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm':\
          \ 0,\n 'model.decoder.layers.0': 0,\n...\n 'model.decoder.layers.5': 0,\n\
          \ 'model.decoder.layers.6': 0,\n 'model.decoder.layers.7': 0,\n 'model.decoder.layers.8':\
          \ 1,\n...\n 'model.decoder.layers.14': 1,\n 'model.decoder.layers.15': 1,\n\
          \ 'model.decoder.layers.16': 1,\n 'model.decoder.layers.17': 1,\n 'model.decoder.layers.18':\
          \ 'cpu',\n...\n 'model.decoder.layers.45': 'cpu',\n 'model.decoder.layers.46':\
          \ 'cpu',\n 'model.decoder.layers.47': 'cpu'}\n</code></pre>\n<p>Change the\
          \ numbers of GPU 0 and GPU 1 to make it a bit less and move those layers\
          \ to the CPU by the following before initializing the model when you execute\
          \ the script the next time: (Note the layers 7 and 14, 15 how I changed\
          \ them to decrease the mapping on gpus 0 and 1)</p>\n<pre><code>device_map\
          \ = {'model.decoder.embed_tokens': 0,\n 'lm_head': 0,\n 'model.decoder.embed_positions':\
          \ 0,\n 'model.decoder.final_layer_norm': 0,\n 'model.decoder.layers.0':\
          \ 0,\n 'model.decoder.layers.1': 0,\n 'model.decoder.layers.2': 0,\n 'model.decoder.layers.3':\
          \ 0,\n 'model.decoder.layers.4': 0,\n 'model.decoder.layers.5': 0,\n 'model.decoder.layers.6':\
          \ 0,\n 'model.decoder.layers.7': 1,\n 'model.decoder.layers.8': 1,\n 'model.decoder.layers.9':\
          \ 1,\n 'model.decoder.layers.10': 1,\n 'model.decoder.layers.11': 1,\n 'model.decoder.layers.12':\
          \ 1,\n 'model.decoder.layers.13': 1,\n 'model.decoder.layers.14': 1,\n 'model.decoder.layers.15':\
          \ 1,\n 'model.decoder.layers.16': 'cpu',\n 'model.decoder.layers.17': 'cpu',\n\
          \ 'model.decoder.layers.18': 'cpu',\n 'model.decoder.layers.19': 'cpu',\n\
          \ 'model.decoder.layers.20': 'cpu',\n 'model.decoder.layers.21': 'cpu',\n\
          \ 'model.decoder.layers.22': 'cpu',\n 'model.decoder.layers.23': 'cpu',\n\
          \ 'model.decoder.layers.24': 'cpu',\n 'model.decoder.layers.25': 'cpu',\n\
          \ 'model.decoder.layers.26': 'cpu',\n 'model.decoder.layers.27': 'cpu',\n\
          \ 'model.decoder.layers.28': 'cpu',\n 'model.decoder.layers.29': 'cpu',\n\
          \ 'model.decoder.layers.30': 'cpu',\n 'model.decoder.layers.31': 'cpu',\n\
          \ 'model.decoder.layers.32': 'cpu',\n 'model.decoder.layers.33': 'cpu',\n\
          \ 'model.decoder.layers.34': 'cpu',\n 'model.decoder.layers.35': 'cpu',\n\
          \ 'model.decoder.layers.36': 'cpu',\n 'model.decoder.layers.37': 'cpu',\n\
          \ 'model.decoder.layers.38': 'cpu',\n 'model.decoder.layers.39': 'cpu',\n\
          \ 'model.decoder.layers.40': 'cpu',\n 'model.decoder.layers.41': 'cpu',\n\
          \ 'model.decoder.layers.42': 'cpu',\n 'model.decoder.layers.43': 'cpu',\n\
          \ 'model.decoder.layers.44': 'cpu',\n 'model.decoder.layers.45': 'cpu',\n\
          \ 'model.decoder.layers.46': 'cpu',\n 'model.decoder.layers.47': 'cpu'}\n\
          \ntokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-30b\")\
          \  \nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-30b\",\
          \ device_map=device_map,  torch_dtype=torch.float16)  # GPU: manually device\
          \ mapped # do not map to disk (no disk offloading)\n</code></pre>\n<p>keep\
          \ experimenting and observe GPU memory utilization in<br>'nvidia-smi`<br>and\
          \ increase/decrease the mapping on gpus until you find the sweet spot.</p>\n\
          <p>Hope that helps somebody :)</p>\n"
        raw: "Answering my own question:\n\nWe should remap the model a little bit\
          \ to make the part of the map that occupies the GPU a little smaller. One\
          \ leyr offloaded to another device like a CPU or another GPU.\n\nHere is\
          \ an example how I did it for the galactica-30b model:\nRun the following\
          \ code to explore the current mapping that gives the CUDA error\n\n```\n\
          tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-30b\")  \
          \  \nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-30b\",\
          \ device_map=device_map,  torch_dtype=torch.float16)  \n\nmodel.hf_device_map\n\
          ```\nIt will output a dictionary of the map like:\n\n```\n{'model.decoder.embed_tokens':\
          \ 0,\n 'lm_head': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm':\
          \ 0,\n 'model.decoder.layers.0': 0,\n...\n 'model.decoder.layers.5': 0,\n\
          \ 'model.decoder.layers.6': 0,\n 'model.decoder.layers.7': 0,\n 'model.decoder.layers.8':\
          \ 1,\n...\n 'model.decoder.layers.14': 1,\n 'model.decoder.layers.15': 1,\n\
          \ 'model.decoder.layers.16': 1,\n 'model.decoder.layers.17': 1,\n 'model.decoder.layers.18':\
          \ 'cpu',\n...\n 'model.decoder.layers.45': 'cpu',\n 'model.decoder.layers.46':\
          \ 'cpu',\n 'model.decoder.layers.47': 'cpu'}\n```\nChange the numbers of\
          \ GPU 0 and GPU 1 to make it a bit less and move those layers to the CPU\
          \ by the following before initializing the model when you execute the script\
          \ the next time: (Note the layers 7 and 14, 15 how I changed them to decrease\
          \ the mapping on gpus 0 and 1)\n\n```\ndevice_map = {'model.decoder.embed_tokens':\
          \ 0,\n 'lm_head': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm':\
          \ 0,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n 'model.decoder.layers.2':\
          \ 0,\n 'model.decoder.layers.3': 0,\n 'model.decoder.layers.4': 0,\n 'model.decoder.layers.5':\
          \ 0,\n 'model.decoder.layers.6': 0,\n 'model.decoder.layers.7': 1,\n 'model.decoder.layers.8':\
          \ 1,\n 'model.decoder.layers.9': 1,\n 'model.decoder.layers.10': 1,\n 'model.decoder.layers.11':\
          \ 1,\n 'model.decoder.layers.12': 1,\n 'model.decoder.layers.13': 1,\n 'model.decoder.layers.14':\
          \ 1,\n 'model.decoder.layers.15': 1,\n 'model.decoder.layers.16': 'cpu',\n\
          \ 'model.decoder.layers.17': 'cpu',\n 'model.decoder.layers.18': 'cpu',\n\
          \ 'model.decoder.layers.19': 'cpu',\n 'model.decoder.layers.20': 'cpu',\n\
          \ 'model.decoder.layers.21': 'cpu',\n 'model.decoder.layers.22': 'cpu',\n\
          \ 'model.decoder.layers.23': 'cpu',\n 'model.decoder.layers.24': 'cpu',\n\
          \ 'model.decoder.layers.25': 'cpu',\n 'model.decoder.layers.26': 'cpu',\n\
          \ 'model.decoder.layers.27': 'cpu',\n 'model.decoder.layers.28': 'cpu',\n\
          \ 'model.decoder.layers.29': 'cpu',\n 'model.decoder.layers.30': 'cpu',\n\
          \ 'model.decoder.layers.31': 'cpu',\n 'model.decoder.layers.32': 'cpu',\n\
          \ 'model.decoder.layers.33': 'cpu',\n 'model.decoder.layers.34': 'cpu',\n\
          \ 'model.decoder.layers.35': 'cpu',\n 'model.decoder.layers.36': 'cpu',\n\
          \ 'model.decoder.layers.37': 'cpu',\n 'model.decoder.layers.38': 'cpu',\n\
          \ 'model.decoder.layers.39': 'cpu',\n 'model.decoder.layers.40': 'cpu',\n\
          \ 'model.decoder.layers.41': 'cpu',\n 'model.decoder.layers.42': 'cpu',\n\
          \ 'model.decoder.layers.43': 'cpu',\n 'model.decoder.layers.44': 'cpu',\n\
          \ 'model.decoder.layers.45': 'cpu',\n 'model.decoder.layers.46': 'cpu',\n\
          \ 'model.decoder.layers.47': 'cpu'}\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          facebook/galactica-30b\")  \nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-30b\"\
          , device_map=device_map,  torch_dtype=torch.float16)  # GPU: manually device\
          \ mapped # do not map to disk (no disk offloading)\n```\n\nkeep experimenting\
          \ and observe GPU memory utilization in\n'nvidia-smi`\nand increase/decrease\
          \ the mapping on gpus until you find the sweet spot.\n\nHope that helps\
          \ somebody :)"
        updatedAt: '2022-12-07T20:13:48.213Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - donnice849
        - RobertKe
        - MeeraGohil
    id: 6390f3fcde25f9eda5714014
    type: comment
  author: hwasiti
  content: "Answering my own question:\n\nWe should remap the model a little bit to\
    \ make the part of the map that occupies the GPU a little smaller. One leyr offloaded\
    \ to another device like a CPU or another GPU.\n\nHere is an example how I did\
    \ it for the galactica-30b model:\nRun the following code to explore the current\
    \ mapping that gives the CUDA error\n\n```\ntokenizer = AutoTokenizer.from_pretrained(\"\
    facebook/galactica-30b\")    \nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-30b\"\
    , device_map=device_map,  torch_dtype=torch.float16)  \n\nmodel.hf_device_map\n\
    ```\nIt will output a dictionary of the map like:\n\n```\n{'model.decoder.embed_tokens':\
    \ 0,\n 'lm_head': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm':\
    \ 0,\n 'model.decoder.layers.0': 0,\n...\n 'model.decoder.layers.5': 0,\n 'model.decoder.layers.6':\
    \ 0,\n 'model.decoder.layers.7': 0,\n 'model.decoder.layers.8': 1,\n...\n 'model.decoder.layers.14':\
    \ 1,\n 'model.decoder.layers.15': 1,\n 'model.decoder.layers.16': 1,\n 'model.decoder.layers.17':\
    \ 1,\n 'model.decoder.layers.18': 'cpu',\n...\n 'model.decoder.layers.45': 'cpu',\n\
    \ 'model.decoder.layers.46': 'cpu',\n 'model.decoder.layers.47': 'cpu'}\n```\n\
    Change the numbers of GPU 0 and GPU 1 to make it a bit less and move those layers\
    \ to the CPU by the following before initializing the model when you execute the\
    \ script the next time: (Note the layers 7 and 14, 15 how I changed them to decrease\
    \ the mapping on gpus 0 and 1)\n\n```\ndevice_map = {'model.decoder.embed_tokens':\
    \ 0,\n 'lm_head': 0,\n 'model.decoder.embed_positions': 0,\n 'model.decoder.final_layer_norm':\
    \ 0,\n 'model.decoder.layers.0': 0,\n 'model.decoder.layers.1': 0,\n 'model.decoder.layers.2':\
    \ 0,\n 'model.decoder.layers.3': 0,\n 'model.decoder.layers.4': 0,\n 'model.decoder.layers.5':\
    \ 0,\n 'model.decoder.layers.6': 0,\n 'model.decoder.layers.7': 1,\n 'model.decoder.layers.8':\
    \ 1,\n 'model.decoder.layers.9': 1,\n 'model.decoder.layers.10': 1,\n 'model.decoder.layers.11':\
    \ 1,\n 'model.decoder.layers.12': 1,\n 'model.decoder.layers.13': 1,\n 'model.decoder.layers.14':\
    \ 1,\n 'model.decoder.layers.15': 1,\n 'model.decoder.layers.16': 'cpu',\n 'model.decoder.layers.17':\
    \ 'cpu',\n 'model.decoder.layers.18': 'cpu',\n 'model.decoder.layers.19': 'cpu',\n\
    \ 'model.decoder.layers.20': 'cpu',\n 'model.decoder.layers.21': 'cpu',\n 'model.decoder.layers.22':\
    \ 'cpu',\n 'model.decoder.layers.23': 'cpu',\n 'model.decoder.layers.24': 'cpu',\n\
    \ 'model.decoder.layers.25': 'cpu',\n 'model.decoder.layers.26': 'cpu',\n 'model.decoder.layers.27':\
    \ 'cpu',\n 'model.decoder.layers.28': 'cpu',\n 'model.decoder.layers.29': 'cpu',\n\
    \ 'model.decoder.layers.30': 'cpu',\n 'model.decoder.layers.31': 'cpu',\n 'model.decoder.layers.32':\
    \ 'cpu',\n 'model.decoder.layers.33': 'cpu',\n 'model.decoder.layers.34': 'cpu',\n\
    \ 'model.decoder.layers.35': 'cpu',\n 'model.decoder.layers.36': 'cpu',\n 'model.decoder.layers.37':\
    \ 'cpu',\n 'model.decoder.layers.38': 'cpu',\n 'model.decoder.layers.39': 'cpu',\n\
    \ 'model.decoder.layers.40': 'cpu',\n 'model.decoder.layers.41': 'cpu',\n 'model.decoder.layers.42':\
    \ 'cpu',\n 'model.decoder.layers.43': 'cpu',\n 'model.decoder.layers.44': 'cpu',\n\
    \ 'model.decoder.layers.45': 'cpu',\n 'model.decoder.layers.46': 'cpu',\n 'model.decoder.layers.47':\
    \ 'cpu'}\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-30b\"\
    )  \nmodel = OPTForCausalLM.from_pretrained(\"facebook/galactica-30b\", device_map=device_map,\
    \  torch_dtype=torch.float16)  # GPU: manually device mapped # do not map to disk\
    \ (no disk offloading)\n```\n\nkeep experimenting and observe GPU memory utilization\
    \ in\n'nvidia-smi`\nand increase/decrease the mapping on gpus until you find the\
    \ sweet spot.\n\nHope that helps somebody :)"
  created_at: 2022-12-07 20:13:48+00:00
  edited: false
  hidden: false
  id: 6390f3fcde25f9eda5714014
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/de2292102685872775ee180a22d467a0.svg
      fullname: Lidor Eliyahu S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LidorPrototype
      type: user
    createdAt: '2023-04-27T05:09:58.000Z'
    data:
      edited: false
      editors:
      - LidorPrototype
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/de2292102685872775ee180a22d467a0.svg
          fullname: Lidor Eliyahu S
          isHf: false
          isPro: false
          name: LidorPrototype
          type: user
        html: '<p>Hey, I''m using the <code>model = BartForSequenceClassification.from_pretrained("facebook/bart-base")</code>
          and it does not work for me your solution? I''m getting the <code>RuntimeError:
          CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling ''cublasCreate(handle)''</code>
          error, I made forum question about it <a rel="nofollow" href="https://discuss.huggingface.co/t/facebook-bart-fine-tuning-transformers-cuda-error-cublas-status-not-initialize/37680">here</a></p>

          '
        raw: 'Hey, I''m using the `model = BartForSequenceClassification.from_pretrained("facebook/bart-base")`
          and it does not work for me your solution? I''m getting the `RuntimeError:
          CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling ''cublasCreate(handle)''`
          error, I made forum question about it [here](https://discuss.huggingface.co/t/facebook-bart-fine-tuning-transformers-cuda-error-cublas-status-not-initialize/37680)'
        updatedAt: '2023-04-27T05:09:58.598Z'
      numEdits: 0
      reactions: []
    id: 644a03a6b3cd701e0a11d657
    type: comment
  author: LidorPrototype
  content: 'Hey, I''m using the `model = BartForSequenceClassification.from_pretrained("facebook/bart-base")`
    and it does not work for me your solution? I''m getting the `RuntimeError: CUDA
    error: CUBLAS_STATUS_NOT_INITIALIZED when calling ''cublasCreate(handle)''` error,
    I made forum question about it [here](https://discuss.huggingface.co/t/facebook-bart-fine-tuning-transformers-cuda-error-cublas-status-not-initialize/37680)'
  created_at: 2023-04-27 04:09:58+00:00
  edited: false
  hidden: false
  id: 644a03a6b3cd701e0a11d657
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c478e6dcec9f53b44981bfc778e3e62.svg
      fullname: Shujie Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: donnice849
      type: user
    createdAt: '2023-07-12T08:01:59.000Z'
    data:
      edited: false
      editors:
      - donnice849
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.933536946773529
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c478e6dcec9f53b44981bfc778e3e62.svg
          fullname: Shujie Chen
          isHf: false
          isPro: false
          name: donnice849
          type: user
        html: '<p>Thanks so much! It helps me to resolve the issue!</p>

          '
        raw: Thanks so much! It helps me to resolve the issue!
        updatedAt: '2023-07-12T08:01:59.061Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - LidorPrototype
    id: 64ae5df7cfb9ae89710c27fa
    type: comment
  author: donnice849
  content: Thanks so much! It helps me to resolve the issue!
  created_at: 2023-07-12 07:01:59+00:00
  edited: false
  hidden: false
  id: 64ae5df7cfb9ae89710c27fa
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: facebook/galactica-6.7b
repo_type: model
status: open
target_branch: null
title: 'CUDA out of memory error when using '
