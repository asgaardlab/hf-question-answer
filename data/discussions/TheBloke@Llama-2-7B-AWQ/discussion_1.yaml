!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ProfessorXZA
conflicting_files: null
created_at: 2023-09-25 13:48:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/54aea849cc370f09222b1516a7bc6f64.svg
      fullname: Mohammed
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProfessorXZA
      type: user
    createdAt: '2023-09-25T14:48:55.000Z'
    data:
      edited: false
      editors:
      - ProfessorXZA
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4626149535179138
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/54aea849cc370f09222b1516a7bc6f64.svg
          fullname: Mohammed
          isHf: false
          isPro: false
          name: ProfessorXZA
          type: user
        html: '<p>Not sure if im doing something wrong here.  I install vLLM from
          pip and copied and pasted code from the model card, producing the below
          error:</p>

          <p>TypeError                                 Traceback (most recent call
          last)</p>

          <p> in &lt;cell line: 11&gt;()<br>      9 sampling_params = SamplingParams(temperature=0.8,
          top_p=0.95)<br>     10<br>---&gt; 11 llm = LLM(model="TheBloke/Llama-2-7B-AWQ",
          quantization="awq")<br>     12<br>     13 outputs = llm.generate(prompts,
          sampling_params)</p>

          <p>/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py in <strong>init</strong>(self,
          model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size,
          dtype, seed, **kwargs)<br>     54         if "disable_log_stats" not in
          kwargs:<br>     55             kwargs["disable_log_stats"] = True<br>---&gt;
          56         engine_args = EngineArgs(<br>     57             model=model,<br>     58             tokenizer=tokenizer,</p>

          <p>TypeError: EngineArgs.<strong>init</strong>() got an unexpected keyword
          argument ''quantization''</p>

          '
        raw: "Not sure if im doing something wrong here.  I install vLLM from pip\
          \ and copied and pasted code from the model card, producing the below error:\r\
          \n\r\nTypeError                                 Traceback (most recent call\
          \ last)\r\n\r\n<ipython-input-15-cab360ee4318> in <cell line: 11>()\r\n\
          \      9 sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\r\
          \n     10 \r\n---> 11 llm = LLM(model=\"TheBloke/Llama-2-7B-AWQ\", quantization=\"\
          awq\")\r\n     12 \r\n     13 outputs = llm.generate(prompts, sampling_params)\r\
          \n\r\n/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py in\
          \ __init__(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size,\
          \ dtype, seed, **kwargs)\r\n     54         if \"disable_log_stats\" not\
          \ in kwargs:\r\n     55             kwargs[\"disable_log_stats\"] = True\r\
          \n---> 56         engine_args = EngineArgs(\r\n     57             model=model,\r\
          \n     58             tokenizer=tokenizer,\r\n\r\nTypeError: EngineArgs.__init__()\
          \ got an unexpected keyword argument 'quantization'"
        updatedAt: '2023-09-25T14:48:55.742Z'
      numEdits: 0
      reactions: []
    id: 65119dd7dead808ac6b62582
    type: comment
  author: ProfessorXZA
  content: "Not sure if im doing something wrong here.  I install vLLM from pip and\
    \ copied and pasted code from the model card, producing the below error:\r\n\r\
    \nTypeError                                 Traceback (most recent call last)\r\
    \n\r\n<ipython-input-15-cab360ee4318> in <cell line: 11>()\r\n      9 sampling_params\
    \ = SamplingParams(temperature=0.8, top_p=0.95)\r\n     10 \r\n---> 11 llm = LLM(model=\"\
    TheBloke/Llama-2-7B-AWQ\", quantization=\"awq\")\r\n     12 \r\n     13 outputs\
    \ = llm.generate(prompts, sampling_params)\r\n\r\n/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\
    \ in __init__(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size,\
    \ dtype, seed, **kwargs)\r\n     54         if \"disable_log_stats\" not in kwargs:\r\
    \n     55             kwargs[\"disable_log_stats\"] = True\r\n---> 56        \
    \ engine_args = EngineArgs(\r\n     57             model=model,\r\n     58   \
    \          tokenizer=tokenizer,\r\n\r\nTypeError: EngineArgs.__init__() got an\
    \ unexpected keyword argument 'quantization'"
  created_at: 2023-09-25 13:48:55+00:00
  edited: false
  hidden: false
  id: 65119dd7dead808ac6b62582
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-25T14:50:09.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9369864463806152
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please try installing vLLM from Github - I think they''ve still
          not put out a release since they added the <code>quantization</code> parameter
          to LLM.</p>

          '
        raw: Please try installing vLLM from Github - I think they've still not put
          out a release since they added the `quantization` parameter to LLM.
        updatedAt: '2023-09-25T14:50:09.109Z'
      numEdits: 0
      reactions: []
    id: 65119e21353a60593bfa6bac
    type: comment
  author: TheBloke
  content: Please try installing vLLM from Github - I think they've still not put
    out a release since they added the `quantization` parameter to LLM.
  created_at: 2023-09-25 13:50:09+00:00
  edited: false
  hidden: false
  id: 65119e21353a60593bfa6bac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/54aea849cc370f09222b1516a7bc6f64.svg
      fullname: Mohammed
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProfessorXZA
      type: user
    createdAt: '2023-09-25T15:09:05.000Z'
    data:
      edited: false
      editors:
      - ProfessorXZA
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8690451979637146
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/54aea849cc370f09222b1516a7bc6f64.svg
          fullname: Mohammed
          isHf: false
          isPro: false
          name: ProfessorXZA
          type: user
        html: '<p>Thank you! Working.</p>

          '
        raw: Thank you! Working.
        updatedAt: '2023-09-25T15:09:05.466Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6511a291367ce825b1d6b374
    id: 6511a291367ce825b1d6b36e
    type: comment
  author: ProfessorXZA
  content: Thank you! Working.
  created_at: 2023-09-25 14:09:05+00:00
  edited: false
  hidden: false
  id: 6511a291367ce825b1d6b36e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/54aea849cc370f09222b1516a7bc6f64.svg
      fullname: Mohammed
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProfessorXZA
      type: user
    createdAt: '2023-09-25T15:09:05.000Z'
    data:
      status: closed
    id: 6511a291367ce825b1d6b374
    type: status-change
  author: ProfessorXZA
  created_at: 2023-09-25 14:09:05+00:00
  id: 6511a291367ce825b1d6b374
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Llama-2-7B-AWQ
repo_type: model
status: closed
target_branch: null
title: EngineArgs.__init__() got an unexpected keyword argument 'quantization'
