!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RedXeol
conflicting_files: null
created_at: 2023-05-18 22:59:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
      fullname: ReDXeoL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RedXeol
      type: user
    createdAt: '2023-05-18T23:59:31.000Z'
    data:
      edited: true
      editors:
      - RedXeol
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
          fullname: ReDXeoL
          isHf: false
          isPro: false
          name: RedXeol
          type: user
        html: "<p>I recently used AUTOGPTQ with GPT-J models and it worked quite well,\
          \ now out of nowhere I get an error with triton even though it indicates\
          \ that it is turned off, has this ever happened to you, do you know of a\
          \ solution,</p>\n<p>error: (base) C:\\Users\\ReDXeoL\\AutoGPTQ\\examples\\\
          quantization&gt;python basic_usage.py<br>triton not installed.<br>Traceback\
          \ (most recent call last):<br>##<br>##<br>ModuleNotFoundError: No module\
          \ named 'triton'</p>\n<p>(base) C:\\Users\\ReDXeoL\\AutoGPTQ\\examples\\\
          quantization&gt;</p>\n<p>this is my code:</p>\n<p>import os</p>\n<p>from\
          \ transformers import AutoTokenizer, TextGenerationPipeline<br>from auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig</p>\n<p>pretrained_model_dir\
          \ = \"A:/LLMs_LOCAL/bertin_gpt_j_6B_alpaca/\"<br>quantized_model_dir = \"\
          bertin-gpt-j-6B-alpaca-4bit-128g\"</p>\n<h1 id=\"osmakedirsquantized_model_dir-exist_oktrue\"\
          >os.makedirs(quantized_model_dir, exist_ok=True)</h1>\n<p>def main():<br>\
          \    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)<br>\
          \    examples = [<br>        tokenizer(<br>            \"auto-gptq es una\
          \ biblioteca de cuantificaci\xF3n de modelos f\xE1cil de usar con API amigables\
          \ para el usuario, basada en el algoritmo GPTQ.\"<br>            ),<br>\
          \        tokenizer(<br>            \"La inteligencia artificial ha avanzado\
          \ significativamente en los \xFAltimos a\xF1os.\"<br>            ),<br>\
          \        tokenizer(<br>            \"La cuantificaci\xF3n de modelos puede\
          \ reducir el tama\xF1o y mejorar la eficiencia del modelo.\"<br>       \
          \     ),<br>        tokenizer(<br>            \"Los algoritmos de cuantificaci\xF3\
          n pueden reducir la cantidad de memoria y energ\xEDa requerida.\"<br>  \
          \          ),<br>        tokenizer(<br>            \"El aprendizaje profundo\
          \ se utiliza en una variedad de aplicaciones, desde la medicina hasta el\
          \ marketing.\"<br>            ),<br>        tokenizer(<br>            \"\
          La arquitectura GPT-4 es la base de muchos modelos de lenguaje de \xFAltima\
          \ generaci\xF3n.\"<br>            ),<br>        tokenizer(<br>         \
          \   \"El procesamiento del lenguaje natural permite a las m\xE1quinas comprender\
          \ y comunicarse en lenguajes humanos.\"<br>            ),<br>        tokenizer(<br>\
          \            \"Las redes neuronales convolucionales se utilizan com\xFA\
          nmente en la visi\xF3n por computadora.\"<br>            ),<br>        tokenizer(<br>\
          \            \"Los algoritmos de optimizaci\xF3n son fundamentales para\
          \ el entrenamiento de modelos de aprendizaje profundo.\"<br>           \
          \ ),<br>        tokenizer(<br>            \"El aprendizaje por refuerzo\
          \ es una t\xE9cnica de aprendizaje autom\xE1tico en la que los agentes aprenden\
          \ a trav\xE9s de la interacci\xF3n con su entorno.\"<br>            )<br>\
          \    ]</p>\n<pre><code>quantize_config = BaseQuantizeConfig(\n    bits=4,\
          \  # quantize model to 4-bit\n    group_size=128,  # it is recommended to\
          \ set the value to 128\n    desc_act=False\n)\n\n# load un-quantized model,\
          \ the model will always be force loaded into cpu\nmodel = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir,\
          \ quantize_config)\n\n# quantize model, the examples should be list of dict\
          \ whose keys contains \"input_ids\" and \"attention_mask\"\n# with value\
          \ under torch.LongTensor type.\nmodel.quantize(examples, use_triton=False)\n\
          \n# save quantized model\nmodel.save_quantized(quantized_model_dir)\n\n\
          # save quantized model using safetensors\nmodel.save_quantized(quantized_model_dir,\
          \ use_safetensors=True)\n\n# load quantized model, currently only support\
          \ cpu or single gpu\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\
          \ device=\"cuda:0\", use_triton=False)\n\n# inference with model.generate\n\
          print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"\
          pt\").to(\"cuda:0\"))[0]))\n\n# or you can also use pipeline\npipeline =\
          \ TextGenerationPipeline(model=model, tokenizer=tokenizer, device=\"cuda:0\"\
          )\nprint(pipeline(\"auto-gptq is\")[0][\"generated_text\"])\n</code></pre>\n\
          <p>if <strong>name</strong> == \"<strong>main</strong>\":<br>    import\
          \ logging</p>\n<pre><code>logging.basicConfig(\n    format=\"%(asctime)s\
          \ %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"\
          %Y-%m-%d %H:%M:%S\"\n)\n\nmain()\n</code></pre>\n"
        raw: "I recently used AUTOGPTQ with GPT-J models and it worked quite well,\
          \ now out of nowhere I get an error with triton even though it indicates\
          \ that it is turned off, has this ever happened to you, do you know of a\
          \ solution,\n\nerror: (base) C:\\Users\\ReDXeoL\\AutoGPTQ\\examples\\quantization>python\
          \ basic_usage.py\ntriton not installed.\nTraceback (most recent call last):\n\
          ##\n##\nModuleNotFoundError: No module named 'triton'\n\n(base) C:\\Users\\\
          ReDXeoL\\AutoGPTQ\\examples\\quantization>\n\n\nthis is my code:\n\nimport\
          \ os\n\nfrom transformers import AutoTokenizer, TextGenerationPipeline\n\
          from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\n\npretrained_model_dir\
          \ = \"A:/LLMs_LOCAL/bertin_gpt_j_6B_alpaca/\"\nquantized_model_dir = \"\
          bertin-gpt-j-6B-alpaca-4bit-128g\"\n\n# os.makedirs(quantized_model_dir,\
          \ exist_ok=True)\n\n\ndef main():\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir,\
          \ use_fast=True)\n    examples = [\n        tokenizer(\n            \"auto-gptq\
          \ es una biblioteca de cuantificaci\xF3n de modelos f\xE1cil de usar con\
          \ API amigables para el usuario, basada en el algoritmo GPTQ.\"\n      \
          \      ),\n        tokenizer(\n            \"La inteligencia artificial\
          \ ha avanzado significativamente en los \xFAltimos a\xF1os.\"\n        \
          \    ),\n        tokenizer(\n            \"La cuantificaci\xF3n de modelos\
          \ puede reducir el tama\xF1o y mejorar la eficiencia del modelo.\"\n   \
          \         ),\n        tokenizer(\n            \"Los algoritmos de cuantificaci\xF3\
          n pueden reducir la cantidad de memoria y energ\xEDa requerida.\"\n    \
          \        ),\n        tokenizer(\n            \"El aprendizaje profundo se\
          \ utiliza en una variedad de aplicaciones, desde la medicina hasta el marketing.\"\
          \n            ),\n        tokenizer(\n            \"La arquitectura GPT-4\
          \ es la base de muchos modelos de lenguaje de \xFAltima generaci\xF3n.\"\
          \n            ),\n        tokenizer(\n            \"El procesamiento del\
          \ lenguaje natural permite a las m\xE1quinas comprender y comunicarse en\
          \ lenguajes humanos.\"\n            ),\n        tokenizer(\n           \
          \ \"Las redes neuronales convolucionales se utilizan com\xFAnmente en la\
          \ visi\xF3n por computadora.\"\n            ),\n        tokenizer(\n   \
          \         \"Los algoritmos de optimizaci\xF3n son fundamentales para el\
          \ entrenamiento de modelos de aprendizaje profundo.\"\n            ),\n\
          \        tokenizer(\n            \"El aprendizaje por refuerzo es una t\xE9\
          cnica de aprendizaje autom\xE1tico en la que los agentes aprenden a trav\xE9\
          s de la interacci\xF3n con su entorno.\"\n            )\n    ]\n\n    quantize_config\
          \ = BaseQuantizeConfig(\n        bits=4,  # quantize model to 4-bit\n  \
          \      group_size=128,  # it is recommended to set the value to 128\n  \
          \      desc_act=False\n    )\n\n    # load un-quantized model, the model\
          \ will always be force loaded into cpu\n    model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir,\
          \ quantize_config)\n\n    # quantize model, the examples should be list\
          \ of dict whose keys contains \"input_ids\" and \"attention_mask\"\n   \
          \ # with value under torch.LongTensor type.\n    model.quantize(examples,\
          \ use_triton=False)\n\n    # save quantized model\n    model.save_quantized(quantized_model_dir)\n\
          \n    # save quantized model using safetensors\n    model.save_quantized(quantized_model_dir,\
          \ use_safetensors=True)\n\n    # load quantized model, currently only support\
          \ cpu or single gpu\n    model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\
          \ device=\"cuda:0\", use_triton=False)\n\n    # inference with model.generate\n\
          \    print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\"\
          , return_tensors=\"pt\").to(\"cuda:0\"))[0]))\n\n    # or you can also use\
          \ pipeline\n    pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer,\
          \ device=\"cuda:0\")\n    print(pipeline(\"auto-gptq is\")[0][\"generated_text\"\
          ])\n\n\nif __name__ == \"__main__\":\n    import logging\n\n    logging.basicConfig(\n\
          \        format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO,\
          \ datefmt=\"%Y-%m-%d %H:%M:%S\"\n    )\n\n    main()"
        updatedAt: '2023-05-19T00:02:03.692Z'
      numEdits: 1
      reactions: []
    id: 6466bbe3e291c33d5886a8c8
    type: comment
  author: RedXeol
  content: "I recently used AUTOGPTQ with GPT-J models and it worked quite well, now\
    \ out of nowhere I get an error with triton even though it indicates that it is\
    \ turned off, has this ever happened to you, do you know of a solution,\n\nerror:\
    \ (base) C:\\Users\\ReDXeoL\\AutoGPTQ\\examples\\quantization>python basic_usage.py\n\
    triton not installed.\nTraceback (most recent call last):\n##\n##\nModuleNotFoundError:\
    \ No module named 'triton'\n\n(base) C:\\Users\\ReDXeoL\\AutoGPTQ\\examples\\\
    quantization>\n\n\nthis is my code:\n\nimport os\n\nfrom transformers import AutoTokenizer,\
    \ TextGenerationPipeline\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
    \n\npretrained_model_dir = \"A:/LLMs_LOCAL/bertin_gpt_j_6B_alpaca/\"\nquantized_model_dir\
    \ = \"bertin-gpt-j-6B-alpaca-4bit-128g\"\n\n# os.makedirs(quantized_model_dir,\
    \ exist_ok=True)\n\n\ndef main():\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir,\
    \ use_fast=True)\n    examples = [\n        tokenizer(\n            \"auto-gptq\
    \ es una biblioteca de cuantificaci\xF3n de modelos f\xE1cil de usar con API amigables\
    \ para el usuario, basada en el algoritmo GPTQ.\"\n            ),\n        tokenizer(\n\
    \            \"La inteligencia artificial ha avanzado significativamente en los\
    \ \xFAltimos a\xF1os.\"\n            ),\n        tokenizer(\n            \"La\
    \ cuantificaci\xF3n de modelos puede reducir el tama\xF1o y mejorar la eficiencia\
    \ del modelo.\"\n            ),\n        tokenizer(\n            \"Los algoritmos\
    \ de cuantificaci\xF3n pueden reducir la cantidad de memoria y energ\xEDa requerida.\"\
    \n            ),\n        tokenizer(\n            \"El aprendizaje profundo se\
    \ utiliza en una variedad de aplicaciones, desde la medicina hasta el marketing.\"\
    \n            ),\n        tokenizer(\n            \"La arquitectura GPT-4 es la\
    \ base de muchos modelos de lenguaje de \xFAltima generaci\xF3n.\"\n         \
    \   ),\n        tokenizer(\n            \"El procesamiento del lenguaje natural\
    \ permite a las m\xE1quinas comprender y comunicarse en lenguajes humanos.\"\n\
    \            ),\n        tokenizer(\n            \"Las redes neuronales convolucionales\
    \ se utilizan com\xFAnmente en la visi\xF3n por computadora.\"\n            ),\n\
    \        tokenizer(\n            \"Los algoritmos de optimizaci\xF3n son fundamentales\
    \ para el entrenamiento de modelos de aprendizaje profundo.\"\n            ),\n\
    \        tokenizer(\n            \"El aprendizaje por refuerzo es una t\xE9cnica\
    \ de aprendizaje autom\xE1tico en la que los agentes aprenden a trav\xE9s de la\
    \ interacci\xF3n con su entorno.\"\n            )\n    ]\n\n    quantize_config\
    \ = BaseQuantizeConfig(\n        bits=4,  # quantize model to 4-bit\n        group_size=128,\
    \  # it is recommended to set the value to 128\n        desc_act=False\n    )\n\
    \n    # load un-quantized model, the model will always be force loaded into cpu\n\
    \    model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n\
    \n    # quantize model, the examples should be list of dict whose keys contains\
    \ \"input_ids\" and \"attention_mask\"\n    # with value under torch.LongTensor\
    \ type.\n    model.quantize(examples, use_triton=False)\n\n    # save quantized\
    \ model\n    model.save_quantized(quantized_model_dir)\n\n    # save quantized\
    \ model using safetensors\n    model.save_quantized(quantized_model_dir, use_safetensors=True)\n\
    \n    # load quantized model, currently only support cpu or single gpu\n    model\
    \ = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=\"cuda:0\"\
    , use_triton=False)\n\n    # inference with model.generate\n    print(tokenizer.decode(model.generate(**tokenizer(\"\
    auto_gptq is\", return_tensors=\"pt\").to(\"cuda:0\"))[0]))\n\n    # or you can\
    \ also use pipeline\n    pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer,\
    \ device=\"cuda:0\")\n    print(pipeline(\"auto-gptq is\")[0][\"generated_text\"\
    ])\n\n\nif __name__ == \"__main__\":\n    import logging\n\n    logging.basicConfig(\n\
    \        format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO,\
    \ datefmt=\"%Y-%m-%d %H:%M:%S\"\n    )\n\n    main()"
  created_at: 2023-05-18 22:59:31+00:00
  edited: true
  hidden: false
  id: 6466bbe3e291c33d5886a8c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-19T00:01:14.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah this is a recent bug in AutoGPTQ.  I pushed a PR that fixes
          it:  <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/pull/85">https://github.com/PanQiWei/AutoGPTQ/pull/85</a></p>

          <p>Hopefully it''ll be merged into main soon. Or pull my PR and build AutoGPTQ
          from that for now</p>

          '
        raw: 'Yeah this is a recent bug in AutoGPTQ.  I pushed a PR that fixes it:  https://github.com/PanQiWei/AutoGPTQ/pull/85


          Hopefully it''ll be merged into main soon. Or pull my PR and build AutoGPTQ
          from that for now'
        updatedAt: '2023-05-19T00:01:14.265Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - RedXeol
        - mikolodz
    id: 6466bc4a310be9cb764646c0
    type: comment
  author: TheBloke
  content: 'Yeah this is a recent bug in AutoGPTQ.  I pushed a PR that fixes it:  https://github.com/PanQiWei/AutoGPTQ/pull/85


    Hopefully it''ll be merged into main soon. Or pull my PR and build AutoGPTQ from
    that for now'
  created_at: 2023-05-18 23:01:14+00:00
  edited: false
  hidden: false
  id: 6466bc4a310be9cb764646c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
      fullname: ReDXeoL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RedXeol
      type: user
    createdAt: '2023-05-19T00:02:12.000Z'
    data:
      edited: false
      editors:
      - RedXeol
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
          fullname: ReDXeoL
          isHf: false
          isPro: false
          name: RedXeol
          type: user
        html: '<p>i get the same error when i want to activate it in text-generation-webui<br>  --autogptq</p>

          '
        raw: "i get the same error when i want to activate it in text-generation-webui\n\
          \  --autogptq"
        updatedAt: '2023-05-19T00:02:12.788Z'
      numEdits: 0
      reactions: []
    id: 6466bc846c080f37e2573e5d
    type: comment
  author: RedXeol
  content: "i get the same error when i want to activate it in text-generation-webui\n\
    \  --autogptq"
  created_at: 2023-05-18 23:02:12+00:00
  edited: false
  hidden: false
  id: 6466bc846c080f37e2573e5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
      fullname: ReDXeoL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RedXeol
      type: user
    createdAt: '2023-05-19T00:36:12.000Z'
    data:
      edited: false
      editors:
      - RedXeol
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
          fullname: ReDXeoL
          isHf: false
          isPro: false
          name: RedXeol
          type: user
        html: '<p>It''s strange, I replaced all the files with the modifications and
          I still get the same error... do you know if I''m doing something wrong?<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64320a1add466752c7412eb1/rfVCHXJmewywomzs_NB1Q.png"><img
          alt="Captura.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/64320a1add466752c7412eb1/rfVCHXJmewywomzs_NB1Q.png"></a></p>

          '
        raw: 'It''s strange, I replaced all the files with the modifications and I
          still get the same error... do you know if I''m doing something wrong?

          ![Captura.PNG](https://cdn-uploads.huggingface.co/production/uploads/64320a1add466752c7412eb1/rfVCHXJmewywomzs_NB1Q.png)'
        updatedAt: '2023-05-19T00:36:12.759Z'
      numEdits: 0
      reactions: []
    id: 6466c47c80e48bb90c36e707
    type: comment
  author: RedXeol
  content: 'It''s strange, I replaced all the files with the modifications and I still
    get the same error... do you know if I''m doing something wrong?

    ![Captura.PNG](https://cdn-uploads.huggingface.co/production/uploads/64320a1add466752c7412eb1/rfVCHXJmewywomzs_NB1Q.png)'
  created_at: 2023-05-18 23:36:12+00:00
  edited: false
  hidden: false
  id: 6466c47c80e48bb90c36e707
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-19T00:41:12.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You did rebuild with <code>pip install .  </code>?</p>

          <p>I''m going to bed now but if its still a problem let me know and I''ll
          look tomorrow. Do double check that that basic example isn''t setting Triton
          to True</p>

          '
        raw: 'You did rebuild with `pip install .  `?


          I''m going to bed now but if its still a problem let me know and I''ll look
          tomorrow. Do double check that that basic example isn''t setting Triton
          to True'
        updatedAt: '2023-05-19T00:41:12.398Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - RedXeol
    id: 6466c5a880e48bb90c36f891
    type: comment
  author: TheBloke
  content: 'You did rebuild with `pip install .  `?


    I''m going to bed now but if its still a problem let me know and I''ll look tomorrow.
    Do double check that that basic example isn''t setting Triton to True'
  created_at: 2023-05-18 23:41:12+00:00
  edited: false
  hidden: false
  id: 6466c5a880e48bb90c36f891
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
      fullname: ReDXeoL
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RedXeol
      type: user
    createdAt: '2023-05-19T00:49:56.000Z'
    data:
      edited: false
      editors:
      - RedXeol
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZEcpodY84sXwqlXnD0QI1.jpeg?w=200&h=200&f=face
          fullname: ReDXeoL
          isHf: false
          isPro: false
          name: RedXeol
          type: user
        html: '<p>sorry, it was my fault, I didn''t know about pip install . ... it
          works now, have a nice night, thank you very much, you are a genius.</p>

          '
        raw: sorry, it was my fault, I didn't know about pip install . ... it works
          now, have a nice night, thank you very much, you are a genius.
        updatedAt: '2023-05-19T00:49:56.327Z'
      numEdits: 0
      reactions: []
    id: 6466c7b4310be9cb7646f714
    type: comment
  author: RedXeol
  content: sorry, it was my fault, I didn't know about pip install . ... it works
    now, have a nice night, thank you very much, you are a genius.
  created_at: 2023-05-18 23:49:56+00:00
  edited: false
  hidden: false
  id: 6466c7b4310be9cb7646f714
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Manticore-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: help me with a question in 4bits model
