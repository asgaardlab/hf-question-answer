!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jl303
conflicting_files: null
created_at: 2023-05-30 09:22:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/05c0642f3de6836d23154291f6fce4f6.svg
      fullname: James Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jl303
      type: user
    createdAt: '2023-05-30T10:22:29.000Z'
    data:
      edited: false
      editors:
      - jl303
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/05c0642f3de6836d23154291f6fce4f6.svg
          fullname: James Lee
          isHf: false
          isPro: false
          name: jl303
          type: user
        html: "<p>When I trie to run the inference from qwopqwop200/GPTQ-for-LLaMa,\
          \ I get \"Unexpected mma -&gt; mma layout conversion failed.\"<br>Which\
          \ branch is used to quantize the model?<br>Here's my command.</p>\n<pre><code>git\
          \ clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\ncd GPTQ-for-LLaMa\n\
          pip install -r requirements.txt\npython llama_inference.py ../Manticore-13B-GPTQ\
          \ --load ../Manticore-13B-GPTQ/Manticore-13B-GPTQ-4bit-128g.no-act-order.safetensors\
          \ --wbits 4 --groupsize 128 --device 0 --text \"once upon a time, \"\n</code></pre>\n\
          <p>Here's the output.</p>\n<pre><code>2023-05-30 10:02:43.888606: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]\
          \ TF-TRT Warning: Could not find TensorRT\nLoading model ...\n/usr/local/lib/python3.10/dist-packages/safetensors/torch.py:99:\
          \ UserWarning: TypedStorage is deprecated. It will be removed in the future\
          \ and UntypedStorage\nwill be the only storage class. This should only matter\
          \ to you if you are using storages directly.  To access UntypedStorage directly,\
          \ use tensor.untyped_storage()\ninstead of tensor.storage()\n  with safe_open(filename,\
          \ framework=\"pt\", device=device) as f:\nFound 3 unique KN Linear values.\n\
          Warming up autotune cache ...\n100% 12/12 [00:40&lt;00:00,  3.39s/it]\n\
          Found 1 unique fused mlp KN values.\nWarming up autotune cache ...\n  0%\
          \ 0/12 [00:00&lt;?, ?it/s]python3: /project/lib/Analysis/Allocation.cpp:42:\
          \ std::pair&lt;llvm::SmallVector&lt;unsigned int&gt;, llvm::SmallVector&lt;unsigned\
          \ int&gt;\n&gt; mlir::triton::getCvtOrder(const mlir::Attribute&amp;, const\
          \ mlir::Attribute&amp;): Assertion `!(srcMmaLayout &amp;&amp; dstMmaLayout)\
          \ &amp;&amp; \n</code></pre>\n<p>Thanks for your help!</p>\n"
        raw: "When I trie to run the inference from qwopqwop200/GPTQ-for-LLaMa, I\
          \ get \"Unexpected mma -> mma layout conversion failed.\"\r\nWhich branch\
          \ is used to quantize the model?\r\nHere's my command.\r\n\r\n```\r\ngit\
          \ clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\r\ncd GPTQ-for-LLaMa\r\
          \npip install -r requirements.txt\r\npython llama_inference.py ../Manticore-13B-GPTQ\
          \ --load ../Manticore-13B-GPTQ/Manticore-13B-GPTQ-4bit-128g.no-act-order.safetensors\
          \ --wbits 4 --groupsize 128 --device 0 --text \"once upon a time, \"\r\n\
          ```\r\n\r\nHere's the output.\r\n\r\n```\r\n2023-05-30 10:02:43.888606:\
          \ W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning:\
          \ Could not find TensorRT\r\nLoading model ...\r\n/usr/local/lib/python3.10/dist-packages/safetensors/torch.py:99:\
          \ UserWarning: TypedStorage is deprecated. It will be removed in the future\
          \ and UntypedStorage\r\nwill be the only storage class. This should only\
          \ matter to you if you are using storages directly.  To access UntypedStorage\
          \ directly, use tensor.untyped_storage()\r\ninstead of tensor.storage()\r\
          \n  with safe_open(filename, framework=\"pt\", device=device) as f:\r\n\
          Found 3 unique KN Linear values.\r\nWarming up autotune cache ...\r\n100%\
          \ 12/12 [00:40<00:00,  3.39s/it]\r\nFound 1 unique fused mlp KN values.\r\
          \nWarming up autotune cache ...\r\n  0% 0/12 [00:00<?, ?it/s]python3: /project/lib/Analysis/Allocation.cpp:42:\
          \ std::pair<llvm::SmallVector<unsigned int>, llvm::SmallVector<unsigned\
          \ int>\r\n> mlir::triton::getCvtOrder(const mlir::Attribute&, const mlir::Attribute&):\
          \ Assertion `!(srcMmaLayout && dstMmaLayout) && \r\n\r\n```\r\n\r\nThanks\
          \ for your help!"
        updatedAt: '2023-05-30T10:22:29.512Z'
      numEdits: 0
      reactions: []
    id: 6475ce65975bc849732160eb
    type: comment
  author: jl303
  content: "When I trie to run the inference from qwopqwop200/GPTQ-for-LLaMa, I get\
    \ \"Unexpected mma -> mma layout conversion failed.\"\r\nWhich branch is used\
    \ to quantize the model?\r\nHere's my command.\r\n\r\n```\r\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\r\
    \ncd GPTQ-for-LLaMa\r\npip install -r requirements.txt\r\npython llama_inference.py\
    \ ../Manticore-13B-GPTQ --load ../Manticore-13B-GPTQ/Manticore-13B-GPTQ-4bit-128g.no-act-order.safetensors\
    \ --wbits 4 --groupsize 128 --device 0 --text \"once upon a time, \"\r\n```\r\n\
    \r\nHere's the output.\r\n\r\n```\r\n2023-05-30 10:02:43.888606: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]\
    \ TF-TRT Warning: Could not find TensorRT\r\nLoading model ...\r\n/usr/local/lib/python3.10/dist-packages/safetensors/torch.py:99:\
    \ UserWarning: TypedStorage is deprecated. It will be removed in the future and\
    \ UntypedStorage\r\nwill be the only storage class. This should only matter to\
    \ you if you are using storages directly.  To access UntypedStorage directly,\
    \ use tensor.untyped_storage()\r\ninstead of tensor.storage()\r\n  with safe_open(filename,\
    \ framework=\"pt\", device=device) as f:\r\nFound 3 unique KN Linear values.\r\
    \nWarming up autotune cache ...\r\n100% 12/12 [00:40<00:00,  3.39s/it]\r\nFound\
    \ 1 unique fused mlp KN values.\r\nWarming up autotune cache ...\r\n  0% 0/12\
    \ [00:00<?, ?it/s]python3: /project/lib/Analysis/Allocation.cpp:42: std::pair<llvm::SmallVector<unsigned\
    \ int>, llvm::SmallVector<unsigned int>\r\n> mlir::triton::getCvtOrder(const mlir::Attribute&,\
    \ const mlir::Attribute&): Assertion `!(srcMmaLayout && dstMmaLayout) && \r\n\r\
    \n```\r\n\r\nThanks for your help!"
  created_at: 2023-05-30 09:22:29+00:00
  edited: false
  hidden: false
  id: 6475ce65975bc849732160eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-30T10:28:59.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh weird. </p>

          <p>I am currently quantising with the ''old'' CUDA fork. Specifically, the
          fork put up by oobabooga, here: <a rel="nofollow" href="https://github.com/oobabooga/GPTQ-for-LLaMa">https://github.com/oobabooga/GPTQ-for-LLaMa</a></p>

          <p>I use this old fork because it maximises the compatibility for the majority
          of people. I''ve found that if I use newer releases of GPTQ-for-LLaMa, it
          can cause various problems for users using the older versions.  Unfortunately
          there''s no version that is guaranteed to work for absolutely everyone,
          so I picked the version that would work for the most people.</p>

          <p>TBH I thought it would work fine with newer GPTQ-for-LLaMas, however
          qwopqwop does keep changing things in his version so obviously something
          has broken.</p>

          <p>To ensure compatibility with what I run, you can use either the old oobabooga
          fork, or the newer and better AutoGPTQ (<a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ">https://github.com/PanQiWei/AutoGPTQ</a>),
          which also works well.  AutoGPTQ is what I recommend to everyone who can
          use it, and I plan to start using that for my quantisations as soon as it''s
          ready for mass adoption. There''s a couple more things that need to be ready
          for that to happen, most notably pre-built binaries being available.  Which
          is coming soon.</p>

          <p>Let me know if I can provide any further help with that.</p>

          '
        raw: "Oh weird. \n\nI am currently quantising with the 'old' CUDA fork. Specifically,\
          \ the fork put up by oobabooga, here: https://github.com/oobabooga/GPTQ-for-LLaMa\n\
          \nI use this old fork because it maximises the compatibility for the majority\
          \ of people. I've found that if I use newer releases of GPTQ-for-LLaMa,\
          \ it can cause various problems for users using the older versions.  Unfortunately\
          \ there's no version that is guaranteed to work for absolutely everyone,\
          \ so I picked the version that would work for the most people.\n\nTBH I\
          \ thought it would work fine with newer GPTQ-for-LLaMas, however qwopqwop\
          \ does keep changing things in his version so obviously something has broken.\n\
          \nTo ensure compatibility with what I run, you can use either the old oobabooga\
          \ fork, or the newer and better AutoGPTQ (https://github.com/PanQiWei/AutoGPTQ),\
          \ which also works well.  AutoGPTQ is what I recommend to everyone who can\
          \ use it, and I plan to start using that for my quantisations as soon as\
          \ it's ready for mass adoption. There's a couple more things that need to\
          \ be ready for that to happen, most notably pre-built binaries being available.\
          \  Which is coming soon.\n\nLet me know if I can provide any further help\
          \ with that."
        updatedAt: '2023-05-30T10:28:59.280Z'
      numEdits: 0
      reactions: []
    id: 6475cfebc894b5c9cf733b54
    type: comment
  author: TheBloke
  content: "Oh weird. \n\nI am currently quantising with the 'old' CUDA fork. Specifically,\
    \ the fork put up by oobabooga, here: https://github.com/oobabooga/GPTQ-for-LLaMa\n\
    \nI use this old fork because it maximises the compatibility for the majority\
    \ of people. I've found that if I use newer releases of GPTQ-for-LLaMa, it can\
    \ cause various problems for users using the older versions.  Unfortunately there's\
    \ no version that is guaranteed to work for absolutely everyone, so I picked the\
    \ version that would work for the most people.\n\nTBH I thought it would work\
    \ fine with newer GPTQ-for-LLaMas, however qwopqwop does keep changing things\
    \ in his version so obviously something has broken.\n\nTo ensure compatibility\
    \ with what I run, you can use either the old oobabooga fork, or the newer and\
    \ better AutoGPTQ (https://github.com/PanQiWei/AutoGPTQ), which also works well.\
    \  AutoGPTQ is what I recommend to everyone who can use it, and I plan to start\
    \ using that for my quantisations as soon as it's ready for mass adoption. There's\
    \ a couple more things that need to be ready for that to happen, most notably\
    \ pre-built binaries being available.  Which is coming soon.\n\nLet me know if\
    \ I can provide any further help with that."
  created_at: 2023-05-30 09:28:59+00:00
  edited: false
  hidden: false
  id: 6475cfebc894b5c9cf733b54
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/Manticore-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: Unexpected mma -> mma layout conversion failed
