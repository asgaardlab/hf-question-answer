!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LeMoussel
conflicting_files: null
created_at: 2024-01-23 13:27:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
      fullname: LeMoussel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LeMoussel
      type: user
    createdAt: '2024-01-23T13:27:38.000Z'
    data:
      edited: true
      editors:
      - LeMoussel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8736135959625244
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
          fullname: LeMoussel
          isHf: false
          isPro: false
          name: LeMoussel
          type: user
        html: '<p>I would like to work on finetuning  this model.<br>It seems that
          we can do this with <a rel="nofollow" href="https://github.com/OpenAccess-AI-Collective/axolotl">axolotl</a></p>

          <p>Which <a rel="nofollow" href="https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples">axolotl
          configuration file</a> to adapt?<br>Or a second solution of using PEFT LoRA
          and bitsandbytes (for exemple: <a rel="nofollow" href="https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o">fine
          tune OPT-6.7b</a>)</p>

          <p>Any recommendations for me on how to do this?</p>

          '
        raw: "I would like to work on finetuning  this model.\nIt seems that we can\
          \ do this with [axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)\n\
          \nWhich [axolotl configuration file](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples)\
          \ to adapt? \nOr a second solution of using PEFT LoRA and bitsandbytes (for\
          \ exemple: [fine tune OPT-6.7b](https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o))\n\
          \nAny recommendations for me on how to do this?"
        updatedAt: '2024-01-23T15:33:42.287Z'
      numEdits: 2
      reactions: []
    id: 65afbeca3db2280ecea403b2
    type: comment
  author: LeMoussel
  content: "I would like to work on finetuning  this model.\nIt seems that we can\
    \ do this with [axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)\n\
    \nWhich [axolotl configuration file](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples)\
    \ to adapt? \nOr a second solution of using PEFT LoRA and bitsandbytes (for exemple:\
    \ [fine tune OPT-6.7b](https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o))\n\
    \nAny recommendations for me on how to do this?"
  created_at: 2024-01-23 13:27:38+00:00
  edited: true
  hidden: false
  id: 65afbeca3db2280ecea403b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
      fullname: LeMoussel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LeMoussel
      type: user
    createdAt: '2024-01-23T13:27:55.000Z'
    data:
      from: How to finetuning this model
      to: How to finetuning this model?
    id: 65afbedb22762a684edf5e17
    type: title-change
  author: LeMoussel
  created_at: 2024-01-23 13:27:55+00:00
  id: 65afbedb22762a684edf5e17
  new_title: How to finetuning this model?
  old_title: How to finetuning this model
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6446be9a15a27291ef8bea10/9_gywXgzL9Jk3MYLdX9RG.jpeg?w=200&h=200&f=face
      fullname: interstellarninja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: interstellarninja
      type: user
    createdAt: '2024-01-24T05:31:27.000Z'
    data:
      edited: false
      editors:
      - interstellarninja
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40637654066085815
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6446be9a15a27291ef8bea10/9_gywXgzL9Jk3MYLdX9RG.jpeg?w=200&h=200&f=face
          fullname: interstellarninja
          isHf: false
          isPro: false
          name: interstellarninja
          type: user
        html: "<p>hi LeMoussel,</p>\n<p>I just finished finetuning the model with\
          \ the following axolotl config:</p>\n<pre><code class=\"language-yml\"><span\
          \ class=\"hljs-attr\">base_model:</span> <span class=\"hljs-string\">stabilityai/stablelm-2-zephyr-1_6b</span>\n\
          <span class=\"hljs-attr\">base_model_config:</span> <span class=\"hljs-string\"\
          >stabilityai/stablelm-2-zephyr-1_6b</span>\n<span class=\"hljs-attr\">model_type:</span>\
          \ <span class=\"hljs-string\">StableLMEpochForCausalLM</span>\n<span class=\"\
          hljs-attr\">tokenizer_type:</span> <span class=\"hljs-string\">AutoTokenizer</span>\n\
          <span class=\"hljs-attr\">trust_remote_code:</span> <span class=\"hljs-literal\"\
          >true</span>\n\n<span class=\"hljs-attr\">load_in_8bit:</span> <span class=\"\
          hljs-literal\">false</span>\n<span class=\"hljs-attr\">load_in_4bit:</span>\
          \ <span class=\"hljs-literal\">true</span>\n<span class=\"hljs-attr\">strict:</span>\
          \ <span class=\"hljs-literal\">false</span>\n\n<span class=\"hljs-attr\"\
          >datasets:</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"\
          hljs-attr\">path:</span> <span class=\"hljs-string\">interstellarninja/tool-calls-multiturn</span>\n\
          \    <span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\"\
          >sharegpt.load_multirole</span>\n    <span class=\"hljs-attr\">conversation:</span>\
          \ <span class=\"hljs-string\">zephyr</span>\n\n<span class=\"hljs-attr\"\
          >val_set_size:</span> <span class=\"hljs-number\">0</span>\n<span class=\"\
          hljs-attr\">dataset_prepared_path:</span> <span class=\"hljs-string\">last_run_prepared</span>\n\
          <span class=\"hljs-attr\">output_dir:</span> <span class=\"hljs-string\"\
          >./stablelm-1_6b-tool-calling-1</span>\n\n<span class=\"hljs-attr\">sequence_len:</span>\
          \ <span class=\"hljs-number\">4096</span>\n<span class=\"hljs-attr\">sample_packing:</span>\
          \ <span class=\"hljs-literal\">false</span>\n<span class=\"hljs-attr\">eval_sample_packing:</span>\
          \ <span class=\"hljs-literal\">false</span>\n<span class=\"hljs-attr\">eval_batch_size:</span>\
          \ <span class=\"hljs-number\">1</span>\n\n<span class=\"hljs-attr\">adapter:</span>\
          \ <span class=\"hljs-string\">qlora</span>\n<span class=\"hljs-attr\">lora_r:</span>\
          \ <span class=\"hljs-number\">32</span>\n<span class=\"hljs-attr\">lora_alpha:</span>\
          \ <span class=\"hljs-number\">16</span>\n<span class=\"hljs-attr\">lora_dropout:</span>\
          \ <span class=\"hljs-number\">0.05</span>\n<span class=\"hljs-attr\">lora_target_linear:</span>\
          \ <span class=\"hljs-literal\">true</span>\n<span class=\"hljs-attr\">lora_on_cpu:</span>\
          \ <span class=\"hljs-literal\">true</span>\n\n<span class=\"hljs-attr\"\
          >lora_modules_to_save:</span>\n  <span class=\"hljs-bullet\">-</span> <span\
          \ class=\"hljs-string\">embed_tokens</span>\n  <span class=\"hljs-bullet\"\
          >-</span> <span class=\"hljs-string\">lm_head</span>\n\n<span class=\"hljs-attr\"\
          >wandb_project:</span> <span class=\"hljs-string\">tool-calling-multiturn-1_6b</span>\n\
          <span class=\"hljs-attr\">wandb_run_id:</span> <span class=\"hljs-string\"\
          >stablelm-1_6b-tool-calling-1</span>\n\n<span class=\"hljs-attr\">data_seed:</span>\
          \ <span class=\"hljs-number\">42</span>\n<span class=\"hljs-attr\">seed:</span>\
          \ <span class=\"hljs-number\">42</span>\n\n<span class=\"hljs-attr\">gradient_accumulation_steps:</span>\
          \ <span class=\"hljs-number\">1</span>\n<span class=\"hljs-attr\">micro_batch_size:</span>\
          \ <span class=\"hljs-number\">1</span>\n<span class=\"hljs-attr\">warmup_steps:</span>\
          \ <span class=\"hljs-number\">25</span>\n<span class=\"hljs-attr\">num_epochs:</span>\
          \ <span class=\"hljs-number\">3</span>\n<span class=\"hljs-attr\">optimizer:</span>\
          \ <span class=\"hljs-string\">adamw_bnb_8bit</span>\n<span class=\"hljs-attr\"\
          >learning_rate:</span> <span class=\"hljs-number\">0.00001</span>\n<span\
          \ class=\"hljs-attr\">lr_scheduler:</span> <span class=\"hljs-string\">cosine</span>\n\
          <span class=\"hljs-attr\">weight_decay:</span> <span class=\"hljs-number\"\
          >0.02</span>\n\n<span class=\"hljs-attr\">train_on_inputs:</span> <span\
          \ class=\"hljs-literal\">false</span>\n<span class=\"hljs-attr\">group_by_length:</span>\
          \ <span class=\"hljs-literal\">true</span>\n<span class=\"hljs-attr\">bf16:</span>\
          \ <span class=\"hljs-literal\">true</span>\n<span class=\"hljs-attr\">fp16:</span>\
          \ <span class=\"hljs-literal\">false</span>\n<span class=\"hljs-attr\">tf32:</span>\
          \ <span class=\"hljs-literal\">true</span>\n\n<span class=\"hljs-attr\"\
          >gradient_checkpointing:</span> <span class=\"hljs-literal\">true</span>\n\
          <span class=\"hljs-attr\">logging_steps:</span> <span class=\"hljs-number\"\
          >1</span>\n<span class=\"hljs-attr\">xformers_attention:</span> <span class=\"\
          hljs-literal\">false</span>\n<span class=\"hljs-attr\">flash_attention:</span>\
          \ <span class=\"hljs-literal\">false</span>\n\n<span class=\"hljs-attr\"\
          >save_strategy:</span> <span class=\"hljs-string\">epoch</span>\n<span class=\"\
          hljs-attr\">save_safetensors:</span> <span class=\"hljs-literal\">true</span>\n\
          <span class=\"hljs-attr\">resume_from_checkpoint:</span> <span class=\"\
          hljs-literal\">false</span>\n\n<span class=\"hljs-attr\">hub_model_id:</span>\
          \ <span class=\"hljs-string\">interstellarninja/stablelm-2-zephyr-1_6b-tool-caller</span>\n\
          </code></pre>\n"
        raw: "hi LeMoussel,\n\nI just finished finetuning the model with the following\
          \ axolotl config:\n\n```yml\nbase_model: stabilityai/stablelm-2-zephyr-1_6b\n\
          base_model_config: stabilityai/stablelm-2-zephyr-1_6b\nmodel_type: StableLMEpochForCausalLM\n\
          tokenizer_type: AutoTokenizer\ntrust_remote_code: true\n\nload_in_8bit:\
          \ false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n  - path: interstellarninja/tool-calls-multiturn\n\
          \    type: sharegpt.load_multirole\n    conversation: zephyr\n\nval_set_size:\
          \ 0\ndataset_prepared_path: last_run_prepared\noutput_dir: ./stablelm-1_6b-tool-calling-1\n\
          \nsequence_len: 4096\nsample_packing: false\neval_sample_packing: false\n\
          eval_batch_size: 1\n\nadapter: qlora\nlora_r: 32\nlora_alpha: 16\nlora_dropout:\
          \ 0.05\nlora_target_linear: true\nlora_on_cpu: true\n\nlora_modules_to_save:\n\
          \  - embed_tokens\n  - lm_head\n\nwandb_project: tool-calling-multiturn-1_6b\n\
          wandb_run_id: stablelm-1_6b-tool-calling-1\n\ndata_seed: 42\nseed: 42\n\n\
          gradient_accumulation_steps: 1\nmicro_batch_size: 1\nwarmup_steps: 25\n\
          num_epochs: 3\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.00001\nlr_scheduler:\
          \ cosine\nweight_decay: 0.02\n\ntrain_on_inputs: false\ngroup_by_length:\
          \ true\nbf16: true\nfp16: false\ntf32: true\n\ngradient_checkpointing: true\n\
          logging_steps: 1\nxformers_attention: false\nflash_attention: false\n\n\
          save_strategy: epoch\nsave_safetensors: true\nresume_from_checkpoint: false\n\
          \nhub_model_id: interstellarninja/stablelm-2-zephyr-1_6b-tool-caller\n\n\
          ```"
        updatedAt: '2024-01-24T05:31:27.248Z'
      numEdits: 0
      reactions: []
    id: 65b0a0af403a23a2fda1eece
    type: comment
  author: interstellarninja
  content: "hi LeMoussel,\n\nI just finished finetuning the model with the following\
    \ axolotl config:\n\n```yml\nbase_model: stabilityai/stablelm-2-zephyr-1_6b\n\
    base_model_config: stabilityai/stablelm-2-zephyr-1_6b\nmodel_type: StableLMEpochForCausalLM\n\
    tokenizer_type: AutoTokenizer\ntrust_remote_code: true\n\nload_in_8bit: false\n\
    load_in_4bit: true\nstrict: false\n\ndatasets:\n  - path: interstellarninja/tool-calls-multiturn\n\
    \    type: sharegpt.load_multirole\n    conversation: zephyr\n\nval_set_size:\
    \ 0\ndataset_prepared_path: last_run_prepared\noutput_dir: ./stablelm-1_6b-tool-calling-1\n\
    \nsequence_len: 4096\nsample_packing: false\neval_sample_packing: false\neval_batch_size:\
    \ 1\n\nadapter: qlora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear:\
    \ true\nlora_on_cpu: true\n\nlora_modules_to_save:\n  - embed_tokens\n  - lm_head\n\
    \nwandb_project: tool-calling-multiturn-1_6b\nwandb_run_id: stablelm-1_6b-tool-calling-1\n\
    \ndata_seed: 42\nseed: 42\n\ngradient_accumulation_steps: 1\nmicro_batch_size:\
    \ 1\nwarmup_steps: 25\nnum_epochs: 3\noptimizer: adamw_bnb_8bit\nlearning_rate:\
    \ 0.00001\nlr_scheduler: cosine\nweight_decay: 0.02\n\ntrain_on_inputs: false\n\
    group_by_length: true\nbf16: true\nfp16: false\ntf32: true\n\ngradient_checkpointing:\
    \ true\nlogging_steps: 1\nxformers_attention: false\nflash_attention: false\n\n\
    save_strategy: epoch\nsave_safetensors: true\nresume_from_checkpoint: false\n\n\
    hub_model_id: interstellarninja/stablelm-2-zephyr-1_6b-tool-caller\n\n```"
  created_at: 2024-01-24 05:31:27+00:00
  edited: false
  hidden: false
  id: 65b0a0af403a23a2fda1eece
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
      fullname: LeMoussel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LeMoussel
      type: user
    createdAt: '2024-01-24T06:41:55.000Z'
    data:
      edited: false
      editors:
      - LeMoussel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.751055896282196
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
          fullname: LeMoussel
          isHf: false
          isPro: false
          name: LeMoussel
          type: user
        html: '<p>Thank you for your help. Very interesting.<br>I don''t find your
          dataset <code>interstellarninja/tool-calls-multiturn</code> on HuggingFace.  Do
          you have an example dataset for finetuning this model?</p>

          '
        raw: 'Thank you for your help. Very interesting.

          I don''t find your dataset `interstellarninja/tool-calls-multiturn` on HuggingFace.  Do
          you have an example dataset for finetuning this model?'
        updatedAt: '2024-01-24T06:41:55.279Z'
      numEdits: 0
      reactions: []
    id: 65b0b133a0b4bf3b0ed493af
    type: comment
  author: LeMoussel
  content: 'Thank you for your help. Very interesting.

    I don''t find your dataset `interstellarninja/tool-calls-multiturn` on HuggingFace.  Do
    you have an example dataset for finetuning this model?'
  created_at: 2024-01-24 06:41:55+00:00
  edited: false
  hidden: false
  id: 65b0b133a0b4bf3b0ed493af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64da2a58c307ee5369b92d36/7xEgll8v5SxxcG_XF86tU.jpeg?w=200&h=200&f=face
      fullname: geronimo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: g-ronimo
      type: user
    createdAt: '2024-01-24T11:31:35.000Z'
    data:
      edited: false
      editors:
      - g-ronimo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4968700706958771
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64da2a58c307ee5369b92d36/7xEgll8v5SxxcG_XF86tU.jpeg?w=200&h=200&f=face
          fullname: geronimo
          isHf: false
          isPro: false
          name: g-ronimo
          type: user
        html: '<p>here''s my notebook for finetuning it, no trainer like axolotl though,
          just HF code<br><a rel="nofollow" href="https://github.com/geronimi73/TinyLlama-versus-StableLM2/blob/main/nb_finetune_StableLM2_OA2.ipynb">https://github.com/geronimi73/TinyLlama-versus-StableLM2/blob/main/nb_finetune_StableLM2_OA2.ipynb</a></p>

          '
        raw: 'here''s my notebook for finetuning it, no trainer like axolotl though,
          just HF code

          https://github.com/geronimi73/TinyLlama-versus-StableLM2/blob/main/nb_finetune_StableLM2_OA2.ipynb'
        updatedAt: '2024-01-24T11:31:35.023Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - davidgortega
    id: 65b0f517ccd520120231d917
    type: comment
  author: g-ronimo
  content: 'here''s my notebook for finetuning it, no trainer like axolotl though,
    just HF code

    https://github.com/geronimi73/TinyLlama-versus-StableLM2/blob/main/nb_finetune_StableLM2_OA2.ipynb'
  created_at: 2024-01-24 11:31:35+00:00
  edited: false
  hidden: false
  id: 65b0f517ccd520120231d917
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
      fullname: LeMoussel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LeMoussel
      type: user
    createdAt: '2024-01-24T12:47:00.000Z'
    data:
      edited: true
      editors:
      - LeMoussel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7523460984230042
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
          fullname: LeMoussel
          isHf: false
          isPro: false
          name: LeMoussel
          type: user
        html: '<p>Thank you so much !<br>You use <a href="https://huggingface.co/datasets/g-ronimo/oasst2_top1_en">g-ronimo/oasst2_top1_en</a>
          as dataset.<br>From what I understand the dataset must be in the form an
          list of array like this<br><code>[ { "content": "Some content user ....",
          "role": "user" }, { "content": "Some content assistant ...", "role": "assistant"
          } ]</code><br>Do you think it is necessary to have <code>content assistant</code>?
          Could this be empty?</p>

          <p>Rem: I want to create a dataset in French.</p>

          '
        raw: "Thank you so much ! \nYou use [g-ronimo/oasst2_top1_en](https://huggingface.co/datasets/g-ronimo/oasst2_top1_en)\
          \ as dataset.  \nFrom what I understand the dataset must be in the form\
          \ an list of array like this \n`[ { \"content\": \"Some content user ....\"\
          , \"role\": \"user\" }, { \"content\": \"Some content assistant ...\", \"\
          role\": \"assistant\" } ]`\nDo you think it is necessary to have `content\
          \ assistant`? Could this be empty?\n\nRem: I want to create a dataset in\
          \ French.\n"
        updatedAt: '2024-01-24T12:47:48.182Z'
      numEdits: 1
      reactions: []
    id: 65b106c49d4b4d7930be4d79
    type: comment
  author: LeMoussel
  content: "Thank you so much ! \nYou use [g-ronimo/oasst2_top1_en](https://huggingface.co/datasets/g-ronimo/oasst2_top1_en)\
    \ as dataset.  \nFrom what I understand the dataset must be in the form an list\
    \ of array like this \n`[ { \"content\": \"Some content user ....\", \"role\"\
    : \"user\" }, { \"content\": \"Some content assistant ...\", \"role\": \"assistant\"\
    \ } ]`\nDo you think it is necessary to have `content assistant`? Could this be\
    \ empty?\n\nRem: I want to create a dataset in French.\n"
  created_at: 2024-01-24 12:47:00+00:00
  edited: true
  hidden: false
  id: 65b106c49d4b4d7930be4d79
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64da2a58c307ee5369b92d36/7xEgll8v5SxxcG_XF86tU.jpeg?w=200&h=200&f=face
      fullname: geronimo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: g-ronimo
      type: user
    createdAt: '2024-01-24T20:24:21.000Z'
    data:
      edited: false
      editors:
      - g-ronimo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9686684608459473
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64da2a58c307ee5369b92d36/7xEgll8v5SxxcG_XF86tU.jpeg?w=200&h=200&f=face
          fullname: geronimo
          isHf: false
          isPro: false
          name: g-ronimo
          type: user
        html: '<blockquote>

          <p>Do you think it is necessary to have <code>content assistant</code>?
          Could this be empty?</p>

          </blockquote>

          <p>could you please rephrase, not sure what you mean</p>

          '
        raw: '> Do you think it is necessary to have `content assistant`? Could this
          be empty?


          could you please rephrase, not sure what you mean


          '
        updatedAt: '2024-01-24T20:24:21.473Z'
      numEdits: 0
      reactions: []
    id: 65b171f54f699ea31968d70a
    type: comment
  author: g-ronimo
  content: '> Do you think it is necessary to have `content assistant`? Could this
    be empty?


    could you please rephrase, not sure what you mean


    '
  created_at: 2024-01-24 20:24:21+00:00
  edited: false
  hidden: false
  id: 65b171f54f699ea31968d70a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
      fullname: LeMoussel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LeMoussel
      type: user
    createdAt: '2024-01-24T21:52:36.000Z'
    data:
      edited: true
      editors:
      - LeMoussel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2802358865737915
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63cb7b071b705cc951ea5b82/_fQ7Z7brwF7fXcoADvY88.jpeg?w=200&h=200&f=face
          fullname: LeMoussel
          isHf: false
          isPro: false
          name: LeMoussel
          type: user
        html: '<p>Does dataset may contain only  <code>Some content user ....</code><br>Eg:  </p>

          <pre><code>[ { "content": "Some content1 ....", "role": "user" }, { "content":
          "", "role": "assistant" } ]

          [ { "content": "Some content2 ....", "role": "user" }, { "content": "",
          "role": "assistant" } ]

          .....

          </code></pre>

          '
        raw: "Does dataset may contain only  `Some content user ....` \nEg:  \n```\n\
          [ { \"content\": \"Some content1 ....\", \"role\": \"user\" }, { \"content\"\
          : \"\", \"role\": \"assistant\" } ]\n[ { \"content\": \"Some content2 ....\"\
          , \"role\": \"user\" }, { \"content\": \"\", \"role\": \"assistant\" } ]\n\
          .....\n```"
        updatedAt: '2024-01-24T21:53:18.298Z'
      numEdits: 2
      reactions: []
    id: 65b186a4d84a1f119fdc8db6
    type: comment
  author: LeMoussel
  content: "Does dataset may contain only  `Some content user ....` \nEg:  \n```\n\
    [ { \"content\": \"Some content1 ....\", \"role\": \"user\" }, { \"content\":\
    \ \"\", \"role\": \"assistant\" } ]\n[ { \"content\": \"Some content2 ....\",\
    \ \"role\": \"user\" }, { \"content\": \"\", \"role\": \"assistant\" } ]\n.....\n\
    ```"
  created_at: 2024-01-24 21:52:36+00:00
  edited: true
  hidden: false
  id: 65b186a4d84a1f119fdc8db6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: stabilityai/stablelm-2-zephyr-1_6b
repo_type: model
status: open
target_branch: null
title: How to finetuning this model?
