!!python/object:huggingface_hub.community.DiscussionWithDetails
author: g-ronimo
conflicting_files: null
created_at: 2024-01-21 20:21:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64da2a58c307ee5369b92d36/7xEgll8v5SxxcG_XF86tU.jpeg?w=200&h=200&f=face
      fullname: geronimo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: g-ronimo
      type: user
    createdAt: '2024-01-21T20:21:48.000Z'
    data:
      edited: false
      editors:
      - g-ronimo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44076719880104065
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64da2a58c307ee5369b92d36/7xEgll8v5SxxcG_XF86tU.jpeg?w=200&h=200&f=face
          fullname: geronimo
          isHf: false
          isPro: false
          name: g-ronimo
          type: user
        html: "<p>the <code>eos_token</code> set in  <code>tokenizer_config.json</code>\
          \ is not respected when loading the tokenizer</p>\n<pre><code class=\"language-python\"\
          >tokenizer = AutoTokenizer.from_pretrained(\n    modelpath,\n    trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>,\n    use_fast=<span class=\"hljs-literal\"\
          >False</span>,\n) \ntokenizer.eos_token\n</code></pre>\n<p>output: <code>'&lt;|endoftext|&gt;'</code></p>\n\
          <p>tokenizer_config.json:</p>\n<pre><code class=\"language-json\"><span\
          \ class=\"hljs-punctuation\">{</span>\n  <span class=\"hljs-attr\">\"added_tokens_decoder\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\"\
          >{</span><span class=\"hljs-punctuation\">}</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"auto_map\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">{</span>\n\
          \    <span class=\"hljs-attr\">\"AutoTokenizer\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-punctuation\">[</span>\n      <span class=\"\
          hljs-string\">\"tokenization_arcade100k.Arcade100kTokenizer\"</span><span\
          \ class=\"hljs-punctuation\">,</span>\n      <span class=\"hljs-literal\"\
          ><span class=\"hljs-keyword\">null</span></span>\n    <span class=\"hljs-punctuation\"\
          >]</span>\n  <span class=\"hljs-punctuation\">}</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"chat_template\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-string\">\"{% if not add_generation_prompt\
          \ is defined %}{% set add_generation_prompt = false %}{% endif %}{% for\
          \ message in messages %}{{'&lt;|im_start|&gt;' + message['role'] + '\\n'\
          \ + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}{% endfor %}{% if add_generation_prompt\
          \ %}{{ '&lt;|im_start|&gt;assistant\\n' }}{% endif %}\"</span><span class=\"\
          hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"clean_up_tokenization_spaces\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-literal\"\
          ><span class=\"hljs-keyword\">true</span></span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"errors\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-string\">\"replace\"</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"model_max_length\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">2048</span><span\
          \ class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"pad_token\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\"\
          >\"&lt;|endoftext|&gt;\"</span><span class=\"hljs-punctuation\">,</span>\n\
          \  <span class=\"hljs-attr\">\"eos_token\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-string\">\"&lt;|im_end|&gt;\"</span><span class=\"\
          hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"tokenizer_class\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\"\
          >\"Arcade100kTokenizer\"</span>\n<span class=\"hljs-punctuation\">}</span>\n\
          </code></pre>\n<p>setting <code>eos_token</code> to something random errors\
          \ when loading the tokenizer, so the information is processed somewhere\
          \ but <code>tokenizer.eos_token</code> is always the same, no matter what\
          \ I set (as long as it exists in the voab)</p>\n<p>model is<code>stabilityai/stablelm-2-zephyr-1_6b</code>\
          \ finetuned  on ChatML conversations<br>thank you for this great model!\
          \ </p>\n"
        raw: "the ```eos_token``` set in  ```tokenizer_config.json``` is not respected\
          \ when loading the tokenizer\r\n\r\n```python\r\ntokenizer = AutoTokenizer.from_pretrained(\r\
          \n    modelpath,\r\n    trust_remote_code=True,\r\n    use_fast=False,\r\
          \n) \r\ntokenizer.eos_token\r\n```\r\noutput: ```'<|endoftext|>'```\r\n\r\
          \ntokenizer_config.json:\r\n```json\r\n{\r\n  \"added_tokens_decoder\":\
          \ {},\r\n  \"auto_map\": {\r\n    \"AutoTokenizer\": [\r\n      \"tokenization_arcade100k.Arcade100kTokenizer\"\
          ,\r\n      null\r\n    ]\r\n  },\r\n  \"chat_template\": \"{% if not add_generation_prompt\
          \ is defined %}{% set add_generation_prompt = false %}{% endif %}{% for\
          \ message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content']\
          \ + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\\
          n' }}{% endif %}\",\r\n  \"clean_up_tokenization_spaces\": true,\r\n  \"\
          errors\": \"replace\",\r\n  \"model_max_length\": 2048,\r\n  \"pad_token\"\
          : \"<|endoftext|>\",\r\n  \"eos_token\": \"<|im_end|>\",\r\n  \"tokenizer_class\"\
          : \"Arcade100kTokenizer\"\r\n}\r\n```\r\n\r\nsetting ```eos_token``` to\
          \ something random errors when loading the tokenizer, so the information\
          \ is processed somewhere but ```tokenizer.eos_token``` is always the same,\
          \ no matter what I set (as long as it exists in the voab)\r\n\r\nmodel is```stabilityai/stablelm-2-zephyr-1_6b```\
          \ finetuned  on ChatML conversations\r\nthank you for this great model!\
          \ \r\n\r\n"
        updatedAt: '2024-01-21T20:21:48.939Z'
      numEdits: 0
      reactions: []
    id: 65ad7cdce0ee7990a63c6ce2
    type: comment
  author: g-ronimo
  content: "the ```eos_token``` set in  ```tokenizer_config.json``` is not respected\
    \ when loading the tokenizer\r\n\r\n```python\r\ntokenizer = AutoTokenizer.from_pretrained(\r\
    \n    modelpath,\r\n    trust_remote_code=True,\r\n    use_fast=False,\r\n) \r\
    \ntokenizer.eos_token\r\n```\r\noutput: ```'<|endoftext|>'```\r\n\r\ntokenizer_config.json:\r\
    \n```json\r\n{\r\n  \"added_tokens_decoder\": {},\r\n  \"auto_map\": {\r\n   \
    \ \"AutoTokenizer\": [\r\n      \"tokenization_arcade100k.Arcade100kTokenizer\"\
    ,\r\n      null\r\n    ]\r\n  },\r\n  \"chat_template\": \"{% if not add_generation_prompt\
    \ is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message\
    \ in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content']\
    \ + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\\
    n' }}{% endif %}\",\r\n  \"clean_up_tokenization_spaces\": true,\r\n  \"errors\"\
    : \"replace\",\r\n  \"model_max_length\": 2048,\r\n  \"pad_token\": \"<|endoftext|>\"\
    ,\r\n  \"eos_token\": \"<|im_end|>\",\r\n  \"tokenizer_class\": \"Arcade100kTokenizer\"\
    \r\n}\r\n```\r\n\r\nsetting ```eos_token``` to something random errors when loading\
    \ the tokenizer, so the information is processed somewhere but ```tokenizer.eos_token```\
    \ is always the same, no matter what I set (as long as it exists in the voab)\r\
    \n\r\nmodel is```stabilityai/stablelm-2-zephyr-1_6b``` finetuned  on ChatML conversations\r\
    \nthank you for this great model! \r\n\r\n"
  created_at: 2024-01-21 20:21:48+00:00
  edited: false
  hidden: false
  id: 65ad7cdce0ee7990a63c6ce2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665261042648-61b2bf4f5b1f7cad1799cfbb.png?w=200&h=200&f=face
      fullname: Jonathan Tow
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jon-tow
      type: user
    createdAt: '2024-01-23T05:10:09.000Z'
    data:
      edited: false
      editors:
      - jon-tow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8465796709060669
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665261042648-61b2bf4f5b1f7cad1799cfbb.png?w=200&h=200&f=face
          fullname: Jonathan Tow
          isHf: false
          isPro: false
          name: jon-tow
          type: user
        html: "<p>Hi, <span data-props=\"{&quot;user&quot;:&quot;g-ronimo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/g-ronimo\"\
          >@<span class=\"underline\">g-ronimo</span></a></span>\n\n\t</span></span>!\
          \ Thanks for bringing this up! The <code>eos_token</code> was hardcoded\
          \ in the tokenizer constructor. It's been <a href=\"https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b/commit/589adbfdd913d96282d43411c87a996f1bc7b000\"\
          >updated</a> to allow for overridability. Let us know if you run into any\
          \ further issues \U0001F64F </p>\n"
        raw: "Hi, @g-ronimo! Thanks for bringing this up! The `eos_token` was hardcoded\
          \ in the tokenizer constructor. It's been [updated](https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b/commit/589adbfdd913d96282d43411c87a996f1bc7b000)\
          \ to allow for overridability. Let us know if you run into any further issues\
          \ \U0001F64F "
        updatedAt: '2024-01-23T05:10:09.404Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65af4a3148481b399a42b6da
    id: 65af4a3148481b399a42b6d2
    type: comment
  author: jon-tow
  content: "Hi, @g-ronimo! Thanks for bringing this up! The `eos_token` was hardcoded\
    \ in the tokenizer constructor. It's been [updated](https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b/commit/589adbfdd913d96282d43411c87a996f1bc7b000)\
    \ to allow for overridability. Let us know if you run into any further issues\
    \ \U0001F64F "
  created_at: 2024-01-23 05:10:09+00:00
  edited: false
  hidden: false
  id: 65af4a3148481b399a42b6d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665261042648-61b2bf4f5b1f7cad1799cfbb.png?w=200&h=200&f=face
      fullname: Jonathan Tow
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jon-tow
      type: user
    createdAt: '2024-01-23T05:10:09.000Z'
    data:
      status: closed
    id: 65af4a3148481b399a42b6da
    type: status-change
  author: jon-tow
  created_at: 2024-01-23 05:10:09+00:00
  id: 65af4a3148481b399a42b6da
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: stabilityai/stablelm-2-zephyr-1_6b
repo_type: model
status: closed
target_branch: null
title: 'tokenizer: setting eos_token in tokenizer_config.json not working'
