!!python/object:huggingface_hub.community.DiscussionWithDetails
author: interstellarninja
conflicting_files: null
created_at: 2024-01-24 05:22:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6446be9a15a27291ef8bea10/9_gywXgzL9Jk3MYLdX9RG.jpeg?w=200&h=200&f=face
      fullname: interstellarninja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: interstellarninja
      type: user
    createdAt: '2024-01-24T05:22:19.000Z'
    data:
      edited: false
      editors:
      - interstellarninja
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5924272537231445
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6446be9a15a27291ef8bea10/9_gywXgzL9Jk3MYLdX9RG.jpeg?w=200&h=200&f=face
          fullname: interstellarninja
          isHf: false
          isPro: false
          name: interstellarninja
          type: user
        html: "<p>I have trained a qlora with stablelm-2-zephyr-1_6b and I'm trying\
          \ to inference the merged model. I have  also downloaded the tokenization.arcade100k.py\
          \ into the merged folder but i still get the error with code below:</p>\n\
          <pre><code class=\"language-python\">       self.bnb_config = BitsAndBytesConfig(\n\
          \            load_in_4bit=<span class=\"hljs-literal\">True</span>,\n  \
          \          bnb_4bit_quant_type=<span class=\"hljs-string\">\"nf4\"</span>,\n\
          \            bnb_4bit_use_double_quant=<span class=\"hljs-literal\">True</span>,\n\
          \        )\n        self.model = AutoModelForCausalLM.from_pretrained(\n\
          \            model_path,\n            trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>,\n            return_dict=<span class=\"hljs-literal\">True</span>,\n\
          \            quantization_config=self.bnb_config,\n            torch_dtype=torch.bfloat16,\n\
          \            device_map=<span class=\"hljs-string\">\"auto\"</span>,\n \
          \       )\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n\
          \        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.padding_side\
          \ = <span class=\"hljs-string\">\"left\"</span>\n</code></pre>\n"
        raw: "I have trained a qlora with stablelm-2-zephyr-1_6b and I'm trying to\
          \ inference the merged model. I have  also downloaded the tokenization.arcade100k.py\
          \ into the merged folder but i still get the error with code below:\r\n\
          \ \r\n```python\r\n       self.bnb_config = BitsAndBytesConfig(\r\n    \
          \        load_in_4bit=True,\r\n            bnb_4bit_quant_type=\"nf4\",\r\
          \n            bnb_4bit_use_double_quant=True,\r\n        )\r\n        self.model\
          \ = AutoModelForCausalLM.from_pretrained(\r\n            model_path,\r\n\
          \            trust_remote_code=True,\r\n            return_dict=True,\r\n\
          \            quantization_config=self.bnb_config,\r\n            torch_dtype=torch.bfloat16,\r\
          \n            device_map=\"auto\",\r\n        )\r\n\r\n        self.tokenizer\
          \ = AutoTokenizer.from_pretrained(model_path)\r\n        self.tokenizer.pad_token\
          \ = self.tokenizer.eos_token\r\n        self.tokenizer.padding_side = \"\
          left\"\r\n```"
        updatedAt: '2024-01-24T05:22:19.115Z'
      numEdits: 0
      reactions: []
    id: 65b09e8bccd5201202097a6f
    type: comment
  author: interstellarninja
  content: "I have trained a qlora with stablelm-2-zephyr-1_6b and I'm trying to inference\
    \ the merged model. I have  also downloaded the tokenization.arcade100k.py into\
    \ the merged folder but i still get the error with code below:\r\n \r\n```python\r\
    \n       self.bnb_config = BitsAndBytesConfig(\r\n            load_in_4bit=True,\r\
    \n            bnb_4bit_quant_type=\"nf4\",\r\n            bnb_4bit_use_double_quant=True,\r\
    \n        )\r\n        self.model = AutoModelForCausalLM.from_pretrained(\r\n\
    \            model_path,\r\n            trust_remote_code=True,\r\n          \
    \  return_dict=True,\r\n            quantization_config=self.bnb_config,\r\n \
    \           torch_dtype=torch.bfloat16,\r\n            device_map=\"auto\",\r\n\
    \        )\r\n\r\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\r\
    \n        self.tokenizer.pad_token = self.tokenizer.eos_token\r\n        self.tokenizer.padding_side\
    \ = \"left\"\r\n```"
  created_at: 2024-01-24 05:22:19+00:00
  edited: false
  hidden: false
  id: 65b09e8bccd5201202097a6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64da2a58c307ee5369b92d36/7xEgll8v5SxxcG_XF86tU.jpeg?w=200&h=200&f=face
      fullname: geronimo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: g-ronimo
      type: user
    createdAt: '2024-01-24T05:38:22.000Z'
    data:
      edited: false
      editors:
      - g-ronimo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47089865803718567
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64da2a58c307ee5369b92d36/7xEgll8v5SxxcG_XF86tU.jpeg?w=200&h=200&f=face
          fullname: geronimo
          isHf: false
          isPro: false
          name: g-ronimo
          type: user
        html: '<p>try this</p>

          <pre><code>self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

          </code></pre>

          '
        raw: 'try this

          ```

          self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

          ```'
        updatedAt: '2024-01-24T05:38:22.568Z'
      numEdits: 0
      reactions: []
    id: 65b0a24ea4953b36be3e64e4
    type: comment
  author: g-ronimo
  content: 'try this

    ```

    self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

    ```'
  created_at: 2024-01-24 05:38:22+00:00
  edited: false
  hidden: false
  id: 65b0a24ea4953b36be3e64e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6446be9a15a27291ef8bea10/9_gywXgzL9Jk3MYLdX9RG.jpeg?w=200&h=200&f=face
      fullname: interstellarninja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: interstellarninja
      type: user
    createdAt: '2024-01-24T05:53:04.000Z'
    data:
      edited: false
      editors:
      - interstellarninja
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7605350017547607
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6446be9a15a27291ef8bea10/9_gywXgzL9Jk3MYLdX9RG.jpeg?w=200&h=200&f=face
          fullname: interstellarninja
          isHf: false
          isPro: false
          name: interstellarninja
          type: user
        html: '<p>thanks g-ronimo but i''m using the local merged qlora model:</p>

          <p>btw this worked importing Arcade100kTokenizer into inference code:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          tokenization_arcade100k <span class="hljs-keyword">import</span> Arcade100kTokenizer

          self.tokenizer = Arcade100kTokenizer.from_pretrained(model_path)

          </code></pre>

          '
        raw: 'thanks g-ronimo but i''m using the local merged qlora model:


          btw this worked importing Arcade100kTokenizer into inference code:

          ```python

          from tokenization_arcade100k import Arcade100kTokenizer

          self.tokenizer = Arcade100kTokenizer.from_pretrained(model_path)

          ```'
        updatedAt: '2024-01-24T05:53:04.682Z'
      numEdits: 0
      reactions: []
    id: 65b0a5c0fdf0890334538c42
    type: comment
  author: interstellarninja
  content: 'thanks g-ronimo but i''m using the local merged qlora model:


    btw this worked importing Arcade100kTokenizer into inference code:

    ```python

    from tokenization_arcade100k import Arcade100kTokenizer

    self.tokenizer = Arcade100kTokenizer.from_pretrained(model_path)

    ```'
  created_at: 2024-01-24 05:53:04+00:00
  edited: false
  hidden: false
  id: 65b0a5c0fdf0890334538c42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665261042648-61b2bf4f5b1f7cad1799cfbb.png?w=200&h=200&f=face
      fullname: Jonathan Tow
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jon-tow
      type: user
    createdAt: '2024-01-25T05:37:33.000Z'
    data:
      edited: false
      editors:
      - jon-tow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.570435643196106
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665261042648-61b2bf4f5b1f7cad1799cfbb.png?w=200&h=200&f=face
          fullname: Jonathan Tow
          isHf: false
          isPro: false
          name: jon-tow
          type: user
        html: "<p>Hi, <span data-props=\"{&quot;user&quot;:&quot;interstellarninja&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/interstellarninja\"\
          >@<span class=\"underline\">interstellarninja</span></a></span>\n\n\t</span></span>\
          \ \U0001F44B  You still need to pass <code>trust_remote_code=True</code>\
          \ to the <code>AutoTokenizer.from_pretrained</code> method even if files\
          \ are local because of the custom tokenizer implementation. See relevant\
          \ code <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/d02d006cf315cf91e3a470eb72b9a9a7d0ecaf90/src/transformers/models/auto/tokenization_auto.py#L781-L790\"\
          >here</a>.</p>\n"
        raw: "Hi, @interstellarninja \U0001F44B  You still need to pass `trust_remote_code=True`\
          \ to the `AutoTokenizer.from_pretrained` method even if files are local\
          \ because of the custom tokenizer implementation. See relevant code [here](https://github.com/huggingface/transformers/blob/d02d006cf315cf91e3a470eb72b9a9a7d0ecaf90/src/transformers/models/auto/tokenization_auto.py#L781-L790)."
        updatedAt: '2024-01-25T05:37:33.791Z'
      numEdits: 0
      reactions: []
    id: 65b1f39ded2107075669b850
    type: comment
  author: jon-tow
  content: "Hi, @interstellarninja \U0001F44B  You still need to pass `trust_remote_code=True`\
    \ to the `AutoTokenizer.from_pretrained` method even if files are local because\
    \ of the custom tokenizer implementation. See relevant code [here](https://github.com/huggingface/transformers/blob/d02d006cf315cf91e3a470eb72b9a9a7d0ecaf90/src/transformers/models/auto/tokenization_auto.py#L781-L790)."
  created_at: 2024-01-25 05:37:33+00:00
  edited: false
  hidden: false
  id: 65b1f39ded2107075669b850
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665261042648-61b2bf4f5b1f7cad1799cfbb.png?w=200&h=200&f=face
      fullname: Jonathan Tow
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jon-tow
      type: user
    createdAt: '2024-01-25T17:28:57.000Z'
    data:
      status: closed
    id: 65b29a59da72b2526d08d56c
    type: status-change
  author: jon-tow
  created_at: 2024-01-25 17:28:57+00:00
  id: 65b29a59da72b2526d08d56c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: stabilityai/stablelm-2-zephyr-1_6b
repo_type: model
status: closed
target_branch: null
title: 'ValueError: Tokenizer class Arcade100kTokenizer does not exist or is not currently
  imported.'
