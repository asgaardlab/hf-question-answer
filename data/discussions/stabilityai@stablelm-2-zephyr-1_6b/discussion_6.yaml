!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kpriyanshu256
conflicting_files: null
created_at: 2024-01-23 23:14:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671473188958-6055578ecc6700532e405a2c.png?w=200&h=200&f=face
      fullname: Priyanshu Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kpriyanshu256
      type: user
    createdAt: '2024-01-23T23:14:53.000Z'
    data:
      edited: true
      editors:
      - kpriyanshu256
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.535386323928833
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671473188958-6055578ecc6700532e405a2c.png?w=200&h=200&f=face
          fullname: Priyanshu Kumar
          isHf: false
          isPro: false
          name: kpriyanshu256
          type: user
        html: '<p>When used with vLLM, I get the following error</p>

          <p><code>AttributeError: ''Arcade100kTokenizer'' object has no attribute
          ''errors''</code></p>

          <p>I tried to modify the tokenizer source code, to set ''errors'' attribute.
          It leads to the following error</p>

          <p><code>File "/home/priyansk/rtp2-src/generations_batch.py", line 405,
          in &lt;module&gt;     main()</code><br><code> File "/home/priyansk/rtp2-src/generations_batch.py",
          line 356, in main     outputs = llm.generate(input_prompts_trimmed, sampling_params)</code><br>  <code>File
          "/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/llm.py",
          line 172, in generate     return self._run_engine(use_tqdm)</code><br><code>File
          "/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/llm.py",
          line 196, in _run_engine     step_outputs = self.llm_engine.step()</code><br><code>File
          "/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py",
          line 756, in step     return self._process_model_outputs(output, scheduler_outputs)</code><br><code>File
          "/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py",
          line 661, in _process_model_outputs     self._process_sequence_group_outputs(seq_group,
          outputs)</code><br><code>File "/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py",
          line 532, in _process_sequence_group_outputs     self._decode_sequence(seq,
          seq_group.sampling_params)</code><br> <code>File "/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py",
          line 837, in _decode_sequence     read_offset) = detokenize_incrementally(</code><br>
          <code>File "/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer.py",
          line 131, in detokenize_incrementally     new_tokens = tokenizer.convert_ids_to_tokens(</code><br>
          <code>File "/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/transformers/tokenization_utils.py",
          line 982, in convert_ids_to_tokens     tokens.append(self._convert_id_to_token(index))</code><br>  <code>File
          "/home/priyansk/.cache/huggingface/modules/transformers_modules/stabilityai/stablelm-2-zephyr-1_6b/8e5b1aa67b32d39c29c489497a8194a3270b55a4/tokenization_arcade100k.py",
          line 261, in _convert_id_to_token     raise ValueError(f"unknown ids {index}")</code><br><code>ValueError:
          unknown ids</code></p>

          '
        raw: "When used with vLLM, I get the following error\n\n`AttributeError: 'Arcade100kTokenizer'\
          \ object has no attribute 'errors'`\n\nI tried to modify the tokenizer source\
          \ code, to set 'errors' attribute. It leads to the following error\n\n`File\
          \ \"/home/priyansk/rtp2-src/generations_batch.py\", line 405, in <module>\n\
          \    main()`\n` File \"/home/priyansk/rtp2-src/generations_batch.py\", line\
          \ 356, in main\n    outputs = llm.generate(input_prompts_trimmed, sampling_params)`\n\
          \  `File \"/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/llm.py\"\
          , line 172, in generate\n    return self._run_engine(use_tqdm)`\n`File \"\
          /home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/llm.py\"\
          , line 196, in _run_engine\n    step_outputs = self.llm_engine.step()`\n\
          `File \"/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
          , line 756, in step\n    return self._process_model_outputs(output, scheduler_outputs)`\n\
          `File \"/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
          , line 661, in _process_model_outputs\n    self._process_sequence_group_outputs(seq_group,\
          \ outputs)`\n`File \"/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
          , line 532, in _process_sequence_group_outputs\n    self._decode_sequence(seq,\
          \ seq_group.sampling_params)`\n `File \"/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
          , line 837, in _decode_sequence\n    read_offset) = detokenize_incrementally(`\n\
          \ `File \"/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer.py\"\
          , line 131, in detokenize_incrementally\n    new_tokens = tokenizer.convert_ids_to_tokens(`\n\
          \ `File \"/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/transformers/tokenization_utils.py\"\
          , line 982, in convert_ids_to_tokens\n    tokens.append(self._convert_id_to_token(index))`\n\
          \  `File \"/home/priyansk/.cache/huggingface/modules/transformers_modules/stabilityai/stablelm-2-zephyr-1_6b/8e5b1aa67b32d39c29c489497a8194a3270b55a4/tokenization_arcade100k.py\"\
          , line 261, in _convert_id_to_token\n    raise ValueError(f\"unknown ids\
          \ {index}\")`\n`ValueError: unknown ids`\n"
        updatedAt: '2024-01-23T23:17:20.622Z'
      numEdits: 1
      reactions: []
    id: 65b0486da4953b36be1da4e4
    type: comment
  author: kpriyanshu256
  content: "When used with vLLM, I get the following error\n\n`AttributeError: 'Arcade100kTokenizer'\
    \ object has no attribute 'errors'`\n\nI tried to modify the tokenizer source\
    \ code, to set 'errors' attribute. It leads to the following error\n\n`File \"\
    /home/priyansk/rtp2-src/generations_batch.py\", line 405, in <module>\n    main()`\n\
    ` File \"/home/priyansk/rtp2-src/generations_batch.py\", line 356, in main\n \
    \   outputs = llm.generate(input_prompts_trimmed, sampling_params)`\n  `File \"\
    /home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/llm.py\"\
    , line 172, in generate\n    return self._run_engine(use_tqdm)`\n`File \"/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/llm.py\"\
    , line 196, in _run_engine\n    step_outputs = self.llm_engine.step()`\n`File\
    \ \"/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
    , line 756, in step\n    return self._process_model_outputs(output, scheduler_outputs)`\n\
    `File \"/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
    , line 661, in _process_model_outputs\n    self._process_sequence_group_outputs(seq_group,\
    \ outputs)`\n`File \"/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
    , line 532, in _process_sequence_group_outputs\n    self._decode_sequence(seq,\
    \ seq_group.sampling_params)`\n `File \"/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
    , line 837, in _decode_sequence\n    read_offset) = detokenize_incrementally(`\n\
    \ `File \"/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer.py\"\
    , line 131, in detokenize_incrementally\n    new_tokens = tokenizer.convert_ids_to_tokens(`\n\
    \ `File \"/home/priyansk/miniforge-pypy3/envs/vllm/lib/python3.10/site-packages/transformers/tokenization_utils.py\"\
    , line 982, in convert_ids_to_tokens\n    tokens.append(self._convert_id_to_token(index))`\n\
    \  `File \"/home/priyansk/.cache/huggingface/modules/transformers_modules/stabilityai/stablelm-2-zephyr-1_6b/8e5b1aa67b32d39c29c489497a8194a3270b55a4/tokenization_arcade100k.py\"\
    , line 261, in _convert_id_to_token\n    raise ValueError(f\"unknown ids {index}\"\
    )`\n`ValueError: unknown ids`\n"
  created_at: 2024-01-23 23:14:53+00:00
  edited: true
  hidden: false
  id: 65b0486da4953b36be1da4e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665261042648-61b2bf4f5b1f7cad1799cfbb.png?w=200&h=200&f=face
      fullname: Jonathan Tow
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jon-tow
      type: user
    createdAt: '2024-01-25T17:28:44.000Z'
    data:
      edited: false
      editors:
      - jon-tow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9255860447883606
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665261042648-61b2bf4f5b1f7cad1799cfbb.png?w=200&h=200&f=face
          fullname: Jonathan Tow
          isHf: false
          isPro: false
          name: jon-tow
          type: user
        html: "<p>Hi, <span data-props=\"{&quot;user&quot;:&quot;kpriyanshu256&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kpriyanshu256\"\
          >@<span class=\"underline\">kpriyanshu256</span></a></span>\n\n\t</span></span>!\
          \ Thanks for reporting this. I've exposed the <code>errors</code> field\
          \ in the tokenizer. If you'd like to try with VLLM support, you can give\
          \ <a rel=\"nofollow\" href=\"https://github.com/dakotamahan-stability/vllm/tree/support-stablelm-2\"\
          >this branch</a> a try until the <a rel=\"nofollow\" href=\"https://github.com/vllm-project/vllm/pull/2598\"\
          >PR</a> is merged.</p>\n"
        raw: Hi, @kpriyanshu256! Thanks for reporting this. I've exposed the `errors`
          field in the tokenizer. If you'd like to try with VLLM support, you can
          give [this branch](https://github.com/dakotamahan-stability/vllm/tree/support-stablelm-2)
          a try until the [PR](https://github.com/vllm-project/vllm/pull/2598) is
          merged.
        updatedAt: '2024-01-25T17:28:44.779Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - kpriyanshu256
      relatedEventId: 65b29a4c7934f3ef3366c855
    id: 65b29a4c7934f3ef3366c851
    type: comment
  author: jon-tow
  content: Hi, @kpriyanshu256! Thanks for reporting this. I've exposed the `errors`
    field in the tokenizer. If you'd like to try with VLLM support, you can give [this
    branch](https://github.com/dakotamahan-stability/vllm/tree/support-stablelm-2)
    a try until the [PR](https://github.com/vllm-project/vllm/pull/2598) is merged.
  created_at: 2024-01-25 17:28:44+00:00
  edited: false
  hidden: false
  id: 65b29a4c7934f3ef3366c851
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665261042648-61b2bf4f5b1f7cad1799cfbb.png?w=200&h=200&f=face
      fullname: Jonathan Tow
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jon-tow
      type: user
    createdAt: '2024-01-25T17:28:44.000Z'
    data:
      status: closed
    id: 65b29a4c7934f3ef3366c855
    type: status-change
  author: jon-tow
  created_at: 2024-01-25 17:28:44+00:00
  id: 65b29a4c7934f3ef3366c855
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: stabilityai/stablelm-2-zephyr-1_6b
repo_type: model
status: closed
target_branch: null
title: Issue with vLLM
