!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lzhtom
conflicting_files: null
created_at: 2023-12-08 15:26:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/471e1df967399fda42bfee54dc1da95c.svg
      fullname: Zhihua Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lzhtom
      type: user
    createdAt: '2023-12-08T15:26:27.000Z'
    data:
      edited: false
      editors:
      - lzhtom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9195823073387146
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/471e1df967399fda42bfee54dc1da95c.svg
          fullname: Zhihua Li
          isHf: false
          isPro: false
          name: lzhtom
          type: user
        html: '<p>I downloaded vicuna-13b-v1.5 and ran it locally. I noticed that
          the performance (accuracy) seems worse than when I tried vicuna-13b at <a
          rel="nofollow" href="https://chat.lmsys.org/">https://chat.lmsys.org/</a>.
          Maybe it''s due to generation parameter setting. At <a rel="nofollow" href="https://chat.lmsys.org/">https://chat.lmsys.org/</a>
          only three parameters were shown: temperature, top_p, and max_new_tokens.
          Is it possible to get a full list of parameters as used by <a rel="nofollow"
          href="https://chat.lmsys.org/">https://chat.lmsys.org/</a> so that the locally
          implemented version matches the performance? Thanks!</p>

          '
        raw: 'I downloaded vicuna-13b-v1.5 and ran it locally. I noticed that the
          performance (accuracy) seems worse than when I tried vicuna-13b at https://chat.lmsys.org/.
          Maybe it''s due to generation parameter setting. At https://chat.lmsys.org/
          only three parameters were shown: temperature, top_p, and max_new_tokens.
          Is it possible to get a full list of parameters as used by https://chat.lmsys.org/
          so that the locally implemented version matches the performance? Thanks!'
        updatedAt: '2023-12-08T15:26:27.265Z'
      numEdits: 0
      reactions: []
    id: 657335a3fc88f842e3a315ce
    type: comment
  author: lzhtom
  content: 'I downloaded vicuna-13b-v1.5 and ran it locally. I noticed that the performance
    (accuracy) seems worse than when I tried vicuna-13b at https://chat.lmsys.org/.
    Maybe it''s due to generation parameter setting. At https://chat.lmsys.org/ only
    three parameters were shown: temperature, top_p, and max_new_tokens. Is it possible
    to get a full list of parameters as used by https://chat.lmsys.org/ so that the
    locally implemented version matches the performance? Thanks!'
  created_at: 2023-12-08 15:26:27+00:00
  edited: false
  hidden: false
  id: 657335a3fc88f842e3a315ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec68d20dcedf4553bb31d9f6e0ded813.svg
      fullname: Wei-Lin Chiang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: weichiang
      type: user
    createdAt: '2023-12-08T18:37:38.000Z'
    data:
      edited: false
      editors:
      - weichiang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9111540913581848
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec68d20dcedf4553bb31d9f6e0ded813.svg
          fullname: Wei-Lin Chiang
          isHf: false
          isPro: false
          name: weichiang
          type: user
        html: '<p>how do you run vicuna-13b locally. use fastchat to make sure setting
          is correct.<br><a rel="nofollow" href="https://github.com/lm-sys/FastChat#model-weights">https://github.com/lm-sys/FastChat#model-weights</a></p>

          '
        raw: 'how do you run vicuna-13b locally. use fastchat to make sure setting
          is correct.

          https://github.com/lm-sys/FastChat#model-weights'
        updatedAt: '2023-12-08T18:37:38.009Z'
      numEdits: 0
      reactions: []
    id: 657362725df6f1fec1677e94
    type: comment
  author: weichiang
  content: 'how do you run vicuna-13b locally. use fastchat to make sure setting is
    correct.

    https://github.com/lm-sys/FastChat#model-weights'
  created_at: 2023-12-08 18:37:38+00:00
  edited: false
  hidden: false
  id: 657362725df6f1fec1677e94
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/471e1df967399fda42bfee54dc1da95c.svg
      fullname: Zhihua Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lzhtom
      type: user
    createdAt: '2023-12-13T01:06:03.000Z'
    data:
      edited: false
      editors:
      - lzhtom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9386299252510071
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/471e1df967399fda42bfee54dc1da95c.svg
          fullname: Zhihua Li
          isHf: false
          isPro: false
          name: lzhtom
          type: user
        html: '<p>I''m in a unique situation: one laptop with internet connection
          but no CUDA, and one workstation with CUDA but no internet. So what I did
          was first use fastchat in the laptop:<br>python -m fastchat.serve.cli --model-path
          lmsys/vicuna-13b-v1.5<br>This downloaded the weights and tried to initiate
          a chat, but then would stop and report an error of no CUDA.<br>Then I uploaded
          the downloaded weights to the workstation, and used hugging face transformer
          there to run the model.<br>There I did find that when using generate parameters
          temperature=0.7, top_p=1 (which amounts to not using top_p I think) did
          give better results than greedy search. However, it''s still worse than
          when I tried it at chat.lmsys.org.</p>

          '
        raw: 'I''m in a unique situation: one laptop with internet connection but
          no CUDA, and one workstation with CUDA but no internet. So what I did was
          first use fastchat in the laptop:

          python -m fastchat.serve.cli --model-path lmsys/vicuna-13b-v1.5

          This downloaded the weights and tried to initiate a chat, but then would
          stop and report an error of no CUDA.

          Then I uploaded the downloaded weights to the workstation, and used hugging
          face transformer there to run the model.

          There I did find that when using generate parameters temperature=0.7, top_p=1
          (which amounts to not using top_p I think) did give better results than
          greedy search. However, it''s still worse than when I tried it at chat.lmsys.org.'
        updatedAt: '2023-12-13T01:06:03.681Z'
      numEdits: 0
      reactions: []
    id: 6579037b2c8d6e12c40e8241
    type: comment
  author: lzhtom
  content: 'I''m in a unique situation: one laptop with internet connection but no
    CUDA, and one workstation with CUDA but no internet. So what I did was first use
    fastchat in the laptop:

    python -m fastchat.serve.cli --model-path lmsys/vicuna-13b-v1.5

    This downloaded the weights and tried to initiate a chat, but then would stop
    and report an error of no CUDA.

    Then I uploaded the downloaded weights to the workstation, and used hugging face
    transformer there to run the model.

    There I did find that when using generate parameters temperature=0.7, top_p=1
    (which amounts to not using top_p I think) did give better results than greedy
    search. However, it''s still worse than when I tried it at chat.lmsys.org.'
  created_at: 2023-12-13 01:06:03+00:00
  edited: false
  hidden: false
  id: 6579037b2c8d6e12c40e8241
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: lmsys/vicuna-13b-v1.5
repo_type: model
status: open
target_branch: null
title: Exact parameters to match https://chat.lmsys.org/
