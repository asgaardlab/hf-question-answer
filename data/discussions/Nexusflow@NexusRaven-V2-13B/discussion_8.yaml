!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nzaveri
conflicting_files: null
created_at: 2023-12-18 14:49:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/32459fcbb09f7940c061e14185639ec0.svg
      fullname: Naitik Zaveri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nzaveri
      type: user
    createdAt: '2023-12-18T14:49:01.000Z'
    data:
      edited: false
      editors:
      - nzaveri
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6181955933570862
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/32459fcbb09f7940c061e14185639ec0.svg
          fullname: Naitik Zaveri
          isHf: false
          isPro: false
          name: nzaveri
          type: user
        html: '<p>Hi,<br>Excellent work on function calling. However, how can I use  to
          save on inference speed and tokens?</p>

          <p>result = pipeline(prompt, max_new_tokens=2048, stop = "", return_full_text=False,
          do_sample=False, temperature=0.001)[0]["generated_text"]<br>print (result)</p>

          <p>Pipeline is:<br>pipeline = pipeline(<br>"text-generation",<br>model="Nexusflow/NexusRaven-V2-13B",<br>torch_dtype="auto",<br>device_map="auto",<br>)</p>

          <p>Error:<br>ValueError: The following model_kwargs are not used by the
          model: [''stop''] (note: typos in the generate arguments will also show
          up in this list)</p>

          '
        raw: "Hi,\r\nExcellent work on function calling. However, how can I use <bot_end>\
          \ to save on inference speed and tokens?\r\n\r\nresult = pipeline(prompt,\
          \ max_new_tokens=2048, stop = \"<bot_end>\", return_full_text=False, do_sample=False,\
          \ temperature=0.001)[0][\"generated_text\"]\r\nprint (result)\r\n\r\nPipeline\
          \ is:\r\npipeline = pipeline(\r\n\"text-generation\",\r\nmodel=\"Nexusflow/NexusRaven-V2-13B\"\
          ,\r\ntorch_dtype=\"auto\",\r\ndevice_map=\"auto\",\r\n)\r\n\r\nError:\r\n\
          ValueError: The following model_kwargs are not used by the model: ['stop']\
          \ (note: typos in the generate arguments will also show up in this list)"
        updatedAt: '2023-12-18T14:49:01.678Z'
      numEdits: 0
      reactions: []
    id: 65805bdde8c8d33e562bff16
    type: comment
  author: nzaveri
  content: "Hi,\r\nExcellent work on function calling. However, how can I use <bot_end>\
    \ to save on inference speed and tokens?\r\n\r\nresult = pipeline(prompt, max_new_tokens=2048,\
    \ stop = \"<bot_end>\", return_full_text=False, do_sample=False, temperature=0.001)[0][\"\
    generated_text\"]\r\nprint (result)\r\n\r\nPipeline is:\r\npipeline = pipeline(\r\
    \n\"text-generation\",\r\nmodel=\"Nexusflow/NexusRaven-V2-13B\",\r\ntorch_dtype=\"\
    auto\",\r\ndevice_map=\"auto\",\r\n)\r\n\r\nError:\r\nValueError: The following\
    \ model_kwargs are not used by the model: ['stop'] (note: typos in the generate\
    \ arguments will also show up in this list)"
  created_at: 2023-12-18 14:49:01+00:00
  edited: false
  hidden: false
  id: 65805bdde8c8d33e562bff16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d2aa2af71b8dd66f7633776ef8ebc212.svg
      fullname: Venkat Srinivasan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: venkat-srinivasan-nexusflow
      type: user
    createdAt: '2023-12-18T16:47:19.000Z'
    data:
      edited: true
      editors:
      - venkat-srinivasan-nexusflow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8241156339645386
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d2aa2af71b8dd66f7633776ef8ebc212.svg
          fullname: Venkat Srinivasan
          isHf: false
          isPro: false
          name: venkat-srinivasan-nexusflow
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;nzaveri&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nzaveri\">@<span class=\"\
          underline\">nzaveri</span></a></span>\n\n\t</span></span>!</p>\n<p>Thank\
          \ you for your interest in the model! There's a couple ways you can implement\
          \ this. The easiest is to just use TGI, as it accepts a stopping criteria\
          \ as one of the arguments in the <a rel=\"nofollow\" href=\"https://github.com/huggingface/text-generation-inference/tree/main\"\
          >payload</a>. You might be able to spin this up and just sent REST-like\
          \ requests to the endpoint with a stopping criteria in the parameter dict\
          \ in your payload. For text generation pipeline, I don't believe there's\
          \ an easy implementation for stopping criteria. You'll likely have to implement\
          \ a StoppingCriteriaList that gets a StoppingCriteria passed in (where you'll\
          \ specify \"&lt;bot_end&gt;\" in its tokenized form). Something like this:\
          \ <a href=\"https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b/commit/072102d1d3462d9b2e18d91f4d22e894d83e7ccf\"\
          >https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b/commit/072102d1d3462d9b2e18d91f4d22e894d83e7ccf</a></p>\n"
        raw: 'Hi @nzaveri!


          Thank you for your interest in the model! There''s a couple ways you can
          implement this. The easiest is to just use TGI, as it accepts a stopping
          criteria as one of the arguments in the [payload](https://github.com/huggingface/text-generation-inference/tree/main).
          You might be able to spin this up and just sent REST-like requests to the
          endpoint with a stopping criteria in the parameter dict in your payload.
          For text generation pipeline, I don''t believe there''s an easy implementation
          for stopping criteria. You''ll likely have to implement a StoppingCriteriaList
          that gets a StoppingCriteria passed in (where you''ll specify "\<bot\_end\>"
          in its tokenized form). Something like this: https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b/commit/072102d1d3462d9b2e18d91f4d22e894d83e7ccf'
        updatedAt: '2023-12-18T16:47:46.644Z'
      numEdits: 1
      reactions: []
    id: 65807797a760379049a9b4d4
    type: comment
  author: venkat-srinivasan-nexusflow
  content: 'Hi @nzaveri!


    Thank you for your interest in the model! There''s a couple ways you can implement
    this. The easiest is to just use TGI, as it accepts a stopping criteria as one
    of the arguments in the [payload](https://github.com/huggingface/text-generation-inference/tree/main).
    You might be able to spin this up and just sent REST-like requests to the endpoint
    with a stopping criteria in the parameter dict in your payload. For text generation
    pipeline, I don''t believe there''s an easy implementation for stopping criteria.
    You''ll likely have to implement a StoppingCriteriaList that gets a StoppingCriteria
    passed in (where you''ll specify "\<bot\_end\>" in its tokenized form). Something
    like this: https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b/commit/072102d1d3462d9b2e18d91f4d22e894d83e7ccf'
  created_at: 2023-12-18 16:47:19+00:00
  edited: true
  hidden: false
  id: 65807797a760379049a9b4d4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: Nexusflow/NexusRaven-V2-13B
repo_type: model
status: open
target_branch: null
title: use of <bot_end>
