!!python/object:huggingface_hub.community.DiscussionWithDetails
author: smcleod
conflicting_files: null
created_at: 2023-08-29 00:08:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630fff3f02ce39336c495fe9/CZmQtRB4eGVbRBYT3_IH3.png?w=200&h=200&f=face
      fullname: Sam McLeod
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smcleod
      type: user
    createdAt: '2023-08-29T01:08:36.000Z'
    data:
      edited: false
      editors:
      - smcleod
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9598814845085144
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630fff3f02ce39336c495fe9/CZmQtRB4eGVbRBYT3_IH3.png?w=200&h=200&f=face
          fullname: Sam McLeod
          isHf: false
          isPro: false
          name: smcleod
          type: user
        html: '<p>Is there any chance there will be slightly smaller version somewhere
          between 13B and 20B~ that''s likely to run on more common GPUs with 16GB
          of vRAM?</p>

          <p>A lot of the decent coding models coming out seem to be focused on folks
          with 24GB+ cards.</p>

          '
        raw: "Is there any chance there will be slightly smaller version somewhere\
          \ between 13B and 20B~ that's likely to run on more common GPUs with 16GB\
          \ of vRAM?\r\n\r\nA lot of the decent coding models coming out seem to be\
          \ focused on folks with 24GB+ cards."
        updatedAt: '2023-08-29T01:08:36.020Z'
      numEdits: 0
      reactions: []
    id: 64ed451425252dc61a9fa45a
    type: comment
  author: smcleod
  content: "Is there any chance there will be slightly smaller version somewhere between\
    \ 13B and 20B~ that's likely to run on more common GPUs with 16GB of vRAM?\r\n\
    \r\nA lot of the decent coding models coming out seem to be focused on folks with\
    \ 24GB+ cards."
  created_at: 2023-08-29 00:08:36+00:00
  edited: false
  hidden: false
  id: 64ed451425252dc61a9fa45a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ce66795de9e1e911710cc2/7BA_4Tp-yvSNYYU_fDKru.jpeg?w=200&h=200&f=face
      fullname: Corey Walker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: corey4005
      type: user
    createdAt: '2023-08-29T13:50:29.000Z'
    data:
      edited: false
      editors:
      - corey4005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9670330882072449
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ce66795de9e1e911710cc2/7BA_4Tp-yvSNYYU_fDKru.jpeg?w=200&h=200&f=face
          fullname: Corey Walker
          isHf: false
          isPro: false
          name: corey4005
          type: user
        html: '<p>How do you know that it is only for a 24 GB card? So if we use the
          code they generated to show how to use it, we have to have a certain set
          of specs? </p>

          '
        raw: 'How do you know that it is only for a 24 GB card? So if we use the code
          they generated to show how to use it, we have to have a certain set of specs? '
        updatedAt: '2023-08-29T13:50:29.265Z'
      numEdits: 0
      reactions: []
    id: 64edf7a55085b3a9f608e897
    type: comment
  author: corey4005
  content: 'How do you know that it is only for a 24 GB card? So if we use the code
    they generated to show how to use it, we have to have a certain set of specs? '
  created_at: 2023-08-29 12:50:29+00:00
  edited: false
  hidden: false
  id: 64edf7a55085b3a9f608e897
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630fff3f02ce39336c495fe9/CZmQtRB4eGVbRBYT3_IH3.png?w=200&h=200&f=face
      fullname: Sam McLeod
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smcleod
      type: user
    createdAt: '2023-08-29T13:59:03.000Z'
    data:
      edited: false
      editors:
      - smcleod
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9381606578826904
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630fff3f02ce39336c495fe9/CZmQtRB4eGVbRBYT3_IH3.png?w=200&h=200&f=face
          fullname: Sam McLeod
          isHf: false
          isPro: false
          name: smcleod
          type: user
        html: "<blockquote>\n<p>How do you know that it is only for a 24 GB card?\
          \ So if we use the code they generated to show how to use it, we have to\
          \ have a certain set of specs?</p>\n</blockquote>\n<p>Because a 34B model\
          \ won\u2019t fit on a 16GB GPU, quantised at 4bit it should however just\
          \ fit on a 24GB GPU.</p>\n"
        raw: "> How do you know that it is only for a 24 GB card? So if we use the\
          \ code they generated to show how to use it, we have to have a certain set\
          \ of specs?\n\nBecause a 34B model won\u2019t fit on a 16GB GPU, quantised\
          \ at 4bit it should however just fit on a 24GB GPU."
        updatedAt: '2023-08-29T13:59:03.699Z'
      numEdits: 0
      reactions: []
    id: 64edf9a75085b3a9f60946dc
    type: comment
  author: smcleod
  content: "> How do you know that it is only for a 24 GB card? So if we use the code\
    \ they generated to show how to use it, we have to have a certain set of specs?\n\
    \nBecause a 34B model won\u2019t fit on a 16GB GPU, quantised at 4bit it should\
    \ however just fit on a 24GB GPU."
  created_at: 2023-08-29 12:59:03+00:00
  edited: false
  hidden: false
  id: 64edf9a75085b3a9f60946dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ce66795de9e1e911710cc2/7BA_4Tp-yvSNYYU_fDKru.jpeg?w=200&h=200&f=face
      fullname: Corey Walker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: corey4005
      type: user
    createdAt: '2023-08-29T14:00:30.000Z'
    data:
      edited: true
      editors:
      - corey4005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9492247104644775
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ce66795de9e1e911710cc2/7BA_4Tp-yvSNYYU_fDKru.jpeg?w=200&h=200&f=face
          fullname: Corey Walker
          isHf: false
          isPro: false
          name: corey4005
          type: user
        html: "<blockquote>\n<blockquote>\n<p>How do you know that it is only for\
          \ a 24 GB card? So if we use the code they generated to show how to use\
          \ it, we have to have a certain set of specs?</p>\n</blockquote>\n<p>Because\
          \ a 34B model won\u2019t fit on a 16GB GPU, quantised at 4bit it should\
          \ however just fit on a 24GB GPU.</p>\n</blockquote>\n<p>Thanks for the\
          \ response. Is there anything I can read that will help me understand the\
          \ math better? In other words, how do you know what fits and what does not?\
          \ I appreciate any information you can pass along. I am assuming that when\
          \ I build my next PC, I need to get a GPU that will be able to handle these\
          \ models, like an RTX 4090?</p>\n"
        raw: "> > How do you know that it is only for a 24 GB card? So if we use the\
          \ code they generated to show how to use it, we have to have a certain set\
          \ of specs?\n> \n> Because a 34B model won\u2019t fit on a 16GB GPU, quantised\
          \ at 4bit it should however just fit on a 24GB GPU.\n\nThanks for the response.\
          \ Is there anything I can read that will help me understand the math better?\
          \ In other words, how do you know what fits and what does not? I appreciate\
          \ any information you can pass along. I am assuming that when I build my\
          \ next PC, I need to get a GPU that will be able to handle these models,\
          \ like an RTX 4090?"
        updatedAt: '2023-08-29T14:01:08.500Z'
      numEdits: 1
      reactions: []
    id: 64edf9fe0673a593b36004f5
    type: comment
  author: corey4005
  content: "> > How do you know that it is only for a 24 GB card? So if we use the\
    \ code they generated to show how to use it, we have to have a certain set of\
    \ specs?\n> \n> Because a 34B model won\u2019t fit on a 16GB GPU, quantised at\
    \ 4bit it should however just fit on a 24GB GPU.\n\nThanks for the response. Is\
    \ there anything I can read that will help me understand the math better? In other\
    \ words, how do you know what fits and what does not? I appreciate any information\
    \ you can pass along. I am assuming that when I build my next PC, I need to get\
    \ a GPU that will be able to handle these models, like an RTX 4090?"
  created_at: 2023-08-29 13:00:30+00:00
  edited: true
  hidden: false
  id: 64edf9fe0673a593b36004f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630fff3f02ce39336c495fe9/CZmQtRB4eGVbRBYT3_IH3.png?w=200&h=200&f=face
      fullname: Sam McLeod
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smcleod
      type: user
    createdAt: '2023-08-31T07:34:40.000Z'
    data:
      edited: false
      editors:
      - smcleod
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5549685955047607
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630fff3f02ce39336c495fe9/CZmQtRB4eGVbRBYT3_IH3.png?w=200&h=200&f=face
          fullname: Sam McLeod
          isHf: false
          isPro: false
          name: smcleod
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;corey4005&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/corey4005\">@<span class=\"\
          underline\">corey4005</span></a></span>\n\n\t</span></span>  - so I was\
          \ able to get the v2 (phind-codellama-34b-v2.Q4_K_M.gguf) of this model\
          \ running on my little Tesla P100 (16GB), but it's very slow (2.5-3tk/s).</p>\n\
          <p>Output generated in 40.89 seconds (2.69 tokens/s, 110 tokens, context\
          \ 454, seed 403749230)<br>MEM[|||||||||||||||||15.560Gi/16.000Gi]</p>\n\
          <p>V2 GGUF - <a href=\"https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v2-GGUF/blob/main/phind-codellama-34b-v2.Q4_K_M.gguf\"\
          >https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v2-GGUF/blob/main/phind-codellama-34b-v2.Q4_K_M.gguf</a></p>\n\
          <p>Settings:</p>\n<ul>\n<li>llamacpp_hf</li>\n<li>gpu layers 33</li>\n<li>tokens\
          \ 1024</li>\n<li>batch 512</li>\n</ul>\n"
        raw: '@corey4005  - so I was able to get the v2 (phind-codellama-34b-v2.Q4_K_M.gguf)
          of this model running on my little Tesla P100 (16GB), but it''s very slow
          (2.5-3tk/s).


          Output generated in 40.89 seconds (2.69 tokens/s, 110 tokens, context 454,
          seed 403749230)

          MEM[|||||||||||||||||15.560Gi/16.000Gi]


          V2 GGUF - https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v2-GGUF/blob/main/phind-codellama-34b-v2.Q4_K_M.gguf


          Settings:


          - llamacpp_hf

          - gpu layers 33

          - tokens 1024

          - batch 512

          '
        updatedAt: '2023-08-31T07:34:40.945Z'
      numEdits: 0
      reactions: []
    id: 64f042902a7a94b68c12335c
    type: comment
  author: smcleod
  content: '@corey4005  - so I was able to get the v2 (phind-codellama-34b-v2.Q4_K_M.gguf)
    of this model running on my little Tesla P100 (16GB), but it''s very slow (2.5-3tk/s).


    Output generated in 40.89 seconds (2.69 tokens/s, 110 tokens, context 454, seed
    403749230)

    MEM[|||||||||||||||||15.560Gi/16.000Gi]


    V2 GGUF - https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v2-GGUF/blob/main/phind-codellama-34b-v2.Q4_K_M.gguf


    Settings:


    - llamacpp_hf

    - gpu layers 33

    - tokens 1024

    - batch 512

    '
  created_at: 2023-08-31 06:34:40+00:00
  edited: false
  hidden: false
  id: 64f042902a7a94b68c12335c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-08-31T18:23:28.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5685786008834839
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>Install it with cublas. It insanely bumps up speed for gpu</p>

          '
        raw: Install it with cublas. It insanely bumps up speed for gpu
        updatedAt: '2023-08-31T18:23:28.826Z'
      numEdits: 0
      reactions: []
    id: 64f0daa0d43a09a257e3c39f
    type: comment
  author: YaTharThShaRma999
  content: Install it with cublas. It insanely bumps up speed for gpu
  created_at: 2023-08-31 17:23:28+00:00
  edited: false
  hidden: false
  id: 64f0daa0d43a09a257e3c39f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630fff3f02ce39336c495fe9/CZmQtRB4eGVbRBYT3_IH3.png?w=200&h=200&f=face
      fullname: Sam McLeod
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smcleod
      type: user
    createdAt: '2023-08-31T20:34:40.000Z'
    data:
      edited: false
      editors:
      - smcleod
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9507613778114319
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630fff3f02ce39336c495fe9/CZmQtRB4eGVbRBYT3_IH3.png?w=200&h=200&f=face
          fullname: Sam McLeod
          isHf: false
          isPro: false
          name: smcleod
          type: user
        html: '<p>Does that let you split between cpu and gpu memory though @johnwick123forevr
          ?</p>

          '
        raw: Does that let you split between cpu and gpu memory though @johnwick123forevr
          ?
        updatedAt: '2023-08-31T20:34:40.415Z'
      numEdits: 0
      reactions: []
    id: 64f0f96069d19679783e86f4
    type: comment
  author: smcleod
  content: Does that let you split between cpu and gpu memory though @johnwick123forevr
    ?
  created_at: 2023-08-31 19:34:40+00:00
  edited: false
  hidden: false
  id: 64f0f96069d19679783e86f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630fff3f02ce39336c495fe9/CZmQtRB4eGVbRBYT3_IH3.png?w=200&h=200&f=face
      fullname: Sam McLeod
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smcleod
      type: user
    createdAt: '2023-09-01T08:22:15.000Z'
    data:
      status: closed
    id: 64f19f370af832a73d0f1634
    type: status-change
  author: smcleod
  created_at: 2023-09-01 07:22:15+00:00
  id: 64f19f370af832a73d0f1634
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-09-01T15:48:39.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9168089032173157
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>yes, you still split between cpu and gpu memory. higher gpu layers=more
          gpu memory</p>

          '
        raw: yes, you still split between cpu and gpu memory. higher gpu layers=more
          gpu memory
        updatedAt: '2023-09-01T15:48:39.843Z'
      numEdits: 0
      reactions: []
    id: 64f207d78be22790b9c654b7
    type: comment
  author: YaTharThShaRma999
  content: yes, you still split between cpu and gpu memory. higher gpu layers=more
    gpu memory
  created_at: 2023-09-01 14:48:39+00:00
  edited: false
  hidden: false
  id: 64f207d78be22790b9c654b7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: Phind/Phind-CodeLlama-34B-v1
repo_type: model
status: closed
target_branch: null
title: Any chance of a 13B-20B version?
