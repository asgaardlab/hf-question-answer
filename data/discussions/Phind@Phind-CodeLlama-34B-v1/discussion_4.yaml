!!python/object:huggingface_hub.community.DiscussionWithDetails
author: olasbondolas
conflicting_files: null
created_at: 2023-08-26 10:40:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/kz56g4OOR6OmGvjebFc65.png?w=200&h=200&f=face
      fullname: Nicholas Erup Larsen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: olasbondolas
      type: user
    createdAt: '2023-08-26T11:40:15.000Z'
    data:
      edited: false
      editors:
      - olasbondolas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.38773053884506226
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/kz56g4OOR6OmGvjebFc65.png?w=200&h=200&f=face
          fullname: Nicholas Erup Larsen
          isHf: false
          isPro: false
          name: olasbondolas
          type: user
        html: "<p>I'm running the provided \"How to reproduce HumanEval Results\"\
          \ code:</p>\n<pre><code>from transformers import AutoTokenizer, LlamaForCausalLM\n\
          from human_eval.data import write_jsonl, read_problems\nfrom tqdm import\
          \ tqdm\n\n# initialize the model\n\nmodel_path = \"Phind/Phind-CodeLlama-34B-v1\"\
          \nmodel = LlamaForCausalLM.from_pretrained(model_path, device_map=\"auto\"\
          )\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# HumanEval\
          \ helper\n\ndef generate_one_completion(prompt: str):\n    tokenizer.pad_token\
          \ = tokenizer.eos_token\n    inputs = tokenizer(prompt, return_tensors=\"\
          pt\", truncation=True, max_length=4096)\n\n    # Generate\n    generate_ids\
          \ = model.generate(inputs.input_ids.to(\"cuda\"), max_new_tokens=256, do_sample=True,\
          \ top_p=0.75, top_k=40, temperature=0.1)\n    completion = tokenizer.batch_decode(generate_ids,\
          \ skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n   \
          \ completion = completion.replace(prompt, \"\").split(\"\\n\\n\\n\")[0]\n\
          \n    return completion\n\n# perform HumanEval\nproblems = read_problems()\n\
          \nnum_samples_per_task = 1\nsamples = [\n    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][\"\
          prompt\"]))\n    for task_id in tqdm(problems)\n    for _ in range(num_samples_per_task)\n\
          ]\nwrite_jsonl(\"samples.jsonl\", samples)\n\n# run `evaluate_functional_correctness\
          \ samples.jsonl` in your HumanEval code sandbox\n</code></pre>\n<p>but keep\
          \ getting the error:</p>\n<pre><code>PS C:\\Users\\nicho\\Desktop\\AI&gt;\
          \ &amp; C:/Users/nicho/AppData/Local/Programs/Python/Python311/python.exe\
          \ c:/Users/nicho/Desktop/AI/HumanEval.py\nTraceback (most recent call last):\n\
          \  File \"c:\\Users\\nicho\\Desktop\\AI\\HumanEval.py\", line 9, in &lt;module&gt;\n\
          \    model = LlamaForCausalLM.from_pretrained(model_path, device_map=\"\
          auto\")\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\nicho\\AppData\\Local\\Programs\\Python\\Python311\\\
          Lib\\site-packages\\transformers\\modeling_utils.py\", line 2862, in from_pretrained\n\
          \    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n\
          \                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\nicho\\AppData\\Local\\Programs\\Python\\Python311\\\
          Lib\\site-packages\\transformers\\utils\\hub.py\", line 1017, in get_checkpoint_shard_files\n\
          \    index = json.loads(f.read())\n            ^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \"C:\\Users\\nicho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\\
          json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nicho\\AppData\\\
          Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 337, in\
          \ decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n      \
          \         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nicho\\\
          AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line\
          \ 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s,\
          \ err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line\
          \ 1 column 1 (char 0)\n</code></pre>\n<p>Has anyone run into the same issue\
          \ and had success fixing it?</p>\n"
        raw: "I'm running the provided \"How to reproduce HumanEval Results\" code:\r\
          \n```\r\nfrom transformers import AutoTokenizer, LlamaForCausalLM\r\nfrom\
          \ human_eval.data import write_jsonl, read_problems\r\nfrom tqdm import\
          \ tqdm\r\n\r\n# initialize the model\r\n\r\nmodel_path = \"Phind/Phind-CodeLlama-34B-v1\"\
          \r\nmodel = LlamaForCausalLM.from_pretrained(model_path, device_map=\"auto\"\
          )\r\ntokenizer = AutoTokenizer.from_pretrained(model_path)\r\n\r\n# HumanEval\
          \ helper\r\n\r\ndef generate_one_completion(prompt: str):\r\n    tokenizer.pad_token\
          \ = tokenizer.eos_token\r\n    inputs = tokenizer(prompt, return_tensors=\"\
          pt\", truncation=True, max_length=4096)\r\n\r\n    # Generate\r\n    generate_ids\
          \ = model.generate(inputs.input_ids.to(\"cuda\"), max_new_tokens=256, do_sample=True,\
          \ top_p=0.75, top_k=40, temperature=0.1)\r\n    completion = tokenizer.batch_decode(generate_ids,\
          \ skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\r\n \
          \   completion = completion.replace(prompt, \"\").split(\"\\n\\n\\n\")[0]\r\
          \n\r\n    return completion\r\n\r\n# perform HumanEval\r\nproblems = read_problems()\r\
          \n\r\nnum_samples_per_task = 1\r\nsamples = [\r\n    dict(task_id=task_id,\
          \ completion=generate_one_completion(problems[task_id][\"prompt\"]))\r\n\
          \    for task_id in tqdm(problems)\r\n    for _ in range(num_samples_per_task)\r\
          \n]\r\nwrite_jsonl(\"samples.jsonl\", samples)\r\n\r\n# run `evaluate_functional_correctness\
          \ samples.jsonl` in your HumanEval code sandbox\r\n```\r\n\r\nbut keep getting\
          \ the error:\r\n```\r\nPS C:\\Users\\nicho\\Desktop\\AI> & C:/Users/nicho/AppData/Local/Programs/Python/Python311/python.exe\
          \ c:/Users/nicho/Desktop/AI/HumanEval.py\r\nTraceback (most recent call\
          \ last):\r\n  File \"c:\\Users\\nicho\\Desktop\\AI\\HumanEval.py\", line\
          \ 9, in <module>\r\n    model = LlamaForCausalLM.from_pretrained(model_path,\
          \ device_map=\"auto\")\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"C:\\Users\\nicho\\AppData\\Local\\Programs\\Python\\Python311\\\
          Lib\\site-packages\\transformers\\modeling_utils.py\", line 2862, in from_pretrained\r\
          \n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\r\
          \n                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"C:\\Users\\nicho\\AppData\\Local\\Programs\\Python\\Python311\\\
          Lib\\site-packages\\transformers\\utils\\hub.py\", line 1017, in get_checkpoint_shard_files\r\
          \n    index = json.loads(f.read())\r\n            ^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"C:\\Users\\nicho\\AppData\\Local\\Programs\\Python\\Python311\\\
          Lib\\json\\__init__.py\", line 346, in loads\r\n    return _default_decoder.decode(s)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicho\\AppData\\\
          Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 337, in\
          \ decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  \
          \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\\
          nicho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\"\
          , line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\"\
          , s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value:\
          \ line 1 column 1 (char 0)\r\n```\r\n\r\nHas anyone run into the same issue\
          \ and had success fixing it?"
        updatedAt: '2023-08-26T11:40:15.096Z'
      numEdits: 0
      reactions: []
    id: 64e9e49fde393e513c37e705
    type: comment
  author: olasbondolas
  content: "I'm running the provided \"How to reproduce HumanEval Results\" code:\r\
    \n```\r\nfrom transformers import AutoTokenizer, LlamaForCausalLM\r\nfrom human_eval.data\
    \ import write_jsonl, read_problems\r\nfrom tqdm import tqdm\r\n\r\n# initialize\
    \ the model\r\n\r\nmodel_path = \"Phind/Phind-CodeLlama-34B-v1\"\r\nmodel = LlamaForCausalLM.from_pretrained(model_path,\
    \ device_map=\"auto\")\r\ntokenizer = AutoTokenizer.from_pretrained(model_path)\r\
    \n\r\n# HumanEval helper\r\n\r\ndef generate_one_completion(prompt: str):\r\n\
    \    tokenizer.pad_token = tokenizer.eos_token\r\n    inputs = tokenizer(prompt,\
    \ return_tensors=\"pt\", truncation=True, max_length=4096)\r\n\r\n    # Generate\r\
    \n    generate_ids = model.generate(inputs.input_ids.to(\"cuda\"), max_new_tokens=256,\
    \ do_sample=True, top_p=0.75, top_k=40, temperature=0.1)\r\n    completion = tokenizer.batch_decode(generate_ids,\
    \ skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\r\n    completion\
    \ = completion.replace(prompt, \"\").split(\"\\n\\n\\n\")[0]\r\n\r\n    return\
    \ completion\r\n\r\n# perform HumanEval\r\nproblems = read_problems()\r\n\r\n\
    num_samples_per_task = 1\r\nsamples = [\r\n    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][\"\
    prompt\"]))\r\n    for task_id in tqdm(problems)\r\n    for _ in range(num_samples_per_task)\r\
    \n]\r\nwrite_jsonl(\"samples.jsonl\", samples)\r\n\r\n# run `evaluate_functional_correctness\
    \ samples.jsonl` in your HumanEval code sandbox\r\n```\r\n\r\nbut keep getting\
    \ the error:\r\n```\r\nPS C:\\Users\\nicho\\Desktop\\AI> & C:/Users/nicho/AppData/Local/Programs/Python/Python311/python.exe\
    \ c:/Users/nicho/Desktop/AI/HumanEval.py\r\nTraceback (most recent call last):\r\
    \n  File \"c:\\Users\\nicho\\Desktop\\AI\\HumanEval.py\", line 9, in <module>\r\
    \n    model = LlamaForCausalLM.from_pretrained(model_path, device_map=\"auto\"\
    )\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"C:\\Users\\nicho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\\
    site-packages\\transformers\\modeling_utils.py\", line 2862, in from_pretrained\r\
    \n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\r\n\
    \                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\
    \  File \"C:\\Users\\nicho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\\
    site-packages\\transformers\\utils\\hub.py\", line 1017, in get_checkpoint_shard_files\r\
    \n    index = json.loads(f.read())\r\n            ^^^^^^^^^^^^^^^^^^^^\r\n  File\
    \ \"C:\\Users\\nicho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\\
    __init__.py\", line 346, in loads\r\n    return _default_decoder.decode(s)\r\n\
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicho\\AppData\\\
    Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 337, in decode\r\
    \n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"C:\\Users\\nicho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\\
    json\\decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting\
    \ value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting\
    \ value: line 1 column 1 (char 0)\r\n```\r\n\r\nHas anyone run into the same issue\
    \ and had success fixing it?"
  created_at: 2023-08-26 10:40:15+00:00
  edited: false
  hidden: false
  id: 64e9e49fde393e513c37e705
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7df71c3d36aa738a09e0bb38e149d043.svg
      fullname: Yuping
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wyphw
      type: user
    createdAt: '2023-08-28T02:16:34.000Z'
    data:
      edited: false
      editors:
      - wyphw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9796492457389832
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7df71c3d36aa738a09e0bb38e149d043.svg
          fullname: Yuping
          isHf: false
          isPro: false
          name: wyphw
          type: user
        html: '<p>I''ve encountered this problem, too. Have you solved it now?</p>

          '
        raw: I've encountered this problem, too. Have you solved it now?
        updatedAt: '2023-08-28T02:16:34.963Z'
      numEdits: 0
      reactions: []
    id: 64ec0382171e6c9862daac73
    type: comment
  author: wyphw
  content: I've encountered this problem, too. Have you solved it now?
  created_at: 2023-08-28 01:16:34+00:00
  edited: false
  hidden: false
  id: 64ec0382171e6c9862daac73
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/kz56g4OOR6OmGvjebFc65.png?w=200&h=200&f=face
      fullname: Nicholas Erup Larsen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: olasbondolas
      type: user
    createdAt: '2023-08-30T14:23:28.000Z'
    data:
      edited: false
      editors:
      - olasbondolas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9097514748573303
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/kz56g4OOR6OmGvjebFc65.png?w=200&h=200&f=face
          fullname: Nicholas Erup Larsen
          isHf: false
          isPro: false
          name: olasbondolas
          type: user
        html: '<p>Yeah, the <code>model_path</code> has to be <code>model_path = "Phind/Phind-CodeLlama-34B-v1"</code>
          and not the copy relative path from VS Code or anything other than that</p>

          '
        raw: Yeah, the `model_path` has to be `model_path = "Phind/Phind-CodeLlama-34B-v1"`
          and not the copy relative path from VS Code or anything other than that
        updatedAt: '2023-08-30T14:23:28.085Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64ef50e00f5a7993222fe678
    id: 64ef50e00f5a7993222fe677
    type: comment
  author: olasbondolas
  content: Yeah, the `model_path` has to be `model_path = "Phind/Phind-CodeLlama-34B-v1"`
    and not the copy relative path from VS Code or anything other than that
  created_at: 2023-08-30 13:23:28+00:00
  edited: false
  hidden: false
  id: 64ef50e00f5a7993222fe677
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/kz56g4OOR6OmGvjebFc65.png?w=200&h=200&f=face
      fullname: Nicholas Erup Larsen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: olasbondolas
      type: user
    createdAt: '2023-08-30T14:23:28.000Z'
    data:
      status: closed
    id: 64ef50e00f5a7993222fe678
    type: status-change
  author: olasbondolas
  created_at: 2023-08-30 13:23:28+00:00
  id: 64ef50e00f5a7993222fe678
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Phind/Phind-CodeLlama-34B-v1
repo_type: model
status: closed
target_branch: null
title: Is anyone else getting a JSON loading error?
