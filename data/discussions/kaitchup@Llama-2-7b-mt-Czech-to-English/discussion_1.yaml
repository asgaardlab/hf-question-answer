!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Moneymaker2023
conflicting_files: null
created_at: 2023-11-18 17:33:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
      fullname: Catalin Ciocea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moneymaker2023
      type: user
    createdAt: '2023-11-18T17:33:54.000Z'
    data:
      edited: false
      editors:
      - Moneymaker2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8981239795684814
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
          fullname: Catalin Ciocea
          isHf: false
          isPro: false
          name: Moneymaker2023
          type: user
        html: '<p>Hello,<br>can you please make an English to Romanian translation
          model finetune ? Can you please use this model: <a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-beta">https://huggingface.co/HuggingFaceH4/zephyr-7b-beta</a>   as
          base because it was already trained on multilingual dataset for 100 languages
          ? It make intelligible still imperfect translation from English to Romanian
          . It is important for me and any romanians because there exist many books
          in English wich worth to be translated and an automatic translation mechanism
          will have huge impact for allowing access to high value education for large
          number of peoples. I do not know if this dataset is good for translation
          training in romanian <a href="https://huggingface.co/datasets/allenai/MADLAD-400/tree/main/data/ro">https://huggingface.co/datasets/allenai/MADLAD-400/tree/main/data/ro</a>
          . Thank you very much !<br>Catalin</p>

          '
        raw: "Hello,\r\ncan you please make an English to Romanian translation model\
          \ finetune ? Can you please use this model: https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\
          \   as base because it was already trained on multilingual dataset for 100\
          \ languages ? It make intelligible still imperfect translation from English\
          \ to Romanian . It is important for me and any romanians because there exist\
          \ many books in English wich worth to be translated and an automatic translation\
          \ mechanism will have huge impact for allowing access to high value education\
          \ for large number of peoples. I do not know if this dataset is good for\
          \ translation training in romanian https://huggingface.co/datasets/allenai/MADLAD-400/tree/main/data/ro\
          \ . Thank you very much !\r\nCatalin"
        updatedAt: '2023-11-18T17:33:54.998Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - afrideva
    id: 6558f58257f3e9ac0790d426
    type: comment
  author: Moneymaker2023
  content: "Hello,\r\ncan you please make an English to Romanian translation model\
    \ finetune ? Can you please use this model: https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\
    \   as base because it was already trained on multilingual dataset for 100 languages\
    \ ? It make intelligible still imperfect translation from English to Romanian\
    \ . It is important for me and any romanians because there exist many books in\
    \ English wich worth to be translated and an automatic translation mechanism will\
    \ have huge impact for allowing access to high value education for large number\
    \ of peoples. I do not know if this dataset is good for translation training in\
    \ romanian https://huggingface.co/datasets/allenai/MADLAD-400/tree/main/data/ro\
    \ . Thank you very much !\r\nCatalin"
  created_at: 2023-11-18 17:33:54+00:00
  edited: false
  hidden: false
  id: 6558f58257f3e9ac0790d426
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dfcd9d8a01634ad22e9140607a696938.svg
      fullname: Benjamin Marie
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bnjmnmarie
      type: user
    createdAt: '2023-11-20T10:34:27.000Z'
    data:
      edited: false
      editors:
      - bnjmnmarie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9772768616676331
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dfcd9d8a01634ad22e9140607a696938.svg
          fullname: Benjamin Marie
          isHf: false
          isPro: false
          name: bnjmnmarie
          type: user
        html: '<p>Hi, </p>

          <p>Yes, I have a plan to fine-tune Mistral 7B for translation but I can
          only confirm that I will fine-tune for translation into English. For the
          translation into other languages such as Romanian, I have to confirm first
          that the translation quality is not too bad.<br>Note that I don''t know
          when I will release these models (maybe in December).</p>

          '
        raw: "Hi, \n\nYes, I have a plan to fine-tune Mistral 7B for translation but\
          \ I can only confirm that I will fine-tune for translation into English.\
          \ For the translation into other languages such as Romanian, I have to confirm\
          \ first that the translation quality is not too bad.\nNote that I don't\
          \ know when I will release these models (maybe in December)."
        updatedAt: '2023-11-20T10:34:27.005Z'
      numEdits: 0
      reactions: []
    id: 655b3633eb411317eb449598
    type: comment
  author: bnjmnmarie
  content: "Hi, \n\nYes, I have a plan to fine-tune Mistral 7B for translation but\
    \ I can only confirm that I will fine-tune for translation into English. For the\
    \ translation into other languages such as Romanian, I have to confirm first that\
    \ the translation quality is not too bad.\nNote that I don't know when I will\
    \ release these models (maybe in December)."
  created_at: 2023-11-20 10:34:27+00:00
  edited: false
  hidden: false
  id: 655b3633eb411317eb449598
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
      fullname: Catalin Ciocea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moneymaker2023
      type: user
    createdAt: '2023-11-21T12:14:09.000Z'
    data:
      edited: false
      editors:
      - Moneymaker2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8280325531959534
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
          fullname: Catalin Ciocea
          isHf: false
          isPro: false
          name: Moneymaker2023
          type: user
        html: '<p>Hello,<br>thank you for answer. On which dataset do you finetune
          a model for translation into a language ? For how long to achieve a good
          result ? How much do you estimate can cost such a finetuning of a mistral
          or zephyr base model into a bilingual translation model.<br>Thank you,<br>Catalin</p>

          '
        raw: 'Hello,

          thank you for answer. On which dataset do you finetune a model for translation
          into a language ? For how long to achieve a good result ? How much do you
          estimate can cost such a finetuning of a mistral or zephyr base model into
          a bilingual translation model.

          Thank you,

          Catalin'
        updatedAt: '2023-11-21T12:14:09.811Z'
      numEdits: 0
      reactions: []
    id: 655c9f11fb8645bdb25095ef
    type: comment
  author: Moneymaker2023
  content: 'Hello,

    thank you for answer. On which dataset do you finetune a model for translation
    into a language ? For how long to achieve a good result ? How much do you estimate
    can cost such a finetuning of a mistral or zephyr base model into a bilingual
    translation model.

    Thank you,

    Catalin'
  created_at: 2023-11-21 12:14:09+00:00
  edited: false
  hidden: false
  id: 655c9f11fb8645bdb25095ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dfcd9d8a01634ad22e9140607a696938.svg
      fullname: Benjamin Marie
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bnjmnmarie
      type: user
    createdAt: '2023-11-21T12:55:38.000Z'
    data:
      edited: false
      editors:
      - bnjmnmarie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8371871113777161
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dfcd9d8a01634ad22e9140607a696938.svg
          fullname: Benjamin Marie
          isHf: false
          isPro: false
          name: bnjmnmarie
          type: user
        html: '<p>I wrote an article explaining the entire process here:<br><a rel="nofollow"
          href="https://kaitchup.substack.com/p/llama-2-mt-turn-llama-2-into-a-translation">https://kaitchup.substack.com/p/llama-2-mt-turn-llama-2-into-a-translation</a></p>

          '
        raw: 'I wrote an article explaining the entire process here:

          https://kaitchup.substack.com/p/llama-2-mt-turn-llama-2-into-a-translation'
        updatedAt: '2023-11-21T12:55:38.401Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - afrideva
    id: 655ca8caaa23904bd102f60a
    type: comment
  author: bnjmnmarie
  content: 'I wrote an article explaining the entire process here:

    https://kaitchup.substack.com/p/llama-2-mt-turn-llama-2-into-a-translation'
  created_at: 2023-11-21 12:55:38+00:00
  edited: false
  hidden: false
  id: 655ca8caaa23904bd102f60a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
      fullname: Catalin Ciocea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moneymaker2023
      type: user
    createdAt: '2023-11-21T16:01:32.000Z'
    data:
      edited: false
      editors:
      - Moneymaker2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9284293055534363
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
          fullname: Catalin Ciocea
          isHf: false
          isPro: false
          name: Moneymaker2023
          type: user
        html: "<p>\"Moreover, since Llama 2 is pre-trained to generate English, I\
          \ only considered fine-tuning Llama 2 to translate into English. In preliminary\
          \ experiments, I tried some reverse directions, for instance, English to\
          \ French, but I judged that the translation quality I obtained wasn\u2019\
          t good enough.\"  - to overcome this you can try to use as base model for\
          \ finetuning this one HuggingFaceH4/zephyr-7b-beta . Using zephyr it translate\
          \ english text into romanian intelligible, but it invent some words ...\
          \ something like what children does sometime...so I guess he have a limited\
          \ vocabulary and it need more finetune on the translation task . I observed\
          \ that Zephyr performs much better than Llama or mistral on translation\
          \ tasks .</p>\n"
        raw: "\"Moreover, since Llama 2 is pre-trained to generate English, I only\
          \ considered fine-tuning Llama 2 to translate into English. In preliminary\
          \ experiments, I tried some reverse directions, for instance, English to\
          \ French, but I judged that the translation quality I obtained wasn\u2019\
          t good enough.\"  - to overcome this you can try to use as base model for\
          \ finetuning this one HuggingFaceH4/zephyr-7b-beta . Using zephyr it translate\
          \ english text into romanian intelligible, but it invent some words ...\
          \ something like what children does sometime...so I guess he have a limited\
          \ vocabulary and it need more finetune on the translation task . I observed\
          \ that Zephyr performs much better than Llama or mistral on translation\
          \ tasks ."
        updatedAt: '2023-11-21T16:01:32.237Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - afrideva
    id: 655cd45cdcbcead18aa25101
    type: comment
  author: Moneymaker2023
  content: "\"Moreover, since Llama 2 is pre-trained to generate English, I only considered\
    \ fine-tuning Llama 2 to translate into English. In preliminary experiments, I\
    \ tried some reverse directions, for instance, English to French, but I judged\
    \ that the translation quality I obtained wasn\u2019t good enough.\"  - to overcome\
    \ this you can try to use as base model for finetuning this one HuggingFaceH4/zephyr-7b-beta\
    \ . Using zephyr it translate english text into romanian intelligible, but it\
    \ invent some words ... something like what children does sometime...so I guess\
    \ he have a limited vocabulary and it need more finetune on the translation task\
    \ . I observed that Zephyr performs much better than Llama or mistral on translation\
    \ tasks ."
  created_at: 2023-11-21 16:01:32+00:00
  edited: false
  hidden: false
  id: 655cd45cdcbcead18aa25101
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
      fullname: Catalin Ciocea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moneymaker2023
      type: user
    createdAt: '2023-12-08T22:26:57.000Z'
    data:
      edited: false
      editors:
      - Moneymaker2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.869322657585144
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
          fullname: Catalin Ciocea
          isHf: false
          isPro: false
          name: Moneymaker2023
          type: user
        html: '<p>Hello, any news about training translation fine tuning with mistral
          or zephyr (zephyr I think is the best as it is showing good multilanguage
          skills natively , yet it needs to be polished )? In case you do not know
          yet I want to highlight for your attention a project that promise a substantial
          acceleration of fine tuning llama 2 models  2x-5x free version and up to
          30x premium version at a future unspecified yet price . It also promise
          that it will reduce the amount of video memory needed with 40-60%. Here
          is their sales pitch:</p>

          <p> "2-5x faster 50% less memory local LLM finetuning<br>    Manual autograd
          engine - hand derived backprop steps.<br>    2x to 5x faster than QLoRA.
          50% less memory usage.<br>    All kernels written in OpenAI''s Triton language.<br>    0%
          loss in accuracy - no approximation methods - all exact.<br>    No change
          of hardware necessary. Supports NVIDIA GPUs since 2018+. Minimum CUDA Compute
          Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc)
          Check your GPU<br>    Flash Attention v2 support via Xformers.<br>    NEW!
          Works on Linux and Windows via WSL.<br>    NEW! Experimental support for
          DPO (Direct Preference Optimization)!<br>    Supports 4bit and 16bit QLoRA
          / LoRA finetuning via bitsandbytes.<br>    Train Slim Orca fully locally
          in 260 hours from 1301 hours (5x faster)." </p>

          <ul>

          <li>here is their project : <a rel="nofollow" href="https://github.com/unslothai/unsloth">https://github.com/unslothai/unsloth</a>   .</li>

          </ul>

          '
        raw: "Hello, any news about training translation fine tuning with mistral\
          \ or zephyr (zephyr I think is the best as it is showing good multilanguage\
          \ skills natively , yet it needs to be polished )? In case you do not know\
          \ yet I want to highlight for your attention a project that promise a substantial\
          \ acceleration of fine tuning llama 2 models  2x-5x free version and up\
          \ to 30x premium version at a future unspecified yet price . It also promise\
          \ that it will reduce the amount of video memory needed with 40-60%. Here\
          \ is their sales pitch:\n\n \"2-5x faster 50% less memory local LLM finetuning\n\
          \    Manual autograd engine - hand derived backprop steps.\n    2x to 5x\
          \ faster than QLoRA. 50% less memory usage.\n    All kernels written in\
          \ OpenAI's Triton language.\n    0% loss in accuracy - no approximation\
          \ methods - all exact.\n    No change of hardware necessary. Supports NVIDIA\
          \ GPUs since 2018+. Minimum CUDA Compute Capability 7.0 (V100, T4, Titan\
          \ V, RTX 20, 30, 40x, A100, H100, L40 etc) Check your GPU\n    Flash Attention\
          \ v2 support via Xformers.\n    NEW! Works on Linux and Windows via WSL.\n\
          \    NEW! Experimental support for DPO (Direct Preference Optimization)!\n\
          \    Supports 4bit and 16bit QLoRA / LoRA finetuning via bitsandbytes.\n\
          \    Train Slim Orca fully locally in 260 hours from 1301 hours (5x faster).\"\
          \ \n\n  - here is their project : https://github.com/unslothai/unsloth \
          \  . "
        updatedAt: '2023-12-08T22:26:57.991Z'
      numEdits: 0
      reactions: []
    id: 6573983114881863157a5a93
    type: comment
  author: Moneymaker2023
  content: "Hello, any news about training translation fine tuning with mistral or\
    \ zephyr (zephyr I think is the best as it is showing good multilanguage skills\
    \ natively , yet it needs to be polished )? In case you do not know yet I want\
    \ to highlight for your attention a project that promise a substantial acceleration\
    \ of fine tuning llama 2 models  2x-5x free version and up to 30x premium version\
    \ at a future unspecified yet price . It also promise that it will reduce the\
    \ amount of video memory needed with 40-60%. Here is their sales pitch:\n\n \"\
    2-5x faster 50% less memory local LLM finetuning\n    Manual autograd engine -\
    \ hand derived backprop steps.\n    2x to 5x faster than QLoRA. 50% less memory\
    \ usage.\n    All kernels written in OpenAI's Triton language.\n    0% loss in\
    \ accuracy - no approximation methods - all exact.\n    No change of hardware\
    \ necessary. Supports NVIDIA GPUs since 2018+. Minimum CUDA Compute Capability\
    \ 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) Check your GPU\n\
    \    Flash Attention v2 support via Xformers.\n    NEW! Works on Linux and Windows\
    \ via WSL.\n    NEW! Experimental support for DPO (Direct Preference Optimization)!\n\
    \    Supports 4bit and 16bit QLoRA / LoRA finetuning via bitsandbytes.\n    Train\
    \ Slim Orca fully locally in 260 hours from 1301 hours (5x faster).\" \n\n  -\
    \ here is their project : https://github.com/unslothai/unsloth   . "
  created_at: 2023-12-08 22:26:57+00:00
  edited: false
  hidden: false
  id: 6573983114881863157a5a93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
      fullname: Catalin Ciocea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moneymaker2023
      type: user
    createdAt: '2023-12-08T22:31:32.000Z'
    data:
      edited: false
      editors:
      - Moneymaker2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9608786702156067
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
          fullname: Catalin Ciocea
          isHf: false
          isPro: false
          name: Moneymaker2023
          type: user
        html: '<p>I want to mention that I have managed to fine tune on my laptop
          with 16 gb ram and nvidia rtx 4050 6 gb video ram  a 7b  llama 2 model .
          This looks amazing for me and I''m very excited about this ! I did not yet
          test to see how effective is this finetuning (I''m a noob and still learning
          this amazing field of AI :) ) BUT it is a huge step forward the ability
          to train locally a 7b model...or at least I feel it is so ! </p>

          '
        raw: 'I want to mention that I have managed to fine tune on my laptop with
          16 gb ram and nvidia rtx 4050 6 gb video ram  a 7b  llama 2 model . This
          looks amazing for me and I''m very excited about this ! I did not yet test
          to see how effective is this finetuning (I''m a noob and still learning
          this amazing field of AI :) ) BUT it is a huge step forward the ability
          to train locally a 7b model...or at least I feel it is so ! '
        updatedAt: '2023-12-08T22:31:32.966Z'
      numEdits: 0
      reactions: []
    id: 65739944c13a2c7478fe9315
    type: comment
  author: Moneymaker2023
  content: 'I want to mention that I have managed to fine tune on my laptop with 16
    gb ram and nvidia rtx 4050 6 gb video ram  a 7b  llama 2 model . This looks amazing
    for me and I''m very excited about this ! I did not yet test to see how effective
    is this finetuning (I''m a noob and still learning this amazing field of AI :)
    ) BUT it is a huge step forward the ability to train locally a 7b model...or at
    least I feel it is so ! '
  created_at: 2023-12-08 22:31:32+00:00
  edited: false
  hidden: false
  id: 65739944c13a2c7478fe9315
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dfcd9d8a01634ad22e9140607a696938.svg
      fullname: Benjamin Marie
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bnjmnmarie
      type: user
    createdAt: '2023-12-09T10:56:32.000Z'
    data:
      edited: false
      editors:
      - bnjmnmarie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.971534013748169
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dfcd9d8a01634ad22e9140607a696938.svg
          fullname: Benjamin Marie
          isHf: false
          isPro: false
          name: bnjmnmarie
          type: user
        html: '<p>Thank you for your feedback!<br>I tried fine-tuning Mistral for
          translation into other languages than English. For now, my results are disappointing
          and not as good as with Llama 2. But I didn''t give up yet.<br>I don''t
          think fine-tuning Zephyr would work. Zephyr is already a fine-tuned model
          with a specific prompt format. To the best of my knowledge, Zephyr has only
          been fine-tuned on English data so it should have the same multilingual
          capabilities as Mistral.</p>

          '
        raw: 'Thank you for your feedback!

          I tried fine-tuning Mistral for translation into other languages than English.
          For now, my results are disappointing and not as good as with Llama 2. But
          I didn''t give up yet.

          I don''t think fine-tuning Zephyr would work. Zephyr is already a fine-tuned
          model with a specific prompt format. To the best of my knowledge, Zephyr
          has only been fine-tuned on English data so it should have the same multilingual
          capabilities as Mistral.

          '
        updatedAt: '2023-12-09T10:56:32.244Z'
      numEdits: 0
      reactions: []
    id: 657447e0f9898ed3ab51c360
    type: comment
  author: bnjmnmarie
  content: 'Thank you for your feedback!

    I tried fine-tuning Mistral for translation into other languages than English.
    For now, my results are disappointing and not as good as with Llama 2. But I didn''t
    give up yet.

    I don''t think fine-tuning Zephyr would work. Zephyr is already a fine-tuned model
    with a specific prompt format. To the best of my knowledge, Zephyr has only been
    fine-tuned on English data so it should have the same multilingual capabilities
    as Mistral.

    '
  created_at: 2023-12-09 10:56:32+00:00
  edited: false
  hidden: false
  id: 657447e0f9898ed3ab51c360
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
      fullname: Catalin Ciocea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moneymaker2023
      type: user
    createdAt: '2023-12-09T15:00:43.000Z'
    data:
      edited: false
      editors:
      - Moneymaker2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9490951299667358
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
          fullname: Catalin Ciocea
          isHf: false
          isPro: false
          name: Moneymaker2023
          type: user
        html: '<p>Zephyr is the only 7b model with which I have a conversation in
          romanian language without any fine tuning (it still invent some words or
          make some mistakes BUT it is surprisingly good and make intelligible conversations).
          I read somewhere that it was finetuned with 104 languages dataset (70 %
          English or so BUT is a BIG step forward that it allready works in other
          languages) The 200k chat which was used to fine tune it I think it is a
          multilingual dataset.  If you do not want to train it yourself can you please
          make a script or a notebook for training zephyr for translation (with ability
          to save checkpoints and reload the last checkpoint) and if possible taking
          advantage by the unsloth technology upgrade and post it for community to
          play with it ? Please! Thank you ! Catalin</p>

          '
        raw: Zephyr is the only 7b model with which I have a conversation in romanian
          language without any fine tuning (it still invent some words or make some
          mistakes BUT it is surprisingly good and make intelligible conversations).
          I read somewhere that it was finetuned with 104 languages dataset (70 %
          English or so BUT is a BIG step forward that it allready works in other
          languages) The 200k chat which was used to fine tune it I think it is a
          multilingual dataset.  If you do not want to train it yourself can you please
          make a script or a notebook for training zephyr for translation (with ability
          to save checkpoints and reload the last checkpoint) and if possible taking
          advantage by the unsloth technology upgrade and post it for community to
          play with it ? Please! Thank you ! Catalin
        updatedAt: '2023-12-09T15:00:43.061Z'
      numEdits: 0
      reactions: []
    id: 6574811b30a4401dfb532402
    type: comment
  author: Moneymaker2023
  content: Zephyr is the only 7b model with which I have a conversation in romanian
    language without any fine tuning (it still invent some words or make some mistakes
    BUT it is surprisingly good and make intelligible conversations). I read somewhere
    that it was finetuned with 104 languages dataset (70 % English or so BUT is a
    BIG step forward that it allready works in other languages) The 200k chat which
    was used to fine tune it I think it is a multilingual dataset.  If you do not
    want to train it yourself can you please make a script or a notebook for training
    zephyr for translation (with ability to save checkpoints and reload the last checkpoint)
    and if possible taking advantage by the unsloth technology upgrade and post it
    for community to play with it ? Please! Thank you ! Catalin
  created_at: 2023-12-09 15:00:43+00:00
  edited: false
  hidden: false
  id: 6574811b30a4401dfb532402
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
      fullname: Catalin Ciocea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moneymaker2023
      type: user
    createdAt: '2023-12-09T15:06:26.000Z'
    data:
      edited: false
      editors:
      - Moneymaker2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9291452169418335
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03f824e744a48612fada28cda1a28540.svg
          fullname: Catalin Ciocea
          isHf: false
          isPro: false
          name: Moneymaker2023
          type: user
        html: '<p>The stakes are very high in this . If it is possible to have a good
          7b model translator from english to any language =&gt; possibility to translate
          vast amount of good books of science in those respective languages and open
          the access to information for many peoples arround the world for whom language
          is a barrier .</p>

          '
        raw: The stakes are very high in this . If it is possible to have a good 7b
          model translator from english to any language => possibility to translate
          vast amount of good books of science in those respective languages and open
          the access to information for many peoples arround the world for whom language
          is a barrier .
        updatedAt: '2023-12-09T15:06:26.399Z'
      numEdits: 0
      reactions: []
    id: 657482723e0cb21bc7e34d45
    type: comment
  author: Moneymaker2023
  content: The stakes are very high in this . If it is possible to have a good 7b
    model translator from english to any language => possibility to translate vast
    amount of good books of science in those respective languages and open the access
    to information for many peoples arround the world for whom language is a barrier
    .
  created_at: 2023-12-09 15:06:26+00:00
  edited: false
  hidden: false
  id: 657482723e0cb21bc7e34d45
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: kaitchup/Llama-2-7b-mt-Czech-to-English
repo_type: model
status: open
target_branch: null
title: Can you please make an English to Romanian translation model ?
