!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AlexanderWillamowski
conflicting_files: null
created_at: 2024-01-07 16:57:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9637fe3744bd17950d305623e1a069b0.svg
      fullname: Alexander Willamowski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AlexanderWillamowski
      type: user
    createdAt: '2024-01-07T16:57:05.000Z'
    data:
      edited: false
      editors:
      - AlexanderWillamowski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5510192513465881
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9637fe3744bd17950d305623e1a069b0.svg
          fullname: Alexander Willamowski
          isHf: false
          isPro: false
          name: AlexanderWillamowski
          type: user
        html: "<p>Hi community,</p>\n<p>constantly I am facing the following issue.\
          \ I downloaded version dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf on my Linux\
          \ Debian 12 machine, 32 RAM both with GPT4all (v2.5.4) and LM Studio (AppImage\
          \ LM+Studio-0.2.8-beta-v1) and on both applications, the model throws the\
          \ following error wenn I want to load the model:</p>\n<pre><code>Error loading\
          \ model: create_tensor: tensor 'blk.0.ffn_gate.weight' not found\nllama_load_model_from_file:\
          \ failed to load model\nllama_init_from_gpt_params: error: failed to load\
          \ model '/media/berion/daten/gpt4all/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf'\n\
          load: error: failed to load model '/media/berion/daten/gpt4all/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf'\n\
          Debug <a href=\"/cognitivecomputations/dolphin-2.5-mixtral-8x7b/discussions/4\"\
          >#4</a>/media/berion/daten/gpt4all/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf\n\
          llama.ts: failed to load model. Error: create_tensor: tensor 'blk.0.ffn_gate.weight'\
          \ not found\n[modelprocess.ts] Error in loadModel: Error: create_tensor:\
          \ tensor 'blk.0.ffn_gate.weight' not found\non message 2 [remotellm.ts]\
          \ LLM process error: Error: create_tensor: tensor 'blk.0.ffn_gate.weight'\
          \ not found\n[modelprocesscontainer.ts] error handler: Error: Error: create_tensor:\
          \ tensor 'blk.0.ffn_gate.weight' not found\n[appserver] Error from LLM:\
          \ Error: Error: create_tensor: tensor 'blk.0.ffn_gate.weight' not found\n\
          [appserver] No inferencingReplyPort to send error to\nError: create_tensor:\
          \ tensor 'blk.0.ffn_gate.weight' not found\n    at t.Llama.&lt;anonymous&gt;\
          \ (/tmp/.mount_LM+Stu4ZNZrJ/resources/app/.webpack/main/utility.js:2:261927)\n\
          \    at Generator.next (&lt;anonymous&gt;)\n    at r (/tmp/.mount_LM+Stu4ZNZrJ/resources/app/.webpack/main/utility.js:2:260462)\n\
          [modelprocesscontainer.ts] #forkUtilityProcessAndLoadModel Error loading\
          \ model: Error: create_tensor: tensor 'blk.0.ffn_gate.weight' not found\n\
          [modelprocesscontainer.ts] error handler: Error: Model failed (exit code:\
          \ 42). Please try loading it again.\n[appserver] Error from LLM: Error:\
          \ Model failed (exit code: 42). Please try loading it again.\n[appserver]\
          \ No inferencingReplyPort to send error to\n[remotellm.ts] INSIDE LOAD PROMISE\
          \ EXIT LLM process exited with code 42.\n</code></pre>\n<p>I tried it also\
          \ with a newer version like <code>dolphin-2.6-mixtral-8x7b.Q3_K_M.gguf</code>\
          \ ending up the same and<br>the minimal model <code>dolphin-2.5-mixtral-8x7b.Q2_K.gguf</code></p>\n\
          <p>I've also checked the SHA256 checksum. Everything should be fine with\
          \ the downloaded GGUF file.</p>\n<pre><code>$ sha256sum dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf\
          \ \nee4474c3f6f9c20d7ba738ab63c7c19d3e1471749a72e8624e5b9db0c827b525  dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf\n\
          </code></pre>\n<p><strong>Any suggestions how to fix this issue?</strong></p>\n\
          <p>My memory configurations:</p>\n<pre><code>$ cat /proc/meminfo\nMemTotal:\
          \       32643500 kB\nMemFree:        10057616 kB\nMemAvailable:   26913816\
          \ kB\nBuffers:         6538016 kB\nCached:         11745012 kB\nSwapCached:\
          \            0 kB\nActive:          1800968 kB\nInactive:       18772184\
          \ kB\nActive(anon):       8208 kB\nInactive(anon):  3581172 kB\nActive(file):\
          \    1792760 kB\nInactive(file): 15191012 kB\nUnevictable:     1229592 kB\n\
          Mlocked:            5964 kB\nSwapTotal:       7812092 kB\nSwapFree:    \
          \    7812092 kB\nZswap:                 0 kB\nZswapped:              0 kB\n\
          Dirty:              5600 kB\nWriteback:             0 kB\nAnonPages:   \
          \    3469052 kB\nMapped:           768908 kB\nShmem:           1294052 kB\n\
          KReclaimable:     341452 kB\nSlab:             511628 kB\nSReclaimable:\
          \     341452 kB\nSUnreclaim:       170176 kB\nKernelStack:       25984 kB\n\
          PageTables:        50308 kB\nSecPageTables:         0 kB\nNFS_Unstable:\
          \          0 kB\nBounce:                0 kB\nWritebackTmp:          0 kB\n\
          CommitLimit:    24133840 kB\nCommitted_AS:   12432884 kB\nVmallocTotal:\
          \   34359738367 kB\nVmallocUsed:       72596 kB\nVmallocChunk:         \
          \ 0 kB\nPercpu:             7520 kB\nHardwareCorrupted:     0 kB\nAnonHugePages:\
          \    811008 kB\nShmemHugePages:  1105920 kB\nShmemPmdMapped:        0 kB\n\
          FileHugePages:         0 kB\nFilePmdMapped:         0 kB\nHugePages_Total:\
          \       0\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:\
          \        0\nHugepagesize:       2048 kB\nHugetlb:               0 kB\nDirectMap4k:\
          \      190068 kB\nDirectMap2M:     3780608 kB\nDirectMap1G:    30408704\
          \ kB\n</code></pre>\n"
        raw: "Hi community,\r\n\r\nconstantly I am facing the following issue. I downloaded\
          \ version dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf on my Linux Debian 12 machine,\
          \ 32 RAM both with GPT4all (v2.5.4) and LM Studio (AppImage LM+Studio-0.2.8-beta-v1)\
          \ and on both applications, the model throws the following error wenn I\
          \ want to load the model:\r\n\r\n```\r\nError loading model: create_tensor:\
          \ tensor 'blk.0.ffn_gate.weight' not found\r\nllama_load_model_from_file:\
          \ failed to load model\r\nllama_init_from_gpt_params: error: failed to load\
          \ model '/media/berion/daten/gpt4all/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf'\r\
          \nload: error: failed to load model '/media/berion/daten/gpt4all/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf'\r\
          \nDebug #4/media/berion/daten/gpt4all/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf\r\
          \nllama.ts: failed to load model. Error: create_tensor: tensor 'blk.0.ffn_gate.weight'\
          \ not found\r\n[modelprocess.ts] Error in loadModel: Error: create_tensor:\
          \ tensor 'blk.0.ffn_gate.weight' not found\r\non message 2 [remotellm.ts]\
          \ LLM process error: Error: create_tensor: tensor 'blk.0.ffn_gate.weight'\
          \ not found\r\n[modelprocesscontainer.ts] error handler: Error: Error: create_tensor:\
          \ tensor 'blk.0.ffn_gate.weight' not found\r\n[appserver] Error from LLM:\
          \ Error: Error: create_tensor: tensor 'blk.0.ffn_gate.weight' not found\r\
          \n[appserver] No inferencingReplyPort to send error to\r\nError: create_tensor:\
          \ tensor 'blk.0.ffn_gate.weight' not found\r\n    at t.Llama.<anonymous>\
          \ (/tmp/.mount_LM+Stu4ZNZrJ/resources/app/.webpack/main/utility.js:2:261927)\r\
          \n    at Generator.next (<anonymous>)\r\n    at r (/tmp/.mount_LM+Stu4ZNZrJ/resources/app/.webpack/main/utility.js:2:260462)\r\
          \n[modelprocesscontainer.ts] #forkUtilityProcessAndLoadModel Error loading\
          \ model: Error: create_tensor: tensor 'blk.0.ffn_gate.weight' not found\r\
          \n[modelprocesscontainer.ts] error handler: Error: Model failed (exit code:\
          \ 42). Please try loading it again.\r\n[appserver] Error from LLM: Error:\
          \ Model failed (exit code: 42). Please try loading it again.\r\n[appserver]\
          \ No inferencingReplyPort to send error to\r\n[remotellm.ts] INSIDE LOAD\
          \ PROMISE EXIT LLM process exited with code 42.\r\n```\r\nI tried it also\
          \ with a newer version like ``dolphin-2.6-mixtral-8x7b.Q3_K_M.gguf`` ending\
          \ up the same and \r\nthe minimal model ``dolphin-2.5-mixtral-8x7b.Q2_K.gguf``\r\
          \n\r\nI've also checked the SHA256 checksum. Everything should be fine with\
          \ the downloaded GGUF file.\r\n```\r\n$ sha256sum dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf\
          \ \r\nee4474c3f6f9c20d7ba738ab63c7c19d3e1471749a72e8624e5b9db0c827b525 \
          \ dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf\r\n```\r\n\r\n**Any suggestions how\
          \ to fix this issue?**\r\n\r\nMy memory configurations:\r\n\r\n```\r\n$\
          \ cat /proc/meminfo\r\nMemTotal:       32643500 kB\r\nMemFree:        10057616\
          \ kB\r\nMemAvailable:   26913816 kB\r\nBuffers:         6538016 kB\r\nCached:\
          \         11745012 kB\r\nSwapCached:            0 kB\r\nActive:        \
          \  1800968 kB\r\nInactive:       18772184 kB\r\nActive(anon):       8208\
          \ kB\r\nInactive(anon):  3581172 kB\r\nActive(file):    1792760 kB\r\nInactive(file):\
          \ 15191012 kB\r\nUnevictable:     1229592 kB\r\nMlocked:            5964\
          \ kB\r\nSwapTotal:       7812092 kB\r\nSwapFree:        7812092 kB\r\nZswap:\
          \                 0 kB\r\nZswapped:              0 kB\r\nDirty:        \
          \      5600 kB\r\nWriteback:             0 kB\r\nAnonPages:       3469052\
          \ kB\r\nMapped:           768908 kB\r\nShmem:           1294052 kB\r\nKReclaimable:\
          \     341452 kB\r\nSlab:             511628 kB\r\nSReclaimable:     341452\
          \ kB\r\nSUnreclaim:       170176 kB\r\nKernelStack:       25984 kB\r\nPageTables:\
          \        50308 kB\r\nSecPageTables:         0 kB\r\nNFS_Unstable:      \
          \    0 kB\r\nBounce:                0 kB\r\nWritebackTmp:          0 kB\r\
          \nCommitLimit:    24133840 kB\r\nCommitted_AS:   12432884 kB\r\nVmallocTotal:\
          \   34359738367 kB\r\nVmallocUsed:       72596 kB\r\nVmallocChunk:     \
          \     0 kB\r\nPercpu:             7520 kB\r\nHardwareCorrupted:     0 kB\r\
          \nAnonHugePages:    811008 kB\r\nShmemHugePages:  1105920 kB\r\nShmemPmdMapped:\
          \        0 kB\r\nFileHugePages:         0 kB\r\nFilePmdMapped:         0\
          \ kB\r\nHugePages_Total:       0\r\nHugePages_Free:        0\r\nHugePages_Rsvd:\
          \        0\r\nHugePages_Surp:        0\r\nHugepagesize:       2048 kB\r\n\
          Hugetlb:               0 kB\r\nDirectMap4k:      190068 kB\r\nDirectMap2M:\
          \     3780608 kB\r\nDirectMap1G:    30408704 kB\r\n```\r\n\r\n"
        updatedAt: '2024-01-07T16:57:05.813Z'
      numEdits: 0
      reactions: []
    id: 659ad7e12fe7ca485f1cfae4
    type: comment
  author: AlexanderWillamowski
  content: "Hi community,\r\n\r\nconstantly I am facing the following issue. I downloaded\
    \ version dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf on my Linux Debian 12 machine,\
    \ 32 RAM both with GPT4all (v2.5.4) and LM Studio (AppImage LM+Studio-0.2.8-beta-v1)\
    \ and on both applications, the model throws the following error wenn I want to\
    \ load the model:\r\n\r\n```\r\nError loading model: create_tensor: tensor 'blk.0.ffn_gate.weight'\
    \ not found\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params:\
    \ error: failed to load model '/media/berion/daten/gpt4all/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf'\r\
    \nload: error: failed to load model '/media/berion/daten/gpt4all/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf'\r\
    \nDebug #4/media/berion/daten/gpt4all/models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF/dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf\r\
    \nllama.ts: failed to load model. Error: create_tensor: tensor 'blk.0.ffn_gate.weight'\
    \ not found\r\n[modelprocess.ts] Error in loadModel: Error: create_tensor: tensor\
    \ 'blk.0.ffn_gate.weight' not found\r\non message 2 [remotellm.ts] LLM process\
    \ error: Error: create_tensor: tensor 'blk.0.ffn_gate.weight' not found\r\n[modelprocesscontainer.ts]\
    \ error handler: Error: Error: create_tensor: tensor 'blk.0.ffn_gate.weight' not\
    \ found\r\n[appserver] Error from LLM: Error: Error: create_tensor: tensor 'blk.0.ffn_gate.weight'\
    \ not found\r\n[appserver] No inferencingReplyPort to send error to\r\nError:\
    \ create_tensor: tensor 'blk.0.ffn_gate.weight' not found\r\n    at t.Llama.<anonymous>\
    \ (/tmp/.mount_LM+Stu4ZNZrJ/resources/app/.webpack/main/utility.js:2:261927)\r\
    \n    at Generator.next (<anonymous>)\r\n    at r (/tmp/.mount_LM+Stu4ZNZrJ/resources/app/.webpack/main/utility.js:2:260462)\r\
    \n[modelprocesscontainer.ts] #forkUtilityProcessAndLoadModel Error loading model:\
    \ Error: create_tensor: tensor 'blk.0.ffn_gate.weight' not found\r\n[modelprocesscontainer.ts]\
    \ error handler: Error: Model failed (exit code: 42). Please try loading it again.\r\
    \n[appserver] Error from LLM: Error: Model failed (exit code: 42). Please try\
    \ loading it again.\r\n[appserver] No inferencingReplyPort to send error to\r\n\
    [remotellm.ts] INSIDE LOAD PROMISE EXIT LLM process exited with code 42.\r\n```\r\
    \nI tried it also with a newer version like ``dolphin-2.6-mixtral-8x7b.Q3_K_M.gguf``\
    \ ending up the same and \r\nthe minimal model ``dolphin-2.5-mixtral-8x7b.Q2_K.gguf``\r\
    \n\r\nI've also checked the SHA256 checksum. Everything should be fine with the\
    \ downloaded GGUF file.\r\n```\r\n$ sha256sum dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf\
    \ \r\nee4474c3f6f9c20d7ba738ab63c7c19d3e1471749a72e8624e5b9db0c827b525  dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf\r\
    \n```\r\n\r\n**Any suggestions how to fix this issue?**\r\n\r\nMy memory configurations:\r\
    \n\r\n```\r\n$ cat /proc/meminfo\r\nMemTotal:       32643500 kB\r\nMemFree:  \
    \      10057616 kB\r\nMemAvailable:   26913816 kB\r\nBuffers:         6538016\
    \ kB\r\nCached:         11745012 kB\r\nSwapCached:            0 kB\r\nActive:\
    \          1800968 kB\r\nInactive:       18772184 kB\r\nActive(anon):       8208\
    \ kB\r\nInactive(anon):  3581172 kB\r\nActive(file):    1792760 kB\r\nInactive(file):\
    \ 15191012 kB\r\nUnevictable:     1229592 kB\r\nMlocked:            5964 kB\r\n\
    SwapTotal:       7812092 kB\r\nSwapFree:        7812092 kB\r\nZswap:         \
    \        0 kB\r\nZswapped:              0 kB\r\nDirty:              5600 kB\r\n\
    Writeback:             0 kB\r\nAnonPages:       3469052 kB\r\nMapped:        \
    \   768908 kB\r\nShmem:           1294052 kB\r\nKReclaimable:     341452 kB\r\n\
    Slab:             511628 kB\r\nSReclaimable:     341452 kB\r\nSUnreclaim:    \
    \   170176 kB\r\nKernelStack:       25984 kB\r\nPageTables:        50308 kB\r\n\
    SecPageTables:         0 kB\r\nNFS_Unstable:          0 kB\r\nBounce:        \
    \        0 kB\r\nWritebackTmp:          0 kB\r\nCommitLimit:    24133840 kB\r\n\
    Committed_AS:   12432884 kB\r\nVmallocTotal:   34359738367 kB\r\nVmallocUsed:\
    \       72596 kB\r\nVmallocChunk:          0 kB\r\nPercpu:             7520 kB\r\
    \nHardwareCorrupted:     0 kB\r\nAnonHugePages:    811008 kB\r\nShmemHugePages:\
    \  1105920 kB\r\nShmemPmdMapped:        0 kB\r\nFileHugePages:         0 kB\r\n\
    FilePmdMapped:         0 kB\r\nHugePages_Total:       0\r\nHugePages_Free:   \
    \     0\r\nHugePages_Rsvd:        0\r\nHugePages_Surp:        0\r\nHugepagesize:\
    \       2048 kB\r\nHugetlb:               0 kB\r\nDirectMap4k:      190068 kB\r\
    \nDirectMap2M:     3780608 kB\r\nDirectMap1G:    30408704 kB\r\n```\r\n\r\n"
  created_at: 2024-01-07 16:57:05+00:00
  edited: false
  hidden: false
  id: 659ad7e12fe7ca485f1cfae4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: cognitivecomputations/dolphin-2.5-mixtral-8x7b
repo_type: model
status: open
target_branch: null
title: 'llama.ts: failed to load model. Error: create_tensor: tensor ''blk.0.ffn_gate.weight''
  not found'
