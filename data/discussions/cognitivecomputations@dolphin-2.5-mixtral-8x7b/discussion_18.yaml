!!python/object:huggingface_hub.community.DiscussionWithDetails
author: neevany
conflicting_files: null
created_at: 2023-12-21 14:54:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/392a5042ca10c38be0d37489b45b5178.svg
      fullname: Navee Y
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neevany
      type: user
    createdAt: '2023-12-21T14:54:14.000Z'
    data:
      edited: false
      editors:
      - neevany
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8666623830795288
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/392a5042ca10c38be0d37489b45b5178.svg
          fullname: Navee Y
          isHf: false
          isPro: false
          name: neevany
          type: user
        html: '<p>I am getting this <code>Error: llama runner process has terminated</code></p>

          '
        raw: 'I am getting this `Error: llama runner process has terminated`'
        updatedAt: '2023-12-21T14:54:14.792Z'
      numEdits: 0
      reactions: []
    id: 658451961fb17c3a9ac7e76b
    type: comment
  author: neevany
  content: 'I am getting this `Error: llama runner process has terminated`'
  created_at: 2023-12-21 14:54:14+00:00
  edited: false
  hidden: false
  id: 658451961fb17c3a9ac7e76b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c741e955e96d2213e722838daabd0bd.svg
      fullname: Robin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fidr
      type: user
    createdAt: '2023-12-21T15:35:40.000Z'
    data:
      edited: false
      editors:
      - fidr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9265664219856262
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c741e955e96d2213e722838daabd0bd.svg
          fullname: Robin
          isHf: false
          isPro: false
          name: fidr
          type: user
        html: '<p>You can run <code>sudo sysctl iogpu.wired_limit_mb=27000</code>
          to increase the available memory. I suppose you shouldn''t put it too high,
          default is about 2/3rd of your mem. I believe the settings is <code>debug.iogpu.wired_limit</code>
          for older OS''s.</p>

          <p>I was able to load the 26gb <code>q4_0</code> weights with this (running
          with ollama).</p>

          '
        raw: 'You can run `sudo sysctl iogpu.wired_limit_mb=27000` to increase the
          available memory. I suppose you shouldn''t put it too high, default is about
          2/3rd of your mem. I believe the settings is `debug.iogpu.wired_limit` for
          older OS''s.


          I was able to load the 26gb `q4_0` weights with this (running with ollama).

          '
        updatedAt: '2023-12-21T15:35:40.298Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - neevany
    id: 65845b4c4b2f0e9f34173dea
    type: comment
  author: fidr
  content: 'You can run `sudo sysctl iogpu.wired_limit_mb=27000` to increase the available
    memory. I suppose you shouldn''t put it too high, default is about 2/3rd of your
    mem. I believe the settings is `debug.iogpu.wired_limit` for older OS''s.


    I was able to load the 26gb `q4_0` weights with this (running with ollama).

    '
  created_at: 2023-12-21 15:35:40+00:00
  edited: false
  hidden: false
  id: 65845b4c4b2f0e9f34173dea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/392a5042ca10c38be0d37489b45b5178.svg
      fullname: Navee Y
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neevany
      type: user
    createdAt: '2023-12-21T16:00:23.000Z'
    data:
      edited: false
      editors:
      - neevany
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7692937254905701
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/392a5042ca10c38be0d37489b45b5178.svg
          fullname: Navee Y
          isHf: false
          isPro: false
          name: neevany
          type: user
        html: '<p>Working, Thank you!</p>

          '
        raw: Working, Thank you!
        updatedAt: '2023-12-21T16:00:23.496Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65846117bef870ffa4af246f
    id: 65846117bef870ffa4af246e
    type: comment
  author: neevany
  content: Working, Thank you!
  created_at: 2023-12-21 16:00:23+00:00
  edited: false
  hidden: false
  id: 65846117bef870ffa4af246e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/392a5042ca10c38be0d37489b45b5178.svg
      fullname: Navee Y
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neevany
      type: user
    createdAt: '2023-12-21T16:00:23.000Z'
    data:
      status: closed
    id: 65846117bef870ffa4af246f
    type: status-change
  author: neevany
  created_at: 2023-12-21 16:00:23+00:00
  id: 65846117bef870ffa4af246f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: cognitivecomputations/dolphin-2.5-mixtral-8x7b
repo_type: model
status: closed
target_branch: null
title: Anyone successful in running this on m2 pro with 32 gb ram?
