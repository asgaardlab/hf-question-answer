!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Thireus
conflicting_files: null
created_at: 2023-12-19 12:45:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-12-19T12:45:01.000Z'
    data:
      edited: false
      editors:
      - Thireus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7812925577163696
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: '<p>I ran a PPL eval and noticed that the PPL is much higher than the
          original Mixtral model on wikitext.</p>

          <ul>

          <li>LoneStriker_dolphin-2.5-mixtral-8x7b-6.0bpw-h6-exl2-2 - 4.464363098144531</li>

          <li>turboderp_Mixtral-8x7B-instruct-exl2_8.0bpw - 3.7087724208831774</li>

          </ul>

          <p>I was wondering if this is expected.</p>

          <p>For ref, 70b dolphin models give me PPLs just below 4:</p>

          <ul>

          <li>LoneStriker_dolphin-2.2-70b-6.0bpw-h6-exl2-2 - 3.965563297271729</li>

          </ul>

          '
        raw: "I ran a PPL eval and noticed that the PPL is much higher than the original\
          \ Mixtral model on wikitext.\r\n\r\n- LoneStriker_dolphin-2.5-mixtral-8x7b-6.0bpw-h6-exl2-2\
          \ - 4.464363098144531\r\n- turboderp_Mixtral-8x7B-instruct-exl2_8.0bpw -\
          \ 3.7087724208831774\r\n\r\nI was wondering if this is expected.\r\n\r\n\
          For ref, 70b dolphin models give me PPLs just below 4:\r\n- LoneStriker_dolphin-2.2-70b-6.0bpw-h6-exl2-2\
          \ - 3.965563297271729"
        updatedAt: '2023-12-19T12:45:01.236Z'
      numEdits: 0
      reactions: []
    id: 6581904d52df98e60c4139d4
    type: comment
  author: Thireus
  content: "I ran a PPL eval and noticed that the PPL is much higher than the original\
    \ Mixtral model on wikitext.\r\n\r\n- LoneStriker_dolphin-2.5-mixtral-8x7b-6.0bpw-h6-exl2-2\
    \ - 4.464363098144531\r\n- turboderp_Mixtral-8x7B-instruct-exl2_8.0bpw - 3.7087724208831774\r\
    \n\r\nI was wondering if this is expected.\r\n\r\nFor ref, 70b dolphin models\
    \ give me PPLs just below 4:\r\n- LoneStriker_dolphin-2.2-70b-6.0bpw-h6-exl2-2\
    \ - 3.965563297271729"
  created_at: 2023-12-19 12:45:01+00:00
  edited: false
  hidden: false
  id: 6581904d52df98e60c4139d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/HodXBlbYUYrfMgR4fDnI4.jpeg?w=200&h=200&f=face
      fullname: Hirose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HiroseKoichi
      type: user
    createdAt: '2023-12-20T22:01:46.000Z'
    data:
      edited: false
      editors:
      - HiroseKoichi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9658492803573608
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/HodXBlbYUYrfMgR4fDnI4.jpeg?w=200&h=200&f=face
          fullname: Hirose
          isHf: false
          isPro: false
          name: HiroseKoichi
          type: user
        html: '<p>You''re comparing 6.0bpw to 8.0bpw, so yes, it''s expected that
          they might have higher perplexity in general, but also with exl2, the quality
          can vary depending on the calibration dataset used during quantization.</p>

          '
        raw: You're comparing 6.0bpw to 8.0bpw, so yes, it's expected that they might
          have higher perplexity in general, but also with exl2, the quality can vary
          depending on the calibration dataset used during quantization.
        updatedAt: '2023-12-20T22:01:46.263Z'
      numEdits: 0
      reactions: []
    id: 6583644ad061a54e71b90a8e
    type: comment
  author: HiroseKoichi
  content: You're comparing 6.0bpw to 8.0bpw, so yes, it's expected that they might
    have higher perplexity in general, but also with exl2, the quality can vary depending
    on the calibration dataset used during quantization.
  created_at: 2023-12-20 22:01:46+00:00
  edited: false
  hidden: false
  id: 6583644ad061a54e71b90a8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-12-20T22:11:59.000Z'
    data:
      edited: false
      editors:
      - Thireus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9442483186721802
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;HiroseKoichi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/HiroseKoichi\"\
          >@<span class=\"underline\">HiroseKoichi</span></a></span>\n\n\t</span></span>,\
          \ it's a 0.76 PPL jump.</p>\n<p>If someone can share the PPL on the non-quantized\
          \ version I'd be interested to see how far it is from Mixtral original model.</p>\n"
        raw: '@HiroseKoichi, it''s a 0.76 PPL jump.


          If someone can share the PPL on the non-quantized version I''d be interested
          to see how far it is from Mixtral original model.'
        updatedAt: '2023-12-20T22:11:59.396Z'
      numEdits: 0
      reactions: []
    id: 658366afaf09ca206bbedc48
    type: comment
  author: Thireus
  content: '@HiroseKoichi, it''s a 0.76 PPL jump.


    If someone can share the PPL on the non-quantized version I''d be interested to
    see how far it is from Mixtral original model.'
  created_at: 2023-12-20 22:11:59+00:00
  edited: false
  hidden: false
  id: 658366afaf09ca206bbedc48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/HodXBlbYUYrfMgR4fDnI4.jpeg?w=200&h=200&f=face
      fullname: Hirose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HiroseKoichi
      type: user
    createdAt: '2023-12-20T23:21:34.000Z'
    data:
      edited: true
      editors:
      - HiroseKoichi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8511773943901062
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/HodXBlbYUYrfMgR4fDnI4.jpeg?w=200&h=200&f=face
          fullname: Hirose
          isHf: false
          isPro: false
          name: HiroseKoichi
          type: user
        html: '<p>Taking a look at Turboderp''s page, it looks like your test is the
          outlier here and dolphin is right in line with the expected numbers: <a
          href="https://huggingface.co/turboderp/Mixtral-8x7B-instruct-exl2">https://huggingface.co/turboderp/Mixtral-8x7B-instruct-exl2</a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/morBWLTQbgoQE8OfBjkmu.png"><img
          alt="Z86yJWJMzT4gljf27mmTF.png" src="https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/morBWLTQbgoQE8OfBjkmu.png"></a><br>I''m
          by no means an expert on exl2 quantization, but wikitext is a popular calibration
          dataset for exl2 quants, which could explain why the perplexity is much
          lower for Mixtral-Instruct. Try running it through a different dataset.</p>

          '
        raw: 'Taking a look at Turboderp''s page, it looks like your test is the outlier
          here and dolphin is right in line with the expected numbers: https://huggingface.co/turboderp/Mixtral-8x7B-instruct-exl2

          ![Z86yJWJMzT4gljf27mmTF.png](https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/morBWLTQbgoQE8OfBjkmu.png)

          I''m by no means an expert on exl2 quantization, but wikitext is a popular
          calibration dataset for exl2 quants, which could explain why the perplexity
          is much lower for Mixtral-Instruct. Try running it through a different dataset.'
        updatedAt: '2023-12-21T00:06:03.718Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Thireus
    id: 658376fec327dc81ff0459f9
    type: comment
  author: HiroseKoichi
  content: 'Taking a look at Turboderp''s page, it looks like your test is the outlier
    here and dolphin is right in line with the expected numbers: https://huggingface.co/turboderp/Mixtral-8x7B-instruct-exl2

    ![Z86yJWJMzT4gljf27mmTF.png](https://cdn-uploads.huggingface.co/production/uploads/63bd2e4f106236e1c151733f/morBWLTQbgoQE8OfBjkmu.png)

    I''m by no means an expert on exl2 quantization, but wikitext is a popular calibration
    dataset for exl2 quants, which could explain why the perplexity is much lower
    for Mixtral-Instruct. Try running it through a different dataset.'
  created_at: 2023-12-20 23:21:34+00:00
  edited: true
  hidden: false
  id: 658376fec327dc81ff0459f9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: cognitivecomputations/dolphin-2.5-mixtral-8x7b
repo_type: model
status: open
target_branch: null
title: Higher PPL than Mixtral?
