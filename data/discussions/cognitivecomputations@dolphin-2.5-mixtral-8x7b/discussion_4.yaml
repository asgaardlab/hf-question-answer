!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Blevlabs
conflicting_files: null
created_at: 2023-12-15 15:22:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
      fullname: Brayden Levangie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Blevlabs
      type: user
    createdAt: '2023-12-15T15:22:40.000Z'
    data:
      edited: false
      editors:
      - Blevlabs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5359786152839661
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
          fullname: Brayden Levangie
          isHf: false
          isPro: false
          name: Blevlabs
          type: user
        html: '<p>Hello, I saw this topic was related to one of the closed tickets,
          and that the user did not provide adequate information. I wanted to share
          my attempt to host it on the inference endpoint as I have been trying to
          use this model.</p>

          <p>Here is an output:</p>

          <pre><code>2023/12/15 10:03:22 ~ {"timestamp":"2023-12-15T15:03:22.451927Z","level":"INFO","fields":{"message":"Successfully
          downloaded weights."},"target":"text_generation_launcher","span":{"name":"download"},"spans":[{"name":"download"}]}

          2023/12/15 10:03:22 ~ {"timestamp":"2023-12-15T15:03:22.452217Z","level":"INFO","fields":{"message":"Starting
          shard"},"target":"text_generation_launcher","span":{"rank":1,"name":"shard-manager"},"spans":[{"rank":1,"name":"shard-manager"}]}

          2023/12/15 10:03:22 ~ {"timestamp":"2023-12-15T15:03:22.452186Z","level":"INFO","fields":{"message":"Starting
          shard"},"target":"text_generation_launcher","span":{"rank":0,"name":"shard-manager"},"spans":[{"rank":0,"name":"shard-manager"}]}

          2023/12/15 10:03:22 ~ {"timestamp":"2023-12-15T15:03:22.452239Z","level":"INFO","fields":{"message":"Starting
          shard"},"target":"text_generation_launcher","span":{"rank":2,"name":"shard-manager"},"spans":[{"rank":2,"name":"shard-manager"}]}

          2023/12/15 10:03:22 ~ {"timestamp":"2023-12-15T15:03:22.452655Z","level":"INFO","fields":{"message":"Starting
          shard"},"target":"text_generation_launcher","span":{"rank":3,"name":"shard-manager"},"spans":[{"rank":3,"name":"shard-manager"}]}

          2023/12/15 10:03:25 ~ {"timestamp":"2023-12-15T15:03:25.517688Z","level":"WARN","fields":{"message":"Unable
          to use Flash Attention V2: GPU with CUDA capability 7 5 is not supported
          for Flash Attention V2\n"},"target":"text_generation_launcher"}

          2023/12/15 10:03:25 ~ {"timestamp":"2023-12-15T15:03:25.524783Z","level":"WARN","fields":{"message":"Unable
          to use Flash Attention V2: GPU with CUDA capability 7 5 is not supported
          for Flash Attention V2\n"},"target":"text_generation_launcher"}

          2023/12/15 10:03:25 ~ {"timestamp":"2023-12-15T15:03:25.526299Z","level":"WARN","fields":{"message":"Unable
          to use Flash Attention V2: GPU with CUDA capability 7 5 is not supported
          for Flash Attention V2\n"},"target":"text_generation_launcher"}

          2023/12/15 10:03:25 ~ {"timestamp":"2023-12-15T15:03:25.529150Z","level":"WARN","fields":{"message":"Unable
          to use Flash Attention V2: GPU with CUDA capability 7 5 is not supported
          for Flash Attention V2\n"},"target":"text_generation_launcher"}

          2023/12/15 10:03:25 ~ {"timestamp":"2023-12-15T15:03:25.537123Z","level":"WARN","fields":{"message":"Could
          not import Mistral model: Mistral model requires flash attn v2\n"},"target":"text_generation_launcher"}

          2023/12/15 10:03:25 ~ {"timestamp":"2023-12-15T15:03:25.543898Z","level":"WARN","fields":{"message":"Could
          not import Mistral model: Mistral model requires flash attn v2\n"},"target":"text_generation_launcher"}

          2023/12/15 10:03:25 ~ {"timestamp":"2023-12-15T15:03:25.545572Z","level":"WARN","fields":{"message":"Could
          not import Mistral model: Mistral model requires flash attn v2\n"},"target":"text_generation_launcher"}

          2023/12/15 10:03:25 ~ {"timestamp":"2023-12-15T15:03:25.548302Z","level":"WARN","fields":{"message":"Could
          not import Mistral model: Mistral model requires flash attn v2\n"},"target":"text_generation_launcher"}

          2023/12/15 10:03:25 ~ {"timestamp":"2023-12-15T15:03:25.591579Z","level":"ERROR","fields":{"message":"Error
          when initializing model\nTraceback (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\",
          line 8, in &lt;module&gt;\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
          line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n  File
          \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in
          __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
          line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
          line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
          line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File
          \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434, in
          invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
          line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
          line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n  File
          \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
          line 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
          line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\",
          line 80, in _run\n    self._context.run(self._callback, *self._args)\n&gt;
          File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
          line 291, in get_model\n    raise ValueError(\"sharded is not supported
          for AutoModel\")\nValueError: sharded is not supported for AutoModel\n"},"target":"text_generation_launcher"}

          2023/12/15 10:03:25 ~ {"timestamp":"2023-12-15T15:03:25.596882Z","level":"ERROR","fields":{"message":"Error
          when initializing model\nTraceback (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\",
          line 8, in &lt;module&gt;\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
          line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n  File
          \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in
          __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
          line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
          line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
          line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File
          \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434, in
          invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
          line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
          line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n  File
          \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
          line 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
          line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\",
          line 80, in _run\n    self._context.run(self._callback, *self._args)\n&gt;
          File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
          line 291, in get_model\n    raise ValueError(\"sharded is not supported
          for AutoModel\")\nValueError: sharded is not supported for AutoModel\n"},"target":"text_generation_launcher"}

          2023/12/15 10:03:25 ~ {"timestamp":"2023-12-15T15:03:25.598776Z","level":"ERROR","fields":{"message":"Error
          when initializing model\nTraceback (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\",
          line 8, in &lt;module&gt;\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
          line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n  File
          \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in
          __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
          line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
          line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
          line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File
          \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434, in
          invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
          line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
          line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n  File
          \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
          line 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
          line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\",
          line 80, in _run\n    self._context.run(self._callback, *self._args)\n&gt;
          File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
          line 291, in get_model\n    raise ValueError(\"sharded is not supported
          for AutoModel\")\nValueError: sharded is not supported for AutoModel\n"},"target":"text_generation_launcher"}

          2023/12/15 10:03:25 ~ {"timestamp":"2023-12-15T15:03:25.601267Z","level":"ERROR","fields":{"message":"Error
          when initializing model\nTraceback (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\",
          line 8, in &lt;module&gt;\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
          line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n  File
          \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in
          __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
          line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
          line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
          line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File
          \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434, in
          invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
          line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
          line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n  File
          \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
          line 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
          line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\",
          line 80, in _run\n    self._context.run(self._callback, *self._args)\n&gt;
          File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
          line 291, in get_model\n    raise ValueError(\"sharded is not supported
          for AutoModel\")\nValueError: sharded is not supported for AutoModel\n"},"target":"text_generation_launcher"}

          2023/12/15 10:03:26 ~ {"timestamp":"2023-12-15T15:03:26.157644Z","level":"ERROR","fields":{"message":"Shard
          complete standard error output:\n\nTraceback (most recent call last):\n\n  File
          \"/opt/conda/bin/text-generation-server\", line 8, in &lt;module&gt;\n    sys.exit(app())\n\n  File
          \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
          line 83, in serve\n    server.serve(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 207, in serve\n    asyncio.run(\n\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
          line 44, in run\n    return loop.run_until_complete(main)\n\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 647, in run_until_complete\n    return future.result()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 159, in serve_inner\n    model = get_model(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
          line 291, in get_model\n    raise ValueError(\"sharded is not supported
          for AutoModel\")\n\nValueError: sharded is not supported for AutoModel\n"},"target":"text_generation_launcher","span":{"rank":1,"name":"shard-manager"},"spans":[{"rank":1,"name":"shard-manager"}]}

          2023/12/15 10:03:26 ~ {"timestamp":"2023-12-15T15:03:26.157779Z","level":"ERROR","fields":{"message":"Shard
          complete standard error output:\n\nTraceback (most recent call last):\n\n  File
          \"/opt/conda/bin/text-generation-server\", line 8, in &lt;module&gt;\n    sys.exit(app())\n\n  File
          \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
          line 83, in serve\n    server.serve(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 207, in serve\n    asyncio.run(\n\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
          line 44, in run\n    return loop.run_until_complete(main)\n\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 647, in run_until_complete\n    return future.result()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 159, in serve_inner\n    model = get_model(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
          line 291, in get_model\n    raise ValueError(\"sharded is not supported
          for AutoModel\")\n\nValueError: sharded is not supported for AutoModel\n"},"target":"text_generation_launcher","span":{"rank":2,"name":"shard-manager"},"spans":[{"rank":2,"name":"shard-manager"}]}

          2023/12/15 10:03:26 ~ {"timestamp":"2023-12-15T15:03:26.157415Z","level":"ERROR","fields":{"message":"Shard
          complete standard error output:\n\nTraceback (most recent call last):\n\n  File
          \"/opt/conda/bin/text-generation-server\", line 8, in &lt;module&gt;\n    sys.exit(app())\n\n  File
          \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
          line 83, in serve\n    server.serve(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 207, in serve\n    asyncio.run(\n\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
          line 44, in run\n    return loop.run_until_complete(main)\n\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 647, in run_until_complete\n    return future.result()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 159, in serve_inner\n    model = get_model(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
          line 291, in get_model\n    raise ValueError(\"sharded is not supported
          for AutoModel\")\n\nValueError: sharded is not supported for AutoModel\n"},"target":"text_generation_launcher","span":{"rank":3,"name":"shard-manager"},"spans":[{"rank":3,"name":"shard-manager"}]}

          2023/12/15 10:03:26 ~ {"timestamp":"2023-12-15T15:03:26.256251Z","level":"ERROR","fields":{"message":"Shard
          3 failed to start"},"target":"text_generation_launcher"}

          2023/12/15 10:03:26 ~ {"timestamp":"2023-12-15T15:03:26.256295Z","level":"INFO","fields":{"message":"Shutting
          down shards"},"target":"text_generation_launcher"}

          2023/12/15 10:03:26 ~ {"timestamp":"2023-12-15T15:03:26.257847Z","level":"ERROR","fields":{"message":"Shard
          complete standard error output:\n\nTraceback (most recent call last):\n\n  File
          \"/opt/conda/bin/text-generation-server\", line 8, in &lt;module&gt;\n    sys.exit(app())\n\n  File
          \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
          line 83, in serve\n    server.serve(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 207, in serve\n    asyncio.run(\n\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
          line 44, in run\n    return loop.run_until_complete(main)\n\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 647, in run_until_complete\n    return future.result()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 159, in serve_inner\n    model = get_model(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
          line 291, in get_model\n    raise ValueError(\"sharded is not supported
          for AutoModel\")\n\nValueError: sharded is not supported for AutoModel\n"},"target":"text_generation_launcher","span":{"rank":0,"name":"shard-manager"},"spans":[{"rank":0,"name":"shard-manager"}]}



          2023/12/15 10:03:26 ~ Error: ShardCannotStart

          2023/12/15 10:04:10 ~ {"timestamp":"2023-12-15T15:04:10.264390Z","level":"INFO","fields":{"message":"Sharding
          model on 4 processes"},"target":"text_generation_launcher"}

          2023/12/15 10:04:10 ~ {"timestamp":"2023-12-15T15:04:10.264345Z","level":"INFO","fields":{"message":"Args
          { model_id: \"/repository\", revision: None, validation_workers: 2, sharded:
          None, num_shard: None, quantize: Some(Bitsandbytes), dtype: None, trust_remote_code:
          false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences:
          4, max_top_n_tokens: 5, max_input_length: 1024, max_total_tokens: 1512,
          waiting_served_ratio: 1.2, max_batch_prefill_tokens: 2048, max_batch_total_tokens:
          None, max_waiting_tokens: 20, hostname: \"levlabs404b-aws-dolphin-2-5-mixtral-8x7b-696-69796f6f98-j6xmn\",
          port: 80, shard_uds_path: \"/tmp/text-generation-server\", master_addr:
          \"localhost\", master_port: 29500, huggingface_hub_cache: Some(\"/data\"),
          weights_cache_override: None, disable_custom_kernels: false, cuda_memory_fraction:
          1.0, rope_scaling: None, rope_factor: None, json_output: true, otlp_endpoint:
          None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None,
          ngrok: false, ngrok_authtoken: None, ngrok_edge: None, env: false }"},"target":"text_generation_launcher"}

          2023/12/15 10:04:10 ~ {"timestamp":"2023-12-15T15:04:10.264479Z","level":"INFO","fields":{"message":"Starting
          download process."},"target":"text_generation_launcher","span":{"name":"download"},"spans":[{"name":"download"}]}

          </code></pre>

          <p>I have the full log file here, as HuggingFace does not support uploading
          files to issues/conversations: <a rel="nofollow" href="https://file.io/TDOE1DBMwnVJ">https://file.io/TDOE1DBMwnVJ</a></p>

          <p>Please let me know if there is any more information I can provide to
          assist in this process. Thank you for your time and efforts into building
          these brilliant models!</p>

          '
        raw: "Hello, I saw this topic was related to one of the closed tickets, and\
          \ that the user did not provide adequate information. I wanted to share\
          \ my attempt to host it on the inference endpoint as I have been trying\
          \ to use this model.\r\n\r\nHere is an output:\r\n```\r\n2023/12/15 10:03:22\
          \ ~ {\"timestamp\":\"2023-12-15T15:03:22.451927Z\",\"level\":\"INFO\",\"\
          fields\":{\"message\":\"Successfully downloaded weights.\"},\"target\":\"\
          text_generation_launcher\",\"span\":{\"name\":\"download\"},\"spans\":[{\"\
          name\":\"download\"}]}\r\n2023/12/15 10:03:22 ~ {\"timestamp\":\"2023-12-15T15:03:22.452217Z\"\
          ,\"level\":\"INFO\",\"fields\":{\"message\":\"Starting shard\"},\"target\"\
          :\"text_generation_launcher\",\"span\":{\"rank\":1,\"name\":\"shard-manager\"\
          },\"spans\":[{\"rank\":1,\"name\":\"shard-manager\"}]}\r\n2023/12/15 10:03:22\
          \ ~ {\"timestamp\":\"2023-12-15T15:03:22.452186Z\",\"level\":\"INFO\",\"\
          fields\":{\"message\":\"Starting shard\"},\"target\":\"text_generation_launcher\"\
          ,\"span\":{\"rank\":0,\"name\":\"shard-manager\"},\"spans\":[{\"rank\":0,\"\
          name\":\"shard-manager\"}]}\r\n2023/12/15 10:03:22 ~ {\"timestamp\":\"2023-12-15T15:03:22.452239Z\"\
          ,\"level\":\"INFO\",\"fields\":{\"message\":\"Starting shard\"},\"target\"\
          :\"text_generation_launcher\",\"span\":{\"rank\":2,\"name\":\"shard-manager\"\
          },\"spans\":[{\"rank\":2,\"name\":\"shard-manager\"}]}\r\n2023/12/15 10:03:22\
          \ ~ {\"timestamp\":\"2023-12-15T15:03:22.452655Z\",\"level\":\"INFO\",\"\
          fields\":{\"message\":\"Starting shard\"},\"target\":\"text_generation_launcher\"\
          ,\"span\":{\"rank\":3,\"name\":\"shard-manager\"},\"spans\":[{\"rank\":3,\"\
          name\":\"shard-manager\"}]}\r\n2023/12/15 10:03:25 ~ {\"timestamp\":\"2023-12-15T15:03:25.517688Z\"\
          ,\"level\":\"WARN\",\"fields\":{\"message\":\"Unable to use Flash Attention\
          \ V2: GPU with CUDA capability 7 5 is not supported for Flash Attention\
          \ V2\\n\"},\"target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:25\
          \ ~ {\"timestamp\":\"2023-12-15T15:03:25.524783Z\",\"level\":\"WARN\",\"\
          fields\":{\"message\":\"Unable to use Flash Attention V2: GPU with CUDA\
          \ capability 7 5 is not supported for Flash Attention V2\\n\"},\"target\"\
          :\"text_generation_launcher\"}\r\n2023/12/15 10:03:25 ~ {\"timestamp\":\"\
          2023-12-15T15:03:25.526299Z\",\"level\":\"WARN\",\"fields\":{\"message\"\
          :\"Unable to use Flash Attention V2: GPU with CUDA capability 7 5 is not\
          \ supported for Flash Attention V2\\n\"},\"target\":\"text_generation_launcher\"\
          }\r\n2023/12/15 10:03:25 ~ {\"timestamp\":\"2023-12-15T15:03:25.529150Z\"\
          ,\"level\":\"WARN\",\"fields\":{\"message\":\"Unable to use Flash Attention\
          \ V2: GPU with CUDA capability 7 5 is not supported for Flash Attention\
          \ V2\\n\"},\"target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:25\
          \ ~ {\"timestamp\":\"2023-12-15T15:03:25.537123Z\",\"level\":\"WARN\",\"\
          fields\":{\"message\":\"Could not import Mistral model: Mistral model requires\
          \ flash attn v2\\n\"},\"target\":\"text_generation_launcher\"}\r\n2023/12/15\
          \ 10:03:25 ~ {\"timestamp\":\"2023-12-15T15:03:25.543898Z\",\"level\":\"\
          WARN\",\"fields\":{\"message\":\"Could not import Mistral model: Mistral\
          \ model requires flash attn v2\\n\"},\"target\":\"text_generation_launcher\"\
          }\r\n2023/12/15 10:03:25 ~ {\"timestamp\":\"2023-12-15T15:03:25.545572Z\"\
          ,\"level\":\"WARN\",\"fields\":{\"message\":\"Could not import Mistral model:\
          \ Mistral model requires flash attn v2\\n\"},\"target\":\"text_generation_launcher\"\
          }\r\n2023/12/15 10:03:25 ~ {\"timestamp\":\"2023-12-15T15:03:25.548302Z\"\
          ,\"level\":\"WARN\",\"fields\":{\"message\":\"Could not import Mistral model:\
          \ Mistral model requires flash attn v2\\n\"},\"target\":\"text_generation_launcher\"\
          }\r\n2023/12/15 10:03:25 ~ {\"timestamp\":\"2023-12-15T15:03:25.591579Z\"\
          ,\"level\":\"ERROR\",\"fields\":{\"message\":\"Error when initializing model\\\
          nTraceback (most recent call last):\\n  File \\\"/opt/conda/bin/text-generation-server\\\
          \", line 8, in <module>\\n    sys.exit(app())\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/main.py\\\
          \", line 311, in __call__\\n    return get_command(self)(*args, **kwargs)\\\
          n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line\
          \ 1157, in __call__\\n    return self.main(*args, **kwargs)\\n  File \\\"\
          /opt/conda/lib/python3.9/site-packages/typer/core.py\\\", line 778, in main\\\
          n    return _main(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/core.py\\\
          \", line 216, in _main\\n    rv = self.invoke(ctx)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\
          \", line 1688, in invoke\\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\\\
          n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line\
          \ 1434, in invoke\\n    return ctx.invoke(self.callback, **ctx.params)\\\
          n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line\
          \ 783, in invoke\\n    return __callback(*args, **kwargs)\\n  File \\\"\
          /opt/conda/lib/python3.9/site-packages/typer/main.py\\\", line 683, in wrapper\\\
          n    return callback(**use_params)  # type: ignore\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
          \", line 83, in serve\\n    server.serve(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 207, in serve\\n    asyncio.run(\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
          \", line 44, in run\\n    return loop.run_until_complete(main)\\n  File\
          \ \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\", line 634, in\
          \ run_until_complete\\n    self.run_forever()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
          \", line 601, in run_forever\\n    self._run_once()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
          \", line 1905, in _run_once\\n    handle._run()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/events.py\\\
          \", line 80, in _run\\n    self._context.run(self._callback, *self._args)\\\
          n> File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 159, in serve_inner\\n    model = get_model(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
          \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
          \ for AutoModel\\\")\\nValueError: sharded is not supported for AutoModel\\\
          n\"},\"target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:25 ~ {\"\
          timestamp\":\"2023-12-15T15:03:25.596882Z\",\"level\":\"ERROR\",\"fields\"\
          :{\"message\":\"Error when initializing model\\nTraceback (most recent call\
          \ last):\\n  File \\\"/opt/conda/bin/text-generation-server\\\", line 8,\
          \ in <module>\\n    sys.exit(app())\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/main.py\\\
          \", line 311, in __call__\\n    return get_command(self)(*args, **kwargs)\\\
          n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line\
          \ 1157, in __call__\\n    return self.main(*args, **kwargs)\\n  File \\\"\
          /opt/conda/lib/python3.9/site-packages/typer/core.py\\\", line 778, in main\\\
          n    return _main(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/core.py\\\
          \", line 216, in _main\\n    rv = self.invoke(ctx)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\
          \", line 1688, in invoke\\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\\\
          n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line\
          \ 1434, in invoke\\n    return ctx.invoke(self.callback, **ctx.params)\\\
          n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line\
          \ 783, in invoke\\n    return __callback(*args, **kwargs)\\n  File \\\"\
          /opt/conda/lib/python3.9/site-packages/typer/main.py\\\", line 683, in wrapper\\\
          n    return callback(**use_params)  # type: ignore\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
          \", line 83, in serve\\n    server.serve(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 207, in serve\\n    asyncio.run(\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
          \", line 44, in run\\n    return loop.run_until_complete(main)\\n  File\
          \ \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\", line 634, in\
          \ run_until_complete\\n    self.run_forever()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
          \", line 601, in run_forever\\n    self._run_once()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
          \", line 1905, in _run_once\\n    handle._run()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/events.py\\\
          \", line 80, in _run\\n    self._context.run(self._callback, *self._args)\\\
          n> File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 159, in serve_inner\\n    model = get_model(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
          \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
          \ for AutoModel\\\")\\nValueError: sharded is not supported for AutoModel\\\
          n\"},\"target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:25 ~ {\"\
          timestamp\":\"2023-12-15T15:03:25.598776Z\",\"level\":\"ERROR\",\"fields\"\
          :{\"message\":\"Error when initializing model\\nTraceback (most recent call\
          \ last):\\n  File \\\"/opt/conda/bin/text-generation-server\\\", line 8,\
          \ in <module>\\n    sys.exit(app())\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/main.py\\\
          \", line 311, in __call__\\n    return get_command(self)(*args, **kwargs)\\\
          n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line\
          \ 1157, in __call__\\n    return self.main(*args, **kwargs)\\n  File \\\"\
          /opt/conda/lib/python3.9/site-packages/typer/core.py\\\", line 778, in main\\\
          n    return _main(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/core.py\\\
          \", line 216, in _main\\n    rv = self.invoke(ctx)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\
          \", line 1688, in invoke\\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\\\
          n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line\
          \ 1434, in invoke\\n    return ctx.invoke(self.callback, **ctx.params)\\\
          n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line\
          \ 783, in invoke\\n    return __callback(*args, **kwargs)\\n  File \\\"\
          /opt/conda/lib/python3.9/site-packages/typer/main.py\\\", line 683, in wrapper\\\
          n    return callback(**use_params)  # type: ignore\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
          \", line 83, in serve\\n    server.serve(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 207, in serve\\n    asyncio.run(\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
          \", line 44, in run\\n    return loop.run_until_complete(main)\\n  File\
          \ \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\", line 634, in\
          \ run_until_complete\\n    self.run_forever()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
          \", line 601, in run_forever\\n    self._run_once()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
          \", line 1905, in _run_once\\n    handle._run()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/events.py\\\
          \", line 80, in _run\\n    self._context.run(self._callback, *self._args)\\\
          n> File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 159, in serve_inner\\n    model = get_model(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
          \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
          \ for AutoModel\\\")\\nValueError: sharded is not supported for AutoModel\\\
          n\"},\"target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:25 ~ {\"\
          timestamp\":\"2023-12-15T15:03:25.601267Z\",\"level\":\"ERROR\",\"fields\"\
          :{\"message\":\"Error when initializing model\\nTraceback (most recent call\
          \ last):\\n  File \\\"/opt/conda/bin/text-generation-server\\\", line 8,\
          \ in <module>\\n    sys.exit(app())\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/main.py\\\
          \", line 311, in __call__\\n    return get_command(self)(*args, **kwargs)\\\
          n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line\
          \ 1157, in __call__\\n    return self.main(*args, **kwargs)\\n  File \\\"\
          /opt/conda/lib/python3.9/site-packages/typer/core.py\\\", line 778, in main\\\
          n    return _main(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/core.py\\\
          \", line 216, in _main\\n    rv = self.invoke(ctx)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\
          \", line 1688, in invoke\\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\\\
          n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line\
          \ 1434, in invoke\\n    return ctx.invoke(self.callback, **ctx.params)\\\
          n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line\
          \ 783, in invoke\\n    return __callback(*args, **kwargs)\\n  File \\\"\
          /opt/conda/lib/python3.9/site-packages/typer/main.py\\\", line 683, in wrapper\\\
          n    return callback(**use_params)  # type: ignore\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
          \", line 83, in serve\\n    server.serve(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 207, in serve\\n    asyncio.run(\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
          \", line 44, in run\\n    return loop.run_until_complete(main)\\n  File\
          \ \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\", line 634, in\
          \ run_until_complete\\n    self.run_forever()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
          \", line 601, in run_forever\\n    self._run_once()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
          \", line 1905, in _run_once\\n    handle._run()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/events.py\\\
          \", line 80, in _run\\n    self._context.run(self._callback, *self._args)\\\
          n> File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 159, in serve_inner\\n    model = get_model(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
          \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
          \ for AutoModel\\\")\\nValueError: sharded is not supported for AutoModel\\\
          n\"},\"target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:26 ~ {\"\
          timestamp\":\"2023-12-15T15:03:26.157644Z\",\"level\":\"ERROR\",\"fields\"\
          :{\"message\":\"Shard complete standard error output:\\n\\nTraceback (most\
          \ recent call last):\\n\\n  File \\\"/opt/conda/bin/text-generation-server\\\
          \", line 8, in <module>\\n    sys.exit(app())\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
          \", line 83, in serve\\n    server.serve(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 207, in serve\\n    asyncio.run(\\n\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
          \", line 44, in run\\n    return loop.run_until_complete(main)\\n\\n  File\
          \ \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\", line 647, in\
          \ run_until_complete\\n    return future.result()\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 159, in serve_inner\\n    model = get_model(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
          \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
          \ for AutoModel\\\")\\n\\nValueError: sharded is not supported for AutoModel\\\
          n\"},\"target\":\"text_generation_launcher\",\"span\":{\"rank\":1,\"name\"\
          :\"shard-manager\"},\"spans\":[{\"rank\":1,\"name\":\"shard-manager\"}]}\r\
          \n2023/12/15 10:03:26 ~ {\"timestamp\":\"2023-12-15T15:03:26.157779Z\",\"\
          level\":\"ERROR\",\"fields\":{\"message\":\"Shard complete standard error\
          \ output:\\n\\nTraceback (most recent call last):\\n\\n  File \\\"/opt/conda/bin/text-generation-server\\\
          \", line 8, in <module>\\n    sys.exit(app())\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
          \", line 83, in serve\\n    server.serve(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 207, in serve\\n    asyncio.run(\\n\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
          \", line 44, in run\\n    return loop.run_until_complete(main)\\n\\n  File\
          \ \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\", line 647, in\
          \ run_until_complete\\n    return future.result()\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 159, in serve_inner\\n    model = get_model(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
          \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
          \ for AutoModel\\\")\\n\\nValueError: sharded is not supported for AutoModel\\\
          n\"},\"target\":\"text_generation_launcher\",\"span\":{\"rank\":2,\"name\"\
          :\"shard-manager\"},\"spans\":[{\"rank\":2,\"name\":\"shard-manager\"}]}\r\
          \n2023/12/15 10:03:26 ~ {\"timestamp\":\"2023-12-15T15:03:26.157415Z\",\"\
          level\":\"ERROR\",\"fields\":{\"message\":\"Shard complete standard error\
          \ output:\\n\\nTraceback (most recent call last):\\n\\n  File \\\"/opt/conda/bin/text-generation-server\\\
          \", line 8, in <module>\\n    sys.exit(app())\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
          \", line 83, in serve\\n    server.serve(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 207, in serve\\n    asyncio.run(\\n\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
          \", line 44, in run\\n    return loop.run_until_complete(main)\\n\\n  File\
          \ \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\", line 647, in\
          \ run_until_complete\\n    return future.result()\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 159, in serve_inner\\n    model = get_model(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
          \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
          \ for AutoModel\\\")\\n\\nValueError: sharded is not supported for AutoModel\\\
          n\"},\"target\":\"text_generation_launcher\",\"span\":{\"rank\":3,\"name\"\
          :\"shard-manager\"},\"spans\":[{\"rank\":3,\"name\":\"shard-manager\"}]}\r\
          \n2023/12/15 10:03:26 ~ {\"timestamp\":\"2023-12-15T15:03:26.256251Z\",\"\
          level\":\"ERROR\",\"fields\":{\"message\":\"Shard 3 failed to start\"},\"\
          target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:26 ~ {\"timestamp\"\
          :\"2023-12-15T15:03:26.256295Z\",\"level\":\"INFO\",\"fields\":{\"message\"\
          :\"Shutting down shards\"},\"target\":\"text_generation_launcher\"}\r\n\
          2023/12/15 10:03:26 ~ {\"timestamp\":\"2023-12-15T15:03:26.257847Z\",\"\
          level\":\"ERROR\",\"fields\":{\"message\":\"Shard complete standard error\
          \ output:\\n\\nTraceback (most recent call last):\\n\\n  File \\\"/opt/conda/bin/text-generation-server\\\
          \", line 8, in <module>\\n    sys.exit(app())\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
          \", line 83, in serve\\n    server.serve(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 207, in serve\\n    asyncio.run(\\n\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
          \", line 44, in run\\n    return loop.run_until_complete(main)\\n\\n  File\
          \ \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\", line 647, in\
          \ run_until_complete\\n    return future.result()\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
          \", line 159, in serve_inner\\n    model = get_model(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
          \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
          \ for AutoModel\\\")\\n\\nValueError: sharded is not supported for AutoModel\\\
          n\"},\"target\":\"text_generation_launcher\",\"span\":{\"rank\":0,\"name\"\
          :\"shard-manager\"},\"spans\":[{\"rank\":0,\"name\":\"shard-manager\"}]}\r\
          \n\r\n\r\n2023/12/15 10:03:26 ~ Error: ShardCannotStart\r\n2023/12/15 10:04:10\
          \ ~ {\"timestamp\":\"2023-12-15T15:04:10.264390Z\",\"level\":\"INFO\",\"\
          fields\":{\"message\":\"Sharding model on 4 processes\"},\"target\":\"text_generation_launcher\"\
          }\r\n2023/12/15 10:04:10 ~ {\"timestamp\":\"2023-12-15T15:04:10.264345Z\"\
          ,\"level\":\"INFO\",\"fields\":{\"message\":\"Args { model_id: \\\"/repository\\\
          \", revision: None, validation_workers: 2, sharded: None, num_shard: None,\
          \ quantize: Some(Bitsandbytes), dtype: None, trust_remote_code: false, max_concurrent_requests:\
          \ 128, max_best_of: 2, max_stop_sequences: 4, max_top_n_tokens: 5, max_input_length:\
          \ 1024, max_total_tokens: 1512, waiting_served_ratio: 1.2, max_batch_prefill_tokens:\
          \ 2048, max_batch_total_tokens: None, max_waiting_tokens: 20, hostname:\
          \ \\\"levlabs404b-aws-dolphin-2-5-mixtral-8x7b-696-69796f6f98-j6xmn\\\"\
          , port: 80, shard_uds_path: \\\"/tmp/text-generation-server\\\", master_addr:\
          \ \\\"localhost\\\", master_port: 29500, huggingface_hub_cache: Some(\\\"\
          /data\\\"), weights_cache_override: None, disable_custom_kernels: false,\
          \ cuda_memory_fraction: 1.0, rope_scaling: None, rope_factor: None, json_output:\
          \ true, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None,\
          \ watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_edge:\
          \ None, env: false }\"},\"target\":\"text_generation_launcher\"}\r\n2023/12/15\
          \ 10:04:10 ~ {\"timestamp\":\"2023-12-15T15:04:10.264479Z\",\"level\":\"\
          INFO\",\"fields\":{\"message\":\"Starting download process.\"},\"target\"\
          :\"text_generation_launcher\",\"span\":{\"name\":\"download\"},\"spans\"\
          :[{\"name\":\"download\"}]}\r\n```\r\n\r\nI have the full log file here,\
          \ as HuggingFace does not support uploading files to issues/conversations:\
          \ https://file.io/TDOE1DBMwnVJ\r\n\r\nPlease let me know if there is any\
          \ more information I can provide to assist in this process. Thank you for\
          \ your time and efforts into building these brilliant models!\r\n"
        updatedAt: '2023-12-15T15:22:40.119Z'
      numEdits: 0
      reactions: []
    id: 657c6f400a3297531f117d86
    type: comment
  author: Blevlabs
  content: "Hello, I saw this topic was related to one of the closed tickets, and\
    \ that the user did not provide adequate information. I wanted to share my attempt\
    \ to host it on the inference endpoint as I have been trying to use this model.\r\
    \n\r\nHere is an output:\r\n```\r\n2023/12/15 10:03:22 ~ {\"timestamp\":\"2023-12-15T15:03:22.451927Z\"\
    ,\"level\":\"INFO\",\"fields\":{\"message\":\"Successfully downloaded weights.\"\
    },\"target\":\"text_generation_launcher\",\"span\":{\"name\":\"download\"},\"\
    spans\":[{\"name\":\"download\"}]}\r\n2023/12/15 10:03:22 ~ {\"timestamp\":\"\
    2023-12-15T15:03:22.452217Z\",\"level\":\"INFO\",\"fields\":{\"message\":\"Starting\
    \ shard\"},\"target\":\"text_generation_launcher\",\"span\":{\"rank\":1,\"name\"\
    :\"shard-manager\"},\"spans\":[{\"rank\":1,\"name\":\"shard-manager\"}]}\r\n2023/12/15\
    \ 10:03:22 ~ {\"timestamp\":\"2023-12-15T15:03:22.452186Z\",\"level\":\"INFO\"\
    ,\"fields\":{\"message\":\"Starting shard\"},\"target\":\"text_generation_launcher\"\
    ,\"span\":{\"rank\":0,\"name\":\"shard-manager\"},\"spans\":[{\"rank\":0,\"name\"\
    :\"shard-manager\"}]}\r\n2023/12/15 10:03:22 ~ {\"timestamp\":\"2023-12-15T15:03:22.452239Z\"\
    ,\"level\":\"INFO\",\"fields\":{\"message\":\"Starting shard\"},\"target\":\"\
    text_generation_launcher\",\"span\":{\"rank\":2,\"name\":\"shard-manager\"},\"\
    spans\":[{\"rank\":2,\"name\":\"shard-manager\"}]}\r\n2023/12/15 10:03:22 ~ {\"\
    timestamp\":\"2023-12-15T15:03:22.452655Z\",\"level\":\"INFO\",\"fields\":{\"\
    message\":\"Starting shard\"},\"target\":\"text_generation_launcher\",\"span\"\
    :{\"rank\":3,\"name\":\"shard-manager\"},\"spans\":[{\"rank\":3,\"name\":\"shard-manager\"\
    }]}\r\n2023/12/15 10:03:25 ~ {\"timestamp\":\"2023-12-15T15:03:25.517688Z\",\"\
    level\":\"WARN\",\"fields\":{\"message\":\"Unable to use Flash Attention V2: GPU\
    \ with CUDA capability 7 5 is not supported for Flash Attention V2\\n\"},\"target\"\
    :\"text_generation_launcher\"}\r\n2023/12/15 10:03:25 ~ {\"timestamp\":\"2023-12-15T15:03:25.524783Z\"\
    ,\"level\":\"WARN\",\"fields\":{\"message\":\"Unable to use Flash Attention V2:\
    \ GPU with CUDA capability 7 5 is not supported for Flash Attention V2\\n\"},\"\
    target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:25 ~ {\"timestamp\"\
    :\"2023-12-15T15:03:25.526299Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"\
    Unable to use Flash Attention V2: GPU with CUDA capability 7 5 is not supported\
    \ for Flash Attention V2\\n\"},\"target\":\"text_generation_launcher\"}\r\n2023/12/15\
    \ 10:03:25 ~ {\"timestamp\":\"2023-12-15T15:03:25.529150Z\",\"level\":\"WARN\"\
    ,\"fields\":{\"message\":\"Unable to use Flash Attention V2: GPU with CUDA capability\
    \ 7 5 is not supported for Flash Attention V2\\n\"},\"target\":\"text_generation_launcher\"\
    }\r\n2023/12/15 10:03:25 ~ {\"timestamp\":\"2023-12-15T15:03:25.537123Z\",\"level\"\
    :\"WARN\",\"fields\":{\"message\":\"Could not import Mistral model: Mistral model\
    \ requires flash attn v2\\n\"},\"target\":\"text_generation_launcher\"}\r\n2023/12/15\
    \ 10:03:25 ~ {\"timestamp\":\"2023-12-15T15:03:25.543898Z\",\"level\":\"WARN\"\
    ,\"fields\":{\"message\":\"Could not import Mistral model: Mistral model requires\
    \ flash attn v2\\n\"},\"target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:25\
    \ ~ {\"timestamp\":\"2023-12-15T15:03:25.545572Z\",\"level\":\"WARN\",\"fields\"\
    :{\"message\":\"Could not import Mistral model: Mistral model requires flash attn\
    \ v2\\n\"},\"target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:25 ~ {\"\
    timestamp\":\"2023-12-15T15:03:25.548302Z\",\"level\":\"WARN\",\"fields\":{\"\
    message\":\"Could not import Mistral model: Mistral model requires flash attn\
    \ v2\\n\"},\"target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:25 ~ {\"\
    timestamp\":\"2023-12-15T15:03:25.591579Z\",\"level\":\"ERROR\",\"fields\":{\"\
    message\":\"Error when initializing model\\nTraceback (most recent call last):\\\
    n  File \\\"/opt/conda/bin/text-generation-server\\\", line 8, in <module>\\n\
    \    sys.exit(app())\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/main.py\\\
    \", line 311, in __call__\\n    return get_command(self)(*args, **kwargs)\\n \
    \ File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line 1157,\
    \ in __call__\\n    return self.main(*args, **kwargs)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/core.py\\\
    \", line 778, in main\\n    return _main(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/core.py\\\
    \", line 216, in _main\\n    rv = self.invoke(ctx)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\
    \", line 1688, in invoke\\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\\\
    n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line 1434,\
    \ in invoke\\n    return ctx.invoke(self.callback, **ctx.params)\\n  File \\\"\
    /opt/conda/lib/python3.9/site-packages/click/core.py\\\", line 783, in invoke\\\
    n    return __callback(*args, **kwargs)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/main.py\\\
    \", line 683, in wrapper\\n    return callback(**use_params)  # type: ignore\\\
    n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
    \", line 83, in serve\\n    server.serve(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 207, in serve\\n    asyncio.run(\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
    \", line 44, in run\\n    return loop.run_until_complete(main)\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
    \", line 634, in run_until_complete\\n    self.run_forever()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
    \", line 601, in run_forever\\n    self._run_once()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
    \", line 1905, in _run_once\\n    handle._run()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/events.py\\\
    \", line 80, in _run\\n    self._context.run(self._callback, *self._args)\\n>\
    \ File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 159, in serve_inner\\n    model = get_model(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
    \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
    \ for AutoModel\\\")\\nValueError: sharded is not supported for AutoModel\\n\"\
    },\"target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:25 ~ {\"timestamp\"\
    :\"2023-12-15T15:03:25.596882Z\",\"level\":\"ERROR\",\"fields\":{\"message\":\"\
    Error when initializing model\\nTraceback (most recent call last):\\n  File \\\
    \"/opt/conda/bin/text-generation-server\\\", line 8, in <module>\\n    sys.exit(app())\\\
    n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/main.py\\\", line 311,\
    \ in __call__\\n    return get_command(self)(*args, **kwargs)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\
    \", line 1157, in __call__\\n    return self.main(*args, **kwargs)\\n  File \\\
    \"/opt/conda/lib/python3.9/site-packages/typer/core.py\\\", line 778, in main\\\
    n    return _main(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/core.py\\\
    \", line 216, in _main\\n    rv = self.invoke(ctx)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\
    \", line 1688, in invoke\\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\\\
    n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line 1434,\
    \ in invoke\\n    return ctx.invoke(self.callback, **ctx.params)\\n  File \\\"\
    /opt/conda/lib/python3.9/site-packages/click/core.py\\\", line 783, in invoke\\\
    n    return __callback(*args, **kwargs)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/main.py\\\
    \", line 683, in wrapper\\n    return callback(**use_params)  # type: ignore\\\
    n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
    \", line 83, in serve\\n    server.serve(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 207, in serve\\n    asyncio.run(\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
    \", line 44, in run\\n    return loop.run_until_complete(main)\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
    \", line 634, in run_until_complete\\n    self.run_forever()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
    \", line 601, in run_forever\\n    self._run_once()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
    \", line 1905, in _run_once\\n    handle._run()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/events.py\\\
    \", line 80, in _run\\n    self._context.run(self._callback, *self._args)\\n>\
    \ File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 159, in serve_inner\\n    model = get_model(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
    \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
    \ for AutoModel\\\")\\nValueError: sharded is not supported for AutoModel\\n\"\
    },\"target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:25 ~ {\"timestamp\"\
    :\"2023-12-15T15:03:25.598776Z\",\"level\":\"ERROR\",\"fields\":{\"message\":\"\
    Error when initializing model\\nTraceback (most recent call last):\\n  File \\\
    \"/opt/conda/bin/text-generation-server\\\", line 8, in <module>\\n    sys.exit(app())\\\
    n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/main.py\\\", line 311,\
    \ in __call__\\n    return get_command(self)(*args, **kwargs)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\
    \", line 1157, in __call__\\n    return self.main(*args, **kwargs)\\n  File \\\
    \"/opt/conda/lib/python3.9/site-packages/typer/core.py\\\", line 778, in main\\\
    n    return _main(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/core.py\\\
    \", line 216, in _main\\n    rv = self.invoke(ctx)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\
    \", line 1688, in invoke\\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\\\
    n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line 1434,\
    \ in invoke\\n    return ctx.invoke(self.callback, **ctx.params)\\n  File \\\"\
    /opt/conda/lib/python3.9/site-packages/click/core.py\\\", line 783, in invoke\\\
    n    return __callback(*args, **kwargs)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/main.py\\\
    \", line 683, in wrapper\\n    return callback(**use_params)  # type: ignore\\\
    n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
    \", line 83, in serve\\n    server.serve(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 207, in serve\\n    asyncio.run(\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
    \", line 44, in run\\n    return loop.run_until_complete(main)\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
    \", line 634, in run_until_complete\\n    self.run_forever()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
    \", line 601, in run_forever\\n    self._run_once()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
    \", line 1905, in _run_once\\n    handle._run()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/events.py\\\
    \", line 80, in _run\\n    self._context.run(self._callback, *self._args)\\n>\
    \ File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 159, in serve_inner\\n    model = get_model(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
    \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
    \ for AutoModel\\\")\\nValueError: sharded is not supported for AutoModel\\n\"\
    },\"target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:25 ~ {\"timestamp\"\
    :\"2023-12-15T15:03:25.601267Z\",\"level\":\"ERROR\",\"fields\":{\"message\":\"\
    Error when initializing model\\nTraceback (most recent call last):\\n  File \\\
    \"/opt/conda/bin/text-generation-server\\\", line 8, in <module>\\n    sys.exit(app())\\\
    n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/main.py\\\", line 311,\
    \ in __call__\\n    return get_command(self)(*args, **kwargs)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\
    \", line 1157, in __call__\\n    return self.main(*args, **kwargs)\\n  File \\\
    \"/opt/conda/lib/python3.9/site-packages/typer/core.py\\\", line 778, in main\\\
    n    return _main(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/core.py\\\
    \", line 216, in _main\\n    rv = self.invoke(ctx)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\
    \", line 1688, in invoke\\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\\\
    n  File \\\"/opt/conda/lib/python3.9/site-packages/click/core.py\\\", line 1434,\
    \ in invoke\\n    return ctx.invoke(self.callback, **ctx.params)\\n  File \\\"\
    /opt/conda/lib/python3.9/site-packages/click/core.py\\\", line 783, in invoke\\\
    n    return __callback(*args, **kwargs)\\n  File \\\"/opt/conda/lib/python3.9/site-packages/typer/main.py\\\
    \", line 683, in wrapper\\n    return callback(**use_params)  # type: ignore\\\
    n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
    \", line 83, in serve\\n    server.serve(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 207, in serve\\n    asyncio.run(\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
    \", line 44, in run\\n    return loop.run_until_complete(main)\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
    \", line 634, in run_until_complete\\n    self.run_forever()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
    \", line 601, in run_forever\\n    self._run_once()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/base_events.py\\\
    \", line 1905, in _run_once\\n    handle._run()\\n  File \\\"/opt/conda/lib/python3.9/asyncio/events.py\\\
    \", line 80, in _run\\n    self._context.run(self._callback, *self._args)\\n>\
    \ File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 159, in serve_inner\\n    model = get_model(\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
    \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
    \ for AutoModel\\\")\\nValueError: sharded is not supported for AutoModel\\n\"\
    },\"target\":\"text_generation_launcher\"}\r\n2023/12/15 10:03:26 ~ {\"timestamp\"\
    :\"2023-12-15T15:03:26.157644Z\",\"level\":\"ERROR\",\"fields\":{\"message\":\"\
    Shard complete standard error output:\\n\\nTraceback (most recent call last):\\\
    n\\n  File \\\"/opt/conda/bin/text-generation-server\\\", line 8, in <module>\\\
    n    sys.exit(app())\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
    \", line 83, in serve\\n    server.serve(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 207, in serve\\n    asyncio.run(\\n\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
    \", line 44, in run\\n    return loop.run_until_complete(main)\\n\\n  File \\\"\
    /opt/conda/lib/python3.9/asyncio/base_events.py\\\", line 647, in run_until_complete\\\
    n    return future.result()\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 159, in serve_inner\\n    model = get_model(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
    \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
    \ for AutoModel\\\")\\n\\nValueError: sharded is not supported for AutoModel\\\
    n\"},\"target\":\"text_generation_launcher\",\"span\":{\"rank\":1,\"name\":\"\
    shard-manager\"},\"spans\":[{\"rank\":1,\"name\":\"shard-manager\"}]}\r\n2023/12/15\
    \ 10:03:26 ~ {\"timestamp\":\"2023-12-15T15:03:26.157779Z\",\"level\":\"ERROR\"\
    ,\"fields\":{\"message\":\"Shard complete standard error output:\\n\\nTraceback\
    \ (most recent call last):\\n\\n  File \\\"/opt/conda/bin/text-generation-server\\\
    \", line 8, in <module>\\n    sys.exit(app())\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
    \", line 83, in serve\\n    server.serve(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 207, in serve\\n    asyncio.run(\\n\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
    \", line 44, in run\\n    return loop.run_until_complete(main)\\n\\n  File \\\"\
    /opt/conda/lib/python3.9/asyncio/base_events.py\\\", line 647, in run_until_complete\\\
    n    return future.result()\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 159, in serve_inner\\n    model = get_model(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
    \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
    \ for AutoModel\\\")\\n\\nValueError: sharded is not supported for AutoModel\\\
    n\"},\"target\":\"text_generation_launcher\",\"span\":{\"rank\":2,\"name\":\"\
    shard-manager\"},\"spans\":[{\"rank\":2,\"name\":\"shard-manager\"}]}\r\n2023/12/15\
    \ 10:03:26 ~ {\"timestamp\":\"2023-12-15T15:03:26.157415Z\",\"level\":\"ERROR\"\
    ,\"fields\":{\"message\":\"Shard complete standard error output:\\n\\nTraceback\
    \ (most recent call last):\\n\\n  File \\\"/opt/conda/bin/text-generation-server\\\
    \", line 8, in <module>\\n    sys.exit(app())\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
    \", line 83, in serve\\n    server.serve(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 207, in serve\\n    asyncio.run(\\n\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
    \", line 44, in run\\n    return loop.run_until_complete(main)\\n\\n  File \\\"\
    /opt/conda/lib/python3.9/asyncio/base_events.py\\\", line 647, in run_until_complete\\\
    n    return future.result()\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 159, in serve_inner\\n    model = get_model(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
    \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
    \ for AutoModel\\\")\\n\\nValueError: sharded is not supported for AutoModel\\\
    n\"},\"target\":\"text_generation_launcher\",\"span\":{\"rank\":3,\"name\":\"\
    shard-manager\"},\"spans\":[{\"rank\":3,\"name\":\"shard-manager\"}]}\r\n2023/12/15\
    \ 10:03:26 ~ {\"timestamp\":\"2023-12-15T15:03:26.256251Z\",\"level\":\"ERROR\"\
    ,\"fields\":{\"message\":\"Shard 3 failed to start\"},\"target\":\"text_generation_launcher\"\
    }\r\n2023/12/15 10:03:26 ~ {\"timestamp\":\"2023-12-15T15:03:26.256295Z\",\"level\"\
    :\"INFO\",\"fields\":{\"message\":\"Shutting down shards\"},\"target\":\"text_generation_launcher\"\
    }\r\n2023/12/15 10:03:26 ~ {\"timestamp\":\"2023-12-15T15:03:26.257847Z\",\"level\"\
    :\"ERROR\",\"fields\":{\"message\":\"Shard complete standard error output:\\n\\\
    nTraceback (most recent call last):\\n\\n  File \\\"/opt/conda/bin/text-generation-server\\\
    \", line 8, in <module>\\n    sys.exit(app())\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\\\
    \", line 83, in serve\\n    server.serve(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 207, in serve\\n    asyncio.run(\\n\\n  File \\\"/opt/conda/lib/python3.9/asyncio/runners.py\\\
    \", line 44, in run\\n    return loop.run_until_complete(main)\\n\\n  File \\\"\
    /opt/conda/lib/python3.9/asyncio/base_events.py\\\", line 647, in run_until_complete\\\
    n    return future.result()\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\\\
    \", line 159, in serve_inner\\n    model = get_model(\\n\\n  File \\\"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\\\
    \", line 291, in get_model\\n    raise ValueError(\\\"sharded is not supported\
    \ for AutoModel\\\")\\n\\nValueError: sharded is not supported for AutoModel\\\
    n\"},\"target\":\"text_generation_launcher\",\"span\":{\"rank\":0,\"name\":\"\
    shard-manager\"},\"spans\":[{\"rank\":0,\"name\":\"shard-manager\"}]}\r\n\r\n\r\
    \n2023/12/15 10:03:26 ~ Error: ShardCannotStart\r\n2023/12/15 10:04:10 ~ {\"timestamp\"\
    :\"2023-12-15T15:04:10.264390Z\",\"level\":\"INFO\",\"fields\":{\"message\":\"\
    Sharding model on 4 processes\"},\"target\":\"text_generation_launcher\"}\r\n\
    2023/12/15 10:04:10 ~ {\"timestamp\":\"2023-12-15T15:04:10.264345Z\",\"level\"\
    :\"INFO\",\"fields\":{\"message\":\"Args { model_id: \\\"/repository\\\", revision:\
    \ None, validation_workers: 2, sharded: None, num_shard: None, quantize: Some(Bitsandbytes),\
    \ dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of:\
    \ 2, max_stop_sequences: 4, max_top_n_tokens: 5, max_input_length: 1024, max_total_tokens:\
    \ 1512, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 2048, max_batch_total_tokens:\
    \ None, max_waiting_tokens: 20, hostname: \\\"levlabs404b-aws-dolphin-2-5-mixtral-8x7b-696-69796f6f98-j6xmn\\\
    \", port: 80, shard_uds_path: \\\"/tmp/text-generation-server\\\", master_addr:\
    \ \\\"localhost\\\", master_port: 29500, huggingface_hub_cache: Some(\\\"/data\\\
    \"), weights_cache_override: None, disable_custom_kernels: false, cuda_memory_fraction:\
    \ 1.0, rope_scaling: None, rope_factor: None, json_output: true, otlp_endpoint:\
    \ None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok:\
    \ false, ngrok_authtoken: None, ngrok_edge: None, env: false }\"},\"target\":\"\
    text_generation_launcher\"}\r\n2023/12/15 10:04:10 ~ {\"timestamp\":\"2023-12-15T15:04:10.264479Z\"\
    ,\"level\":\"INFO\",\"fields\":{\"message\":\"Starting download process.\"},\"\
    target\":\"text_generation_launcher\",\"span\":{\"name\":\"download\"},\"spans\"\
    :[{\"name\":\"download\"}]}\r\n```\r\n\r\nI have the full log file here, as HuggingFace\
    \ does not support uploading files to issues/conversations: https://file.io/TDOE1DBMwnVJ\r\
    \n\r\nPlease let me know if there is any more information I can provide to assist\
    \ in this process. Thank you for your time and efforts into building these brilliant\
    \ models!\r\n"
  created_at: 2023-12-15 15:22:40+00:00
  edited: false
  hidden: false
  id: 657c6f400a3297531f117d86
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/53fe807d9ffaf2c23ac8a13756a2486b.svg
      fullname: Nick Durkee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ndurkee
      type: user
    createdAt: '2023-12-15T16:01:54.000Z'
    data:
      edited: false
      editors:
      - ndurkee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9459167122840881
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/53fe807d9ffaf2c23ac8a13756a2486b.svg
          fullname: Nick Durkee
          isHf: false
          isPro: false
          name: ndurkee
          type: user
        html: '<p>I wonder if this is related to my issue with vllm. The files might
          have to be in the safetensors format.</p>

          '
        raw: I wonder if this is related to my issue with vllm. The files might have
          to be in the safetensors format.
        updatedAt: '2023-12-15T16:01:54.476Z'
      numEdits: 0
      reactions: []
    id: 657c7872f0c583c9ef556299
    type: comment
  author: ndurkee
  content: I wonder if this is related to my issue with vllm. The files might have
    to be in the safetensors format.
  created_at: 2023-12-15 16:01:54+00:00
  edited: false
  hidden: false
  id: 657c7872f0c583c9ef556299
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
      fullname: Brayden Levangie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Blevlabs
      type: user
    createdAt: '2023-12-15T16:22:33.000Z'
    data:
      edited: false
      editors:
      - Blevlabs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8511185646057129
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
          fullname: Brayden Levangie
          isHf: false
          isPro: false
          name: Blevlabs
          type: user
        html: '<p>Perhaps. I am using the default configuration available. Plus, it
          looks like regular Mistral is functioning fine on Inference Endpoints, just
          not this fine-tuned variant</p>

          '
        raw: Perhaps. I am using the default configuration available. Plus, it looks
          like regular Mistral is functioning fine on Inference Endpoints, just not
          this fine-tuned variant
        updatedAt: '2023-12-15T16:22:33.133Z'
      numEdits: 0
      reactions: []
    id: 657c7d4961839fe2a85f20f9
    type: comment
  author: Blevlabs
  content: Perhaps. I am using the default configuration available. Plus, it looks
    like regular Mistral is functioning fine on Inference Endpoints, just not this
    fine-tuned variant
  created_at: 2023-12-15 16:22:33+00:00
  edited: false
  hidden: false
  id: 657c7d4961839fe2a85f20f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/13ce293ec58f74139fbe531199e322de.svg
      fullname: James
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: james1b
      type: user
    createdAt: '2023-12-15T16:38:33.000Z'
    data:
      edited: false
      editors:
      - james1b
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9932704567909241
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/13ce293ec58f74139fbe531199e322de.svg
          fullname: James
          isHf: false
          isPro: false
          name: james1b
          type: user
        html: '<p>I am also having exactly the same issue</p>

          '
        raw: I am also having exactly the same issue
        updatedAt: '2023-12-15T16:38:33.118Z'
      numEdits: 0
      reactions: []
    id: 657c81091a690c8e0587cee5
    type: comment
  author: james1b
  content: I am also having exactly the same issue
  created_at: 2023-12-15 16:38:33+00:00
  edited: false
  hidden: false
  id: 657c81091a690c8e0587cee5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7178b64adb2467a753266de4faf34ea.svg
      fullname: Yen Kim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Beta-Lee
      type: user
    createdAt: '2023-12-15T20:38:21.000Z'
    data:
      edited: false
      editors:
      - Beta-Lee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8178477883338928
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7178b64adb2467a753266de4faf34ea.svg
          fullname: Yen Kim
          isHf: false
          isPro: false
          name: Beta-Lee
          type: user
        html: '<p>Same issue here </p>

          '
        raw: 'Same issue here '
        updatedAt: '2023-12-15T20:38:21.019Z'
      numEdits: 0
      reactions: []
    id: 657cb93d49ec77d48e49b953
    type: comment
  author: Beta-Lee
  content: 'Same issue here '
  created_at: 2023-12-15 20:38:21+00:00
  edited: false
  hidden: false
  id: 657cb93d49ec77d48e49b953
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/646e53038cd73b4cf07a1c6a15a1a6a4.svg
      fullname: John McCluskey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: jmccluskey
      type: user
    createdAt: '2023-12-16T18:49:31.000Z'
    data:
      edited: false
      editors:
      - jmccluskey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8846815824508667
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/646e53038cd73b4cf07a1c6a15a1a6a4.svg
          fullname: John McCluskey
          isHf: false
          isPro: true
          name: jmccluskey
          type: user
        html: '<p>Ditto...  Tried to upgrade the endpoint to A100, and got this message:  Quota
          exceeded for p4de. Currently available: 0, requested: 1. Please contact
          us at <a rel="nofollow" href="mailto:api-enterprise@huggingface.co">api-enterprise@huggingface.co</a>
          to increase your quota.</p>

          <p>Looks like A100 endpoints are ALL in use.  </p>

          '
        raw: 'Ditto...  Tried to upgrade the endpoint to A100, and got this message:  Quota
          exceeded for p4de. Currently available: 0, requested: 1. Please contact
          us at api-enterprise@huggingface.co to increase your quota.


          Looks like A100 endpoints are ALL in use.  '
        updatedAt: '2023-12-16T18:49:31.272Z'
      numEdits: 0
      reactions: []
    id: 657df13bcec775bfe0cd9ab6
    type: comment
  author: jmccluskey
  content: 'Ditto...  Tried to upgrade the endpoint to A100, and got this message:  Quota
    exceeded for p4de. Currently available: 0, requested: 1. Please contact us at
    api-enterprise@huggingface.co to increase your quota.


    Looks like A100 endpoints are ALL in use.  '
  created_at: 2023-12-16 18:49:31+00:00
  edited: false
  hidden: false
  id: 657df13bcec775bfe0cd9ab6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
      fullname: Brayden Levangie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Blevlabs
      type: user
    createdAt: '2023-12-16T19:02:16.000Z'
    data:
      edited: false
      editors:
      - Blevlabs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.995514988899231
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
          fullname: Brayden Levangie
          isHf: false
          isPro: false
          name: Blevlabs
          type: user
        html: '<p>This is when I was trying to run it on an A100, I do not think it
          is related to that. I also tried different and no quantized configurations,
          same issue.</p>

          '
        raw: This is when I was trying to run it on an A100, I do not think it is
          related to that. I also tried different and no quantized configurations,
          same issue.
        updatedAt: '2023-12-16T19:02:16.199Z'
      numEdits: 0
      reactions: []
    id: 657df4388888ccb894d6abec
    type: comment
  author: Blevlabs
  content: This is when I was trying to run it on an A100, I do not think it is related
    to that. I also tried different and no quantized configurations, same issue.
  created_at: 2023-12-16 19:02:16+00:00
  edited: false
  hidden: false
  id: 657df4388888ccb894d6abec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
      fullname: Brayden Levangie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Blevlabs
      type: user
    createdAt: '2023-12-16T19:40:29.000Z'
    data:
      edited: false
      editors:
      - Blevlabs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9736514091491699
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77e3a26a1b604ccb5e4ac4ff4a5b5d81.svg
          fullname: Brayden Levangie
          isHf: false
          isPro: false
          name: Blevlabs
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ehartford&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ehartford\">@<span class=\"\
          underline\">ehartford</span></a></span>\n\n\t</span></span> Do you have\
          \ any thoughts here?</p>\n"
        raw: '@ehartford Do you have any thoughts here?'
        updatedAt: '2023-12-16T19:40:29.265Z'
      numEdits: 0
      reactions: []
    id: 657dfd2deaad53ff67a24cb8
    type: comment
  author: Blevlabs
  content: '@ehartford Do you have any thoughts here?'
  created_at: 2023-12-16 19:40:29+00:00
  edited: false
  hidden: false
  id: 657dfd2deaad53ff67a24cb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-12-16T21:09:11.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6311308145523071
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>Inference if not my area of expertise</p>

          '
        raw: Inference if not my area of expertise
        updatedAt: '2023-12-16T21:09:11.825Z'
      numEdits: 0
      reactions: []
    id: 657e11f7e1116d68e9953a15
    type: comment
  author: ehartford
  content: Inference if not my area of expertise
  created_at: 2023-12-16 21:09:11+00:00
  edited: false
  hidden: false
  id: 657e11f7e1116d68e9953a15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/646e53038cd73b4cf07a1c6a15a1a6a4.svg
      fullname: John McCluskey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: jmccluskey
      type: user
    createdAt: '2023-12-17T16:53:01.000Z'
    data:
      edited: false
      editors:
      - jmccluskey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9444616436958313
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/646e53038cd73b4cf07a1c6a15a1a6a4.svg
          fullname: John McCluskey
          isHf: false
          isPro: true
          name: jmccluskey
          type: user
        html: '<p>So I asked Grok the question:  "What version of CUDA does Flash
          Attention V2 require?" with this response:  "Flash Attention V2 requires
          CUDA 11.0 or later. However, a recent update to the library (Dec 13) has
          added an automatic fallback from v2 to v1 depth map library for older GPUs,
          allowing for better compatibility."   it also provided a link to this X
          posting:  <a rel="nofollow" href="https://twitter.com/polarization_yu/status/1734939306807157243">https://twitter.com/polarization_yu/status/1734939306807157243</a></p>

          <p>So my question now is pretty basic..   what AWS instances support CUDA
          11.0 or later?   The T4 seems to be at 7.5, which appears to be useless.  I
          suspect that the 4XA100 instance is at CUDA 11.0 or higher.   I''m thinking
          that this problem can be solved with a newer docker image for AWS, but it''s
          not clear how to do this.</p>

          '
        raw: 'So I asked Grok the question:  "What version of CUDA does Flash Attention
          V2 require?" with this response:  "Flash Attention V2 requires CUDA 11.0
          or later. However, a recent update to the library (Dec 13) has added an
          automatic fallback from v2 to v1 depth map library for older GPUs, allowing
          for better compatibility."   it also provided a link to this X posting:  https://twitter.com/polarization_yu/status/1734939306807157243


          So my question now is pretty basic..   what AWS instances support CUDA 11.0
          or later?   The T4 seems to be at 7.5, which appears to be useless.  I suspect
          that the 4XA100 instance is at CUDA 11.0 or higher.   I''m thinking that
          this problem can be solved with a newer docker image for AWS, but it''s
          not clear how to do this.'
        updatedAt: '2023-12-17T16:53:01.111Z'
      numEdits: 0
      reactions: []
    id: 657f276df010d76b6e5200b9
    type: comment
  author: jmccluskey
  content: 'So I asked Grok the question:  "What version of CUDA does Flash Attention
    V2 require?" with this response:  "Flash Attention V2 requires CUDA 11.0 or later.
    However, a recent update to the library (Dec 13) has added an automatic fallback
    from v2 to v1 depth map library for older GPUs, allowing for better compatibility."   it
    also provided a link to this X posting:  https://twitter.com/polarization_yu/status/1734939306807157243


    So my question now is pretty basic..   what AWS instances support CUDA 11.0 or
    later?   The T4 seems to be at 7.5, which appears to be useless.  I suspect that
    the 4XA100 instance is at CUDA 11.0 or higher.   I''m thinking that this problem
    can be solved with a newer docker image for AWS, but it''s not clear how to do
    this.'
  created_at: 2023-12-17 16:53:01+00:00
  edited: false
  hidden: false
  id: 657f276df010d76b6e5200b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-12-17T17:17:37.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9646662473678589
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>I doubt you will find help for aws here.</p>

          '
        raw: I doubt you will find help for aws here.
        updatedAt: '2023-12-17T17:17:37.866Z'
      numEdits: 0
      reactions: []
    id: 657f2d319a9e347d4bde1dc7
    type: comment
  author: ehartford
  content: I doubt you will find help for aws here.
  created_at: 2023-12-17 17:17:37+00:00
  edited: false
  hidden: false
  id: 657f2d319a9e347d4bde1dc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/646e53038cd73b4cf07a1c6a15a1a6a4.svg
      fullname: John McCluskey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: jmccluskey
      type: user
    createdAt: '2023-12-18T01:19:50.000Z'
    data:
      edited: false
      editors:
      - jmccluskey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9118543863296509
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/646e53038cd73b4cf07a1c6a15a1a6a4.svg
          fullname: John McCluskey
          isHf: false
          isPro: true
          name: jmccluskey
          type: user
        html: '<p>I tried a good half dozen different AWS container images that had
          newer versions of CUDA, using the custom container configuration.   Not
          a single one started properly, running into one error or another.    My
          advice is to avoid trying to create an inference endpoint until somebody
          figures out the right recipe to make this model run.      Avoid this model
          for now, unless you can run it locally on your own machine.</p>

          '
        raw: I tried a good half dozen different AWS container images that had newer
          versions of CUDA, using the custom container configuration.   Not a single
          one started properly, running into one error or another.    My advice is
          to avoid trying to create an inference endpoint until somebody figures out
          the right recipe to make this model run.      Avoid this model for now,
          unless you can run it locally on your own machine.
        updatedAt: '2023-12-18T01:19:50.172Z'
      numEdits: 0
      reactions: []
    id: 657f9e363e3b5bea665809b0
    type: comment
  author: jmccluskey
  content: I tried a good half dozen different AWS container images that had newer
    versions of CUDA, using the custom container configuration.   Not a single one
    started properly, running into one error or another.    My advice is to avoid
    trying to create an inference endpoint until somebody figures out the right recipe
    to make this model run.      Avoid this model for now, unless you can run it locally
    on your own machine.
  created_at: 2023-12-18 01:19:50+00:00
  edited: false
  hidden: false
  id: 657f9e363e3b5bea665809b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1db2be7c5254512546d5cbd26a478abf.svg
      fullname: waheb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: waheb00000005
      type: user
    createdAt: '2023-12-19T09:16:21.000Z'
    data:
      edited: true
      editors:
      - waheb00000005
      hidden: false
      identifiedLanguage:
        language: sah
        probability: 0.1989881843328476
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1db2be7c5254512546d5cbd26a478abf.svg
          fullname: waheb
          isHf: false
          isPro: false
          name: waheb00000005
          type: user
        html: "<p>023-12-19 10:09:13 INFO:Loading ehartford_dolphin-2.5-mixtral-8x7b...\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u258E                                               \
          \  | 3.30G /4.22G  41.7MiB/s<br>2023-12-19 10:09:13 ERROR:Failed to load\
          \ the model.\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u258A           | 4.01G /4.22G  62.5MiB/s<br>Traceback\
          \ (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A| 4.22G /4.22G\
          \  89.0MiB/s<br>pinokio\\api\\oobabooga.pinokio.git\\text-generation-webui\\\
          modules\\ui_model_menu.py\", line 209, in load_model_wrapper<br>    shared.model,\
          \ shared.tokenizer = load_model(shared.model_name, loader)<br>i\\pinokio\\\
          api\\oobabooga.pinokio.git\\text-generation-webui\\modules\\models.py\"\
          , line 85, in load_model<br>    output = load_func_map<a rel=\"nofollow\"\
          \ href=\"model_name\">loader</a><br>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>pinokio\\\
          api\\oobabooga.pinokio.git\\text-generation-webui\\modules\\models.py\"\
          , line 142, in huggingface_loader<br>    config = AutoConfig.from_pretrained(path_to_model,\
          \ trust_remote_code=params['trust_remote_code'])<br>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>\\\
          pinokio\\api\\oobabooga.pinokio.git\\text-generation-webui\\installer_files\\\
          env\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\"\
          , line 1064, in from_pretrained<br>    config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]<br>                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^</p>\n\
          <p>\\pinokio\\api\\oobabooga.pinokio.git\\text-generation-webui\\installer_files\\\
          env\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\"\
          , line 761, in <strong>getitem</strong><br>    raise KeyError(key)<br>KeyError:\
          \ 'mixtral'</p>\n<p>any body knows what is the issue ?</p>\n"
        raw: "023-12-19 10:09:13 INFO:Loading ehartford_dolphin-2.5-mixtral-8x7b...\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u258E                                               \
          \  | 3.30G /4.22G  41.7MiB/s\n2023-12-19 10:09:13 ERROR:Failed to load the\
          \ model.\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u258A           | 4.01G /4.22G  62.5MiB/s\nTraceback (most\
          \ recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A| 4.22G /4.22G  89.0MiB/s\n\
          pinokio\\api\\oobabooga.pinokio.git\\text-generation-webui\\modules\\ui_model_menu.py\"\
          , line 209, in load_model_wrapper\n    shared.model, shared.tokenizer =\
          \ load_model(shared.model_name, loader)\ni\\pinokio\\api\\oobabooga.pinokio.git\\\
          text-generation-webui\\modules\\models.py\", line 85, in load_model\n  \
          \  output = load_func_map[loader](model_name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          pinokio\\api\\oobabooga.pinokio.git\\text-generation-webui\\modules\\models.py\"\
          , line 142, in huggingface_loader\n    config = AutoConfig.from_pretrained(path_to_model,\
          \ trust_remote_code=params['trust_remote_code'])\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \\pinokio\\api\\oobabooga.pinokio.git\\text-generation-webui\\installer_files\\\
          env\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\"\
          , line 1064, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]\n                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \n\\pinokio\\api\\oobabooga.pinokio.git\\text-generation-webui\\installer_files\\\
          env\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\"\
          , line 761, in __getitem__\n    raise KeyError(key)\nKeyError: 'mixtral'\n\
          \n\nany body knows what is the issue ?"
        updatedAt: '2023-12-19T09:17:32.876Z'
      numEdits: 1
      reactions: []
    id: 65815f65a2284a018e628860
    type: comment
  author: waheb00000005
  content: "023-12-19 10:09:13 INFO:Loading ehartford_dolphin-2.5-mixtral-8x7b...\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258E                             \
    \                    | 3.30G /4.22G  41.7MiB/s\n2023-12-19 10:09:13 ERROR:Failed\
    \ to load the model.\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u258A           | 4.01G /4.22G  62.5MiB/s\nTraceback (most\
    \ recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u258A| 4.22G /4.22G  89.0MiB/s\npinokio\\api\\\
    oobabooga.pinokio.git\\text-generation-webui\\modules\\ui_model_menu.py\", line\
    \ 209, in load_model_wrapper\n    shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\ni\\pinokio\\api\\oobabooga.pinokio.git\\text-generation-webui\\modules\\\
    models.py\", line 85, in load_model\n    output = load_func_map[loader](model_name)\n\
    \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npinokio\\api\\oobabooga.pinokio.git\\\
    text-generation-webui\\modules\\models.py\", line 142, in huggingface_loader\n\
    \    config = AutoConfig.from_pretrained(path_to_model, trust_remote_code=params['trust_remote_code'])\n\
    \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \\pinokio\\api\\oobabooga.pinokio.git\\text-generation-webui\\installer_files\\\
    env\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\",\
    \ line 1064, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
    model_type\"]]\n                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \n\\pinokio\\api\\oobabooga.pinokio.git\\text-generation-webui\\installer_files\\\
    env\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\",\
    \ line 761, in __getitem__\n    raise KeyError(key)\nKeyError: 'mixtral'\n\n\n\
    any body knows what is the issue ?"
  created_at: 2023-12-19 09:16:21+00:00
  edited: true
  hidden: false
  id: 65815f65a2284a018e628860
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/646e53038cd73b4cf07a1c6a15a1a6a4.svg
      fullname: John McCluskey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: jmccluskey
      type: user
    createdAt: '2023-12-20T03:16:24.000Z'
    data:
      edited: false
      editors:
      - jmccluskey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.503381073474884
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/646e53038cd73b4cf07a1c6a15a1a6a4.svg
          fullname: John McCluskey
          isHf: false
          isPro: true
          name: jmccluskey
          type: user
        html: '<p>Tried again to start Dolphin on an A100 inference endpoint, which
          DOES support Flash Attention V2....   But it''s still NO GOOD.    Sharding
          manager fails now.</p>

          <p>67bb6bbb9fn7r4s 2023-12-20T02:36:52.737Z {"timestamp":"2023-12-20T02:36:52.736867Z","level":"ERROR","fields":{"message":"Error
          when initializing model\nTraceback (most recent call last):\n  File "/opt/conda/bin/text-generation-server",
          line 8, in \n    sys.exit(app())\n  File "/opt/conda/lib/python3.9/site-packages/typer/main.py",
          line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n  File
          "/opt/conda/lib/python3.9/site-packages/click/core.py", line 1157, in __call__\n    return
          self.main(*args, **kwargs)\n  File "/opt/conda/lib/python3.9/site-packages/typer/core.py",
          line 778, in main\n    return _main(\n  File "/opt/conda/lib/python3.9/site-packages/typer/core.py",
          line 216, in _main\n    rv = self.invoke(ctx)\n  File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File
          "/opt/conda/lib/python3.9/site-packages/click/core.py", line 1434, in invoke\n    return
          ctx.invoke(self.callback, **ctx.params)\n  File "/opt/conda/lib/python3.9/site-packages/click/core.py",
          line 783, in invoke\n    return __callback(*args, **kwargs)\n  File "/opt/conda/lib/python3.9/site-packages/typer/main.py",
          line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n  File
          "/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py",
          line 83, in serve\n    server.serve(\n  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 207, in serve\n    asyncio.run(\n  File "/opt/conda/lib/python3.9/asyncio/runners.py",
          line 44, in run\n    return loop.run_until_complete(main)\n  File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 634, in run_until_complete\n    self.run_forever()\n  File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 601, in run_forever\n    self._run_once()\n  File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 1905, in _run_once\n    handle._run()\n  File "/opt/conda/lib/python3.9/asyncio/events.py",
          line 80, in _run\n    self._context.run(self._callback, *self._args)\n&gt;
          File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 159, in serve_inner\n    model = get_model(\n  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/<strong>init</strong>.py",
          line 336, in get_model\n    raise ValueError(f"Unsupported model type {model_type}")\nValueError:
          Unsupported model type mixtral\n"},"target":"text_generation_launcher"}</p>

          <p>67bb6bbb9fn7r4s 2023-12-20T02:36:53.230Z {"timestamp":"2023-12-20T02:36:53.230152Z","level":"ERROR","fields":{"message":"Shard
          complete standard error output:\n\nTraceback (most recent call last):\n\n  File
          "/opt/conda/bin/text-generation-server", line 8, in \n    sys.exit(app())\n\n  File
          "/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py",
          line 83, in serve\n    server.serve(\n\n  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 207, in serve\n    asyncio.run(\n\n  File "/opt/conda/lib/python3.9/asyncio/runners.py",
          line 44, in run\n    return loop.run_until_complete(main)\n\n  File "/opt/conda/lib/python3.9/asyncio/base_events.py",
          line 647, in run_until_complete\n    return future.result()\n\n  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py",
          line 159, in serve_inner\n    model = get_model(\n\n  File "/opt/conda/lib/python3.9/site-packages/text_generation_server/models/<strong>init</strong>.py",
          line 336, in get_model\n    raise ValueError(f"Unsupported model type {model_type}")\n\nValueError:
          Unsupported model type mixtral\n"},"target":"text_generation_launcher","span":{"rank":0,"name":"shard-manager"},"spans":[{"rank":0,"name":"shard-manager"}]}<br>67bb6bbb9fn7r4s
          2023-12-20T02:36:53.328Z {"timestamp":"2023-12-20T02:36:53.328062Z","level":"INFO","fields":{"message":"Shutting
          down shards"},"target":"text_generation_launcher"}<br>67bb6bbb9fn7r4s 2023-12-20T02:36:53.328Z
          Error: ShardCannotStart<br>67bb6bbb9fn7r4s 2023-12-20T02:36:53.328Z {"timestamp":"2023-12-20T02:36:53.328003Z","level":"ERROR","fields":{"message":"Shard
          0 failed to start"},"target":"text_generation_launcher"}</p>

          '
        raw: 'Tried again to start Dolphin on an A100 inference endpoint, which DOES
          support Flash Attention V2....   But it''s still NO GOOD.    Sharding manager
          fails now.


          67bb6bbb9fn7r4s 2023-12-20T02:36:52.737Z {"timestamp":"2023-12-20T02:36:52.736867Z","level":"ERROR","fields":{"message":"Error
          when initializing model\nTraceback (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\",
          line 8, in <module>\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
          line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n  File
          \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in
          __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
          line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
          line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
          line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File
          \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434, in
          invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
          line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
          line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n  File
          \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
          line 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
          line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\",
          line 80, in _run\n    self._context.run(self._callback, *self._args)\n>
          File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
          line 336, in get_model\n    raise ValueError(f\"Unsupported model type {model_type}\")\nValueError:
          Unsupported model type mixtral\n"},"target":"text_generation_launcher"}


          67bb6bbb9fn7r4s 2023-12-20T02:36:53.230Z {"timestamp":"2023-12-20T02:36:53.230152Z","level":"ERROR","fields":{"message":"Shard
          complete standard error output:\n\nTraceback (most recent call last):\n\n  File
          \"/opt/conda/bin/text-generation-server\", line 8, in <module>\n    sys.exit(app())\n\n  File
          \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\",
          line 83, in serve\n    server.serve(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 207, in serve\n    asyncio.run(\n\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
          line 44, in run\n    return loop.run_until_complete(main)\n\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
          line 647, in run_until_complete\n    return future.result()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
          line 159, in serve_inner\n    model = get_model(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
          line 336, in get_model\n    raise ValueError(f\"Unsupported model type {model_type}\")\n\nValueError:
          Unsupported model type mixtral\n"},"target":"text_generation_launcher","span":{"rank":0,"name":"shard-manager"},"spans":[{"rank":0,"name":"shard-manager"}]}

          67bb6bbb9fn7r4s 2023-12-20T02:36:53.328Z {"timestamp":"2023-12-20T02:36:53.328062Z","level":"INFO","fields":{"message":"Shutting
          down shards"},"target":"text_generation_launcher"}

          67bb6bbb9fn7r4s 2023-12-20T02:36:53.328Z Error: ShardCannotStart

          67bb6bbb9fn7r4s 2023-12-20T02:36:53.328Z {"timestamp":"2023-12-20T02:36:53.328003Z","level":"ERROR","fields":{"message":"Shard
          0 failed to start"},"target":"text_generation_launcher"}'
        updatedAt: '2023-12-20T03:16:24.045Z'
      numEdits: 0
      reactions: []
    id: 65825c88253cc8df6959eb2d
    type: comment
  author: jmccluskey
  content: 'Tried again to start Dolphin on an A100 inference endpoint, which DOES
    support Flash Attention V2....   But it''s still NO GOOD.    Sharding manager
    fails now.


    67bb6bbb9fn7r4s 2023-12-20T02:36:52.737Z {"timestamp":"2023-12-20T02:36:52.736867Z","level":"ERROR","fields":{"message":"Error
    when initializing model\nTraceback (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\",
    line 8, in <module>\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
    line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
    line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
    line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\",
    line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
    line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File
    \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434, in invoke\n    return
    ctx.invoke(self.callback, **ctx.params)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\",
    line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\",
    line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n  File
    \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\", line
    83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
    line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
    line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
    line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
    line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
    line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\",
    line 80, in _run\n    self._context.run(self._callback, *self._args)\n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
    line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
    line 336, in get_model\n    raise ValueError(f\"Unsupported model type {model_type}\")\nValueError:
    Unsupported model type mixtral\n"},"target":"text_generation_launcher"}


    67bb6bbb9fn7r4s 2023-12-20T02:36:53.230Z {"timestamp":"2023-12-20T02:36:53.230152Z","level":"ERROR","fields":{"message":"Shard
    complete standard error output:\n\nTraceback (most recent call last):\n\n  File
    \"/opt/conda/bin/text-generation-server\", line 8, in <module>\n    sys.exit(app())\n\n  File
    \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\", line
    83, in serve\n    server.serve(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
    line 207, in serve\n    asyncio.run(\n\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\",
    line 44, in run\n    return loop.run_until_complete(main)\n\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\",
    line 647, in run_until_complete\n    return future.result()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\",
    line 159, in serve_inner\n    model = get_model(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\",
    line 336, in get_model\n    raise ValueError(f\"Unsupported model type {model_type}\")\n\nValueError:
    Unsupported model type mixtral\n"},"target":"text_generation_launcher","span":{"rank":0,"name":"shard-manager"},"spans":[{"rank":0,"name":"shard-manager"}]}

    67bb6bbb9fn7r4s 2023-12-20T02:36:53.328Z {"timestamp":"2023-12-20T02:36:53.328062Z","level":"INFO","fields":{"message":"Shutting
    down shards"},"target":"text_generation_launcher"}

    67bb6bbb9fn7r4s 2023-12-20T02:36:53.328Z Error: ShardCannotStart

    67bb6bbb9fn7r4s 2023-12-20T02:36:53.328Z {"timestamp":"2023-12-20T02:36:53.328003Z","level":"ERROR","fields":{"message":"Shard
    0 failed to start"},"target":"text_generation_launcher"}'
  created_at: 2023-12-20 03:16:24+00:00
  edited: false
  hidden: false
  id: 65825c88253cc8df6959eb2d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: cognitivecomputations/dolphin-2.5-mixtral-8x7b
repo_type: model
status: open
target_branch: null
title: HuggingFace Inference Endpoints Issue (Detailed Information)
