!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ywlee88
conflicting_files: null
created_at: 2024-01-23 01:12:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641b9dd5f902cc42730f6067/MuDOmsEUHralcoVGmACYG.jpeg?w=200&h=200&f=face
      fullname: Youngwan Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ywlee88
      type: user
    createdAt: '2024-01-23T01:12:05.000Z'
    data:
      edited: false
      editors:
      - ywlee88
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9151175022125244
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641b9dd5f902cc42730f6067/MuDOmsEUHralcoVGmACYG.jpeg?w=200&h=200&f=face
          fullname: Youngwan Lee
          isHf: false
          isPro: false
          name: ywlee88
          type: user
        html: '<p>Hi,</p>

          <p>I''m impressed by your amazing work.</p>

          <p>Could you describe the training details (e.g., batch size, lr, scheduler,
          etc) for lcm-lora-sdxl model?</p>

          <p>When I tried to train lcm-lora-sdxl model with the official diffusers''s
          <a rel="nofollow" href="https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_lora_sdxl.py">training
          script</a>, the intermediate validation result images were not as good as
          yours.</p>

          <p>Thanks in advance.</p>

          '
        raw: "Hi,\r\n\r\nI'm impressed by your amazing work.\r\n\r\nCould you describe\
          \ the training details (e.g., batch size, lr, scheduler, etc) for lcm-lora-sdxl\
          \ model?\r\n\r\nWhen I tried to train lcm-lora-sdxl model with the official\
          \ diffusers's [training script](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_lora_sdxl.py),\
          \ the intermediate validation result images were not as good as yours.\r\
          \n\r\nThanks in advance."
        updatedAt: '2024-01-23T01:12:05.860Z'
      numEdits: 0
      reactions: []
    id: 65af12653e876a6389662658
    type: comment
  author: ywlee88
  content: "Hi,\r\n\r\nI'm impressed by your amazing work.\r\n\r\nCould you describe\
    \ the training details (e.g., batch size, lr, scheduler, etc) for lcm-lora-sdxl\
    \ model?\r\n\r\nWhen I tried to train lcm-lora-sdxl model with the official diffusers's\
    \ [training script](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/train_lcm_distill_lora_sdxl.py),\
    \ the intermediate validation result images were not as good as yours.\r\n\r\n\
    Thanks in advance."
  created_at: 2024-01-23 01:12:05+00:00
  edited: false
  hidden: false
  id: 65af12653e876a6389662658
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg?w=200&h=200&f=face
      fullname: Sayak Paul
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sayakpaul
      type: user
    createdAt: '2024-01-23T01:13:18.000Z'
    data:
      edited: false
      editors:
      - sayakpaul
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8822668194770813
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg?w=200&h=200&f=face
          fullname: Sayak Paul
          isHf: true
          isPro: false
          name: sayakpaul
          type: user
        html: '<p>Did you try the exact same training setup? Dataset, hyperparameters,
          etc?</p>

          '
        raw: Did you try the exact same training setup? Dataset, hyperparameters,
          etc?
        updatedAt: '2024-01-23T01:13:18.316Z'
      numEdits: 0
      reactions: []
    id: 65af12ae617ea3d2fe5372f2
    type: comment
  author: sayakpaul
  content: Did you try the exact same training setup? Dataset, hyperparameters, etc?
  created_at: 2024-01-23 01:13:18+00:00
  edited: false
  hidden: false
  id: 65af12ae617ea3d2fe5372f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641b9dd5f902cc42730f6067/MuDOmsEUHralcoVGmACYG.jpeg?w=200&h=200&f=face
      fullname: Youngwan Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ywlee88
      type: user
    createdAt: '2024-01-23T05:02:47.000Z'
    data:
      edited: true
      editors:
      - ywlee88
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.28101393580436707
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641b9dd5f902cc42730f6067/MuDOmsEUHralcoVGmACYG.jpeg?w=200&h=200&f=face
          fullname: Youngwan Lee
          isHf: false
          isPro: false
          name: ywlee88
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sayakpaul&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sayakpaul\">@<span class=\"\
          underline\">sayakpaul</span></a></span>\n\n\t</span></span> </p>\n<p>Thank\
          \ you for your quick response.</p>\n<p>Yes, except for the training data.\
          \ </p>\n<p>I used a subset of laion-aesthetic dataset (11K text-image pairs)\
          \ provided by <a rel=\"nofollow\" href=\"https://github.com/Nota-NetsPresso/BK-SDM\"\
          >BK-SDM</a>. </p>\n<p>I shared validated generation images at 700 iterations.</p>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/641b9dd5f902cc42730f6067/a8ORMqn49TMjJ9452OvVE.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/641b9dd5f902cc42730f6067/a8ORMqn49TMjJ9452OvVE.png\"\
          ></a></p>\n<p>This is hyper-params:</p>\n<blockquote>\n<p>--train_data_dir=./data/laion_aes/preprocessed_11k\
          \ --pretrained_teacher_model=stabilityai/stable-diffusion-xl-base-1.0 --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix\
          \ --output_dir=./results/TOY_LCM_LORA_LAION/lcm_lora_sdxl_base_24x1x1_lr_1e-4\
          \ --tracker_project_name=TOY_LCM_LORA_LAION --tracker_output_name=lcm_lora_sdxl_base_24x1x1_lr_1e-4\
          \ --mixed_precision=fp16 --resolution=1024 --train_batch_size=24 --gradient_accumulation_steps=1\
          \ --gradient_checkpointing --use_8bit_adam --lora_rank=64 --learning_rate=1e-4\
          \ --report_to=wandb --lr_scheduler=constant --lr_warmup_steps=0 --max_train_steps=100000\
          \ --checkpointing_steps=2000 --validation_steps=20 --seed=0 --report_to=wandb</p>\n\
          </blockquote>\n"
        raw: "@sayakpaul \n\nThank you for your quick response.\n\nYes, except for\
          \ the training data. \n\nI used a subset of laion-aesthetic dataset (11K\
          \ text-image pairs) provided by [BK-SDM](https://github.com/Nota-NetsPresso/BK-SDM).\
          \ \n\nI shared validated generation images at 700 iterations.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b9dd5f902cc42730f6067/a8ORMqn49TMjJ9452OvVE.png)\n\
          \n\nThis is hyper-params:\n> --train_data_dir=./data/laion_aes/preprocessed_11k\
          \ --pretrained_teacher_model=stabilityai/stable-diffusion-xl-base-1.0 --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix\
          \ --output_dir=./results/TOY_LCM_LORA_LAION/lcm_lora_sdxl_base_24x1x1_lr_1e-4\
          \ --tracker_project_name=TOY_LCM_LORA_LAION --tracker_output_name=lcm_lora_sdxl_base_24x1x1_lr_1e-4\
          \ --mixed_precision=fp16 --resolution=1024 --train_batch_size=24 --gradient_accumulation_steps=1\
          \ --gradient_checkpointing --use_8bit_adam --lora_rank=64 --learning_rate=1e-4\
          \ --report_to=wandb --lr_scheduler=constant --lr_warmup_steps=0 --max_train_steps=100000\
          \ --checkpointing_steps=2000 --validation_steps=20 --seed=0 --report_to=wandb\n"
        updatedAt: '2024-01-23T06:04:43.536Z'
      numEdits: 3
      reactions: []
    id: 65af48775f62b76444f95eed
    type: comment
  author: ywlee88
  content: "@sayakpaul \n\nThank you for your quick response.\n\nYes, except for the\
    \ training data. \n\nI used a subset of laion-aesthetic dataset (11K text-image\
    \ pairs) provided by [BK-SDM](https://github.com/Nota-NetsPresso/BK-SDM). \n\n\
    I shared validated generation images at 700 iterations.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b9dd5f902cc42730f6067/a8ORMqn49TMjJ9452OvVE.png)\n\
    \n\nThis is hyper-params:\n> --train_data_dir=./data/laion_aes/preprocessed_11k\
    \ --pretrained_teacher_model=stabilityai/stable-diffusion-xl-base-1.0 --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix\
    \ --output_dir=./results/TOY_LCM_LORA_LAION/lcm_lora_sdxl_base_24x1x1_lr_1e-4\
    \ --tracker_project_name=TOY_LCM_LORA_LAION --tracker_output_name=lcm_lora_sdxl_base_24x1x1_lr_1e-4\
    \ --mixed_precision=fp16 --resolution=1024 --train_batch_size=24 --gradient_accumulation_steps=1\
    \ --gradient_checkpointing --use_8bit_adam --lora_rank=64 --learning_rate=1e-4\
    \ --report_to=wandb --lr_scheduler=constant --lr_warmup_steps=0 --max_train_steps=100000\
    \ --checkpointing_steps=2000 --validation_steps=20 --seed=0 --report_to=wandb\n"
  created_at: 2024-01-23 05:02:47+00:00
  edited: true
  hidden: false
  id: 65af48775f62b76444f95eed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641b9dd5f902cc42730f6067/MuDOmsEUHralcoVGmACYG.jpeg?w=200&h=200&f=face
      fullname: Youngwan Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ywlee88
      type: user
    createdAt: '2024-01-23T06:12:33.000Z'
    data:
      edited: true
      editors:
      - ywlee88
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7806186079978943
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641b9dd5f902cc42730f6067/MuDOmsEUHralcoVGmACYG.jpeg?w=200&h=200&f=face
          fullname: Youngwan Lee
          isHf: false
          isPro: false
          name: ywlee88
          type: user
        html: "<p>I have another question.</p>\n<p>Could you let me know what data\
          \ is used for training <a href=\"https://huggingface.co/latent-consistency/lcm-lora-ssd-1b\"\
          >lcm-lora-ssd-1b</a> model and <a href=\"https://huggingface.co/latent-consistency/lcm-lora-sdxl\"\
          >lcm-lora-sdxl</a>?</p>\n<p>When I generated some samples, the result of\
          \ lcm-lora-ssd-1b showed better quality than that of lcm-lora-sdxl.</p>\n\
          <p>I wonder if this difference in generation quality is caused by differences\
          \ in the data used for training.</p>\n<p>For the sake of the community,\
          \ it would be very helpful if you could share the training details of the\
          \ <code>lcm-lora-sdxl</code> and <code>lcm-lora-ssd-1b</code> models.</p>\n\
          <p>In my case, I'm trying to create an lcm-lora version of the <a href=\"\
          https://huggingface.co/etri-vilab/koala-700m-llava-cap\">koala</a> model,\
          \ which is a lightweight T2I model like ssd-1b.</p>\n<p>Thanks in advance.</p>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/641b9dd5f902cc42730f6067/sQq09EVqdcdsRxAgA8-YQ.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/641b9dd5f902cc42730f6067/sQq09EVqdcdsRxAgA8-YQ.png\"\
          ></a></p>\n<p>For <code>lcm-lora-ssd-1b</code>:</p>\n<pre><code class=\"\
          language-python\">model_id = <span class=\"hljs-string\">\"segmind/SSD-1B\"\
          </span>\nadapter_id = <span class=\"hljs-string\">\"latent-consistency/lcm-lora-ssd-1b\"\
          </span>\n\npipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16,\
          \ variant=<span class=\"hljs-string\">\"fp16\"</span>)\npipe.scheduler =\
          \ LCMScheduler.from_config(pipe.scheduler.config)\npipe.to(<span class=\"\
          hljs-string\">\"cuda\"</span>)\n\n<span class=\"hljs-comment\"># load and\
          \ fuse lcm lora</span>\npipe.load_lora_weights(adapter_id)\npipe.fuse_lora()\n\
          \nprompt = <span class=\"hljs-string\">\"Portrait photo of a standing girl,\
          \ photograph, golden hair, depth of field, moody light, golden hour, centered,\
          \ extremely detailed, award winning photography, realistic.\"</span>\nimage\
          \ = pipe(prompt=prompt, num_inference_steps=<span class=\"hljs-number\"\
          >4</span>, guidance_scale=<span class=\"hljs-number\">1</span>).images[<span\
          \ class=\"hljs-number\">0</span>]\n</code></pre>\n<p>For <code>lcm-lora-sdxl</code>:</p>\n\
          <pre><code class=\"language-python\">pipe2 = AutoPipelineForText2Image.from_pretrained(<span\
          \ class=\"hljs-string\">\"stabilityai/stable-diffusion-xl-base-1.0\"</span>)\
          \ \npipe2.scheduler = LCMScheduler.from_config(pipe2.scheduler.config)\n\
          pipe2.to(<span class=\"hljs-string\">\"cuda:4\"</span>)\n\n<span class=\"\
          hljs-comment\"># load and fuse lcm lora</span>\npipe2.load_lora_weights(<span\
          \ class=\"hljs-string\">\"latent-consistency/lcm-lora-sdxl\"</span>) \n\
          pipe2.fuse_lora()\n\nprompt = <span class=\"hljs-string\">\"Portrait photo\
          \ of a standing girl, photograph, golden hair, depth of field, moody light,\
          \ golden hour, centered, extremely detailed, award winning photography,\
          \ realistic.\"</span>\nimage2 = pipe2(prompt=prompt, num_inference_steps=<span\
          \ class=\"hljs-number\">4</span>, guidance_scale=<span class=\"hljs-number\"\
          >1</span>).images[<span class=\"hljs-number\">0</span>]\n</code></pre>\n"
        raw: "I have another question.\n\nCould you let me know what data is used\
          \ for training [lcm-lora-ssd-1b](https://huggingface.co/latent-consistency/lcm-lora-ssd-1b)\
          \ model and [lcm-lora-sdxl](https://huggingface.co/latent-consistency/lcm-lora-sdxl)?\n\
          \nWhen I generated some samples, the result of lcm-lora-ssd-1b showed better\
          \ quality than that of lcm-lora-sdxl.\n\nI wonder if this difference in\
          \ generation quality is caused by differences in the data used for training.\n\
          \nFor the sake of the community, it would be very helpful if you could share\
          \ the training details of the `lcm-lora-sdxl` and `lcm-lora-ssd-1b` models.\n\
          \nIn my case, I'm trying to create an lcm-lora version of the [koala](https://huggingface.co/etri-vilab/koala-700m-llava-cap)\
          \ model, which is a lightweight T2I model like ssd-1b.\n\n\nThanks in advance.\n\
          \n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b9dd5f902cc42730f6067/sQq09EVqdcdsRxAgA8-YQ.png)\n\
          \n\nFor `lcm-lora-ssd-1b`:\n```python\nmodel_id = \"segmind/SSD-1B\"\nadapter_id\
          \ = \"latent-consistency/lcm-lora-ssd-1b\"\n\npipe = AutoPipelineForText2Image.from_pretrained(model_id,\
          \ torch_dtype=torch.float16, variant=\"fp16\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\
          pipe.to(\"cuda\")\n\n# load and fuse lcm lora\npipe.load_lora_weights(adapter_id)\n\
          pipe.fuse_lora()\n\nprompt = \"Portrait photo of a standing girl, photograph,\
          \ golden hair, depth of field, moody light, golden hour, centered, extremely\
          \ detailed, award winning photography, realistic.\"\nimage = pipe(prompt=prompt,\
          \ num_inference_steps=4, guidance_scale=1).images[0]\n```\n\nFor `lcm-lora-sdxl`:\n\
          \n```python\npipe2 = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\"\
          ) \npipe2.scheduler = LCMScheduler.from_config(pipe2.scheduler.config)\n\
          pipe2.to(\"cuda:4\")\n\n# load and fuse lcm lora\npipe2.load_lora_weights(\"\
          latent-consistency/lcm-lora-sdxl\") \npipe2.fuse_lora()\n\nprompt = \"Portrait\
          \ photo of a standing girl, photograph, golden hair, depth of field, moody\
          \ light, golden hour, centered, extremely detailed, award winning photography,\
          \ realistic.\"\nimage2 = pipe2(prompt=prompt, num_inference_steps=4, guidance_scale=1).images[0]\n\
          ```"
        updatedAt: '2024-01-23T06:23:10.603Z'
      numEdits: 3
      reactions: []
    id: 65af58d1fd56b86c9ccdc8a3
    type: comment
  author: ywlee88
  content: "I have another question.\n\nCould you let me know what data is used for\
    \ training [lcm-lora-ssd-1b](https://huggingface.co/latent-consistency/lcm-lora-ssd-1b)\
    \ model and [lcm-lora-sdxl](https://huggingface.co/latent-consistency/lcm-lora-sdxl)?\n\
    \nWhen I generated some samples, the result of lcm-lora-ssd-1b showed better quality\
    \ than that of lcm-lora-sdxl.\n\nI wonder if this difference in generation quality\
    \ is caused by differences in the data used for training.\n\nFor the sake of the\
    \ community, it would be very helpful if you could share the training details\
    \ of the `lcm-lora-sdxl` and `lcm-lora-ssd-1b` models.\n\nIn my case, I'm trying\
    \ to create an lcm-lora version of the [koala](https://huggingface.co/etri-vilab/koala-700m-llava-cap)\
    \ model, which is a lightweight T2I model like ssd-1b.\n\n\nThanks in advance.\n\
    \n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b9dd5f902cc42730f6067/sQq09EVqdcdsRxAgA8-YQ.png)\n\
    \n\nFor `lcm-lora-ssd-1b`:\n```python\nmodel_id = \"segmind/SSD-1B\"\nadapter_id\
    \ = \"latent-consistency/lcm-lora-ssd-1b\"\n\npipe = AutoPipelineForText2Image.from_pretrained(model_id,\
    \ torch_dtype=torch.float16, variant=\"fp16\")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\
    pipe.to(\"cuda\")\n\n# load and fuse lcm lora\npipe.load_lora_weights(adapter_id)\n\
    pipe.fuse_lora()\n\nprompt = \"Portrait photo of a standing girl, photograph,\
    \ golden hair, depth of field, moody light, golden hour, centered, extremely detailed,\
    \ award winning photography, realistic.\"\nimage = pipe(prompt=prompt, num_inference_steps=4,\
    \ guidance_scale=1).images[0]\n```\n\nFor `lcm-lora-sdxl`:\n\n```python\npipe2\
    \ = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\"\
    ) \npipe2.scheduler = LCMScheduler.from_config(pipe2.scheduler.config)\npipe2.to(\"\
    cuda:4\")\n\n# load and fuse lcm lora\npipe2.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\"\
    ) \npipe2.fuse_lora()\n\nprompt = \"Portrait photo of a standing girl, photograph,\
    \ golden hair, depth of field, moody light, golden hour, centered, extremely detailed,\
    \ award winning photography, realistic.\"\nimage2 = pipe2(prompt=prompt, num_inference_steps=4,\
    \ guidance_scale=1).images[0]\n```"
  created_at: 2024-01-23 06:12:33+00:00
  edited: true
  hidden: false
  id: 65af58d1fd56b86c9ccdc8a3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: latent-consistency/lcm-lora-sdxl
repo_type: model
status: open
target_branch: null
title: Training details
