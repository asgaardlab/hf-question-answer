!!python/object:huggingface_hub.community.DiscussionWithDetails
author: terryryu
conflicting_files: null
created_at: 2023-09-08 10:44:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0e627c74d301d65f3b364e8229dd78ab.svg
      fullname: Nuri Ryu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: terryryu
      type: user
    createdAt: '2023-09-08T11:44:06.000Z'
    data:
      edited: false
      editors:
      - terryryu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6299583315849304
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0e627c74d301d65f3b364e8229dd78ab.svg
          fullname: Nuri Ryu
          isHf: false
          isPro: false
          name: terryryu
          type: user
        html: '<p>Hi, thank you for sharing this interesting work!</p>

          <p>But, I have a question about the provided VAE''s input and output channels.<br>In
          ldm3d-4c/vae/config.json it is written that the VAE has four input channels,
          which is different from the expected six channels written in the paper (RGB
          + RGB-like depth map). Ignoring this I just tried encoding my data with
          the provided VAE, but was interrupted by the following error.</p>

          <p>  File "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py",
          line 242, in encode<br>    h = self.encoder(x)<br>  File "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/vae.py",
          line 111, in forward<br>    sample = self.conv_in(sample)<br>  File "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/conv.py",
          line 463, in forward<br>    return self._conv_forward(input, self.weight,
          self.bias)<br>  File "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/conv.py",
          line 459, in _conv_forward<br>    return F.conv2d(input, weight, bias, self.stride,<br>RuntimeError:
          Given groups=1, weight of size [128, 4, 3, 3], expected input[1, 6, 512,
          512] to have 4 channels, but got 6 channels instead</p>

          <p>So, I just turned my depth map into a 8 bit image and merged it with
          the RGB image. But then, I was faced with yet another error </p>

          <p>  File "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py",
          line 242, in encode<br>    h = self.encoder(x)<br>  File "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/vae.py",
          line 140, in forward<br>    sample = down_block(sample)<br>  File "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py",
          line 1214, in forward<br>    hidden_states = resnet(hidden_states, temb=None)<br>  File
          "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/resnet.py",
          line 597, in forward<br>    hidden_states = self.norm1(hidden_states)<br>  File
          "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/normalization.py",
          line 273, in forward<br>    return F.group_norm(<br>  File "/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/functional.py",
          line 2530, in group_norm<br>    return torch.group_norm(input, num_groups,
          weight, bias, eps, torch.backends.cudnn.enabled)<br>RuntimeError: Expected
          weight to be a vector of size equal to the number of channels in input,
          but got weight of shape [128] and input of shape [128, 512, 512]</p>

          <p>I guess I have misunderstood something? Also, did you use bicubic upsampling
          to scale the depthmap resolution from 384 x 384 to 512 x 512?</p>

          '
        raw: "Hi, thank you for sharing this interesting work!\r\n\r\nBut, I have\
          \ a question about the provided VAE's input and output channels.\r\nIn ldm3d-4c/vae/config.json\
          \ it is written that the VAE has four input channels, which is different\
          \ from the expected six channels written in the paper (RGB + RGB-like depth\
          \ map). Ignoring this I just tried encoding my data with the provided VAE,\
          \ but was interrupted by the following error.\r\n\r\n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py\"\
          , line 242, in encode\r\n    h = self.encoder(x)\r\n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/vae.py\"\
          , line 111, in forward\r\n    sample = self.conv_in(sample)\r\n  File \"\
          /home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/conv.py\"\
          , line 463, in forward\r\n    return self._conv_forward(input, self.weight,\
          \ self.bias)\r\n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/conv.py\"\
          , line 459, in _conv_forward\r\n    return F.conv2d(input, weight, bias,\
          \ self.stride,\r\nRuntimeError: Given groups=1, weight of size [128, 4,\
          \ 3, 3], expected input[1, 6, 512, 512] to have 4 channels, but got 6 channels\
          \ instead\r\n\r\nSo, I just turned my depth map into a 8 bit image and merged\
          \ it with the RGB image. But then, I was faced with yet another error \r\
          \n\r\n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py\"\
          , line 242, in encode\r\n    h = self.encoder(x)\r\n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/vae.py\"\
          , line 140, in forward\r\n    sample = down_block(sample)\r\n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py\"\
          , line 1214, in forward\r\n    hidden_states = resnet(hidden_states, temb=None)\r\
          \n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/resnet.py\"\
          , line 597, in forward\r\n    hidden_states = self.norm1(hidden_states)\r\
          \n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/normalization.py\"\
          , line 273, in forward\r\n    return F.group_norm(\r\n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/functional.py\"\
          , line 2530, in group_norm\r\n    return torch.group_norm(input, num_groups,\
          \ weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: Expected\
          \ weight to be a vector of size equal to the number of channels in input,\
          \ but got weight of shape [128] and input of shape [128, 512, 512]\r\n\r\
          \nI guess I have misunderstood something? Also, did you use bicubic upsampling\
          \ to scale the depthmap resolution from 384 x 384 to 512 x 512?"
        updatedAt: '2023-09-08T11:44:06.655Z'
      numEdits: 0
      reactions: []
    id: 64fb090620a2d04cc177ae3b
    type: comment
  author: terryryu
  content: "Hi, thank you for sharing this interesting work!\r\n\r\nBut, I have a\
    \ question about the provided VAE's input and output channels.\r\nIn ldm3d-4c/vae/config.json\
    \ it is written that the VAE has four input channels, which is different from\
    \ the expected six channels written in the paper (RGB + RGB-like depth map). Ignoring\
    \ this I just tried encoding my data with the provided VAE, but was interrupted\
    \ by the following error.\r\n\r\n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py\"\
    , line 242, in encode\r\n    h = self.encoder(x)\r\n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/vae.py\"\
    , line 111, in forward\r\n    sample = self.conv_in(sample)\r\n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/conv.py\"\
    , line 463, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\
    \n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/conv.py\"\
    , line 459, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\
    \nRuntimeError: Given groups=1, weight of size [128, 4, 3, 3], expected input[1,\
    \ 6, 512, 512] to have 4 channels, but got 6 channels instead\r\n\r\nSo, I just\
    \ turned my depth map into a 8 bit image and merged it with the RGB image. But\
    \ then, I was faced with yet another error \r\n\r\n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py\"\
    , line 242, in encode\r\n    h = self.encoder(x)\r\n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/vae.py\"\
    , line 140, in forward\r\n    sample = down_block(sample)\r\n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py\"\
    , line 1214, in forward\r\n    hidden_states = resnet(hidden_states, temb=None)\r\
    \n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/diffusers/models/resnet.py\"\
    , line 597, in forward\r\n    hidden_states = self.norm1(hidden_states)\r\n  File\
    \ \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/modules/normalization.py\"\
    , line 273, in forward\r\n    return F.group_norm(\r\n  File \"/home/terryryu/miniconda3/envs/p3dpp/lib/python3.10/site-packages/torch/nn/functional.py\"\
    , line 2530, in group_norm\r\n    return torch.group_norm(input, num_groups, weight,\
    \ bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: Expected weight to\
    \ be a vector of size equal to the number of channels in input, but got weight\
    \ of shape [128] and input of shape [128, 512, 512]\r\n\r\nI guess I have misunderstood\
    \ something? Also, did you use bicubic upsampling to scale the depthmap resolution\
    \ from 384 x 384 to 512 x 512?"
  created_at: 2023-09-08 10:44:06+00:00
  edited: false
  hidden: false
  id: 64fb090620a2d04cc177ae3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/13dc0a370bb24271653c352897a2673e.svg
      fullname: Estelle Aflalo
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: estellea
      type: user
    createdAt: '2023-10-02T12:46:30.000Z'
    data:
      edited: true
      editors:
      - estellea
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9021496772766113
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/13dc0a370bb24271653c352897a2673e.svg
          fullname: Estelle Aflalo
          isHf: false
          isPro: false
          name: estellea
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;terryryu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/terryryu\"\
          >@<span class=\"underline\">terryryu</span></a></span>\n\n\t</span></span>!!</p>\n\
          <blockquote>\n<p>In ldm3d-4c/vae/config.json it is written that the VAE\
          \ has four input channels, which is different from the expected six channels\
          \ written in the paper (RGB + RGB-like depth map).</p>\n</blockquote>\n\
          <p>Indeed! For this version of ldm3d, that we called \"ldm3d-4c\", we mapped\
          \ the depth to a 1D vector, making the input a 4-channel vector (3 for RGB\
          \ and 1 for Depth).</p>\n<blockquote>\n<p> Also, did you use bicubic upsampling\
          \ to scale the depthmap resolution from 384 x 384 to 512 x 512?</p>\n</blockquote>\n\
          <p> We are using dpt-512 on a 512 input size. The output is automatically\
          \ of 512 resolution</p>\n<p>As for the error, I am not sure. A bit hard\
          \ to answer. Maybe you have a short snippet so I can try to reproduce?</p>\n\
          <p>Best<br>Estelle</p>\n"
        raw: "Hi @terryryu!!\n> In ldm3d-4c/vae/config.json it is written that the\
          \ VAE has four input channels, which is different from the expected six\
          \ channels written in the paper (RGB + RGB-like depth map).\n\n\nIndeed!\
          \ For this version of ldm3d, that we called \"ldm3d-4c\", we mapped the\
          \ depth to a 1D vector, making the input a 4-channel vector (3 for RGB and\
          \ 1 for Depth).\n\n>  Also, did you use bicubic upsampling to scale the\
          \ depthmap resolution from 384 x 384 to 512 x 512?\n\n We are using dpt-512\
          \ on a 512 input size. The output is automatically of 512 resolution\n\n\
          As for the error, I am not sure. A bit hard to answer. Maybe you have a\
          \ short snippet so I can try to reproduce?\n\nBest\nEstelle"
        updatedAt: '2023-10-02T12:47:25.688Z'
      numEdits: 3
      reactions: []
    id: 651abba64dba2d9ed143c3de
    type: comment
  author: estellea
  content: "Hi @terryryu!!\n> In ldm3d-4c/vae/config.json it is written that the VAE\
    \ has four input channels, which is different from the expected six channels written\
    \ in the paper (RGB + RGB-like depth map).\n\n\nIndeed! For this version of ldm3d,\
    \ that we called \"ldm3d-4c\", we mapped the depth to a 1D vector, making the\
    \ input a 4-channel vector (3 for RGB and 1 for Depth).\n\n>  Also, did you use\
    \ bicubic upsampling to scale the depthmap resolution from 384 x 384 to 512 x\
    \ 512?\n\n We are using dpt-512 on a 512 input size. The output is automatically\
    \ of 512 resolution\n\nAs for the error, I am not sure. A bit hard to answer.\
    \ Maybe you have a short snippet so I can try to reproduce?\n\nBest\nEstelle"
  created_at: 2023-10-02 11:46:30+00:00
  edited: true
  hidden: false
  id: 651abba64dba2d9ed143c3de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0e627c74d301d65f3b364e8229dd78ab.svg
      fullname: Nuri Ryu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: terryryu
      type: user
    createdAt: '2023-10-05T07:47:42.000Z'
    data:
      edited: true
      editors:
      - terryryu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3299736976623535
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0e627c74d301d65f3b364e8229dd78ab.svg
          fullname: Nuri Ryu
          isHf: false
          isPro: false
          name: terryryu
          type: user
        html: "<p>Thank you for your help <span data-props=\"{&quot;user&quot;:&quot;estellea&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/estellea\"\
          >@<span class=\"underline\">estellea</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>I've come up with a minimal example that encodes and  decodes the RGBD\
          \ output of the lemon example.</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >import</span> cv2\n<span class=\"hljs-keyword\">import</span> numpy <span\
          \ class=\"hljs-keyword\">as</span> np\n<span class=\"hljs-keyword\">from</span>\
          \ einops <span class=\"hljs-keyword\">import</span> rearrange\n<span class=\"\
          hljs-keyword\">from</span> diffusers <span class=\"hljs-keyword\">import</span>\
          \ AutoencoderKL\n<span class=\"hljs-keyword\">from</span> diffusers.image_processor\
          \ <span class=\"hljs-keyword\">import</span> VaeImageProcessorLDM3D\n\n\
          <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >load_images</span>(<span class=\"hljs-params\">rgb_path, depth_path</span>):\n\
          \    rgb_img = cv2.imread(rgb_path) / <span class=\"hljs-number\">255.</span>\n\
          \    depth_img = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED) <span class=\"\
          hljs-comment\"># ensures 16-bit is preserved</span>\n\n    <span class=\"\
          hljs-keyword\">if</span> depth_img.dtype != np.uint16:\n        <span class=\"\
          hljs-keyword\">raise</span> ValueError(<span class=\"hljs-string\">\"Depth\
          \ image is not 16-bit!\"</span>)\n\n    depth_img = depth_img / <span class=\"\
          hljs-number\">65536.</span>\n\n\n    depth_img_expanded = np.expand_dims(depth_img,\
          \ axis=-<span class=\"hljs-number\">1</span>)\n    merged_img = np.concatenate([rgb_img,\
          \ depth_img_expanded], axis=-<span class=\"hljs-number\">1</span>)\n\n \
          \   <span class=\"hljs-keyword\">return</span> merged_img\n\n<span class=\"\
          hljs-keyword\">with</span> torch.no_grad():\n\n    test_rgbd = load_images(<span\
          \ class=\"hljs-string\">\"/home/terryryu/Experiments/LDM3D/lemons_ldm3d_rgb.jpg\"\
          </span>, <span class=\"hljs-string\">\"/home/terryryu/Experiments/LDM3D/lemons_ldm3d_depth.png\"\
          </span>)\n\n    vae = AutoencoderKL.from_pretrained(<span class=\"hljs-string\"\
          >\"/home/terryryu/Weights/LDM3D/vae/\"</span>, local_files_only=<span class=\"\
          hljs-literal\">True</span>, torch_dtype=torch.float16).cuda()\n    vae_scale_factor\
          \ = <span class=\"hljs-number\">2</span> ** (<span class=\"hljs-built_in\"\
          >len</span>(vae.config.block_out_channels) - <span class=\"hljs-number\"\
          >1</span>)\n    processor = VaeImageProcessorLDM3D(vae_scale_factor=vae_scale_factor)\n\
          \    \n    test_rgbd = rearrange(test_rgbd, <span class=\"hljs-string\"\
          >\"h w c -&gt; 1 c h w\"</span>)\n    test_rgbd = torch.cuda.HalfTensor(test_rgbd)\n\
          \    latents = vae.encode(test_rgbd).latent_dist.mode()\n    image = vae.decode(latents\
          \ / vae.config.scaling_factor, return_dict=<span class=\"hljs-literal\"\
          >False</span>)[<span class=\"hljs-number\">0</span>]\n    output_type =\
          \ <span class=\"hljs-string\">\"pil\"</span>\n    do_denormalize = [<span\
          \ class=\"hljs-literal\">True</span>] * image.shape[<span class=\"hljs-number\"\
          >0</span>]\n    rgb, depth = processor.postprocess(image, output_type=output_type,\
          \ do_denormalize=do_denormalize)\n    \n    rgb[<span class=\"hljs-number\"\
          >0</span>].save(<span class=\"hljs-string\">\"./minimal_test_rgb.png\"</span>)\n\
          \    depth[<span class=\"hljs-number\">0</span>].save(<span class=\"hljs-string\"\
          >\"./minimal_test_depth.png\"</span>)\n</code></pre>\n<p>Strangely, the\
          \ color of the decoded result is broken. Maybe I've made a minor mistake\
          \ somewhere...?</p>\n<div class=\"max-w-full overflow-auto\">\n\t<table>\n\
          \t\t<thead><tr>\n<th align=\"center\">Input RGB</th>\n<th align=\"center\"\
          >Input Depth</th>\n<th align=\"center\">Output RGB</th>\n<th align=\"center\"\
          >Output Depth</th>\n</tr>\n\n\t\t</thead><tbody><tr>\n<td align=\"center\"\
          ><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/ta_ufsbT4pbte1KzPn_w6.jpeg\"\
          ><img alt=\"lemons_ldm3d_rgb.jpg\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/ta_ufsbT4pbte1KzPn_w6.jpeg\"\
          ></a></td>\n<td align=\"center\"><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/a5mfUeRdsLV__WNPBQMtv.png\"\
          ><img alt=\"lemons_ldm3d_depth.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/a5mfUeRdsLV__WNPBQMtv.png\"\
          ></a></td>\n<td align=\"center\"><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/-7TVA-0xPGp9EZUi-wxHa.png\"\
          ><img alt=\"minimal_test_rgb.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/-7TVA-0xPGp9EZUi-wxHa.png\"\
          ></a></td>\n<td align=\"center\"><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/_YqfYKzYB2Vgt7Jk6R3Ux.png\"\
          ><img alt=\"minimal_test_depth.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/_YqfYKzYB2Vgt7Jk6R3Ux.png\"\
          ></a></td>\n</tr>\n</tbody>\n\t</table>\n</div>\n<p>Best,<br>Ryu</p>\n"
        raw: "Thank you for your help @estellea!\n\nI've come up with a minimal example\
          \ that encodes and  decodes the RGBD output of the lemon example.\n\n```python\n\
          import torch\nimport cv2\nimport numpy as np\nfrom einops import rearrange\n\
          from diffusers import AutoencoderKL\nfrom diffusers.image_processor import\
          \ VaeImageProcessorLDM3D\n\ndef load_images(rgb_path, depth_path):\n   \
          \ rgb_img = cv2.imread(rgb_path) / 255.\n    depth_img = cv2.imread(depth_path,\
          \ cv2.IMREAD_UNCHANGED) # ensures 16-bit is preserved\n\n    if depth_img.dtype\
          \ != np.uint16:\n        raise ValueError(\"Depth image is not 16-bit!\"\
          )\n\n    depth_img = depth_img / 65536.\n\n\n    depth_img_expanded = np.expand_dims(depth_img,\
          \ axis=-1)\n    merged_img = np.concatenate([rgb_img, depth_img_expanded],\
          \ axis=-1)\n\n    return merged_img\n\nwith torch.no_grad():\n\n    test_rgbd\
          \ = load_images(\"/home/terryryu/Experiments/LDM3D/lemons_ldm3d_rgb.jpg\"\
          , \"/home/terryryu/Experiments/LDM3D/lemons_ldm3d_depth.png\")\n\n    vae\
          \ = AutoencoderKL.from_pretrained(\"/home/terryryu/Weights/LDM3D/vae/\"\
          , local_files_only=True, torch_dtype=torch.float16).cuda()\n    vae_scale_factor\
          \ = 2 ** (len(vae.config.block_out_channels) - 1)\n    processor = VaeImageProcessorLDM3D(vae_scale_factor=vae_scale_factor)\n\
          \    \n    test_rgbd = rearrange(test_rgbd, \"h w c -> 1 c h w\")\n    test_rgbd\
          \ = torch.cuda.HalfTensor(test_rgbd)\n    latents = vae.encode(test_rgbd).latent_dist.mode()\n\
          \    image = vae.decode(latents / vae.config.scaling_factor, return_dict=False)[0]\n\
          \    output_type = \"pil\"\n    do_denormalize = [True] * image.shape[0]\n\
          \    rgb, depth = processor.postprocess(image, output_type=output_type,\
          \ do_denormalize=do_denormalize)\n    \n    rgb[0].save(\"./minimal_test_rgb.png\"\
          )\n    depth[0].save(\"./minimal_test_depth.png\")\n```\n\nStrangely, the\
          \ color of the decoded result is broken. Maybe I've made a minor mistake\
          \ somewhere...?\n\n\nInput RGB             |  Input Depth        | Output\
          \ RGB             |  Output Depth\n:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n\
          ![lemons_ldm3d_rgb.jpg](https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/ta_ufsbT4pbte1KzPn_w6.jpeg)\
          \ | ![lemons_ldm3d_depth.png](https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/a5mfUeRdsLV__WNPBQMtv.png)\
          \ | ![minimal_test_rgb.png](https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/-7TVA-0xPGp9EZUi-wxHa.png)\
          \ | ![minimal_test_depth.png](https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/_YqfYKzYB2Vgt7Jk6R3Ux.png)\n\
          \n\nBest,\nRyu"
        updatedAt: '2023-10-05T07:48:05.685Z'
      numEdits: 1
      reactions: []
    id: 651e6a1e4720dd0c17cb6d10
    type: comment
  author: terryryu
  content: "Thank you for your help @estellea!\n\nI've come up with a minimal example\
    \ that encodes and  decodes the RGBD output of the lemon example.\n\n```python\n\
    import torch\nimport cv2\nimport numpy as np\nfrom einops import rearrange\nfrom\
    \ diffusers import AutoencoderKL\nfrom diffusers.image_processor import VaeImageProcessorLDM3D\n\
    \ndef load_images(rgb_path, depth_path):\n    rgb_img = cv2.imread(rgb_path) /\
    \ 255.\n    depth_img = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED) # ensures\
    \ 16-bit is preserved\n\n    if depth_img.dtype != np.uint16:\n        raise ValueError(\"\
    Depth image is not 16-bit!\")\n\n    depth_img = depth_img / 65536.\n\n\n    depth_img_expanded\
    \ = np.expand_dims(depth_img, axis=-1)\n    merged_img = np.concatenate([rgb_img,\
    \ depth_img_expanded], axis=-1)\n\n    return merged_img\n\nwith torch.no_grad():\n\
    \n    test_rgbd = load_images(\"/home/terryryu/Experiments/LDM3D/lemons_ldm3d_rgb.jpg\"\
    , \"/home/terryryu/Experiments/LDM3D/lemons_ldm3d_depth.png\")\n\n    vae = AutoencoderKL.from_pretrained(\"\
    /home/terryryu/Weights/LDM3D/vae/\", local_files_only=True, torch_dtype=torch.float16).cuda()\n\
    \    vae_scale_factor = 2 ** (len(vae.config.block_out_channels) - 1)\n    processor\
    \ = VaeImageProcessorLDM3D(vae_scale_factor=vae_scale_factor)\n    \n    test_rgbd\
    \ = rearrange(test_rgbd, \"h w c -> 1 c h w\")\n    test_rgbd = torch.cuda.HalfTensor(test_rgbd)\n\
    \    latents = vae.encode(test_rgbd).latent_dist.mode()\n    image = vae.decode(latents\
    \ / vae.config.scaling_factor, return_dict=False)[0]\n    output_type = \"pil\"\
    \n    do_denormalize = [True] * image.shape[0]\n    rgb, depth = processor.postprocess(image,\
    \ output_type=output_type, do_denormalize=do_denormalize)\n    \n    rgb[0].save(\"\
    ./minimal_test_rgb.png\")\n    depth[0].save(\"./minimal_test_depth.png\")\n```\n\
    \nStrangely, the color of the decoded result is broken. Maybe I've made a minor\
    \ mistake somewhere...?\n\n\nInput RGB             |  Input Depth        | Output\
    \ RGB             |  Output Depth\n:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n\
    ![lemons_ldm3d_rgb.jpg](https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/ta_ufsbT4pbte1KzPn_w6.jpeg)\
    \ | ![lemons_ldm3d_depth.png](https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/a5mfUeRdsLV__WNPBQMtv.png)\
    \ | ![minimal_test_rgb.png](https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/-7TVA-0xPGp9EZUi-wxHa.png)\
    \ | ![minimal_test_depth.png](https://cdn-uploads.huggingface.co/production/uploads/63106aac95c34b954078ef67/_YqfYKzYB2Vgt7Jk6R3Ux.png)\n\
    \n\nBest,\nRyu"
  created_at: 2023-10-05 06:47:42+00:00
  edited: true
  hidden: false
  id: 651e6a1e4720dd0c17cb6d10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/be2c71dd32d3db19d930fb59c94e33a2.svg
      fullname: Maguire
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maguire
      type: user
    createdAt: '2023-10-20T09:58:58.000Z'
    data:
      edited: false
      editors:
      - Maguire
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9034518003463745
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/be2c71dd32d3db19d930fb59c94e33a2.svg
          fullname: Maguire
          isHf: false
          isPro: false
          name: Maguire
          type: user
        html: '<p>I have the same problem how to use the ldm3d-4c vae to reconstruct
          the RGB and depth image?</p>

          '
        raw: 'I have the same problem how to use the ldm3d-4c vae to reconstruct the
          RGB and depth image?

          '
        updatedAt: '2023-10-20T09:58:58.950Z'
      numEdits: 0
      reactions: []
    id: 65324f629824bcc3f44e4f4f
    type: comment
  author: Maguire
  content: 'I have the same problem how to use the ldm3d-4c vae to reconstruct the
    RGB and depth image?

    '
  created_at: 2023-10-20 08:58:58+00:00
  edited: false
  hidden: false
  id: 65324f629824bcc3f44e4f4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f5a1d5d90abf8aafe8864bc4f1b076cc.svg
      fullname: gabriela ben melech
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gabsm
      type: user
    createdAt: '2023-10-26T13:39:34.000Z'
    data:
      edited: false
      editors:
      - gabsm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44764044880867004
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f5a1d5d90abf8aafe8864bc4f1b076cc.svg
          fullname: gabriela ben melech
          isHf: false
          isPro: false
          name: gabsm
          type: user
        html: "<p>Before encoding, make sure to normalize both the image and the depth\
          \ to the range of [-1, 1], by adding this to 'load_images':<br>rgb_img =\
          \ 2. * rgb_img - 1.<br>depth_img = 2. * depth_img - 1.<br>also, there is\
          \ no need to divide the latents by any scaling factor during the reconstruction\
          \ since the latent space does not undergo scaling at this stage. However,\
          \ when utilizing the diffusion aspect of the process, ensure that you scale\
          \ the latent space both prior to and after the diffusion to achieve the\
          \ desired results.<br><span data-props=\"{&quot;user&quot;:&quot;terryryu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/terryryu\"\
          >@<span class=\"underline\">terryryu</span></a></span>\n\n\t</span></span>,\
          \ attaching updated example:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >import</span> cv2\n<span class=\"hljs-keyword\">import</span> numpy <span\
          \ class=\"hljs-keyword\">as</span> np\n<span class=\"hljs-keyword\">from</span>\
          \ einops <span class=\"hljs-keyword\">import</span> rearrange\n<span class=\"\
          hljs-keyword\">from</span> diffusers <span class=\"hljs-keyword\">import</span>\
          \ AutoencoderKL\n<span class=\"hljs-keyword\">from</span> diffusers.image_processor\
          \ <span class=\"hljs-keyword\">import</span> VaeImageProcessorLDM3D\n\n\
          <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >load_images</span>(<span class=\"hljs-params\">rgb_path, depth_path</span>):\n\
          \    rgb_img = cv2.imread(rgb_path) / <span class=\"hljs-number\">255.</span>\n\
          \    rgb_img = <span class=\"hljs-number\">2.</span>*rgb_img - <span class=\"\
          hljs-number\">1.</span>\n    depth_img = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\
          \ <span class=\"hljs-comment\"># ensures 16-bit is preserved</span>\n\n\
          \    <span class=\"hljs-keyword\">if</span> depth_img.dtype != np.uint16:\n\
          \        <span class=\"hljs-keyword\">raise</span> ValueError(<span class=\"\
          hljs-string\">\"Depth image is not 16-bit!\"</span>)\n\n    depth_img =\
          \ depth_img / <span class=\"hljs-number\">65536.</span>\n    depth_img =\
          \ <span class=\"hljs-number\">2.</span>*depth_img - <span class=\"hljs-number\"\
          >1.</span>\n\n\n    depth_img_expanded = np.expand_dims(depth_img, axis=-<span\
          \ class=\"hljs-number\">1</span>)\n    merged_img = np.concatenate([rgb_img,\
          \ depth_img_expanded], axis=-<span class=\"hljs-number\">1</span>)\n\n \
          \   <span class=\"hljs-keyword\">return</span> merged_img\n\n<span class=\"\
          hljs-keyword\">with</span> torch.no_grad():\n\n    test_rgbd = load_images(<span\
          \ class=\"hljs-string\">\"/home/terryryu/Experiments/LDM3D/lemons_ldm3d_rgb.jpg\"\
          </span>, <span class=\"hljs-string\">\"/home/terryryu/Experiments/LDM3D/lemons_ldm3d_depth.png\"\
          </span>)\n\n    vae = AutoencoderKL.from_pretrained(<span class=\"hljs-string\"\
          >\"/home/terryryu/Weights/LDM3D/vae/\"</span>, local_files_only=<span class=\"\
          hljs-literal\">True</span>, torch_dtype=torch.float16).cuda()\n    vae_scale_factor\
          \ = <span class=\"hljs-number\">2</span> ** (<span class=\"hljs-built_in\"\
          >len</span>(vae.config.block_out_channels) - <span class=\"hljs-number\"\
          >1</span>)\n    processor = VaeImageProcessorLDM3D(vae_scale_factor=vae_scale_factor)\n\
          \    \n    test_rgbd = rearrange(test_rgbd, <span class=\"hljs-string\"\
          >\"h w c -&gt; 1 c h w\"</span>)\n    test_rgbd = torch.cuda.HalfTensor(test_rgbd)\n\
          \    latents = vae.encode(test_rgbd).latent_dist.mode()\n    image = vae.decode(latents,\
          \ return_dict=<span class=\"hljs-literal\">False</span>)[<span class=\"\
          hljs-number\">0</span>]\n    output_type = <span class=\"hljs-string\">\"\
          pil\"</span>\n    do_denormalize = [<span class=\"hljs-literal\">True</span>]\
          \ * image.shape[<span class=\"hljs-number\">0</span>]\n    rgb, depth =\
          \ processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n\
          \    \n    rgb[<span class=\"hljs-number\">0</span>].save(<span class=\"\
          hljs-string\">\"./minimal_test_rgb.png\"</span>)\n    depth[<span class=\"\
          hljs-number\">0</span>].save(<span class=\"hljs-string\">\"./minimal_test_depth.png\"\
          </span>)\n</code></pre>\n<p>Best,<br>Gabi</p>\n"
        raw: "Before encoding, make sure to normalize both the image and the depth\
          \ to the range of [-1, 1], by adding this to 'load_images':\nrgb_img = 2.\
          \ * rgb_img - 1.\ndepth_img = 2. * depth_img - 1.\nalso, there is no need\
          \ to divide the latents by any scaling factor during the reconstruction\
          \ since the latent space does not undergo scaling at this stage. However,\
          \ when utilizing the diffusion aspect of the process, ensure that you scale\
          \ the latent space both prior to and after the diffusion to achieve the\
          \ desired results.\n@terryryu, attaching updated example:\n```python\nimport\
          \ torch\nimport cv2\nimport numpy as np\nfrom einops import rearrange\n\
          from diffusers import AutoencoderKL\nfrom diffusers.image_processor import\
          \ VaeImageProcessorLDM3D\n\ndef load_images(rgb_path, depth_path):\n   \
          \ rgb_img = cv2.imread(rgb_path) / 255.\n    rgb_img = 2.*rgb_img - 1.\n\
          \    depth_img = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED) # ensures\
          \ 16-bit is preserved\n\n    if depth_img.dtype != np.uint16:\n        raise\
          \ ValueError(\"Depth image is not 16-bit!\")\n\n    depth_img = depth_img\
          \ / 65536.\n    depth_img = 2.*depth_img - 1.\n\n\n    depth_img_expanded\
          \ = np.expand_dims(depth_img, axis=-1)\n    merged_img = np.concatenate([rgb_img,\
          \ depth_img_expanded], axis=-1)\n\n    return merged_img\n\nwith torch.no_grad():\n\
          \n    test_rgbd = load_images(\"/home/terryryu/Experiments/LDM3D/lemons_ldm3d_rgb.jpg\"\
          , \"/home/terryryu/Experiments/LDM3D/lemons_ldm3d_depth.png\")\n\n    vae\
          \ = AutoencoderKL.from_pretrained(\"/home/terryryu/Weights/LDM3D/vae/\"\
          , local_files_only=True, torch_dtype=torch.float16).cuda()\n    vae_scale_factor\
          \ = 2 ** (len(vae.config.block_out_channels) - 1)\n    processor = VaeImageProcessorLDM3D(vae_scale_factor=vae_scale_factor)\n\
          \    \n    test_rgbd = rearrange(test_rgbd, \"h w c -> 1 c h w\")\n    test_rgbd\
          \ = torch.cuda.HalfTensor(test_rgbd)\n    latents = vae.encode(test_rgbd).latent_dist.mode()\n\
          \    image = vae.decode(latents, return_dict=False)[0]\n    output_type\
          \ = \"pil\"\n    do_denormalize = [True] * image.shape[0]\n    rgb, depth\
          \ = processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n\
          \    \n    rgb[0].save(\"./minimal_test_rgb.png\")\n    depth[0].save(\"\
          ./minimal_test_depth.png\")\n```\nBest,\nGabi"
        updatedAt: '2023-10-26T13:39:34.621Z'
      numEdits: 0
      reactions: []
    id: 653a6c16d994e992e25d50ad
    type: comment
  author: gabsm
  content: "Before encoding, make sure to normalize both the image and the depth to\
    \ the range of [-1, 1], by adding this to 'load_images':\nrgb_img = 2. * rgb_img\
    \ - 1.\ndepth_img = 2. * depth_img - 1.\nalso, there is no need to divide the\
    \ latents by any scaling factor during the reconstruction since the latent space\
    \ does not undergo scaling at this stage. However, when utilizing the diffusion\
    \ aspect of the process, ensure that you scale the latent space both prior to\
    \ and after the diffusion to achieve the desired results.\n@terryryu, attaching\
    \ updated example:\n```python\nimport torch\nimport cv2\nimport numpy as np\n\
    from einops import rearrange\nfrom diffusers import AutoencoderKL\nfrom diffusers.image_processor\
    \ import VaeImageProcessorLDM3D\n\ndef load_images(rgb_path, depth_path):\n  \
    \  rgb_img = cv2.imread(rgb_path) / 255.\n    rgb_img = 2.*rgb_img - 1.\n    depth_img\
    \ = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED) # ensures 16-bit is preserved\n\
    \n    if depth_img.dtype != np.uint16:\n        raise ValueError(\"Depth image\
    \ is not 16-bit!\")\n\n    depth_img = depth_img / 65536.\n    depth_img = 2.*depth_img\
    \ - 1.\n\n\n    depth_img_expanded = np.expand_dims(depth_img, axis=-1)\n    merged_img\
    \ = np.concatenate([rgb_img, depth_img_expanded], axis=-1)\n\n    return merged_img\n\
    \nwith torch.no_grad():\n\n    test_rgbd = load_images(\"/home/terryryu/Experiments/LDM3D/lemons_ldm3d_rgb.jpg\"\
    , \"/home/terryryu/Experiments/LDM3D/lemons_ldm3d_depth.png\")\n\n    vae = AutoencoderKL.from_pretrained(\"\
    /home/terryryu/Weights/LDM3D/vae/\", local_files_only=True, torch_dtype=torch.float16).cuda()\n\
    \    vae_scale_factor = 2 ** (len(vae.config.block_out_channels) - 1)\n    processor\
    \ = VaeImageProcessorLDM3D(vae_scale_factor=vae_scale_factor)\n    \n    test_rgbd\
    \ = rearrange(test_rgbd, \"h w c -> 1 c h w\")\n    test_rgbd = torch.cuda.HalfTensor(test_rgbd)\n\
    \    latents = vae.encode(test_rgbd).latent_dist.mode()\n    image = vae.decode(latents,\
    \ return_dict=False)[0]\n    output_type = \"pil\"\n    do_denormalize = [True]\
    \ * image.shape[0]\n    rgb, depth = processor.postprocess(image, output_type=output_type,\
    \ do_denormalize=do_denormalize)\n    \n    rgb[0].save(\"./minimal_test_rgb.png\"\
    )\n    depth[0].save(\"./minimal_test_depth.png\")\n```\nBest,\nGabi"
  created_at: 2023-10-26 12:39:34+00:00
  edited: false
  hidden: false
  id: 653a6c16d994e992e25d50ad
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: Intel/ldm3d-4c
repo_type: model
status: open
target_branch: null
title: Question about the VAE's input/output channel dimensions
