!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-12-16 08:49:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-16T08:49:52.000Z'
    data:
      edited: true
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7878188490867615
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p>Hi,</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ this amazing method seems to be fast in generating the quantized model\
          \ (claimed to be 50x faster than generating GPTQ for llama2 70b) with NO\
          \ calibration data required. You should pay attention to it</p>\n<p>PS<br><a\
          \ rel=\"nofollow\" href=\"https://mobiusml.github.io/hqq_blog/\">https://mobiusml.github.io/hqq_blog/</a><br><a\
          \ rel=\"nofollow\" href=\"https://github.com/oobabooga/text-generation-webui/pull/4888\"\
          >https://github.com/oobabooga/text-generation-webui/pull/4888</a></p>\n"
        raw: 'Hi,


          @TheBloke this amazing method seems to be fast in generating the quantized
          model (claimed to be 50x faster than generating GPTQ for llama2 70b) with
          NO calibration data required. You should pay attention to it


          PS

          https://mobiusml.github.io/hqq_blog/

          https://github.com/oobabooga/text-generation-webui/pull/4888'
        updatedAt: '2023-12-16T08:50:08.153Z'
      numEdits: 1
      reactions: []
    id: 657d64b03e3b5bea66fc2486
    type: comment
  author: Yhyu13
  content: 'Hi,


    @TheBloke this amazing method seems to be fast in generating the quantized model
    (claimed to be 50x faster than generating GPTQ for llama2 70b) with NO calibration
    data required. You should pay attention to it


    PS

    https://mobiusml.github.io/hqq_blog/

    https://github.com/oobabooga/text-generation-webui/pull/4888'
  created_at: 2023-12-16 08:49:52+00:00
  edited: true
  hidden: false
  id: 657d64b03e3b5bea66fc2486
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d3793b508a6313e31d2dc0/dZGO3zW131TSgqs5RKci1.jpeg?w=200&h=200&f=face
      fullname: Dr. Hicham Badri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mobicham
      type: user
    createdAt: '2023-12-16T10:37:20.000Z'
    data:
      edited: false
      editors:
      - mobicham
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9710172414779663
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d3793b508a6313e31d2dc0/dZGO3zW131TSgqs5RKci1.jpeg?w=200&h=200&f=face
          fullname: Dr. Hicham Badri
          isHf: false
          isPro: false
          name: mobicham
          type: user
        html: "<p>Thanks a lot <span data-props=\"{&quot;user&quot;:&quot;Yhyu13&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Yhyu13\"\
          >@<span class=\"underline\">Yhyu13</span></a></span>\n\n\t</span></span>\
          \  ! We are gonna publish a new 2-bit Mixtral quantized model that is much\
          \ better than this one very soon !</p>\n"
        raw: Thanks a lot @Yhyu13  ! We are gonna publish a new 2-bit Mixtral quantized
          model that is much better than this one very soon !
        updatedAt: '2023-12-16T10:37:20.497Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - Yhyu13
    id: 657d7de0869d5bb0e557e201
    type: comment
  author: mobicham
  content: Thanks a lot @Yhyu13  ! We are gonna publish a new 2-bit Mixtral quantized
    model that is much better than this one very soon !
  created_at: 2023-12-16 10:37:20+00:00
  edited: false
  hidden: false
  id: 657d7de0869d5bb0e557e201
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d3793b508a6313e31d2dc0/dZGO3zW131TSgqs5RKci1.jpeg?w=200&h=200&f=face
      fullname: Dr. Hicham Badri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mobicham
      type: user
    createdAt: '2023-12-18T16:39:45.000Z'
    data:
      edited: false
      editors:
      - mobicham
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46239861845970154
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d3793b508a6313e31d2dc0/dZGO3zW131TSgqs5RKci1.jpeg?w=200&h=200&f=face
          fullname: Dr. Hicham Badri
          isHf: false
          isPro: false
          name: mobicham
          type: user
        html: '<p>The new models are accessible now:<br>Base: <a href="https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-v0.1-hf-attn-4bit-moe-2bit-HQQ">https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-v0.1-hf-attn-4bit-moe-2bit-HQQ</a><br>Instruct:
          <a href="https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-HQQ">https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-HQQ</a></p>

          '
        raw: 'The new models are accessible now:

          Base: https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-v0.1-hf-attn-4bit-moe-2bit-HQQ

          Instruct: https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-HQQ'
        updatedAt: '2023-12-18T16:39:45.531Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Yhyu13
    id: 658075d16fecfe8936134722
    type: comment
  author: mobicham
  content: 'The new models are accessible now:

    Base: https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-v0.1-hf-attn-4bit-moe-2bit-HQQ

    Instruct: https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-HQQ'
  created_at: 2023-12-18 16:39:45+00:00
  edited: false
  hidden: false
  id: 658075d16fecfe8936134722
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-2bit_g16_s128-HQQ
repo_type: model
status: open
target_branch: null
title: New Exciting quant method
