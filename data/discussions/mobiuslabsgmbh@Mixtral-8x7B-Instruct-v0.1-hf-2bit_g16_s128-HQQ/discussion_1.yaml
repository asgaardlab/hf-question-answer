!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Mlemoyne
conflicting_files: null
created_at: 2023-12-13 00:15:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/521116a47e6629414bdd7ead0c366749.svg
      fullname: M. Lemoyne
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mlemoyne
      type: user
    createdAt: '2023-12-13T00:15:39.000Z'
    data:
      edited: false
      editors:
      - Mlemoyne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9754508137702942
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/521116a47e6629414bdd7ead0c366749.svg
          fullname: M. Lemoyne
          isHf: false
          isPro: false
          name: Mlemoyne
          type: user
        html: '<p>ANyone tested it?</p>

          '
        raw: ANyone tested it?
        updatedAt: '2023-12-13T00:15:39.970Z'
      numEdits: 0
      reactions: []
    id: 6578f7abc37954680ad7a70e
    type: comment
  author: Mlemoyne
  content: ANyone tested it?
  created_at: 2023-12-13 00:15:39+00:00
  edited: false
  hidden: false
  id: 6578f7abc37954680ad7a70e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d3793b508a6313e31d2dc0/dZGO3zW131TSgqs5RKci1.jpeg?w=200&h=200&f=face
      fullname: Dr. Hicham Badri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mobicham
      type: user
    createdAt: '2023-12-13T08:44:02.000Z'
    data:
      edited: false
      editors:
      - mobicham
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8282022476196289
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d3793b508a6313e31d2dc0/dZGO3zW131TSgqs5RKci1.jpeg?w=200&h=200&f=face
          fullname: Dr. Hicham Badri
          isHf: false
          isPro: false
          name: mobicham
          type: user
        html: '<p>You can find numbers for the base model and comparison with bitsandbytes
          below:</p>

          <pre><code>Wikitext2 PPL/Memory: HQQ vs bitsandbytes (BNB)


          #8-bit (group_size=128)

          Mixtral-8x7B-v0.1 / BNB : 3.64 | (54.5 GB)

          Mixtral-8x7B-v0.1 / HQQ : 3.63 | (47 GB)


          <a href="/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-2bit_g16_s128-HQQ/discussions/4">#4</a>-bit
          (group_size=64)

          Mixtral-8x7B-v0.1 / BNB : 3.97 | (27 GB)

          Mixtral-8x7B-v0.1 / HQQ : 3.79 | (26 GB)


          <a href="/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-2bit_g16_s128-HQQ/discussions/3">#3</a>-bit
          (group_size=128)

          Mixtral-8x7B-v0.1 / HQQ : 4.76 | (21.8 GB)


          <a href="/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-2bit_g16_s128-HQQ/discussions/2">#2</a>-bit
          (group_size=16 | scale_g128/zero=8-bit):

          Mixtral-8x7B-v0.1 / HQQ : 5.90 | (18 GB)

          </code></pre>

          <p>I wouldn''t recommend using the 2-bit in production, rather use the 4-bit
          version. But we wanted to provide the community with a model that can run
          on  a single 24 GB card so they can play with it and see if they like the
          feel of the model compared to others. I have personally played with it and
          the instruct model is working surprisingly fine with this 2-bit settings.
          </p>

          '
        raw: "You can find numbers for the base model and comparison with bitsandbytes\
          \ below:\n\n    Wikitext2 PPL/Memory: HQQ vs bitsandbytes (BNB)\n\n    #8-bit\
          \ (group_size=128)\n    Mixtral-8x7B-v0.1 / BNB : 3.64 | (54.5 GB)\n   \
          \ Mixtral-8x7B-v0.1 / HQQ : 3.63 | (47 GB)\n\n    #4-bit (group_size=64)\n\
          \    Mixtral-8x7B-v0.1 / BNB : 3.97 | (27 GB)\n    Mixtral-8x7B-v0.1 / HQQ\
          \ : 3.79 | (26 GB)\n\n    #3-bit (group_size=128)\n    Mixtral-8x7B-v0.1\
          \ / HQQ : 4.76 | (21.8 GB)\n\n    #2-bit (group_size=16 | scale_g128/zero=8-bit):\n\
          \    Mixtral-8x7B-v0.1 / HQQ : 5.90 | (18 GB)\n\nI wouldn't recommend using\
          \ the 2-bit in production, rather use the 4-bit version. But we wanted to\
          \ provide the community with a model that can run on  a single 24 GB card\
          \ so they can play with it and see if they like the feel of the model compared\
          \ to others. I have personally played with it and the instruct model is\
          \ working surprisingly fine with this 2-bit settings. "
        updatedAt: '2023-12-13T08:44:02.876Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - appoose
        - ulymp
        - Yhyu13
        - format37
    id: 65796ed2d54197901f9111b1
    type: comment
  author: mobicham
  content: "You can find numbers for the base model and comparison with bitsandbytes\
    \ below:\n\n    Wikitext2 PPL/Memory: HQQ vs bitsandbytes (BNB)\n\n    #8-bit\
    \ (group_size=128)\n    Mixtral-8x7B-v0.1 / BNB : 3.64 | (54.5 GB)\n    Mixtral-8x7B-v0.1\
    \ / HQQ : 3.63 | (47 GB)\n\n    #4-bit (group_size=64)\n    Mixtral-8x7B-v0.1\
    \ / BNB : 3.97 | (27 GB)\n    Mixtral-8x7B-v0.1 / HQQ : 3.79 | (26 GB)\n\n   \
    \ #3-bit (group_size=128)\n    Mixtral-8x7B-v0.1 / HQQ : 4.76 | (21.8 GB)\n\n\
    \    #2-bit (group_size=16 | scale_g128/zero=8-bit):\n    Mixtral-8x7B-v0.1 /\
    \ HQQ : 5.90 | (18 GB)\n\nI wouldn't recommend using the 2-bit in production,\
    \ rather use the 4-bit version. But we wanted to provide the community with a\
    \ model that can run on  a single 24 GB card so they can play with it and see\
    \ if they like the feel of the model compared to others. I have personally played\
    \ with it and the instruct model is working surprisingly fine with this 2-bit\
    \ settings. "
  created_at: 2023-12-13 08:44:02+00:00
  edited: false
  hidden: false
  id: 65796ed2d54197901f9111b1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-2bit_g16_s128-HQQ
repo_type: model
status: open
target_branch: null
title: How is the performance of the model with 2bits only?
