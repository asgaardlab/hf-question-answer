!!python/object:huggingface_hub.community.DiscussionWithDetails
author: baby1
conflicting_files: null
created_at: 2023-03-21 11:39:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3339361a9d1f500eb6f3e5e2a3c8955d.svg
      fullname: Astar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: baby1
      type: user
    createdAt: '2023-03-21T12:39:43.000Z'
    data:
      edited: false
      editors:
      - baby1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3339361a9d1f500eb6f3e5e2a3c8955d.svg
          fullname: Astar
          isHf: false
          isPro: false
          name: baby1
          type: user
        html: '<p>using  4 * A100 80GB GPU,<br>how much time will cost on train 13B
          data<br>it seem to train 7B about 24 hours?</p>

          <p>the same command with stanford alpaca<br><a rel="nofollow" href="https://github.com/tatsu-lab/stanford_alpaca#fine-tuning">https://github.com/tatsu-lab/stanford_alpaca#fine-tuning</a><br>torchrun
          --nproc_per_node=4 --master_port= train.py <br>    --model_name_or_path  <br>    --data_path
          ./alpaca_data.json <br>    --bf16 True <br>    --output_dir  <br>    --num_train_epochs
          3 <br>    --per_device_train_batch_size 4 <br>    --per_device_eval_batch_size
          4 <br>    --gradient_accumulation_steps 8 <br>    --evaluation_strategy
          "no" <br>    --save_strategy "steps" <br>    --save_steps 2000 <br>    --save_total_limit
          1 <br>    --learning_rate 2e-5 <br>    --weight_decay 0. <br>    --warmup_ratio
          0.03 <br>    --lr_scheduler_type "cosine" <br>    --logging_steps 1 <br>    --fsdp
          "full_shard auto_wrap" <br>    --fsdp_transformer_layer_cls_to_wrap ''LLaMADecoderLayer''
          <br>    --tf32 True</p>

          '
        raw: "using  4 * A100 80GB GPU, \r\nhow much time will cost on train 13B data\r\
          \nit seem to train 7B about 24 hours?\r\n\r\nthe same command with stanford\
          \ alpaca \r\nhttps://github.com/tatsu-lab/stanford_alpaca#fine-tuning\r\n\
          torchrun --nproc_per_node=4 --master_port=<your_random_port> train.py \\\
          \r\n    --model_name_or_path <your_path_to_hf_converted_llama_ckpt_and_tokenizer>\
          \ \\\r\n    --data_path ./alpaca_data.json \\\r\n    --bf16 True \\\r\n\
          \    --output_dir <your_output_dir> \\\r\n    --num_train_epochs 3 \\\r\n\
          \    --per_device_train_batch_size 4 \\\r\n    --per_device_eval_batch_size\
          \ 4 \\\r\n    --gradient_accumulation_steps 8 \\\r\n    --evaluation_strategy\
          \ \"no\" \\\r\n    --save_strategy \"steps\" \\\r\n    --save_steps 2000\
          \ \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate 2e-5 \\\r\n \
          \   --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type\
          \ \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --fsdp \"full_shard auto_wrap\"\
          \ \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LLaMADecoderLayer' \\\r\
          \n    --tf32 True"
        updatedAt: '2023-03-21T12:39:43.591Z'
      numEdits: 0
      reactions: []
    id: 6419a58fb4adb0e101b0a72f
    type: comment
  author: baby1
  content: "using  4 * A100 80GB GPU, \r\nhow much time will cost on train 13B data\r\
    \nit seem to train 7B about 24 hours?\r\n\r\nthe same command with stanford alpaca\
    \ \r\nhttps://github.com/tatsu-lab/stanford_alpaca#fine-tuning\r\ntorchrun --nproc_per_node=4\
    \ --master_port=<your_random_port> train.py \\\r\n    --model_name_or_path <your_path_to_hf_converted_llama_ckpt_and_tokenizer>\
    \ \\\r\n    --data_path ./alpaca_data.json \\\r\n    --bf16 True \\\r\n    --output_dir\
    \ <your_output_dir> \\\r\n    --num_train_epochs 3 \\\r\n    --per_device_train_batch_size\
    \ 4 \\\r\n    --per_device_eval_batch_size 4 \\\r\n    --gradient_accumulation_steps\
    \ 8 \\\r\n    --evaluation_strategy \"no\" \\\r\n    --save_strategy \"steps\"\
    \ \\\r\n    --save_steps 2000 \\\r\n    --save_total_limit 1 \\\r\n    --learning_rate\
    \ 2e-5 \\\r\n    --weight_decay 0. \\\r\n    --warmup_ratio 0.03 \\\r\n    --lr_scheduler_type\
    \ \"cosine\" \\\r\n    --logging_steps 1 \\\r\n    --fsdp \"full_shard auto_wrap\"\
    \ \\\r\n    --fsdp_transformer_layer_cls_to_wrap 'LLaMADecoderLayer' \\\r\n  \
    \  --tf32 True"
  created_at: 2023-03-21 11:39:43+00:00
  edited: false
  hidden: false
  id: 6419a58fb4adb0e101b0a72f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3339361a9d1f500eb6f3e5e2a3c8955d.svg
      fullname: Astar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: baby1
      type: user
    createdAt: '2023-03-21T13:33:05.000Z'
    data:
      edited: false
      editors:
      - baby1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3339361a9d1f500eb6f3e5e2a3c8955d.svg
          fullname: Astar
          isHf: false
          isPro: false
          name: baby1
          type: user
        html: '<p>it is the best way to accelerate?<br>fsdp = ShardingStrategy.FULL_SHARD</p>

          '
        raw: 'it is the best way to accelerate?

          fsdp = ShardingStrategy.FULL_SHARD'
        updatedAt: '2023-03-21T13:33:05.284Z'
      numEdits: 0
      reactions: []
    id: 6419b211e4e6552b05d908ff
    type: comment
  author: baby1
  content: 'it is the best way to accelerate?

    fsdp = ShardingStrategy.FULL_SHARD'
  created_at: 2023-03-21 12:33:05+00:00
  edited: false
  hidden: false
  id: 6419b211e4e6552b05d908ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb448511090323ea9c8654a500963dc3.svg
      fullname: dd
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Dogge
      type: user
    createdAt: '2023-03-21T19:45:09.000Z'
    data:
      edited: true
      editors:
      - Dogge
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb448511090323ea9c8654a500963dc3.svg
          fullname: dd
          isHf: false
          isPro: false
          name: Dogge
          type: user
        html: '<p>I used <a rel="nofollow" href="https://github.com/tloen/alpaca-lora.git">https://github.com/tloen/alpaca-lora.git</a>
          finetune.py before export_hf_checkpoint.py</p>

          <p>maybe it takes 10hours less.</p>

          <p>here is my repo include 13B code.</p>

          <p><a rel="nofollow" href="https://github.com/Yanggum/alpaca-lora.git">https://github.com/Yanggum/alpaca-lora.git</a></p>

          '
        raw: 'I used https://github.com/tloen/alpaca-lora.git finetune.py before export_hf_checkpoint.py


          maybe it takes 10hours less.


          here is my repo include 13B code.


          https://github.com/Yanggum/alpaca-lora.git'
        updatedAt: '2023-03-21T20:00:32.282Z'
      numEdits: 2
      reactions: []
    id: 641a0945a366a5e7b6a6dee5
    type: comment
  author: Dogge
  content: 'I used https://github.com/tloen/alpaca-lora.git finetune.py before export_hf_checkpoint.py


    maybe it takes 10hours less.


    here is my repo include 13B code.


    https://github.com/Yanggum/alpaca-lora.git'
  created_at: 2023-03-21 18:45:09+00:00
  edited: true
  hidden: false
  id: 641a0945a366a5e7b6a6dee5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-03-22T00:07:56.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>Sir the model doesn''t load how do we fix it? What do you use to
          run the model? Also isn''t that for LORAs not full fine tunes?</p>

          '
        raw: Sir the model doesn't load how do we fix it? What do you use to run the
          model? Also isn't that for LORAs not full fine tunes?
        updatedAt: '2023-03-22T00:07:56.532Z'
      numEdits: 0
      reactions: []
    id: 641a46dcf1ad1c1173d9b25e
    type: comment
  author: teknium
  content: Sir the model doesn't load how do we fix it? What do you use to run the
    model? Also isn't that for LORAs not full fine tunes?
  created_at: 2023-03-21 23:07:56+00:00
  edited: false
  hidden: false
  id: 641a46dcf1ad1c1173d9b25e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Dogge/alpaca-13b
repo_type: model
status: open
target_branch: null
title: How long  train on 13B
