!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fuliu2023
conflicting_files: null
created_at: 2023-08-12 13:54:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/81e77f378de0d67c870fc560f6446e3c.svg
      fullname: guangwei she
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fuliu2023
      type: user
    createdAt: '2023-08-12T14:54:15.000Z'
    data:
      edited: false
      editors:
      - fuliu2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4768568277359009
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/81e77f378de0d67c870fc560f6446e3c.svg
          fullname: guangwei she
          isHf: false
          isPro: false
          name: fuliu2023
          type: user
        html: "<p>import torch<br>from langchain import PromptTemplate, LLMChain<br>from\
          \ langchain.llms import HuggingFacePipeline<br>from transformers import\
          \ AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM</p>\n\
          <p>model_id = \"baichuan-inc/Baichuan-13B-Chat\"<br>tokenizer = AutoTokenizer.from_pretrained(model_id,\
          \ use_fast=False, trust_remote_code=True)<br>model = AutoModelForCausalLM.from_pretrained(model_id,\
          \ device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)<br>#model\
          \ = model.quantize(8).cuda()</p>\n<p>pipe = pipeline(<br>    \"text-generation\"\
          ,<br>    model=model,<br>    tokenizer=tokenizer,<br>    max_length=1000<br>)</p>\n\
          <p>local_llm = HuggingFacePipeline(pipeline=pipe)<br>print(local_llm('What\
          \ is the capital of France? '))</p>\n<p>template = \"\"\"Question: {question}\
          \ Answer: Let's think step by step.\"\"\"<br>prompt = PromptTemplate(template=template,\
          \ input_variables=[\"question\"])</p>\n<p>llm_chain = LLMChain(prompt=prompt,\
          \ llm=local_llm)<br>question = \"What is the capital of England?\"<br>print(llm_chain.run(question))</p>\n\
          <p>\u4F7F\u7528langchain\u8C03\u7528\u62A5\u9519The model 'BaichuanForCausalLM'\
          \ is not supported for text-generation.</p>\n"
        raw: "import torch\r\nfrom langchain import PromptTemplate, LLMChain\r\nfrom\
          \ langchain.llms import HuggingFacePipeline\r\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\r\
          \n\r\nmodel_id = \"baichuan-inc/Baichuan-13B-Chat\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_id,\
          \ use_fast=False, trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\
          \ device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\r\
          \n#model = model.quantize(8).cuda()\r\n\r\n\r\npipe = pipeline(\r\n    \"\
          text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n \
          \   max_length=1000\r\n)\r\n\r\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\r\
          \nprint(local_llm('What is the capital of France? '))\r\n\r\n\r\ntemplate\
          \ = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\"\r\
          \nprompt = PromptTemplate(template=template, input_variables=[\"question\"\
          ])\r\n\r\nllm_chain = LLMChain(prompt=prompt, llm=local_llm)\r\nquestion\
          \ = \"What is the capital of England?\"\r\nprint(llm_chain.run(question))\r\
          \n\r\n\u4F7F\u7528langchain\u8C03\u7528\u62A5\u9519The model 'BaichuanForCausalLM'\
          \ is not supported for text-generation."
        updatedAt: '2023-08-12T14:54:15.395Z'
      numEdits: 0
      reactions: []
    id: 64d79d1736b05ef8fff04360
    type: comment
  author: fuliu2023
  content: "import torch\r\nfrom langchain import PromptTemplate, LLMChain\r\nfrom\
    \ langchain.llms import HuggingFacePipeline\r\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\r\n\r\nmodel_id = \"baichuan-inc/Baichuan-13B-Chat\"\
    \r\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, trust_remote_code=True)\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\",\
    \ torch_dtype=torch.float16, trust_remote_code=True)\r\n#model = model.quantize(8).cuda()\r\
    \n\r\n\r\npipe = pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\n\
    \    tokenizer=tokenizer,\r\n    max_length=1000\r\n)\r\n\r\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\r\
    \nprint(local_llm('What is the capital of France? '))\r\n\r\n\r\ntemplate = \"\
    \"\"Question: {question} Answer: Let's think step by step.\"\"\"\r\nprompt = PromptTemplate(template=template,\
    \ input_variables=[\"question\"])\r\n\r\nllm_chain = LLMChain(prompt=prompt, llm=local_llm)\r\
    \nquestion = \"What is the capital of England?\"\r\nprint(llm_chain.run(question))\r\
    \n\r\n\u4F7F\u7528langchain\u8C03\u7528\u62A5\u9519The model 'BaichuanForCausalLM'\
    \ is not supported for text-generation."
  created_at: 2023-08-12 13:54:15+00:00
  edited: false
  hidden: false
  id: 64d79d1736b05ef8fff04360
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 28
repo_id: baichuan-inc/Baichuan-13B-Chat
repo_type: model
status: open
target_branch: null
title: The model 'BaichuanForCausalLM' is not supported for text-generation.
