!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rredl
conflicting_files: null
created_at: 2023-12-11 10:11:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c3cbc21149564eff6662258b214e089.svg
      fullname: Robert Redl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rredl
      type: user
    createdAt: '2023-12-11T10:11:19.000Z'
    data:
      edited: false
      editors:
      - rredl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47961583733558655
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c3cbc21149564eff6662258b214e089.svg
          fullname: Robert Redl
          isHf: false
          isPro: false
          name: rredl
          type: user
        html: "<p>I'm trying to use this model with TGI (Version 1.2, docker image),\
          \ but I get the following error:</p>\n<pre><code>2023-12-11T10:08:09.585078Z\
          \ ERROR text_generation_launcher: Method Decode encountered an error.\n\
          Traceback (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in &lt;module&gt;\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
          , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py\"\
          , line 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.10/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 636, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/grpc_interceptor/server.py\"\
          , line 165, in invoke_intercept_method\n    return await self.intercept(\n\
          &gt; File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/interceptor.py\"\
          , line 21, in intercept\n    return await response\n  File \"/opt/conda/lib/python3.10/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
          , line 82, in _unary_interceptor\n    raise error\n  File \"/opt/conda/lib/python3.10/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
          , line 73, in _unary_interceptor\n    return await behavior(request_or_iterator,\
          \ context)\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 121, in Decode\n    generations, next_batch = self.model.generate_token(batch)\n\
          \  File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\n\
          \    return func(*args, **kwds)\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 893, in generate_token\n    next_token_text, prefix_offset, read_offset\
          \ = self.decode_token(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/model.py\"\
          , line 83, in decode_token\n    new_text = self.tokenizer.decode(\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 3550, in decode\n    return self._decode(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py\"\
          , line 938, in _decode\n    filtered_tokens = self.convert_ids_to_tokens(token_ids,\
          \ skip_special_tokens=skip_special_tokens)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py\"\
          , line 919, in convert_ids_to_tokens\n    tokens.append(self._convert_id_to_token(index))\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py\"\
          , line 243, in _convert_id_to_token\n    token = self.sp_model.IdToPiece(index)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 1045, in _batched_func\n    return _func(self, arg)\n  File \"/opt/conda/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 1038, in _func\n    raise IndexError('piece id is out of range.')\n\
          IndexError: piece id is out of range.\n</code></pre>\n"
        raw: "I'm trying to use this model with TGI (Version 1.2, docker image), but\
          \ I get the following error:\r\n\r\n```\r\n2023-12-11T10:08:09.585078Z ERROR\
          \ text_generation_launcher: Method Decode encountered an error.\r\nTraceback\
          \ (most recent call last):\r\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
          , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.10/site-packages/typer/core.py\", line 778, in main\r\
          \n    return _main(\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
          , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
          , line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.10/site-packages/typer/main.py\", line 683, in wrapper\r\
          \n    return callback(**use_params)  # type: ignore\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py\"\
          , line 83, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 207, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.10/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 636, in run_until_complete\r\
          \n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 603, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 1909, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\"\
          , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/grpc_interceptor/server.py\"\
          , line 165, in invoke_intercept_method\r\n    return await self.intercept(\r\
          \n> File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/interceptor.py\"\
          , line 21, in intercept\r\n    return await response\r\n  File \"/opt/conda/lib/python3.10/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
          , line 82, in _unary_interceptor\r\n    raise error\r\n  File \"/opt/conda/lib/python3.10/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
          , line 73, in _unary_interceptor\r\n    return await behavior(request_or_iterator,\
          \ context)\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 121, in Decode\r\n    generations, next_batch = self.model.generate_token(batch)\r\
          \n  File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\r\
          \n    return func(*args, **kwds)\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py\"\
          , line 893, in generate_token\r\n    next_token_text, prefix_offset, read_offset\
          \ = self.decode_token(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/model.py\"\
          , line 83, in decode_token\r\n    new_text = self.tokenizer.decode(\r\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
          , line 3550, in decode\r\n    return self._decode(\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py\"\
          , line 938, in _decode\r\n    filtered_tokens = self.convert_ids_to_tokens(token_ids,\
          \ skip_special_tokens=skip_special_tokens)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py\"\
          , line 919, in convert_ids_to_tokens\r\n    tokens.append(self._convert_id_to_token(index))\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py\"\
          , line 243, in _convert_id_to_token\r\n    token = self.sp_model.IdToPiece(index)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 1045, in _batched_func\r\n    return _func(self, arg)\r\n  File \"\
          /opt/conda/lib/python3.10/site-packages/sentencepiece/__init__.py\", line\
          \ 1038, in _func\r\n    raise IndexError('piece id is out of range.')\r\n\
          IndexError: piece id is out of range.\r\n```"
        updatedAt: '2023-12-11T10:11:19.872Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - qnnect
    id: 6576e0476da136b50fef4abe
    type: comment
  author: rredl
  content: "I'm trying to use this model with TGI (Version 1.2, docker image), but\
    \ I get the following error:\r\n\r\n```\r\n2023-12-11T10:08:09.585078Z ERROR text_generation_launcher:\
    \ Method Decode encountered an error.\r\nTraceback (most recent call last):\r\n\
    \  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\n   \
    \ sys.exit(app())\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
    , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\n \
    \ File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line 1157, in\
    \ __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
    , line 778, in main\r\n    return _main(\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
    , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
    , line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line 1434,\
    \ in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\
    /opt/conda/lib/python3.10/site-packages/click/core.py\", line 783, in invoke\r\
    \n    return __callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
    , line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n\
    \  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py\"\
    , line 83, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
    , line 207, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.10/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 636, in run_until_complete\r\n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 603, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 1909, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\"\
    , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n \
    \ File \"/opt/conda/lib/python3.10/site-packages/grpc_interceptor/server.py\"\
    , line 165, in invoke_intercept_method\r\n    return await self.intercept(\r\n\
    > File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/interceptor.py\"\
    , line 21, in intercept\r\n    return await response\r\n  File \"/opt/conda/lib/python3.10/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
    , line 82, in _unary_interceptor\r\n    raise error\r\n  File \"/opt/conda/lib/python3.10/site-packages/opentelemetry/instrumentation/grpc/_aio_server.py\"\
    , line 73, in _unary_interceptor\r\n    return await behavior(request_or_iterator,\
    \ context)\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
    , line 121, in Decode\r\n    generations, next_batch = self.model.generate_token(batch)\r\
    \n  File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\r\n  \
    \  return func(*args, **kwds)\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_causal_lm.py\"\
    , line 893, in generate_token\r\n    next_token_text, prefix_offset, read_offset\
    \ = self.decode_token(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/model.py\"\
    , line 83, in decode_token\r\n    new_text = self.tokenizer.decode(\r\n  File\
    \ \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\"\
    , line 3550, in decode\r\n    return self._decode(\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py\"\
    , line 938, in _decode\r\n    filtered_tokens = self.convert_ids_to_tokens(token_ids,\
    \ skip_special_tokens=skip_special_tokens)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py\"\
    , line 919, in convert_ids_to_tokens\r\n    tokens.append(self._convert_id_to_token(index))\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py\"\
    , line 243, in _convert_id_to_token\r\n    token = self.sp_model.IdToPiece(index)\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
    , line 1045, in _batched_func\r\n    return _func(self, arg)\r\n  File \"/opt/conda/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
    , line 1038, in _func\r\n    raise IndexError('piece id is out of range.')\r\n\
    IndexError: piece id is out of range.\r\n```"
  created_at: 2023-12-11 10:11:19+00:00
  edited: false
  hidden: false
  id: 6576e0476da136b50fef4abe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: LeoLM/leo-hessianai-70b-chat
repo_type: model
status: open
target_branch: null
title: Not compatible with text-generation-inference
