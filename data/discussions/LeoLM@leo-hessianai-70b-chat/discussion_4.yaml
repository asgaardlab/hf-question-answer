!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Fabian96
conflicting_files: null
created_at: 2023-12-12 14:10:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cffc93b62bcadfb23bac351a27b51fe7.svg
      fullname: Fabian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fabian96
      type: user
    createdAt: '2023-12-12T14:10:22.000Z'
    data:
      edited: false
      editors:
      - Fabian96
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8825045824050903
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cffc93b62bcadfb23bac351a27b51fe7.svg
          fullname: Fabian
          isHf: false
          isPro: false
          name: Fabian96
          type: user
        html: '<p>Hi, </p>

          <p>i get the error: (model has 32128, but models/LionM-70B/tokenizer.model
          has 32000) when trying to convert the model to FP16 format before quantization.
          This seems to be a mismatch between the used tokenizer and the config.json.</p>

          <p>An easy fix is to just set the  "vocab_size" parameter to 32000, however,
          this results inn problems further down the line, when quantizing. </p>

          <p>Any suggestions?</p>

          '
        raw: "Hi, \r\n\r\ni get the error: (model has 32128, but models/LionM-70B/tokenizer.model\
          \ has 32000) when trying to convert the model to FP16 format before quantization.\
          \ This seems to be a mismatch between the used tokenizer and the config.json.\r\
          \n\r\nAn easy fix is to just set the  \"vocab_size\" parameter to 32000,\
          \ however, this results inn problems further down the line, when quantizing.\
          \ \r\n\r\nAny suggestions?"
        updatedAt: '2023-12-12T14:10:22.686Z'
      numEdits: 0
      reactions: []
    id: 657869ce6cb1dcb957e78448
    type: comment
  author: Fabian96
  content: "Hi, \r\n\r\ni get the error: (model has 32128, but models/LionM-70B/tokenizer.model\
    \ has 32000) when trying to convert the model to FP16 format before quantization.\
    \ This seems to be a mismatch between the used tokenizer and the config.json.\r\
    \n\r\nAn easy fix is to just set the  \"vocab_size\" parameter to 32000, however,\
    \ this results inn problems further down the line, when quantizing. \r\n\r\nAny\
    \ suggestions?"
  created_at: 2023-12-12 14:10:22+00:00
  edited: false
  hidden: false
  id: 657869ce6cb1dcb957e78448
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: LeoLM/leo-hessianai-70b-chat
repo_type: model
status: open
target_branch: null
title: Trying to quantize the model using llama.cpp
