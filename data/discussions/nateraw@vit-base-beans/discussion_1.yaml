!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sudo-s
conflicting_files: null
created_at: 2022-06-05 13:43:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84bfca2828c8c7e557ab68fc253e085d.svg
      fullname: Ayoub Omalek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sudo-s
      type: user
    createdAt: '2022-06-05T14:43:55.000Z'
    data:
      edited: false
      editors:
      - sudo-s
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84bfca2828c8c7e557ab68fc253e085d.svg
          fullname: Ayoub Omalek
          isHf: false
          isPro: false
          name: sudo-s
          type: user
        html: '<p>Hi All, Please how can I make predictions?</p>

          '
        raw: Hi All, Please how can I make predictions?
        updatedAt: '2022-06-05T14:43:55.787Z'
      numEdits: 0
      reactions: []
    id: 629cc12b3a3221bb210e8b1e
    type: comment
  author: sudo-s
  content: Hi All, Please how can I make predictions?
  created_at: 2022-06-05 13:43:55+00:00
  edited: false
  hidden: false
  id: 629cc12b3a3221bb210e8b1e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594936097363-noauth.jpeg?w=200&h=200&f=face
      fullname: Nate Raw
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nateraw
      type: user
    createdAt: '2022-06-06T20:51:38.000Z'
    data:
      edited: false
      editors:
      - nateraw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594936097363-noauth.jpeg?w=200&h=200&f=face
          fullname: Nate Raw
          isHf: false
          isPro: false
          name: nateraw
          type: user
        html: "<p>Hey there, you can make predictions with the <code>pipeline</code>\
          \ abstraction.</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">import</span> requests\n<span class=\"hljs-keyword\">from</span>\
          \ PIL <span class=\"hljs-keyword\">import</span> Image\n\n<span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ pipeline\n\n<span class=\"hljs-comment\"># Initialize pipeline from repo</span>\n\
          pipe = pipeline(model=<span class=\"hljs-string\">\"nateraw/vit-base-beans\"\
          </span>)\n\n<span class=\"hljs-comment\"># Get example image from repo</span>\n\
          url = <span class=\"hljs-string\">'https://huggingface.co/nateraw/vit-base-beans/resolve/main/angular_leaf_spot.jpeg'</span>\n\
          image = Image.<span class=\"hljs-built_in\">open</span>(requests.get(url,\
          \ stream=<span class=\"hljs-literal\">True</span>).raw)\n\npreds = pipe(image)\n\
          </code></pre>\n<p>When you print <code>preds</code>, you get this:</p>\n\
          <pre><code>[{'label': 'angular_leaf_spot', 'score': 0.9455905556678772},\n\
          \ {'label': 'bean_rust', 'score': 0.0272879246622324},\n {'label': 'healthy',\
          \ 'score': 0.027121491730213165}]\n</code></pre>\n"
        raw: "Hey there, you can make predictions with the `pipeline` abstraction.\n\
          \n```python\nimport requests\nfrom PIL import Image\n\nfrom transformers\
          \ import pipeline\n\n# Initialize pipeline from repo\npipe = pipeline(model=\"\
          nateraw/vit-base-beans\")\n\n# Get example image from repo\nurl = 'https://huggingface.co/nateraw/vit-base-beans/resolve/main/angular_leaf_spot.jpeg'\n\
          image = Image.open(requests.get(url, stream=True).raw)\n\npreds = pipe(image)\n\
          ```\n\nWhen you print `preds`, you get this:\n\n```\n[{'label': 'angular_leaf_spot',\
          \ 'score': 0.9455905556678772},\n {'label': 'bean_rust', 'score': 0.0272879246622324},\n\
          \ {'label': 'healthy', 'score': 0.027121491730213165}]\n```"
        updatedAt: '2022-06-06T20:51:38.576Z'
      numEdits: 0
      reactions: []
    id: 629e68da46b4826be2cbdcca
    type: comment
  author: nateraw
  content: "Hey there, you can make predictions with the `pipeline` abstraction.\n\
    \n```python\nimport requests\nfrom PIL import Image\n\nfrom transformers import\
    \ pipeline\n\n# Initialize pipeline from repo\npipe = pipeline(model=\"nateraw/vit-base-beans\"\
    )\n\n# Get example image from repo\nurl = 'https://huggingface.co/nateraw/vit-base-beans/resolve/main/angular_leaf_spot.jpeg'\n\
    image = Image.open(requests.get(url, stream=True).raw)\n\npreds = pipe(image)\n\
    ```\n\nWhen you print `preds`, you get this:\n\n```\n[{'label': 'angular_leaf_spot',\
    \ 'score': 0.9455905556678772},\n {'label': 'bean_rust', 'score': 0.0272879246622324},\n\
    \ {'label': 'healthy', 'score': 0.027121491730213165}]\n```"
  created_at: 2022-06-06 19:51:38+00:00
  edited: false
  hidden: false
  id: 629e68da46b4826be2cbdcca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84bfca2828c8c7e557ab68fc253e085d.svg
      fullname: Ayoub Omalek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sudo-s
      type: user
    createdAt: '2022-06-07T01:37:25.000Z'
    data:
      edited: false
      editors:
      - sudo-s
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84bfca2828c8c7e557ab68fc253e085d.svg
          fullname: Ayoub Omalek
          isHf: false
          isPro: false
          name: sudo-s
          type: user
        html: '<p>Thanks a lot<br>I have a question : I want to use your code on a
          another datasets , so how can I save my model like you did with yours "nateraw/vit-base-beans"
          so that I can use my model for predections?</p>

          '
        raw: 'Thanks a lot

          I have a question : I want to use your code on a another datasets , so how
          can I save my model like you did with yours "nateraw/vit-base-beans" so
          that I can use my model for predections?'
        updatedAt: '2022-06-07T01:37:25.969Z'
      numEdits: 0
      reactions: []
    id: 629eabd51fec321a2d34f963
    type: comment
  author: sudo-s
  content: 'Thanks a lot

    I have a question : I want to use your code on a another datasets , so how can
    I save my model like you did with yours "nateraw/vit-base-beans" so that I can
    use my model for predections?'
  created_at: 2022-06-07 00:37:25+00:00
  edited: false
  hidden: false
  id: 629eabd51fec321a2d34f963
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22817fafcb798fe4460458424fb3f5e9.svg
      fullname: Phillip Bock
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: friesel
      type: user
    createdAt: '2023-03-24T12:52:55.000Z'
    data:
      edited: false
      editors:
      - friesel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22817fafcb798fe4460458424fb3f5e9.svg
          fullname: Phillip Bock
          isHf: false
          isPro: false
          name: friesel
          type: user
        html: '<p>Cant get inference to run, even though training went well. When
          I then take your code above I get:</p>

          <p>Downloading pytorch_model.bin:   0%|                                                                                                                                             |
          0.00/343M [00:10&lt;?, ?B/s]<br>Traceback (most recent call last):<br>  File
          "inference.py", line 15, in <br>    pipe = pipeline(model="nateraw/vit-base-beans")<br>  File
          "/usr/local/lib/python3.8/dist-packages/transformers/pipelines/<strong>init</strong>.py",
          line 776, in pipeline<br>    framework, model = infer_framework_load_model(<br>  File
          "/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py",
          line 271, in infer_framework_load_model<br>    raise ValueError(f"Could
          not load model {model} with any of the following classes: {class_tuple}.")<br>ValueError:
          Could not load model nateraw/vit-base-beans with any of the following classes:
          (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForImageClassification''&gt;,
          &lt;class ''transformers.models.vit.modeling_vit.ViTForImageClassification''&gt;).</p>

          '
        raw: "Cant get inference to run, even though training went well. When I then\
          \ take your code above I get:\n\nDownloading pytorch_model.bin:   0%|  \
          \                                                                      \
          \                                                                     |\
          \ 0.00/343M [00:10<?, ?B/s]\nTraceback (most recent call last):\n  File\
          \ \"inference.py\", line 15, in <module>\n    pipe = pipeline(model=\"nateraw/vit-base-beans\"\
          )\n  File \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/__init__.py\"\
          , line 776, in pipeline\n    framework, model = infer_framework_load_model(\n\
          \  File \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\"\
          , line 271, in infer_framework_load_model\n    raise ValueError(f\"Could\
          \ not load model {model} with any of the following classes: {class_tuple}.\"\
          )\nValueError: Could not load model nateraw/vit-base-beans with any of the\
          \ following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForImageClassification'>,\
          \ <class 'transformers.models.vit.modeling_vit.ViTForImageClassification'>)."
        updatedAt: '2023-03-24T12:52:55.081Z'
      numEdits: 0
      reactions: []
    id: 641d9d27847134d584b93ba7
    type: comment
  author: friesel
  content: "Cant get inference to run, even though training went well. When I then\
    \ take your code above I get:\n\nDownloading pytorch_model.bin:   0%|        \
    \                                                                            \
    \                                                         | 0.00/343M [00:10<?,\
    \ ?B/s]\nTraceback (most recent call last):\n  File \"inference.py\", line 15,\
    \ in <module>\n    pipe = pipeline(model=\"nateraw/vit-base-beans\")\n  File \"\
    /usr/local/lib/python3.8/dist-packages/transformers/pipelines/__init__.py\", line\
    \ 776, in pipeline\n    framework, model = infer_framework_load_model(\n  File\
    \ \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\", line\
    \ 271, in infer_framework_load_model\n    raise ValueError(f\"Could not load model\
    \ {model} with any of the following classes: {class_tuple}.\")\nValueError: Could\
    \ not load model nateraw/vit-base-beans with any of the following classes: (<class\
    \ 'transformers.models.auto.modeling_auto.AutoModelForImageClassification'>, <class\
    \ 'transformers.models.vit.modeling_vit.ViTForImageClassification'>)."
  created_at: 2023-03-24 11:52:55+00:00
  edited: false
  hidden: false
  id: 641d9d27847134d584b93ba7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fca3efe9303944abaa48f683d6596f64.svg
      fullname: Putri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dhika
      type: user
    createdAt: '2023-05-15T05:05:01.000Z'
    data:
      edited: false
      editors:
      - Dhika
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fca3efe9303944abaa48f683d6596f64.svg
          fullname: Putri
          isHf: false
          isPro: false
          name: Dhika
          type: user
        html: '<blockquote>

          <p>Thanks a lot<br>I have a question : I want to use your code on a another
          datasets , so how can I save my model like you did with yours "nateraw/vit-base-beans"
          so that I can use my model for predections?</p>

          </blockquote>

          <p>i have same question with you, i want save my model so i can use it for
          my prediction? what should i do?</p>

          '
        raw: '> Thanks a lot

          > I have a question : I want to use your code on a another datasets , so
          how can I save my model like you did with yours "nateraw/vit-base-beans"
          so that I can use my model for predections?


          i have same question with you, i want save my model so i can use it for
          my prediction? what should i do?'
        updatedAt: '2023-05-15T05:05:01.997Z'
      numEdits: 0
      reactions: []
    id: 6461bd7d3cc3259ce05ecd04
    type: comment
  author: Dhika
  content: '> Thanks a lot

    > I have a question : I want to use your code on a another datasets , so how can
    I save my model like you did with yours "nateraw/vit-base-beans" so that I can
    use my model for predections?


    i have same question with you, i want save my model so i can use it for my prediction?
    what should i do?'
  created_at: 2023-05-15 04:05:01+00:00
  edited: false
  hidden: false
  id: 6461bd7d3cc3259ce05ecd04
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22817fafcb798fe4460458424fb3f5e9.svg
      fullname: Phillip Bock
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: friesel
      type: user
    createdAt: '2023-05-15T07:23:23.000Z'
    data:
      edited: false
      editors:
      - friesel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22817fafcb798fe4460458424fb3f5e9.svg
          fullname: Phillip Bock
          isHf: false
          isPro: false
          name: friesel
          type: user
        html: '<p>Got it to run round trip.</p>

          <p>Here my training:</p>

          <p>from transformers import ViTFeatureExtractor</p>

          <p>model_name_or_path = ''google/vit-base-patch16-224-in21k''<br>feature_extractor
          = ViTFeatureExtractor.from_pretrained(model_name_or_path)</p>

          <p>print (feature_extractor)<br>print (feature_extractor(image, return_tensors=''pt''))
          # as a torch tensor</p>

          <p>def process_example(example):<br>    inputs = feature_extractor(example[''image''],
          return_tensors=''pt'')<br>    inputs[''labels''] = example[''labels'']<br>    return
          inputs</p>

          <p>process_example(ds[''train''][0])</p>

          <p>def transform(example_batch):<br>    # Take a list of PIL images and
          turn them to pixel values<br>    inputs = feature_extractor([x for x in
          example_batch[''image'']], return_tensors=''pt'')</p>

          <pre><code># Don''t forget to include the labels!

          inputs[''labels''] = example_batch[''labels'']

          return inputs

          </code></pre>

          <p>prepared_ds = ds.with_transform(transform)</p>

          <p>prepared_ds[''train''][0:2]</p>

          <p>import torch</p>

          <p>def collate_fn(batch):<br>    return {<br>        ''pixel_values'': torch.stack([x[''pixel_values'']
          for x in batch]),<br>        ''labels'': torch.tensor([x[''labels''] for
          x in batch])<br>    }</p>

          <p>import numpy as np<br>from datasets import load_metric</p>

          <p>metric = load_metric("accuracy")<br>def compute_metrics(p):<br>    return
          metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)</p>

          <p>from transformers import ViTForImageClassification</p>

          <p>labels = ds[''train''].features[''labels''].names</p>

          <p>print (labels)</p>

          <p>model = ViTForImageClassification.from_pretrained(<br>    model_name_or_path,<br>    num_labels=len(labels),<br>    id2label={str(i):
          c for i, c in enumerate(labels)},<br>    label2id={c: str(i) for i, c in
          enumerate(labels)}<br>).to("cuda")</p>

          <p>from transformers import TrainingArguments</p>

          <p>training_args = TrainingArguments(<br>  output_dir="tarkov_models/vit-base-tarkov-home_bs128_8ep",<br>  per_device_train_batch_size=128,<br>  evaluation_strategy="steps",<br>  num_train_epochs=8,
          # 4<br>  #max_steps=16, # 4<br>  fp16=False, # True<br>  save_steps=1000,
          # 100<br>  eval_steps=1000, # 100<br>  logging_steps=1000,<br>  learning_rate=2e-4,<br>  save_total_limit=3,<br>  remove_unused_columns=False,<br>  push_to_hub=False,<br>  report_to=''tensorboard'',<br>  load_best_model_at_end=True,<br>)</p>

          <p>from transformers import Trainer</p>

          <p>trainer = Trainer(<br>    model=model,<br>    args=training_args,<br>    data_collator=collate_fn,<br>    compute_metrics=compute_metrics,<br>    train_dataset=prepared_ds["train"],<br>    eval_dataset=prepared_ds["validation"],<br>    tokenizer=feature_extractor,<br>)</p>

          <p>train_results = trainer.train()<br>trainer.save_model()<br>trainer.log_metrics("train",
          train_results.metrics)<br>trainer.save_metrics("train", train_results.metrics)<br>trainer.save_state()</p>

          <p>metrics = trainer.evaluate(prepared_ds[''validation''])<br>trainer.log_metrics("eval",
          metrics)<br>trainer.save_metrics("eval", metrics)</p>

          '
        raw: "Got it to run round trip.\n\nHere my training:\n\n\nfrom transformers\
          \ import ViTFeatureExtractor\n\nmodel_name_or_path = 'google/vit-base-patch16-224-in21k'\n\
          feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)\n\
          \nprint (feature_extractor)\nprint (feature_extractor(image, return_tensors='pt'))\
          \ # as a torch tensor\n\n\ndef process_example(example):\n    inputs = feature_extractor(example['image'],\
          \ return_tensors='pt')\n    inputs['labels'] = example['labels']\n    return\
          \ inputs\n\n\nprocess_example(ds['train'][0])\n\ndef transform(example_batch):\n\
          \    # Take a list of PIL images and turn them to pixel values\n    inputs\
          \ = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n\
          \n    # Don't forget to include the labels!\n    inputs['labels'] = example_batch['labels']\n\
          \    return inputs\n\nprepared_ds = ds.with_transform(transform)\n\nprepared_ds['train'][0:2]\n\
          \nimport torch\n\ndef collate_fn(batch):\n    return {\n        'pixel_values':\
          \ torch.stack([x['pixel_values'] for x in batch]),\n        'labels': torch.tensor([x['labels']\
          \ for x in batch])\n    }\n\nimport numpy as np\nfrom datasets import load_metric\n\
          \nmetric = load_metric(\"accuracy\")\ndef compute_metrics(p):\n    return\
          \ metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n\
          \nfrom transformers import ViTForImageClassification\n\nlabels = ds['train'].features['labels'].names\n\
          \nprint (labels)\n\nmodel = ViTForImageClassification.from_pretrained(\n\
          \    model_name_or_path,\n    num_labels=len(labels),\n    id2label={str(i):\
          \ c for i, c in enumerate(labels)},\n    label2id={c: str(i) for i, c in\
          \ enumerate(labels)}\n).to(\"cuda\")\n\nfrom transformers import TrainingArguments\n\
          \ntraining_args = TrainingArguments(\n  output_dir=\"tarkov_models/vit-base-tarkov-home_bs128_8ep\"\
          ,\n  per_device_train_batch_size=128,\n  evaluation_strategy=\"steps\",\n\
          \  num_train_epochs=8, # 4\n  #max_steps=16, # 4\n  fp16=False, # True\n\
          \  save_steps=1000, # 100\n  eval_steps=1000, # 100\n  logging_steps=1000,\n\
          \  learning_rate=2e-4,\n  save_total_limit=3,\n  remove_unused_columns=False,\n\
          \  push_to_hub=False,\n  report_to='tensorboard',\n  load_best_model_at_end=True,\n\
          )\n\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n\
          \    args=training_args,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n\
          \    train_dataset=prepared_ds[\"train\"],\n    eval_dataset=prepared_ds[\"\
          validation\"],\n    tokenizer=feature_extractor,\n)\n\ntrain_results = trainer.train()\n\
          trainer.save_model()\ntrainer.log_metrics(\"train\", train_results.metrics)\n\
          trainer.save_metrics(\"train\", train_results.metrics)\ntrainer.save_state()\n\
          \nmetrics = trainer.evaluate(prepared_ds['validation'])\ntrainer.log_metrics(\"\
          eval\", metrics)\ntrainer.save_metrics(\"eval\", metrics)"
        updatedAt: '2023-05-15T07:23:23.947Z'
      numEdits: 0
      reactions: []
    id: 6461ddeb76f92cf538f9d12f
    type: comment
  author: friesel
  content: "Got it to run round trip.\n\nHere my training:\n\n\nfrom transformers\
    \ import ViTFeatureExtractor\n\nmodel_name_or_path = 'google/vit-base-patch16-224-in21k'\n\
    feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)\n\n\
    print (feature_extractor)\nprint (feature_extractor(image, return_tensors='pt'))\
    \ # as a torch tensor\n\n\ndef process_example(example):\n    inputs = feature_extractor(example['image'],\
    \ return_tensors='pt')\n    inputs['labels'] = example['labels']\n    return inputs\n\
    \n\nprocess_example(ds['train'][0])\n\ndef transform(example_batch):\n    # Take\
    \ a list of PIL images and turn them to pixel values\n    inputs = feature_extractor([x\
    \ for x in example_batch['image']], return_tensors='pt')\n\n    # Don't forget\
    \ to include the labels!\n    inputs['labels'] = example_batch['labels']\n   \
    \ return inputs\n\nprepared_ds = ds.with_transform(transform)\n\nprepared_ds['train'][0:2]\n\
    \nimport torch\n\ndef collate_fn(batch):\n    return {\n        'pixel_values':\
    \ torch.stack([x['pixel_values'] for x in batch]),\n        'labels': torch.tensor([x['labels']\
    \ for x in batch])\n    }\n\nimport numpy as np\nfrom datasets import load_metric\n\
    \nmetric = load_metric(\"accuracy\")\ndef compute_metrics(p):\n    return metric.compute(predictions=np.argmax(p.predictions,\
    \ axis=1), references=p.label_ids)\n\nfrom transformers import ViTForImageClassification\n\
    \nlabels = ds['train'].features['labels'].names\n\nprint (labels)\n\nmodel = ViTForImageClassification.from_pretrained(\n\
    \    model_name_or_path,\n    num_labels=len(labels),\n    id2label={str(i): c\
    \ for i, c in enumerate(labels)},\n    label2id={c: str(i) for i, c in enumerate(labels)}\n\
    ).to(\"cuda\")\n\nfrom transformers import TrainingArguments\n\ntraining_args\
    \ = TrainingArguments(\n  output_dir=\"tarkov_models/vit-base-tarkov-home_bs128_8ep\"\
    ,\n  per_device_train_batch_size=128,\n  evaluation_strategy=\"steps\",\n  num_train_epochs=8,\
    \ # 4\n  #max_steps=16, # 4\n  fp16=False, # True\n  save_steps=1000, # 100\n\
    \  eval_steps=1000, # 100\n  logging_steps=1000,\n  learning_rate=2e-4,\n  save_total_limit=3,\n\
    \  remove_unused_columns=False,\n  push_to_hub=False,\n  report_to='tensorboard',\n\
    \  load_best_model_at_end=True,\n)\n\nfrom transformers import Trainer\n\ntrainer\
    \ = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n\
    \    compute_metrics=compute_metrics,\n    train_dataset=prepared_ds[\"train\"\
    ],\n    eval_dataset=prepared_ds[\"validation\"],\n    tokenizer=feature_extractor,\n\
    )\n\ntrain_results = trainer.train()\ntrainer.save_model()\ntrainer.log_metrics(\"\
    train\", train_results.metrics)\ntrainer.save_metrics(\"train\", train_results.metrics)\n\
    trainer.save_state()\n\nmetrics = trainer.evaluate(prepared_ds['validation'])\n\
    trainer.log_metrics(\"eval\", metrics)\ntrainer.save_metrics(\"eval\", metrics)"
  created_at: 2023-05-15 06:23:23+00:00
  edited: false
  hidden: false
  id: 6461ddeb76f92cf538f9d12f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22817fafcb798fe4460458424fb3f5e9.svg
      fullname: Phillip Bock
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: friesel
      type: user
    createdAt: '2023-05-15T07:28:52.000Z'
    data:
      edited: false
      editors:
      - friesel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22817fafcb798fe4460458424fb3f5e9.svg
          fullname: Phillip Bock
          isHf: false
          isPro: false
          name: friesel
          type: user
        html: "<p>And inference:</p>\n<p>sPath_model = 'path to your local model'</p>\n\
          <p>config = ViTConfig.from_pretrained(sPath_model)<br>model = ViTForImageClassification.from_pretrained(sPath_model,\
          \ config=config)<br>processor = ViTImageProcessor.from_pretrained(sPath_model,\
          \ config=config)<br>feature_extractor = ViTFeatureExtractor.from_pretrained(sPath_model)</p>\n\
          <p>def softmax(x):<br>    e_x = np.exp(x - np.max(x))<br>    return e_x\
          \ / e_x.sum(axis=0)</p>\n<p>def main(image):<br>    inputs = processor(images=image,\
          \ return_tensors=\"pt\")<br>    output = model(**inputs)<br>    logits =\
          \ output.logits<br>    logits_detached = logits.detach().numpy()[0]<br>\
          \    logits_detached_play = logits_detached.copy()</p>\n<pre><code>aSoftmaxed\
          \ = softmax(logits_detached)\n\naPred = []\naScor = []\n\nprint (logits_detached_play)\n\
          </code></pre>\n<h1 id=\"show-top-ten-results\">show top ten results</h1>\n\
          <pre><code>for i in range(0,10):  \n    pred_class = np.argmax(logits_detached_play)\n\
          \    score = logits_detached[pred_class]\n    probability = aSoftmaxed[pred_class]\n\
          \    sLabel = config.id2label[pred_class]\n\n    aPred.append((score, probability,\
          \ sLabel))\n\n    logits_detached_play[pred_class] = -999999\n \nreturn\
          \ aPred\n</code></pre>\n"
        raw: "And inference:\n\nsPath_model = 'path to your local model'\n\nconfig\
          \ = ViTConfig.from_pretrained(sPath_model)\nmodel = ViTForImageClassification.from_pretrained(sPath_model,\
          \ config=config)\nprocessor = ViTImageProcessor.from_pretrained(sPath_model,\
          \ config=config)\nfeature_extractor = ViTFeatureExtractor.from_pretrained(sPath_model)\n\
          \ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\
          \ndef main(image):\n    inputs = processor(images=image, return_tensors=\"\
          pt\")\n    output = model(**inputs)\n    logits = output.logits\n    logits_detached\
          \ = logits.detach().numpy()[0]\n    logits_detached_play = logits_detached.copy()\n\
          \n    aSoftmaxed = softmax(logits_detached)\n\n    aPred = []\n    aScor\
          \ = []\n\n    print (logits_detached_play)\n    \n   # show top ten results\n\
          \    for i in range(0,10):  \n        pred_class = np.argmax(logits_detached_play)\n\
          \        score = logits_detached[pred_class]\n        probability = aSoftmaxed[pred_class]\n\
          \        sLabel = config.id2label[pred_class]\n\n        aPred.append((score,\
          \ probability, sLabel))\n\n        logits_detached_play[pred_class] = -999999\n\
          \     \n    return aPred"
        updatedAt: '2023-05-15T07:28:52.426Z'
      numEdits: 0
      reactions: []
    id: 6461df34e7de30225da9f0bb
    type: comment
  author: friesel
  content: "And inference:\n\nsPath_model = 'path to your local model'\n\nconfig =\
    \ ViTConfig.from_pretrained(sPath_model)\nmodel = ViTForImageClassification.from_pretrained(sPath_model,\
    \ config=config)\nprocessor = ViTImageProcessor.from_pretrained(sPath_model, config=config)\n\
    feature_extractor = ViTFeatureExtractor.from_pretrained(sPath_model)\n\ndef softmax(x):\n\
    \    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef main(image):\n\
    \    inputs = processor(images=image, return_tensors=\"pt\")\n    output = model(**inputs)\n\
    \    logits = output.logits\n    logits_detached = logits.detach().numpy()[0]\n\
    \    logits_detached_play = logits_detached.copy()\n\n    aSoftmaxed = softmax(logits_detached)\n\
    \n    aPred = []\n    aScor = []\n\n    print (logits_detached_play)\n    \n \
    \  # show top ten results\n    for i in range(0,10):  \n        pred_class = np.argmax(logits_detached_play)\n\
    \        score = logits_detached[pred_class]\n        probability = aSoftmaxed[pred_class]\n\
    \        sLabel = config.id2label[pred_class]\n\n        aPred.append((score,\
    \ probability, sLabel))\n\n        logits_detached_play[pred_class] = -999999\n\
    \     \n    return aPred"
  created_at: 2023-05-15 06:28:52+00:00
  edited: false
  hidden: false
  id: 6461df34e7de30225da9f0bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fca3efe9303944abaa48f683d6596f64.svg
      fullname: Putri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dhika
      type: user
    createdAt: '2023-05-18T11:38:51.000Z'
    data:
      edited: false
      editors:
      - Dhika
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fca3efe9303944abaa48f683d6596f64.svg
          fullname: Putri
          isHf: false
          isPro: false
          name: Dhika
          type: user
        html: "<blockquote>\n<p>And inference:</p>\n<p>sPath_model = 'path to your\
          \ local model'</p>\n<p>config = ViTConfig.from_pretrained(sPath_model)<br>model\
          \ = ViTForImageClassification.from_pretrained(sPath_model, config=config)<br>processor\
          \ = ViTImageProcessor.from_pretrained(sPath_model, config=config)<br>feature_extractor\
          \ = ViTFeatureExtractor.from_pretrained(sPath_model)</p>\n<p>def softmax(x):<br>\
          \    e_x = np.exp(x - np.max(x))<br>    return e_x / e_x.sum(axis=0)</p>\n\
          <p>def main(image):<br>    inputs = processor(images=image, return_tensors=\"\
          pt\")<br>    output = model(**inputs)<br>    logits = output.logits<br>\
          \    logits_detached = logits.detach().numpy()[0]<br>    logits_detached_play\
          \ = logits_detached.copy()</p>\n<pre><code>aSoftmaxed = softmax(logits_detached)\n\
          \naPred = []\naScor = []\n\nprint (logits_detached_play)\n</code></pre>\n\
          <h1 id=\"show-top-ten-results\">show top ten results</h1>\n<pre><code>for\
          \ i in range(0,10):  \n    pred_class = np.argmax(logits_detached_play)\n\
          \    score = logits_detached[pred_class]\n    probability = aSoftmaxed[pred_class]\n\
          \    sLabel = config.id2label[pred_class]\n\n    aPred.append((score, probability,\
          \ sLabel))\n\n    logits_detached_play[pred_class] = -999999\n \nreturn\
          \ aPred\n</code></pre>\n</blockquote>\n<p>i got error in logits_detached_play\
          \ not defined. what should i do?</p>\n"
        raw: "> And inference:\n> \n> sPath_model = 'path to your local model'\n>\
          \ \n> config = ViTConfig.from_pretrained(sPath_model)\n> model = ViTForImageClassification.from_pretrained(sPath_model,\
          \ config=config)\n> processor = ViTImageProcessor.from_pretrained(sPath_model,\
          \ config=config)\n> feature_extractor = ViTFeatureExtractor.from_pretrained(sPath_model)\n\
          > \n> def softmax(x):\n>     e_x = np.exp(x - np.max(x))\n>     return e_x\
          \ / e_x.sum(axis=0)\n> \n> def main(image):\n>     inputs = processor(images=image,\
          \ return_tensors=\"pt\")\n>     output = model(**inputs)\n>     logits =\
          \ output.logits\n>     logits_detached = logits.detach().numpy()[0]\n> \
          \    logits_detached_play = logits_detached.copy()\n> \n>     aSoftmaxed\
          \ = softmax(logits_detached)\n> \n>     aPred = []\n>     aScor = []\n>\
          \ \n>     print (logits_detached_play)\n>     \n>    # show top ten results\n\
          >     for i in range(0,10):  \n>         pred_class = np.argmax(logits_detached_play)\n\
          >         score = logits_detached[pred_class]\n>         probability = aSoftmaxed[pred_class]\n\
          >         sLabel = config.id2label[pred_class]\n> \n>         aPred.append((score,\
          \ probability, sLabel))\n> \n>         logits_detached_play[pred_class]\
          \ = -999999\n>      \n>     return aPred\n\ni got error in logits_detached_play\
          \ not defined. what should i do?"
        updatedAt: '2023-05-18T11:38:51.710Z'
      numEdits: 0
      reactions: []
    id: 64660e4b119ad94383cc29bd
    type: comment
  author: Dhika
  content: "> And inference:\n> \n> sPath_model = 'path to your local model'\n> \n\
    > config = ViTConfig.from_pretrained(sPath_model)\n> model = ViTForImageClassification.from_pretrained(sPath_model,\
    \ config=config)\n> processor = ViTImageProcessor.from_pretrained(sPath_model,\
    \ config=config)\n> feature_extractor = ViTFeatureExtractor.from_pretrained(sPath_model)\n\
    > \n> def softmax(x):\n>     e_x = np.exp(x - np.max(x))\n>     return e_x / e_x.sum(axis=0)\n\
    > \n> def main(image):\n>     inputs = processor(images=image, return_tensors=\"\
    pt\")\n>     output = model(**inputs)\n>     logits = output.logits\n>     logits_detached\
    \ = logits.detach().numpy()[0]\n>     logits_detached_play = logits_detached.copy()\n\
    > \n>     aSoftmaxed = softmax(logits_detached)\n> \n>     aPred = []\n>     aScor\
    \ = []\n> \n>     print (logits_detached_play)\n>     \n>    # show top ten results\n\
    >     for i in range(0,10):  \n>         pred_class = np.argmax(logits_detached_play)\n\
    >         score = logits_detached[pred_class]\n>         probability = aSoftmaxed[pred_class]\n\
    >         sLabel = config.id2label[pred_class]\n> \n>         aPred.append((score,\
    \ probability, sLabel))\n> \n>         logits_detached_play[pred_class] = -999999\n\
    >      \n>     return aPred\n\ni got error in logits_detached_play not defined.\
    \ what should i do?"
  created_at: 2023-05-18 10:38:51+00:00
  edited: false
  hidden: false
  id: 64660e4b119ad94383cc29bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fca3efe9303944abaa48f683d6596f64.svg
      fullname: Putri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dhika
      type: user
    createdAt: '2023-05-18T11:40:19.000Z'
    data:
      edited: false
      editors:
      - Dhika
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fca3efe9303944abaa48f683d6596f64.svg
          fullname: Putri
          isHf: false
          isPro: false
          name: Dhika
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64508b4902e7c57d3962766f/qW-1adXWP7ualoTVr8Rfc.png"><img
          alt="Screenshot 2023-05-18 184425.png" src="https://cdn-uploads.huggingface.co/production/uploads/64508b4902e7c57d3962766f/qW-1adXWP7ualoTVr8Rfc.png"></a></p>

          '
        raw: '![Screenshot 2023-05-18 184425.png](https://cdn-uploads.huggingface.co/production/uploads/64508b4902e7c57d3962766f/qW-1adXWP7ualoTVr8Rfc.png)'
        updatedAt: '2023-05-18T11:40:19.627Z'
      numEdits: 0
      reactions: []
    id: 64660ea39c627c78f8667232
    type: comment
  author: Dhika
  content: '![Screenshot 2023-05-18 184425.png](https://cdn-uploads.huggingface.co/production/uploads/64508b4902e7c57d3962766f/qW-1adXWP7ualoTVr8Rfc.png)'
  created_at: 2023-05-18 10:40:19+00:00
  edited: false
  hidden: false
  id: 64660ea39c627c78f8667232
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22817fafcb798fe4460458424fb3f5e9.svg
      fullname: Phillip Bock
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: friesel
      type: user
    createdAt: '2023-05-19T06:04:14.000Z'
    data:
      edited: false
      editors:
      - friesel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22817fafcb798fe4460458424fb3f5e9.svg
          fullname: Phillip Bock
          isHf: false
          isPro: false
          name: friesel
          type: user
        html: '<p>That cell needs to be indented (part of main())</p>

          '
        raw: That cell needs to be indented (part of main())
        updatedAt: '2023-05-19T06:04:14.129Z'
      numEdits: 0
      reactions: []
    id: 6467115eab75d9cb3c3f1863
    type: comment
  author: friesel
  content: That cell needs to be indented (part of main())
  created_at: 2023-05-19 05:04:14+00:00
  edited: false
  hidden: false
  id: 6467115eab75d9cb3c3f1863
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/589a786b58692458ee920f2ddc114509.svg
      fullname: lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yonglee7015
      type: user
    createdAt: '2023-08-25T02:47:31.000Z'
    data:
      edited: false
      editors:
      - yonglee7015
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6147037148475647
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/589a786b58692458ee920f2ddc114509.svg
          fullname: lee
          isHf: false
          isPro: false
          name: yonglee7015
          type: user
        html: '<blockquote>

          <p>Thanks a lot<br>I have a question : I want to use your code on a another
          datasets , so how can I save my model like you did with yours "nateraw/vit-base-beans"
          so that I can use my model for predections?</p>

          </blockquote>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/645c41b87848314a460ce108/u31-izNwj93mRs3J5Q8OL.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/645c41b87848314a460ce108/u31-izNwj93mRs3J5Q8OL.png"></a></p>

          <p>from PIL import Image</p>

          <p>feature_extractor = ViTImageProcessor.from_pretrained(model_name_or_path)</p>

          <p>image = Image.open("/content/angular_leaf_spot.jpeg")<br>model = ViTForImageClassification.from_pretrained(''/content/vit-base-beans'')<br>inputs
          = feature_extractor(images =image, return_tensors=''pt'')</p>

          <p>outputs = model(**inputs)<br>logits = outputs.logits</p>

          <p>predicted_class_idx = logits.argmax(-1).item()<br>print("Predicted class:",
          model.config.id2label[predicted_class_idx])</p>

          '
        raw: '> Thanks a lot

          > I have a question : I want to use your code on a another datasets , so
          how can I save my model like you did with yours "nateraw/vit-base-beans"
          so that I can use my model for predections?



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/645c41b87848314a460ce108/u31-izNwj93mRs3J5Q8OL.png)


          from PIL import Image


          feature_extractor = ViTImageProcessor.from_pretrained(model_name_or_path)


          image = Image.open("/content/angular_leaf_spot.jpeg")

          model = ViTForImageClassification.from_pretrained(''/content/vit-base-beans'')

          inputs = feature_extractor(images =image, return_tensors=''pt'')


          outputs = model(**inputs)

          logits = outputs.logits



          predicted_class_idx = logits.argmax(-1).item()

          print("Predicted class:", model.config.id2label[predicted_class_idx])'
        updatedAt: '2023-08-25T02:47:31.925Z'
      numEdits: 0
      reactions: []
    id: 64e81643fb0dc2f84ca4d511
    type: comment
  author: yonglee7015
  content: '> Thanks a lot

    > I have a question : I want to use your code on a another datasets , so how can
    I save my model like you did with yours "nateraw/vit-base-beans" so that I can
    use my model for predections?



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/645c41b87848314a460ce108/u31-izNwj93mRs3J5Q8OL.png)


    from PIL import Image


    feature_extractor = ViTImageProcessor.from_pretrained(model_name_or_path)


    image = Image.open("/content/angular_leaf_spot.jpeg")

    model = ViTForImageClassification.from_pretrained(''/content/vit-base-beans'')

    inputs = feature_extractor(images =image, return_tensors=''pt'')


    outputs = model(**inputs)

    logits = outputs.logits



    predicted_class_idx = logits.argmax(-1).item()

    print("Predicted class:", model.config.id2label[predicted_class_idx])'
  created_at: 2023-08-25 01:47:31+00:00
  edited: false
  hidden: false
  id: 64e81643fb0dc2f84ca4d511
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: nateraw/vit-base-beans
repo_type: model
status: open
target_branch: null
title: Make Predictions?
