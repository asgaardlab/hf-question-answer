!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ppsking
conflicting_files: null
created_at: 2023-08-24 11:48:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/95fece5aedb69be71e470ec7ddb0671a.svg
      fullname: Michael Peng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ppsking
      type: user
    createdAt: '2023-08-24T12:48:47.000Z'
    data:
      edited: false
      editors:
      - ppsking
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.861813485622406
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/95fece5aedb69be71e470ec7ddb0671a.svg
          fullname: Michael Peng
          isHf: false
          isPro: false
          name: ppsking
          type: user
        html: '<p>I''m wondering how to use this fantastic model to do batch inference,
          when using tokenizer to pad the batch inputs, it turns out with "Asking
          to pad but the tokenizer does not have a padding token."</p>

          '
        raw: I'm wondering how to use this fantastic model to do batch inference,
          when using tokenizer to pad the batch inputs, it turns out with "Asking
          to pad but the tokenizer does not have a padding token."
        updatedAt: '2023-08-24T12:48:47.071Z'
      numEdits: 0
      reactions: []
    id: 64e751af94f2ce209779a6c5
    type: comment
  author: ppsking
  content: I'm wondering how to use this fantastic model to do batch inference, when
    using tokenizer to pad the batch inputs, it turns out with "Asking to pad but
    the tokenizer does not have a padding token."
  created_at: 2023-08-24 11:48:47+00:00
  edited: false
  hidden: false
  id: 64e751af94f2ce209779a6c5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: upstage/SOLAR-0-70b-8bit
repo_type: model
status: open
target_branch: null
title: How to do batch inference?
