!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JohanAR
conflicting_files: null
created_at: 2023-08-16 16:40:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5414f98a1bb569a70e0868c12662bc9f.svg
      fullname: Johan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JohanAR
      type: user
    createdAt: '2023-08-16T17:40:52.000Z'
    data:
      edited: false
      editors:
      - JohanAR
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9071465730667114
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5414f98a1bb569a70e0868c12662bc9f.svg
          fullname: Johan
          isHf: false
          isPro: false
          name: JohanAR
          type: user
        html: '<p>It seems a bit inconsistent with generating &lt;|end_of_turn|&gt;
          at the end of responses, sometimes I get "|&lt;|end_of_turn|&gt;" or "|end_of_turn]"
          etc, especially when asking about subjects that trigger the censoring, e.g.
          "where can I buy drugs?"</p>

          <p>I''ve copied the oobabooga settings from <a href="https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B">https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B</a>
          but I don''t know if I''m doing something wrong, it''s a bug in text-generation-webui/llama.cpp,
          or if this is expected from this model.</p>

          '
        raw: "It seems a bit inconsistent with generating <|end_of_turn|> at the end\
          \ of responses, sometimes I get \"|<|end_of_turn|>\" or \"|end_of_turn]\"\
          \ etc, especially when asking about subjects that trigger the censoring,\
          \ e.g. \"where can I buy drugs?\"\r\n\r\nI've copied the oobabooga settings\
          \ from https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B but\
          \ I don't know if I'm doing something wrong, it's a bug in text-generation-webui/llama.cpp,\
          \ or if this is expected from this model."
        updatedAt: '2023-08-16T17:40:52.985Z'
      numEdits: 0
      reactions: []
    id: 64dd0a24249785efad9c346d
    type: comment
  author: JohanAR
  content: "It seems a bit inconsistent with generating <|end_of_turn|> at the end\
    \ of responses, sometimes I get \"|<|end_of_turn|>\" or \"|end_of_turn]\" etc,\
    \ especially when asking about subjects that trigger the censoring, e.g. \"where\
    \ can I buy drugs?\"\r\n\r\nI've copied the oobabooga settings from https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B\
    \ but I don't know if I'm doing something wrong, it's a bug in text-generation-webui/llama.cpp,\
    \ or if this is expected from this model."
  created_at: 2023-08-16 16:40:52+00:00
  edited: false
  hidden: false
  id: 64dd0a24249785efad9c346d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672826665031-noauth.jpeg?w=200&h=200&f=face
      fullname: Ankit Pasi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: apasi
      type: user
    createdAt: '2023-10-12T18:29:45.000Z'
    data:
      edited: false
      editors:
      - apasi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9665795564651489
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672826665031-noauth.jpeg?w=200&h=200&f=face
          fullname: Ankit Pasi
          isHf: false
          isPro: false
          name: apasi
          type: user
        html: '<p>Same here. I see inconsistent tokens for end of turn. The model
          responds well but had various versions of &lt;|end_of_turn|&gt; often with
          few missing preceeding or end characters.</p>

          '
        raw: Same here. I see inconsistent tokens for end of turn. The model responds
          well but had various versions of <|end_of_turn|> often with few missing
          preceeding or end characters.
        updatedAt: '2023-10-12T18:29:45.030Z'
      numEdits: 0
      reactions: []
    id: 65283b192dc77a70e2efc84f
    type: comment
  author: apasi
  content: Same here. I see inconsistent tokens for end of turn. The model responds
    well but had various versions of <|end_of_turn|> often with few missing preceeding
    or end characters.
  created_at: 2023-10-12 17:29:45+00:00
  edited: false
  hidden: false
  id: 65283b192dc77a70e2efc84f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5414f98a1bb569a70e0868c12662bc9f.svg
      fullname: Johan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JohanAR
      type: user
    createdAt: '2023-10-12T18:37:39.000Z'
    data:
      edited: false
      editors:
      - JohanAR
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9696629047393799
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5414f98a1bb569a70e0868c12662bc9f.svg
          fullname: Johan
          isHf: false
          isPro: false
          name: JohanAR
          type: user
        html: '<p>It looks like this model has &lt;|end_of_turn|&gt; added as a special
          token, and I have recently learned that this might not be supported by exllama
          and llama.cpp. I don''t have a copy of this model around, but if you''re
          using ooba''s webui try using one of the model loaders with _HF suffix.</p>

          '
        raw: It looks like this model has <|end_of_turn|> added as a special token,
          and I have recently learned that this might not be supported by exllama
          and llama.cpp. I don't have a copy of this model around, but if you're using
          ooba's webui try using one of the model loaders with _HF suffix.
        updatedAt: '2023-10-12T18:37:39.374Z'
      numEdits: 0
      reactions: []
    id: 65283cf3e3b3844f5e460b9b
    type: comment
  author: JohanAR
  content: It looks like this model has <|end_of_turn|> added as a special token,
    and I have recently learned that this might not be supported by exllama and llama.cpp.
    I don't have a copy of this model around, but if you're using ooba's webui try
    using one of the model loaders with _HF suffix.
  created_at: 2023-10-12 17:37:39+00:00
  edited: false
  hidden: false
  id: 65283cf3e3b3844f5e460b9b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: Open-Orca/OpenOrca-Platypus2-13B
repo_type: model
status: open
target_branch: null
title: Inconsistencies with end_of_turn generation
