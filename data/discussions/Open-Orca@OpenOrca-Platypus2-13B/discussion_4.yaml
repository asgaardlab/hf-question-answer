!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cvdbdo
conflicting_files: null
created_at: 2023-08-17 08:51:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f9745ac821dfb5ec97425484dfb417ee.svg
      fullname: Corentin vdbdo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cvdbdo
      type: user
    createdAt: '2023-08-17T09:51:09.000Z'
    data:
      edited: false
      editors:
      - cvdbdo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6371352076530457
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f9745ac821dfb5ec97425484dfb417ee.svg
          fullname: Corentin vdbdo
          isHf: false
          isPro: false
          name: cvdbdo
          type: user
        html: '<p>When generating text I get either  or &lt;|end_of_turn|&gt;  as
          EOS token. With &lt;|end_of_turn|&gt; the generation doesn''t stop.</p>

          '
        raw: When generating text I get either </s> or <|end_of_turn|>  as EOS token.
          With <|end_of_turn|> the generation doesn't stop.
        updatedAt: '2023-08-17T09:51:09.574Z'
      numEdits: 0
      reactions: []
    id: 64dded8da6767c7fd6733390
    type: comment
  author: cvdbdo
  content: When generating text I get either </s> or <|end_of_turn|>  as EOS token.
    With <|end_of_turn|> the generation doesn't stop.
  created_at: 2023-08-17 08:51:09+00:00
  edited: false
  hidden: false
  id: 64dded8da6767c7fd6733390
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9441d895e3308a15a50c5dab942454ff.svg
      fullname: Bleys
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: bleysg
      type: user
    createdAt: '2023-08-17T09:55:07.000Z'
    data:
      edited: false
      editors:
      - bleysg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9119259119033813
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9441d895e3308a15a50c5dab942454ff.svg
          fullname: Bleys
          isHf: false
          isPro: true
          name: bleysg
          type: user
        html: "<p>Can you provide more details on the execution environment? Which\
          \ prompt format are you using? We\u2019ve only tested with the one from\
          \ OpenOrca model.</p>\n"
        raw: "Can you provide more details on the execution environment? Which prompt\
          \ format are you using? We\u2019ve only tested with the one from OpenOrca\
          \ model."
        updatedAt: '2023-08-17T09:55:07.637Z'
      numEdits: 0
      reactions: []
    id: 64ddee7b2ff9848f9d78ccf3
    type: comment
  author: bleysg
  content: "Can you provide more details on the execution environment? Which prompt\
    \ format are you using? We\u2019ve only tested with the one from OpenOrca model."
  created_at: 2023-08-17 08:55:07+00:00
  edited: false
  hidden: false
  id: 64ddee7b2ff9848f9d78ccf3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/f9745ac821dfb5ec97425484dfb417ee.svg
      fullname: Corentin vdbdo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cvdbdo
      type: user
    createdAt: '2023-08-17T12:58:45.000Z'
    data:
      from: Inconsistency with end of strins token
      to: Inconsistency with end of string token
    id: 64de19855e1929850547c545
    type: title-change
  author: cvdbdo
  created_at: 2023-08-17 11:58:45+00:00
  id: 64de19855e1929850547c545
  new_title: Inconsistency with end of string token
  old_title: Inconsistency with end of strins token
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f9745ac821dfb5ec97425484dfb417ee.svg
      fullname: Corentin vdbdo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cvdbdo
      type: user
    createdAt: '2023-08-17T13:06:51.000Z'
    data:
      edited: true
      editors:
      - cvdbdo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8200723528862
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f9745ac821dfb5ec97425484dfb417ee.svg
          fullname: Corentin vdbdo
          isHf: false
          isPro: false
          name: cvdbdo
          type: user
        html: '<p>I use Alpaca Instruct format, with a Open-Orca/OpenOrca-Platypus2-13B
          fine tuned on a specialized instruct dataset. The behaviour described is
          common to the base and fine tuned models. It happens with or without quantization
          (4 &amp; 8 bits). I load them with a simple AutoModelForCausalLM.</p>

          <p>generation_config = GenerationConfig(<br>    temperature=.0001,<br>    top_p=0,<br>    top_k=0,<br>    repetition_penalty=1,<br>)</p>

          <p>The problem is mainly a performance one because the model keeps generating
          after the eos token.</p>

          '
        raw: "I use Alpaca Instruct format, with a Open-Orca/OpenOrca-Platypus2-13B\
          \ fine tuned on a specialized instruct dataset. The behaviour described\
          \ is common to the base and fine tuned models. It happens with or without\
          \ quantization (4 & 8 bits). I load them with a simple AutoModelForCausalLM.\n\
          \ \ngeneration_config = GenerationConfig(\n    temperature=.0001,\n    top_p=0,\n\
          \    top_k=0,\n    repetition_penalty=1,\n)\n\nThe problem is mainly a performance\
          \ one because the model keeps generating after the eos token."
        updatedAt: '2023-09-11T13:03:52.988Z'
      numEdits: 1
      reactions: []
    id: 64de1b6b2aeee6796923e914
    type: comment
  author: cvdbdo
  content: "I use Alpaca Instruct format, with a Open-Orca/OpenOrca-Platypus2-13B\
    \ fine tuned on a specialized instruct dataset. The behaviour described is common\
    \ to the base and fine tuned models. It happens with or without quantization (4\
    \ & 8 bits). I load them with a simple AutoModelForCausalLM.\n \ngeneration_config\
    \ = GenerationConfig(\n    temperature=.0001,\n    top_p=0,\n    top_k=0,\n  \
    \  repetition_penalty=1,\n)\n\nThe problem is mainly a performance one because\
    \ the model keeps generating after the eos token."
  created_at: 2023-08-17 12:06:51+00:00
  edited: true
  hidden: false
  id: 64de1b6b2aeee6796923e914
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/901c3825c60bae453772c01d65a0e1d9.svg
      fullname: Darshan Kholakiya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darshan12
      type: user
    createdAt: '2023-09-07T13:48:54.000Z'
    data:
      edited: true
      editors:
      - darshan12
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.811983048915863
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/901c3825c60bae453772c01d65a0e1d9.svg
          fullname: Darshan Kholakiya
          isHf: false
          isPro: false
          name: darshan12
          type: user
        html: '<p>How to fine tune this model can anyone please connect with me and
          help me, I want to learn how to fine tune this data?<br>Email: <a rel="nofollow"
          href="mailto:darshankholakiya12@gmail.com">darshankholakiya12@gmail.com</a><br>LinkedIn:
          <a rel="nofollow" href="https://www.linkedin.com/in/darshankholakiya/">https://www.linkedin.com/in/darshankholakiya/</a></p>

          '
        raw: 'How to fine tune this model can anyone please connect with me and help
          me, I want to learn how to fine tune this data?

          Email: darshankholakiya12@gmail.com

          LinkedIn: https://www.linkedin.com/in/darshankholakiya/'
        updatedAt: '2023-09-07T13:49:18.096Z'
      numEdits: 1
      reactions: []
    id: 64f9d4c687c368f6184bfd54
    type: comment
  author: darshan12
  content: 'How to fine tune this model can anyone please connect with me and help
    me, I want to learn how to fine tune this data?

    Email: darshankholakiya12@gmail.com

    LinkedIn: https://www.linkedin.com/in/darshankholakiya/'
  created_at: 2023-09-07 12:48:54+00:00
  edited: true
  hidden: false
  id: 64f9d4c687c368f6184bfd54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-09-12T04:09:31.000Z'
    data:
      edited: true
      editors:
      - deleted
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7962051033973694
      isReport: false
      latest:
        html: '<blockquote>

          <p>I use Alpaca Instruct format, with a Open-Orca/OpenOrca-Platypus2-13B
          fine tuned on a specialized instruct dataset. The behaviour described is
          common to the base and fine tuned models. It happens with or without quantization
          (4 &amp; 8 bits). I load them with a simple AutoModelForCausalLM.</p>

          <p>generation_config = GenerationConfig(<br>    temperature=.0001,<br>    top_p=0,<br>    top_k=0,<br>    repetition_penalty=1,<br>)</p>

          <p>The problem is mainly a performance one because the model keeps generating
          after the eos token.</p>

          </blockquote>

          <p>Set the end to turn token as stop token in GenerationConfig. If you don''t
          know the token id then simple encode the &lt;|end_of_turn|&gt; using the
          tokenizer you will get the id then set stop token to this id.</p>

          '
        raw: "> I use Alpaca Instruct format, with a Open-Orca/OpenOrca-Platypus2-13B\
          \ fine tuned on a specialized instruct dataset. The behaviour described\
          \ is common to the base and fine tuned models. It happens with or without\
          \ quantization (4 & 8 bits). I load them with a simple AutoModelForCausalLM.\n\
          >  \n> generation_config = GenerationConfig(\n>     temperature=.0001,\n\
          >     top_p=0,\n>     top_k=0,\n>     repetition_penalty=1,\n> )\n> \n>\
          \ The problem is mainly a performance one because the model keeps generating\
          \ after the eos token.\n\nSet the end to turn token as stop token in GenerationConfig.\
          \ If you don't know the token id then simple encode the <|end_of_turn|>\
          \ using the tokenizer you will get the id then set stop token to this id."
        updatedAt: '2023-09-12T04:10:21.088Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - imone
    id: 64ffe47b70b6b05c5ac332fd
    type: comment
  author: deleted
  content: "> I use Alpaca Instruct format, with a Open-Orca/OpenOrca-Platypus2-13B\
    \ fine tuned on a specialized instruct dataset. The behaviour described is common\
    \ to the base and fine tuned models. It happens with or without quantization (4\
    \ & 8 bits). I load them with a simple AutoModelForCausalLM.\n>  \n> generation_config\
    \ = GenerationConfig(\n>     temperature=.0001,\n>     top_p=0,\n>     top_k=0,\n\
    >     repetition_penalty=1,\n> )\n> \n> The problem is mainly a performance one\
    \ because the model keeps generating after the eos token.\n\nSet the end to turn\
    \ token as stop token in GenerationConfig. If you don't know the token id then\
    \ simple encode the <|end_of_turn|> using the tokenizer you will get the id then\
    \ set stop token to this id."
  created_at: 2023-09-12 03:09:31+00:00
  edited: true
  hidden: false
  id: 64ffe47b70b6b05c5ac332fd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Open-Orca/OpenOrca-Platypus2-13B
repo_type: model
status: open
target_branch: null
title: Inconsistency with end of string token
