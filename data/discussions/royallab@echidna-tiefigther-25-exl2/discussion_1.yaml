!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ThenMagician
conflicting_files: null
created_at: 2023-12-14 01:27:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6a5500fce40882c2c37ff570a4026ef.svg
      fullname: ThenMagician
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThenMagician
      type: user
    createdAt: '2023-12-14T01:27:32.000Z'
    data:
      edited: false
      editors:
      - ThenMagician
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9467597603797913
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6a5500fce40882c2c37ff570a4026ef.svg
          fullname: ThenMagician
          isHf: false
          isPro: false
          name: ThenMagician
          type: user
        html: '<p>Can you release 4.5bpw versions? I have a RTX 3060 with 12gb and
          running at 4.5 takes 11.6~ GB of VRAM while reaching 15 t/s at 4k context.<br>I
          don''t know if headbit 8 increases VRAM costs, but I tried some 4bpw h8
          models and the improvement is quite noticeable, so maybe a 4.5bpw h8 version
          too?</p>

          '
        raw: "Can you release 4.5bpw versions? I have a RTX 3060 with 12gb and running\
          \ at 4.5 takes 11.6~ GB of VRAM while reaching 15 t/s at 4k context.\r\n\
          I don't know if headbit 8 increases VRAM costs, but I tried some 4bpw h8\
          \ models and the improvement is quite noticeable, so maybe a 4.5bpw h8 version\
          \ too?"
        updatedAt: '2023-12-14T01:27:32.628Z'
      numEdits: 0
      reactions: []
    id: 657a5a04f10a984150a006cb
    type: comment
  author: ThenMagician
  content: "Can you release 4.5bpw versions? I have a RTX 3060 with 12gb and running\
    \ at 4.5 takes 11.6~ GB of VRAM while reaching 15 t/s at 4k context.\r\nI don't\
    \ know if headbit 8 increases VRAM costs, but I tried some 4bpw h8 models and\
    \ the improvement is quite noticeable, so maybe a 4.5bpw h8 version too?"
  created_at: 2023-12-14 01:27:32+00:00
  edited: false
  hidden: false
  id: 657a5a04f10a984150a006cb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: royallab/echidna-tiefigther-25-exl2
repo_type: model
status: open
target_branch: null
title: 4.5bpw?
