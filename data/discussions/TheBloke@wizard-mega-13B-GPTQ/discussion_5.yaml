!!python/object:huggingface_hub.community.DiscussionWithDetails
author: A2Hero
conflicting_files: null
created_at: 2023-06-27 13:37:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3b9d4ab14d1d77b068237f0beb61d07.svg
      fullname: Abbas Aleali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: A2Hero
      type: user
    createdAt: '2023-06-27T14:37:21.000Z'
    data:
      edited: true
      editors:
      - A2Hero
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9806239008903503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3b9d4ab14d1d77b068237f0beb61d07.svg
          fullname: Abbas Aleali
          isHf: false
          isPro: false
          name: A2Hero
          type: user
        html: '<p>Lads is there any one know how to do that?<br>even the chat gpt
          4 can''t help me well.<br>I don''t know if I doing something wrong or not
          but I feel like there is way like bypassing ROCm in windows with help of
          wsl 2 or something like that. Please any one have a solution for it ?</p>

          '
        raw: 'Lads is there any one know how to do that?

          even the chat gpt 4 can''t help me well.

          I don''t know if I doing something wrong or not but I feel like there is
          way like bypassing ROCm in windows with help of wsl 2 or something like
          that. Please any one have a solution for it ?'
        updatedAt: '2023-06-27T21:11:36.236Z'
      numEdits: 1
      reactions: []
    id: 649af421d0b365aee08229e1
    type: comment
  author: A2Hero
  content: 'Lads is there any one know how to do that?

    even the chat gpt 4 can''t help me well.

    I don''t know if I doing something wrong or not but I feel like there is way like
    bypassing ROCm in windows with help of wsl 2 or something like that. Please any
    one have a solution for it ?'
  created_at: 2023-06-27 13:37:21+00:00
  edited: true
  hidden: false
  id: 649af421d0b365aee08229e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-06-29T02:30:40.000Z'
    data:
      edited: true
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9729871153831482
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>13B GPTQ models is about 14GB size, which won;t fit on 8GB cards,
          There is no offloading technique for GPTQ so far, you probably need to refer
          to flexgen and with unquantized weights</p>

          '
        raw: 13B GPTQ models is about 14GB size, which won;t fit on 8GB cards, There
          is no offloading technique for GPTQ so far, you probably need to refer to
          flexgen and with unquantized weights
        updatedAt: '2023-06-29T02:30:55.485Z'
      numEdits: 1
      reactions: []
    id: 649cecd07b70dc8a4a890e38
    type: comment
  author: Yhyu13
  content: 13B GPTQ models is about 14GB size, which won;t fit on 8GB cards, There
    is no offloading technique for GPTQ so far, you probably need to refer to flexgen
    and with unquantized weights
  created_at: 2023-06-29 01:30:40+00:00
  edited: true
  hidden: false
  id: 649cecd07b70dc8a4a890e38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-29T09:44:53.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9638658165931702
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>There is offloading in GPTQ-for-LLaMa but it's really, really slow,\
          \ and I don't know if it works for ROCm implementations of GPTQ-for-LLaMa.\
          \  ExLlama has ROCm but no offloading, which I imagine is what you're referring\
          \ to.</p>\n<p>But it sounds like the OP is using Windows and there's no\
          \ ROCm for Windows, not even in WSL, so that's a deadend I'm afraid.  </p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;A2Hero&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/A2Hero\">@<span class=\"\
          underline\">A2Hero</span></a></span>\n\n\t</span></span> I would suggest\
          \ you use GGML, which can work on your AMD card via OpenCL acceleration.</p>\n"
        raw: "There is offloading in GPTQ-for-LLaMa but it's really, really slow,\
          \ and I don't know if it works for ROCm implementations of GPTQ-for-LLaMa.\
          \  ExLlama has ROCm but no offloading, which I imagine is what you're referring\
          \ to.\n\nBut it sounds like the OP is using Windows and there's no ROCm\
          \ for Windows, not even in WSL, so that's a deadend I'm afraid.  \n\n@A2Hero\
          \ I would suggest you use GGML, which can work on your AMD card via OpenCL\
          \ acceleration."
        updatedAt: '2023-06-29T09:44:53.624Z'
      numEdits: 0
      reactions: []
    id: 649d52953d3687f0a9f426b6
    type: comment
  author: TheBloke
  content: "There is offloading in GPTQ-for-LLaMa but it's really, really slow, and\
    \ I don't know if it works for ROCm implementations of GPTQ-for-LLaMa.  ExLlama\
    \ has ROCm but no offloading, which I imagine is what you're referring to.\n\n\
    But it sounds like the OP is using Windows and there's no ROCm for Windows, not\
    \ even in WSL, so that's a deadend I'm afraid.  \n\n@A2Hero I would suggest you\
    \ use GGML, which can work on your AMD card via OpenCL acceleration."
  created_at: 2023-06-29 08:44:53+00:00
  edited: false
  hidden: false
  id: 649d52953d3687f0a9f426b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3b9d4ab14d1d77b068237f0beb61d07.svg
      fullname: Abbas Aleali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: A2Hero
      type: user
    createdAt: '2023-06-30T10:21:46.000Z'
    data:
      edited: false
      editors:
      - A2Hero
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8500726222991943
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3b9d4ab14d1d77b068237f0beb61d07.svg
          fullname: Abbas Aleali
          isHf: false
          isPro: false
          name: A2Hero
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Yhyu13&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Yhyu13\">@<span class=\"\
          underline\">Yhyu13</span></a></span>\n\n\t</span></span> I meant any kidna\
          \ of a.i model. even 6b or lower.</p>\n"
        raw: '@Yhyu13 I meant any kidna of a.i model. even 6b or lower.'
        updatedAt: '2023-06-30T10:21:46.155Z'
      numEdits: 0
      reactions: []
    id: 649eacbac0f0a9557cb67a0e
    type: comment
  author: A2Hero
  content: '@Yhyu13 I meant any kidna of a.i model. even 6b or lower.'
  created_at: 2023-06-30 09:21:46+00:00
  edited: false
  hidden: false
  id: 649eacbac0f0a9557cb67a0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3b9d4ab14d1d77b068237f0beb61d07.svg
      fullname: Abbas Aleali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: A2Hero
      type: user
    createdAt: '2023-06-30T10:59:58.000Z'
    data:
      edited: false
      editors:
      - A2Hero
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9404404163360596
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3b9d4ab14d1d77b068237f0beb61d07.svg
          fullname: Abbas Aleali
          isHf: false
          isPro: false
          name: A2Hero
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span><br>sure i will\
          \ try ggml with something like TheBloke/orca_mini_3B-GGML. (my cpu is i5\
          \ 4690 and i have 16ram)<br>but i really hope that someday ( and i hope\
          \ is near ) amd support rocm in windows or anything can help to run TheBloke/wizard-mega-13B-GPTQ.<br>thanks\
          \ for the advice! </p>\n"
        raw: "@TheBloke \nsure i will try ggml with something like TheBloke/orca_mini_3B-GGML.\
          \ (my cpu is i5 4690 and i have 16ram)\nbut i really hope that someday (\
          \ and i hope is near ) amd support rocm in windows or anything can help\
          \ to run TheBloke/wizard-mega-13B-GPTQ.\nthanks for the advice! "
        updatedAt: '2023-06-30T10:59:58.563Z'
      numEdits: 0
      reactions: []
    id: 649eb5ae1b8a453e89a35098
    type: comment
  author: A2Hero
  content: "@TheBloke \nsure i will try ggml with something like TheBloke/orca_mini_3B-GGML.\
    \ (my cpu is i5 4690 and i have 16ram)\nbut i really hope that someday ( and i\
    \ hope is near ) amd support rocm in windows or anything can help to run TheBloke/wizard-mega-13B-GPTQ.\n\
    thanks for the advice! "
  created_at: 2023-06-30 09:59:58+00:00
  edited: false
  hidden: false
  id: 649eb5ae1b8a453e89a35098
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/wizard-mega-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: Help! How to Run an A.I with an AMD GPU (Rx 580 8Gb)
