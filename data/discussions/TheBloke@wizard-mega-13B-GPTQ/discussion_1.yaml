!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Unknown37
conflicting_files: null
created_at: 2023-05-18 21:58:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50f49e5eaf9c97237a275355f24b3cf9.svg
      fullname: Chase Raines
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Unknown37
      type: user
    createdAt: '2023-05-18T22:58:53.000Z'
    data:
      edited: false
      editors:
      - Unknown37
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50f49e5eaf9c97237a275355f24b3cf9.svg
          fullname: Chase Raines
          isHf: false
          isPro: false
          name: Unknown37
          type: user
        html: '<p>What are the differences between Wizard Mega 13B by Openaccess-AI-Collective,
          GGML, and this model GPTQ?</p>

          '
        raw: What are the differences between Wizard Mega 13B by Openaccess-AI-Collective,
          GGML, and this model GPTQ?
        updatedAt: '2023-05-18T22:58:53.415Z'
      numEdits: 0
      reactions: []
    id: 6466adade9906a259f35c9e0
    type: comment
  author: Unknown37
  content: What are the differences between Wizard Mega 13B by Openaccess-AI-Collective,
    GGML, and this model GPTQ?
  created_at: 2023-05-18 21:58:53+00:00
  edited: false
  hidden: false
  id: 6466adade9906a259f35c9e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-18T23:01:53.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>GGML is quantised for CPU-based inference (and now also supports
          acceleration from a CUDA GPU), from C++ based clients<br>GPTQ is quantised
          for GPU-based inference, from Python code<br>The base repo is float16, also
          for GPU based inference but requires a lot more VRAM.</p>

          '
        raw: 'GGML is quantised for CPU-based inference (and now also supports acceleration
          from a CUDA GPU), from C++ based clients

          GPTQ is quantised for GPU-based inference, from Python code

          The base repo is float16, also for GPU based inference but requires a lot
          more VRAM.'
        updatedAt: '2023-05-18T23:03:04.343Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - masterluke7
        - streetyogi
    id: 6466ae61e0fe831b4798cb80
    type: comment
  author: TheBloke
  content: 'GGML is quantised for CPU-based inference (and now also supports acceleration
    from a CUDA GPU), from C++ based clients

    GPTQ is quantised for GPU-based inference, from Python code

    The base repo is float16, also for GPU based inference but requires a lot more
    VRAM.'
  created_at: 2023-05-18 22:01:53+00:00
  edited: true
  hidden: false
  id: 6466ae61e0fe831b4798cb80
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/545d0331700ac1dbb978b01052c89b95.svg
      fullname: Miron Tewfik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: streetyogi
      type: user
    createdAt: '2023-07-23T22:42:09.000Z'
    data:
      edited: false
      editors:
      - streetyogi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8563644289970398
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/545d0331700ac1dbb978b01052c89b95.svg
          fullname: Miron Tewfik
          isHf: false
          isPro: false
          name: streetyogi
          type: user
        html: '<p>So now with CUDA acceleration, GGML should be faster due to C++
          clients, even llama-cpp-python and ctransformers, compared to exllama?</p>

          '
        raw: So now with CUDA acceleration, GGML should be faster due to C++ clients,
          even llama-cpp-python and ctransformers, compared to exllama?
        updatedAt: '2023-07-23T22:42:09.266Z'
      numEdits: 0
      reactions: []
    id: 64bdacc1c05a0df0d2a47f78
    type: comment
  author: streetyogi
  content: So now with CUDA acceleration, GGML should be faster due to C++ clients,
    even llama-cpp-python and ctransformers, compared to exllama?
  created_at: 2023-07-23 21:42:09+00:00
  edited: false
  hidden: false
  id: 64bdacc1c05a0df0d2a47f78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-24T09:23:24.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9234351515769958
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>No, ExLlama is still the performance king.</p>

          <p>GGML with full CUDA acceleration is fast, much faster than it used to
          be.  But ExLlama still outperforms it.  For example on a 7B model with 4090
          GPU and good CPU you will be able to get 100-120 tokens/s with GGML, but
          ExLlama will do 140 - 170 tokens/s.</p>

          '
        raw: 'No, ExLlama is still the performance king.


          GGML with full CUDA acceleration is fast, much faster than it used to be.  But
          ExLlama still outperforms it.  For example on a 7B model with 4090 GPU and
          good CPU you will be able to get 100-120 tokens/s with GGML, but ExLlama
          will do 140 - 170 tokens/s.'
        updatedAt: '2023-07-24T09:23:24.518Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - streetyogi
    id: 64be430cc733e8552fed0c23
    type: comment
  author: TheBloke
  content: 'No, ExLlama is still the performance king.


    GGML with full CUDA acceleration is fast, much faster than it used to be.  But
    ExLlama still outperforms it.  For example on a 7B model with 4090 GPU and good
    CPU you will be able to get 100-120 tokens/s with GGML, but ExLlama will do 140
    - 170 tokens/s.'
  created_at: 2023-07-24 08:23:24+00:00
  edited: false
  hidden: false
  id: 64be430cc733e8552fed0c23
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/wizard-mega-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: Differences
