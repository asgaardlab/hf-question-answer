!!python/object:huggingface_hub.community.DiscussionWithDetails
author: insaf-im
conflicting_files: null
created_at: 2022-10-10 22:18:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ca47de125dc4ac0372046320820341da.svg
      fullname: Insaf Ismath
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: insaf-im
      type: user
    createdAt: '2022-10-10T23:18:18.000Z'
    data:
      edited: false
      editors:
      - insaf-im
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ca47de125dc4ac0372046320820341da.svg
          fullname: Insaf Ismath
          isHf: false
          isPro: false
          name: insaf-im
          type: user
        html: '<p>model = VideoMAEModel.from_pretrained("MCG-NJU/videomae-base")<br>list(last_hidden_states.shape)<br>[1,
          1568, 768]</p>

          <p>The output of the VideoMAE encoder is 768 features of length 1568. (1
          is the batch size)<br>Can I please know which is the [CLS] token?</p>

          '
        raw: "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\r\n\
          list(last_hidden_states.shape)\r\n[1, 1568, 768]\r\n\r\nThe output of the\
          \ VideoMAE encoder is 768 features of length 1568. (1 is the batch size)\r\
          \nCan I please know which is the [CLS] token?"
        updatedAt: '2022-10-10T23:18:18.109Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - insaf-im
        - venkateshtata
    id: 6344a83a4143f38ac402cffb
    type: comment
  author: insaf-im
  content: "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\r\nlist(last_hidden_states.shape)\r\
    \n[1, 1568, 768]\r\n\r\nThe output of the VideoMAE encoder is 768 features of\
    \ length 1568. (1 is the batch size)\r\nCan I please know which is the [CLS] token?"
  created_at: 2022-10-10 22:18:18+00:00
  edited: false
  hidden: false
  id: 6344a83a4143f38ac402cffb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-12-21T13:39:24.000Z'
    data:
      edited: true
      editors:
      - nielsr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5371484756469727
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Hi,</p>

          <p>VideoMAE does not use a CLS token. The sequence length is equal to (num_frames
          // tubelet_size) * num_patches_per_frame, with num_patches_per_frame = (image_size
          // patch_size) ** 2.</p>

          <p>Hence, in this case: (16//2) * (224 // 16)**2 = 1568.</p>

          <p>To get a representation of an entire video, you can simply average pool
          the last hidden states along the sequence dimension:</p>

          <pre><code>import torch


          video_features = torch.mean(last_hidden_state, dim=1)

          </code></pre>

          '
        raw: 'Hi,


          VideoMAE does not use a CLS token. The sequence length is equal to (num_frames
          // tubelet_size) * num_patches_per_frame, with num_patches_per_frame = (image_size
          // patch_size) ** 2.


          Hence, in this case: (16//2) * (224 // 16)**2 = 1568.


          To get a representation of an entire video, you can simply average pool
          the last hidden states along the sequence dimension:

          ```

          import torch


          video_features = torch.mean(last_hidden_state, dim=1)

          ```'
        updatedAt: '2023-12-21T13:40:01.963Z'
      numEdits: 1
      reactions: []
    id: 6584400cb02f38ef3492b49b
    type: comment
  author: nielsr
  content: 'Hi,


    VideoMAE does not use a CLS token. The sequence length is equal to (num_frames
    // tubelet_size) * num_patches_per_frame, with num_patches_per_frame = (image_size
    // patch_size) ** 2.


    Hence, in this case: (16//2) * (224 // 16)**2 = 1568.


    To get a representation of an entire video, you can simply average pool the last
    hidden states along the sequence dimension:

    ```

    import torch


    video_features = torch.mean(last_hidden_state, dim=1)

    ```'
  created_at: 2023-12-21 13:39:24+00:00
  edited: true
  hidden: false
  id: 6584400cb02f38ef3492b49b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: MCG-NJU/videomae-base
repo_type: model
status: open
target_branch: null
title: '[CLS] Token'
