!!python/object:huggingface_hub.community.DiscussionWithDetails
author: doguaraci
conflicting_files: null
created_at: 2023-06-09 15:24:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b0280ab296d18c913cda8b6ba4939a14.svg
      fullname: Dogu Araci
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: doguaraci
      type: user
    createdAt: '2023-06-09T16:24:34.000Z'
    data:
      edited: false
      editors:
      - doguaraci
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5453005433082581
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b0280ab296d18c913cda8b6ba4939a14.svg
          fullname: Dogu Araci
          isHf: false
          isPro: true
          name: doguaraci
          type: user
        html: "<p>I've been trying the model with the triton on/off with the below\
          \ code, and triton is almost 3 times slower in my environment (A10G). Do\
          \ you have any guidance on this?</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> time\n<span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM, AutoConfig,\
          \ AutoTokenizer\n\nconfig = AutoConfig.from_pretrained(\n    <span class=\"\
          hljs-string\">\"replit/replit-code-v1-3b\"</span>,\n    trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>\n)\n\nconfig.attn_config[<span class=\"\
          hljs-string\">'attn_impl'</span>] = <span class=\"hljs-string\">'triton'</span>\
          \ <span class=\"hljs-comment\"># I'm commenting out this to try with 'torch'\
          \ implementation</span>\n\nmodel = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">'replit/replit-code-v1-3b'</span>, config=config,\
          \ trust_remote_code=<span class=\"hljs-literal\">True</span>)\nmodel.to(device=<span\
          \ class=\"hljs-string\">'cuda:0'</span>, dtype=torch.bfloat16)\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">'replit/replit-code-v1-3b'</span>,\
          \ trust_remote_code=<span class=\"hljs-literal\">True</span>)\nx = tokenizer.encode(<span\
          \ class=\"hljs-string\">'def hello():\\n  print(\"hello world\")\\n'</span>,\
          \ return_tensors=<span class=\"hljs-string\">'pt'</span>).to(<span class=\"\
          hljs-string\">'cuda'</span>)\n\nstart = time.time()\ny = model.generate(x,\
          \ max_new_tokens=<span class=\"hljs-number\">64</span>)\nend = time.time()\n\
          \n<span class=\"hljs-built_in\">print</span>(end - start)\n</code></pre>\n"
        raw: "I've been trying the model with the triton on/off with the below code,\
          \ and triton is almost 3 times slower in my environment (A10G). Do you have\
          \ any guidance on this?\r\n\r\n```python\r\nimport time\r\nimport torch\r\
          \nfrom transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\r\
          \n\r\nconfig = AutoConfig.from_pretrained(\r\n    \"replit/replit-code-v1-3b\"\
          ,\r\n    trust_remote_code=True\r\n)\r\n\r\nconfig.attn_config['attn_impl']\
          \ = 'triton' # I'm commenting out this to try with 'torch' implementation\r\
          \n\r\nmodel = AutoModelForCausalLM.from_pretrained('replit/replit-code-v1-3b',\
          \ config=config, trust_remote_code=True)\r\nmodel.to(device='cuda:0', dtype=torch.bfloat16)\r\
          \n\r\ntokenizer = AutoTokenizer.from_pretrained('replit/replit-code-v1-3b',\
          \ trust_remote_code=True)\r\nx = tokenizer.encode('def hello():\\n  print(\"\
          hello world\")\\n', return_tensors='pt').to('cuda')\r\n\r\nstart = time.time()\r\
          \ny = model.generate(x, max_new_tokens=64)\r\nend = time.time()\r\n\r\n\
          print(end - start)\r\n```"
        updatedAt: '2023-06-09T16:24:34.244Z'
      numEdits: 0
      reactions: []
    id: 64835242b785e80ba2da22a0
    type: comment
  author: doguaraci
  content: "I've been trying the model with the triton on/off with the below code,\
    \ and triton is almost 3 times slower in my environment (A10G). Do you have any\
    \ guidance on this?\r\n\r\n```python\r\nimport time\r\nimport torch\r\nfrom transformers\
    \ import AutoModelForCausalLM, AutoConfig, AutoTokenizer\r\n\r\nconfig = AutoConfig.from_pretrained(\r\
    \n    \"replit/replit-code-v1-3b\",\r\n    trust_remote_code=True\r\n)\r\n\r\n\
    config.attn_config['attn_impl'] = 'triton' # I'm commenting out this to try with\
    \ 'torch' implementation\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained('replit/replit-code-v1-3b',\
    \ config=config, trust_remote_code=True)\r\nmodel.to(device='cuda:0', dtype=torch.bfloat16)\r\
    \n\r\ntokenizer = AutoTokenizer.from_pretrained('replit/replit-code-v1-3b', trust_remote_code=True)\r\
    \nx = tokenizer.encode('def hello():\\n  print(\"hello world\")\\n', return_tensors='pt').to('cuda')\r\
    \n\r\nstart = time.time()\r\ny = model.generate(x, max_new_tokens=64)\r\nend =\
    \ time.time()\r\n\r\nprint(end - start)\r\n```"
  created_at: 2023-06-09 15:24:34+00:00
  edited: false
  hidden: false
  id: 64835242b785e80ba2da22a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b0280ab296d18c913cda8b6ba4939a14.svg
      fullname: Dogu Araci
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: doguaraci
      type: user
    createdAt: '2023-07-06T16:21:17.000Z'
    data:
      status: closed
    id: 64a6e9fd0c36529d75efee4d
    type: status-change
  author: doguaraci
  created_at: 2023-07-06 15:21:17+00:00
  id: 64a6e9fd0c36529d75efee4d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: replit/replit-code-v1-3b
repo_type: model
status: closed
target_branch: null
title: Triton is slower?
