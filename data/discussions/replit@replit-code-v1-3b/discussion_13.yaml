!!python/object:huggingface_hub.community.DiscussionWithDetails
author: arminnorouzi
conflicting_files: null
created_at: 2023-05-15 17:41:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69b3c3b05da38c2720979885a307d77d.svg
      fullname: armin norouzi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arminnorouzi
      type: user
    createdAt: '2023-05-15T18:41:15.000Z'
    data:
      edited: false
      editors:
      - arminnorouzi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69b3c3b05da38c2720979885a307d77d.svg
          fullname: armin norouzi
          isHf: false
          isPro: false
          name: arminnorouzi
          type: user
        html: '<p>Did you test deploying on sagemaker using huggigfacemodel API similar
          to this notebook:</p>

          <p><a rel="nofollow" href="https://github.com/huggingface/notebooks/blob/main/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb">https://github.com/huggingface/notebooks/blob/main/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb</a></p>

          <p>I cloned repo and uploaded model.tar.gz to s3. When deploying it, I got
          error that task need to be set:</p>

          <p>An error occurred (ModelError) when calling the InvokeEndpoint operation:
          Received client error (400) from primary with message "{<br>  "code": 400,<br>  "type":
          "InternalServerException",<br>  "message": "Task couldn\u0027t be inferenced
          from ReplitLM.Inference Toolkit can only inference tasks from architectures
          ending with [\u0027TapasForQuestionAnswering\u0027, \u0027ForQuestionAnswering\u0027,
          \u0027ForTokenClassification\u0027, \u0027ForSequenceClassification\u0027,
          \u0027ForMultipleChoice\u0027, \u0027ForMaskedLM\u0027, \u0027ForCausalLM\u0027,
          \u0027ForConditionalGeneration\u0027, \u0027MTModel\u0027, \u0027EncoderDecoderModel\u0027,
          \u0027GPT2LMHeadModel\u0027, \u0027T5WithLMHeadModel\u0027].Use env <code>HF_TASK</code>
          to define your task."<br>}</p>

          <p>After setting task by feeding HF_TASK as env variables, I got this error:</p>

          <p>An error occurred (ModelError) when calling the InvokeEndpoint operation:
          Received client error (400) from primary with message "{<br>  "code": 400,<br>  "type":
          "InternalServerException",<br>  "message": "Loading /.sagemaker/mms/models/model
          requires you to execute the configuration file in that repo on your local
          machine. Make sure you have read the code there to avoid malicious use,
          then set the option <code>trust_remote_code\u003dTrue</code> to remove this
          error."<br>}</p>

          <p>Here is my implementation: </p>

          <p>hub = {<br> ''HF_TASK'':''text-generation''<br>}</p>

          <p>huggingface_model = HuggingFaceModel(<br>   model_data=s3_location,       #
          path to your model and script<br>   role=role,                    # iam
          role with permissions to create an Endpoint<br>   transformers_version="4.17.0",  #
          transformers version used<br>   pytorch_version="1.10.2",        # pytorch
          version used<br>   py_version=''py38'',            # python version used<br>    env=hub<br>)</p>

          <h1 id="deploy-the-endpoint-endpoint">deploy the endpoint endpoint</h1>

          <p>predictor = huggingface_model.deploy(<br>    initial_instance_count=1,<br>    instance_type="ml.g4dn.8xlarge",<br>    )</p>

          <p>Would you please help me.</p>

          '
        raw: "Did you test deploying on sagemaker using huggigfacemodel API similar\
          \ to this notebook:\r\n\r\nhttps://github.com/huggingface/notebooks/blob/main/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb\r\
          \n\r\nI cloned repo and uploaded model.tar.gz to s3. When deploying it,\
          \ I got error that task need to be set:\r\n\r\nAn error occurred (ModelError)\
          \ when calling the InvokeEndpoint operation: Received client error (400)\
          \ from primary with message \"{\r\n  \"code\": 400,\r\n  \"type\": \"InternalServerException\"\
          ,\r\n  \"message\": \"Task couldn\\u0027t be inferenced from ReplitLM.Inference\
          \ Toolkit can only inference tasks from architectures ending with [\\u0027TapasForQuestionAnswering\\\
          u0027, \\u0027ForQuestionAnswering\\u0027, \\u0027ForTokenClassification\\\
          u0027, \\u0027ForSequenceClassification\\u0027, \\u0027ForMultipleChoice\\\
          u0027, \\u0027ForMaskedLM\\u0027, \\u0027ForCausalLM\\u0027, \\u0027ForConditionalGeneration\\\
          u0027, \\u0027MTModel\\u0027, \\u0027EncoderDecoderModel\\u0027, \\u0027GPT2LMHeadModel\\\
          u0027, \\u0027T5WithLMHeadModel\\u0027].Use env `HF_TASK` to define your\
          \ task.\"\r\n}\r\n\r\nAfter setting task by feeding HF_TASK as env variables,\
          \ I got this error:\r\n\r\nAn error occurred (ModelError) when calling the\
          \ InvokeEndpoint operation: Received client error (400) from primary with\
          \ message \"{\r\n  \"code\": 400,\r\n  \"type\": \"InternalServerException\"\
          ,\r\n  \"message\": \"Loading /.sagemaker/mms/models/model requires you\
          \ to execute the configuration file in that repo on your local machine.\
          \ Make sure you have read the code there to avoid malicious use, then set\
          \ the option `trust_remote_code\\u003dTrue` to remove this error.\"\r\n\
          }\r\n\r\n\r\nHere is my implementation: \r\n\r\nhub = {\r\n 'HF_TASK':'text-generation'\r\
          \n}\r\n\r\n\r\nhuggingface_model = HuggingFaceModel(\r\n   model_data=s3_location,\
          \       # path to your model and script\r\n   role=role,               \
          \     # iam role with permissions to create an Endpoint\r\n   transformers_version=\"\
          4.17.0\",  # transformers version used\r\n   pytorch_version=\"1.10.2\"\
          ,        # pytorch version used\r\n   py_version='py38',            # python\
          \ version used\r\n    env=hub\r\n)\r\n\r\n# deploy the endpoint endpoint\r\
          \npredictor = huggingface_model.deploy(\r\n    initial_instance_count=1,\r\
          \n    instance_type=\"ml.g4dn.8xlarge\",\r\n    )\r\n\r\nWould you please\
          \ help me."
        updatedAt: '2023-05-15T18:41:15.631Z'
      numEdits: 0
      reactions: []
    id: 64627ccbb438438da3c1534c
    type: comment
  author: arminnorouzi
  content: "Did you test deploying on sagemaker using huggigfacemodel API similar\
    \ to this notebook:\r\n\r\nhttps://github.com/huggingface/notebooks/blob/main/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb\r\
    \n\r\nI cloned repo and uploaded model.tar.gz to s3. When deploying it, I got\
    \ error that task need to be set:\r\n\r\nAn error occurred (ModelError) when calling\
    \ the InvokeEndpoint operation: Received client error (400) from primary with\
    \ message \"{\r\n  \"code\": 400,\r\n  \"type\": \"InternalServerException\",\r\
    \n  \"message\": \"Task couldn\\u0027t be inferenced from ReplitLM.Inference Toolkit\
    \ can only inference tasks from architectures ending with [\\u0027TapasForQuestionAnswering\\\
    u0027, \\u0027ForQuestionAnswering\\u0027, \\u0027ForTokenClassification\\u0027,\
    \ \\u0027ForSequenceClassification\\u0027, \\u0027ForMultipleChoice\\u0027, \\\
    u0027ForMaskedLM\\u0027, \\u0027ForCausalLM\\u0027, \\u0027ForConditionalGeneration\\\
    u0027, \\u0027MTModel\\u0027, \\u0027EncoderDecoderModel\\u0027, \\u0027GPT2LMHeadModel\\\
    u0027, \\u0027T5WithLMHeadModel\\u0027].Use env `HF_TASK` to define your task.\"\
    \r\n}\r\n\r\nAfter setting task by feeding HF_TASK as env variables, I got this\
    \ error:\r\n\r\nAn error occurred (ModelError) when calling the InvokeEndpoint\
    \ operation: Received client error (400) from primary with message \"{\r\n  \"\
    code\": 400,\r\n  \"type\": \"InternalServerException\",\r\n  \"message\": \"\
    Loading /.sagemaker/mms/models/model requires you to execute the configuration\
    \ file in that repo on your local machine. Make sure you have read the code there\
    \ to avoid malicious use, then set the option `trust_remote_code\\u003dTrue` to\
    \ remove this error.\"\r\n}\r\n\r\n\r\nHere is my implementation: \r\n\r\nhub\
    \ = {\r\n 'HF_TASK':'text-generation'\r\n}\r\n\r\n\r\nhuggingface_model = HuggingFaceModel(\r\
    \n   model_data=s3_location,       # path to your model and script\r\n   role=role,\
    \                    # iam role with permissions to create an Endpoint\r\n   transformers_version=\"\
    4.17.0\",  # transformers version used\r\n   pytorch_version=\"1.10.2\",     \
    \   # pytorch version used\r\n   py_version='py38',            # python version\
    \ used\r\n    env=hub\r\n)\r\n\r\n# deploy the endpoint endpoint\r\npredictor\
    \ = huggingface_model.deploy(\r\n    initial_instance_count=1,\r\n    instance_type=\"\
    ml.g4dn.8xlarge\",\r\n    )\r\n\r\nWould you please help me."
  created_at: 2023-05-15 17:41:15+00:00
  edited: false
  hidden: false
  id: 64627ccbb438438da3c1534c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c92fc2fa4372a336c70098f42cc2b93.svg
      fullname: Joselin Sushma J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JoselinSushma
      type: user
    createdAt: '2023-05-23T07:15:19.000Z'
    data:
      edited: false
      editors:
      - JoselinSushma
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c92fc2fa4372a336c70098f42cc2b93.svg
          fullname: Joselin Sushma J
          isHf: false
          isPro: false
          name: JoselinSushma
          type: user
        html: '<p>+1</p>

          '
        raw: '+1'
        updatedAt: '2023-05-23T07:15:19.577Z'
      numEdits: 0
      reactions: []
    id: 646c68072c29b8753acc2a9f
    type: comment
  author: JoselinSushma
  content: '+1'
  created_at: 2023-05-23 06:15:19+00:00
  edited: false
  hidden: false
  id: 646c68072c29b8753acc2a9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c92fc2fa4372a336c70098f42cc2b93.svg
      fullname: Joselin Sushma J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JoselinSushma
      type: user
    createdAt: '2023-05-24T15:43:26.000Z'
    data:
      edited: false
      editors:
      - JoselinSushma
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c92fc2fa4372a336c70098f42cc2b93.svg
          fullname: Joselin Sushma J
          isHf: false
          isPro: false
          name: JoselinSushma
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;arminnorouzi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/arminnorouzi\"\
          >@<span class=\"underline\">arminnorouzi</span></a></span>\n\n\t</span></span>\
          \ Can you please tell how did you download the model and upload to s3. i\
          \ am using sagemaker notebook to do that but running out of space even if\
          \ larger instance is used.</p>\n"
        raw: '@arminnorouzi Can you please tell how did you download the model and
          upload to s3. i am using sagemaker notebook to do that but running out of
          space even if larger instance is used.'
        updatedAt: '2023-05-24T15:43:26.952Z'
      numEdits: 0
      reactions: []
    id: 646e309ee34b2ec2d2e291d7
    type: comment
  author: JoselinSushma
  content: '@arminnorouzi Can you please tell how did you download the model and upload
    to s3. i am using sagemaker notebook to do that but running out of space even
    if larger instance is used.'
  created_at: 2023-05-24 14:43:26+00:00
  edited: false
  hidden: false
  id: 646e309ee34b2ec2d2e291d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69b3c3b05da38c2720979885a307d77d.svg
      fullname: armin norouzi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arminnorouzi
      type: user
    createdAt: '2023-05-24T15:55:58.000Z'
    data:
      edited: false
      editors:
      - arminnorouzi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69b3c3b05da38c2720979885a307d77d.svg
          fullname: armin norouzi
          isHf: false
          isPro: false
          name: arminnorouzi
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;JoselinSushma&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/JoselinSushma\"\
          >@<span class=\"underline\">JoselinSushma</span></a></span>\n\n\t</span></span>\
          \ I used relatively large instances as I was trying a larger model (starcoder).\
          \  I tried this, and it worked: ml.g4dn.2xlarge</p>\n"
        raw: '@JoselinSushma I used relatively large instances as I was trying a larger
          model (starcoder).  I tried this, and it worked: ml.g4dn.2xlarge'
        updatedAt: '2023-05-24T15:55:58.587Z'
      numEdits: 0
      reactions: []
    id: 646e338e9e252f776e46586a
    type: comment
  author: arminnorouzi
  content: '@JoselinSushma I used relatively large instances as I was trying a larger
    model (starcoder).  I tried this, and it worked: ml.g4dn.2xlarge'
  created_at: 2023-05-24 14:55:58+00:00
  edited: false
  hidden: false
  id: 646e338e9e252f776e46586a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c92fc2fa4372a336c70098f42cc2b93.svg
      fullname: Joselin Sushma J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JoselinSushma
      type: user
    createdAt: '2023-05-25T15:11:10.000Z'
    data:
      edited: true
      editors:
      - JoselinSushma
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c92fc2fa4372a336c70098f42cc2b93.svg
          fullname: Joselin Sushma J
          isHf: false
          isPro: false
          name: JoselinSushma
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;arminnorouzi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/arminnorouzi\"\
          >@<span class=\"underline\">arminnorouzi</span></a></span>\n\n\t</span></span>\
          \ ,  Thank you,<br>I tried adding custom function for model_fn and predict_fn.\
          \ Reference : <a href=\"https://huggingface.co/docs/sagemaker/inference\"\
          >https://huggingface.co/docs/sagemaker/inference</a><br>model_fn to load\
          \ model with trust_remote=True as given in model card . It worked. </p>\n\
          <p>However Invocation in sagemaker takes more than 60sec which timesout\
          \ if a longer code has to be generated.</p>\n"
        raw: "@arminnorouzi ,  Thank you,\nI tried adding custom function for model_fn\
          \ and predict_fn. Reference : https://huggingface.co/docs/sagemaker/inference\n\
          model_fn to load model with trust_remote=True as given in model card . It\
          \ worked. \n\nHowever Invocation in sagemaker takes more than 60sec which\
          \ timesout if a longer code has to be generated."
        updatedAt: '2023-05-25T15:12:39.108Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pirroh
    id: 646f7a8ed1f1b73079e7d8b3
    type: comment
  author: JoselinSushma
  content: "@arminnorouzi ,  Thank you,\nI tried adding custom function for model_fn\
    \ and predict_fn. Reference : https://huggingface.co/docs/sagemaker/inference\n\
    model_fn to load model with trust_remote=True as given in model card . It worked.\
    \ \n\nHowever Invocation in sagemaker takes more than 60sec which timesout if\
    \ a longer code has to be generated."
  created_at: 2023-05-25 14:11:10+00:00
  edited: true
  hidden: false
  id: 646f7a8ed1f1b73079e7d8b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69b3c3b05da38c2720979885a307d77d.svg
      fullname: armin norouzi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arminnorouzi
      type: user
    createdAt: '2023-05-25T15:54:17.000Z'
    data:
      edited: false
      editors:
      - arminnorouzi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69b3c3b05da38c2720979885a307d77d.svg
          fullname: armin norouzi
          isHf: false
          isPro: false
          name: arminnorouzi
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;JoselinSushma&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/JoselinSushma\"\
          >@<span class=\"underline\">JoselinSushma</span></a></span>\n\n\t</span></span>\
          \ is it possible to share the custom function you wrote here? Also, for\
          \ deployment, did you use these versions?</p>\n<p>transformers_version=\"\
          4.17.0\", # transformers version used<br>pytorch_version=\"1.10.2\", # pytorch\
          \ version used<br>py_version='py38', # python version used</p>\n"
        raw: '@JoselinSushma is it possible to share the custom function you wrote
          here? Also, for deployment, did you use these versions?


          transformers_version="4.17.0", # transformers version used

          pytorch_version="1.10.2", # pytorch version used

          py_version=''py38'', # python version used'
        updatedAt: '2023-05-25T15:54:17.706Z'
      numEdits: 0
      reactions: []
    id: 646f84a96098ee820fbb8d14
    type: comment
  author: arminnorouzi
  content: '@JoselinSushma is it possible to share the custom function you wrote here?
    Also, for deployment, did you use these versions?


    transformers_version="4.17.0", # transformers version used

    pytorch_version="1.10.2", # pytorch version used

    py_version=''py38'', # python version used'
  created_at: 2023-05-25 14:54:17+00:00
  edited: false
  hidden: false
  id: 646f84a96098ee820fbb8d14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c92fc2fa4372a336c70098f42cc2b93.svg
      fullname: Joselin Sushma J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JoselinSushma
      type: user
    createdAt: '2023-05-26T06:43:38.000Z'
    data:
      edited: false
      editors:
      - JoselinSushma
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c92fc2fa4372a336c70098f42cc2b93.svg
          fullname: Joselin Sushma J
          isHf: false
          isPro: false
          name: JoselinSushma
          type: user
        html: '<p>def model_fn(model_dir):<br>    tokenizer = AutoTokenizer.from_pretrained(model_dir,
          trust_remote_code=True)<br>    model = AutoModelForCausalLM.from_pretrained(model_dir,
          trust_remote_code=True)<br>    code_generator = pipeline("text-generation",
          model=model, tokenizer=tokenizer)<br>    return code_generator</p>

          '
        raw: "def model_fn(model_dir):\n    tokenizer = AutoTokenizer.from_pretrained(model_dir,\
          \ trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(model_dir,\
          \ trust_remote_code=True)\n    code_generator = pipeline(\"text-generation\"\
          , model=model, tokenizer=tokenizer)\n    return code_generator"
        updatedAt: '2023-05-26T06:43:38.601Z'
      numEdits: 0
      reactions: []
    id: 6470551a9fe78d69a8aa2e2f
    type: comment
  author: JoselinSushma
  content: "def model_fn(model_dir):\n    tokenizer = AutoTokenizer.from_pretrained(model_dir,\
    \ trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(model_dir,\
    \ trust_remote_code=True)\n    code_generator = pipeline(\"text-generation\",\
    \ model=model, tokenizer=tokenizer)\n    return code_generator"
  created_at: 2023-05-26 05:43:38+00:00
  edited: false
  hidden: false
  id: 6470551a9fe78d69a8aa2e2f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c92fc2fa4372a336c70098f42cc2b93.svg
      fullname: Joselin Sushma J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JoselinSushma
      type: user
    createdAt: '2023-05-26T06:44:13.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/1c92fc2fa4372a336c70098f42cc2b93.svg
          fullname: Joselin Sushma J
          isHf: false
          isPro: false
          name: JoselinSushma
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-05-26T06:45:03.356Z'
      numEdits: 0
      reactions: []
    id: 6470553d3df93fddecdbe6c0
    type: comment
  author: JoselinSushma
  content: This comment has been hidden
  created_at: 2023-05-26 05:44:13+00:00
  edited: true
  hidden: true
  id: 6470553d3df93fddecdbe6c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6434ad9cea46c009904c91c2/Ofa-wDlcUHSWH48LgOHeZ.jpeg?w=200&h=200&f=face
      fullname: Michele Catasta
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pirroh
      type: user
    createdAt: '2023-06-05T03:19:37.000Z'
    data:
      edited: false
      editors:
      - pirroh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9806270003318787
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6434ad9cea46c009904c91c2/Ofa-wDlcUHSWH48LgOHeZ.jpeg?w=200&h=200&f=face
          fullname: Michele Catasta
          isHf: false
          isPro: false
          name: pirroh
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;JoselinSushma&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/JoselinSushma\"\
          >@<span class=\"underline\">JoselinSushma</span></a></span>\n\n\t</span></span>\
          \ for providing support on this issue!<br>Closing for now, as you seem to\
          \ have made it work correctly \U0001F64C</p>\n"
        raw: "Thanks @JoselinSushma for providing support on this issue!\nClosing\
          \ for now, as you seem to have made it work correctly \U0001F64C"
        updatedAt: '2023-06-05T03:19:37.072Z'
      numEdits: 0
      reactions: []
      relatedEventId: 647d54491c0644de8d3e1ac7
    id: 647d54491c0644de8d3e1ac6
    type: comment
  author: pirroh
  content: "Thanks @JoselinSushma for providing support on this issue!\nClosing for\
    \ now, as you seem to have made it work correctly \U0001F64C"
  created_at: 2023-06-05 02:19:37+00:00
  edited: false
  hidden: false
  id: 647d54491c0644de8d3e1ac6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6434ad9cea46c009904c91c2/Ofa-wDlcUHSWH48LgOHeZ.jpeg?w=200&h=200&f=face
      fullname: Michele Catasta
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pirroh
      type: user
    createdAt: '2023-06-05T03:19:37.000Z'
    data:
      status: closed
    id: 647d54491c0644de8d3e1ac7
    type: status-change
  author: pirroh
  created_at: 2023-06-05 02:19:37+00:00
  id: 647d54491c0644de8d3e1ac7
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: replit/replit-code-v1-3b
repo_type: model
status: closed
target_branch: null
title: 'huggigfacemodel deployment '
