!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zeroing
conflicting_files: null
created_at: 2023-05-03 20:07:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/73ef44325cd3ed23d1ade3703773f912.svg
      fullname: PM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zeroing
      type: user
    createdAt: '2023-05-03T21:07:34.000Z'
    data:
      edited: false
      editors:
      - zeroing
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/73ef44325cd3ed23d1ade3703773f912.svg
          fullname: PM
          isHf: false
          isPro: false
          name: zeroing
          type: user
        html: "<p>Title is self-explanatory. \U0001F601</p>\n"
        raw: "Title is self-explanatory. \U0001F601"
        updatedAt: '2023-05-03T21:07:34.620Z'
      numEdits: 0
      reactions: []
    id: 6452cd16fc4b47877abcf17c
    type: comment
  author: zeroing
  content: "Title is self-explanatory. \U0001F601"
  created_at: 2023-05-03 20:07:34+00:00
  edited: false
  hidden: false
  id: 6452cd16fc4b47877abcf17c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6434ad9cea46c009904c91c2/Ofa-wDlcUHSWH48LgOHeZ.jpeg?w=200&h=200&f=face
      fullname: Michele Catasta
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pirroh
      type: user
    createdAt: '2023-05-03T21:18:24.000Z'
    data:
      edited: false
      editors:
      - pirroh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6434ad9cea46c009904c91c2/Ofa-wDlcUHSWH48LgOHeZ.jpeg?w=200&h=200&f=face
          fullname: Michele Catasta
          isHf: false
          isPro: false
          name: pirroh
          type: user
        html: "<p>Not easy to give you a reliable answer, given that \"minimum\" requirement\
          \ could be also CPU inference... if you are willing to wait minutes for\
          \ a few tokens \U0001F604<br>That said, we are hosting <a href=\"https://huggingface.co/spaces/replit/replit-code-v1-3b-demo\"\
          >our demo</a> on an NVIDIA A10G, and it looks pretty fast!<br>Further quantization\
          \ with <a href=\"https://huggingface.co/blog/hf-bitsandbytes-integration\"\
          >LLM.int8()</a> would help you load the model even on smaller GPUs.</p>\n"
        raw: "Not easy to give you a reliable answer, given that \"minimum\" requirement\
          \ could be also CPU inference... if you are willing to wait minutes for\
          \ a few tokens \U0001F604\nThat said, we are hosting [our demo](https://huggingface.co/spaces/replit/replit-code-v1-3b-demo)\
          \ on an NVIDIA A10G, and it looks pretty fast!\nFurther quantization with\
          \ [LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration)\
          \ would help you load the model even on smaller GPUs."
        updatedAt: '2023-05-03T21:18:24.835Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - zeroing
        - madhavatreplit
        - muhtasham
        - qftie
    id: 6452cfa0a0c0a664a24ae71a
    type: comment
  author: pirroh
  content: "Not easy to give you a reliable answer, given that \"minimum\" requirement\
    \ could be also CPU inference... if you are willing to wait minutes for a few\
    \ tokens \U0001F604\nThat said, we are hosting [our demo](https://huggingface.co/spaces/replit/replit-code-v1-3b-demo)\
    \ on an NVIDIA A10G, and it looks pretty fast!\nFurther quantization with [LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration)\
    \ would help you load the model even on smaller GPUs."
  created_at: 2023-05-03 20:18:24+00:00
  edited: false
  hidden: false
  id: 6452cfa0a0c0a664a24ae71a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0640fbbf09e7f5bc19d4f483e64e7678.svg
      fullname: merlinchan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: merlinarer
      type: user
    createdAt: '2023-05-04T12:31:28.000Z'
    data:
      edited: false
      editors:
      - merlinarer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0640fbbf09e7f5bc19d4f483e64e7678.svg
          fullname: merlinchan
          isHf: false
          isPro: false
          name: merlinarer
          type: user
        html: "<blockquote>\n<p>Not easy to give you a reliable answer, given that\
          \ \"minimum\" requirement could be also CPU inference... if you are willing\
          \ to wait minutes for a few tokens \U0001F604<br>That said, we are hosting\
          \ <a href=\"https://huggingface.co/spaces/replit/replit-code-v1-3b-demo\"\
          >our demo</a> on an NVIDIA A10G, and it looks pretty fast!<br>Further quantization\
          \ with <a href=\"https://huggingface.co/blog/hf-bitsandbytes-integration\"\
          >LLM.int8()</a> would help you load the model even on smaller GPUs.</p>\n\
          </blockquote>\n<p>hello, is the demo hosted naively with transfomers as\
          \ described in model card ? I thought it is hosted with optimized method\
          \ such as fastertransfomer.</p>\n"
        raw: "> Not easy to give you a reliable answer, given that \"minimum\" requirement\
          \ could be also CPU inference... if you are willing to wait minutes for\
          \ a few tokens \U0001F604\n> That said, we are hosting [our demo](https://huggingface.co/spaces/replit/replit-code-v1-3b-demo)\
          \ on an NVIDIA A10G, and it looks pretty fast!\n> Further quantization with\
          \ [LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration)\
          \ would help you load the model even on smaller GPUs.\n\nhello, is the demo\
          \ hosted naively with transfomers as described in model card ? I thought\
          \ it is hosted with optimized method such as fastertransfomer."
        updatedAt: '2023-05-04T12:31:28.628Z'
      numEdits: 0
      reactions: []
    id: 6453a5a0fef4d71cdd712e3f
    type: comment
  author: merlinarer
  content: "> Not easy to give you a reliable answer, given that \"minimum\" requirement\
    \ could be also CPU inference... if you are willing to wait minutes for a few\
    \ tokens \U0001F604\n> That said, we are hosting [our demo](https://huggingface.co/spaces/replit/replit-code-v1-3b-demo)\
    \ on an NVIDIA A10G, and it looks pretty fast!\n> Further quantization with [LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration)\
    \ would help you load the model even on smaller GPUs.\n\nhello, is the demo hosted\
    \ naively with transfomers as described in model card ? I thought it is hosted\
    \ with optimized method such as fastertransfomer."
  created_at: 2023-05-04 11:31:28+00:00
  edited: false
  hidden: false
  id: 6453a5a0fef4d71cdd712e3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fa933dbbbfe85eb59f73c12e065ed712.svg
      fullname: Madhav
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: madhavatreplit
      type: user
    createdAt: '2023-05-05T19:50:12.000Z'
    data:
      edited: false
      editors:
      - madhavatreplit
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fa933dbbbfe85eb59f73c12e065ed712.svg
          fullname: Madhav
          isHf: false
          isPro: false
          name: madhavatreplit
          type: user
        html: "<p>It's hosted natively on the GPU as described in the model card with\
          \ bfloat16 precision, but without any flash attention \u2014&nbsp;i.e. the\
          \ <code>attn_impl</code> kwarg defaults to 'torch'.</p>\n"
        raw: "It's hosted natively on the GPU as described in the model card with\
          \ bfloat16 precision, but without any flash attention \u2014\_i.e. the `attn_impl`\
          \ kwarg defaults to 'torch'."
        updatedAt: '2023-05-05T19:50:12.317Z'
      numEdits: 0
      reactions: []
    id: 64555df4a473375be5774670
    type: comment
  author: madhavatreplit
  content: "It's hosted natively on the GPU as described in the model card with bfloat16\
    \ precision, but without any flash attention \u2014\_i.e. the `attn_impl` kwarg\
    \ defaults to 'torch'."
  created_at: 2023-05-05 18:50:12+00:00
  edited: false
  hidden: false
  id: 64555df4a473375be5774670
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e88ae33e3b4b8501e37817261478d03.svg
      fullname: xingxiangrui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luoji12345
      type: user
    createdAt: '2023-05-17T08:12:35.000Z'
    data:
      edited: false
      editors:
      - luoji12345
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e88ae33e3b4b8501e37817261478d03.svg
          fullname: xingxiangrui
          isHf: false
          isPro: false
          name: luoji12345
          type: user
        html: "<blockquote>\n<p>It's hosted natively on the GPU as described in the\
          \ model card with bfloat16 precision, but without any flash attention \u2014\
          &nbsp;i.e. the <code>attn_impl</code> kwarg defaults to 'torch'.</p>\n</blockquote>\n\
          <p>When I  set  <code>attn_impl</code> to <code>flash</code> ,  the <code>flash_attn</code>\
          \ can not be matched with <code>alibi</code> , so  <code>wpe</code> layer\
          \ must be newly initialized.<br>My question is :</p>\n<ul>\n<li>Whether\
          \ you will release the <code>flash_attn</code> version model  ?  That model\
          \ have the <code>wpe</code> layer that match the <code>flash_attn</code>\
          \ config.</li>\n<li>How faster is the <code>flash_attn</code> than <code>torch</code>\
          \  ?  Will it be faster in the inference process ?</li>\n<li>Weather you\
          \ will release faster_transformer version of code ?  It will benefit many\
          \ people.</li>\n</ul>\n"
        raw: "> It's hosted natively on the GPU as described in the model card with\
          \ bfloat16 precision, but without any flash attention \u2014\_i.e. the `attn_impl`\
          \ kwarg defaults to 'torch'.\n\nWhen I  set  `attn_impl` to `flash` ,  the\
          \ `flash_attn` can not be matched with `alibi` , so  `wpe` layer must be\
          \ newly initialized. \nMy question is :\n* Whether you will release the\
          \ `flash_attn` version model  ?  That model have the `wpe` layer that match\
          \ the `flash_attn` config.\n* How faster is the `flash_attn` than `torch`\
          \  ?  Will it be faster in the inference process ?\n* Weather you will release\
          \ faster_transformer version of code ?  It will benefit many people."
        updatedAt: '2023-05-17T08:12:35.109Z'
      numEdits: 0
      reactions: []
    id: 64648c7327d27c1fe741224a
    type: comment
  author: luoji12345
  content: "> It's hosted natively on the GPU as described in the model card with\
    \ bfloat16 precision, but without any flash attention \u2014\_i.e. the `attn_impl`\
    \ kwarg defaults to 'torch'.\n\nWhen I  set  `attn_impl` to `flash` ,  the `flash_attn`\
    \ can not be matched with `alibi` , so  `wpe` layer must be newly initialized.\
    \ \nMy question is :\n* Whether you will release the `flash_attn` version model\
    \  ?  That model have the `wpe` layer that match the `flash_attn` config.\n* How\
    \ faster is the `flash_attn` than `torch`  ?  Will it be faster in the inference\
    \ process ?\n* Weather you will release faster_transformer version of code ? \
    \ It will benefit many people."
  created_at: 2023-05-17 07:12:35+00:00
  edited: false
  hidden: false
  id: 64648c7327d27c1fe741224a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/83e5ffe4ea98251acd54eb52fa10cead.svg
      fullname: Vlad Grichina
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vgrichina
      type: user
    createdAt: '2023-05-17T23:29:48.000Z'
    data:
      edited: false
      editors:
      - vgrichina
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/83e5ffe4ea98251acd54eb52fa10cead.svg
          fullname: Vlad Grichina
          isHf: false
          isPro: false
          name: vgrichina
          type: user
        html: '<p>ggml just merged CPU inference implementation:<br><a rel="nofollow"
          href="https://github.com/ggerganov/ggml/tree/master/examples/replit">https://github.com/ggerganov/ggml/tree/master/examples/replit</a></p>

          <p>it''s pretty fast on my M1 16GB MB Air</p>

          '
        raw: 'ggml just merged CPU inference implementation:

          https://github.com/ggerganov/ggml/tree/master/examples/replit


          it''s pretty fast on my M1 16GB MB Air'
        updatedAt: '2023-05-17T23:29:48.647Z'
      numEdits: 0
      reactions: []
    id: 6465636c6ceebdc7fd90babc
    type: comment
  author: vgrichina
  content: 'ggml just merged CPU inference implementation:

    https://github.com/ggerganov/ggml/tree/master/examples/replit


    it''s pretty fast on my M1 16GB MB Air'
  created_at: 2023-05-17 22:29:48+00:00
  edited: false
  hidden: false
  id: 6465636c6ceebdc7fd90babc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: replit/replit-code-v1-3b
repo_type: model
status: open
target_branch: null
title: Expected minimum hardware requirements for inference?
