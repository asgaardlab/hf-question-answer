!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nudelbrot
conflicting_files: null
created_at: 2023-06-15 10:43:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/57908591b00256c1a185ad8a165b8d70.svg
      fullname: Chris K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nudelbrot
      type: user
    createdAt: '2023-06-15T11:43:59.000Z'
    data:
      edited: false
      editors:
      - nudelbrot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.13465629518032074
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/57908591b00256c1a185ad8a165b8d70.svg
          fullname: Chris K
          isHf: false
          isPro: false
          name: nudelbrot
          type: user
        html: "<p> in :12                                                        \
          \                           \u2502<br>\u2502                           \
          \                                                                      \
          \ \u2502<br>\u2502    9                                                \
          \                                             \u2502<br>\u2502   10 #config.max_seq_len\
          \ = 4096 # (input + output) tokens can now be up to 4096               \
          \   \u2502<br>\u2502   11                                              \
          \                                               \u2502<br>\u2502 \u2771\
          \ 12 model = transformers.AutoModelForCausalLM.from_pretrained(        \
          \                          \u2502<br>\u2502   13   name,               \
          \                                                                      \u2502\
          <br>\u2502   14   device_map=\"auto\", load_in_4bit=True, low_cpu_mem_usage=True,\
          \                             \u2502<br>\u2502   15   #config=config,  \
          \                                                                      \
          \   \u2502<br>\u2502                                                   \
          \                                               \u2502<br>\u2502 /work/jupyter/transformers/src/transformers/models/auto/auto_factory.py:479\
          \ in from_pretrained   \u2502<br>\u2502                                \
          \                                                                  \u2502\
          <br>\u2502   476 \u2502   \u2502   \u2502   \u2502   class_ref, pretrained_model_name_or_path,\
          \ **hub_kwargs, **kwargs           \u2502<br>\u2502   477 \u2502   \u2502\
          \   \u2502   )                                                         \
          \                     \u2502<br>\u2502   478 \u2502   \u2502   \u2502  \
          \ _ = hub_kwargs.pop(\"code_revision\", None)                          \
          \            \u2502<br>\u2502 \u2771 479 \u2502   \u2502   \u2502   return\
          \ model_class.from_pretrained(                                         \
          \   \u2502<br>\u2502   480 \u2502   \u2502   \u2502   \u2502   pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs,   \u2502<br>\u2502   481 \u2502\
          \   \u2502   \u2502   )                                                \
          \                              \u2502<br>\u2502   482 \u2502   \u2502  \
          \ elif type(config) in cls._model_mapping.keys():                      \
          \              \u2502<br>\u2502                                        \
          \                                                          \u2502<br>\u2502\
          \ /work/jupyter/transformers/src/transformers/modeling_utils.py:2600 in\
          \ from_pretrained            \u2502<br>\u2502                          \
          \                                                                      \
          \  \u2502<br>\u2502   2597 \u2502   \u2502   if from_pt:               \
          \                                                        \u2502<br>\u2502\
          \   2598 \u2502   \u2502   \u2502   if not is_sharded and state_dict is\
          \ None:                                     \u2502<br>\u2502   2599 \u2502\
          \   \u2502   \u2502   \u2502   # Time to load the checkpoint           \
          \                                  \u2502<br>\u2502 \u2771 2600 \u2502 \
          \  \u2502   \u2502   \u2502   state_dict = load_state_dict(resolved_archive_file)\
          \                       \u2502<br>\u2502   2601 \u2502   \u2502   \u2502\
          \                                                                      \
          \           \u2502<br>\u2502   2602 \u2502   \u2502   \u2502   # set dtype\
          \ to instantiate the model under:                                   \u2502\
          <br>\u2502   2603 \u2502   \u2502   \u2502   # 1. If torch_dtype is not\
          \ None, we use that dtype                            \u2502<br>\u2502  \
          \                                                                      \
          \                          \u2502<br>\u2502 /work/jupyter/transformers/src/transformers/modeling_utils.py:442\
          \ in load_state_dict             \u2502<br>\u2502                      \
          \                                                                      \
          \      \u2502<br>\u2502    439 \u2502   \u2502   # Check format of the archive\
          \                                                     \u2502<br>\u2502 \
          \   440 \u2502   \u2502   with safe_open(checkpoint_file, framework=\"pt\"\
          ) as f:                             \u2502<br>\u2502    441 \u2502   \u2502\
          \   \u2502   metadata = f.metadata()                                   \
          \                    \u2502<br>\u2502 \u2771  442 \u2502   \u2502   if metadata.get(\"\
          format\") not in [\"pt\", \"tf\", \"flax\"]:                           \
          \ \u2502<br>\u2502    443 \u2502   \u2502   \u2502   raise OSError(    \
          \                                                            \u2502<br>\u2502\
          \    444 \u2502   \u2502   \u2502   \u2502   f\"The safetensors archive\
          \ passed at {checkpoint_file} does not contain t  \u2502<br>\u2502    445\
          \ \u2502   \u2502   \u2502   \u2502   \"you save your model with the <code>save_pretrained</code>\
          \ method.\"                  \u2502<br>\u2570\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F<br>AttributeError:\
          \ 'NoneType' object has no attribute 'get'</p>\n"
        raw: " in <module>:12                                                    \
          \                               \u2502\r\n\u2502                       \
          \                                                                      \
          \     \u2502\r\n\u2502    9                                            \
          \                                                 \u2502\r\n\u2502   10\
          \ #config.max_seq_len = 4096 # (input + output) tokens can now be up to\
          \ 4096                  \u2502\r\n\u2502   11                          \
          \                                                                   \u2502\
          \r\n\u2502 \u2771 12 model = transformers.AutoModelForCausalLM.from_pretrained(\
          \                                  \u2502\r\n\u2502   13   name,       \
          \                                                                      \
          \        \u2502\r\n\u2502   14   device_map=\"auto\", load_in_4bit=True,\
          \ low_cpu_mem_usage=True,                             \u2502\r\n\u2502 \
          \  15   #config=config,                                                \
          \                           \u2502\r\n\u2502                           \
          \                                                                      \
          \ \u2502\r\n\u2502 /work/jupyter/transformers/src/transformers/models/auto/auto_factory.py:479\
          \ in from_pretrained   \u2502\r\n\u2502                                \
          \                                                                  \u2502\
          \r\n\u2502   476 \u2502   \u2502   \u2502   \u2502   class_ref, pretrained_model_name_or_path,\
          \ **hub_kwargs, **kwargs           \u2502\r\n\u2502   477 \u2502   \u2502\
          \   \u2502   )                                                         \
          \                     \u2502\r\n\u2502   478 \u2502   \u2502   \u2502  \
          \ _ = hub_kwargs.pop(\"code_revision\", None)                          \
          \            \u2502\r\n\u2502 \u2771 479 \u2502   \u2502   \u2502   return\
          \ model_class.from_pretrained(                                         \
          \   \u2502\r\n\u2502   480 \u2502   \u2502   \u2502   \u2502   pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs,   \u2502\r\n\u2502   481 \u2502\
          \   \u2502   \u2502   )                                                \
          \                              \u2502\r\n\u2502   482 \u2502   \u2502  \
          \ elif type(config) in cls._model_mapping.keys():                      \
          \              \u2502\r\n\u2502                                        \
          \                                                          \u2502\r\n\u2502\
          \ /work/jupyter/transformers/src/transformers/modeling_utils.py:2600 in\
          \ from_pretrained            \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502   2597 \u2502   \u2502   if from_pt:               \
          \                                                        \u2502\r\n\u2502\
          \   2598 \u2502   \u2502   \u2502   if not is_sharded and state_dict is\
          \ None:                                     \u2502\r\n\u2502   2599 \u2502\
          \   \u2502   \u2502   \u2502   # Time to load the checkpoint           \
          \                                  \u2502\r\n\u2502 \u2771 2600 \u2502 \
          \  \u2502   \u2502   \u2502   state_dict = load_state_dict(resolved_archive_file)\
          \                       \u2502\r\n\u2502   2601 \u2502   \u2502   \u2502\
          \                                                                      \
          \           \u2502\r\n\u2502   2602 \u2502   \u2502   \u2502   # set dtype\
          \ to instantiate the model under:                                   \u2502\
          \r\n\u2502   2603 \u2502   \u2502   \u2502   # 1. If torch_dtype is not\
          \ None, we use that dtype                            \u2502\r\n\u2502  \
          \                                                                      \
          \                          \u2502\r\n\u2502 /work/jupyter/transformers/src/transformers/modeling_utils.py:442\
          \ in load_state_dict             \u2502\r\n\u2502                      \
          \                                                                      \
          \      \u2502\r\n\u2502    439 \u2502   \u2502   # Check format of the archive\
          \                                                     \u2502\r\n\u2502 \
          \   440 \u2502   \u2502   with safe_open(checkpoint_file, framework=\"pt\"\
          ) as f:                             \u2502\r\n\u2502    441 \u2502   \u2502\
          \   \u2502   metadata = f.metadata()                                   \
          \                    \u2502\r\n\u2502 \u2771  442 \u2502   \u2502   if metadata.get(\"\
          format\") not in [\"pt\", \"tf\", \"flax\"]:                           \
          \ \u2502\r\n\u2502    443 \u2502   \u2502   \u2502   raise OSError(    \
          \                                                            \u2502\r\n\u2502\
          \    444 \u2502   \u2502   \u2502   \u2502   f\"The safetensors archive\
          \ passed at {checkpoint_file} does not contain t  \u2502\r\n\u2502    445\
          \ \u2502   \u2502   \u2502   \u2502   \"you save your model with the `save_pretrained`\
          \ method.\"                  \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\r\nAttributeError:\
          \ 'NoneType' object has no attribute 'get'"
        updatedAt: '2023-06-15T11:43:59.288Z'
      numEdits: 0
      reactions: []
    id: 648af97fe0fefb4e692213e8
    type: comment
  author: nudelbrot
  content: " in <module>:12                                                      \
    \                             \u2502\r\n\u2502                               \
    \                                                                   \u2502\r\n\
    \u2502    9                                                                  \
    \                           \u2502\r\n\u2502   10 #config.max_seq_len = 4096 #\
    \ (input + output) tokens can now be up to 4096                  \u2502\r\n\u2502\
    \   11                                                                       \
    \                      \u2502\r\n\u2502 \u2771 12 model = transformers.AutoModelForCausalLM.from_pretrained(\
    \                                  \u2502\r\n\u2502   13   name,             \
    \                                                                        \u2502\
    \r\n\u2502   14   device_map=\"auto\", load_in_4bit=True, low_cpu_mem_usage=True,\
    \                             \u2502\r\n\u2502   15   #config=config,        \
    \                                                                   \u2502\r\n\
    \u2502                                                                       \
    \                           \u2502\r\n\u2502 /work/jupyter/transformers/src/transformers/models/auto/auto_factory.py:479\
    \ in from_pretrained   \u2502\r\n\u2502                                      \
    \                                                            \u2502\r\n\u2502\
    \   476 \u2502   \u2502   \u2502   \u2502   class_ref, pretrained_model_name_or_path,\
    \ **hub_kwargs, **kwargs           \u2502\r\n\u2502   477 \u2502   \u2502   \u2502\
    \   )                                                                        \
    \      \u2502\r\n\u2502   478 \u2502   \u2502   \u2502   _ = hub_kwargs.pop(\"\
    code_revision\", None)                                      \u2502\r\n\u2502 \u2771\
    \ 479 \u2502   \u2502   \u2502   return model_class.from_pretrained(         \
    \                                   \u2502\r\n\u2502   480 \u2502   \u2502   \u2502\
    \   \u2502   pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,\
    \   \u2502\r\n\u2502   481 \u2502   \u2502   \u2502   )                      \
    \                                                        \u2502\r\n\u2502   482\
    \ \u2502   \u2502   elif type(config) in cls._model_mapping.keys():          \
    \                          \u2502\r\n\u2502                                  \
    \                                                                \u2502\r\n\u2502\
    \ /work/jupyter/transformers/src/transformers/modeling_utils.py:2600 in from_pretrained\
    \            \u2502\r\n\u2502                                                \
    \                                                  \u2502\r\n\u2502   2597 \u2502\
    \   \u2502   if from_pt:                                                     \
    \                  \u2502\r\n\u2502   2598 \u2502   \u2502   \u2502   if not is_sharded\
    \ and state_dict is None:                                     \u2502\r\n\u2502\
    \   2599 \u2502   \u2502   \u2502   \u2502   # Time to load the checkpoint   \
    \                                          \u2502\r\n\u2502 \u2771 2600 \u2502\
    \   \u2502   \u2502   \u2502   state_dict = load_state_dict(resolved_archive_file)\
    \                       \u2502\r\n\u2502   2601 \u2502   \u2502   \u2502     \
    \                                                                            \u2502\
    \r\n\u2502   2602 \u2502   \u2502   \u2502   # set dtype to instantiate the model\
    \ under:                                   \u2502\r\n\u2502   2603 \u2502   \u2502\
    \   \u2502   # 1. If torch_dtype is not None, we use that dtype              \
    \              \u2502\r\n\u2502                                              \
    \                                                    \u2502\r\n\u2502 /work/jupyter/transformers/src/transformers/modeling_utils.py:442\
    \ in load_state_dict             \u2502\r\n\u2502                            \
    \                                                                      \u2502\r\
    \n\u2502    439 \u2502   \u2502   # Check format of the archive              \
    \                                       \u2502\r\n\u2502    440 \u2502   \u2502\
    \   with safe_open(checkpoint_file, framework=\"pt\") as f:                  \
    \           \u2502\r\n\u2502    441 \u2502   \u2502   \u2502   metadata = f.metadata()\
    \                                                       \u2502\r\n\u2502 \u2771\
    \  442 \u2502   \u2502   if metadata.get(\"format\") not in [\"pt\", \"tf\", \"\
    flax\"]:                            \u2502\r\n\u2502    443 \u2502   \u2502  \
    \ \u2502   raise OSError(                                                    \
    \            \u2502\r\n\u2502    444 \u2502   \u2502   \u2502   \u2502   f\"The\
    \ safetensors archive passed at {checkpoint_file} does not contain t  \u2502\r\
    \n\u2502    445 \u2502   \u2502   \u2502   \u2502   \"you save your model with\
    \ the `save_pretrained` method.\"                  \u2502\r\n\u2570\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u256F\r\nAttributeError: 'NoneType' object has\
    \ no attribute 'get'"
  created_at: 2023-06-15 10:43:59+00:00
  edited: false
  hidden: false
  id: 648af97fe0fefb4e692213e8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: 4bit/mpt-7b-storywriter-4bit-128g
repo_type: model
status: open
target_branch: null
title: howto load in 4bit?
