!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xldistance
conflicting_files: null
created_at: 2024-01-08 00:53:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e0cdc05d0f7ed0015c5c425a1d392d9f.svg
      fullname: xldistance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xldistance
      type: user
    createdAt: '2024-01-08T00:53:02.000Z'
    data:
      edited: true
      editors:
      - xldistance
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9498112201690674
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e0cdc05d0f7ed0015c5c425a1d392d9f.svg
          fullname: xldistance
          isHf: false
          isPro: false
          name: xldistance
          type: user
        html: '<p>Thanks for quantifying the model</p>

          '
        raw: Thanks for quantifying the model
        updatedAt: '2024-01-08T03:29:04.234Z'
      numEdits: 1
      reactions: []
    id: 659b476e28676374f308e53e
    type: comment
  author: xldistance
  content: Thanks for quantifying the model
  created_at: 2024-01-08 00:53:02+00:00
  edited: true
  hidden: false
  id: 659b476e28676374f308e53e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/42f6623fc921d64e4627ecbf886f9d56.svg
      fullname: Matt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mo137
      type: user
    createdAt: '2024-01-08T20:27:51.000Z'
    data:
      edited: false
      editors:
      - mo137
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9797768592834473
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/42f6623fc921d64e4627ecbf886f9d56.svg
          fullname: Matt
          isHf: false
          isPro: false
          name: mo137
          type: user
        html: '<p>I think 2.8 bpw might fit in 24 GB VRAM, but I''m not able to load
          3.0 bpw.</p>

          '
        raw: I think 2.8 bpw might fit in 24 GB VRAM, but I'm not able to load 3.0
          bpw.
        updatedAt: '2024-01-08T20:27:51.419Z'
      numEdits: 0
      reactions: []
    id: 659c5ac710ebb6da46ca9c46
    type: comment
  author: mo137
  content: I think 2.8 bpw might fit in 24 GB VRAM, but I'm not able to load 3.0 bpw.
  created_at: 2024-01-08 20:27:51+00:00
  edited: false
  hidden: false
  id: 659c5ac710ebb6da46ca9c46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e0cdc05d0f7ed0015c5c425a1d392d9f.svg
      fullname: xldistance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xldistance
      type: user
    createdAt: '2024-01-08T23:54:07.000Z'
    data:
      edited: false
      editors:
      - xldistance
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9297798871994019
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e0cdc05d0f7ed0015c5c425a1d392d9f.svg
          fullname: xldistance
          isHf: false
          isPro: false
          name: xldistance
          type: user
        html: '<blockquote>

          <p>I think 2.8 bpw might fit in 24 GB VRAM, but I''m not able to load 3.0
          bpw.</p>

          </blockquote>

          <p>You can modify config.json''s max_position_embeddings to 10000 and then
          you can use it under 3.0bpw, but the reply speed is only about 3 tokens/s,
          very slow!</p>

          '
        raw: '> I think 2.8 bpw might fit in 24 GB VRAM, but I''m not able to load
          3.0 bpw.


          You can modify config.json''s max_position_embeddings to 10000 and then
          you can use it under 3.0bpw, but the reply speed is only about 3 tokens/s,
          very slow!'
        updatedAt: '2024-01-08T23:54:07.139Z'
      numEdits: 0
      reactions: []
    id: 659c8b1f10ebb6da46d60fbc
    type: comment
  author: xldistance
  content: '> I think 2.8 bpw might fit in 24 GB VRAM, but I''m not able to load 3.0
    bpw.


    You can modify config.json''s max_position_embeddings to 10000 and then you can
    use it under 3.0bpw, but the reply speed is only about 3 tokens/s, very slow!'
  created_at: 2024-01-08 23:54:07+00:00
  edited: false
  hidden: false
  id: 659c8b1f10ebb6da46d60fbc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e0cdc05d0f7ed0015c5c425a1d392d9f.svg
      fullname: xldistance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xldistance
      type: user
    createdAt: '2024-01-09T00:03:08.000Z'
    data:
      edited: false
      editors:
      - xldistance
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8297930955886841
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e0cdc05d0f7ed0015c5c425a1d392d9f.svg
          fullname: xldistance
          isHf: false
          isPro: false
          name: xldistance
          type: user
        html: '<p>2.65bpw quantization set max_position_embeddings to 10000, occupy
          more than 24GB of video memory, 4090 graphics card with very bad</p>

          '
        raw: 2.65bpw quantization set max_position_embeddings to 10000, occupy more
          than 24GB of video memory, 4090 graphics card with very bad
        updatedAt: '2024-01-09T00:03:08.239Z'
      numEdits: 0
      reactions: []
    id: 659c8d3c93d383899ce3d085
    type: comment
  author: xldistance
  content: 2.65bpw quantization set max_position_embeddings to 10000, occupy more
    than 24GB of video memory, 4090 graphics card with very bad
  created_at: 2024-01-09 00:03:08+00:00
  edited: false
  hidden: false
  id: 659c8d3c93d383899ce3d085
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2024-01-09T12:17:16.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8985647559165955
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>I generally just take the original models'' configurations. You
          can edit the file locally if you need it different than the base.</p>

          '
        raw: I generally just take the original models' configurations. You can edit
          the file locally if you need it different than the base.
        updatedAt: '2024-01-09T12:17:16.645Z'
      numEdits: 0
      reactions: []
    id: 659d394cdda11310d8a22e78
    type: comment
  author: LoneStriker
  content: I generally just take the original models' configurations. You can edit
    the file locally if you need it different than the base.
  created_at: 2024-01-09 12:17:16+00:00
  edited: false
  hidden: false
  id: 659d394cdda11310d8a22e78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e0cdc05d0f7ed0015c5c425a1d392d9f.svg
      fullname: xldistance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xldistance
      type: user
    createdAt: '2024-01-16T09:06:45.000Z'
    data:
      edited: false
      editors:
      - xldistance
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8069373965263367
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e0cdc05d0f7ed0015c5c425a1d392d9f.svg
          fullname: xldistance
          isHf: false
          isPro: false
          name: xldistance
          type: user
        html: '<p>extremely grateful</p>

          '
        raw: extremely grateful
        updatedAt: '2024-01-16T09:06:45.780Z'
      numEdits: 0
      reactions: []
    id: 65a64725c6e67a3029da1f04
    type: comment
  author: xldistance
  content: extremely grateful
  created_at: 2024-01-16 09:06:45+00:00
  edited: false
  hidden: false
  id: 65a64725c6e67a3029da1f04
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/Mixtral_34Bx2_MoE_60B-3.0bpw-h6-exl2
repo_type: model
status: open
target_branch: null
title: Can you make a 2.4bpw quantization?
