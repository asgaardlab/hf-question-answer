!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cmp-nct
conflicting_files: null
created_at: 2023-12-04 01:41:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-12-04T01:41:29.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9442729949951172
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>I compared the quality for german translations with models up to
          70B in size, basically every german fine tune available, all the Falcon
          variants etc.<br>The upside:<br>The quality of the translation is flawless.
          Every single model I tested makes grammar mistakes and uses strange words
          sometimes.<br>Only GPT4 appears to be good, but even GPT4 makes grammar
          mistakes sometimes.</p>

          <p>Now the problems that appear to make this un-useable for now:</p>

          <ol>

          <li><p>The inference speed is roughly 1/10 of what it should be compared
          to other models.<br>When looking at a 13B model on llama.cpp/ggml on a 4090
          (in any quantization that fits) we see a speed of 60-80 tokens/sec generation.<br>However,
          this model is not supported - the best I could do was bitsandbytes on cuda
          at 4 bit inference - it''s at 8 tokens/second generation and appears to
          be as slow in prompt ingestion (which would run at 2000/sec in ggml)</p>

          </li>

          <li><p>Context !<br>When feeding a paragraph to this model it just responds
          with gibberish repeated phrases from the paragraph.<br>When splitting the
          paragraph into sentences, prepending each with &lt;2de&gt; the output is
          of high quality but it does nott see the previous sentences anymore so the
          translation can become totally wrong.<br>For example: "A display case is
          green. The case is filled"<br>The context is clear, it''s a display case!
          But the second sentence suddenls transforms into a "court case"</p>

          </li>

          </ol>

          <p>This would need to be integrated into llama.cpp, then the speed likely
          could approach 100 tokens/sec on the 10B model, also comes with a superior
          quantization.<br>For context I don''t know, maybe there is a solution to
          add previous generations into it''s total context so it understands what''s
          going on.</p>

          '
        raw: "I compared the quality for german translations with models up to 70B\
          \ in size, basically every german fine tune available, all the Falcon variants\
          \ etc.\r\nThe upside:\r\nThe quality of the translation is flawless. Every\
          \ single model I tested makes grammar mistakes and uses strange words sometimes.\r\
          \nOnly GPT4 appears to be good, but even GPT4 makes grammar mistakes sometimes.\r\
          \n\r\nNow the problems that appear to make this un-useable for now:\r\n\
          1) The inference speed is roughly 1/10 of what it should be compared to\
          \ other models.\r\nWhen looking at a 13B model on llama.cpp/ggml on a 4090\
          \ (in any quantization that fits) we see a speed of 60-80 tokens/sec generation.\r\
          \nHowever, this model is not supported - the best I could do was bitsandbytes\
          \ on cuda at 4 bit inference - it's at 8 tokens/second generation and appears\
          \ to be as slow in prompt ingestion (which would run at 2000/sec in ggml)\r\
          \n\r\n2) Context !\r\nWhen feeding a paragraph to this model it just responds\
          \ with gibberish repeated phrases from the paragraph.\r\nWhen splitting\
          \ the paragraph into sentences, prepending each with <2de> the output is\
          \ of high quality but it does nott see the previous sentences anymore so\
          \ the translation can become totally wrong.\r\nFor example: \"A display\
          \ case is green. The case is filled\" \r\nThe context is clear, it's a display\
          \ case! But the second sentence suddenls transforms into a \"court case\"\
          \r\n\r\n\r\nThis would need to be integrated into llama.cpp, then the speed\
          \ likely could approach 100 tokens/sec on the 10B model, also comes with\
          \ a superior quantization.\r\nFor context I don't know, maybe there is a\
          \ solution to add previous generations into it's total context so it understands\
          \ what's going on."
        updatedAt: '2023-12-04T01:41:29.511Z'
      numEdits: 0
      reactions: []
    id: 656d2e499ced9d5ff58384cb
    type: comment
  author: cmp-nct
  content: "I compared the quality for german translations with models up to 70B in\
    \ size, basically every german fine tune available, all the Falcon variants etc.\r\
    \nThe upside:\r\nThe quality of the translation is flawless. Every single model\
    \ I tested makes grammar mistakes and uses strange words sometimes.\r\nOnly GPT4\
    \ appears to be good, but even GPT4 makes grammar mistakes sometimes.\r\n\r\n\
    Now the problems that appear to make this un-useable for now:\r\n1) The inference\
    \ speed is roughly 1/10 of what it should be compared to other models.\r\nWhen\
    \ looking at a 13B model on llama.cpp/ggml on a 4090 (in any quantization that\
    \ fits) we see a speed of 60-80 tokens/sec generation.\r\nHowever, this model\
    \ is not supported - the best I could do was bitsandbytes on cuda at 4 bit inference\
    \ - it's at 8 tokens/second generation and appears to be as slow in prompt ingestion\
    \ (which would run at 2000/sec in ggml)\r\n\r\n2) Context !\r\nWhen feeding a\
    \ paragraph to this model it just responds with gibberish repeated phrases from\
    \ the paragraph.\r\nWhen splitting the paragraph into sentences, prepending each\
    \ with <2de> the output is of high quality but it does nott see the previous sentences\
    \ anymore so the translation can become totally wrong.\r\nFor example: \"A display\
    \ case is green. The case is filled\" \r\nThe context is clear, it's a display\
    \ case! But the second sentence suddenls transforms into a \"court case\"\r\n\r\
    \n\r\nThis would need to be integrated into llama.cpp, then the speed likely could\
    \ approach 100 tokens/sec on the 10B model, also comes with a superior quantization.\r\
    \nFor context I don't know, maybe there is a solution to add previous generations\
    \ into it's total context so it understands what's going on."
  created_at: 2023-12-04 01:41:29+00:00
  edited: false
  hidden: false
  id: 656d2e499ced9d5ff58384cb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: google/madlad400-10b-mt
repo_type: model
status: open
target_branch: null
title: German translation - an upside and 2 huge problems in actually using this
