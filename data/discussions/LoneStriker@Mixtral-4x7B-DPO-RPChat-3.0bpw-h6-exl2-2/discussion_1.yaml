!!python/object:huggingface_hub.community.DiscussionWithDetails
author: CulturedMan
conflicting_files: null
created_at: 2023-12-27 04:16:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/vBkymjdBoi737ZgJFX8rm.png?w=200&h=200&f=face
      fullname: Andy Williams
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CulturedMan
      type: user
    createdAt: '2023-12-27T04:16:26.000Z'
    data:
      edited: true
      editors:
      - CulturedMan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9811164140701294
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/vBkymjdBoi737ZgJFX8rm.png?w=200&h=200&f=face
          fullname: Andy Williams
          isHf: false
          isPro: false
          name: CulturedMan
          type: user
        html: '<p>Thanks for the 3.0bpw! I am currently stuck with 12gb vram, and
          I''m able to run this with 6144 context / alpha 1.75 with minimal spillover
          to the CPU.</p>

          '
        raw: Thanks for the 3.0bpw! I am currently stuck with 12gb vram, and I'm able
          to run this with 6144 context / alpha 1.75 with minimal spillover to the
          CPU.
        updatedAt: '2023-12-28T07:16:43.887Z'
      numEdits: 1
      reactions: []
    id: 658ba51a33b0bf491db1733a
    type: comment
  author: CulturedMan
  content: Thanks for the 3.0bpw! I am currently stuck with 12gb vram, and I'm able
    to run this with 6144 context / alpha 1.75 with minimal spillover to the CPU.
  created_at: 2023-12-27 04:16:26+00:00
  edited: true
  hidden: false
  id: 658ba51a33b0bf491db1733a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-28T07:30:34.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9673371315002441
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Thanks for the feedback. Picking quant sizes can be tricky with
          all of the different model sizes (from "interesting" merges) as well as
          GPU VRAM sizes.</p>

          '
        raw: Thanks for the feedback. Picking quant sizes can be tricky with all of
          the different model sizes (from "interesting" merges) as well as GPU VRAM
          sizes.
        updatedAt: '2023-12-28T07:30:34.651Z'
      numEdits: 0
      reactions: []
    id: 658d241a38672a848be35eb8
    type: comment
  author: LoneStriker
  content: Thanks for the feedback. Picking quant sizes can be tricky with all of
    the different model sizes (from "interesting" merges) as well as GPU VRAM sizes.
  created_at: 2023-12-28 07:30:34+00:00
  edited: false
  hidden: false
  id: 658d241a38672a848be35eb8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/Mixtral-4x7B-DPO-RPChat-3.0bpw-h6-exl2-2
repo_type: model
status: open
target_branch: null
title: Thank you!
