!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2023-12-22 20:05:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-12-22T20:05:09.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8560040593147278
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>Bro you cant post a model that a fine tuned version of one of the
          best coding models in existance and not post human eval scores. </p>

          <p>PLEASE. Im begging you, post some human eval, and human eval+ scores
          for Gods sakes</p>

          '
        raw: "Bro you cant post a model that a fine tuned version of one of the best\
          \ coding models in existance and not post human eval scores. \r\n\r\nPLEASE.\
          \ Im begging you, post some human eval, and human eval+ scores for Gods\
          \ sakes"
        updatedAt: '2023-12-22T20:05:09.489Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F91D"
        users:
        - beowolx
        - IoTeacher
      - count: 2
        reaction: "\U0001F44D"
        users:
        - yehiaserag
        - IoTeacher
    id: 6585ebf5551f455e2e617440
    type: comment
  author: rombodawg
  content: "Bro you cant post a model that a fine tuned version of one of the best\
    \ coding models in existance and not post human eval scores. \r\n\r\nPLEASE. Im\
    \ begging you, post some human eval, and human eval+ scores for Gods sakes"
  created_at: 2023-12-22 20:05:09+00:00
  edited: false
  hidden: false
  id: 6585ebf5551f455e2e617440
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b566ab04fa6584c03b5247/RLo7-b5kmh_omlSPM9Jwe.png?w=200&h=200&f=face
      fullname: beowulf
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: beowolx
      type: user
    createdAt: '2023-12-22T21:02:44.000Z'
    data:
      edited: false
      editors:
      - beowolx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.531955361366272
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b566ab04fa6584c03b5247/RLo7-b5kmh_omlSPM9Jwe.png?w=200&h=200&f=face
          fullname: beowulf
          isHf: false
          isPro: false
          name: beowolx
          type: user
        html: "<p>lol yeah this is what i'm trying to do right now but it's taking\
          \ too long to generate the sample file \U0001F605 i'm going probably run\
          \ out of runpod credits before it's finished</p>\n<p>If you or someone else\
          \ want to try, please feel free! </p>\n<p>Here is the code that I've been\
          \ using to generate the sample:</p>\n<pre><code class=\"language-python\"\
          >!pip install human-<span class=\"hljs-built_in\">eval</span>\n!pip install\
          \ evalplus --upgrade\n!pip install transformers\n!pip install accelerate\n\
          !pip install sentencepiece\n!pip install protobuf\n\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ AutoModelForCausalLM\n<span class=\"hljs-keyword\">from</span> evalplus.data\
          \ <span class=\"hljs-keyword\">import</span> get_human_eval_plus, write_jsonl\n\
          <span class=\"hljs-keyword\">import</span> torch\n\n<span class=\"hljs-comment\"\
          ># initialize the model</span>\nmodel_path = <span class=\"hljs-string\"\
          >\"beowolx/CodeNinja-1.0-OpenChat-7B\"</span>\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
          \ device_map=<span class=\"hljs-string\">\"auto\"</span>)\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"openchat/openchat-3.5-1210\"</span>, use_fast=<span\
          \ class=\"hljs-literal\">True</span>)\n\n<span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">generate_one_completion</span>(<span\
          \ class=\"hljs-params\">prompt: <span class=\"hljs-built_in\">str</span></span>):\n\
          \    messages = [\n        {<span class=\"hljs-string\">\"role\"</span>:\
          \ <span class=\"hljs-string\">\"user\"</span>, <span class=\"hljs-string\"\
          >\"content\"</span>: prompt},\n        {<span class=\"hljs-string\">\"role\"\
          </span>: <span class=\"hljs-string\">\"assistant\"</span>, <span class=\"\
          hljs-string\">\"content\"</span>: <span class=\"hljs-string\">\"\"</span>}\
          \  <span class=\"hljs-comment\"># Placeholder for the model's response</span>\n\
          \    ]\n\n    <span class=\"hljs-comment\"># Apply the chat template to\
          \ get the list of token IDs</span>\n    tokenizer.pad_token = tokenizer.eos_token\n\
          \    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=<span\
          \ class=\"hljs-literal\">True</span>, truncation=<span class=\"hljs-literal\"\
          >True</span>, max_length=<span class=\"hljs-number\">4096</span>)\n\n  \
          \  <span class=\"hljs-comment\"># Generate completion</span>\n    generate_ids\
          \ = model.generate(\n        torch.tensor([input_ids]).to(<span class=\"\
          hljs-string\">\"cuda\"</span>),  <span class=\"hljs-comment\"># Convert\
          \ list to tensor and send to GPU</span>\n        max_new_tokens=<span class=\"\
          hljs-number\">384</span>,\n        do_sample=<span class=\"hljs-literal\"\
          >True</span>,\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id\n\
          \    )\n\n    <span class=\"hljs-comment\"># Decode and clean up the completion</span>\n\
          \    completion = tokenizer.decode(generate_ids[<span class=\"hljs-number\"\
          >0</span>], skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n\
          \    completion = completion.split(<span class=\"hljs-string\">\"\\n\\n\\\
          n\"</span>)[<span class=\"hljs-number\">0</span>].strip()\n\n    <span class=\"\
          hljs-keyword\">return</span> completion\n\n\nsamples = [\n    <span class=\"\
          hljs-built_in\">dict</span>(task_id=task_id, solution=generate_one_completion(problem[<span\
          \ class=\"hljs-string\">\"prompt\"</span>]))\n    <span class=\"hljs-keyword\"\
          >for</span> task_id, problem <span class=\"hljs-keyword\">in</span> get_human_eval_plus().items()\n\
          ]\nwrite_jsonl(<span class=\"hljs-string\">\"samples.jsonl\"</span>, samples)\n\
          </code></pre>\n"
        raw: "lol yeah this is what i'm trying to do right now but it's taking too\
          \ long to generate the sample file \U0001F605 i'm going probably run out\
          \ of runpod credits before it's finished\n\nIf you or someone else want\
          \ to try, please feel free! \n\nHere is the code that I've been using to\
          \ generate the sample:\n\n```python\n!pip install human-eval\n!pip install\
          \ evalplus --upgrade\n!pip install transformers\n!pip install accelerate\n\
          !pip install sentencepiece\n!pip install protobuf\n\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\nfrom evalplus.data import get_human_eval_plus,\
          \ write_jsonl\nimport torch\n\n# initialize the model\nmodel_path = \"beowolx/CodeNinja-1.0-OpenChat-7B\"\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"\
          auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"openchat/openchat-3.5-1210\"\
          , use_fast=True)\n\ndef generate_one_completion(prompt: str):\n    messages\
          \ = [\n        {\"role\": \"user\", \"content\": prompt},\n        {\"role\"\
          : \"assistant\", \"content\": \"\"}  # Placeholder for the model's response\n\
          \    ]\n\n    # Apply the chat template to get the list of token IDs\n \
          \   tokenizer.pad_token = tokenizer.eos_token\n    input_ids = tokenizer.apply_chat_template(messages,\
          \ add_generation_prompt=True, truncation=True, max_length=4096)\n\n    #\
          \ Generate completion\n    generate_ids = model.generate(\n        torch.tensor([input_ids]).to(\"\
          cuda\"),  # Convert list to tensor and send to GPU\n        max_new_tokens=384,\n\
          \        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id,\n\
          \        eos_token_id=tokenizer.eos_token_id\n    )\n\n    # Decode and\
          \ clean up the completion\n    completion = tokenizer.decode(generate_ids[0],\
          \ skip_special_tokens=True)\n    completion = completion.split(\"\\n\\n\\\
          n\")[0].strip()\n\n    return completion\n\n\nsamples = [\n    dict(task_id=task_id,\
          \ solution=generate_one_completion(problem[\"prompt\"]))\n    for task_id,\
          \ problem in get_human_eval_plus().items()\n]\nwrite_jsonl(\"samples.jsonl\"\
          , samples)\n```"
        updatedAt: '2023-12-22T21:02:44.515Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - eramax
        - IoTeacher
    id: 6585f97446ed30388a8a7bf2
    type: comment
  author: beowolx
  content: "lol yeah this is what i'm trying to do right now but it's taking too long\
    \ to generate the sample file \U0001F605 i'm going probably run out of runpod\
    \ credits before it's finished\n\nIf you or someone else want to try, please feel\
    \ free! \n\nHere is the code that I've been using to generate the sample:\n\n\
    ```python\n!pip install human-eval\n!pip install evalplus --upgrade\n!pip install\
    \ transformers\n!pip install accelerate\n!pip install sentencepiece\n!pip install\
    \ protobuf\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom\
    \ evalplus.data import get_human_eval_plus, write_jsonl\nimport torch\n\n# initialize\
    \ the model\nmodel_path = \"beowolx/CodeNinja-1.0-OpenChat-7B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
    \ device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"openchat/openchat-3.5-1210\"\
    , use_fast=True)\n\ndef generate_one_completion(prompt: str):\n    messages =\
    \ [\n        {\"role\": \"user\", \"content\": prompt},\n        {\"role\": \"\
    assistant\", \"content\": \"\"}  # Placeholder for the model's response\n    ]\n\
    \n    # Apply the chat template to get the list of token IDs\n    tokenizer.pad_token\
    \ = tokenizer.eos_token\n    input_ids = tokenizer.apply_chat_template(messages,\
    \ add_generation_prompt=True, truncation=True, max_length=4096)\n\n    # Generate\
    \ completion\n    generate_ids = model.generate(\n        torch.tensor([input_ids]).to(\"\
    cuda\"),  # Convert list to tensor and send to GPU\n        max_new_tokens=384,\n\
    \        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id,\n     \
    \   eos_token_id=tokenizer.eos_token_id\n    )\n\n    # Decode and clean up the\
    \ completion\n    completion = tokenizer.decode(generate_ids[0], skip_special_tokens=True)\n\
    \    completion = completion.split(\"\\n\\n\\n\")[0].strip()\n\n    return completion\n\
    \n\nsamples = [\n    dict(task_id=task_id, solution=generate_one_completion(problem[\"\
    prompt\"]))\n    for task_id, problem in get_human_eval_plus().items()\n]\nwrite_jsonl(\"\
    samples.jsonl\", samples)\n```"
  created_at: 2023-12-22 21:02:44+00:00
  edited: false
  hidden: false
  id: 6585f97446ed30388a8a7bf2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b566ab04fa6584c03b5247/RLo7-b5kmh_omlSPM9Jwe.png?w=200&h=200&f=face
      fullname: beowulf
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: beowolx
      type: user
    createdAt: '2023-12-30T12:58:05.000Z'
    data:
      status: closed
    id: 659013ddf5a209eeac09aa93
    type: status-change
  author: beowolx
  created_at: 2023-12-30 12:58:05+00:00
  id: 659013ddf5a209eeac09aa93
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: beowolx/CodeNinja-1.0-OpenChat-7B
repo_type: model
status: closed
target_branch: null
title: HUMAN EVAL SCORE!!!!
