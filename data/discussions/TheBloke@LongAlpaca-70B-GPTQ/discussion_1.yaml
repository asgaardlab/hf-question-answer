!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kerni
conflicting_files: null
created_at: 2023-10-16 08:21:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661203415773-noauth.png?w=200&h=200&f=face
      fullname: Michael Kernbach
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kerni
      type: user
    createdAt: '2023-10-16T09:21:09.000Z'
    data:
      edited: false
      editors:
      - Kerni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5027032494544983
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661203415773-noauth.png?w=200&h=200&f=face
          fullname: Michael Kernbach
          isHf: false
          isPro: false
          name: Kerni
          type: user
        html: '<p>Hello there,<br> I downloaded both the 4 bit 32 and 128g weights,
          and on my machine the model spurts out only gibberish.</p>

          <p>I used Text gen webUI as backend with Exllama V1 and Exllama V2 for testing
          with multiple parameters.<br>--model TheBlokeLongAlpaca-70B-GPTQ --loader
          exllamav2 --api --verbose --gpu-split 21,21<br>--model TheBlokeLongAlpaca-70B-GPTQ
          --loader exllamav2 --api --verbose --gpu-split 21,21 --alpha_value 2 --max_seq_len
          8192<br>(Sillty tavern for the front end)</p>

          <p>Other models work perfectly.<br>(Xwin 70b for example)</p>

          <p>Can anyone confirm this or am I just an idiot &gt;_&lt; ?</p>

          '
        raw: "Hello there,\r\n I downloaded both the 4 bit 32 and 128g weights, and\
          \ on my machine the model spurts out only gibberish.\r\n\r\nI used Text\
          \ gen webUI as backend with Exllama V1 and Exllama V2 for testing with multiple\
          \ parameters.\r\n--model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2\
          \ --api --verbose --gpu-split 21,21\r\n--model TheBlokeLongAlpaca-70B-GPTQ\
          \ --loader exllamav2 --api --verbose --gpu-split 21,21 --alpha_value 2 --max_seq_len\
          \ 8192\r\n(Sillty tavern for the front end)\r\n\r\nOther models work perfectly.\r\
          \n(Xwin 70b for example)\r\n\r\nCan anyone confirm this or am I just an\
          \ idiot >_< ?"
        updatedAt: '2023-10-16T09:21:09.041Z'
      numEdits: 0
      reactions: []
    id: 652d0085ec10d7e4811cc405
    type: comment
  author: Kerni
  content: "Hello there,\r\n I downloaded both the 4 bit 32 and 128g weights, and\
    \ on my machine the model spurts out only gibberish.\r\n\r\nI used Text gen webUI\
    \ as backend with Exllama V1 and Exllama V2 for testing with multiple parameters.\r\
    \n--model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api --verbose --gpu-split\
    \ 21,21\r\n--model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api --verbose\
    \ --gpu-split 21,21 --alpha_value 2 --max_seq_len 8192\r\n(Sillty tavern for the\
    \ front end)\r\n\r\nOther models work perfectly.\r\n(Xwin 70b for example)\r\n\
    \r\nCan anyone confirm this or am I just an idiot >_< ?"
  created_at: 2023-10-16 08:21:09+00:00
  edited: false
  hidden: false
  id: 652d0085ec10d7e4811cc405
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-16T09:22:11.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9391384124755859
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Can you show me an example of the gibberish - is it one word repeated
          over and over?</p>

          '
        raw: Can you show me an example of the gibberish - is it one word repeated
          over and over?
        updatedAt: '2023-10-16T09:22:11.973Z'
      numEdits: 0
      reactions: []
    id: 652d00c3680fa36b3b310dce
    type: comment
  author: TheBloke
  content: Can you show me an example of the gibberish - is it one word repeated over
    and over?
  created_at: 2023-10-16 08:22:11+00:00
  edited: false
  hidden: false
  id: 652d00c3680fa36b3b310dce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-10-16T09:56:57.000Z'
    data:
      edited: true
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6233016848564148
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>--max_seq_len 8192 should be 32768 as it is the default for LongAlpca,
          and no alpha_value</p>

          <p>exllama needs to maully scale max_seq_len based on alpha_value. e.g.
          if alpha_value=2,  max_seq_lenneeds to be 32768*2</p>

          '
        raw: '--max_seq_len 8192 should be 32768 as it is the default for LongAlpca,
          and no alpha_value


          exllama needs to maully scale max_seq_len based on alpha_value. e.g. if
          alpha_value=2,  max_seq_lenneeds to be 32768*2'
        updatedAt: '2023-10-16T09:57:18.579Z'
      numEdits: 1
      reactions: []
    id: 652d08e975ff3107a2f4f700
    type: comment
  author: Yhyu13
  content: '--max_seq_len 8192 should be 32768 as it is the default for LongAlpca,
    and no alpha_value


    exllama needs to maully scale max_seq_len based on alpha_value. e.g. if alpha_value=2,  max_seq_lenneeds
    to be 32768*2'
  created_at: 2023-10-16 08:56:57+00:00
  edited: true
  hidden: false
  id: 652d08e975ff3107a2f4f700
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661203415773-noauth.png?w=200&h=200&f=face
      fullname: Michael Kernbach
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kerni
      type: user
    createdAt: '2023-10-16T10:37:03.000Z'
    data:
      edited: false
      editors:
      - Kerni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6416487097740173
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661203415773-noauth.png?w=200&h=200&f=face
          fullname: Michael Kernbach
          isHf: false
          isPro: false
          name: Kerni
          type: user
        html: '<p>Hello Again, sorry for the late reply (I did some testing around
          after Yhyu13 posted his comment)<br>(I used the 128g 4 bit weights for testing
          this time)</p>

          <blockquote>

          <p>Can you show me an example of the gibberish - is it one word repeated
          over and over?</p>

          </blockquote>

          <p>Yes , it is kind of like that. (this time i used : --model TheBlokeLongAlpaca-70B-GPTQ
          --loader exllamav2 --api --verbose --gpu-split 21,21 --max_seq_len 16384)<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/ZR8brO5X1O1tccJOG75aG.png"><img
          alt="grafik.png" src="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/ZR8brO5X1O1tccJOG75aG.png"></a></p>

          <blockquote>

          <p>--max_seq_len 8192 should be 32768 as it is the default for LongAlpca,
          and no alpha_value</p>

          <p>exllama needs to maully scale max_seq_len based on alpha_value. e.g.
          if alpha_value=2,  max_seq_lenneeds to be 32768*2</p>

          </blockquote>

          <p>I think you are right but i can not test it , my 2x 3090 do not want
          to load the weights with :<br>--model TheBlokeLongAlpaca-70B-GPTQ --loader
          exllamav2 --api --verbose --gpu-split 24,24 --max_seq_len 32768<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/-TvXSwL3c1M3jffBJmY81.png"><img
          alt="grafik.png" src="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/-TvXSwL3c1M3jffBJmY81.png"></a></p>

          <p>I guess that one is on me &gt;_&lt;</p>

          '
        raw: "Hello Again, sorry for the late reply (I did some testing around after\
          \ Yhyu13 posted his comment)\n(I used the 128g 4 bit weights for testing\
          \ this time)\n\n> Can you show me an example of the gibberish - is it one\
          \ word repeated over and over?\n\nYes , it is kind of like that. (this time\
          \ i used : --model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api\
          \ --verbose --gpu-split 21,21 --max_seq_len 16384)\n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/ZR8brO5X1O1tccJOG75aG.png)\n\
          \n\n> --max_seq_len 8192 should be 32768 as it is the default for LongAlpca,\
          \ and no alpha_value\n> \n> exllama needs to maully scale max_seq_len based\
          \ on alpha_value. e.g. if alpha_value=2,  max_seq_lenneeds to be 32768*2\n\
          \nI think you are right but i can not test it , my 2x 3090 do not want to\
          \ load the weights with : \n--model TheBlokeLongAlpaca-70B-GPTQ --loader\
          \ exllamav2 --api --verbose --gpu-split 24,24 --max_seq_len 32768\n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/-TvXSwL3c1M3jffBJmY81.png)\n\
          \nI guess that one is on me >_<"
        updatedAt: '2023-10-16T10:37:03.813Z'
      numEdits: 0
      reactions: []
    id: 652d124f15ada1d927a95af9
    type: comment
  author: Kerni
  content: "Hello Again, sorry for the late reply (I did some testing around after\
    \ Yhyu13 posted his comment)\n(I used the 128g 4 bit weights for testing this\
    \ time)\n\n> Can you show me an example of the gibberish - is it one word repeated\
    \ over and over?\n\nYes , it is kind of like that. (this time i used : --model\
    \ TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api --verbose --gpu-split 21,21\
    \ --max_seq_len 16384)\n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/ZR8brO5X1O1tccJOG75aG.png)\n\
    \n\n> --max_seq_len 8192 should be 32768 as it is the default for LongAlpca, and\
    \ no alpha_value\n> \n> exllama needs to maully scale max_seq_len based on alpha_value.\
    \ e.g. if alpha_value=2,  max_seq_lenneeds to be 32768*2\n\nI think you are right\
    \ but i can not test it , my 2x 3090 do not want to load the weights with : \n\
    --model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api --verbose --gpu-split\
    \ 24,24 --max_seq_len 32768\n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/-TvXSwL3c1M3jffBJmY81.png)\n\
    \nI guess that one is on me >_<"
  created_at: 2023-10-16 09:37:03+00:00
  edited: false
  hidden: false
  id: 652d124f15ada1d927a95af9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-16T10:39:43.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.776646077632904
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes, that''s a sequence length issue as we thought</p>

          <p>Can you try with <code>--max_seq_len 8192</code> - and no alpha parameter
          specified</p>

          '
        raw: 'Yes, that''s a sequence length issue as we thought


          Can you try with `--max_seq_len 8192` - and no alpha parameter specified


          '
        updatedAt: '2023-10-16T10:39:43.678Z'
      numEdits: 0
      reactions: []
    id: 652d12efb45e702eb0541259
    type: comment
  author: TheBloke
  content: 'Yes, that''s a sequence length issue as we thought


    Can you try with `--max_seq_len 8192` - and no alpha parameter specified


    '
  created_at: 2023-10-16 09:39:43+00:00
  edited: false
  hidden: false
  id: 652d12efb45e702eb0541259
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661203415773-noauth.png?w=200&h=200&f=face
      fullname: Michael Kernbach
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kerni
      type: user
    createdAt: '2023-10-16T10:48:35.000Z'
    data:
      edited: false
      editors:
      - Kerni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33140790462493896
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661203415773-noauth.png?w=200&h=200&f=face
          fullname: Michael Kernbach
          isHf: false
          isPro: false
          name: Kerni
          type: user
        html: '<p>Okay , i was able to do inference with --max_seq_len 32768<br>I
          used : --model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api --verbose
          --gpu-split 19,23 --max_seq_len 32768</p>

          <p>But.. ahm....<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/rncmvOBLNvEuCbCcITb9L.png"><img
          alt="grafik.png" src="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/rncmvOBLNvEuCbCcITb9L.png"></a></p>

          <blockquote>

          <p>Yes, that''s a sequence length issue as we thought</p>

          <p>Can you try with <code>--max_seq_len 8192</code> - and no alpha parameter
          specified</p>

          </blockquote>

          <p>Of course,<br>here is the result with using : --model TheBlokeLongAlpaca-70B-GPTQ
          --loader exllamav2 --api --verbose --gpu-split 19,23 --max_seq_len 8192</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/0fRxqZzZFBpAcNLIVDSzz.png"><img
          alt="grafik.png" src="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/0fRxqZzZFBpAcNLIVDSzz.png"></a></p>

          '
        raw: "Okay , i was able to do inference with --max_seq_len 32768\nI used :\
          \ --model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api --verbose\
          \ --gpu-split 19,23 --max_seq_len 32768\n\nBut.. ahm....\n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/rncmvOBLNvEuCbCcITb9L.png)\n\
          \n\n> Yes, that's a sequence length issue as we thought\n> \n> Can you try\
          \ with `--max_seq_len 8192` - and no alpha parameter specified\n\nOf course,\
          \ \nhere is the result with using : --model TheBlokeLongAlpaca-70B-GPTQ\
          \ --loader exllamav2 --api --verbose --gpu-split 19,23 --max_seq_len 8192\n\
          \n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/0fRxqZzZFBpAcNLIVDSzz.png)\n"
        updatedAt: '2023-10-16T10:48:35.225Z'
      numEdits: 0
      reactions: []
    id: 652d150393d96e2801627e9b
    type: comment
  author: Kerni
  content: "Okay , i was able to do inference with --max_seq_len 32768\nI used : --model\
    \ TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api --verbose --gpu-split 19,23\
    \ --max_seq_len 32768\n\nBut.. ahm....\n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/rncmvOBLNvEuCbCcITb9L.png)\n\
    \n\n> Yes, that's a sequence length issue as we thought\n> \n> Can you try with\
    \ `--max_seq_len 8192` - and no alpha parameter specified\n\nOf course, \nhere\
    \ is the result with using : --model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2\
    \ --api --verbose --gpu-split 19,23 --max_seq_len 8192\n\n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/0fRxqZzZFBpAcNLIVDSzz.png)\n"
  created_at: 2023-10-16 09:48:35+00:00
  edited: false
  hidden: false
  id: 652d150393d96e2801627e9b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-16T10:53:56.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9630035161972046
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Not sure then, sorry - maybe it only works at 32768.  I''ve not
          played around with sequence length in a UI like text-generation-webui in
          a while.  I thought it was meant to also work at lower sequence lengths</p>

          <p>What about if you use <code>--compress_pos_emb 2 --max_seq_len 8192 </code>
          - you''ll need to check that''s the correct name for compress_pos_emb, but
          it''s something like that</p>

          '
        raw: 'Not sure then, sorry - maybe it only works at 32768.  I''ve not played
          around with sequence length in a UI like text-generation-webui in a while.  I
          thought it was meant to also work at lower sequence lengths


          What about if you use `--compress_pos_emb 2 --max_seq_len 8192 ` - you''ll
          need to check that''s the correct name for compress_pos_emb, but it''s something
          like that'
        updatedAt: '2023-10-16T10:53:56.405Z'
      numEdits: 0
      reactions: []
    id: 652d16446ccf31d8c1a749b1
    type: comment
  author: TheBloke
  content: 'Not sure then, sorry - maybe it only works at 32768.  I''ve not played
    around with sequence length in a UI like text-generation-webui in a while.  I
    thought it was meant to also work at lower sequence lengths


    What about if you use `--compress_pos_emb 2 --max_seq_len 8192 ` - you''ll need
    to check that''s the correct name for compress_pos_emb, but it''s something like
    that'
  created_at: 2023-10-16 09:53:56+00:00
  edited: false
  hidden: false
  id: 652d16446ccf31d8c1a749b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661203415773-noauth.png?w=200&h=200&f=face
      fullname: Michael Kernbach
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kerni
      type: user
    createdAt: '2023-10-16T11:22:19.000Z'
    data:
      edited: false
      editors:
      - Kerni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.531230628490448
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661203415773-noauth.png?w=200&h=200&f=face
          fullname: Michael Kernbach
          isHf: false
          isPro: false
          name: Kerni
          type: user
        html: '<blockquote>

          <p>Not sure then, sorry - maybe it only works at 32768.  I''ve not played
          around with sequence length in a UI like text-generation-webui in a while.  I
          thought it was meant to also work at lower sequence lengths</p>

          <p>What about if you use <code>--compress_pos_emb 2 --max_seq_len 8192 </code>
          - you''ll need to check that''s the correct name for compress_pos_emb, but
          it''s something like that</p>

          </blockquote>

          <p>This was an excellent idea actually, I tested it now with :  --model
          TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api --verbose --gpu-split
          21,21 --compress_pos_emb 2 --max_seq_len 8192<br>And.. it is.. ahm.. kind
          of okay ?</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/YgU7c6iMSJiZY_51c00in.png"><img
          alt="grafik.png" src="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/YgU7c6iMSJiZY_51c00in.png"></a></p>

          <p>Then i tested it with : --model TheBlokeLongAlpaca-70B-GPTQ --loader
          exllamav2 --api --verbose --gpu-split 21,21 --compress_pos_emb 2 --max_seq_len
          16384</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/sRXNr2Qp45p_D5zR0F7qj.png"><img
          alt="grafik.png" src="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/sRXNr2Qp45p_D5zR0F7qj.png"></a></p>

          <p>And with : --model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api
          --verbose --gpu-split 21,21 --compress_pos_emb 4 --max_seq_len 16384</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/aAWmEmAiR1aeXYnO4bwB7.png"><img
          alt="grafik.png" src="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/aAWmEmAiR1aeXYnO4bwB7.png"></a><br>Ahm..
          okay.. din''t know that our tower was half a kilometer long.. O.o</p>

          <p>And with : --model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api
          --verbose --gpu-split 19,23 --compress_pos_emb 8 --max_seq_len 32768</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/NeO3cX2R9QKRWYtO1SwYo.png"><img
          alt="grafik.png" src="https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/NeO3cX2R9QKRWYtO1SwYo.png"></a></p>

          <p>I guess this is the way to go then , i initially thought that this model
          does not need compress_pos_emb or alpha value to function.<br>So.. i guess
          we can close this one now ?</p>

          '
        raw: "> Not sure then, sorry - maybe it only works at 32768.  I've not played\
          \ around with sequence length in a UI like text-generation-webui in a while.\
          \  I thought it was meant to also work at lower sequence lengths\n> \n>\
          \ What about if you use `--compress_pos_emb 2 --max_seq_len 8192 ` - you'll\
          \ need to check that's the correct name for compress_pos_emb, but it's something\
          \ like that\n\nThis was an excellent idea actually, I tested it now with\
          \ :  --model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api --verbose\
          \ --gpu-split 21,21 --compress_pos_emb 2 --max_seq_len 8192\nAnd.. it is..\
          \ ahm.. kind of okay ?\n\n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/YgU7c6iMSJiZY_51c00in.png)\n\
          \nThen i tested it with : --model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2\
          \ --api --verbose --gpu-split 21,21 --compress_pos_emb 2 --max_seq_len 16384\n\
          \n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/sRXNr2Qp45p_D5zR0F7qj.png)\n\
          \nAnd with : --model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api\
          \ --verbose --gpu-split 21,21 --compress_pos_emb 4 --max_seq_len 16384\n\
          \n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/aAWmEmAiR1aeXYnO4bwB7.png)\n\
          Ahm.. okay.. din't know that our tower was half a kilometer long.. O.o\n\
          \nAnd with : --model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api\
          \ --verbose --gpu-split 19,23 --compress_pos_emb 8 --max_seq_len 32768\n\
          \n\n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/NeO3cX2R9QKRWYtO1SwYo.png)\n\
          \n\nI guess this is the way to go then , i initially thought that this model\
          \ does not need compress_pos_emb or alpha value to function.\nSo.. i guess\
          \ we can close this one now ?"
        updatedAt: '2023-10-16T11:22:19.020Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - Yhyu13
    id: 652d1ceb30cb7e8885476652
    type: comment
  author: Kerni
  content: "> Not sure then, sorry - maybe it only works at 32768.  I've not played\
    \ around with sequence length in a UI like text-generation-webui in a while. \
    \ I thought it was meant to also work at lower sequence lengths\n> \n> What about\
    \ if you use `--compress_pos_emb 2 --max_seq_len 8192 ` - you'll need to check\
    \ that's the correct name for compress_pos_emb, but it's something like that\n\
    \nThis was an excellent idea actually, I tested it now with :  --model TheBlokeLongAlpaca-70B-GPTQ\
    \ --loader exllamav2 --api --verbose --gpu-split 21,21 --compress_pos_emb 2 --max_seq_len\
    \ 8192\nAnd.. it is.. ahm.. kind of okay ?\n\n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/YgU7c6iMSJiZY_51c00in.png)\n\
    \nThen i tested it with : --model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2\
    \ --api --verbose --gpu-split 21,21 --compress_pos_emb 2 --max_seq_len 16384\n\
    \n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/sRXNr2Qp45p_D5zR0F7qj.png)\n\
    \nAnd with : --model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api --verbose\
    \ --gpu-split 21,21 --compress_pos_emb 4 --max_seq_len 16384\n\n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/aAWmEmAiR1aeXYnO4bwB7.png)\n\
    Ahm.. okay.. din't know that our tower was half a kilometer long.. O.o\n\nAnd\
    \ with : --model TheBlokeLongAlpaca-70B-GPTQ --loader exllamav2 --api --verbose\
    \ --gpu-split 19,23 --compress_pos_emb 8 --max_seq_len 32768\n\n\n![grafik.png](https://cdn-uploads.huggingface.co/production/uploads/6303f3db1dd5d3c6248562db/NeO3cX2R9QKRWYtO1SwYo.png)\n\
    \n\nI guess this is the way to go then , i initially thought that this model does\
    \ not need compress_pos_emb or alpha value to function.\nSo.. i guess we can close\
    \ this one now ?"
  created_at: 2023-10-16 10:22:19+00:00
  edited: false
  hidden: false
  id: 652d1ceb30cb7e8885476652
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8b1419f999f31ce3fdcb8ad994b5351.svg
      fullname: MB
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: MB7977
      type: user
    createdAt: '2023-10-17T02:01:02.000Z'
    data:
      edited: false
      editors:
      - MB7977
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8554659485816956
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8b1419f999f31ce3fdcb8ad994b5351.svg
          fullname: MB
          isHf: false
          isPro: true
          name: MB7977
          type: user
        html: '<p>I tested this and got good, coherent output at max_sequence_length
          32768 and compress_pos_emb of 8 using exllama_hf (not exllamav2). Other
          sequence lengths produced less coherent but still kind of usable output.
          Seems important to set it to 32K. </p>

          '
        raw: 'I tested this and got good, coherent output at max_sequence_length 32768
          and compress_pos_emb of 8 using exllama_hf (not exllamav2). Other sequence
          lengths produced less coherent but still kind of usable output. Seems important
          to set it to 32K. '
        updatedAt: '2023-10-17T02:01:02.000Z'
      numEdits: 0
      reactions: []
    id: 652deade8549ffcd45b8d308
    type: comment
  author: MB7977
  content: 'I tested this and got good, coherent output at max_sequence_length 32768
    and compress_pos_emb of 8 using exllama_hf (not exllamav2). Other sequence lengths
    produced less coherent but still kind of usable output. Seems important to set
    it to 32K. '
  created_at: 2023-10-17 01:01:02+00:00
  edited: false
  hidden: false
  id: 652deade8549ffcd45b8d308
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-10-17T10:13:41.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.897609531879425
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<blockquote>

          <p>I tested this and got good, coherent output at max_sequence_length 32768
          and compress_pos_emb of 8 using exllama_hf (not exllamav2). Other sequence
          lengths produced less coherent but still kind of usable output. Seems important
          to set it to 32K.</p>

          </blockquote>

          <p>Just note, that exllama_hf uses huggingface implementation of transformer
          which is much slower than exllama with flash attention on cuda devices</p>

          <p>exllama_hf at max hit 40% usage rate for single card inference for a
          7B model on RTX3090, where as exllama with flash attention would easily
          achieve &gt;95% usage rate</p>

          '
        raw: '> I tested this and got good, coherent output at max_sequence_length
          32768 and compress_pos_emb of 8 using exllama_hf (not exllamav2). Other
          sequence lengths produced less coherent but still kind of usable output.
          Seems important to set it to 32K.


          Just note, that exllama_hf uses huggingface implementation of transformer
          which is much slower than exllama with flash attention on cuda devices


          exllama_hf at max hit 40% usage rate for single card inference for a 7B
          model on RTX3090, where as exllama with flash attention would easily achieve
          >95% usage rate'
        updatedAt: '2023-10-17T10:13:41.502Z'
      numEdits: 0
      reactions: []
    id: 652e5e55abc673c42034c84c
    type: comment
  author: Yhyu13
  content: '> I tested this and got good, coherent output at max_sequence_length 32768
    and compress_pos_emb of 8 using exllama_hf (not exllamav2). Other sequence lengths
    produced less coherent but still kind of usable output. Seems important to set
    it to 32K.


    Just note, that exllama_hf uses huggingface implementation of transformer which
    is much slower than exllama with flash attention on cuda devices


    exllama_hf at max hit 40% usage rate for single card inference for a 7B model
    on RTX3090, where as exllama with flash attention would easily achieve >95% usage
    rate'
  created_at: 2023-10-17 09:13:41+00:00
  edited: false
  hidden: false
  id: 652e5e55abc673c42034c84c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8b1419f999f31ce3fdcb8ad994b5351.svg
      fullname: MB
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: MB7977
      type: user
    createdAt: '2023-10-17T12:05:38.000Z'
    data:
      edited: false
      editors:
      - MB7977
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9845961928367615
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8b1419f999f31ce3fdcb8ad994b5351.svg
          fullname: MB
          isHf: false
          isPro: true
          name: MB7977
          type: user
        html: "<p>Thank you. I\u2019ll try the model out on my build on Ubuntu, that\
          \ has FA2 installed. First run was Windows.</p>\n"
        raw: "Thank you. I\u2019ll try the model out on my build on Ubuntu, that has\
          \ FA2 installed. First run was Windows."
        updatedAt: '2023-10-17T12:05:38.520Z'
      numEdits: 0
      reactions: []
    id: 652e7892d1796ce8215a393e
    type: comment
  author: MB7977
  content: "Thank you. I\u2019ll try the model out on my build on Ubuntu, that has\
    \ FA2 installed. First run was Windows."
  created_at: 2023-10-17 11:05:38+00:00
  edited: false
  hidden: false
  id: 652e7892d1796ce8215a393e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/LongAlpaca-70B-GPTQ
repo_type: model
status: open
target_branch: null
title: Weights broken ?
