!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sanchit-gandhi
conflicting_files: null
created_at: 2023-11-02 16:43:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-11-02T17:43:35.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9598094820976257
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;rsonavane&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rsonavane\"\
          >@<span class=\"underline\">rsonavane</span></a></span>\n\n\t</span></span>!\
          \ We've been working extensively on <a rel=\"nofollow\" href=\"https://github.com/huggingface/distil-whisper\"\
          >distilling Whisper</a> and just came across your model. Super cool to see\
          \ that you tried a similar technique to us (KL + CE loss for decoder distillation)\
          \ and got some good results! We'd like to include this model in the acknowledgements\
          \ as a nod that you too had experimented with this early on. Also keen to\
          \ hear whether you experimented any further with this, i.e. by scaling it\
          \ up to larger datasets or higher model compression. Would love to share\
          \ notes!</p>\n"
        raw: Hey @rsonavane! We've been working extensively on [distilling Whisper](https://github.com/huggingface/distil-whisper)
          and just came across your model. Super cool to see that you tried a similar
          technique to us (KL + CE loss for decoder distillation) and got some good
          results! We'd like to include this model in the acknowledgements as a nod
          that you too had experimented with this early on. Also keen to hear whether
          you experimented any further with this, i.e. by scaling it up to larger
          datasets or higher model compression. Would love to share notes!
        updatedAt: '2023-11-02T17:43:35.327Z'
      numEdits: 0
      reactions: []
    id: 6543dfc7dc39aa9faba21819
    type: comment
  author: sanchit-gandhi
  content: Hey @rsonavane! We've been working extensively on [distilling Whisper](https://github.com/huggingface/distil-whisper)
    and just came across your model. Super cool to see that you tried a similar technique
    to us (KL + CE loss for decoder distillation) and got some good results! We'd
    like to include this model in the acknowledgements as a nod that you too had experimented
    with this early on. Also keen to hear whether you experimented any further with
    this, i.e. by scaling it up to larger datasets or higher model compression. Would
    love to share notes!
  created_at: 2023-11-02 16:43:35+00:00
  edited: false
  hidden: false
  id: 6543dfc7dc39aa9faba21819
  type: comment
- !!python/object:huggingface_hub.community.DiscussionEvent
  _event:
    author:
      avatarUrl: /avatars/1788d7a35814f68bc3985b07dfe06598.svg
      fullname: Raghav Sonavane
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: rsonavane
      type: user
    createdAt: '2023-11-02T19:00:29.000Z'
    data:
      pinned: true
    id: 6543f1cd893aec5da94f7770
    type: pinning-change
  author: rsonavane
  created_at: 2023-11-02 18:00:29+00:00
  id: 6543f1cd893aec5da94f7770
  type: pinning-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1788d7a35814f68bc3985b07dfe06598.svg
      fullname: Raghav Sonavane
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: rsonavane
      type: user
    createdAt: '2023-11-02T19:35:09.000Z'
    data:
      edited: false
      editors:
      - rsonavane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9413689374923706
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1788d7a35814f68bc3985b07dfe06598.svg
          fullname: Raghav Sonavane
          isHf: false
          isPro: false
          name: rsonavane
          type: user
        html: "<p>For this experiment, I only considered common voice and librispeech\
          \ datasets. My goal was to create a model with near whisper-large-v2 accuracy\
          \ while achieving wav2vec2 onnx inference speed. Though started as a fun\
          \ experiment,  it concisely helped me understand the effect of decoder pruning.\
          \ I benchmarked latency, WER, and CER numbers on A100 for short-length transcription\
          \ audios while changing the number of decoder layers to prune. As I read\
          \ your paper and the experiments you've presented, your conclusions are\
          \ spot-on.<br>Thanks <span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>\
          \  for a note of acknowledgment. </p>\n"
        raw: "For this experiment, I only considered common voice and librispeech\
          \ datasets. My goal was to create a model with near whisper-large-v2 accuracy\
          \ while achieving wav2vec2 onnx inference speed. Though started as a fun\
          \ experiment,  it concisely helped me understand the effect of decoder pruning.\
          \ I benchmarked latency, WER, and CER numbers on A100 for short-length transcription\
          \ audios while changing the number of decoder layers to prune. As I read\
          \ your paper and the experiments you've presented, your conclusions are\
          \ spot-on. \nThanks @sanchit-gandhi  for a note of acknowledgment. \n"
        updatedAt: '2023-11-02T19:35:09.286Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - sanchit-gandhi
        - wakoinc
    id: 6543f9ed5f985a9bf29cc85b
    type: comment
  author: rsonavane
  content: "For this experiment, I only considered common voice and librispeech datasets.\
    \ My goal was to create a model with near whisper-large-v2 accuracy while achieving\
    \ wav2vec2 onnx inference speed. Though started as a fun experiment,  it concisely\
    \ helped me understand the effect of decoder pruning. I benchmarked latency, WER,\
    \ and CER numbers on A100 for short-length transcription audios while changing\
    \ the number of decoder layers to prune. As I read your paper and the experiments\
    \ you've presented, your conclusions are spot-on. \nThanks @sanchit-gandhi  for\
    \ a note of acknowledgment. \n"
  created_at: 2023-11-02 18:35:09+00:00
  edited: false
  hidden: false
  id: 6543f9ed5f985a9bf29cc85b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: rsonavane/distil-whisper-large-v2-8-ls
repo_type: model
status: open
target_branch: null
title: Distil-Whisper Acknowledgements
