!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wehapi
conflicting_files: null
created_at: 2023-12-26 05:16:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2521356af1ad03baa684cf4d5cf399d5.svg
      fullname: wehapi1315@aseall.com
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wehapi
      type: user
    createdAt: '2023-12-26T05:16:21.000Z'
    data:
      edited: true
      editors:
      - wehapi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.36601221561431885
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2521356af1ad03baa684cf4d5cf399d5.svg
          fullname: wehapi1315@aseall.com
          isHf: false
          isPro: false
          name: wehapi
          type: user
        html: "<p>Here's the code </p>\n<pre><code>from ctransformers import AutoModelForCausalLM\n\
          \n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if\
          \ no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"\
          OpenHermes-2.5-Mistral-7B-GGUF\", model_file=\"openhermes-2.5-mistral-7b.Q4_K_M.gguf\"\
          , model_type=\"llama\", gpu_layers=50)\n\nprint(llm(\"AI is going to\"))\n\
          </code></pre>\n<p>Here's the error</p>\n<pre><code>Model type 'mistral'\
          \ is not supported.\nTraceback (most recent call last):\n  File \"/home/ubuntu/projects/nlprocessing/huggingface/models/message.py\"\
          , line 13, in &lt;module&gt;\n    llm = AutoModelForCausalLM.from_pretrained(\"\
          OpenHermes-2.5-Mistral-7B-GGUF\", model_file=\"openhermes-2.5-mistral-7b.Q4_K_M.gguf\"\
          , model_type=\"mistral\", gpu_layers=50)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/ubuntu/projects/nlprocessing/huggingface/env/lib/python3.11/site-packages/ctransformers/hub.py\"\
          , line 175, in from_pretrained\n    llm = LLM(\n          ^^^^\n  File \"\
          /home/ubuntu/projects/nlprocessing/huggingface/env/lib/python3.11/site-packages/ctransformers/llm.py\"\
          , line 253, in __init__\n    raise RuntimeError(\nRuntimeError: Failed to\
          \ create LLM 'mistral' from '/home/ubuntu/projects/nlprocessing/huggingface/models/OpenHermes-2.5-Mistral-7B-GGUF/openhermes-2.5-mistral-7b.Q4_K_M.gguf'.\n\
          </code></pre>\n<p>Tried to set <code>gpu_layers</code> to <code>0</code>\
          \ but not working.</p>\n"
        raw: "Here's the code \n```\nfrom ctransformers import AutoModelForCausalLM\n\
          \n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if\
          \ no GPU acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"\
          OpenHermes-2.5-Mistral-7B-GGUF\", model_file=\"openhermes-2.5-mistral-7b.Q4_K_M.gguf\"\
          , model_type=\"llama\", gpu_layers=50)\n\nprint(llm(\"AI is going to\"))\n\
          ```\n\nHere's the error\n```\nModel type 'mistral' is not supported.\nTraceback\
          \ (most recent call last):\n  File \"/home/ubuntu/projects/nlprocessing/huggingface/models/message.py\"\
          , line 13, in <module>\n    llm = AutoModelForCausalLM.from_pretrained(\"\
          OpenHermes-2.5-Mistral-7B-GGUF\", model_file=\"openhermes-2.5-mistral-7b.Q4_K_M.gguf\"\
          , model_type=\"mistral\", gpu_layers=50)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/ubuntu/projects/nlprocessing/huggingface/env/lib/python3.11/site-packages/ctransformers/hub.py\"\
          , line 175, in from_pretrained\n    llm = LLM(\n          ^^^^\n  File \"\
          /home/ubuntu/projects/nlprocessing/huggingface/env/lib/python3.11/site-packages/ctransformers/llm.py\"\
          , line 253, in __init__\n    raise RuntimeError(\nRuntimeError: Failed to\
          \ create LLM 'mistral' from '/home/ubuntu/projects/nlprocessing/huggingface/models/OpenHermes-2.5-Mistral-7B-GGUF/openhermes-2.5-mistral-7b.Q4_K_M.gguf'.\n\
          ```\n\nTried to set `gpu_layers` to `0` but not working."
        updatedAt: '2023-12-26T05:21:38.340Z'
      numEdits: 1
      reactions: []
    id: 658a61a5509bcae23fbffcd6
    type: comment
  author: wehapi
  content: "Here's the code \n```\nfrom ctransformers import AutoModelForCausalLM\n\
    \n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU\
    \ acceleration is available on your system.\nllm = AutoModelForCausalLM.from_pretrained(\"\
    OpenHermes-2.5-Mistral-7B-GGUF\", model_file=\"openhermes-2.5-mistral-7b.Q4_K_M.gguf\"\
    , model_type=\"llama\", gpu_layers=50)\n\nprint(llm(\"AI is going to\"))\n```\n\
    \nHere's the error\n```\nModel type 'mistral' is not supported.\nTraceback (most\
    \ recent call last):\n  File \"/home/ubuntu/projects/nlprocessing/huggingface/models/message.py\"\
    , line 13, in <module>\n    llm = AutoModelForCausalLM.from_pretrained(\"OpenHermes-2.5-Mistral-7B-GGUF\"\
    , model_file=\"openhermes-2.5-mistral-7b.Q4_K_M.gguf\", model_type=\"mistral\"\
    , gpu_layers=50)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/ubuntu/projects/nlprocessing/huggingface/env/lib/python3.11/site-packages/ctransformers/hub.py\"\
    , line 175, in from_pretrained\n    llm = LLM(\n          ^^^^\n  File \"/home/ubuntu/projects/nlprocessing/huggingface/env/lib/python3.11/site-packages/ctransformers/llm.py\"\
    , line 253, in __init__\n    raise RuntimeError(\nRuntimeError: Failed to create\
    \ LLM 'mistral' from '/home/ubuntu/projects/nlprocessing/huggingface/models/OpenHermes-2.5-Mistral-7B-GGUF/openhermes-2.5-mistral-7b.Q4_K_M.gguf'.\n\
    ```\n\nTried to set `gpu_layers` to `0` but not working."
  created_at: 2023-12-26 05:16:21+00:00
  edited: true
  hidden: false
  id: 658a61a5509bcae23fbffcd6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677984031204-noauth.png?w=200&h=200&f=face
      fullname: Rhone Eiler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TKDKid1000
      type: user
    createdAt: '2023-12-26T06:10:18.000Z'
    data:
      edited: false
      editors:
      - TKDKid1000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9826668500900269
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677984031204-noauth.png?w=200&h=200&f=face
          fullname: Rhone Eiler
          isHf: false
          isPro: false
          name: TKDKid1000
          type: user
        html: '<p>I strongly advise against using ctransformers anymore. It hasn''t
          been updated since April, and Mistral came out in September. It doesn''t
          have support, so you won''t be able to use it. Look at llama-cpp-python
          for something still frequently maintained.</p>

          '
        raw: I strongly advise against using ctransformers anymore. It hasn't been
          updated since April, and Mistral came out in September. It doesn't have
          support, so you won't be able to use it. Look at llama-cpp-python for something
          still frequently maintained.
        updatedAt: '2023-12-26T06:10:18.622Z'
      numEdits: 0
      reactions: []
    id: 658a6e4a16a6a0082059cb99
    type: comment
  author: TKDKid1000
  content: I strongly advise against using ctransformers anymore. It hasn't been updated
    since April, and Mistral came out in September. It doesn't have support, so you
    won't be able to use it. Look at llama-cpp-python for something still frequently
    maintained.
  created_at: 2023-12-26 06:10:18+00:00
  edited: false
  hidden: false
  id: 658a6e4a16a6a0082059cb99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/2521356af1ad03baa684cf4d5cf399d5.svg
      fullname: wehapi1315@aseall.com
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wehapi
      type: user
    createdAt: '2023-12-28T08:53:39.000Z'
    data:
      status: closed
    id: 658d3793e7f846313c206ccf
    type: status-change
  author: wehapi
  created_at: 2023-12-28 08:53:39+00:00
  id: 658d3793e7f846313c206ccf
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/OpenHermes-2.5-Mistral-7B-GGUF
repo_type: model
status: closed
target_branch: null
title: Getting error, Model type 'mistral' is not supported.
