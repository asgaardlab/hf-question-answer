!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SamAct
conflicting_files: null
created_at: 2022-09-01 20:42:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b85eade484f8a8b312e0ae7f13d3f1a.svg
      fullname: SamAct
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SamAct
      type: user
    createdAt: '2022-09-01T21:42:28.000Z'
    data:
      edited: false
      editors:
      - SamAct
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8b85eade484f8a8b312e0ae7f13d3f1a.svg
          fullname: SamAct
          isHf: false
          isPro: false
          name: SamAct
          type: user
        html: '<p>Recently have struggled running these model locally, with some runs
          taking over an hour to quench out a set of paragraphs.</p>

          <ol>

          <li>Any suggestion on how to improve my run time? </li>

          <li>Do we have any outlook on how to setup onnx conversions?</li>

          <li>What else can be done?</li>

          </ol>

          '
        raw: "Recently have struggled running these model locally, with some runs\
          \ taking over an hour to quench out a set of paragraphs.\r\n1. Any suggestion\
          \ on how to improve my run time? \r\n2. Do we have any outlook on how to\
          \ setup onnx conversions?\r\n3. What else can be done?\r\n"
        updatedAt: '2022-09-01T21:42:28.395Z'
      numEdits: 0
      reactions: []
    id: 6311274464939fabc00d3b11
    type: comment
  author: SamAct
  content: "Recently have struggled running these model locally, with some runs taking\
    \ over an hour to quench out a set of paragraphs.\r\n1. Any suggestion on how\
    \ to improve my run time? \r\n2. Do we have any outlook on how to setup onnx conversions?\r\
    \n3. What else can be done?\r\n"
  created_at: 2022-09-01 20:42:28+00:00
  edited: false
  hidden: false
  id: 6311274464939fabc00d3b11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-09-02T17:29:49.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: "<p>Hey! Thanks for reaching out. I\u2019ll do my best to answer with\
          \ reasonable formatting (on mobile at the moment): </p>\n<ol>\n<li>What\
          \ does your setup look like? More specifically, are you running this on\
          \ a GPU? I realized that it\u2019s not on the model card, but inference\
          \ on CPU with summarization models (especially if you are trying to summarize\
          \ 16384 tokens at once) takes forever due to the length of the inputs <em>even\
          \ with methods to make this more efficient</em>.</li>\n</ol>\n<ul>\n<li>there\
          \ is an example on <code>pszemraj/led-large-book-summary</code> Now of how\
          \ to do this, I\u2019ll add to this card later \U0001F44D</li>\n<li>for\
          \ LED models, the way text is encoded <strong>and</strong> decoded really\
          \ matter. Check out <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/12INTTR6n64TzS4RrXZxMSXfrOd9Xzamo?usp=sharing\"\
          >this notebook</a> for a full example of what that means and how it works.</li>\n\
          </ul>\n<ol start=\"2\">\n<li>unsure on ONNX but I\u2019ll look into it over\
          \ the coming weeks </li>\n<li>I would first make sure things are set up\
          \ correctly on GPU side, try the notebook above. Then, I would try adjusting\
          \ the parameters. With the \u201Ctoken batching\u201D approach in the notebook\
          \ it will iterate through a file 4092, 8192 tokens at a time,</li>\n</ol>\n\
          <ul>\n<li>also, try preprocessing/simple text cleaning on inputs. I have\
          \ not found out why yet but \u201Ccompute workload\u201D can vary drastically\
          \ among the same amount of text depending on composition with no obvious\
          \ difference like one text was written by a baby or something. </li>\n<li>if\
          \ you are on GPU and still having issues even after adjusting params, it\u2019\
          s possible we could try the new 8bit inference. Let me know if you still\
          \ have issues</li>\n</ul>\n<p>Happy to answer other questions too, just\
          \ let me know \U0001F44D\U0001F44D</p>\n"
        raw: "Hey! Thanks for reaching out. I\u2019ll do my best to answer with reasonable\
          \ formatting (on mobile at the moment): \n\n1. What does your setup look\
          \ like? More specifically, are you running this on a GPU? I realized that\
          \ it\u2019s not on the model card, but inference on CPU with summarization\
          \ models (especially if you are trying to summarize 16384 tokens at once)\
          \ takes forever due to the length of the inputs _even with methods to make\
          \ this more efficient_.\n- there is an example on `pszemraj/led-large-book-summary`\
          \ Now of how to do this, I\u2019ll add to this card later \U0001F44D\n-\
          \ for LED models, the way text is encoded **and** decoded really matter.\
          \ Check out [this notebook](https://colab.research.google.com/drive/12INTTR6n64TzS4RrXZxMSXfrOd9Xzamo?usp=sharing)\
          \ for a full example of what that means and how it works. \n2) unsure on\
          \ ONNX but I\u2019ll look into it over the coming weeks \n3) I would first\
          \ make sure things are set up correctly on GPU side, try the notebook above.\
          \ Then, I would try adjusting the parameters. With the \u201Ctoken batching\u201D\
          \ approach in the notebook it will iterate through a file 4092, 8192 tokens\
          \ at a time, \n- also, try preprocessing/simple text cleaning on inputs.\
          \ I have not found out why yet but \u201Ccompute workload\u201D can vary\
          \ drastically among the same amount of text depending on composition with\
          \ no obvious difference like one text was written by a baby or something.\
          \ \n- if you are on GPU and still having issues even after adjusting params,\
          \ it\u2019s possible we could try the new 8bit inference. Let me know if\
          \ you still have issues \n\nHappy to answer other questions too, just let\
          \ me know \U0001F44D\U0001F44D"
        updatedAt: '2022-09-02T17:29:49.334Z'
      numEdits: 0
      reactions: []
    id: 63123d8d830f549852f3cb32
    type: comment
  author: pszemraj
  content: "Hey! Thanks for reaching out. I\u2019ll do my best to answer with reasonable\
    \ formatting (on mobile at the moment): \n\n1. What does your setup look like?\
    \ More specifically, are you running this on a GPU? I realized that it\u2019s\
    \ not on the model card, but inference on CPU with summarization models (especially\
    \ if you are trying to summarize 16384 tokens at once) takes forever due to the\
    \ length of the inputs _even with methods to make this more efficient_.\n- there\
    \ is an example on `pszemraj/led-large-book-summary` Now of how to do this, I\u2019\
    ll add to this card later \U0001F44D\n- for LED models, the way text is encoded\
    \ **and** decoded really matter. Check out [this notebook](https://colab.research.google.com/drive/12INTTR6n64TzS4RrXZxMSXfrOd9Xzamo?usp=sharing)\
    \ for a full example of what that means and how it works. \n2) unsure on ONNX\
    \ but I\u2019ll look into it over the coming weeks \n3) I would first make sure\
    \ things are set up correctly on GPU side, try the notebook above. Then, I would\
    \ try adjusting the parameters. With the \u201Ctoken batching\u201D approach in\
    \ the notebook it will iterate through a file 4092, 8192 tokens at a time, \n\
    - also, try preprocessing/simple text cleaning on inputs. I have not found out\
    \ why yet but \u201Ccompute workload\u201D can vary drastically among the same\
    \ amount of text depending on composition with no obvious difference like one\
    \ text was written by a baby or something. \n- if you are on GPU and still having\
    \ issues even after adjusting params, it\u2019s possible we could try the new\
    \ 8bit inference. Let me know if you still have issues \n\nHappy to answer other\
    \ questions too, just let me know \U0001F44D\U0001F44D"
  created_at: 2022-09-02 16:29:49+00:00
  edited: false
  hidden: false
  id: 63123d8d830f549852f3cb32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b85eade484f8a8b312e0ae7f13d3f1a.svg
      fullname: SamAct
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SamAct
      type: user
    createdAt: '2022-09-02T22:23:42.000Z'
    data:
      edited: true
      editors:
      - SamAct
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8b85eade484f8a8b312e0ae7f13d3f1a.svg
          fullname: SamAct
          isHf: false
          isPro: false
          name: SamAct
          type: user
        html: "<p>Interesting. Thank you for this brilliant piece of advise.  Based\
          \ on your inputs I consolidated the notebook as below. I am trying to use\
          \ this <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing#scrollTo=8RClMjZOZCPO\"\
          >notebook</a> to work on fine tuning the model.  Some of the values for\
          \  <code>extravagant</code> runs are also borrowed from your model cards.\
          \ I will try to play around and see what's work best.</p>\n<p>I am on a\
          \ mid tired computer (mid RTX and old cpu) configuration, but with the code\
          \ below ran within a 30 seconds to a minute most of the time. Of course,\
          \ I need to validate the outputs with that of the model card.<br>I am planning\
          \ to do a in-depth validation  with multiple parameters next.</p>\n<p>Note:\
          \ I have found that skipping <code>max_length</code> input especially when\
          \ the<code> input_length</code> is less then <code>1024</code> tokens, drastically\
          \ increases the run time.<br>In the mean time, Could you explain the point\
          \ 3 <code>token_batching</code>  in the list above? Thank you again for\
          \ you answer.</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">alt_led_on_cuda</span>(<span\
          \ class=\"hljs-params\">newchunk, model_path, max_length = <span class=\"\
          hljs-number\">512</span>, extravagant=<span class=\"hljs-literal\">False</span></span>):\n\
          \    _model = LEDForConditionalGeneration.from_pretrained(model_path, low_cpu_mem_usage=<span\
          \ class=\"hljs-literal\">True</span>,\n    torch_dtype=<span class=\"hljs-string\"\
          >\"auto\"</span>                  \n                ).to(<span class=\"\
          hljs-string\">\"cuda\"</span>).half()\n    _tokenizer = define_tokenizer(model=<span\
          \ class=\"hljs-number\">0</span>, hf_name=model_path\n                 \
          \                 )\n    inputs_dict = _tokenizer(newchunk, padding=<span\
          \ class=\"hljs-string\">\"max_length\"</span>, max_length=<span class=\"\
          hljs-number\">16384</span>, return_tensors=<span class=\"hljs-string\">\"\
          pt\"</span>, truncation=<span class=\"hljs-literal\">True</span>)\n    input_ids\
          \ = inputs_dict.input_ids.to(<span class=\"hljs-string\">\"cuda\"</span>)\n\
          \    attention_mask = inputs_dict.attention_mask.to(<span class=\"hljs-string\"\
          >\"cuda\"</span>)\n    global_attention_mask = torch.zeros_like(attention_mask)\n\
          \    global_attention_mask[:, <span class=\"hljs-number\">0</span>] = <span\
          \ class=\"hljs-number\">1</span>\n    <span class=\"hljs-keyword\">if</span>\
          \ extravagant:\n        predicted_abstract_ids = _model.generate(input_ids,\
          \ attention_mask=attention_mask, global_attention_mask=global_attention_mask,\
          \ max_length=max_length, num_beams=<span class=\"hljs-number\">4</span>,\
          \ \\\n                                                 do_sample=<span class=\"\
          hljs-literal\">False</span>, no_repeat_ngram_size=<span class=\"hljs-number\"\
          >3</span>, \\\n                                                        \
          \                                 encoder_no_repeat_ngram_size =<span class=\"\
          hljs-number\">3</span>,\\\n                                            \
          \                                             repetition_penalty=<span class=\"\
          hljs-number\">3.7</span>,\\\n                                          \
          \                                               early_stopping=<span class=\"\
          hljs-literal\">True</span>)\n    <span class=\"hljs-keyword\">else</span>:\n\
          \        predicted_abstract_ids = _model.generate(input_ids, attention_mask=attention_mask,\
          \ global_attention_mask=global_attention_mask, max_length=max_length, num_beams=<span\
          \ class=\"hljs-number\">4</span>)\n    result = _tokenizer.batch_decode(predicted_abstract_ids,\
          \ skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n    <span\
          \ class=\"hljs-keyword\">return</span> result\n</code></pre>\n"
        raw: "Interesting. Thank you for this brilliant piece of advise.  Based on\
          \ your inputs I consolidated the notebook as below. I am trying to use this\
          \ [notebook](https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing#scrollTo=8RClMjZOZCPO)\
          \ to work on fine tuning the model.  Some of the values for  ```extravagant```\
          \ runs are also borrowed from your model cards. I will try to play around\
          \ and see what's work best.\n\nI am on a mid tired computer (mid RTX and\
          \ old cpu) configuration, but with the code below ran within a 30 seconds\
          \ to a minute most of the time. Of course, I need to validate the outputs\
          \ with that of the model card.\nI am planning to do a in-depth validation\
          \  with multiple parameters next.\n\nNote: I have found that skipping ```max_length```\
          \ input especially when the``` input_length``` is less then ```1024``` tokens,\
          \ drastically increases the run time. \nIn the mean time, Could you explain\
          \ the point 3 ```token_batching```  in the list above? Thank you again for\
          \ you answer.\n\n```python\ndef alt_led_on_cuda(newchunk, model_path, max_length\
          \ = 512, extravagant=False):\n    _model = LEDForConditionalGeneration.from_pretrained(model_path,\
          \ low_cpu_mem_usage=True,\n    torch_dtype=\"auto\"                  \n\
          \                ).to(\"cuda\").half()\n    _tokenizer = define_tokenizer(model=0,\
          \ hf_name=model_path\n                                  )\n    inputs_dict\
          \ = _tokenizer(newchunk, padding=\"max_length\", max_length=16384, return_tensors=\"\
          pt\", truncation=True)\n    input_ids = inputs_dict.input_ids.to(\"cuda\"\
          )\n    attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n    global_attention_mask\
          \ = torch.zeros_like(attention_mask)\n    global_attention_mask[:, 0] =\
          \ 1\n    if extravagant:\n        predicted_abstract_ids = _model.generate(input_ids,\
          \ attention_mask=attention_mask, global_attention_mask=global_attention_mask,\
          \ max_length=max_length, num_beams=4, \\\n                             \
          \                    do_sample=False, no_repeat_ngram_size=3, \\\n     \
          \                                                                      \
          \              encoder_no_repeat_ngram_size =3,\\\n                    \
          \                                                                     repetition_penalty=3.7,\\\
          \n                                                                     \
          \                    early_stopping=True)\n    else:\n        predicted_abstract_ids\
          \ = _model.generate(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask,\
          \ max_length=max_length, num_beams=4)\n    result = _tokenizer.batch_decode(predicted_abstract_ids,\
          \ skip_special_tokens=True)\n    return result\n```"
        updatedAt: '2022-09-02T22:25:24.129Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pszemraj
    id: 6312826efa3e7ba13f17f462
    type: comment
  author: SamAct
  content: "Interesting. Thank you for this brilliant piece of advise.  Based on your\
    \ inputs I consolidated the notebook as below. I am trying to use this [notebook](https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing#scrollTo=8RClMjZOZCPO)\
    \ to work on fine tuning the model.  Some of the values for  ```extravagant```\
    \ runs are also borrowed from your model cards. I will try to play around and\
    \ see what's work best.\n\nI am on a mid tired computer (mid RTX and old cpu)\
    \ configuration, but with the code below ran within a 30 seconds to a minute most\
    \ of the time. Of course, I need to validate the outputs with that of the model\
    \ card.\nI am planning to do a in-depth validation  with multiple parameters next.\n\
    \nNote: I have found that skipping ```max_length``` input especially when the```\
    \ input_length``` is less then ```1024``` tokens, drastically increases the run\
    \ time. \nIn the mean time, Could you explain the point 3 ```token_batching```\
    \  in the list above? Thank you again for you answer.\n\n```python\ndef alt_led_on_cuda(newchunk,\
    \ model_path, max_length = 512, extravagant=False):\n    _model = LEDForConditionalGeneration.from_pretrained(model_path,\
    \ low_cpu_mem_usage=True,\n    torch_dtype=\"auto\"                  \n      \
    \          ).to(\"cuda\").half()\n    _tokenizer = define_tokenizer(model=0, hf_name=model_path\n\
    \                                  )\n    inputs_dict = _tokenizer(newchunk, padding=\"\
    max_length\", max_length=16384, return_tensors=\"pt\", truncation=True)\n    input_ids\
    \ = inputs_dict.input_ids.to(\"cuda\")\n    attention_mask = inputs_dict.attention_mask.to(\"\
    cuda\")\n    global_attention_mask = torch.zeros_like(attention_mask)\n    global_attention_mask[:,\
    \ 0] = 1\n    if extravagant:\n        predicted_abstract_ids = _model.generate(input_ids,\
    \ attention_mask=attention_mask, global_attention_mask=global_attention_mask,\
    \ max_length=max_length, num_beams=4, \\\n                                   \
    \              do_sample=False, no_repeat_ngram_size=3, \\\n                 \
    \                                                                        encoder_no_repeat_ngram_size\
    \ =3,\\\n                                                                    \
    \                     repetition_penalty=3.7,\\\n                            \
    \                                                             early_stopping=True)\n\
    \    else:\n        predicted_abstract_ids = _model.generate(input_ids, attention_mask=attention_mask,\
    \ global_attention_mask=global_attention_mask, max_length=max_length, num_beams=4)\n\
    \    result = _tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n\
    \    return result\n```"
  created_at: 2022-09-02 21:23:42+00:00
  edited: true
  hidden: false
  id: 6312826efa3e7ba13f17f462
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-09-08T16:23:00.000Z'
    data:
      edited: true
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>Nice! that notebook is closely aligned with what I used to fine-tune
          as well. It should work fine. Also, interesting finding on the inference
          time! Good to know. Most of my usage of the model is with 4096-8092 tokens
          input at a time, so I haven''t explored that domain much.</p>

          <p>For token batching, I forgot to link you to the notebook I use for summarizing
          more text than the model can handle at once. I went back and cleaned it
          up a bit and now also put it on this model''s card; it''s <a rel="nofollow"
          href="https://colab.research.google.com/gist/pszemraj/36950064ca76161d9d258e5cdbfa6833/led-base-demo-token-batching.ipynb">here</a>.
          It should illustrate the concept pretty well, but by "token batching"  I
          mean the process of:</p>

          <ol>

          <li>tokenize the entire body of text into batches of <code>token_batch_length</code>
          tokens each, overlapping (repeating same tokens) by <code>batch_stride</code>
          tokens.</li>

          </ol>

          <ul>

          <li>I usually set <code>batch_stride</code> to 20 or so, about a sentence</li>

          </ul>

          <ol start="2">

          <li>run summarization model on all batches</li>

          <li>check the model output probability scores / read to make sure things
          make sense</li>

          </ol>

          '
        raw: "Nice! that notebook is closely aligned with what I used to fine-tune\
          \ as well. It should work fine. Also, interesting finding on the inference\
          \ time! Good to know. Most of my usage of the model is with 4096-8092 tokens\
          \ input at a time, so I haven't explored that domain much.\n\nFor token\
          \ batching, I forgot to link you to the notebook I use for summarizing more\
          \ text than the model can handle at once. I went back and cleaned it up\
          \ a bit and now also put it on this model's card; it's [here](https://colab.research.google.com/gist/pszemraj/36950064ca76161d9d258e5cdbfa6833/led-base-demo-token-batching.ipynb).\
          \ It should illustrate the concept pretty well, but by \"token batching\"\
          \  I mean the process of:\n\n1) tokenize the entire body of text into batches\
          \ of `token_batch_length` tokens each, overlapping (repeating same tokens)\
          \ by `batch_stride` tokens. \n- I usually set `batch_stride` to 20 or so,\
          \ about a sentence\n2) run summarization model on all batches\n3) check\
          \ the model output probability scores / read to make sure things make sense"
        updatedAt: '2022-09-08T16:24:17.009Z'
      numEdits: 1
      reactions: []
    id: 631a16e40867652f5385111b
    type: comment
  author: pszemraj
  content: "Nice! that notebook is closely aligned with what I used to fine-tune as\
    \ well. It should work fine. Also, interesting finding on the inference time!\
    \ Good to know. Most of my usage of the model is with 4096-8092 tokens input at\
    \ a time, so I haven't explored that domain much.\n\nFor token batching, I forgot\
    \ to link you to the notebook I use for summarizing more text than the model can\
    \ handle at once. I went back and cleaned it up a bit and now also put it on this\
    \ model's card; it's [here](https://colab.research.google.com/gist/pszemraj/36950064ca76161d9d258e5cdbfa6833/led-base-demo-token-batching.ipynb).\
    \ It should illustrate the concept pretty well, but by \"token batching\"  I mean\
    \ the process of:\n\n1) tokenize the entire body of text into batches of `token_batch_length`\
    \ tokens each, overlapping (repeating same tokens) by `batch_stride` tokens. \n\
    - I usually set `batch_stride` to 20 or so, about a sentence\n2) run summarization\
    \ model on all batches\n3) check the model output probability scores / read to\
    \ make sure things make sense"
  created_at: 2022-09-08 15:23:00+00:00
  edited: true
  hidden: false
  id: 631a16e40867652f5385111b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1651944399858-61ab68e85b18a6f7072b5fb2.jpeg?w=200&h=200&f=face
      fullname: Nicholas Muchinguri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nickmuchi
      type: user
    createdAt: '2022-10-17T20:14:41.000Z'
    data:
      edited: false
      editors:
      - nickmuchi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1651944399858-61ab68e85b18a6f7072b5fb2.jpeg?w=200&h=200&f=face
          fullname: Nicholas Muchinguri
          isHf: false
          isPro: false
          name: nickmuchi
          type: user
        html: '<p>I tried the led-base on text that was 5000 words long, first chunked
          the text into batches of 1000 then summarized the list of batches, took
          quite a while on CPU, any idea how I can speed it up by varying some of
          the args? thanks</p>

          '
        raw: I tried the led-base on text that was 5000 words long, first chunked
          the text into batches of 1000 then summarized the list of batches, took
          quite a while on CPU, any idea how I can speed it up by varying some of
          the args? thanks
        updatedAt: '2022-10-17T20:14:41.142Z'
      numEdits: 0
      reactions: []
    id: 634db7b13baaaab768b060f7
    type: comment
  author: nickmuchi
  content: I tried the led-base on text that was 5000 words long, first chunked the
    text into batches of 1000 then summarized the list of batches, took quite a while
    on CPU, any idea how I can speed it up by varying some of the args? thanks
  created_at: 2022-10-17 19:14:41+00:00
  edited: false
  hidden: false
  id: 634db7b13baaaab768b060f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-10-19T02:31:21.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>Hey! I will answer you on both threads - sorry for the delay. In
          general, things that can affect/improve/make runtime more consistent:</p>

          <ul>

          <li>try chunking your text in X tokens as opposed to words. Sometimes numbers
          and other digits can screw up the counts, and the reality is that the tokens
          are what matters 9_i.e. if you have a lot of words that map to 4+ tokens
          or so, that batch might take forever). <a href="https://huggingface.co/spaces/pszemraj/summarize-long-text/blob/main/summarize.py">example
          code here</a></li>

          <li>decrease <code>num_beams</code> to 1 for greedy search decoding</li>

          <li>then, you can remove the penalties: set <code>length_penalty=1</code>
          and/or <code>repetition_penalty=1</code>. While you can get rid of them,
          I think some form of preventing repetition is likely needed, so I would
          try keeping <code>no_repeat_ngram_size=3</code> etc.</li>

          </ul>

          <p>Try those, but I think the long-token models are just compute-intensive.
          I think spaces used to have more resources (just a feeling I get with compute
          times now), but if it can''t run on CPU on spaces, it''s probably not viable
          without a GPU. You could also try the longt5-base model on my profile and
          see if that is more efficient</p>

          '
        raw: 'Hey! I will answer you on both threads - sorry for the delay. In general,
          things that can affect/improve/make runtime more consistent:


          - try chunking your text in X tokens as opposed to words. Sometimes numbers
          and other digits can screw up the counts, and the reality is that the tokens
          are what matters 9_i.e. if you have a lot of words that map to 4+ tokens
          or so, that batch might take forever). [example code here](https://huggingface.co/spaces/pszemraj/summarize-long-text/blob/main/summarize.py)

          - decrease `num_beams` to 1 for greedy search decoding

          - then, you can remove the penalties: set `length_penalty=1` and/or `repetition_penalty=1`.
          While you can get rid of them, I think some form of preventing repetition
          is likely needed, so I would try keeping `no_repeat_ngram_size=3` etc.



          Try those, but I think the long-token models are just compute-intensive.
          I think spaces used to have more resources (just a feeling I get with compute
          times now), but if it can''t run on CPU on spaces, it''s probably not viable
          without a GPU. You could also try the longt5-base model on my profile and
          see if that is more efficient'
        updatedAt: '2022-10-19T02:31:21.357Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nickmuchi
    id: 634f61798d0f051a45091ee5
    type: comment
  author: pszemraj
  content: 'Hey! I will answer you on both threads - sorry for the delay. In general,
    things that can affect/improve/make runtime more consistent:


    - try chunking your text in X tokens as opposed to words. Sometimes numbers and
    other digits can screw up the counts, and the reality is that the tokens are what
    matters 9_i.e. if you have a lot of words that map to 4+ tokens or so, that batch
    might take forever). [example code here](https://huggingface.co/spaces/pszemraj/summarize-long-text/blob/main/summarize.py)

    - decrease `num_beams` to 1 for greedy search decoding

    - then, you can remove the penalties: set `length_penalty=1` and/or `repetition_penalty=1`.
    While you can get rid of them, I think some form of preventing repetition is likely
    needed, so I would try keeping `no_repeat_ngram_size=3` etc.



    Try those, but I think the long-token models are just compute-intensive. I think
    spaces used to have more resources (just a feeling I get with compute times now),
    but if it can''t run on CPU on spaces, it''s probably not viable without a GPU.
    You could also try the longt5-base model on my profile and see if that is more
    efficient'
  created_at: 2022-10-19 01:31:21+00:00
  edited: false
  hidden: false
  id: 634f61798d0f051a45091ee5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-10-19T02:32:05.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>I''m going to close this for now, but if there are any issues related
          to the parameters, feel free to open it again! </p>

          '
        raw: 'I''m going to close this for now, but if there are any issues related
          to the parameters, feel free to open it again! '
        updatedAt: '2022-10-19T02:32:05.383Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - nickmuchi
        - SamAct
      relatedEventId: 634f61a52b8c811f3cc55d42
    id: 634f61a52b8c811f3cc55d41
    type: comment
  author: pszemraj
  content: 'I''m going to close this for now, but if there are any issues related
    to the parameters, feel free to open it again! '
  created_at: 2022-10-19 01:32:05+00:00
  edited: false
  hidden: false
  id: 634f61a52b8c811f3cc55d41
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-10-19T02:32:05.000Z'
    data:
      status: closed
    id: 634f61a52b8c811f3cc55d42
    type: status-change
  author: pszemraj
  created_at: 2022-10-19 01:32:05+00:00
  id: 634f61a52b8c811f3cc55d42
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: pszemraj/led-base-book-summary
repo_type: model
status: closed
target_branch: null
title: Suggestions for faster inference.
