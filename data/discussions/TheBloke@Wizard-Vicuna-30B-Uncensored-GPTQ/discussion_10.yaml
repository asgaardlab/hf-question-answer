!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mehbebe
conflicting_files: null
created_at: 2023-06-14 12:01:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d2a76a82188171be70ce2c212d9178dc.svg
      fullname: none given
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mehbebe
      type: user
    createdAt: '2023-06-14T13:01:47.000Z'
    data:
      edited: true
      editors:
      - mehbebe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.677808940410614
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d2a76a82188171be70ce2c212d9178dc.svg
          fullname: none given
          isHf: false
          isPro: false
          name: mehbebe
          type: user
        html: "<p>Hi there, I tried to load this model using oobabooga webui, following\
          \ the Readme file for this model.<br>Followed all instructions in the readme,\
          \ and I get the following error/traceback:</p>\n<p>Traceback (most recent\
          \ call last): File \u201CI:\\Deep-learning\\chat\\oogabooga\\oogabooga\\\
          text-generation-webui\\server.py\u201D, line 71, in load_model_wrapper shared.model,\
          \ shared.tokenizer = load_model(shared.model_name) File \u201CI:\\Deep-learning\\\
          chat\\oogabooga\\oogabooga\\text-generation-webui\\modules\\models.py\u201D\
          , line 95, in load_model output = load_func(model_name) File \u201CI:\\\
          Deep-learning\\chat\\oogabooga\\oogabooga\\text-generation-webui\\modules\\\
          models.py\u201D, line 289, in GPTQ_loader model = modules.GPTQ_loader.load_quantized(model_name)\
          \ File \u201CI:\\Deep-learning\\chat\\oogabooga\\oogabooga\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 177, in load_quantized model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
          \ File \u201CI:\\Deep-learning\\chat\\oogabooga\\oogabooga\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 84, in _load_quant model.load_state_dict(safe_load(checkpoint),\
          \ strict=False) File \u201CI:\\Deep-learning\\chat\\oogabooga\\oogabooga\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D\
          , line 2041, in load_state_dict raise RuntimeError(\u2018Error(s) in loading\
          \ state_dict for {}:\\n\\t{}\u2019.format( RuntimeError: Error(s) in loading\
          \ state_dict for LlamaForCausalLM: size mismatch for model.embed_tokens.weight:\
          \ copying a param with shape torch.Size([32001, 6656]) from checkpoint,\
          \ the shape in current model is torch.Size([32000, 6656]). size mismatch\
          \ for lm_head.weight: copying a param with shape torch.Size([32001, 6656])\
          \ from checkpoint, the shape in current model is torch.Size([32000, 6656]).</p>\n\
          <p>I have been able to load the 13B version of this model, but not this\
          \ 30B model. If someone could please help me figure out what is causing\
          \ this, and how to fix it, I'd appreciate that very much.<br>P.s. I noticed\
          \ another person posted about a similar sounding issue \"size mismatch\"\
          , to which the solution was setting the group size to 0. I have already\
          \ set mine to 0 (and was sure to save the model settings before reloading\
          \ it) and I still get this error.</p>\n"
        raw: "Hi there, I tried to load this model using oobabooga webui, following\
          \ the Readme file for this model.\nFollowed all instructions in the readme,\
          \ and I get the following error/traceback:\n\nTraceback (most recent call\
          \ last): File \u201CI:\\Deep-learning\\chat\\oogabooga\\oogabooga\\text-generation-webui\\\
          server.py\u201D, line 71, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CI:\\Deep-learning\\chat\\oogabooga\\\
          oogabooga\\text-generation-webui\\modules\\models.py\u201D, line 95, in\
          \ load_model output = load_func(model_name) File \u201CI:\\Deep-learning\\\
          chat\\oogabooga\\oogabooga\\text-generation-webui\\modules\\models.py\u201D\
          , line 289, in GPTQ_loader model = modules.GPTQ_loader.load_quantized(model_name)\
          \ File \u201CI:\\Deep-learning\\chat\\oogabooga\\oogabooga\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 177, in load_quantized model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
          \ File \u201CI:\\Deep-learning\\chat\\oogabooga\\oogabooga\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 84, in _load_quant model.load_state_dict(safe_load(checkpoint),\
          \ strict=False) File \u201CI:\\Deep-learning\\chat\\oogabooga\\oogabooga\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D\
          , line 2041, in load_state_dict raise RuntimeError(\u2018Error(s) in loading\
          \ state_dict for {}:\\n\\t{}\u2019.format( RuntimeError: Error(s) in loading\
          \ state_dict for LlamaForCausalLM: size mismatch for model.embed_tokens.weight:\
          \ copying a param with shape torch.Size([32001, 6656]) from checkpoint,\
          \ the shape in current model is torch.Size([32000, 6656]). size mismatch\
          \ for lm_head.weight: copying a param with shape torch.Size([32001, 6656])\
          \ from checkpoint, the shape in current model is torch.Size([32000, 6656]).\n\
          \nI have been able to load the 13B version of this model, but not this 30B\
          \ model. If someone could please help me figure out what is causing this,\
          \ and how to fix it, I'd appreciate that very much.\nP.s. I noticed another\
          \ person posted about a similar sounding issue \"size mismatch\", to which\
          \ the solution was setting the group size to 0. I have already set mine\
          \ to 0 (and was sure to save the model settings before reloading it) and\
          \ I still get this error."
        updatedAt: '2023-06-14T13:37:48.652Z'
      numEdits: 1
      reactions: []
    id: 6489ba3ba33518a589322de8
    type: comment
  author: mehbebe
  content: "Hi there, I tried to load this model using oobabooga webui, following\
    \ the Readme file for this model.\nFollowed all instructions in the readme, and\
    \ I get the following error/traceback:\n\nTraceback (most recent call last): File\
    \ \u201CI:\\Deep-learning\\chat\\oogabooga\\oogabooga\\text-generation-webui\\\
    server.py\u201D, line 71, in load_model_wrapper shared.model, shared.tokenizer\
    \ = load_model(shared.model_name) File \u201CI:\\Deep-learning\\chat\\oogabooga\\\
    oogabooga\\text-generation-webui\\modules\\models.py\u201D, line 95, in load_model\
    \ output = load_func(model_name) File \u201CI:\\Deep-learning\\chat\\oogabooga\\\
    oogabooga\\text-generation-webui\\modules\\models.py\u201D, line 289, in GPTQ_loader\
    \ model = modules.GPTQ_loader.load_quantized(model_name) File \u201CI:\\Deep-learning\\\
    chat\\oogabooga\\oogabooga\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
    , line 177, in load_quantized model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
    \ File \u201CI:\\Deep-learning\\chat\\oogabooga\\oogabooga\\text-generation-webui\\\
    modules\\GPTQ_loader.py\u201D, line 84, in _load_quant model.load_state_dict(safe_load(checkpoint),\
    \ strict=False) File \u201CI:\\Deep-learning\\chat\\oogabooga\\oogabooga\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 2041, in load_state_dict\
    \ raise RuntimeError(\u2018Error(s) in loading state_dict for {}:\\n\\t{}\u2019\
    .format( RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM: size\
    \ mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([32001,\
    \ 6656]) from checkpoint, the shape in current model is torch.Size([32000, 6656]).\
    \ size mismatch for lm_head.weight: copying a param with shape torch.Size([32001,\
    \ 6656]) from checkpoint, the shape in current model is torch.Size([32000, 6656]).\n\
    \nI have been able to load the 13B version of this model, but not this 30B model.\
    \ If someone could please help me figure out what is causing this, and how to\
    \ fix it, I'd appreciate that very much.\nP.s. I noticed another person posted\
    \ about a similar sounding issue \"size mismatch\", to which the solution was\
    \ setting the group size to 0. I have already set mine to 0 (and was sure to save\
    \ the model settings before reloading it) and I still get this error."
  created_at: 2023-06-14 12:01:47+00:00
  edited: true
  hidden: false
  id: 6489ba3ba33518a589322de8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-16T11:41:03.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.77788245677948
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Latest text-generation-webui now uses AutoGPTQ which automatically
          sets GPTQ parameters.</p>

          <p>On some of my older models I''ve not yet updated the README to explain
          that. I have just updated this README, so please re-read the section "How
          to easily download and use.." to see the new instructions.</p>

          <p>In short: all GPTQ parameters should be set to default values. This will
          then load the model using AutoGPTQ, and these parameters will be loaded
          from the file <code>quantize_config.json</code>.  If you set any GPTQ parameters
          manually it will override these settings, and possibly give it a bad setting.</p>

          '
        raw: 'Latest text-generation-webui now uses AutoGPTQ which automatically sets
          GPTQ parameters.


          On some of my older models I''ve not yet updated the README to explain that.
          I have just updated this README, so please re-read the section "How to easily
          download and use.." to see the new instructions.


          In short: all GPTQ parameters should be set to default values. This will
          then load the model using AutoGPTQ, and these parameters will be loaded
          from the file `quantize_config.json`.  If you set any GPTQ parameters manually
          it will override these settings, and possibly give it a bad setting.'
        updatedAt: '2023-06-16T11:41:03.476Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mehbebe
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - mehbebe
    id: 648c4a4fe1b6e19cc56f22ed
    type: comment
  author: TheBloke
  content: 'Latest text-generation-webui now uses AutoGPTQ which automatically sets
    GPTQ parameters.


    On some of my older models I''ve not yet updated the README to explain that. I
    have just updated this README, so please re-read the section "How to easily download
    and use.." to see the new instructions.


    In short: all GPTQ parameters should be set to default values. This will then
    load the model using AutoGPTQ, and these parameters will be loaded from the file
    `quantize_config.json`.  If you set any GPTQ parameters manually it will override
    these settings, and possibly give it a bad setting.'
  created_at: 2023-06-16 10:41:03+00:00
  edited: false
  hidden: false
  id: 648c4a4fe1b6e19cc56f22ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03665de475376a3550ef312debb23811.svg
      fullname: dams air
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: damsair
      type: user
    createdAt: '2023-06-17T12:13:47.000Z'
    data:
      edited: false
      editors:
      - damsair
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8288382291793823
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03665de475376a3550ef312debb23811.svg
          fullname: dams air
          isHf: false
          isPro: false
          name: damsair
          type: user
        html: '<blockquote>

          <p>Latest text-generation-webui now uses AutoGPTQ which automatically sets
          GPTQ parameters.</p>

          <p>On some of my older models I''ve not yet updated the README to explain
          that. I have just updated this README, so please re-read the section "How
          to easily download and use.." to see the new instructions.</p>

          <p>In short: all GPTQ parameters should be set to default values. This will
          then load the model using AutoGPTQ, and these parameters will be loaded
          from the file <code>quantize_config.json</code>.  If you set any GPTQ parameters
          manually it will override these settings, and possibly give it a bad setting.</p>

          </blockquote>

          <p>I have the same error, can load the 13B version too without any trouble.
          Doing what you mentioned above doesn''t change anything for me sadly.</p>

          '
        raw: "> Latest text-generation-webui now uses AutoGPTQ which automatically\
          \ sets GPTQ parameters.\n> \n> On some of my older models I've not yet updated\
          \ the README to explain that. I have just updated this README, so please\
          \ re-read the section \"How to easily download and use..\" to see the new\
          \ instructions.\n> \n> In short: all GPTQ parameters should be set to default\
          \ values. This will then load the model using AutoGPTQ, and these parameters\
          \ will be loaded from the file `quantize_config.json`.  If you set any GPTQ\
          \ parameters manually it will override these settings, and possibly give\
          \ it a bad setting.\n\nI have the same error, can load the 13B version too\
          \ without any trouble. Doing what you mentioned above doesn't change anything\
          \ for me sadly."
        updatedAt: '2023-06-17T12:13:47.497Z'
      numEdits: 0
      reactions: []
    id: 648da37b80bf2c9599354fb4
    type: comment
  author: damsair
  content: "> Latest text-generation-webui now uses AutoGPTQ which automatically sets\
    \ GPTQ parameters.\n> \n> On some of my older models I've not yet updated the\
    \ README to explain that. I have just updated this README, so please re-read the\
    \ section \"How to easily download and use..\" to see the new instructions.\n\
    > \n> In short: all GPTQ parameters should be set to default values. This will\
    \ then load the model using AutoGPTQ, and these parameters will be loaded from\
    \ the file `quantize_config.json`.  If you set any GPTQ parameters manually it\
    \ will override these settings, and possibly give it a bad setting.\n\nI have\
    \ the same error, can load the 13B version too without any trouble. Doing what\
    \ you mentioned above doesn't change anything for me sadly."
  created_at: 2023-06-17 11:13:47+00:00
  edited: false
  hidden: false
  id: 648da37b80bf2c9599354fb4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03665de475376a3550ef312debb23811.svg
      fullname: dams air
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: damsair
      type: user
    createdAt: '2023-06-17T12:41:43.000Z'
    data:
      edited: false
      editors:
      - damsair
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3439888060092926
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03665de475376a3550ef312debb23811.svg
          fullname: dams air
          isHf: false
          isPro: false
          name: damsair
          type: user
        html: "<p>And now i get this error :</p>\n<p>\"Traceback (most recent call\
          \ last): File \u201CD:\\SD\\GPT\\oobabooga_windows\\text-generation-webui\\\
          server.py\u201D, line 73, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name, loader) File \u201CD:\\SD\\GPT\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 65, in load_model\
          \ output = load_func_maploader File \u201CD:\\SD\\GPT\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 271, in AutoGPTQ_loader\
          \ return modules.AutoGPTQ_loader.load_quantized(model_name) File \u201C\
          D:\\SD\\GPT\\oobabooga_windows\\text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D\
          , line 55, in load_quantized model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params) File \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\auto_gptq\\modeling\\auto.py\u201D, line 82, in\
          \ from_quantized return quant_func( File \u201CD:\\SD\\GPT\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\auto_gptq\\modeling_base.py\u201D\
          , line 717, in from_quantized model = AutoModelForCausalLM.from_config(\
          \ File \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\transformers\\models\\auto\\auto_factory.py\u201D, line 425,\
          \ in from_config return model_class._from_config(config, **kwargs) File\
          \ \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\u201D, line 1143, in _from_config model\
          \ = cls(config, **kwargs) File \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
          , line 615, in init self.model = LlamaModel(config) File \u201CD:\\SD\\\
          GPT\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\u201D, line 446, in init self.layers =\
          \ nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\
          \ File \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D, line\
          \ 446, in self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in\
          \ range(config.num_hidden_layers)]) File \u201CD:\\SD\\GPT\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\u201D, line 256, in init self.mlp = LlamaMLP( File \u201C\
          D:\\SD\\GPT\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 149, in init\
          \ self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\
          \ File \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\torch\\nn\\modules\\linear.py\u201D, line 96, in init self.weight\
          \ = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\
          \ RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\\
          core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory:\
          \ you tried to allocate 238551040 bytes.\"</p>\n<p>Trying to load this one\
          \ on a RTX 4090.</p>\n"
        raw: "And now i get this error :\n\n\"Traceback (most recent call last): File\
          \ \u201CD:\\SD\\GPT\\oobabooga_windows\\text-generation-webui\\server.py\u201D\
          , line 73, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader) File \u201CD:\\SD\\GPT\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 65, in load_model output = load_func_maploader\
          \ File \u201CD:\\SD\\GPT\\oobabooga_windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 271, in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
          \ File \u201CD:\\SD\\GPT\\oobabooga_windows\\text-generation-webui\\modules\\\
          AutoGPTQ_loader.py\u201D, line 55, in load_quantized model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params) File \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\auto_gptq\\modeling\\auto.py\u201D, line 82, in\
          \ from_quantized return quant_func( File \u201CD:\\SD\\GPT\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\auto_gptq\\modeling_base.py\u201D\
          , line 717, in from_quantized model = AutoModelForCausalLM.from_config(\
          \ File \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\transformers\\models\\auto\\auto_factory.py\u201D, line 425,\
          \ in from_config return model_class._from_config(config, **kwargs) File\
          \ \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\u201D, line 1143, in _from_config model\
          \ = cls(config, **kwargs) File \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
          , line 615, in init self.model = LlamaModel(config) File \u201CD:\\SD\\\
          GPT\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\u201D, line 446, in init self.layers =\
          \ nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\
          \ File \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D, line\
          \ 446, in self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in\
          \ range(config.num_hidden_layers)]) File \u201CD:\\SD\\GPT\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\u201D, line 256, in init self.mlp = LlamaMLP( File \u201C\
          D:\\SD\\GPT\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\u201D, line 149, in init\
          \ self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\
          \ File \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\torch\\nn\\modules\\linear.py\u201D, line 96, in init self.weight\
          \ = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\
          \ RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\\
          core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory:\
          \ you tried to allocate 238551040 bytes.\"\n\nTrying to load this one on\
          \ a RTX 4090."
        updatedAt: '2023-06-17T12:41:43.657Z'
      numEdits: 0
      reactions: []
    id: 648daa070c223ec38bdaf730
    type: comment
  author: damsair
  content: "And now i get this error :\n\n\"Traceback (most recent call last): File\
    \ \u201CD:\\SD\\GPT\\oobabooga_windows\\text-generation-webui\\server.py\u201D\
    , line 73, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader) File \u201CD:\\SD\\GPT\\oobabooga_windows\\text-generation-webui\\modules\\\
    models.py\u201D, line 65, in load_model output = load_func_maploader File \u201C\
    D:\\SD\\GPT\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
    , line 271, in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
    \ File \u201CD:\\SD\\GPT\\oobabooga_windows\\text-generation-webui\\modules\\\
    AutoGPTQ_loader.py\u201D, line 55, in load_quantized model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
    \ **params) File \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\env\\\
    lib\\site-packages\\auto_gptq\\modeling\\auto.py\u201D, line 82, in from_quantized\
    \ return quant_func( File \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\auto_gptq\\modeling_base.py\u201D, line 717, in from_quantized\
    \ model = AutoModelForCausalLM.from_config( File \u201CD:\\SD\\GPT\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u201D\
    , line 425, in from_config return model_class._from_config(config, **kwargs) File\
    \ \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\modeling_utils.py\u201D, line 1143, in _from_config model = cls(config,\
    \ **kwargs) File \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\env\\\
    lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D, line\
    \ 615, in init self.model = LlamaModel(config) File \u201CD:\\SD\\GPT\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
    , line 446, in init self.layers = nn.ModuleList([LlamaDecoderLayer(config) for\
    \ _ in range(config.num_hidden_layers)]) File \u201CD:\\SD\\GPT\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
    , line 446, in self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in\
    \ range(config.num_hidden_layers)]) File \u201CD:\\SD\\GPT\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
    , line 256, in init self.mlp = LlamaMLP( File \u201CD:\\SD\\GPT\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\u201D\
    , line 149, in init self.gate_proj = nn.Linear(hidden_size, intermediate_size,\
    \ bias=False) File \u201CD:\\SD\\GPT\\oobabooga_windows\\installer_files\\env\\\
    lib\\site-packages\\torch\\nn\\modules\\linear.py\u201D, line 96, in init self.weight\
    \ = Parameter(torch.empty((out_features, in_features), **factory_kwargs)) RuntimeError:\
    \ [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72]\
    \ data. DefaultCPUAllocator: not enough memory: you tried to allocate 238551040\
    \ bytes.\"\n\nTrying to load this one on a RTX 4090."
  created_at: 2023-06-17 11:41:43+00:00
  edited: false
  hidden: false
  id: 648daa070c223ec38bdaf730
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-17T19:15:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9386301040649414
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;damsair&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/damsair\">@<span class=\"\
          underline\">damsair</span></a></span>\n\n\t</span></span> </p>\n<p>That\
          \ error means that you need to increase the Windows Pagefile size.  Either\
          \ manually increase it to around 90GB in size, or else set it to Auto and\
          \ make sure there's at least 90GB free on C: (or whatever drive the Pagefile\
          \ is on)</p>\n<p>With that done, the model should load.</p>\n<p>This is\
          \ a common issue for Windows users, especially with the larger models. Windows\
          \ loads the model first into RAM before it goes to VRAM. And for some reason\
          \ it requires that you have a large amount of Pagefile free for this purpose,\
          \ even for users who have plenty of free RAM.</p>\n"
        raw: "@damsair \n\nThat error means that you need to increase the Windows\
          \ Pagefile size.  Either manually increase it to around 90GB in size, or\
          \ else set it to Auto and make sure there's at least 90GB free on C: (or\
          \ whatever drive the Pagefile is on)\n\nWith that done, the model should\
          \ load.\n\nThis is a common issue for Windows users, especially with the\
          \ larger models. Windows loads the model first into RAM before it goes to\
          \ VRAM. And for some reason it requires that you have a large amount of\
          \ Pagefile free for this purpose, even for users who have plenty of free\
          \ RAM."
        updatedAt: '2023-06-17T19:15:22.099Z'
      numEdits: 0
      reactions: []
    id: 648e064a24746db241f14df1
    type: comment
  author: TheBloke
  content: "@damsair \n\nThat error means that you need to increase the Windows Pagefile\
    \ size.  Either manually increase it to around 90GB in size, or else set it to\
    \ Auto and make sure there's at least 90GB free on C: (or whatever drive the Pagefile\
    \ is on)\n\nWith that done, the model should load.\n\nThis is a common issue for\
    \ Windows users, especially with the larger models. Windows loads the model first\
    \ into RAM before it goes to VRAM. And for some reason it requires that you have\
    \ a large amount of Pagefile free for this purpose, even for users who have plenty\
    \ of free RAM."
  created_at: 2023-06-17 18:15:22+00:00
  edited: false
  hidden: false
  id: 648e064a24746db241f14df1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03665de475376a3550ef312debb23811.svg
      fullname: dams air
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: damsair
      type: user
    createdAt: '2023-06-17T20:36:48.000Z'
    data:
      edited: false
      editors:
      - damsair
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5867010951042175
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03665de475376a3550ef312debb23811.svg
          fullname: dams air
          isHf: false
          isPro: false
          name: damsair
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> </p>\n<p>Thanks\
          \ for this, i set it to Auto (windows pagefile size), restarted, but still\
          \ impossible to use this model. When i load it here is what i get :<br>2023-06-17\
          \ 22:32:44 INFO:Loading TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ...<br>2023-06-17\
          \ 22:32:44 INFO:The AutoGPTQ params are: {'model_basename': 'Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order',\
          \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': None, 'quantize_config': None}<br>2023-06-17 22:32:59\
          \ WARNING:The model weights are not tied. Please use the <code>tie_weights</code>\
          \ method before using the <code>infer_auto_device</code> function.<br>2023-06-17\
          \ 22:32:59 WARNING:The safetensors archive passed at models\\TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\\\
          Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order.safetensors does not contain\
          \ metadata. Make sure to save your model with the <code>save_pretrained</code>\
          \ method. Defaulting to 'pt' metadata.</p>\n<p>And then nothing, the GUI\
          \ doesn't work anymore. </p>\n"
        raw: "@TheBloke \n\nThanks for this, i set it to Auto (windows pagefile size),\
          \ restarted, but still impossible to use this model. When i load it here\
          \ is what i get :\n2023-06-17 22:32:44 INFO:Loading TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ...\n\
          2023-06-17 22:32:44 INFO:The AutoGPTQ params are: {'model_basename': 'Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order',\
          \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': None, 'quantize_config': None}\n2023-06-17 22:32:59\
          \ WARNING:The model weights are not tied. Please use the `tie_weights` method\
          \ before using the `infer_auto_device` function.\n2023-06-17 22:32:59 WARNING:The\
          \ safetensors archive passed at models\\TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\\\
          Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order.safetensors does not contain\
          \ metadata. Make sure to save your model with the `save_pretrained` method.\
          \ Defaulting to 'pt' metadata.\n\nAnd then nothing, the GUI doesn't work\
          \ anymore. "
        updatedAt: '2023-06-17T20:36:48.304Z'
      numEdits: 0
      reactions: []
    id: 648e196041aa8acdac834c92
    type: comment
  author: damsair
  content: "@TheBloke \n\nThanks for this, i set it to Auto (windows pagefile size),\
    \ restarted, but still impossible to use this model. When i load it here is what\
    \ i get :\n2023-06-17 22:32:44 INFO:Loading TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ...\n\
    2023-06-17 22:32:44 INFO:The AutoGPTQ params are: {'model_basename': 'Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order',\
    \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp':\
    \ True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory': None,\
    \ 'quantize_config': None}\n2023-06-17 22:32:59 WARNING:The model weights are\
    \ not tied. Please use the `tie_weights` method before using the `infer_auto_device`\
    \ function.\n2023-06-17 22:32:59 WARNING:The safetensors archive passed at models\\\
    TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\\Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\n\nAnd then nothing, the GUI doesn't work\
    \ anymore. "
  created_at: 2023-06-17 19:36:48+00:00
  edited: false
  hidden: false
  id: 648e196041aa8acdac834c92
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-17T20:44:58.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9474682211875916
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>How long did you wait? Those messages indicate it''s still loading
          the model.  The next thing you''d see, once it''s loaded, is</p>

          <pre><code>Loaded the model in X seconds.

          Running on local URL:  http://0.0.0.0:7860

          </code></pre>

          '
        raw: 'How long did you wait? Those messages indicate it''s still loading the
          model.  The next thing you''d see, once it''s loaded, is

          ```

          Loaded the model in X seconds.

          Running on local URL:  http://0.0.0.0:7860

          ```


          '
        updatedAt: '2023-06-17T20:44:58.987Z'
      numEdits: 0
      reactions: []
    id: 648e1b4a3c8611c2ba112781
    type: comment
  author: TheBloke
  content: 'How long did you wait? Those messages indicate it''s still loading the
    model.  The next thing you''d see, once it''s loaded, is

    ```

    Loaded the model in X seconds.

    Running on local URL:  http://0.0.0.0:7860

    ```


    '
  created_at: 2023-06-17 19:44:58+00:00
  edited: false
  hidden: false
  id: 648e1b4a3c8611c2ba112781
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03665de475376a3550ef312debb23811.svg
      fullname: dams air
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: damsair
      type: user
    createdAt: '2023-06-17T21:02:15.000Z'
    data:
      edited: false
      editors:
      - damsair
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8636789917945862
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03665de475376a3550ef312debb23811.svg
          fullname: dams air
          isHf: false
          isPro: false
          name: damsair
          type: user
        html: '<p>I waited several minutes, just after the error text I copied above,
          there appears a line saying "press any key to continue...". If I press any
          key,  the window closes (and GUI show many red ERROR messages).</p>

          '
        raw: I waited several minutes, just after the error text I copied above, there
          appears a line saying "press any key to continue...". If I press any key,  the
          window closes (and GUI show many red ERROR messages).
        updatedAt: '2023-06-17T21:02:15.365Z'
      numEdits: 0
      reactions: []
    id: 648e1f57434eed3599ae57fa
    type: comment
  author: damsair
  content: I waited several minutes, just after the error text I copied above, there
    appears a line saying "press any key to continue...". If I press any key,  the
    window closes (and GUI show many red ERROR messages).
  created_at: 2023-06-17 20:02:15+00:00
  edited: false
  hidden: false
  id: 648e1f57434eed3599ae57fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-17T21:03:06.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8865194916725159
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK, the ''press any key to continue'' message means it still needs
          more Pagefile.  Try increasing it further.</p>

          '
        raw: OK, the 'press any key to continue' message means it still needs more
          Pagefile.  Try increasing it further.
        updatedAt: '2023-06-17T21:03:06.785Z'
      numEdits: 0
      reactions: []
    id: 648e1f8ae662f247e7b44c3e
    type: comment
  author: TheBloke
  content: OK, the 'press any key to continue' message means it still needs more Pagefile.  Try
    increasing it further.
  created_at: 2023-06-17 20:03:06+00:00
  edited: false
  hidden: false
  id: 648e1f8ae662f247e7b44c3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03665de475376a3550ef312debb23811.svg
      fullname: dams air
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: damsair
      type: user
    createdAt: '2023-06-17T21:30:45.000Z'
    data:
      edited: true
      editors:
      - damsair
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.94337397813797
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03665de475376a3550ef312debb23811.svg
          fullname: dams air
          isHf: false
          isPro: false
          name: damsair
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>,\
          \ it's finally working and i'm going to test all the 30B models on your\
          \ page now :) it was the same error each time ! Thanks again i just learnt\
          \ something ! </p>\n"
        raw: 'Thank you @TheBloke, it''s finally working and i''m going to test all
          the 30B models on your page now :) it was the same error each time ! Thanks
          again i just learnt something ! '
        updatedAt: '2023-06-17T21:31:10.880Z'
      numEdits: 1
      reactions: []
    id: 648e26050539107748190799
    type: comment
  author: damsair
  content: 'Thank you @TheBloke, it''s finally working and i''m going to test all
    the 30B models on your page now :) it was the same error each time ! Thanks again
    i just learnt something ! '
  created_at: 2023-06-17 20:30:45+00:00
  edited: true
  hidden: false
  id: 648e26050539107748190799
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: size mismatch for model.embed_tokens.weight - model refuses to load.
