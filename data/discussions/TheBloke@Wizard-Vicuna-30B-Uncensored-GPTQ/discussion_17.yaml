!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Wazzz
conflicting_files: null
created_at: 2023-07-10 02:30:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/001bc6bc2af5fc0fa3baa9473952ae9a.svg
      fullname: WaZi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Wazzz
      type: user
    createdAt: '2023-07-10T03:30:06.000Z'
    data:
      edited: false
      editors:
      - Wazzz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5384947061538696
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/001bc6bc2af5fc0fa3baa9473952ae9a.svg
          fullname: WaZi
          isHf: false
          isPro: false
          name: Wazzz
          type: user
        html: "<p>RuntimeError: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72]\
          \ data. DefaultCPUAllocator: not enough memory: you tried to allocate 238551040\
          \ bytes.</p>\n<p>My C drive has 106G of space, and the memory stick is 32G;\
          \ is there something wrong with my swap partition settings?<br>After the\
          \ size of the C drive shrinks to 83G during the running process, the program\
          \ reports this exception.<br>The demo.py used is as follows.</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-comment\">#git clone https://github.com/PanQiWei/AutoGPTQ</span>\n\
          <span class=\"hljs-comment\">#cd AutoGPTQ</span>\n<span class=\"hljs-comment\"\
          >#pip install .</span>\n\n\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer, pipeline, logging\n\
          <span class=\"hljs-keyword\">from</span> auto_gptq <span class=\"hljs-keyword\"\
          >import</span> AutoGPTQForCausalLM, BaseQuantizeConfig\n<span class=\"hljs-keyword\"\
          >import</span> argparse\n\n<span class=\"hljs-comment\"># change this path\
          \ to match where you downloaded the model</span>\nquantized_model_dir =\
          \ <span class=\"hljs-string\">\"E:\\\\AI\\\\module\\\\uncensored-chat\\\\\
          Wizard-Vicuna-30B-Uncensored-GPTQ\"</span>\n\nmodel_basename = <span class=\"\
          hljs-string\">\"Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order\"</span>\n\
          \nuse_triton = <span class=\"hljs-literal\">False</span>\n\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=<span class=\"\
          hljs-literal\">True</span>)\n<span class=\"hljs-comment\">#print(dir(tokenizer))</span>\n\
          \nquantize_config = BaseQuantizeConfig(\n        bits=<span class=\"hljs-number\"\
          >4</span>,\n        group_size=<span class=\"hljs-number\">128</span>,\n\
          \        desc_act=<span class=\"hljs-literal\">False</span>\n    )\n   \
          \ \n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path=quantized_model_dir,\n\
          \        use_safetensors=<span class=\"hljs-literal\">True</span>,\n   \
          \     model_basename=model_basename,\n        device=<span class=\"hljs-string\"\
          >\"cuda:0\"</span>,\n        use_triton=use_triton,\n        quantize_config=quantize_config)\n\
          \n<span class=\"hljs-comment\"># Prevent printing spurious transformers\
          \ error when using pipeline with AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\
          \nprompt = <span class=\"hljs-string\">\"Tell me about AI\"</span>\nprompt_template=<span\
          \ class=\"hljs-string\">f'''### Human: <span class=\"hljs-subst\">{prompt}</span></span>\n\
          <span class=\"hljs-string\">### Assistant:'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"</span>)\npipe\
          \ = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"</span>,\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span class=\"\
          hljs-number\">512</span>,\n    temperature=<span class=\"hljs-number\">0.7</span>,\n\
          \    top_p=<span class=\"hljs-number\">0.95</span>,\n    repetition_penalty=<span\
          \ class=\"hljs-number\">1.15</span>\n)\n\n<span class=\"hljs-built_in\"\
          >print</span>(pipe(prompt_template)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">'generated_text'</span>])\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n"
        raw: "RuntimeError: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72]\
          \ data. DefaultCPUAllocator: not enough memory: you tried to allocate 238551040\
          \ bytes.\r\n\r\nMy C drive has 106G of space, and the memory stick is 32G;\
          \ is there something wrong with my swap partition settings?\r\nAfter the\
          \ size of the C drive shrinks to 83G during the running process, the program\
          \ reports this exception.\r\nThe demo.py used is as follows.\r\n```python\r\
          \n#git clone https://github.com/PanQiWei/AutoGPTQ\r\n#cd AutoGPTQ\r\n#pip\
          \ install .\r\n\r\n\r\nfrom transformers import AutoTokenizer, pipeline,\
          \ logging\r\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\
          \nimport argparse\r\n\r\n# change this path to match where you downloaded\
          \ the model\r\nquantized_model_dir = \"E:\\\\AI\\\\module\\\\uncensored-chat\\\
          \\Wizard-Vicuna-30B-Uncensored-GPTQ\"\r\n\r\nmodel_basename = \"Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order\"\
          \r\n\r\nuse_triton = False\r\n\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=True)\r\n#print(dir(tokenizer))\r\n\r\nquantize_config = BaseQuantizeConfig(\r\
          \n        bits=4,\r\n        group_size=128,\r\n        desc_act=False\r\
          \n    )\r\n    \r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path=quantized_model_dir,\r\
          \n        use_safetensors=True,\r\n        model_basename=model_basename,\r\
          \n        device=\"cuda:0\",\r\n        use_triton=use_triton,\r\n     \
          \   quantize_config=quantize_config)\r\n\r\n# Prevent printing spurious\
          \ transformers error when using pipeline with AutoGPTQ\r\nlogging.set_verbosity(logging.CRITICAL)\r\
          \n\r\nprompt = \"Tell me about AI\"\r\nprompt_template=f'''### Human: {prompt}\r\
          \n### Assistant:'''\r\n\r\nprint(\"*** Pipeline:\")\r\npipe = pipeline(\r\
          \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
          \n    max_new_tokens=512,\r\n    temperature=0.7,\r\n    top_p=0.95,\r\n\
          \    repetition_penalty=1.15\r\n)\r\n\r\nprint(pipe(prompt_template)[0]['generated_text'])\r\
          \n\r\nprint(\"\\n\\n*** Generate:\")\r\n\r\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\r\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, max_new_tokens=512)\r\nprint(tokenizer.decode(output[0]))\r\
          \n```"
        updatedAt: '2023-07-10T03:30:06.554Z'
      numEdits: 0
      reactions: []
    id: 64ab7b3eb35f48e37d3bcc12
    type: comment
  author: Wazzz
  content: "RuntimeError: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72]\
    \ data. DefaultCPUAllocator: not enough memory: you tried to allocate 238551040\
    \ bytes.\r\n\r\nMy C drive has 106G of space, and the memory stick is 32G; is\
    \ there something wrong with my swap partition settings?\r\nAfter the size of\
    \ the C drive shrinks to 83G during the running process, the program reports this\
    \ exception.\r\nThe demo.py used is as follows.\r\n```python\r\n#git clone https://github.com/PanQiWei/AutoGPTQ\r\
    \n#cd AutoGPTQ\r\n#pip install .\r\n\r\n\r\nfrom transformers import AutoTokenizer,\
    \ pipeline, logging\r\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\
    \nimport argparse\r\n\r\n# change this path to match where you downloaded the\
    \ model\r\nquantized_model_dir = \"E:\\\\AI\\\\module\\\\uncensored-chat\\\\Wizard-Vicuna-30B-Uncensored-GPTQ\"\
    \r\n\r\nmodel_basename = \"Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order\"\r\
    \n\r\nuse_triton = False\r\n\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
    \ use_fast=True)\r\n#print(dir(tokenizer))\r\n\r\nquantize_config = BaseQuantizeConfig(\r\
    \n        bits=4,\r\n        group_size=128,\r\n        desc_act=False\r\n   \
    \ )\r\n    \r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path=quantized_model_dir,\r\
    \n        use_safetensors=True,\r\n        model_basename=model_basename,\r\n\
    \        device=\"cuda:0\",\r\n        use_triton=use_triton,\r\n        quantize_config=quantize_config)\r\
    \n\r\n# Prevent printing spurious transformers error when using pipeline with\
    \ AutoGPTQ\r\nlogging.set_verbosity(logging.CRITICAL)\r\n\r\nprompt = \"Tell me\
    \ about AI\"\r\nprompt_template=f'''### Human: {prompt}\r\n### Assistant:'''\r\
    \n\r\nprint(\"*** Pipeline:\")\r\npipe = pipeline(\r\n    \"text-generation\"\
    ,\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n    max_new_tokens=512,\r\
    \n    temperature=0.7,\r\n    top_p=0.95,\r\n    repetition_penalty=1.15\r\n)\r\
    \n\r\nprint(pipe(prompt_template)[0]['generated_text'])\r\n\r\nprint(\"\\n\\n***\
    \ Generate:\")\r\n\r\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\r\
    \noutput = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\r\
    \nprint(tokenizer.decode(output[0]))\r\n```"
  created_at: 2023-07-10 02:30:06+00:00
  edited: false
  hidden: false
  id: 64ab7b3eb35f48e37d3bcc12
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/001bc6bc2af5fc0fa3baa9473952ae9a.svg
      fullname: WaZi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Wazzz
      type: user
    createdAt: '2023-07-10T03:30:54.000Z'
    data:
      edited: false
      editors:
      - Wazzz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.298164039850235
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/001bc6bc2af5fc0fa3baa9473952ae9a.svg
          fullname: WaZi
          isHf: false
          isPro: false
          name: Wazzz
          type: user
        html: '<p>+---------------------------------------------------------------------------------------+<br>|
          NVIDIA-SMI 531.79                 Driver Version: 531.79       CUDA Version:
          12.1     |<br>|-----------------------------------------+----------------------+----------------------+<br>|
          GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile
          Uncorr. ECC |<br>| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage
          | GPU-Util  Compute M. |<br>|                                         |                      |               MIG
          M. |<br>|=========================================+======================+======================|<br>|   0  NVIDIA
          GeForce RTX 3090       WDDM | 00000000:1F:00.0  On |                  N/A
          |<br>| 53%   38C    P8               49W / 390W|    442MiB / 24576MiB |     11%      Default
          |<br>|                                         |                      |                  N/A
          |<br>+-----------------------------------------+----------------------+----------------------+</p>

          '
        raw: '

          +---------------------------------------------------------------------------------------+

          | NVIDIA-SMI 531.79                 Driver Version: 531.79       CUDA Version:
          12.1     |

          |-----------------------------------------+----------------------+----------------------+

          | GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile
          Uncorr. ECC |

          | Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute
          M. |

          |                                         |                      |               MIG
          M. |

          |=========================================+======================+======================|

          |   0  NVIDIA GeForce RTX 3090       WDDM | 00000000:1F:00.0  On |                  N/A
          |

          | 53%   38C    P8               49W / 390W|    442MiB / 24576MiB |     11%      Default
          |

          |                                         |                      |                  N/A
          |

          +-----------------------------------------+----------------------+----------------------+'
        updatedAt: '2023-07-10T03:30:54.816Z'
      numEdits: 0
      reactions: []
    id: 64ab7b6eeb47b35522d3082a
    type: comment
  author: Wazzz
  content: '

    +---------------------------------------------------------------------------------------+

    | NVIDIA-SMI 531.79                 Driver Version: 531.79       CUDA Version:
    12.1     |

    |-----------------------------------------+----------------------+----------------------+

    | GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr.
    ECC |

    | Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute
    M. |

    |                                         |                      |               MIG
    M. |

    |=========================================+======================+======================|

    |   0  NVIDIA GeForce RTX 3090       WDDM | 00000000:1F:00.0  On |                  N/A
    |

    | 53%   38C    P8               49W / 390W|    442MiB / 24576MiB |     11%      Default
    |

    |                                         |                      |                  N/A
    |

    +-----------------------------------------+----------------------+----------------------+'
  created_at: 2023-07-10 02:30:54+00:00
  edited: false
  hidden: false
  id: 64ab7b6eeb47b35522d3082a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9cbaff58244b1fb0907de4ffdba492ca.svg
      fullname: sun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: guoqingsimon
      type: user
    createdAt: '2023-07-14T02:28:14.000Z'
    data:
      edited: false
      editors:
      - guoqingsimon
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.8831824660301208
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9cbaff58244b1fb0907de4ffdba492ca.svg
          fullname: sun
          isHf: false
          isPro: false
          name: guoqingsimon
          type: user
        html: "<p>\u4F60\u597D\uFF0C\u5199\u4E00\u4E2A\u89C6\u89C9\u5F00\u53D1\u7A0B\
          \u5E8F</p>\n"
        raw: "\u4F60\u597D\uFF0C\u5199\u4E00\u4E2A\u89C6\u89C9\u5F00\u53D1\u7A0B\u5E8F"
        updatedAt: '2023-07-14T02:28:14.932Z'
      numEdits: 0
      reactions: []
    id: 64b0b2befaa55d9b614be854
    type: comment
  author: guoqingsimon
  content: "\u4F60\u597D\uFF0C\u5199\u4E00\u4E2A\u89C6\u89C9\u5F00\u53D1\u7A0B\u5E8F"
  created_at: 2023-07-14 01:28:14+00:00
  edited: false
  hidden: false
  id: 64b0b2befaa55d9b614be854
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: Error while loading Model :use AutoGPTO
