!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Satya93
conflicting_files: null
created_at: 2023-06-25 13:52:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-06-25T14:52:53.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7182070016860962
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<p>Hi, I can load this model using Oobabooga autogptq, but get the
          error in the title in a notebook. I''m using a rtx 4090 with 64gb of ram.
          Below is my command to load:</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(pretrained_model_dir, use_safetensors=True,<br>        model_basename=model_basename,low_cpu_mem_usage=True,device_map="auto",
          use_triton=False)</p>

          <p>Any clue what the problem may be?</p>

          '
        raw: "Hi, I can load this model using Oobabooga autogptq, but get the error\
          \ in the title in a notebook. I'm using a rtx 4090 with 64gb of ram. Below\
          \ is my command to load:\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(pretrained_model_dir,\
          \ use_safetensors=True,\\\r\n        model_basename=model_basename,low_cpu_mem_usage=True,device_map=\"\
          auto\", use_triton=False)\r\n\r\nAny clue what the problem may be?"
        updatedAt: '2023-06-25T14:52:53.980Z'
      numEdits: 0
      reactions: []
    id: 649854c5289f760ae8fc43be
    type: comment
  author: Satya93
  content: "Hi, I can load this model using Oobabooga autogptq, but get the error\
    \ in the title in a notebook. I'm using a rtx 4090 with 64gb of ram. Below is\
    \ my command to load:\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(pretrained_model_dir,\
    \ use_safetensors=True,\\\r\n        model_basename=model_basename,low_cpu_mem_usage=True,device_map=\"\
    auto\", use_triton=False)\r\n\r\nAny clue what the problem may be?"
  created_at: 2023-06-25 13:52:53+00:00
  edited: false
  hidden: false
  id: 649854c5289f760ae8fc43be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-06-25T14:58:51.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9594318270683289
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<p>EDIT: Ooobabooga actually didn''t work with Auto GPTQ, I had gotten
          it working with Exllama-so some issue with this model, at least for me,
          getting it going with Auto GPTQ. For reference I am using the compiled build
          0.3.0.dev0 from the cloned <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ">https://github.com/PanQiWei/AutoGPTQ</a>
          repo.</p>

          '
        raw: 'EDIT: Ooobabooga actually didn''t work with Auto GPTQ, I had gotten
          it working with Exllama-so some issue with this model, at least for me,
          getting it going with Auto GPTQ. For reference I am using the compiled build
          0.3.0.dev0 from the cloned https://github.com/PanQiWei/AutoGPTQ repo.'
        updatedAt: '2023-06-25T14:58:51.360Z'
      numEdits: 0
      reactions: []
    id: 6498562b6a7013b36933ac8b
    type: comment
  author: Satya93
  content: 'EDIT: Ooobabooga actually didn''t work with Auto GPTQ, I had gotten it
    working with Exllama-so some issue with this model, at least for me, getting it
    going with Auto GPTQ. For reference I am using the compiled build 0.3.0.dev0 from
    the cloned https://github.com/PanQiWei/AutoGPTQ repo.'
  created_at: 2023-06-25 13:58:51+00:00
  edited: false
  hidden: false
  id: 6498562b6a7013b36933ac8b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-06-25T16:27:39.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7843151688575745
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<p>Solved it! Set 150GB pagefile size AND removed low_cpu_mem_usage=True.
          The latter setting enabled me to load Tulu, avoiding the OOM error, but
          it doesn''t work for Wizard Vicuna 30B.</p>

          '
        raw: Solved it! Set 150GB pagefile size AND removed low_cpu_mem_usage=True.
          The latter setting enabled me to load Tulu, avoiding the OOM error, but
          it doesn't work for Wizard Vicuna 30B.
        updatedAt: '2023-06-25T16:27:39.079Z'
      numEdits: 0
      reactions: []
    id: 64986afbe486365ca6c27651
    type: comment
  author: Satya93
  content: Solved it! Set 150GB pagefile size AND removed low_cpu_mem_usage=True.
    The latter setting enabled me to load Tulu, avoiding the OOM error, but it doesn't
    work for Wizard Vicuna 30B.
  created_at: 2023-06-25 15:27:39+00:00
  edited: false
  hidden: false
  id: 64986afbe486365ca6c27651
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-25T16:41:53.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9976731538772583
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>That''s very odd, but glad it''s working for you now!</p>

          '
        raw: That's very odd, but glad it's working for you now!
        updatedAt: '2023-06-25T16:41:53.984Z'
      numEdits: 0
      reactions: []
    id: 64986e514391cd583ae50b41
    type: comment
  author: TheBloke
  content: That's very odd, but glad it's working for you now!
  created_at: 2023-06-25 15:41:53+00:00
  edited: false
  hidden: false
  id: 64986e514391cd583ae50b41
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-06-25T16:53:44.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9639790058135986
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<p>Thanks, it was a mystery! Crazy spike on ram usage on initialization!</p>

          '
        raw: Thanks, it was a mystery! Crazy spike on ram usage on initialization!
        updatedAt: '2023-06-25T16:53:44.389Z'
      numEdits: 0
      reactions: []
    id: 649871188fadf0ae763108f4
    type: comment
  author: Satya93
  content: Thanks, it was a mystery! Crazy spike on ram usage on initialization!
  created_at: 2023-06-25 15:53:44+00:00
  edited: false
  hidden: false
  id: 649871188fadf0ae763108f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-06-25T17:04:24.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7501713037490845
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<p>I get this on EVERY auto gptq loaded model:</p>

          <pre><code>   The model weights are not tied. Please use the `tie_weights`
          method before using the `infer_auto_device` function.

          </code></pre>

          <p>Then on establishing pipeline:</p>

          <pre><code> The model ''LlamaGPTQForCausalLM'' is not supported for text-generation.
          Supported models are.....

          </code></pre>

          <p>And this, on safetensors:</p>

          <p>The safetensors archive passed at E:\models\vicuna-33B-preview-GPTQ\vicuna-33b-preview-GPTQ-4bit--1g.act.order.safetensors
          does not contain metadata. Make sure to save your model with the <code>save_pretrained</code>
          method. Defaulting to ''pt'' metadata.</p>

          <p>If these are not real issues, can the warning messages be turned off?</p>

          <p>Thanks again!</p>

          '
        raw: "I get this on EVERY auto gptq loaded model:\n\n       The model weights\
          \ are not tied. Please use the `tie_weights` method before using the `infer_auto_device`\
          \ function.\n\nThen on establishing pipeline:\n \n     The model 'LlamaGPTQForCausalLM'\
          \ is not supported for text-generation. Supported models are.....\n\nAnd\
          \ this, on safetensors:\n\nThe safetensors archive passed at E:\\models\\\
          vicuna-33B-preview-GPTQ\\vicuna-33b-preview-GPTQ-4bit--1g.act.order.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\n\nIf these are not real issues,\
          \ can the warning messages be turned off?\n\nThanks again!"
        updatedAt: '2023-06-25T17:04:24.648Z'
      numEdits: 0
      reactions: []
    id: 6498739823d7604e8ebd3dfb
    type: comment
  author: Satya93
  content: "I get this on EVERY auto gptq loaded model:\n\n       The model weights\
    \ are not tied. Please use the `tie_weights` method before using the `infer_auto_device`\
    \ function.\n\nThen on establishing pipeline:\n \n     The model 'LlamaGPTQForCausalLM'\
    \ is not supported for text-generation. Supported models are.....\n\nAnd this,\
    \ on safetensors:\n\nThe safetensors archive passed at E:\\models\\vicuna-33B-preview-GPTQ\\\
    vicuna-33b-preview-GPTQ-4bit--1g.act.order.safetensors does not contain metadata.\
    \ Make sure to save your model with the `save_pretrained` method. Defaulting to\
    \ 'pt' metadata.\n\nIf these are not real issues, can the warning messages be\
    \ turned off?\n\nThanks again!"
  created_at: 2023-06-25 16:04:24+00:00
  edited: false
  hidden: false
  id: 6498739823d7604e8ebd3dfb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-25T17:08:30.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8375455141067505
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Yes they can all be ignored.</p>\n<p>The pipeline message comes\
          \ from Hugging Face transformers and there's nothing AutoGPTQ can do about\
          \ it. It can be blocked with this code:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> logging\n<span class=\"hljs-comment\"># Prevent printing\
          \ spurious transformers error when using pipeline with AutoGPTQ</span>\n\
          logging.set_verbosity(logging.CRITICAL)\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"</span>)\npipe\
          \ = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"</span>,\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span class=\"\
          hljs-number\">512</span>,\n    temperature=<span class=\"hljs-number\">0.7</span>,\n\
          \    top_p=<span class=\"hljs-number\">0.95</span>,\n    repetition_penalty=<span\
          \ class=\"hljs-number\">1.15</span>\n)\n\n<span class=\"hljs-built_in\"\
          >print</span>(pipe(prompt_template)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">'generated_text'</span>])\n</code></pre>\n<p>The\
          \ <code>safetensors</code> message comes from either transformers or safetensors\
          \ library, not sure which. I don't know how to block that message or if\
          \ it could be hidden by AutoGPTQ - possibly.</p>\n<p>The <code>model weights</code>\
          \ message comes from Accelerate, and I think could be prevented by AutoGPTQ.</p>\n\
          <p>You could raise this as an issue on the AutoGPTQ Github.</p>\n"
        raw: "Yes they can all be ignored.\n\nThe pipeline message comes from Hugging\
          \ Face transformers and there's nothing AutoGPTQ can do about it. It can\
          \ be blocked with this code:\n```python\nfrom transformers import logging\n\
          # Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\nprint(\"*** Pipeline:\"\
          )\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n\
          )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n```\n\nThe `safetensors`\
          \ message comes from either transformers or safetensors library, not sure\
          \ which. I don't know how to block that message or if it could be hidden\
          \ by AutoGPTQ - possibly.\n\nThe `model weights` message comes from Accelerate,\
          \ and I think could be prevented by AutoGPTQ.\n\nYou could raise this as\
          \ an issue on the AutoGPTQ Github."
        updatedAt: '2023-06-25T17:08:30.687Z'
      numEdits: 0
      reactions: []
    id: 6498748e5e95532769f64b83
    type: comment
  author: TheBloke
  content: "Yes they can all be ignored.\n\nThe pipeline message comes from Hugging\
    \ Face transformers and there's nothing AutoGPTQ can do about it. It can be blocked\
    \ with this code:\n```python\nfrom transformers import logging\n# Prevent printing\
    \ spurious transformers error when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
    \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n\
    \    tokenizer=tokenizer,\n    max_new_tokens=512,\n    temperature=0.7,\n   \
    \ top_p=0.95,\n    repetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    ```\n\nThe `safetensors` message comes from either transformers or safetensors\
    \ library, not sure which. I don't know how to block that message or if it could\
    \ be hidden by AutoGPTQ - possibly.\n\nThe `model weights` message comes from\
    \ Accelerate, and I think could be prevented by AutoGPTQ.\n\nYou could raise this\
    \ as an issue on the AutoGPTQ Github."
  created_at: 2023-06-25 16:08:30+00:00
  edited: false
  hidden: false
  id: 6498748e5e95532769f64b83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-06-25T17:35:37.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9207534790039062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<p>The transformers verbosity level command got rid of most of it,
          I''ll look into AutoGPTQ logging, thanks!!</p>

          '
        raw: The transformers verbosity level command got rid of most of it, I'll
          look into AutoGPTQ logging, thanks!!
        updatedAt: '2023-06-25T17:35:37.741Z'
      numEdits: 0
      reactions: []
    id: 64987ae9898757f7d157ccdc
    type: comment
  author: Satya93
  content: The transformers verbosity level command got rid of most of it, I'll look
    into AutoGPTQ logging, thanks!!
  created_at: 2023-06-25 16:35:37+00:00
  edited: false
  hidden: false
  id: 64987ae9898757f7d157ccdc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: 'NotImplementedError: Cannot copy out of meta tensor; no data!'
