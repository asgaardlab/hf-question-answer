!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nafnlaus
conflicting_files: null
created_at: 2023-06-20 14:56:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-06-20T15:56:24.000Z'
    data:
      edited: true
      editors:
      - Nafnlaus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6741905212402344
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: '<p>Before you dismiss this as "something is wrong with your setup":</p>

          <ul>

          <li>I can run other models without getting this</li>

          <li>It happens with multiple loaders (have tried exllama and rwkv)</li>

          <li>It happens on multiple of your models (have verified on this model and
          its 13B counterpart)</li>

          <li>When I mentioned it on the Oobabooga Discord, another person chimed
          up that they had the exact same problem with one of your models ("I got
          that illegal memory access error last night with ExLlama and TheBloke_WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ").</li>

          <li>My text-generation-webui is fully up to date.</li>

          </ul>

          <p>That out of the way: I can run this (great!) model at first on my RTX
          3090, but after a dozen or two generations, it switches to giving only:</p>

          <hr>

          <p>Exception occurred during processing of request from (''127.0.0.1'',
          57152)<br>Traceback (most recent call last):<br>  File "/usr/lib64/python3.10/socketserver.py",
          line 683, in process_request_thread<br>    self.finish_request(request,
          client_address)<br>  File "/usr/lib64/python3.10/socketserver.py", line
          360, in finish_request<br>    self.RequestHandlerClass(request, client_address,
          self)<br>  File "/usr/lib64/python3.10/socketserver.py", line 747, in <strong>init</strong><br>    self.handle()<br>  File
          "/usr/lib64/python3.10/http/server.py", line 433, in handle<br>    self.handle_one_request()<br>  File
          "/usr/lib64/python3.10/http/server.py", line 421, in handle_one_request<br>    method()<br>  File
          "/home/user/text-generation-webui/extensions/api/blocking_api.py", line
          86, in do_POST<br>    for a in generator:<br>  File "/home/user/text-generation-webui/modules/chat.py",
          line 317, in generate_chat_reply<br>    for history in chatbot_wrapper(text,
          history, state, regenerate=regenerate, _continue=_continue, loading_message=loading_message):<br>  File
          "/home/user/text-generation-webui/modules/chat.py", line 234, in chatbot_wrapper<br>    for
          j, reply in enumerate(generate_reply(prompt + cumulative_reply, state, eos_token=eos_token,
          stopping_strings=stopping_strings, is_chat=True)):<br>  File "/home/user/text-generation-webui/modules/text_generation.py",
          line 23, in generate_reply<br>    for result in _generate_reply(*args, **kwargs):<br>  File
          "/home/user/text-generation-webui/modules/text_generation.py", line 176,
          in _generate_reply<br>    clear_torch_cache()<br>  File "/home/user/text-generation-webui/modules/models.py",
          line 309, in clear_torch_cache<br>    torch.cuda.empty_cache()<br>  File
          "/home/user/.local/lib/python3.10/site-packages/torch/cuda/memory.py", line
          133, in empty_cache<br>    torch._C._cuda_emptyCache()<br>RuntimeError:
          CUDA error: an illegal memory access was encountered<br>Compile with <code>TORCH_USE_CUDA_DSA</code>
          to enable device-side assertions.</p>

          <hr>

          <p>I''ve tried running it in a number of ways, but for example: </p>

          <p>CUDA_VISIBLE_DEVICES=0 python server.py --listen --listen-port 1234 --loader
          exllama --model "TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ" --api --verbose</p>

          <p>For simplicity I''m calling it with the non-streaming api.py script,
          only minimally modified (different text provided, and set to make calls
          repeatedly in a loop).</p>

          <p>====<br>Also, and this is offtopic, but: the random seed, or setting
          seed to -1, seems to make no difference: given a fixed input, it seems to
          only want to make the same output.  Just an aside in case you have an easy
          solution.</p>

          '
        raw: "Before you dismiss this as \"something is wrong with your setup\":\n\
          \n* I can run other models without getting this\n* It happens with multiple\
          \ loaders (have tried exllama and rwkv)\n* It happens on multiple of your\
          \ models (have verified on this model and its 13B counterpart)\n* When I\
          \ mentioned it on the Oobabooga Discord, another person chimed up that they\
          \ had the exact same problem with one of your models (\"I got that illegal\
          \ memory access error last night with ExLlama and TheBloke_WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ\"\
          ).\n* My text-generation-webui is fully up to date.\n\nThat out of the way:\
          \ I can run this (great!) model at first on my RTX 3090, but after a dozen\
          \ or two generations, it switches to giving only:\n\n----------------------------------------\
          \      \nException occurred during processing of request from ('127.0.0.1',\
          \ 57152)\nTraceback (most recent call last):\n  File \"/usr/lib64/python3.10/socketserver.py\"\
          , line 683, in process_request_thread\n    self.finish_request(request,\
          \ client_address)\n  File \"/usr/lib64/python3.10/socketserver.py\", line\
          \ 360, in finish_request\n    self.RequestHandlerClass(request, client_address,\
          \ self)\n  File \"/usr/lib64/python3.10/socketserver.py\", line 747, in\
          \ __init__\n    self.handle()\n  File \"/usr/lib64/python3.10/http/server.py\"\
          , line 433, in handle\n    self.handle_one_request()\n  File \"/usr/lib64/python3.10/http/server.py\"\
          , line 421, in handle_one_request\n    method()\n  File \"/home/user/text-generation-webui/extensions/api/blocking_api.py\"\
          , line 86, in do_POST\n    for a in generator:\n  File \"/home/user/text-generation-webui/modules/chat.py\"\
          , line 317, in generate_chat_reply\n    for history in chatbot_wrapper(text,\
          \ history, state, regenerate=regenerate, _continue=_continue, loading_message=loading_message):\n\
          \  File \"/home/user/text-generation-webui/modules/chat.py\", line 234,\
          \ in chatbot_wrapper\n    for j, reply in enumerate(generate_reply(prompt\
          \ + cumulative_reply, state, eos_token=eos_token, stopping_strings=stopping_strings,\
          \ is_chat=True)):\n  File \"/home/user/text-generation-webui/modules/text_generation.py\"\
          , line 23, in generate_reply\n    for result in _generate_reply(*args, **kwargs):\n\
          \  File \"/home/user/text-generation-webui/modules/text_generation.py\"\
          , line 176, in _generate_reply\n    clear_torch_cache()\n  File \"/home/user/text-generation-webui/modules/models.py\"\
          , line 309, in clear_torch_cache\n    torch.cuda.empty_cache()\n  File \"\
          /home/user/.local/lib/python3.10/site-packages/torch/cuda/memory.py\", line\
          \ 133, in empty_cache\n    torch._C._cuda_emptyCache()\nRuntimeError: CUDA\
          \ error: an illegal memory access was encountered\nCompile with `TORCH_USE_CUDA_DSA`\
          \ to enable device-side assertions.\n\n----------------------------------------\
          \      \n\nI've tried running it in a number of ways, but for example: \n\
          \nCUDA_VISIBLE_DEVICES=0 python server.py --listen --listen-port 1234 --loader\
          \ exllama --model \"TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\" --api --verbose\n\
          \nFor simplicity I'm calling it with the non-streaming api.py script, only\
          \ minimally modified (different text provided, and set to make calls repeatedly\
          \ in a loop).\n\n====\nAlso, and this is offtopic, but: the random seed,\
          \ or setting seed to -1, seems to make no difference: given a fixed input,\
          \ it seems to only want to make the same output.  Just an aside in case\
          \ you have an easy solution."
        updatedAt: '2023-06-20T16:02:28.334Z'
      numEdits: 1
      reactions: []
    id: 6491cc2887315e332ce89abf
    type: comment
  author: Nafnlaus
  content: "Before you dismiss this as \"something is wrong with your setup\":\n\n\
    * I can run other models without getting this\n* It happens with multiple loaders\
    \ (have tried exllama and rwkv)\n* It happens on multiple of your models (have\
    \ verified on this model and its 13B counterpart)\n* When I mentioned it on the\
    \ Oobabooga Discord, another person chimed up that they had the exact same problem\
    \ with one of your models (\"I got that illegal memory access error last night\
    \ with ExLlama and TheBloke_WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ\"\
    ).\n* My text-generation-webui is fully up to date.\n\nThat out of the way: I\
    \ can run this (great!) model at first on my RTX 3090, but after a dozen or two\
    \ generations, it switches to giving only:\n\n----------------------------------------\
    \      \nException occurred during processing of request from ('127.0.0.1', 57152)\n\
    Traceback (most recent call last):\n  File \"/usr/lib64/python3.10/socketserver.py\"\
    , line 683, in process_request_thread\n    self.finish_request(request, client_address)\n\
    \  File \"/usr/lib64/python3.10/socketserver.py\", line 360, in finish_request\n\
    \    self.RequestHandlerClass(request, client_address, self)\n  File \"/usr/lib64/python3.10/socketserver.py\"\
    , line 747, in __init__\n    self.handle()\n  File \"/usr/lib64/python3.10/http/server.py\"\
    , line 433, in handle\n    self.handle_one_request()\n  File \"/usr/lib64/python3.10/http/server.py\"\
    , line 421, in handle_one_request\n    method()\n  File \"/home/user/text-generation-webui/extensions/api/blocking_api.py\"\
    , line 86, in do_POST\n    for a in generator:\n  File \"/home/user/text-generation-webui/modules/chat.py\"\
    , line 317, in generate_chat_reply\n    for history in chatbot_wrapper(text, history,\
    \ state, regenerate=regenerate, _continue=_continue, loading_message=loading_message):\n\
    \  File \"/home/user/text-generation-webui/modules/chat.py\", line 234, in chatbot_wrapper\n\
    \    for j, reply in enumerate(generate_reply(prompt + cumulative_reply, state,\
    \ eos_token=eos_token, stopping_strings=stopping_strings, is_chat=True)):\n  File\
    \ \"/home/user/text-generation-webui/modules/text_generation.py\", line 23, in\
    \ generate_reply\n    for result in _generate_reply(*args, **kwargs):\n  File\
    \ \"/home/user/text-generation-webui/modules/text_generation.py\", line 176, in\
    \ _generate_reply\n    clear_torch_cache()\n  File \"/home/user/text-generation-webui/modules/models.py\"\
    , line 309, in clear_torch_cache\n    torch.cuda.empty_cache()\n  File \"/home/user/.local/lib/python3.10/site-packages/torch/cuda/memory.py\"\
    , line 133, in empty_cache\n    torch._C._cuda_emptyCache()\nRuntimeError: CUDA\
    \ error: an illegal memory access was encountered\nCompile with `TORCH_USE_CUDA_DSA`\
    \ to enable device-side assertions.\n\n----------------------------------------\
    \      \n\nI've tried running it in a number of ways, but for example: \n\nCUDA_VISIBLE_DEVICES=0\
    \ python server.py --listen --listen-port 1234 --loader exllama --model \"TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\"\
    \ --api --verbose\n\nFor simplicity I'm calling it with the non-streaming api.py\
    \ script, only minimally modified (different text provided, and set to make calls\
    \ repeatedly in a loop).\n\n====\nAlso, and this is offtopic, but: the random\
    \ seed, or setting seed to -1, seems to make no difference: given a fixed input,\
    \ it seems to only want to make the same output.  Just an aside in case you have\
    \ an easy solution."
  created_at: 2023-06-20 14:56:24+00:00
  edited: true
  hidden: false
  id: 6491cc2887315e332ce89abf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-20T16:16:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9157688021659851
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK, interesting.  I''m confused by you saying you tested it with
          the RWKV loader? Isn''t that just for RWKV models?  If not can you link
          me to info on using that loader for GPTQ models so I can see how to test
          it.</p>

          <p>Otherwise it looks most like an ExLlama specific problem.  I haven''t
          tested this model specifically with ExLlama, though I see that the 13B model
          is shown as compatible on <a rel="nofollow" href="https://github.com/turboderp/exllama/blob/master/doc/model_compatibility.md">ExLlama''s
          model compatibility list</a>.</p>

          <p>Let me know regarding RKWV so I can understand if this is exclusive to
          ExLlama or not.</p>

          '
        raw: 'OK, interesting.  I''m confused by you saying you tested it with the
          RWKV loader? Isn''t that just for RWKV models?  If not can you link me to
          info on using that loader for GPTQ models so I can see how to test it.


          Otherwise it looks most like an ExLlama specific problem.  I haven''t tested
          this model specifically with ExLlama, though I see that the 13B model is
          shown as compatible on [ExLlama''s model compatibility list](https://github.com/turboderp/exllama/blob/master/doc/model_compatibility.md).


          Let me know regarding RKWV so I can understand if this is exclusive to ExLlama
          or not.'
        updatedAt: '2023-06-20T16:16:51.456Z'
      numEdits: 0
      reactions: []
    id: 6491d0f3ee0827f06c19dd5c
    type: comment
  author: TheBloke
  content: 'OK, interesting.  I''m confused by you saying you tested it with the RWKV
    loader? Isn''t that just for RWKV models?  If not can you link me to info on using
    that loader for GPTQ models so I can see how to test it.


    Otherwise it looks most like an ExLlama specific problem.  I haven''t tested this
    model specifically with ExLlama, though I see that the 13B model is shown as compatible
    on [ExLlama''s model compatibility list](https://github.com/turboderp/exllama/blob/master/doc/model_compatibility.md).


    Let me know regarding RKWV so I can understand if this is exclusive to ExLlama
    or not.'
  created_at: 2023-06-20 15:16:51+00:00
  edited: false
  hidden: false
  id: 6491d0f3ee0827f06c19dd5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-06-20T17:25:17.000Z'
    data:
      edited: false
      editors:
      - Nafnlaus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9871193766593933
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: '<p>I don''t have experience on the technical side with loaders (they
          didn''t even exist back when I had been using the UI previously). All I
          did was specify --loader rwkv and the model loaded and ran.  But experienced
          the same problems.  Perhaps it just fell back to exllama and this might
          be an exllama issue?   The other user reporting the same problem was using
          exllama as well.</p>

          '
        raw: I don't have experience on the technical side with loaders (they didn't
          even exist back when I had been using the UI previously). All I did was
          specify --loader rwkv and the model loaded and ran.  But experienced the
          same problems.  Perhaps it just fell back to exllama and this might be an
          exllama issue?   The other user reporting the same problem was using exllama
          as well.
        updatedAt: '2023-06-20T17:25:17.864Z'
      numEdits: 0
      reactions: []
    id: 6491e0fd9ea0fccb034acd4a
    type: comment
  author: Nafnlaus
  content: I don't have experience on the technical side with loaders (they didn't
    even exist back when I had been using the UI previously). All I did was specify
    --loader rwkv and the model loaded and ran.  But experienced the same problems.  Perhaps
    it just fell back to exllama and this might be an exllama issue?   The other user
    reporting the same problem was using exllama as well.
  created_at: 2023-06-20 16:25:17+00:00
  edited: false
  hidden: false
  id: 6491e0fd9ea0fccb034acd4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-20T17:27:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9661316275596619
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK yeah I think it would have ignored <code>--loader rkwv</code>
          as that''s only for RWKV models. </p>

          <p>Leave it with me and I''ll test it myself.  If I can re-create it with
          ExLlama I''ll raise an issue on the ExLlama github.</p>

          <p>Have you tried --loader autogptq?</p>

          '
        raw: "OK yeah I think it would have ignored `--loader rkwv` as that's only\
          \ for RWKV models. \n\nLeave it with me and I'll test it myself.  If I can\
          \ re-create it with ExLlama I'll raise an issue on the ExLlama github.\n\
          \nHave you tried --loader autogptq?"
        updatedAt: '2023-06-20T17:27:51.848Z'
      numEdits: 0
      reactions: []
    id: 6491e19788235ca49b7e1856
    type: comment
  author: TheBloke
  content: "OK yeah I think it would have ignored `--loader rkwv` as that's only for\
    \ RWKV models. \n\nLeave it with me and I'll test it myself.  If I can re-create\
    \ it with ExLlama I'll raise an issue on the ExLlama github.\n\nHave you tried\
    \ --loader autogptq?"
  created_at: 2023-06-20 16:27:51+00:00
  edited: false
  hidden: false
  id: 6491e19788235ca49b7e1856
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-06-20T18:31:49.000Z'
    data:
      edited: false
      editors:
      - Nafnlaus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9885925054550171
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: '<p>Yeah, autogptq didn''t run.   I''d need to debug what requirements
          or whatnot it''s wanting.  I could ask the other user who experienced this
          problem to do so if you think it''d help.</p>

          '
        raw: Yeah, autogptq didn't run.   I'd need to debug what requirements or whatnot
          it's wanting.  I could ask the other user who experienced this problem to
          do so if you think it'd help.
        updatedAt: '2023-06-20T18:31:49.125Z'
      numEdits: 0
      reactions: []
    id: 6491f0959efe870cc87ba702
    type: comment
  author: Nafnlaus
  content: Yeah, autogptq didn't run.   I'd need to debug what requirements or whatnot
    it's wanting.  I could ask the other user who experienced this problem to do so
    if you think it'd help.
  created_at: 2023-06-20 17:31:49+00:00
  edited: false
  hidden: false
  id: 6491f0959efe870cc87ba702
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-06-20T18:41:39.000Z'
    data:
      edited: false
      editors:
      - Nafnlaus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: '<p>ED: the autogptq error was:</p>

          <p>2023-06-20 18:40:41 INFO:Loading settings from settings.json...<br>2023-06-20
          18:40:41 INFO:Loading TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ...<br>2023-06-20
          18:40:42 INFO:The AutoGPTQ params are: {''model_basename'': ''Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order'',
          ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'':
          True, ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
          False, ''max_memory'': None, ''quantize_config'': None}<br>Traceback (most
          recent call last):<br>  File "/home/user/text-generation-webui/server.py",
          line 1003, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "/home/user/text-generation-webui/modules/models.py", line 65, in load_model<br>    output
          = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "/home/user/text-generation-webui/modules/models.py", line 271, in AutoGPTQ_loader<br>    return
          modules.AutoGPTQ_loader.load_quantized(model_name)<br>  File "/home/user/text-generation-webui/modules/AutoGPTQ_loader.py",
          line 55, in load_quantized<br>    model = AutoGPTQForCausalLM.from_quantized(path_to_model,
          **params)<br>  File "/home/user/.local/lib/python3.10/site-packages/auto_gptq/modeling/auto.py",
          line 79, in from_quantized<br>    model_type = check_and_get_model_type(save_dir
          or model_name_or_path, trust_remote_code)<br>  File "/home/user/.local/lib/python3.10/site-packages/auto_gptq/modeling/_utils.py",
          line 125, in check_and_get_model_type<br>    raise TypeError(f"{config.model_type}
          isn''t supported yet.")<br>TypeError: llama isn''t supported yet.</p>

          '
        raw: "ED: the autogptq error was:\n\n2023-06-20 18:40:41 INFO:Loading settings\
          \ from settings.json...\n2023-06-20 18:40:41 INFO:Loading TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ...\n\
          2023-06-20 18:40:42 INFO:The AutoGPTQ params are: {'model_basename': 'Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order',\
          \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': None, 'quantize_config': None}\nTraceback (most recent\
          \ call last):\n  File \"/home/user/text-generation-webui/server.py\", line\
          \ 1003, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"/home/user/text-generation-webui/modules/models.py\", line 65,\
          \ in load_model\n    output = load_func_map[loader](model_name)\n  File\
          \ \"/home/user/text-generation-webui/modules/models.py\", line 271, in AutoGPTQ_loader\n\
          \    return modules.AutoGPTQ_loader.load_quantized(model_name)\n  File \"\
          /home/user/text-generation-webui/modules/AutoGPTQ_loader.py\", line 55,\
          \ in load_quantized\n    model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params)\n  File \"/home/user/.local/lib/python3.10/site-packages/auto_gptq/modeling/auto.py\"\
          , line 79, in from_quantized\n    model_type = check_and_get_model_type(save_dir\
          \ or model_name_or_path, trust_remote_code)\n  File \"/home/user/.local/lib/python3.10/site-packages/auto_gptq/modeling/_utils.py\"\
          , line 125, in check_and_get_model_type\n    raise TypeError(f\"{config.model_type}\
          \ isn't supported yet.\")\nTypeError: llama isn't supported yet."
        updatedAt: '2023-06-20T18:41:39.144Z'
      numEdits: 0
      reactions: []
    id: 6491f2e3fe1c0c0843de0a95
    type: comment
  author: Nafnlaus
  content: "ED: the autogptq error was:\n\n2023-06-20 18:40:41 INFO:Loading settings\
    \ from settings.json...\n2023-06-20 18:40:41 INFO:Loading TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ...\n\
    2023-06-20 18:40:42 INFO:The AutoGPTQ params are: {'model_basename': 'Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order',\
    \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp':\
    \ True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory': None,\
    \ 'quantize_config': None}\nTraceback (most recent call last):\n  File \"/home/user/text-generation-webui/server.py\"\
    , line 1003, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    \  File \"/home/user/text-generation-webui/modules/models.py\", line 65, in load_model\n\
    \    output = load_func_map[loader](model_name)\n  File \"/home/user/text-generation-webui/modules/models.py\"\
    , line 271, in AutoGPTQ_loader\n    return modules.AutoGPTQ_loader.load_quantized(model_name)\n\
    \  File \"/home/user/text-generation-webui/modules/AutoGPTQ_loader.py\", line\
    \ 55, in load_quantized\n    model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
    \ **params)\n  File \"/home/user/.local/lib/python3.10/site-packages/auto_gptq/modeling/auto.py\"\
    , line 79, in from_quantized\n    model_type = check_and_get_model_type(save_dir\
    \ or model_name_or_path, trust_remote_code)\n  File \"/home/user/.local/lib/python3.10/site-packages/auto_gptq/modeling/_utils.py\"\
    , line 125, in check_and_get_model_type\n    raise TypeError(f\"{config.model_type}\
    \ isn't supported yet.\")\nTypeError: llama isn't supported yet."
  created_at: 2023-06-20 17:41:39+00:00
  edited: false
  hidden: false
  id: 6491f2e3fe1c0c0843de0a95
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
      fullname: "Karen R\xF3bertsd\xF3ttir"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nafnlaus
      type: user
    createdAt: '2023-06-22T19:04:11.000Z'
    data:
      edited: false
      editors:
      - Nafnlaus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9173187613487244
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1356365d9e233708d1b7388827b4e0e2.svg
          fullname: "Karen R\xF3bertsd\xF3ttir"
          isHf: false
          isPro: false
          name: Nafnlaus
          type: user
        html: '<p>(For the record, I have a viable, though slightly annoying workaround
          going, which is: whenever the API calls return an error, it automatically
          kills the server, which is running in a while loop, so it starts back up,
          and the script can continue  ;)  )</p>

          '
        raw: '(For the record, I have a viable, though slightly annoying workaround
          going, which is: whenever the API calls return an error, it automatically
          kills the server, which is running in a while loop, so it starts back up,
          and the script can continue  ;)  )'
        updatedAt: '2023-06-22T19:04:11.842Z'
      numEdits: 0
      reactions: []
    id: 64949b2b218decddb622a148
    type: comment
  author: Nafnlaus
  content: '(For the record, I have a viable, though slightly annoying workaround
    going, which is: whenever the API calls return an error, it automatically kills
    the server, which is running in a while loop, so it starts back up, and the script
    can continue  ;)  )'
  created_at: 2023-06-22 18:04:11+00:00
  edited: false
  hidden: false
  id: 64949b2b218decddb622a148
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: CUDA error: an illegal memory access was encountered'
