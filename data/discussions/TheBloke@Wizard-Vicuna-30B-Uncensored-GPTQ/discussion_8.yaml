!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Pb-207
conflicting_files: null
created_at: 2023-06-03 16:44:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679033974295-noauth.png?w=200&h=200&f=face
      fullname: Pb-207
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pb-207
      type: user
    createdAt: '2023-06-03T17:44:07.000Z'
    data:
      edited: false
      editors:
      - Pb-207
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6799138784408569
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679033974295-noauth.png?w=200&h=200&f=face
          fullname: Pb-207
          isHf: false
          isPro: false
          name: Pb-207
          type: user
        html: '<p>This model is the only one pass boiling-water-obtuse-angle test
          among tested open-source models. Even better than falcon-40b<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/6414068157d68cf865dda21c/RhfR6_f3v5AxxUO6R5U2X.png"><img
          alt="Text generation web UI - Google Chrome 2023_6_3 2(1)__01.png" src="https://cdn-uploads.huggingface.co/production/uploads/6414068157d68cf865dda21c/RhfR6_f3v5AxxUO6R5U2X.png"></a><br>And
          can run on single 4090 with satisfying speed (about 8 token/s on win11)</p>

          '
        raw: "This model is the only one pass boiling-water-obtuse-angle test among\
          \ tested open-source models. Even better than falcon-40b\r\n![Text generation\
          \ web UI - Google Chrome 2023_6_3 2(1)__01.png](https://cdn-uploads.huggingface.co/production/uploads/6414068157d68cf865dda21c/RhfR6_f3v5AxxUO6R5U2X.png)\r\
          \nAnd can run on single 4090 with satisfying speed (about 8 token/s on win11)"
        updatedAt: '2023-06-03T17:44:07.093Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F917"
        users:
        - chraac
        - Mansib
        - joon09
    id: 647b7be76a79fbf5e99770e8
    type: comment
  author: Pb-207
  content: "This model is the only one pass boiling-water-obtuse-angle test among\
    \ tested open-source models. Even better than falcon-40b\r\n![Text generation\
    \ web UI - Google Chrome 2023_6_3 2(1)__01.png](https://cdn-uploads.huggingface.co/production/uploads/6414068157d68cf865dda21c/RhfR6_f3v5AxxUO6R5U2X.png)\r\
    \nAnd can run on single 4090 with satisfying speed (about 8 token/s on win11)"
  created_at: 2023-06-03 16:44:07+00:00
  edited: false
  hidden: false
  id: 647b7be76a79fbf5e99770e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-05T10:16:10.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9520236253738403
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Excellent! Thanks for reporting. That''s a good test!</p>

          '
        raw: Excellent! Thanks for reporting. That's a good test!
        updatedAt: '2023-06-05T10:16:10.557Z'
      numEdits: 0
      reactions: []
    id: 647db5ea32c471a7fa84738f
    type: comment
  author: TheBloke
  content: Excellent! Thanks for reporting. That's a good test!
  created_at: 2023-06-05 09:16:10+00:00
  edited: false
  hidden: false
  id: 647db5ea32c471a7fa84738f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/47dd86f5a66bab33152d215b0c4b74ef.svg
      fullname: Prakash M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Prakash1206
      type: user
    createdAt: '2023-06-05T16:33:39.000Z'
    data:
      edited: false
      editors:
      - Prakash1206
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8277333378791809
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/47dd86f5a66bab33152d215b0c4b74ef.svg
          fullname: Prakash M
          isHf: false
          isPro: false
          name: Prakash1206
          type: user
        html: '<p>May i know memory requirement to run this model on GPU.<br>i assume
          min 17GB video memory.<br>currently i''m on 32GB Ram with GTX 1080ti (11GB
          Vid Memory)<br>when i turn to load this model on text-generation-webui,
          it fills ram (bit of mem swapping) then it crashes.<br>may i know how much
          RAM is required to run this and is it possible to run only on CPU mode (selecting
          cpu under model settings in text-generatino-webui didn''t help)</p>

          <p>following model works fine on GPU<br>TheBloke_stable-vicuna-13B-GPTQ<br>Getting
          about 8tokens/sec<br>tried same question as in this thread</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63922ee773bf5173e3630448/pzTiVZFG_0zIbv18lsz5s.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/63922ee773bf5173e3630448/pzTiVZFG_0zIbv18lsz5s.png"></a></p>

          <p>Thanks</p>

          '
        raw: "May i know memory requirement to run this model on GPU.\ni assume min\
          \ 17GB video memory.\ncurrently i'm on 32GB Ram with GTX 1080ti (11GB Vid\
          \ Memory)\nwhen i turn to load this model on text-generation-webui, it fills\
          \ ram (bit of mem swapping) then it crashes. \nmay i know how much RAM is\
          \ required to run this and is it possible to run only on CPU mode (selecting\
          \ cpu under model settings in text-generatino-webui didn't help)\n\nfollowing\
          \ model works fine on GPU\nTheBloke_stable-vicuna-13B-GPTQ\nGetting about\
          \ 8tokens/sec\ntried same question as in this thread\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63922ee773bf5173e3630448/pzTiVZFG_0zIbv18lsz5s.png)\n\
          \nThanks"
        updatedAt: '2023-06-05T16:33:39.005Z'
      numEdits: 0
      reactions: []
    id: 647e0e635214d172cbbe39af
    type: comment
  author: Prakash1206
  content: "May i know memory requirement to run this model on GPU.\ni assume min\
    \ 17GB video memory.\ncurrently i'm on 32GB Ram with GTX 1080ti (11GB Vid Memory)\n\
    when i turn to load this model on text-generation-webui, it fills ram (bit of\
    \ mem swapping) then it crashes. \nmay i know how much RAM is required to run\
    \ this and is it possible to run only on CPU mode (selecting cpu under model settings\
    \ in text-generatino-webui didn't help)\n\nfollowing model works fine on GPU\n\
    TheBloke_stable-vicuna-13B-GPTQ\nGetting about 8tokens/sec\ntried same question\
    \ as in this thread\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63922ee773bf5173e3630448/pzTiVZFG_0zIbv18lsz5s.png)\n\
    \nThanks"
  created_at: 2023-06-05 15:33:39+00:00
  edited: false
  hidden: false
  id: 647e0e635214d172cbbe39af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/09de569b8f981a079c613c0cda5db9b6.svg
      fullname: Hongrui Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chraac
      type: user
    createdAt: '2023-06-08T15:37:32.000Z'
    data:
      edited: false
      editors:
      - chraac
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6079448461532593
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/09de569b8f981a079c613c0cda5db9b6.svg
          fullname: Hongrui Chen
          isHf: false
          isPro: false
          name: chraac
          type: user
        html: '<p>Got different result with a littel bit wording change, :)<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644cbf7076c0ab1880b224f6/Cn1YLlGUv7tih7ANk9aSJ.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644cbf7076c0ab1880b224f6/Cn1YLlGUv7tih7ANk9aSJ.png"></a></p>

          '
        raw: 'Got different result with a littel bit wording change, :)

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644cbf7076c0ab1880b224f6/Cn1YLlGUv7tih7ANk9aSJ.png)'
        updatedAt: '2023-06-08T15:37:32.014Z'
      numEdits: 0
      reactions: []
    id: 6481f5bc9135a74ca53f8966
    type: comment
  author: chraac
  content: 'Got different result with a littel bit wording change, :)

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644cbf7076c0ab1880b224f6/Cn1YLlGUv7tih7ANk9aSJ.png)'
  created_at: 2023-06-08 14:37:32+00:00
  edited: false
  hidden: false
  id: 6481f5bc9135a74ca53f8966
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679033974295-noauth.png?w=200&h=200&f=face
      fullname: Pb-207
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pb-207
      type: user
    createdAt: '2023-06-09T11:46:44.000Z'
    data:
      edited: false
      editors:
      - Pb-207
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8514370322227478
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679033974295-noauth.png?w=200&h=200&f=face
          fullname: Pb-207
          isHf: false
          isPro: false
          name: Pb-207
          type: user
        html: '<blockquote>

          <p>May i know memory requirement to run this model on GPU.<br>i assume min
          17GB video memory.<br>currently i''m on 32GB Ram with GTX 1080ti (11GB Vid
          Memory)<br>when i turn to load this model on text-generation-webui, it fills
          ram (bit of mem swapping) then it crashes.<br>may i know how much RAM is
          required to run this and is it possible to run only on CPU mode (selecting
          cpu under model settings in text-generatino-webui didn''t help)</p>

          <p>following model works fine on GPU<br>TheBloke_stable-vicuna-13B-GPTQ<br>Getting
          about 8tokens/sec<br>tried same question as in this thread</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63922ee773bf5173e3630448/pzTiVZFG_0zIbv18lsz5s.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/63922ee773bf5173e3630448/pzTiVZFG_0zIbv18lsz5s.png"></a></p>

          <p>Thanks</p>

          </blockquote>

          <p>You need to choose ggml version to run on cpu,  GPTQ is only for GPU.
          This model requires at least 18G to load, and the usage of vram will increase
          to 21G after several chats, so i suggest using GPU with at least 24G VRAM.<br>You
          need to confine the usage of  VRAM and leave some VRAM for chat to get rid
          of "CUDA: OUT OF MEMEROY". In oob-webui that''s --gpu-memory 8 (8 is an
          example) This will decrease the generating speed, but decrease the demand
          of VRAM.</p>

          '
        raw: "> May i know memory requirement to run this model on GPU.\n> i assume\
          \ min 17GB video memory.\n> currently i'm on 32GB Ram with GTX 1080ti (11GB\
          \ Vid Memory)\n> when i turn to load this model on text-generation-webui,\
          \ it fills ram (bit of mem swapping) then it crashes. \n> may i know how\
          \ much RAM is required to run this and is it possible to run only on CPU\
          \ mode (selecting cpu under model settings in text-generatino-webui didn't\
          \ help)\n> \n> following model works fine on GPU\n> TheBloke_stable-vicuna-13B-GPTQ\n\
          > Getting about 8tokens/sec\n> tried same question as in this thread\n>\
          \ \n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63922ee773bf5173e3630448/pzTiVZFG_0zIbv18lsz5s.png)\n\
          > \n> Thanks\n\nYou need to choose ggml version to run on cpu,  GPTQ is\
          \ only for GPU. This model requires at least 18G to load, and the usage\
          \ of vram will increase to 21G after several chats, so i suggest using GPU\
          \ with at least 24G VRAM. \nYou need to confine the usage of  VRAM and leave\
          \ some VRAM for chat to get rid of \"CUDA: OUT OF MEMEROY\". In oob-webui\
          \ that's --gpu-memory 8 (8 is an example) This will decrease the generating\
          \ speed, but decrease the demand of VRAM."
        updatedAt: '2023-06-09T11:46:44.061Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Prakash1206
    id: 648311242e71b42e887c27ed
    type: comment
  author: Pb-207
  content: "> May i know memory requirement to run this model on GPU.\n> i assume\
    \ min 17GB video memory.\n> currently i'm on 32GB Ram with GTX 1080ti (11GB Vid\
    \ Memory)\n> when i turn to load this model on text-generation-webui, it fills\
    \ ram (bit of mem swapping) then it crashes. \n> may i know how much RAM is required\
    \ to run this and is it possible to run only on CPU mode (selecting cpu under\
    \ model settings in text-generatino-webui didn't help)\n> \n> following model\
    \ works fine on GPU\n> TheBloke_stable-vicuna-13B-GPTQ\n> Getting about 8tokens/sec\n\
    > tried same question as in this thread\n> \n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63922ee773bf5173e3630448/pzTiVZFG_0zIbv18lsz5s.png)\n\
    > \n> Thanks\n\nYou need to choose ggml version to run on cpu,  GPTQ is only for\
    \ GPU. This model requires at least 18G to load, and the usage of vram will increase\
    \ to 21G after several chats, so i suggest using GPU with at least 24G VRAM. \n\
    You need to confine the usage of  VRAM and leave some VRAM for chat to get rid\
    \ of \"CUDA: OUT OF MEMEROY\". In oob-webui that's --gpu-memory 8 (8 is an example)\
    \ This will decrease the generating speed, but decrease the demand of VRAM."
  created_at: 2023-06-09 10:46:44+00:00
  edited: false
  hidden: false
  id: 648311242e71b42e887c27ed
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: Awesome model !!!
