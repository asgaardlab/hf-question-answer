!!python/object:huggingface_hub.community.DiscussionWithDetails
author: naptaps
conflicting_files: null
created_at: 2023-06-23 16:53:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7a772b98c9a54c5b52dbeef1753977ae.svg
      fullname: Lars M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: naptaps
      type: user
    createdAt: '2023-06-23T17:53:57.000Z'
    data:
      edited: false
      editors:
      - naptaps
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8281437754631042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7a772b98c9a54c5b52dbeef1753977ae.svg
          fullname: Lars M
          isHf: false
          isPro: false
          name: naptaps
          type: user
        html: '<p>Allways getting this Error while loading the Model in oobabooga.</p>

          <p>WARNING:The safetensors archive passed at models\TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order.safetensors
          does not contain metadata. Make sure to save your model with the <code>save_pretrained</code>
          method. Defaulting to ''pt'' metadata.</p>

          <p>and after that it just shuts down.</p>

          <p>Tried to update and reinstalled the launcher and loaded the model different
          times manually or with the interface.<br>tried it with the default model
          loader and model loader set up to Autogpt but didnt changed anything either.</p>

          '
        raw: "Allways getting this Error while loading the Model in oobabooga.\r\n\
          \r\nWARNING:The safetensors archive passed at models\\TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\\\
          Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order.safetensors does not contain\
          \ metadata. Make sure to save your model with the `save_pretrained` method.\
          \ Defaulting to 'pt' metadata.\r\n\r\nand after that it just shuts down.\r\
          \n\r\nTried to update and reinstalled the launcher and loaded the model\
          \ different times manually or with the interface. \r\ntried it with the\
          \ default model loader and model loader set up to Autogpt but didnt changed\
          \ anything either."
        updatedAt: '2023-06-23T17:53:57.653Z'
      numEdits: 0
      reactions: []
    id: 6495dc354429f385c2825d86
    type: comment
  author: naptaps
  content: "Allways getting this Error while loading the Model in oobabooga.\r\n\r\
    \nWARNING:The safetensors archive passed at models\\TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\\\
    Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order.safetensors does not contain\
    \ metadata. Make sure to save your model with the `save_pretrained` method. Defaulting\
    \ to 'pt' metadata.\r\n\r\nand after that it just shuts down.\r\n\r\nTried to\
    \ update and reinstalled the launcher and loaded the model different times manually\
    \ or with the interface. \r\ntried it with the default model loader and model\
    \ loader set up to Autogpt but didnt changed anything either."
  created_at: 2023-06-23 16:53:57+00:00
  edited: false
  hidden: false
  id: 6495dc354429f385c2825d86
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03d6e699256f638a0dcabb9214b72782.svg
      fullname: Kimi Morgam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kimimorgam
      type: user
    createdAt: '2023-06-23T18:42:11.000Z'
    data:
      edited: false
      editors:
      - kimimorgam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9159225225448608
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03d6e699256f638a0dcabb9214b72782.svg
          fullname: Kimi Morgam
          isHf: false
          isPro: false
          name: kimimorgam
          type: user
        html: '<p>i`m having this issue too.</p>

          <p>i try everthing in options, but only errors :/</p>

          '
        raw: 'i`m having this issue too.


          i try everthing in options, but only errors :/'
        updatedAt: '2023-06-23T18:42:11.568Z'
      numEdits: 0
      reactions: []
    id: 6495e783990b342dcc92acf5
    type: comment
  author: kimimorgam
  content: 'i`m having this issue too.


    i try everthing in options, but only errors :/'
  created_at: 2023-06-23 17:42:11+00:00
  edited: false
  hidden: false
  id: 6495e783990b342dcc92acf5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-23T20:13:33.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9643566608428955
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Does it say "Done" ?</p>

          <p>This is caused on Windows when you don''t have a large enough Pagefile.  Increase
          your Pagefile to 100GB, or if the Pagefile is set to Auto make sure you
          have 100+GB free on the pagefile drive (C: by default)</p>

          <p>This is a very common problem and I should really add mention of it to
          the README.</p>

          '
        raw: 'Does it say "Done" ?


          This is caused on Windows when you don''t have a large enough Pagefile.  Increase
          your Pagefile to 100GB, or if the Pagefile is set to Auto make sure you
          have 100+GB free on the pagefile drive (C: by default)


          This is a very common problem and I should really add mention of it to the
          README.'
        updatedAt: '2023-06-23T20:13:33.560Z'
      numEdits: 0
      reactions: []
    id: 6495fceddfefdeaf7827d699
    type: comment
  author: TheBloke
  content: 'Does it say "Done" ?


    This is caused on Windows when you don''t have a large enough Pagefile.  Increase
    your Pagefile to 100GB, or if the Pagefile is set to Auto make sure you have 100+GB
    free on the pagefile drive (C: by default)


    This is a very common problem and I should really add mention of it to the README.'
  created_at: 2023-06-23 19:13:33+00:00
  edited: false
  hidden: false
  id: 6495fceddfefdeaf7827d699
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7a772b98c9a54c5b52dbeef1753977ae.svg
      fullname: Lars M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: naptaps
      type: user
    createdAt: '2023-06-23T20:35:49.000Z'
    data:
      edited: true
      editors:
      - naptaps
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8487606644630432
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7a772b98c9a54c5b52dbeef1753977ae.svg
          fullname: Lars M
          isHf: false
          isPro: false
          name: naptaps
          type: user
        html: '<p>yeah that worked thanks !<br>now ive got that error ..</p>

          <p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00
          MiB (GPU 0; 12.00 GiB total capacity; 11.17 GiB already allocated; 0 bytes
          free; 11.31 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt;
          allocated memory try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>tried it with "--gpu-memory 10000MiB" didnt changed anything .. is 12gb
          gpu memory too little ? worked well with the 13b version</p>

          '
        raw: 'yeah that worked thanks !

          now ive got that error ..


          torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00
          MiB (GPU 0; 12.00 GiB total capacity; 11.17 GiB already allocated; 0 bytes
          free; 11.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated
          memory try setting max_split_size_mb to avoid fragmentation.  See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF


          tried it with "--gpu-memory 10000MiB" didnt changed anything .. is 12gb
          gpu memory too little ? worked well with the 13b version'
        updatedAt: '2023-06-23T20:39:09.509Z'
      numEdits: 1
      reactions: []
    id: 649602252b5a296dbc57567d
    type: comment
  author: naptaps
  content: 'yeah that worked thanks !

    now ive got that error ..


    torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB (GPU
    0; 12.00 GiB total capacity; 11.17 GiB already allocated; 0 bytes free; 11.31
    GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try
    setting max_split_size_mb to avoid fragmentation.  See documentation for Memory
    Management and PYTORCH_CUDA_ALLOC_CONF


    tried it with "--gpu-memory 10000MiB" didnt changed anything .. is 12gb gpu memory
    too little ? worked well with the 13b version'
  created_at: 2023-06-23 19:35:49+00:00
  edited: true
  hidden: false
  id: 649602252b5a296dbc57567d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-23T20:46:44.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9213634133338928
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes, 12GB is too little for 30B.  13B maximum.   I thought GPU memory
          would work, however even if it does it will be horribly slow.</p>

          <p>I recommend to use a GGML instead, with GPU offload so it''s part on
          CPU and part on GPU.  That will have acceptable performance.  Check the
          text-generation-webui docs for details on how to get llama-cpp-python compiled
          for GPU acceleration.</p>

          '
        raw: 'Yes, 12GB is too little for 30B.  13B maximum.   I thought GPU memory
          would work, however even if it does it will be horribly slow.


          I recommend to use a GGML instead, with GPU offload so it''s part on CPU
          and part on GPU.  That will have acceptable performance.  Check the text-generation-webui
          docs for details on how to get llama-cpp-python compiled for GPU acceleration.'
        updatedAt: '2023-06-23T20:46:44.211Z'
      numEdits: 0
      reactions: []
    id: 649604b44a9a7e1fe41656d2
    type: comment
  author: TheBloke
  content: 'Yes, 12GB is too little for 30B.  13B maximum.   I thought GPU memory
    would work, however even if it does it will be horribly slow.


    I recommend to use a GGML instead, with GPU offload so it''s part on CPU and part
    on GPU.  That will have acceptable performance.  Check the text-generation-webui
    docs for details on how to get llama-cpp-python compiled for GPU acceleration.'
  created_at: 2023-06-23 19:46:44+00:00
  edited: false
  hidden: false
  id: 649604b44a9a7e1fe41656d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/315d6e5a1418a215b1bb188435b536d0.svg
      fullname: JBK
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JB0007
      type: user
    createdAt: '2023-11-06T00:19:31.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/315d6e5a1418a215b1bb188435b536d0.svg
          fullname: JBK
          isHf: false
          isPro: false
          name: JB0007
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-11-06T00:30:51.051Z'
      numEdits: 0
      reactions: []
    id: 654831139295970f87ada653
    type: comment
  author: JB0007
  content: This comment has been hidden
  created_at: 2023-11-06 00:19:31+00:00
  edited: true
  hidden: true
  id: 654831139295970f87ada653
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0b82e5786011ca348164749a4e28ead4.svg
      fullname: Star
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Steelman
      type: user
    createdAt: '2023-12-16T14:27:47.000Z'
    data:
      edited: false
      editors:
      - Steelman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.27095654606819153
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0b82e5786011ca348164749a4e28ead4.svg
          fullname: Star
          isHf: false
          isPro: false
          name: Steelman
          type: user
        html: '<p>Made my page file 100000Mb but still getting error:</p>

          <p>Traceback (most recent call last):</p>

          <p>File "E:\Ai\Oobabooga\installer_files\env\Lib\site-packages\transformers\configuration_utils.py",
          line 729, in _get_config_dict</p>

          <p>config_dict = cls._dict_from_json_file(resolved_config_file)</p>

          <pre><code>          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "E:\Ai\Oobabooga\installer_files\env\Lib\site-packages\transformers\configuration_utils.py",
          line 827, in _dict_from_json_file</p>

          <p>text = reader.read()</p>

          <pre><code>   ^^^^^^^^^^^^^

          </code></pre>

          <p>File "", line 322, in decode</p>

          <p>UnicodeDecodeError: ''utf-8'' codec can''t decode byte 0xc8 in position
          0: invalid continuation byte</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):</p>

          <p>File "E:\Ai\Oobabooga\modules\ui_model_menu.py", line 209, in load_model_wrapper</p>

          <p>shared.model, shared.tokenizer = load_model(selected_model, loader)</p>

          <pre><code>                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "E:\Ai\Oobabooga\modules\models.py", line 89, in load_model</p>

          <p>output = load_func_map<a rel="nofollow" href="model_name">loader</a></p>

          <pre><code>     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "E:\Ai\Oobabooga\modules\models.py", line 147, in huggingface_loader</p>

          <p>config = AutoConfig.from_pretrained(path_to_model, trust_remote_code=params[''trust_remote_code''])</p>

          <pre><code>     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "E:\Ai\Oobabooga\installer_files\env\Lib\site-packages\transformers\models\auto\configuration_auto.py",
          line 1082, in from_pretrained</p>

          <p>config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,
          **kwargs)</p>

          <pre><code>                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "E:\Ai\Oobabooga\installer_files\env\Lib\site-packages\transformers\configuration_utils.py",
          line 644, in get_config_dict</p>

          <p>config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path,
          **kwargs)</p>

          <pre><code>                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "E:\Ai\Oobabooga\installer_files\env\Lib\site-packages\transformers\configuration_utils.py",
          line 732, in _get_config_dict</p>

          <p>raise EnvironmentError(<br>OSError: It looks like the config file at
          ''models\Wizard-Vicuna-30B-Uncensored-GPTQ.safetensors'' is not a valid
          JSON file.</p>

          '
        raw: "Made my page file 100000Mb but still getting error:\n\nTraceback (most\
          \ recent call last):\n\nFile \"E:\\Ai\\Oobabooga\\installer_files\\env\\\
          Lib\\site-packages\\transformers\\configuration_utils.py\", line 729, in\
          \ _get_config_dict\n\n\nconfig_dict = cls._dict_from_json_file(resolved_config_file)\n\
          \n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"\
          E:\\Ai\\Oobabooga\\installer_files\\env\\Lib\\site-packages\\transformers\\\
          configuration_utils.py\", line 827, in _dict_from_json_file\n\n\ntext =\
          \ reader.read()\n\n       ^^^^^^^^^^^^^\nFile \"\", line 322, in decode\n\
          \nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xc8 in position 0:\
          \ invalid continuation byte\n\nDuring handling of the above exception, another\
          \ exception occurred:\n\nTraceback (most recent call last):\n\nFile \"E:\\\
          Ai\\Oobabooga\\modules\\ui_model_menu.py\", line 209, in load_model_wrapper\n\
          \n\nshared.model, shared.tokenizer = load_model(selected_model, loader)\n\
          \n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"E:\\Ai\\Oobabooga\\modules\\models.py\", line 89, in load_model\n\
          \n\noutput = load_func_map[loader](model_name)\n\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"E:\\Ai\\Oobabooga\\modules\\models.py\", line 147, in huggingface_loader\n\
          \n\nconfig = AutoConfig.from_pretrained(path_to_model, trust_remote_code=params['trust_remote_code'])\n\
          \n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"E:\\Ai\\Oobabooga\\installer_files\\env\\Lib\\site-packages\\transformers\\\
          models\\auto\\configuration_auto.py\", line 1082, in from_pretrained\n\n\
          \nconfig_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
          \ **kwargs)\n\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"E:\\Ai\\Oobabooga\\installer_files\\env\\Lib\\site-packages\\transformers\\\
          configuration_utils.py\", line 644, in get_config_dict\n\n\nconfig_dict,\
          \ kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n\
          \n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"E:\\Ai\\Oobabooga\\installer_files\\env\\Lib\\site-packages\\transformers\\\
          configuration_utils.py\", line 732, in _get_config_dict\n\n\nraise EnvironmentError(\n\
          OSError: It looks like the config file at 'models\\Wizard-Vicuna-30B-Uncensored-GPTQ.safetensors'\
          \ is not a valid JSON file."
        updatedAt: '2023-12-16T14:27:47.293Z'
      numEdits: 0
      reactions: []
    id: 657db3e3c7a58172f5e63db1
    type: comment
  author: Steelman
  content: "Made my page file 100000Mb but still getting error:\n\nTraceback (most\
    \ recent call last):\n\nFile \"E:\\Ai\\Oobabooga\\installer_files\\env\\Lib\\\
    site-packages\\transformers\\configuration_utils.py\", line 729, in _get_config_dict\n\
    \n\nconfig_dict = cls._dict_from_json_file(resolved_config_file)\n\n         \
    \     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"E:\\Ai\\Oobabooga\\\
    installer_files\\env\\Lib\\site-packages\\transformers\\configuration_utils.py\"\
    , line 827, in _dict_from_json_file\n\n\ntext = reader.read()\n\n       ^^^^^^^^^^^^^\n\
    File \"\", line 322, in decode\n\nUnicodeDecodeError: 'utf-8' codec can't decode\
    \ byte 0xc8 in position 0: invalid continuation byte\n\nDuring handling of the\
    \ above exception, another exception occurred:\n\nTraceback (most recent call\
    \ last):\n\nFile \"E:\\Ai\\Oobabooga\\modules\\ui_model_menu.py\", line 209, in\
    \ load_model_wrapper\n\n\nshared.model, shared.tokenizer = load_model(selected_model,\
    \ loader)\n\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \"E:\\Ai\\Oobabooga\\modules\\models.py\", line 89, in load_model\n\n\noutput\
    \ = load_func_map[loader](model_name)\n\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \"E:\\Ai\\Oobabooga\\modules\\models.py\", line 147, in huggingface_loader\n\
    \n\nconfig = AutoConfig.from_pretrained(path_to_model, trust_remote_code=params['trust_remote_code'])\n\
    \n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \"E:\\Ai\\Oobabooga\\installer_files\\env\\Lib\\site-packages\\transformers\\\
    models\\auto\\configuration_auto.py\", line 1082, in from_pretrained\n\n\nconfig_dict,\
    \ unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path,\
    \ **kwargs)\n\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \"E:\\Ai\\Oobabooga\\installer_files\\env\\Lib\\site-packages\\transformers\\\
    configuration_utils.py\", line 644, in get_config_dict\n\n\nconfig_dict, kwargs\
    \ = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n\n        \
    \              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \"E:\\Ai\\Oobabooga\\installer_files\\env\\Lib\\site-packages\\transformers\\\
    configuration_utils.py\", line 732, in _get_config_dict\n\n\nraise EnvironmentError(\n\
    OSError: It looks like the config file at 'models\\Wizard-Vicuna-30B-Uncensored-GPTQ.safetensors'\
    \ is not a valid JSON file."
  created_at: 2023-12-16 14:27:47+00:00
  edited: false
  hidden: false
  id: 657db3e3c7a58172f5e63db1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: Error while loading Model
