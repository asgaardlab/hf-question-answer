!!python/object:huggingface_hub.community.DiscussionWithDetails
author: daniq
conflicting_files: null
created_at: 2023-06-17 15:28:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d695dd6d0a3bb29920885bc9fbb0f0e5.svg
      fullname: Mussabekov Daniyar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daniq
      type: user
    createdAt: '2023-06-17T16:28:36.000Z'
    data:
      edited: false
      editors:
      - daniq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9265449643135071
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d695dd6d0a3bb29920885bc9fbb0f0e5.svg
          fullname: Mussabekov Daniyar
          isHf: false
          isPro: false
          name: daniq
          type: user
        html: '<p>I''m using hosted VM with these specs:</p>

          <ul>

          <li>NVIDIA A40 with 46GB VRAM</li>

          <li>AMD EPYC 7252 CPU</li>

          <li>52 GB RAM</li>

          </ul>

          <p>I''ve loaded this model into oobabooga text ui. Each generation of response
          takes too much time. For example, generating 351 tokens took about 435 seconds.
          I''m using default configs of this text ui.</p>

          <p>Why it is like that? Does these specs are not enough or I need to tweak
          some configs?</p>

          '
        raw: "I'm using hosted VM with these specs:\r\n\r\n- NVIDIA A40 with 46GB\
          \ VRAM\r\n- AMD EPYC 7252 CPU\r\n- 52 GB RAM\r\n\r\nI've loaded this model\
          \ into oobabooga text ui. Each generation of response takes too much time.\
          \ For example, generating 351 tokens took about 435 seconds. I'm using default\
          \ configs of this text ui.\r\n\r\nWhy it is like that? Does these specs\
          \ are not enough or I need to tweak some configs?"
        updatedAt: '2023-06-17T16:28:36.844Z'
      numEdits: 0
      reactions: []
    id: 648ddf342a766c79a48834e6
    type: comment
  author: daniq
  content: "I'm using hosted VM with these specs:\r\n\r\n- NVIDIA A40 with 46GB VRAM\r\
    \n- AMD EPYC 7252 CPU\r\n- 52 GB RAM\r\n\r\nI've loaded this model into oobabooga\
    \ text ui. Each generation of response takes too much time. For example, generating\
    \ 351 tokens took about 435 seconds. I'm using default configs of this text ui.\r\
    \n\r\nWhy it is like that? Does these specs are not enough or I need to tweak\
    \ some configs?"
  created_at: 2023-06-17 15:28:36+00:00
  edited: false
  hidden: false
  id: 648ddf342a766c79a48834e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d695dd6d0a3bb29920885bc9fbb0f0e5.svg
      fullname: Mussabekov Daniyar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daniq
      type: user
    createdAt: '2023-06-19T15:21:47.000Z'
    data:
      edited: false
      editors:
      - daniq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9066621661186218
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d695dd6d0a3bb29920885bc9fbb0f0e5.svg
          fullname: Mussabekov Daniyar
          isHf: false
          isPro: false
          name: daniq
          type: user
        html: '<p>I''ve been using ooba one  click installer for my text ui. I asked
          some people in discord and they gave me one solution.</p>

          <p>The solution was to create symlink like this:</p>

          <pre><code>ln -s "&lt;installer_path&gt;/installer_files/env/lib" "&lt;installer_path&gt;/installer_files/env/lib64"

          </code></pre>

          <p>Someone already made Merge Request that fixes this solution  <a rel="nofollow"
          href="https://github.com/oobabooga/one-click-installers/pull/84">https://github.com/oobabooga/one-click-installers/pull/84</a></p>

          <p>I hope they will accept it.</p>

          '
        raw: "I've been using ooba one  click installer for my text ui. I asked some\
          \ people in discord and they gave me one solution.\n\nThe solution was to\
          \ create symlink like this:\n\n    ln -s \"<installer_path>/installer_files/env/lib\"\
          \ \"<installer_path>/installer_files/env/lib64\"\n\nSomeone already made\
          \ Merge Request that fixes this solution  https://github.com/oobabooga/one-click-installers/pull/84\n\
          \nI hope they will accept it.\n"
        updatedAt: '2023-06-19T15:21:47.691Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6490728bfcd28ea7e3bdcccf
    id: 6490728bfcd28ea7e3bdccce
    type: comment
  author: daniq
  content: "I've been using ooba one  click installer for my text ui. I asked some\
    \ people in discord and they gave me one solution.\n\nThe solution was to create\
    \ symlink like this:\n\n    ln -s \"<installer_path>/installer_files/env/lib\"\
    \ \"<installer_path>/installer_files/env/lib64\"\n\nSomeone already made Merge\
    \ Request that fixes this solution  https://github.com/oobabooga/one-click-installers/pull/84\n\
    \nI hope they will accept it.\n"
  created_at: 2023-06-19 14:21:47+00:00
  edited: false
  hidden: false
  id: 6490728bfcd28ea7e3bdccce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d695dd6d0a3bb29920885bc9fbb0f0e5.svg
      fullname: Mussabekov Daniyar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daniq
      type: user
    createdAt: '2023-06-19T15:21:47.000Z'
    data:
      status: closed
    id: 6490728bfcd28ea7e3bdcccf
    type: status-change
  author: daniq
  created_at: 2023-06-19 14:21:47+00:00
  id: 6490728bfcd28ea7e3bdcccf
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-20T10:08:09.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7135745286941528
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>That''s strange that that was required.</p>

          <p>By the way if you want to use cloud GPUs for text-generation-webui, I
          have a Runpod template with text-generation-webui + AutoGPTQ + ExLlama (for
          2x faster inference on Llama GPTQ) + llama-cpp-python for GPU accelerated
          GGMLs.</p>

          <p>It''s all set up and ready to go and very easy to use.  The templates
          are:</p>

          <ul>

          <li><a rel="nofollow" href="https://runpod.io/gsc?template=qk29nkmbfr&amp;ref=eexqfacd">TheBloke
          Local LLMs One-Click UI</a></li>

          <li><a rel="nofollow" href="https://runpod.io/gsc?template=f1pf20op0z&amp;ref=eexqfacd">TheBloke
          Local LLMs One-Click UI and API</a></li>

          </ul>

          <p>Or if you want to use the Docker config as a base for your own, then
          the source is here: <a rel="nofollow" href="https://github.com/TheBlokeAI/dockerLLM">https://github.com/TheBlokeAI/dockerLLM</a></p>

          '
        raw: 'That''s strange that that was required.


          By the way if you want to use cloud GPUs for text-generation-webui, I have
          a Runpod template with text-generation-webui + AutoGPTQ + ExLlama (for 2x
          faster inference on Llama GPTQ) + llama-cpp-python for GPU accelerated GGMLs.


          It''s all set up and ready to go and very easy to use.  The templates are:

          - [TheBloke Local LLMs One-Click UI](https://runpod.io/gsc?template=qk29nkmbfr&ref=eexqfacd)

          - [TheBloke Local LLMs One-Click UI and API](https://runpod.io/gsc?template=f1pf20op0z&ref=eexqfacd)


          Or if you want to use the Docker config as a base for your own, then the
          source is here: https://github.com/TheBlokeAI/dockerLLM'
        updatedAt: '2023-06-20T10:08:09.534Z'
      numEdits: 0
      reactions: []
    id: 64917a89ddee981c98daa125
    type: comment
  author: TheBloke
  content: 'That''s strange that that was required.


    By the way if you want to use cloud GPUs for text-generation-webui, I have a Runpod
    template with text-generation-webui + AutoGPTQ + ExLlama (for 2x faster inference
    on Llama GPTQ) + llama-cpp-python for GPU accelerated GGMLs.


    It''s all set up and ready to go and very easy to use.  The templates are:

    - [TheBloke Local LLMs One-Click UI](https://runpod.io/gsc?template=qk29nkmbfr&ref=eexqfacd)

    - [TheBloke Local LLMs One-Click UI and API](https://runpod.io/gsc?template=f1pf20op0z&ref=eexqfacd)


    Or if you want to use the Docker config as a base for your own, then the source
    is here: https://github.com/TheBlokeAI/dockerLLM'
  created_at: 2023-06-20 09:08:09+00:00
  edited: false
  hidden: false
  id: 64917a89ddee981c98daa125
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d695dd6d0a3bb29920885bc9fbb0f0e5.svg
      fullname: Mussabekov Daniyar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daniq
      type: user
    createdAt: '2023-06-23T05:40:09.000Z'
    data:
      edited: false
      editors:
      - daniq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9851728081703186
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d695dd6d0a3bb29920885bc9fbb0f0e5.svg
          fullname: Mussabekov Daniyar
          isHf: false
          isPro: false
          name: daniq
          type: user
        html: '<p>Thanks for templates. However  I''m using Vast.ai which is cheaper
          for me, but I still look it, maybe it is possible to use it there too.</p>

          '
        raw: Thanks for templates. However  I'm using Vast.ai which is cheaper for
          me, but I still look it, maybe it is possible to use it there too.
        updatedAt: '2023-06-23T05:40:09.783Z'
      numEdits: 0
      reactions: []
    id: 649530390cb367bd3d604a57
    type: comment
  author: daniq
  content: Thanks for templates. However  I'm using Vast.ai which is cheaper for me,
    but I still look it, maybe it is possible to use it there too.
  created_at: 2023-06-23 04:40:09+00:00
  edited: false
  hidden: false
  id: 649530390cb367bd3d604a57
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ
repo_type: model
status: closed
target_branch: null
title: Why generating response is too slow?
