!!python/object:huggingface_hub.community.DiscussionWithDetails
author: PandaEspresso
conflicting_files: null
created_at: 2023-05-31 01:26:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9be2047de2d24664ec8beccd1dd41199.svg
      fullname: PandaEspresso
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PandaEspresso
      type: user
    createdAt: '2023-05-31T02:26:41.000Z'
    data:
      edited: true
      editors:
      - PandaEspresso
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9be2047de2d24664ec8beccd1dd41199.svg
          fullname: PandaEspresso
          isHf: false
          isPro: false
          name: PandaEspresso
          type: user
        html: '<p>When I load the model, it shows size mismatch errors for all model
          layers from layer 0 to layer 59.<br>Any clue what could be the reason for
          that? I can load your 13B Wizard model fine.</p>

          <p>2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.k_proj.qzeros:
          copying a param with shape torch.Size([1, 832]) from checkpoint, the shape
          in current model is torch.Size([52, 832]).<br>2023-05-30 22:20:29 | ERROR
          | stderr |         size mismatch for model.layers.0.self_attn.k_proj.scales:
          copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape
          in current model is torch.Size([52, 6656]).<br>2023-05-30 22:20:29 | ERROR
          | stderr |         size mismatch for model.layers.0.self_attn.o_proj.qzeros:
          copying a param with shape torch.Size([1, 832]) from checkpoint, the shape
          in current model is torch.Size([52, 832]).<br>2023-05-30 22:20:29 | ERROR
          | stderr |         size mismatch for model.layers.0.self_attn.o_proj.scales:
          copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape
          in current model is torch.Size([52, 6656]).<br>2023-05-30 22:20:29 | ERROR
          | stderr |         size mismatch for model.layers.0.self_attn.q_proj.qzeros:
          copying a param with shape torch.Size([1, 832]) from checkpoint, the shape
          in current model is torch.Size([52, 832]).<br>2023-05-30 22:20:29 | ERROR
          | stderr |         size mismatch for model.layers.0.self_attn.q_proj.scales:
          copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape
          in current model is torch.Size([52, 6656]).<br>2023-05-30 22:20:29 | ERROR
          | stderr |         size mismatch for model.layers.0.self_attn.v_proj.qzeros:
          copying a param with shape torch.Size([1, 832]) from checkpoint, the shape
          in current model is torch.Size([52, 832]).<br>2023-05-30 22:20:29 | ERROR
          | stderr |         size mismatch for model.layers.0.self_attn.v_proj.scales:
          copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape
          in current model is torch.Size([52, 6656]).<br>2023-05-30 22:20:29 | ERROR
          | stderr |         size mismatch for model.layers.0.mlp.down_proj.qzeros:
          copying a param with shape torch.Size([1, 832]) from checkpoint, the shape
          in current model is torch.Size([140, 832]).<br>2023-05-30 22:20:29 | ERROR
          | stderr |         size mismatch for model.layers.0.mlp.down_proj.scales:
          copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape
          in current model is torch.Size([140, 6656]).<br>2023-05-30 22:20:29 | ERROR
          | stderr |         size mismatch for model.layers.0.mlp.gate_proj.qzeros:
          copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape
          in current model is torch.Size([52, 2240]).<br>2023-05-30 22:20:29 | ERROR
          | stderr |         size mismatch for model.layers.0.mlp.gate_proj.scales:
          copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape
          in current model is torch.Size([52, 17920]).<br>2023-05-30 22:20:29 | ERROR
          | stderr |         size mismatch for model.layers.0.mlp.up_proj.qzeros:
          copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape
          in current model is torch.Size([52, 2240]).</p>

          '
        raw: 'When I load the model, it shows size mismatch errors for all model layers
          from layer 0 to layer 59.

          Any clue what could be the reason for that? I can load your 13B Wizard model
          fine.


          2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.k_proj.qzeros:
          copying a param with shape torch.Size([1, 832]) from checkpoint, the shape
          in current model is torch.Size([52, 832]).

          2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.k_proj.scales:
          copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape
          in current model is torch.Size([52, 6656]).

          2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.o_proj.qzeros:
          copying a param with shape torch.Size([1, 832]) from checkpoint, the shape
          in current model is torch.Size([52, 832]).

          2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.o_proj.scales:
          copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape
          in current model is torch.Size([52, 6656]).

          2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.q_proj.qzeros:
          copying a param with shape torch.Size([1, 832]) from checkpoint, the shape
          in current model is torch.Size([52, 832]).

          2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.q_proj.scales:
          copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape
          in current model is torch.Size([52, 6656]).

          2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.v_proj.qzeros:
          copying a param with shape torch.Size([1, 832]) from checkpoint, the shape
          in current model is torch.Size([52, 832]).

          2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.v_proj.scales:
          copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape
          in current model is torch.Size([52, 6656]).

          2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.mlp.down_proj.qzeros:
          copying a param with shape torch.Size([1, 832]) from checkpoint, the shape
          in current model is torch.Size([140, 832]).

          2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.mlp.down_proj.scales:
          copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape
          in current model is torch.Size([140, 6656]).

          2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.mlp.gate_proj.qzeros:
          copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape
          in current model is torch.Size([52, 2240]).

          2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.mlp.gate_proj.scales:
          copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape
          in current model is torch.Size([52, 17920]).

          2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.mlp.up_proj.qzeros:
          copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape
          in current model is torch.Size([52, 2240]).'
        updatedAt: '2023-05-31T02:29:23.250Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - flexai
    id: 6476b061a0bde42d3f942dc6
    type: comment
  author: PandaEspresso
  content: 'When I load the model, it shows size mismatch errors for all model layers
    from layer 0 to layer 59.

    Any clue what could be the reason for that? I can load your 13B Wizard model fine.


    2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.k_proj.qzeros:
    copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in
    current model is torch.Size([52, 832]).

    2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.k_proj.scales:
    copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape in
    current model is torch.Size([52, 6656]).

    2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.o_proj.qzeros:
    copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in
    current model is torch.Size([52, 832]).

    2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.o_proj.scales:
    copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape in
    current model is torch.Size([52, 6656]).

    2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.q_proj.qzeros:
    copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in
    current model is torch.Size([52, 832]).

    2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.q_proj.scales:
    copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape in
    current model is torch.Size([52, 6656]).

    2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.v_proj.qzeros:
    copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in
    current model is torch.Size([52, 832]).

    2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.self_attn.v_proj.scales:
    copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape in
    current model is torch.Size([52, 6656]).

    2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.mlp.down_proj.qzeros:
    copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in
    current model is torch.Size([140, 832]).

    2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.mlp.down_proj.scales:
    copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape in
    current model is torch.Size([140, 6656]).

    2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.mlp.gate_proj.qzeros:
    copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape in
    current model is torch.Size([52, 2240]).

    2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.mlp.gate_proj.scales:
    copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape in
    current model is torch.Size([52, 17920]).

    2023-05-30 22:20:29 | ERROR | stderr |         size mismatch for model.layers.0.mlp.up_proj.qzeros:
    copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape in
    current model is torch.Size([52, 2240]).'
  created_at: 2023-05-31 01:26:41+00:00
  edited: true
  hidden: false
  id: 6476b061a0bde42d3f942dc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/228157dfff872f87fe8057ec536b9a3e.svg
      fullname: TF
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TobiTobi
      type: user
    createdAt: '2023-05-31T13:30:25.000Z'
    data:
      edited: false
      editors:
      - TobiTobi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/228157dfff872f87fe8057ec536b9a3e.svg
          fullname: TF
          isHf: false
          isPro: false
          name: TobiTobi
          type: user
        html: '<p>I got a similar message when I tried to start the model with group
          size 128. With no group size/group size None it starts fine for me.</p>

          <p>I think that''s also one difference between this model and the 13B one</p>

          '
        raw: 'I got a similar message when I tried to start the model with group size
          128. With no group size/group size None it starts fine for me.


          I think that''s also one difference between this model and the 13B one'
        updatedAt: '2023-05-31T13:30:25.667Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - PandaEspresso
        - kristafari
    id: 64774bf133a888101f73e178
    type: comment
  author: TobiTobi
  content: 'I got a similar message when I tried to start the model with group size
    128. With no group size/group size None it starts fine for me.


    I think that''s also one difference between this model and the 13B one'
  created_at: 2023-05-31 12:30:25+00:00
  edited: false
  hidden: false
  id: 64774bf133a888101f73e178
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T13:33:37.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah group_size should be None.  This error happens when it''s set
          to 128 by mistake</p>

          <p>Please see instructions in the README regarding setting the correct params.  And
          if you''re still having trouble after that, please try updating text-generation-webui.  There
          was a bug recently where it would overwrite "groupsize = None" with "groupsize
          = 128", but I believe that''s been fixed now.</p>

          '
        raw: 'Yeah group_size should be None.  This error happens when it''s set to
          128 by mistake


          Please see instructions in the README regarding setting the correct params.  And
          if you''re still having trouble after that, please try updating text-generation-webui.  There
          was a bug recently where it would overwrite "groupsize = None" with "groupsize
          = 128", but I believe that''s been fixed now.'
        updatedAt: '2023-05-31T13:33:37.089Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PandaEspresso
    id: 64774cb1bb7681ad67043c50
    type: comment
  author: TheBloke
  content: 'Yeah group_size should be None.  This error happens when it''s set to
    128 by mistake


    Please see instructions in the README regarding setting the correct params.  And
    if you''re still having trouble after that, please try updating text-generation-webui.  There
    was a bug recently where it would overwrite "groupsize = None" with "groupsize
    = 128", but I believe that''s been fixed now.'
  created_at: 2023-05-31 12:33:37+00:00
  edited: false
  hidden: false
  id: 64774cb1bb7681ad67043c50
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9be2047de2d24664ec8beccd1dd41199.svg
      fullname: PandaEspresso
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PandaEspresso
      type: user
    createdAt: '2023-05-31T17:10:28.000Z'
    data:
      edited: false
      editors:
      - PandaEspresso
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9be2047de2d24664ec8beccd1dd41199.svg
          fullname: PandaEspresso
          isHf: false
          isPro: false
          name: PandaEspresso
          type: user
        html: '<p>Thank you guys, you are right, I found this line</p>

          <p>def load_quantized(model_name, wbits=4, groupsize=128, threshold=128):</p>

          <p>changing the group size default to -1 solves the problem.</p>

          '
        raw: 'Thank you guys, you are right, I found this line


          def load_quantized(model_name, wbits=4, groupsize=128, threshold=128):


          changing the group size default to -1 solves the problem.'
        updatedAt: '2023-05-31T17:10:28.726Z'
      numEdits: 0
      reactions: []
    id: 64777f84bb7681ad670928cc
    type: comment
  author: PandaEspresso
  content: 'Thank you guys, you are right, I found this line


    def load_quantized(model_name, wbits=4, groupsize=128, threshold=128):


    changing the group size default to -1 solves the problem.'
  created_at: 2023-05-31 16:10:28+00:00
  edited: false
  hidden: false
  id: 64777f84bb7681ad670928cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-31T17:18:54.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Oh sorry didn't notice you were using custom code.  In which case\
          \ you should check out AutoGPTQ. It makes it much easier to handle loading\
          \ GPTQ models. And you don't need to set params manually in your code because\
          \ they get loaded from quantize_config.json</p>\n<p>Example AutoGPTQ code:</p>\n\
          <p>First download and build AutoGPTQ from source:</p>\n<pre><code>git clone\
          \ https://github.com/PanQiWei/AutoGPTQ\ncd AutoGPTQ\npip install .\n</code></pre>\n\
          <p>Then:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ pipeline, logging\n<span class=\"hljs-keyword\">from</span> auto_gptq\
          \ <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM\n<span\
          \ class=\"hljs-keyword\">import</span> argparse\n\n<span class=\"hljs-comment\"\
          ># path to directory containing local model</span>\nquantized_model_dir\
          \ = <span class=\"hljs-string\">\"/workspace/models/TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\"\
          </span>\n\nmodel_basename = <span class=\"hljs-string\">\"Wizard-vicuna-30B-Uncensored-GPTQ-4bit.act-order\"\
          </span>\n\nuse_triton = <span class=\"hljs-literal\">False</span>\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=<span class=\"\
          hljs-literal\">True</span>)\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=<span class=\"hljs-literal\">True</span>,\n   \
          \     model_basename=model_basename,\n        device=<span class=\"hljs-string\"\
          >\"cuda:0\"</span>,\n        use_triton=use_triton,\n        quantize_config=<span\
          \ class=\"hljs-literal\">None</span>)\n\n<span class=\"hljs-comment\">#\
          \ Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\nprompt = <span\
          \ class=\"hljs-string\">\"Tell me about AI\"</span>\nprompt_template=<span\
          \ class=\"hljs-string\">f'''### Human: <span class=\"hljs-subst\">{prompt}</span></span>\n\
          <span class=\"hljs-string\">### Assistant:'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"</span>)\npipe\
          \ = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"</span>,\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span class=\"\
          hljs-number\">512</span>,\n    temperature=<span class=\"hljs-number\">0.7</span>,\n\
          \    top_p=<span class=\"hljs-number\">0.95</span>,\n    repetition_penalty=<span\
          \ class=\"hljs-number\">1.15</span>\n)\n\n<span class=\"hljs-built_in\"\
          >print</span>(pipe(prompt_template)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">'generated_text'</span>])\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n"
        raw: "Oh sorry didn't notice you were using custom code.  In which case you\
          \ should check out AutoGPTQ. It makes it much easier to handle loading GPTQ\
          \ models. And you don't need to set params manually in your code because\
          \ they get loaded from quantize_config.json\n\nExample AutoGPTQ code:\n\n\
          First download and build AutoGPTQ from source:\n```\ngit clone https://github.com/PanQiWei/AutoGPTQ\n\
          cd AutoGPTQ\npip install .\n```\n\nThen:\n```python\nfrom transformers import\
          \ AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM\n\
          import argparse\n\n# path to directory containing local model\nquantized_model_dir\
          \ = \"/workspace/models/TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\"\n\n\
          model_basename = \"Wizard-vicuna-30B-Uncensored-GPTQ-4bit.act-order\"\n\n\
          use_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=True,\n        model_basename=model_basename,\n\
          \        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          \n# Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\nprompt = \"Tell me\
          \ about AI\"\nprompt_template=f'''### Human: {prompt}\n### Assistant:'''\n\
          \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n \
          \   temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\n\
          print(pipe(prompt_template)[0]['generated_text'])\n\nprint(\"\\n\\n*** Generate:\"\
          )\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n```"
        updatedAt: '2023-05-31T17:19:11.495Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - PandaEspresso
        - Hypersniper
        - leocnj
    id: 6477817e33a888101f791687
    type: comment
  author: TheBloke
  content: "Oh sorry didn't notice you were using custom code.  In which case you\
    \ should check out AutoGPTQ. It makes it much easier to handle loading GPTQ models.\
    \ And you don't need to set params manually in your code because they get loaded\
    \ from quantize_config.json\n\nExample AutoGPTQ code:\n\nFirst download and build\
    \ AutoGPTQ from source:\n```\ngit clone https://github.com/PanQiWei/AutoGPTQ\n\
    cd AutoGPTQ\npip install .\n```\n\nThen:\n```python\nfrom transformers import\
    \ AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM\n\
    import argparse\n\n# path to directory containing local model\nquantized_model_dir\
    \ = \"/workspace/models/TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\"\n\nmodel_basename\
    \ = \"Wizard-vicuna-30B-Uncensored-GPTQ-4bit.act-order\"\n\nuse_triton = False\n\
    \ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)\n\
    \nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n        use_safetensors=True,\n\
    \        model_basename=model_basename,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
    \        quantize_config=None)\n\n# Prevent printing spurious transformers error\
    \ when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
    \nprompt = \"Tell me about AI\"\nprompt_template=f'''### Human: {prompt}\n###\
    \ Assistant:'''\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\"\
    ,\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    temperature=0.7,\n\
    \    top_p=0.95,\n    repetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    \nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n```"
  created_at: 2023-05-31 16:18:54+00:00
  edited: true
  hidden: false
  id: 6477817e33a888101f791687
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: size mismatch for model.layers
