!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gnomealone
conflicting_files: null
created_at: 2023-09-09 11:40:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/efacf65b052b236d859b56506576fa97.svg
      fullname: gnome
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gnomealone
      type: user
    createdAt: '2023-09-09T12:40:57.000Z'
    data:
      edited: false
      editors:
      - gnomealone
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8242184519767761
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/efacf65b052b236d859b56506576fa97.svg
          fullname: gnome
          isHf: false
          isPro: false
          name: gnomealone
          type: user
        html: '<p>can you release a Phind codellama 34B or Falcon 180B model please?</p>

          '
        raw: can you release a Phind codellama 34B or Falcon 180B model please?
        updatedAt: '2023-09-09T12:40:57.798Z'
      numEdits: 0
      reactions: []
    id: 64fc67d99a62bb2791c5e578
    type: comment
  author: gnomealone
  content: can you release a Phind codellama 34B or Falcon 180B model please?
  created_at: 2023-09-09 11:40:57+00:00
  edited: false
  hidden: false
  id: 64fc67d99a62bb2791c5e578
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c8dc0df57596c526b2bccea21835f53.svg
      fullname: Mengzhao Chen
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ChenMnZ
      type: user
    createdAt: '2023-09-09T12:43:02.000Z'
    data:
      edited: false
      editors:
      - ChenMnZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9946518540382385
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c8dc0df57596c526b2bccea21835f53.svg
          fullname: Mengzhao Chen
          isHf: false
          isPro: false
          name: ChenMnZ
          type: user
        html: '<p>Falcon 180B is on the way. Please wait a few days.</p>

          '
        raw: Falcon 180B is on the way. Please wait a few days.
        updatedAt: '2023-09-09T12:43:02.218Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - gnomealone
    id: 64fc68569ecd05d5bf65ed9e
    type: comment
  author: ChenMnZ
  content: Falcon 180B is on the way. Please wait a few days.
  created_at: 2023-09-09 11:43:02+00:00
  edited: false
  hidden: false
  id: 64fc68569ecd05d5bf65ed9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c8dc0df57596c526b2bccea21835f53.svg
      fullname: Mengzhao Chen
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ChenMnZ
      type: user
    createdAt: '2023-09-11T08:21:06.000Z'
    data:
      edited: false
      editors:
      - ChenMnZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9732173085212708
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c8dc0df57596c526b2bccea21835f53.svg
          fullname: Mengzhao Chen
          isHf: false
          isPro: false
          name: ChenMnZ
          type: user
        html: '<p>We have released Falcon-180B, refer <a rel="nofollow" href="https://github.com/OpenGVLab/OmniQuant">https://github.com/OpenGVLab/OmniQuant</a>
          for more details.</p>

          '
        raw: We have released Falcon-180B, refer https://github.com/OpenGVLab/OmniQuant
          for more details.
        updatedAt: '2023-09-11T08:21:06.663Z'
      numEdits: 0
      reactions: []
    id: 64fecdf230715b6d04485d3d
    type: comment
  author: ChenMnZ
  content: We have released Falcon-180B, refer https://github.com/OpenGVLab/OmniQuant
    for more details.
  created_at: 2023-09-11 07:21:06+00:00
  edited: false
  hidden: false
  id: 64fecdf230715b6d04485d3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/efacf65b052b236d859b56506576fa97.svg
      fullname: gnome
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gnomealone
      type: user
    createdAt: '2023-09-11T12:24:40.000Z'
    data:
      edited: false
      editors:
      - gnomealone
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6214975118637085
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/efacf65b052b236d859b56506576fa97.svg
          fullname: gnome
          isHf: false
          isPro: false
          name: gnomealone
          type: user
        html: '<p>thank you!</p>

          '
        raw: thank you!
        updatedAt: '2023-09-11T12:24:40.765Z'
      numEdits: 0
      reactions: []
    id: 64ff0708075c57ceed339794
    type: comment
  author: gnomealone
  content: thank you!
  created_at: 2023-09-11 11:24:40+00:00
  edited: false
  hidden: false
  id: 64ff0708075c57ceed339794
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c692a0c5cd9e574b00eeef515b743e8d.svg
      fullname: haurais
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TK4000
      type: user
    createdAt: '2023-09-12T09:59:38.000Z'
    data:
      edited: false
      editors:
      - TK4000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6765249967575073
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c692a0c5cd9e574b00eeef515b743e8d.svg
          fullname: haurais
          isHf: false
          isPro: false
          name: TK4000
          type: user
        html: '<p>Can it works on Mac Studio on m1 ultra with 128go  using metal ??</p>

          '
        raw: Can it works on Mac Studio on m1 ultra with 128go  using metal ??
        updatedAt: '2023-09-12T09:59:38.177Z'
      numEdits: 0
      reactions: []
    id: 6500368a1e14749e84ed2b65
    type: comment
  author: TK4000
  content: Can it works on Mac Studio on m1 ultra with 128go  using metal ??
  created_at: 2023-09-12 08:59:38+00:00
  edited: false
  hidden: false
  id: 6500368a1e14749e84ed2b65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c8dc0df57596c526b2bccea21835f53.svg
      fullname: Mengzhao Chen
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ChenMnZ
      type: user
    createdAt: '2023-09-12T10:20:38.000Z'
    data:
      edited: false
      editors:
      - ChenMnZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9062021374702454
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c8dc0df57596c526b2bccea21835f53.svg
          fullname: Mengzhao Chen
          isHf: false
          isPro: false
          name: ChenMnZ
          type: user
        html: '<p>The current implementation uses CUDA kernels for the quantized model,
          so it''s GPU-compatible.<br>In theory, Falcon-180b with w3a16g512 quantization
          can operate on any device with over 80GB of free memory. However, adapting
          the weight-only quantization kernels on other devices requires additional
          effort.</p>

          '
        raw: "The current implementation uses CUDA kernels for the quantized model,\
          \ so it's GPU-compatible. \nIn theory, Falcon-180b with w3a16g512 quantization\
          \ can operate on any device with over 80GB of free memory. However, adapting\
          \ the weight-only quantization kernels on other devices requires additional\
          \ effort."
        updatedAt: '2023-09-12T10:20:38.236Z'
      numEdits: 0
      reactions: []
    id: 65003b76cc88b84c06b8c512
    type: comment
  author: ChenMnZ
  content: "The current implementation uses CUDA kernels for the quantized model,\
    \ so it's GPU-compatible. \nIn theory, Falcon-180b with w3a16g512 quantization\
    \ can operate on any device with over 80GB of free memory. However, adapting the\
    \ weight-only quantization kernels on other devices requires additional effort."
  created_at: 2023-09-12 09:20:38+00:00
  edited: false
  hidden: false
  id: 65003b76cc88b84c06b8c512
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8e3b018051f89a227f415d1dcf7cf99c.svg
      fullname: Enju
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Naugustogi
      type: user
    createdAt: '2023-09-12T23:48:48.000Z'
    data:
      edited: false
      editors:
      - Naugustogi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9811311960220337
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8e3b018051f89a227f415d1dcf7cf99c.svg
          fullname: Enju
          isHf: false
          isPro: false
          name: Naugustogi
          type: user
        html: '<p>How to run any of this?</p>

          '
        raw: How to run any of this?
        updatedAt: '2023-09-12T23:48:48.510Z'
      numEdits: 0
      reactions: []
    id: 6500f8e00e8369f6a8036f18
    type: comment
  author: Naugustogi
  content: How to run any of this?
  created_at: 2023-09-12 22:48:48+00:00
  edited: false
  hidden: false
  id: 6500f8e00e8369f6a8036f18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c8dc0df57596c526b2bccea21835f53.svg
      fullname: Mengzhao Chen
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ChenMnZ
      type: user
    createdAt: '2023-09-13T01:57:02.000Z'
    data:
      edited: false
      editors:
      - ChenMnZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6602672338485718
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c8dc0df57596c526b2bccea21835f53.svg
          fullname: Mengzhao Chen
          isHf: false
          isPro: false
          name: ChenMnZ
          type: user
        html: '<p>Refer <a rel="nofollow" href="https://github.com/OpenGVLab/OmniQuant/blob/main/runing_falcon180b_on_single_a100_80g.ipynb">https://github.com/OpenGVLab/OmniQuant/blob/main/runing_falcon180b_on_single_a100_80g.ipynb</a>
          for more details.</p>

          '
        raw: Refer [https://github.com/OpenGVLab/OmniQuant/blob/main/runing_falcon180b_on_single_a100_80g.ipynb](https://github.com/OpenGVLab/OmniQuant/blob/main/runing_falcon180b_on_single_a100_80g.ipynb)
          for more details.
        updatedAt: '2023-09-13T01:57:02.511Z'
      numEdits: 0
      reactions: []
    id: 650116eed117b28d30134290
    type: comment
  author: ChenMnZ
  content: Refer [https://github.com/OpenGVLab/OmniQuant/blob/main/runing_falcon180b_on_single_a100_80g.ipynb](https://github.com/OpenGVLab/OmniQuant/blob/main/runing_falcon180b_on_single_a100_80g.ipynb)
    for more details.
  created_at: 2023-09-13 00:57:02+00:00
  edited: false
  hidden: false
  id: 650116eed117b28d30134290
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c692a0c5cd9e574b00eeef515b743e8d.svg
      fullname: haurais
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TK4000
      type: user
    createdAt: '2023-09-13T05:50:05.000Z'
    data:
      edited: false
      editors:
      - TK4000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9721758961677551
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c692a0c5cd9e574b00eeef515b743e8d.svg
          fullname: haurais
          isHf: false
          isPro: false
          name: TK4000
          type: user
        html: '<p>Thanks, i will wait until i can find a version  embeded the metal  librairy,
          perhaps MLC</p>

          '
        raw: Thanks, i will wait until i can find a version  embeded the metal  librairy,
          perhaps MLC
        updatedAt: '2023-09-13T05:50:05.896Z'
      numEdits: 0
      reactions: []
    id: 65014d8d85a884a964e1ee8b
    type: comment
  author: TK4000
  content: Thanks, i will wait until i can find a version  embeded the metal  librairy,
    perhaps MLC
  created_at: 2023-09-13 04:50:05+00:00
  edited: false
  hidden: false
  id: 65014d8d85a884a964e1ee8b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/efacf65b052b236d859b56506576fa97.svg
      fullname: gnome
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gnomealone
      type: user
    createdAt: '2023-09-17T15:50:06.000Z'
    data:
      edited: false
      editors:
      - gnomealone
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9480355978012085
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/efacf65b052b236d859b56506576fa97.svg
          fullname: gnome
          isHf: false
          isPro: false
          name: gnomealone
          type: user
        html: '<blockquote>

          <p>The current implementation uses CUDA kernels for the quantized model,
          so it''s GPU-compatible.<br>In theory, Falcon-180b with w3a16g512 quantization
          can operate on any device with over 80GB of free memory. However, adapting
          the weight-only quantization kernels on other devices requires additional
          effort.</p>

          </blockquote>

          <p>are those additional efforts being made or planned by your team?</p>

          '
        raw: "> The current implementation uses CUDA kernels for the quantized model,\
          \ so it's GPU-compatible. \n> In theory, Falcon-180b with w3a16g512 quantization\
          \ can operate on any device with over 80GB of free memory. However, adapting\
          \ the weight-only quantization kernels on other devices requires additional\
          \ effort.\n\nare those additional efforts being made or planned by your\
          \ team?"
        updatedAt: '2023-09-17T15:50:06.699Z'
      numEdits: 0
      reactions: []
    id: 6507202eaf387baff48fd492
    type: comment
  author: gnomealone
  content: "> The current implementation uses CUDA kernels for the quantized model,\
    \ so it's GPU-compatible. \n> In theory, Falcon-180b with w3a16g512 quantization\
    \ can operate on any device with over 80GB of free memory. However, adapting the\
    \ weight-only quantization kernels on other devices requires additional effort.\n\
    \nare those additional efforts being made or planned by your team?"
  created_at: 2023-09-17 14:50:06+00:00
  edited: false
  hidden: false
  id: 6507202eaf387baff48fd492
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c8dc0df57596c526b2bccea21835f53.svg
      fullname: Mengzhao Chen
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ChenMnZ
      type: user
    createdAt: '2023-09-19T04:29:45.000Z'
    data:
      edited: false
      editors:
      - ChenMnZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9841384887695312
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c8dc0df57596c526b2bccea21835f53.svg
          fullname: Mengzhao Chen
          isHf: false
          isPro: false
          name: ChenMnZ
          type: user
        html: '<p>No, we currently have no plan about doing this by ourself.<br>MLC
          LLM is an excellent platform which support various devices. We can wait
          until MLC LLM supports Falcon-180B.</p>

          '
        raw: 'No, we currently have no plan about doing this by ourself.

          MLC LLM is an excellent platform which support various devices. We can wait
          until MLC LLM supports Falcon-180B.'
        updatedAt: '2023-09-19T04:29:45.226Z'
      numEdits: 0
      reactions: []
    id: 650923b9e64ee37323b21146
    type: comment
  author: ChenMnZ
  content: 'No, we currently have no plan about doing this by ourself.

    MLC LLM is an excellent platform which support various devices. We can wait until
    MLC LLM supports Falcon-180B.'
  created_at: 2023-09-19 03:29:45+00:00
  edited: false
  hidden: false
  id: 650923b9e64ee37323b21146
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ChenMnZ/OmniQuant
repo_type: model
status: open
target_branch: null
title: more models?
