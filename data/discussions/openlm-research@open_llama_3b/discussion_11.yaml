!!python/object:huggingface_hub.community.DiscussionWithDetails
author: deepakkaura26
conflicting_files: null
created_at: 2023-08-09 13:49:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
      fullname: Deepak  Kaura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkaura26
      type: user
    createdAt: '2023-08-09T14:49:45.000Z'
    data:
      edited: false
      editors:
      - deepakkaura26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5423018932342529
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
          fullname: Deepak  Kaura
          isHf: false
          isPro: false
          name: deepakkaura26
          type: user
        html: '<p>import torch<br>from transformers import LlamaTokenizer, LlamaForCausalLM</p>

          <p>model_path = ''openlm-research/open_llama_3b''</p>

          <h1 id="model_path--openlm-researchopen_llama_7b">model_path = ''openlm-research/open_llama_7b''</h1>

          <p>tokenizer = LlamaTokenizer.from_pretrained(model_path)<br>model = LlamaForCausalLM.from_pretrained(<br>    model_path,
          torch_dtype=torch.float16, device_map=''auto'',<br>)</p>

          <p>prompt = ''Q: What is the largest animal?\nA:''<br>input_ids = tokenizer(prompt,
          return_tensors="pt").input_ids</p>

          <p>generation_output = model.generate(<br>    input_ids=input_ids, max_new_tokens=32<br>)<br>print(tokenizer.decode(generation_output[0]))</p>

          <p>Can someone help me with this </p>

          <p>"How to run above codes on colab, means... CPU or GPU and what libraries
          needs to install?"</p>

          '
        raw: "import torch\r\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\r\
          \n\r\nmodel_path = 'openlm-research/open_llama_3b'\r\n# model_path = 'openlm-research/open_llama_7b'\r\
          \n\r\ntokenizer = LlamaTokenizer.from_pretrained(model_path)\r\nmodel =\
          \ LlamaForCausalLM.from_pretrained(\r\n    model_path, torch_dtype=torch.float16,\
          \ device_map='auto',\r\n)\r\n\r\nprompt = 'Q: What is the largest animal?\\\
          nA:'\r\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\r\
          \n\r\ngeneration_output = model.generate(\r\n    input_ids=input_ids, max_new_tokens=32\r\
          \n)\r\nprint(tokenizer.decode(generation_output[0]))\r\n\r\n\r\nCan someone\
          \ help me with this \r\n\r\n\"How to run above codes on colab, means...\
          \ CPU or GPU and what libraries needs to install?\""
        updatedAt: '2023-08-09T14:49:45.522Z'
      numEdits: 0
      reactions: []
    id: 64d3a789ac500d78b2796fd4
    type: comment
  author: deepakkaura26
  content: "import torch\r\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\r\
    \n\r\nmodel_path = 'openlm-research/open_llama_3b'\r\n# model_path = 'openlm-research/open_llama_7b'\r\
    \n\r\ntokenizer = LlamaTokenizer.from_pretrained(model_path)\r\nmodel = LlamaForCausalLM.from_pretrained(\r\
    \n    model_path, torch_dtype=torch.float16, device_map='auto',\r\n)\r\n\r\nprompt\
    \ = 'Q: What is the largest animal?\\nA:'\r\ninput_ids = tokenizer(prompt, return_tensors=\"\
    pt\").input_ids\r\n\r\ngeneration_output = model.generate(\r\n    input_ids=input_ids,\
    \ max_new_tokens=32\r\n)\r\nprint(tokenizer.decode(generation_output[0]))\r\n\r\
    \n\r\nCan someone help me with this \r\n\r\n\"How to run above codes on colab,\
    \ means... CPU or GPU and what libraries needs to install?\""
  created_at: 2023-08-09 13:49:45+00:00
  edited: false
  hidden: false
  id: 64d3a789ac500d78b2796fd4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/847d7903e413cb984cc7da06b39102b2.svg
      fullname: Team
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Laza2023LLM
      type: user
    createdAt: '2023-10-06T11:19:23.000Z'
    data:
      edited: false
      editors:
      - Laza2023LLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.548313558101654
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/847d7903e413cb984cc7da06b39102b2.svg
          fullname: Team
          isHf: false
          isPro: false
          name: Laza2023LLM
          type: user
        html: '<p>Hi, </p>

          <p>Please check this : <a rel="nofollow" href="https://colab.research.google.com/drive/1XwG2dAejKa_dkowpfewv5K1Rd8_qQTq3?usp=sharing">Google
          colab notebook to run Openllama model</a></p>

          <p>Enjoy.</p>

          '
        raw: "Hi, \n\nPlease check this : [Google colab notebook to run Openllama\
          \ model](https://colab.research.google.com/drive/1XwG2dAejKa_dkowpfewv5K1Rd8_qQTq3?usp=sharing)\n\
          \nEnjoy.\n\n"
        updatedAt: '2023-10-06T11:19:23.877Z'
      numEdits: 0
      reactions: []
    id: 651fed3b725b9ebd6e09a000
    type: comment
  author: Laza2023LLM
  content: "Hi, \n\nPlease check this : [Google colab notebook to run Openllama model](https://colab.research.google.com/drive/1XwG2dAejKa_dkowpfewv5K1Rd8_qQTq3?usp=sharing)\n\
    \nEnjoy.\n\n"
  created_at: 2023-10-06 10:19:23+00:00
  edited: false
  hidden: false
  id: 651fed3b725b9ebd6e09a000
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: openlm-research/open_llama_3b
repo_type: model
status: open
target_branch: null
title: How to run on colab ?
