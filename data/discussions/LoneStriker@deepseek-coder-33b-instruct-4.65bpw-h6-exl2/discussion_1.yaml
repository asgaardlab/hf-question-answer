!!python/object:huggingface_hub.community.DiscussionWithDetails
author: VertexMachine
conflicting_files: null
created_at: 2023-12-16 13:25:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41e0e701938486283ec2292f12f3f716.svg
      fullname: Bartosz Broda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VertexMachine
      type: user
    createdAt: '2023-12-16T13:25:26.000Z'
    data:
      edited: false
      editors:
      - VertexMachine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.34023603796958923
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41e0e701938486283ec2292f12f3f716.svg
          fullname: Bartosz Broda
          isHf: false
          isPro: false
          name: VertexMachine
          type: user
        html: "<p>Not sure if this is the problem with the model or how I'm using\
          \ it, but when I select it in oobabooga I get:</p>\n<pre><code>Traceback\
          \ (most recent call last):\n  File \"F:\\LLMs\\oobabooga\\installer_files\\\
          env\\Lib\\site-packages\\gradio\\queueing.py\", line 407, in call_prediction\n\
          \    output = await route_utils.call_process_api(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"F:\\LLMs\\oobabooga\\installer_files\\env\\Lib\\site-packages\\\
          gradio\\route_utils.py\", line 226, in call_process_api\n    output = await\
          \ app.get_blocks().process_api(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"F:\\LLMs\\oobabooga\\installer_files\\env\\Lib\\site-packages\\\
          gradio\\blocks.py\", line 1550, in process_api\n    result = await self.call_function(\n\
          \             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\LLMs\\oobabooga\\\
          installer_files\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1185,\
          \ in call_function\n    prediction = await anyio.to_thread.run_sync(\n \
          \                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\LLMs\\oobabooga\\\
          installer_files\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 33,\
          \ in run_sync\n    return await get_asynclib().run_sync_in_worker_thread(\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\\
          LLMs\\oobabooga\\installer_files\\env\\Lib\\site-packages\\anyio\\_backends\\\
          _asyncio.py\", line 877, in run_sync_in_worker_thread\n    return await\
          \ future\n           ^^^^^^^^^^^^\n  File \"F:\\LLMs\\oobabooga\\installer_files\\\
          env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n\
          \    result = context.run(func, *args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"F:\\LLMs\\oobabooga\\installer_files\\env\\Lib\\site-packages\\\
          gradio\\utils.py\", line 661, in wrapper\n    response = f(*args, **kwargs)\n\
          \               ^^^^^^^^^^^^^^^^^^\n  File \"F:\\LLMs\\oobabooga\\modules\\\
          models_settings.py\", line 216, in apply_model_settings_to_state\n    model_settings\
          \ = get_model_metadata(model)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"F:\\LLMs\\oobabooga\\modules\\models_settings.py\", line 105,\
          \ in get_model_metadata\n    metadata = json.loads(open(path, 'r').read())\n\
          \                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\LLMs\\oobabooga\\\
          installer_files\\env\\Lib\\encodings\\cp1252.py\", line 23, in decode\n\
          \    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nUnicodeDecodeError:\
          \ 'charmap' codec can't decode byte 0x81 in position 125: character maps\
          \ to &lt;undefined&gt;\n</code></pre>\n"
        raw: "Not sure if this is the problem with the model or how I'm using it,\
          \ but when I select it in oobabooga I get:\r\n\r\n```\r\nTraceback (most\
          \ recent call last):\r\n  File \"F:\\LLMs\\oobabooga\\installer_files\\\
          env\\Lib\\site-packages\\gradio\\queueing.py\", line 407, in call_prediction\r\
          \n    output = await route_utils.call_process_api(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"F:\\LLMs\\oobabooga\\installer_files\\env\\Lib\\site-packages\\\
          gradio\\route_utils.py\", line 226, in call_process_api\r\n    output =\
          \ await app.get_blocks().process_api(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"F:\\LLMs\\oobabooga\\installer_files\\env\\Lib\\site-packages\\\
          gradio\\blocks.py\", line 1550, in process_api\r\n    result = await self.call_function(\r\
          \n             ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\LLMs\\oobabooga\\\
          installer_files\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1185,\
          \ in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\
          \n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\LLMs\\\
          oobabooga\\installer_files\\env\\Lib\\site-packages\\anyio\\to_thread.py\"\
          , line 33, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
          F:\\LLMs\\oobabooga\\installer_files\\env\\Lib\\site-packages\\anyio\\_backends\\\
          _asyncio.py\", line 877, in run_sync_in_worker_thread\r\n    return await\
          \ future\r\n           ^^^^^^^^^^^^\r\n  File \"F:\\LLMs\\oobabooga\\installer_files\\\
          env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\r\
          \n    result = context.run(func, *args)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"F:\\LLMs\\oobabooga\\installer_files\\env\\Lib\\site-packages\\\
          gradio\\utils.py\", line 661, in wrapper\r\n    response = f(*args, **kwargs)\r\
          \n               ^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\LLMs\\oobabooga\\modules\\\
          models_settings.py\", line 216, in apply_model_settings_to_state\r\n   \
          \ model_settings = get_model_metadata(model)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"F:\\LLMs\\oobabooga\\modules\\models_settings.py\", line 105,\
          \ in get_model_metadata\r\n    metadata = json.loads(open(path, 'r').read())\r\
          \n                          ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\LLMs\\\
          oobabooga\\installer_files\\env\\Lib\\encodings\\cp1252.py\", line 23, in\
          \ decode\r\n    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\
          UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 125:\
          \ character maps to <undefined>\r\n```"
        updatedAt: '2023-12-16T13:25:26.117Z'
      numEdits: 0
      reactions: []
    id: 657da546504da7f6f3ac8e5a
    type: comment
  author: VertexMachine
  content: "Not sure if this is the problem with the model or how I'm using it, but\
    \ when I select it in oobabooga I get:\r\n\r\n```\r\nTraceback (most recent call\
    \ last):\r\n  File \"F:\\LLMs\\oobabooga\\installer_files\\env\\Lib\\site-packages\\\
    gradio\\queueing.py\", line 407, in call_prediction\r\n    output = await route_utils.call_process_api(\r\
    \n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\LLMs\\oobabooga\\\
    installer_files\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 226,\
    \ in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n\
    \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\LLMs\\oobabooga\\\
    installer_files\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1550, in process_api\r\
    \n    result = await self.call_function(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"F:\\LLMs\\oobabooga\\installer_files\\env\\Lib\\site-packages\\gradio\\\
    blocks.py\", line 1185, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\
    \n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\LLMs\\oobabooga\\\
    installer_files\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\r\
    \n    return await get_asynclib().run_sync_in_worker_thread(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"F:\\LLMs\\oobabooga\\installer_files\\env\\Lib\\site-packages\\anyio\\\
    _backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\r\n    return\
    \ await future\r\n           ^^^^^^^^^^^^\r\n  File \"F:\\LLMs\\oobabooga\\installer_files\\\
    env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\r\n\
    \    result = context.run(func, *args)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"F:\\LLMs\\oobabooga\\installer_files\\env\\Lib\\site-packages\\gradio\\\
    utils.py\", line 661, in wrapper\r\n    response = f(*args, **kwargs)\r\n    \
    \           ^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\LLMs\\oobabooga\\modules\\models_settings.py\"\
    , line 216, in apply_model_settings_to_state\r\n    model_settings = get_model_metadata(model)\r\
    \n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\LLMs\\oobabooga\\\
    modules\\models_settings.py\", line 105, in get_model_metadata\r\n    metadata\
    \ = json.loads(open(path, 'r').read())\r\n                          ^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"F:\\LLMs\\oobabooga\\installer_files\\env\\Lib\\encodings\\cp1252.py\"\
    , line 23, in decode\r\n    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nUnicodeDecodeError:\
    \ 'charmap' codec can't decode byte 0x81 in position 125: character maps to <undefined>\r\
    \n```"
  created_at: 2023-12-16 13:25:26+00:00
  edited: false
  hidden: false
  id: 657da546504da7f6f3ac8e5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-16T13:32:24.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8543894290924072
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Model works fine for me under ooba.  Please make sure your ooba
          and exllamav2 loader are up-to-date.</p>

          '
        raw: Model works fine for me under ooba.  Please make sure your ooba and exllamav2
          loader are up-to-date.
        updatedAt: '2023-12-16T13:32:24.901Z'
      numEdits: 0
      reactions: []
    id: 657da6e8869d5bb0e55d621c
    type: comment
  author: LoneStriker
  content: Model works fine for me under ooba.  Please make sure your ooba and exllamav2
    loader are up-to-date.
  created_at: 2023-12-16 13:32:24+00:00
  edited: false
  hidden: false
  id: 657da6e8869d5bb0e55d621c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41e0e701938486283ec2292f12f3f716.svg
      fullname: Bartosz Broda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VertexMachine
      type: user
    createdAt: '2023-12-16T14:06:18.000Z'
    data:
      edited: false
      editors:
      - VertexMachine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7302574515342712
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41e0e701938486283ec2292f12f3f716.svg
          fullname: Bartosz Broda
          isHf: false
          isPro: false
          name: VertexMachine
          type: user
        html: '<p>I''ve updated earlier today (with update_windows.bat), but just
          checked now:</p>

          <pre><code>(F:\LLMs\oobabooga\installer_files\env) F:\LLMs\oobabooga&gt;pip
          list | grep exll

          exllama                   0.0.18+cu121

          exllamav2                 0.0.10+cu121

          </code></pre>

          <p>and</p>

          <pre><code>(F:\LLMs\oobabooga\installer_files\env) F:\LLMs\oobabooga&gt;git
          pull

          Already up to date.

          </code></pre>

          '
        raw: 'I''ve updated earlier today (with update_windows.bat), but just checked
          now:


          ```

          (F:\LLMs\oobabooga\installer_files\env) F:\LLMs\oobabooga>pip list | grep
          exll

          exllama                   0.0.18+cu121

          exllamav2                 0.0.10+cu121

          ```


          and


          ```

          (F:\LLMs\oobabooga\installer_files\env) F:\LLMs\oobabooga>git pull

          Already up to date.

          ```'
        updatedAt: '2023-12-16T14:06:18.182Z'
      numEdits: 0
      reactions: []
    id: 657daedad70b7308f35c0fff
    type: comment
  author: VertexMachine
  content: 'I''ve updated earlier today (with update_windows.bat), but just checked
    now:


    ```

    (F:\LLMs\oobabooga\installer_files\env) F:\LLMs\oobabooga>pip list | grep exll

    exllama                   0.0.18+cu121

    exllamav2                 0.0.10+cu121

    ```


    and


    ```

    (F:\LLMs\oobabooga\installer_files\env) F:\LLMs\oobabooga>git pull

    Already up to date.

    ```'
  created_at: 2023-12-16 14:06:18+00:00
  edited: false
  hidden: false
  id: 657daedad70b7308f35c0fff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-16T14:09:32.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9530936479568481
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>The stack trace seems to indicate an issue reading the model''s
          config. I don''t know if it''s the config.yaml in ooba or the model''s config
          json it''s having issues with. You can put a <code>print(path)</code> statement
          in the models_settings before line 105 to see what file it isn''t happy
          with.</p>

          '
        raw: The stack trace seems to indicate an issue reading the model's config.
          I don't know if it's the config.yaml in ooba or the model's config json
          it's having issues with. You can put a `print(path)` statement in the models_settings
          before line 105 to see what file it isn't happy with.
        updatedAt: '2023-12-16T14:09:32.958Z'
      numEdits: 0
      reactions: []
    id: 657daf9c9a9e347d4b9fc544
    type: comment
  author: LoneStriker
  content: The stack trace seems to indicate an issue reading the model's config.
    I don't know if it's the config.yaml in ooba or the model's config json it's having
    issues with. You can put a `print(path)` statement in the models_settings before
    line 105 to see what file it isn't happy with.
  created_at: 2023-12-16 14:09:32+00:00
  edited: false
  hidden: false
  id: 657daf9c9a9e347d4b9fc544
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41e0e701938486283ec2292f12f3f716.svg
      fullname: Bartosz Broda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VertexMachine
      type: user
    createdAt: '2023-12-16T14:51:23.000Z'
    data:
      edited: false
      editors:
      - VertexMachine
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8070871829986572
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41e0e701938486283ec2292f12f3f716.svg
          fullname: Bartosz Broda
          isHf: false
          isPro: false
          name: VertexMachine
          type: user
        html: '<p>It was complaining about models\LoneStriker_deepseek-coder-33b-instruct-4.65bpw-h6-exl2\tokenizer_config.json</p>

          <p>...</p>

          <p>it''s bug in ooba</p>

          <p>After I added encoding=''utf-8'' to </p>

          <p>metadata = json.loads(open(path, ''r'', encoding=''utf-8'').read()) </p>

          <p>it works.</p>

          <p>Thanks for your help!</p>

          '
        raw: "It was complaining about models\\LoneStriker_deepseek-coder-33b-instruct-4.65bpw-h6-exl2\\\
          tokenizer_config.json\n\n...\n\nit's bug in ooba\n\nAfter I added encoding='utf-8'\
          \ to \n\nmetadata = json.loads(open(path, 'r', encoding='utf-8').read())\
          \ \n\nit works.\n\nThanks for your help!"
        updatedAt: '2023-12-16T14:51:23.299Z'
      numEdits: 0
      reactions: []
      relatedEventId: 657db96ba982e9093f67529a
    id: 657db96ba982e9093f675299
    type: comment
  author: VertexMachine
  content: "It was complaining about models\\LoneStriker_deepseek-coder-33b-instruct-4.65bpw-h6-exl2\\\
    tokenizer_config.json\n\n...\n\nit's bug in ooba\n\nAfter I added encoding='utf-8'\
    \ to \n\nmetadata = json.loads(open(path, 'r', encoding='utf-8').read()) \n\n\
    it works.\n\nThanks for your help!"
  created_at: 2023-12-16 14:51:23+00:00
  edited: false
  hidden: false
  id: 657db96ba982e9093f675299
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/41e0e701938486283ec2292f12f3f716.svg
      fullname: Bartosz Broda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VertexMachine
      type: user
    createdAt: '2023-12-16T14:51:23.000Z'
    data:
      status: closed
    id: 657db96ba982e9093f67529a
    type: status-change
  author: VertexMachine
  created_at: 2023-12-16 14:51:23+00:00
  id: 657db96ba982e9093f67529a
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/649a54b896d5747b35e2163b/tdZmsov6fN1VHztaE5kX9.jpeg?w=200&h=200&f=face
      fullname: Vezora
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vezora
      type: user
    createdAt: '2023-12-27T05:34:11.000Z'
    data:
      edited: false
      editors:
      - Vezora
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7005209922790527
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/649a54b896d5747b35e2163b/tdZmsov6fN1VHztaE5kX9.jpeg?w=200&h=200&f=face
          fullname: Vezora
          isHf: false
          isPro: false
          name: Vezora
          type: user
        html: '<p>What ooba file did you add this too "metadata = json.loads(open(path,
          ''r'', encoding=''utf-8'').read())"</p>

          '
        raw: What ooba file did you add this too "metadata = json.loads(open(path,
          'r', encoding='utf-8').read())"
        updatedAt: '2023-12-27T05:34:11.033Z'
      numEdits: 0
      reactions: []
    id: 658bb753cffe7d22087f1d34
    type: comment
  author: Vezora
  content: What ooba file did you add this too "metadata = json.loads(open(path, 'r',
    encoding='utf-8').read())"
  created_at: 2023-12-27 05:34:11+00:00
  edited: false
  hidden: false
  id: 658bb753cffe7d22087f1d34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-12-29T00:28:05.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6273342967033386
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Vezora&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Vezora\">@<span class=\"\
          underline\">Vezora</span></a></span>\n\n\t</span></span><br>Look at this\
          \ you'll get the solution<br><a rel=\"nofollow\" href=\"https://github.com/oobabooga/text-generation-webui/issues/4947#issuecomment-1871623484\"\
          >https://github.com/oobabooga/text-generation-webui/issues/4947#issuecomment-1871623484</a></p>\n"
        raw: "@Vezora \nLook at this you'll get the solution\nhttps://github.com/oobabooga/text-generation-webui/issues/4947#issuecomment-1871623484\n\
          \n"
        updatedAt: '2023-12-29T00:28:05.861Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - LoneStriker
    id: 658e129550d39af7f49cd33b
    type: comment
  author: TheYuriLover
  content: "@Vezora \nLook at this you'll get the solution\nhttps://github.com/oobabooga/text-generation-webui/issues/4947#issuecomment-1871623484\n\
    \n"
  created_at: 2023-12-29 00:28:05+00:00
  edited: false
  hidden: false
  id: 658e129550d39af7f49cd33b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/649a54b896d5747b35e2163b/tdZmsov6fN1VHztaE5kX9.jpeg?w=200&h=200&f=face
      fullname: Vezora
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vezora
      type: user
    createdAt: '2024-01-01T06:41:33.000Z'
    data:
      edited: false
      editors:
      - Vezora
      hidden: false
      identifiedLanguage:
        language: es
        probability: 0.4021458029747009
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/649a54b896d5747b35e2163b/tdZmsov6fN1VHztaE5kX9.jpeg?w=200&h=200&f=face
          fullname: Vezora
          isHf: false
          isPro: false
          name: Vezora
          type: user
        html: '<p>Thank you:)</p>

          '
        raw: Thank you:)
        updatedAt: '2024-01-01T06:41:33.846Z'
      numEdits: 0
      reactions: []
    id: 65925e9d89145cbc7c16b318
    type: comment
  author: Vezora
  content: Thank you:)
  created_at: 2024-01-01 06:41:33+00:00
  edited: false
  hidden: false
  id: 65925e9d89145cbc7c16b318
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/deepseek-coder-33b-instruct-4.65bpw-h6-exl2
repo_type: model
status: closed
target_branch: null
title: Charmap issue
