!!python/object:huggingface_hub.community.DiscussionWithDetails
author: clown134
conflicting_files: null
created_at: 2023-09-27 16:33:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90cf9700f14d6bf04c9f6d7eaef0f345.svg
      fullname: katie barber
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: clown134
      type: user
    createdAt: '2023-09-27T17:33:06.000Z'
    data:
      edited: true
      editors:
      - clown134
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.20058535039424896
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90cf9700f14d6bf04c9f6d7eaef0f345.svg
          fullname: katie barber
          isHf: false
          isPro: false
          name: clown134
          type: user
        html: '<p>hi ive been googling for hours but i cant seem to track down whats
          causing the issue</p>

          <p>it seems like it wants to load. but something goes wrong at the very
          last second </p>

          <p>log ~~~~~~~~~~~~~~~~~~~<br>Running on local URL:  <a rel="nofollow" href="http://127.0.0.1:7860">http://127.0.0.1:7860</a></p>

          <p>To create a public link, set <code>share=True</code> in <code>launch()</code>.<br>2023-09-27
          12:31:21 INFO:Loading xwin-lm-7b-v0.1.Q4_K_M.gguf...<br>2023-09-27 12:31:21
          INFO:llama.cpp weights detected: models\xwin-lm-7b-v0.1.Q4_K_M.gguf<br>2023-09-27
          12:31:21 INFO:Cache capacity is 0 bytes<br>ggml_init_cublas: found 1 CUDA
          devices:<br>  Device 0: NVIDIA GeForce GTX 1060 6GB, compute capability
          6.1<br>llama_model_loader: loaded meta data with 19 key-value pairs and
          291 tensors from models\xwin-lm-7b-v0.1.Q4_K_M.gguf (version GGUF V2 (latest))<br>llama_model_loader:
          - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1
          ]<br>llama_model_loader: - tensor    1:              blk.0.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor    2:              blk.0.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor    3:              blk.0.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor    4:         blk.0.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor    6:              blk.0.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor    7:            blk.0.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor    8:           blk.0.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   10:              blk.1.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   11:              blk.1.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   12:              blk.1.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   13:         blk.1.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   15:              blk.1.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   16:            blk.1.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor   17:           blk.1.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   19:              blk.2.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   20:              blk.2.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   21:              blk.2.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   22:         blk.2.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   24:              blk.2.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   25:            blk.2.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor   26:           blk.2.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   28:              blk.3.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   29:              blk.3.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   30:              blk.3.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   31:         blk.3.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   33:              blk.3.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   34:            blk.3.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor   35:           blk.3.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   37:              blk.4.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   38:              blk.4.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   39:              blk.4.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   40:         blk.4.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   42:              blk.4.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   43:            blk.4.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor   44:           blk.4.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   46:              blk.5.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   47:              blk.5.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   48:              blk.5.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   49:         blk.5.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   51:              blk.5.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   52:            blk.5.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor   53:           blk.5.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   55:              blk.6.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   56:              blk.6.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   57:              blk.6.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   58:         blk.6.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   60:              blk.6.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   61:            blk.6.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor   62:           blk.6.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   64:              blk.7.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   65:              blk.7.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   66:              blk.7.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   67:         blk.7.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   69:              blk.7.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   70:            blk.7.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor   71:           blk.7.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   73:              blk.8.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   74:              blk.8.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   75:              blk.8.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   76:         blk.8.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   78:              blk.8.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   79:            blk.8.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor   80:           blk.8.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   82:              blk.9.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   83:              blk.9.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   84:              blk.9.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   85:         blk.9.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   87:              blk.9.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   88:            blk.9.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor   89:           blk.9.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   91:             blk.10.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   92:             blk.10.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   93:             blk.10.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   94:        blk.10.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   96:             blk.10.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor   97:           blk.10.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor   98:          blk.10.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  100:             blk.11.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  101:             blk.11.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  102:             blk.11.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  103:        blk.11.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  105:             blk.11.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  106:           blk.11.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  107:          blk.11.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  109:             blk.12.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  110:             blk.12.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  111:             blk.12.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  112:        blk.12.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  114:             blk.12.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  115:           blk.12.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  116:          blk.12.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  118:             blk.13.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  119:             blk.13.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  120:             blk.13.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  121:        blk.13.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  123:             blk.13.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  124:           blk.13.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  125:          blk.13.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  127:             blk.14.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  128:             blk.14.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  129:             blk.14.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  130:        blk.14.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  132:             blk.14.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  133:           blk.14.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  134:          blk.14.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  136:             blk.15.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  137:             blk.15.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  138:             blk.15.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  139:        blk.15.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  141:             blk.15.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  142:           blk.15.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  143:          blk.15.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  145:             blk.16.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  146:             blk.16.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  147:             blk.16.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  148:        blk.16.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  150:             blk.16.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  151:           blk.16.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  152:          blk.16.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  154:             blk.17.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  155:             blk.17.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  156:             blk.17.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  157:        blk.17.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  159:             blk.17.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  160:           blk.17.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  161:          blk.17.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  163:             blk.18.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  164:             blk.18.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  165:             blk.18.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  166:        blk.18.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  168:             blk.18.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  169:           blk.18.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  170:          blk.18.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  172:             blk.19.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  173:             blk.19.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  174:             blk.19.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  175:        blk.19.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  177:             blk.19.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  178:           blk.19.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  179:          blk.19.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  181:             blk.20.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  182:             blk.20.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  183:             blk.20.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  184:        blk.20.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  186:             blk.20.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  187:           blk.20.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  188:          blk.20.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  190:             blk.21.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  191:             blk.21.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  192:             blk.21.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  193:        blk.21.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  195:             blk.21.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  196:           blk.21.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  197:          blk.21.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  199:             blk.22.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  200:             blk.22.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  201:             blk.22.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  202:        blk.22.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  204:             blk.22.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  205:           blk.22.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  206:          blk.22.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  208:             blk.23.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  209:             blk.23.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  210:             blk.23.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  211:        blk.23.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  213:             blk.23.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  214:           blk.23.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  215:          blk.23.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  217:             blk.24.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  218:             blk.24.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  219:             blk.24.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  220:        blk.24.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  222:             blk.24.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  223:           blk.24.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  224:          blk.24.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  226:             blk.25.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  227:             blk.25.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  228:             blk.25.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  229:        blk.25.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  231:             blk.25.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  232:           blk.25.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  233:          blk.25.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  235:             blk.26.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  236:             blk.26.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  237:             blk.26.attn_v.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  238:        blk.26.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  240:             blk.26.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  241:           blk.26.ffn_down.weight
          q4_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  242:          blk.26.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  244:             blk.27.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  245:             blk.27.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  246:             blk.27.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  247:        blk.27.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  249:             blk.27.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  250:           blk.27.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  251:          blk.27.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  253:             blk.28.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  254:             blk.28.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  255:             blk.28.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  256:        blk.28.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  258:             blk.28.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  259:           blk.28.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  260:          blk.28.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  262:             blk.29.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  263:             blk.29.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  264:             blk.29.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  265:        blk.29.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  267:             blk.29.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  268:           blk.29.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  269:          blk.29.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  271:             blk.30.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  272:             blk.30.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  273:             blk.30.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  274:        blk.30.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  276:             blk.30.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  277:           blk.30.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  278:          blk.30.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  280:             blk.31.attn_q.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  281:             blk.31.attn_k.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  282:             blk.31.attn_v.weight
          q6_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  283:        blk.31.attn_output.weight
          q4_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  285:             blk.31.ffn_up.weight
          q4_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor  286:           blk.31.ffn_down.weight
          q6_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor  287:          blk.31.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  289:               output_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  290:                    output.weight
          q6_K     [  4096, 32000,     1,     1 ]<br>llama_model_loader: - kv   0:                       general.architecture
          str<br>llama_model_loader: - kv   1:                               general.name
          str<br>llama_model_loader: - kv   2:                       llama.context_length
          u32<br>llama_model_loader: - kv   3:                     llama.embedding_length
          u32<br>llama_model_loader: - kv   4:                          llama.block_count
          u32<br>llama_model_loader: - kv   5:                  llama.feed_forward_length
          u32<br>llama_model_loader: - kv   6:                 llama.rope.dimension_count
          u32<br>llama_model_loader: - kv   7:                 llama.attention.head_count
          u32<br>llama_model_loader: - kv   8:              llama.attention.head_count_kv
          u32<br>llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon
          f32<br>llama_model_loader: - kv  10:                          general.file_type
          u32<br>llama_model_loader: - kv  11:                       tokenizer.ggml.model
          str<br>llama_model_loader: - kv  12:                      tokenizer.ggml.tokens
          arr<br>llama_model_loader: - kv  13:                      tokenizer.ggml.scores
          arr<br>llama_model_loader: - kv  14:                  tokenizer.ggml.token_type
          arr<br>llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id
          u32<br>llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id
          u32<br>llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id
          u32<br>llama_model_loader: - kv  18:               general.quantization_version
          u32<br>llama_model_loader: - type  f32:   65 tensors<br>llama_model_loader:
          - type q4_K:  193 tensors<br>llama_model_loader: - type q6_K:   33 tensors<br>2023-09-27
          12:31:22 ERROR:Failed to load the model.<br>Traceback (most recent call
          last):<br>  File "E:\text-generation-webui-1.6.1\modules\ui_model_menu.py",
          line 198, in load_model_wrapper<br>    shared.model, shared.tokenizer =
          load_model(shared.model_name, loader)<br>  File "E:\text-generation-webui-1.6.1\modules\models.py",
          line 78, in load_model<br>    output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "E:\text-generation-webui-1.6.1\modules\models.py", line 232, in llamacpp_loader<br>    model,
          tokenizer = LlamaCppModel.from_pretrained(model_file)<br>  File "E:\text-generation-webui-1.6.1\modules\llamacpp_model.py",
          line 90, in from_pretrained<br>    result.model = Llama(**params)<br>  File
          "E:\text-generation-webui-1.6.1\installer_files\env\lib\site-packages\llama_cpp_cuda\llama.py",
          line 332, in <strong>init</strong><br>    self.model = llama_cpp.llama_load_model_from_file(<br>  File
          "E:\text-generation-webui-1.6.1\installer_files\env\lib\site-packages\llama_cpp_cuda\llama_cpp.py",
          line 434, in llama_load_model_from_file<br>    return _lib.llama_load_model_from_file(path_model,
          params)<br>OSError: [WinError -1073741795] Windows Error 0xc000001d</p>

          <p>Exception ignored in: &lt;function LlamaCppModel.__del__ at 0x000001D328E2BBE0&gt;<br>Traceback
          (most recent call last):<br>  File "E:\text-generation-webui-1.6.1\modules\llamacpp_model.py",
          line 49, in <strong>del</strong><br>    self.model.<strong>del</strong>()<br>AttributeError:
          ''LlamaCppModel'' object has no attribute ''model''</p>

          <p>----------line break<br>i have tried both directly inside the /models/
          folder, and inside a named subfolder with the included config.json, both
          same result.</p>

          <p>thanks for reading and trying to help in advance, i have also had this
          issue while trying another .gguf model, i have not been able to use any
          llama.cpp type yet.</p>

          <p>system amd fx 4800 cpu,<br>gtx 1060 6gb vram.<br>16gb sysram</p>

          <p>i was able to get a gguf model to work using the non llama.cpp loader,
          but i can no longer get even that to work as of this moment , and it ran
          completely on cpu/hdd </p>

          '
        raw: "hi ive been googling for hours but i cant seem to track down whats causing\
          \ the issue\n\nit seems like it wants to load. but something goes wrong\
          \ at the very last second \n\nlog ~~~~~~~~~~~~~~~~~~~\nRunning on local\
          \ URL:  http://127.0.0.1:7860\n\nTo create a public link, set `share=True`\
          \ in `launch()`.\n2023-09-27 12:31:21 INFO:Loading xwin-lm-7b-v0.1.Q4_K_M.gguf...\n\
          2023-09-27 12:31:21 INFO:llama.cpp weights detected: models\\xwin-lm-7b-v0.1.Q4_K_M.gguf\n\
          2023-09-27 12:31:21 INFO:Cache capacity is 0 bytes\nggml_init_cublas: found\
          \ 1 CUDA devices:\n  Device 0: NVIDIA GeForce GTX 1060 6GB, compute capability\
          \ 6.1\nllama_model_loader: loaded meta data with 19 key-value pairs and\
          \ 291 tensors from models\\xwin-lm-7b-v0.1.Q4_K_M.gguf (version GGUF V2\
          \ (latest))\nllama_model_loader: - tensor    0:                token_embd.weight\
          \ q4_K     [  4096, 32000,     1,     1 ]\nllama_model_loader: - tensor\
          \    1:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \    3:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor    4:         blk.0.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \    5:            blk.0.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \    7:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor    8:           blk.0.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   10:              blk.1.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   11:              blk.1.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   12:              blk.1.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   13:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   14:            blk.1.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   15:              blk.1.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   16:            blk.1.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   18:            blk.1.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   19:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   20:              blk.2.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   21:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   22:         blk.2.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   23:            blk.2.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   24:              blk.2.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   25:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   26:           blk.2.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   28:              blk.3.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   29:              blk.3.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   30:              blk.3.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   31:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   32:            blk.3.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   33:              blk.3.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   34:            blk.3.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   36:            blk.3.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   37:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   38:              blk.4.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   39:              blk.4.attn_v.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   40:         blk.4.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   41:            blk.4.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   42:              blk.4.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   43:            blk.4.ffn_down.weight q4_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   44:           blk.4.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   46:              blk.5.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   47:              blk.5.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   48:              blk.5.attn_v.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   49:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   50:            blk.5.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   51:              blk.5.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   52:            blk.5.ffn_down.weight\
          \ q4_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   54:            blk.5.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   55:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   56:              blk.6.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   57:              blk.6.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   58:         blk.6.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   59:            blk.6.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   60:              blk.6.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   61:            blk.6.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   62:           blk.6.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   64:              blk.7.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   65:              blk.7.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   66:              blk.7.attn_v.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   67:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   68:            blk.7.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   69:              blk.7.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   70:            blk.7.ffn_down.weight\
          \ q4_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   72:            blk.7.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   73:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   74:              blk.8.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   75:              blk.8.attn_v.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   76:         blk.8.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   77:            blk.8.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   78:              blk.8.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   79:            blk.8.ffn_down.weight q4_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   80:           blk.8.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   82:              blk.9.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   83:              blk.9.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   84:              blk.9.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   85:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   86:            blk.9.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   87:              blk.9.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   88:            blk.9.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   90:            blk.9.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   91:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   92:             blk.10.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   93:             blk.10.attn_v.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   94:        blk.10.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   95:           blk.10.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor   96:             blk.10.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \   97:           blk.10.ffn_down.weight q4_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   98:          blk.10.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  100:             blk.11.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  101:             blk.11.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  102:             blk.11.attn_v.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  103:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  104:           blk.11.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  105:             blk.11.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  106:           blk.11.ffn_down.weight\
          \ q4_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  108:           blk.11.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  109:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  110:             blk.12.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  111:             blk.12.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  112:        blk.12.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  113:           blk.12.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  114:             blk.12.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  115:           blk.12.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  116:          blk.12.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  118:             blk.13.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  119:             blk.13.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  120:             blk.13.attn_v.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  121:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  122:           blk.13.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  123:             blk.13.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  124:           blk.13.ffn_down.weight\
          \ q4_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  126:           blk.13.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  127:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  128:             blk.14.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  129:             blk.14.attn_v.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  130:        blk.14.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  131:           blk.14.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  132:             blk.14.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  133:           blk.14.ffn_down.weight q4_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  134:          blk.14.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  136:             blk.15.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  137:             blk.15.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  138:             blk.15.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  139:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  141:             blk.15.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  142:           blk.15.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  144:           blk.15.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  145:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  146:             blk.16.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  147:             blk.16.attn_v.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  148:        blk.16.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  149:           blk.16.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  150:             blk.16.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  151:           blk.16.ffn_down.weight q4_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  152:          blk.16.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  154:             blk.17.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  155:             blk.17.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  156:             blk.17.attn_v.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  157:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  158:           blk.17.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  159:             blk.17.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  160:           blk.17.ffn_down.weight\
          \ q4_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  162:           blk.17.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  163:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  164:             blk.18.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  165:             blk.18.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  166:        blk.18.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  167:           blk.18.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  168:             blk.18.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  169:           blk.18.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  170:          blk.18.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  172:             blk.19.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  173:             blk.19.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  174:             blk.19.attn_v.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  175:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  176:           blk.19.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  177:             blk.19.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  178:           blk.19.ffn_down.weight\
          \ q4_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  180:           blk.19.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  181:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  182:             blk.20.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  183:             blk.20.attn_v.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  184:        blk.20.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  185:           blk.20.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  186:             blk.20.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  187:           blk.20.ffn_down.weight q4_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  188:          blk.20.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  190:             blk.21.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  191:             blk.21.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  192:             blk.21.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  193:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  194:           blk.21.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  195:             blk.21.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  196:           blk.21.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  198:           blk.21.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  199:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  200:             blk.22.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  201:             blk.22.attn_v.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  202:        blk.22.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  203:           blk.22.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  204:             blk.22.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  205:           blk.22.ffn_down.weight q4_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  206:          blk.22.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  208:             blk.23.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  209:             blk.23.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  210:             blk.23.attn_v.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  211:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  212:           blk.23.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  213:             blk.23.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  214:           blk.23.ffn_down.weight\
          \ q4_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  216:           blk.23.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  217:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  218:             blk.24.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  219:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  220:        blk.24.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  221:           blk.24.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  222:             blk.24.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  223:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  224:          blk.24.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  226:             blk.25.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  227:             blk.25.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  228:             blk.25.attn_v.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  229:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  230:           blk.25.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  231:             blk.25.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  232:           blk.25.ffn_down.weight\
          \ q4_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  234:           blk.25.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  235:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  236:             blk.26.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  237:             blk.26.attn_v.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  238:        blk.26.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  239:           blk.26.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  240:             blk.26.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  241:           blk.26.ffn_down.weight q4_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  242:          blk.26.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  244:             blk.27.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  245:             blk.27.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  246:             blk.27.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  247:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  248:           blk.27.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  249:             blk.27.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  250:           blk.27.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  252:           blk.27.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  253:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  254:             blk.28.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  255:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  256:        blk.28.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  257:           blk.28.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  258:             blk.28.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  259:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  260:          blk.28.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  262:             blk.29.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  263:             blk.29.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  264:             blk.29.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  265:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  266:           blk.29.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  267:             blk.29.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  268:           blk.29.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  270:           blk.29.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  271:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  272:             blk.30.attn_k.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  273:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  274:        blk.30.attn_output.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  275:           blk.30.ffn_gate.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  276:             blk.30.ffn_up.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  277:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  278:          blk.30.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  280:             blk.31.attn_q.weight\
          \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  281:             blk.31.attn_k.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  282:             blk.31.attn_v.weight\
          \ q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  283:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  284:           blk.31.ffn_gate.weight\
          \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \  285:             blk.31.ffn_up.weight q4_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor  286:           blk.31.ffn_down.weight\
          \ q6_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  288:           blk.31.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  289:               output_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  290:                    output.weight\
          \ q6_K     [  4096, 32000,     1,     1 ]\nllama_model_loader: - kv   0:\
          \                       general.architecture str\nllama_model_loader: -\
          \ kv   1:                               general.name str\nllama_model_loader:\
          \ - kv   2:                       llama.context_length u32\nllama_model_loader:\
          \ - kv   3:                     llama.embedding_length u32\nllama_model_loader:\
          \ - kv   4:                          llama.block_count u32\nllama_model_loader:\
          \ - kv   5:                  llama.feed_forward_length u32\nllama_model_loader:\
          \ - kv   6:                 llama.rope.dimension_count u32\nllama_model_loader:\
          \ - kv   7:                 llama.attention.head_count u32\nllama_model_loader:\
          \ - kv   8:              llama.attention.head_count_kv u32\nllama_model_loader:\
          \ - kv   9:     llama.attention.layer_norm_rms_epsilon f32\nllama_model_loader:\
          \ - kv  10:                          general.file_type u32\nllama_model_loader:\
          \ - kv  11:                       tokenizer.ggml.model str\nllama_model_loader:\
          \ - kv  12:                      tokenizer.ggml.tokens arr\nllama_model_loader:\
          \ - kv  13:                      tokenizer.ggml.scores arr\nllama_model_loader:\
          \ - kv  14:                  tokenizer.ggml.token_type arr\nllama_model_loader:\
          \ - kv  15:                tokenizer.ggml.bos_token_id u32\nllama_model_loader:\
          \ - kv  16:                tokenizer.ggml.eos_token_id u32\nllama_model_loader:\
          \ - kv  17:            tokenizer.ggml.unknown_token_id u32\nllama_model_loader:\
          \ - kv  18:               general.quantization_version u32\nllama_model_loader:\
          \ - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  193 tensors\n\
          llama_model_loader: - type q6_K:   33 tensors\n2023-09-27 12:31:22 ERROR:Failed\
          \ to load the model.\nTraceback (most recent call last):\n  File \"E:\\\
          text-generation-webui-1.6.1\\modules\\ui_model_menu.py\", line 198, in load_model_wrapper\n\
          \    shared.model, shared.tokenizer = load_model(shared.model_name, loader)\n\
          \  File \"E:\\text-generation-webui-1.6.1\\modules\\models.py\", line 78,\
          \ in load_model\n    output = load_func_map[loader](model_name)\n  File\
          \ \"E:\\text-generation-webui-1.6.1\\modules\\models.py\", line 232, in\
          \ llamacpp_loader\n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n\
          \  File \"E:\\text-generation-webui-1.6.1\\modules\\llamacpp_model.py\"\
          , line 90, in from_pretrained\n    result.model = Llama(**params)\n  File\
          \ \"E:\\text-generation-webui-1.6.1\\installer_files\\env\\lib\\site-packages\\\
          llama_cpp_cuda\\llama.py\", line 332, in __init__\n    self.model = llama_cpp.llama_load_model_from_file(\n\
          \  File \"E:\\text-generation-webui-1.6.1\\installer_files\\env\\lib\\site-packages\\\
          llama_cpp_cuda\\llama_cpp.py\", line 434, in llama_load_model_from_file\n\
          \    return _lib.llama_load_model_from_file(path_model, params)\nOSError:\
          \ [WinError -1073741795] Windows Error 0xc000001d\n\nException ignored in:\
          \ <function LlamaCppModel.__del__ at 0x000001D328E2BBE0>\nTraceback (most\
          \ recent call last):\n  File \"E:\\text-generation-webui-1.6.1\\modules\\\
          llamacpp_model.py\", line 49, in __del__\n    self.model.__del__()\nAttributeError:\
          \ 'LlamaCppModel' object has no attribute 'model'\n\n\n----------line break\n\
          i have tried both directly inside the /models/ folder, and inside a named\
          \ subfolder with the included config.json, both same result.\n\nthanks for\
          \ reading and trying to help in advance, i have also had this issue while\
          \ trying another .gguf model, i have not been able to use any llama.cpp\
          \ type yet.\n\nsystem amd fx 4800 cpu, \ngtx 1060 6gb vram. \n16gb sysram\n\
          \ni was able to get a gguf model to work using the non llama.cpp loader,\
          \ but i can no longer get even that to work as of this moment , and it ran\
          \ completely on cpu/hdd \n"
        updatedAt: '2023-09-27T18:03:02.716Z'
      numEdits: 1
      reactions: []
    id: 65146752a2bd34dc4a1f21c6
    type: comment
  author: clown134
  content: "hi ive been googling for hours but i cant seem to track down whats causing\
    \ the issue\n\nit seems like it wants to load. but something goes wrong at the\
    \ very last second \n\nlog ~~~~~~~~~~~~~~~~~~~\nRunning on local URL:  http://127.0.0.1:7860\n\
    \nTo create a public link, set `share=True` in `launch()`.\n2023-09-27 12:31:21\
    \ INFO:Loading xwin-lm-7b-v0.1.Q4_K_M.gguf...\n2023-09-27 12:31:21 INFO:llama.cpp\
    \ weights detected: models\\xwin-lm-7b-v0.1.Q4_K_M.gguf\n2023-09-27 12:31:21 INFO:Cache\
    \ capacity is 0 bytes\nggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA\
    \ GeForce GTX 1060 6GB, compute capability 6.1\nllama_model_loader: loaded meta\
    \ data with 19 key-value pairs and 291 tensors from models\\xwin-lm-7b-v0.1.Q4_K_M.gguf\
    \ (version GGUF V2 (latest))\nllama_model_loader: - tensor    0:             \
    \   token_embd.weight q4_K     [  4096, 32000,     1,     1 ]\nllama_model_loader:\
    \ - tensor    1:              blk.0.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    3:\
    \              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor    4:         blk.0.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor    5:            blk.0.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor    6:\
    \              blk.0.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor    7:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor    8:           blk.0.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor    9:\
    \            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   10:              blk.1.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   11:              blk.1.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   12:\
    \              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   13:         blk.1.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   14:            blk.1.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   15:\
    \              blk.1.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor   16:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   17:           blk.1.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   18:\
    \            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   19:              blk.2.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   20:              blk.2.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   21:\
    \              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   22:         blk.2.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   23:            blk.2.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   24:\
    \              blk.2.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor   25:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   26:           blk.2.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   27:\
    \            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   28:              blk.3.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   29:              blk.3.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   30:\
    \              blk.3.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   31:         blk.3.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   32:            blk.3.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   33:\
    \              blk.3.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor   34:            blk.3.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   35:           blk.3.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   36:\
    \            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   37:              blk.4.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   38:              blk.4.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   39:\
    \              blk.4.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   40:         blk.4.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   41:            blk.4.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   42:\
    \              blk.4.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor   43:            blk.4.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   44:           blk.4.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   45:\
    \            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   46:              blk.5.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   47:              blk.5.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   48:\
    \              blk.5.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   49:         blk.5.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   50:            blk.5.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   51:\
    \              blk.5.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor   52:            blk.5.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   53:           blk.5.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   54:\
    \            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   55:              blk.6.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   56:              blk.6.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   57:\
    \              blk.6.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   58:         blk.6.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   59:            blk.6.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   60:\
    \              blk.6.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor   61:            blk.6.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   62:           blk.6.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   63:\
    \            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   64:              blk.7.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   65:              blk.7.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   66:\
    \              blk.7.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   67:         blk.7.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   68:            blk.7.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   69:\
    \              blk.7.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor   70:            blk.7.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   71:           blk.7.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   72:\
    \            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   73:              blk.8.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   74:              blk.8.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   75:\
    \              blk.8.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   76:         blk.8.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   77:            blk.8.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   78:\
    \              blk.8.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor   79:            blk.8.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   80:           blk.8.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   81:\
    \            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   82:              blk.9.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   83:              blk.9.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   84:\
    \              blk.9.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   85:         blk.9.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   86:            blk.9.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   87:\
    \              blk.9.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor   88:            blk.9.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   89:           blk.9.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   90:\
    \            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   91:             blk.10.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   92:             blk.10.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   93:\
    \             blk.10.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   94:        blk.10.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   95:           blk.10.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor   96:\
    \             blk.10.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor   97:           blk.10.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor   98:          blk.10.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   99:\
    \           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  100:             blk.11.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  101:             blk.11.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  102:\
    \             blk.11.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  103:        blk.11.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  104:           blk.11.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  105:\
    \             blk.11.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  106:           blk.11.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  107:          blk.11.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  108:\
    \           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  109:             blk.12.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  110:             blk.12.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  111:\
    \             blk.12.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  112:        blk.12.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  113:           blk.12.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  114:\
    \             blk.12.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  115:           blk.12.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  116:          blk.12.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  117:\
    \           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  118:             blk.13.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  119:             blk.13.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  120:\
    \             blk.13.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  121:        blk.13.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  122:           blk.13.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  123:\
    \             blk.13.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  124:           blk.13.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  125:          blk.13.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  126:\
    \           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  127:             blk.14.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  128:             blk.14.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  129:\
    \             blk.14.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  130:        blk.14.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  131:           blk.14.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  132:\
    \             blk.14.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  133:           blk.14.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  134:          blk.14.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  135:\
    \           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  136:             blk.15.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  137:             blk.15.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  138:\
    \             blk.15.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  139:        blk.15.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  141:\
    \             blk.15.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  142:           blk.15.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  143:          blk.15.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  144:\
    \           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  145:             blk.16.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  146:             blk.16.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  147:\
    \             blk.16.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  148:        blk.16.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  149:           blk.16.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  150:\
    \             blk.16.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  151:           blk.16.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  152:          blk.16.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  153:\
    \           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  154:             blk.17.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  155:             blk.17.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  156:\
    \             blk.17.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  157:        blk.17.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  158:           blk.17.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  159:\
    \             blk.17.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  160:           blk.17.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  161:          blk.17.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  162:\
    \           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  163:             blk.18.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  164:             blk.18.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  165:\
    \             blk.18.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  166:        blk.18.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  167:           blk.18.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  168:\
    \             blk.18.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  169:           blk.18.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  170:          blk.18.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  171:\
    \           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  172:             blk.19.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  173:             blk.19.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  174:\
    \             blk.19.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  175:        blk.19.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  176:           blk.19.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  177:\
    \             blk.19.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  178:           blk.19.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  179:          blk.19.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  180:\
    \           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  181:             blk.20.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  182:             blk.20.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  183:\
    \             blk.20.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  184:        blk.20.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  185:           blk.20.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  186:\
    \             blk.20.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  187:           blk.20.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  188:          blk.20.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  189:\
    \           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  190:             blk.21.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  191:             blk.21.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  192:\
    \             blk.21.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  193:        blk.21.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  194:           blk.21.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  195:\
    \             blk.21.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  196:           blk.21.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  197:          blk.21.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  198:\
    \           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  199:             blk.22.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  200:             blk.22.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  201:\
    \             blk.22.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  202:        blk.22.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  203:           blk.22.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  204:\
    \             blk.22.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  205:           blk.22.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  206:          blk.22.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  207:\
    \           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  208:             blk.23.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  209:             blk.23.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  210:\
    \             blk.23.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  211:        blk.23.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  212:           blk.23.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  213:\
    \             blk.23.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  214:           blk.23.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  215:          blk.23.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  216:\
    \           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  217:             blk.24.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  218:             blk.24.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  219:\
    \             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  220:        blk.24.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  221:           blk.24.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  222:\
    \             blk.24.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  223:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  224:          blk.24.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  225:\
    \           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  226:             blk.25.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  227:             blk.25.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  228:\
    \             blk.25.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  229:        blk.25.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  230:           blk.25.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  231:\
    \             blk.25.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  232:           blk.25.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  233:          blk.25.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  234:\
    \           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  235:             blk.26.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  236:             blk.26.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  237:\
    \             blk.26.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  238:        blk.26.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  239:           blk.26.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  240:\
    \             blk.26.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  241:           blk.26.ffn_down.weight q4_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  242:          blk.26.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  243:\
    \           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  244:             blk.27.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  245:             blk.27.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  246:\
    \             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  247:        blk.27.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  248:           blk.27.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  249:\
    \             blk.27.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  250:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  251:          blk.27.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  252:\
    \           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  253:             blk.28.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  254:             blk.28.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  255:\
    \             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  256:        blk.28.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  257:           blk.28.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  258:\
    \             blk.28.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  259:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  260:          blk.28.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  261:\
    \           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  262:             blk.29.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  263:             blk.29.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  264:\
    \             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  265:        blk.29.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  266:           blk.29.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  267:\
    \             blk.29.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  268:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  269:          blk.29.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  270:\
    \           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  271:             blk.30.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  272:             blk.30.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  273:\
    \             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  274:        blk.30.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  275:           blk.30.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  276:\
    \             blk.30.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  277:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  278:          blk.30.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  279:\
    \           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  280:             blk.31.attn_q.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  281:             blk.31.attn_k.weight\
    \ q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  282:\
    \             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  283:        blk.31.attn_output.weight q4_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  284:           blk.31.ffn_gate.weight\
    \ q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor  285:\
    \             blk.31.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\nllama_model_loader:\
    \ - tensor  286:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor  287:          blk.31.attn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  288:\
    \           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  289:               output_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  290:                    output.weight\
    \ q6_K     [  4096, 32000,     1,     1 ]\nllama_model_loader: - kv   0:     \
    \                  general.architecture str\nllama_model_loader: - kv   1:   \
    \                            general.name str\nllama_model_loader: - kv   2: \
    \                      llama.context_length u32\nllama_model_loader: - kv   3:\
    \                     llama.embedding_length u32\nllama_model_loader: - kv   4:\
    \                          llama.block_count u32\nllama_model_loader: - kv   5:\
    \                  llama.feed_forward_length u32\nllama_model_loader: - kv   6:\
    \                 llama.rope.dimension_count u32\nllama_model_loader: - kv   7:\
    \                 llama.attention.head_count u32\nllama_model_loader: - kv   8:\
    \              llama.attention.head_count_kv u32\nllama_model_loader: - kv   9:\
    \     llama.attention.layer_norm_rms_epsilon f32\nllama_model_loader: - kv  10:\
    \                          general.file_type u32\nllama_model_loader: - kv  11:\
    \                       tokenizer.ggml.model str\nllama_model_loader: - kv  12:\
    \                      tokenizer.ggml.tokens arr\nllama_model_loader: - kv  13:\
    \                      tokenizer.ggml.scores arr\nllama_model_loader: - kv  14:\
    \                  tokenizer.ggml.token_type arr\nllama_model_loader: - kv  15:\
    \                tokenizer.ggml.bos_token_id u32\nllama_model_loader: - kv  16:\
    \                tokenizer.ggml.eos_token_id u32\nllama_model_loader: - kv  17:\
    \            tokenizer.ggml.unknown_token_id u32\nllama_model_loader: - kv  18:\
    \               general.quantization_version u32\nllama_model_loader: - type \
    \ f32:   65 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader:\
    \ - type q6_K:   33 tensors\n2023-09-27 12:31:22 ERROR:Failed to load the model.\n\
    Traceback (most recent call last):\n  File \"E:\\text-generation-webui-1.6.1\\\
    modules\\ui_model_menu.py\", line 198, in load_model_wrapper\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name, loader)\n  File \"E:\\text-generation-webui-1.6.1\\\
    modules\\models.py\", line 78, in load_model\n    output = load_func_map[loader](model_name)\n\
    \  File \"E:\\text-generation-webui-1.6.1\\modules\\models.py\", line 232, in\
    \ llamacpp_loader\n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n\
    \  File \"E:\\text-generation-webui-1.6.1\\modules\\llamacpp_model.py\", line\
    \ 90, in from_pretrained\n    result.model = Llama(**params)\n  File \"E:\\text-generation-webui-1.6.1\\\
    installer_files\\env\\lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 332,\
    \ in __init__\n    self.model = llama_cpp.llama_load_model_from_file(\n  File\
    \ \"E:\\text-generation-webui-1.6.1\\installer_files\\env\\lib\\site-packages\\\
    llama_cpp_cuda\\llama_cpp.py\", line 434, in llama_load_model_from_file\n    return\
    \ _lib.llama_load_model_from_file(path_model, params)\nOSError: [WinError -1073741795]\
    \ Windows Error 0xc000001d\n\nException ignored in: <function LlamaCppModel.__del__\
    \ at 0x000001D328E2BBE0>\nTraceback (most recent call last):\n  File \"E:\\text-generation-webui-1.6.1\\\
    modules\\llamacpp_model.py\", line 49, in __del__\n    self.model.__del__()\n\
    AttributeError: 'LlamaCppModel' object has no attribute 'model'\n\n\n----------line\
    \ break\ni have tried both directly inside the /models/ folder, and inside a named\
    \ subfolder with the included config.json, both same result.\n\nthanks for reading\
    \ and trying to help in advance, i have also had this issue while trying another\
    \ .gguf model, i have not been able to use any llama.cpp type yet.\n\nsystem amd\
    \ fx 4800 cpu, \ngtx 1060 6gb vram. \n16gb sysram\n\ni was able to get a gguf\
    \ model to work using the non llama.cpp loader, but i can no longer get even that\
    \ to work as of this moment , and it ran completely on cpu/hdd \n"
  created_at: 2023-09-27 16:33:06+00:00
  edited: true
  hidden: false
  id: 65146752a2bd34dc4a1f21c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90cf9700f14d6bf04c9f6d7eaef0f345.svg
      fullname: katie barber
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: clown134
      type: user
    createdAt: '2023-09-27T18:21:53.000Z'
    data:
      edited: false
      editors:
      - clown134
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8891928195953369
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90cf9700f14d6bf04c9f6d7eaef0f345.svg
          fullname: katie barber
          isHf: false
          isPro: false
          name: clown134
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/issues/3276#issuecomment-1648532571">https://github.com/oobabooga/text-generation-webui/issues/3276#issuecomment-1648532571</a></p>

          <p>this ended up being the solution... i think i may have tried one other
          thing that i cant remember but im almost 100% certain this was the solution</p>

          '
        raw: 'https://github.com/oobabooga/text-generation-webui/issues/3276#issuecomment-1648532571


          this ended up being the solution... i think i may have tried one other thing
          that i cant remember but im almost 100% certain this was the solution'
        updatedAt: '2023-09-27T18:21:53.810Z'
      numEdits: 0
      reactions: []
    id: 651472c18f20a8ccf7aa8569
    type: comment
  author: clown134
  content: 'https://github.com/oobabooga/text-generation-webui/issues/3276#issuecomment-1648532571


    this ended up being the solution... i think i may have tried one other thing that
    i cant remember but im almost 100% certain this was the solution'
  created_at: 2023-09-27 17:21:53+00:00
  edited: false
  hidden: false
  id: 651472c18f20a8ccf7aa8569
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Xwin-LM-7B-V0.1-GGUF
repo_type: model
status: open
target_branch: null
title: 'AttributeError: ''LlamaCppModel'' object has no attribute ''model'''
