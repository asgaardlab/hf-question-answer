!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zenitica
conflicting_files: null
created_at: 2023-09-08 08:54:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/637ba13901f272413a0c886f8eb700b2.svg
      fullname: Zion Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zenitica
      type: user
    createdAt: '2023-09-08T09:54:55.000Z'
    data:
      edited: false
      editors:
      - zenitica
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2510758638381958
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/637ba13901f272413a0c886f8eb700b2.svg
          fullname: Zion Zhang
          isHf: false
          isPro: false
          name: zenitica
          type: user
        html: "<p>I'm using GGUF model to run llama.cpp (newest code version), but\
          \ encountered errors of <em>ggml-cuda.cu:5974: an illegal memory access\
          \ was encountered</em>. The error msg is:</p>\n<details>\n    <summary>The\
          \ error message</summary>\n    (base) PS C:\\Users\\x\\code\\llama.cpp_new\\\
          llama.cpp&gt; .\\build\\bin\\Release\\main.exe -m ..\\..\\llama-cpp-python\\\
          models\\llama-2-13b-chat.Q8_0.gguf --color --ctx_size 2048 -n -1 -ins -b\
          \ 256 --top_k 10000 --temp 0.2 --repeat_penalty 1.1 -t 8 -ngl 20\n\n<p>Log\
          \ start</p>\n<p>main: build = 1198 (ebc9608)</p>\n<p>main: seed  = 1694166414</p>\n\
          <p>ggml_init_cublas: found 3 CUDA devices:</p>\n<p>  Device 0: NVIDIA GeForce\
          \ RTX 3080, compute capability 8.6</p>\n<p>  Device 1: NVIDIA GeForce RTX\
          \ 3080, compute capability 8.6</p>\n<p>  Device 2: NVIDIA GeForce RTX 3080,\
          \ compute capability 8.6</p>\n<p>llama_model_loader: loaded meta data with\
          \ 19 key-value pairs and 363 tensors from ....\\llama-cpp-python\\models\\\
          llama-2-13b-chat.Q8_0.gguf (version GGUF V2 (latest))</p>\n<p>llama_model_loader:\
          \ - tensor    0:                token_embd.weight q8_0     [  5120, 32000,\
          \     1,     1 ]</p>\n<p>llama_model_loader: - tensor    1:           blk.0.attn_norm.weight\
          \ f32      [  5120,     1,     1,     1 ]</p>\n<p>llama_model_loader: -\
          \ tensor    2:            blk.0.ffn_down.weight q8_0     [ 13824,  5120,\
          \     1,     1 ]</p>\n<p>......</p>\n<p>llama_model_loader: - tensor  360:\
          \             blk.39.attn_q.weight q8_0     [  5120,  5120,     1,     1\
          \ ]</p>\n<p>llama_model_loader: - tensor  361:             blk.39.attn_v.weight\
          \ q8_0     [  5120,  5120,     1,     1 ]</p>\n<p>llama_model_loader: -\
          \ tensor  362:               output_norm.weight f32      [  5120,     1,\
          \     1,     1 ]</p>\n<p>llama_model_loader: - kv   0:                 \
          \      general.architecture str</p>\n<p>llama_model_loader: - kv   1:  \
          \                             general.name str</p>\n<p>llama_model_loader:\
          \ - kv   2:                       llama.context_length u32</p>\n<p>llama_model_loader:\
          \ - kv   3:                     llama.embedding_length u32</p>\n<p>llama_model_loader:\
          \ - kv   4:                          llama.block_count u32</p>\n<p>llama_model_loader:\
          \ - kv   5:                  llama.feed_forward_length u32</p>\n<p>llama_model_loader:\
          \ - kv   6:                 llama.rope.dimension_count u32</p>\n<p>llama_model_loader:\
          \ - kv   7:                 llama.attention.head_count u32</p>\n<p>llama_model_loader:\
          \ - kv   8:              llama.attention.head_count_kv u32</p>\n<p>llama_model_loader:\
          \ - kv   9:     llama.attention.layer_norm_rms_epsilon f32</p>\n<p>llama_model_loader:\
          \ - kv  10:                          general.file_type u32</p>\n<p>llama_model_loader:\
          \ - kv  11:                       tokenizer.ggml.model str</p>\n<p>llama_model_loader:\
          \ - kv  12:                      tokenizer.ggml.tokens arr</p>\n<p>llama_model_loader:\
          \ - kv  13:                      tokenizer.ggml.scores arr</p>\n<p>llama_model_loader:\
          \ - kv  14:                  tokenizer.ggml.token_type arr</p>\n<p>llama_model_loader:\
          \ - kv  15:                tokenizer.ggml.bos_token_id u32</p>\n<p>llama_model_loader:\
          \ - kv  16:                tokenizer.ggml.eos_token_id u32</p>\n<p>llama_model_loader:\
          \ - kv  17:            tokenizer.ggml.unknown_token_id u32</p>\n<p>llama_model_loader:\
          \ - kv  18:               general.quantization_version u32</p>\n<p>llama_model_loader:\
          \ - type  f32:   81 tensors</p>\n<p>llama_model_loader: - type q8_0:  282\
          \ tensors</p>\n<p>llm_load_print_meta: format         = GGUF V2 (latest)</p>\n\
          <p>llm_load_print_meta: arch           = llama</p>\n<p>llm_load_print_meta:\
          \ vocab type     = SPM</p>\n<p>llm_load_print_meta: n_vocab        = 32000</p>\n\
          <p>llm_load_print_meta: n_merges       = 0</p>\n<p>llm_load_print_meta:\
          \ n_ctx_train    = 4096</p>\n<p>llm_load_print_meta: n_ctx          = 2048</p>\n\
          <p>llm_load_print_meta: n_embd         = 5120</p>\n<p>llm_load_print_meta:\
          \ n_head         = 40</p>\n<p>llm_load_print_meta: n_head_kv      = 40</p>\n\
          <p>llm_load_print_meta: n_layer        = 40</p>\n<p>llm_load_print_meta:\
          \ n_rot          = 128</p>\n<p>llm_load_print_meta: n_gqa          = 1</p>\n\
          <p>llm_load_print_meta: f_norm_eps     = 1.0e-05</p>\n<p>llm_load_print_meta:\
          \ f_norm_rms_eps = 1.0e-05</p>\n<p>llm_load_print_meta: n_ff           =\
          \ 13824</p>\n<p>llm_load_print_meta: freq_base      = 10000.0</p>\n<p>llm_load_print_meta:\
          \ freq_scale     = 1</p>\n<p>llm_load_print_meta: model type     = 13B</p>\n\
          <p>llm_load_print_meta: model ftype    = mostly Q8_0</p>\n<p>llm_load_print_meta:\
          \ model size     = 13.02 B</p>\n<p>llm_load_print_meta: general.name   =\
          \ LLaMA v2</p>\n<p>llm_load_print_meta: BOS token = 1 '<s>'</s></p><s>\n\
          </s><p><s>llm_load_print_meta: EOS token = 2 '</s>'</p>\n<p>llm_load_print_meta:\
          \ UNK token = 0 ''</p>\n<p>llm_load_print_meta: LF token  = 13 '&lt;0x0A&gt;'</p>\n\
          <p>llm_load_tensors: ggml ctx size =    0.12 MB</p>\n<p>llm_load_tensors:\
          \ using CUDA for GPU acceleration</p>\n<p>ggml_cuda_set_main_device: using\
          \ device 0 (NVIDIA GeForce RTX 3080) as main device</p>\n<p>llm_load_tensors:\
          \ mem required  = 6761.07 MB (+ 1600.00 MB per state)</p>\n<p>llm_load_tensors:\
          \ offloading 20 repeating layers to GPU</p>\n<p>llm_load_tensors: offloaded\
          \ 20/43 layers to GPU</p>\n<p>llm_load_tensors: VRAM used: 6429 MB</p>\n\
          <p>....................................................................................................</p>\n\
          <p>llama_new_context_with_model: kv self size  = 1600.00 MB</p>\n<p>llama_new_context_with_model:\
          \ compute buffer total size =   96.47 MB</p>\n<p>llama_new_context_with_model:\
          \ VRAM scratch buffer: 95.00 MB</p>\n<p>CUDA error 700 at C:\\Users\\xxx\\\
          Code\\llama.cpp_new\\llama.cpp\\ggml-cuda.cu:5974: an illegal memory access\
          \ was encountered</p>\n</details>"
        raw: "I'm using GGUF model to run llama.cpp (newest code version), but encountered\
          \ errors of *ggml-cuda.cu:5974: an illegal memory access was encountered*.\
          \ The error msg is:\r\n\r\n<details>\r\n    <summary>The error message</summary>\r\
          \n    (base) PS C:\\Users\\x\\code\\llama.cpp_new\\llama.cpp> .\\build\\\
          bin\\Release\\main.exe -m ..\\..\\llama-cpp-python\\models\\llama-2-13b-chat.Q8_0.gguf\
          \ --color --ctx_size 2048 -n -1 -ins -b 256 --top_k 10000 --temp 0.2 --repeat_penalty\
          \ 1.1 -t 8 -ngl 20\r\n\r\nLog start\r\n\r\nmain: build = 1198 (ebc9608)\r\
          \n\r\nmain: seed  = 1694166414\r\n\r\nggml_init_cublas: found 3 CUDA devices:\r\
          \n\r\n  Device 0: NVIDIA GeForce RTX 3080, compute capability 8.6\r\n\r\n\
          \  Device 1: NVIDIA GeForce RTX 3080, compute capability 8.6\r\n\r\n  Device\
          \ 2: NVIDIA GeForce RTX 3080, compute capability 8.6\r\n\r\nllama_model_loader:\
          \ loaded meta data with 19 key-value pairs and 363 tensors from ..\\..\\\
          llama-cpp-python\\models\\llama-2-13b-chat.Q8_0.gguf (version GGUF V2 (latest))\r\
          \n\r\nllama_model_loader: - tensor    0:                token_embd.weight\
          \ q8_0     [  5120, 32000,     1,     1 ]\r\n\r\nllama_model_loader: - tensor\
          \    1:           blk.0.attn_norm.weight f32      [  5120,     1,     1,\
          \     1 ]\r\n\r\nllama_model_loader: - tensor    2:            blk.0.ffn_down.weight\
          \ q8_0     [ 13824,  5120,     1,     1 ]\r\n\r\n......\r\n\r\nllama_model_loader:\
          \ - tensor  360:             blk.39.attn_q.weight q8_0     [  5120,  5120,\
          \     1,     1 ]\r\n\r\nllama_model_loader: - tensor  361:             blk.39.attn_v.weight\
          \ q8_0     [  5120,  5120,     1,     1 ]\r\n\r\nllama_model_loader: - tensor\
          \  362:               output_norm.weight f32      [  5120,     1,     1,\
          \     1 ]\r\n\r\nllama_model_loader: - kv   0:                       general.architecture\
          \ str\r\n\r\nllama_model_loader: - kv   1:                             \
          \  general.name str\r\n\r\nllama_model_loader: - kv   2:               \
          \        llama.context_length u32\r\n\r\nllama_model_loader: - kv   3: \
          \                    llama.embedding_length u32\r\n\r\nllama_model_loader:\
          \ - kv   4:                          llama.block_count u32\r\n\r\nllama_model_loader:\
          \ - kv   5:                  llama.feed_forward_length u32\r\n\r\nllama_model_loader:\
          \ - kv   6:                 llama.rope.dimension_count u32\r\n\r\nllama_model_loader:\
          \ - kv   7:                 llama.attention.head_count u32\r\n\r\nllama_model_loader:\
          \ - kv   8:              llama.attention.head_count_kv u32\r\n\r\nllama_model_loader:\
          \ - kv   9:     llama.attention.layer_norm_rms_epsilon f32\r\n\r\nllama_model_loader:\
          \ - kv  10:                          general.file_type u32\r\n\r\nllama_model_loader:\
          \ - kv  11:                       tokenizer.ggml.model str\r\n\r\nllama_model_loader:\
          \ - kv  12:                      tokenizer.ggml.tokens arr\r\n\r\nllama_model_loader:\
          \ - kv  13:                      tokenizer.ggml.scores arr\r\n\r\nllama_model_loader:\
          \ - kv  14:                  tokenizer.ggml.token_type arr\r\n\r\nllama_model_loader:\
          \ - kv  15:                tokenizer.ggml.bos_token_id u32\r\n\r\nllama_model_loader:\
          \ - kv  16:                tokenizer.ggml.eos_token_id u32\r\n\r\nllama_model_loader:\
          \ - kv  17:            tokenizer.ggml.unknown_token_id u32\r\n\r\nllama_model_loader:\
          \ - kv  18:               general.quantization_version u32\r\n\r\nllama_model_loader:\
          \ - type  f32:   81 tensors\r\n\r\nllama_model_loader: - type q8_0:  282\
          \ tensors\r\n\r\nllm_load_print_meta: format         = GGUF V2 (latest)\r\
          \n\r\nllm_load_print_meta: arch           = llama\r\n\r\nllm_load_print_meta:\
          \ vocab type     = SPM\r\n\r\nllm_load_print_meta: n_vocab        = 32000\r\
          \n\r\nllm_load_print_meta: n_merges       = 0\r\n\r\nllm_load_print_meta:\
          \ n_ctx_train    = 4096\r\n\r\nllm_load_print_meta: n_ctx          = 2048\r\
          \n\r\nllm_load_print_meta: n_embd         = 5120\r\n\r\nllm_load_print_meta:\
          \ n_head         = 40\r\n\r\nllm_load_print_meta: n_head_kv      = 40\r\n\
          \r\nllm_load_print_meta: n_layer        = 40\r\n\r\nllm_load_print_meta:\
          \ n_rot          = 128\r\n\r\nllm_load_print_meta: n_gqa          = 1\r\n\
          \r\nllm_load_print_meta: f_norm_eps     = 1.0e-05\r\n\r\nllm_load_print_meta:\
          \ f_norm_rms_eps = 1.0e-05\r\n\r\nllm_load_print_meta: n_ff           =\
          \ 13824\r\n\r\nllm_load_print_meta: freq_base      = 10000.0\r\n\r\nllm_load_print_meta:\
          \ freq_scale     = 1\r\n\r\nllm_load_print_meta: model type     = 13B\r\n\
          \r\nllm_load_print_meta: model ftype    = mostly Q8_0\r\n\r\nllm_load_print_meta:\
          \ model size     = 13.02 B\r\n\r\nllm_load_print_meta: general.name   =\
          \ LLaMA v2\r\n\r\nllm_load_print_meta: BOS token = 1 '<s>'\r\n\r\nllm_load_print_meta:\
          \ EOS token = 2 '</s>'\r\n\r\nllm_load_print_meta: UNK token = 0 '<unk>'\r\
          \n\r\nllm_load_print_meta: LF token  = 13 '<0x0A>'\r\n\r\nllm_load_tensors:\
          \ ggml ctx size =    0.12 MB\r\n\r\nllm_load_tensors: using CUDA for GPU\
          \ acceleration\r\n\r\nggml_cuda_set_main_device: using device 0 (NVIDIA\
          \ GeForce RTX 3080) as main device\r\n\r\nllm_load_tensors: mem required\
          \  = 6761.07 MB (+ 1600.00 MB per state)\r\n\r\nllm_load_tensors: offloading\
          \ 20 repeating layers to GPU\r\n\r\nllm_load_tensors: offloaded 20/43 layers\
          \ to GPU\r\n\r\nllm_load_tensors: VRAM used: 6429 MB\r\n\r\n....................................................................................................\r\
          \n\r\nllama_new_context_with_model: kv self size  = 1600.00 MB\r\n\r\nllama_new_context_with_model:\
          \ compute buffer total size =   96.47 MB\r\n\r\nllama_new_context_with_model:\
          \ VRAM scratch buffer: 95.00 MB\r\n\r\nCUDA error 700 at C:\\Users\\xxx\\\
          Code\\llama.cpp_new\\llama.cpp\\ggml-cuda.cu:5974: an illegal memory access\
          \ was encountered\r\n</details>"
        updatedAt: '2023-09-08T09:54:55.768Z'
      numEdits: 0
      reactions: []
    id: 64faef6f60017eeec976e256
    type: comment
  author: zenitica
  content: "I'm using GGUF model to run llama.cpp (newest code version), but encountered\
    \ errors of *ggml-cuda.cu:5974: an illegal memory access was encountered*. The\
    \ error msg is:\r\n\r\n<details>\r\n    <summary>The error message</summary>\r\
    \n    (base) PS C:\\Users\\x\\code\\llama.cpp_new\\llama.cpp> .\\build\\bin\\\
    Release\\main.exe -m ..\\..\\llama-cpp-python\\models\\llama-2-13b-chat.Q8_0.gguf\
    \ --color --ctx_size 2048 -n -1 -ins -b 256 --top_k 10000 --temp 0.2 --repeat_penalty\
    \ 1.1 -t 8 -ngl 20\r\n\r\nLog start\r\n\r\nmain: build = 1198 (ebc9608)\r\n\r\n\
    main: seed  = 1694166414\r\n\r\nggml_init_cublas: found 3 CUDA devices:\r\n\r\n\
    \  Device 0: NVIDIA GeForce RTX 3080, compute capability 8.6\r\n\r\n  Device 1:\
    \ NVIDIA GeForce RTX 3080, compute capability 8.6\r\n\r\n  Device 2: NVIDIA GeForce\
    \ RTX 3080, compute capability 8.6\r\n\r\nllama_model_loader: loaded meta data\
    \ with 19 key-value pairs and 363 tensors from ..\\..\\llama-cpp-python\\models\\\
    llama-2-13b-chat.Q8_0.gguf (version GGUF V2 (latest))\r\n\r\nllama_model_loader:\
    \ - tensor    0:                token_embd.weight q8_0     [  5120, 32000,   \
    \  1,     1 ]\r\n\r\nllama_model_loader: - tensor    1:           blk.0.attn_norm.weight\
    \ f32      [  5120,     1,     1,     1 ]\r\n\r\nllama_model_loader: - tensor\
    \    2:            blk.0.ffn_down.weight q8_0     [ 13824,  5120,     1,     1\
    \ ]\r\n\r\n......\r\n\r\nllama_model_loader: - tensor  360:             blk.39.attn_q.weight\
    \ q8_0     [  5120,  5120,     1,     1 ]\r\n\r\nllama_model_loader: - tensor\
    \  361:             blk.39.attn_v.weight q8_0     [  5120,  5120,     1,     1\
    \ ]\r\n\r\nllama_model_loader: - tensor  362:               output_norm.weight\
    \ f32      [  5120,     1,     1,     1 ]\r\n\r\nllama_model_loader: - kv   0:\
    \                       general.architecture str\r\n\r\nllama_model_loader: -\
    \ kv   1:                               general.name str\r\n\r\nllama_model_loader:\
    \ - kv   2:                       llama.context_length u32\r\n\r\nllama_model_loader:\
    \ - kv   3:                     llama.embedding_length u32\r\n\r\nllama_model_loader:\
    \ - kv   4:                          llama.block_count u32\r\n\r\nllama_model_loader:\
    \ - kv   5:                  llama.feed_forward_length u32\r\n\r\nllama_model_loader:\
    \ - kv   6:                 llama.rope.dimension_count u32\r\n\r\nllama_model_loader:\
    \ - kv   7:                 llama.attention.head_count u32\r\n\r\nllama_model_loader:\
    \ - kv   8:              llama.attention.head_count_kv u32\r\n\r\nllama_model_loader:\
    \ - kv   9:     llama.attention.layer_norm_rms_epsilon f32\r\n\r\nllama_model_loader:\
    \ - kv  10:                          general.file_type u32\r\n\r\nllama_model_loader:\
    \ - kv  11:                       tokenizer.ggml.model str\r\n\r\nllama_model_loader:\
    \ - kv  12:                      tokenizer.ggml.tokens arr\r\n\r\nllama_model_loader:\
    \ - kv  13:                      tokenizer.ggml.scores arr\r\n\r\nllama_model_loader:\
    \ - kv  14:                  tokenizer.ggml.token_type arr\r\n\r\nllama_model_loader:\
    \ - kv  15:                tokenizer.ggml.bos_token_id u32\r\n\r\nllama_model_loader:\
    \ - kv  16:                tokenizer.ggml.eos_token_id u32\r\n\r\nllama_model_loader:\
    \ - kv  17:            tokenizer.ggml.unknown_token_id u32\r\n\r\nllama_model_loader:\
    \ - kv  18:               general.quantization_version u32\r\n\r\nllama_model_loader:\
    \ - type  f32:   81 tensors\r\n\r\nllama_model_loader: - type q8_0:  282 tensors\r\
    \n\r\nllm_load_print_meta: format         = GGUF V2 (latest)\r\n\r\nllm_load_print_meta:\
    \ arch           = llama\r\n\r\nllm_load_print_meta: vocab type     = SPM\r\n\r\
    \nllm_load_print_meta: n_vocab        = 32000\r\n\r\nllm_load_print_meta: n_merges\
    \       = 0\r\n\r\nllm_load_print_meta: n_ctx_train    = 4096\r\n\r\nllm_load_print_meta:\
    \ n_ctx          = 2048\r\n\r\nllm_load_print_meta: n_embd         = 5120\r\n\r\
    \nllm_load_print_meta: n_head         = 40\r\n\r\nllm_load_print_meta: n_head_kv\
    \      = 40\r\n\r\nllm_load_print_meta: n_layer        = 40\r\n\r\nllm_load_print_meta:\
    \ n_rot          = 128\r\n\r\nllm_load_print_meta: n_gqa          = 1\r\n\r\n\
    llm_load_print_meta: f_norm_eps     = 1.0e-05\r\n\r\nllm_load_print_meta: f_norm_rms_eps\
    \ = 1.0e-05\r\n\r\nllm_load_print_meta: n_ff           = 13824\r\n\r\nllm_load_print_meta:\
    \ freq_base      = 10000.0\r\n\r\nllm_load_print_meta: freq_scale     = 1\r\n\r\
    \nllm_load_print_meta: model type     = 13B\r\n\r\nllm_load_print_meta: model\
    \ ftype    = mostly Q8_0\r\n\r\nllm_load_print_meta: model size     = 13.02 B\r\
    \n\r\nllm_load_print_meta: general.name   = LLaMA v2\r\n\r\nllm_load_print_meta:\
    \ BOS token = 1 '<s>'\r\n\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\n\r\n\
    llm_load_print_meta: UNK token = 0 '<unk>'\r\n\r\nllm_load_print_meta: LF token\
    \  = 13 '<0x0A>'\r\n\r\nllm_load_tensors: ggml ctx size =    0.12 MB\r\n\r\nllm_load_tensors:\
    \ using CUDA for GPU acceleration\r\n\r\nggml_cuda_set_main_device: using device\
    \ 0 (NVIDIA GeForce RTX 3080) as main device\r\n\r\nllm_load_tensors: mem required\
    \  = 6761.07 MB (+ 1600.00 MB per state)\r\n\r\nllm_load_tensors: offloading 20\
    \ repeating layers to GPU\r\n\r\nllm_load_tensors: offloaded 20/43 layers to GPU\r\
    \n\r\nllm_load_tensors: VRAM used: 6429 MB\r\n\r\n....................................................................................................\r\
    \n\r\nllama_new_context_with_model: kv self size  = 1600.00 MB\r\n\r\nllama_new_context_with_model:\
    \ compute buffer total size =   96.47 MB\r\n\r\nllama_new_context_with_model:\
    \ VRAM scratch buffer: 95.00 MB\r\n\r\nCUDA error 700 at C:\\Users\\xxx\\Code\\\
    llama.cpp_new\\llama.cpp\\ggml-cuda.cu:5974: an illegal memory access was encountered\r\
    \n</details>"
  created_at: 2023-09-08 08:54:55+00:00
  edited: false
  hidden: false
  id: 64faef6f60017eeec976e256
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ed881909f5c533aaec359b31324163e.svg
      fullname: Michael Holm
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: michaelholm
      type: user
    createdAt: '2023-09-25T20:21:36.000Z'
    data:
      edited: false
      editors:
      - michaelholm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7217146754264832
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ed881909f5c533aaec359b31324163e.svg
          fullname: Michael Holm
          isHf: false
          isPro: false
          name: michaelholm
          type: user
        html: '<p>Try setting environment variable: "export VISIBLE_CUDA_DEVICES=0"
          or similar, and then set "device=''cuda:0''" in your definition pipeline.  </p>

          '
        raw: 'Try setting environment variable: "export VISIBLE_CUDA_DEVICES=0" or
          similar, and then set "device=''cuda:0''" in your definition pipeline.  '
        updatedAt: '2023-09-25T20:21:36.983Z'
      numEdits: 0
      reactions: []
    id: 6511ebd05b4590211deb55f3
    type: comment
  author: michaelholm
  content: 'Try setting environment variable: "export VISIBLE_CUDA_DEVICES=0" or similar,
    and then set "device=''cuda:0''" in your definition pipeline.  '
  created_at: 2023-09-25 19:21:36+00:00
  edited: false
  hidden: false
  id: 6511ebd05b4590211deb55f3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Llama-2-13B-chat-GGUF
repo_type: model
status: open
target_branch: null
title: 'ggml-cuda.cu:5974: an illegal memory access was encountered'
