!!python/object:huggingface_hub.community.DiscussionWithDetails
author: praxis-dev
conflicting_files: null
created_at: 2023-10-19 19:55:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/lGAQrPgJMuqdtQAwmlIa7.jpeg?w=200&h=200&f=face
      fullname: Igor Chesnokov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: praxis-dev
      type: user
    createdAt: '2023-10-19T20:55:38.000Z'
    data:
      edited: false
      editors:
      - praxis-dev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8257670998573303
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/lGAQrPgJMuqdtQAwmlIa7.jpeg?w=200&h=200&f=face
          fullname: Igor Chesnokov
          isHf: false
          isPro: false
          name: praxis-dev
          type: user
        html: '<p>hi. this model gives me 1.5 minutes of waiting time before responding
          on double teslas t4 on modal .com platform. how should I perceive this number?
          Fast, long? Can It be optimized? </p>

          '
        raw: 'hi. this model gives me 1.5 minutes of waiting time before responding
          on double teslas t4 on modal .com platform. how should I perceive this number?
          Fast, long? Can It be optimized? '
        updatedAt: '2023-10-19T20:55:38.095Z'
      numEdits: 0
      reactions: []
    id: 653197ca7b5d1a9c7b5c6a41
    type: comment
  author: praxis-dev
  content: 'hi. this model gives me 1.5 minutes of waiting time before responding
    on double teslas t4 on modal .com platform. how should I perceive this number?
    Fast, long? Can It be optimized? '
  created_at: 2023-10-19 19:55:38+00:00
  edited: false
  hidden: false
  id: 653197ca7b5d1a9c7b5c6a41
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-10-20T14:04:59.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9687912464141846
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>Hmm I think the problem is that first, you have to install llama
          cpp with cublas. Then also put the gpu layers as like 50.<br>This should
          increase speed massively!<br>However, the fastest way to run a model on
          gpus is with exllama v2 so try that instead.</p>

          '
        raw: 'Hmm I think the problem is that first, you have to install llama cpp
          with cublas. Then also put the gpu layers as like 50.

          This should increase speed massively!

          However, the fastest way to run a model on gpus is with exllama v2 so try
          that instead.'
        updatedAt: '2023-10-20T14:04:59.075Z'
      numEdits: 0
      reactions: []
    id: 6532890b9860c1cb3795a590
    type: comment
  author: YaTharThShaRma999
  content: 'Hmm I think the problem is that first, you have to install llama cpp with
    cublas. Then also put the gpu layers as like 50.

    This should increase speed massively!

    However, the fastest way to run a model on gpus is with exllama v2 so try that
    instead.'
  created_at: 2023-10-20 13:04:59+00:00
  edited: false
  hidden: false
  id: 6532890b9860c1cb3795a590
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Llama-2-13B-chat-GGUF
repo_type: model
status: open
target_branch: null
title: Response time
