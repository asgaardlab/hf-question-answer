!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hrud
conflicting_files: null
created_at: 2023-09-28 11:46:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad1669ec1833236ccf61c078ec42cc1a.svg
      fullname: Hrushikesh Dhumal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hrud
      type: user
    createdAt: '2023-09-28T12:46:59.000Z'
    data:
      edited: false
      editors:
      - hrud
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6355385780334473
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad1669ec1833236ccf61c078ec42cc1a.svg
          fullname: Hrushikesh Dhumal
          isHf: false
          isPro: false
          name: hrud
          type: user
        html: '<p>I am using Python 3.8.5, latest<code>llama.cpp(commit id: 4aea3b846ec151cc6d08f93a8889eae13b286b06)</code><br>I
          downloaded the models using git lfs from TheBloke as git lfs clone <a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF">https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF</a></p>

          <pre><code class="language-bash">bash-4.2$ ./main -ngl 32 -m ../models/Llama-2-13B-chat-GGUF/llama-2-13b-chat.Q2_K.gguf
          --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p <span class="hljs-string">"[INST]
          &lt;&lt;SYS&gt;&gt;\nYou are a helpful, respectful and honest assistant.
          Always answer as helpfully as possible, while being safe.  Your answers
          should not include any harmful, unethical, racist, sexist, toxic, dangerous,
          or illegal content. Please ensure that your responses are socially unbiased
          and positive in nature. If a question does not make any sense, or is not
          factually coherent, explain why instead of answering something not correct.
          If you don''t know the answer to a question, please don''t share false information.\n&lt;&lt;/SYS&gt;&gt;\n{prompt}[/INST]"</span>

          warning: not compiled with GPU offload support, --n-gpu-layers option will
          be ignored

          warning: see main README.md <span class="hljs-keyword">for</span> information
          on enabling GPU BLAS support

          Log start

          main: warning: changing RoPE frequency base to 0 (default 10000.0)

          main: warning: scaling RoPE frequency by 0 (default 1.0)

          main: build = 1281 (4aea3b8)

          main: built with cc (GCC) 7.3.1 20180712 (Red Hat 7.3.1-15) <span class="hljs-keyword">for</span>
          x86_64-redhat-linux

          main: seed  = 1695905013

          gguf_init_from_file: invalid magic number 73726576

          error loading model: llama_model_loader: failed to load model from ../models/Llama-2-13B-chat-GGUF/llama-2-13b-chat.Q2_K.gguf


          llama_load_model_from_file: failed to load model

          llama_init_from_gpt_params: error: failed to load model <span class="hljs-string">''../models/Llama-2-13B-chat-GGUF/llama-2-13b-chat.Q2_K.gguf''</span>

          main: error: unable to load model

          </code></pre>

          '
        raw: "I am using Python 3.8.5, latest`llama.cpp(commit id: 4aea3b846ec151cc6d08f93a8889eae13b286b06)`\r\
          \nI downloaded the models using git lfs from TheBloke as git lfs clone https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF\r\
          \n\r\n```bash\r\nbash-4.2$ ./main -ngl 32 -m ../models/Llama-2-13B-chat-GGUF/llama-2-13b-chat.Q2_K.gguf\
          \ --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"[INST] <<SYS>>\\\
          nYou are a helpful, respectful and honest assistant. Always answer as helpfully\
          \ as possible, while being safe.  Your answers should not include any harmful,\
          \ unethical, racist, sexist, toxic, dangerous, or illegal content. Please\
          \ ensure that your responses are socially unbiased and positive in nature.\
          \ If a question does not make any sense, or is not factually coherent, explain\
          \ why instead of answering something not correct. If you don't know the\
          \ answer to a question, please don't share false information.\\n<</SYS>>\\\
          n{prompt}[/INST]\"\r\nwarning: not compiled with GPU offload support, --n-gpu-layers\
          \ option will be ignored\r\nwarning: see main README.md for information\
          \ on enabling GPU BLAS support\r\nLog start\r\nmain: warning: changing RoPE\
          \ frequency base to 0 (default 10000.0)\r\nmain: warning: scaling RoPE frequency\
          \ by 0 (default 1.0)\r\nmain: build = 1281 (4aea3b8)\r\nmain: built with\
          \ cc (GCC) 7.3.1 20180712 (Red Hat 7.3.1-15) for x86_64-redhat-linux\r\n\
          main: seed  = 1695905013\r\ngguf_init_from_file: invalid magic number 73726576\r\
          \nerror loading model: llama_model_loader: failed to load model from ../models/Llama-2-13B-chat-GGUF/llama-2-13b-chat.Q2_K.gguf\r\
          \n\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params:\
          \ error: failed to load model '../models/Llama-2-13B-chat-GGUF/llama-2-13b-chat.Q2_K.gguf'\r\
          \nmain: error: unable to load model\r\n```"
        updatedAt: '2023-09-28T12:46:59.559Z'
      numEdits: 0
      reactions: []
    id: 651575c37f8b9fc0f78e282e
    type: comment
  author: hrud
  content: "I am using Python 3.8.5, latest`llama.cpp(commit id: 4aea3b846ec151cc6d08f93a8889eae13b286b06)`\r\
    \nI downloaded the models using git lfs from TheBloke as git lfs clone https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF\r\
    \n\r\n```bash\r\nbash-4.2$ ./main -ngl 32 -m ../models/Llama-2-13B-chat-GGUF/llama-2-13b-chat.Q2_K.gguf\
    \ --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"[INST] <<SYS>>\\\
    nYou are a helpful, respectful and honest assistant. Always answer as helpfully\
    \ as possible, while being safe.  Your answers should not include any harmful,\
    \ unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure\
    \ that your responses are socially unbiased and positive in nature. If a question\
    \ does not make any sense, or is not factually coherent, explain why instead of\
    \ answering something not correct. If you don't know the answer to a question,\
    \ please don't share false information.\\n<</SYS>>\\n{prompt}[/INST]\"\r\nwarning:\
    \ not compiled with GPU offload support, --n-gpu-layers option will be ignored\r\
    \nwarning: see main README.md for information on enabling GPU BLAS support\r\n\
    Log start\r\nmain: warning: changing RoPE frequency base to 0 (default 10000.0)\r\
    \nmain: warning: scaling RoPE frequency by 0 (default 1.0)\r\nmain: build = 1281\
    \ (4aea3b8)\r\nmain: built with cc (GCC) 7.3.1 20180712 (Red Hat 7.3.1-15) for\
    \ x86_64-redhat-linux\r\nmain: seed  = 1695905013\r\ngguf_init_from_file: invalid\
    \ magic number 73726576\r\nerror loading model: llama_model_loader: failed to\
    \ load model from ../models/Llama-2-13B-chat-GGUF/llama-2-13b-chat.Q2_K.gguf\r\
    \n\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params:\
    \ error: failed to load model '../models/Llama-2-13B-chat-GGUF/llama-2-13b-chat.Q2_K.gguf'\r\
    \nmain: error: unable to load model\r\n```"
  created_at: 2023-09-28 11:46:59+00:00
  edited: false
  hidden: false
  id: 651575c37f8b9fc0f78e282e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad1669ec1833236ccf61c078ec42cc1a.svg
      fullname: Hrushikesh Dhumal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hrud
      type: user
    createdAt: '2023-09-28T17:14:32.000Z'
    data:
      edited: false
      editors:
      - hrud
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9655612707138062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad1669ec1833236ccf61c078ec42cc1a.svg
          fullname: Hrushikesh Dhumal
          isHf: false
          isPro: false
          name: hrud
          type: user
        html: '<p>I was able to fix this by manually downloading the model file. Somehow
          git lfs is not downloading the complete file.</p>

          '
        raw: I was able to fix this by manually downloading the model file. Somehow
          git lfs is not downloading the complete file.
        updatedAt: '2023-09-28T17:14:32.443Z'
      numEdits: 0
      reactions: []
    id: 6515b4786137c317300594bd
    type: comment
  author: hrud
  content: I was able to fix this by manually downloading the model file. Somehow
    git lfs is not downloading the complete file.
  created_at: 2023-09-28 16:14:32+00:00
  edited: false
  hidden: false
  id: 6515b4786137c317300594bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-28T17:55:50.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9277432560920715
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>As discussed in the Readme, I strongly discourage anyone from using
          Git to download files from HF, and especially GGUF model files</p>

          '
        raw: As discussed in the Readme, I strongly discourage anyone from using Git
          to download files from HF, and especially GGUF model files
        updatedAt: '2023-09-28T17:55:50.123Z'
      numEdits: 0
      reactions: []
    id: 6515be26ff0ecf2255f46406
    type: comment
  author: TheBloke
  content: As discussed in the Readme, I strongly discourage anyone from using Git
    to download files from HF, and especially GGUF model files
  created_at: 2023-09-28 16:55:50+00:00
  edited: false
  hidden: false
  id: 6515be26ff0ecf2255f46406
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60987217c7627b6818b0ce526c39f89b.svg
      fullname: Naina Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: naina28-03
      type: user
    createdAt: '2023-11-05T19:38:03.000Z'
    data:
      edited: false
      editors:
      - naina28-03
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7407302260398865
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60987217c7627b6818b0ce526c39f89b.svg
          fullname: Naina Sharma
          isHf: false
          isPro: false
          name: naina28-03
          type: user
        html: '<p>I have downloaded the model ''llama-2-13b-chat.Q8_0.gguf'' from
          HF. Still, I am unable to load the model  using Llama from llama_cpp.</p>

          '
        raw: 'I have downloaded the model ''llama-2-13b-chat.Q8_0.gguf'' from HF.
          Still, I am unable to load the model  using Llama from llama_cpp.

          '
        updatedAt: '2023-11-05T19:38:03.239Z'
      numEdits: 0
      reactions: []
    id: 6547ef1bcf50edb69f1bbb02
    type: comment
  author: naina28-03
  content: 'I have downloaded the model ''llama-2-13b-chat.Q8_0.gguf'' from HF. Still,
    I am unable to load the model  using Llama from llama_cpp.

    '
  created_at: 2023-11-05 19:38:03+00:00
  edited: false
  hidden: false
  id: 6547ef1bcf50edb69f1bbb02
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Llama-2-13B-chat-GGUF
repo_type: model
status: open
target_branch: null
title: llama.cpp unable to load the .gguf model
