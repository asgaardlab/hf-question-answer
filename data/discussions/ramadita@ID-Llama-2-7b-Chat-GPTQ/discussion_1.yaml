!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YanaS
conflicting_files: null
created_at: 2023-12-15 06:56:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3a7b00385f85338c009de202174ef7c.svg
      fullname: Yana Stamenova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YanaS
      type: user
    createdAt: '2023-12-15T06:56:45.000Z'
    data:
      edited: true
      editors:
      - YanaS
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8373414278030396
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3a7b00385f85338c009de202174ef7c.svg
          fullname: Yana Stamenova
          isHf: false
          isPro: false
          name: YanaS
          type: user
        html: "<p>Hey, I see you have liked Photolens/llama-2-7b-langchain-chat model\
          \ and I wondered is your model here is cloned from his'? I as because I\
          \ tried to use his model but with a standard prompt template, I got terrible\
          \ answers and I assume there is some change in the template for chat.<br>For\
          \ example, my prompt template looks like this:<br>         [INST]&lt;&lt;SYS&gt;&gt;<br>\
          \        You are a helpful assistant, etc.<br>        &lt;&lt;/SYS&gt;&gt;</p>\n\
          <pre><code>    Context: {context}\n    User: {question}\n\n    [/INST]\n\
          </code></pre>\n<p>It works perfectly with TheBloke's model but with this\
          \ one, I get results formatted like this:</p>\n<pre><code class=\"language-json\"\
          ><span class=\"hljs-punctuation\">{</span><span class=\"hljs-attr\">\"action\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\"\
          >\"Final Answer\"</span><span class=\"hljs-punctuation\">,</span> <span\
          \ class=\"hljs-attr\">\"action_input\"</span><span class=\"hljs-punctuation\"\
          >:</span> &lt;text&gt;<span class=\"hljs-punctuation\">}</span>\n```  \u0441\
          \u0456\u0447\u043D\u044F <span class=\"hljs-number\">18</span><span class=\"\
          hljs-punctuation\">,</span> <span class=\"hljs-number\">2023</span><span\
          \ class=\"hljs-punctuation\">,</span> <span class=\"hljs-number\">15</span><span\
          \ class=\"hljs-punctuation\">:</span><span class=\"hljs-number\">45</span>\
          \ (UTC)\n\n&lt;INST&gt; &lt;text&gt;<span class=\"hljs-punctuation\">[</span>/INST<span\
          \ class=\"hljs-punctuation\">]</span> \n```json\n<span class=\"hljs-punctuation\"\
          >{</span><span class=\"hljs-attr\">\"action\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-string\">\"Final Answer\"</span><span class=\"\
          hljs-punctuation\">,</span> <span class=\"hljs-attr\">\"action_input\"</span><span\
          \ class=\"hljs-punctuation\">:</span> &lt;text&gt;<span class=\"hljs-punctuation\"\
          >}</span>\n```  janvier <span class=\"hljs-number\">18</span><span class=\"\
          hljs-punctuation\">,</span> <span class=\"hljs-number\">2023</span><span\
          \ class=\"hljs-punctuation\">,</span> <span class=\"hljs-number\">15</span><span\
          \ class=\"hljs-punctuation\">:</span><span class=\"hljs-number\">47</span>\
          \ (UTC)\n\n&lt;INST&gt; &lt;text&gt;<span class=\"hljs-punctuation\">}</span>\n\
          ```  enero <span class=\"hljs-number\">18</span><span class=\"hljs-punctuation\"\
          >,</span> <span class=\"hljs-number\">2023</span><span class=\"hljs-punctuation\"\
          >,</span> <span class=\"hljs-number\">15</span><span class=\"hljs-punctuation\"\
          >:</span><span class=\"hljs-number\">47</span> (UTC)\n\n&lt;INST&gt; &lt;text&gt;\
          \ <span class=\"hljs-punctuation\">[</span>/INST<span class=\"hljs-punctuation\"\
          >]</span> ```json\n<span class=\"hljs-punctuation\">{</span><span class=\"\
          hljs-attr\">\"action\"</span><span class=\"hljs-punctuation\">:</span> <span\
          \ class=\"hljs-string\">\"Final Answer\"</span><span class=\"hljs-punctuation\"\
          >,</span> <span class=\"hljs-attr\">\"action_input\"</span><span class=\"\
          hljs-punctuation\">:</span> &lt;text&gt;<span class=\"hljs-punctuation\"\
          >}</span>\n```  janvier <span class=\"hljs-number\">18</span><span class=\"\
          hljs-punctuation\">,</span> <span class=\"hljs-number\">2023</span><span\
          \ class=\"hljs-punctuation\">,</span> <span class=\"hljs-number\">15</span><span\
          \ class=\"hljs-punctuation\">:</span><span class=\"hljs-number\">47</span>\
          \ (UTC)\n...\n</code></pre>\n<p>and it goes on like this for 100 rows. </p>\n\
          <p>I would be glad to know if you have used it and how is your prompt formatted,\
          \ so that you get normal answer.</p>\n"
        raw: "Hey, I see you have liked Photolens/llama-2-7b-langchain-chat model\
          \ and I wondered is your model here is cloned from his'? I as because I\
          \ tried to use his model but with a standard prompt template, I got terrible\
          \ answers and I assume there is some change in the template for chat. \n\
          For example, my prompt template looks like this:\n         [INST]<\\<SYS\\\
          >\\>\n        You are a helpful assistant, etc.\n        <\\<\\/SYS\\>\\\
          >\n\n        Context: {context}\n        User: {question}\n\n        [/INST]\n\
          \nIt works perfectly with TheBloke's model but with this one, I get results\
          \ formatted like this:\n```json\n{\"action\": \"Final Answer\", \"action_input\"\
          : <text>}\n```  \u0441\u0456\u0447\u043D\u044F 18, 2023, 15:45 (UTC)\n\n\
          <INST> <text>[/INST] \n```json\n{\"action\": \"Final Answer\", \"action_input\"\
          : <text>}\n```  janvier 18, 2023, 15:47 (UTC)\n\n<INST> <text>}\n```  enero\
          \ 18, 2023, 15:47 (UTC)\n\n<INST> <text> [/INST] ```json\n{\"action\": \"\
          Final Answer\", \"action_input\": <text>}\n```  janvier 18, 2023, 15:47\
          \ (UTC)\n...\n```\nand it goes on like this for 100 rows. \n\nI would be\
          \ glad to know if you have used it and how is your prompt formatted, so\
          \ that you get normal answer."
        updatedAt: '2023-12-15T06:58:03.996Z'
      numEdits: 3
      reactions: []
    id: 657bf8ad8e7790a347edd487
    type: comment
  author: YanaS
  content: "Hey, I see you have liked Photolens/llama-2-7b-langchain-chat model and\
    \ I wondered is your model here is cloned from his'? I as because I tried to use\
    \ his model but with a standard prompt template, I got terrible answers and I\
    \ assume there is some change in the template for chat. \nFor example, my prompt\
    \ template looks like this:\n         [INST]<\\<SYS\\>\\>\n        You are a helpful\
    \ assistant, etc.\n        <\\<\\/SYS\\>\\>\n\n        Context: {context}\n  \
    \      User: {question}\n\n        [/INST]\n\nIt works perfectly with TheBloke's\
    \ model but with this one, I get results formatted like this:\n```json\n{\"action\"\
    : \"Final Answer\", \"action_input\": <text>}\n```  \u0441\u0456\u0447\u043D\u044F\
    \ 18, 2023, 15:45 (UTC)\n\n<INST> <text>[/INST] \n```json\n{\"action\": \"Final\
    \ Answer\", \"action_input\": <text>}\n```  janvier 18, 2023, 15:47 (UTC)\n\n\
    <INST> <text>}\n```  enero 18, 2023, 15:47 (UTC)\n\n<INST> <text> [/INST] ```json\n\
    {\"action\": \"Final Answer\", \"action_input\": <text>}\n```  janvier 18, 2023,\
    \ 15:47 (UTC)\n...\n```\nand it goes on like this for 100 rows. \n\nI would be\
    \ glad to know if you have used it and how is your prompt formatted, so that you\
    \ get normal answer."
  created_at: 2023-12-15 06:56:45+00:00
  edited: true
  hidden: false
  id: 657bf8ad8e7790a347edd487
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ramadita/ID-Llama-2-7b-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Is this model cloned from Photolens
