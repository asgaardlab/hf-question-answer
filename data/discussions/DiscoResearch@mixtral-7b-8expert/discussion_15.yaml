!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wiseluke
conflicting_files: null
created_at: 2023-12-16 12:10:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14fac8bb4fa717dff206ba8d80fe2ec5.svg
      fullname: luke smith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wiseluke
      type: user
    createdAt: '2023-12-16T12:10:40.000Z'
    data:
      edited: false
      editors:
      - wiseluke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5049840807914734
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14fac8bb4fa717dff206ba8d80fe2ec5.svg
          fullname: luke smith
          isHf: false
          isPro: false
          name: wiseluke
          type: user
        html: '<p>test code:<br>import torch<br>from transformers import AutoModelForCausalLM,
          AutoTokenizer</p>

          <p>#model = AutoModelForCausalLM.from_pretrained("./model", low_cpu_mem_usage=True,
          device_map="cpu", trust_remote_code=True)<br>model = AutoModelForCausalLM.from_pretrained("./model",
          torch_dtype=torch.float16, device_map="cpu", trust_remote_code=True)<br>tok
          = AutoTokenizer.from_pretrained("./model").to("cpu")<br>x<br>= tok.encode("The
          mistral wind in is a phenomenon ", return_tensors="pt")<br>x = model.generate(x,
          max_new_tokens=128)<br>print(tok.batch_decode(x))</p>

          <p>error message:<br>/home//.local/lib/python3.10/site-packages/torch/onnx/_internal/_beartype.py:30:
          UserWarning: module ''beartype.roar'' has no attribute ''BeartypeDecorHintPep585DeprecationWarning''<br>  warnings.warn(f"{e}")<br>Traceback
          (most recent call last):<br>  File "/home//04_files/Model/mixtral/./test0.py",
          line 5, in <br>    model = AutoModelForCausalLM.from_pretrained("./model",
          torch_dtype=torch.float16, device_map="cpu", trust_remote_code=True)<br>  File
          "/home//.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py",
          line 553, in from_pretrained<br>    model_class = get_class_from_dynamic_module(<br>  File
          "/home//.local/lib/python3.10/site-packages/transformers/dynamic_module_utils.py",
          line 488, in get_class_from_dynamic_module<br>    final_module = get_cached_module_file(<br>  File
          "/home//.local/lib/python3.10/site-packages/transformers/dynamic_module_utils.py",
          line 315, in get_cached_module_file<br>    modules_needed = check_imports(resolved_module_file)<br>  File
          "/home//.local/lib/python3.10/site-packages/transformers/dynamic_module_utils.py",
          line 180, in check_imports<br>    raise ImportError(<br>ImportError: This
          modeling file requires the following packages that were not found in your
          environment: flash_attn. Run <code>pip install flash_attn</code><br>I tried
          to install flash_attn ,but it cannot install succ without a cuda driver,
          so this model cannot work on cpu now?</p>

          '
        raw: "test code:\r\nimport torch\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\r\n\r\n#model = AutoModelForCausalLM.from_pretrained(\"\
          ./model\", low_cpu_mem_usage=True, device_map=\"cpu\", trust_remote_code=True)\r\
          \nmodel = AutoModelForCausalLM.from_pretrained(\"./model\", torch_dtype=torch.float16,\
          \ device_map=\"cpu\", trust_remote_code=True)\r\ntok = AutoTokenizer.from_pretrained(\"\
          ./model\").to(\"cpu\")\r\nx\r\n= tok.encode(\"The mistral wind in is a phenomenon\
          \ \", return_tensors=\"pt\")\r\nx = model.generate(x, max_new_tokens=128)\r\
          \nprint(tok.batch_decode(x))\r\n\r\nerror message:\r\n/home//.local/lib/python3.10/site-packages/torch/onnx/_internal/_beartype.py:30:\
          \ UserWarning: module 'beartype.roar' has no attribute 'BeartypeDecorHintPep585DeprecationWarning'\r\
          \n  warnings.warn(f\"{e}\")\r\nTraceback (most recent call last):\r\n  File\
          \ \"/home//04_files/Model/mixtral/./test0.py\", line 5, in <module>\r\n\
          \    model = AutoModelForCausalLM.from_pretrained(\"./model\", torch_dtype=torch.float16,\
          \ device_map=\"cpu\", trust_remote_code=True)\r\n  File \"/home//.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 553, in from_pretrained\r\n    model_class = get_class_from_dynamic_module(\r\
          \n  File \"/home//.local/lib/python3.10/site-packages/transformers/dynamic_module_utils.py\"\
          , line 488, in get_class_from_dynamic_module\r\n    final_module = get_cached_module_file(\r\
          \n  File \"/home//.local/lib/python3.10/site-packages/transformers/dynamic_module_utils.py\"\
          , line 315, in get_cached_module_file\r\n    modules_needed = check_imports(resolved_module_file)\r\
          \n  File \"/home//.local/lib/python3.10/site-packages/transformers/dynamic_module_utils.py\"\
          , line 180, in check_imports\r\n    raise ImportError(\r\nImportError: This\
          \ modeling file requires the following packages that were not found in your\
          \ environment: flash_attn. Run `pip install flash_attn`\r\nI tried to install\
          \ flash_attn ,but it cannot install succ without a cuda driver, so this\
          \ model cannot work on cpu now?"
        updatedAt: '2023-12-16T12:10:40.423Z'
      numEdits: 0
      reactions: []
    id: 657d93c0a2ef321675cdc7c9
    type: comment
  author: wiseluke
  content: "test code:\r\nimport torch\r\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\r\n\r\n#model = AutoModelForCausalLM.from_pretrained(\"./model\"\
    , low_cpu_mem_usage=True, device_map=\"cpu\", trust_remote_code=True)\r\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(\"./model\", torch_dtype=torch.float16,\
    \ device_map=\"cpu\", trust_remote_code=True)\r\ntok = AutoTokenizer.from_pretrained(\"\
    ./model\").to(\"cpu\")\r\nx\r\n= tok.encode(\"The mistral wind in is a phenomenon\
    \ \", return_tensors=\"pt\")\r\nx = model.generate(x, max_new_tokens=128)\r\n\
    print(tok.batch_decode(x))\r\n\r\nerror message:\r\n/home//.local/lib/python3.10/site-packages/torch/onnx/_internal/_beartype.py:30:\
    \ UserWarning: module 'beartype.roar' has no attribute 'BeartypeDecorHintPep585DeprecationWarning'\r\
    \n  warnings.warn(f\"{e}\")\r\nTraceback (most recent call last):\r\n  File \"\
    /home//04_files/Model/mixtral/./test0.py\", line 5, in <module>\r\n    model =\
    \ AutoModelForCausalLM.from_pretrained(\"./model\", torch_dtype=torch.float16,\
    \ device_map=\"cpu\", trust_remote_code=True)\r\n  File \"/home//.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 553, in from_pretrained\r\n    model_class = get_class_from_dynamic_module(\r\
    \n  File \"/home//.local/lib/python3.10/site-packages/transformers/dynamic_module_utils.py\"\
    , line 488, in get_class_from_dynamic_module\r\n    final_module = get_cached_module_file(\r\
    \n  File \"/home//.local/lib/python3.10/site-packages/transformers/dynamic_module_utils.py\"\
    , line 315, in get_cached_module_file\r\n    modules_needed = check_imports(resolved_module_file)\r\
    \n  File \"/home//.local/lib/python3.10/site-packages/transformers/dynamic_module_utils.py\"\
    , line 180, in check_imports\r\n    raise ImportError(\r\nImportError: This modeling\
    \ file requires the following packages that were not found in your environment:\
    \ flash_attn. Run `pip install flash_attn`\r\nI tried to install flash_attn ,but\
    \ it cannot install succ without a cuda driver, so this model cannot work on cpu\
    \ now?"
  created_at: 2023-12-16 12:10:40+00:00
  edited: false
  hidden: false
  id: 657d93c0a2ef321675cdc7c9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: DiscoResearch/mixtral-7b-8expert
repo_type: model
status: open
target_branch: null
title: can this model run on cpu, I had a error when I test it on cpu, this with a
  error about should install flash_attn
