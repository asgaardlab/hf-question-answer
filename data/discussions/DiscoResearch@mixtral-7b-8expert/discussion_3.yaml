!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AARon99
conflicting_files: null
created_at: 2023-12-10 18:45:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c148a32d2e7deeeea6db252b90973ea0.svg
      fullname: AaRon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AARon99
      type: user
    createdAt: '2023-12-10T18:45:52.000Z'
    data:
      edited: false
      editors:
      - AARon99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9425164461135864
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c148a32d2e7deeeea6db252b90973ea0.svg
          fullname: AaRon
          isHf: false
          isPro: false
          name: AARon99
          type: user
        html: '<p>Hello, I saw your discord message in mistral when you first uploaded
          the converted files.  I snagged them right away and did some testing in
          oobabooga''s textgen-webui.  I made this post on reddit: <a rel="nofollow"
          href="https://www.reddit.com/r/Oobabooga/comments/18e5wi7/mixtral7b8expert_working_in_oobabooga_unquantized/">https://www.reddit.com/r/Oobabooga/comments/18e5wi7/mixtral7b8expert_working_in_oobabooga_unquantized/</a><br>Discussing
          how interesting the model was.</p>

          <p>The next day I wanted to mess around with it more, but noticed the spark
          I had experienced the previous day was gone.  It failed the riddle question,
          giving answers very similar to other LLMs and the code wasn''t formatted
          right and incorrect.  I spent a while trying to pinpoint what happened,
          and a way to resolve the issue.  The .py files you updated resulted in what
          seems to be significant changes in the models'' output.  I am now using
          the original .py files directly and forcing the model to use those local
          files instead of the .cache files it had overwritten with the updated .py
          data.</p>

          <p>The files and an explanation are here: <a rel="nofollow" href="https://github.com/RandomInternetPreson/MiscFiles/blob/main/DiscoResearch/mixtral-7b-8expert/info.md">https://github.com/RandomInternetPreson/MiscFiles/blob/main/DiscoResearch/mixtral-7b-8expert/info.md</a></p>

          <p>Anyway, I know the model is still a bit of an enigma and trying to run
          it is guess work, but wanted to reach out and let you know, I think you
          were running the model better with the original .py file.</p>

          '
        raw: "Hello, I saw your discord message in mistral when you first uploaded\
          \ the converted files.  I snagged them right away and did some testing in\
          \ oobabooga's textgen-webui.  I made this post on reddit: https://www.reddit.com/r/Oobabooga/comments/18e5wi7/mixtral7b8expert_working_in_oobabooga_unquantized/\r\
          \nDiscussing how interesting the model was.\r\n\r\nThe next day I wanted\
          \ to mess around with it more, but noticed the spark I had experienced the\
          \ previous day was gone.  It failed the riddle question, giving answers\
          \ very similar to other LLMs and the code wasn't formatted right and incorrect.\
          \  I spent a while trying to pinpoint what happened, and a way to resolve\
          \ the issue.  The .py files you updated resulted in what seems to be significant\
          \ changes in the models' output.  I am now using the original .py files\
          \ directly and forcing the model to use those local files instead of the\
          \ .cache files it had overwritten with the updated .py data.\r\n\r\nThe\
          \ files and an explanation are here: https://github.com/RandomInternetPreson/MiscFiles/blob/main/DiscoResearch/mixtral-7b-8expert/info.md\r\
          \n\r\nAnyway, I know the model is still a bit of an enigma and trying to\
          \ run it is guess work, but wanted to reach out and let you know, I think\
          \ you were running the model better with the original .py file."
        updatedAt: '2023-12-10T18:45:52.185Z'
      numEdits: 0
      reactions: []
    id: 65760760d0ed8f57614a8d10
    type: comment
  author: AARon99
  content: "Hello, I saw your discord message in mistral when you first uploaded the\
    \ converted files.  I snagged them right away and did some testing in oobabooga's\
    \ textgen-webui.  I made this post on reddit: https://www.reddit.com/r/Oobabooga/comments/18e5wi7/mixtral7b8expert_working_in_oobabooga_unquantized/\r\
    \nDiscussing how interesting the model was.\r\n\r\nThe next day I wanted to mess\
    \ around with it more, but noticed the spark I had experienced the previous day\
    \ was gone.  It failed the riddle question, giving answers very similar to other\
    \ LLMs and the code wasn't formatted right and incorrect.  I spent a while trying\
    \ to pinpoint what happened, and a way to resolve the issue.  The .py files you\
    \ updated resulted in what seems to be significant changes in the models' output.\
    \  I am now using the original .py files directly and forcing the model to use\
    \ those local files instead of the .cache files it had overwritten with the updated\
    \ .py data.\r\n\r\nThe files and an explanation are here: https://github.com/RandomInternetPreson/MiscFiles/blob/main/DiscoResearch/mixtral-7b-8expert/info.md\r\
    \n\r\nAnyway, I know the model is still a bit of an enigma and trying to run it\
    \ is guess work, but wanted to reach out and let you know, I think you were running\
    \ the model better with the original .py file."
  created_at: 2023-12-10 18:45:52+00:00
  edited: false
  hidden: false
  id: 65760760d0ed8f57614a8d10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-12-10T18:56:18.000Z'
    data:
      edited: true
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9703579545021057
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>Yeah it seems to be the case that the change we made (following
          the equations from some other MoE papers) is not in line with the original
          implementation. It''s interesting that you noticed this in normal use. Benchmarks
          for the current method were a few % higher. For now, I''ll revert these
          changes in form of a pr and then you can get the new version by calling
          <code>revision="refs/pr/5"</code>.</p>

          '
        raw: Yeah it seems to be the case that the change we made (following the equations
          from some other MoE papers) is not in line with the original implementation.
          It's interesting that you noticed this in normal use. Benchmarks for the
          current method were a few % higher. For now, I'll revert these changes in
          form of a pr and then you can get the new version by calling `revision="refs/pr/5"`.
        updatedAt: '2023-12-10T18:57:32.781Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - AARon99
        - Go2Heart
    id: 657609d20cfadb728abf5b58
    type: comment
  author: bjoernp
  content: Yeah it seems to be the case that the change we made (following the equations
    from some other MoE papers) is not in line with the original implementation. It's
    interesting that you noticed this in normal use. Benchmarks for the current method
    were a few % higher. For now, I'll revert these changes in form of a pr and then
    you can get the new version by calling `revision="refs/pr/5"`.
  created_at: 2023-12-10 18:56:18+00:00
  edited: true
  hidden: false
  id: 657609d20cfadb728abf5b58
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c148a32d2e7deeeea6db252b90973ea0.svg
      fullname: AaRon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AARon99
      type: user
    createdAt: '2023-12-10T19:59:43.000Z'
    data:
      edited: false
      editors:
      - AARon99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9891351461410522
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c148a32d2e7deeeea6db252b90973ea0.svg
          fullname: AaRon
          isHf: false
          isPro: false
          name: AARon99
          type: user
        html: '<p>Oh wow that is extremely interesting!! It seemed that there really
          was something different about mixtral when I was using it initially, and
          it took me a while to figure out what happened.  It is such a relief to
          know that the troubleshooting I did was accurate.  I was thinking that''s
          why the change was made, because it might have scored higher, I wish I understood
          the complexity of the issue better.  I have personal questions I use with
          deterministic settings (as deterministic as possible) to see if I want to
          spend time working with a model and the change to the .py files was like
          night and day; I had some initial trepidation about making the post because
          I couldn''t point to a score to demonstrate the difference.  Thank you again,
          I''m really interested to see where this model goes and how it changes the
          open source llm landscape.  I''m also really looking forward to trying out
          your fine-tuned version!!!</p>

          '
        raw: Oh wow that is extremely interesting!! It seemed that there really was
          something different about mixtral when I was using it initially, and it
          took me a while to figure out what happened.  It is such a relief to know
          that the troubleshooting I did was accurate.  I was thinking that's why
          the change was made, because it might have scored higher, I wish I understood
          the complexity of the issue better.  I have personal questions I use with
          deterministic settings (as deterministic as possible) to see if I want to
          spend time working with a model and the change to the .py files was like
          night and day; I had some initial trepidation about making the post because
          I couldn't point to a score to demonstrate the difference.  Thank you again,
          I'm really interested to see where this model goes and how it changes the
          open source llm landscape.  I'm also really looking forward to trying out
          your fine-tuned version!!!
        updatedAt: '2023-12-10T19:59:43.097Z'
      numEdits: 0
      reactions: []
    id: 657618af12ae60c542c28540
    type: comment
  author: AARon99
  content: Oh wow that is extremely interesting!! It seemed that there really was
    something different about mixtral when I was using it initially, and it took me
    a while to figure out what happened.  It is such a relief to know that the troubleshooting
    I did was accurate.  I was thinking that's why the change was made, because it
    might have scored higher, I wish I understood the complexity of the issue better.  I
    have personal questions I use with deterministic settings (as deterministic as
    possible) to see if I want to spend time working with a model and the change to
    the .py files was like night and day; I had some initial trepidation about making
    the post because I couldn't point to a score to demonstrate the difference.  Thank
    you again, I'm really interested to see where this model goes and how it changes
    the open source llm landscape.  I'm also really looking forward to trying out
    your fine-tuned version!!!
  created_at: 2023-12-10 19:59:43+00:00
  edited: false
  hidden: false
  id: 657618af12ae60c542c28540
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: DiscoResearch/mixtral-7b-8expert
repo_type: model
status: open
target_branch: null
title: Really appreciate the work put into this! I have noticed a change in the model
  output since first release.
