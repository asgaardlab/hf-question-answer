!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Thalesian
conflicting_files: null
created_at: 2023-12-10 18:52:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5ec3d068aa882da3224bff59a1f9f258.svg
      fullname: Lee Drake
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thalesian
      type: user
    createdAt: '2023-12-10T18:52:06.000Z'
    data:
      edited: false
      editors:
      - Thalesian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9737848043441772
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5ec3d068aa882da3224bff59a1f9f258.svg
          fullname: Lee Drake
          isHf: false
          isPro: false
          name: Thalesian
          type: user
        html: '<p>Title says it all. This should run on a Mac M1 architecture (some
          have VRAM &gt; 98, so can run this). However flash_attn is called on repeatedly,
          and hard to re-code without it. </p>

          '
        raw: 'Title says it all. This should run on a Mac M1 architecture (some have
          VRAM > 98, so can run this). However flash_attn is called on repeatedly,
          and hard to re-code without it. '
        updatedAt: '2023-12-10T18:52:06.303Z'
      numEdits: 0
      reactions: []
    id: 657608d6177b3b4663791f7a
    type: comment
  author: Thalesian
  content: 'Title says it all. This should run on a Mac M1 architecture (some have
    VRAM > 98, so can run this). However flash_attn is called on repeatedly, and hard
    to re-code without it. '
  created_at: 2023-12-10 18:52:06+00:00
  edited: false
  hidden: false
  id: 657608d6177b3b4663791f7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-12-10T18:58:52.000Z'
    data:
      edited: false
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.879203200340271
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>The code is equivalent to the standard mistral 7b code other than
          the MoE integration which does not use attention. Flash attention should
          only be used when loading the model with <code>use_flash_attn_2=True</code>,
          otherwise it should be good. Have you tried it?</p>

          '
        raw: The code is equivalent to the standard mistral 7b code other than the
          MoE integration which does not use attention. Flash attention should only
          be used when loading the model with `use_flash_attn_2=True`, otherwise it
          should be good. Have you tried it?
        updatedAt: '2023-12-10T18:58:52.600Z'
      numEdits: 0
      reactions: []
    id: 65760a6c556a38db58dd120d
    type: comment
  author: bjoernp
  content: The code is equivalent to the standard mistral 7b code other than the MoE
    integration which does not use attention. Flash attention should only be used
    when loading the model with `use_flash_attn_2=True`, otherwise it should be good.
    Have you tried it?
  created_at: 2023-12-10 18:58:52+00:00
  edited: false
  hidden: false
  id: 65760a6c556a38db58dd120d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5ec3d068aa882da3224bff59a1f9f258.svg
      fullname: Lee Drake
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thalesian
      type: user
    createdAt: '2023-12-10T19:35:20.000Z'
    data:
      edited: false
      editors:
      - Thalesian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4512801468372345
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5ec3d068aa882da3224bff59a1f9f258.svg
          fullname: Lee Drake
          isHf: false
          isPro: false
          name: Thalesian
          type: user
        html: '<p>import torch<br>from transformers import AutoModelForCausalLM, AutoTokenizer</p>

          <p>model = AutoModelForCausalLM.from_pretrained("~/LLM/mixtral-8x7b-32kseqlen",
          low_cpu_mem_usage=True, device_map="auto", trust_remote_code=True)</p>

          <p>Traceback (most recent call last):<br>  File "", line 1, in <br>  File
          "<del>/miniconda3/envs/textgen/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py",
          line 553, in from_pretrained<br>    model_class = get_class_from_dynamic_module(<br>                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "</del>/miniconda3/envs/textgen/lib/python3.11/site-packages/transformers/dynamic_module_utils.py",
          line 487, in get_class_from_dynamic_module<br>    final_module = get_cached_module_file(<br>                   ^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "<del>/miniconda3/envs/textgen/lib/python3.11/site-packages/transformers/dynamic_module_utils.py",
          line 314, in get_cached_module_file<br>    modules_needed = check_imports(resolved_module_file)<br>                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "</del>/miniconda3/envs/textgen/lib/python3.11/site-packages/transformers/dynamic_module_utils.py",
          line 179, in check_imports<br>    raise ImportError(<br>ImportError: This
          modeling file requires the following packages that were not found in your
          environment: flash_attn. Run <code>pip install flash_attn</code></p>

          <p>Not sure why it keeps trying to get flash attention - this hasn''t been
          a problem with other models. </p>

          '
        raw: "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(\"~/LLM/mixtral-8x7b-32kseqlen\"\
          , low_cpu_mem_usage=True, device_map=\"auto\", trust_remote_code=True)\n\
          \nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n\
          \  File \"~/miniconda3/envs/textgen/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 553, in from_pretrained\n    model_class = get_class_from_dynamic_module(\n\
          \                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"~/miniconda3/envs/textgen/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 487, in get_class_from_dynamic_module\n    final_module = get_cached_module_file(\n\
          \                   ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"~/miniconda3/envs/textgen/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 314, in get_cached_module_file\n    modules_needed = check_imports(resolved_module_file)\n\
          \                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"~/miniconda3/envs/textgen/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 179, in check_imports\n    raise ImportError(\nImportError: This\
          \ modeling file requires the following packages that were not found in your\
          \ environment: flash_attn. Run `pip install flash_attn`\n\nNot sure why\
          \ it keeps trying to get flash attention - this hasn't been a problem with\
          \ other models. "
        updatedAt: '2023-12-10T19:35:20.342Z'
      numEdits: 0
      reactions: []
    id: 657612f8d7f487de5f075afb
    type: comment
  author: Thalesian
  content: "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
    \nmodel = AutoModelForCausalLM.from_pretrained(\"~/LLM/mixtral-8x7b-32kseqlen\"\
    , low_cpu_mem_usage=True, device_map=\"auto\", trust_remote_code=True)\n\nTraceback\
    \ (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"\
    ~/miniconda3/envs/textgen/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 553, in from_pretrained\n    model_class = get_class_from_dynamic_module(\n\
    \                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"~/miniconda3/envs/textgen/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
    , line 487, in get_class_from_dynamic_module\n    final_module = get_cached_module_file(\n\
    \                   ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"~/miniconda3/envs/textgen/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
    , line 314, in get_cached_module_file\n    modules_needed = check_imports(resolved_module_file)\n\
    \                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"~/miniconda3/envs/textgen/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
    , line 179, in check_imports\n    raise ImportError(\nImportError: This modeling\
    \ file requires the following packages that were not found in your environment:\
    \ flash_attn. Run `pip install flash_attn`\n\nNot sure why it keeps trying to\
    \ get flash attention - this hasn't been a problem with other models. "
  created_at: 2023-12-10 19:35:20+00:00
  edited: false
  hidden: false
  id: 657612f8d7f487de5f075afb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
      fullname: mrfakename
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrfakename
      type: user
    createdAt: '2023-12-10T21:42:41.000Z'
    data:
      edited: false
      editors:
      - mrfakename
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9640820622444153
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
          fullname: mrfakename
          isHf: false
          isPro: false
          name: mrfakename
          type: user
        html: '<blockquote>

          <p>Title says it all. This should run on a Mac M1 architecture (some have
          VRAM &gt; 98, so can run this). However flash_attn is called on repeatedly,
          and hard to re-code without it.</p>

          </blockquote>

          <p>Some have VRAM &gt; 98 GB for CPU or GPU?</p>

          '
        raw: '> Title says it all. This should run on a Mac M1 architecture (some
          have VRAM > 98, so can run this). However flash_attn is called on repeatedly,
          and hard to re-code without it.


          Some have VRAM > 98 GB for CPU or GPU?'
        updatedAt: '2023-12-10T21:42:41.195Z'
      numEdits: 0
      reactions: []
    id: 657630d158ce19fa1e1b1d53
    type: comment
  author: mrfakename
  content: '> Title says it all. This should run on a Mac M1 architecture (some have
    VRAM > 98, so can run this). However flash_attn is called on repeatedly, and hard
    to re-code without it.


    Some have VRAM > 98 GB for CPU or GPU?'
  created_at: 2023-12-10 21:42:41+00:00
  edited: false
  hidden: false
  id: 657630d158ce19fa1e1b1d53
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: DiscoResearch/mixtral-7b-8expert
repo_type: model
status: open
target_branch: null
title: Flash dependency (locks out non-NVIDIA GPUs)
