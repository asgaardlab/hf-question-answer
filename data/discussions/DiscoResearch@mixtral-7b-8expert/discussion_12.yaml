!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rvsh
conflicting_files: null
created_at: 2023-12-12 20:43:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/929e55c828065fe1e43723b44af79ce8.svg
      fullname: rvsh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rvsh
      type: user
    createdAt: '2023-12-12T20:43:21.000Z'
    data:
      edited: false
      editors:
      - rvsh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7832546234130859
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/929e55c828065fe1e43723b44af79ce8.svg
          fullname: rvsh
          isHf: false
          isPro: false
          name: rvsh
          type: user
        html: '<p>Can this model be fine tuned? For example with this dataset for
          polish language?<br><a href="https://huggingface.co/datasets/mmosiolek/pl_alpaca_data_cleaned">https://huggingface.co/datasets/mmosiolek/pl_alpaca_data_cleaned</a></p>

          '
        raw: "Can this model be fine tuned? For example with this dataset for polish\
          \ language?\r\nhttps://huggingface.co/datasets/mmosiolek/pl_alpaca_data_cleaned\r\
          \n\r\n"
        updatedAt: '2023-12-12T20:43:21.129Z'
      numEdits: 0
      reactions: []
    id: 6578c5e9f7d98f6f612bae3e
    type: comment
  author: rvsh
  content: "Can this model be fine tuned? For example with this dataset for polish\
    \ language?\r\nhttps://huggingface.co/datasets/mmosiolek/pl_alpaca_data_cleaned\r\
    \n\r\n"
  created_at: 2023-12-12 20:43:21+00:00
  edited: false
  hidden: false
  id: 6578c5e9f7d98f6f612bae3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a4c9555f81779975bfec368c7f351c6e.svg
      fullname: xinrui zhu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: myzz
      type: user
    createdAt: '2023-12-14T05:39:04.000Z'
    data:
      edited: false
      editors:
      - myzz
      hidden: false
      identifiedLanguage:
        language: fr
        probability: 0.49138373136520386
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a4c9555f81779975bfec368c7f351c6e.svg
          fullname: xinrui zhu
          isHf: false
          isPro: false
          name: myzz
          type: user
        html: '<p>3232</p>

          '
        raw: '3232'
        updatedAt: '2023-12-14T05:39:04.196Z'
      numEdits: 0
      reactions: []
    id: 657a94f8c222807b36f06230
    type: comment
  author: myzz
  content: '3232'
  created_at: 2023-12-14 05:39:04+00:00
  edited: false
  hidden: false
  id: 657a94f8c222807b36f06230
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c44f5ddd7e6bca734d3ab701d399ce81.svg
      fullname: ChrisLiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cxxs
      type: user
    createdAt: '2023-12-15T13:03:35.000Z'
    data:
      edited: false
      editors:
      - Cxxs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7931908965110779
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c44f5ddd7e6bca734d3ab701d399ce81.svg
          fullname: ChrisLiu
          isHf: false
          isPro: false
          name: Cxxs
          type: user
        html: '<p>LLaMA2-Accessory now supports the inference and instruction finetuning
          (both full-parameter and PEFT like LoRA) of mixtral-8x7b-32kseqlen. It supports
          the load balancing loss and some other MoE support. The document is available
          <a rel="nofollow" href="https://llama2-accessory.readthedocs.io/en/latest/projects/mixtral-8x7b.html">here</a></p>

          '
        raw: LLaMA2-Accessory now supports the inference and instruction finetuning
          (both full-parameter and PEFT like LoRA) of mixtral-8x7b-32kseqlen. It supports
          the load balancing loss and some other MoE support. The document is available
          [here](https://llama2-accessory.readthedocs.io/en/latest/projects/mixtral-8x7b.html)
        updatedAt: '2023-12-15T13:03:35.601Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - myzz
    id: 657c4ea72c19352966487c1e
    type: comment
  author: Cxxs
  content: LLaMA2-Accessory now supports the inference and instruction finetuning
    (both full-parameter and PEFT like LoRA) of mixtral-8x7b-32kseqlen. It supports
    the load balancing loss and some other MoE support. The document is available
    [here](https://llama2-accessory.readthedocs.io/en/latest/projects/mixtral-8x7b.html)
  created_at: 2023-12-15 13:03:35+00:00
  edited: false
  hidden: false
  id: 657c4ea72c19352966487c1e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: DiscoResearch/mixtral-7b-8expert
repo_type: model
status: open
target_branch: null
title: Can this model be fine tuned?
