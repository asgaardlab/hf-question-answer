!!python/object:huggingface_hub.community.DiscussionWithDetails
author: coltonbehannon
conflicting_files: null
created_at: 2023-12-06 22:50:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647e2ab011084fb5831be93c/aPMisEKQYFsKfBVpZ_-Bb.png?w=200&h=200&f=face
      fullname: Colton Behannon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: coltonbehannon
      type: user
    createdAt: '2023-12-06T22:50:39.000Z'
    data:
      edited: false
      editors:
      - coltonbehannon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4864852726459503
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647e2ab011084fb5831be93c/aPMisEKQYFsKfBVpZ_-Bb.png?w=200&h=200&f=face
          fullname: Colton Behannon
          isHf: false
          isPro: false
          name: coltonbehannon
          type: user
        html: "<p>Hello,</p>\n<p>When running text-generation-inference with:<br><code>text-generation-launcher\
          \ --model-id TheBloke/neural-chat-7B-v3-2-GPTQ --revision main --quantize\
          \ gptq</code><br>everything works great and it is able to run.<br>However,\
          \ when I run:<br><code>text-generation-launcher --model-id TheBloke/neural-chat-7B-v3-2-GPTQ\
          \ --revision gptq-4bit-32g-actorder_True --quantize gptq</code><br>I get\
          \ the following error:</p>\n<pre><code>ERROR text_generation_launcher: Error\
          \ when initializing model\nTraceback (most recent call last):\n  File \"\
          /opt/conda/bin/text-generation-server\", line 8, in &lt;module&gt;\n   \
          \ sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157,\
          \ in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434,\
          \ in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 783, in\
          \ invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          &gt; File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 252, in get_model\n    return FlashMistral(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 321, in __init__\n    model = FlashMistralForCausalLM(config, weights)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 480, in __init__\n    self.model = MistralModel(config, weights)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 416, in __init__\n    [\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 417, in &lt;listcomp&gt;\n    MistralLayer(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 351, in __init__\n    self.self_attn = MistralAttention(\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 227, in __init__\n    self.query_key_value = load_attention(config,\
          \ prefix, weights)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 156, in load_attention\n    return _load_gqa(config, prefix, weights)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 189, in _load_gqa\n    get_linear(weight, bias=None, quantize=config.quantize)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 311, in get_linear\n    linear = Ex4bitLinear(qweight, qzeros, scales,\
          \ g_idx, bias, bits, groupsize)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\"\
          , line 115, in __init__\n    assert groupsize == self.groupsize\nAssertionError\n\
          \n2023-12-06T22:44:50.981264Z ERROR shard-manager: text_generation_launcher:\
          \ Shard complete standard error output:\n\nTraceback (most recent call last):\n\
          \n  File \"/opt/conda/bin/text-generation-server\", line 8, in &lt;module&gt;\n\
          \    sys.exit(app())\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 83, in serve\n    server.serve(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 207, in serve\n    asyncio.run(\n\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n\n  File \"\
          /opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n\
          \    return future.result()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 159, in serve_inner\n    model = get_model(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 252, in get_model\n    return FlashMistral(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 321, in __init__\n    model = FlashMistralForCausalLM(config, weights)\n\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 480, in __init__\n    self.model = MistralModel(config, weights)\n\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 416, in __init__\n    [\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 417, in &lt;listcomp&gt;\n    MistralLayer(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 351, in __init__\n    self.self_attn = MistralAttention(\n\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 227, in __init__\n    self.query_key_value = load_attention(config,\
          \ prefix, weights)\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 156, in load_attention\n    return _load_gqa(config, prefix, weights)\n\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 189, in _load_gqa\n    get_linear(weight, bias=None, quantize=config.quantize)\n\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 311, in get_linear\n    linear = Ex4bitLinear(qweight, qzeros, scales,\
          \ g_idx, bias, bits, groupsize)\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\"\
          , line 115, in __init__\n    assert groupsize == self.groupsize\n\nAssertionError\n\
          \ rank=0\n2023-12-06T22:44:51.079122Z ERROR text_generation_launcher: Shard\
          \ 0 failed to start\n2023-12-06T22:44:51.079162Z  INFO text_generation_launcher:\
          \ Shutting down shards\nError: ShardCannotStart\n</code></pre>\n<p>Any suggestions?</p>\n"
        raw: "Hello,\r\n\r\nWhen running text-generation-inference with:\r\n```text-generation-launcher\
          \ --model-id TheBloke/neural-chat-7B-v3-2-GPTQ --revision main --quantize\
          \ gptq``` \r\neverything works great and it is able to run. \r\nHowever,\
          \ when I run:\r\n```text-generation-launcher --model-id TheBloke/neural-chat-7B-v3-2-GPTQ\
          \ --revision gptq-4bit-32g-actorder_True --quantize gptq``` \r\nI get the\
          \ following error:\r\n\r\n```\r\nERROR text_generation_launcher: Error when\
          \ initializing model\r\nTraceback (most recent call last):\r\n  File \"\
          /opt/conda/bin/text-generation-server\", line 8, in <module>\r\n    sys.exit(app())\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\", line\
          \ 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157,\
          \ in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\r\n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line\
          \ 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\r\
          \n    return callback(**use_params)  # type: ignore\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 83, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 207, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 634, in run_until_complete\r\
          \n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\
          \n> File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 159, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 252, in get_model\r\n    return FlashMistral(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 321, in __init__\r\n    model = FlashMistralForCausalLM(config, weights)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 480, in __init__\r\n    self.model = MistralModel(config, weights)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 416, in __init__\r\n    [\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 417, in <listcomp>\r\n    MistralLayer(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 351, in __init__\r\n    self.self_attn = MistralAttention(\r\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 227, in __init__\r\n    self.query_key_value = load_attention(config,\
          \ prefix, weights)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 156, in load_attention\r\n    return _load_gqa(config, prefix, weights)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 189, in _load_gqa\r\n    get_linear(weight, bias=None, quantize=config.quantize)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 311, in get_linear\r\n    linear = Ex4bitLinear(qweight, qzeros,\
          \ scales, g_idx, bias, bits, groupsize)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\"\
          , line 115, in __init__\r\n    assert groupsize == self.groupsize\r\nAssertionError\r\
          \n\r\n2023-12-06T22:44:50.981264Z ERROR shard-manager: text_generation_launcher:\
          \ Shard complete standard error output:\r\n\r\nTraceback (most recent call\
          \ last):\r\n\r\n  File \"/opt/conda/bin/text-generation-server\", line 8,\
          \ in <module>\r\n    sys.exit(app())\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 83, in serve\r\n    server.serve(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 207, in serve\r\n    asyncio.run(\r\n\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\r\
          \n    return future.result()\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 159, in serve_inner\r\n    model = get_model(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 252, in get_model\r\n    return FlashMistral(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 321, in __init__\r\n    model = FlashMistralForCausalLM(config, weights)\r\
          \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 480, in __init__\r\n    self.model = MistralModel(config, weights)\r\
          \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 416, in __init__\r\n    [\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 417, in <listcomp>\r\n    MistralLayer(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 351, in __init__\r\n    self.self_attn = MistralAttention(\r\n\r\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 227, in __init__\r\n    self.query_key_value = load_attention(config,\
          \ prefix, weights)\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 156, in load_attention\r\n    return _load_gqa(config, prefix, weights)\r\
          \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
          , line 189, in _load_gqa\r\n    get_linear(weight, bias=None, quantize=config.quantize)\r\
          \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
          , line 311, in get_linear\r\n    linear = Ex4bitLinear(qweight, qzeros,\
          \ scales, g_idx, bias, bits, groupsize)\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\"\
          , line 115, in __init__\r\n    assert groupsize == self.groupsize\r\n\r\n\
          AssertionError\r\n rank=0\r\n2023-12-06T22:44:51.079122Z ERROR text_generation_launcher:\
          \ Shard 0 failed to start\r\n2023-12-06T22:44:51.079162Z  INFO text_generation_launcher:\
          \ Shutting down shards\r\nError: ShardCannotStart\r\n```\r\n\r\nAny suggestions?"
        updatedAt: '2023-12-06T22:50:39.669Z'
      numEdits: 0
      reactions: []
    id: 6570fabf2711e943ed77d92c
    type: comment
  author: coltonbehannon
  content: "Hello,\r\n\r\nWhen running text-generation-inference with:\r\n```text-generation-launcher\
    \ --model-id TheBloke/neural-chat-7B-v3-2-GPTQ --revision main --quantize gptq```\
    \ \r\neverything works great and it is able to run. \r\nHowever, when I run:\r\
    \n```text-generation-launcher --model-id TheBloke/neural-chat-7B-v3-2-GPTQ --revision\
    \ gptq-4bit-32g-actorder_True --quantize gptq``` \r\nI get the following error:\r\
    \n\r\n```\r\nERROR text_generation_launcher: Error when initializing model\r\n\
    Traceback (most recent call last):\r\n  File \"/opt/conda/bin/text-generation-server\"\
    , line 8, in <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\n \
    \ File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in\
    \ __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 778, in main\r\n    return _main(\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434,\
    \ in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/click/core.py\", line 783, in invoke\r\n\
    \    return __callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 83, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 207, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 634, in run_until_complete\r\n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 601, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 1905, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n>\
    \ File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 159, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 252, in get_model\r\n    return FlashMistral(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_mistral.py\"\
    , line 321, in __init__\r\n    model = FlashMistralForCausalLM(config, weights)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 480, in __init__\r\n    self.model = MistralModel(config, weights)\r\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 416, in __init__\r\n    [\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 417, in <listcomp>\r\n    MistralLayer(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 351, in __init__\r\n    self.self_attn = MistralAttention(\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 227, in __init__\r\n    self.query_key_value = load_attention(config, prefix,\
    \ weights)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 156, in load_attention\r\n    return _load_gqa(config, prefix, weights)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 189, in _load_gqa\r\n    get_linear(weight, bias=None, quantize=config.quantize)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
    , line 311, in get_linear\r\n    linear = Ex4bitLinear(qweight, qzeros, scales,\
    \ g_idx, bias, bits, groupsize)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\"\
    , line 115, in __init__\r\n    assert groupsize == self.groupsize\r\nAssertionError\r\
    \n\r\n2023-12-06T22:44:50.981264Z ERROR shard-manager: text_generation_launcher:\
    \ Shard complete standard error output:\r\n\r\nTraceback (most recent call last):\r\
    \n\r\n  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\n\
    \    sys.exit(app())\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 83, in serve\r\n    server.serve(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 207, in serve\r\n    asyncio.run(\r\n\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n\r\n  File \"\
    /opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\r\
    \n    return future.result()\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 159, in serve_inner\r\n    model = get_model(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 252, in get_model\r\n    return FlashMistral(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_mistral.py\"\
    , line 321, in __init__\r\n    model = FlashMistralForCausalLM(config, weights)\r\
    \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 480, in __init__\r\n    self.model = MistralModel(config, weights)\r\n\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 416, in __init__\r\n    [\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 417, in <listcomp>\r\n    MistralLayer(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 351, in __init__\r\n    self.self_attn = MistralAttention(\r\n\r\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 227, in __init__\r\n    self.query_key_value = load_attention(config, prefix,\
    \ weights)\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 156, in load_attention\r\n    return _load_gqa(config, prefix, weights)\r\
    \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py\"\
    , line 189, in _load_gqa\r\n    get_linear(weight, bias=None, quantize=config.quantize)\r\
    \n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/layers.py\"\
    , line 311, in get_linear\r\n    linear = Ex4bitLinear(qweight, qzeros, scales,\
    \ g_idx, bias, bits, groupsize)\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/gptq/exllama.py\"\
    , line 115, in __init__\r\n    assert groupsize == self.groupsize\r\n\r\nAssertionError\r\
    \n rank=0\r\n2023-12-06T22:44:51.079122Z ERROR text_generation_launcher: Shard\
    \ 0 failed to start\r\n2023-12-06T22:44:51.079162Z  INFO text_generation_launcher:\
    \ Shutting down shards\r\nError: ShardCannotStart\r\n```\r\n\r\nAny suggestions?"
  created_at: 2023-12-06 22:50:39+00:00
  edited: false
  hidden: false
  id: 6570fabf2711e943ed77d92c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647e2ab011084fb5831be93c/aPMisEKQYFsKfBVpZ_-Bb.png?w=200&h=200&f=face
      fullname: Colton Behannon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: coltonbehannon
      type: user
    createdAt: '2023-12-06T22:54:01.000Z'
    data:
      edited: false
      editors:
      - coltonbehannon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6484960913658142
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647e2ab011084fb5831be93c/aPMisEKQYFsKfBVpZ_-Bb.png?w=200&h=200&f=face
          fullname: Colton Behannon
          isHf: false
          isPro: false
          name: coltonbehannon
          type: user
        html: '<p>This is using text-generation-inference 1.0</p>

          '
        raw: This is using text-generation-inference 1.0
        updatedAt: '2023-12-06T22:54:01.493Z'
      numEdits: 0
      reactions: []
    id: 6570fb89458930ab288bb9fc
    type: comment
  author: coltonbehannon
  content: This is using text-generation-inference 1.0
  created_at: 2023-12-06 22:54:01+00:00
  edited: false
  hidden: false
  id: 6570fb89458930ab288bb9fc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/neural-chat-7B-v3-2-GPTQ
repo_type: model
status: open
target_branch: null
title: 'ERROR text_generation_launcher: Error when initializing model'
