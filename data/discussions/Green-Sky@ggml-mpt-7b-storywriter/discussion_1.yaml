!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mirek190
conflicting_files: null
created_at: 2023-05-12 17:14:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-12T18:14:27.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>F:\LLAMA&gt;koboldcpp-1.20.exe --model ../llama.cpp/models/mpt-7b-storywriter_v0-q5_1.bin
          --threads 8 --smartcontext --highpriority --usemlock  --useclblast 1 0 --stream
          --contextsize 2048<br>Welcome to KoboldCpp - Version 1.20<br>Setting process
          to Higher Priority - Use Caution<br>High Priority for Windows Set: Priority.NORMAL_PRIORITY_CLASS
          to Priority.HIGH_PRIORITY_CLASS<br>Attempting to use CLBlast library for
          faster prompt ingestion. A compatible clblast will be required.<br>Initializing
          dynamic library: koboldcpp_clblast.dll<br>==========<br>Loading model: F:\LLAMA\llama.cpp\models\mpt-7b-storywriter_v0-q5_1.bin<br>[Threads:
          8, BlasThreads: 8, SmartContext: True]</p>

          <hr>

          <p>Identified as GPT-NEO-X model: (ver 401)<br>Attempting to Load...</p>

          <hr>

          <p>System Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI
          = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD
          = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |<br>stablelm_model_load: loading model
          from ''F:\LLAMA\llama.cpp\models\mpt-7b-storywriter_v0-q5_1.bin'' - please
          wait ...<br>stablelm_model_load: n_vocab = 4096<br>stablelm_model_load:
          n_ctx   = 65536<br>stablelm_model_load: n_embd  = 32<br>stablelm_model_load:
          n_head  = 32<br>stablelm_model_load: n_layer = 50432<br>stablelm_model_load:
          n_rot   = 9<br>stablelm_model_load: ftype   = 13<br>Traceback (most recent
          call last):<br>  File "koboldcpp.py", line 643, in <br>  File "koboldcpp.py",
          line 574, in main<br>  File "koboldcpp.py", line 157, in load_model<br>OSError:
          exception: access violation reading 0x0000000E961F0000<br>[54088] Failed
          to execute script ''koboldcpp'' due to unhandled exception!</p>

          '
        raw: "F:\\LLAMA\\>koboldcpp-1.20.exe --model ../llama.cpp/models/mpt-7b-storywriter_v0-q5_1.bin\
          \ --threads 8 --smartcontext --highpriority --usemlock  --useclblast 1 0\
          \ --stream --contextsize 2048\nWelcome to KoboldCpp - Version 1.20\nSetting\
          \ process to Higher Priority - Use Caution\nHigh Priority for Windows Set:\
          \ Priority.NORMAL_PRIORITY_CLASS to Priority.HIGH_PRIORITY_CLASS\nAttempting\
          \ to use CLBlast library for faster prompt ingestion. A compatible clblast\
          \ will be required.\nInitializing dynamic library: koboldcpp_clblast.dll\n\
          ==========\nLoading model: F:\\LLAMA\\llama.cpp\\models\\mpt-7b-storywriter_v0-q5_1.bin\n\
          [Threads: 8, BlasThreads: 8, SmartContext: True]\n\n---\nIdentified as GPT-NEO-X\
          \ model: (ver 401)\nAttempting to Load...\n---\nSystem Info: AVX = 1 | AVX2\
          \ = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON\
          \ = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1\
          \ | SSE3 = 1 | VSX = 0 |\nstablelm_model_load: loading model from 'F:\\\
          LLAMA\\llama.cpp\\models\\mpt-7b-storywriter_v0-q5_1.bin' - please wait\
          \ ...\nstablelm_model_load: n_vocab = 4096\nstablelm_model_load: n_ctx \
          \  = 65536\nstablelm_model_load: n_embd  = 32\nstablelm_model_load: n_head\
          \  = 32\nstablelm_model_load: n_layer = 50432\nstablelm_model_load: n_rot\
          \   = 9\nstablelm_model_load: ftype   = 13\nTraceback (most recent call\
          \ last):\n  File \"koboldcpp.py\", line 643, in <module>\n  File \"koboldcpp.py\"\
          , line 574, in main\n  File \"koboldcpp.py\", line 157, in load_model\n\
          OSError: exception: access violation reading 0x0000000E961F0000\n[54088]\
          \ Failed to execute script 'koboldcpp' due to unhandled exception!"
        updatedAt: '2023-05-12T19:58:14.408Z'
      numEdits: 1
      reactions: []
    id: 645e8203a3c5cd8a16ef0489
    type: comment
  author: mirek190
  content: "F:\\LLAMA\\>koboldcpp-1.20.exe --model ../llama.cpp/models/mpt-7b-storywriter_v0-q5_1.bin\
    \ --threads 8 --smartcontext --highpriority --usemlock  --useclblast 1 0 --stream\
    \ --contextsize 2048\nWelcome to KoboldCpp - Version 1.20\nSetting process to\
    \ Higher Priority - Use Caution\nHigh Priority for Windows Set: Priority.NORMAL_PRIORITY_CLASS\
    \ to Priority.HIGH_PRIORITY_CLASS\nAttempting to use CLBlast library for faster\
    \ prompt ingestion. A compatible clblast will be required.\nInitializing dynamic\
    \ library: koboldcpp_clblast.dll\n==========\nLoading model: F:\\LLAMA\\llama.cpp\\\
    models\\mpt-7b-storywriter_v0-q5_1.bin\n[Threads: 8, BlasThreads: 8, SmartContext:\
    \ True]\n\n---\nIdentified as GPT-NEO-X model: (ver 401)\nAttempting to Load...\n\
    ---\nSystem Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
    \ = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
    \ = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\nstablelm_model_load: loading model from\
    \ 'F:\\LLAMA\\llama.cpp\\models\\mpt-7b-storywriter_v0-q5_1.bin' - please wait\
    \ ...\nstablelm_model_load: n_vocab = 4096\nstablelm_model_load: n_ctx   = 65536\n\
    stablelm_model_load: n_embd  = 32\nstablelm_model_load: n_head  = 32\nstablelm_model_load:\
    \ n_layer = 50432\nstablelm_model_load: n_rot   = 9\nstablelm_model_load: ftype\
    \   = 13\nTraceback (most recent call last):\n  File \"koboldcpp.py\", line 643,\
    \ in <module>\n  File \"koboldcpp.py\", line 574, in main\n  File \"koboldcpp.py\"\
    , line 157, in load_model\nOSError: exception: access violation reading 0x0000000E961F0000\n\
    [54088] Failed to execute script 'koboldcpp' due to unhandled exception!"
  created_at: 2023-05-12 17:14:27+00:00
  edited: true
  hidden: false
  id: 645e8203a3c5cd8a16ef0489
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/463e621d6252efcf7a38221522b4e6aa.svg
      fullname: Bharat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Bharatvid
      type: user
    createdAt: '2023-05-12T18:53:51.000Z'
    data:
      edited: false
      editors:
      - Bharatvid
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/463e621d6252efcf7a38221522b4e6aa.svg
          fullname: Bharat
          isHf: false
          isPro: false
          name: Bharatvid
          type: user
        html: '<p>Read Model card</p>

          '
        raw: Read Model card
        updatedAt: '2023-05-12T18:53:51.467Z'
      numEdits: 0
      reactions: []
    id: 645e8b3fbafcebea7d9d2c48
    type: comment
  author: Bharatvid
  content: Read Model card
  created_at: 2023-05-12 17:53:51+00:00
  edited: false
  hidden: false
  id: 645e8b3fbafcebea7d9d2c48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
      fullname: Erik Scholz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Green-Sky
      type: user
    createdAt: '2023-05-12T19:05:05.000Z'
    data:
      edited: false
      editors:
      - Green-Sky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
          fullname: Erik Scholz
          isHf: false
          isPro: false
          name: Green-Sky
          type: user
        html: '<ol>

          <li>even with the pr, it does not yet work</li>

          <li>are you sure you are in the right repo? the error looks like you try
          to load a different model :)</li>

          </ol>

          '
        raw: '1. even with the pr, it does not yet work

          2. are you sure you are in the right repo? the error looks like you try
          to load a different model :)'
        updatedAt: '2023-05-12T19:05:05.242Z'
      numEdits: 0
      reactions: []
    id: 645e8de16d343f4bb613a19f
    type: comment
  author: Green-Sky
  content: '1. even with the pr, it does not yet work

    2. are you sure you are in the right repo? the error looks like you try to load
    a different model :)'
  created_at: 2023-05-12 18:05:05+00:00
  edited: false
  hidden: false
  id: 645e8de16d343f4bb613a19f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-12T19:58:38.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Sorry ...wrong log    .. FIXED   :P<br>Just changed name a bit.</p>

          '
        raw: 'Sorry ...wrong log    .. FIXED   :P

          Just changed name a bit.'
        updatedAt: '2023-05-12T19:59:44.785Z'
      numEdits: 1
      reactions: []
    id: 645e9a6e5f8408a151ff5ce1
    type: comment
  author: mirek190
  content: 'Sorry ...wrong log    .. FIXED   :P

    Just changed name a bit.'
  created_at: 2023-05-12 18:58:38+00:00
  edited: true
  hidden: false
  id: 645e9a6e5f8408a151ff5ce1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-05-15T07:19:03.000Z'
    data:
      edited: false
      editors:
      - FenixInDarkSolo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
          fullname: fenixlam
          isHf: false
          isPro: false
          name: FenixInDarkSolo
          type: user
        html: "<p>I am sorry. I still didn't understand how to run this... orz<br>I\
          \ try to copy mirek190's calling.</p>\n<pre><code>koboldcpp_121.exe  --model\
          \ ./mpt-7b-storywriter-ggml_v0-q5_1.bin --threads 12 --smartcontext --highpriority\
          \ --usemlock --useclblast 1 0 --stream --contextsize 2048\nWelcome to KoboldCpp\
          \ - Version 1.21.1\nSetting process to Higher Priority - Use Caution\nHigh\
          \ Priority for Windows Set: Priority.NORMAL_PRIORITY_CLASS to Priority.HIGH_PRIORITY_CLASS\n\
          Attempting to use CLBlast library for faster prompt ingestion. A compatible\
          \ clblast will be required.\nInitializing dynamic library: koboldcpp_clblast.dll\n\
          ==========\nLoading model: D:\\program\\koboldcpp\\mpt-7b-storywriter-ggml_v0-q5_1.bin\n\
          [Threads: 12, BlasThreads: 12, SmartContext: True]\n\n---\nIdentified as\
          \ GPT-NEO-X model: (ver 401)\nAttempting to Load...\n---\nSystem Info: AVX\
          \ = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA\
          \ = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0\
          \ | BLAS = 1 | SSE3 = 1 | VSX = 0 |\nstablelm_model_load: loading model\
          \ from 'D:\\program\\koboldcpp\\mpt-7b-storywriter-ggml_v0-q5_1.bin' - please\
          \ wait ...\nstablelm_model_load: n_vocab = 4096\nstablelm_model_load: n_ctx\
          \   = 65536\nstablelm_model_load: n_embd  = 32\nstablelm_model_load: n_head\
          \  = 32\nstablelm_model_load: n_layer = 50432\nstablelm_model_load: n_rot\
          \   = 1098907648\nstablelm_model_load: ftype   = 1086324736\nTraceback (most\
          \ recent call last):\n  File \"koboldcpp.py\", line 645, in &lt;module&gt;\n\
          \  File \"koboldcpp.py\", line 576, in main\n  File \"koboldcpp.py\", line\
          \ 159, in load_model\nOSError: [WinError -1073741569] Windows Error 0xc00000ff\n\
          [97616] Failed to execute script 'koboldcpp' due to unhandled exception!\n\
          </code></pre>\n"
        raw: "I am sorry. I still didn't understand how to run this... orz\nI try\
          \ to copy mirek190's calling.\n```\nkoboldcpp_121.exe  --model ./mpt-7b-storywriter-ggml_v0-q5_1.bin\
          \ --threads 12 --smartcontext --highpriority --usemlock --useclblast 1 0\
          \ --stream --contextsize 2048\nWelcome to KoboldCpp - Version 1.21.1\nSetting\
          \ process to Higher Priority - Use Caution\nHigh Priority for Windows Set:\
          \ Priority.NORMAL_PRIORITY_CLASS to Priority.HIGH_PRIORITY_CLASS\nAttempting\
          \ to use CLBlast library for faster prompt ingestion. A compatible clblast\
          \ will be required.\nInitializing dynamic library: koboldcpp_clblast.dll\n\
          ==========\nLoading model: D:\\program\\koboldcpp\\mpt-7b-storywriter-ggml_v0-q5_1.bin\n\
          [Threads: 12, BlasThreads: 12, SmartContext: True]\n\n---\nIdentified as\
          \ GPT-NEO-X model: (ver 401)\nAttempting to Load...\n---\nSystem Info: AVX\
          \ = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA\
          \ = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0\
          \ | BLAS = 1 | SSE3 = 1 | VSX = 0 |\nstablelm_model_load: loading model\
          \ from 'D:\\program\\koboldcpp\\mpt-7b-storywriter-ggml_v0-q5_1.bin' - please\
          \ wait ...\nstablelm_model_load: n_vocab = 4096\nstablelm_model_load: n_ctx\
          \   = 65536\nstablelm_model_load: n_embd  = 32\nstablelm_model_load: n_head\
          \  = 32\nstablelm_model_load: n_layer = 50432\nstablelm_model_load: n_rot\
          \   = 1098907648\nstablelm_model_load: ftype   = 1086324736\nTraceback (most\
          \ recent call last):\n  File \"koboldcpp.py\", line 645, in <module>\n \
          \ File \"koboldcpp.py\", line 576, in main\n  File \"koboldcpp.py\", line\
          \ 159, in load_model\nOSError: [WinError -1073741569] Windows Error 0xc00000ff\n\
          [97616] Failed to execute script 'koboldcpp' due to unhandled exception!\n\
          ```"
        updatedAt: '2023-05-15T07:19:03.594Z'
      numEdits: 0
      reactions: []
    id: 6461dce776f92cf538f9c381
    type: comment
  author: FenixInDarkSolo
  content: "I am sorry. I still didn't understand how to run this... orz\nI try to\
    \ copy mirek190's calling.\n```\nkoboldcpp_121.exe  --model ./mpt-7b-storywriter-ggml_v0-q5_1.bin\
    \ --threads 12 --smartcontext --highpriority --usemlock --useclblast 1 0 --stream\
    \ --contextsize 2048\nWelcome to KoboldCpp - Version 1.21.1\nSetting process to\
    \ Higher Priority - Use Caution\nHigh Priority for Windows Set: Priority.NORMAL_PRIORITY_CLASS\
    \ to Priority.HIGH_PRIORITY_CLASS\nAttempting to use CLBlast library for faster\
    \ prompt ingestion. A compatible clblast will be required.\nInitializing dynamic\
    \ library: koboldcpp_clblast.dll\n==========\nLoading model: D:\\program\\koboldcpp\\\
    mpt-7b-storywriter-ggml_v0-q5_1.bin\n[Threads: 12, BlasThreads: 12, SmartContext:\
    \ True]\n\n---\nIdentified as GPT-NEO-X model: (ver 401)\nAttempting to Load...\n\
    ---\nSystem Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
    \ = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
    \ = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\nstablelm_model_load: loading model from\
    \ 'D:\\program\\koboldcpp\\mpt-7b-storywriter-ggml_v0-q5_1.bin' - please wait\
    \ ...\nstablelm_model_load: n_vocab = 4096\nstablelm_model_load: n_ctx   = 65536\n\
    stablelm_model_load: n_embd  = 32\nstablelm_model_load: n_head  = 32\nstablelm_model_load:\
    \ n_layer = 50432\nstablelm_model_load: n_rot   = 1098907648\nstablelm_model_load:\
    \ ftype   = 1086324736\nTraceback (most recent call last):\n  File \"koboldcpp.py\"\
    , line 645, in <module>\n  File \"koboldcpp.py\", line 576, in main\n  File \"\
    koboldcpp.py\", line 159, in load_model\nOSError: [WinError -1073741569] Windows\
    \ Error 0xc00000ff\n[97616] Failed to execute script 'koboldcpp' due to unhandled\
    \ exception!\n```"
  created_at: 2023-05-15 06:19:03+00:00
  edited: false
  hidden: false
  id: 6461dce776f92cf538f9c381
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-15T07:40:57.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: "<p>For me doesn't work either.<br>Previously I just  used wrong log\
          \  so I fixed my mistake \U0001F605.</p>\n"
        raw: "For me doesn't work either.\nPreviously I just  used wrong log  so I\
          \ fixed my mistake \U0001F605."
        updatedAt: '2023-05-15T07:40:57.678Z'
      numEdits: 0
      reactions: []
    id: 6461e2093026e7f163c6f382
    type: comment
  author: mirek190
  content: "For me doesn't work either.\nPreviously I just  used wrong log  so I fixed\
    \ my mistake \U0001F605."
  created_at: 2023-05-15 06:40:57+00:00
  edited: false
  hidden: false
  id: 6461e2093026e7f163c6f382
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-05-15T09:23:31.000Z'
    data:
      edited: false
      editors:
      - FenixInDarkSolo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
          fullname: fenixlam
          isHf: false
          isPro: false
          name: FenixInDarkSolo
          type: user
        html: '<p>And it didn''t work on llama.cpp. I wonder if I have not enough
          memory to run it? or the parameter is not correct...<br>In koboldcpp, it
          will use about 90GB ram to crash the system (My iGPU can use shared memory
          from ram and virtual memory)<br>Here is the llama.cpp.</p>

          <pre><code>main -m ./models/mpt-7b-storywriter-ggml_v0-q4_0.bin -t 12 -n
          -1 -c 2048 --keep -1 --repeat_last_n 2048 --top_k 160 --top_p 0.95 --color
          -ins -r "User:" --keep -1 --interactive-first

          main: build = 536 (cdd5350)

          main: seed  = 1684142257

          llama.cpp: loading model from ./models/mpt-7b-storywriter-ggml_v0-q4_0.bin


          D:\program\llama.cpp&gt;git rev-parse HEAD

          cdd5350892b1d4e521e930c77341f858fcfcd433

          </code></pre>

          <p>And it will crash without error report...</p>

          '
        raw: 'And it didn''t work on llama.cpp. I wonder if I have not enough memory
          to run it? or the parameter is not correct...

          In koboldcpp, it will use about 90GB ram to crash the system (My iGPU can
          use shared memory from ram and virtual memory)

          Here is the llama.cpp.

          ```

          main -m ./models/mpt-7b-storywriter-ggml_v0-q4_0.bin -t 12 -n -1 -c 2048
          --keep -1 --repeat_last_n 2048 --top_k 160 --top_p 0.95 --color -ins -r
          "User:" --keep -1 --interactive-first

          main: build = 536 (cdd5350)

          main: seed  = 1684142257

          llama.cpp: loading model from ./models/mpt-7b-storywriter-ggml_v0-q4_0.bin


          D:\program\llama.cpp>git rev-parse HEAD

          cdd5350892b1d4e521e930c77341f858fcfcd433

          ```

          And it will crash without error report...'
        updatedAt: '2023-05-15T09:23:31.839Z'
      numEdits: 0
      reactions: []
    id: 6461fa13e7de30225dab190d
    type: comment
  author: FenixInDarkSolo
  content: 'And it didn''t work on llama.cpp. I wonder if I have not enough memory
    to run it? or the parameter is not correct...

    In koboldcpp, it will use about 90GB ram to crash the system (My iGPU can use
    shared memory from ram and virtual memory)

    Here is the llama.cpp.

    ```

    main -m ./models/mpt-7b-storywriter-ggml_v0-q4_0.bin -t 12 -n -1 -c 2048 --keep
    -1 --repeat_last_n 2048 --top_k 160 --top_p 0.95 --color -ins -r "User:" --keep
    -1 --interactive-first

    main: build = 536 (cdd5350)

    main: seed  = 1684142257

    llama.cpp: loading model from ./models/mpt-7b-storywriter-ggml_v0-q4_0.bin


    D:\program\llama.cpp>git rev-parse HEAD

    cdd5350892b1d4e521e930c77341f858fcfcd433

    ```

    And it will crash without error report...'
  created_at: 2023-05-15 08:23:31+00:00
  edited: false
  hidden: false
  id: 6461fa13e7de30225dab190d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
      fullname: Erik Scholz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Green-Sky
      type: user
    createdAt: '2023-05-15T10:51:30.000Z'
    data:
      edited: false
      editors:
      - Green-Sky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
          fullname: Erik Scholz
          isHf: false
          isPro: false
          name: Green-Sky
          type: user
        html: '<p>As stated in the readme, it require this pr <a rel="nofollow" href="https://github.com/ggerganov/ggml/pull/145">https://github.com/ggerganov/ggml/pull/145</a>
          and the mpt binary produced by the ggml repo. I can add a CI build to the
          ggml repo later when there is demand.<br>Otherwise talk to the kobolddevs
          about adding mpt support and reference the pr :)</p>

          '
        raw: 'As stated in the readme, it require this pr https://github.com/ggerganov/ggml/pull/145
          and the mpt binary produced by the ggml repo. I can add a CI build to the
          ggml repo later when there is demand.

          Otherwise talk to the kobolddevs about adding mpt support and reference
          the pr :)'
        updatedAt: '2023-05-15T10:51:30.754Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - mirek190
        - FenixInDarkSolo
    id: 64620eb2c7a7faf84d0059b5
    type: comment
  author: Green-Sky
  content: 'As stated in the readme, it require this pr https://github.com/ggerganov/ggml/pull/145
    and the mpt binary produced by the ggml repo. I can add a CI build to the ggml
    repo later when there is demand.

    Otherwise talk to the kobolddevs about adding mpt support and reference the pr
    :)'
  created_at: 2023-05-15 09:51:30+00:00
  edited: false
  hidden: false
  id: 64620eb2c7a7faf84d0059b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
      fullname: Erik Scholz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Green-Sky
      type: user
    createdAt: '2023-05-15T10:52:43.000Z'
    data:
      edited: false
      editors:
      - Green-Sky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
          fullname: Erik Scholz
          isHf: false
          isPro: false
          name: Green-Sky
          type: user
        html: '<p>Also it is still work in progress. I don''t think the fileformat
          will change soon, but it did change 2 times yesterday, FYI.</p>

          '
        raw: Also it is still work in progress. I don't think the fileformat will
          change soon, but it did change 2 times yesterday, FYI.
        updatedAt: '2023-05-15T10:52:43.187Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - mirek190
    id: 64620efb4d31d0c83f4a9a8b
    type: comment
  author: Green-Sky
  content: Also it is still work in progress. I don't think the fileformat will change
    soon, but it did change 2 times yesterday, FYI.
  created_at: 2023-05-15 09:52:43+00:00
  edited: false
  hidden: false
  id: 64620efb4d31d0c83f4a9a8b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Green-Sky/ggml-mpt-7b-storywriter
repo_type: model
status: open
target_branch: null
title: Hello - how to run it?
