!!python/object:huggingface_hub.community.DiscussionWithDetails
author: raquelhortab
conflicting_files: null
created_at: 2022-06-23 15:44:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcc23e3f06e3cc53dea79388ee96e836.svg
      fullname: Raquel Horta Bartomeu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: raquelhortab
      type: user
    createdAt: '2022-06-23T16:44:46.000Z'
    data:
      edited: false
      editors:
      - raquelhortab
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcc23e3f06e3cc53dea79388ee96e836.svg
          fullname: Raquel Horta Bartomeu
          isHf: false
          isPro: false
          name: raquelhortab
          type: user
        html: '<p>Hi, [beware I am new to using transformers]<br>I am learning to
          use transformers and I am dealing with a text classification task. I have
          been suggested to only use the [CLS] token vector (I understand it represents
          the whole sentence) since it is quicker than using the whole 768*num_tokens
          vectors.  I was wondering id this models does that.</p>

          <p>Also, if you don''t mind my "newbieness", if I directly evaluate the
          model (<code>model.evaluate(tokenized_data)</code>) I get around a 80% accuracy
          but if I try to fine-tune it using the  <code>fit</code> function it goes
          down to 50%. I created a detailed question on StackOverflow in case someone
          here can help: <a rel="nofollow" href="https://stackoverflow.com/questions/72704214/transformers-pretrained-models-accuracy-decreases-after-fine-tuning">https://stackoverflow.com/questions/72704214/transformers-pretrained-models-accuracy-decreases-after-fine-tuning</a></p>

          '
        raw: "Hi, [beware I am new to using transformers]\r\nI am learning to use\
          \ transformers and I am dealing with a text classification task. I have\
          \ been suggested to only use the [CLS] token vector (I understand it represents\
          \ the whole sentence) since it is quicker than using the whole 768*num_tokens\
          \ vectors.  I was wondering id this models does that.\r\n\r\nAlso, if you\
          \ don't mind my \"newbieness\", if I directly evaluate the model (`model.evaluate(tokenized_data)`)\
          \ I get around a 80% accuracy but if I try to fine-tune it using the  `fit`\
          \ function it goes down to 50%. I created a detailed question on StackOverflow\
          \ in case someone here can help: https://stackoverflow.com/questions/72704214/transformers-pretrained-models-accuracy-decreases-after-fine-tuning"
        updatedAt: '2022-06-23T16:44:46.320Z'
      numEdits: 0
      reactions: []
    id: 62b4987efb40ed0050e954cf
    type: comment
  author: raquelhortab
  content: "Hi, [beware I am new to using transformers]\r\nI am learning to use transformers\
    \ and I am dealing with a text classification task. I have been suggested to only\
    \ use the [CLS] token vector (I understand it represents the whole sentence) since\
    \ it is quicker than using the whole 768*num_tokens vectors.  I was wondering\
    \ id this models does that.\r\n\r\nAlso, if you don't mind my \"newbieness\",\
    \ if I directly evaluate the model (`model.evaluate(tokenized_data)`) I get around\
    \ a 80% accuracy but if I try to fine-tune it using the  `fit` function it goes\
    \ down to 50%. I created a detailed question on StackOverflow in case someone\
    \ here can help: https://stackoverflow.com/questions/72704214/transformers-pretrained-models-accuracy-decreases-after-fine-tuning"
  created_at: 2022-06-23 15:44:46+00:00
  edited: false
  hidden: false
  id: 62b4987efb40ed0050e954cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2022-06-25T17:10:29.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;raquelhortab&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/raquelhortab\"\
          >@<span class=\"underline\">raquelhortab</span></a></span>\n\n\t</span></span>\
          \ ,</p>\n<p>Good question! This model has indeed been fine-tuned using the\
          \ CLS token which you can see here: <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/cc5c061e346365252458126abb699b87cda5dcc0/src/transformers/models/distilbert/modeling_distilbert.py#L759\"\
          >https://github.com/huggingface/transformers/blob/cc5c061e346365252458126abb699b87cda5dcc0/src/transformers/models/distilbert/modeling_distilbert.py#L759</a></p>\n\
          <p>It only uses the first token - the CLS token as the \"pooled output\"\
          .</p>\n<p>Regarding your second question, I don't really know - I'd recommend\
          \ using the forum to get an answer for this :-) <a rel=\"nofollow\" href=\"\
          https://discuss.huggingface.co/\">https://discuss.huggingface.co/</a></p>\n"
        raw: 'Hey @raquelhortab ,


          Good question! This model has indeed been fine-tuned using the CLS token
          which you can see here: https://github.com/huggingface/transformers/blob/cc5c061e346365252458126abb699b87cda5dcc0/src/transformers/models/distilbert/modeling_distilbert.py#L759


          It only uses the first token - the CLS token as the "pooled output".


          Regarding your second question, I don''t really know - I''d recommend using
          the forum to get an answer for this :-) https://discuss.huggingface.co/'
        updatedAt: '2022-06-25T17:10:29.335Z'
      numEdits: 0
      reactions: []
    id: 62b74185aa928870cedc14f1
    type: comment
  author: patrickvonplaten
  content: 'Hey @raquelhortab ,


    Good question! This model has indeed been fine-tuned using the CLS token which
    you can see here: https://github.com/huggingface/transformers/blob/cc5c061e346365252458126abb699b87cda5dcc0/src/transformers/models/distilbert/modeling_distilbert.py#L759


    It only uses the first token - the CLS token as the "pooled output".


    Regarding your second question, I don''t really know - I''d recommend using the
    forum to get an answer for this :-) https://discuss.huggingface.co/'
  created_at: 2022-06-25 16:10:29+00:00
  edited: false
  hidden: false
  id: 62b74185aa928870cedc14f1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: distilbert-base-uncased-finetuned-sst-2-english
repo_type: model
status: open
target_branch: null
title: Does this model use all the 768*num_tokens vectors or just the [CLS] one?
