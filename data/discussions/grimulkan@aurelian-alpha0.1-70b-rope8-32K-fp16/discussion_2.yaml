!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nexesenex
conflicting_files: null
created_at: 2024-01-15 02:17:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-15T02:17:35.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9389958381652832
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Your model seems promising, and would deserve such quants for the
          less fortunate to test it at long context.</p>

          <p>From my ongoing tests, the IQ2_XS quant allows a PPL of around 4.5-4.6
          on finetuned models at 512ctx (probably around 4 at 4096), a bit more maybe
          with linear rope but that''s still fine. And it would allow to run it at
          6-8k context wit KV cache 16 bits, and even 8-12k context with K cache 8bits
          on 24GB of VRAM.</p>

          <p>A Q2_K (new version, smaller and made also with the help of an importance
          matrix) lowers the ppl by 0.3-0.4 and would allow to close on 32k context
          with K cache 8 bits on 36GB of VRAM (like in my case!).</p>

          <p>If you or someone else can spare the compute to provide these quants
          (I have only a i7-6700k..), that''d be great!</p>

          '
        raw: 'Your model seems promising, and would deserve such quants for the less
          fortunate to test it at long context.


          From my ongoing tests, the IQ2_XS quant allows a PPL of around 4.5-4.6 on
          finetuned models at 512ctx (probably around 4 at 4096), a bit more maybe
          with linear rope but that''s still fine. And it would allow to run it at
          6-8k context wit KV cache 16 bits, and even 8-12k context with K cache 8bits
          on 24GB of VRAM.


          A Q2_K (new version, smaller and made also with the help of an importance
          matrix) lowers the ppl by 0.3-0.4 and would allow to close on 32k context
          with K cache 8 bits on 36GB of VRAM (like in my case!).


          If you or someone else can spare the compute to provide these quants (I
          have only a i7-6700k..), that''d be great!'
        updatedAt: '2024-01-15T02:43:45.726Z'
      numEdits: 4
      reactions: []
    id: 65a495bf8df9302d1561c6ef
    type: comment
  author: Nexesenex
  content: 'Your model seems promising, and would deserve such quants for the less
    fortunate to test it at long context.


    From my ongoing tests, the IQ2_XS quant allows a PPL of around 4.5-4.6 on finetuned
    models at 512ctx (probably around 4 at 4096), a bit more maybe with linear rope
    but that''s still fine. And it would allow to run it at 6-8k context wit KV cache
    16 bits, and even 8-12k context with K cache 8bits on 24GB of VRAM.


    A Q2_K (new version, smaller and made also with the help of an importance matrix)
    lowers the ppl by 0.3-0.4 and would allow to close on 32k context with K cache
    8 bits on 36GB of VRAM (like in my case!).


    If you or someone else can spare the compute to provide these quants (I have only
    a i7-6700k..), that''d be great!'
  created_at: 2024-01-15 02:17:35+00:00
  edited: true
  hidden: false
  id: 65a495bf8df9302d1561c6ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-15T06:37:00.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9793350100517273
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>I can add it. Is it just Q2_K, or any other settings? I don''t actually
          test the GGUFs, so I''d rely on people like you too see if they actually
          work well.</p>

          '
        raw: I can add it. Is it just Q2_K, or any other settings? I don't actually
          test the GGUFs, so I'd rely on people like you too see if they actually
          work well.
        updatedAt: '2024-01-15T06:37:00.681Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65a4d28ca1ad28350946c5ea
    type: comment
  author: grimulkan
  content: I can add it. Is it just Q2_K, or any other settings? I don't actually
    test the GGUFs, so I'd rely on people like you too see if they actually work well.
  created_at: 2024-01-15 06:37:00+00:00
  edited: false
  hidden: false
  id: 65a4d28ca1ad28350946c5ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-15T06:38:18.000Z'
    data:
      edited: true
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6518319845199585
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>BTW GGUFs for the next iteration (v0.5) are here: <a href="https://huggingface.co/grimulkan/aurelian-v0.5-70b-rope8-32K_GGUF">https://huggingface.co/grimulkan/aurelian-v0.5-70b-rope8-32K_GGUF</a></p>

          <p>I will add Q2_K to that repo.</p>

          <p>EDIT: Are you sure you want Q2_K and not Q5_K_S? Latter seems more efficient
          for the size, but I am not up-to-date on the GGUF quant methods. TheBloke
          seems to recommend that over Q2_K here: <a href="https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF">https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF</a></p>

          '
        raw: 'BTW GGUFs for the next iteration (v0.5) are here: https://huggingface.co/grimulkan/aurelian-v0.5-70b-rope8-32K_GGUF


          I will add Q2_K to that repo.


          EDIT: Are you sure you want Q2_K and not Q5_K_S? Latter seems more efficient
          for the size, but I am not up-to-date on the GGUF quant methods. TheBloke
          seems to recommend that over Q2_K here: https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF'
        updatedAt: '2024-01-15T06:42:58.895Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65a4d2da81a46e7dd937db43
    type: comment
  author: grimulkan
  content: 'BTW GGUFs for the next iteration (v0.5) are here: https://huggingface.co/grimulkan/aurelian-v0.5-70b-rope8-32K_GGUF


    I will add Q2_K to that repo.


    EDIT: Are you sure you want Q2_K and not Q5_K_S? Latter seems more efficient for
    the size, but I am not up-to-date on the GGUF quant methods. TheBloke seems to
    recommend that over Q2_K here: https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF'
  created_at: 2024-01-15 06:38:18+00:00
  edited: true
  hidden: false
  id: 65a4d2da81a46e7dd937db43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-15T10:20:15.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9099018573760986
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Hey!</p>

          <p>For those of us who run RTX 3090 or 4090, hence 24GB, the best quants
          available for a full offload are the GGUF 2 bits recently committed (SOTA
          2 bits) on LlamaCPP (always get the last version lol, because they are still
          fresh!). Exllama 2, epecially since v 0.0.11, was ahead for a while in terms
          of ratio quality/size on small quants (2 to 3.5 bits), but LlamaCPP just
          caught-up and presents 2 advantages :</p>

          <ul>

          <li>For now, it uses less compute power on the GPU, so it is quite slower
          than Exllama 2 but spares the hardware.</li>

          <li>But also, it allows to run quants on CPU/RAM only, on GPU/VRAM, or a
          mix of both to get the best of both worlds : the speed of the GPU and its
          VRAM, and the additional memory space granted by the computer DDR-RAM at
          the cost of some speed compared to full GPU offload.</li>

          </ul>

          <p>Exllama 2, on its side, is faster and have a full KV cache 8 bits (only
          K for LlamaCPP, and it''s even slower than KV16).</p>

          <p>Llama CPP GGUF quantizations IQ2_XXS and IQ2_XS allow to run model on
          the sole GPU with 4k context or more (IQ2_XS is a great compromise for perplexity,
          I tested Aurora Nights 70b in IQ2_XS and was impressed by its coherence
          for only 2.36bpw). Q2_K_S and Q2_K (the new version, not the old one which
          is a Q3_K_S light), which are bigger, allow to run with a few layers out
          of the GPU, and at a decent speed with a much better perplexity for the
          new Q2_K (Q2_K_S is less reliable from my initial tests) compared to the
          IQ2_XXS and IQ2_XS.</p>

          <p>All these 2 bits quants, especially the 3 smallest (IQ2_XXS, IQ2_XS,
          and Q2_K_S), require to prepare an importance matrix first, which is quite
          CPU hungry (1h for a Ryzen 5xxx 32 cores for a 70b model if I understood
          properly).</p>

          <p>TheBloke is of course correct about quants, but it''s also about what
          people can run effectively, beyond what they should run ideally : Q4_K_S
          and beyond are for folks with 2 big GPU (48GB of VRAM) or a high-end Mac
          with 400-800 GB/s of unified memory bandwith.</p>

          <p>For me, who has a 3090+3060 setup, I can do with Q3_K_S to have a decently
          sized context (10-12k maybe) offloaded on GPUs with a decent perplexity.
          Q3_K_M will grant me barely half of it, but is the best 70b quant I can
          use to start a task/story at 4096ctx before switching on a smaller quant
          for more context.<br>Or with Exllama 2 (0.0.11 or more recent), I can use
          3bpw for a 25-28k context, 3.25bpw for a 15-20k, 3.5bpw for a 10-12k, and
          3.75bpw for 4-6k.</p>

          <p>So, for me, Q3_K_M and even more importantly, Q3_K_S will be great if
          you''re short on compute, because they don''t require an importance matrix
          and are the smallest of the solid quants bringing an experience remotely
          comparable to fp16.</p>

          <p>For Q2_K (the new version, not the old one which is basically a Q3_K_S
          light), an importance matrix is quite preferable (minus 0.2-0.3 ppl, so
          it actually becomes worth it and reasonably holds its own against the bigger
          quants), but I believe not mandatory.<br>For Q2_K_S, IQ2_XS, and IQ2_XXS,
          it''s mandatory.</p>

          <p>Here''s some relevant read-ups, because these GGUF quants are worth looking
          at to diffuse more widely your hard work and get feedback :</p>

          <p><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/4773">https://github.com/ggerganov/llama.cpp/pull/4773</a><br><a
          rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/4897">https://github.com/ggerganov/llama.cpp/pull/4897</a><br><a
          rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/4861">https://github.com/ggerganov/llama.cpp/pull/4861</a><br><a
          rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/4930">https://github.com/ggerganov/llama.cpp/pull/4930</a><br><a
          rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/4957">https://github.com/ggerganov/llama.cpp/pull/4957</a></p>

          '
        raw: "Hey!\n\nFor those of us who run RTX 3090 or 4090, hence 24GB, the best\
          \ quants available for a full offload are the GGUF 2 bits recently committed\
          \ (SOTA 2 bits) on LlamaCPP (always get the last version lol, because they\
          \ are still fresh!). Exllama 2, epecially since v 0.0.11, was ahead for\
          \ a while in terms of ratio quality/size on small quants (2 to 3.5 bits),\
          \ but LlamaCPP just caught-up and presents 2 advantages :\n- For now, it\
          \ uses less compute power on the GPU, so it is quite slower than Exllama\
          \ 2 but spares the hardware.\n- But also, it allows to run quants on CPU/RAM\
          \ only, on GPU/VRAM, or a mix of both to get the best of both worlds : the\
          \ speed of the GPU and its VRAM, and the additional memory space granted\
          \ by the computer DDR-RAM at the cost of some speed compared to full GPU\
          \ offload.\n\nExllama 2, on its side, is faster and have a full KV cache\
          \ 8 bits (only K for LlamaCPP, and it's even slower than KV16).\n\nLlama\
          \ CPP GGUF quantizations IQ2_XXS and IQ2_XS allow to run model on the sole\
          \ GPU with 4k context or more (IQ2_XS is a great compromise for perplexity,\
          \ I tested Aurora Nights 70b in IQ2_XS and was impressed by its coherence\
          \ for only 2.36bpw). Q2_K_S and Q2_K (the new version, not the old one which\
          \ is a Q3_K_S light), which are bigger, allow to run with a few layers out\
          \ of the GPU, and at a decent speed with a much better perplexity for the\
          \ new Q2_K (Q2_K_S is less reliable from my initial tests) compared to the\
          \ IQ2_XXS and IQ2_XS.\n\nAll these 2 bits quants, especially the 3 smallest\
          \ (IQ2_XXS, IQ2_XS, and Q2_K_S), require to prepare an importance matrix\
          \ first, which is quite CPU hungry (1h for a Ryzen 5xxx 32 cores for a 70b\
          \ model if I understood properly).\n\nTheBloke is of course correct about\
          \ quants, but it's also about what people can run effectively, beyond what\
          \ they should run ideally : Q4_K_S and beyond are for folks with 2 big GPU\
          \ (48GB of VRAM) or a high-end Mac with 400-800 GB/s of unified memory bandwith.\n\
          \nFor me, who has a 3090+3060 setup, I can do with Q3_K_S to have a decently\
          \ sized context (10-12k maybe) offloaded on GPUs with a decent perplexity.\
          \ Q3_K_M will grant me barely half of it, but is the best 70b quant I can\
          \ use to start a task/story at 4096ctx before switching on a smaller quant\
          \ for more context. \nOr with Exllama 2 (0.0.11 or more recent), I can use\
          \ 3bpw for a 25-28k context, 3.25bpw for a 15-20k, 3.5bpw for a 10-12k,\
          \ and 3.75bpw for 4-6k.\n\nSo, for me, Q3_K_M and even more importantly,\
          \ Q3_K_S will be great if you're short on compute, because they don't require\
          \ an importance matrix and are the smallest of the solid quants bringing\
          \ an experience remotely comparable to fp16.\n\nFor Q2_K (the new version,\
          \ not the old one which is basically a Q3_K_S light), an importance matrix\
          \ is quite preferable (minus 0.2-0.3 ppl, so it actually becomes worth it\
          \ and reasonably holds its own against the bigger quants), but I believe\
          \ not mandatory.\nFor Q2_K_S, IQ2_XS, and IQ2_XXS, it's mandatory.\n\nHere's\
          \ some relevant read-ups, because these GGUF quants are worth looking at\
          \ to diffuse more widely your hard work and get feedback :\n\nhttps://github.com/ggerganov/llama.cpp/pull/4773\n\
          https://github.com/ggerganov/llama.cpp/pull/4897\nhttps://github.com/ggerganov/llama.cpp/pull/4861\n\
          https://github.com/ggerganov/llama.cpp/pull/4930\nhttps://github.com/ggerganov/llama.cpp/pull/4957"
        updatedAt: '2024-01-16T00:27:45.077Z'
      numEdits: 8
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - grimulkan
    id: 65a506df3e3a00a9c140ab00
    type: comment
  author: Nexesenex
  content: "Hey!\n\nFor those of us who run RTX 3090 or 4090, hence 24GB, the best\
    \ quants available for a full offload are the GGUF 2 bits recently committed (SOTA\
    \ 2 bits) on LlamaCPP (always get the last version lol, because they are still\
    \ fresh!). Exllama 2, epecially since v 0.0.11, was ahead for a while in terms\
    \ of ratio quality/size on small quants (2 to 3.5 bits), but LlamaCPP just caught-up\
    \ and presents 2 advantages :\n- For now, it uses less compute power on the GPU,\
    \ so it is quite slower than Exllama 2 but spares the hardware.\n- But also, it\
    \ allows to run quants on CPU/RAM only, on GPU/VRAM, or a mix of both to get the\
    \ best of both worlds : the speed of the GPU and its VRAM, and the additional\
    \ memory space granted by the computer DDR-RAM at the cost of some speed compared\
    \ to full GPU offload.\n\nExllama 2, on its side, is faster and have a full KV\
    \ cache 8 bits (only K for LlamaCPP, and it's even slower than KV16).\n\nLlama\
    \ CPP GGUF quantizations IQ2_XXS and IQ2_XS allow to run model on the sole GPU\
    \ with 4k context or more (IQ2_XS is a great compromise for perplexity, I tested\
    \ Aurora Nights 70b in IQ2_XS and was impressed by its coherence for only 2.36bpw).\
    \ Q2_K_S and Q2_K (the new version, not the old one which is a Q3_K_S light),\
    \ which are bigger, allow to run with a few layers out of the GPU, and at a decent\
    \ speed with a much better perplexity for the new Q2_K (Q2_K_S is less reliable\
    \ from my initial tests) compared to the IQ2_XXS and IQ2_XS.\n\nAll these 2 bits\
    \ quants, especially the 3 smallest (IQ2_XXS, IQ2_XS, and Q2_K_S), require to\
    \ prepare an importance matrix first, which is quite CPU hungry (1h for a Ryzen\
    \ 5xxx 32 cores for a 70b model if I understood properly).\n\nTheBloke is of course\
    \ correct about quants, but it's also about what people can run effectively, beyond\
    \ what they should run ideally : Q4_K_S and beyond are for folks with 2 big GPU\
    \ (48GB of VRAM) or a high-end Mac with 400-800 GB/s of unified memory bandwith.\n\
    \nFor me, who has a 3090+3060 setup, I can do with Q3_K_S to have a decently sized\
    \ context (10-12k maybe) offloaded on GPUs with a decent perplexity. Q3_K_M will\
    \ grant me barely half of it, but is the best 70b quant I can use to start a task/story\
    \ at 4096ctx before switching on a smaller quant for more context. \nOr with Exllama\
    \ 2 (0.0.11 or more recent), I can use 3bpw for a 25-28k context, 3.25bpw for\
    \ a 15-20k, 3.5bpw for a 10-12k, and 3.75bpw for 4-6k.\n\nSo, for me, Q3_K_M and\
    \ even more importantly, Q3_K_S will be great if you're short on compute, because\
    \ they don't require an importance matrix and are the smallest of the solid quants\
    \ bringing an experience remotely comparable to fp16.\n\nFor Q2_K (the new version,\
    \ not the old one which is basically a Q3_K_S light), an importance matrix is\
    \ quite preferable (minus 0.2-0.3 ppl, so it actually becomes worth it and reasonably\
    \ holds its own against the bigger quants), but I believe not mandatory.\nFor\
    \ Q2_K_S, IQ2_XS, and IQ2_XXS, it's mandatory.\n\nHere's some relevant read-ups,\
    \ because these GGUF quants are worth looking at to diffuse more widely your hard\
    \ work and get feedback :\n\nhttps://github.com/ggerganov/llama.cpp/pull/4773\n\
    https://github.com/ggerganov/llama.cpp/pull/4897\nhttps://github.com/ggerganov/llama.cpp/pull/4861\n\
    https://github.com/ggerganov/llama.cpp/pull/4930\nhttps://github.com/ggerganov/llama.cpp/pull/4957"
  created_at: 2024-01-15 10:20:15+00:00
  edited: true
  hidden: false
  id: 65a506df3e3a00a9c140ab00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-16T00:17:11.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9664619565010071
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Much appreciate the input.</p>

          <p>Quantizing Q3_K_M, Q3_K_S and Q2_K (with imatrix) now.</p>

          <p>By the way, my comment about TheBloke''s recommendation of Q5_K_S may
          seem strange, but it is actually comparable in size to Q2_K, though that
          may not be SOTA. I was not referring to the larger quant sizes (the 5 bits
          here is a bit misleading, it is meant to be compared to Q2_K).</p>

          '
        raw: 'Much appreciate the input.


          Quantizing Q3_K_M, Q3_K_S and Q2_K (with imatrix) now.


          By the way, my comment about TheBloke''s recommendation of Q5_K_S may seem
          strange, but it is actually comparable in size to Q2_K, though that may
          not be SOTA. I was not referring to the larger quant sizes (the 5 bits here
          is a bit misleading, it is meant to be compared to Q2_K).'
        updatedAt: '2024-01-16T00:17:11.667Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65a5cb07e1e787bdecb8d124
    type: comment
  author: grimulkan
  content: 'Much appreciate the input.


    Quantizing Q3_K_M, Q3_K_S and Q2_K (with imatrix) now.


    By the way, my comment about TheBloke''s recommendation of Q5_K_S may seem strange,
    but it is actually comparable in size to Q2_K, though that may not be SOTA. I
    was not referring to the larger quant sizes (the 5 bits here is a bit misleading,
    it is meant to be compared to Q2_K).'
  created_at: 2024-01-16 00:17:11+00:00
  edited: false
  hidden: false
  id: 65a5cb07e1e787bdecb8d124
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-16T00:31:59.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9495336413383484
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Q5_K_S is much bigger than Q2_K, even the old version (5+ bpw vs
          approx 3.4 bpw). Maybe you forget to account for a split in 2 parts of a
          Q5_K_S model on hugging face?</p>

          <p>Thank you for the quants you are making, I''ll test them with gratitude!</p>

          <p>P.S : <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/4957">https://github.com/ggerganov/llama.cpp/pull/4957</a>
          is the link toward a PR for a GPU offload of the matrix, which accelerates
          drastically its creation compared with a CPU run.</p>

          '
        raw: 'Q5_K_S is much bigger than Q2_K, even the old version (5+ bpw vs approx
          3.4 bpw). Maybe you forget to account for a split in 2 parts of a Q5_K_S
          model on hugging face?


          Thank you for the quants you are making, I''ll test them with gratitude!


          P.S : https://github.com/ggerganov/llama.cpp/pull/4957 is the link toward
          a PR for a GPU offload of the matrix, which accelerates drastically its
          creation compared with a CPU run.'
        updatedAt: '2024-01-16T00:33:53.867Z'
      numEdits: 3
      reactions: []
    id: 65a5ce7fee220af178c52b0c
    type: comment
  author: Nexesenex
  content: 'Q5_K_S is much bigger than Q2_K, even the old version (5+ bpw vs approx
    3.4 bpw). Maybe you forget to account for a split in 2 parts of a Q5_K_S model
    on hugging face?


    Thank you for the quants you are making, I''ll test them with gratitude!


    P.S : https://github.com/ggerganov/llama.cpp/pull/4957 is the link toward a PR
    for a GPU offload of the matrix, which accelerates drastically its creation compared
    with a CPU run.'
  created_at: 2024-01-16 00:31:59+00:00
  edited: true
  hidden: false
  id: 65a5ce7fee220af178c52b0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-16T00:55:00.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.545671284198761
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: "<p>I was referring to this:</p>\n<div class=\"max-w-full overflow-auto\"\
          >\n\t<table>\n\t\t<thead><tr>\n<th>Name</th>\n<th>Quant method</th>\n<th>Bits</th>\n\
          <th>Size</th>\n<th>Max RAM required</th>\n<th>Use case</th>\n</tr>\n\n\t\
          \t</thead><tbody><tr>\n<td><a href=\"https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/blob/main/llama-2-70b-chat.Q2_K.gguf\"\
          >llama-2-70b-chat.Q2_K.gguf</a></td>\n<td>Q2_K</td>\n<td>2</td>\n<td>29.28\
          \ GB</td>\n<td>31.78 GB</td>\n<td>smallest, significant quality loss - not\
          \ recommended for most purposes</td>\n</tr>\n<tr>\n<td><a href=\"https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/blob/main/llama-2-70b-chat.Q3_K_S.gguf\"\
          >llama-2-70b-chat.Q3_K_S.gguf</a></td>\n<td>Q3_K_S</td>\n<td>3</td>\n<td>29.92\
          \ GB</td>\n<td>32.42 GB</td>\n<td>very small, high quality loss</td>\n</tr>\n\
          <tr>\n<td><a href=\"https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/blob/main/llama-2-70b-chat.Q5_K_S.gguf\"\
          >llama-2-70b-chat.Q5_K_S.gguf</a></td>\n<td>Q5_K_S</td>\n<td>5</td>\n<td>30.57\
          \ GB</td>\n<td>33.07 GB</td>\n<td>large, low quality loss - recommended</td>\n\
          </tr>\n<tr>\n<td>No splits. Maybe a typo on TheBloke's part?</td>\n<td></td>\n\
          <td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n\t</table>\n\
          </div>\n<p>Thanks for the GPU offload link, though all my GPUs are in use\
          \ and it will have to \"chug along\" on a TR PRO!</p>\n"
        raw: 'I was referring to this:

          | Name | Quant method | Bits | Size | Max RAM required | Use case |

          | ---- | ---- | ---- | ---- | ---- | ----- |

          | [llama-2-70b-chat.Q2_K.gguf](https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/blob/main/llama-2-70b-chat.Q2_K.gguf)
          | Q2_K | 2 | 29.28 GB| 31.78 GB | smallest, significant quality loss - not
          recommended for most purposes |

          | [llama-2-70b-chat.Q3_K_S.gguf](https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/blob/main/llama-2-70b-chat.Q3_K_S.gguf)
          | Q3_K_S | 3 | 29.92 GB| 32.42 GB | very small, high quality loss |

          | [llama-2-70b-chat.Q5_K_S.gguf](https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/blob/main/llama-2-70b-chat.Q5_K_S.gguf)
          | Q5_K_S | 5 | 30.57 GB| 33.07 GB | large, low quality loss - recommended
          |

          No splits. Maybe a typo on TheBloke''s part?


          Thanks for the GPU offload link, though all my GPUs are in use and it will
          have to "chug along" on a TR PRO!'
        updatedAt: '2024-01-16T00:55:00.705Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65a5d3e4f2e68a4148381cdd
    type: comment
  author: grimulkan
  content: 'I was referring to this:

    | Name | Quant method | Bits | Size | Max RAM required | Use case |

    | ---- | ---- | ---- | ---- | ---- | ----- |

    | [llama-2-70b-chat.Q2_K.gguf](https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/blob/main/llama-2-70b-chat.Q2_K.gguf)
    | Q2_K | 2 | 29.28 GB| 31.78 GB | smallest, significant quality loss - not recommended
    for most purposes |

    | [llama-2-70b-chat.Q3_K_S.gguf](https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/blob/main/llama-2-70b-chat.Q3_K_S.gguf)
    | Q3_K_S | 3 | 29.92 GB| 32.42 GB | very small, high quality loss |

    | [llama-2-70b-chat.Q5_K_S.gguf](https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/blob/main/llama-2-70b-chat.Q5_K_S.gguf)
    | Q5_K_S | 5 | 30.57 GB| 33.07 GB | large, low quality loss - recommended |

    No splits. Maybe a typo on TheBloke''s part?


    Thanks for the GPU offload link, though all my GPUs are in use and it will have
    to "chug along" on a TR PRO!'
  created_at: 2024-01-16 00:55:00+00:00
  edited: false
  hidden: false
  id: 65a5d3e4f2e68a4148381cdd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-16T01:29:10.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5586515069007874
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>This particular quant is broken indeed : <a href="https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF/discussions/5">https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF/discussions/5</a></p>

          '
        raw: 'This particular quant is broken indeed : https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF/discussions/5'
        updatedAt: '2024-01-16T01:29:10.205Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - grimulkan
    id: 65a5dbe6576772f531bf3c62
    type: comment
  author: Nexesenex
  content: 'This particular quant is broken indeed : https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF/discussions/5'
  created_at: 2024-01-16 01:29:10+00:00
  edited: false
  hidden: false
  id: 65a5dbe6576772f531bf3c62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-16T17:27:50.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9385952353477478
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Posted: <a href="https://huggingface.co/grimulkan/aurelian-v0.5-70b-rope8-32K_GGUF">https://huggingface.co/grimulkan/aurelian-v0.5-70b-rope8-32K_GGUF</a></p>

          <p>I derived the imatrix from the same dataset I used for the EXL2 quants,
          which took much longer than I thought with CPU-only processing.</p>

          '
        raw: 'Posted: https://huggingface.co/grimulkan/aurelian-v0.5-70b-rope8-32K_GGUF


          I derived the imatrix from the same dataset I used for the EXL2 quants,
          which took much longer than I thought with CPU-only processing.'
        updatedAt: '2024-01-16T17:27:50.729Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65a6bc960bbb07551f459be6
    type: comment
  author: grimulkan
  content: 'Posted: https://huggingface.co/grimulkan/aurelian-v0.5-70b-rope8-32K_GGUF


    I derived the imatrix from the same dataset I used for the EXL2 quants, which
    took much longer than I thought with CPU-only processing.'
  created_at: 2024-01-16 17:27:50+00:00
  edited: false
  hidden: false
  id: 65a6bc960bbb07551f459be6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-16T18:53:19.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8859477639198303
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Fantastic, Grimulkan.<br>You''re also the first to use an iMatrix
          for Q3_K quants, let alone on a 70b model.. with an extended context via
          linear rope.<br>I''m gonna download them all, put a testrun of wikitext
          and hellaswag on them, at various context length, and report the results
          when I get them before actually enjoying the model.<br>If you have time
          for a last quant, drop in the IQ2_XS, because it''s the best compromise
          of all new lower quants than Q2_K (IQ2_XXS is the second, Q2_K_S needs more
          testing, I will hellaswag them to see if beyond the low perplexity decrease
          compared to IQ2_XS, the hellaswag increase is decent), especially for a
          long context model like yours on 24GB VRAM.</p>

          '
        raw: 'Fantastic, Grimulkan.

          You''re also the first to use an iMatrix for Q3_K quants, let alone on a
          70b model.. with an extended context via linear rope.

          I''m gonna download them all, put a testrun of wikitext and hellaswag on
          them, at various context length, and report the results when I get them
          before actually enjoying the model.

          If you have time for a last quant, drop in the IQ2_XS, because it''s the
          best compromise of all new lower quants than Q2_K (IQ2_XXS is the second,
          Q2_K_S needs more testing, I will hellaswag them to see if beyond the low
          perplexity decrease compared to IQ2_XS, the hellaswag increase is decent),
          especially for a long context model like yours on 24GB VRAM.'
        updatedAt: '2024-01-16T19:01:14.742Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - grimulkan
    id: 65a6d09fd9c4c62f7674c7ee
    type: comment
  author: Nexesenex
  content: 'Fantastic, Grimulkan.

    You''re also the first to use an iMatrix for Q3_K quants, let alone on a 70b model..
    with an extended context via linear rope.

    I''m gonna download them all, put a testrun of wikitext and hellaswag on them,
    at various context length, and report the results when I get them before actually
    enjoying the model.

    If you have time for a last quant, drop in the IQ2_XS, because it''s the best
    compromise of all new lower quants than Q2_K (IQ2_XXS is the second, Q2_K_S needs
    more testing, I will hellaswag them to see if beyond the low perplexity decrease
    compared to IQ2_XS, the hellaswag increase is decent), especially for a long context
    model like yours on 24GB VRAM.'
  created_at: 2024-01-16 18:53:19+00:00
  edited: true
  hidden: false
  id: 65a6d09fd9c4c62f7674c7ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-16T19:50:30.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9983444809913635
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Uploaded IQ2_XS as well.</p>

          '
        raw: Uploaded IQ2_XS as well.
        updatedAt: '2024-01-16T19:50:30.791Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65a6de06d6e5c1ed6c3ae78d
    type: comment
  author: grimulkan
  content: Uploaded IQ2_XS as well.
  created_at: 2024-01-16 19:50:30+00:00
  edited: false
  hidden: false
  id: 65a6de06d6e5c1ed6c3ae78d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-16T20:19:18.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8153671026229858
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Thanks!</p>

          <p>My testrun started on Aurelian.<br>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,56.5,,400,2024-01-16
          21:15:00,PEC8,70b,Llama_2</p>

          <p>for information :<br>Llama-v2-70b-Q2_K_S-2.66bpw.gguf,-,hellaswag,57.25,,400
          (edit, hellaswag test was broken in LlamaCPP and they fixed it, I have to
          remake all my last days hellaswag now) : </p>

          <ul>

          <li>aurelian-v0.5-70b-rope8-32K.IQ2_XS.gguf,-,wikitext,11.7134,512 (that''s
          way too high)</li>

          <li>aurelian-v0.5-70b-rope8-32K.IQ2_XS.gguf,-,hellaswag,48 (quant is problematic)</li>

          <li>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,5.6184,512
          (that''s still a bit high (+1-1.5 compared to normal), but it could be the
          rope)</li>

          <li>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,72.75 (instead
          of 56.5)</li>

          <li>aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,hellaswag,75.25 (still
          a bit low)</li>

          <li>aurelian-v0.5-70b-rope8-32K.Q3_K_M-3.85bpw.gguf,-,hellaswag,74.25 (weird
          +1 compared to Q3_K_S)</li>

          </ul>

          <p>Those Hellaswag values are a bit low, PPL a bit high, but I need to make
          more tests with the fixed hellaswag to get a sense of measure. I''ll keep
          you informed, as well as on the perplexity, and later impressions in usage.</p>

          <p>If you can make me a Q3_K_S quant without importance matrix when you
          have a bit of spare time, I''d appreciate it so I can see if it''s the iMatrix
          which is problematic on higher quants, especially when its disavantages
          outweight in the higher quants the advantages it brought to the lower quant.</p>

          '
        raw: "Thanks!\n\nMy testrun started on Aurelian. \naurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,56.5,,400,2024-01-16\
          \ 21:15:00,PEC8,70b,Llama_2\n\nfor information :\nLlama-v2-70b-Q2_K_S-2.66bpw.gguf,-,hellaswag,57.25,,400\
          \ (edit, hellaswag test was broken in LlamaCPP and they fixed it, I have\
          \ to remake all my last days hellaswag now) : \n- aurelian-v0.5-70b-rope8-32K.IQ2_XS.gguf,-,wikitext,11.7134,512\
          \ (that's way too high)\n- aurelian-v0.5-70b-rope8-32K.IQ2_XS.gguf,-,hellaswag,48\
          \ (quant is problematic)\n- aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,5.6184,512\
          \ (that's still a bit high (+1-1.5 compared to normal), but it could be\
          \ the rope)\n- aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,72.75\
          \ (instead of 56.5)\n- aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,hellaswag,75.25\
          \ (still a bit low)\n- aurelian-v0.5-70b-rope8-32K.Q3_K_M-3.85bpw.gguf,-,hellaswag,74.25\
          \ (weird +1 compared to Q3_K_S)\n\nThose Hellaswag values are a bit low,\
          \ PPL a bit high, but I need to make more tests with the fixed hellaswag\
          \ to get a sense of measure. I'll keep you informed, as well as on the perplexity,\
          \ and later impressions in usage.\n\nIf you can make me a Q3_K_S quant without\
          \ importance matrix when you have a bit of spare time, I'd appreciate it\
          \ so I can see if it's the iMatrix which is problematic on higher quants,\
          \ especially when its disavantages outweight in the higher quants the advantages\
          \ it brought to the lower quant."
        updatedAt: '2024-01-16T23:24:28.654Z'
      numEdits: 8
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - grimulkan
    id: 65a6e4c64555358d2cbcbefa
    type: comment
  author: Nexesenex
  content: "Thanks!\n\nMy testrun started on Aurelian. \naurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,56.5,,400,2024-01-16\
    \ 21:15:00,PEC8,70b,Llama_2\n\nfor information :\nLlama-v2-70b-Q2_K_S-2.66bpw.gguf,-,hellaswag,57.25,,400\
    \ (edit, hellaswag test was broken in LlamaCPP and they fixed it, I have to remake\
    \ all my last days hellaswag now) : \n- aurelian-v0.5-70b-rope8-32K.IQ2_XS.gguf,-,wikitext,11.7134,512\
    \ (that's way too high)\n- aurelian-v0.5-70b-rope8-32K.IQ2_XS.gguf,-,hellaswag,48\
    \ (quant is problematic)\n- aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,5.6184,512\
    \ (that's still a bit high (+1-1.5 compared to normal), but it could be the rope)\n\
    - aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,72.75 (instead of\
    \ 56.5)\n- aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,hellaswag,75.25 (still\
    \ a bit low)\n- aurelian-v0.5-70b-rope8-32K.Q3_K_M-3.85bpw.gguf,-,hellaswag,74.25\
    \ (weird +1 compared to Q3_K_S)\n\nThose Hellaswag values are a bit low, PPL a\
    \ bit high, but I need to make more tests with the fixed hellaswag to get a sense\
    \ of measure. I'll keep you informed, as well as on the perplexity, and later\
    \ impressions in usage.\n\nIf you can make me a Q3_K_S quant without importance\
    \ matrix when you have a bit of spare time, I'd appreciate it so I can see if\
    \ it's the iMatrix which is problematic on higher quants, especially when its\
    \ disavantages outweight in the higher quants the advantages it brought to the\
    \ lower quant."
  created_at: 2024-01-16 20:19:18+00:00
  edited: true
  hidden: false
  id: 65a6e4c64555358d2cbcbefa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-16T23:33:14.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8763929009437561
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>I will upload Q3_K_S without imatrix, check in an hour or so.</p>

          <p>Is it 72.75 or 56.5 for hellaswag Aurelian Q2_K?</p>

          <p>Hope I didn''t mess something basic up. You''re testing with rope scaling
          8 right? (I used the same when computing the imatrix).</p>

          '
        raw: 'I will upload Q3_K_S without imatrix, check in an hour or so.


          Is it 72.75 or 56.5 for hellaswag Aurelian Q2_K?


          Hope I didn''t mess something basic up. You''re testing with rope scaling
          8 right? (I used the same when computing the imatrix).'
        updatedAt: '2024-01-16T23:33:14.963Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65a7123a79130b4be25d97f7
    type: comment
  author: grimulkan
  content: 'I will upload Q3_K_S without imatrix, check in an hour or so.


    Is it 72.75 or 56.5 for hellaswag Aurelian Q2_K?


    Hope I didn''t mess something basic up. You''re testing with rope scaling 8 right?
    (I used the same when computing the imatrix).'
  created_at: 2024-01-16 23:33:14+00:00
  edited: false
  hidden: false
  id: 65a7123a79130b4be25d97f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-16T23:38:27.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9738619923591614
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>72.75, the 56.5 was obtained with a broken hellaswag.<br>Yes, I
          scale the rope with linear 8. 4 gives worst results, I tested as well (some
          old models had benefit to lower the linear rope like Benhrym14''s, but not
          yours).</p>

          <p>As for the iMatrix, it seems easy to mess it up. TheBloke might have
          as well on his first SOTA quant (A Yi 34b). Apparently, the iMatrix must
          be made on specific parameters (ctx 512, not long context) to get the best
          PPL (and possibly Hellaswag?). See <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/4957">https://github.com/ggerganov/llama.cpp/pull/4957</a></p>

          <p>You Exl2 dataset might also not be the best for LlamaCPP usage (I don''t
          know, just trying to find hypothesis), I would need to check that on an
          Exl2 (v0.0.11 release or ulterior commit) quant of yours, preferably a 3bpw
          because that''s those I know the best the behavior of thanks to LoneStriker
          quants which are a fit for my VRAM amount at long context while retaining
          a reasonable perplexity.</p>

          <p>Anyway, the Q3_K_S without iMatrix will help a lot! Thanks! -&gt; It''s
          downloading, I put it on the top of the test batch asap.</p>

          '
        raw: '72.75, the 56.5 was obtained with a broken hellaswag.

          Yes, I scale the rope with linear 8. 4 gives worst results, I tested as
          well (some old models had benefit to lower the linear rope like Benhrym14''s,
          but not yours).


          As for the iMatrix, it seems easy to mess it up. TheBloke might have as
          well on his first SOTA quant (A Yi 34b). Apparently, the iMatrix must be
          made on specific parameters (ctx 512, not long context) to get the best
          PPL (and possibly Hellaswag?). See https://github.com/ggerganov/llama.cpp/pull/4957


          You Exl2 dataset might also not be the best for LlamaCPP usage (I don''t
          know, just trying to find hypothesis), I would need to check that on an
          Exl2 (v0.0.11 release or ulterior commit) quant of yours, preferably a 3bpw
          because that''s those I know the best the behavior of thanks to LoneStriker
          quants which are a fit for my VRAM amount at long context while retaining
          a reasonable perplexity.


          Anyway, the Q3_K_S without iMatrix will help a lot! Thanks! -> It''s downloading,
          I put it on the top of the test batch asap.'
        updatedAt: '2024-01-16T23:56:10.881Z'
      numEdits: 9
      reactions: []
    id: 65a7137312a69b97373b6419
    type: comment
  author: Nexesenex
  content: '72.75, the 56.5 was obtained with a broken hellaswag.

    Yes, I scale the rope with linear 8. 4 gives worst results, I tested as well (some
    old models had benefit to lower the linear rope like Benhrym14''s, but not yours).


    As for the iMatrix, it seems easy to mess it up. TheBloke might have as well on
    his first SOTA quant (A Yi 34b). Apparently, the iMatrix must be made on specific
    parameters (ctx 512, not long context) to get the best PPL (and possibly Hellaswag?).
    See https://github.com/ggerganov/llama.cpp/pull/4957


    You Exl2 dataset might also not be the best for LlamaCPP usage (I don''t know,
    just trying to find hypothesis), I would need to check that on an Exl2 (v0.0.11
    release or ulterior commit) quant of yours, preferably a 3bpw because that''s
    those I know the best the behavior of thanks to LoneStriker quants which are a
    fit for my VRAM amount at long context while retaining a reasonable perplexity.


    Anyway, the Q3_K_S without iMatrix will help a lot! Thanks! -> It''s downloading,
    I put it on the top of the test batch asap.'
  created_at: 2024-01-16 23:38:27+00:00
  edited: true
  hidden: false
  id: 65a7137312a69b97373b6419
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-17T00:17:59.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9748778939247131
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Let me know. I could also re-do a wikitext imatrix like everyone
          else is doing. I did use ctx 512 for imatrix.</p>

          '
        raw: Let me know. I could also re-do a wikitext imatrix like everyone else
          is doing. I did use ctx 512 for imatrix.
        updatedAt: '2024-01-17T00:17:59.346Z'
      numEdits: 0
      reactions: []
    id: 65a71cb7e94d40886a6b4f42
    type: comment
  author: grimulkan
  content: Let me know. I could also re-do a wikitext imatrix like everyone else is
    doing. I did use ctx 512 for imatrix.
  created_at: 2024-01-17 00:17:59+00:00
  edited: false
  hidden: false
  id: 65a71cb7e94d40886a6b4f42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-17T00:42:06.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7870983481407166
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,hellaswag,76.5,,<br>That''s
          quite better. Perplexity running.<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,5.1829,512<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,wikitext,5.1811,512<br>Only
          a very little decrease here.<br>Wikitext iMatrix could be interesting, I''ll
          test it if you make it.</p>

          <p>aurelian-v0.5-70b-rope8-32K.Q3_K_M-3.85bpw.gguf,-,wikitext,5.3966,512,<br>aurelian-v0.5-70b-rope8-32K.Q3_K_M-3.85bpw.gguf,-,hellaswag,74.25,</p>

          <p>The Q3_K_M seems to have suffered with the iMatrix, it''s actually less
          good than the Q3_K_S. (And yes, I used linear 8 rope).</p>

          '
        raw: 'aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,hellaswag,76.5,,

          That''s quite better. Perplexity running.

          aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,5.1829,512

          aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,wikitext,5.1811,512

          Only a very little decrease here.

          Wikitext iMatrix could be interesting, I''ll test it if you make it.


          aurelian-v0.5-70b-rope8-32K.Q3_K_M-3.85bpw.gguf,-,wikitext,5.3966,512,

          aurelian-v0.5-70b-rope8-32K.Q3_K_M-3.85bpw.gguf,-,hellaswag,74.25,


          The Q3_K_M seems to have suffered with the iMatrix, it''s actually less
          good than the Q3_K_S. (And yes, I used linear 8 rope).'
        updatedAt: '2024-01-17T02:00:10.096Z'
      numEdits: 3
      reactions: []
    id: 65a7225e838b3acc53b0f668
    type: comment
  author: Nexesenex
  content: 'aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,hellaswag,76.5,,

    That''s quite better. Perplexity running.

    aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,5.1829,512

    aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,wikitext,5.1811,512

    Only a very little decrease here.

    Wikitext iMatrix could be interesting, I''ll test it if you make it.


    aurelian-v0.5-70b-rope8-32K.Q3_K_M-3.85bpw.gguf,-,wikitext,5.3966,512,

    aurelian-v0.5-70b-rope8-32K.Q3_K_M-3.85bpw.gguf,-,hellaswag,74.25,


    The Q3_K_M seems to have suffered with the iMatrix, it''s actually less good than
    the Q3_K_S. (And yes, I used linear 8 rope).'
  created_at: 2024-01-17 00:42:06+00:00
  edited: true
  hidden: false
  id: 65a7225e838b3acc53b0f668
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-17T03:02:37.000Z'
    data:
      edited: true
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9808716773986816
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Okay, it''ll take me a bit to make a wikitext imatrix. Which ones
          would you want me to try with that? All 2 and 3 bit quants?</p>

          <p>Also, to make sure I didn''t screw something up in general, are you able
          to run any higher bit quants and confirm a good PPL/wikitext/hellaswag?</p>

          '
        raw: 'Okay, it''ll take me a bit to make a wikitext imatrix. Which ones would
          you want me to try with that? All 2 and 3 bit quants?


          Also, to make sure I didn''t screw something up in general, are you able
          to run any higher bit quants and confirm a good PPL/wikitext/hellaswag?'
        updatedAt: '2024-01-17T03:03:37.256Z'
      numEdits: 1
      reactions: []
    id: 65a7434d293f73160dee1122
    type: comment
  author: grimulkan
  content: 'Okay, it''ll take me a bit to make a wikitext imatrix. Which ones would
    you want me to try with that? All 2 and 3 bit quants?


    Also, to make sure I didn''t screw something up in general, are you able to run
    any higher bit quants and confirm a good PPL/wikitext/hellaswag?'
  created_at: 2024-01-17 03:02:37+00:00
  edited: true
  hidden: false
  id: 65a7434d293f73160dee1122
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-17T04:21:34.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8372147083282471
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>For the higher quants, my hardware will be extremely sluggish.<br>If
          you have 30 mins for a 48GB GPU setup, the kind of command you need to run
          looks like that (I''m using Windows, adapt to Linux if needs be) :<br>perplexity
          -m aurelian-v0.5-70b-rope8-32K.Q4_K_M.gguf -f wiki.test.raw -ngl 100 -b
          512  --rope-scale 8 -c 512<br>Otherwise, Q3_K_S is a standardized enough
          quant to trust the results.</p>

          <p>Table at 512ctx, rope 1 10000 :</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/K7DxXfzU7opRcyqAnyusH.png"><img
          alt="Screenshot 2024-01-09 at 13-58-22 r_LocalLLaMA - How much does Quantization
          actually impact models - KL Divergence Tests.png" src="https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/K7DxXfzU7opRcyqAnyusH.png"></a></p>

          <p>To test on another way, I think we could also go on a 3bpw and/or a 3.5bpw
          (this, on Exllama v2 &gt;0.0.11), with your Exl2 dataset and also why not
          with the built-in one, just in order to spot if the ppl (wikitext, but also
          ptb that I can test on Ooba) and hellaswag discrepencies are inherent to
          Aurelian or simply to its calibration.<br>Otherwise, I''ll grab any new
          set of GGUF quants &lt;=Q3_K_M that you throw at me and test them.</p>

          <p>I''m off, computer will test the Q2K and Q3_K_S of Aurelian among other
          models while I sleep. I''ll check what''s up tomorrow!</p>

          '
        raw: 'For the higher quants, my hardware will be extremely sluggish.

          If you have 30 mins for a 48GB GPU setup, the kind of command you need to
          run looks like that (I''m using Windows, adapt to Linux if needs be) :

          perplexity -m aurelian-v0.5-70b-rope8-32K.Q4_K_M.gguf -f wiki.test.raw -ngl
          100 -b 512  --rope-scale 8 -c 512

          Otherwise, Q3_K_S is a standardized enough quant to trust the results.


          Table at 512ctx, rope 1 10000 :



          ![Screenshot 2024-01-09 at 13-58-22 r_LocalLLaMA - How much does Quantization
          actually impact models - KL Divergence Tests.png](https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/K7DxXfzU7opRcyqAnyusH.png)



          To test on another way, I think we could also go on a 3bpw and/or a 3.5bpw
          (this, on Exllama v2 >0.0.11), with your Exl2 dataset and also why not with
          the built-in one, just in order to spot if the ppl (wikitext, but also ptb
          that I can test on Ooba) and hellaswag discrepencies are inherent to Aurelian
          or simply to its calibration.

          Otherwise, I''ll grab any new set of GGUF quants <=Q3_K_M that you throw
          at me and test them.


          I''m off, computer will test the Q2K and Q3_K_S of Aurelian among other
          models while I sleep. I''ll check what''s up tomorrow!'
        updatedAt: '2024-01-17T04:30:54.870Z'
      numEdits: 7
      reactions: []
    id: 65a755ce5c58475cf9118de4
    type: comment
  author: Nexesenex
  content: 'For the higher quants, my hardware will be extremely sluggish.

    If you have 30 mins for a 48GB GPU setup, the kind of command you need to run
    looks like that (I''m using Windows, adapt to Linux if needs be) :

    perplexity -m aurelian-v0.5-70b-rope8-32K.Q4_K_M.gguf -f wiki.test.raw -ngl 100
    -b 512  --rope-scale 8 -c 512

    Otherwise, Q3_K_S is a standardized enough quant to trust the results.


    Table at 512ctx, rope 1 10000 :



    ![Screenshot 2024-01-09 at 13-58-22 r_LocalLLaMA - How much does Quantization
    actually impact models - KL Divergence Tests.png](https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/K7DxXfzU7opRcyqAnyusH.png)



    To test on another way, I think we could also go on a 3bpw and/or a 3.5bpw (this,
    on Exllama v2 >0.0.11), with your Exl2 dataset and also why not with the built-in
    one, just in order to spot if the ppl (wikitext, but also ptb that I can test
    on Ooba) and hellaswag discrepencies are inherent to Aurelian or simply to its
    calibration.

    Otherwise, I''ll grab any new set of GGUF quants <=Q3_K_M that you throw at me
    and test them.


    I''m off, computer will test the Q2K and Q3_K_S of Aurelian among other models
    while I sleep. I''ll check what''s up tomorrow!'
  created_at: 2024-01-17 04:21:34+00:00
  edited: true
  hidden: false
  id: 65a755ce5c58475cf9118de4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-17T13:08:14.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.24248497188091278
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>More data for Q2_K :<br>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,4.6868,6144<br>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,4.6706,4096<br>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,4.8079,2048<br>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,5.0473,1024<br>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,5.6184,512,<br>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,72.75</p>

          '
        raw: 'More data for Q2_K :

          aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,4.6868,6144

          aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,4.6706,4096

          aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,4.8079,2048

          aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,5.0473,1024

          aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,5.6184,512,

          aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,72.75'
        updatedAt: '2024-01-17T13:08:14.553Z'
      numEdits: 0
      reactions: []
    id: 65a7d13e8581aad8c9ffd190
    type: comment
  author: Nexesenex
  content: 'More data for Q2_K :

    aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,4.6868,6144

    aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,4.6706,4096

    aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,4.8079,2048

    aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,5.0473,1024

    aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,wikitext,5.6184,512,

    aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,72.75'
  created_at: 2024-01-17 13:08:14+00:00
  edited: false
  hidden: false
  id: 65a7d13e8581aad8c9ffd190
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-17T17:29:08.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9318454265594482
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>At least the wikitext is going down with ctx, so it''s not a broken
          model.</p>

          <p>Still working on wik.test.raw imatrix quants. I''ll post here when they''re
          done.</p>

          <p>I will test PPL on larger quants (and EXL2 quants) when I get the compute
          and post here.</p>

          '
        raw: 'At least the wikitext is going down with ctx, so it''s not a broken
          model.


          Still working on wik.test.raw imatrix quants. I''ll post here when they''re
          done.


          I will test PPL on larger quants (and EXL2 quants) when I get the compute
          and post here.'
        updatedAt: '2024-01-17T17:29:08.182Z'
      numEdits: 0
      reactions: []
    id: 65a80e64a6fe31817bd87b51
    type: comment
  author: grimulkan
  content: 'At least the wikitext is going down with ctx, so it''s not a broken model.


    Still working on wik.test.raw imatrix quants. I''ll post here when they''re done.


    I will test PPL on larger quants (and EXL2 quants) when I get the compute and
    post here.'
  created_at: 2024-01-17 17:29:08+00:00
  edited: false
  hidden: false
  id: 65a80e64a6fe31817bd87b51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-17T18:07:33.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5698458552360535
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>More, on Q3_K_S with iMatrix.<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.3091,4096,<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.3094,6144,<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.7315,8192,<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.6160,12288<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.4439,2048,<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.6671,1024,<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,5.1829,512,<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,hellaswag,75.25,<br>And
          without :<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,wikitext,4.4473,2048<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,wikitext,4.6705,1024<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,wikitext,5.1811,512<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,hellaswag,76.5</p>

          <p>But its perplexity and Hellaswag are impaired by something, because the
          ppl is almost 1.5 pts too high for a Q3_K_S, and the hellaswag 6-8 points
          too low. The rope doesn''t explain it fully.</p>

          <p>Here comes a test I made on a 13b 32k model with Linear 8 rope :<br>Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.589,12288<br>Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.6224,10240<br>Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.6898,6144<br>Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.6959,4096<br>Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.723,12288<br>Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.7255,8192<br>Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.8577,2048<br>Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.9523,16384<br>Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,5.0608,1024<br>Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,5.4942,20480<br>Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,5.5108,512<br>Its
          perplexity is 0.3points higher than the best 13b models with rope 1 10000,
          and doesn''t benefit from a reduction of the linear rope (as yours doesn''t
          either).<br>I''ll test this model Hellaswag to see if it suffers from the
          Linear 8 or not (quants do not affect too much the Hellaswag score)</p>

          '
        raw: 'More, on Q3_K_S with iMatrix.

          aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.3091,4096,

          aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.3094,6144,

          aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.7315,8192,

          aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.6160,12288

          aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.4439,2048,

          aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.6671,1024,

          aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,5.1829,512,

          aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,hellaswag,75.25,

          And without :

          aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,wikitext,4.4473,2048

          aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,wikitext,4.6705,1024

          aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,wikitext,5.1811,512

          aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,hellaswag,76.5


          But its perplexity and Hellaswag are impaired by something, because the
          ppl is almost 1.5 pts too high for a Q3_K_S, and the hellaswag 6-8 points
          too low. The rope doesn''t explain it fully.


          Here comes a test I made on a 13b 32k model with Linear 8 rope :

          Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.589,12288

          Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.6224,10240

          Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.6898,6144

          Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.6959,4096

          Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.723,12288

          Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.7255,8192

          Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.8577,2048

          Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.9523,16384

          Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,5.0608,1024

          Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,5.4942,20480

          Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,5.5108,512

          Its perplexity is 0.3points higher than the best 13b models with rope 1
          10000, and doesn''t benefit from a reduction of the linear rope (as yours
          doesn''t either).

          I''ll test this model Hellaswag to see if it suffers from the Linear 8 or
          not (quants do not affect too much the Hellaswag score)'
        updatedAt: '2024-01-17T18:09:39.480Z'
      numEdits: 2
      reactions: []
    id: 65a8176516e8e332e70535c4
    type: comment
  author: Nexesenex
  content: 'More, on Q3_K_S with iMatrix.

    aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.3091,4096,

    aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.3094,6144,

    aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.7315,8192,

    aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.6160,12288

    aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.4439,2048,

    aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,4.6671,1024,

    aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,wikitext,5.1829,512,

    aurelian-v0.5-70b-rope8-32K.Q3_K_S-3.47bpw.gguf,-,hellaswag,75.25,

    And without :

    aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,wikitext,4.4473,2048

    aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,wikitext,4.6705,1024

    aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,wikitext,5.1811,512

    aurelian-v0.5-70b-rope8-32K.Q3_K_S.no_imatrix.gguf,-,hellaswag,76.5


    But its perplexity and Hellaswag are impaired by something, because the ppl is
    almost 1.5 pts too high for a Q3_K_S, and the hellaswag 6-8 points too low. The
    rope doesn''t explain it fully.


    Here comes a test I made on a 13b 32k model with Linear 8 rope :

    Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.589,12288

    Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.6224,10240

    Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.6898,6144

    Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.6959,4096

    Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.723,12288

    Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.7255,8192

    Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.8577,2048

    Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,4.9523,16384

    Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,5.0608,1024

    Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,5.4942,20480

    Giraffe-v2-13b-32k.Q5_K_M.gguf,-,wikitext,5.5108,512

    Its perplexity is 0.3points higher than the best 13b models with rope 1 10000,
    and doesn''t benefit from a reduction of the linear rope (as yours doesn''t either).

    I''ll test this model Hellaswag to see if it suffers from the Linear 8 or not
    (quants do not affect too much the Hellaswag score)'
  created_at: 2024-01-17 18:07:33+00:00
  edited: true
  hidden: false
  id: 65a8176516e8e332e70535c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-17T20:39:24.000Z'
    data:
      edited: true
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8602775931358337
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Uploaded:</p>

          <p>Q3_K_M without imatrix<br>Q2_K, Q3_K_M, Q3_K_S with wikitext imatrix</p>

          <p>If you think any of that is useful.</p>

          <p>Still can''t spare the GPUs, but I''m running a Q5_K_M PPL eval on wikitext
          on the CPU which will take a while.</p>

          '
        raw: 'Uploaded:


          Q3_K_M without imatrix

          Q2_K, Q3_K_M, Q3_K_S with wikitext imatrix


          If you think any of that is useful.


          Still can''t spare the GPUs, but I''m running a Q5_K_M PPL eval on wikitext
          on the CPU which will take a while.

          '
        updatedAt: '2024-01-17T21:36:14.324Z'
      numEdits: 1
      reactions: []
    id: 65a83afc0237119dc5eebf2a
    type: comment
  author: grimulkan
  content: 'Uploaded:


    Q3_K_M without imatrix

    Q2_K, Q3_K_M, Q3_K_S with wikitext imatrix


    If you think any of that is useful.


    Still can''t spare the GPUs, but I''m running a Q5_K_M PPL eval on wikitext on
    the CPU which will take a while.

    '
  created_at: 2024-01-17 20:39:24+00:00
  edited: true
  hidden: false
  id: 65a83afc0237119dc5eebf2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-17T22:42:16.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8498625755310059
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Nice ! I will download the Q2_K with wikimatrix and put it on the
          testlist right away!<br>If progress, I''ll test the rest!</p>

          <p>Note : Giraffe-v2-13b-32k.Q5_K_M.gguf,-,hellaswag,75.75</p>

          '
        raw: 'Nice ! I will download the Q2_K with wikimatrix and put it on the testlist
          right away!

          If progress, I''ll test the rest!


          Note : Giraffe-v2-13b-32k.Q5_K_M.gguf,-,hellaswag,75.75'
        updatedAt: '2024-01-17T23:37:27.762Z'
      numEdits: 2
      reactions: []
    id: 65a857c85e3029d4d58bb643
    type: comment
  author: Nexesenex
  content: 'Nice ! I will download the Q2_K with wikimatrix and put it on the testlist
    right away!

    If progress, I''ll test the rest!


    Note : Giraffe-v2-13b-32k.Q5_K_M.gguf,-,hellaswag,75.75'
  created_at: 2024-01-17 22:42:16+00:00
  edited: true
  hidden: false
  id: 65a857c85e3029d4d58bb643
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-18T00:25:35.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.26997652649879456
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Here are my results :<br>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,wikitext,5.0275,1024<br>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,wikitext,5.5936,512<br>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,hellaswag,72<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S.wiki_imatrix.gguf,-,wikitext,5.1619,512<br>aurelian-v0.5-70b-rope8-32K.Q3_K_S.wiki_imatrix.gguf,-,hellaswag,74.75</p>

          '
        raw: 'Here are my results :

          aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,wikitext,5.0275,1024

          aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,wikitext,5.5936,512

          aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,hellaswag,72

          aurelian-v0.5-70b-rope8-32K.Q3_K_S.wiki_imatrix.gguf,-,wikitext,5.1619,512

          aurelian-v0.5-70b-rope8-32K.Q3_K_S.wiki_imatrix.gguf,-,hellaswag,74.75'
        updatedAt: '2024-01-18T01:31:39.296Z'
      numEdits: 1
      reactions: []
    id: 65a86fff91ec5d1ec6d4ac82
    type: comment
  author: Nexesenex
  content: 'Here are my results :

    aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,wikitext,5.0275,1024

    aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,wikitext,5.5936,512

    aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,hellaswag,72

    aurelian-v0.5-70b-rope8-32K.Q3_K_S.wiki_imatrix.gguf,-,wikitext,5.1619,512

    aurelian-v0.5-70b-rope8-32K.Q3_K_S.wiki_imatrix.gguf,-,hellaswag,74.75'
  created_at: 2024-01-18 00:25:35+00:00
  edited: true
  hidden: false
  id: 65a86fff91ec5d1ec6d4ac82
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-18T02:32:34.000Z'
    data:
      edited: true
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.979362428188324
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Seems slightly better with wiki_imatrix on wikitext, as I suppose
          it is supposed to be. Hellaswag still too low? I''ll test wikitext on Q5
          and report, so at least I''ll know if it is loss from the quant or something
          in the model itself.</p>

          '
        raw: Seems slightly better with wiki_imatrix on wikitext, as I suppose it
          is supposed to be. Hellaswag still too low? I'll test wikitext on Q5 and
          report, so at least I'll know if it is loss from the quant or something
          in the model itself.
        updatedAt: '2024-01-18T02:35:15.134Z'
      numEdits: 1
      reactions: []
    id: 65a88dc29fa2a0f9a2a20bed
    type: comment
  author: grimulkan
  content: Seems slightly better with wiki_imatrix on wikitext, as I suppose it is
    supposed to be. Hellaswag still too low? I'll test wikitext on Q5 and report,
    so at least I'll know if it is loss from the quant or something in the model itself.
  created_at: 2024-01-18 02:32:34+00:00
  edited: true
  hidden: false
  id: 65a88dc29fa2a0f9a2a20bed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-18T02:57:19.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9454155564308167
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Last results :<br>aurelian-v0.5-70b-rope8-32K.Q3_K_M.wiki_imatrix.gguf,-,wikitext,5.2200,512<br>aurelian-v0.5-70b-rope8-32K.Q3_K_M.wiki_imatrix.gguf,-,hellaswag,75.25,<br>No
          need to test further on small quants, except on 3-3.5bpw on Exllama v2 &gt;=
          0.0.11 to see if that''s the GGUF format (but I doubt it).<br>Nevertheless,
          benchmarks are one thing. RP is another. A Hellaswag of 75 doesn''t make
          a model debile, and Hellaswag itself is flawed with 36% of questionable
          Q/A (Some call it Hellabad). As for perplexity, I don''t know much about
          how can a training (method, learning rate, loss, etc) can affect the overall
          perplexity of a model, but what matters most is the usage feeling. Does
          it feel like a 13b, or like a 70b (or at least 33b)?<br>I will have a chat
          with it tomorrow!</p>

          '
        raw: 'Last results :

          aurelian-v0.5-70b-rope8-32K.Q3_K_M.wiki_imatrix.gguf,-,wikitext,5.2200,512

          aurelian-v0.5-70b-rope8-32K.Q3_K_M.wiki_imatrix.gguf,-,hellaswag,75.25,

          No need to test further on small quants, except on 3-3.5bpw on Exllama v2
          >= 0.0.11 to see if that''s the GGUF format (but I doubt it).

          Nevertheless, benchmarks are one thing. RP is another. A Hellaswag of 75
          doesn''t make a model debile, and Hellaswag itself is flawed with 36% of
          questionable Q/A (Some call it Hellabad). As for perplexity, I don''t know
          much about how can a training (method, learning rate, loss, etc) can affect
          the overall perplexity of a model, but what matters most is the usage feeling.
          Does it feel like a 13b, or like a 70b (or at least 33b)?

          I will have a chat with it tomorrow!'
        updatedAt: '2024-01-18T04:08:42.008Z'
      numEdits: 2
      reactions: []
    id: 65a8938f55b4aa2cf41dd6d0
    type: comment
  author: Nexesenex
  content: 'Last results :

    aurelian-v0.5-70b-rope8-32K.Q3_K_M.wiki_imatrix.gguf,-,wikitext,5.2200,512

    aurelian-v0.5-70b-rope8-32K.Q3_K_M.wiki_imatrix.gguf,-,hellaswag,75.25,

    No need to test further on small quants, except on 3-3.5bpw on Exllama v2 >= 0.0.11
    to see if that''s the GGUF format (but I doubt it).

    Nevertheless, benchmarks are one thing. RP is another. A Hellaswag of 75 doesn''t
    make a model debile, and Hellaswag itself is flawed with 36% of questionable Q/A
    (Some call it Hellabad). As for perplexity, I don''t know much about how can a
    training (method, learning rate, loss, etc) can affect the overall perplexity
    of a model, but what matters most is the usage feeling. Does it feel like a 13b,
    or like a 70b (or at least 33b)?

    I will have a chat with it tomorrow!'
  created_at: 2024-01-18 02:57:19+00:00
  edited: true
  hidden: false
  id: 65a8938f55b4aa2cf41dd6d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-18T03:17:14.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8712697625160217
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Great! BTW, on the <a rel="nofollow" href="%5Bhttps://www.reddit.com/r/LocalLLaMA/comments/197pcmu/aurelian_70b_32k_context_v05_interim_update/%5D(https://www.reddit.com/r/LocalLLaMA/comments/197pcmu/aurelian_70b_32k_context_v05_interim_update/kid2hki/?context=8&amp;depth=9)">reddit
          thread</a>, u/a_beautiful_rhind pointed out that chatting does have a few
          issues. It overfit on game data, and that seems to bleed in to chats sometimes.
          It does not do this for story-writing, so let''s see how your experience
          goes. If you''re using SillyTavern, apparently you also need to stick the
          char info outside the system prompt, like they pointed out.</p>

          '
        raw: Great! BTW, on the [reddit thread]([https://www.reddit.com/r/LocalLLaMA/comments/197pcmu/aurelian_70b_32k_context_v05_interim_update/](https://www.reddit.com/r/LocalLLaMA/comments/197pcmu/aurelian_70b_32k_context_v05_interim_update/kid2hki/?context=8&depth=9)),
          u/a_beautiful_rhind pointed out that chatting does have a few issues. It
          overfit on game data, and that seems to bleed in to chats sometimes. It
          does not do this for story-writing, so let's see how your experience goes.
          If you're using SillyTavern, apparently you also need to stick the char
          info outside the system prompt, like they pointed out.
        updatedAt: '2024-01-18T03:17:14.014Z'
      numEdits: 0
      reactions: []
    id: 65a8983ad0803e7abc598b09
    type: comment
  author: grimulkan
  content: Great! BTW, on the [reddit thread]([https://www.reddit.com/r/LocalLLaMA/comments/197pcmu/aurelian_70b_32k_context_v05_interim_update/](https://www.reddit.com/r/LocalLLaMA/comments/197pcmu/aurelian_70b_32k_context_v05_interim_update/kid2hki/?context=8&depth=9)),
    u/a_beautiful_rhind pointed out that chatting does have a few issues. It overfit
    on game data, and that seems to bleed in to chats sometimes. It does not do this
    for story-writing, so let's see how your experience goes. If you're using SillyTavern,
    apparently you also need to stick the char info outside the system prompt, like
    they pointed out.
  created_at: 2024-01-18 03:17:14+00:00
  edited: false
  hidden: false
  id: 65a8983ad0803e7abc598b09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-18T04:24:04.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9346717596054077
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Thanks for the tips. I use ST Indeed for fun and creative tasks.</p>

          <p>Also, I grabbed a quant of a mysterious other model (a 32k version of
          Saofiq''s WinterGoddess, with a Linear 8 rope apparently). Q4_K_S only is
          available, I had no choice. I planned a testrun for it this night.<br>I''m
          running Hellaswag now at linear rope 8, and it hits 84.5 after 400 steps
          (85-86 after 200 steps), as a model at rope 1 10000 would. You''d need to
          test your model on Q4_K_S (or 4_K_M, the offset is quite small between them
          at 70b) to compare with the measures of my night run to come.</p>

          <p><a href="https://huggingface.co/mishima/WinterGoddess-1.4x-limarpv3-70B-L2-32k.GGUF">https://huggingface.co/mishima/WinterGoddess-1.4x-limarpv3-70B-L2-32k.GGUF</a></p>

          <p>Sadly, the FP16 seems to be nowhere to be found, but Saofiq might have
          a clue.</p>

          <p>Point is, once you fully train your model toward what you expect, you
          could maybe use a subtle merge with another 32k finetune which doesn''t
          encounter either or both the Hellaswag/Perplexity loss/bump of yours (if
          any remains at the final stage) to push up its coherence. Yarn Llama 70b
          32k and LongLora 70b 32k are available in fp16 for testing that if/when
          the time comes. LongAlpaca 70b 32k might have a problem, considering what
          I observed on Long Alpaca 13b (high perplexity).</p>

          '
        raw: 'Thanks for the tips. I use ST Indeed for fun and creative tasks.


          Also, I grabbed a quant of a mysterious other model (a 32k version of Saofiq''s
          WinterGoddess, with a Linear 8 rope apparently). Q4_K_S only is available,
          I had no choice. I planned a testrun for it this night.

          I''m running Hellaswag now at linear rope 8, and it hits 84.5 after 400
          steps (85-86 after 200 steps), as a model at rope 1 10000 would. You''d
          need to test your model on Q4_K_S (or 4_K_M, the offset is quite small between
          them at 70b) to compare with the measures of my night run to come.


          https://huggingface.co/mishima/WinterGoddess-1.4x-limarpv3-70B-L2-32k.GGUF


          Sadly, the FP16 seems to be nowhere to be found, but Saofiq might have a
          clue.


          Point is, once you fully train your model toward what you expect, you could
          maybe use a subtle merge with another 32k finetune which doesn''t encounter
          either or both the Hellaswag/Perplexity loss/bump of yours (if any remains
          at the final stage) to push up its coherence. Yarn Llama 70b 32k and LongLora
          70b 32k are available in fp16 for testing that if/when the time comes. LongAlpaca
          70b 32k might have a problem, considering what I observed on Long Alpaca
          13b (high perplexity).'
        updatedAt: '2024-01-18T05:50:58.665Z'
      numEdits: 2
      reactions: []
    id: 65a8a7e453191d1bb4493e94
    type: comment
  author: Nexesenex
  content: 'Thanks for the tips. I use ST Indeed for fun and creative tasks.


    Also, I grabbed a quant of a mysterious other model (a 32k version of Saofiq''s
    WinterGoddess, with a Linear 8 rope apparently). Q4_K_S only is available, I had
    no choice. I planned a testrun for it this night.

    I''m running Hellaswag now at linear rope 8, and it hits 84.5 after 400 steps
    (85-86 after 200 steps), as a model at rope 1 10000 would. You''d need to test
    your model on Q4_K_S (or 4_K_M, the offset is quite small between them at 70b)
    to compare with the measures of my night run to come.


    https://huggingface.co/mishima/WinterGoddess-1.4x-limarpv3-70B-L2-32k.GGUF


    Sadly, the FP16 seems to be nowhere to be found, but Saofiq might have a clue.


    Point is, once you fully train your model toward what you expect, you could maybe
    use a subtle merge with another 32k finetune which doesn''t encounter either or
    both the Hellaswag/Perplexity loss/bump of yours (if any remains at the final
    stage) to push up its coherence. Yarn Llama 70b 32k and LongLora 70b 32k are available
    in fp16 for testing that if/when the time comes. LongAlpaca 70b 32k might have
    a problem, considering what I observed on Long Alpaca 13b (high perplexity).'
  created_at: 2024-01-18 04:24:04+00:00
  edited: true
  hidden: false
  id: 65a8a7e453191d1bb4493e94
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-18T06:45:17.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5605875849723816
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>aurelian-v0.5-70b-rope8-32K.Q5_K_M.gguf (no imatrix) got 4.98 PPL
          @ 512 ctx on wiki.test.raw. Didn''t run EXL2 yet.</p>

          '
        raw: aurelian-v0.5-70b-rope8-32K.Q5_K_M.gguf (no imatrix) got 4.98 PPL @ 512
          ctx on wiki.test.raw. Didn't run EXL2 yet.
        updatedAt: '2024-01-18T06:45:17.514Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65a8c8fd1012559732bb94e7
    type: comment
  author: grimulkan
  content: aurelian-v0.5-70b-rope8-32K.Q5_K_M.gguf (no imatrix) got 4.98 PPL @ 512
    ctx on wiki.test.raw. Didn't run EXL2 yet.
  created_at: 2024-01-18 06:45:17+00:00
  edited: false
  hidden: false
  id: 65a8c8fd1012559732bb94e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-18T18:17:22.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7722805142402649
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>An useful ongoing thread on the LlamaCPP Github for our iMatrix
          problem :<br><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/discussions/5006">https://github.com/ggerganov/llama.cpp/discussions/5006</a></p>

          <p>And another on Reddit :<br><a rel="nofollow" href="https://www.reddit.com/r/LocalLLaMA/comments/1993iro/comment/kifils5/?context=3">https://www.reddit.com/r/LocalLLaMA/comments/1993iro/comment/kifils5/?context=3</a></p>

          '
        raw: 'An useful ongoing thread on the LlamaCPP Github for our iMatrix problem
          :

          https://github.com/ggerganov/llama.cpp/discussions/5006


          And another on Reddit :

          https://www.reddit.com/r/LocalLLaMA/comments/1993iro/comment/kifils5/?context=3'
        updatedAt: '2024-01-18T18:19:45.934Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - grimulkan
    id: 65a96b3250f85a33ed6c897c
    type: comment
  author: Nexesenex
  content: 'An useful ongoing thread on the LlamaCPP Github for our iMatrix problem
    :

    https://github.com/ggerganov/llama.cpp/discussions/5006


    And another on Reddit :

    https://www.reddit.com/r/LocalLLaMA/comments/1993iro/comment/kifils5/?context=3'
  created_at: 2024-01-18 18:17:22+00:00
  edited: true
  hidden: false
  id: 65a96b3250f85a33ed6c897c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
      fullname: Grimulkan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: grimulkan
      type: user
    createdAt: '2024-01-23T18:53:57.000Z'
    data:
      edited: false
      editors:
      - grimulkan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9091833829879761
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/636d6fceb1079bc9b2475e7e/VefNYYXAg4xfwSEF-xL1Y.png?w=200&h=200&f=face
          fullname: Grimulkan
          isHf: false
          isPro: false
          name: grimulkan
          type: user
        html: '<p>Just a heads up: I''m going to rename things around in the GGUF
          repo to make it less confusing for end-users, based on the results so far.</p>

          <p>For Q3_K_*, I will make the non-imatrix the default.<br>For Q2_K, not
          sure. Probably will make the wikitext imatrix the default (it is more random).<br>For
          IQ2_XS , we didn''t try it, but the existing specialized imatrix test seemed
          terrible from your results (48 hellaswag), so I''ll probably try with wikitext
          imatrix. Anything is better than the current one.</p>

          '
        raw: 'Just a heads up: I''m going to rename things around in the GGUF repo
          to make it less confusing for end-users, based on the results so far.


          For Q3_K_*, I will make the non-imatrix the default.

          For Q2_K, not sure. Probably will make the wikitext imatrix the default
          (it is more random).

          For IQ2_XS , we didn''t try it, but the existing specialized imatrix test
          seemed terrible from your results (48 hellaswag), so I''ll probably try
          with wikitext imatrix. Anything is better than the current one.'
        updatedAt: '2024-01-23T18:53:57.655Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65b00b4529ae836e9ed2520a
    type: comment
  author: grimulkan
  content: 'Just a heads up: I''m going to rename things around in the GGUF repo to
    make it less confusing for end-users, based on the results so far.


    For Q3_K_*, I will make the non-imatrix the default.

    For Q2_K, not sure. Probably will make the wikitext imatrix the default (it is
    more random).

    For IQ2_XS , we didn''t try it, but the existing specialized imatrix test seemed
    terrible from your results (48 hellaswag), so I''ll probably try with wikitext
    imatrix. Anything is better than the current one.'
  created_at: 2024-01-23 18:53:57+00:00
  edited: false
  hidden: false
  id: 65b00b4529ae836e9ed2520a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-23T21:10:43.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9554620981216431
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>I will give you new results tomorrow, some hellaswags are not correct
          and llamacpp has been fixed since.<br>I just need to rerun the weird ones.</p>

          <p>Actually, some ran already (PEC : linear rope) :</p>

          <ul>

          <li>aurelian-v0.5-70b-rope8-32K.IQ2_XS.gguf,-,hellaswag,80.25,,400,,PEC8</li>

          <li>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,79.25,,400,,PEC4,</li>

          <li>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,80.25,,400,,PEC8,</li>

          <li>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,hellaswag,80.25,,400,,PEC4,</li>

          <li>aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,hellaswag,79.75,,400,,PEC8,</li>

          </ul>

          <p>I deleted already the non-imatrix models, because they bench less good
          overall on PPL. Your model is relatively fine on Hellaswag, even if I get
          better results with WinterGoddess 32k (maybe contamination is involved in
          one of the models merged in WG).</p>

          <p>I made also more experiences on different models and what I observe is
          that wikitest is the most consistent for iMatrix, up to Q3_K_S.<br>It always
          lowers the PPL (a goal per-se), and the Hellaswag (with corrected Llama
          CPP) do not really change beyond the margin of error from a matrix to another
          or even without, and the best hellaswag you can get in IQ2/Q2 overlap easily
          with the lowest you can get in Q3. Your model doesn''t behave differently</p>

          <p><a href="https://huggingface.co/ikawrakow/various-2bit-sota-gguf">Iwankrawk</a>
          are the best quants available from a griven size, and it''s normal : he''s
          the iMatrix developper on LlamaCPP.</p>

          <p>My own settings (I lack of compute power) is wikitext train ctx 32 and
          2500 chunks (but even 32ctx/25chunks give decent results, lol).</p>

          <p>Artefact2 uses wikitext train ctx 512 and 2000 chunks, and these settings
          are a little bit better of course.</p>

          <p>Best settings are probably those of  Iwan, tho. I asked him what he used
          on his model repo.</p>

          '
        raw: 'I will give you new results tomorrow, some hellaswags are not correct
          and llamacpp has been fixed since.

          I just need to rerun the weird ones.


          Actually, some ran already (PEC : linear rope) :


          - aurelian-v0.5-70b-rope8-32K.IQ2_XS.gguf,-,hellaswag,80.25,,400,,PEC8

          - aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,79.25,,400,,PEC4,

          - aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,80.25,,400,,PEC8,

          - aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,hellaswag,80.25,,400,,PEC4,

          - aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,hellaswag,79.75,,400,,PEC8,


          I deleted already the non-imatrix models, because they bench less good overall
          on PPL. Your model is relatively fine on Hellaswag, even if I get better
          results with WinterGoddess 32k (maybe contamination is involved in one of
          the models merged in WG).


          I made also more experiences on different models and what I observe is that
          wikitest is the most consistent for iMatrix, up to Q3_K_S.

          It always lowers the PPL (a goal per-se), and the Hellaswag (with corrected
          Llama CPP) do not really change beyond the margin of error from a matrix
          to another or even without, and the best hellaswag you can get in IQ2/Q2
          overlap easily with the lowest you can get in Q3. Your model doesn''t behave
          differently


          [Iwankrawk](https://huggingface.co/ikawrakow/various-2bit-sota-gguf) are
          the best quants available from a griven size, and it''s normal : he''s the
          iMatrix developper on LlamaCPP.


          My own settings (I lack of compute power) is wikitext train ctx 32 and 2500
          chunks (but even 32ctx/25chunks give decent results, lol).


          Artefact2 uses wikitext train ctx 512 and 2000 chunks, and these settings
          are a little bit better of course.


          Best settings are probably those of  Iwan, tho. I asked him what he used
          on his model repo.'
        updatedAt: '2024-01-23T21:38:01.468Z'
      numEdits: 3
      reactions: []
    id: 65b02b539205404b1a512f76
    type: comment
  author: Nexesenex
  content: 'I will give you new results tomorrow, some hellaswags are not correct
    and llamacpp has been fixed since.

    I just need to rerun the weird ones.


    Actually, some ran already (PEC : linear rope) :


    - aurelian-v0.5-70b-rope8-32K.IQ2_XS.gguf,-,hellaswag,80.25,,400,,PEC8

    - aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,79.25,,400,,PEC4,

    - aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.gguf,-,hellaswag,80.25,,400,,PEC8,

    - aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,hellaswag,80.25,,400,,PEC4,

    - aurelian-v0.5-70b-rope8-32K.Q2_K-2.95bpw.wiki_imatrix.gguf,-,hellaswag,79.75,,400,,PEC8,


    I deleted already the non-imatrix models, because they bench less good overall
    on PPL. Your model is relatively fine on Hellaswag, even if I get better results
    with WinterGoddess 32k (maybe contamination is involved in one of the models merged
    in WG).


    I made also more experiences on different models and what I observe is that wikitest
    is the most consistent for iMatrix, up to Q3_K_S.

    It always lowers the PPL (a goal per-se), and the Hellaswag (with corrected Llama
    CPP) do not really change beyond the margin of error from a matrix to another
    or even without, and the best hellaswag you can get in IQ2/Q2 overlap easily with
    the lowest you can get in Q3. Your model doesn''t behave differently


    [Iwankrawk](https://huggingface.co/ikawrakow/various-2bit-sota-gguf) are the best
    quants available from a griven size, and it''s normal : he''s the iMatrix developper
    on LlamaCPP.


    My own settings (I lack of compute power) is wikitext train ctx 32 and 2500 chunks
    (but even 32ctx/25chunks give decent results, lol).


    Artefact2 uses wikitext train ctx 512 and 2000 chunks, and these settings are
    a little bit better of course.


    Best settings are probably those of  Iwan, tho. I asked him what he used on his
    model repo.'
  created_at: 2024-01-23 21:10:43+00:00
  edited: true
  hidden: false
  id: 65b02b539205404b1a512f76
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: grimulkan/aurelian-alpha0.1-70b-rope8-32K-fp16
repo_type: model
status: open
target_branch: null
title: 2 bits GGUF SOTA quants?
