!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gardner
conflicting_files: null
created_at: 2024-01-07 00:36:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
      fullname: Gardner Bickford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gardner
      type: user
    createdAt: '2024-01-07T00:36:13.000Z'
    data:
      edited: false
      editors:
      - gardner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8828373551368713
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
          fullname: Gardner Bickford
          isHf: false
          isPro: false
          name: gardner
          type: user
        html: '<p>Thank you for publishing these weights. The base model is impressive.
          I am keen to try out the instruct tuned model.</p>

          <p>What is the prompt format?</p>

          <p>CodeLlama 7B Instruct, for example, uses a prompt similar to:</p>

          <pre><code>[INST] Write code to solve the following coding problem that
          obeys the constraints and passes the example test cases. Please wrap your
          code answer using ```:

          {prompt}

          [/INST]

          </code></pre>

          '
        raw: "Thank you for publishing these weights. The base model is impressive.\
          \ I am keen to try out the instruct tuned model.\r\n\r\nWhat is the prompt\
          \ format?\r\n\r\nCodeLlama 7B Instruct, for example, uses a prompt similar\
          \ to:\r\n\r\n```\r\n[INST] Write code to solve the following coding problem\
          \ that obeys the constraints and passes the example test cases. Please wrap\
          \ your code answer using ```:\r\n{prompt}\r\n[/INST]\r\n```\r\n"
        updatedAt: '2024-01-07T00:36:13.225Z'
      numEdits: 0
      reactions: []
    id: 6599f1fd0c972f4c77d531ed
    type: comment
  author: gardner
  content: "Thank you for publishing these weights. The base model is impressive.\
    \ I am keen to try out the instruct tuned model.\r\n\r\nWhat is the prompt format?\r\
    \n\r\nCodeLlama 7B Instruct, for example, uses a prompt similar to:\r\n\r\n```\r\
    \n[INST] Write code to solve the following coding problem that obeys the constraints\
    \ and passes the example test cases. Please wrap your code answer using ```:\r\
    \n{prompt}\r\n[/INST]\r\n```\r\n"
  created_at: 2024-01-07 00:36:13+00:00
  edited: false
  hidden: false
  id: 6599f1fd0c972f4c77d531ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7faf8c6f71fc318a0113d780d376c381.svg
      fullname: Wu Chengyue
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: WuChengyue
      type: user
    createdAt: '2024-01-07T01:16:25.000Z'
    data:
      edited: false
      editors:
      - WuChengyue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7532126307487488
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7faf8c6f71fc318a0113d780d376c381.svg
          fullname: Wu Chengyue
          isHf: false
          isPro: false
          name: WuChengyue
          type: user
        html: '<p>Thanks for your comment! I follow the format of tulu in the instruction
          tuning. Here is the codebase I use for the instruction tuning <a rel="nofollow"
          href="https://github.com/allenai/open-instruct">https://github.com/allenai/open-instruct</a>.
          We will release our code of instruction tuning later in the GitHub. The
          specific code for instruction data processing is here <a rel="nofollow"
          href="https://github.com/allenai/open-instruct/blob/9ebcb582cfc243a6dab75b4302fa432784db26c2/open_instruct/finetune.py#L273">https://github.com/allenai/open-instruct/blob/9ebcb582cfc243a6dab75b4302fa432784db26c2/open_instruct/finetune.py#L273</a>.
          Here is the format tulu used for your reference. </p>

          <pre><code>&lt;|user|&gt;

          Your message here!

          &lt;|assistant|&gt;

          </code></pre>

          <p>For best results, format all inputs in this manner. <strong>Make sure
          to include a newline after <code>&lt;|assistant|&gt;</code>, this can affect
          generation quality quite a bit.</strong></p>

          '
        raw: "Thanks for your comment! I follow the format of tulu in the instruction\
          \ tuning. Here is the codebase I use for the instruction tuning https://github.com/allenai/open-instruct.\
          \ We will release our code of instruction tuning later in the GitHub. The\
          \ specific code for instruction data processing is here https://github.com/allenai/open-instruct/blob/9ebcb582cfc243a6dab75b4302fa432784db26c2/open_instruct/finetune.py#L273.\
          \ Here is the format tulu used for your reference. \n\n```\n<|user|>\nYour\
          \ message here!\n<|assistant|>\n```\n\nFor best results, format all inputs\
          \ in this manner. **Make sure to include a newline after `<|assistant|>`,\
          \ this can affect generation quality quite a bit.**\n\n"
        updatedAt: '2024-01-07T01:16:25.382Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - gardner
        - roffmonster
        - joaobatista
        - tshrjn
        - tatsuyashirakawa
    id: 6599fb6928676374f3a25055
    type: comment
  author: WuChengyue
  content: "Thanks for your comment! I follow the format of tulu in the instruction\
    \ tuning. Here is the codebase I use for the instruction tuning https://github.com/allenai/open-instruct.\
    \ We will release our code of instruction tuning later in the GitHub. The specific\
    \ code for instruction data processing is here https://github.com/allenai/open-instruct/blob/9ebcb582cfc243a6dab75b4302fa432784db26c2/open_instruct/finetune.py#L273.\
    \ Here is the format tulu used for your reference. \n\n```\n<|user|>\nYour message\
    \ here!\n<|assistant|>\n```\n\nFor best results, format all inputs in this manner.\
    \ **Make sure to include a newline after `<|assistant|>`, this can affect generation\
    \ quality quite a bit.**\n\n"
  created_at: 2024-01-07 01:16:25+00:00
  edited: false
  hidden: false
  id: 6599fb6928676374f3a25055
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1338458e053dfa374fceb739d6549654.svg
      fullname: sean
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sean-public
      type: user
    createdAt: '2024-01-07T01:34:20.000Z'
    data:
      edited: true
      editors:
      - sean-public
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9192178249359131
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1338458e053dfa374fceb739d6549654.svg
          fullname: sean
          isHf: false
          isPro: false
          name: sean-public
          type: user
        html: '<p>Having so many prompt formats is making it difficult to create high
          quality tooling that can work with multiple models for both further training
          and inferencing. Unlike some of the other parameters and specifications
          on a model, like <code>num_hidden_layers = 40</code> that can commonly be
          found in the supplied <code>config.json</code>, we are left reading the
          model card, README, or reading papers and code to figure out the appropriate
          format.</p>

          <blockquote>

          <p>Make sure to include a newline after &lt;|assistant|&gt;, this can affect
          generation quality quite a bit.</p>

          </blockquote>

          <p>It''s frustrating to roll out support for this great-looking new model
          with the extra effort of adopting yet another uncommon format and maintaining
          that mapping (LLaMA-Pro -&gt; Allen AI tulu format) in each of our tools
          to be sure our users have the best experience with it. If the prompt format
          is off by a little or the tooling makes heavy use of "system prompts", we
          will see degraded performance until we figure out a workaround for each
          different format there''s demand for (and there has been no demand for this
          format until today).</p>

          <p>How do you choose instruct formats?</p>

          '
        raw: 'Having so many prompt formats is making it difficult to create high
          quality tooling that can work with multiple models for both further training
          and inferencing. Unlike some of the other parameters and specifications
          on a model, like `num_hidden_layers = 40` that can commonly be found in
          the supplied `config.json`, we are left reading the model card, README,
          or reading papers and code to figure out the appropriate format.


          > Make sure to include a newline after <|assistant|>, this can affect generation
          quality quite a bit.


          It''s frustrating to roll out support for this great-looking new model with
          the extra effort of adopting yet another uncommon format and maintaining
          that mapping (LLaMA-Pro -> Allen AI tulu format) in each of our tools to
          be sure our users have the best experience with it. If the prompt format
          is off by a little or the tooling makes heavy use of "system prompts", we
          will see degraded performance until we figure out a workaround for each
          different format there''s demand for (and there has been no demand for this
          format until today).


          How do you choose instruct formats?'
        updatedAt: '2024-01-07T01:38:52.410Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Yanniedog
    id: 6599ff9cc6457161cac9ee46
    type: comment
  author: sean-public
  content: 'Having so many prompt formats is making it difficult to create high quality
    tooling that can work with multiple models for both further training and inferencing.
    Unlike some of the other parameters and specifications on a model, like `num_hidden_layers
    = 40` that can commonly be found in the supplied `config.json`, we are left reading
    the model card, README, or reading papers and code to figure out the appropriate
    format.


    > Make sure to include a newline after <|assistant|>, this can affect generation
    quality quite a bit.


    It''s frustrating to roll out support for this great-looking new model with the
    extra effort of adopting yet another uncommon format and maintaining that mapping
    (LLaMA-Pro -> Allen AI tulu format) in each of our tools to be sure our users
    have the best experience with it. If the prompt format is off by a little or the
    tooling makes heavy use of "system prompts", we will see degraded performance
    until we figure out a workaround for each different format there''s demand for
    (and there has been no demand for this format until today).


    How do you choose instruct formats?'
  created_at: 2024-01-07 01:34:20+00:00
  edited: true
  hidden: false
  id: 6599ff9cc6457161cac9ee46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
      fullname: Gardner Bickford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gardner
      type: user
    createdAt: '2024-01-07T04:52:23.000Z'
    data:
      edited: true
      editors:
      - gardner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7032604813575745
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
          fullname: Gardner Bickford
          isHf: false
          isPro: false
          name: gardner
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;WuChengyue&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/WuChengyue\"\
          >@<span class=\"underline\">WuChengyue</span></a></span>\n\n\t</span></span>\
          \ \U0001F64F</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;sean-public&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sean-public\"\
          >@<span class=\"underline\">sean-public</span></a></span>\n\n\t</span></span><br>The\
          \ formats will always be diverse. There is some work being done to include\
          \ it in configuration files. See <a href=\"https://huggingface.co/docs/transformers/main/chat_templating\"\
          >Templates for Chat Models\n</a>.</p>\n<p>An example is <code>chat_template</code>\
          \ included in the <a href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/blob/d2953fb360de291c66caf55739473ca4b72eca0b/tokenizer_config.json#L32\"\
          >tokenizer_config.json</a> of <code>Mistral-7B-Instruct-v0.1</code></p>\n\
          <p>Some libraries are beginning to support it as well: <a rel=\"nofollow\"\
          \ href=\"https://github.com/abetlen/llama-cpp-python/pull/790\">https://github.com/abetlen/llama-cpp-python/pull/790</a></p>\n\
          <p>Allen AI has one tokenizer_config.json that <a href=\"https://huggingface.co/allenai/tulu-2-dpo-70b/blob/f33beddfdbbc2ccb4e349f71f515aa3ad983d49b/tokenizer_config.json#L35\"\
          >includes a chat_template</a>.</p>\n<p>You can see that in action by running\
          \ the following python code:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"allenai/tulu-2-dpo-70b\"</span>, legacy=<span\
          \ class=\"hljs-literal\">False</span>)\n\nchat = [\n   {<span class=\"hljs-string\"\
          >\"role\"</span>: <span class=\"hljs-string\">\"user\"</span>, <span class=\"\
          hljs-string\">\"content\"</span>: <span class=\"hljs-string\">\"Hello, how\
          \ are you?\"</span>},\n   {<span class=\"hljs-string\">\"role\"</span>:\
          \ <span class=\"hljs-string\">\"assistant\"</span>, <span class=\"hljs-string\"\
          >\"content\"</span>: <span class=\"hljs-string\">\"I'm doing great. How\
          \ can I help you today?\"</span>},\n   {<span class=\"hljs-string\">\"role\"\
          </span>: <span class=\"hljs-string\">\"user\"</span>, <span class=\"hljs-string\"\
          >\"content\"</span>: <span class=\"hljs-string\">\"I'd like to show off\
          \ how chat templating works!\"</span>},\n   {<span class=\"hljs-string\"\
          >\"role\"</span>: <span class=\"hljs-string\">\"assistant\"</span>, <span\
          \ class=\"hljs-string\">\"content\"</span>: <span class=\"hljs-string\"\
          >\"Great, please let me know if I can help.\"</span>},\n]\n\n<span class=\"\
          hljs-built_in\">print</span>(tokenizer.apply_chat_template(chat, tokenize=<span\
          \ class=\"hljs-literal\">False</span>))\n</code></pre>\n<p>Which outputs:</p>\n\
          <pre><code>$ python3 main.py \n&lt;|user|&gt;\nHello, how are you?\n&lt;|assistant|&gt;\n\
          I'm doing great. How can I help you today?&lt;/s&gt;\n&lt;|user|&gt;\nI'd\
          \ like to show off how chat templating works!\n&lt;|assistant|&gt;\nGreat,\
          \ please let me know if I can help.&lt;/s&gt;\n</code></pre>\n"
        raw: "Thank you @WuChengyue \U0001F64F\n\n\n@sean-public \nThe formats will\
          \ always be diverse. There is some work being done to include it in configuration\
          \ files. See [Templates for Chat Models\n](https://huggingface.co/docs/transformers/main/chat_templating).\n\
          \nAn example is `chat_template` included in the [tokenizer_config.json](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/blob/d2953fb360de291c66caf55739473ca4b72eca0b/tokenizer_config.json#L32)\
          \ of `Mistral-7B-Instruct-v0.1`\n\nSome libraries are beginning to support\
          \ it as well: https://github.com/abetlen/llama-cpp-python/pull/790\n\n\n\
          Allen AI has one tokenizer_config.json that [includes a chat_template](https://huggingface.co/allenai/tulu-2-dpo-70b/blob/f33beddfdbbc2ccb4e349f71f515aa3ad983d49b/tokenizer_config.json#L35).\n\
          \nYou can see that in action by running the following python code:\n\n```python\n\
          from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"\
          allenai/tulu-2-dpo-70b\", legacy=False)\n\nchat = [\n   {\"role\": \"user\"\
          , \"content\": \"Hello, how are you?\"},\n   {\"role\": \"assistant\", \"\
          content\": \"I'm doing great. How can I help you today?\"},\n   {\"role\"\
          : \"user\", \"content\": \"I'd like to show off how chat templating works!\"\
          },\n   {\"role\": \"assistant\", \"content\": \"Great, please let me know\
          \ if I can help.\"},\n]\n\nprint(tokenizer.apply_chat_template(chat, tokenize=False))\n\
          ```\n\nWhich outputs:\n\n```\n$ python3 main.py \n<|user|>\nHello, how are\
          \ you?\n<|assistant|>\nI'm doing great. How can I help you today?</s>\n\
          <|user|>\nI'd like to show off how chat templating works!\n<|assistant|>\n\
          Great, please let me know if I can help.</s>\n\n```"
        updatedAt: '2024-01-07T05:07:20.009Z'
      numEdits: 4
      reactions: []
    id: 659a2e076d20ab21b0511a01
    type: comment
  author: gardner
  content: "Thank you @WuChengyue \U0001F64F\n\n\n@sean-public \nThe formats will\
    \ always be diverse. There is some work being done to include it in configuration\
    \ files. See [Templates for Chat Models\n](https://huggingface.co/docs/transformers/main/chat_templating).\n\
    \nAn example is `chat_template` included in the [tokenizer_config.json](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/blob/d2953fb360de291c66caf55739473ca4b72eca0b/tokenizer_config.json#L32)\
    \ of `Mistral-7B-Instruct-v0.1`\n\nSome libraries are beginning to support it\
    \ as well: https://github.com/abetlen/llama-cpp-python/pull/790\n\n\nAllen AI\
    \ has one tokenizer_config.json that [includes a chat_template](https://huggingface.co/allenai/tulu-2-dpo-70b/blob/f33beddfdbbc2ccb4e349f71f515aa3ad983d49b/tokenizer_config.json#L35).\n\
    \nYou can see that in action by running the following python code:\n\n```python\n\
    from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"\
    allenai/tulu-2-dpo-70b\", legacy=False)\n\nchat = [\n   {\"role\": \"user\", \"\
    content\": \"Hello, how are you?\"},\n   {\"role\": \"assistant\", \"content\"\
    : \"I'm doing great. How can I help you today?\"},\n   {\"role\": \"user\", \"\
    content\": \"I'd like to show off how chat templating works!\"},\n   {\"role\"\
    : \"assistant\", \"content\": \"Great, please let me know if I can help.\"},\n\
    ]\n\nprint(tokenizer.apply_chat_template(chat, tokenize=False))\n```\n\nWhich\
    \ outputs:\n\n```\n$ python3 main.py \n<|user|>\nHello, how are you?\n<|assistant|>\n\
    I'm doing great. How can I help you today?</s>\n<|user|>\nI'd like to show off\
    \ how chat templating works!\n<|assistant|>\nGreat, please let me know if I can\
    \ help.</s>\n\n```"
  created_at: 2024-01-07 04:52:23+00:00
  edited: true
  hidden: false
  id: 659a2e076d20ab21b0511a01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
      fullname: Gardner Bickford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gardner
      type: user
    createdAt: '2024-01-07T05:19:20.000Z'
    data:
      edited: false
      editors:
      - gardner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8579725623130798
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
          fullname: Gardner Bickford
          isHf: false
          isPro: false
          name: gardner
          type: user
        html: "<p>If we run the same code for LLaMa-Pro-8B-Instruct, we can see:</p>\n\
          <pre><code class=\"language-python\">$ python3 main.py \nNo chat template\
          \ <span class=\"hljs-keyword\">is</span> defined <span class=\"hljs-keyword\"\
          >for</span> this tokenizer - using the default template <span class=\"hljs-keyword\"\
          >for</span> the LlamaTokenizerFast <span class=\"hljs-keyword\">class</span>.\
          \ If the default <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\"\
          >not</span> appropriate <span class=\"hljs-keyword\">for</span> your model,\
          \ please <span class=\"hljs-built_in\">set</span> `tokenizer.chat_template`\
          \ to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating\
          \ <span class=\"hljs-keyword\">for</span> more information.\n\n&lt;s&gt;[INST]\
          \ &lt;&lt;SYS&gt;&gt;\nYou are a helpful, respectful <span class=\"hljs-keyword\"\
          >and</span> honest assistant. Always answer <span class=\"hljs-keyword\"\
          >as</span> helpfully <span class=\"hljs-keyword\">as</span> possible, <span\
          \ class=\"hljs-keyword\">while</span> being safe. Your answers should <span\
          \ class=\"hljs-keyword\">not</span> include <span class=\"hljs-built_in\"\
          >any</span> harmful, unethical, racist, sexist, toxic, dangerous, <span\
          \ class=\"hljs-keyword\">or</span> illegal content. Please ensure that your\
          \ responses are socially unbiased <span class=\"hljs-keyword\">and</span>\
          \ positive <span class=\"hljs-keyword\">in</span> nature.\n\nIf a question\
          \ does <span class=\"hljs-keyword\">not</span> make <span class=\"hljs-built_in\"\
          >any</span> sense, <span class=\"hljs-keyword\">or</span> <span class=\"\
          hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> factually\
          \ coherent, explain why instead of answering something <span class=\"hljs-keyword\"\
          >not</span> correct. If you don<span class=\"hljs-string\">'t know the answer\
          \ to a question, please don'</span>t share false information.\n&lt;&lt;/SYS&gt;&gt;\n\
          \nHello, how are you? [/INST] I<span class=\"hljs-string\">'m doing great.\
          \ How can I help you today? &lt;/s&gt;&lt;s&gt;[INST] I'</span>d like to\
          \ show off how chat templating works! [/INST] Great, please let me know\
          \ <span class=\"hljs-keyword\">if</span> I can <span class=\"hljs-built_in\"\
          >help</span>. &lt;/s&gt;\n</code></pre>\n<p>if we modify the tokenizer to\
          \ use a <code>chat_template</code>, we can see the difference:</p>\n<pre><code\
          \ class=\"language-diff\">from transformers import AutoTokenizer\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"TencentARC/LLaMA-Pro-8B-Instruct\",\
          \ legacy=False)\n\n<span class=\"hljs-addition\">+ tokenizer.chat_template\
          \ = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\\
          n{{ '&lt;|user|&gt;\\n' + message['content'] }}\\n{% elif message['role']\
          \ == 'assistant' %}\\n{{ '&lt;|assistant|&gt;\\n'  + message['content']\
          \ + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt\
          \ %}\\n{{ '&lt;|assistant|&gt;' }}\\n{% endif %}\\n{% endfor %}\"</span>\n\
          \nchat = [\n   {\"role\": \"user\", \"content\": \"Hello, how are you?\"\
          },\n   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can\
          \ I help you today?\"},\n   {\"role\": \"user\", \"content\": \"I'd like\
          \ to show off how chat templating works!\"},\n   {\"role\": \"assistant\"\
          , \"content\": \"Great, please let me know if I can help.\"},\n]\n\nprint(tokenizer.apply_chat_template(chat,\
          \ tokenize=False))\n</code></pre>\n<p>Which outputs:</p>\n<pre><code>$ python3\
          \ main.py \n&lt;|user|&gt;\nHello, how are you?\n&lt;|assistant|&gt;\nI'm\
          \ doing great. How can I help you today?&lt;/s&gt;\n&lt;|user|&gt;\nI'd\
          \ like to show off how chat templating works!\n&lt;|assistant|&gt;\nGreat,\
          \ please let me know if I can help.&lt;/s&gt;\n</code></pre>\n"
        raw: "If we run the same code for LLaMa-Pro-8B-Instruct, we can see:\n```python\n\
          $ python3 main.py \nNo chat template is defined for this tokenizer - using\
          \ the default template for the LlamaTokenizerFast class. If the default\
          \ is not appropriate for your model, please set `tokenizer.chat_template`\
          \ to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating\
          \ for more information.\n\n<s>[INST] <<SYS>>\nYou are a helpful, respectful\
          \ and honest assistant. Always answer as helpfully as possible, while being\
          \ safe. Your answers should not include any harmful, unethical, racist,\
          \ sexist, toxic, dangerous, or illegal content. Please ensure that your\
          \ responses are socially unbiased and positive in nature.\n\nIf a question\
          \ does not make any sense, or is not factually coherent, explain why instead\
          \ of answering something not correct. If you don't know the answer to a\
          \ question, please don't share false information.\n<</SYS>>\n\nHello, how\
          \ are you? [/INST] I'm doing great. How can I help you today? </s><s>[INST]\
          \ I'd like to show off how chat templating works! [/INST] Great, please\
          \ let me know if I can help. </s>\n```\n\nif we modify the tokenizer to\
          \ use a `chat_template`, we can see the difference:\n```diff\nfrom transformers\
          \ import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"TencentARC/LLaMA-Pro-8B-Instruct\"\
          , legacy=False)\n\n+ tokenizer.chat_template = \"{% for message in messages\
          \ %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content']\
          \ }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'\
          \  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and\
          \ add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor\
          \ %}\"\n\nchat = [\n   {\"role\": \"user\", \"content\": \"Hello, how are\
          \ you?\"},\n   {\"role\": \"assistant\", \"content\": \"I'm doing great.\
          \ How can I help you today?\"},\n   {\"role\": \"user\", \"content\": \"\
          I'd like to show off how chat templating works!\"},\n   {\"role\": \"assistant\"\
          , \"content\": \"Great, please let me know if I can help.\"},\n]\n\nprint(tokenizer.apply_chat_template(chat,\
          \ tokenize=False))\n```\n\nWhich outputs:\n\n```\n$ python3 main.py \n<|user|>\n\
          Hello, how are you?\n<|assistant|>\nI'm doing great. How can I help you\
          \ today?</s>\n<|user|>\nI'd like to show off how chat templating works!\n\
          <|assistant|>\nGreat, please let me know if I can help.</s>\n\n```\n"
        updatedAt: '2024-01-07T05:19:20.678Z'
      numEdits: 0
      reactions: []
    id: 659a3458acaab7bec36c38cb
    type: comment
  author: gardner
  content: "If we run the same code for LLaMa-Pro-8B-Instruct, we can see:\n```python\n\
    $ python3 main.py \nNo chat template is defined for this tokenizer - using the\
    \ default template for the LlamaTokenizerFast class. If the default is not appropriate\
    \ for your model, please set `tokenizer.chat_template` to an appropriate template.\
    \ See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n\
    \n<s>[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always\
    \ answer as helpfully as possible, while being safe. Your answers should not include\
    \ any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\
    \ Please ensure that your responses are socially unbiased and positive in nature.\n\
    \nIf a question does not make any sense, or is not factually coherent, explain\
    \ why instead of answering something not correct. If you don't know the answer\
    \ to a question, please don't share false information.\n<</SYS>>\n\nHello, how\
    \ are you? [/INST] I'm doing great. How can I help you today? </s><s>[INST] I'd\
    \ like to show off how chat templating works! [/INST] Great, please let me know\
    \ if I can help. </s>\n```\n\nif we modify the tokenizer to use a `chat_template`,\
    \ we can see the difference:\n```diff\nfrom transformers import AutoTokenizer\n\
    tokenizer = AutoTokenizer.from_pretrained(\"TencentARC/LLaMA-Pro-8B-Instruct\"\
    , legacy=False)\n\n+ tokenizer.chat_template = \"{% for message in messages %}\\\
    n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] }}\\\
    n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content']\
    \ + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\\
    n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n\nchat = [\n   {\"role\"\
    : \"user\", \"content\": \"Hello, how are you?\"},\n   {\"role\": \"assistant\"\
    , \"content\": \"I'm doing great. How can I help you today?\"},\n   {\"role\"\
    : \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n\
    \   {\"role\": \"assistant\", \"content\": \"Great, please let me know if I can\
    \ help.\"},\n]\n\nprint(tokenizer.apply_chat_template(chat, tokenize=False))\n\
    ```\n\nWhich outputs:\n\n```\n$ python3 main.py \n<|user|>\nHello, how are you?\n\
    <|assistant|>\nI'm doing great. How can I help you today?</s>\n<|user|>\nI'd like\
    \ to show off how chat templating works!\n<|assistant|>\nGreat, please let me\
    \ know if I can help.</s>\n\n```\n"
  created_at: 2024-01-07 05:19:20+00:00
  edited: false
  hidden: false
  id: 659a3458acaab7bec36c38cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
      fullname: Gardner Bickford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gardner
      type: user
    createdAt: '2024-01-07T10:13:10.000Z'
    data:
      edited: false
      editors:
      - gardner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8489351272583008
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
          fullname: Gardner Bickford
          isHf: false
          isPro: false
          name: gardner
          type: user
        html: '<p>A change adding <code>chat_template</code> to the <code>tokenizer_config.json</code>
          has been merged.  Thank you!</p>

          <p>Closing this disussion.</p>

          '
        raw: 'A change adding `chat_template` to the `tokenizer_config.json` has been
          merged.  Thank you!


          Closing this disussion.'
        updatedAt: '2024-01-07T10:13:10.270Z'
      numEdits: 0
      reactions: []
      relatedEventId: 659a7936f82b15d6635488fd
    id: 659a7936f82b15d6635488fb
    type: comment
  author: gardner
  content: 'A change adding `chat_template` to the `tokenizer_config.json` has been
    merged.  Thank you!


    Closing this disussion.'
  created_at: 2024-01-07 10:13:10+00:00
  edited: false
  hidden: false
  id: 659a7936f82b15d6635488fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
      fullname: Gardner Bickford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gardner
      type: user
    createdAt: '2024-01-07T10:13:10.000Z'
    data:
      status: closed
    id: 659a7936f82b15d6635488fd
    type: status-change
  author: gardner
  created_at: 2024-01-07 10:13:10+00:00
  id: 659a7936f82b15d6635488fd
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TencentARC/LLaMA-Pro-8B-Instruct
repo_type: model
status: closed
target_branch: null
title: Prompt Format?
