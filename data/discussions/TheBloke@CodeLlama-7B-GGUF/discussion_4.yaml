!!python/object:huggingface_hub.community.DiscussionWithDetails
author: OceanMind
conflicting_files: null
created_at: 2023-09-28 22:11:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a872652a7e1ac449552b80405449081d.svg
      fullname: SeaHamburg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: OceanMind
      type: user
    createdAt: '2023-09-28T23:11:46.000Z'
    data:
      edited: false
      editors:
      - OceanMind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4377645254135132
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a872652a7e1ac449552b80405449081d.svg
          fullname: SeaHamburg
          isHf: false
          isPro: false
          name: OceanMind
          type: user
        html: '<p>from llama_cpp import Llama<br>print("Loading model...")<br>llm
          = Llama(model_path=''./models/codellama-7b.Q2_K.gguf'')<br>print("Model
          loaded.")<br>output = llm("Q: Name the planets in the solar system? A: ",
          max_tokens=32, stop=["Q:", "\n"], echo=True)<br>print("output", output)</p>

          <hr>

          <p>SeaHamburg@box:<del>/llama$ pip3 show llama_cpp_python<br>Name: llama_cpp_python<br>Version:
          0.2.7<br>Summary: Python bindings for the llama.cpp library<br>Home-page:<br>Author:<br>Author-email:
          Andrei Betlen <a rel="nofollow" href="mailto:abetlen@gmail.com">abetlen@gmail.com</a><br>License:
          MIT<br>Location: /home/SeaHamburg/.local/lib/python3.11/site-packages<br>Requires:
          diskcache, numpy, typing-extensions<br>Required-by:<br>SeaHamburg@box:</del>/llama$
          python3 app.py<br>Loading model...<br>Illegal instruction<br>SeaHamburg@box:~/llama$
          </p>

          <hr>

          <p>i cant find someone else with similar issue online</p>

          '
        raw: "from llama_cpp import Llama\r\nprint(\"Loading model...\")\r\nllm =\
          \ Llama(model_path='./models/codellama-7b.Q2_K.gguf')\r\nprint(\"Model loaded.\"\
          )         \r\noutput = llm(\"Q: Name the planets in the solar system? A:\
          \ \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)\r\nprint(\"output\"\
          , output)\r\n\r\n\r\n____ \r\n\r\nSeaHamburg@box:~/llama$ pip3 show llama_cpp_python\r\
          \nName: llama_cpp_python\r\nVersion: 0.2.7\r\nSummary: Python bindings for\
          \ the llama.cpp library\r\nHome-page: \r\nAuthor: \r\nAuthor-email: Andrei\
          \ Betlen <abetlen@gmail.com>\r\nLicense: MIT\r\nLocation: /home/SeaHamburg/.local/lib/python3.11/site-packages\r\
          \nRequires: diskcache, numpy, typing-extensions\r\nRequired-by: \r\nSeaHamburg@box:~/llama$\
          \ python3 app.py\r\nLoading model...\r\nIllegal instruction\r\nSeaHamburg@box:~/llama$\
          \ \r\n\r\n___ \r\n\r\ni cant find someone else with similar issue online\r\
          \n"
        updatedAt: '2023-09-28T23:11:46.903Z'
      numEdits: 0
      reactions: []
    id: 65160832bc063171986ca712
    type: comment
  author: OceanMind
  content: "from llama_cpp import Llama\r\nprint(\"Loading model...\")\r\nllm = Llama(model_path='./models/codellama-7b.Q2_K.gguf')\r\
    \nprint(\"Model loaded.\")         \r\noutput = llm(\"Q: Name the planets in the\
    \ solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)\r\nprint(\"\
    output\", output)\r\n\r\n\r\n____ \r\n\r\nSeaHamburg@box:~/llama$ pip3 show llama_cpp_python\r\
    \nName: llama_cpp_python\r\nVersion: 0.2.7\r\nSummary: Python bindings for the\
    \ llama.cpp library\r\nHome-page: \r\nAuthor: \r\nAuthor-email: Andrei Betlen\
    \ <abetlen@gmail.com>\r\nLicense: MIT\r\nLocation: /home/SeaHamburg/.local/lib/python3.11/site-packages\r\
    \nRequires: diskcache, numpy, typing-extensions\r\nRequired-by: \r\nSeaHamburg@box:~/llama$\
    \ python3 app.py\r\nLoading model...\r\nIllegal instruction\r\nSeaHamburg@box:~/llama$\
    \ \r\n\r\n___ \r\n\r\ni cant find someone else with similar issue online\r\n"
  created_at: 2023-09-28 22:11:46+00:00
  edited: false
  hidden: false
  id: 65160832bc063171986ca712
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/CodeLlama-7B-GGUF
repo_type: model
status: open
target_branch: null
title: Illegal Instruction Issue on ubuntu 20 server
