!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Asaf-Yehudai
conflicting_files: null
created_at: 2023-04-15 21:34:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg
      fullname: Asaf Yehudai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Asaf-Yehudai
      type: user
    createdAt: '2023-04-15T22:34:21.000Z'
    data:
      edited: true
      editors:
      - Asaf-Yehudai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg
          fullname: Asaf Yehudai
          isHf: false
          isPro: false
          name: Asaf-Yehudai
          type: user
        html: '<p>What is the minimal piece of code I can run to get similar results
          to the demo?</p>

          <p>I started with that, but it is still not there yet. Do you have any idea?</p>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM<br>tokenizer
          = AutoTokenizer.from_pretrained("eachadea/vicuna-13b-1.1")<br>model = AutoModelForCausalLM.from_pretrained("eachadea/vicuna-13b-1.1")<br>system="A
          chat between a curious user and an artificial intelligence assistant. The
          assistant gives helpful, detailed, and polite answers to the user''s questions."</p>

          <p>def text_raper(text):<br>    return system + f"\nUSER: {text}\nASSISTANT:"</p>

          <p>input_text = "I love reading books"<br>input_text = text_raper(input_text)</p>

          <p>input_ids = tokenizer.encode(input_text, return_tensors=''pt'')<br>output
          = model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)<br>response
          = tokenizer.decode(output[0], skip_special_tokens=True)<br>print(response)</p>

          '
        raw: "What is the minimal piece of code I can run to get similar results to\
          \ the demo?\n\nI started with that, but it is still not there yet. Do you\
          \ have any idea?\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          tokenizer = AutoTokenizer.from_pretrained(\"eachadea/vicuna-13b-1.1\")\n\
          model = AutoModelForCausalLM.from_pretrained(\"eachadea/vicuna-13b-1.1\"\
          )\nsystem=\"A chat between a curious user and an artificial intelligence\
          \ assistant. The assistant gives helpful, detailed, and polite answers to\
          \ the user's questions.\"\n\ndef text_raper(text):\n    return system +\
          \ f\"\\nUSER: {text}\\nASSISTANT:\"\n\ninput_text = \"I love reading books\"\
          \ninput_text = text_raper(input_text)\n\ninput_ids = tokenizer.encode(input_text,\
          \ return_tensors='pt')\noutput = model.generate(input_ids, max_length=1024,\
          \ do_sample=True, temperature=0.7)\nresponse = tokenizer.decode(output[0],\
          \ skip_special_tokens=True)\nprint(response)"
        updatedAt: '2023-04-23T10:37:17.184Z'
      numEdits: 2
      reactions: []
    id: 643b266da856622f978e44fc
    type: comment
  author: Asaf-Yehudai
  content: "What is the minimal piece of code I can run to get similar results to\
    \ the demo?\n\nI started with that, but it is still not there yet. Do you have\
    \ any idea?\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
    tokenizer = AutoTokenizer.from_pretrained(\"eachadea/vicuna-13b-1.1\")\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(\"eachadea/vicuna-13b-1.1\")\nsystem=\"\
    A chat between a curious user and an artificial intelligence assistant. The assistant\
    \ gives helpful, detailed, and polite answers to the user's questions.\"\n\ndef\
    \ text_raper(text):\n    return system + f\"\\nUSER: {text}\\nASSISTANT:\"\n\n\
    input_text = \"I love reading books\"\ninput_text = text_raper(input_text)\n\n\
    input_ids = tokenizer.encode(input_text, return_tensors='pt')\noutput = model.generate(input_ids,\
    \ max_length=1024, do_sample=True, temperature=0.7)\nresponse = tokenizer.decode(output[0],\
    \ skip_special_tokens=True)\nprint(response)"
  created_at: 2023-04-15 21:34:21+00:00
  edited: true
  hidden: false
  id: 643b266da856622f978e44fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642b2aa85df44ff24543d8be/mEnbGB_0Flleoa6SWp5b6.jpeg?w=200&h=200&f=face
      fullname: amkdg
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: eachadea
      type: user
    createdAt: '2023-04-21T15:50:54.000Z'
    data:
      edited: false
      editors:
      - eachadea
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642b2aa85df44ff24543d8be/mEnbGB_0Flleoa6SWp5b6.jpeg?w=200&h=200&f=face
          fullname: amkdg
          isHf: false
          isPro: false
          name: eachadea
          type: user
        html: '<p>Aside from using fastchat, you should just copy the parameters the
          demo uses as well as the prompt template.</p>

          '
        raw: Aside from using fastchat, you should just copy the parameters the demo
          uses as well as the prompt template.
        updatedAt: '2023-04-21T15:50:54.677Z'
      numEdits: 0
      reactions: []
    id: 6442b0dedad68e008d0767cb
    type: comment
  author: eachadea
  content: Aside from using fastchat, you should just copy the parameters the demo
    uses as well as the prompt template.
  created_at: 2023-04-21 14:50:54+00:00
  edited: false
  hidden: false
  id: 6442b0dedad68e008d0767cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg
      fullname: Asaf Yehudai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Asaf-Yehudai
      type: user
    createdAt: '2023-04-23T10:39:20.000Z'
    data:
      edited: false
      editors:
      - Asaf-Yehudai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg
          fullname: Asaf Yehudai
          isHf: false
          isPro: false
          name: Asaf-Yehudai
          type: user
        html: '<p>Okay, thanks.<br>Indeed update the prompt now, and I get similar
          results to the demo. (see above)<br>Thanks for uploading the model! :)</p>

          '
        raw: "Okay, thanks. \nIndeed update the prompt now, and I get similar results\
          \ to the demo. (see above)\nThanks for uploading the model! :)"
        updatedAt: '2023-04-23T10:39:20.196Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - eachadea
      relatedEventId: 64450ad8d1460e859d175b68
    id: 64450ad8d1460e859d175b67
    type: comment
  author: Asaf-Yehudai
  content: "Okay, thanks. \nIndeed update the prompt now, and I get similar results\
    \ to the demo. (see above)\nThanks for uploading the model! :)"
  created_at: 2023-04-23 09:39:20+00:00
  edited: false
  hidden: false
  id: 64450ad8d1460e859d175b67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg
      fullname: Asaf Yehudai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Asaf-Yehudai
      type: user
    createdAt: '2023-04-23T10:39:20.000Z'
    data:
      status: closed
    id: 64450ad8d1460e859d175b68
    type: status-change
  author: Asaf-Yehudai
  created_at: 2023-04-23 09:39:20+00:00
  id: 64450ad8d1460e859d175b68
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: eachadea/vicuna-13b-1.1
repo_type: model
status: closed
target_branch: null
title: 'running the model in Python '
