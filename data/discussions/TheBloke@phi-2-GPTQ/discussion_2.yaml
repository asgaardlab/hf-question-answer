!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Pumba2
conflicting_files: null
created_at: 2023-12-15 16:29:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64ac7b4159bd86340458e53d3e30aee2.svg
      fullname: Bumba
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pumba2
      type: user
    createdAt: '2023-12-15T16:29:15.000Z'
    data:
      edited: false
      editors:
      - Pumba2
      hidden: false
      identifiedLanguage:
        language: sh
        probability: 0.8550101518630981
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64ac7b4159bd86340458e53d3e30aee2.svg
          fullname: Bumba
          isHf: false
          isPro: false
          name: Pumba2
          type: user
        html: '<p>...</p>

          '
        raw: '...'
        updatedAt: '2023-12-15T16:29:15.659Z'
      numEdits: 0
      reactions: []
    id: 657c7edbadd154d163e7e0c0
    type: comment
  author: Pumba2
  content: '...'
  created_at: 2023-12-15 16:29:15+00:00
  edited: false
  hidden: false
  id: 657c7edbadd154d163e7e0c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45d50fb8dc44ee2f308b595eaa27cf90.svg
      fullname: G. Campos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: guser
      type: user
    createdAt: '2023-12-15T21:41:17.000Z'
    data:
      edited: false
      editors:
      - guser
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9987578988075256
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45d50fb8dc44ee2f308b595eaa27cf90.svg
          fullname: G. Campos
          isHf: false
          isPro: false
          name: guser
          type: user
        html: '<p>also waiting!</p>

          '
        raw: also waiting!
        updatedAt: '2023-12-15T21:41:17.131Z'
      numEdits: 0
      reactions: []
    id: 657cc7fdaf1698aaac058ee7
    type: comment
  author: guser
  content: also waiting!
  created_at: 2023-12-15 21:41:17+00:00
  edited: false
  hidden: false
  id: 657cc7fdaf1698aaac058ee7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6a5bde4a2ea6ffd1a7552e6821615af.svg
      fullname: Rob
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kroonen
      type: user
    createdAt: '2023-12-16T01:47:10.000Z'
    data:
      edited: false
      editors:
      - kroonen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.42717352509498596
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6a5bde4a2ea6ffd1a7552e6821615af.svg
          fullname: Rob
          isHf: false
          isPro: false
          name: kroonen
          type: user
        html: '<p>When trying to convert with llamacpp I have :</p>

          <p>(sati) rob@Robins-MBP-2 llama.cpp % python3 ./convert.py ../phi-2<br>Loading
          model file ../phi-2/model-00001-of-00002.safetensors<br>Loading model file
          ../phi-2/model-00001-of-00002.safetensors<br>Loading model file ../phi-2/model-00002-of-00002.safetensors<br>Traceback
          (most recent call last):<br>  File "/Users/rob/Downloads/llama.cpp/./convert.py",
          line 1228, in <br>    main()<br>  File "/Users/rob/Downloads/llama.cpp/./convert.py",
          line 1161, in main<br>    model_plus = load_some_model(args.model)<br>                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/rob/Downloads/llama.cpp/./convert.py", line 1078, in load_some_model<br>    model_plus
          = merge_multifile_models(models_plus)<br>                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/rob/Downloads/llama.cpp/./convert.py", line 593, in merge_multifile_models<br>    model
          = merge_sharded([mp.model for mp in models_plus])<br>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/rob/Downloads/llama.cpp/./convert.py", line 572, in merge_sharded<br>    return
          {name: convert(name) for name in names}<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/rob/Downloads/llama.cpp/./convert.py", line 572, in <br>    return
          {name: convert(name) for name in names}<br>                  ^^^^^^^^^^^^^<br>  File
          "/Users/rob/Downloads/llama.cpp/./convert.py", line 547, in convert<br>    lazy_tensors:
          list[LazyTensor] = [model[name] for model in models]<br>                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/rob/Downloads/llama.cpp/./convert.py", line 547, in <br>    lazy_tensors:
          list[LazyTensor] = [model[name] for model in models]<br>                                      ~~~~~^^^^^^<br>KeyError:
          ''transformer.embd.wte.weight''<br>(sati) rob@Robins-MBP-2 llama.cpp % </p>

          '
        raw: "When trying to convert with llamacpp I have :\n\n(sati) rob@Robins-MBP-2\
          \ llama.cpp % python3 ./convert.py ../phi-2\nLoading model file ../phi-2/model-00001-of-00002.safetensors\n\
          Loading model file ../phi-2/model-00001-of-00002.safetensors\nLoading model\
          \ file ../phi-2/model-00002-of-00002.safetensors\nTraceback (most recent\
          \ call last):\n  File \"/Users/rob/Downloads/llama.cpp/./convert.py\", line\
          \ 1228, in <module>\n    main()\n  File \"/Users/rob/Downloads/llama.cpp/./convert.py\"\
          , line 1161, in main\n    model_plus = load_some_model(args.model)\n   \
          \              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/rob/Downloads/llama.cpp/./convert.py\"\
          , line 1078, in load_some_model\n    model_plus = merge_multifile_models(models_plus)\n\
          \                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/rob/Downloads/llama.cpp/./convert.py\"\
          , line 593, in merge_multifile_models\n    model = merge_sharded([mp.model\
          \ for mp in models_plus])\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/Users/rob/Downloads/llama.cpp/./convert.py\", line 572, in merge_sharded\n\
          \    return {name: convert(name) for name in names}\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/Users/rob/Downloads/llama.cpp/./convert.py\", line 572, in <dictcomp>\n\
          \    return {name: convert(name) for name in names}\n                  ^^^^^^^^^^^^^\n\
          \  File \"/Users/rob/Downloads/llama.cpp/./convert.py\", line 547, in convert\n\
          \    lazy_tensors: list[LazyTensor] = [model[name] for model in models]\n\
          \                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/Users/rob/Downloads/llama.cpp/./convert.py\", line 547, in <listcomp>\n\
          \    lazy_tensors: list[LazyTensor] = [model[name] for model in models]\n\
          \                                      ~~~~~^^^^^^\nKeyError: 'transformer.embd.wte.weight'\n\
          (sati) rob@Robins-MBP-2 llama.cpp % "
        updatedAt: '2023-12-16T01:47:10.124Z'
      numEdits: 0
      reactions: []
    id: 657d019ef4f72f2c4cf5d0f7
    type: comment
  author: kroonen
  content: "When trying to convert with llamacpp I have :\n\n(sati) rob@Robins-MBP-2\
    \ llama.cpp % python3 ./convert.py ../phi-2\nLoading model file ../phi-2/model-00001-of-00002.safetensors\n\
    Loading model file ../phi-2/model-00001-of-00002.safetensors\nLoading model file\
    \ ../phi-2/model-00002-of-00002.safetensors\nTraceback (most recent call last):\n\
    \  File \"/Users/rob/Downloads/llama.cpp/./convert.py\", line 1228, in <module>\n\
    \    main()\n  File \"/Users/rob/Downloads/llama.cpp/./convert.py\", line 1161,\
    \ in main\n    model_plus = load_some_model(args.model)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/Users/rob/Downloads/llama.cpp/./convert.py\", line 1078, in load_some_model\n\
    \    model_plus = merge_multifile_models(models_plus)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/Users/rob/Downloads/llama.cpp/./convert.py\", line 593, in merge_multifile_models\n\
    \    model = merge_sharded([mp.model for mp in models_plus])\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/Users/rob/Downloads/llama.cpp/./convert.py\", line 572, in merge_sharded\n\
    \    return {name: convert(name) for name in names}\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/Users/rob/Downloads/llama.cpp/./convert.py\", line 572, in <dictcomp>\n\
    \    return {name: convert(name) for name in names}\n                  ^^^^^^^^^^^^^\n\
    \  File \"/Users/rob/Downloads/llama.cpp/./convert.py\", line 547, in convert\n\
    \    lazy_tensors: list[LazyTensor] = [model[name] for model in models]\n    \
    \                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
    /Users/rob/Downloads/llama.cpp/./convert.py\", line 547, in <listcomp>\n    lazy_tensors:\
    \ list[LazyTensor] = [model[name] for model in models]\n                     \
    \                 ~~~~~^^^^^^\nKeyError: 'transformer.embd.wte.weight'\n(sati)\
    \ rob@Robins-MBP-2 llama.cpp % "
  created_at: 2023-12-16 01:47:10+00:00
  edited: false
  hidden: false
  id: 657d019ef4f72f2c4cf5d0f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6a5bde4a2ea6ffd1a7552e6821615af.svg
      fullname: Rob
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kroonen
      type: user
    createdAt: '2023-12-16T02:12:06.000Z'
    data:
      edited: false
      editors:
      - kroonen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3221931755542755
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6a5bde4a2ea6ffd1a7552e6821615af.svg
          fullname: Rob
          isHf: false
          isPro: false
          name: kroonen
          type: user
        html: '<p>Been following this : <a rel="nofollow" href="https://github.com/mrgraycode/llama.cpp/commit/12cc80cb8975aea3bc9f39d3c9b84f7001ab94c5#diff-150dc86746a90bad4fc2c3334aeb9b5887b3adad3cc1459446717638605348efR6239">https://github.com/mrgraycode/llama.cpp/commit/12cc80cb8975aea3bc9f39d3c9b84f7001ab94c5#diff-150dc86746a90bad4fc2c3334aeb9b5887b3adad3cc1459446717638605348efR6239</a></p>

          <p>Here you go : <a href="https://huggingface.co/kroonen/phi-2-GGUF/tree/main">https://huggingface.co/kroonen/phi-2-GGUF/tree/main</a></p>

          '
        raw: 'Been following this : https://github.com/mrgraycode/llama.cpp/commit/12cc80cb8975aea3bc9f39d3c9b84f7001ab94c5#diff-150dc86746a90bad4fc2c3334aeb9b5887b3adad3cc1459446717638605348efR6239


          Here you go : https://huggingface.co/kroonen/phi-2-GGUF/tree/main'
        updatedAt: '2023-12-16T02:12:06.835Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - guser
    id: 657d0776f010d76b6efb73a3
    type: comment
  author: kroonen
  content: 'Been following this : https://github.com/mrgraycode/llama.cpp/commit/12cc80cb8975aea3bc9f39d3c9b84f7001ab94c5#diff-150dc86746a90bad4fc2c3334aeb9b5887b3adad3cc1459446717638605348efR6239


    Here you go : https://huggingface.co/kroonen/phi-2-GGUF/tree/main'
  created_at: 2023-12-16 02:12:06+00:00
  edited: false
  hidden: false
  id: 657d0776f010d76b6efb73a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64ac7b4159bd86340458e53d3e30aee2.svg
      fullname: Bumba
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pumba2
      type: user
    createdAt: '2023-12-16T02:15:10.000Z'
    data:
      edited: true
      editors:
      - Pumba2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.39437592029571533
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64ac7b4159bd86340458e53d3e30aee2.svg
          fullname: Bumba
          isHf: false
          isPro: false
          name: Pumba2
          type: user
        html: '<blockquote>

          <p>Been following this : <a rel="nofollow" href="https://github.com/mrgraycode/llama.cpp/commit/12cc80cb8975aea3bc9f39d3c9b84f7001ab94c5#diff-150dc86746a90bad4fc2c3334aeb9b5887b3adad3cc1459446717638605348efR6239">https://github.com/mrgraycode/llama.cpp/commit/12cc80cb8975aea3bc9f39d3c9b84f7001ab94c5#diff-150dc86746a90bad4fc2c3334aeb9b5887b3adad3cc1459446717638605348efR6239</a></p>

          <p>Here you go : <a href="https://huggingface.co/kroonen/phi-2-GGUF/tree/main">https://huggingface.co/kroonen/phi-2-GGUF/tree/main</a></p>

          </blockquote>

          <p>Hi kroonen,<br>Could you create a 4Q version ? The 8Q ver. might be too
          slow for my cpu. :P<br>Thanks !</p>

          '
        raw: "> Been following this : https://github.com/mrgraycode/llama.cpp/commit/12cc80cb8975aea3bc9f39d3c9b84f7001ab94c5#diff-150dc86746a90bad4fc2c3334aeb9b5887b3adad3cc1459446717638605348efR6239\n\
          > \n> Here you go : https://huggingface.co/kroonen/phi-2-GGUF/tree/main\n\
          \nHi kroonen,\nCould you create a 4Q version ? The 8Q ver. might be too\
          \ slow for my cpu. :P\nThanks !"
        updatedAt: '2023-12-16T02:17:08.584Z'
      numEdits: 2
      reactions: []
    id: 657d082ea575d54a1e382ef6
    type: comment
  author: Pumba2
  content: "> Been following this : https://github.com/mrgraycode/llama.cpp/commit/12cc80cb8975aea3bc9f39d3c9b84f7001ab94c5#diff-150dc86746a90bad4fc2c3334aeb9b5887b3adad3cc1459446717638605348efR6239\n\
    > \n> Here you go : https://huggingface.co/kroonen/phi-2-GGUF/tree/main\n\nHi\
    \ kroonen,\nCould you create a 4Q version ? The 8Q ver. might be too slow for\
    \ my cpu. :P\nThanks !"
  created_at: 2023-12-16 02:15:10+00:00
  edited: true
  hidden: false
  id: 657d082ea575d54a1e382ef6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/phi-2-GPTQ
repo_type: model
status: open
target_branch: null
title: No GGUF ?
