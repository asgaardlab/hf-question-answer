!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RichRuns
conflicting_files: null
created_at: 2023-12-19 20:43:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/431328e8f4a96a12683d7eefd9986ddf.svg
      fullname: Richard Jenkins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RichRuns
      type: user
    createdAt: '2023-12-19T20:43:42.000Z'
    data:
      edited: false
      editors:
      - RichRuns
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33527320623397827
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/431328e8f4a96a12683d7eefd9986ddf.svg
          fullname: Richard Jenkins
          isHf: false
          isPro: false
          name: RichRuns
          type: user
        html: '<p>I have tried the Q3 and Q4  models. They fail to load with llama.cpp
          in the text gen web ui. I am on Linux.<br>In installed the text gen web
          ui yesterday so it should be up to date. Other models work fine.</p>

          <p>This is the error from the phi-2.Q3_K_M.gguf model:<br>The error on the
          command line where server.py is running says: 2023-12-19 15:38:43 ERROR:Failed
          to load the model.<br>Traceback (most recent call last):<br>  File "/home/somename/text-generation-webui/modules/ui_model_menu.py",
          line 210, in load_model_wrapper<br>    shared.model, shared.tokenizer =
          load_model(selected_model, loader)<br>                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/home/xxx/text-generation-webui/modules/models.py", line 89, in load_model<br>    output
          = load_func_map<a rel="nofollow" href="model_name">loader</a><br>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/home/somename/text-generation-webui/modules/models.py", line 259, in llamacpp_loader<br>    model,
          tokenizer = LlamaCppModel.from_pretrained(model_file)<br>                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/home/somename/text-generation-webui/modules/llamacpp_model.py", line 91,
          in from_pretrained<br>    result.model = Llama(**params)<br>                   ^^^^^^^^^^^^^^^<br>  File
          "/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py",
          line 957, in <strong>init</strong><br>    self._n_vocab = self.n_vocab()<br>                    ^^^^^^^^^^^^^^<br>  File
          "/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py",
          line 2264, in n_vocab<br>    return self._model.n_vocab()<br>           ^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py",
          line 252, in n_vocab<br>    assert self.model is not None<br>           ^^^^^^^^^^^^^^^^^^^^^^<br>AssertionError</p>

          <p>Exception ignored in: &lt;function LlamaCppModel.__del__ at 0x7fd3c3130180&gt;<br>Traceback
          (most recent call last):<br>  File "/home/somename/text-generation-webui/modules/llamacpp_model.py",
          line 49, in <strong>del</strong><br>    del self.model<br>        ^^^^^^^^^^<br>AttributeError:
          ''LlamaCppModel'' object has no attribute ''model''</p>

          <p>========================</p>

          <p>========================</p>

          <p>========================</p>

          <p>========================</p>

          <p>The error in the web console says:<br>Traceback (most recent call last):</p>

          <p>File "/home/somename/text-generation-webui/modules/ui_model_menu.py",
          line 210, in load_model_wrapper</p>

          <p>shared.model, shared.tokenizer = load_model(selected_model, loader)</p>

          <pre><code>                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "/home/somename/text-generation-webui/modules/models.py", line 89,
          in load_model</p>

          <p>output = load_func_map<a rel="nofollow" href="model_name">loader</a></p>

          <pre><code>     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "/home/somename/text-generation-webui/modules/models.py", line 259,
          in llamacpp_loader</p>

          <p>model, tokenizer = LlamaCppModel.from_pretrained(model_file)</p>

          <pre><code>               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "/home/somename/text-generation-webui/modules/llamacpp_model.py",
          line 91, in from_pretrained</p>

          <p>result.model = Llama(**params)</p>

          <pre><code>           ^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py",
          line 957, in init</p>

          <p>self._n_vocab = self.n_vocab()</p>

          <pre><code>            ^^^^^^^^^^^^^^

          </code></pre>

          <p>File "/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py",
          line 2264, in n_vocab</p>

          <p>return self._model.n_vocab()</p>

          <pre><code>   ^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py",
          line 252, in n_vocab</p>

          <p>assert self.model is not None</p>

          <pre><code>   ^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>AssertionError</p>

          '
        raw: "I have tried the Q3 and Q4  models. They fail to load with llama.cpp\
          \ in the text gen web ui. I am on Linux. \r\nIn installed the text gen web\
          \ ui yesterday so it should be up to date. Other models work fine.\r\n\r\
          \nThis is the error from the phi-2.Q3_K_M.gguf model:\r\nThe error on the\
          \ command line where server.py is running says: 2023-12-19 15:38:43 ERROR:Failed\
          \ to load the model.\r\nTraceback (most recent call last):\r\n  File \"\
          /home/somename/text-generation-webui/modules/ui_model_menu.py\", line 210,\
          \ in load_model_wrapper\r\n    shared.model, shared.tokenizer = load_model(selected_model,\
          \ loader)\r\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/home/xxx/text-generation-webui/modules/models.py\", line 89,\
          \ in load_model\r\n    output = load_func_map[loader](model_name)\r\n  \
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/somename/text-generation-webui/modules/models.py\"\
          , line 259, in llamacpp_loader\r\n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
          \n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n \
          \ File \"/home/somename/text-generation-webui/modules/llamacpp_model.py\"\
          , line 91, in from_pretrained\r\n    result.model = Llama(**params)\r\n\
          \                   ^^^^^^^^^^^^^^^\r\n  File \"/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py\"\
          , line 957, in __init__\r\n    self._n_vocab = self.n_vocab()\r\n      \
          \              ^^^^^^^^^^^^^^\r\n  File \"/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py\"\
          , line 2264, in n_vocab\r\n    return self._model.n_vocab()\r\n        \
          \   ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py\"\
          , line 252, in n_vocab\r\n    assert self.model is not None\r\n        \
          \   ^^^^^^^^^^^^^^^^^^^^^^\r\nAssertionError\r\n\r\nException ignored in:\
          \ <function LlamaCppModel.__del__ at 0x7fd3c3130180>\r\nTraceback (most\
          \ recent call last):\r\n  File \"/home/somename/text-generation-webui/modules/llamacpp_model.py\"\
          , line 49, in __del__\r\n    del self.model\r\n        ^^^^^^^^^^\r\nAttributeError:\
          \ 'LlamaCppModel' object has no attribute 'model'\r\n\r\n========================\r\
          \n\r\n========================\r\n\r\n========================\r\n\r\n========================\r\
          \n\r\n\r\nThe error in the web console says: \r\nTraceback (most recent\
          \ call last):\r\n\r\nFile \"/home/somename/text-generation-webui/modules/ui_model_menu.py\"\
          , line 210, in load_model_wrapper\r\n\r\nshared.model, shared.tokenizer\
          \ = load_model(selected_model, loader)\r\n\r\n                         \
          \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nFile \"/home/somename/text-generation-webui/modules/models.py\"\
          , line 89, in load_model\r\n\r\noutput = load_func_map[loader](model_name)\r\
          \n\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nFile \"/home/somename/text-generation-webui/modules/models.py\"\
          , line 259, in llamacpp_loader\r\n\r\nmodel, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
          \n\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\
          \nFile \"/home/somename/text-generation-webui/modules/llamacpp_model.py\"\
          , line 91, in from_pretrained\r\n\r\nresult.model = Llama(**params)\r\n\r\
          \n               ^^^^^^^^^^^^^^^\r\n\r\nFile \"/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py\"\
          , line 957, in init\r\n\r\nself._n_vocab = self.n_vocab()\r\n\r\n      \
          \          ^^^^^^^^^^^^^^\r\n\r\nFile \"/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py\"\
          , line 2264, in n_vocab\r\n\r\nreturn self._model.n_vocab()\r\n\r\n    \
          \   ^^^^^^^^^^^^^^^^^^^^^\r\n\r\nFile \"/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py\"\
          , line 252, in n_vocab\r\n\r\nassert self.model is not None\r\n\r\n    \
          \   ^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nAssertionError"
        updatedAt: '2023-12-19T20:43:42.239Z'
      numEdits: 0
      reactions: []
    id: 6582007eb0c93d55390a407a
    type: comment
  author: RichRuns
  content: "I have tried the Q3 and Q4  models. They fail to load with llama.cpp in\
    \ the text gen web ui. I am on Linux. \r\nIn installed the text gen web ui yesterday\
    \ so it should be up to date. Other models work fine.\r\n\r\nThis is the error\
    \ from the phi-2.Q3_K_M.gguf model:\r\nThe error on the command line where server.py\
    \ is running says: 2023-12-19 15:38:43 ERROR:Failed to load the model.\r\nTraceback\
    \ (most recent call last):\r\n  File \"/home/somename/text-generation-webui/modules/ui_model_menu.py\"\
    , line 210, in load_model_wrapper\r\n    shared.model, shared.tokenizer = load_model(selected_model,\
    \ loader)\r\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/home/xxx/text-generation-webui/modules/models.py\", line 89, in load_model\r\
    \n    output = load_func_map[loader](model_name)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/home/somename/text-generation-webui/modules/models.py\", line 259,\
    \ in llamacpp_loader\r\n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
    \n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
    /home/somename/text-generation-webui/modules/llamacpp_model.py\", line 91, in\
    \ from_pretrained\r\n    result.model = Llama(**params)\r\n                  \
    \ ^^^^^^^^^^^^^^^\r\n  File \"/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py\"\
    , line 957, in __init__\r\n    self._n_vocab = self.n_vocab()\r\n            \
    \        ^^^^^^^^^^^^^^\r\n  File \"/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py\"\
    , line 2264, in n_vocab\r\n    return self._model.n_vocab()\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py\"\
    , line 252, in n_vocab\r\n    assert self.model is not None\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\
    \nAssertionError\r\n\r\nException ignored in: <function LlamaCppModel.__del__\
    \ at 0x7fd3c3130180>\r\nTraceback (most recent call last):\r\n  File \"/home/somename/text-generation-webui/modules/llamacpp_model.py\"\
    , line 49, in __del__\r\n    del self.model\r\n        ^^^^^^^^^^\r\nAttributeError:\
    \ 'LlamaCppModel' object has no attribute 'model'\r\n\r\n========================\r\
    \n\r\n========================\r\n\r\n========================\r\n\r\n========================\r\
    \n\r\n\r\nThe error in the web console says: \r\nTraceback (most recent call last):\r\
    \n\r\nFile \"/home/somename/text-generation-webui/modules/ui_model_menu.py\",\
    \ line 210, in load_model_wrapper\r\n\r\nshared.model, shared.tokenizer = load_model(selected_model,\
    \ loader)\r\n\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n\r\nFile \"/home/somename/text-generation-webui/modules/models.py\", line 89,\
    \ in load_model\r\n\r\noutput = load_func_map[loader](model_name)\r\n\r\n    \
    \     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nFile \"/home/somename/text-generation-webui/modules/models.py\"\
    , line 259, in llamacpp_loader\r\n\r\nmodel, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
    \n\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nFile\
    \ \"/home/somename/text-generation-webui/modules/llamacpp_model.py\", line 91,\
    \ in from_pretrained\r\n\r\nresult.model = Llama(**params)\r\n\r\n           \
    \    ^^^^^^^^^^^^^^^\r\n\r\nFile \"/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py\"\
    , line 957, in init\r\n\r\nself._n_vocab = self.n_vocab()\r\n\r\n            \
    \    ^^^^^^^^^^^^^^\r\n\r\nFile \"/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py\"\
    , line 2264, in n_vocab\r\n\r\nreturn self._model.n_vocab()\r\n\r\n       ^^^^^^^^^^^^^^^^^^^^^\r\
    \n\r\nFile \"/home/somename/miniconda3/envs/textgen/lib/python3.11/site-packages/llama_cpp_cuda/llama.py\"\
    , line 252, in n_vocab\r\n\r\nassert self.model is not None\r\n\r\n       ^^^^^^^^^^^^^^^^^^^^^^\r\
    \n\r\nAssertionError"
  created_at: 2023-12-19 20:43:42+00:00
  edited: false
  hidden: false
  id: 6582007eb0c93d55390a407a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6342619a9948f573f37a4a60/DUELM_XzzKuN-D20Y_aSK.jpeg?w=200&h=200&f=face
      fullname: Nonetici Alberto
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nonetrix
      type: user
    createdAt: '2024-01-19T01:30:49.000Z'
    data:
      edited: false
      editors:
      - nonetrix
      hidden: false
      identifiedLanguage:
        language: es
        probability: 0.1530763953924179
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6342619a9948f573f37a4a60/DUELM_XzzKuN-D20Y_aSK.jpeg?w=200&h=200&f=face
          fullname: Nonetici Alberto
          isHf: false
          isPro: false
          name: nonetrix
          type: user
        html: '<p>Same</p>

          '
        raw: Same
        updatedAt: '2024-01-19T01:30:49.647Z'
      numEdits: 0
      reactions: []
    id: 65a9d0c91145be22c8c28d2a
    type: comment
  author: nonetrix
  content: Same
  created_at: 2024-01-19 01:30:49+00:00
  edited: false
  hidden: false
  id: 65a9d0c91145be22c8c28d2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c768c4bed7d65af6f7599eadc9b0f17b.svg
      fullname: dai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: linnnnnk
      type: user
    createdAt: '2024-01-21T17:52:18.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/c768c4bed7d65af6f7599eadc9b0f17b.svg
          fullname: dai
          isHf: false
          isPro: false
          name: linnnnnk
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2024-01-21T17:52:53.149Z'
      numEdits: 0
      reactions: []
    id: 65ad59d246d2f7fe54b0f938
    type: comment
  author: linnnnnk
  content: This comment has been hidden
  created_at: 2024-01-21 17:52:18+00:00
  edited: true
  hidden: true
  id: 65ad59d246d2f7fe54b0f938
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6342619a9948f573f37a4a60/DUELM_XzzKuN-D20Y_aSK.jpeg?w=200&h=200&f=face
      fullname: Nonetici Alberto
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nonetrix
      type: user
    createdAt: '2024-01-22T23:20:29.000Z'
    data:
      edited: false
      editors:
      - nonetrix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8537734746932983
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6342619a9948f573f37a4a60/DUELM_XzzKuN-D20Y_aSK.jpeg?w=200&h=200&f=face
          fullname: Nonetici Alberto
          isHf: false
          isPro: false
          name: nonetrix
          type: user
        html: '<p>Is there any relevant merge requests or issues? </p>

          '
        raw: 'Is there any relevant merge requests or issues? '
        updatedAt: '2024-01-22T23:20:29.010Z'
      numEdits: 0
      reactions: []
    id: 65aef83d0214b35f1b0c2f7b
    type: comment
  author: nonetrix
  content: 'Is there any relevant merge requests or issues? '
  created_at: 2024-01-22 23:20:29+00:00
  edited: false
  hidden: false
  id: 65aef83d0214b35f1b0c2f7b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/phi-2-GPTQ
repo_type: model
status: open
target_branch: null
title: Not loading on Text Gen Web UI
