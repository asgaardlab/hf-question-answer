!!python/object:huggingface_hub.community.DiscussionWithDetails
author: CreativeUsername
conflicting_files: null
created_at: 2023-08-02 12:31:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2cddb1ca8363886341b02b237501d0a0.svg
      fullname: Kay Konstantin Jersch
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CreativeUsername
      type: user
    createdAt: '2023-08-02T13:31:25.000Z'
    data:
      edited: false
      editors:
      - CreativeUsername
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3659214377403259
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2cddb1ca8363886341b02b237501d0a0.svg
          fullname: Kay Konstantin Jersch
          isHf: false
          isPro: false
          name: CreativeUsername
          type: user
        html: '<p>When loading this model using: AutoModelForCausalLM.from_pretrained("anas-awadalla/mpt-1b-redpajama-200b-dolly")
          I keep getting the following Error:<br>Traceback (most recent call last):<br>  File
          "of_test.py", line 4, in <br>    AutoModelForCausalLM.from_pretrained("mosaicml/mpt-1b-redpajama-200b-dolly")<br>  File
          "/home/myName/miniconda3/envs/omnigibson/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py",
          line 461, in from_pretrained<br>    **kwargs,<br>  File "/home/myName/miniconda3/envs/omnigibson/lib/python3.7/site-packages/transformers/models/auto/configuration_auto.py",
          line 953, in from_pretrained<br>    config_class = get_class_from_dynamic_module(class_ref,
          pretrained_model_name_or_path, **kwargs)<br>  File "/home/myName/miniconda3/envs/omnigibson/lib/python3.7/site-packages/transformers/dynamic_module_utils.py",
          line 443, in get_class_from_dynamic_module<br>    return get_class_in_module(class_name,
          final_module.replace(".py", ""))<br>  File "/home/myName/miniconda3/envs/omnigibson/lib/python3.7/site-packages/transformers/dynamic_module_utils.py",
          line 164, in get_class_in_module<br>    module = importlib.import_module(module_path)<br>  File
          "/home/myName/miniconda3/envs/omnigibson/lib/python3.7/importlib/<strong>init</strong>.py",
          line 127, in import_module<br>    return _bootstrap._gcd_import(name[level:],
          package, level)<br>  File "", line 1006, in _gcd_import<br>  File "", line
          983, in _find_and_load<br>  File "", line 967, in _find_and_load_unlocked<br>  File
          "", line 677, in _load_unlocked<br>  File "", line 724, in exec_module<br>  File
          "", line 860, in get_code<br>  File "", line 791, in source_to_code<br>  File
          "", line 219, in _call_with_frames_removed<br>  File "", line 1<br>    (self.logit_scale=)<br>                     ^<br>SyntaxError:
          invalid syntax</p>

          <p>Im running transformers-3.40.2 on python 3.7 and an Ubuntu 22.04.2 machine  with
          a NVIDIA GeForce RTX 2080 SUPER<br>Has anyone else experienced this? How
          do I fix this?</p>

          '
        raw: "When loading this model using: AutoModelForCausalLM.from_pretrained(\"\
          anas-awadalla/mpt-1b-redpajama-200b-dolly\") I keep getting the following\
          \ Error:\r\nTraceback (most recent call last):\r\n  File \"of_test.py\"\
          , line 4, in <module>\r\n    AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-1b-redpajama-200b-dolly\"\
          )\r\n  File \"/home/myName/miniconda3/envs/omnigibson/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 461, in from_pretrained\r\n    **kwargs,\r\n  File \"/home/myName/miniconda3/envs/omnigibson/lib/python3.7/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 953, in from_pretrained\r\n    config_class = get_class_from_dynamic_module(class_ref,\
          \ pretrained_model_name_or_path, **kwargs)\r\n  File \"/home/myName/miniconda3/envs/omnigibson/lib/python3.7/site-packages/transformers/dynamic_module_utils.py\"\
          , line 443, in get_class_from_dynamic_module\r\n    return get_class_in_module(class_name,\
          \ final_module.replace(\".py\", \"\"))\r\n  File \"/home/myName/miniconda3/envs/omnigibson/lib/python3.7/site-packages/transformers/dynamic_module_utils.py\"\
          , line 164, in get_class_in_module\r\n    module = importlib.import_module(module_path)\r\
          \n  File \"/home/myName/miniconda3/envs/omnigibson/lib/python3.7/importlib/__init__.py\"\
          , line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:],\
          \ package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006,\
          \ in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983,\
          \ in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967,\
          \ in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\"\
          , line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\"\
          , line 724, in exec_module\r\n  File \"<frozen importlib._bootstrap_external>\"\
          , line 860, in get_code\r\n  File \"<frozen importlib._bootstrap_external>\"\
          , line 791, in source_to_code\r\n  File \"<frozen importlib._bootstrap>\"\
          , line 219, in _call_with_frames_removed\r\n  File \"<fstring>\", line 1\r\
          \n    (self.logit_scale=)\r\n                     ^\r\nSyntaxError: invalid\
          \ syntax\r\n\r\nIm running transformers-3.40.2 on python 3.7 and an Ubuntu\
          \ 22.04.2 machine  with a NVIDIA GeForce RTX 2080 SUPER\r\nHas anyone else\
          \ experienced this? How do I fix this?"
        updatedAt: '2023-08-02T13:31:25.245Z'
      numEdits: 0
      reactions: []
    id: 64ca5aad38837b12d5ed1ffa
    type: comment
  author: CreativeUsername
  content: "When loading this model using: AutoModelForCausalLM.from_pretrained(\"\
    anas-awadalla/mpt-1b-redpajama-200b-dolly\") I keep getting the following Error:\r\
    \nTraceback (most recent call last):\r\n  File \"of_test.py\", line 4, in <module>\r\
    \n    AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-1b-redpajama-200b-dolly\"\
    )\r\n  File \"/home/myName/miniconda3/envs/omnigibson/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 461, in from_pretrained\r\n    **kwargs,\r\n  File \"/home/myName/miniconda3/envs/omnigibson/lib/python3.7/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 953, in from_pretrained\r\n    config_class = get_class_from_dynamic_module(class_ref,\
    \ pretrained_model_name_or_path, **kwargs)\r\n  File \"/home/myName/miniconda3/envs/omnigibson/lib/python3.7/site-packages/transformers/dynamic_module_utils.py\"\
    , line 443, in get_class_from_dynamic_module\r\n    return get_class_in_module(class_name,\
    \ final_module.replace(\".py\", \"\"))\r\n  File \"/home/myName/miniconda3/envs/omnigibson/lib/python3.7/site-packages/transformers/dynamic_module_utils.py\"\
    , line 164, in get_class_in_module\r\n    module = importlib.import_module(module_path)\r\
    \n  File \"/home/myName/miniconda3/envs/omnigibson/lib/python3.7/importlib/__init__.py\"\
    , line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:],\
    \ package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\
    \n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File\
    \ \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n\
    \  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File\
    \ \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\r\n  File\
    \ \"<frozen importlib._bootstrap_external>\", line 860, in get_code\r\n  File\
    \ \"<frozen importlib._bootstrap_external>\", line 791, in source_to_code\r\n\
    \  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\
    \n  File \"<fstring>\", line 1\r\n    (self.logit_scale=)\r\n                \
    \     ^\r\nSyntaxError: invalid syntax\r\n\r\nIm running transformers-3.40.2 on\
    \ python 3.7 and an Ubuntu 22.04.2 machine  with a NVIDIA GeForce RTX 2080 SUPER\r\
    \nHas anyone else experienced this? How do I fix this?"
  created_at: 2023-08-02 12:31:25+00:00
  edited: false
  hidden: false
  id: 64ca5aad38837b12d5ed1ffa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f784fa423fd84fffb4683fa837ffc5a3.svg
      fullname: Anas Awadalla
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: anas-awadalla
      type: user
    createdAt: '2023-08-02T21:53:22.000Z'
    data:
      edited: false
      editors:
      - anas-awadalla
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3302852213382721
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f784fa423fd84fffb4683fa837ffc5a3.svg
          fullname: Anas Awadalla
          isHf: false
          isPro: false
          name: anas-awadalla
          type: user
        html: '<p>Can you try adding the trust code parameter? Like this:</p>

          <p>model = AutoModelForCausalLM.from_pretrained("anas-awadalla/mpt-1b-redpajama-200b-dolly",
          trust_remote_code=True)</p>

          '
        raw: 'Can you try adding the trust code parameter? Like this:


          model = AutoModelForCausalLM.from_pretrained("anas-awadalla/mpt-1b-redpajama-200b-dolly",
          trust_remote_code=True)'
        updatedAt: '2023-08-02T21:53:22.454Z'
      numEdits: 0
      reactions: []
    id: 64cad052ec160b67ca683806
    type: comment
  author: anas-awadalla
  content: 'Can you try adding the trust code parameter? Like this:


    model = AutoModelForCausalLM.from_pretrained("anas-awadalla/mpt-1b-redpajama-200b-dolly",
    trust_remote_code=True)'
  created_at: 2023-08-02 20:53:22+00:00
  edited: false
  hidden: false
  id: 64cad052ec160b67ca683806
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2cddb1ca8363886341b02b237501d0a0.svg
      fullname: Kay Konstantin Jersch
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CreativeUsername
      type: user
    createdAt: '2023-08-02T23:14:43.000Z'
    data:
      edited: false
      editors:
      - CreativeUsername
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.993614673614502
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2cddb1ca8363886341b02b237501d0a0.svg
          fullname: Kay Konstantin Jersch
          isHf: false
          isPro: false
          name: CreativeUsername
          type: user
        html: '<p>That didn''t work. I still got the same error</p>

          '
        raw: That didn't work. I still got the same error
        updatedAt: '2023-08-02T23:14:43.034Z'
      numEdits: 0
      reactions: []
    id: 64cae363f9c57cdcb2575ad2
    type: comment
  author: CreativeUsername
  content: That didn't work. I still got the same error
  created_at: 2023-08-02 22:14:43+00:00
  edited: false
  hidden: false
  id: 64cae363f9c57cdcb2575ad2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: anas-awadalla/mpt-1b-redpajama-200b-dolly
repo_type: model
status: open
target_branch: null
title: 'SyntaxError: (self.logit_scale=) When trying to load this model'
