!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HighlandGNU
conflicting_files: null
created_at: 2023-09-10 09:01:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/527b8e16cbff4ddc7d186c8ac92d3eef.svg
      fullname: nn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HighlandGNU
      type: user
    createdAt: '2023-09-10T10:01:28.000Z'
    data:
      edited: false
      editors:
      - HighlandGNU
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9193679094314575
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/527b8e16cbff4ddc7d186c8ac92d3eef.svg
          fullname: nn
          isHf: false
          isPro: false
          name: HighlandGNU
          type: user
        html: '<p>Thank you for creating and publishing this model. The 13b version
          is brilliant.</p>

          <p>When announcing your models, I hope that you will consider accompanying
          them with a couple of brief statements:</p>

          <ol>

          <li>The minimum consumer grade hardware that would be required to run the
          model, with any suggested settings for that mimimum (e.g. which quantization
          etc) and the sort of inferencing rate to be expected.</li>

          <li>The relative strengths of the model, e.g. it is stronger at programming
          than story telling, how strongly compliant it is/alignment.</li>

          </ol>

          <p>In the case of this model, for example, could you run it with 16GB VRAM
          and 64 GB RAM?</p>

          '
        raw: "Thank you for creating and publishing this model. The 13b version is\
          \ brilliant.\r\n\r\nWhen announcing your models, I hope that you will consider\
          \ accompanying them with a couple of brief statements:\r\n\r\n1) The minimum\
          \ consumer grade hardware that would be required to run the model, with\
          \ any suggested settings for that mimimum (e.g. which quantization etc)\
          \ and the sort of inferencing rate to be expected.\r\n2) The relative strengths\
          \ of the model, e.g. it is stronger at programming than story telling, how\
          \ strongly compliant it is/alignment.\r\n\r\nIn the case of this model,\
          \ for example, could you run it with 16GB VRAM and 64 GB RAM?"
        updatedAt: '2023-09-10T10:01:28.702Z'
      numEdits: 0
      reactions: []
    id: 64fd93f8b3eee10ba54f932a
    type: comment
  author: HighlandGNU
  content: "Thank you for creating and publishing this model. The 13b version is brilliant.\r\
    \n\r\nWhen announcing your models, I hope that you will consider accompanying\
    \ them with a couple of brief statements:\r\n\r\n1) The minimum consumer grade\
    \ hardware that would be required to run the model, with any suggested settings\
    \ for that mimimum (e.g. which quantization etc) and the sort of inferencing rate\
    \ to be expected.\r\n2) The relative strengths of the model, e.g. it is stronger\
    \ at programming than story telling, how strongly compliant it is/alignment.\r\
    \n\r\nIn the case of this model, for example, could you run it with 16GB VRAM\
    \ and 64 GB RAM?"
  created_at: 2023-09-10 09:01:28+00:00
  edited: false
  hidden: false
  id: 64fd93f8b3eee10ba54f932a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-09-11T07:03:40.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9822511672973633
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>It was too large for me to benchmark so I can''t say other than
          what huggingface leaderboard says, but it did have roleplaying data, so
          possibly better than most at it.</p>

          <p>As for min requirement, 2x3090s or 4090s or an a6000 48gb is required
          to inference in 4bit</p>

          '
        raw: 'It was too large for me to benchmark so I can''t say other than what
          huggingface leaderboard says, but it did have roleplaying data, so possibly
          better than most at it.


          As for min requirement, 2x3090s or 4090s or an a6000 48gb is required to
          inference in 4bit'
        updatedAt: '2023-09-11T07:03:40.265Z'
      numEdits: 0
      reactions: []
    id: 64febbcc30715b6d04458882
    type: comment
  author: teknium
  content: 'It was too large for me to benchmark so I can''t say other than what huggingface
    leaderboard says, but it did have roleplaying data, so possibly better than most
    at it.


    As for min requirement, 2x3090s or 4090s or an a6000 48gb is required to inference
    in 4bit'
  created_at: 2023-09-11 06:03:40+00:00
  edited: false
  hidden: false
  id: 64febbcc30715b6d04458882
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: NousResearch/Nous-Hermes-Llama2-70b
repo_type: model
status: open
target_branch: null
title: Go/No Go
