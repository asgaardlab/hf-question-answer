!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ybelkada
conflicting_files: null
created_at: 2023-08-24 09:38:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-08-24T10:38:33.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.783910870552063
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>As per the recent PEFT integration in transformers (available on
          main branch): <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/25077">https://github.com/huggingface/transformers/pull/25077</a>
          / <a href="https://huggingface.co/docs/transformers/main/en/peft">https://huggingface.co/docs/transformers/main/en/peft</a>
          for users that have PEFT and transformers from source installed <code>from_pretrained</code>
          will load the adapters, load the base model specified in the adapter config
          and inject the trained adapters in-place to the model. This should lead
          to the same thing as doing:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          peft <span class="hljs-keyword">import</span> AutoPeftModelForCausalLM


          model = AutoPeftModelForCausalLM.from_pretrained(<span class="hljs-string">"NousResearch/Nous-Hermes-Llama2-70b"</span>)

          </code></pre>

          <p>but instead of returning a <code>PeftModel</code> it will return a <code>AutoModelForCausalLM</code>.
          I suggest you push the adapter weights and config in a separate repository
          to clearly distinguish between the merged final model and the adapter weights.
          </p>

          '
        raw: 'As per the recent PEFT integration in transformers (available on main
          branch): https://github.com/huggingface/transformers/pull/25077 / https://huggingface.co/docs/transformers/main/en/peft
          for users that have PEFT and transformers from source installed `from_pretrained`
          will load the adapters, load the base model specified in the adapter config
          and inject the trained adapters in-place to the model. This should lead
          to the same thing as doing:


          ```python

          from peft import AutoPeftModelForCausalLM


          model = AutoPeftModelForCausalLM.from_pretrained("NousResearch/Nous-Hermes-Llama2-70b")

          ```


          but instead of returning a `PeftModel` it will return a `AutoModelForCausalLM`.
          I suggest you push the adapter weights and config in a separate repository
          to clearly distinguish between the merged final model and the adapter weights. '
        updatedAt: '2023-08-24T10:44:08.745Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - nacs
        - Samarth0109
    id: 64e73329ccfe005d2b0d6b2f
    type: comment
  author: ybelkada
  content: 'As per the recent PEFT integration in transformers (available on main
    branch): https://github.com/huggingface/transformers/pull/25077 / https://huggingface.co/docs/transformers/main/en/peft
    for users that have PEFT and transformers from source installed `from_pretrained`
    will load the adapters, load the base model specified in the adapter config and
    inject the trained adapters in-place to the model. This should lead to the same
    thing as doing:


    ```python

    from peft import AutoPeftModelForCausalLM


    model = AutoPeftModelForCausalLM.from_pretrained("NousResearch/Nous-Hermes-Llama2-70b")

    ```


    but instead of returning a `PeftModel` it will return a `AutoModelForCausalLM`.
    I suggest you push the adapter weights and config in a separate repository to
    clearly distinguish between the merged final model and the adapter weights. '
  created_at: 2023-08-24 09:38:33+00:00
  edited: true
  hidden: false
  id: 64e73329ccfe005d2b0d6b2f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-08-27T19:55:28.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7780783176422119
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<blockquote>

          <p>As per the recent PEFT integration in transformers (available on main
          branch): <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/25077">https://github.com/huggingface/transformers/pull/25077</a>
          / <a href="https://huggingface.co/docs/transformers/main/en/peft">https://huggingface.co/docs/transformers/main/en/peft</a>
          for users that have PEFT and transformers from source installed <code>from_pretrained</code>
          will load the adapters, load the base model specified in the adapter config
          and inject the trained adapters in-place to the model. This should lead
          to the same thing as doing:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          peft <span class="hljs-keyword">import</span> AutoPeftModelForCausalLM


          model = AutoPeftModelForCausalLM.from_pretrained(<span class="hljs-string">"NousResearch/Nous-Hermes-Llama2-70b"</span>)

          </code></pre>

          <p>but instead of returning a <code>PeftModel</code> it will return a <code>AutoModelForCausalLM</code>.
          I suggest you push the adapter weights and config in a separate repository
          to clearly distinguish between the merged final model and the adapter weights.</p>

          </blockquote>

          <p>Will do in a bit</p>

          '
        raw: "> As per the recent PEFT integration in transformers (available on main\
          \ branch): https://github.com/huggingface/transformers/pull/25077 / https://huggingface.co/docs/transformers/main/en/peft\
          \ for users that have PEFT and transformers from source installed `from_pretrained`\
          \ will load the adapters, load the base model specified in the adapter config\
          \ and inject the trained adapters in-place to the model. This should lead\
          \ to the same thing as doing:\n> \n> ```python\n> from peft import AutoPeftModelForCausalLM\n\
          > \n> model = AutoPeftModelForCausalLM.from_pretrained(\"NousResearch/Nous-Hermes-Llama2-70b\"\
          )\n> ```\n> \n> but instead of returning a `PeftModel` it will return a\
          \ `AutoModelForCausalLM`. I suggest you push the adapter weights and config\
          \ in a separate repository to clearly distinguish between the merged final\
          \ model and the adapter weights.\n\nWill do in a bit"
        updatedAt: '2023-08-27T19:55:28.153Z'
      numEdits: 0
      reactions: []
    id: 64ebaa305eeedae7f8574e2e
    type: comment
  author: teknium
  content: "> As per the recent PEFT integration in transformers (available on main\
    \ branch): https://github.com/huggingface/transformers/pull/25077 / https://huggingface.co/docs/transformers/main/en/peft\
    \ for users that have PEFT and transformers from source installed `from_pretrained`\
    \ will load the adapters, load the base model specified in the adapter config\
    \ and inject the trained adapters in-place to the model. This should lead to the\
    \ same thing as doing:\n> \n> ```python\n> from peft import AutoPeftModelForCausalLM\n\
    > \n> model = AutoPeftModelForCausalLM.from_pretrained(\"NousResearch/Nous-Hermes-Llama2-70b\"\
    )\n> ```\n> \n> but instead of returning a `PeftModel` it will return a `AutoModelForCausalLM`.\
    \ I suggest you push the adapter weights and config in a separate repository to\
    \ clearly distinguish between the merged final model and the adapter weights.\n\
    \nWill do in a bit"
  created_at: 2023-08-27 18:55:28+00:00
  edited: false
  hidden: false
  id: 64ebaa305eeedae7f8574e2e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: NousResearch/Nous-Hermes-Llama2-70b
repo_type: model
status: open
target_branch: null
title: '[Request] Push adapter model in another repo'
