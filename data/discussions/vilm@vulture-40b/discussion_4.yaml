!!python/object:huggingface_hub.community.DiscussionWithDetails
author: batman9x
conflicting_files: null
created_at: 2023-10-14 02:51:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/47e56ee6188bb8258a579109121d40ae.svg
      fullname: Dao Xuan Do
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: batman9x
      type: user
    createdAt: '2023-10-14T03:51:57.000Z'
    data:
      edited: false
      editors:
      - batman9x
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7229083180427551
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/47e56ee6188bb8258a579109121d40ae.svg
          fullname: Dao Xuan Do
          isHf: false
          isPro: false
          name: batman9x
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/Vx8xj4Mhhy3hPVoG8ESiP.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/Vx8xj4Mhhy3hPVoG8ESiP.png"></a></p>

          <p>Why do I run code but not respond? How to fix that</p>

          '
        raw: "\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/Vx8xj4Mhhy3hPVoG8ESiP.png)\r\
          \n\r\nWhy do I run code but not respond? How to fix that"
        updatedAt: '2023-10-14T03:51:57.055Z'
      numEdits: 0
      reactions: []
    id: 652a105d1198736f31e2995f
    type: comment
  author: batman9x
  content: "\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/Vx8xj4Mhhy3hPVoG8ESiP.png)\r\
    \n\r\nWhy do I run code but not respond? How to fix that"
  created_at: 2023-10-14 02:51:57+00:00
  edited: false
  hidden: false
  id: 652a105d1198736f31e2995f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b86328c76e635315401c7b609d861fc.svg
      fullname: Quan Nguyen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: qnguyen3
      type: user
    createdAt: '2023-10-15T12:51:32.000Z'
    data:
      edited: false
      editors:
      - qnguyen3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.956606924533844
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4b86328c76e635315401c7b609d861fc.svg
          fullname: Quan Nguyen
          isHf: false
          isPro: false
          name: qnguyen3
          type: user
        html: '<p>i can''t give you any advice without seeing your current code</p>

          '
        raw: i can't give you any advice without seeing your current code
        updatedAt: '2023-10-15T12:51:32.116Z'
      numEdits: 0
      reactions: []
    id: 652be0544d8e7e764605666d
    type: comment
  author: qnguyen3
  content: i can't give you any advice without seeing your current code
  created_at: 2023-10-15 11:51:32+00:00
  edited: false
  hidden: false
  id: 652be0544d8e7e764605666d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/47e56ee6188bb8258a579109121d40ae.svg
      fullname: Dao Xuan Do
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: batman9x
      type: user
    createdAt: '2023-10-15T16:10:44.000Z'
    data:
      edited: false
      editors:
      - batman9x
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4152405560016632
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/47e56ee6188bb8258a579109121d40ae.svg
          fullname: Dao Xuan Do
          isHf: false
          isPro: false
          name: batman9x
          type: user
        html: "<blockquote>\n<p>i can't give you any advice without seeing your current\
          \ code</p>\n</blockquote>\n<p>i use code example </p>\n<p>from transformers\
          \ import AutoTokenizer, AutoModelForCausalLM<br>import transformers<br>import\
          \ torch</p>\n<p>model = \"vilm/vulture-40B\"</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(model)<br>m\
          \ = AutoModelForCausalLM.from_pretrained(model, torch_dtype=torch.bfloat16,\
          \ device_map=\"auto\", trust_remote_code=True )</p>\n<p>prompt = \"A chat\
          \ between a curious user and an artificial intelligence assistant.\\n\\\
          nUSER:Th\xE0nh ph\u1ED1 H\u1ED3 Ch\xED Minh n\u1EB1m \u1EDF \u0111\xE2u?&lt;|endoftext|&gt;ASSISTANT:\"\
          </p>\n<p>inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")</p>\n\
          <p>output = m.generate(input_ids=inputs[\"input_ids\"],<br>            \
          \        attention_mask=inputs[\"attention_mask\"],<br>                \
          \    do_sample=True,<br>                    temperature=0.6,<br>       \
          \             top_p=0.9,<br>                    max_new_tokens=50,)<br>output\
          \ = output[0].to(\"cpu\")<br>print(tokenizer.decode(output))</p>\n"
        raw: "> i can't give you any advice without seeing your current code\n\ni\
          \ use code example \n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          import transformers\nimport torch\n\nmodel = \"vilm/vulture-40B\"\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model)\nm = AutoModelForCausalLM.from_pretrained(model,\
          \ torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True\
          \ )\n\nprompt = \"A chat between a curious user and an artificial intelligence\
          \ assistant.\\n\\nUSER:Th\xE0nh ph\u1ED1 H\u1ED3 Ch\xED Minh n\u1EB1m \u1EDF\
          \ \u0111\xE2u?<|endoftext|>ASSISTANT:\"\n\ninputs = tokenizer(prompt, return_tensors=\"\
          pt\").to(\"cuda\")\n\noutput = m.generate(input_ids=inputs[\"input_ids\"\
          ],\n                    attention_mask=inputs[\"attention_mask\"],\n   \
          \                 do_sample=True,\n                    temperature=0.6,\n\
          \                    top_p=0.9,\n                    max_new_tokens=50,)\n\
          output = output[0].to(\"cpu\")\nprint(tokenizer.decode(output))"
        updatedAt: '2023-10-15T16:10:44.533Z'
      numEdits: 0
      reactions: []
    id: 652c0f04789d84df6f291220
    type: comment
  author: batman9x
  content: "> i can't give you any advice without seeing your current code\n\ni use\
    \ code example \n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
    import transformers\nimport torch\n\nmodel = \"vilm/vulture-40B\"\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model)\nm = AutoModelForCausalLM.from_pretrained(model,\
    \ torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True )\n\n\
    prompt = \"A chat between a curious user and an artificial intelligence assistant.\\\
    n\\nUSER:Th\xE0nh ph\u1ED1 H\u1ED3 Ch\xED Minh n\u1EB1m \u1EDF \u0111\xE2u?<|endoftext|>ASSISTANT:\"\
    \n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\noutput =\
    \ m.generate(input_ids=inputs[\"input_ids\"],\n                    attention_mask=inputs[\"\
    attention_mask\"],\n                    do_sample=True,\n                    temperature=0.6,\n\
    \                    top_p=0.9,\n                    max_new_tokens=50,)\noutput\
    \ = output[0].to(\"cpu\")\nprint(tokenizer.decode(output))"
  created_at: 2023-10-15 15:10:44+00:00
  edited: false
  hidden: false
  id: 652c0f04789d84df6f291220
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e063fefb2327146bb475e7daf4647423.svg
      fullname: Dung Minh Dao
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: DungMinhDao
      type: user
    createdAt: '2023-10-15T19:03:31.000Z'
    data:
      edited: false
      editors:
      - DungMinhDao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45833733677864075
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e063fefb2327146bb475e7daf4647423.svg
          fullname: Dung Minh Dao
          isHf: false
          isPro: false
          name: DungMinhDao
          type: user
        html: "<blockquote>\n<blockquote>\n<p>i can't give you any advice without\
          \ seeing your current code</p>\n</blockquote>\n<p>i use code example </p>\n\
          <p>from transformers import AutoTokenizer, AutoModelForCausalLM<br>import\
          \ transformers<br>import torch</p>\n<p>model = \"vilm/vulture-40B\"</p>\n\
          <p>tokenizer = AutoTokenizer.from_pretrained(model)<br>m = AutoModelForCausalLM.from_pretrained(model,\
          \ torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True\
          \ )</p>\n<p>prompt = \"A chat between a curious user and an artificial intelligence\
          \ assistant.\\n\\nUSER:Th\xE0nh ph\u1ED1 H\u1ED3 Ch\xED Minh n\u1EB1m \u1EDF\
          \ \u0111\xE2u?&lt;|endoftext|&gt;ASSISTANT:\"</p>\n<p>inputs = tokenizer(prompt,\
          \ return_tensors=\"pt\").to(\"cuda\")</p>\n<p>output = m.generate(input_ids=inputs[\"\
          input_ids\"],<br>                    attention_mask=inputs[\"attention_mask\"\
          ],<br>                    do_sample=True,<br>                    temperature=0.6,<br>\
          \                    top_p=0.9,<br>                    max_new_tokens=50,)<br>output\
          \ = output[0].to(\"cpu\")<br>print(tokenizer.decode(output))</p>\n</blockquote>\n\
          <p>Can you give us more details on your hardware configuration and the status\
          \ of your CPU or GPU when you run the code?</p>\n"
        raw: "> > i can't give you any advice without seeing your current code\n>\
          \ \n> i use code example \n> \n> from transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\n> import transformers\n> import torch\n> \n> model\
          \ = \"vilm/vulture-40B\"\n> \n> tokenizer = AutoTokenizer.from_pretrained(model)\n\
          > m = AutoModelForCausalLM.from_pretrained(model, torch_dtype=torch.bfloat16,\
          \ device_map=\"auto\", trust_remote_code=True )\n> \n> prompt = \"A chat\
          \ between a curious user and an artificial intelligence assistant.\\n\\\
          nUSER:Th\xE0nh ph\u1ED1 H\u1ED3 Ch\xED Minh n\u1EB1m \u1EDF \u0111\xE2u?<|endoftext|>ASSISTANT:\"\
          \n> \n> inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\
          > \n> output = m.generate(input_ids=inputs[\"input_ids\"],\n>          \
          \           attention_mask=inputs[\"attention_mask\"],\n>              \
          \       do_sample=True,\n>                     temperature=0.6,\n>     \
          \                top_p=0.9,\n>                     max_new_tokens=50,)\n\
          > output = output[0].to(\"cpu\")\n> print(tokenizer.decode(output))\n\n\
          Can you give us more details on your hardware configuration and the status\
          \ of your CPU or GPU when you run the code?"
        updatedAt: '2023-10-15T19:03:31.769Z'
      numEdits: 0
      reactions: []
    id: 652c37833b5997ed71b6d591
    type: comment
  author: DungMinhDao
  content: "> > i can't give you any advice without seeing your current code\n> \n\
    > i use code example \n> \n> from transformers import AutoTokenizer, AutoModelForCausalLM\n\
    > import transformers\n> import torch\n> \n> model = \"vilm/vulture-40B\"\n> \n\
    > tokenizer = AutoTokenizer.from_pretrained(model)\n> m = AutoModelForCausalLM.from_pretrained(model,\
    \ torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True )\n\
    > \n> prompt = \"A chat between a curious user and an artificial intelligence\
    \ assistant.\\n\\nUSER:Th\xE0nh ph\u1ED1 H\u1ED3 Ch\xED Minh n\u1EB1m \u1EDF \u0111\
    \xE2u?<|endoftext|>ASSISTANT:\"\n> \n> inputs = tokenizer(prompt, return_tensors=\"\
    pt\").to(\"cuda\")\n> \n> output = m.generate(input_ids=inputs[\"input_ids\"],\n\
    >                     attention_mask=inputs[\"attention_mask\"],\n>          \
    \           do_sample=True,\n>                     temperature=0.6,\n>       \
    \              top_p=0.9,\n>                     max_new_tokens=50,)\n> output\
    \ = output[0].to(\"cpu\")\n> print(tokenizer.decode(output))\n\nCan you give us\
    \ more details on your hardware configuration and the status of your CPU or GPU\
    \ when you run the code?"
  created_at: 2023-10-15 18:03:31+00:00
  edited: false
  hidden: false
  id: 652c37833b5997ed71b6d591
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/47e56ee6188bb8258a579109121d40ae.svg
      fullname: Dao Xuan Do
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: batman9x
      type: user
    createdAt: '2023-10-16T04:15:59.000Z'
    data:
      edited: false
      editors:
      - batman9x
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44272804260253906
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/47e56ee6188bb8258a579109121d40ae.svg
          fullname: Dao Xuan Do
          isHf: false
          isPro: false
          name: batman9x
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<p>i can't give you any advice\
          \ without seeing your current code</p>\n</blockquote>\n<p>i use code example\
          \ </p>\n<p>from transformers import AutoTokenizer, AutoModelForCausalLM<br>import\
          \ transformers<br>import torch</p>\n<p>model = \"vilm/vulture-40B\"</p>\n\
          <p>tokenizer = AutoTokenizer.from_pretrained(model)<br>m = AutoModelForCausalLM.from_pretrained(model,\
          \ torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True\
          \ )</p>\n<p>prompt = \"A chat between a curious user and an artificial intelligence\
          \ assistant.\\n\\nUSER:Th\xE0nh ph\u1ED1 H\u1ED3 Ch\xED Minh n\u1EB1m \u1EDF\
          \ \u0111\xE2u?&lt;|endoftext|&gt;ASSISTANT:\"</p>\n<p>inputs = tokenizer(prompt,\
          \ return_tensors=\"pt\").to(\"cuda\")</p>\n<p>output = m.generate(input_ids=inputs[\"\
          input_ids\"],<br>                    attention_mask=inputs[\"attention_mask\"\
          ],<br>                    do_sample=True,<br>                    temperature=0.6,<br>\
          \                    top_p=0.9,<br>                    max_new_tokens=50,)<br>output\
          \ = output[0].to(\"cpu\")<br>print(tokenizer.decode(output))</p>\n</blockquote>\n\
          <p>Can you give us more details on your hardware configuration and the status\
          \ of your CPU or GPU when you run the code?</p>\n</blockquote>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/xudGf-pYnAk4ZUZ3jylWI.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/xudGf-pYnAk4ZUZ3jylWI.png\"\
          ></a></p>\n"
        raw: "\n\n> > > i can't give you any advice without seeing your current code\n\
          > > \n> > i use code example \n> > \n> > from transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\n> > import transformers\n> > import torch\n> > \n\
          > > model = \"vilm/vulture-40B\"\n> > \n> > tokenizer = AutoTokenizer.from_pretrained(model)\n\
          > > m = AutoModelForCausalLM.from_pretrained(model, torch_dtype=torch.bfloat16,\
          \ device_map=\"auto\", trust_remote_code=True )\n> > \n> > prompt = \"A\
          \ chat between a curious user and an artificial intelligence assistant.\\\
          n\\nUSER:Th\xE0nh ph\u1ED1 H\u1ED3 Ch\xED Minh n\u1EB1m \u1EDF \u0111\xE2\
          u?<|endoftext|>ASSISTANT:\"\n> > \n> > inputs = tokenizer(prompt, return_tensors=\"\
          pt\").to(\"cuda\")\n> > \n> > output = m.generate(input_ids=inputs[\"input_ids\"\
          ],\n> >                     attention_mask=inputs[\"attention_mask\"],\n\
          > >                     do_sample=True,\n> >                     temperature=0.6,\n\
          > >                     top_p=0.9,\n> >                     max_new_tokens=50,)\n\
          > > output = output[0].to(\"cpu\")\n> > print(tokenizer.decode(output))\n\
          > \n> Can you give us more details on your hardware configuration and the\
          \ status of your CPU or GPU when you run the code?\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/xudGf-pYnAk4ZUZ3jylWI.png)\n"
        updatedAt: '2023-10-16T04:15:59.968Z'
      numEdits: 0
      reactions: []
    id: 652cb8ff90e317f243aee676
    type: comment
  author: batman9x
  content: "\n\n> > > i can't give you any advice without seeing your current code\n\
    > > \n> > i use code example \n> > \n> > from transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\n> > import transformers\n> > import torch\n> > \n> > model\
    \ = \"vilm/vulture-40B\"\n> > \n> > tokenizer = AutoTokenizer.from_pretrained(model)\n\
    > > m = AutoModelForCausalLM.from_pretrained(model, torch_dtype=torch.bfloat16,\
    \ device_map=\"auto\", trust_remote_code=True )\n> > \n> > prompt = \"A chat between\
    \ a curious user and an artificial intelligence assistant.\\n\\nUSER:Th\xE0nh\
    \ ph\u1ED1 H\u1ED3 Ch\xED Minh n\u1EB1m \u1EDF \u0111\xE2u?<|endoftext|>ASSISTANT:\"\
    \n> > \n> > inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n>\
    \ > \n> > output = m.generate(input_ids=inputs[\"input_ids\"],\n> >          \
    \           attention_mask=inputs[\"attention_mask\"],\n> >                  \
    \   do_sample=True,\n> >                     temperature=0.6,\n> >           \
    \          top_p=0.9,\n> >                     max_new_tokens=50,)\n> > output\
    \ = output[0].to(\"cpu\")\n> > print(tokenizer.decode(output))\n> \n> Can you\
    \ give us more details on your hardware configuration and the status of your CPU\
    \ or GPU when you run the code?\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/xudGf-pYnAk4ZUZ3jylWI.png)\n"
  created_at: 2023-10-16 03:15:59+00:00
  edited: false
  hidden: false
  id: 652cb8ff90e317f243aee676
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b86328c76e635315401c7b609d861fc.svg
      fullname: Quan Nguyen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: qnguyen3
      type: user
    createdAt: '2023-10-16T04:56:46.000Z'
    data:
      edited: false
      editors:
      - qnguyen3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5494144558906555
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4b86328c76e635315401c7b609d861fc.svg
          fullname: Quan Nguyen
          isHf: false
          isPro: false
          name: qnguyen3
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>i can't\
          \ give you any advice without seeing your current code</p>\n</blockquote>\n\
          <p>i use code example </p>\n<p>from transformers import AutoTokenizer, AutoModelForCausalLM<br>import\
          \ transformers<br>import torch</p>\n<p>model = \"vilm/vulture-40B\"</p>\n\
          <p>tokenizer = AutoTokenizer.from_pretrained(model)<br>m = AutoModelForCausalLM.from_pretrained(model,\
          \ torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True\
          \ )</p>\n<p>prompt = \"A chat between a curious user and an artificial intelligence\
          \ assistant.\\n\\nUSER:Th\xE0nh ph\u1ED1 H\u1ED3 Ch\xED Minh n\u1EB1m \u1EDF\
          \ \u0111\xE2u?&lt;|endoftext|&gt;ASSISTANT:\"</p>\n<p>inputs = tokenizer(prompt,\
          \ return_tensors=\"pt\").to(\"cuda\")</p>\n<p>output = m.generate(input_ids=inputs[\"\
          input_ids\"],<br>                    attention_mask=inputs[\"attention_mask\"\
          ],<br>                    do_sample=True,<br>                    temperature=0.6,<br>\
          \                    top_p=0.9,<br>                    max_new_tokens=50,)<br>output\
          \ = output[0].to(\"cpu\")<br>print(tokenizer.decode(output))</p>\n</blockquote>\n\
          <p>Can you give us more details on your hardware configuration and the status\
          \ of your CPU or GPU when you run the code?</p>\n</blockquote>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/xudGf-pYnAk4ZUZ3jylWI.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/xudGf-pYnAk4ZUZ3jylWI.png\"\
          ></a></p>\n</blockquote>\n<p>Hi, I believe you will need more than 24GB\
          \ of VRAM to do inference on 40B. Maybe try <code>load_in_4bits=True</code>,\
          \ I do not know how much quantization will help as I have not tried it yet!</p>\n"
        raw: "> > > > i can't give you any advice without seeing your current code\n\
          > > > \n> > > i use code example \n> > > \n> > > from transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\n> > > import transformers\n> > >\
          \ import torch\n> > > \n> > > model = \"vilm/vulture-40B\"\n> > > \n> >\
          \ > tokenizer = AutoTokenizer.from_pretrained(model)\n> > > m = AutoModelForCausalLM.from_pretrained(model,\
          \ torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True\
          \ )\n> > > \n> > > prompt = \"A chat between a curious user and an artificial\
          \ intelligence assistant.\\n\\nUSER:Th\xE0nh ph\u1ED1 H\u1ED3 Ch\xED Minh\
          \ n\u1EB1m \u1EDF \u0111\xE2u?<|endoftext|>ASSISTANT:\"\n> > > \n> > > inputs\
          \ = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n> > > \n> > >\
          \ output = m.generate(input_ids=inputs[\"input_ids\"],\n> > >          \
          \           attention_mask=inputs[\"attention_mask\"],\n> > >          \
          \           do_sample=True,\n> > >                     temperature=0.6,\n\
          > > >                     top_p=0.9,\n> > >                     max_new_tokens=50,)\n\
          > > > output = output[0].to(\"cpu\")\n> > > print(tokenizer.decode(output))\n\
          > > \n> > Can you give us more details on your hardware configuration and\
          \ the status of your CPU or GPU when you run the code?\n> \n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/xudGf-pYnAk4ZUZ3jylWI.png)\n\
          \nHi, I believe you will need more than 24GB of VRAM to do inference on\
          \ 40B. Maybe try `load_in_4bits=True`, I do not know how much quantization\
          \ will help as I have not tried it yet!"
        updatedAt: '2023-10-16T04:56:46.186Z'
      numEdits: 0
      reactions: []
    id: 652cc28ec543a08aa92ae803
    type: comment
  author: qnguyen3
  content: "> > > > i can't give you any advice without seeing your current code\n\
    > > > \n> > > i use code example \n> > > \n> > > from transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\n> > > import transformers\n> > > import torch\n> > > \n\
    > > > model = \"vilm/vulture-40B\"\n> > > \n> > > tokenizer = AutoTokenizer.from_pretrained(model)\n\
    > > > m = AutoModelForCausalLM.from_pretrained(model, torch_dtype=torch.bfloat16,\
    \ device_map=\"auto\", trust_remote_code=True )\n> > > \n> > > prompt = \"A chat\
    \ between a curious user and an artificial intelligence assistant.\\n\\nUSER:Th\xE0\
    nh ph\u1ED1 H\u1ED3 Ch\xED Minh n\u1EB1m \u1EDF \u0111\xE2u?<|endoftext|>ASSISTANT:\"\
    \n> > > \n> > > inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\"\
    )\n> > > \n> > > output = m.generate(input_ids=inputs[\"input_ids\"],\n> > > \
    \                    attention_mask=inputs[\"attention_mask\"],\n> > >       \
    \              do_sample=True,\n> > >                     temperature=0.6,\n>\
    \ > >                     top_p=0.9,\n> > >                     max_new_tokens=50,)\n\
    > > > output = output[0].to(\"cpu\")\n> > > print(tokenizer.decode(output))\n\
    > > \n> > Can you give us more details on your hardware configuration and the\
    \ status of your CPU or GPU when you run the code?\n> \n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/xudGf-pYnAk4ZUZ3jylWI.png)\n\
    \nHi, I believe you will need more than 24GB of VRAM to do inference on 40B. Maybe\
    \ try `load_in_4bits=True`, I do not know how much quantization will help as I\
    \ have not tried it yet!"
  created_at: 2023-10-16 03:56:46+00:00
  edited: false
  hidden: false
  id: 652cc28ec543a08aa92ae803
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/47e56ee6188bb8258a579109121d40ae.svg
      fullname: Dao Xuan Do
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: batman9x
      type: user
    createdAt: '2023-10-16T09:13:49.000Z'
    data:
      edited: false
      editors:
      - batman9x
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6466854810714722
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/47e56ee6188bb8258a579109121d40ae.svg
          fullname: Dao Xuan Do
          isHf: false
          isPro: false
          name: batman9x
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n\
          <p>i can't give you any advice without seeing your current code</p>\n</blockquote>\n\
          <p>i use code example </p>\n<p>from transformers import AutoTokenizer, AutoModelForCausalLM<br>import\
          \ transformers<br>import torch</p>\n<p>model = \"vilm/vulture-40B\"</p>\n\
          <p>tokenizer = AutoTokenizer.from_pretrained(model)<br>m = AutoModelForCausalLM.from_pretrained(model,\
          \ torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True\
          \ )</p>\n<p>prompt = \"A chat between a curious user and an artificial intelligence\
          \ assistant.\\n\\nUSER:Th\xE0nh ph\u1ED1 H\u1ED3 Ch\xED Minh n\u1EB1m \u1EDF\
          \ \u0111\xE2u?&lt;|endoftext|&gt;ASSISTANT:\"</p>\n<p>inputs = tokenizer(prompt,\
          \ return_tensors=\"pt\").to(\"cuda\")</p>\n<p>output = m.generate(input_ids=inputs[\"\
          input_ids\"],<br>                    attention_mask=inputs[\"attention_mask\"\
          ],<br>                    do_sample=True,<br>                    temperature=0.6,<br>\
          \                    top_p=0.9,<br>                    max_new_tokens=50,)<br>output\
          \ = output[0].to(\"cpu\")<br>print(tokenizer.decode(output))</p>\n</blockquote>\n\
          <p>Can you give us more details on your hardware configuration and the status\
          \ of your CPU or GPU when you run the code?</p>\n</blockquote>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/xudGf-pYnAk4ZUZ3jylWI.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/xudGf-pYnAk4ZUZ3jylWI.png\"\
          ></a></p>\n</blockquote>\n<p>Hi, I believe you will need more than 24GB\
          \ of VRAM to do inference on 40B. Maybe try <code>load_in_4bits=True</code>,\
          \ I do not know how much quantization will help as I have not tried it yet!</p>\n\
          </blockquote>\n<p>You don't understand what I mean, what I mean is that\
          \ the model loads normally, but when running, it freezes and nothing appears.\
          \ Your entire code is missing a parameter, trust_remote_code=True, which\
          \ when using your code then RWForCausalLM cannot use load_in_4bits ? I don't\
          \ know if your side is mistaken somewhere :((</p>\n"
        raw: "> > > > > i can't give you any advice without seeing your current code\n\
          > > > > \n> > > > i use code example \n> > > > \n> > > > from transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\n> > > > import transformers\n\
          > > > > import torch\n> > > > \n> > > > model = \"vilm/vulture-40B\"\n>\
          \ > > > \n> > > > tokenizer = AutoTokenizer.from_pretrained(model)\n> >\
          \ > > m = AutoModelForCausalLM.from_pretrained(model, torch_dtype=torch.bfloat16,\
          \ device_map=\"auto\", trust_remote_code=True )\n> > > > \n> > > > prompt\
          \ = \"A chat between a curious user and an artificial intelligence assistant.\\\
          n\\nUSER:Th\xE0nh ph\u1ED1 H\u1ED3 Ch\xED Minh n\u1EB1m \u1EDF \u0111\xE2\
          u?<|endoftext|>ASSISTANT:\"\n> > > > \n> > > > inputs = tokenizer(prompt,\
          \ return_tensors=\"pt\").to(\"cuda\")\n> > > > \n> > > > output = m.generate(input_ids=inputs[\"\
          input_ids\"],\n> > > >                     attention_mask=inputs[\"attention_mask\"\
          ],\n> > > >                     do_sample=True,\n> > > >               \
          \      temperature=0.6,\n> > > >                     top_p=0.9,\n> > > >\
          \                     max_new_tokens=50,)\n> > > > output = output[0].to(\"\
          cpu\")\n> > > > print(tokenizer.decode(output))\n> > > \n> > > Can you give\
          \ us more details on your hardware configuration and the status of your\
          \ CPU or GPU when you run the code?\n> > \n> > \n> > ![image.png](https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/xudGf-pYnAk4ZUZ3jylWI.png)\n\
          > \n> Hi, I believe you will need more than 24GB of VRAM to do inference\
          \ on 40B. Maybe try `load_in_4bits=True`, I do not know how much quantization\
          \ will help as I have not tried it yet!\n\nYou don't understand what I mean,\
          \ what I mean is that the model loads normally, but when running, it freezes\
          \ and nothing appears. Your entire code is missing a parameter, trust_remote_code=True,\
          \ which when using your code then RWForCausalLM cannot use load_in_4bits\
          \ ? I don't know if your side is mistaken somewhere :(("
        updatedAt: '2023-10-16T09:13:49.845Z'
      numEdits: 0
      reactions: []
    id: 652cfecd60f06c6e5241bddc
    type: comment
  author: batman9x
  content: "> > > > > i can't give you any advice without seeing your current code\n\
    > > > > \n> > > > i use code example \n> > > > \n> > > > from transformers import\
    \ AutoTokenizer, AutoModelForCausalLM\n> > > > import transformers\n> > > > import\
    \ torch\n> > > > \n> > > > model = \"vilm/vulture-40B\"\n> > > > \n> > > > tokenizer\
    \ = AutoTokenizer.from_pretrained(model)\n> > > > m = AutoModelForCausalLM.from_pretrained(model,\
    \ torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True )\n\
    > > > > \n> > > > prompt = \"A chat between a curious user and an artificial intelligence\
    \ assistant.\\n\\nUSER:Th\xE0nh ph\u1ED1 H\u1ED3 Ch\xED Minh n\u1EB1m \u1EDF \u0111\
    \xE2u?<|endoftext|>ASSISTANT:\"\n> > > > \n> > > > inputs = tokenizer(prompt,\
    \ return_tensors=\"pt\").to(\"cuda\")\n> > > > \n> > > > output = m.generate(input_ids=inputs[\"\
    input_ids\"],\n> > > >                     attention_mask=inputs[\"attention_mask\"\
    ],\n> > > >                     do_sample=True,\n> > > >                     temperature=0.6,\n\
    > > > >                     top_p=0.9,\n> > > >                     max_new_tokens=50,)\n\
    > > > > output = output[0].to(\"cpu\")\n> > > > print(tokenizer.decode(output))\n\
    > > > \n> > > Can you give us more details on your hardware configuration and\
    \ the status of your CPU or GPU when you run the code?\n> > \n> > \n> > ![image.png](https://cdn-uploads.huggingface.co/production/uploads/629a5dc2bf6ace24dae3194f/xudGf-pYnAk4ZUZ3jylWI.png)\n\
    > \n> Hi, I believe you will need more than 24GB of VRAM to do inference on 40B.\
    \ Maybe try `load_in_4bits=True`, I do not know how much quantization will help\
    \ as I have not tried it yet!\n\nYou don't understand what I mean, what I mean\
    \ is that the model loads normally, but when running, it freezes and nothing appears.\
    \ Your entire code is missing a parameter, trust_remote_code=True, which when\
    \ using your code then RWForCausalLM cannot use load_in_4bits ? I don't know if\
    \ your side is mistaken somewhere :(("
  created_at: 2023-10-16 08:13:49+00:00
  edited: false
  hidden: false
  id: 652cfecd60f06c6e5241bddc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: vilm/vulture-40b
repo_type: model
status: open
target_branch: null
title: Code don't run
