!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mrsjane
conflicting_files: null
created_at: 2023-01-09 21:47:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49a7ac4a7072ddad83edb423a9a3af1f.svg
      fullname: Jane Doe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrsjane
      type: user
    createdAt: '2023-01-09T21:47:00.000Z'
    data:
      edited: false
      editors:
      - mrsjane
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49a7ac4a7072ddad83edb423a9a3af1f.svg
          fullname: Jane Doe
          isHf: false
          isPro: false
          name: mrsjane
          type: user
        html: '<p>I noticed that example 4 has more than 2000 words, but when I wanted
          to try the summary pipeline myself, I faced with below error:</p>

          <p>"Token indices sequence length is longer than the specified maximum sequence
          length for this model (2114 &gt; 1024). Running this sequence through the
          model will result in indexing errors."</p>

          <p>Is there a parameter I need to pass in specifically?</p>

          '
        raw: "I noticed that example 4 has more than 2000 words, but when I wanted\
          \ to try the summary pipeline myself, I faced with below error:\r\n\r\n\"\
          Token indices sequence length is longer than the specified maximum sequence\
          \ length for this model (2114 > 1024). Running this sequence through the\
          \ model will result in indexing errors.\"\r\n\r\nIs there a parameter I\
          \ need to pass in specifically?"
        updatedAt: '2023-01-09T21:47:00.283Z'
      numEdits: 0
      reactions: []
    id: 63bc8b54d8d676a229a50b80
    type: comment
  author: mrsjane
  content: "I noticed that example 4 has more than 2000 words, but when I wanted to\
    \ try the summary pipeline myself, I faced with below error:\r\n\r\n\"Token indices\
    \ sequence length is longer than the specified maximum sequence length for this\
    \ model (2114 > 1024). Running this sequence through the model will result in\
    \ indexing errors.\"\r\n\r\nIs there a parameter I need to pass in specifically?"
  created_at: 2023-01-09 21:47:00+00:00
  edited: false
  hidden: false
  id: 63bc8b54d8d676a229a50b80
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd3cb5453a91a7e5ff2614f892a9c22a.svg
      fullname: Karthick Kaliannan Neelamohan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: knkarthick
      type: user
    createdAt: '2023-01-09T23:06:51.000Z'
    data:
      edited: false
      editors:
      - knkarthick
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd3cb5453a91a7e5ff2614f892a9c22a.svg
          fullname: Karthick Kaliannan Neelamohan
          isHf: false
          isPro: false
          name: knkarthick
          type: user
        html: '<p>Try giving the argument truncation=True while calling the model.</p>

          '
        raw: Try giving the argument truncation=True while calling the model.
        updatedAt: '2023-01-09T23:06:51.868Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mrsjane
    id: 63bc9e0b0718d5f0c1f36ec3
    type: comment
  author: knkarthick
  content: Try giving the argument truncation=True while calling the model.
  created_at: 2023-01-09 23:06:51+00:00
  edited: false
  hidden: false
  id: 63bc9e0b0718d5f0c1f36ec3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c041d47192f71a60d5f1fd64524b49b3.svg
      fullname: IDRISSI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FatiAI
      type: user
    createdAt: '2023-02-23T11:32:43.000Z'
    data:
      edited: false
      editors:
      - FatiAI
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c041d47192f71a60d5f1fd64524b49b3.svg
          fullname: IDRISSI
          isHf: false
          isPro: false
          name: FatiAI
          type: user
        html: '<p>when we accept the truncation it summarize the whole text or it
          stops at the maximum length plz ?</p>

          '
        raw: when we accept the truncation it summarize the whole text or it stops
          at the maximum length plz ?
        updatedAt: '2023-02-23T11:32:43.679Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PyaePK
    id: 63f74edb11ccb733ad202f81
    type: comment
  author: FatiAI
  content: when we accept the truncation it summarize the whole text or it stops at
    the maximum length plz ?
  created_at: 2023-02-23 11:32:43+00:00
  edited: false
  hidden: false
  id: 63f74edb11ccb733ad202f81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
      fullname: Jonathan Yankovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tensiondriven
      type: user
    createdAt: '2023-06-24T15:37:28.000Z'
    data:
      edited: false
      editors:
      - tensiondriven
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.821843147277832
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
          fullname: Jonathan Yankovich
          isHf: false
          isPro: false
          name: tensiondriven
          type: user
        html: '<p>From what I can understaand, <code>truncation=True</code> will truncate
          to the context length.  Based on the error above, I suspect this is 1024
          tokens.</p>

          '
        raw: From what I can understaand, `truncation=True` will truncate to the context
          length.  Based on the error above, I suspect this is 1024 tokens.
        updatedAt: '2023-06-24T15:37:28.060Z'
      numEdits: 0
      reactions: []
    id: 64970db86401266eb672ca96
    type: comment
  author: tensiondriven
  content: From what I can understaand, `truncation=True` will truncate to the context
    length.  Based on the error above, I suspect this is 1024 tokens.
  created_at: 2023-06-24 14:37:28+00:00
  edited: false
  hidden: false
  id: 64970db86401266eb672ca96
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd3cb5453a91a7e5ff2614f892a9c22a.svg
      fullname: Karthick Kaliannan Neelamohan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: knkarthick
      type: user
    createdAt: '2023-09-13T11:00:42.000Z'
    data:
      edited: false
      editors:
      - knkarthick
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.90495365858078
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd3cb5453a91a7e5ff2614f892a9c22a.svg
          fullname: Karthick Kaliannan Neelamohan
          isHf: false
          isPro: false
          name: knkarthick
          type: user
        html: '<p>you need to split the text into chunks of 1024 (This model limitation
          is 1024) and then get the summary and then append the results to get summary.
          if you want to, you can pass the final concatenated result to the model,
          to get the summary of summaries.</p>

          '
        raw: you need to split the text into chunks of 1024 (This model limitation
          is 1024) and then get the summary and then append the results to get summary.
          if you want to, you can pass the final concatenated result to the model,
          to get the summary of summaries.
        updatedAt: '2023-09-13T11:00:42.290Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6501965acf999715fd90025d
    id: 6501965acf999715fd90025c
    type: comment
  author: knkarthick
  content: you need to split the text into chunks of 1024 (This model limitation is
    1024) and then get the summary and then append the results to get summary. if
    you want to, you can pass the final concatenated result to the model, to get the
    summary of summaries.
  created_at: 2023-09-13 10:00:42+00:00
  edited: false
  hidden: false
  id: 6501965acf999715fd90025c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/cd3cb5453a91a7e5ff2614f892a9c22a.svg
      fullname: Karthick Kaliannan Neelamohan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: knkarthick
      type: user
    createdAt: '2023-09-13T11:00:42.000Z'
    data:
      status: closed
    id: 6501965acf999715fd90025d
    type: status-change
  author: knkarthick
  created_at: 2023-09-13 10:00:42+00:00
  id: 6501965acf999715fd90025d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: knkarthick/MEETING_SUMMARY
repo_type: model
status: closed
target_branch: null
title: 'Input text size '
