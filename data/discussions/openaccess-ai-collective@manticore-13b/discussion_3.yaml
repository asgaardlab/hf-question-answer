!!python/object:huggingface_hub.community.DiscussionWithDetails
author: polymer
conflicting_files: null
created_at: 2023-05-19 09:29:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
      fullname: polymer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: polymer
      type: user
    createdAt: '2023-05-19T10:29:59.000Z'
    data:
      edited: true
      editors:
      - polymer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
          fullname: polymer
          isHf: false
          isPro: false
          name: polymer
          type: user
        html: "<p>Just a few ideas:</p>\n<p>Some of the datasets here contain <em>information</em>,\
          \ such as scientific knowledge or logical progressions, while others are\
          \ somewhat more towards the <em>style</em> of writing and various connections\
          \ between parts of the context. The model should benefit from epoch adaptations,\
          \ such as running the information datasets through the model for more iterations\
          \ overall.</p>\n<p>I believe the epochs were performed with a pre-scrambled\
          \ dataset as well, looking at the clear dips in the training loss graph.\
          \ Dataset scrambling done after every epoch should be something important\
          \ to look into for better approximation of the <em>global</em> minimum in\
          \ training. Otherwise, the model will theoretically be biased in a certain\
          \ direction depending on current epoch completion, and this <em>will</em>\
          \ harm the training as it drifts away and back towards the optimum. This\
          \ is important for our fine-tune dataset sizes, as it really takes place\
          \ with a relatively small amount of data.</p>\n<p>I do have some more specific\
          \ thoughts for better performance with the data we have access to \u2014\
          \ if that sounds interesting for you/your team \u2014 that would require\
          \ some more tinkering.</p>\n<p>Great work nonetheless!</p>\n"
        raw: "Just a few ideas:\n\nSome of the datasets here contain *information*,\
          \ such as scientific knowledge or logical progressions, while others are\
          \ somewhat more towards the *style* of writing and various connections between\
          \ parts of the context. The model should benefit from epoch adaptations,\
          \ such as running the information datasets through the model for more iterations\
          \ overall.\n\nI believe the epochs were performed with a pre-scrambled dataset\
          \ as well, looking at the clear dips in the training loss graph. Dataset\
          \ scrambling done after every epoch should be something important to look\
          \ into for better approximation of the *global* minimum in training. Otherwise,\
          \ the model will theoretically be biased in a certain direction depending\
          \ on current epoch completion, and this *will* harm the training as it drifts\
          \ away and back towards the optimum. This is important for our fine-tune\
          \ dataset sizes, as it really takes place with a relatively small amount\
          \ of data.\n\nI do have some more specific thoughts for better performance\
          \ with the data we have access to \u2014 if that sounds interesting for\
          \ you/your team \u2014 that would require some more tinkering.\n\nGreat\
          \ work nonetheless!"
        updatedAt: '2023-05-19T10:43:00.452Z'
      numEdits: 1
      reactions: []
    id: 64674fa7e13ff05d61867768
    type: comment
  author: polymer
  content: "Just a few ideas:\n\nSome of the datasets here contain *information*,\
    \ such as scientific knowledge or logical progressions, while others are somewhat\
    \ more towards the *style* of writing and various connections between parts of\
    \ the context. The model should benefit from epoch adaptations, such as running\
    \ the information datasets through the model for more iterations overall.\n\n\
    I believe the epochs were performed with a pre-scrambled dataset as well, looking\
    \ at the clear dips in the training loss graph. Dataset scrambling done after\
    \ every epoch should be something important to look into for better approximation\
    \ of the *global* minimum in training. Otherwise, the model will theoretically\
    \ be biased in a certain direction depending on current epoch completion, and\
    \ this *will* harm the training as it drifts away and back towards the optimum.\
    \ This is important for our fine-tune dataset sizes, as it really takes place\
    \ with a relatively small amount of data.\n\nI do have some more specific thoughts\
    \ for better performance with the data we have access to \u2014 if that sounds\
    \ interesting for you/your team \u2014 that would require some more tinkering.\n\
    \nGreat work nonetheless!"
  created_at: 2023-05-19 09:29:59+00:00
  edited: true
  hidden: false
  id: 64674fa7e13ff05d61867768
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-05-19T11:53:19.000Z'
    data:
      edited: false
      editors:
      - winglian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: '<p>Thanks for the feedback! Re-shuffling after each epoch is a bit
          labor intensive, but I did incorporate your feedback and stopped it at 3of
          4 epocks and I''m starting again at a lower peak lr with a re shuffled dataset.
          I would definitely be interested in whatever other thoughts you have for
          incremental improvements. Thanks!</p>

          '
        raw: Thanks for the feedback! Re-shuffling after each epoch is a bit labor
          intensive, but I did incorporate your feedback and stopped it at 3of 4 epocks
          and I'm starting again at a lower peak lr with a re shuffled dataset. I
          would definitely be interested in whatever other thoughts you have for incremental
          improvements. Thanks!
        updatedAt: '2023-05-19T11:53:19.902Z'
      numEdits: 0
      reactions: []
    id: 6467632fe92e2372d5cced00
    type: comment
  author: winglian
  content: Thanks for the feedback! Re-shuffling after each epoch is a bit labor intensive,
    but I did incorporate your feedback and stopped it at 3of 4 epocks and I'm starting
    again at a lower peak lr with a re shuffled dataset. I would definitely be interested
    in whatever other thoughts you have for incremental improvements. Thanks!
  created_at: 2023-05-19 10:53:19+00:00
  edited: false
  hidden: false
  id: 6467632fe92e2372d5cced00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
      fullname: polymer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: polymer
      type: user
    createdAt: '2023-05-19T13:48:47.000Z'
    data:
      edited: true
      editors:
      - polymer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
          fullname: polymer
          isHf: false
          isPro: false
          name: polymer
          type: user
        html: "<p>Wow, you sure do move things along quite quickly!</p>\n<p>After\
          \ a bit of morning coffee, the loss graph <em>itself</em> may look similar\
          \ after revision, lol. <em>But</em>, the theory of preventing verbatim biasing\
          \ will likely remain applicable, so this is not an issue. It should certainly\
          \ be reshuffled despite the extra compute.</p>\n<p>The other ideas I had\
          \ were about training more for the inner information content from these\
          \ small datasets, where we extract non-verbatim information to better target\
          \ inner thought processes (which are more generalizable) for the models.\
          \ Bit tight on time at the moment, so I\u2019ll add some details later.</p>\n"
        raw: "Wow, you sure do move things along quite quickly!\n\nAfter a bit of\
          \ morning coffee, the loss graph *itself* may look similar after revision,\
          \ lol. *But*, the theory of preventing verbatim biasing will likely remain\
          \ applicable, so this is not an issue. It should certainly be reshuffled\
          \ despite the extra compute.\n\nThe other ideas I had were about training\
          \ more for the inner information content from these small datasets, where\
          \ we extract non-verbatim information to better target inner thought processes\
          \ (which are more generalizable) for the models. Bit tight on time at the\
          \ moment, so I\u2019ll add some details later."
        updatedAt: '2023-05-19T14:44:30.585Z'
      numEdits: 1
      reactions: []
    id: 64677e3fa48c2b6f0d6107fa
    type: comment
  author: polymer
  content: "Wow, you sure do move things along quite quickly!\n\nAfter a bit of morning\
    \ coffee, the loss graph *itself* may look similar after revision, lol. *But*,\
    \ the theory of preventing verbatim biasing will likely remain applicable, so\
    \ this is not an issue. It should certainly be reshuffled despite the extra compute.\n\
    \nThe other ideas I had were about training more for the inner information content\
    \ from these small datasets, where we extract non-verbatim information to better\
    \ target inner thought processes (which are more generalizable) for the models.\
    \ Bit tight on time at the moment, so I\u2019ll add some details later."
  created_at: 2023-05-19 12:48:47+00:00
  edited: true
  hidden: false
  id: 64677e3fa48c2b6f0d6107fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-05-19T14:04:50.000Z'
    data:
      edited: false
      editors:
      - winglian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: '<p>thanks! one quick question, after restarting the eval loss is on
          the uptick. Is this normal? Wait it out? <a rel="nofollow" href="https://wandb.ai/wing-lian/manticore-13b">https://wandb.ai/wing-lian/manticore-13b</a>.
          Btw, you on discord?</p>

          '
        raw: thanks! one quick question, after restarting the eval loss is on the
          uptick. Is this normal? Wait it out? https://wandb.ai/wing-lian/manticore-13b.
          Btw, you on discord?
        updatedAt: '2023-05-19T14:04:50.079Z'
      numEdits: 0
      reactions: []
    id: 64678202ab75d9cb3c479c7c
    type: comment
  author: winglian
  content: thanks! one quick question, after restarting the eval loss is on the uptick.
    Is this normal? Wait it out? https://wandb.ai/wing-lian/manticore-13b. Btw, you
    on discord?
  created_at: 2023-05-19 13:04:50+00:00
  edited: false
  hidden: false
  id: 64678202ab75d9cb3c479c7c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
      fullname: polymer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: polymer
      type: user
    createdAt: '2023-05-19T14:37:35.000Z'
    data:
      edited: false
      editors:
      - polymer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
          fullname: polymer
          isHf: false
          isPro: false
          name: polymer
          type: user
        html: '<p>If you are referring to manticore pre-alpha &lt;discarded&gt;, the
          peak learning rate was set lower, so it''d only be reasonable to see higher
          eval loss initially (there were less changes made, of course!). Lower learning
          rates would help later loss as the (longer) training converges more steadily,
          but <em>only</em> if previous rates were set too high to begin with in allowing
          convergence. Diminishing returns are definitely expected, because setting
          it too low will waste lots of compute from the longer training needed: there''s
          some room for experimentation. I''m on and off all the time, so no Discord.
          But I will be happy to discuss here as I can see messages when I check my
          email.</p>

          <p>You must have checked this already, but I would also ensure the shuffling
          doesn''t mix different datasets with no split in the middle. I mean, you
          wouldn''t want a Vicuna conversation with some mmlu nonsense being said
          by the user, right?</p>

          '
        raw: 'If you are referring to manticore pre-alpha \<discarded>, the peak learning
          rate was set lower, so it''d only be reasonable to see higher eval loss
          initially (there were less changes made, of course!). Lower learning rates
          would help later loss as the (longer) training converges more steadily,
          but *only* if previous rates were set too high to begin with in allowing
          convergence. Diminishing returns are definitely expected, because setting
          it too low will waste lots of compute from the longer training needed: there''s
          some room for experimentation. I''m on and off all the time, so no Discord.
          But I will be happy to discuss here as I can see messages when I check my
          email.


          You must have checked this already, but I would also ensure the shuffling
          doesn''t mix different datasets with no split in the middle. I mean, you
          wouldn''t want a Vicuna conversation with some mmlu nonsense being said
          by the user, right?'
        updatedAt: '2023-05-19T14:37:35.336Z'
      numEdits: 0
      reactions: []
    id: 646789afa48c2b6f0d61dcf3
    type: comment
  author: polymer
  content: 'If you are referring to manticore pre-alpha \<discarded>, the peak learning
    rate was set lower, so it''d only be reasonable to see higher eval loss initially
    (there were less changes made, of course!). Lower learning rates would help later
    loss as the (longer) training converges more steadily, but *only* if previous
    rates were set too high to begin with in allowing convergence. Diminishing returns
    are definitely expected, because setting it too low will waste lots of compute
    from the longer training needed: there''s some room for experimentation. I''m
    on and off all the time, so no Discord. But I will be happy to discuss here as
    I can see messages when I check my email.


    You must have checked this already, but I would also ensure the shuffling doesn''t
    mix different datasets with no split in the middle. I mean, you wouldn''t want
    a Vicuna conversation with some mmlu nonsense being said by the user, right?'
  created_at: 2023-05-19 13:37:35+00:00
  edited: false
  hidden: false
  id: 646789afa48c2b6f0d61dcf3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-05-19T16:47:18.000Z'
    data:
      edited: false
      editors:
      - winglian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: '<p>I do shuffle the datasets too. the various examples from different
          datasets would always be seperated by a bos and eos token, right? I''ve
          always shuffled them (<a rel="nofollow" href="https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/src/axolotl/utils/data.py#L141-L143">https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/src/axolotl/utils/data.py#L141-L143</a>)  before
          packing them into groups of 2048 tokens (similar to what stackllama does
          <a href="https://huggingface.co/blog/stackllama#supervised-fine-tuning">https://huggingface.co/blog/stackllama#supervised-fine-tuning</a>)</p>

          '
        raw: I do shuffle the datasets too. the various examples from different datasets
          would always be seperated by a bos and eos token, right? I've always shuffled
          them (https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/src/axolotl/utils/data.py#L141-L143)  before
          packing them into groups of 2048 tokens (similar to what stackllama does
          https://huggingface.co/blog/stackllama#supervised-fine-tuning)
        updatedAt: '2023-05-19T16:47:18.072Z'
      numEdits: 0
      reactions: []
    id: 6467a816ab75d9cb3c4a3c75
    type: comment
  author: winglian
  content: I do shuffle the datasets too. the various examples from different datasets
    would always be seperated by a bos and eos token, right? I've always shuffled
    them (https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/src/axolotl/utils/data.py#L141-L143)  before
    packing them into groups of 2048 tokens (similar to what stackllama does https://huggingface.co/blog/stackllama#supervised-fine-tuning)
  created_at: 2023-05-19 15:47:18+00:00
  edited: false
  hidden: false
  id: 6467a816ab75d9cb3c4a3c75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
      fullname: polymer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: polymer
      type: user
    createdAt: '2023-05-19T17:52:09.000Z'
    data:
      edited: false
      editors:
      - polymer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
          fullname: polymer
          isHf: false
          isPro: false
          name: polymer
          type: user
        html: "<p>Ah, the EOS-then-BOS couple is the kind of split I was talking about.\
          \ This should be okay; I can't see any other issues that may arise in training\
          \ for now. Although . . . I do wonder how the lack of an EOS before the\
          \ BOS affects the inference in LLaMa implementations. This is what llama.cpp\
          \ gives to the model for a given prompt (token with ID 1 is BOS):</p>\n\
          <pre><code>     1 -&gt; ''\n   319 -&gt; ' A'\n 13563 -&gt; ' chat'\n  1546\
          \ -&gt; ' between'\n   263 -&gt; ' a'\n 12758 -&gt; ' curious'\n  1404 -&gt;\
          \ ' user'\n   322 -&gt; ' and'\n   385 -&gt; ' an'\n 23116 -&gt; ' artificial'\n\
          </code></pre>\n<p>Didn't see any new runs in wandb, but has training begun\
          \ already with per-epoch shuffling added?</p>\n"
        raw: "Ah, the EOS-then-BOS couple is the kind of split I was talking about.\
          \ This should be okay; I can't see any other issues that may arise in training\
          \ for now. Although . . . I do wonder how the lack of an EOS before the\
          \ BOS affects the inference in LLaMa implementations. This is what llama.cpp\
          \ gives to the model for a given prompt (token with ID 1 is BOS):\n```\n\
          \     1 -> ''\n   319 -> ' A'\n 13563 -> ' chat'\n  1546 -> ' between'\n\
          \   263 -> ' a'\n 12758 -> ' curious'\n  1404 -> ' user'\n   322 -> ' and'\n\
          \   385 -> ' an'\n 23116 -> ' artificial'\n```\nDidn't see any new runs\
          \ in wandb, but has training begun already with per-epoch shuffling added?"
        updatedAt: '2023-05-19T17:52:09.764Z'
      numEdits: 0
      reactions: []
    id: 6467b749a48c2b6f0d64e90b
    type: comment
  author: polymer
  content: "Ah, the EOS-then-BOS couple is the kind of split I was talking about.\
    \ This should be okay; I can't see any other issues that may arise in training\
    \ for now. Although . . . I do wonder how the lack of an EOS before the BOS affects\
    \ the inference in LLaMa implementations. This is what llama.cpp gives to the\
    \ model for a given prompt (token with ID 1 is BOS):\n```\n     1 -> ''\n   319\
    \ -> ' A'\n 13563 -> ' chat'\n  1546 -> ' between'\n   263 -> ' a'\n 12758 ->\
    \ ' curious'\n  1404 -> ' user'\n   322 -> ' and'\n   385 -> ' an'\n 23116 ->\
    \ ' artificial'\n```\nDidn't see any new runs in wandb, but has training begun\
    \ already with per-epoch shuffling added?"
  created_at: 2023-05-19 16:52:09+00:00
  edited: false
  hidden: false
  id: 6467b749a48c2b6f0d64e90b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-05-19T20:49:34.000Z'
    data:
      edited: true
      editors:
      - winglian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: '<p>if you go up to the project level you can see the shuffled one.<a
          rel="nofollow" href="https://wandb.ai/wing-lian/manticore-13b?workspace=user-wing-lian">
          https://wandb.ai/wing-lian/manticore-13b/runs/tcspiljt?workspace=user-wing-lian</a></p>

          <p>here''s a screenshot of what the tokens look like, red are label ignored
          out, values that follow are label, attention_mask, and input iirc</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/1leQ2EmjowWDMTMVX1pTF.png"><img
          alt="Screenshot 2023-05-19 at 4.47.18 PM.png" src="https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/1leQ2EmjowWDMTMVX1pTF.png"></a></p>

          '
        raw: 'if you go up to the project level you can see the shuffled one.[ https://wandb.ai/wing-lian/manticore-13b/runs/tcspiljt?workspace=user-wing-lian](https://wandb.ai/wing-lian/manticore-13b?workspace=user-wing-lian)


          here''s a screenshot of what the tokens look like, red are label ignored
          out, values that follow are label, attention_mask, and input iirc


          ![Screenshot 2023-05-19 at 4.47.18 PM.png](https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/1leQ2EmjowWDMTMVX1pTF.png)'
        updatedAt: '2023-05-19T20:51:40.553Z'
      numEdits: 1
      reactions: []
    id: 6467e0deab75d9cb3c4d9943
    type: comment
  author: winglian
  content: 'if you go up to the project level you can see the shuffled one.[ https://wandb.ai/wing-lian/manticore-13b/runs/tcspiljt?workspace=user-wing-lian](https://wandb.ai/wing-lian/manticore-13b?workspace=user-wing-lian)


    here''s a screenshot of what the tokens look like, red are label ignored out,
    values that follow are label, attention_mask, and input iirc


    ![Screenshot 2023-05-19 at 4.47.18 PM.png](https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/1leQ2EmjowWDMTMVX1pTF.png)'
  created_at: 2023-05-19 19:49:34+00:00
  edited: true
  hidden: false
  id: 6467e0deab75d9cb3c4d9943
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-05-19T20:52:32.000Z'
    data:
      edited: false
      editors:
      - winglian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: '<p>also, I think I''m going to call it quits on that 2nd run. it''s
          looking like it''s going to improve</p>

          '
        raw: also, I think I'm going to call it quits on that 2nd run. it's looking
          like it's going to improve
        updatedAt: '2023-05-19T20:52:32.436Z'
      numEdits: 0
      reactions: []
    id: 6467e190a48c2b6f0d675cc7
    type: comment
  author: winglian
  content: also, I think I'm going to call it quits on that 2nd run. it's looking
    like it's going to improve
  created_at: 2023-05-19 19:52:32+00:00
  edited: false
  hidden: false
  id: 6467e190a48c2b6f0d675cc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
      fullname: polymer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: polymer
      type: user
    createdAt: '2023-05-19T21:53:38.000Z'
    data:
      edited: true
      editors:
      - polymer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
          fullname: polymer
          isHf: false
          isPro: false
          name: polymer
          type: user
        html: "<p>Hmm. Yeah, it is a bit strange with how BOS and EOS are split in\
          \ LLaMA, whereas GPT does not have a separate BOS (I think) to have a headache\
          \ about.</p>\n<p>I have a hunch that <em>somehow</em> getting rid of the\
          \ ending EOS will benefit the generation, as it results in a single unified\
          \ separator between examples. As it stands, we have no way of guaranteeing\
          \ the transformer only cares about the beginning <code>&lt;s&gt;</code>\
          \ token in the context before generation. It\u2019s most often a <code>&lt;/s&gt;&lt;s&gt;</code>,\
          \ then the beginning of the prompt. For all we know, a good generation could\
          \ depend on there being a <code>&lt;/s&gt;</code> as well. No implementation\
          \ out there <em>prepends</em> a <code>&lt;/s&gt;</code> in the prompt .\
          \ . . Something to have a think about, I guess.</p>\n<p><em>Read through\
          \ entire paragraph</em>:</p>\n<p>For the scrambled run, I thought you meant\
          \ the training was going to restart from the beginning! My bad, damn. This\
          \ suggestion was meant to hopefully combat overfitting behavior, allowing\
          \ more generalized parts of the model to catch up. Might not help too much\
          \ with a model already halfway. <em>But</em> \u2026 is eval using the exact\
          \ same (or very similar) dataset to train? Higher loss might actually indicate\
          \ less overfitting, if that\u2019s the case. The loss is only a proxy for\
          \ how \u201Cwell\u201D the model\u2019s doing, and lowest loss is <em>not</em>\
          \ actually the goal. Please do share more details about the eval dataset;\
          \ the fourth epoch could still be an improvement and be very usable, if\
          \ this really is the case!</p>\n<p>It also appears the <code>&lt;/s&gt;</code>\
          \ token is still present in text form somewhere. I think we should check\
          \ out some of the Vicuna examples packed into the context: <a rel=\"nofollow\"\
          \ href=\"https://reddit.com/r/LocalLLaMA/comments/13m44w6/getting_rid_of_s/\"\
          >LocalLLaMA Reddit</a></p>\n<p>Edit: wording.</p>\n"
        raw: "Hmm. Yeah, it is a bit strange with how BOS and EOS are split in LLaMA,\
          \ whereas GPT does not have a separate BOS (I think) to have a headache\
          \ about.\n\nI have a hunch that *somehow* getting rid of the ending EOS\
          \ will benefit the generation, as it results in a single unified separator\
          \ between examples. As it stands, we have no way of guaranteeing the transformer\
          \ only cares about the beginning `<s>` token in the context before generation.\
          \ It\u2019s most often a `</s><s>`, then the beginning of the prompt. For\
          \ all we know, a good generation could depend on there being a `</s>` as\
          \ well. No implementation out there *prepends* a `</s>` in the prompt .\
          \ . . Something to have a think about, I guess.\n\n*Read through entire\
          \ paragraph*:\n\nFor the scrambled run, I thought you meant the training\
          \ was going to restart from the beginning! My bad, damn. This suggestion\
          \ was meant to hopefully combat overfitting behavior, allowing more generalized\
          \ parts of the model to catch up. Might not help too much with a model already\
          \ halfway. *But* \u2026 is eval using the exact same (or very similar) dataset\
          \ to train? Higher loss might actually indicate less overfitting, if that\u2019\
          s the case. The loss is only a proxy for how \u201Cwell\u201D the model\u2019\
          s doing, and lowest loss is *not* actually the goal. Please do share more\
          \ details about the eval dataset; the fourth epoch could still be an improvement\
          \ and be very usable, if this really is the case!\n\nIt also appears the\
          \ `</s>` token is still present in text form somewhere. I think we should\
          \ check out some of the Vicuna examples packed into the context: [LocalLLaMA\
          \ Reddit](https://reddit.com/r/LocalLLaMA/comments/13m44w6/getting_rid_of_s/)\n\
          \nEdit: wording."
        updatedAt: '2023-05-20T06:00:34.455Z'
      numEdits: 1
      reactions: []
    id: 6467efe2e92e2372d5d60d3a
    type: comment
  author: polymer
  content: "Hmm. Yeah, it is a bit strange with how BOS and EOS are split in LLaMA,\
    \ whereas GPT does not have a separate BOS (I think) to have a headache about.\n\
    \nI have a hunch that *somehow* getting rid of the ending EOS will benefit the\
    \ generation, as it results in a single unified separator between examples. As\
    \ it stands, we have no way of guaranteeing the transformer only cares about the\
    \ beginning `<s>` token in the context before generation. It\u2019s most often\
    \ a `</s><s>`, then the beginning of the prompt. For all we know, a good generation\
    \ could depend on there being a `</s>` as well. No implementation out there *prepends*\
    \ a `</s>` in the prompt . . . Something to have a think about, I guess.\n\n*Read\
    \ through entire paragraph*:\n\nFor the scrambled run, I thought you meant the\
    \ training was going to restart from the beginning! My bad, damn. This suggestion\
    \ was meant to hopefully combat overfitting behavior, allowing more generalized\
    \ parts of the model to catch up. Might not help too much with a model already\
    \ halfway. *But* \u2026 is eval using the exact same (or very similar) dataset\
    \ to train? Higher loss might actually indicate less overfitting, if that\u2019\
    s the case. The loss is only a proxy for how \u201Cwell\u201D the model\u2019\
    s doing, and lowest loss is *not* actually the goal. Please do share more details\
    \ about the eval dataset; the fourth epoch could still be an improvement and be\
    \ very usable, if this really is the case!\n\nIt also appears the `</s>` token\
    \ is still present in text form somewhere. I think we should check out some of\
    \ the Vicuna examples packed into the context: [LocalLLaMA Reddit](https://reddit.com/r/LocalLLaMA/comments/13m44w6/getting_rid_of_s/)\n\
    \nEdit: wording."
  created_at: 2023-05-19 20:53:38+00:00
  edited: true
  hidden: false
  id: 6467efe2e92e2372d5d60d3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
      fullname: polymer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: polymer
      type: user
    createdAt: '2023-05-19T22:03:55.000Z'
    data:
      edited: false
      editors:
      - polymer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c2a62179099b6e9d40407534f04abead.svg
          fullname: polymer
          isHf: false
          isPro: false
          name: polymer
          type: user
        html: "<p>The \u201Cspiky\u201D loss is then likely indicating the model jumping\
          \ in and out of a slight overfit. A nice looking loss curve wouldn\u2019\
          t actually indicate good, generalized performance: perfect memorization\
          \ will give you zero loss, but not actually what you want \u2026</p>\n"
        raw: "The \u201Cspiky\u201D loss is then likely indicating the model jumping\
          \ in and out of a slight overfit. A nice looking loss curve wouldn\u2019\
          t actually indicate good, generalized performance: perfect memorization\
          \ will give you zero loss, but not actually what you want \u2026"
        updatedAt: '2023-05-19T22:03:55.945Z'
      numEdits: 0
      reactions: []
    id: 6467f24b3a7c8dda230c6569
    type: comment
  author: polymer
  content: "The \u201Cspiky\u201D loss is then likely indicating the model jumping\
    \ in and out of a slight overfit. A nice looking loss curve wouldn\u2019t actually\
    \ indicate good, generalized performance: perfect memorization will give you zero\
    \ loss, but not actually what you want \u2026"
  created_at: 2023-05-19 21:03:55+00:00
  edited: false
  hidden: false
  id: 6467f24b3a7c8dda230c6569
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: openaccess-ai-collective/manticore-13b
repo_type: model
status: open
target_branch: null
title: Some suggestions for optimization
