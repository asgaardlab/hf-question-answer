!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jeffwadsworth
conflicting_files: null
created_at: 2023-11-30 23:54:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0fa6d53891ffd28118296b5cf51cb70d.svg
      fullname: Jeff Wadsworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jeffwadsworth
      type: user
    createdAt: '2023-11-30T23:54:05.000Z'
    data:
      edited: false
      editors:
      - jeffwadsworth
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9575936794281006
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0fa6d53891ffd28118296b5cf51cb70d.svg
          fullname: Jeff Wadsworth
          isHf: false
          isPro: false
          name: jeffwadsworth
          type: user
        html: '<p>It is the only model I have tested that correctly answers this "tricky"
          question.<br>Mary, a woman, has 3 brothers.  Her brothers have 2 sisters.  How
          many sisters does Mary have?<br>It answers 1.  Most other models (except
          for airoboros 70b 8bit) get this wrong.</p>

          '
        raw: "It is the only model I have tested that correctly answers this \"tricky\"\
          \ question.\r\nMary, a woman, has 3 brothers.  Her brothers have 2 sisters.\
          \  How many sisters does Mary have?\r\nIt answers 1.  Most other models\
          \ (except for airoboros 70b 8bit) get this wrong."
        updatedAt: '2023-11-30T23:54:05.297Z'
      numEdits: 0
      reactions: []
    id: 6569209d8369400a580e66a9
    type: comment
  author: jeffwadsworth
  content: "It is the only model I have tested that correctly answers this \"tricky\"\
    \ question.\r\nMary, a woman, has 3 brothers.  Her brothers have 2 sisters.  How\
    \ many sisters does Mary have?\r\nIt answers 1.  Most other models (except for\
    \ airoboros 70b 8bit) get this wrong."
  created_at: 2023-11-30 23:54:05+00:00
  edited: false
  hidden: false
  id: 6569209d8369400a580e66a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64548986cd09ceba0e1709cb/muGiatjmPfzxYb3Rjcqas.jpeg?w=200&h=200&f=face
      fullname: j
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 21world
      type: user
    createdAt: '2023-12-07T12:51:00.000Z'
    data:
      edited: false
      editors:
      - 21world
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4805578887462616
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64548986cd09ceba0e1709cb/muGiatjmPfzxYb3Rjcqas.jpeg?w=200&h=200&f=face
          fullname: j
          isHf: false
          isPro: false
          name: 21world
          type: user
        html: "<p>llm_load_print_meta: model ftype      = mostly Q4_K - Small<br>llm_load_print_meta:\
          \ model params     = 67.43 B<br>llm_load_print_meta: model size       =\
          \ 35.62 GiB (4.54 BPW)<br>llm_load_print_meta: general.name   = LLaMA v2<br>llm_load_print_meta:\
          \ BOS token = 100000 '&lt;\uFF5Cbegin\u2581of\u2581sentence\uFF5C&gt;'<br>llm_load_print_meta:\
          \ EOS token = 100001 '&lt;\uFF5Cend\u2581of\u2581sentence\uFF5C&gt;'<br>llm_load_print_meta:\
          \ PAD token = 100001 '&lt;\uFF5Cend\u2581of\u2581sentence\uFF5C&gt;'<br>llm_load_print_meta:\
          \ LF token  = 126 '\xC4'<br>llm_load_tensors: ggml ctx size =    0.31 MiB<br>llm_load_tensors:\
          \ using OpenCL for GPU acceleration<br>llm_load_tensors: mem required  =\
          \ 36471.28 MiB<br>llm_load_tensors: offloading 0 repeating layers to GPU<br>llm_load_tensors:\
          \ offloaded 0/96 layers to GPU<br>llm_load_tensors: VRAM used: 0.00 MiB<br>...................................................................................................<br>llama_new_context_with_model:\
          \ n_ctx      = 512<br>llama_new_context_with_model: freq_base  = 10000.0<br>llama_new_context_with_model:\
          \ freq_scale = 1<br>llama_new_context_with_model: kv self size  =  190.00\
          \ MiB<br>llama_build_graph: non-view tensors processed: 2189/2189<br>llama_new_context_with_model:\
          \ compute buffer total size = 219.06 MiB</p>\n<blockquote>\n<p>hello<br>Hello!\
          \ How can I assist you today?</p>\n</blockquote>\n<blockquote>\n<p>Mary,\
          \ a woman, has 3 brothers. Her brothers have 2 sisters. How many sisters\
          \ does Mary have?<br>If Mary has 3 brothers and her brothers have 2 sisters,\
          \ then Mary is one of the two sisters that her brothers are referring to.\
          \ Therefore, Mary has only one sister.</p>\n</blockquote>\n<p>============================================================================<br>\
          \ ./build/bin/main -m ./models/deepseek_coder/deepseek-llm-67b-chat.Q4_K_S.gguf\
          \ <br>                -ins <br>                --top-p 0.80 <br>       \
          \         --top-k 200 <br>                --temp 0.144 <br>            \
          \    -t 18 <br>                --multiline-input <br>                --color\
          \ <br>                --log-disable</p>\n"
        raw: "llm_load_print_meta: model ftype      = mostly Q4_K - Small\nllm_load_print_meta:\
          \ model params     = 67.43 B\nllm_load_print_meta: model size       = 35.62\
          \ GiB (4.54 BPW) \nllm_load_print_meta: general.name   = LLaMA v2\nllm_load_print_meta:\
          \ BOS token = 100000 '<\uFF5Cbegin\u2581of\u2581sentence\uFF5C>'\nllm_load_print_meta:\
          \ EOS token = 100001 '<\uFF5Cend\u2581of\u2581sentence\uFF5C>'\nllm_load_print_meta:\
          \ PAD token = 100001 '<\uFF5Cend\u2581of\u2581sentence\uFF5C>'\nllm_load_print_meta:\
          \ LF token  = 126 '\xC4'\nllm_load_tensors: ggml ctx size =    0.31 MiB\n\
          llm_load_tensors: using OpenCL for GPU acceleration\nllm_load_tensors: mem\
          \ required  = 36471.28 MiB\nllm_load_tensors: offloading 0 repeating layers\
          \ to GPU\nllm_load_tensors: offloaded 0/96 layers to GPU\nllm_load_tensors:\
          \ VRAM used: 0.00 MiB\n...................................................................................................\n\
          llama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model:\
          \ freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_new_context_with_model:\
          \ kv self size  =  190.00 MiB\nllama_build_graph: non-view tensors processed:\
          \ 2189/2189\nllama_new_context_with_model: compute buffer total size = 219.06\
          \ MiB\n\n> hello \nHello! How can I assist you today?\n\n> Mary, a woman,\
          \ has 3 brothers. Her brothers have 2 sisters. How many sisters does Mary\
          \ have? \nIf Mary has 3 brothers and her brothers have 2 sisters, then Mary\
          \ is one of the two sisters that her brothers are referring to. Therefore,\
          \ Mary has only one sister.\n\n============================================================================\
          \ \n ./build/bin/main -m ./models/deepseek_coder/deepseek-llm-67b-chat.Q4_K_S.gguf\
          \ \\\n                -ins \\\n                --top-p 0.80 \\\n       \
          \         --top-k 200 \\\n                --temp 0.144 \\\n            \
          \    -t 18 \\\n                --multiline-input \\\n                --color\
          \ \\\n                --log-disable\n"
        updatedAt: '2023-12-07T12:51:00.637Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - jeffwadsworth
    id: 6571bfb483b8f5ce431833fd
    type: comment
  author: 21world
  content: "llm_load_print_meta: model ftype      = mostly Q4_K - Small\nllm_load_print_meta:\
    \ model params     = 67.43 B\nllm_load_print_meta: model size       = 35.62 GiB\
    \ (4.54 BPW) \nllm_load_print_meta: general.name   = LLaMA v2\nllm_load_print_meta:\
    \ BOS token = 100000 '<\uFF5Cbegin\u2581of\u2581sentence\uFF5C>'\nllm_load_print_meta:\
    \ EOS token = 100001 '<\uFF5Cend\u2581of\u2581sentence\uFF5C>'\nllm_load_print_meta:\
    \ PAD token = 100001 '<\uFF5Cend\u2581of\u2581sentence\uFF5C>'\nllm_load_print_meta:\
    \ LF token  = 126 '\xC4'\nllm_load_tensors: ggml ctx size =    0.31 MiB\nllm_load_tensors:\
    \ using OpenCL for GPU acceleration\nllm_load_tensors: mem required  = 36471.28\
    \ MiB\nllm_load_tensors: offloading 0 repeating layers to GPU\nllm_load_tensors:\
    \ offloaded 0/96 layers to GPU\nllm_load_tensors: VRAM used: 0.00 MiB\n...................................................................................................\n\
    llama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model:\
    \ freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_new_context_with_model:\
    \ kv self size  =  190.00 MiB\nllama_build_graph: non-view tensors processed:\
    \ 2189/2189\nllama_new_context_with_model: compute buffer total size = 219.06\
    \ MiB\n\n> hello \nHello! How can I assist you today?\n\n> Mary, a woman, has\
    \ 3 brothers. Her brothers have 2 sisters. How many sisters does Mary have? \n\
    If Mary has 3 brothers and her brothers have 2 sisters, then Mary is one of the\
    \ two sisters that her brothers are referring to. Therefore, Mary has only one\
    \ sister.\n\n============================================================================\
    \ \n ./build/bin/main -m ./models/deepseek_coder/deepseek-llm-67b-chat.Q4_K_S.gguf\
    \ \\\n                -ins \\\n                --top-p 0.80 \\\n             \
    \   --top-k 200 \\\n                --temp 0.144 \\\n                -t 18 \\\n\
    \                --multiline-input \\\n                --color \\\n          \
    \      --log-disable\n"
  created_at: 2023-12-07 12:51:00+00:00
  edited: false
  hidden: false
  id: 6571bfb483b8f5ce431833fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0fa6d53891ffd28118296b5cf51cb70d.svg
      fullname: Jeff Wadsworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jeffwadsworth
      type: user
    createdAt: '2023-12-09T16:26:15.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/0fa6d53891ffd28118296b5cf51cb70d.svg
          fullname: Jeff Wadsworth
          isHf: false
          isPro: false
          name: jeffwadsworth
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-09T16:27:13.052Z'
      numEdits: 0
      reactions: []
    id: 6574952712ae60c5427ea227
    type: comment
  author: jeffwadsworth
  content: This comment has been hidden
  created_at: 2023-12-09 16:26:15+00:00
  edited: true
  hidden: true
  id: 6574952712ae60c5427ea227
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/deepseek-llm-67b-chat-GGUF
repo_type: model
status: open
target_branch: null
title: This is a very impressive model.  Using the 8bit version.
