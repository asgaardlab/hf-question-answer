!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lamhieu
conflicting_files: null
created_at: 2023-12-17 07:32:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/600ae38cc92b79f54efd4556/i-3xlAydNHkECnOkOqXmZ.png?w=200&h=200&f=face
      fullname: Hieu Lam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lamhieu
      type: user
    createdAt: '2023-12-17T07:32:28.000Z'
    data:
      edited: false
      editors:
      - lamhieu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9185670614242554
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/600ae38cc92b79f54efd4556/i-3xlAydNHkECnOkOqXmZ.png?w=200&h=200&f=face
          fullname: Hieu Lam
          isHf: false
          isPro: false
          name: lamhieu
          type: user
        html: '<p>Honestly, this is a very good multilingual model for embeddings
          for relevance search tasks, but the problem is that the input limit is too
          short. I wonder if you guys plan to make another version with a larger limit
          like 256 or 512? I think it will be great.</p>

          '
        raw: Honestly, this is a very good multilingual model for embeddings for relevance
          search tasks, but the problem is that the input limit is too short. I wonder
          if you guys plan to make another version with a larger limit like 256 or
          512? I think it will be great.
        updatedAt: '2023-12-17T07:32:28.413Z'
      numEdits: 0
      reactions: []
    id: 657ea40c7295872e015f1d62
    type: comment
  author: lamhieu
  content: Honestly, this is a very good multilingual model for embeddings for relevance
    search tasks, but the problem is that the input limit is too short. I wonder if
    you guys plan to make another version with a larger limit like 256 or 512? I think
    it will be great.
  created_at: 2023-12-17 07:32:28+00:00
  edited: false
  hidden: false
  id: 657ea40c7295872e015f1d62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
      fullname: Tom Aarsen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: tomaarsen
      type: user
    createdAt: '2023-12-18T11:04:21.000Z'
    data:
      edited: false
      editors:
      - tomaarsen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.957990825176239
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
          fullname: Tom Aarsen
          isHf: true
          isPro: false
          name: tomaarsen
          type: user
        html: '<p>Hello!</p>

          <p>It''s certainly possible to train models with higher limits, but the
          real bottleneck is that there''s not a lot of good training data with lengths
          larger than e.g. 128. It also gets harder to annotate/label those longer
          texts well. As a result, we can train a larger model easily, but it simply
          won''t work well with longer texts. That''s sadly not exactly an improvement
          over the current situation.</p>

          <ul>

          <li>Tom Aarsen</li>

          </ul>

          '
        raw: 'Hello!


          It''s certainly possible to train models with higher limits, but the real
          bottleneck is that there''s not a lot of good training data with lengths
          larger than e.g. 128. It also gets harder to annotate/label those longer
          texts well. As a result, we can train a larger model easily, but it simply
          won''t work well with longer texts. That''s sadly not exactly an improvement
          over the current situation.


          - Tom Aarsen'
        updatedAt: '2023-12-18T11:04:21.759Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - lamhieu
    id: 658027357a76aad30fc77054
    type: comment
  author: tomaarsen
  content: 'Hello!


    It''s certainly possible to train models with higher limits, but the real bottleneck
    is that there''s not a lot of good training data with lengths larger than e.g.
    128. It also gets harder to annotate/label those longer texts well. As a result,
    we can train a larger model easily, but it simply won''t work well with longer
    texts. That''s sadly not exactly an improvement over the current situation.


    - Tom Aarsen'
  created_at: 2023-12-18 11:04:21+00:00
  edited: false
  hidden: false
  id: 658027357a76aad30fc77054
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/600ae38cc92b79f54efd4556/i-3xlAydNHkECnOkOqXmZ.png?w=200&h=200&f=face
      fullname: Hieu Lam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lamhieu
      type: user
    createdAt: '2023-12-19T03:00:58.000Z'
    data:
      edited: true
      editors:
      - lamhieu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9640012979507446
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/600ae38cc92b79f54efd4556/i-3xlAydNHkECnOkOqXmZ.png?w=200&h=200&f=face
          fullname: Hieu Lam
          isHf: false
          isPro: false
          name: lamhieu
          type: user
        html: '<p>Thank you, I understand. These are just current embeddings models.
          If the limit is higher, the quality is not as good as this model (even paid).</p>

          '
        raw: Thank you, I understand. These are just current embeddings models. If
          the limit is higher, the quality is not as good as this model (even paid).
        updatedAt: '2023-12-19T03:01:13.746Z'
      numEdits: 1
      reactions: []
    id: 6581076a7508d04097c7dab0
    type: comment
  author: lamhieu
  content: Thank you, I understand. These are just current embeddings models. If the
    limit is higher, the quality is not as good as this model (even paid).
  created_at: 2023-12-19 03:00:58+00:00
  edited: true
  hidden: false
  id: 6581076a7508d04097c7dab0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: sentence-transformers/paraphrase-multilingual-mpnet-base-v2
repo_type: model
status: open
target_branch: null
title: Is a new version supporting a larger maximum sentence limit (256 or 512) possible
  in the future?
