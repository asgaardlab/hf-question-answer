!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yuricampbell
conflicting_files: null
created_at: 2023-01-07 14:50:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673100772179-62431bf7b1fc6718f4fb0e89.jpeg?w=200&h=200&f=face
      fullname: Yuri Campbell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yuricampbell
      type: user
    createdAt: '2023-01-07T14:50:22.000Z'
    data:
      edited: false
      editors:
      - yuricampbell
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673100772179-62431bf7b1fc6718f4fb0e89.jpeg?w=200&h=200&f=face
          fullname: Yuri Campbell
          isHf: false
          isPro: false
          name: yuricampbell
          type: user
        html: "<p>Hi dear community,</p>\n<p>I and my team are very grateful for the\
          \ multilingual models of Sentence Transformers. We also have been using\
          \ some of them in our work and would like to cite it properly.<br>Hence,\
          \ I would like to know which is the training dataset for the MPNet model/module\
          \ in the SBERT-like model architecture.</p>\n<p>In the paper --- Reimers,\
          \ Nils; Gurevych, Iryna (2020): Making Monolingual Sentence Embeddings Multilingual\
          \ using Knowledge Distillation: arXiv, 2020. Available online at <a rel=\"\
          nofollow\" href=\"https://arxiv.org/pdf/2004.09813\">https://arxiv.org/pdf/2004.09813</a>\
          \ ---<br>there is only the following passages which hints to the dataset\
          \ used in a similar setup:</p>\n<ol>\n<li><p>\"XLM-R \u2190 SBERT-paraphrases:\
          \ We train XLM-R to imitate SBERT-paraphrases, a RoBERTa model trained on\
          \ more than 50 Million English paraphrase pairs.\"</p>\n</li>\n<li><p>\"\
          ... Even though SBERT-nli-stsb was trained on the STSbenchmark train set,\
          \ we observe the best performance by SBERT-paraphrase, which was not trained\
          \ with any STS dataset. Instead, it was trained on a large and broad paraphrase\
          \ corpus, mainly derived from Wikipedia, which generalizes well to various\
          \ topics.\"</p>\n</li>\n</ol>\n<p>Looking further on model card <a href=\"\
          https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\
          >https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2</a>\
          \ is not of much help currently.<br>However, while looking for similar models,\
          \ I found that a similar question was written about the model sentence-transformers/paraphrase-MiniLM-L6-v2,\
          \ in this link: <a href=\"https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2/discussions/2\"\
          >https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2/discussions/2</a></p>\n\
          <p>There, <span data-props=\"{&quot;user&quot;:&quot;nreimers&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nreimers\">@<span class=\"\
          underline\">nreimers</span></a></span>\n\n\t</span></span> points kindly\
          \ to the documentation: <a rel=\"nofollow\" href=\"https://www.sbert.net/examples/training/paraphrases/README.html#pre-trained-models\"\
          >https://www.sbert.net/examples/training/paraphrases/README.html#pre-trained-models</a><br>Which\
          \ indeed answers the question of the user CSHorten. However, on the page\
          \ there is no reference to sentence-transformers/paraphrase-multilingual-mpnet-base-v2</p>\n\
          <p>Hence, I wonder if the training data for the teacher model in this case\
          \ is similar to the data of paraphrase-MiniLM-L6-v2 or if it is similar\
          \ to the SBERT-paraphrase (from the paper). In the case of the later, I\
          \ wonder which dataset this exactly might be. Because, on the documentation\
          \ <a rel=\"nofollow\" href=\"https://www.sbert.net/examples/training/paraphrases/README.html#datasets\"\
          >https://www.sbert.net/examples/training/paraphrases/README.html#datasets</a>,\
          \ I could not find the reference to a dataset --mainly derived from Wikipedia--\
          \ and with --more than 50 million English paraphrase pairs--. </p>\n<p>If\
          \ you are reading this question, N.Reimers, thank you for your amazing work.\
          \ </p>\n"
        raw: "Hi dear community,\r\n\r\nI and my team are very grateful for the multilingual\
          \ models of Sentence Transformers. We also have been using some of them\
          \ in our work and would like to cite it properly.\r\nHence, I would like\
          \ to know which is the training dataset for the MPNet model/module in the\
          \ SBERT-like model architecture.\r\n\r\nIn the paper --- Reimers, Nils;\
          \ Gurevych, Iryna (2020): Making Monolingual Sentence Embeddings Multilingual\
          \ using Knowledge Distillation: arXiv, 2020. Available online at https://arxiv.org/pdf/2004.09813\
          \ --- \r\nthere is only the following passages which hints to the dataset\
          \ used in a similar setup:\r\n\r\n1) \"XLM-R \u2190 SBERT-paraphrases: We\
          \ train XLM-R to imitate SBERT-paraphrases, a RoBERTa model trained on more\
          \ than 50 Million English paraphrase pairs.\"\r\n\r\n2) \"... Even though\
          \ SBERT-nli-stsb was trained on the STSbenchmark train set, we observe the\
          \ best performance by SBERT-paraphrase, which was not trained with any STS\
          \ dataset. Instead, it was trained on a large and broad paraphrase corpus,\
          \ mainly derived from Wikipedia, which generalizes well to various topics.\"\
          \r\n\r\nLooking further on model card https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\
          \ is not of much help currently.\r\nHowever, while looking for similar models,\
          \ I found that a similar question was written about the model sentence-transformers/paraphrase-MiniLM-L6-v2,\
          \ in this link: https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2/discussions/2\r\
          \n\r\nThere, @nreimers points kindly to the documentation: https://www.sbert.net/examples/training/paraphrases/README.html#pre-trained-models\r\
          \nWhich indeed answers the question of the user CSHorten. However, on the\
          \ page there is no reference to sentence-transformers/paraphrase-multilingual-mpnet-base-v2\r\
          \n\r\nHence, I wonder if the training data for the teacher model in this\
          \ case is similar to the data of paraphrase-MiniLM-L6-v2 or if it is similar\
          \ to the SBERT-paraphrase (from the paper). In the case of the later, I\
          \ wonder which dataset this exactly might be. Because, on the documentation\
          \ https://www.sbert.net/examples/training/paraphrases/README.html#datasets,\
          \ I could not find the reference to a dataset --mainly derived from Wikipedia--\
          \ and with --more than 50 million English paraphrase pairs--. \r\n\r\nIf\
          \ you are reading this question, N.Reimers, thank you for your amazing work. "
        updatedAt: '2023-01-07T14:50:22.647Z'
      numEdits: 0
      reactions: []
    id: 63b986ae205688cd2f8cf96b
    type: comment
  author: yuricampbell
  content: "Hi dear community,\r\n\r\nI and my team are very grateful for the multilingual\
    \ models of Sentence Transformers. We also have been using some of them in our\
    \ work and would like to cite it properly.\r\nHence, I would like to know which\
    \ is the training dataset for the MPNet model/module in the SBERT-like model architecture.\r\
    \n\r\nIn the paper --- Reimers, Nils; Gurevych, Iryna (2020): Making Monolingual\
    \ Sentence Embeddings Multilingual using Knowledge Distillation: arXiv, 2020.\
    \ Available online at https://arxiv.org/pdf/2004.09813 --- \r\nthere is only the\
    \ following passages which hints to the dataset used in a similar setup:\r\n\r\
    \n1) \"XLM-R \u2190 SBERT-paraphrases: We train XLM-R to imitate SBERT-paraphrases,\
    \ a RoBERTa model trained on more than 50 Million English paraphrase pairs.\"\r\
    \n\r\n2) \"... Even though SBERT-nli-stsb was trained on the STSbenchmark train\
    \ set, we observe the best performance by SBERT-paraphrase, which was not trained\
    \ with any STS dataset. Instead, it was trained on a large and broad paraphrase\
    \ corpus, mainly derived from Wikipedia, which generalizes well to various topics.\"\
    \r\n\r\nLooking further on model card https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\
    \ is not of much help currently.\r\nHowever, while looking for similar models,\
    \ I found that a similar question was written about the model sentence-transformers/paraphrase-MiniLM-L6-v2,\
    \ in this link: https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2/discussions/2\r\
    \n\r\nThere, @nreimers points kindly to the documentation: https://www.sbert.net/examples/training/paraphrases/README.html#pre-trained-models\r\
    \nWhich indeed answers the question of the user CSHorten. However, on the page\
    \ there is no reference to sentence-transformers/paraphrase-multilingual-mpnet-base-v2\r\
    \n\r\nHence, I wonder if the training data for the teacher model in this case\
    \ is similar to the data of paraphrase-MiniLM-L6-v2 or if it is similar to the\
    \ SBERT-paraphrase (from the paper). In the case of the later, I wonder which\
    \ dataset this exactly might be. Because, on the documentation https://www.sbert.net/examples/training/paraphrases/README.html#datasets,\
    \ I could not find the reference to a dataset --mainly derived from Wikipedia--\
    \ and with --more than 50 million English paraphrase pairs--. \r\n\r\nIf you are\
    \ reading this question, N.Reimers, thank you for your amazing work. "
  created_at: 2023-01-07 14:50:22+00:00
  edited: false
  hidden: false
  id: 63b986ae205688cd2f8cf96b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6d2fe8117535665d7790e0b700b1fbd.svg
      fullname: Wilfredo Martel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wilfoderek
      type: user
    createdAt: '2023-05-27T23:12:12.000Z'
    data:
      edited: false
      editors:
      - wilfoderek
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6d2fe8117535665d7790e0b700b1fbd.svg
          fullname: Wilfredo Martel
          isHf: false
          isPro: false
          name: wilfoderek
          type: user
        html: '<p>Did you get any answer?</p>

          '
        raw: Did you get any answer?
        updatedAt: '2023-05-27T23:12:12.518Z'
      numEdits: 0
      reactions: []
    id: 64728e4c97a75cc77aba6ca6
    type: comment
  author: wilfoderek
  content: Did you get any answer?
  created_at: 2023-05-27 22:12:12+00:00
  edited: false
  hidden: false
  id: 64728e4c97a75cc77aba6ca6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: sentence-transformers/paraphrase-multilingual-mpnet-base-v2
repo_type: model
status: open
target_branch: null
title: Which is the paraphrase training dataset used for the teacher model?
