!!python/object:huggingface_hub.community.DiscussionWithDetails
author: marella
conflicting_files: null
created_at: 2023-05-21 02:17:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/93b6433d2741979d473811732948b04d.svg
      fullname: Ravindra Marella
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marella
      type: user
    createdAt: '2023-05-21T03:17:08.000Z'
    data:
      edited: false
      editors:
      - marella
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/93b6433d2741979d473811732948b04d.svg
          fullname: Ravindra Marella
          isHf: false
          isPro: false
          name: marella
          type: user
        html: '<p>Hi, thanks for uploading the models.</p>

          <p>I created Python bindings for the GGML models <a rel="nofollow" href="https://github.com/marella/ctransformers">https://github.com/marella/ctransformers</a>
          which currently supports MPT, LLaMA and many other models (see <a rel="nofollow"
          href="https://github.com/marella/ctransformers#supported-models">Supported
          Models</a>).</p>

          <p>It provides a unified interface for all models, supports LangChain and
          can be used with Hugging Face Hub models:</p>

          <pre><code class="language-py"><span class="hljs-keyword">from</span> ctransformers
          <span class="hljs-keyword">import</span> AutoModelForCausalLM


          llm = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">''TheBloke/MPT-7B-GGML''</span>,
          model_type=<span class="hljs-string">''mpt''</span>, model_file=<span class="hljs-string">''mpt-7b.ggmlv3.q4_0.bin''</span>)


          <span class="hljs-built_in">print</span>(llm(<span class="hljs-string">''AI
          is going to''</span>))

          </code></pre>

          <hr>

          <p>If you add <code>config.json</code> file to your model repo with <code>{
          "model_type": "mpt" }</code> then <code>model_type</code> can be omitted:</p>

          <pre><code class="language-py">llm = AutoModelForCausalLM.from_pretrained(<span
          class="hljs-string">''TheBloke/MPT-7B-GGML''</span>, model_file=<span class="hljs-string">''mpt-7b.ggmlv3.q4_0.bin''</span>)

          </code></pre>

          <p>If there is only one model file in repo then <code>model_file</code>
          can be omitted:</p>

          <pre><code class="language-py">llm = AutoModelForCausalLM.from_pretrained(<span
          class="hljs-string">''TheBloke/MPT-7B-GGML''</span>)

          </code></pre>

          <p>Please see <a href="https://huggingface.co/marella/gpt-2-ggml">marella/gpt-2-ggml</a>
          for reference.</p>

          '
        raw: "Hi, thanks for uploading the models.\r\n\r\nI created Python bindings\
          \ for the GGML models https://github.com/marella/ctransformers which currently\
          \ supports MPT, LLaMA and many other models (see [Supported Models](https://github.com/marella/ctransformers#supported-models)).\r\
          \n\r\nIt provides a unified interface for all models, supports LangChain\
          \ and can be used with Hugging Face Hub models:\r\n\r\n```py\r\nfrom ctransformers\
          \ import AutoModelForCausalLM\r\n\r\nllm = AutoModelForCausalLM.from_pretrained('TheBloke/MPT-7B-GGML',\
          \ model_type='mpt', model_file='mpt-7b.ggmlv3.q4_0.bin')\r\n\r\nprint(llm('AI\
          \ is going to'))\r\n```\r\n\r\n---\r\n\r\nIf you add `config.json` file\
          \ to your model repo with `{ \"model_type\": \"mpt\" }` then `model_type`\
          \ can be omitted:\r\n\r\n```py\r\nllm = AutoModelForCausalLM.from_pretrained('TheBloke/MPT-7B-GGML',\
          \ model_file='mpt-7b.ggmlv3.q4_0.bin')\r\n```\r\n\r\nIf there is only one\
          \ model file in repo then `model_file` can be omitted:\r\n\r\n```py\r\n\
          llm = AutoModelForCausalLM.from_pretrained('TheBloke/MPT-7B-GGML')\r\n```\r\
          \n\r\nPlease see [marella/gpt-2-ggml](https://huggingface.co/marella/gpt-2-ggml)\
          \ for reference."
        updatedAt: '2023-05-21T03:17:08.561Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - vsns
    id: 64698d3454873f0043b07a75
    type: comment
  author: marella
  content: "Hi, thanks for uploading the models.\r\n\r\nI created Python bindings\
    \ for the GGML models https://github.com/marella/ctransformers which currently\
    \ supports MPT, LLaMA and many other models (see [Supported Models](https://github.com/marella/ctransformers#supported-models)).\r\
    \n\r\nIt provides a unified interface for all models, supports LangChain and can\
    \ be used with Hugging Face Hub models:\r\n\r\n```py\r\nfrom ctransformers import\
    \ AutoModelForCausalLM\r\n\r\nllm = AutoModelForCausalLM.from_pretrained('TheBloke/MPT-7B-GGML',\
    \ model_type='mpt', model_file='mpt-7b.ggmlv3.q4_0.bin')\r\n\r\nprint(llm('AI\
    \ is going to'))\r\n```\r\n\r\n---\r\n\r\nIf you add `config.json` file to your\
    \ model repo with `{ \"model_type\": \"mpt\" }` then `model_type` can be omitted:\r\
    \n\r\n```py\r\nllm = AutoModelForCausalLM.from_pretrained('TheBloke/MPT-7B-GGML',\
    \ model_file='mpt-7b.ggmlv3.q4_0.bin')\r\n```\r\n\r\nIf there is only one model\
    \ file in repo then `model_file` can be omitted:\r\n\r\n```py\r\nllm = AutoModelForCausalLM.from_pretrained('TheBloke/MPT-7B-GGML')\r\
    \n```\r\n\r\nPlease see [marella/gpt-2-ggml](https://huggingface.co/marella/gpt-2-ggml)\
    \ for reference."
  created_at: 2023-05-21 02:17:08+00:00
  edited: false
  hidden: false
  id: 64698d3454873f0043b07a75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-21T08:54:05.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Oh wonderful, this looks really great <span data-props=\"{&quot;user&quot;:&quot;marella&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/marella\"\
          >@<span class=\"underline\">marella</span></a></span>\n\n\t</span></span>\
          \ .  I will update the README to point people to this great tool.</p>\n\
          <p>Does it support CUDA offloading?</p>\n"
        raw: 'Oh wonderful, this looks really great @marella .  I will update the
          README to point people to this great tool.


          Does it support CUDA offloading?'
        updatedAt: '2023-05-21T08:54:05.601Z'
      numEdits: 0
      reactions: []
    id: 6469dc2d3721aab2edeeff26
    type: comment
  author: TheBloke
  content: 'Oh wonderful, this looks really great @marella .  I will update the README
    to point people to this great tool.


    Does it support CUDA offloading?'
  created_at: 2023-05-21 07:54:05+00:00
  edited: false
  hidden: false
  id: 6469dc2d3721aab2edeeff26
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/93b6433d2741979d473811732948b04d.svg
      fullname: Ravindra Marella
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marella
      type: user
    createdAt: '2023-05-21T10:18:36.000Z'
    data:
      edited: false
      editors:
      - marella
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/93b6433d2741979d473811732948b04d.svg
          fullname: Ravindra Marella
          isHf: false
          isPro: false
          name: marella
          type: user
        html: '<p>Currently it doesn''t support CUDA but I''m planning to add it in
          future after adding few more features and models.</p>

          '
        raw: Currently it doesn't support CUDA but I'm planning to add it in future
          after adding few more features and models.
        updatedAt: '2023-05-21T10:18:36.088Z'
      numEdits: 0
      reactions: []
    id: 6469effc96cfe72aef75e56a
    type: comment
  author: marella
  content: Currently it doesn't support CUDA but I'm planning to add it in future
    after adding few more features and models.
  created_at: 2023-05-21 09:18:36+00:00
  edited: false
  hidden: false
  id: 6469effc96cfe72aef75e56a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-21T10:34:44.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK thanks. I know I''m going to get a lot of people asking about
          that.  Like this guy: <a href="https://huggingface.co/TheBloke/MPT-7B-GGML/discussions/1#6469176ab2321e47d3288721">https://huggingface.co/TheBloke/MPT-7B-GGML/discussions/1#6469176ab2321e47d3288721</a></p>

          <p>I''ve added your library and GPT4All-UI to the READMEs for my three MPT
          models</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/aasy4IAFtSGndiRWRPxwQ.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/aasy4IAFtSGndiRWRPxwQ.png"></a></p>

          '
        raw: 'OK thanks. I know I''m going to get a lot of people asking about that.  Like
          this guy: https://huggingface.co/TheBloke/MPT-7B-GGML/discussions/1#6469176ab2321e47d3288721


          I''ve added your library and GPT4All-UI to the READMEs for my three MPT
          models


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/aasy4IAFtSGndiRWRPxwQ.png)'
        updatedAt: '2023-05-21T10:34:44.830Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - marella
    id: 6469f3c496cfe72aef761bbf
    type: comment
  author: TheBloke
  content: 'OK thanks. I know I''m going to get a lot of people asking about that.  Like
    this guy: https://huggingface.co/TheBloke/MPT-7B-GGML/discussions/1#6469176ab2321e47d3288721


    I''ve added your library and GPT4All-UI to the READMEs for my three MPT models


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/aasy4IAFtSGndiRWRPxwQ.png)'
  created_at: 2023-05-21 09:34:44+00:00
  edited: false
  hidden: false
  id: 6469f3c496cfe72aef761bbf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/MPT-7B-GGML
repo_type: model
status: open
target_branch: null
title: Python library
