!!python/object:huggingface_hub.community.DiscussionWithDetails
author: FriendlyVisage
conflicting_files: null
created_at: 2023-05-20 17:54:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c9e354ce5414bced11d5abf4bd26d27.svg
      fullname: Friendly Visage
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FriendlyVisage
      type: user
    createdAt: '2023-05-20T18:54:34.000Z'
    data:
      edited: false
      editors:
      - FriendlyVisage
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c9e354ce5414bced11d5abf4bd26d27.svg
          fullname: Friendly Visage
          isHf: false
          isPro: false
          name: FriendlyVisage
          type: user
        html: '<p>I see that it (currently) only works with ggml and rustformers''
          llm. Do either of those support GPU? If not, are GPTQ versions of MPT-7B-*
          possible?</p>

          '
        raw: I see that it (currently) only works with ggml and rustformers' llm.
          Do either of those support GPU? If not, are GPTQ versions of MPT-7B-* possible?
        updatedAt: '2023-05-20T18:54:34.453Z'
      numEdits: 0
      reactions: []
    id: 6469176ab2321e47d3288721
    type: comment
  author: FriendlyVisage
  content: I see that it (currently) only works with ggml and rustformers' llm. Do
    either of those support GPU? If not, are GPTQ versions of MPT-7B-* possible?
  created_at: 2023-05-20 17:54:34+00:00
  edited: false
  hidden: false
  id: 6469176ab2321e47d3288721
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-21T10:28:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>There''s now also a Python library it works with - <a rel="nofollow"
          href="https://github.com/marella/ctransformers">https://github.com/marella/ctransformers</a></p>

          <p>The Python library doesn''t yet support CUDA but it is planned to be
          added soon.</p>

          <p>There is already a GPTQ model for MPT 7B on Huggingface. I''ve not tried
          it myself yet but I''m told it works in KobaldAI and text-generation-webui.
          You need to <code>pip install einops</code> and then apparently it should
          work.</p>

          <p>I''m planning to make my own at some point, once I''ve investigated the
          process a bit more.</p>

          '
        raw: 'There''s now also a Python library it works with - https://github.com/marella/ctransformers


          The Python library doesn''t yet support CUDA but it is planned to be added
          soon.


          There is already a GPTQ model for MPT 7B on Huggingface. I''ve not tried
          it myself yet but I''m told it works in KobaldAI and text-generation-webui.
          You need to `pip install einops` and then apparently it should work.


          I''m planning to make my own at some point, once I''ve investigated the
          process a bit more.'
        updatedAt: '2023-05-21T10:28:51.522Z'
      numEdits: 0
      reactions: []
    id: 6469f26396cfe72aef7606eb
    type: comment
  author: TheBloke
  content: 'There''s now also a Python library it works with - https://github.com/marella/ctransformers


    The Python library doesn''t yet support CUDA but it is planned to be added soon.


    There is already a GPTQ model for MPT 7B on Huggingface. I''ve not tried it myself
    yet but I''m told it works in KobaldAI and text-generation-webui. You need to
    `pip install einops` and then apparently it should work.


    I''m planning to make my own at some point, once I''ve investigated the process
    a bit more.'
  created_at: 2023-05-21 09:28:51+00:00
  edited: false
  hidden: false
  id: 6469f26396cfe72aef7606eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c9e354ce5414bced11d5abf4bd26d27.svg
      fullname: Friendly Visage
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FriendlyVisage
      type: user
    createdAt: '2023-05-22T02:46:54.000Z'
    data:
      edited: false
      editors:
      - FriendlyVisage
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c9e354ce5414bced11d5abf4bd26d27.svg
          fullname: Friendly Visage
          isHf: false
          isPro: false
          name: FriendlyVisage
          type: user
        html: '<p>I''ve seen the storywriter MPT-7b, but not the chat or the instruct.
          (The two that I''m really interested in.)</p>

          '
        raw: I've seen the storywriter MPT-7b, but not the chat or the instruct. (The
          two that I'm really interested in.)
        updatedAt: '2023-05-22T02:46:54.950Z'
      numEdits: 0
      reactions: []
    id: 646ad79e3721aab2edfe4a5d
    type: comment
  author: FriendlyVisage
  content: I've seen the storywriter MPT-7b, but not the chat or the instruct. (The
    two that I'm really interested in.)
  created_at: 2023-05-22 01:46:54+00:00
  edited: false
  hidden: false
  id: 646ad79e3721aab2edfe4a5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676674250492-63a9009d2e05ca32e3511832.png?w=200&h=200&f=face
      fullname: Simon Slamka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ongkn
      type: user
    createdAt: '2023-05-22T10:54:45.000Z'
    data:
      edited: false
      editors:
      - ongkn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676674250492-63a9009d2e05ca32e3511832.png?w=200&h=200&f=face
          fullname: Simon Slamka
          isHf: false
          isPro: false
          name: ongkn
          type: user
        html: '<blockquote>

          <p>I''ve seen the storywriter MPT-7b, but not the chat or the instruct.
          (The two that I''m really interested in.)</p>

          </blockquote>

          <p>currently, those two have a context window of only 2048 and 1024 (errored
          out when inputting larger context, but perhaps there''s some config that
          overrides that. haven''t had time to investigate yet)</p>

          '
        raw: '> I''ve seen the storywriter MPT-7b, but not the chat or the instruct.
          (The two that I''m really interested in.)


          currently, those two have a context window of only 2048 and 1024 (errored
          out when inputting larger context, but perhaps there''s some config that
          overrides that. haven''t had time to investigate yet)'
        updatedAt: '2023-05-22T10:54:45.628Z'
      numEdits: 0
      reactions: []
    id: 646b49f5db697c798a358766
    type: comment
  author: ongkn
  content: '> I''ve seen the storywriter MPT-7b, but not the chat or the instruct.
    (The two that I''m really interested in.)


    currently, those two have a context window of only 2048 and 1024 (errored out
    when inputting larger context, but perhaps there''s some config that overrides
    that. haven''t had time to investigate yet)'
  created_at: 2023-05-22 09:54:45+00:00
  edited: false
  hidden: false
  id: 646b49f5db697c798a358766
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c9e354ce5414bced11d5abf4bd26d27.svg
      fullname: Friendly Visage
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FriendlyVisage
      type: user
    createdAt: '2023-05-22T14:06:40.000Z'
    data:
      edited: false
      editors:
      - FriendlyVisage
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c9e354ce5414bced11d5abf4bd26d27.svg
          fullname: Friendly Visage
          isHf: false
          isPro: false
          name: FriendlyVisage
          type: user
        html: "<p>According to their documentation, the context windows can be lengthened:</p>\n\
          <blockquote>\n<p>Although the model was trained with a sequence length of\
          \ 2048, ALiBi enables users to increase the maximum sequence length during\
          \ finetuning and/or inference. For example:</p>\n</blockquote>\n<pre><code>config\
          \ = transformers.AutoConfig.from_pretrained(\n  'mosaicml/mpt-7b-instruct',\n\
          \  trust_remote_code=True\n)\nconfig.update({\"max_seq_len\": 4096})\nmodel\
          \ = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b-instruct',\n\
          \  config=config,\n  trust_remote_code=True\n)\n</code></pre>\n<p>Unless\
          \ you're saying that they errored out even with <code>max_seq_len</code>\
          \ increased.</p>\n"
        raw: "According to their documentation, the context windows can be lengthened:\n\
          \n>Although the model was trained with a sequence length of 2048, ALiBi\
          \ enables users to increase the maximum sequence length during finetuning\
          \ and/or inference. For example:\n```\nconfig = transformers.AutoConfig.from_pretrained(\n\
          \  'mosaicml/mpt-7b-instruct',\n  trust_remote_code=True\n)\nconfig.update({\"\
          max_seq_len\": 4096})\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \  'mosaicml/mpt-7b-instruct',\n  config=config,\n  trust_remote_code=True\n\
          )\n```\n\nUnless you're saying that they errored out even with `max_seq_len`\
          \ increased."
        updatedAt: '2023-05-22T14:06:40.037Z'
      numEdits: 0
      reactions: []
    id: 646b76f0ed228272132e1189
    type: comment
  author: FriendlyVisage
  content: "According to their documentation, the context windows can be lengthened:\n\
    \n>Although the model was trained with a sequence length of 2048, ALiBi enables\
    \ users to increase the maximum sequence length during finetuning and/or inference.\
    \ For example:\n```\nconfig = transformers.AutoConfig.from_pretrained(\n  'mosaicml/mpt-7b-instruct',\n\
    \  trust_remote_code=True\n)\nconfig.update({\"max_seq_len\": 4096})\nmodel =\
    \ transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b-instruct',\n\
    \  config=config,\n  trust_remote_code=True\n)\n```\n\nUnless you're saying that\
    \ they errored out even with `max_seq_len` increased."
  created_at: 2023-05-22 13:06:40+00:00
  edited: false
  hidden: false
  id: 646b76f0ed228272132e1189
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676674250492-63a9009d2e05ca32e3511832.png?w=200&h=200&f=face
      fullname: Simon Slamka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ongkn
      type: user
    createdAt: '2023-05-23T13:43:42.000Z'
    data:
      edited: false
      editors:
      - ongkn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676674250492-63a9009d2e05ca32e3511832.png?w=200&h=200&f=face
          fullname: Simon Slamka
          isHf: false
          isPro: false
          name: ongkn
          type: user
        html: "<blockquote>\n<p>According to their documentation, the context windows\
          \ can be lengthened:</p>\n<blockquote>\n<p>Although the model was trained\
          \ with a sequence length of 2048, ALiBi enables users to increase the maximum\
          \ sequence length during finetuning and/or inference. For example:</p>\n\
          </blockquote>\n<pre><code>config = transformers.AutoConfig.from_pretrained(\n\
          \  'mosaicml/mpt-7b-instruct',\n  trust_remote_code=True\n)\nconfig.update({\"\
          max_seq_len\": 4096})\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \  'mosaicml/mpt-7b-instruct',\n  config=config,\n  trust_remote_code=True\n\
          )\n</code></pre>\n<p>Unless you're saying that they errored out even with\
          \ <code>max_seq_len</code> increased.</p>\n</blockquote>\n<p>Gotta try.\
          \ Thank you. I must have missed that when reading the docs.</p>\n"
        raw: "> According to their documentation, the context windows can be lengthened:\n\
          > \n> >Although the model was trained with a sequence length of 2048, ALiBi\
          \ enables users to increase the maximum sequence length during finetuning\
          \ and/or inference. For example:\n> ```\n> config = transformers.AutoConfig.from_pretrained(\n\
          >   'mosaicml/mpt-7b-instruct',\n>   trust_remote_code=True\n> )\n> config.update({\"\
          max_seq_len\": 4096})\n> model = transformers.AutoModelForCausalLM.from_pretrained(\n\
          >   'mosaicml/mpt-7b-instruct',\n>   config=config,\n>   trust_remote_code=True\n\
          > )\n> ```\n> \n> Unless you're saying that they errored out even with `max_seq_len`\
          \ increased.\n\nGotta try. Thank you. I must have missed that when reading\
          \ the docs."
        updatedAt: '2023-05-23T13:43:42.815Z'
      numEdits: 0
      reactions: []
    id: 646cc30e87ed262149b2a42f
    type: comment
  author: ongkn
  content: "> According to their documentation, the context windows can be lengthened:\n\
    > \n> >Although the model was trained with a sequence length of 2048, ALiBi enables\
    \ users to increase the maximum sequence length during finetuning and/or inference.\
    \ For example:\n> ```\n> config = transformers.AutoConfig.from_pretrained(\n>\
    \   'mosaicml/mpt-7b-instruct',\n>   trust_remote_code=True\n> )\n> config.update({\"\
    max_seq_len\": 4096})\n> model = transformers.AutoModelForCausalLM.from_pretrained(\n\
    >   'mosaicml/mpt-7b-instruct',\n>   config=config,\n>   trust_remote_code=True\n\
    > )\n> ```\n> \n> Unless you're saying that they errored out even with `max_seq_len`\
    \ increased.\n\nGotta try. Thank you. I must have missed that when reading the\
    \ docs."
  created_at: 2023-05-23 12:43:42+00:00
  edited: false
  hidden: false
  id: 646cc30e87ed262149b2a42f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/MPT-7B-GGML
repo_type: model
status: open
target_branch: null
title: Non llama.cpp GPU compatible libraries?
