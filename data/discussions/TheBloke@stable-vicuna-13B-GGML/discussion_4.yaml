!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ankurkaul17
conflicting_files: null
created_at: 2023-05-21 18:44:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/032b8a1ad833ee26d9fd88d534962242.svg
      fullname: Ankur Kaul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ankurkaul17
      type: user
    createdAt: '2023-05-21T19:44:07.000Z'
    data:
      edited: false
      editors:
      - ankurkaul17
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/032b8a1ad833ee26d9fd88d534962242.svg
          fullname: Ankur Kaul
          isHf: false
          isPro: false
          name: ankurkaul17
          type: user
        html: '<p>Can this model be used with langchain llamacpp ? If so would you
          be kind enough to provide code. Thanks</p>

          '
        raw: Can this model be used with langchain llamacpp ? If so would you be kind
          enough to provide code. Thanks
        updatedAt: '2023-05-21T19:44:07.579Z'
      numEdits: 0
      reactions: []
    id: 646a7487a5dd10c9a4a0cd88
    type: comment
  author: ankurkaul17
  content: Can this model be used with langchain llamacpp ? If so would you be kind
    enough to provide code. Thanks
  created_at: 2023-05-21 18:44:07+00:00
  edited: false
  hidden: false
  id: 646a7487a5dd10c9a4a0cd88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-21T19:59:43.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Yeah - install <a rel=\"nofollow\" href=\"https://github.com/abetlen/llama-cpp-python\"\
          >llama-cpp-python</a> then here is a quick example:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> llama_cpp <span\
          \ class=\"hljs-keyword\">import</span> Llama\n<span class=\"hljs-keyword\"\
          >import</span> random\nllm = Llama(model_path=<span class=\"hljs-string\"\
          >\"/path/to/stable-vicuna-13B.ggmlv3.q5_1.bin\"</span>, n_gpu_layers=<span\
          \ class=\"hljs-number\">40</span>, seed=random.randint(<span class=\"hljs-number\"\
          >1</span>, <span class=\"hljs-number\">2</span>**<span class=\"hljs-number\"\
          >31</span>))\ntokens = llm.tokenize(<span class=\"hljs-string\">b\"### Human:\
          \ Write a story about llamas\\n### Assistant:\"</span>)\n\noutput = <span\
          \ class=\"hljs-string\">b\"\"</span>\ncount = <span class=\"hljs-number\"\
          >0</span>\n<span class=\"hljs-keyword\">for</span> token <span class=\"\
          hljs-keyword\">in</span> llm.generate(tokens, top_k=<span class=\"hljs-number\"\
          >40</span>, top_p=<span class=\"hljs-number\">0.95</span>, temp=<span class=\"\
          hljs-number\">0.72</span>, repeat_penalty=<span class=\"hljs-number\">1.1</span>):\n\
          \     text = llm.detokenize([token])\n     <span class=\"hljs-built_in\"\
          >print</span>(text.decode(), end=<span class=\"hljs-string\">''</span>,\
          \ flush=<span class=\"hljs-literal\">True</span>)\n     output += text\n\
          \n     count +=<span class=\"hljs-number\">1</span>\n     <span class=\"\
          hljs-keyword\">if</span> count &gt;= <span class=\"hljs-number\">500</span>\
          \ <span class=\"hljs-keyword\">or</span> (token == llm.token_eos()):\n \
          \        <span class=\"hljs-keyword\">break</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"Full response:\"</span>, output.decode())\n\
          </code></pre>\n"
        raw: "Yeah - install [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)\
          \ then here is a quick example:\n\n```python\nfrom llama_cpp import Llama\n\
          import random\nllm = Llama(model_path=\"/path/to/stable-vicuna-13B.ggmlv3.q5_1.bin\"\
          , n_gpu_layers=40, seed=random.randint(1, 2**31))\ntokens = llm.tokenize(b\"\
          ### Human: Write a story about llamas\\n### Assistant:\")\n\noutput = b\"\
          \"\ncount = 0\nfor token in llm.generate(tokens, top_k=40, top_p=0.95, temp=0.72,\
          \ repeat_penalty=1.1):\n     text = llm.detokenize([token])\n     print(text.decode(),\
          \ end='', flush=True)\n     output += text\n\n     count +=1\n     if count\
          \ >= 500 or (token == llm.token_eos()):\n         break\n\nprint(\"Full\
          \ response:\", output.decode())\n```"
        updatedAt: '2023-05-21T19:59:43.810Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ankurkaul17
    id: 646a782f13396ee0a5682a85
    type: comment
  author: TheBloke
  content: "Yeah - install [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)\
    \ then here is a quick example:\n\n```python\nfrom llama_cpp import Llama\nimport\
    \ random\nllm = Llama(model_path=\"/path/to/stable-vicuna-13B.ggmlv3.q5_1.bin\"\
    , n_gpu_layers=40, seed=random.randint(1, 2**31))\ntokens = llm.tokenize(b\"###\
    \ Human: Write a story about llamas\\n### Assistant:\")\n\noutput = b\"\"\ncount\
    \ = 0\nfor token in llm.generate(tokens, top_k=40, top_p=0.95, temp=0.72, repeat_penalty=1.1):\n\
    \     text = llm.detokenize([token])\n     print(text.decode(), end='', flush=True)\n\
    \     output += text\n\n     count +=1\n     if count >= 500 or (token == llm.token_eos()):\n\
    \         break\n\nprint(\"Full response:\", output.decode())\n```"
  created_at: 2023-05-21 18:59:43+00:00
  edited: false
  hidden: false
  id: 646a782f13396ee0a5682a85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/032b8a1ad833ee26d9fd88d534962242.svg
      fullname: Ankur Kaul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ankurkaul17
      type: user
    createdAt: '2023-05-21T22:04:00.000Z'
    data:
      edited: true
      editors:
      - ankurkaul17
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/032b8a1ad833ee26d9fd88d534962242.svg
          fullname: Ankur Kaul
          isHf: false
          isPro: false
          name: ankurkaul17
          type: user
        html: '<p>Thanks for the code but getting a assertion error . Using llama-cpp-python
          == 0.1.52. Using the ggmlv3.q5_1 bin file.</p>

          <p>assert self.ctx is not None<br>AssertionError</p>

          <p>Would you know if this bin file  is compatible with the package version.
          Thank you for your help</p>

          '
        raw: 'Thanks for the code but getting a assertion error . Using llama-cpp-python
          == 0.1.52. Using the ggmlv3.q5_1 bin file.


          assert self.ctx is not None

          AssertionError


          Would you know if this bin file  is compatible with the package version.
          Thank you for your help'
        updatedAt: '2023-05-21T22:06:59.120Z'
      numEdits: 2
      reactions: []
    id: 646a955096cfe72aef7f79c2
    type: comment
  author: ankurkaul17
  content: 'Thanks for the code but getting a assertion error . Using llama-cpp-python
    == 0.1.52. Using the ggmlv3.q5_1 bin file.


    assert self.ctx is not None

    AssertionError


    Would you know if this bin file  is compatible with the package version. Thank
    you for your help'
  created_at: 2023-05-21 21:04:00+00:00
  edited: true
  hidden: false
  id: 646a955096cfe72aef7f79c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b95a531a8f5b359f14cef4b04f773ea.svg
      fullname: Vasanth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vsns
      type: user
    createdAt: '2023-05-22T05:53:21.000Z'
    data:
      edited: false
      editors:
      - vsns
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4b95a531a8f5b359f14cef4b04f773ea.svg
          fullname: Vasanth
          isHf: false
          isPro: false
          name: vsns
          type: user
        html: '<p>With langchain this <a rel="nofollow" href="https://github.com/marella/ctransformers">https://github.com/marella/ctransformers</a>
          could also be used, had issues with llama-cpp-python(asking for visual studio),
          but ctransformers (had libraries precompiled) helped. (I haven''t tried
          this model with that though)</p>

          '
        raw: With langchain this https://github.com/marella/ctransformers could also
          be used, had issues with llama-cpp-python(asking for visual studio), but
          ctransformers (had libraries precompiled) helped. (I haven't tried this
          model with that though)
        updatedAt: '2023-05-22T05:53:21.454Z'
      numEdits: 0
      reactions: []
    id: 646b035196cfe72aef87afaf
    type: comment
  author: vsns
  content: With langchain this https://github.com/marella/ctransformers could also
    be used, had issues with llama-cpp-python(asking for visual studio), but ctransformers
    (had libraries precompiled) helped. (I haven't tried this model with that though)
  created_at: 2023-05-22 04:53:21+00:00
  edited: false
  hidden: false
  id: 646b035196cfe72aef87afaf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-05-22T11:09:50.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<blockquote>

          <p>Thanks for the code but getting a assertion error . Using llama-cpp-python
          == 0.1.52. Using the ggmlv3.q5_1 bin file.</p>

          <p>assert self.ctx is not None<br>AssertionError</p>

          <p>Would you know if this bin file  is compatible with the package version.
          Thank you for your help</p>

          </blockquote>

          <p>I had that same issue, and had to use the ggmlv2 version. I think you
          have to build the newer llama.cpp for the ggmlv3, but I could be wrong.</p>

          '
        raw: "> Thanks for the code but getting a assertion error . Using llama-cpp-python\
          \ == 0.1.52. Using the ggmlv3.q5_1 bin file.\n> \n> assert self.ctx is not\
          \ None\n> AssertionError\n> \n> Would you know if this bin file  is compatible\
          \ with the package version. Thank you for your help\n\nI had that same issue,\
          \ and had to use the ggmlv2 version. I think you have to build the newer\
          \ llama.cpp for the ggmlv3, but I could be wrong."
        updatedAt: '2023-05-22T11:09:50.104Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ankurkaul17
    id: 646b4d7e5d68f5c15a1c92cd
    type: comment
  author: Satya93
  content: "> Thanks for the code but getting a assertion error . Using llama-cpp-python\
    \ == 0.1.52. Using the ggmlv3.q5_1 bin file.\n> \n> assert self.ctx is not None\n\
    > AssertionError\n> \n> Would you know if this bin file  is compatible with the\
    \ package version. Thank you for your help\n\nI had that same issue, and had to\
    \ use the ggmlv2 version. I think you have to build the newer llama.cpp for the\
    \ ggmlv3, but I could be wrong."
  created_at: 2023-05-22 10:09:50+00:00
  edited: false
  hidden: false
  id: 646b4d7e5d68f5c15a1c92cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-22T11:21:00.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>llama-cpp-python got updated to support GGMLv3 about 10 hours ago.  Version
          0.1.53 supports GGMLv3.</p>

          <p>You can install llama-cpp-python 0.1.53 on Windows without compiling
          with: <code>pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.1.53/llama_cpp_python-0.1.53-cp310-cp310-win_amd64.whl</code></p>

          <p>Or yes use <code>ctransformers</code>, which can be installed with <code>pip
          install ctransformers</code></p>

          '
        raw: 'llama-cpp-python got updated to support GGMLv3 about 10 hours ago.  Version
          0.1.53 supports GGMLv3.


          You can install llama-cpp-python 0.1.53 on Windows without compiling with:
          `pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.1.53/llama_cpp_python-0.1.53-cp310-cp310-win_amd64.whl`


          Or yes use `ctransformers`, which can be installed with `pip install ctransformers`'
        updatedAt: '2023-05-22T11:21:00.424Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Satya93
        - ankurkaul17
    id: 646b501cdb697c798a369710
    type: comment
  author: TheBloke
  content: 'llama-cpp-python got updated to support GGMLv3 about 10 hours ago.  Version
    0.1.53 supports GGMLv3.


    You can install llama-cpp-python 0.1.53 on Windows without compiling with: `pip
    install https://github.com/abetlen/llama-cpp-python/releases/download/v0.1.53/llama_cpp_python-0.1.53-cp310-cp310-win_amd64.whl`


    Or yes use `ctransformers`, which can be installed with `pip install ctransformers`'
  created_at: 2023-05-22 10:21:00+00:00
  edited: false
  hidden: false
  id: 646b501cdb697c798a369710
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/032b8a1ad833ee26d9fd88d534962242.svg
      fullname: Ankur Kaul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ankurkaul17
      type: user
    createdAt: '2023-05-22T17:40:34.000Z'
    data:
      edited: false
      editors:
      - ankurkaul17
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/032b8a1ad833ee26d9fd88d534962242.svg
          fullname: Ankur Kaul
          isHf: false
          isPro: false
          name: ankurkaul17
          type: user
        html: '<p>Thanks guys</p>

          '
        raw: Thanks guys
        updatedAt: '2023-05-22T17:40:34.017Z'
      numEdits: 0
      reactions: []
    id: 646ba912db697c798a44a1f2
    type: comment
  author: ankurkaul17
  content: Thanks guys
  created_at: 2023-05-22 16:40:34+00:00
  edited: false
  hidden: false
  id: 646ba912db697c798a44a1f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6f9d21d8602bd099827d511f6b15a85f.svg
      fullname: Vasileios
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vgkortsas
      type: user
    createdAt: '2023-07-03T05:09:54.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/6f9d21d8602bd099827d511f6b15a85f.svg
          fullname: Vasileios
          isHf: false
          isPro: false
          name: vgkortsas
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-07-07T06:23:12.861Z'
      numEdits: 1
      reactions: []
    id: 64a25822422bdee276af0ff5
    type: comment
  author: vgkortsas
  content: This comment has been hidden
  created_at: 2023-07-03 04:09:54+00:00
  edited: true
  hidden: true
  id: 64a25822422bdee276af0ff5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/stable-vicuna-13B-GGML
repo_type: model
status: open
target_branch: null
title: 'Run with langchain '
