!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GaoQiQiang
conflicting_files: null
created_at: 2023-02-07 03:33:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676618024499-63da075125575d85ad0cf4a3.png?w=200&h=200&f=face
      fullname: GaoQiQiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GaoQiQiang
      type: user
    createdAt: '2023-02-07T03:33:02.000Z'
    data:
      edited: false
      editors:
      - GaoQiQiang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676618024499-63da075125575d85ad0cf4a3.png?w=200&h=200&f=face
          fullname: GaoQiQiang
          isHf: false
          isPro: false
          name: GaoQiQiang
          type: user
        html: '<p>I wanna run this model on gpu. I tried "pipe = pipeline(''text-generation'',
          model=model, tokenizer=tokenizer, max_new_tokens=50).to("cuda")", but I
          got " AttributeError: ''TextGenerationPipeline'' object has no attribute
          ''to'' ". Also, I tried "model=model.to("cuda")", I got "RuntimeError: Expected
          all tensors to be on the same device, but found at least two devices, cuda:0
          and cpu! (when checking argument for argument index in method wrapper__index_select)".
          Please</p>

          '
        raw: 'I wanna run this model on gpu. I tried "pipe = pipeline(''text-generation'',
          model=model, tokenizer=tokenizer, max_new_tokens=50).to("cuda")", but I
          got " AttributeError: ''TextGenerationPipeline'' object has no attribute
          ''to'' ". Also, I tried "model=model.to("cuda")", I got "RuntimeError: Expected
          all tensors to be on the same device, but found at least two devices, cuda:0
          and cpu! (when checking argument for argument index in method wrapper__index_select)".
          Please'
        updatedAt: '2023-02-07T03:33:02.507Z'
      numEdits: 0
      reactions: []
    id: 63e1c66ef0039731dfe174d9
    type: comment
  author: GaoQiQiang
  content: 'I wanna run this model on gpu. I tried "pipe = pipeline(''text-generation'',
    model=model, tokenizer=tokenizer, max_new_tokens=50).to("cuda")", but I got "
    AttributeError: ''TextGenerationPipeline'' object has no attribute ''to'' ". Also,
    I tried "model=model.to("cuda")", I got "RuntimeError: Expected all tensors to
    be on the same device, but found at least two devices, cuda:0 and cpu! (when checking
    argument for argument index in method wrapper__index_select)". Please'
  created_at: 2023-02-07 03:33:02+00:00
  edited: false
  hidden: false
  id: 63e1c66ef0039731dfe174d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/606d6848ccc9593009992e87/yexGi3Ccac6IxJGxijiUK.png?w=200&h=200&f=face
      fullname: Aditya Jitta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gradjitta
      type: user
    createdAt: '2023-02-10T11:54:21.000Z'
    data:
      edited: false
      editors:
      - gradjitta
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/606d6848ccc9593009992e87/yexGi3Ccac6IxJGxijiUK.png?w=200&h=200&f=face
          fullname: Aditya Jitta
          isHf: false
          isPro: false
          name: gradjitta
          type: user
        html: '<p>You can make it working by the following</p>

          <pre><code>tokenizer = AutoTokenizer.from_pretrained("Gustavosta/MagicPrompt-Stable-Diffusion")

          model = AutoModelForCausalLM.from_pretrained("Gustavosta/MagicPrompt-Stable-Diffusion")

          gpt2_pipe = pipeline("text-generation", model= model, tokenizer = tokenizer,
          device = CUDA_DEVICE)

          </code></pre>

          '
        raw: 'You can make it working by the following

          ```

          tokenizer = AutoTokenizer.from_pretrained("Gustavosta/MagicPrompt-Stable-Diffusion")

          model = AutoModelForCausalLM.from_pretrained("Gustavosta/MagicPrompt-Stable-Diffusion")

          gpt2_pipe = pipeline("text-generation", model= model, tokenizer = tokenizer,
          device = CUDA_DEVICE)

          ```'
        updatedAt: '2023-02-10T11:54:21.134Z'
      numEdits: 0
      reactions: []
    id: 63e6306d63037c7d960bbb5a
    type: comment
  author: gradjitta
  content: 'You can make it working by the following

    ```

    tokenizer = AutoTokenizer.from_pretrained("Gustavosta/MagicPrompt-Stable-Diffusion")

    model = AutoModelForCausalLM.from_pretrained("Gustavosta/MagicPrompt-Stable-Diffusion")

    gpt2_pipe = pipeline("text-generation", model= model, tokenizer = tokenizer, device
    = CUDA_DEVICE)

    ```'
  created_at: 2023-02-10 11:54:21+00:00
  edited: false
  hidden: false
  id: 63e6306d63037c7d960bbb5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676618024499-63da075125575d85ad0cf4a3.png?w=200&h=200&f=face
      fullname: GaoQiQiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GaoQiQiang
      type: user
    createdAt: '2023-02-13T02:16:59.000Z'
    data:
      edited: true
      editors:
      - GaoQiQiang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676618024499-63da075125575d85ad0cf4a3.png?w=200&h=200&f=face
          fullname: GaoQiQiang
          isHf: false
          isPro: false
          name: GaoQiQiang
          type: user
        html: '<p>Thanks for your code, it works. Followed your github@<a href="https://huggingface.co/Gustavosta/MagicPrompt-Stable-Diffusion/discussions/6#63e6306d63037c7d960bbb5a">https://huggingface.co/Gustavosta/MagicPrompt-Stable-Diffusion/discussions/6#63e6306d63037c7d960bbb5a</a></p>

          '
        raw: Thanks for your code, it works. Followed your github@https://huggingface.co/Gustavosta/MagicPrompt-Stable-Diffusion/discussions/6#63e6306d63037c7d960bbb5a
        updatedAt: '2023-02-13T02:17:31.809Z'
      numEdits: 1
      reactions: []
    id: 63e99d9b919e2797bb86216d
    type: comment
  author: GaoQiQiang
  content: Thanks for your code, it works. Followed your github@https://huggingface.co/Gustavosta/MagicPrompt-Stable-Diffusion/discussions/6#63e6306d63037c7d960bbb5a
  created_at: 2023-02-13 02:16:59+00:00
  edited: true
  hidden: false
  id: 63e99d9b919e2797bb86216d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: Gustavosta/MagicPrompt-Stable-Diffusion
repo_type: model
status: open
target_branch: null
title: How can run the model on GPU?
