!!python/object:huggingface_hub.community.DiscussionWithDetails
author: leonardlin
conflicting_files: null
created_at: 2023-12-07 13:11:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
      fullname: lhl
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: leonardlin
      type: user
    createdAt: '2023-12-07T13:11:10.000Z'
    data:
      edited: true
      editors:
      - leonardlin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9272173047065735
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
          fullname: lhl
          isHf: false
          isPro: false
          name: leonardlin
          type: user
        html: '<p>GGUFs can be created, but currently llama.cpp has a known bug that
          causes it to crash with certain tokenizers:</p>

          <pre><code>GGML_ASSERT: llama.cpp:2683: codepoints_from_utf8(word).size()
          &gt; 0

          Aborted (core dumped)

          </code></pre>

          <p>This bug was reported in September and October 2023 and seems to also
          affect at least  <code>ELYZA-japanese-Llama-2-7b-fast-instruct</code> and
          <code>InternLM</code></p>

          <ul>

          <li><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/3133">https://github.com/ggerganov/llama.cpp/issues/3133</a></li>

          <li><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/discussions/3498">https://github.com/ggerganov/llama.cpp/discussions/3498</a></li>

          </ul>

          <p>I did some poking and submitted a new issue, and those looking  to see
          the status (or wanting to have a poke) can look here: <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/issues/4360">https://github.com/ggerganov/llama.cpp/issues/4360</a></p>

          '
        raw: 'GGUFs can be created, but currently llama.cpp has a known bug that causes
          it to crash with certain tokenizers:

          ```

          GGML_ASSERT: llama.cpp:2683: codepoints_from_utf8(word).size() > 0

          Aborted (core dumped)

          ```

          This bug was reported in September and October 2023 and seems to also affect
          at least  `ELYZA-japanese-Llama-2-7b-fast-instruct` and `InternLM`

          * https://github.com/ggerganov/llama.cpp/issues/3133

          * https://github.com/ggerganov/llama.cpp/discussions/3498


          I did some poking and submitted a new issue, and those looking  to see the
          status (or wanting to have a poke) can look here: https://github.com/ggerganov/llama.cpp/issues/4360'
        updatedAt: '2023-12-08T01:23:36.205Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - userzyzz
    id: 6571c46e5ad5f099a62c0b9d
    type: comment
  author: leonardlin
  content: 'GGUFs can be created, but currently llama.cpp has a known bug that causes
    it to crash with certain tokenizers:

    ```

    GGML_ASSERT: llama.cpp:2683: codepoints_from_utf8(word).size() > 0

    Aborted (core dumped)

    ```

    This bug was reported in September and October 2023 and seems to also affect at
    least  `ELYZA-japanese-Llama-2-7b-fast-instruct` and `InternLM`

    * https://github.com/ggerganov/llama.cpp/issues/3133

    * https://github.com/ggerganov/llama.cpp/discussions/3498


    I did some poking and submitted a new issue, and those looking  to see the status
    (or wanting to have a poke) can look here: https://github.com/ggerganov/llama.cpp/issues/4360'
  created_at: 2023-12-07 13:11:10+00:00
  edited: true
  hidden: false
  id: 6571c46e5ad5f099a62c0b9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
      fullname: lhl
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: leonardlin
      type: user
    createdAt: '2023-12-08T07:08:55.000Z'
    data:
      edited: false
      editors:
      - leonardlin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8527719974517822
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
          fullname: lhl
          isHf: false
          isPro: false
          name: leonardlin
          type: user
        html: '<p>Looks like there is a (rather involved) workaround for this llama.cpp''s
          handling of extended unicode here: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/4360#issuecomment-1846617653">https://github.com/ggerganov/llama.cpp/issues/4360#issuecomment-1846617653</a></p>

          <p>Just leaving it for those who <em>really</em> need to use llama.cpp for
          some reason (GPTQ and AWQ quants both tested to work fine).</p>

          '
        raw: 'Looks like there is a (rather involved) workaround for this llama.cpp''s
          handling of extended unicode here: https://github.com/ggerganov/llama.cpp/issues/4360#issuecomment-1846617653


          Just leaving it for those who *really* need to use llama.cpp for some reason
          (GPTQ and AWQ quants both tested to work fine).'
        updatedAt: '2023-12-08T07:08:55.071Z'
      numEdits: 0
      reactions: []
    id: 6572c1079a5c2d6df9fe8fb8
    type: comment
  author: leonardlin
  content: 'Looks like there is a (rather involved) workaround for this llama.cpp''s
    handling of extended unicode here: https://github.com/ggerganov/llama.cpp/issues/4360#issuecomment-1846617653


    Just leaving it for those who *really* need to use llama.cpp for some reason (GPTQ
    and AWQ quants both tested to work fine).'
  created_at: 2023-12-08 07:08:55+00:00
  edited: false
  hidden: false
  id: 6572c1079a5c2d6df9fe8fb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cbb6cd40588ec59bc33fa82c1c21c669.svg
      fullname: alex koo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alexkoo300
      type: user
    createdAt: '2023-12-09T03:45:11.000Z'
    data:
      edited: true
      editors:
      - alexkoo300
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9930019974708557
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cbb6cd40588ec59bc33fa82c1c21c669.svg
          fullname: alex koo
          isHf: false
          isPro: false
          name: alexkoo300
          type: user
        html: '<p>Thanks Leonard.  I was just in the middle of trying to convert shisa
          to gguf.  Good to hear the other quant methods were ok.</p>

          '
        raw: Thanks Leonard.  I was just in the middle of trying to convert shisa
          to gguf.  Good to hear the other quant methods were ok.
        updatedAt: '2023-12-09T03:45:26.199Z'
      numEdits: 1
      reactions: []
    id: 6573e2c733e5a4bf5b1d1267
    type: comment
  author: alexkoo300
  content: Thanks Leonard.  I was just in the middle of trying to convert shisa to
    gguf.  Good to hear the other quant methods were ok.
  created_at: 2023-12-09 03:45:11+00:00
  edited: true
  hidden: false
  id: 6573e2c733e5a4bf5b1d1267
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
      fullname: lhl
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: leonardlin
      type: user
    createdAt: '2023-12-10T16:17:46.000Z'
    data:
      edited: false
      editors:
      - leonardlin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7154543995857239
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
          fullname: lhl
          isHf: false
          isPro: false
          name: leonardlin
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;alexkoo300&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/alexkoo300\">@<span class=\"\
          underline\">alexkoo300</span></a></span>\n\n\t</span></span> Take a look\
          \ at <a href=\"https://huggingface.co/mmnga/shisa-7b-v1-gguf\">https://huggingface.co/mmnga/shisa-7b-v1-gguf</a>\
          \ - mmnga managed to use a modify conversion to merge/use spm instead of\
          \ bpe. Seems to work!</p>\n"
        raw: '@alexkoo300 Take a look at https://huggingface.co/mmnga/shisa-7b-v1-gguf
          - mmnga managed to use a modify conversion to merge/use spm instead of bpe.
          Seems to work!'
        updatedAt: '2023-12-10T16:17:46.513Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - alexkoo300
    id: 6575e4aad40e6ed32652ad36
    type: comment
  author: leonardlin
  content: '@alexkoo300 Take a look at https://huggingface.co/mmnga/shisa-7b-v1-gguf
    - mmnga managed to use a modify conversion to merge/use spm instead of bpe. Seems
    to work!'
  created_at: 2023-12-10 16:17:46+00:00
  edited: false
  hidden: false
  id: 6575e4aad40e6ed32652ad36
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: augmxnt/shisa-7b-v1
repo_type: model
status: open
target_branch: null
title: llama.cpp doesn't work w/ Shisa, has a bug that affects certain BPE tokenizers
  like ours
