!!python/object:huggingface_hub.community.DiscussionWithDetails
author: leonardlin
conflicting_files: null
created_at: 2023-12-08 07:15:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
      fullname: lhl
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: leonardlin
      type: user
    createdAt: '2023-12-08T07:15:13.000Z'
    data:
      edited: true
      editors:
      - leonardlin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8693450093269348
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
          fullname: lhl
          isHf: false
          isPro: false
          name: leonardlin
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ for doing his thing :)</p>\n<p>I'll keep this list updated if GGUFs come\
          \ along (See <a href=\"https://huggingface.co/augmxnt/shisa-7b-v1/discussions/1\"\
          >https://huggingface.co/augmxnt/shisa-7b-v1/discussions/1</a> to follow\
          \ along on that story, basically llama.cpp bugged atm for most BPE tokenizers\
          \ so no point in quantizing).</p>\n<ul>\n<li>AWQ: <a href=\"https://huggingface.co/TheBloke/shisa-7B-v1-AWQ\"\
          >https://huggingface.co/TheBloke/shisa-7B-v1-AWQ</a></li>\n<li>GPTQ: <a\
          \ href=\"https://huggingface.co/TheBloke/shisa-7B-v1-GPTQ\">https://huggingface.co/TheBloke/shisa-7B-v1-GPTQ</a></li>\n\
          </ul>\n<p>It looks like <a href=\"https://huggingface.co/mmnga\">mmnga</a>\
          \ was able to get GGUF conversion working with a <a rel=\"nofollow\" href=\"\
          https://gist.github.com/mmnga/7c315d79a8296657334d5102af0207fa\">custom_shisa.py</a>\
          \ conversion script that combines the extra BPE characters into the spm\
          \ tokenizer. Seems to run great, thanks!</p>\n<ul>\n<li>GGUF: <a href=\"\
          https://huggingface.co/mmnga/shisa-7b-v1-gguf\">https://huggingface.co/mmnga/shisa-7b-v1-gguf</a></li>\n\
          </ul>\n<p>If anyone does their own (EXLs, etc) feel free to post it in here.</p>\n"
        raw: "Thanks @TheBloke for doing his thing :)\n \nI'll keep this list updated\
          \ if GGUFs come along (See https://huggingface.co/augmxnt/shisa-7b-v1/discussions/1\
          \ to follow along on that story, basically llama.cpp bugged atm for most\
          \ BPE tokenizers so no point in quantizing).\n\n* AWQ: https://huggingface.co/TheBloke/shisa-7B-v1-AWQ\n\
          * GPTQ: https://huggingface.co/TheBloke/shisa-7B-v1-GPTQ\n\nIt looks like\
          \ [mmnga](https://huggingface.co/mmnga) was able to get GGUF conversion\
          \ working with a [custom_shisa.py](https://gist.github.com/mmnga/7c315d79a8296657334d5102af0207fa)\
          \ conversion script that combines the extra BPE characters into the spm\
          \ tokenizer. Seems to run great, thanks!\n* GGUF: https://huggingface.co/mmnga/shisa-7b-v1-gguf\n\
          \nIf anyone does their own (EXLs, etc) feel free to post it in here."
        updatedAt: '2023-12-10T13:22:24.226Z'
      numEdits: 1
      reactions: []
    id: 6572c281536d823668ef1605
    type: comment
  author: leonardlin
  content: "Thanks @TheBloke for doing his thing :)\n \nI'll keep this list updated\
    \ if GGUFs come along (See https://huggingface.co/augmxnt/shisa-7b-v1/discussions/1\
    \ to follow along on that story, basically llama.cpp bugged atm for most BPE tokenizers\
    \ so no point in quantizing).\n\n* AWQ: https://huggingface.co/TheBloke/shisa-7B-v1-AWQ\n\
    * GPTQ: https://huggingface.co/TheBloke/shisa-7B-v1-GPTQ\n\nIt looks like [mmnga](https://huggingface.co/mmnga)\
    \ was able to get GGUF conversion working with a [custom_shisa.py](https://gist.github.com/mmnga/7c315d79a8296657334d5102af0207fa)\
    \ conversion script that combines the extra BPE characters into the spm tokenizer.\
    \ Seems to run great, thanks!\n* GGUF: https://huggingface.co/mmnga/shisa-7b-v1-gguf\n\
    \nIf anyone does their own (EXLs, etc) feel free to post it in here."
  created_at: 2023-12-08 07:15:13+00:00
  edited: true
  hidden: false
  id: 6572c281536d823668ef1605
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
      fullname: lhl
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: leonardlin
      type: user
    createdAt: '2023-12-12T06:58:36.000Z'
    data:
      edited: false
      editors:
      - leonardlin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.713282585144043
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
          fullname: lhl
          isHf: false
          isPro: false
          name: leonardlin
          type: user
        html: "<p>I noted while clicking around that <span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \ made some EXL2 quants (thanks!):</p>\n<ul>\n<li><a href=\"https://huggingface.co/LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2\"\
          >https://huggingface.co/LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2</a></li>\n\
          <li><a href=\"https://huggingface.co/LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2\"\
          >https://huggingface.co/LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2</a></li>\n\
          <li><a href=\"https://huggingface.co/LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2\"\
          >https://huggingface.co/LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2</a></li>\n\
          <li><a href=\"https://huggingface.co/LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2\"\
          >https://huggingface.co/LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2</a></li>\n\
          <li><a href=\"https://huggingface.co/LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2\"\
          >https://huggingface.co/LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2</a></li>\n\
          </ul>\n<p>Note, also of interest, while I was doing inference benchmarking,\
          \ I also created an H6 4.63BPW quant to match the BPW of <span data-props=\"\
          {&quot;user&quot;:&quot;mmnga&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/mmnga\">@<span class=\"underline\">mmnga</span></a></span>\n\
          \n\t</span></span>'s q4_K_M GGUF: <a href=\"https://huggingface.co/augmxnt/shisa-7b-v1-exl2-h6-4.63bpw\"\
          >https://huggingface.co/augmxnt/shisa-7b-v1-exl2-h6-4.63bpw</a></p>\n"
        raw: 'I noted while clicking around that @LoneStriker made some EXL2 quants
          (thanks!):

          * https://huggingface.co/LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2

          * https://huggingface.co/LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2

          * https://huggingface.co/LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2

          * https://huggingface.co/LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2

          * https://huggingface.co/LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2


          Note, also of interest, while I was doing inference benchmarking, I also
          created an H6 4.63BPW quant to match the BPW of @mmnga''s q4_K_M GGUF: https://huggingface.co/augmxnt/shisa-7b-v1-exl2-h6-4.63bpw'
        updatedAt: '2023-12-12T06:58:36.119Z'
      numEdits: 0
      reactions: []
    id: 6578049c3ceeb2f078fb7f84
    type: comment
  author: leonardlin
  content: 'I noted while clicking around that @LoneStriker made some EXL2 quants
    (thanks!):

    * https://huggingface.co/LoneStriker/shisa-7b-v1-8.0bpw-h8-exl2

    * https://huggingface.co/LoneStriker/shisa-7b-v1-6.0bpw-h6-exl2

    * https://huggingface.co/LoneStriker/shisa-7b-v1-5.0bpw-h6-exl2

    * https://huggingface.co/LoneStriker/shisa-7b-v1-4.0bpw-h6-exl2

    * https://huggingface.co/LoneStriker/shisa-7b-v1-3.0bpw-h6-exl2


    Note, also of interest, while I was doing inference benchmarking, I also created
    an H6 4.63BPW quant to match the BPW of @mmnga''s q4_K_M GGUF: https://huggingface.co/augmxnt/shisa-7b-v1-exl2-h6-4.63bpw'
  created_at: 2023-12-12 06:58:36+00:00
  edited: false
  hidden: false
  id: 6578049c3ceeb2f078fb7f84
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: augmxnt/shisa-7b-v1
repo_type: model
status: open
target_branch: null
title: Quants
