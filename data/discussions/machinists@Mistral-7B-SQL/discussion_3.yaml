!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dnovak232
conflicting_files: null
created_at: 2024-01-08 19:18:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/92dafa71e2e1be5c2296df0e3e8945fa.svg
      fullname: Dino Novak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dnovak232
      type: user
    createdAt: '2024-01-08T19:18:40.000Z'
    data:
      edited: false
      editors:
      - dnovak232
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7047992944717407
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/92dafa71e2e1be5c2296df0e3e8945fa.svg
          fullname: Dino Novak
          isHf: false
          isPro: false
          name: dnovak232
          type: user
        html: '<p>HI,</p>

          <p>I would like to train Mistral 7B Instruct with text-to-sql support for
          Microsoft SQL Server.<br>I have created a custom training set <a href="https://huggingface.co/datasets/dnovak232/sql_create_context-v4-mssql-instruct">https://huggingface.co/datasets/dnovak232/sql_create_context-v4-mssql-instruct</a></p>

          <p>But when training the model with Lora nd on 3000 iterations created model
          does not produce any code,<br>it simply aborts request after some time.<br>Can
          you please advise how to fix either prompt template in dataset or whatever
          I am doing wrong.</p>

          <p>Here is output of vLLM:<br>INFO 01-08 19:11:50 async_llm_engine.py:383]
          Received request cmpl-cc01502bb6a34de7ab8d34362b71105c: prompt: ''<s>[INST]  Generate
          a correct SQL query from the following database schema. \n CREATE TABLE
          head (age INTEGER)  \nHow many heads of the departments are older than 56
          ? [/INST]'', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0,
          frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=1.0,
          top_k=-1, min_p=0.0, use_beam_search=False, length_penalty=1.0, early_stopping=False,
          stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False,
          max_tokens=15951, logprobs=None, prompt_logprobs=None, skip_special_tokens=True,
          spaces_between_special_tokens=True), prompt token ids: [1, 1, 733, 16289,
          28793, 28705, 26075, 264, 4714, 13208, 5709, 477, 272, 2296, 7499, 12493,
          28723, 28705, 13, 334, 896, 3148, 23740, 1335, 325, 465, 2693, 3392, 11832,
          28731, 259, 13, 5660, 1287, 10478, 302, 272, 25626, 460, 6402, 821, 28705,
          28782, 28784, 1550, 733, 28748, 16289, 28793].<br>INFO 01-08 19:11:50 llm_engine.py:706]
          Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.5 tokens/s,
          Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%,
          CPU KV cache usage: 0.0%<br>INFO 01-08 19:11:55 llm_engine.py:706] Avg prompt
          throughput: 0.0 tokens/s, Avg generation throughput: 47.6 tokens/s, Running:
          1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU
          KV cache usage: 0.0%<br>INFO 01-08 19:12:00 llm_engine.py:706] Avg prompt
          throughput: 0.0 tokens/s, Avg generation throughput: 46.6 tokens/s, Running:
          1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU
          KV cache usage: 0.0%<br>INFO 01-08 19:12:05 llm_engine.py:706] Avg prompt
          throughput: 0.0 tokens/s, Avg generation throughput: 46.1 tokens/s, Running:
          1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU
          KV cache usage: 0.0%<br>INFO 01-08 19:12:10 llm_engine.py:706] Avg prompt
          throughput: 0.0 tokens/s, Avg generation throughput: 46.4 tokens/s, Running:
          1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU
          KV cache usage: 0.0%<br>INFO 01-08 19:12:15 llm_engine.py:706] Avg prompt
          throughput: 0.0 tokens/s, Avg generation throughput: 46.2 tokens/s, Running:
          1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU
          KV cache usage: 0.0%<br>INFO 01-08 19:12:20 llm_engine.py:706] Avg prompt
          throughput: 0.0 tokens/s, Avg generation throughput: 46.0 tokens/s, Running:
          1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU
          KV cache usage: 0.0%<br>INFO 01-08 19:12:25 llm_engine.py:706] Avg prompt
          throughput: 0.0 tokens/s, Avg generation throughput: 45.6 tokens/s, Running:
          1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU
          KV cache usage: 0.0%<br>INFO 01-08 19:12:30 llm_engine.py:706] Avg prompt
          throughput: 0.0 tokens/s, Avg generation throughput: 45.9 tokens/s, Running:
          1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.4%, CPU
          KV cache usage: 0.0%<br>INFO 01-08 19:12:35 llm_engine.py:706] Avg prompt
          throughput: 0.0 tokens/s, Avg generation throughput: 45.9 tokens/s, Running:
          1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU
          KV cache usage: 0.0%<br>INFO 01-08 19:12:40 llm_engine.py:706] Avg prompt
          throughput: 0.0 tokens/s, Avg generation throughput: 46.6 tokens/s, Running:
          1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU
          KV cache usage: 0.0%<br>INFO 01-08 19:12:45 llm_engine.py:706] Avg prompt
          throughput: 0.0 tokens/s, Avg generation throughput: 45.8 tokens/s, Running:
          1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU
          KV cache usage: 0.0%<br>INFO 01-08 19:12:49 async_llm_engine.py:134] Aborted
          request cmpl-cc01502bb6a34de7ab8d34362b71105c. </s></p><s>

          </s>'
        raw: "HI,\r\n\r\nI would like to train Mistral 7B Instruct with text-to-sql\
          \ support for Microsoft SQL Server.\r\nI have created a custom training\
          \ set https://huggingface.co/datasets/dnovak232/sql_create_context-v4-mssql-instruct\r\
          \n\r\nBut when training the model with Lora nd on 3000 iterations created\
          \ model does not produce any code,\r\nit simply aborts request after some\
          \ time.\r\nCan you please advise how to fix either prompt template in dataset\
          \ or whatever I am doing wrong.\r\n\r\nHere is output of vLLM:\r\nINFO 01-08\
          \ 19:11:50 async_llm_engine.py:383] Received request cmpl-cc01502bb6a34de7ab8d34362b71105c:\
          \ prompt: '<s>[INST]  Generate a correct SQL query from the following database\
          \ schema. \\n CREATE TABLE head (age INTEGER)  \\nHow many heads of the\
          \ departments are older than 56 ? [/INST]', sampling params: SamplingParams(n=1,\
          \ best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0,\
          \ temperature=0.1, top_p=1.0, top_k=-1, min_p=0.0, use_beam_search=False,\
          \ length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[],\
          \ include_stop_str_in_output=False, ignore_eos=False, max_tokens=15951,\
          \ logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True),\
          \ prompt token ids: [1, 1, 733, 16289, 28793, 28705, 26075, 264, 4714, 13208,\
          \ 5709, 477, 272, 2296, 7499, 12493, 28723, 28705, 13, 334, 896, 3148, 23740,\
          \ 1335, 325, 465, 2693, 3392, 11832, 28731, 259, 13, 5660, 1287, 10478,\
          \ 302, 272, 25626, 460, 6402, 821, 28705, 28782, 28784, 1550, 733, 28748,\
          \ 16289, 28793].\r\nINFO 01-08 19:11:50 llm_engine.py:706] Avg prompt throughput:\
          \ 0.0 tokens/s, Avg generation throughput: 27.5 tokens/s, Running: 1 reqs,\
          \ Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache\
          \ usage: 0.0%\r\nINFO 01-08 19:11:55 llm_engine.py:706] Avg prompt throughput:\
          \ 0.0 tokens/s, Avg generation throughput: 47.6 tokens/s, Running: 1 reqs,\
          \ Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache\
          \ usage: 0.0%\r\nINFO 01-08 19:12:00 llm_engine.py:706] Avg prompt throughput:\
          \ 0.0 tokens/s, Avg generation throughput: 46.6 tokens/s, Running: 1 reqs,\
          \ Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache\
          \ usage: 0.0%\r\nINFO 01-08 19:12:05 llm_engine.py:706] Avg prompt throughput:\
          \ 0.0 tokens/s, Avg generation throughput: 46.1 tokens/s, Running: 1 reqs,\
          \ Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache\
          \ usage: 0.0%\r\nINFO 01-08 19:12:10 llm_engine.py:706] Avg prompt throughput:\
          \ 0.0 tokens/s, Avg generation throughput: 46.4 tokens/s, Running: 1 reqs,\
          \ Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache\
          \ usage: 0.0%\r\nINFO 01-08 19:12:15 llm_engine.py:706] Avg prompt throughput:\
          \ 0.0 tokens/s, Avg generation throughput: 46.2 tokens/s, Running: 1 reqs,\
          \ Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache\
          \ usage: 0.0%\r\nINFO 01-08 19:12:20 llm_engine.py:706] Avg prompt throughput:\
          \ 0.0 tokens/s, Avg generation throughput: 46.0 tokens/s, Running: 1 reqs,\
          \ Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache\
          \ usage: 0.0%\r\nINFO 01-08 19:12:25 llm_engine.py:706] Avg prompt throughput:\
          \ 0.0 tokens/s, Avg generation throughput: 45.6 tokens/s, Running: 1 reqs,\
          \ Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache\
          \ usage: 0.0%\r\nINFO 01-08 19:12:30 llm_engine.py:706] Avg prompt throughput:\
          \ 0.0 tokens/s, Avg generation throughput: 45.9 tokens/s, Running: 1 reqs,\
          \ Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.4%, CPU KV cache\
          \ usage: 0.0%\r\nINFO 01-08 19:12:35 llm_engine.py:706] Avg prompt throughput:\
          \ 0.0 tokens/s, Avg generation throughput: 45.9 tokens/s, Running: 1 reqs,\
          \ Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%, CPU KV cache\
          \ usage: 0.0%\r\nINFO 01-08 19:12:40 llm_engine.py:706] Avg prompt throughput:\
          \ 0.0 tokens/s, Avg generation throughput: 46.6 tokens/s, Running: 1 reqs,\
          \ Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache\
          \ usage: 0.0%\r\nINFO 01-08 19:12:45 llm_engine.py:706] Avg prompt throughput:\
          \ 0.0 tokens/s, Avg generation throughput: 45.8 tokens/s, Running: 1 reqs,\
          \ Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache\
          \ usage: 0.0%\r\nINFO 01-08 19:12:49 async_llm_engine.py:134] Aborted request\
          \ cmpl-cc01502bb6a34de7ab8d34362b71105c. "
        updatedAt: '2024-01-08T19:18:40.641Z'
      numEdits: 0
      reactions: []
    id: 659c4a9093d383899cd50731
    type: comment
  author: dnovak232
  content: "HI,\r\n\r\nI would like to train Mistral 7B Instruct with text-to-sql\
    \ support for Microsoft SQL Server.\r\nI have created a custom training set https://huggingface.co/datasets/dnovak232/sql_create_context-v4-mssql-instruct\r\
    \n\r\nBut when training the model with Lora nd on 3000 iterations created model\
    \ does not produce any code,\r\nit simply aborts request after some time.\r\n\
    Can you please advise how to fix either prompt template in dataset or whatever\
    \ I am doing wrong.\r\n\r\nHere is output of vLLM:\r\nINFO 01-08 19:11:50 async_llm_engine.py:383]\
    \ Received request cmpl-cc01502bb6a34de7ab8d34362b71105c: prompt: '<s>[INST] \
    \ Generate a correct SQL query from the following database schema. \\n CREATE\
    \ TABLE head (age INTEGER)  \\nHow many heads of the departments are older than\
    \ 56 ? [/INST]', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0,\
    \ frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=1.0, top_k=-1,\
    \ min_p=0.0, use_beam_search=False, length_penalty=1.0, early_stopping=False,\
    \ stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False,\
    \ max_tokens=15951, logprobs=None, prompt_logprobs=None, skip_special_tokens=True,\
    \ spaces_between_special_tokens=True), prompt token ids: [1, 1, 733, 16289, 28793,\
    \ 28705, 26075, 264, 4714, 13208, 5709, 477, 272, 2296, 7499, 12493, 28723, 28705,\
    \ 13, 334, 896, 3148, 23740, 1335, 325, 465, 2693, 3392, 11832, 28731, 259, 13,\
    \ 5660, 1287, 10478, 302, 272, 25626, 460, 6402, 821, 28705, 28782, 28784, 1550,\
    \ 733, 28748, 16289, 28793].\r\nINFO 01-08 19:11:50 llm_engine.py:706] Avg prompt\
    \ throughput: 0.0 tokens/s, Avg generation throughput: 27.5 tokens/s, Running:\
    \ 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache\
    \ usage: 0.0%\r\nINFO 01-08 19:11:55 llm_engine.py:706] Avg prompt throughput:\
    \ 0.0 tokens/s, Avg generation throughput: 47.6 tokens/s, Running: 1 reqs, Swapped:\
    \ 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%\r\
    \nINFO 01-08 19:12:00 llm_engine.py:706] Avg prompt throughput: 0.0 tokens/s,\
    \ Avg generation throughput: 46.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs,\
    \ Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%\r\nINFO\
    \ 01-08 19:12:05 llm_engine.py:706] Avg prompt throughput: 0.0 tokens/s, Avg generation\
    \ throughput: 46.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs,\
    \ GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%\r\nINFO 01-08 19:12:10 llm_engine.py:706]\
    \ Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 46.4 tokens/s,\
    \ Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%,\
    \ CPU KV cache usage: 0.0%\r\nINFO 01-08 19:12:15 llm_engine.py:706] Avg prompt\
    \ throughput: 0.0 tokens/s, Avg generation throughput: 46.2 tokens/s, Running:\
    \ 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache\
    \ usage: 0.0%\r\nINFO 01-08 19:12:20 llm_engine.py:706] Avg prompt throughput:\
    \ 0.0 tokens/s, Avg generation throughput: 46.0 tokens/s, Running: 1 reqs, Swapped:\
    \ 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%\r\
    \nINFO 01-08 19:12:25 llm_engine.py:706] Avg prompt throughput: 0.0 tokens/s,\
    \ Avg generation throughput: 45.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs,\
    \ Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%\r\nINFO\
    \ 01-08 19:12:30 llm_engine.py:706] Avg prompt throughput: 0.0 tokens/s, Avg generation\
    \ throughput: 45.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs,\
    \ GPU KV cache usage: 4.4%, CPU KV cache usage: 0.0%\r\nINFO 01-08 19:12:35 llm_engine.py:706]\
    \ Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 45.9 tokens/s,\
    \ Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.9%,\
    \ CPU KV cache usage: 0.0%\r\nINFO 01-08 19:12:40 llm_engine.py:706] Avg prompt\
    \ throughput: 0.0 tokens/s, Avg generation throughput: 46.6 tokens/s, Running:\
    \ 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.4%, CPU KV cache\
    \ usage: 0.0%\r\nINFO 01-08 19:12:45 llm_engine.py:706] Avg prompt throughput:\
    \ 0.0 tokens/s, Avg generation throughput: 45.8 tokens/s, Running: 1 reqs, Swapped:\
    \ 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.9%, CPU KV cache usage: 0.0%\r\
    \nINFO 01-08 19:12:49 async_llm_engine.py:134] Aborted request cmpl-cc01502bb6a34de7ab8d34362b71105c. "
  created_at: 2024-01-08 19:18:40+00:00
  edited: false
  hidden: false
  id: 659c4a9093d383899cd50731
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: machinists/Mistral-7B-SQL
repo_type: model
status: open
target_branch: null
title: Training with mssql support
