!!python/object:huggingface_hub.community.DiscussionWithDetails
author: caveman1
conflicting_files: null
created_at: 2023-11-21 13:39:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cff82bdc54220922be764f5d4029f7ee.svg
      fullname: vl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: caveman1
      type: user
    createdAt: '2023-11-21T13:39:44.000Z'
    data:
      edited: false
      editors:
      - caveman1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3386169672012329
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cff82bdc54220922be764f5d4029f7ee.svg
          fullname: vl
          isHf: false
          isPro: false
          name: caveman1
          type: user
        html: "<h2 id=\"\u043F\u0440\u0438-\u0437\u0430\u043F\u0443\u0441\u043A\u0435\
          -\u0432-\u043A\u043E\u043B\u043B\u0430\u0431\u0435-\u0434\u0430-\u0438-\u043D\
          \u0435-\u0442\u043E\u043B\u044C\u043A\u043E-\u043F\u043E\u0445\u043E\u0436\
          \u0435-\u0447\u0442\u043E-\u0432\u0435\u0440\u0441\u0438\u0438-\u043F\u0430\
          \u043A\u0435\u0442\u043E\u0432-\u0443\u0448\u043B\u0438-\u0432\u043F\u0435\
          \u0440\u0451\u0434\">\u043F\u0440\u0438 \u0437\u0430\u043F\u0443\u0441\u043A\
          \u0435 \u0432 \u043A\u043E\u043B\u043B\u0430\u0431\u0435, \u0434\u0430 \u0438\
          \ \u043D\u0435 \u0442\u043E\u043B\u044C\u043A\u043E, \u043F\u043E\u0445\u043E\
          \u0436\u0435, \u0447\u0442\u043E \u0432\u0435\u0440\u0441\u0438\u0438 \u043F\
          \u0430\u043A\u0435\u0442\u043E\u0432 \u0443\u0448\u043B\u0438 \u0432\u043F\
          \u0435\u0440\u0451\u0434....</h2>\n<p>AttributeError                   \
          \         Traceback (most recent call last)<br> in &lt;cell line: 4&gt;()<br>\
          \      2<br>      3 model_name = \"IlyaGusev/ru-word-stress-transformer\"\
          <br>----&gt; 4 tokenizer = AutoTokenizer.from_pretrained(<br>      5   \
          \  model_name,<br>      6     trust_remote_code=True,</p>\n<p>6 frames<br>/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)<br>\
          \    753             if os.path.isdir(pretrained_model_name_or_path):<br>\
          \    754                 tokenizer_class.register_for_auto_class()<br>--&gt;\
          \ 755             return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)<br>    756         elif config_tokenizer_class is not\
          \ None:<br>    757             tokenizer_class = None</p>\n<p>/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, cache_dir, force_download,\
          \ local_files_only, token, revision, *init_inputs, **kwargs)<br>   2022\
          \                 logger.info(f\"loading file {file_path} from cache at\
          \ {resolved_vocab_files[file_id]}\")<br>   2023<br>-&gt; 2024         return\
          \ cls._from_pretrained(<br>   2025             resolved_vocab_files,<br>\
          \   2026             pretrained_model_name_or_path,</p>\n<p>/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\
          \ in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
          \ init_configuration, token, cache_dir, local_files_only, _commit_hash,\
          \ _is_local, *init_inputs, **kwargs)<br>   2254         # Instantiate the\
          \ tokenizer.<br>   2255         try:<br>-&gt; 2256             tokenizer\
          \ = cls(*init_inputs, **init_kwargs)<br>   2257         except OSError:<br>\
          \   2258             raise OSError(</p>\n<p>~/.cache/huggingface/modules/transformers_modules/IlyaGusev/ru-word-stress-transformer/bae83ddbb1ac2aa9295d24bbd111eaa7caf18cf5/char_tokenizer.py\
          \ in <strong>init</strong>(self, vocab_file, pad_token, unk_token, bos_token,\
          \ eos_token, do_lower_case, *args, **kwargs)<br>     31         **kwargs<br>\
          \     32     ):<br>---&gt; 33         super().<strong>init</strong>(<br>\
          \     34             pad_token=pad_token,<br>     35             unk_token=unk_token,</p>\n\
          <p>/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\
          \ in <strong>init</strong>(self, **kwargs)<br>    365         # 4. If some\
          \ of the special tokens are not part of the vocab, we add them, at the end.<br>\
          \    366         # the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES\
          \ following <code>tokenizers</code><br>--&gt; 367         self._add_tokens(<br>\
          \    368             [token for token in self.all_special_tokens_extended\
          \ if token not in self._added_tokens_encoder],<br>    369             special_tokens=True,</p>\n\
          <p>/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\
          \ in _add_tokens(self, new_tokens, special_tokens)<br>    465          \
          \   return added_tokens<br>    466         # TODO this is fairly slow to\
          \ improve!<br>--&gt; 467         current_vocab = self.get_vocab().copy()<br>\
          \    468         new_idx = len(current_vocab)  # only call this once, len\
          \ gives the last index + 1<br>    469         for token in new_tokens:</p>\n\
          <p>~/.cache/huggingface/modules/transformers_modules/IlyaGusev/ru-word-stress-transformer/bae83ddbb1ac2aa9295d24bbd111eaa7caf18cf5/char_tokenizer.py\
          \ in get_vocab(self)<br>     70<br>     71     def get_vocab(self):<br>---&gt;\
          \ 72         return self.vocab<br>     73<br>     74     def _convert_token_to_id(self,\
          \ token):</p>\n<p>AttributeError: 'CharTokenizer' object has no attribute\
          \ 'vocab'</p>\n"
        raw: "\u043F\u0440\u0438 \u0437\u0430\u043F\u0443\u0441\u043A\u0435 \u0432\
          \ \u043A\u043E\u043B\u043B\u0430\u0431\u0435, \u0434\u0430 \u0438 \u043D\
          \u0435 \u0442\u043E\u043B\u044C\u043A\u043E, \u043F\u043E\u0445\u043E\u0436\
          \u0435, \u0447\u0442\u043E \u0432\u0435\u0440\u0441\u0438\u0438 \u043F\u0430\
          \u043A\u0435\u0442\u043E\u0432 \u0443\u0448\u043B\u0438 \u0432\u043F\u0435\
          \u0440\u0451\u0434....\r\n---------------------------------------------------------------------------\r\
          \nAttributeError                            Traceback (most recent call\
          \ last)\r\n<ipython-input-2-92bd7470b39f> in <cell line: 4>()\r\n      2\
          \ \r\n      3 model_name = \"IlyaGusev/ru-word-stress-transformer\"\r\n\
          ----> 4 tokenizer = AutoTokenizer.from_pretrained(\r\n      5     model_name,\r\
          \n      6     trust_remote_code=True,\r\n\r\n6 frames\r\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\r\
          \n    753             if os.path.isdir(pretrained_model_name_or_path):\r\
          \n    754                 tokenizer_class.register_for_auto_class()\r\n\
          --> 755             return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    756         elif config_tokenizer_class is not\
          \ None:\r\n    757             tokenizer_class = None\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, cache_dir, force_download,\
          \ local_files_only, token, revision, *init_inputs, **kwargs)\r\n   2022\
          \                 logger.info(f\"loading file {file_path} from cache at\
          \ {resolved_vocab_files[file_id]}\")\r\n   2023 \r\n-> 2024         return\
          \ cls._from_pretrained(\r\n   2025             resolved_vocab_files,\r\n\
          \   2026             pretrained_model_name_or_path,\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\
          \ in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
          \ init_configuration, token, cache_dir, local_files_only, _commit_hash,\
          \ _is_local, *init_inputs, **kwargs)\r\n   2254         # Instantiate the\
          \ tokenizer.\r\n   2255         try:\r\n-> 2256             tokenizer =\
          \ cls(*init_inputs, **init_kwargs)\r\n   2257         except OSError:\r\n\
          \   2258             raise OSError(\r\n\r\n~/.cache/huggingface/modules/transformers_modules/IlyaGusev/ru-word-stress-transformer/bae83ddbb1ac2aa9295d24bbd111eaa7caf18cf5/char_tokenizer.py\
          \ in __init__(self, vocab_file, pad_token, unk_token, bos_token, eos_token,\
          \ do_lower_case, *args, **kwargs)\r\n     31         **kwargs\r\n     32\
          \     ):\r\n---> 33         super().__init__(\r\n     34             pad_token=pad_token,\r\
          \n     35             unk_token=unk_token,\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\
          \ in __init__(self, **kwargs)\r\n    365         # 4. If some of the special\
          \ tokens are not part of the vocab, we add them, at the end.\r\n    366\
          \         # the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES\
          \ following `tokenizers`\r\n--> 367         self._add_tokens(\r\n    368\
          \             [token for token in self.all_special_tokens_extended if token\
          \ not in self._added_tokens_encoder],\r\n    369             special_tokens=True,\r\
          \n\r\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\
          \ in _add_tokens(self, new_tokens, special_tokens)\r\n    465          \
          \   return added_tokens\r\n    466         # TODO this is fairly slow to\
          \ improve!\r\n--> 467         current_vocab = self.get_vocab().copy()\r\n\
          \    468         new_idx = len(current_vocab)  # only call this once, len\
          \ gives the last index + 1\r\n    469         for token in new_tokens:\r\
          \n\r\n~/.cache/huggingface/modules/transformers_modules/IlyaGusev/ru-word-stress-transformer/bae83ddbb1ac2aa9295d24bbd111eaa7caf18cf5/char_tokenizer.py\
          \ in get_vocab(self)\r\n     70 \r\n     71     def get_vocab(self):\r\n\
          ---> 72         return self.vocab\r\n     73 \r\n     74     def _convert_token_to_id(self,\
          \ token):\r\n\r\nAttributeError: 'CharTokenizer' object has no attribute\
          \ 'vocab'"
        updatedAt: '2023-11-21T13:39:44.892Z'
      numEdits: 0
      reactions: []
    id: 655cb3206a7612f0790bce61
    type: comment
  author: caveman1
  content: "\u043F\u0440\u0438 \u0437\u0430\u043F\u0443\u0441\u043A\u0435 \u0432 \u043A\
    \u043E\u043B\u043B\u0430\u0431\u0435, \u0434\u0430 \u0438 \u043D\u0435 \u0442\u043E\
    \u043B\u044C\u043A\u043E, \u043F\u043E\u0445\u043E\u0436\u0435, \u0447\u0442\u043E\
    \ \u0432\u0435\u0440\u0441\u0438\u0438 \u043F\u0430\u043A\u0435\u0442\u043E\u0432\
    \ \u0443\u0448\u043B\u0438 \u0432\u043F\u0435\u0440\u0451\u0434....\r\n---------------------------------------------------------------------------\r\
    \nAttributeError                            Traceback (most recent call last)\r\
    \n<ipython-input-2-92bd7470b39f> in <cell line: 4>()\r\n      2 \r\n      3 model_name\
    \ = \"IlyaGusev/ru-word-stress-transformer\"\r\n----> 4 tokenizer = AutoTokenizer.from_pretrained(\r\
    \n      5     model_name,\r\n      6     trust_remote_code=True,\r\n\r\n6 frames\r\
    \n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\r\n\
    \    753             if os.path.isdir(pretrained_model_name_or_path):\r\n    754\
    \                 tokenizer_class.register_for_auto_class()\r\n--> 755       \
    \      return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    756         elif config_tokenizer_class is not None:\r\n  \
    \  757             tokenizer_class = None\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, cache_dir, force_download,\
    \ local_files_only, token, revision, *init_inputs, **kwargs)\r\n   2022      \
    \           logger.info(f\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\
    )\r\n   2023 \r\n-> 2024         return cls._from_pretrained(\r\n   2025     \
    \        resolved_vocab_files,\r\n   2026             pretrained_model_name_or_path,\r\
    \n\r\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\
    \ in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local,\
    \ *init_inputs, **kwargs)\r\n   2254         # Instantiate the tokenizer.\r\n\
    \   2255         try:\r\n-> 2256             tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n   2257         except OSError:\r\n   2258             raise OSError(\r\n\r\n\
    ~/.cache/huggingface/modules/transformers_modules/IlyaGusev/ru-word-stress-transformer/bae83ddbb1ac2aa9295d24bbd111eaa7caf18cf5/char_tokenizer.py\
    \ in __init__(self, vocab_file, pad_token, unk_token, bos_token, eos_token, do_lower_case,\
    \ *args, **kwargs)\r\n     31         **kwargs\r\n     32     ):\r\n---> 33  \
    \       super().__init__(\r\n     34             pad_token=pad_token,\r\n    \
    \ 35             unk_token=unk_token,\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\
    \ in __init__(self, **kwargs)\r\n    365         # 4. If some of the special tokens\
    \ are not part of the vocab, we add them, at the end.\r\n    366         # the\
    \ order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`\r\
    \n--> 367         self._add_tokens(\r\n    368             [token for token in\
    \ self.all_special_tokens_extended if token not in self._added_tokens_encoder],\r\
    \n    369             special_tokens=True,\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\
    \ in _add_tokens(self, new_tokens, special_tokens)\r\n    465             return\
    \ added_tokens\r\n    466         # TODO this is fairly slow to improve!\r\n-->\
    \ 467         current_vocab = self.get_vocab().copy()\r\n    468         new_idx\
    \ = len(current_vocab)  # only call this once, len gives the last index + 1\r\n\
    \    469         for token in new_tokens:\r\n\r\n~/.cache/huggingface/modules/transformers_modules/IlyaGusev/ru-word-stress-transformer/bae83ddbb1ac2aa9295d24bbd111eaa7caf18cf5/char_tokenizer.py\
    \ in get_vocab(self)\r\n     70 \r\n     71     def get_vocab(self):\r\n---> 72\
    \         return self.vocab\r\n     73 \r\n     74     def _convert_token_to_id(self,\
    \ token):\r\n\r\nAttributeError: 'CharTokenizer' object has no attribute 'vocab'"
  created_at: 2023-11-21 13:39:44+00:00
  edited: false
  hidden: false
  id: 655cb3206a7612f0790bce61
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: IlyaGusev/ru-word-stress-transformer
repo_type: model
status: open
target_branch: null
title: 'AttributeError: ''CharTokenizer'' object has no attribute ''vocab'''
