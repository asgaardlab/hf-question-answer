!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Jeximo
conflicting_files: null
created_at: 2023-05-23 17:16:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64554c42273f649830277263/A7Gpv_D00XZwZyJyGdUZ_.jpeg?w=200&h=200&f=face
      fullname: Jack Jollimore
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jeximo
      type: user
    createdAt: '2023-05-23T18:16:55.000Z'
    data:
      edited: false
      editors:
      - Jeximo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64554c42273f649830277263/A7Gpv_D00XZwZyJyGdUZ_.jpeg?w=200&h=200&f=face
          fullname: Jack Jollimore
          isHf: false
          isPro: false
          name: Jeximo
          type: user
        html: '<p>Hi,</p>

          <p>I appreciate that you''ve converted to v3 and uploaded for us, just like
          you did with v2.</p>

          <p>Llama.cpp is roughly 2seconds per token slower on v3 for me which I didn''t
          expect.</p>

          <p>I''m requesting an upload or link Wizard-Vicuna-7B-Uncensored.ggmlv2.q4_0.bin
          if it''s possible.</p>

          <p>Thanks for your consideration.</p>

          '
        raw: "Hi,\r\n\r\nI appreciate that you've converted to v3 and uploaded for\
          \ us, just like you did with v2.\r\n\r\nLlama.cpp is roughly 2seconds per\
          \ token slower on v3 for me which I didn't expect.\r\n\r\nI'm requesting\
          \ an upload or link Wizard-Vicuna-7B-Uncensored.ggmlv2.q4_0.bin if it's\
          \ possible.\r\n\r\nThanks for your consideration."
        updatedAt: '2023-05-23T18:16:55.223Z'
      numEdits: 0
      reactions: []
    id: 646d03172e6b41386b495fb7
    type: comment
  author: Jeximo
  content: "Hi,\r\n\r\nI appreciate that you've converted to v3 and uploaded for us,\
    \ just like you did with v2.\r\n\r\nLlama.cpp is roughly 2seconds per token slower\
    \ on v3 for me which I didn't expect.\r\n\r\nI'm requesting an upload or link\
    \ Wizard-Vicuna-7B-Uncensored.ggmlv2.q4_0.bin if it's possible.\r\n\r\nThanks\
    \ for your consideration."
  created_at: 2023-05-23 17:16:55+00:00
  edited: false
  hidden: false
  id: 646d03172e6b41386b495fb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-23T18:42:33.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh that''s very surprising. On which quant type?</p>

          <p>I''ll look into making ggml v2s this evening</p>

          '
        raw: 'Oh that''s very surprising. On which quant type?


          I''ll look into making ggml v2s this evening'
        updatedAt: '2023-05-23T18:42:33.597Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Jeximo
    id: 646d09192abe5323fe16ae0a
    type: comment
  author: TheBloke
  content: 'Oh that''s very surprising. On which quant type?


    I''ll look into making ggml v2s this evening'
  created_at: 2023-05-23 17:42:33+00:00
  edited: false
  hidden: false
  id: 646d09192abe5323fe16ae0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64554c42273f649830277263/A7Gpv_D00XZwZyJyGdUZ_.jpeg?w=200&h=200&f=face
      fullname: Jack Jollimore
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jeximo
      type: user
    createdAt: '2023-05-23T18:52:59.000Z'
    data:
      edited: false
      editors:
      - Jeximo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64554c42273f649830277263/A7Gpv_D00XZwZyJyGdUZ_.jpeg?w=200&h=200&f=face
          fullname: Jack Jollimore
          isHf: false
          isPro: false
          name: Jeximo
          type: user
        html: '<blockquote>

          <p>Oh that''s very surprising. On which quant type?</p>

          <p>I''ll look into making ggml v2s this evening</p>

          </blockquote>

          <p>Thanks for your response.</p>

          <p>I use q4_0 and found the v3 of the same type is slower. I was surprised
          too so I made a post on llama.cpp hit raising the issue.</p>

          <p>(I''m guessing the quant type means q4_0, please correct me if I''m not
          referring to the correct thing)</p>

          <p>:)</p>

          '
        raw: "> Oh that's very surprising. On which quant type?\n> \n> I'll look into\
          \ making ggml v2s this evening\n\nThanks for your response.\n\nI use q4_0\
          \ and found the v3 of the same type is slower. I was surprised too so I\
          \ made a post on llama.cpp hit raising the issue.\n\n(I'm guessing the quant\
          \ type means q4_0, please correct me if I'm not referring to the correct\
          \ thing)\n\n:)"
        updatedAt: '2023-05-23T18:52:59.889Z'
      numEdits: 0
      reactions: []
    id: 646d0b8bacc13867a1311af1
    type: comment
  author: Jeximo
  content: "> Oh that's very surprising. On which quant type?\n> \n> I'll look into\
    \ making ggml v2s this evening\n\nThanks for your response.\n\nI use q4_0 and\
    \ found the v3 of the same type is slower. I was surprised too so I made a post\
    \ on llama.cpp hit raising the issue.\n\n(I'm guessing the quant type means q4_0,\
    \ please correct me if I'm not referring to the correct thing)\n\n:)"
  created_at: 2023-05-23 17:52:59+00:00
  edited: false
  hidden: false
  id: 646d0b8bacc13867a1311af1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-23T18:54:44.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh i just realised what model you were writing about. Ggml v2s are
          still available. Check branch previous_llama_ggmlv2</p>

          '
        raw: Oh i just realised what model you were writing about. Ggml v2s are still
          available. Check branch previous_llama_ggmlv2
        updatedAt: '2023-05-23T18:54:44.338Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Jeximo
    id: 646d0bf4acc13867a1312b44
    type: comment
  author: TheBloke
  content: Oh i just realised what model you were writing about. Ggml v2s are still
    available. Check branch previous_llama_ggmlv2
  created_at: 2023-05-23 17:54:44+00:00
  edited: false
  hidden: false
  id: 646d0bf4acc13867a1312b44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-23T18:55:19.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes quant type is q4_0 etc. How odd, I thought v3 was meant to be
          quicker</p>

          '
        raw: Yes quant type is q4_0 etc. How odd, I thought v3 was meant to be quicker
        updatedAt: '2023-05-23T18:55:19.582Z'
      numEdits: 0
      reactions: []
    id: 646d0c17e0c5e3957351ed03
    type: comment
  author: TheBloke
  content: Yes quant type is q4_0 etc. How odd, I thought v3 was meant to be quicker
  created_at: 2023-05-23 17:55:19+00:00
  edited: false
  hidden: false
  id: 646d0c17e0c5e3957351ed03
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64554c42273f649830277263/A7Gpv_D00XZwZyJyGdUZ_.jpeg?w=200&h=200&f=face
      fullname: Jack Jollimore
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jeximo
      type: user
    createdAt: '2023-05-23T19:04:49.000Z'
    data:
      edited: false
      editors:
      - Jeximo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64554c42273f649830277263/A7Gpv_D00XZwZyJyGdUZ_.jpeg?w=200&h=200&f=face
          fullname: Jack Jollimore
          isHf: false
          isPro: false
          name: Jeximo
          type: user
        html: '<blockquote>

          <p>Yes quant type is q4_0 etc. How odd, I thought v3 was meant to be quicker</p>

          </blockquote>

          <p>Oh, there they are! (<a href="https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGML/tree/previous_llama_ggmlv2">https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGML/tree/previous_llama_ggmlv2</a>)
          Thanks for pointing that out for me.</p>

          <p>Yeah, I can see the ram requirements for v3 q4_0 model has decreased,
          but my timings with v2 is better.</p>

          <p>I hope to get both benefits, y''know? :)</p>

          <p>Thanks again!</p>

          '
        raw: '> Yes quant type is q4_0 etc. How odd, I thought v3 was meant to be
          quicker


          Oh, there they are! (https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGML/tree/previous_llama_ggmlv2)
          Thanks for pointing that out for me.


          Yeah, I can see the ram requirements for v3 q4_0 model has decreased, but
          my timings with v2 is better.


          I hope to get both benefits, y''know? :)


          Thanks again!'
        updatedAt: '2023-05-23T19:04:49.402Z'
      numEdits: 0
      reactions: []
    id: 646d0e514a2db7744379c896
    type: comment
  author: Jeximo
  content: '> Yes quant type is q4_0 etc. How odd, I thought v3 was meant to be quicker


    Oh, there they are! (https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGML/tree/previous_llama_ggmlv2)
    Thanks for pointing that out for me.


    Yeah, I can see the ram requirements for v3 q4_0 model has decreased, but my timings
    with v2 is better.


    I hope to get both benefits, y''know? :)


    Thanks again!'
  created_at: 2023-05-23 18:04:49+00:00
  edited: false
  hidden: false
  id: 646d0e514a2db7744379c896
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64554c42273f649830277263/A7Gpv_D00XZwZyJyGdUZ_.jpeg?w=200&h=200&f=face
      fullname: Jack Jollimore
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jeximo
      type: user
    createdAt: '2023-05-23T19:04:56.000Z'
    data:
      status: closed
    id: 646d0e582abe5323fe177e99
    type: status-change
  author: Jeximo
  created_at: 2023-05-23 18:04:56+00:00
  id: 646d0e582abe5323fe177e99
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Wizard-Vicuna-7B-Uncensored-GGML
repo_type: model
status: closed
target_branch: null
title: Ggml v2 Request
