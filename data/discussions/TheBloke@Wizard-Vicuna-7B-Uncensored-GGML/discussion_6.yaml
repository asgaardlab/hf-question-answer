!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Omniscience
conflicting_files: null
created_at: 2023-07-04 12:45:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66967d48e167915926b2510d1a210d8e.svg
      fullname: Omniscience
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Omniscience
      type: user
    createdAt: '2023-07-04T13:45:30.000Z'
    data:
      edited: true
      editors:
      - Omniscience
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9592915773391724
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66967d48e167915926b2510d1a210d8e.svg
          fullname: Omniscience
          isHf: false
          isPro: false
          name: Omniscience
          type: user
        html: '<p>Hey thanks for making these models for the community.</p>

          <p>Question about the tokenizer in these files (as well as all the other
          Wizard-Vicuna-Uncensored ones I''ve looked at)<br>First I noticed it tokenizes
          <code>&lt;s&gt;</code> <code>&lt;/s&gt;</code> and <code>[PAD]</code> as
          multiple tokens each during prompt parsing instead of 1 token as expected.<br>Then
          I opened the GGML in a hex editor and found those 3 strings aren''t in the
          vocabulary table at the top of the file.<br>Then I tracked down the upstream
          model <a href="https://huggingface.co/ehartford/Wizard-Vicuna-7B-Uncensored/blob/main/tokenizer.json">https://huggingface.co/ehartford/Wizard-Vicuna-7B-Uncensored/blob/main/tokenizer.json</a><br>It
          looks like the tokens ARE in there, as well as in the tokenizer_config.json.</p>

          <p>Is there any reason for this? Is it possible your conversion missed these
          token strings somehow? I''d really like to be able to encode the BOS/EOS
          to break up my prompt instead of relying on \n or something, so the model
          knows it can use EOS in the right place, and is free to use \n in the body
          of its responses.</p>

          <p>For reference I''m using koboldcpp-v1.33''s REST API<br>And I''m trying
          to replicate the prompt format found here for Vicuna v1.1:<br><a rel="nofollow"
          href="https://github.com/lm-sys/FastChat/blob/7ae721fa3c881e1e24cf181305d127a316acd463/docs/vicuna_weights_version.md#example-prompt-weight-v11">https://github.com/lm-sys/FastChat/blob/7ae721fa3c881e1e24cf181305d127a316acd463/docs/vicuna_weights_version.md#example-prompt-weight-v11</a></p>

          <pre><code>USER: Hello!

          ASSISTANT: Hello!&lt;/s&gt;

          USER: How are you?

          ASSISTANT: I am good.&lt;/s&gt;

          </code></pre>

          '
        raw: 'Hey thanks for making these models for the community.


          Question about the tokenizer in these files (as well as all the other Wizard-Vicuna-Uncensored
          ones I''ve looked at)

          First I noticed it tokenizes `<s>` `</s>` and `[PAD]` as multiple tokens
          each during prompt parsing instead of 1 token as expected.

          Then I opened the GGML in a hex editor and found those 3 strings aren''t
          in the vocabulary table at the top of the file.

          Then I tracked down the upstream model https://huggingface.co/ehartford/Wizard-Vicuna-7B-Uncensored/blob/main/tokenizer.json

          It looks like the tokens ARE in there, as well as in the tokenizer_config.json.


          Is there any reason for this? Is it possible your conversion missed these
          token strings somehow? I''d really like to be able to encode the BOS/EOS
          to break up my prompt instead of relying on \n or something, so the model
          knows it can use EOS in the right place, and is free to use \n in the body
          of its responses.


          For reference I''m using koboldcpp-v1.33''s REST API

          And I''m trying to replicate the prompt format found here for Vicuna v1.1:

          https://github.com/lm-sys/FastChat/blob/7ae721fa3c881e1e24cf181305d127a316acd463/docs/vicuna_weights_version.md#example-prompt-weight-v11

          ```

          USER: Hello!

          ASSISTANT: Hello!</s>

          USER: How are you?

          ASSISTANT: I am good.</s>

          ```'
        updatedAt: '2023-07-04T14:07:56.272Z'
      numEdits: 3
      reactions: []
    id: 64a4227a492a7aeb73339fc6
    type: comment
  author: Omniscience
  content: 'Hey thanks for making these models for the community.


    Question about the tokenizer in these files (as well as all the other Wizard-Vicuna-Uncensored
    ones I''ve looked at)

    First I noticed it tokenizes `<s>` `</s>` and `[PAD]` as multiple tokens each
    during prompt parsing instead of 1 token as expected.

    Then I opened the GGML in a hex editor and found those 3 strings aren''t in the
    vocabulary table at the top of the file.

    Then I tracked down the upstream model https://huggingface.co/ehartford/Wizard-Vicuna-7B-Uncensored/blob/main/tokenizer.json

    It looks like the tokens ARE in there, as well as in the tokenizer_config.json.


    Is there any reason for this? Is it possible your conversion missed these token
    strings somehow? I''d really like to be able to encode the BOS/EOS to break up
    my prompt instead of relying on \n or something, so the model knows it can use
    EOS in the right place, and is free to use \n in the body of its responses.


    For reference I''m using koboldcpp-v1.33''s REST API

    And I''m trying to replicate the prompt format found here for Vicuna v1.1:

    https://github.com/lm-sys/FastChat/blob/7ae721fa3c881e1e24cf181305d127a316acd463/docs/vicuna_weights_version.md#example-prompt-weight-v11

    ```

    USER: Hello!

    ASSISTANT: Hello!</s>

    USER: How are you?

    ASSISTANT: I am good.</s>

    ```'
  created_at: 2023-07-04 12:45:30+00:00
  edited: true
  hidden: false
  id: 64a4227a492a7aeb73339fc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-07T13:17:35.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9851316809654236
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>No, nothing was missed, that''s how GGML is at the moment. It doesn''t
          support special tokens, so they are tokenised as if they were normal text.  They
          should still work, but yes they''re not treated as a single token like they
          would be in Python.</p>

          <p>A PR was opened three weeks to add initial support for special tokens
          in Llama GGML, but it hasn''t been merged yet: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/1931">https://github.com/ggerganov/llama.cpp/pull/1931</a></p>

          <p>Looks like it''s stalled, pending implementation of a new file format
          for GGML, called GGUF.  So it''s unknown when that support will come, but
          I expect it will happen eventually.</p>

          '
        raw: 'No, nothing was missed, that''s how GGML is at the moment. It doesn''t
          support special tokens, so they are tokenised as if they were normal text.  They
          should still work, but yes they''re not treated as a single token like they
          would be in Python.


          A PR was opened three weeks to add initial support for special tokens in
          Llama GGML, but it hasn''t been merged yet: https://github.com/ggerganov/llama.cpp/pull/1931


          Looks like it''s stalled, pending implementation of a new file format for
          GGML, called GGUF.  So it''s unknown when that support will come, but I
          expect it will happen eventually.'
        updatedAt: '2023-07-07T13:17:35.547Z'
      numEdits: 0
      reactions: []
    id: 64a8106f18ff0e67bd4966e2
    type: comment
  author: TheBloke
  content: 'No, nothing was missed, that''s how GGML is at the moment. It doesn''t
    support special tokens, so they are tokenised as if they were normal text.  They
    should still work, but yes they''re not treated as a single token like they would
    be in Python.


    A PR was opened three weeks to add initial support for special tokens in Llama
    GGML, but it hasn''t been merged yet: https://github.com/ggerganov/llama.cpp/pull/1931


    Looks like it''s stalled, pending implementation of a new file format for GGML,
    called GGUF.  So it''s unknown when that support will come, but I expect it will
    happen eventually.'
  created_at: 2023-07-07 12:17:35+00:00
  edited: false
  hidden: false
  id: 64a8106f18ff0e67bd4966e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66967d48e167915926b2510d1a210d8e.svg
      fullname: Omniscience
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Omniscience
      type: user
    createdAt: '2023-07-07T14:05:26.000Z'
    data:
      edited: false
      editors:
      - Omniscience
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9861611723899841
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66967d48e167915926b2510d1a210d8e.svg
          fullname: Omniscience
          isHf: false
          isPro: false
          name: Omniscience
          type: user
        html: '<p>Ah, I saw that big GGUF thread, but I also saw an &lt;!endoftext&gt;
          in the vocab table of some other random DistilGPT-2 GGML I found so I figured
          GGML might still support it after all. It''s likely that model was created
          a long time ago though, so maybe it just didn''t follow the same standards.</p>

          <p>GGUF can''t come soon enough <em>sigh</em></p>

          '
        raw: 'Ah, I saw that big GGUF thread, but I also saw an <!endoftext> in the
          vocab table of some other random DistilGPT-2 GGML I found so I figured GGML
          might still support it after all. It''s likely that model was created a
          long time ago though, so maybe it just didn''t follow the same standards.


          GGUF can''t come soon enough *sigh*'
        updatedAt: '2023-07-07T14:05:26.226Z'
      numEdits: 0
      reactions: []
    id: 64a81ba6314f5098837a1978
    type: comment
  author: Omniscience
  content: 'Ah, I saw that big GGUF thread, but I also saw an <!endoftext> in the
    vocab table of some other random DistilGPT-2 GGML I found so I figured GGML might
    still support it after all. It''s likely that model was created a long time ago
    though, so maybe it just didn''t follow the same standards.


    GGUF can''t come soon enough *sigh*'
  created_at: 2023-07-07 13:05:26+00:00
  edited: false
  hidden: false
  id: 64a81ba6314f5098837a1978
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/66967d48e167915926b2510d1a210d8e.svg
      fullname: Omniscience
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Omniscience
      type: user
    createdAt: '2023-07-07T14:05:50.000Z'
    data:
      status: closed
    id: 64a81bbeed54ffdf6148e5cd
    type: status-change
  author: Omniscience
  created_at: 2023-07-07 13:05:50+00:00
  id: 64a81bbeed54ffdf6148e5cd
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/Wizard-Vicuna-7B-Uncensored-GGML
repo_type: model
status: closed
target_branch: null
title: GGML Vocabulary
